Thank youender
million
that is going
Technical question.
We can edit the video later.
Let's start very lighthearted then.
Maybe a shell of hands.
Who here has heard of mutation testing?
Amazing.
I can go very quickly to some flights then.
Who has never heard of mutation testing?
For who is the completely new concept?
I will cover it for you guys.
That's nice.
Of course I'm here, but I'd like to promote Striker a little bit.
Who of you is actually using Striker already?
Nobody. One person.
Well, that's my colleague. He's also working on it.
Cool. You guys are definitely going to learn some stuff today then.
I can just start right?
Sure.
Welcome everybody to the talk,
How Mutation Testing Got Practical?
I'm really focusing on the Got Practical in this talk.
I will be explaining mutation testing a bit,
but I'm really looking deep into the internals
and why is the idea really old?
It's a practical use case, relatively new.
So I'll get into that.
But first, a little introduction.
My name is Jan Dele Kester.
I'm a self-engineering consultant at InfoSupport.
It's a consultancy organization in the Netherlands.
I'm also a trainer there, and I'm a research supervisor.
And that last one is very relevant today.
You'll hear why soon.
If you want to contact me afterwards,
you can use the link then, I guess,
but you can also find me on GitHub.
And as I said, I'm here on behalf of Striker.
Striker is a mutation testing framework
for JavaScript and TypeScripts,
C-Sharp, Scala, and hopefully at some point, Kotlin.
There's like a partial implementation there already.
We're working hard on it.
You can find us at StrikerMutator.io,
and of course, I have all these nice socks for you guys.
So if you're really good with asking questions
and reacting to my questions, you might get some.
And otherwise, you'll see us after. We'll have some more.
So in this talk, in this next 25 minutes,
because I am going to try to leave room for questions as well,
I first want to talk about why do we actually need
to understand our tests?
Why is just writing a test not good enough?
I also want to go into what mutation testing is
for the people that don't know yet.
And finally, and that's the major part,
hopefully if I don't run out of time,
I'm going to go deep in how we got to practical applicability,
how we got into this state now
where we can actually run mutation testing in our real projects.
And that means talking about some state of art,
state of art performance improvements.
But first, we have to talk about the false sense of security.
Because this is a promotional image I copied shamelessly
from the Sonacube website.
And they show this nice dashboard where they say,
well, everything is good.
There's no issues, no bugs, it's all fine.
And there's even 76% test coverage.
Who would be happy with that?
Okay, who wants to say why?
Why is 76% good according to you?
Lots of green.
Lots of green.
Larger small socks.
I don't wear small socks.
You get some anyway.
Sorry about that. They're hard to throw in this room.
I would say I would not be happy with that.
Because, I mean, our tests,
apparently when we're running our tests,
we reach 76% of our code.
I don't think that's enough.
Because there is more than 20% of our code
that is not even getting executed for doing the test.
That's a problem.
But even 100% code coverage is actually,
it doesn't say much.
Because coverage only means that code is being executed.
We are only testing, in a worst-case scenario,
that the program does not crash.
What you actually want to know
is if our tests do something,
and I can very easily get 100% code coverage on the program
without writing any assertions,
that I'm just checking that the test execution does not crash.
So we need a way of testing our tests.
And no, we're not going to write unit tests
to test our test logic, because that would be stupid.
I mean, it would never end.
We need to be smart about that.
And that's where mutation testing comes in.
So what we're going to do is to introduce changes
in production code automatically.
That's a tool that's doing that.
We're going to test again to see whether the tests start failing.
Because when the tests start failing,
at that point, you know that your tests
are actually able to catch that bug
that we purposefully introduced.
So it's also from a white box testing,
because we really have to know stuff about the internals of the code
to change it, and see whether the tests are good enough to catch that.
And this is really not a new idea,
because there's this nice paper from 79 already,
where they talk about a new way of a new type of software test.
And you can actually find this on Google,
and you can read it if you want.
But even back then, 40 years ago, 45 years ago,
they were already talking about it.
But only recently, and I talk recently in a very broad sense here,
because I wasn't even in high school, I think,
when I'm talking about reasons.
Then it got more traction, because the problem is,
in the 70s, or the late 70s, it was just a good idea.
We did not have the resources to actually apply it in practice.
And what you see here, the dark-colored bars are publications,
research publications about practical applicability,
and they really spike early this millennium.
And there are reasons for that.
Mostly also, I think, because our computers got fast enough.
And why that is important is because of how mutation testing works,
what the process behind it is.
Because we start with our source code,
and we are feeling very happy about it, of course,
because we made all this nice code, we even wrote tests for it,
so we're very confident.
And what the tool is then doing is going to introduce mutants in your code.
And mutants are just changes.
And for every change that is made, the tests are executed again.
And we can have two results.
Either the tests start failing, which in this case is good,
because we then detected that mutant, we found the bug,
so we say that mutant is killed, or the mutant survived.
And that means that your test is not complete.
And when you do that for everything, in the end,
we get a nice report out of it, like a covers report,
except a bit more detail.
And how that process actually works is that there are operators for that.
And an operator is basically a transformation.
Given a certain thing in your code,
what kind of changes can we do that might fail your tests?
And some examples are here.
There are way more, and also one operator on the left could result into,
or one source code, original source code on the left,
could result into multiple mutants.
But you could do, like, just switch the operators,
or throw away a whole block of code.
And when we do that for every of these mutants,
we measure something, right?
So I already talked about killed and survived.
But that's only in an ideal scenario,
because in practice, there might be code that does not even reach by test.
So you can say, well, we have no coverage, or we have a timeout.
And timeout basically means that there are mutants caused by an infinite loop.
And we should consider that, okay, well, infinite loop tests actually failed then,
so that's maybe kind of killed.
But you can also get runtime errors, or compile errors,
because we're just introducing weird code changes
without looking at what the code is actually supposed to be doing.
And finally, mutants can also be ignored,
because that's what a developer said, I don't want a test for this.
So I don't want to see the report anymore.
So it's just like suppressing warnings for your code code dejects.
Who here does that very often?
I do, actually, but...
And then just like with the code coverage score,
we want to have a nice metric.
We want to know how good we are doing.
And for that, we can compute the score.
And we call that a striker-deorientation score.
And basically what we say here is we want to express in a nice number
on a scale of 0 to 100%.
How many mutants did you actually manage to kill?
So how many unexpected changes in your code are your tests actually catching?
And that's this nice formula, but basically what you do here
is everything above the line is what you consider that you killed.
And everything below the line, so we divide that by everything
that was actually a valid change.
So we exclude the crashes, for example.
And that gives you, for your whole program, for your whole code base,
an indication of how good your tests actually are.
But what if you don't have that many tests yet?
Well, they can also compute a variant of the mutation score.
We just look at the code that's actually being tested.
So you see here that we do not include the mutants without coverage anymore.
So one might think, just like with code coverage,
we should maybe strive for a high number.
We should maybe have 100% mutation coverage, 100% mutation score.
That would be nice.
But there we actually ran into a problem
because we cannot actually kill all the mutants.
It's very easy, well, relatively easy at least,
to reach all your codes, to make sure all your code is getting executed.
So basically to get 100% code coverage, that's relatively simple.
But because you're still calling functions from the outside,
you might not be able to test every single operation
happening inside of these functions.
And actually, some mutants are, if you split up your whole code base completely,
you can never kill them.
And what category of these is equivalent mutants?
Which is also a problem.
So given this code, we have a nice for loop,
and we say, well, we want to iterate 10 times.
You can also write like this, and it will still work.
So this mutant, we cannot kill
because even though we changed our code,
semantically it's doing the same thing.
So that's where you might want to ignore, basically.
And mutation testing is also very challenging.
That's actually where that practical application problem comes in,
because you can imagine that mutation testing,
basically changing your code and then running all the tests again,
doesn't take a lot of time.
And if you have a very large code base,
that might actually not finish in a reasonable time.
You also need a lot of configuration.
The mutation testing tool needs to know stuff.
It needs to know how it can run tests.
It needs to know how it can verify that those tests did complete successfully or not.
It also needs to know stuff about your programming language, for example,
in order to make sure that it rewrites the code in a correct way.
So that also needs to be a lot of tuning support to make it work.
And for a long time, mutation testing was simply not feasible or not easy to do.
But we're bracing the gap.
Not specifically at Striker.
A lot of stuff has already been done.
Luckily for us.
But when we're looking at performance, this is basically the worst-case scenario.
So the time it takes to run a single mutant, to analyze a single mutant,
is basically the time it takes to run on your test cases.
So we can approximate it by saying,
well, let's just count the number of tests that you have.
And then the time it takes to mutate your whole program is just the sum of that.
So it basically means that you multiply the number of test cases that you have in your code
times the number of mutants that you need to check,
because mutants need to be checked in isolation,
because they can influence each other.
So you can imagine already with a very small program,
this number, the time approximation, can get really, really, really large.
So we need to be smarter about that.
We want to make sure that the total time is a lot less,
not just a bit less, a lot less than just a multiplication.
And basically there are three approaches to get there.
It's either to do it faster.
And doing it faster, for example, I think we're going to paralyze it.
We're going to use love course.
Like take a big machine, nowadays it's relatively simple to get a machine with 128 cores,
so we can do 128 things at the same time.
You can try to do fewer.
So you can maybe try to make smart choices and say,
well, certain stuff we maybe don't need to analyze,
because we kind of know that it's probably fine.
Or you can try to do it smarter.
And the study I referenced here, they really did an analysis in there,
it's like a litigator review.
And most of the studies are actually focusing on fewer or smarter.
And common techniques there, some of us here,
I won't have time to go into detail on all of these.
But you can think of like random mutation,
like we're just randomly checking stuff sometimes.
We're just randomly picking some mutation that we're going to do.
But that's not deterministic.
So that might not give you the best knowledge about what the quality of the test actually is.
Parallel execution already mentioned.
You can also do stuff like data flow and control flow analysis
and try to reduce the set that way.
Or maybe look at AI to try to pick smarter sets of stuff that you're actually going to check.
But if you want to use that mutation score as a benchmark, as a comparison,
for example, with the pull request, did you actually improve it or not?
Or to give you a good indication of how good your tests currently actually are,
you actually need to execute everything.
So just the approach of we're just going to run less, that doesn't always work.
And one big way how this process can be sped up is by looking at how we actually change the code.
So a very new eave approach would, for example, be just changing the source code,
running the compiler again, running the test again,
and then making another change in the source code,
and then running the compiler again, running the test again.
And if you have a compiled language with a fairly slow compiler,
that's quite problematic because then it gets really, really slow.
A bit better might be bytecode mutation.
So, for example, the JVM languages have bytecodes, they have an intermediate step,
maybe you can mutate that.
And they only have to compile the source code once,
they just change the bytecode and then run it.
And while that is a lot faster, it's also a lot more complicated,
but it has one big downside, and that is that every change that is possible in the bytecode
is not necessarily something you can do in your source code,
especially for Java, for example.
If you write Kotlin or Scala, there's a lot of, a very simple thing in Scala, for example,
can result in a lot of bytecodes,
and if you're trying to mutate that with the assumption that the Java compiler compiled it,
it might come up with mutants that you did not kill,
but you cannot actually kill them because they don't exist in your source code.
So, who thinks, who has an idea, how we can, how we can do this smarter?
Any ideas? Yes.
Are you a step?
Sorry, what did you say?
You compile all the different mutants, the same as people, and you select which mutants you get from the other side.
Exactly. Large or small socks?
Large or small socks? You have socks with a logo on it.
Large.
This is the, the larger the other ones, right?
Sorry about that. Thank you, Niko.
For testing good device.
I didn't test my own system, no.
Silver written software to do this.
But, yeah, basically, so the answer that was given was basically this,
and we call that mutant schematic, and this just makes sure that you compile all the mutants into your code once,
and then use an environment variable to just switch them on or not.
So, if you do this, your compiled code is just full of if statements,
that is, check a certain number.
And that is complicated, but it is manageable, it's not that hard.
The main problem is in keeping track of it, but if you assign every mutant a unique number, it should be fine.
And this really helps with compiled languages, especially with stuff that's a bit slower, like Scala.
And this is actually relatively new.
In the world of mutation testing, this is relatively new.
It is from 1993, though, so it's the same age as me.
I wouldn't say that I am relatively new.
But, as something else, Martin, that you can do is coverage analysis.
That's also something that has been part of Striker for a long time already.
So we actually do an initial run where we just check which tests are reaching what code.
So we also know if you change one part of the code, which tests actually need to run and which don't.
So you can also get that number of test cases down a lot, depending on where you mutate the code.
Some codes you don't really know, so if you have something static, for example,
it's not extreme defined somewhere, that, you know, you might not be able to figure out how that is used.
You might still need to have to run the whole test suite, but you try not to.
Something else you can do is incremental analysis.
So just try to div some previously stored state and try to guess which mutants you actually need to check.
This is very hard to do, fully foolproof, fully complete, but you can get there like 99%.
And that means that if you make a small change, a small pull request,
that checking whether your changes are tested properly is relatively simple, is relatively fast.
So if, uh, Nico here in front, he gave it all yesterday in the JavaScript dev room,
and he actually showed this feature, and I think that there was a difference between a couple of seconds
and like 30 seconds and three seconds, something like that, on a small project.
Another cool thing is mutation levels, and that's where you actually give the user a choice.
Do you care about testing it fully, or do you care about performance?
And the choice that the user wants to make can depend on the type of project or the domain.
Do you have code where it's really important that every single thing is tested,
or isn't it that important actually? Do you actually want to spend the time?
Or maybe you want to do a quick and dirty but pretty good analysis for every pull request,
which you do in the nightly build where you test it fully.
There's different approaches here, and this is actually something that is researched
by one of my colleagues at InfoSport, he did his master's thesis on it.
So it's really cool.
But what could be downside of this approach? Any ideas?
Remember, you can get some socks.
Yes?
So the answer was the feedback loop is longer, basically.
It might take time to find if your tests are not that great.
That's the one. I have another slide, but yeah, another guest.
Sorry?
The problem is very useful because you run nightly and...
Yeah.
What kind of size socks do you have?
Large.
Sorry?
Large.
Large.
It thinks too far, just come get them later.
I will put them aside for you.
What's your size?
Small.
Small.
Ah, damn it.
I'm not good at throwing.
But yeah, so the mutation score that you compute, if you choose to run other mutants,
the mutation score might not be comparable.
So you really need to take care of that.
And the tool that my colleague actually created,
analyzes a code base so that you can do this actually for a specific project.
It analyzes a code base and it analyzes the test and it's trying to find a nice balance
between accuracy and the number of test executions that you need to do.
So it tries to see if there are some mutants that we can exclude
that will gain a lot of performance.
So it speeds it up massively, but doesn't reduce a lot in accuracy.
So that's really nice.
And you can actually find his thesis online.
So if you go to the FOSDAM page for this talk, you can have a link there as well.
And I'm very hard to press actually, so there is not even documentation for that
and it's not even merged.
It's project Xavier and that is actually implementing that idea
because it was very theoretical, actually implementing that idea in Striker for JavaScript.
So if you're really interested in how that all works and what decisions they made,
I honestly don't know yet myself.
Go look at the request.
And it's also a very cool example.
Mike is dead. Oh, it's back again.
We're a project group from a university, in this case the University of Twente,
contributed to Striker in that way and they actually built this.
Yeah, documentation as I said, still to follow.
And this is also a very new thing that also a student is currently working on
is to do more analysis on static analysis on the code
to figure out if we can run in one run of test cases, if we can analyze multiple mutants.
But in order to do that, we actually need to make sure that these mutants do not influence each other.
So this only works if you know for sure that they don't cancel these order out
or if you can still, given that the test fields can say with confidence which mutant it was.
So this is really, really complicated.
Again, I'm not entirely sure on this progress yet, but question.
So this is sort of static data for more complicated.
Yeah, so the question is whether modularizing your application would help.
And it would help because if your modules are smaller, then the test runs are also smaller.
They would take less time.
But it only works if a normal boot rest for you, normal change that you want to do,
if that's only contained to one module or two modules.
But if you split your code up instead of changing all modules, then it doesn't help.
So it might help.
But that's also with, like, in general, I want to make my CI pipeline quicker,
just make more repos or build less modules.
So yeah, that would definitely work.
Come grab a pair of socks later.
And now it really is time to also start testing your tests.
So if you're not using mutation testing in your products already, it's really good now.
We can actually use it.
There's been a lot of progress in 45 years.
We have better hardware.
We have process improvements.
And actually, there's a lot of research going on still to make it actually faster every time.
We also have production ready tooling.
There are many great libraries out there.
Some of them are more mature than others.
Some of them are faster than others.
Not everybody integrates the same process improvements, for example.
But in general, for most popular programming languages, there is a tool available
and you can run it in your pipeline.
And most of these tools just integrate with the build tool that you expect.
They use information that the test runner already gives you.
So that's great.
Here's an overview of some suggestions.
But if you just Google your favorite programming languages plus mutation testing,
I'm sure you'll probably the first result will be the right one.
So in summary, when we're talking about mutation testing,
we're really talking about testing your tests,
making sure that your tests actually expect what you do.
And something, if there's anything,
only one thing that you take away from this talk,
don't rely on code coverage, please,
because it doesn't say anything.
And a lot of research has been gone into performance improvements.
There's lots of research still being done.
There's still always students coming to us interested in contributing
to an open source project with research.
So there's plenty of open research questions.
And it's also applicable now.
So if you're maintaining an open source project,
at least consider mutation testing,
because especially in open source, where there's many contributors,
there's a really good metric on to get an idea about the quality of the tests
that somebody wrote for our poll request.
If you want to know more of the code implementation details,
as I said, my colleague gave a talk yesterday in the JavaScript Dev Room,
it's probably online at some point, so you can go check that out.
And that was my talk, so thank you for listening.
APPLAUSE
Exactly 25 minutes as well, so that went great.
Any questions? Yes?
I could determine which expressions do mutate.
Sorry?
I can determine which expressions do mutate.
OK, the question is how do we determine which expressions do mutate?
Basically, there's a lookup.
So it does abstract syntax free analysis,
and it just checks a certain node,
and there's a lookup table to say, OK, if we have this kind of operation,
these are the options that we can...
These are the mutations that we do.
So there's basically a big mapping file with all the options,
and that is probably not complete for every standard library out there,
but for a lot of the logic, comparisons and stuff like that,
you can do it pretty complete.
Yes?
I couldn't hear the last part. Can you repeat that, please?
What is the way to find the way to test your...
OK, so the question is what is the baseline
when you start with mutation testing?
Right, so if you have a new code base,
if you have green fields and you implement mutation testing from the start,
it's actually relatively easy to get high 90% mutation score.
When you have an assisting project, it's usually very hard.
So actually Striker for JavaScript has a mutation score of around 80%,
which is actually pretty good.
It's really hard to get very high scores.
So it's not very... Not like covered.
If you're anywhere close to 80%, you're actually doing pretty well, I think.
Yes?
Yeah?
Yeah, so the question is when the purpose of mutation testing
is to make sure your test are good, if you're doing selective mutation,
how do you know you're not missing something?
Actually, you don't. You might miss stuff.
That's the point is because it can take a really long time
that you have at least the option like,
OK, I'm OK with 80% accuracy if it's half the time,
because that's for me a good balance for some use cases.
But yeah, you have to accept that you're missing stuff
because you're just not running all the mutants.
So the combination of mutation testing and test-driven developments.
Well, you have to write your test first then,
but you can only mutate once the implementation is there.
So once you test that green, you can check whether you actually did a good job
before writing your implementation, which is kind of strange, actually.
Yeah, but that's very nice, actually.
If you have to change your testing of changing requirements
and you re-implement part of your code,
then mutation testing will check whether your test are still complete.
So that's actually very good, very nice.
Yeah, actually, if you really want to go into that,
property-based testing, you're only going to test all possible inputs,
even though for sure that is correct, but that's not feasible yet.
Property-based testing is really hard, too.
Do you have time for more questions?
Four minutes.
All the time in the world, up front.
The question is, from your experience currently,
if, for example, you have normal unit tests
and they run, let's say, it's called one minute,
how many minutes will they be around using the framework?
So the question is, if I know how long my unit tests run for,
how do I know how long mutation testing will take?
And there's only one answer.
It really depends, because it really depends why your tests take a minute,
but it's going to be a lot longer.
It's like, it's not going to be four minutes or five minutes.
It's way more than that.
The only way to find it out is to just actually run it,
because the problem is it really depends on how many mutations
can be generated for your specific code,
because that is what makes it slow.
And because of all these optimizations,
you cannot really predict how long it will take.
I didn't hear that one.
So the question is, how do we report it?
And for Striker, and you can go to the talk with my colleague,
he went into that in a bit more detail.
We have a standardized adjustment for that.
That's good. Oh, it's a bit.
So there's a nice dashboard, but go watch this talk
and you will know more.
Up front, yes.
So you already run one mutation at a time?
Yeah, you have to.
If you want to do more, you have to prove first
that these will not influence each other,
so that you need to know that if one test fails,
because of which mutant it is.
With coverage, you can have reached that?
No, you really have to do data flow, control flow analysis,
and stuff like that.
So that's very, very difficult.
But yeah, there's somebody working on it right now.
So maybe in half a year's time,
we'll have some more to talk about.
Yeah, Sci-GridNet already has an implementation for it,
but it's not scientifically proven,
so we do not know 100% where that's correct,
but at least 95% there.
So if that was the last question,
if there's any more question if you want to talk some stuff,
I'll be outside in the hall.
And if you ask a question, feel free to come
grab your socks here up front.
There's plenty. Thank you.
APPLAUSE
