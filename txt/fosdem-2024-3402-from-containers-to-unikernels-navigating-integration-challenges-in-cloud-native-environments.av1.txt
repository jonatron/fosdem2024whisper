So, thank you.
My name is Ion Blackus and in this presentation we're going to talk about the Unikernels and
their integration challenges in cloud native environments.
So, first we're going to set the scene, talk about containers and some books mechanism
and then we're going to read the Unikernels and talk about our own OCI compatible container
runtime, Uran-C. And after that some demos and evaluation results sections.
So, yeah, we're a team of researchers with mixed industrial and academic background,
mostly focused in virtualization, container untimes and hardware acceleration.
So, yeah, containers is the standard right way to deploy and package your application.
They're portable, they can be run both in cloud and net.
They're easy to scale with the support of a wide ecosystem and they have super fast
phone times. But they come with major risk when it comes to multi-tendent scenarios.
Multiple containers serve the same kernel and rely on software components for their
isolation. So, yeah, in a malicious workload scenario, in a previous escalation, container
impact the entire host. So, what about vendors did was to either use software-assisted
solutions with tools like SACOM or a Parmore or hardware-assisted solutions, VM type solutions
with tools like Farcracker and Deviser. So, yeah, the thing becomes like this.
We have now, we deploy our application inside the container, inside the VM, totally isolated
from the last system and we get to keep actually the benefits of container, the portability
and the scalability and we also kind of resolve the isolation issue. But of course, this comes
with side effects, since higher overhead because of CPU and memory provisioning for the VM
and the much more complex system stack that needs to be maintained. By taking a step back,
we can see that the application does not need all these parts, not run. It just needs the
run time, the libraries and some parts of the OS, like the drivers. And, yeah, this kind
of modeling is enabled by technology called the Unikanal. So, what's Unikanal? It's a
specialized single-hatter space that contains exactly the necessary parts for the application
to run. So, yeah, this leads to reduced attack surface and faster boot times, which is especially
crucial in serverless scenarios where responsiveness matters. And, yeah, but Unikanals are not
widely adopted yet. And why is that? We identified too many issues. The first one is
packaging and, yeah, Unikanals should look like an OS image in order to utilize because
this is support. And the second one, the more, more than, is deployment and the execution of
Unikanals. Container run times need to be extended. Additional logic is needed in order to
execute Unikanals. And with this, I would like to give a floor to your viewers to talk about
your ANSI. Thank you, Yanis. Hello, everyone. So, to solve the deployment challenge of Unikanals,
we introduce URAN-C, which is a Unikanal container runtime. It is fully CRI-compatible. It's written
in Go. Actually, it's a CLI tool which makes use of interconnected Go packages to actually
spawn the Unikanals. It reads Unikanals as processes. So, in a way, it directly manages the
application and not the system in which the application runs. The images, the Unikanal images
required to run these Unikanals are typical OSI artifacts. And in order to actually spawn these
Unikanal VMs, we make use of underlying hypervisors. So, first, let's take a look at how Unikanal
images looks like. First of all, they are standard OSI images, so they can be managed via
standard tooling and can be distributed using already existing registries. But there is one
differentiating factor. URAN-C needs some specific annotations to function. These annotations are
the Unikanal binary path inside the root of S, the Unikanal type, the hypervisor type, the command
line that we need to pass to the Unikanal. And optionally, if we are using IneterD, the path to
the IneterD file. So, to facilitate the packaging of the Unikanal, we created a simple image
builder called BIMMA, which uses simple Docker file like syntax to create the images. As you can see, it's
pretty typical for anyone who has used Docker. It's practically the same thing. So, now we have seen how
an OSI Unikanal image looks. Let's take a closer look at how Unikanal actually spawns a Unikanal.
First of all, container.dc invokes URAN-C create. URAN-C create then sets up a new network name space, create a
new name space, sets up a pseudo terminal if it's required, and spawns URAN-C reexec process inside that name
space. The reexec process then notifies the parent process that had started. Then, URAN-C, the URAN-C
create, the original process, saves the state, the PID of the reexec process, etc. Executes any create runtime hooks, and
then sends an OK IPC message to the reexec process, executes any create container hooks, and then exits. Then,
container.dc invokes URAN-C start, which sends an IPC message to the reexec process, executes post start hooks, and
exits as well. So, now it's the most interesting part. The reexec process actually sets up any necessary network and storage
components, for example, the W device, etc. Executes any start container hooks, and actually spawns the Unikanal VM. So, as you
can see, this is a pretty typical life cycle for any container runtime, with just some minor adjustments to facilitate the Unikanal
execution. So, to actually spawn the Unikanals, we use hypervisors. We made it really easy to integrate any new hypervisors you want to
implement in the system. So, I'm going to show you how to do this with the Unikanal VM. So, in this case, you can just
implement this interface, which is mostly just the exec V function. So, it's really easy. Currently, we have support for
Solo 5, Kimu, and Firecracker. For storage, we have support for block device via the map or
snapshotter. We have support for InitrD, which is packed inside the image. And we also have support for shared effects coming soon.
In the diagram, you can see how an image looks like, the layers look like. So, for the network part, we followed the every simple
approach that is also used by sandboxed runtimes, like Cata containers. We create a new top device inside the container network
loading space. And then we breed all the data, all the traffic to the VF endpoint provided by CNI. We do this using traffic control.
To integrate Unikanals in Kubernetes, we had the Asphalt Challenge. That's because we need to actually spawn non-Unikanal
containers inside the same pod. For example, the POS container or any other side-con containers. To achieve this, we use RunC to spawn the
generic containers. And then, RunC handles the Unikanal containers inside the network namespace of the pod. So, there are some really
interesting use cases, for example, KN80, in which we need to have intrapod Unikanal container communication. In KN80, for example, the QPROXY
container needs to be able to communicate with a user function, which is Unikanal. To achieve this, we implement a static network
configuration. We provide the static IP to the top device. So, we handle it that way. So, now, let's see RunC in action. We will see a simple
deployment using an HDL. We will pull the image from the registry. And using an HDL, we will actually spawn an NGNX Unikanal inside
VM. So, as we can see, there are no containers running right now. We pull the image from our registry. Okay, it's already existing. And now, we can run it using
an HDL. We have to define the runtime. So, we do that. Okay, it spawned. And now, we can see that it started six seconds ago. It was created. Perfect. So, now, we can
inspect the container to find the IP address. Okay. And if we curl it, we can see that it's an NGNX server built using Unicraft. Pretty typical. So, now, we can see the actual
run. And we can see that it's running. And the container is in the RunC process. That's also running. Okay. And now, with that, I will give the floor back to
Janis to show you a more elaborate example with K-nate. Okay. So, now, just... Okay, that's bad.
Now, let's deploy a serverless workload with RunC. So, what we first do here is that we see that we have another RunC process running in the
cluster. And after this, what we need to do is to define the RunC class for the K-H cluster. You can see here, we apply the RunC class. And then, it's time to define the K-native
service. We can see that your RunC container around them is specified. And a simple HTTP-reply-server workload is used as the workload of the serverless function. We apply the
K-native service. And then, we will retrieve back the URL endpoint, which by triggering it with a simple HTTP request, a simple HTTP get, essentially, we start the execution of the serverless workload. So, here, we can see the
curve. And after this, the pods are going to be running. And underneath, there will be the RunC process with the K-mode hypervisors with the sandbox workload. So, yeah. That's it. And...
So, the evaluation section. In order to evaluate your RunC, we convert with other container runtimes, such as divisor and other containers. And, yeah, in that process, we utilize the tool called K-perf responsible for generating and triggering K-native services via HTTP request, as we saw in the demo. And also, responsible for reporting the service latencies.
So, yeah, the scale from zero, evaluation scenarios like this, for a number of iterations, we scale. It's a K-native service. And we report at the end of the number of executions, we report the responsible latency. We do this for every other container
runtime. So, these are the results. We can see on the X axis the different container runtimes used for that process. And on the Y axis, the service response latency, seconds. And, yeah, of course, lower is better. So, on the blog post, with the experiment setup and all the parameters setting for K-perf. That's all. Thank you.
Thank you.
Okay. So, the question is about memory benchmarking, right? Yeah. Memory benchmarking is not yet on our work, but we have plans on that also. Yeah. Something that we can do.
Sorry.
So, the question is if we have run in Germany, AWS, I don't know.
Actually, this experiment was on the Prime Service. We have not yet experienced any big lab vendors and deployments. So, hopefully, maybe the next evaluation will be also part with major vendors.
That was the end.
Okay.
So, okay. I heard something about paravirtualization, right? But, yeah.
Okay.
I'm not sure that that's.
Okay.
So, I think that's it. Thank you. Thank you so much.
