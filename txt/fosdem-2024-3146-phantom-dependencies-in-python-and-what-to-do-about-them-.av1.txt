Good afternoon everyone.
having a moment of pleasure in
I will begin.
You're in green.
I'm on green, yes.
Okay. Hello everybody.
My name is George Goussius.
I'm head of research at Tendall Labs,
and part-time associate professor
at the Delfi University of Technology in a nearby country here.
Let me say a few things about myself.
Since it's customary here to introduce the speaker.
So I have been on this particular field of S-bombs,
and dependency analysis, and so on.
As a researcher, since 2015 more or less.
So I have seen all the failures coming up in real time,
like left pad, then solar weeds, and then so on.
In 2020, we had organized the dependency management room,
which was a pre-care thorough,
perhaps it was slightly conflicting with this particular room,
where we introduced the Fasten project,
which was one of the first projects that basically did
the reachability-based analysis of S-bombs and software dependencies.
This project, one thing led to another,
and this project basically became a startup,
perhaps now it's a scale-up, we can call it,
which is called Ender Labs.
It is based in the Bay Area.
We are basically providing solutions in the space of
software composition analysis.
Plus plus. We will see what the plus plus is today.
Well, by describing my history so far,
you might have understood that I'm of a certain age.
Parts of this being of a certain age,
is that I have a teenage daughter,
that actually is into pop music,
and lately, I came up,
she came up actually with this song,
which I really, really like because it describes
the problem that I'm going to talk about
in almost in excruciating detail, I would say.
Let's read a bit the lyrics.
So like ships in the night,
you keep passing me by,
just wasting time,
wasting time trying to prove who's right.
If it all goes crashing into the sea,
it's just you and me trying to find the light.
What are the ships? Any guesses?
Let me help you a bit.
There are two ships.
One is the Package Manager,
where the developer declares their intended differences,
and the other ship is
the compiler and the order and time of the language,
depending on whether you're using Python or C or so on,
which has its own view of dependencies.
Right? Which ship is right?
Who says both?
Let's see. One vote for both, but who?
It depends. Okay. All right.
Who says that the compiler is right, is always right?
Yes.
Is it ground truth though?
They're both wrong all the time.
The compiler is ground truth.
It can be ground truth,
but what about when you load code dynamically?
It's runtime.
Okay.
Yeah, that's the issue.
So the developer over describes
the dependencies for testing coverage and contract validation.
Horrible things happen because you build too much code,
bring it all in later on and things go even further.
Fantastic. Yes, exactly.
Yes. This is where I was trying to get to things.
Maybe you have to repeat all that.
Okay. I mean, it will come through the presentation.
So traditional dependency management and
software composition analysis works.
I mean, we're in the S-bomb room,
we guess everybody knows how it works.
So we start a new project,
we create a Package Manager manifest,
requirements TXT, POM XML, what not,
Gradle, whatever tool we're using.
Well, the build system and the Package Manager,
when we're trying to build a download stuff from the Internet,
side parenthesis here,
how can we trust our data on
just random stuff we download from the Internet?
That's a different question, close parenthesis.
The Package Manager copies all the files in
a directory and then the compiler starts using
those dependencies in order to compile or run the project.
This is what we know.
But if we think a bit from a higher level point of view,
we get more or less to this.
So the developer declares their intent in a manifest file.
So that's the requirements TXT file.
A Package Manager does the dependency resolution,
we get the dependencies onto our system,
we have a compiler.
Now, the developer themselves also write source code.
The source code might or might be using
some of the dependencies,
might be declaring dependencies that they are not using,
might be actually depending on
dependencies that are transitive that other dependencies bring in.
All right. All those nice things that make
software composition analysis in a way that most of
the tools are doing this at this point.
Well, I wouldn't say wrong, but perhaps incomplete.
Okay. The output is always a program,
and the program is the source of truth of everything.
So what I want to advocate here is that when we're
doing software composition analysis,
there is a lot of stuff that we don't really identify.
Can you guess where the stuff comes from?
Always. So for example,
I write a Python program and I have
some kind of packets that I have installed throughout get.
What else? A copy and code into my repository.
That's a dependency, right?
But I don't maintain it somehow. What else?
All of those fancy shift left dev tools.
Shift left dev tools. Okay. So how?
Right. They're pulling things in there.
They're updating and flagging,
and sometimes automatically changing.
Okay. Yes. There are some tools that are indeed pulling in.
Bazel, for example, but Bazel also depends on
the versions that you provide to it.
How else can I have, let's say,
dependencies on code on my program? Yes.
One-time library from your compiler.
Fantastic. Yes. Lib C. This is a dependency.
Okay. This gets installed, for example,
from the operating system,
but sometimes when you, for example,
do some kind of JNI calls through Java,
you depend on Lib C. Okay.
Various ways. So things also tend to become out of sync.
So developers import new dependencies.
The dependencies are in the environment.
In some cases, dependencies can be declared in a testing scope,
but we're still using them into production
for misconfiguration reasons or for any other reason.
In some cases, dependencies are removed from the code,
but not for the package manifest.
So we have an extra dependency that we somehow need to maintain,
but we're not using it,
so it's basically redundant.
Then we have Python.
This is the average Python repository,
especially if you're dealing with machine learning and AI stuff.
You have requirements to use the file or a poetry file,
and then we have a list of instructions that looks like this.
That tells you please install TensorFlow at this version.
Please install NAMPi at this version,
but with this patch for this particular GPU,
because otherwise the thing is going to be dog slow.
Okay. So how can we actually maintain this?
How can we discover first those dependencies?
How can we maintain this?
Let's take a look actually at this particular project.
So this is from OpenAI.
It's called baseline.
It's pretty old, as you can see from the
time stamps over there,
but still it has exactly this problem.
So it tells you to create a virtual environment,
and then it tells you that you need to install TensorFlow
between 1.4 or 1.15 by hand.
It's not part of the requirements CXT file for some reason.
Using our tooling,
I have run an initial scan of this project
without considering what I call phantom dependencies.
All right.
What you would see here is more or less the same thing
that most SCA tools would give you.
It's all the files that are in requirements
CXT plus their transitive dependencies.
Okay. So we have some direct dependencies,
and then we have some transitive dependence over here.
That's it. Is that it though?
Well, we will see at the end of this presentation,
but first I would need to run a full scan of the project.
So by also enabling phantom dependencies,
I will have leaving running in the background.
Yes. I mean, the idea here is not to see this.
But what I want to show is basically that the thing actually works.
So it's not a vaporware.
All right. So what are phantom dependencies?
Basically, phantom dependencies is the thing that we have discussed.
It is dependencies that are provided by the system,
and they're assumed to be working basically,
to be available somehow in the runtime of the project,
can come from various locations,
some of which we have already described.
If you think this is just a problem with Python,
it's not just a problem with Python.
It's problem with NPM as well,
with Java, if you have plugins with,
even with native environments.
All right. We discussed this.
As I said, what I consider the sips in the night,
according to the original talk,
is basically the two things,
the Package Manager and the compiler in the runtime view.
The Package Manager usually sees way more dependencies
than the actual runtime or the compiler uses,
because there are a lot of
transitive dependencies for which we don't have
any reachability path to them.
So when we start from the client,
there is no path, calling path from the client into
the actual transitive dependency.
So usually what we have found also in the company is that,
from all the source code that is being
imported into a repository,
which is around 80 percent of the code
that on an average repository is imported,
around 15 to 30 percent is actually
being used. So there's a lot of codes that we import.
It perhaps forms an attack surface that's never been used.
It would be very nice to clean this up.
Yes. One way to do that is with
reachability but that's not my talk.
That's another talk.
Okay. So how can we identify
a fun dependencies and do this type of cleanup?
We need to do program analysis.
Any idea what program analysis is?
Yes? Some people might have seen that.
It is. No, it's not parsing.
It's not parsing. It's like one component of program analysis.
The first step in the whole program analysis chain.
Sorry?
I disagree with your first bullet.
The source of truth is the source code?
Yes. It is not?
Could it be part of us,
C programming project or
the biggest thing in JavaScript to pull in dynamic code or
anything written in list for code and data of the same?
So I love the optimism of
the source code understand with dependency graph.
It's actually the binary.
Could be true. Yes.
For languages that have binaries,
for languages that have source code,
that are source code executable,
the source code is the truth. Right?
No?
I agree with nature of source code and data,
especially when we move into data.
When we're talking about why are we doing the program analysis,
we try to understand what we're bringing into the net.
So let's stop now and then we have to get back.
Yes. I agree and disagree.
We need to start with program analysis,
and program analysis starts necessarily on the source code.
So perhaps this formulation here is not precise,
but it is something we can work on hopefully.
So why do we need proper program analysis?
I mean, somebody could say, well, yes.
If I track all the imports in my Python code,
it will be easy to identify all the libraries.
So it's easy. I'm just going to write
a Perl script or Python script those days to take
the imports and try to find what libraries have those imports.
This is perhaps hopefully comprehensive,
way of doing importing in Python.
You can import a module, a function from a module,
all functions in a module,
and alias a module to a different name,
a packets, static import,
a relative import in the source code,
using the import leap,
in which case you can also rename the import leap.
So you can alias basically the import leap and that you need
to de-alias it in order to be able to track this,
and you can even basically do an evil, an import code.
Good luck doing that with a Python script.
So we need-
No, the code may be inside it.
Exactly. That's my next point.
Sorry.
Exactly. All those things can be in a conditional statement.
You can have a try import for otherwise import bar,
or you can do a new condition on some variable,
and then import the custom library.
Those are the reasons for why we need
a proper program analysis to do that.
Okay. So the steps that we have taken to solve this problem,
first of all, we need to start with the source code.
Okay. We start, let's say, with the client code,
and for each file in the client code,
we follow all the imports.
How can we follow all the imports?
We first need to have analyzed the program,
the client code, and the virtual environment,
and the site packages that come with the operating system.
Basically, all locations from which Python can tell you
where it can find code.
So if you open the Python interpreter and you
configure it with a particular virtual environment,
for example, you can ask it,
please give me all locations where I can find
code for this particular execution.
Okay. So we start with that.
After we have analyzed the map everything,
we can start then from the client code
and do case by case import analysis.
So I import this particular library,
I go into the file that is into the module basically,
that creates this particular library,
look at it, see its imports,
and go, let's say,
transitively until the whole thing has been exhausted.
This is being done for a bunch of
when they're doing the bills for resolving dependencies.
Did you leverage that code?
No.
Okay.
Repeat the question for the audience, sorry.
The question was that this analysis has been done before,
did we reuse the analysis?
The answer was no.
Thanks.
Okay. Now how to do program analysis in Python?
As I said, you need to have resolved
past everything to begin with,
resolve the types for everything.
If you have type information,
it's way more precise because you can track
basically specific function calls onto types.
Then you can also take into account one of
the existing static type checkers,
like mypy or pywrite,
we're using pywrite for that matter,
to basically parse the code and do all the resolution.
Okay. So it's not extremely hard to do,
but you need to be aware that it needs to be done.
I will show you the results,
hopefully of the scan.
Yes, the scan has finished just in time.
As you can see before, we had 11 differences.
Now by doing this fandom dependency discovery,
we have found 51.
According to our findings tooling here,
what we can see is that in one of those differences,
we found an actual vulnerability.
It's of course in setup tools,
so it's not necessarily something that can be
actively exploited,
but it can be exploited while
the packets has been installing.
So what we have found is that there is
this vulnerability and this is a call chain,
let's say, but goes from the client source code
into the vulnerable code.
If I didn't do this analysis,
I wouldn't be able to track this or do anything with this.
I mean, this would be information that I wouldn't know.
Of course, this is a trivial example that we're using for demos.
We have found actual vulnerabilities when
running this analysis from clients that I cannot disclose.
But this at least to me gives an indication of
how this fandom dependency problem can be
tracked and solved hopefully with program analysis.
After that, everybody wins.
Developers can know what is vulnerable to their code.
They can accurately map and create
accurate S-bombs on what their application is consuming.
CISOs can be aware that their vulnerabilities there
that otherwise they might not have been aware of.
So everybody wins. That's it. Thank you.
Thank you.
Yes.
Yes. I'm going to go first.
So yeah, great insights on Python and I'm a Python developer.
So I know at least about the majority of these things.
But the tricky thing is,
I mean here you are talking to the people who are interested in S-bombs.
How do we spread the information?
How do we get all the open source communities to know about all these issues?
How do we make them publishing their S-bombs?
This is, your stuff is important but making all the communities aware of this is more important.
Yes. This is part of why we're giving these talks so that we make the community.
Yes. Excuse me.
So the question is how can we make the communities aware of the problems that
I have been describing and my answer to that is this is part of this effort.
We're trying to make the communities aware that those problems exist.
The tooling that I have described, it might sound, let's say, extremely complicated and whatnot.
But if you're using, I can actually show you it running.
So it is, I have it here.
So this is our basically analysis tool.
This is closed source at the moment but it can easily be re-implemented, I think.
If you ask me, I can re-implemented it in like a couple of days but perhaps somebody else could re-implemented it a bit faster.
As you can see, it just goes over, transitively, over all the code that is available to this particular project.
Yes. No, there was a question.
You've answered the question because I said, is this open source?
No, it's not open source. Sorry.
What you've done actually is a great benefit to the whole of the open source community.
So if you maybe could describe the architecture you've drawn and the algorithm,
then we're sure the community will then jump on and do the different projects.
So is that something you could share with the community?
We need to think about that, yes.
I mean, the question again was whether this is open source.
The answer is we need to think about this as a company.
Yeah, I don't know is the answer. Yes.
Are you scanning for Python binary dependencies as well because TensorFlow, for example, includes FFMPEG?
So this requires, I mean, the question is whether we are actually considering the binary dependencies into Python packages in the wheels, for example.
The answer is not but not yet.
So we are trying to get into cross-language analysis.
This assumes modeling basically of the interface between Python and the native library.
We're getting there.
Yes.
We're doing static, the question is again, we're doing static analysis and static analysis has false positives.
How can we prevent those false positives?
In this particular case, this analysis that we do doesn't have false positives because it is basically only considering imports.
False positives and static analysis come from the fact that, for example, you have a virtual dispatch call site.
You might be linking to multiple implementations of particular interface, for example.
In which case, you might be basically over-linking.
We're not doing this at all here.
Sorry?
Where do we have false negatives too?
False negatives, we cannot have false negatives here because we are considering the source code as ground truth,
which means that we don't basically everything that is in the source code will be parsed and reported.
Except for eval.
Sorry?
Except for eval.
Except for eval.
Eval is eval, yes.
Right.
Final question?
Interesting, some method that tracks the imports.
Well, what if there is an imported module that one function is using but is not reachable from the main code?
It's kind of...
If you're analyzing an imported module, it's not being called.
I don't see what the problem is.
As you mentioned, there are false positives.
But if there is a file which is not imported and it's by a function, it's not being called.
Yes, we will still analyze it because it has an import.
It's not being called, right?
We have some helper problems.
It's never being called.
It will be checked as a risk and it's a risk.
If it's not being called...
So, sorry.
Excuse me, Alex.
So the question is what will happen if I understand correctly?
If we have a file that has an import in a function that's never been called.
Yes, we will not analyze this because first we consider the call graph of the whole thing.
Thank you again, Jordan.
Thank you.
