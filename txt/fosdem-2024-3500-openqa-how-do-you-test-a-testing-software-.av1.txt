I'm going to go to the next one.
It's always interesting to see people here and this group.
A lot of people I've seen before and a lot of people I've seen giving talks before.
Somehow, it never happened that Tina gave a talk here in Boston.
She's been here before but actually she's first of all, thank you for a great welcome.
Hello, hello, can everyone hear me?
The microphone might not be on.
What, is it on? No.
Okay, alright.
So I'm trying my best but remind me if I'm getting too low.
So, yeah, I'm going to do a talk about OpenQA.
Who of you has heard of OpenQA?
Okay, a couple of.
And are you using OpenQA?
Okay.
So I'm doing Perl since 1998 and I'm working now as an engineer through the software solutions.
And I'm in the tools team where we develop OpenQA and it's written in Perl.
And just to give you a little short demo.
So with OpenQA you can test an installation of an operating system and you can start applications.
You can pretty much test everything you want.
So here you see the installation process of Open to the tumbleweed.
It's not real time.
That's a bit fast forward.
But yeah, here it gets more boring.
Okay.
So it's also, it's not only used by OpenSusul but also for Fedora and Debian and actually more.
I think AlmaLinux is using it.
And in this talk I'm going to demonstrate you a little bit the web UI and show some relevant test API functions, the project structure and how we deploy and how we actually develop and test it.
Okay.
I think I'm going to sit down for...
Okay.
Is this readable?
So here you can see all our tests and they are grouped into so-called bills.
And here we have tumbleweed and on AR64, PowerPC.
And then we can click on a build and see all tests of this build.
There are three main states of a test.
It can be passed.
It can be failed or it can be soft failed and soft fail this like, okay, we know about this bug.
And it's not critical for the release but we mark it as we need to look at it later.
And let's look at some actual tests.
So this is a tumbleweed DVD installation.
And you see all these boxes, most of them are screenshots but there can also be informational things.
And here, we can move through those screenshots.
And here we see a screenshot of the installation where you have to choose a time zone.
And we call these screenshots needles actually.
So a needle is something that we want to match for.
And here at the top it says 99% matching.
So that means the screenshot that we got is matching our expectations to 99%.
And why is that?
So we have this bar here and at the left side you can see the actual needle, what we expect.
And on the right side it's the actual screenshot.
And you see that the font has changed a little bit.
And we don't care much about that.
It's still okay.
And that's why we set a threshold of something like 90 something percent and it's still matching and it's okay.
And here's another needle.
And here you can see, so the upper area of the picture is what we want to match.
And you also can see this gray area where the penguin is supposed to run around.
And it's gray because we can actually go into the so-called needle editor.
So we can actually live edit such a needle.
And here you can see the screen area is that what we want to match.
There are also some red areas and I'm sorry it's not colorblind friendly yet.
But you can use some such red areas to exclude certain areas because we don't know where the penguin will be at the time of the screenshot.
So we just exclude that.
And then you can also review the JSON.
So a needle actually consists of a picture and a JSON file that says which areas should be matched.
So here's another needle and it's showing the desktop runner.
And this is actually showing another purpose of a needle.
We don't want to only make sure that we get what we expect.
But we also need this to proceed in a test.
So if I'm in the test and I tell OpenQA to send the shortcut for the desktop runner.
And then immediately type something or tell it to type something then it wouldn't work because it takes a millisecond until that pop-up is actually there.
And the easy way would be to just sleep one second right?
Or maybe to be sure two seconds or maybe rather five.
It can actually sometimes take longer because the worker on the test is running tests in parallel.
So there is this function called a third screen and you can give it a timeout.
So for example 60.
And then it will take a picture every second until it gets this picture what we expect.
And then it knows okay now I can type the command.
So because otherwise if we always sleep five seconds then the test would take a long time.
We can also look at the log files of the test.
And settings.
And here we have all these job groups so you can see what kind of stuff we are testing.
We are actually testing OpenQA itself.
And actually, yeah I don't have any screenshots here.
And here you can see a screenshot of OpenQA inside of OpenQA.
So we use our own software to test ourselves.
Okay so that was the demo.
Here are some code examples.
So here you can see the assert script run call for example.
Which is just sending some text to run and it's asserting that the exit code is zero.
It also has a timeout.
And this is the job group configuration.
We use YAML for that.
The YAML can be huge and so we are actually using the YAML merge key to avoid duplication.
Okay so far about the demonstration.
And yeah these are the test API functions we have.
The most relevant you would probably like to know is you can send a key.
You can also send a key until a needle matches.
So something like enter until you get into the BIOS.
You can have screenshot related functions.
You have the mouse functions.
So mouse drag is a function.
And click and double click.
What we don't yet have is you cannot see the cursor, the pointer moving.
So if you want to use it for demonstration which is actually a very good use case for OpenQA.
Just demo your software with writing a test and having a demo at the same time.
But you don't see the mouse pointer moving yet.
I have proof of concept pull request for that but hasn't gotten in yet.
So and you can even write test modules in Python now.
But that's boring for you because you're in the pull and drag room.
And this is how it would look like.
So we have all these functions like send keys and set var help.
I'm in a Python script, trapped in a pull script.
And okay now to the project structure.
It's split in two parts.
O S O 2 inst which is a name.
I don't like much because it's hard to type and pronounce.
That's the actual code that's running the test.
It started with this project and OpenQA is all the stuff around it.
For viewing the test, configuring, worker schedule, managing assets like ISO files and Q-CAL files,
API and WebSocket.
And it all started in 2009 by Bernhard Wiedemann working at SUSE.
And our code is using Modulicious by now.
Using as the HTTP user agent and for the web server and the classes.
We're using DBIX class and it's really helpful.
We're using now subroutine signatures.
In our tests we use test warnings to avoid that we actually get any unexpected warnings.
We're using test mock module, mock object and mock time.
And for the tidiness we are using pearl critic and pearl tidy and develop cover of course.
But we also have a lot of JavaScript, Python and shell code.
O S O 2 like I said, that's the heart of the software actually.
It's called ISO2Video, the main script.
It takes an ISO and makes a video.
When you develop a test you can actually run it directly if you have an ISO file and some vars.
Then you can actually start VNC viewer to watch what's happening and also change something.
If your test is actually bad and you have to want to try out stuff.
And our deployment is fully automated.
We just merge pull requests and then, okay, with every new commit,
the open SUSE build service will fetch the new commit.
And then we also do a separate update on the web UI regularly and on the workahouse.
And necessary service restarts are happening and database changes will also be done automatically thanks to DBIX class deployment feature.
The open SUSE build service, it's used for all open SUSE packages.
It can build RBM and also other packages.
So here we have all our related packages to open QA.
And how about testing?
So here in open QA we have 98% code coverage and for OS outer hints we have 95.
So how did we achieve such a high test coverage?
We cheated.
Well, at least we do cheat a little bit if you look at that.
So there's this feature about Devil Cover which lets you add this comment, uncover the statement.
And we have a couple of them and most of them are actually just, thank you, in the test directory.
Yeah, but compared to 37,000 lines, I think that's okay.
And here's the coverage trend we get from CodeCov.
And yeah, our general tests are under T and then we have API tests, UI tests.
We are using Selenium currently but we consider use changing to Playwright.
And yeah, the tests are actually included in the coverage.
So, and yeah, we also use open QA to test open QA.
I showed you that.
Some of our tests are forking and yeah, ideally everything should be turned into a unit test where I can, where I don't need to fork,
but still Devil Cover is able to do that if at the end of the process you add this line and then the coverage of the fork will also be collected.
And CodeCov will complain if pull regret adds uncovered code.
It will also complain if the percentage goes below a certain threshold and some directories are actually already marked as fully covered.
So, if there's any line that goes uncovered there, it will also complain.
And since we are using the Merify bot, then nothing can get merged if it's failing some of those tests.
And so you need two approvals and no failing test and then it gets merged automatically.
And that's working quite well.
But checking CodeCov might not be enough.
You know, having 100% CodeCov, which doesn't guarantee you anything, well a little bit, but so pull request authors are encouraged to add new tests for every pull request.
And writing tests is seen as a part of every ticket we work on.
And refactoring is also encouraged and for every regression we encourage people to think what we could do to prevent similar things in the future.
And yeah, I showed you that already.
And okay, I don't know how many times we have a question, but that's it for me. Thank you.
Any questions? One minute.
Okay, no questions? Then, alright.
Thank you.
