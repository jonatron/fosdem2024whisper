I'm Leonardo and you're not from ARM and I think it's going to be a great end of the
day so looking forward.
Well, hi everyone.
So this is a talk about remote attestation and yeah, because we think remote attestation
is sort of an inflection point and it's becoming increasingly available and used and any new
technology when it comes to the fore you have to consider different aspects, societal
and technical as well.
And yeah, so we're here to talk about this.
Possibly interesting things.
My name is Thomas as Fritz said.
I'm your notes.
The ghost of Hannes is here with us.
He couldn't come to Brussels but he's here in spirit.
Yep, that's us.
Okay, so I wanted to start with this sort of timeline that tries to capture some of the
more relevant events in the history of remote attestation starting from the theoretical
underpinnings with the DDSA paper from the fine people at PARC in 1983.
And you have to wait sort of 15 years before research trickles down into industry.
You have at the end of the century the first industrial consortium that is formed to actually
define what a trusted computing architecture is in terms of behavior, in terms of the interfaces
that it needs to expose.
And so we have TCPA formed which then morphs into TCG, the trusted computing group.
These are the guys that are responsible for producing the TPM12, TPM20 specs, among other
things.
So you have the first decade of the 2000s that is sort of where attestation, because
you know, TPM has a strong attestation story bound to the idea of using the TPM as a route
of trust for reporting.
So you have the first decade that is sort of driven by trusted computing use cases.
Then enter the second decade and you have AMD and Intel SGX cropping up.
And this starts the sort of confidential computing driven decade where you have the first, second
and finally the third iterations of the architectures, which culminate into the SCV, SNP, Intel
TDAX and ARM CCA.
And you have a few other interesting events in that period.
You have the riot paper from the Microsoft guys that sort of articulates fully the ideas
that were in the DDSSA paper.
So 30 and odd year later, you have finally the dice ideas on paper and not just on paper
and code.
And you also have the PSA attestation from ARM, which is sort of an IOT targeting attestation
for IOT platforms like riot as well.
So you have also this area of the users of the, well, this is covered by attestation
primitives.
Where attestation primitives starts to enter.
And then you get into 2020s and so on.
And here is where we sort of see some kind of maturity in terms of the standards that
are actually coming to the fore.
Not just standard in terms of standardized formats, data formats and rats that was mentioned
before.
And that is coming out this year.
Is one example, but also software standards.
So the configure FSTSM ABI that Linux kernel is just upstream is one very, very concrete
example of standardization in that space, in the software space.
So we are here, as I said, we're probably at an inflection point.
The primitive is increasingly available, not just in the configuration computing space,
although CC is a very, very prominent area, right, that drives this.
But also, so you have use cases in IOT, you have use cases in TCP remediation.
Well it's also cropping up in your devices with interesting societal fullbacks.
And so basically the idea is that, like Dave Taylor said, every authentication use case
is also an authentication use case.
For wherever you have the need for authentication, authentication which is effectively a stronger
authentication primitive, stronger identification primitive is something that could be used
to either reinforce or supplant your previous thing.
So that's where we are.
And yeah, so I think when you have, as I said, when you have this new technologies, you need
to look at the bigger picture and try to understand what are the implications that the use of
these technologies have on the wider ecosystem.
One of the interesting things here is the centralization risks that are involved with
attestation.
Another one is privacy.
Well, let's start with centralization.
I think it's here, yeah.
So if you have looked at the RATS architecture picture that was in the talk before, you have
seen that the verifier is at the very center of the image.
And it's not just visual biases, it's really a central architecture upon a choke point
of the architecture where all the message flows are intercepted basically.
And also where the decisions are made because the verifier box has a verifier owner attached
to it.
And the verifier owner is the guy that has the power to decide who talks to who, right,
which attest has the right to talk to a reliant party.
So he's actually gating the information flow.
And therefore he's a very powerful entity.
And the risk here is associated with monopoly, right?
So there are the situations where if you don't look at the carefully at your design and your
architecture, you slip into this potential centralization risks, which we have seen in
a way.
I don't know whether you have followed that.
Environment integrity is something that exploded last summer.
And yeah, it's the cautionary tale.
It's the perfect story of vertical integration where you have basically a monopolist actor
that takes care of the whole thing and basically subversely.
Well, it creates problems.
So the fact here is that centralization can be sort of tackled, we think.
The RAS architecture has a nice, it basically cuts through the, it has sort of, it curves
out the roles in a way that you can actually, across a long tussle boundaries.
So you can actually remodel the roles in a way that, you know, for example, you're moving
the very fine function towards the user, in a user centric way.
But you know, not all use cases are, not for all use cases, it is possible to do this,
this rearrangement of roles because sometimes you would end up in a conflict of interest
situation or something like that.
So maybe one idea is to run the verifier as a neutral entity, a multi stakeholder entity.
Analyze and script, that's what they did with when they democratized the X599 world
by creating this multi stakeholder consortium that runs the less encrypt function, which
is another example of this kind of centralization opportunities.
Yeah.
Privacy is another aspect.
So all the flows, all the message flows go through the verifier, the verifier has to
see the claims to make the ref value matching.
Therefore, you know, it sees everything.
So the potential of abusing this position is great because PII are maybe not in the evidence,
but can be actually indirectly obtained from that.
And so, so this is, this is a risk.
There are things in the, in the, in the toolbox.
There are basically two kinds of ways to deal with this.
One is to inflate your anonymity set, either by cryptographic primitives, group signatures
and stuff like that, or using methods like anonymization in the hardware, like, you know,
creating a batch of devices, like FIDO does, like ARM CCA does, in certain configuration.
The other thing is, well, yeah, is you reduce the, the claim set.
So what you need to expose to the, okay, to the outside world by claim reduction and,
and other patterns like selective disclosure and, etc.
So things are there.
So these were the societal aspects.
This one is instead the technical aspects.
So we have, we have been in a situation where mostly the, the designs were, so we're, we're,
we're transitioning from a situation where the, the solutions were experimental, right?
So we are, we were mostly in research mode.
Now we need to move to a different approach, a more engineering oriented approach and,
yeah, more structural approach.
And we think we have, you know, some suggestion to make and I'll let Yonis, and I'll let,
sorry for taking so long.
Hey.
Okay.
So I want to talk to you a bit about IEDF and why we think it's a good venue to try
to standardize standards relating to remote out the station.
So first off, let's look a bit at some of the IEDF principles that form the core of
its mission and why we think these are relevant to, to the hacker crowd here at Fosnum.
So we started openness, open process, so everyone can get involved and can, can, can read standards
that are being worked on.
And this includes not just technical folks, but also members of the, let's say, civil
society who have things to, to say about things that are being standardized or drafted.
The second is technical expertise or competence, meaning that the IEDF only works on things
that it has the competence to talk to.
And it will, it will listen to technically competent input from whatever source there
is.
And this, the third, third principle is that of practical ethos.
So rough consensus and running codes or trying to base all our standards on our engineering
judgment and our real world experience.
And more pragmatically, it means that all the standards need to come accompanied by
some, some code for verification and hopefully multiple, multi-employment, implementations
that are interoperable.
So let's look at, at the station in the IEDF.
I think the rats working group has already been mentioned and the major milestone that's
been achieved about a year ago, the, the remote out station procedures architecture document
from which this diagram is taken shows that the roles involved in making the remote out
station usable.
And the rats working group is there to essentially standardize around this diagram, around the
roles, mechanisms, data formats inherent in this.
But if you want to look at remote out station as an authentication mechanism, then we need
to go beyond the rats and this diagram.
And we need to look at cases where the attester and their lying party are trying to interact
over different protocols like O-Auf, TLS, ESD, stuff like that.
So let's start by looking at credential insurance.
And in this case, I mean, for example, X5-9 certificates.
So the enrollment over secure transport and certificate management protocols are central
to public infrastructure.
And it allows an entity to request from a registration or certification authority to
generate a certificate.
And a recent requirement from the CA browser forum has put in place a need from RA or CA
for the entity to prove the security state of the key that's being certified.
So that's why we're trying to integrate remote out station to make this happen.
So the way remote out station works here is the verifier sends an ounce to the entity
and entity that uses that to generate evidence and package it up in the CSR.
And then the RA-CA can get an out station result back and decide whether it wants to
trust the entity and issue the certificate.
The identifiers there are for the places where you can find more information about how this
all works.
If we look at ACME, it's again for certificate insurance.
And as you can see, the diagram looks pretty much the same.
The only difference is in the fact that the evidence is carried in a different format
defined by the W3C, so web-alpha format.
So just to highlight the fact that we're pretty open and pragmatic about what we use.
And if there's something ready, then we can just use that.
The second type of credential that we care about, for example, in this case is OAuth,
where a client might want to get an identifier and perhaps some credentials from an authorization
server.
Again, pretty much the same diagram.
And then if we move on to secure channel establishment protocols like TLS, these are quite different
because of their symmetricity compared to credential issuance.
And we've tried to preserve that.
In the diagram here, you can see one type of flow where the server is the one that testing
itself, but you can have the same on both sides.
So both the client and server can test themselves.
They can use either attestation results or evidence as credentials, and they can use
credentials, these credentials instead of PKI or alongside PKI.
So obviously, we're dealing with some sensitive stuff here, and we want to make sure that
our specifications are as secure as possible.
And the way we do this is obviously we use our experience with these protocols, making
sure that they're secure, and we use implementations to drive testing and make sure that we catch
any bugs.
But obviously, we can't just rely on that because we can't do proper thorough testing.
So recently, we've been integrating formal verification into our work, trying to prove
that the security properties that we care about are upheld by our designs.
And actually, in IETF, we have a new usable formal methods proposed research group to take
care of this more broadly.
So I want to leave you with one message, which is please join us.
Please join us in drafting these standards and implementing them and making sure that
they work properly in the real world.
Yeah, and we tend to lurk around in the ROTS working group and the CCC at the station.
Thank you.
Okay, I'll repeat the question.
Is there like a representation for applying the service?
I think there probably is.
Yes, so the question was for ACME, for the ACME integration of remote other stations,
whether there is example codes or reference implementation.
I think there probably is.
I think I've seen a demo from the person who was drafting this.
But yeah, we can get in touch.
