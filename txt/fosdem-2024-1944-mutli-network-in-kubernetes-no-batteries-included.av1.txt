Check one two, check one two, all right.
Thank you everybody for coming to our talk about multi-network in Kubernetes and how
there are no batteries included.
My name is Doug Smith.
I'm a maintainer of Multis CNI, which is a multi-networking plugin and also part of a
working group related to it and I'm joined by Miguel.
I'm Miguel Duarte.
I'm a software engineer working for Red Hat, particularly in the OpenShift Virtualization
Networking Team.
I'm also a member of the Network Blumbers workgroup and yeah, sometimes work with Doug
on this kind of stuff.
Awesome.
So, we've got to rip through this pretty rapidly and it's a pretty complex problem space,
but we're going to run you through it as quick as we can.
So we're going to look at what exactly multi-networking is in Kubernetes and kind of show you what
the problem is that we're looking at.
There's also kind of like current set of solutions and then also future solutions that we're
looking at as well.
And even if you're not necessarily interested in the multi-networking problems in Kubernetes,
we kind of hope that you're going to be interested in sort of the problems that we've identified
that we think are really common to a lot of engineering problems in general and especially
for open source communities.
We also have a demo for you to watch at home because we have some short time.
So the first question we should be asking is exactly what is multi-networking in Kubernetes.
So the thing is it kind of isn't because it's not something that Kubernetes actually
is interested in solving.
What do I mean by this?
So the Kubernetes networking model pretty much says that any pod on the cluster can reach
any other pod in the system.
Cool.
How does it do it?
Like one interface on each pod connected to the same network.
One interface.
If you need more, well, it's outside of Kubernetes.
The community pitched in together and implemented that out of three.
But well, first, why do you want multiple networks in Kubernetes?
For instance, like network isolation, let's say you want to meet the compliance requirements
of like you need to separate traffic not only in software but only physically in the network.
This kind of thing happens every day.
And for that you need multiple interfaces.
Or for instance, you want to implement like a firewall or something.
Well, you'll need at least two interfaces.
So this is a reality.
There's a need for it and Kubernetes does not do it on its own.
So the problem, you don't have batteries for this.
You can do it.
The community has provided ways for this to happen but it's out of three.
And you need to deploy a bunch of stuff for this.
So you need to deploy like controllers on the nodes.
You need to add more and more pieces.
So it's solvable but it's not entry.
It's not native to Kubernetes.
Furthermore, while it works, its user experience is challenging to say the least.
Like it's cumbersome to use.
It feels clumsy.
There are a lot of ways for you to get it wrong.
Like if you just put an attribute that does not exist or put a typo on it.
Well, it depends on the implementation what will happen.
And at the end of the day, if you have something that is error prone, a lot of people are going
to make errors in it.
In one word, this is pretty much like Arcane Knowledge that needs to be used in it.
So current solution for it is multi-CNI.
So multi-CNI is a CNI multiplexer.
So CNI is your container network interface.
It's an API that allows you to specify how you are going to run plugins that talk to
this API in order to plumb your networks, how you're going to connect your network interfaces
into your pod to the rest of your network in Kubernetes.
What MULTIS is designed to do is to multiplex CNI plugins.
So you use custom resources, which are extensions of the Kubernetes API.
They're not natively part of the API.
They're a way that you extend it.
And they give you a way to quote unquote kind of trick the platform.
So you add MULTIS CNI into your network.
You populate these custom resources with CNI configurations, but CNI configurations are
JSON and Kubernetes resources are YAML from a user perspective.
So you kind of mix both of those, and I'll give an example of that in a moment.
But we also have an effort that's ongoing for a Kubernetes native multi-networking.
So what this would do is take kind of this concept that we have out of tree and get these
pieces natively into the Kubernetes API.
So we would actually extend that.
And probably as a building block, we may actually implement them as custom resources, which
is a detail here.
The one thing to keep in mind, though, is that this will be an extension of the API
without an implementation for you natively.
So it will actually still require an add on itself, which is also a bit of a challenge.
But we really like the idea of extending the API.
If you take a look here, you're going to see this is a Kubernetes PodSpec.
You've probably seen them before, but we use an annotation.
And those are freebies.
Anyone can add an annotation.
We have a specification for how it should look, and we see that specification.
But it's got JSON in there.
So if you're walking through this object in the API, you hit this.
What do you have to do?
You have to parse it, which is no fun, no fun in the least.
Now with the Kubernetes native, you are going to just have it all YAML.
So if you're writing a Kubernetes controller and you're using client go, you're just going
to walk through this easy as pie.
So it should be a lot easier.
But we have to ask ourselves, what does the future look like?
It's kind of a complicated scenario.
Number one, we'll probably still have multi-CNI.
It's out there.
People use it.
They're going to continue to use it.
And then we might also have Kubernetes native multi-networking.
So we've got these two things.
But there's a bunch of other projects in the space that may be up and coming as well.
So CNI 1.0 has been around for quite a while.
And CNI plugins run on disk on the host.
They're not actually containerized.
CNI 2.0 would make a step forward to being able to containerize those and also would
give us an opportunity to kind of update this API.
We also have in the works the Kubernetes network interface, which is a proposal which would
bring CNI and Kubernetes potentially closer.
CNI itself is container orchestration agnostic.
It doesn't actually relate specifically to Kubernetes.
It was actually kind of invented in a parallel time to Kubernetes.
So before Kubernetes won the container orchestration engine war, there were a bunch of different
container orchestration engines.
So it tried to fit the needs of all of them.
But Kubernetes is the winner, and we kind of need a way to get a little bit closer.
So let's figure out the lessons that we have learned.
The first one is sometimes you have political problems.
We want to extend the pod network and the pod object to get these items here natively
in there.
And maybe it's not so much a political problem as it is like a people and intentions kind
of problem, like what are we trying to solve here?
And not everyone sees this exactly the way.
And this is a very core part of Kubernetes.
If you've ever used Kubernetes, you've definitely spun up a pod, you've definitely touched a
pod object before, or an abstraction of it like a deployment or whatnot.
So to extend this, to add this network is hotly contested.
And there's more than that.
Like first, as Doug said before, things is that APIs are forever.
Not in a sense that you have to maintain stuff backwards compatible, but pretty much that
Maltes exists, solves the problem.
And if you want to have multiple interfaces in a pod, Maltes is already doing that.
Are you actually going to update all the manifests of your deployments to comply with this new
API to stuff that is running in production?
Well, maybe not.
So there's this to kind of have in mind.
Next scope creep.
Like everybody wants to solve a different problem.
And it's very hard to get quite focused on the, let's say, the least common denominator
of the problem space.
I mean, just doing that has been extremely challenging in these last six, seven, eight
months, year.
I don't know, like I lost track of it.
And finally, handling a technological problem is a lot simpler than dealing with people
and opinions.
It's very, very, very easy to clash on those.
Like it's hard to get to choose like a restaurant between your four friends to go out tonight.
So it's even harder to get you to like agree on what the API would look like on something
so critical and so, let's say, central and paramount as the pod spec, for instance.
And here's like the, we would really like you to take a look at this demo.
But again, like, yeah, better do that at home.
Like just hit or scan this or hit the link there.
And it's short, couple of minutes.
And you'll see how the native current effort for native multi-networking looks like from
the user's perspective.
And yeah, that's pretty much it.
So any questions you have?
Well, fire away.
Any questions?
Can these additional interfaces be used to connect devices that are outside of the data
center via VPN, for example, which is a problem I've been trying to deal with and couldn't
find manageable solutions?
Yeah, thank you.
Okay.
If these interfaces can be used to connect the pod, for example, to via VPN to external
networks.
Oh, yeah, absolutely.
So this, something that you can do is to use these to connect to existing resources that
are already in your network.
So I guess a VPN absolutely could be an example.
And so oftentimes the reason that people use these additional networks is they have existing
infrastructure and they go to deploy Kubernetes to become more of a like a cloud native approach.
But they have legacy systems that they need to integrate with.
So if you've got existing networks, this would be a reason to do that kind of as a sidecar
that you could go out to a network like that.
Great question.
Can you talk a bit more about KNI and how it relates to the multi-network problem?
So that is an excellent question.
So I would say that one thing that we're trying to solve is that the way that, for example,
multis CNI works is somewhat inefficient.
You have this flow to create a pod and in that flow is a call to CNI, which then is
called today you would usually use multis to do that.
When multis is called, it stops that creation of the pod and goes and queries the API, the
Kubernetes API itself.
And KNI may be one opportunity to make this flow linear in order to pass information directly
to some type of multi-network solution that already has the information from the API,
for example, instead of having to interrupt that flow to call the API.
So that's one possibility.
Another possibility is that, at least from my perspective, as Miguel said, APIs are forever.
So multis, I'm a maintainer of it.
Certainly it'll be around for a while, but as we may get the Kubernetes native multi-networking,
it may also be a layer to do compatibility as well between kind of the new way of thinking
and the old way of thinking.
Is that helpful?
Nice to see you, Gargay.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
