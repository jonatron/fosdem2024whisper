WEBVTT

00:00.000 --> 00:01.000
Okay.

00:01.000 --> 00:02.000
All right.

00:02.000 --> 00:03.000
All right.

00:03.000 --> 00:04.000
Okay.

00:04.000 --> 00:06.000
We are ready to go.

00:06.000 --> 00:07.000
All right.

00:07.000 --> 00:08.000
Thanks, everybody.

00:08.000 --> 00:11.000
Thanks for sticking around till the end today.

00:11.000 --> 00:15.000
And a special shout out to those of you on the live stream as well.

00:15.000 --> 00:17.000
My name is Adam Litke and this is Near Software.

00:17.000 --> 00:23.000
And today we're going to get our money's worth out of this laptop.

00:23.000 --> 00:29.000
Something is not right here.

00:29.000 --> 00:31.000
I keep flipping on and off.

00:31.000 --> 00:32.000
Let's see.

00:32.000 --> 00:33.000
I'll do my best here.

00:33.000 --> 00:39.000
So we've come a long way since Linus introduced Linux to the world back in 1991.

00:39.000 --> 00:44.000
What started off on his personal computer is deployed pretty much everywhere these days

00:44.000 --> 00:47.000
in increasingly complex scenarios.

00:47.000 --> 00:50.000
Take Kubernetes, for example.

00:50.000 --> 00:53.000
Everyone's favorite clustered container orchestrator,

00:53.000 --> 00:58.000
which runs open source up and down the entire stack from the cluster nodes

00:58.000 --> 01:02.000
to the Kubelet and to the container runtime itself.

01:02.000 --> 01:09.000
And developers haven't stopped building or debugging screens.

01:09.000 --> 01:17.000
Kubevert is a Kubernetes add-on that allows you to seamlessly run your virtual machines in Kubernetes.

01:17.000 --> 01:24.000
And since the VMs are running in pods, like any other workload on Kubernetes,

01:24.000 --> 01:27.000
they integrate really well with whatever else is deployed there,

01:27.000 --> 01:33.000
be it your applications, storage, networking, monitoring, et cetera.

01:33.000 --> 01:40.000
And as people continue to deploy Kubernetes and Kubevert to host their mission-critical workloads,

01:40.000 --> 01:44.000
naturally they wonder what will happen when disaster strikes.

01:44.000 --> 01:48.000
Disaster recovery software exists to protect your data

01:48.000 --> 01:53.000
and return to normal operations as quickly as possible.

01:53.000 --> 01:57.000
And this is typically achieved using redundancy.

01:57.000 --> 02:03.000
So data can be replicated from primary environments to secondary environments,

02:03.000 --> 02:06.000
and applications, including virtual machines,

02:06.000 --> 02:10.000
are able to be started on the secondary environment at a moment's notice,

02:10.000 --> 02:12.000
should that be required.

02:12.000 --> 02:19.000
In this particular scenario, we have a primary data center DR1 in the west,

02:19.000 --> 02:23.000
a secondary data center DR2 in the east,

02:23.000 --> 02:27.000
and a hub cluster located somewhere in between.

02:27.000 --> 02:30.000
Now we prefer to run our applications on the primary environment

02:30.000 --> 02:33.000
because it's closer to where our customers are.

02:33.000 --> 02:37.000
But thanks to continuous data replication, we can rest easy knowing.

02:37.000 --> 02:45.000
We can start the application up on DR2 when required.

02:45.000 --> 02:50.000
So ramen DR is software that enables disaster recovery

02:50.000 --> 02:53.000
for multi-cluster Kubernetes environments.

02:53.000 --> 02:56.000
It does this by working with the storage to enable data replication

02:56.000 --> 03:01.000
according to a DR policy set by the administrator.

03:01.000 --> 03:07.000
And it talks with open cluster management to manage application placement,

03:07.000 --> 03:12.000
failover, and relocation flows.

03:12.000 --> 03:14.000
Today we're going to simulate this disaster for you.

03:14.000 --> 03:18.000
We're going to start by disabling the primary environment.

03:18.000 --> 03:23.000
We can then failover our virtual machine to the secondary environment.

03:23.000 --> 03:27.000
And I just want to note here that failover is different from live migration

03:27.000 --> 03:31.000
because live migration would require both environments to be up.

03:31.000 --> 03:35.000
In this specific scenario, obviously, we don't have access to DR1.

03:35.000 --> 03:39.000
So failover is going to take a couple of minutes,

03:39.000 --> 03:42.000
but we can be confident that the app can start back up on the secondary

03:42.000 --> 03:47.000
and environment with minimal data loss.

03:47.000 --> 03:50.000
So I've been kind of introducing a bunch of different components here

03:50.000 --> 03:53.000
that's quite the menu of open source ingredients.

03:53.000 --> 03:58.000
KubeVert is a operator managed Kubernetes deployment,

03:58.000 --> 04:02.000
which packages libvert and QMU into a container image,

04:02.000 --> 04:06.000
allowing you to run your virtual machines inside of a pod.

04:06.000 --> 04:09.000
It also comes with other utilities to help you manage your virtual machine

04:09.000 --> 04:13.000
storage and networking requirements.

04:13.000 --> 04:17.000
Rook is software that manages and integrates

04:17.000 --> 04:20.000
self-storage into the Kubernetes platform.

04:20.000 --> 04:25.000
Open cluster management stitches together multiple Kubernetes clusters

04:25.000 --> 04:29.000
and provides for application management, placement, scheduling.

04:29.000 --> 04:38.000
And then RoninDR is adding on those DR flows to open cluster management.

04:38.000 --> 04:43.000
So when we're considering a realistic multi-cluster DR environment,

04:43.000 --> 04:47.000
it's a beautiful thing, kind of like this bowl of ramen here to tempt you

04:47.000 --> 04:48.000
at dinner time.

04:48.000 --> 04:52.000
However, it's also complicated and expensive to operate,

04:52.000 --> 04:56.000
especially when we consider like the single developer use case.

04:56.000 --> 05:00.000
So the question we're trying to answer here is how can we enable development

05:00.000 --> 05:07.000
in this open source software stack without huge cloud budgets?

05:07.000 --> 05:11.000
And our answer is to scale down that environment so that it can run inside

05:11.000 --> 05:18.000
the kind of laptop that most of us are carrying around with us each day.

05:18.000 --> 05:23.000
And NIR has prepared a live demo right on this laptop that you're looking at

05:23.000 --> 05:25.000
that's going to show all this stuff working together,

05:25.000 --> 05:27.000
and we're going to simulate that disaster for you.

05:27.000 --> 05:28.000
So take it away.

05:28.000 --> 05:29.000
Yep.

05:29.000 --> 05:34.000
And I'm going to mute it so we don't annoy the live stream people.

05:37.000 --> 05:38.000
Okay.

05:38.000 --> 05:47.000
Put that in your pocket.

05:47.000 --> 05:48.000
Yep.

05:48.000 --> 05:49.000
Okay.

05:49.000 --> 05:54.000
So this is our stack, right?

05:54.000 --> 05:55.000
Three clusters.

05:55.000 --> 05:58.000
We have two identical clusters.

05:58.000 --> 05:59.000
Everything is ramen.

05:59.000 --> 06:04.000
And we are going to put it inside this laptop to see that we can do it

06:04.000 --> 06:07.000
because they are small and cheap.

06:07.000 --> 06:13.000
So what we want to do today is to stuff three data centers with ramen and

06:13.000 --> 06:18.000
kovir and stuff and a lot of other components and large part of Europe

06:18.000 --> 06:21.000
and stuff everything inside this laptop.

06:21.000 --> 06:23.000
Now note about this environment.

06:23.000 --> 06:25.000
The clusters are all in the same laptop,

06:25.000 --> 06:31.000
but they are remote clusters on different regions.

06:31.000 --> 06:34.000
And the cluster is standalone with its own storage.

06:34.000 --> 06:40.000
So how can we prepare this laptop for the demo?

06:40.000 --> 06:46.000
So I have a pack of instant ramen DR, which is very easy to use.

06:46.000 --> 06:47.000
You want one command.

06:47.000 --> 06:52.000
DRM start with the environment of your flavor.

06:52.000 --> 06:55.000
This in case is a kovir environment.

06:55.000 --> 07:00.000
And then you let it cook for 10 minutes until everything is ready.

07:01.000 --> 07:04.000
Sorry.

07:04.000 --> 07:07.000
So we are not going to wait 10 minutes now because it's a little thing.

07:07.000 --> 07:12.000
I prepared the environment before the talk and we'll just use it.

07:12.000 --> 07:16.000
So whatever we need, we need a Git repo because we're going to use GitOps.

07:16.000 --> 07:23.000
We will give Ocm some Git repo to pull the VM resources and deploy the application.

07:23.000 --> 07:27.000
So we use Adam's repo, Ocm kovir samples.

07:27.000 --> 07:32.000
And I talked it to customize the VM with SSH public key.

07:32.000 --> 07:41.000
So let's jump into the demo and increase the font size a bit.

07:41.000 --> 07:52.000
So I'm using a little tool to save my typing error for you and make everything more colorful.

07:52.000 --> 07:56.000
So first look what we have in this laptop.

07:56.000 --> 07:58.000
We have three clusters.

07:58.000 --> 08:04.000
DR1 is the primary cluster where we run the VM.

08:04.000 --> 08:06.000
DR2 is the secondary cluster.

08:06.000 --> 08:10.000
Something bad happens to DR1 and something bad will happen.

08:10.000 --> 08:11.000
Don't tell anyone.

08:11.000 --> 08:18.000
And Hub is orchestrating everything and controlling the other clusters.

08:18.000 --> 08:24.000
Now each of these are libvirt VMs inside the laptop.

08:24.000 --> 08:28.000
So let's look inside one of the clusters.

08:28.000 --> 08:34.000
We can use kubectl, just a normal kubectl clusters.

08:34.000 --> 08:39.000
And we see that we have a lot of stuff that DR1 installed for us.

08:39.000 --> 08:45.000
The most important parts for the demo are the kovir parts that you will run the VM,

08:45.000 --> 08:52.000
the CDI that will provision the VM disk from container image.

08:52.000 --> 08:54.000
Of course, it will be stored.

08:54.000 --> 09:03.000
So we have a complete RookSafe system inside that using the local disk of the cluster.

09:03.000 --> 09:09.000
And this will provide storage for the VM disk and volume replication between the clusters.

09:09.000 --> 09:14.000
And to protect the VM, we have the Raman DR cluster operator,

09:14.000 --> 09:18.000
which orchestrates the DR flows.

09:18.000 --> 09:25.000
And finally, we need the open cluster management components that lets Raman control the clusters.

09:25.000 --> 09:34.000
Because Raman extends open cluster management and depend on it.

09:34.000 --> 09:39.000
So let's look inside the Git repo. I'm running this inside the clone of the Git repo from other.

09:39.000 --> 09:46.000
The important parts in this repo for the demo are this VM, VM standalone pbc.js.

09:46.000 --> 09:49.000
This is VM optimized for the environment.

09:49.000 --> 09:53.000
The subscription, which is OCM resources for the VM.

09:53.000 --> 09:57.000
And DR are the Raman DR resources.

09:57.000 --> 10:01.000
So let's look first at the VM.

10:01.000 --> 10:05.000
We have a quick look to see what we have there.

10:05.000 --> 10:10.000
We will not look inside the YAMLs. You can check the Git repo later.

10:10.000 --> 10:16.000
We have a VM configuration. This VM is using a pbc because we are using a pbc-based VM.

10:16.000 --> 10:21.000
So we have this pbc here. And we need to provision a pbc somehow.

10:21.000 --> 10:27.000
So we have the source YAML, which tells CDI how to provision the pbc disk.

10:27.000 --> 10:31.000
So we can apply this customization to cluster DR1.

10:31.000 --> 10:35.000
And this will start the VM on cluster DR1, but we are not going to do it.

10:35.000 --> 10:38.000
Because nobody will protect this VM.

10:38.000 --> 10:43.000
It's just like a port that you start and it goes down and nobody protects it.

10:43.000 --> 10:48.000
So what we want to do is create OCM application.

10:48.000 --> 10:52.000
OCM application. We will use subscription-based application.

10:52.000 --> 10:58.000
These resources tells OCM how to protect the application, how to create it,

10:58.000 --> 11:05.000
like which cluster set to use, where Git repo is, the namespace that the VM is running,

11:05.000 --> 11:10.000
where to place the VM, and subscription ties everything together.

11:10.000 --> 11:15.000
So to start the VM, we apply this customization to the hub.

11:15.000 --> 11:21.000
Everything is done on the hub. And then OCM and Raman later will do the right thing.

11:21.000 --> 11:30.000
So at this point, OCM is starting the VM on cluster DR1 and we can watch it.

11:30.000 --> 11:35.000
Using kubctl to get the VM, VMI port and pbc.

11:35.000 --> 11:40.000
And we can see that the pbc is already bound.

11:40.000 --> 11:50.000
And Virt launcher is running and we have an IP address, so ROVM is running.

11:50.000 --> 11:57.000
But let's inspect the VM a little bit more to see where is our disk.

11:57.000 --> 12:03.000
So we can use ROVM Cess kubctl plugin to look at Cess layer.

12:03.000 --> 12:09.000
And we can run the RBDU command, in this case on cluster DR1.

12:09.000 --> 12:14.000
And we see that we have RBD image created for our VM.

12:14.000 --> 12:20.000
If we look inside the pbc, we will find this volume there.

12:20.000 --> 12:26.000
So if something bad happened to cluster DR1, we lose the running VM and the disk.

12:26.000 --> 12:31.000
And this image will be gone and we lost all the data.

12:31.000 --> 12:34.000
So how can we prevent it?

12:34.000 --> 12:37.000
We want to protect this VM is Raman.

12:37.000 --> 12:46.000
So to do this, we must tell OCM first that Raman is going to take over this VM and ACM should not change it.

12:46.000 --> 12:54.000
We do this by notating the placement with a special annotation and at this point Raman can take over.

12:54.000 --> 12:57.000
So how do we protect this with Raman?

12:57.000 --> 13:00.000
We need to apply the Raman resources.

13:00.000 --> 13:03.000
Basically it's one resource, a DRPC.

13:03.000 --> 13:11.000
The DRPC tells Raman how to find the application, how to protect it, which pbc should be protected and what is the policy.

13:11.000 --> 13:15.000
We are not going to look inside now, we can check the gtrepolator.

13:15.000 --> 13:19.000
So to protect the VM, we apply disk customization.

13:19.000 --> 13:25.000
Again on the hub, then Raman will do the right thing on the right cluster.

13:25.000 --> 13:30.000
So once we did it, our VM is protected Raman and you can watch it again.

13:31.000 --> 13:35.000
This time I'm watching also VRG and VR resources.

13:35.000 --> 13:38.000
VRG is the volume replication group.

13:38.000 --> 13:51.000
We have one such resource per each protected application and volume replication is the resource that entails the volume replication for each pbc.

13:51.000 --> 13:55.000
So we have one of them replication for every pbc.

13:55.000 --> 14:05.000
Now both of them are ready and primary, primary windows, this is the primary cluster, replicating data to the secondary cluster.

14:05.000 --> 14:12.000
So what does it mean that we replicate data to the other cluster?

14:12.000 --> 14:21.000
If you look again on the RBD images, if you remember we have seen that we have an RBD image on the primary cluster.

14:21.000 --> 14:28.000
If you run the same command on the secondary cluster, and this time I'm running the same command on context DR2.

14:28.000 --> 14:38.000
And we will find that we have an image on the primary cluster and we have the same image on the secondary cluster.

14:38.000 --> 14:44.000
So what's going on under the cover is that when Raman enables volume replication,

14:44.000 --> 14:52.000
a secondary replica of the image is created on the secondary cluster,

14:52.000 --> 15:02.000
and the RBD mirror demon is starting to replicate rights from this image on the primary cluster to this image on the secondary cluster.

15:02.000 --> 15:09.000
So if something bad happens to cluster DR1, we can use the secondary image to start the VM at the time of the last replication.

15:10.000 --> 15:19.000
So the last thing to show about the VM is that we have a logger inside updating the log file every 10 seconds.

15:19.000 --> 15:24.000
We can access the log file using the VIRT CTL SSH.

15:24.000 --> 15:32.000
We just run this command to see the start of the log and we see the line where the service was started,

15:32.000 --> 15:36.000
and then we see one line every 10 seconds.

15:36.000 --> 15:45.000
This will help us verify later when we recover from a disaster that we got the right data from the disk.

15:45.000 --> 15:49.000
So now we are ready for everything.

15:49.000 --> 15:53.000
Let's try to create a disaster.

15:53.000 --> 15:59.000
So one thing that is easy to do on the laptop is to suspend the cluster DR1.

15:59.000 --> 16:02.000
If you remember this is a Libre VM, so we can just suspend it.

16:02.000 --> 16:05.000
Now everything running there stopped.

16:05.000 --> 16:11.000
So let's try to access the VM again with VIRT CTL SSH.

16:11.000 --> 16:19.000
Let's try to tail the log and let's see if it works.

16:19.000 --> 16:26.000
Well, it does not seem to work because of course we suspended the VM so nothing there is accessible.

16:26.000 --> 16:33.000
If we had an important service on this VM, our users would be not happy now.

16:34.000 --> 16:37.000
So how can we fix this?

16:37.000 --> 16:39.000
Adam, do we have any idea?

16:39.000 --> 16:41.000
I was hoping you would tell us.

16:41.000 --> 16:47.000
Yes, so because our VM is replicated, we can just failover to the other cluster quickly.

16:47.000 --> 16:49.000
How would we failover?

16:49.000 --> 16:54.000
If you remember that we installed the DRPC, we can patch the DRPC.

16:54.000 --> 16:59.000
We set the action to failover and we set the failover cluster.

16:59.000 --> 17:04.000
And once we did it, Raman starts the failover.

17:04.000 --> 17:07.000
And you can start watching the failover on the other cluster.

17:07.000 --> 17:14.000
I'm running this again on the DR2 cluster because DR1 is not accessible.

17:14.000 --> 17:16.000
And we see that we have a PPC.

17:16.000 --> 17:17.000
It's impending.

17:17.000 --> 17:20.000
We have a volume replication group.

17:20.000 --> 17:26.000
We have a volume replication, but the volume replication book is not primary yet.

17:26.000 --> 17:28.000
It will take a while until the VM is stopped.

17:28.000 --> 17:33.000
So while you wait for it, let's understand what's going on under the cover.

17:33.000 --> 17:44.000
So the RBD image on the secondary cluster was replica, pulling data from the master for the main cluster.

17:44.000 --> 17:51.000
Raman has to stop this replication and promote it to a primary image that will replicate to another cluster.

17:51.000 --> 17:59.000
Once this is done, the VRG will be marked as primary and it should happen any second.

17:59.000 --> 18:03.000
And at this point Raman will change the application placement.

18:03.000 --> 18:05.000
It just became primary.

18:05.000 --> 18:09.000
So now Raman changed the placement of the application.

18:09.000 --> 18:17.000
And Ocm will see the change and will redeploy the VM on the second cluster using the subscription.

18:17.000 --> 18:21.000
And this should happen any second now.

18:21.000 --> 18:30.000
When Ocm deploys the application, it will reuse the PPC that Raman has restored and connected to the right RBD image.

18:30.000 --> 18:32.000
And it just happened.

18:32.000 --> 18:36.000
We see that the VRT Launcher is running.

18:36.000 --> 18:38.000
The VM is up.

18:38.000 --> 18:39.000
We have an IP address.

18:39.000 --> 18:45.000
So if we add this important service of the VM, this service should be absent.

18:45.000 --> 18:47.000
And it will be used as a VRT.

18:47.000 --> 18:50.000
But how do we know that we got the right data?

18:50.000 --> 18:54.000
Maybe we got just a new application with empty disk.

18:54.000 --> 18:57.000
Let's check the disk.

18:57.000 --> 18:59.000
Again, we can use the logger.

18:59.000 --> 19:04.000
We just dumped the entire log using SSH.

19:04.000 --> 19:09.000
This time I'm connecting to the cluster, the R2.

19:09.000 --> 19:20.000
And we see all the logs from the VM that run on cluster DR1 until we created the disaster.

19:20.000 --> 19:25.000
And we see the new logs when the VM started to run on cluster DR2.

19:25.000 --> 19:35.000
Note that we have a gap here between the last line logged when the VM was running on DR1 and the first log,

19:35.000 --> 19:39.000
which is about three minutes in this case.

19:39.000 --> 19:48.000
This gap depends on how fast we started the failover and the tech that there was an issue with the cluster.

19:48.000 --> 19:50.000
So we had a short downtime.

19:50.000 --> 19:51.000
The VM is running.

19:51.000 --> 19:52.000
We got all the data.

19:52.000 --> 19:55.000
Looks like a successful failover.

19:55.000 --> 19:57.000
So what's next?

19:57.000 --> 20:02.000
In a real cluster, you would try to fix the cluster, recover it.

20:02.000 --> 20:04.000
Maybe you need to reinstall it.

20:04.000 --> 20:09.000
At this point, you can relocate the VM or maybe later during some maintenance middle,

20:09.000 --> 20:12.000
you will relocate the VM back to the primary cluster.

20:12.000 --> 20:14.000
In this demo, we are done.

20:14.000 --> 20:17.000
And it's time for questions.

20:17.000 --> 20:22.000
The first three questions we'll get in Sotramen.

20:34.000 --> 20:38.000
Go ahead.

20:38.000 --> 20:42.000
The question is what about the IP address of the virtual machine?

20:42.000 --> 20:45.000
We're paying attention and noted that change.

20:45.000 --> 20:47.000
So what would you suggest?

20:47.000 --> 20:51.000
Sotramen does not do anything about the IP address.

20:51.000 --> 20:58.000
I think in a real application, you will have some load balancer to make sure that you can switch from one cluster to the other cluster.

20:58.000 --> 21:03.000
Probably using the DNS system because you have a nice name for the DNS.

21:03.000 --> 21:08.000
But basically, you will do what VIRT CTL is doing when you connect to the VM.

21:08.000 --> 21:21.000
You use the VM name, the name space, and you have to find the actual address.

21:21.000 --> 21:22.000
Yes.

21:22.000 --> 21:24.000
Very nice demo.

21:24.000 --> 21:26.000
How much you run at home?

21:26.000 --> 21:29.000
I don't have any cloud copies.

21:29.000 --> 21:33.000
I can re-image it to 4G.

21:33.000 --> 21:35.000
Is 60G enough?

21:35.000 --> 21:38.000
16 will be too low.

21:38.000 --> 21:39.000
Yes.

21:39.000 --> 21:42.000
So the question is what do we need to run it at home?

21:42.000 --> 21:47.000
So first, you can try it at home, but you need big enough laptop.

21:47.000 --> 21:52.000
I think for COVID, you need like 32G with Sotramen.

21:52.000 --> 21:53.000
Yes.

21:53.000 --> 22:00.000
Because we have two clusters with 8GB, maybe you can trim them down a little bit, but 16 will be too low.

22:00.000 --> 22:02.000
And...

22:02.000 --> 22:04.000
Maybe you need a laptop.

22:04.000 --> 22:05.000
Yes.

22:05.000 --> 22:06.000
Maybe you need a laptop.

22:06.000 --> 22:08.000
If a door is wide, it's not so.

22:08.000 --> 22:11.000
If a door would be easier because this is what we use.

22:11.000 --> 22:14.000
But it should work on anything.

22:15.000 --> 22:18.000
We continue the question.

22:18.000 --> 22:25.000
Can I use two laptops, one for disaster recovery and one for the one that you made and the other for disaster recovery?

22:25.000 --> 22:29.000
Let's say old laptop.

22:29.000 --> 22:32.000
Repeat the question.

22:32.000 --> 22:35.000
I didn't answer the question exactly.

22:35.000 --> 22:37.000
Can you repeat it?

22:37.000 --> 22:41.000
Your presentation is from the same laptop.

22:41.000 --> 22:45.000
Can I use the solution for two laptops?

22:45.000 --> 22:51.000
So the question is, can we use different machines for this?

22:51.000 --> 22:57.000
You can, but it will be harder because you need to set up the network between them.

22:57.000 --> 23:04.000
In the same laptop, it's much easier because MiniCube handed most of the stuff for you.

23:04.000 --> 23:10.000
If you use different machines, you need to make sure that the clusters are accessible to...

23:10.000 --> 23:13.000
So it will be harder.

23:13.000 --> 23:16.000
I've got one over here.

23:16.000 --> 23:17.000
Yes.

23:17.000 --> 23:22.000
Is it required to use SIF or you can use an OSR by system?

23:22.000 --> 23:23.000
Repeat the question.

23:23.000 --> 23:25.000
Do we have to use SIF?

23:25.000 --> 23:31.000
Currently, we work with SIF, so it's optimized for SIF and it works.

23:31.000 --> 23:36.000
And we have a complete tool that you set and configure it.

23:36.000 --> 23:41.000
If you want to use something else, we have support for any storage, basically,

23:41.000 --> 23:43.000
but it doesn't work on Kubernetes yet.

23:43.000 --> 23:45.000
It's very commonly on the shift.

23:45.000 --> 23:52.000
It needs more work.

23:52.000 --> 23:53.000
Yes.

23:53.000 --> 23:56.000
The primary site is down.

23:56.000 --> 24:02.000
Is there any mechanism for the extra machine for starting by mistake, by itself?

24:02.000 --> 24:12.000
The question was, once the cluster is down, do we have any protection that the virtual machine will not start again on the same cluster?

24:12.000 --> 24:16.000
So we don't have any protection at the ramen level because SIF is protecting us.

24:16.000 --> 24:23.000
If the same VM starts again, let's say we resume the cluster,

24:23.000 --> 24:28.000
the application is still running then it will continue to run and try to write to the disk,

24:28.000 --> 24:32.000
which is fine because the disk is not replicated at this point.

24:32.000 --> 24:37.000
Because the application is done on the destination cluster.

24:37.000 --> 24:40.000
It's pulling data from the primary.

24:40.000 --> 24:46.000
Usually it will just fail or go into some error state and in real application,

24:46.000 --> 24:51.000
when ramen detects that it should not run, it will shut it down.

24:51.000 --> 24:56.000
So it's safe.

24:56.000 --> 24:58.000
There is one more question.

24:58.000 --> 24:59.000
Yes.

24:59.000 --> 25:01.000
Just because it's the end of the day.

25:01.000 --> 25:02.000
Just one more question.

25:02.000 --> 25:07.000
What happens when the hub that was controlling the two data centers goes down?

25:08.000 --> 25:12.000
The question was what happens when the hub goes down?

25:12.000 --> 25:13.000
Very good question.

25:13.000 --> 25:20.000
In a real setup, in OpenShift you have a hub recovery setup, so actually you need two hubs.

25:20.000 --> 25:22.000
One passive hub and one active hub.

25:22.000 --> 25:28.000
And there is a lot of setup that backup the hub and restore it.

25:28.000 --> 25:33.000
But for testing it doesn't matter.

25:33.000 --> 25:39.000
And also hopefully you're not running customer visible or end user visible workloads on the hub.

25:39.000 --> 25:46.000
So if it goes down you can repair it and people won't be quite as urgent of a disaster.

25:46.000 --> 25:51.000
So hopefully the other sites don't fail at the same time.

25:51.000 --> 25:53.000
Alright, thanks everybody for coming.

25:53.000 --> 25:54.000
What a good question.

