WEBVTT

00:00.000 --> 00:07.000
So, welcome to the next talk and welcome to Peter Levin, who's trying to answer the

00:11.280 --> 00:16.280
question if the first AGI instance will be free or open source software.

00:16.280 --> 00:23.280
Okay. Thank you very much for the introduction and welcome all to this talk, indeed.

00:23.280 --> 00:30.280
Maybe I'll wait a minute. Okay. So, indeed, I will be talking about AGI or general artificial

00:44.840 --> 00:51.840
intelligence and I will be trying to look at the question whether we can expect this

00:52.520 --> 00:59.520
to be open source or free software. And I'm a professor from the VUB. There where I do

01:00.400 --> 01:07.000
research in artificial intelligence. So, VUB is a sister university of VLB, where we are.

01:07.000 --> 01:14.000
So, I'm very happy to be here. And before diving straight into AGI, let's first have

01:14.000 --> 01:21.000
a look at the advances that we saw over the last decade. Because we did see some advances

01:21.320 --> 01:28.320
that were quite significant. First of all, I think 2015 we saw a big breakthrough made

01:29.680 --> 01:36.680
by Google DeepMinds, where they developed an agent that could actually learn to play

01:36.800 --> 01:41.480
video games, Atari video games. It's an old video game system, but it's a video game

01:41.480 --> 01:47.360
system that I used to play when I was a kid. So, this is generally considered to be quite

01:47.360 --> 01:53.680
a breakthrough because an AI system was developed that could on its own play these games and

01:53.680 --> 02:00.680
play a wide range of these games. Next, they made another breakthrough, Google DeepMinds.

02:01.160 --> 02:05.160
They came up with an agent that could actually play the game of Go. And the game of Go is

02:05.160 --> 02:12.160
a board game comparable to board games such as chess, but generally it's a harder board

02:13.080 --> 02:17.560
game. So, it is a board game for which it was not expected that we could actually build

02:17.560 --> 02:24.560
an agent that could solve this difficult game. But they did achieve to make such an agent

02:27.000 --> 02:33.720
and they actually beat Lisa Doll, who is here on the slides, who was the world champion

02:33.720 --> 02:40.720
in Go. This program, this AI agent was actually able to beat this human player.

02:41.560 --> 02:48.560
Next, in 2017, they developed AlphaGo Zero, another Go agent. But here, instead of learning

02:52.680 --> 02:59.600
from data, which was the case in the former agents, they actually learned solely by playing

02:59.600 --> 03:04.840
against themselves. So, the agent was just playing against another agent and there was

03:04.840 --> 03:11.840
no data used from any human games. So, this was also assumed to be quite a breakthrough

03:12.200 --> 03:17.520
because, of course, if you can build systems that go out of distribution that can learn

03:17.520 --> 03:21.560
more than data has to offer, that's of course something that is very interesting because

03:21.560 --> 03:28.560
that would allow us to learn also outside of the capabilities that we as human beings have.

03:29.440 --> 03:35.940
Later, also DeepMinds, they came up with AlphaFold, another problem that was thought to be very

03:35.940 --> 03:42.940
complex where you want to predict the structure of proteins simply based on the nucleotide

03:43.800 --> 03:50.800
sequences. But they also were able to construct a deep neural network to do this job. And

03:51.160 --> 03:58.160
of course, more recently we saw, for example, OpenAI coming up with Dali, a big neural network

03:59.560 --> 04:06.560
which you can prompt to generate images. For example, here, it was prompted to ask for

04:07.080 --> 04:12.120
a Matisse interpretation of a robot playing chess. So, you see that these engines are

04:12.120 --> 04:18.640
really becoming more and more powerful. And maybe the most shocking element of it all

04:18.640 --> 04:25.280
was ChatGPT, which was released in 2022. That was actually an agent that can act like a

04:25.320 --> 04:31.120
ChatGPT. And this is something that in a way really feels like you're talking to a human

04:31.120 --> 04:38.120
being. And this made a big impression on many scientists, but also the general public because

04:38.600 --> 04:45.600
this is really a type of AI that is approachable by the general population.

04:46.600 --> 04:53.600
So, of course, this is just AI. Let's say it's a bit unrespectful, just AI. But in this

04:54.600 --> 05:01.600
talk, we're going to have a look at AGI, artificial general intelligence. And to do so, we need

05:02.160 --> 05:09.160
a definition. And the debate is on what AGI is exactly is. But I chose this definition

05:12.360 --> 05:19.360
from Wikipedia, which is a consensus website. And what Wikipedia says is that it is an

05:19.360 --> 05:25.600
intelligent agent that could learn to accomplish any intellectual tasks that human beings can

05:25.600 --> 05:29.960
perform. That is what Wikipedia comes up with. And this is supported by an article in The

05:29.960 --> 05:36.960
Economist. And we can interpret this as a system that can behave like, that can do the

05:38.040 --> 05:42.480
things that humans beings can do. I think that's a reasonable interpretation of this.

05:42.480 --> 05:48.160
And does that mean that we're talking about average human being, or are we talking about

05:48.240 --> 05:55.040
the upper bound of what humanity is capable of? That's still up for interpretation. But

05:55.040 --> 06:00.680
I think we can agree that once you can actually emulate an average human being and the cognitive

06:00.680 --> 06:07.680
abilities of an average human being, this would really be a great breakthrough. And you might

06:07.680 --> 06:14.920
think that this idea is kind of a hype, and you're writing that. But it's also important

06:14.960 --> 06:20.240
to remember that this idea was there from the start. So this is a picture from the Dartmouth

06:20.240 --> 06:27.240
workshop where a lot of very smart people assembled. And it was 1956 already, quite a few years

06:29.520 --> 06:36.520
ago. You have Claude Shannon there. You have Marvin Minsky, who founded the MIT AI Laboratory.

06:37.520 --> 06:44.520
And you also have John McCarty that came up with Lisp, for example. And they came together

06:45.120 --> 06:52.120
with this task. They thought we're going to try to simulate or build a machine that can

06:52.880 --> 06:59.280
simulate all features of intelligence, which is very closely related to the definition

06:59.280 --> 07:04.680
that I just gave you. So the idea really is around for some time. So it's not only a

07:04.720 --> 07:11.720
hype, it's really what relates at the foundation of the field of AI. So maybe this claimer.

07:13.760 --> 07:20.760
So in this talk, I will not try to make predictions on AGI. That's a very hard thing to do. I

07:21.200 --> 07:28.200
will not try to do that. And I will not try to take too strong of a stance. I will present

07:28.440 --> 07:33.440
you with what is out there. I will present you with some of the difficulties to which

07:33.440 --> 07:39.200
I think at the free and open source software community can really make an important contribution.

07:39.200 --> 07:45.600
But I will refrain from taking too strong of a stance. What I will do is I will discuss

07:45.600 --> 07:52.600
how a scientific approach to AGI is, in my opinion, really important or even crucial.

07:53.600 --> 08:00.600
And to do that, you need reproducibility. Reproducibility in a scientific context. And in my opinion,

08:00.960 --> 08:07.160
this is very closely related to free and open source software. And I believe that we, as

08:07.160 --> 08:12.960
a scientific community, and the free and open source software community, can really have

08:12.960 --> 08:19.960
a big influence on each other in this regard. So why should we care about AGI now? So why

08:20.960 --> 08:26.960
is it becoming a hype? Well, there are two good reasons for that. Two elements that popped

08:27.840 --> 08:33.000
up over the last couple of years that are really quite influential. First of all, large

08:33.000 --> 08:37.880
language models and the secondly, reinforcement learning. And I'm aware, I'm very well aware

08:37.880 --> 08:42.960
that this list is not complete, but in the interest of time, I will focus on these. So

08:42.960 --> 08:49.360
large language models, I will try to make as intuitive as possible to explain. But basically,

08:49.440 --> 08:53.600
what we have is we have a language model and this language model will try to predict the

08:53.600 --> 08:59.520
next word. Based on a sequence of words, it will try to predict which is the most likely

08:59.520 --> 09:05.240
next word to be chose. And you can do that with all kinds of machine learning. You can

09:05.240 --> 09:10.920
do that with hidden Markov models. But when we're talking about large language models,

09:10.920 --> 09:15.520
what we're actually referring to is using a deep neural network. And here we have a

09:15.560 --> 09:20.920
very simple neural network with just an input layer, an output layer, and one hidden layer.

09:20.920 --> 09:24.960
But when we're talking about deep neural nets, what we actually mean is that you have many

09:24.960 --> 09:31.960
hidden layers and different kinds of architectures to make this advanced learning possible. But

09:35.040 --> 09:39.360
the general principles are still the same. We still have different layers and these layers

09:39.360 --> 09:43.400
are connected and the weights that we have on these connections between these layers

09:43.440 --> 09:49.440
is basically the parameters of our model. And this is what we will use to actually do

09:49.440 --> 09:56.440
a learning. And so this is a large neural network. Training it is really a complex thing. And

09:56.840 --> 10:03.160
this is also something we will discuss later. But this is also something on which all these

10:03.160 --> 10:08.280
research institutes really build upon free and open source software. The operating system

10:08.320 --> 10:15.320
that also the ways to do the networking of such trainings. Now of course large language

10:16.200 --> 10:23.200
models. There was evolution and the evolution started with relatively simple things that

10:23.240 --> 10:30.240
already showed some capabilities to what we now have in chat GPT, a system that is actually

10:30.240 --> 10:37.000
quite impressive. And this tweet, it's just a tweet, but it's indeed not AGI, but there

10:37.040 --> 10:44.040
are some capabilities that are really remarkable. And this is really something that many people

10:44.040 --> 10:51.040
used to make very impressive demonstrations. For example, you can write poem just by chatting

10:51.480 --> 10:56.520
to this bot. It will generate codes that you can actually use to play poem. So this is

10:56.520 --> 11:02.520
really something. The next thing is reinforcement learning. Reinforcement learning that's actually

11:02.760 --> 11:09.760
the main topic of my research in the AI lab of the VUB. And what we actually have, we have

11:10.240 --> 11:16.320
an agent and an environment. And we want the agent to learn to behave optimally in this

11:16.320 --> 11:22.000
environment. And the agent can do so by performing actions in the environment. And in this simple

11:22.000 --> 11:28.520
environment where we have Super Mario, this corresponds to pushing buttons on the controller.

11:28.520 --> 11:34.280
And when an action is performed, we can observe the state, which here is simply the screen

11:34.280 --> 11:40.720
that we can see, and a reward signal that tells us how well we're doing. It's a kind

11:40.720 --> 11:46.960
of feedback signal. So if we can build an agent that through these actions and through

11:46.960 --> 11:50.960
the observations of states and rewards can actually learn how to behave optimally, that's

11:50.960 --> 11:56.440
when we're talking about the field of reinforcement learning. And this is a simple video game,

11:56.480 --> 12:02.520
but it does not take that much imagination to see if we replace the video game with the

12:02.520 --> 12:08.520
world. The state space will be much more complex. The action space will be much more complex.

12:08.520 --> 12:15.520
But you can see that if you can make a sufficiently advanced agent, you would result in AGI.

12:17.680 --> 12:23.000
And this is exactly what Silver et al. have some influential researchers in the fields

12:23.040 --> 12:30.040
of AI set in a paper in 2022. They said reward is enough. Reinforcement learning, where you

12:30.040 --> 12:35.880
follow a simple reward, is sufficient to learn advanced capabilities. And they gave this

12:35.880 --> 12:42.880
example of the squirrel. The squirrel wants to maximize the nuts sense. He likes to eat

12:43.560 --> 12:48.240
nuts sense. So he wants to maximize nuts. And in order to maximize nuts, if we use this

12:48.240 --> 12:53.480
simple reward signal, the squirrel will need to learn advanced capabilities. For example,

12:53.480 --> 12:58.240
it will need to recognize trees. It will need to be able to climb in trees. It will need

12:58.240 --> 13:04.120
to be able to pick acorns. It will need to be able to store them for winter and so on.

13:04.120 --> 13:11.120
So by following a very simple reward signal, we can actually produce quite complex capabilities.

13:11.440 --> 13:15.920
And this is something that we will get back to, because this is also not without risk,

13:16.000 --> 13:23.000
but this is something we will get back to. Okay. A third thing, I said LLMs, I said

13:23.200 --> 13:29.700
reinforcement learning. But a very important thing is also compute. Since about a decade

13:29.700 --> 13:35.720
or maybe a little bit longer, we really upped our compute. We have now GPUs that allow us

13:35.720 --> 13:42.720
to do very complex, to allow us to train very complex models. And this is also really, has

13:42.880 --> 13:49.080
been really a game changer and the amount of compute that we have at our availability.

13:49.080 --> 13:54.880
Now, I said I will not go into predict when we will get a GI, but I will show you some

13:54.880 --> 14:01.880
quotes because the opinions are actually quite divergent. And you might know this guy. This

14:02.360 --> 14:09.360
is a professor, Jeffrey Hinton, he's a British Columbian AI scientist. And he's a professor

14:12.720 --> 14:16.560
and he's really considered to be one of the Godfathers of deep learning. And deep learning

14:16.560 --> 14:22.160
is really what lies at the foundation of all these influential models that we see show.

14:22.160 --> 14:29.160
And he thinks that we might be only 20 years away from AGI, General Purpose AI or AGI. And

14:31.240 --> 14:36.480
this is quite remarkable. And he made this statement in March 2023. And he also said

14:36.480 --> 14:41.560
that this is a statement that he would not have made 10 years earlier. So it's really

14:41.560 --> 14:48.000
based on the recent developments. On the other hand, people like Jan Le Koon, he's also a

14:48.000 --> 14:54.800
very influential AI researcher. He's also considered to be one of the fathers of deep

14:54.800 --> 15:01.800
learning, but he has a different opinion. He says it will take us decades to even only

15:01.800 --> 15:08.800
touch upon what AGI could be. So the opinions really diverge. Then let's have a look at

15:09.800 --> 15:15.800
what these big AI companies are actually thinking about it. Because this will also be important

15:15.800 --> 15:22.800
in this talk. For example, this is Jean Leg, he founded DeepMind together with Demis Hassabis

15:22.800 --> 15:29.800
in 2010. And he thinks that AGI is likely by 2028. He says so with a probability of

15:29.880 --> 15:36.380
0.5. So it makes it easier to make predictions, of course, like that. But this is a statement

15:36.380 --> 15:43.380
by him. And he's one of the founders of DeepMind. So it will also have some... What these people

15:43.540 --> 15:50.540
say, of course, also resonates with many people in the community. Next, maybe you know this

15:51.580 --> 15:58.580
guy, this is the CEO of OpenAI. And he thinks it will take us to 2030 or 2031 before we

15:59.540 --> 16:06.540
will get to AGI. And I think it's very important to state these are very influential people

16:07.140 --> 16:12.580
within their company, within huge companies. And of course, these companies, they have

16:12.580 --> 16:18.260
significant bias. This might be a very self-fulfilling prophecy, the predictions that they make here.

16:18.260 --> 16:22.740
So it's very important to keep this in mind. Because, of course, the more people get hyped

16:22.740 --> 16:29.740
about AGI, the easier it will be for them to actually get their interest and capital

16:33.020 --> 16:38.580
to work on this research. And again, Altman also... I need to make this disclaimer. Altman

16:38.580 --> 16:45.580
also said that it will take 20 to 2030, but with a huge confidence interval. But of course,

16:45.900 --> 16:50.500
with huge confidence intervals, it's always easier to make predictions. So that's also

16:50.820 --> 16:57.820
something to take into account. Okay. All this sparked interest and maybe some concern.

16:58.540 --> 17:05.540
And even Snoop Dogg, he came up with this statement that he was concerned about AI. I had to remove

17:05.940 --> 17:11.460
some curse words here, but basically that is what he said. He heard this old dude talking...

17:11.460 --> 17:15.500
that created AI talking about that it is not safe that these things would get their own

17:16.100 --> 17:22.060
minds. So he's talking here about Jeffrey Hinton, maybe a bit disrespectful. But I think

17:22.060 --> 17:28.740
this really resonates with the general population. And it might be a bit tough to really grasp

17:28.740 --> 17:35.740
what this actually means, these breakthroughs, and where it will lead. Okay. Now, we could

17:36.740 --> 17:42.500
ask ourselves the question, are we aiming for AGI? As we will see, AGI has a lot of potential,

17:42.860 --> 17:49.860
has a lot of positive elements associated with it, but also some significant risks. Are

17:49.980 --> 17:54.100
we aiming for AGI? Well, don't take my word for it. I'm just a simple professor at the

17:54.100 --> 18:01.100
VUB. In my lab, it will not happen. I can be sure of that. But other companies like Open

18:01.140 --> 18:07.980
AI, they put on their website that they're working on that. So this is not something that

18:08.020 --> 18:12.740
we're coming up with. This is not just science fiction. Companies are actually trying to build

18:12.740 --> 18:19.740
this, and not only Open AI, but also Google DeepMinds. They want to build AR responsibly,

18:19.780 --> 18:25.300
okay? And if you look a little bit lower, they also mentioned AGI. So this is something

18:25.300 --> 18:32.300
that companies are working on. And more recently, also Meta, the CEO of Meta, came up with a

18:32.980 --> 18:38.060
wish to make this kind of technology. So it's really something that companies are working

18:38.060 --> 18:44.060
on. So as we will see, there will be an impact of this, so it's important to be aware of

18:44.060 --> 18:51.060
this. So what will be the impact of AGI, or the potential impact of AGI? Well, let's

18:51.260 --> 18:56.940
start with the goods. First of all, we will be able to tackle complex problems, maybe

18:57.500 --> 19:03.500
visit the stars. That would be a very nice achievement for humanity. We could automate

19:03.780 --> 19:10.060
things. Advanced automation, maybe even complete automation. And when we're talking about automation,

19:10.060 --> 19:15.820
we might think about automating the assembly of cars. We might think about automating a

19:15.820 --> 19:21.580
bakery. But of course, once your system is sufficiently smart, this will also allow us

19:21.660 --> 19:28.340
to automate coding jobs, for example, or research and teaching jobs. So this is really

19:28.340 --> 19:34.340
something that could have a huge impact. It would allow us to automate things. But if

19:34.340 --> 19:39.980
you automate things and you do not distribute the wealth that is generated like this, you

19:39.980 --> 19:46.060
will also be in serious trouble. And then of course, we can hope that we can enhance our

19:46.100 --> 19:53.100
human capabilities, because that would also be quite an interesting byproduct of AGI.

19:54.820 --> 20:01.100
Now the bad, many of these good things could result to serious social disruptions. And

20:01.100 --> 20:06.140
in a way, what is happening on social media, how people are being influenced, this is already

20:06.140 --> 20:11.660
going on. And you can assume that once you have agents that are even more intelligent,

20:11.700 --> 20:16.460
this will be even a bigger problem. Also, when starting to automate everything, if you

20:16.460 --> 20:21.820
do not take the politics into consideration when doing so, this might also lead to serious

20:21.820 --> 20:27.820
social disruptions. Another aspect is misalignment with humanity's goals. And this is something

20:27.820 --> 20:32.620
even within humanity, it's not easy to align our goals. There are many different views

20:32.620 --> 20:39.620
on how society should work. So how should we align an AGI to do what is best for us?

20:39.900 --> 20:46.900
Can we even define this? And then the ugly, this is of course the existential risk. And

20:49.060 --> 20:56.060
this is something that is of course an important concern. It's something that is explored extensively

20:56.060 --> 21:02.900
in science fiction literature, but it is actually an important risk. And we might also even

21:02.900 --> 21:08.740
go further than AGI, we might build a super intelligent system that is able to greatly

21:08.780 --> 21:15.780
outcompet us. And this might have even more advanced implications for our society. And

21:16.900 --> 21:23.900
you might know the books of Isaac Azimov, who really explored how we can try to align

21:24.780 --> 21:31.380
human beings with what we as humanity would like to do. But this really is not such an

21:31.380 --> 21:38.380
easy thing. And the existential risk of AGI is really something that has its own wicked

21:38.820 --> 21:44.700
Wikipedia page. So if we have an AGI, there are many ways that you could think of that

21:44.700 --> 21:51.540
it could influence our society or even really wipe out the human race. This is a really

21:51.540 --> 21:57.100
negative point of view and I'm not saying that it will necessarily be like this, but

21:57.100 --> 22:02.060
there are many options to do it for an AGI. So it is something that we should take into

22:02.060 --> 22:08.580
account if we make the balance. And this is maybe an example that you already heard

22:08.620 --> 22:14.900
of. It was introduced by MIG Bostrom. It's about the paperclip maximizer. And paperclip

22:14.900 --> 22:21.900
maximizer is about an AI system that is defined by a set of humans to maximize the number

22:23.420 --> 22:30.340
of paperclips. And at the start, the AI system is able to do that in a very efficient way.

22:30.340 --> 22:35.300
It is able to find or it is able to mine in a very efficient way and make many, many,

22:35.500 --> 22:41.100
many paperclips. But then when the mines are getting empty, there is a problem. The AI

22:41.100 --> 22:47.180
can no longer make paperclips, so it will start to use atoms of other things to also

22:47.180 --> 22:54.180
make paperclips. And in the end, also human beings and our entire earth will be transformed

22:54.620 --> 23:00.300
into paperclips, which is quite concerning. And maybe it's good to think about the reward

23:00.300 --> 23:07.300
is enough paper where we had this setting where we had an acorn that wanted to maximize

23:07.460 --> 23:14.460
its nuts. And in a way, this is very similar to the paperclip maximizer problem. And to

23:15.460 --> 23:22.460
say the least, that is actually really concerning. If we're not specifying our objective functions

23:22.460 --> 23:29.620
in a save and meaningful way, we really might run into troubles. And this is something that

23:29.620 --> 23:34.140
you also might have heard of, this probability of doom. It's something that is uttered a

23:34.140 --> 23:40.740
lot on social media. This is the probability of this existential risk. Currently, we do

23:40.740 --> 23:45.300
not have a formal framework to reason about this. So making statements about this is purely

23:45.300 --> 23:49.860
intuitive and in my opinion, at this point, not very relevant. But it is something that

23:49.860 --> 23:55.900
I think that most scientists would agree on, that this P doom is not zero. And if it is

23:55.940 --> 24:02.060
not zero, then you only need to make an AGI once, that actually for which this P doom

24:02.060 --> 24:06.900
will not be zero. And we will be in big trouble. So this is really something that we need to

24:06.900 --> 24:13.900
be very well aware of. And we also counter this paper. Reward is enough in our own paper

24:16.300 --> 24:22.060
where we set that scalar reward is not enough. If you have an agent that just follows one

24:22.060 --> 24:28.420
reward signal, for example, acorns or paper clips, you might end up in deep trouble. It

24:28.420 --> 24:33.660
might be smarter to look into multi-objective or multi-criteria reward signal set where

24:33.660 --> 24:39.620
you can say, I want to maximize the number of paper clips, but I also want to keep alive

24:39.620 --> 24:45.620
most of humanity, for example. So this is really something that might be something to

24:45.620 --> 24:49.940
consider when developing such systems. But we also make the disclaimer that even when

24:49.980 --> 24:55.740
making this multi-criteria reward signal, this is not a guarantee to avoid this existential

24:55.740 --> 25:02.740
risk. So this is something that we really should be aware of. And in my opinion, safety

25:02.740 --> 25:08.380
should be key. I think there are many positive aspects to AGI, but we should be aware of

25:08.380 --> 25:15.380
safety. And to do that, I think a scientific approach is necessary. And a scientific approach

25:15.380 --> 25:21.340
means that we need to formulate hypothesis about a risk, about safety, but also about

25:21.340 --> 25:27.620
purpose and impact. And when we can formulate hypothesis, we can also do experiments. And

25:27.620 --> 25:34.620
to do experiments, our science needs to be reproducible. Now, reproducibility in science

25:34.620 --> 25:42.020
is not so easy. It's a very important topic. It's not trivial. In a wet lab, you have many

25:42.060 --> 25:47.100
things that can go wrong. You have equipment, you have lab temperature, you have the purity

25:47.100 --> 25:51.780
of chemicals that you use, you have the skills of your technicians that might be different.

25:51.780 --> 25:58.780
And also the sex of your technicians, actually. So it has been shown by Sergei that the sex

26:02.100 --> 26:09.100
of the technicians who operate on rodents actually influences your experiments. Because

26:09.860 --> 26:16.060
for example, if you have male technicians, it will stress out your rats more than if

26:16.060 --> 26:21.460
you have female technicians. So reproducing experiments is something that is really

26:21.460 --> 26:28.460
challenging. But of course, in silico, in computer, on a computer system, in simulation, it could

26:29.220 --> 26:35.740
be much better. To reproduce things, we need two things. We need codes or a very rigorous

26:35.820 --> 26:42.820
description of what is going on in the codes. And we need data. Unfortunately, and this is

26:44.100 --> 26:51.100
a survey from 2018, not all papers, not all scientific papers that are done in AI actually

26:51.700 --> 26:56.220
come with codes. So as you can see here, there are a lot of papers that come with pseudo

26:56.220 --> 27:01.500
codes. There are some papers that have some test data, but not that many papers come with

27:01.580 --> 27:08.300
code. And that is really concerning. And major AI conferences, and we as AI scientists,

27:08.300 --> 27:15.300
we publish a lot in conferences where we present our peer-reviewed papers. These conferences

27:17.380 --> 27:24.380
are really becoming more and more aware of this problem. And they really make it an issue

27:25.180 --> 27:32.180
that people share codes with their manuscripts. But there are still journals like Nature and

27:33.220 --> 27:40.220
Science, really influential journals that do not enforce this, and this is really a pity.

27:41.940 --> 27:47.940
I have to say that there is this code science manifesto that says that basically doing science

27:47.940 --> 27:54.460
outside of the wet lab really coincides with releasing your codes. You need your code to

27:54.460 --> 28:00.500
do that. So this is a manifesto that has raised quite some awareness. And in AI research, there

28:00.500 --> 28:07.500
is a growing awareness of this. However, when we are talking about AI research these days,

28:08.300 --> 28:14.740
we indeed have academia. We have academia where research is being done. But we also have

28:14.820 --> 28:21.820
research institutes like Google DeepMind, like OpenAI, that are inherently different organizations.

28:22.980 --> 28:28.980
For many academic institutions, myself included, I think it is important that experiments are

28:28.980 --> 28:34.980
reproducible. I think it is important that we make our source code available so that

28:34.980 --> 28:41.220
other researchers can really build on top of our finance. But what about these research

28:41.260 --> 28:47.020
institutes? Well, the picture is really not black or white. So for example, DeepMinds,

28:47.020 --> 28:53.620
they developed this alpha fold system to predict protein structures, and the code to train

28:53.620 --> 29:00.620
a neural network was not available. And what happens, a set of researchers actually developed

29:01.460 --> 29:08.460
OpenFold, which was an open source implementation of this functionality, which really was able

29:08.460 --> 29:13.240
to reproduce this work. But this is what happened already in the free and open source

29:13.240 --> 29:20.240
software community so many times. Due to the need to have software as open source, people

29:21.220 --> 29:26.700
spent their time trying to rebuild things. But of course, this is of course a very good

29:26.700 --> 29:33.700
thing. But it would be better if the scientists just shared the code straight away. On the

29:34.620 --> 29:41.620
other hand, DeepMinds also made very important libraries available in a purely open source

29:41.620 --> 29:47.620
fashion, for example, Jax, which is a library that allows you to do very performant computational

29:47.620 --> 29:53.120
analysis. Also, TensorFlow for building these large neural networks. So these libraries

29:53.120 --> 29:58.620
are all very influential, and they really shaped how research is being done at this

29:58.660 --> 30:04.540
point. So there has been a major impact on AI research from these companies. Also, Alpha

30:04.540 --> 30:11.540
Zero, the agent that learned to play go, its code was not released with the paper, but

30:13.780 --> 30:19.580
recently they did release their code in an open source fashion, I think even a free

30:19.580 --> 30:26.580
software fashion, to actually allow other scientists to work on it. On the other hand,

30:26.940 --> 30:33.940
they also have open AI. They have their stable baselines, a library, which is a reinforcement

30:34.500 --> 30:40.180
learning library that also incorporates many algorithms and really lies at the foundation

30:40.180 --> 30:47.180
of many research that is being conducted. But on the other hand, we also have chatGPT,

30:47.300 --> 30:53.220
which is completely closed. It's near impossible to reproduce, not only because we do not have

30:53.260 --> 30:58.140
the codes, but we also do not have the description of the methods. We do not have a description

30:58.140 --> 31:02.940
of the infrastructure. We don't even know how big the dataset they use this or how big

31:02.940 --> 31:09.940
the neural network they use is actually, how big it actually is. So this is really something

31:10.620 --> 31:17.620
that is concerning. Google Gemini, same thing. Also, no source, only a black box that we

31:18.180 --> 31:24.980
can interact with via the network. There is one big exception, and that is the work

31:24.980 --> 31:31.980
done by Meta, where Jan Le Coon is the president of, and they did release many LLMs, where

31:33.740 --> 31:40.740
for which the source code is actually available. So the landscape is really divergent. Now,

31:41.060 --> 31:48.060
the question of today, or the first, the sparks of AGI, this was a paper written by Sebastian

31:50.100 --> 31:57.100
Bubek, is a brilliant scientist, and he wanted to investigate what the capabilities of chatGPT

31:57.100 --> 32:03.300
were. And so, for example, he used different versions of GPT, which he prompted to draw

32:03.300 --> 32:09.800
a unicorn index. This is what he reported in a paper, but this is not something that

32:09.880 --> 32:14.360
we could reproduce. These experiments, we cannot reproduce. First of all, we cannot

32:14.360 --> 32:21.360
seat GPT. So GPT is a stochastic agent. So in order to really reproduce what is going

32:22.640 --> 32:29.640
on, we would need to have the seat. But also, and maybe even more concerning, is that very

32:29.960 --> 32:36.960
recently a set of influential scientists actually showed that this black box axis is insufficient

32:37.920 --> 32:44.920
to properly audit AIs. So not having the code, not being able to look at the internals of

32:46.000 --> 32:51.120
this big neural network, what is going on, where, when you ask a certain prompt, is really

32:51.120 --> 32:56.840
not sufficient to really understand what is going on, and is not sufficient to get to

32:56.840 --> 33:03.840
safe AGI. So, to go back to this question at the first slide, will the first AGI be

33:03.840 --> 33:10.840
free or open source software? Well, it's really hard to know what drives these companies,

33:11.520 --> 33:18.520
but there is sure no commitment from their side to do so. OpenAI, DeepMinds, Anthropiq,

33:21.760 --> 33:27.280
they do make no mention of free software or open source software. But very recently,

33:27.280 --> 33:33.760
Meta, the CEO of Facebook, actually introduced that they will be developing AGI and they

33:33.760 --> 33:39.360
will also make it available in an open source fashion. And it was surprising to many, but

33:39.360 --> 33:45.120
maybe not too many who follow Jan Le Koon on Twitter, because he's tweeting about this

33:45.120 --> 33:50.480
all the time. He thinks this is really important. AI should be open source, should be free

33:50.480 --> 33:56.600
software, and this is indeed what they tried to do. So, okay, good. So we have these different

33:56.680 --> 34:03.680
viewpoints. Only a few days after Zuckerberg put this statement out, we already get these

34:05.720 --> 34:12.000
articles that are predicting doom, that saying that it is very scary, that such an influential

34:12.000 --> 34:18.600
technology would become available in an open source fashion, even comparing it to nuclear

34:18.600 --> 34:23.000
weapons. And if we're talking about existential threats, you could indeed see the comparison

34:23.080 --> 34:30.080
there. And we might also not make the recipe for a nuclear weapon available on a free license.

34:31.400 --> 34:37.080
So there is something to it. And so maybe we should ask ourselves the question, should

34:37.080 --> 34:43.760
the first AGI be free and open source software? And I'm not taking a stance here. It's really

34:43.760 --> 34:49.760
up for debate. And in general, I'm very much in favor of making things free and open source

34:49.800 --> 34:55.800
software. But this is the first kind of software that I really have my doubts about, because

34:56.240 --> 35:02.240
it will have a huge impact on society. And what will this mean if we have this available

35:02.240 --> 35:07.160
as open source so that everybody can access it? So this is really something that I believe

35:07.160 --> 35:13.280
is up for debate, because AGI will have a major impact on individuals, societies, but also

35:13.280 --> 35:19.280
governments. A government that has AGI will be a different government than a government

35:19.320 --> 35:26.320
that does not have access to an AGI. And this is all up for debate. But what I do think is

35:30.040 --> 35:37.040
that there should be oversight. And currently, there is not. There is no governmental oversight.

35:37.480 --> 35:41.720
Also, much of this research is happening in the United States. There has been some hearing

35:41.720 --> 35:47.200
in Congress, but this has also not been that much in depth. So for the moment, I think

35:47.240 --> 35:52.080
there is no oversight. So we have companies that are working on AGI at a certain point

35:52.080 --> 35:57.520
they might actually reach it. And what will we do next? So this is really something that

35:57.520 --> 36:04.520
we should be concerned about. And I will take this stance. So quite recently, Satya Nadella

36:06.600 --> 36:11.640
from Microsoft made this statement. If you look at inflation adjusted, there is currently

36:11.640 --> 36:17.240
no economic growth. And in a world like that, we may need a new input. And with this new

36:17.240 --> 36:24.240
input, he meant AI being the general purpose technology that drives economic growth. I

36:24.240 --> 36:30.760
think these kind of statements are really quite concerning, because of course economic

36:30.760 --> 36:35.800
growth is not everything. And as I said, there might be very positive things from AGI, but

36:35.800 --> 36:41.600
we also need to be very much aware of these risks. And in the end, if this is the case,

36:41.600 --> 36:46.120
if we have good things, if we have bad things, then basically it becomes a question, how

36:46.120 --> 36:52.000
do we balance these? And in order to balance these, we will need to be able to quantify

36:52.000 --> 36:59.200
what is this risk. And if we know, if we have a way to formally quantify this, then we will

36:59.200 --> 37:06.200
also need to make it a democratic question, because this is something that is a democratic

37:06.360 --> 37:12.440
problem. We need to have society decide on this topic. How important is it that we as

37:12.440 --> 37:19.440
a species remain? And how do we balance that to maybe the good things that come with AGI?

37:20.120 --> 37:27.120
And maybe something to close off with, the OpenAI came up with their work on superalignment.

37:27.800 --> 37:32.400
That means they expect that we will not only have AGI, but we will have a superintelligent

37:32.400 --> 37:38.400
system. And they were very proud to announce that, I think it was somewhere at the start

37:38.400 --> 37:45.400
of 2023, they were going to work on this superalignment. And they were going to put 20% of the computer

37:45.400 --> 37:51.600
they have, and they have a lot of computers, like many, many thousands of GPUs. They were

37:51.600 --> 37:57.560
going to dedicate 20% of that on safety. And they felt very happy about this statement,

37:57.560 --> 38:02.760
but I was confused, because I would think that we needed to do it the other way around,

38:02.760 --> 38:09.760
spend 80% on safety and 20% on capabilities. So this is something that I wanted to close

38:10.960 --> 38:16.560
with. So to wrap up, in my opinion, I did not take many stances in this talk, I think,

38:16.560 --> 38:23.560
but in my opinion, safety should be the first concern. And in this regard, we should study

38:24.560 --> 38:31.560
risk and the balance to make will be a democratic choice. So it will be something that societies

38:31.560 --> 38:38.560
will need to decide. Very importantly, oversight in the development of AGI is needed. And this is

38:38.560 --> 38:45.560
really something that, in my opinion, is lacking. And the debate of how free and open source

38:45.560 --> 38:50.560
software will be involved in this process is really important. And I think the community of

38:50.560 --> 38:57.560
free and open source software and the AI community has a lot to learn from each other. Maybe we will

38:57.560 --> 39:05.560
need new kind of licenses in order to deal with this kind of technologies. And many people say that AGI

39:05.560 --> 39:11.560
is something that we really need. And I fully agree that it would be a blessing for society if

39:11.560 --> 39:18.560
things go right. But in a way, if we can make AGI that it will be saved in only a thousand years,

39:18.560 --> 39:24.560
that should also be a good thing. The only thing to really want AGI now is if we want to solve a problem

39:24.560 --> 39:31.560
that is also an existential risk and that there is no other way to solve it. That would be a clear

39:31.560 --> 39:37.560
balance that we could make. But otherwise, I think safety should be of main concern. And that closes

39:37.560 --> 39:41.560
my talk. And if you have any questions, I will be happy to answer them.

39:49.560 --> 39:54.560
So are there any questions up there?

40:04.560 --> 40:13.560
Hi. So I was wondering, assuming AGI becomes open source and accessible to everyone, what are the

40:13.560 --> 40:20.560
material constraints that you think it could have in the future in the sense that, yeah, it might be open

40:20.560 --> 40:27.560
source, but then only very few people may be able to run it because it requires really powerful hardware?

40:27.560 --> 40:30.560
So what do you think in this regard?

40:30.560 --> 40:37.560
Yes, that's an excellent comment. In the interest of time, I didn't include it in this presentation. But this

40:38.560 --> 40:45.560
is indeed a real concern. This is already the case with the LMS that we now have here at our university,

40:45.560 --> 40:50.560
we would not be able to reproduce this research. But this is again something where governmental

40:50.560 --> 40:56.560
oversight is really important. If our governments really think AGI should happen, then they should also

40:56.560 --> 41:02.560
have the infrastructure to test these things on and make it possible to do this kind of research. And this is

41:02.560 --> 41:09.560
something where the EU also should step up, I think, to make this feasible. That being said, if we collaborate

41:09.560 --> 41:16.560
across our universities, we do have a significant amount of compute. So there is some compute, but that would also

41:16.560 --> 41:21.560
require us to really collaborate intensely in this front. But yeah, very good question. Thank you.

41:22.560 --> 41:29.560
Thank you for your talk. I'm wondering what kind of, so if deep neural networks are for what we have today,

41:29.560 --> 41:36.560
what we have to do more to have an AGI? Is just some fundamental research that we have to crack or is just

41:36.560 --> 41:41.560
more data, more training, more computer power?

41:42.560 --> 41:49.560
Well, the opinions differ. Some people say our project we're following now will lead eventually to AGI.

41:49.560 --> 41:57.560
So just using more data, just using bigger neural networks, that is a line of thinking. But on the other hand,

41:57.560 --> 42:04.560
this is not how we work. We as human beings, we do not need all literature that was ever produced in order to

42:04.560 --> 42:11.560
learn things. We can do that, but with just a fraction of this data that is available. So personally, I think we're

42:11.560 --> 42:18.560
still missing some things. There's some fundamental things that will require us to step up in order to get there.

42:18.560 --> 42:26.560
But at this point, it's really hard to say. I would also not have expected what JetGPT has become 10 years ago.

42:26.560 --> 42:33.560
I would also not have expected what JetGPT has become 10 years ago. I would also not have expected what

42:33.560 --> 42:38.560
JetGPT has become 10 years ago. I would also not have thought that this would have been possible. So it's really hard to say,

42:38.560 --> 42:47.560
but indeed, one advantage of making things from scratch to build the capabilities from a more fundamental base is maybe that we

42:47.560 --> 42:54.560
might have more control over what is going on. Because of course, if you train neural networks with a lot of data, it's really hard to

42:55.560 --> 42:59.560
know what to expect at the end of the training cycle.

42:59.560 --> 43:10.560
So I wanted to ask about whether an open source model or a commercial model is going to be first. So OpenAI took a very clear

43:10.560 --> 43:18.560
stance on this, right? In multiple interviews, they've stated that basically developing AGI will just require too much

43:18.560 --> 43:26.560
resources to be done by anything else than a commercial party. What do you think about that?

43:26.560 --> 43:36.560
Well, I think it's something that we need to work on. The question of being an open source will also allow us to look at the

43:36.560 --> 43:42.560
code, which will also give us insights in how these things work. That's one thing. But on the other hand, we will also need

43:42.560 --> 43:49.560
a computer to do this kind of research. And this is something that the European Union can indeed step up on. But this is something

43:49.560 --> 43:58.560
that will be important. Because indeed, without compute, it will be difficult to run these models. On the other hand,

43:58.560 --> 44:05.560
the previous question, it might be possible that we will have breakthroughs that will allow us to do things with much less

44:05.560 --> 44:13.560
compute. So this is something that is also very interesting. At these deep neural networks, they really are models with a huge number

44:13.560 --> 44:21.560
of parameters. So to train them, that is really an infrastructural nightmare. But maybe we can come up with things with fundamental

44:21.560 --> 44:27.560
concepts that will allow us to do much more with much less compute.

44:27.560 --> 44:38.560
So I had a question. What do you think of stopping the research or strengthening the research right now until we have

44:38.560 --> 44:46.560
proper security in place to be sure that none of those existential crises can occur?

44:47.560 --> 44:57.560
Yes, I think that's what I meant with oversight. There should be debate on how it should be conducted. There should be

44:57.560 --> 45:05.560
debate which direction we're reaching. And I think this is missing a lot at this point. So this is really something that needs to be

45:05.560 --> 45:13.560
debated. And we will need to... I think governments will need to be on top of this rather than following the companies that do this

45:13.560 --> 45:21.560
research. Because in the end, if a company develops this kind of technology, who will be responsible? There is a lot of ethical but also

45:21.560 --> 45:33.560
legal issues with that. So there is a lot of work to discuss this. And as I said, we might have AGI in 10 years, might be in

45:33.560 --> 45:40.560
1000 years, but it doesn't hurt to start thinking about these processes now in due time.

45:40.560 --> 45:50.560
Hi. Can you have explained a lot of problems? Can AGI help with those problems?

45:50.560 --> 45:51.560
Sorry?

45:51.560 --> 45:55.560
So can AGI help with the problems of AGI?

45:55.560 --> 46:04.560
Well, then it might be too late, of course. There might be some circular aspects to it that is indeed in a way that many of these

46:04.560 --> 46:12.560
alignments, the ideas to alignments actually use the same kind of technologies and methods that are used to develop capability.

46:12.560 --> 46:20.560
So in a way that is indeed being sought after. But if you have a sufficiently complex model, this model might be trying to deceit you.

46:20.560 --> 46:28.560
So if this is the case, it becomes really hard to understand what is going on and to understand whether this model is really working with you or

46:28.560 --> 46:38.560
working against you. So in the end, our brains might be too small to still follow what is going on there. And that is where things get complicated.

46:39.560 --> 46:55.560
Appear. Don't you think that maybe we have a bigger probability of dying by disease that AGI can cure?

46:55.560 --> 47:03.560
That's a good question. So if you have other existential risks, we need to think about their probability. So for example, I do a lot of research in

47:03.560 --> 47:13.560
pandemic preparedness and indeed, might be possible that we have a virus that will be very destructive. On the other hand, in human history, we did not have any

47:13.560 --> 47:24.560
viruses or pathogens that really wiped out the entire species. So existential risk is really wiping out your species. So balancing that to other existential

47:25.560 --> 47:37.560
risks is important. But then we have to make sure that we have a formal framework to reason about this probability. Because otherwise, you're just comparing apples and oranges without actually knowing how they compare to each other.

47:37.560 --> 47:39.560
Does that answer your question a bit?

47:40.560 --> 47:49.560
Yes, but if you all have 100% more of a disease that might be dying, so maybe AGI can cure the disease.

47:49.560 --> 47:55.560
Yes, but that's how it has been.

47:55.560 --> 47:58.560
That's a good question because it was not with the microphone.

47:58.560 --> 48:17.560
So I think you said that we now have 100% chance of dying, that AGI might fix it. Well, that's true, but that's also what humanity is about. We are mortal beings. Should we put that in the balance of making ourselves

48:17.560 --> 48:35.560
maybe live forever, but on the other hand, maybe we might swipe out our species. I'm not an ethical expert in ethics, but these are things that we should think about. And maybe society should, through a democratic voice, decide on this.

48:35.560 --> 48:43.560
But that's not something that I think we can decide here today, but there are different angles to it. That is definitely the case.

48:43.560 --> 48:58.560
Hi. You mentioned two papers. The first one that argued that reward might be enough to achieve artificial general intelligence, and then the second one which argues that it may not be enough.

48:58.560 --> 49:07.560
Obviously, artificial general intelligence should be able to learn how to behave ethically in the same way all of humanity does.

49:07.560 --> 49:13.560
Sorry. This will understand. Sorry.

49:14.560 --> 49:29.560
Do you think there is a good approach of teaching artificial general intelligence to behave ethically, like humans do? If it can save the same sort of problems, it surely can understand the ethical reasons we do.

49:29.560 --> 49:39.560
Yes. Well, asking me, is there a good way to do that, said that would solve the problem in a way? So, unfortunately, I do not have the answer to that.

49:39.560 --> 49:50.560
We did do research that at least said that a multi-criteria approach makes a lot of sense. And this is also how we as human beings work.

49:50.560 --> 50:06.560
We do not have only this acorn to follow. We have different things that we deem important. So, formulating things in this fashion might be a good way to do that, but we also make the disclaimer that this is no guarantee to make this work out.

50:06.560 --> 50:23.560
So, there is a lot of work that will be necessary in order to, first of all, get some ideas on this probability of existential risk, but also ways to make it more likely that we will be heading towards a safe agia.

50:23.560 --> 50:25.560
Okay.

50:25.560 --> 50:30.560
So, thank you very much.

50:30.560 --> 50:32.560
Thank you.

