WEBVTT

00:00.000 --> 00:06.000
Thanks.

00:06.000 --> 00:17.000
So today I'd like to talk a bit about zero as you touch less infrastructure for your container and Kubernetes workloads, most specifically about container optimized Linux.

00:17.000 --> 00:24.000
And hi, I'm Tilo. You can find my GitHub or mustad on. I work for Microsoft. If you want to reach out to me, just drop me an email.

00:24.000 --> 00:34.000
All right. So container optimized Linux is a very special way to look at Linux distros and a very special way to look at your whole application stack.

00:34.000 --> 00:46.000
I'm going to introduce a few fundamental foundational concepts of that and then I'll touch a topic that is a little bit neurologic with operators with large fleets and that is how to keep the operating system up to date in a safe manner.

00:46.000 --> 00:56.000
Also, we talk about compositability and that is like extending your operating system in a way that keeps your extensions from the operating system reasonably separated.

00:56.000 --> 01:03.000
And lastly, if you want to reach out to the project, I'll show you a few ways to interact with the community.

01:03.000 --> 01:13.000
But first of all, what is container optimized Linux? Do we have some special tweaks in the kernel or do we ship some user space bits that make containers run exceptionally well?

01:13.000 --> 01:31.000
Well, yeah, that's one way of looking at it, but it is actually taking a step back and looking at your Linux distro, not as a general purpose distro that does all the things, but at a very special purpose operating system that operates one thing, that is container workloads.

01:31.000 --> 01:48.000
You would imagine the operating system as not being anything special, just another piece in your application stack and make the operating system operatable like you operate your container workloads or your Kubernetes ports.

01:48.000 --> 01:57.000
Have an image based operating system in containers and Kubernetes, you basically create your application as an instance of a pre-built application image.

01:57.000 --> 02:06.000
And if you do all that, you can actually leverage the isolation that you get in container apps from the operating system side and there's some neat things you can do with that.

02:06.000 --> 02:23.000
First of all, let's talk a bit about the user experience side of things. So general purpose operating systems do a very great job to give you like all of the diversity, all of the choice, all of the knobs and levers that you can have to tweak and twist your operating system and make it fit for your purpose.

02:23.000 --> 02:39.000
The thing is for specialized workloads, you sometimes have to, even if you don't want to. So that's not how we perceive container optimized Linux instead, we're trying to focus on doing one thing and one thing very good.

02:39.000 --> 02:51.000
So we measure ourselves not by the features we ship, not by like the options that you have, but we measure ourselves by basically our function or one thing that we provide to users.

02:51.000 --> 03:04.000
A light switch can come like in 20 different colors, but you know if the green switch doesn't manage to turn on the light in 20% of the time, then there's kind of a disconnect between the designers of the light switch and the users.

03:04.000 --> 03:21.000
And we don't want to be like that. Talking about provisioning, if you provision a container app or a Kubernetes pod, you basically take a declarative configuration that specifies only the business logic bits that you need to adapt the image to.

03:21.000 --> 03:33.000
And then you apply that to a prebuilt application image and as a result, you will get an instance of that image with your config live in your cluster and that's your app.

03:34.000 --> 03:49.000
If you take that idea and if you apply it on how you would provision your operating system, then you would have a declarative way to configure the bits and pieces that you absolutely have to that are not that are not saying defaults.

03:49.000 --> 04:02.000
So your business logic of your notes, you would apply that to an operating system image and now would create an instance of that note which has the properties of the configuration you gave in.

04:02.000 --> 04:14.000
You could of course add some bootstrapping code in order to bring up a control plane or if you have a single purpose app to bring up that app, but this is kind of where we draw the line from the operating system point of view.

04:14.000 --> 04:21.000
So we don't want to ship another application control plane because there are great control planes out there.

04:21.000 --> 04:45.000
Alright, so if you take all those principles and do a provisioning which I'll do with the QEMO instance here locally, first of all, this is the config I was talking about and you won't see any like OS specific twists in here.

04:45.000 --> 04:47.000
This is purely my business logic.

04:47.000 --> 04:49.000
So I'll define a user here.

04:49.000 --> 04:53.000
I inline like a little bit of static HTML here.

04:53.000 --> 05:09.000
I have specified a small image here that will be worked into the configuration and then I define a system D unit that basically brings up KD container with the config that I provided and that is my application.

05:09.000 --> 05:26.000
So as is tradition, we compile YAML into JSON and this will basically take the image that I specified there and work into the configuration.

05:26.000 --> 05:34.000
So now if I boot, this is a web script.

05:34.000 --> 05:39.000
I just start the M and I pass this web jay and I generate it.

05:39.000 --> 05:45.000
Now what will happen is this is the first time this virtual machine is provisioned.

05:45.000 --> 05:54.000
So it applies the configuration that we added because this is how we set up nodes.

05:54.000 --> 06:05.000
It takes a few seconds and then after this configuration has been applied, the system D unit will start up that we specified the web server for.

06:05.000 --> 06:13.000
It will basically pull, I hope, do we have Wi-Fi?

06:13.000 --> 06:15.000
I hope.

06:15.000 --> 06:17.000
So I have Wi-Fi then.

06:17.000 --> 06:23.000
Obviously, there's no way for this virtual machine to pull KD.

06:23.000 --> 06:31.000
So what you can do though, my phone has Wi-Fi.

06:31.000 --> 06:32.000
My phone has Wi-Fi.

06:32.000 --> 06:34.000
My laptop does not have Wi-Fi.

06:34.000 --> 06:38.000
Come again.

06:38.000 --> 06:40.000
You'll fix better.

06:40.000 --> 06:57.000
Or I can use.

06:57.000 --> 07:09.000
So I'll shut this down.

07:09.000 --> 07:18.000
Because I'm not sure how the virtual machine is dealing with changing Wi-Fi.

07:18.000 --> 07:22.000
But I can try a hotspot.

07:22.000 --> 07:35.000
This is the great thing with live demos.

07:35.000 --> 07:38.000
Okay.

07:38.000 --> 07:41.000
There's Wi-Fi that hopefully works.

07:41.000 --> 07:45.000
And just reassure not again.

07:45.000 --> 07:47.000
Here we go.

07:47.000 --> 07:54.000
Again, for the virtual machine, this is the first boot because I interrupted it in its original first boot.

07:54.000 --> 08:00.000
And so it'll set up the, all of the base system using same default.

08:00.000 --> 08:09.000
And it will now that it has connection pull KD and then the web service will start.

08:09.000 --> 08:11.000
Fortunately, KD is a pretty small image.

08:11.000 --> 08:18.000
So we should, this is to see with a console, we should see output from Docker any second here.

08:18.000 --> 08:21.000
That will, yeah, there is.

08:21.000 --> 08:34.000
So you see that there wasn't anything on port 8080 before, but if I reload now, this is our static.

08:34.000 --> 08:40.000
This will get very interesting with the update demo later because the update is significantly larger.

08:40.000 --> 08:47.000
The basic idea is to give you an operating system that just behaves as your container application.

08:47.000 --> 08:51.000
And then you just, you know, make your config, you throw it on your cluster.

08:51.000 --> 08:55.000
It comes up the way you find it or it breaks while coming up.

08:55.000 --> 08:57.000
So you'll see there's something wrong.

08:57.000 --> 08:59.000
There's like one way or the other.

08:59.000 --> 09:07.000
And this way by configuring it once at provisioning time, just like you do your container apps, you don't have any config drift.

09:07.000 --> 09:16.000
Like if there's a note that you've been provisioned months ago, and if there's a new note that you provisioned based on the same config, those nodes will have the same configurations.

09:16.000 --> 09:25.000
And you don't need to fill around the way I did here, supported on many cloud providers like we have specialized images for those cloud providers.

09:25.000 --> 09:27.000
Terraform integration, go by means.

09:27.000 --> 09:30.000
If you have existing automation, you want to do that programmatically.

09:30.000 --> 09:34.000
And we have cluster API integration, more on that in a second.

09:34.000 --> 09:35.000
There's this weird thing.

09:35.000 --> 09:39.000
That's the difference between us and general purpose operating systems.

09:39.000 --> 09:43.000
And that is once, configure once before the operating system is even deployed.

09:43.000 --> 09:46.000
No SSHing into nodes and fiddling around with configs.

09:46.000 --> 09:57.000
And kind of the mental image that we have to comparison is, if you apply a service YAML to your cluster, you do not cube cut a lex egg into that port, then fiddle around with config files and only then your app works.

09:57.000 --> 09:59.000
That's a weird concept.

09:59.000 --> 10:03.000
And we are trying to make that happen for operating systems.

10:03.000 --> 10:04.000
All right.

10:04.000 --> 10:12.000
So cluster API is a method to deploy Kubernetes clusters based on the management Kubernetes cluster flat car.

10:12.000 --> 10:17.000
The project I'm working on is integrated in copy upstream.

10:17.000 --> 10:28.000
We support a number of providers and they're currently pioneering kind of a new way to provision images or as images of cluster API.

10:28.000 --> 10:33.000
Previously, those were built into the images and Kubernetes were built in the single image.

10:33.000 --> 10:36.000
That was the golden cluster API image which you provisioned.

10:36.000 --> 10:43.000
And then you had a problem because you couldn't update the operating system or Kubernetes independently because they were merged.

10:43.000 --> 10:55.000
So we're working on a provisioning time composition, more on that later, based on system these are the eggs and we have a number of proof of concept providers implemented.

10:55.000 --> 10:58.000
What's the benefits of having an image based operating system?

10:58.000 --> 11:09.000
Well, you provision from immutable images, you're creating an instance just like you create an instance of your application with a container image.

11:09.000 --> 11:14.000
Those images are always built from scratch and they are very easily testable this way.

11:14.000 --> 11:22.000
And just like you have no config drift with provisioning time configuration, you have no version drift with image based provisioning.

11:22.000 --> 11:33.000
That means that if you see a node and it runs operating system version A, then you have the full version set of all of the binaries shipped with the operating system just relating to that number.

11:33.000 --> 11:41.000
There's no individual tools updating in their own pace and that means you don't really need to chase those versions, right?

11:41.000 --> 11:49.000
In our case, everything is on a separate partition in the slash USR directory and that was the invariant you protected.

11:49.000 --> 11:56.000
So if you fiddle with the bits in that partition, then it will break because the VN verity checks will be invalid.

11:56.000 --> 12:01.000
It offers us the opportunity to do a partition update.

12:01.000 --> 12:06.000
So if you're a database operator on a Kubernetes cluster and you want to retain local state, then that's good news for you.

12:06.000 --> 12:12.000
Talking of which, I was mentioning the isolation of container applications to the operating system.

12:12.000 --> 12:21.000
That makes your app portable. It makes your app run on many, many operating systems just basically making sure that the runtime exists.

12:21.000 --> 12:25.000
You don't have any shared dependencies that we have an operating system.

12:25.000 --> 12:34.000
And if you look at this from the OS point of view, then the only thing you're giving to your applications is a well-defined contract,

12:34.000 --> 12:37.000
which is the container runtime on some kernel bits.

12:37.000 --> 12:41.000
And there are no other relations to have to those applications.

12:41.000 --> 12:49.000
And if you focus on that only, there's ways to thoroughly test that contract and make sure you never break.

12:49.000 --> 12:51.000
You always uphold this contract.

12:51.000 --> 12:57.000
If you look at the screenshot here, then you'll see our test suite running.

12:57.000 --> 13:02.000
Every single one of these green buttons is a scenario test that we run.

13:02.000 --> 13:06.000
We run those in our CIs for PRs to the operating system.

13:06.000 --> 13:09.000
And this makes sure that this contract is always uphold.

13:09.000 --> 13:16.000
Now, if you do that, it doesn't really matter for your applications, like which major version of the operating system you're running.

13:16.000 --> 13:23.000
Up to the point where it doesn't really matter which Docker version or container D version or kernel version is there,

13:23.000 --> 13:28.000
as long as you absolutely make sure that you uphold this contract to your application.

13:28.000 --> 13:35.000
And you might have guessed by now that this layer, this contract, that's our light switch.

13:35.000 --> 13:39.000
This gives us atomic in-place updates.

13:39.000 --> 13:45.000
So what we can do with this is we can stage a new operating system in a separate partition,

13:45.000 --> 13:52.000
and then we can switch over by means of a single reboot, atomically, like there's no intermediate state.

13:52.000 --> 14:03.000
The application comes up, sanity checks run, and everything's great, except when the application meets some edge cases

14:03.000 --> 14:10.000
where it has problems with the new release and in that case, you just roll back,

14:10.000 --> 14:14.000
and you have a known good state that's still there, and you know that it works.

14:14.000 --> 14:20.000
OK, so what I'm going to show now, I hope it works over my mobile connection,

14:20.000 --> 14:28.000
is something that you usually will not see, because usually that's automated.

14:28.000 --> 14:35.000
So with my configuration already deployed earlier, I disabled the update functionality,

14:35.000 --> 14:41.000
and that's because I don't want my great web server being interrupted by updates.

14:41.000 --> 14:50.000
But now that we demo updates, I can enable it, and I can check.

14:50.000 --> 14:57.000
So the program I'm calling here is the update client-client, and just interfaces with the update client,

14:57.000 --> 15:05.000
and it says that I am currently thrittling thumbs, and I have never checked for updates.

15:05.000 --> 15:09.000
So we just kindly ask it to please check for updates.

15:09.000 --> 15:12.000
Now, there's an update available.

15:12.000 --> 15:17.000
The reason for that is I'm not using the current version, I'm using a previous alpha version,

15:17.000 --> 15:22.000
and of course it'll find an update because we've published a new version in the team.

15:22.000 --> 15:30.000
Now, it'll start downloading, and you see the progress that is basically percentages,

15:30.000 --> 15:37.000
we're at 5% now, I'm not sure if it's going to make it to the end of the stock, but it is what it is.

15:37.000 --> 15:43.000
While it's trying its best, let's talk about updating and rebooting.

15:43.000 --> 15:49.000
For many people, reboots are scary, not really for us, and we're trying to make them not scary for everybody else.

15:49.000 --> 15:59.000
But we understand that that's an interruption of your service, so there's a variety of ways that you can configure reboots.

15:59.000 --> 16:04.000
The cheapest thing is to just have maintenance windows, and you configure that,

16:04.000 --> 16:08.000
so your instances are only allowed to reboot in specific time windows,

16:08.000 --> 16:10.000
but there are more advanced options available.

16:10.000 --> 16:14.000
For instance, if you have a small number of nodes without a control plane,

16:14.000 --> 16:17.000
you can make those nodes synchronize the update using a CD-LOX,

16:17.000 --> 16:21.000
so only a certain number of nodes updates at any point in time.

16:21.000 --> 16:28.000
Also, if you operate Kubernetes, there are a number of Kubernetes operators that you can use,

16:28.000 --> 16:34.000
and this is really great on advanced stuff, so those operators will detect when an update has been staged,

16:34.000 --> 16:39.000
it will drain, carefully drain the node, only after the node is drained, it will reboot the node,

16:39.000 --> 16:44.000
and it will uncoordinate the node after reboot, so it's repopulated, your operating system is updated,

16:44.000 --> 16:47.000
your cluster didn't notice a thing, and your applications are happy.

16:47.000 --> 16:55.000
You can also, of course, stagger updates and make sure that only a certain amount of nodes are out at any given point in time.

16:55.000 --> 17:02.000
This happens via a stateful update server, and our implementation of that is open source as well as part of the project,

17:02.000 --> 17:05.000
you can just download and use it, it's actually pretty easy to operate.

17:05.000 --> 17:11.000
We're using the Omaha protocol, which is the same protocol that Chromebooks use to update,

17:11.000 --> 17:13.000
so it's kind of pretty solid protocol.

17:13.000 --> 17:18.000
As I said, the update server is easy to self-host, so if you have a larger fleet,

17:18.000 --> 17:23.000
and you want to have your own update procedures and your own update rollout process,

17:23.000 --> 17:25.000
that's a straightforward way to get it.

17:25.000 --> 17:32.000
If you don't, however, but you want to be part of the stabilization process that we do for our releases,

17:32.000 --> 17:36.000
that's a very simple way to hope yourself in there.

17:36.000 --> 17:39.000
Let me carefully check if there is a chance.

17:39.000 --> 17:42.000
Oh, 63%, okay.

17:42.000 --> 17:45.000
And then it stopped.

17:45.000 --> 17:49.000
Anyways, maybe it will do it.

17:49.000 --> 17:54.000
So, Flatcar releases in basically just like an application.

17:54.000 --> 17:59.000
We split off the branch of a major release from a main branch,

17:59.000 --> 18:03.000
and that will go through Alpha, Beta, and Stable stages.

18:03.000 --> 18:08.000
Alpha is fully tested, but it may contain half-done features,

18:08.000 --> 18:10.000
which are for development, so Alpha is for devs, basically.

18:10.000 --> 18:14.000
Beta is also fully tested, and Beta is what we consider stable.

18:15.000 --> 18:21.000
Stable has seen some user workloads, and this is where you come in, right?

18:21.000 --> 18:28.000
So, if you operate larger clusters, it's absolutely safe to have a few Beta canneries in those clusters.

18:28.000 --> 18:33.000
If those canneries detect any issues with your workloads, you just get back to us and yell at us.

18:33.000 --> 18:39.000
You file a few issues against those Beta's, and we will never promote the Beta to Stable that has known issues.

18:39.000 --> 18:44.000
So, this way, you're keeping the Stable nodes in your cluster safe.

18:46.000 --> 18:50.000
And now you're supposed to demo Composability, and I think I skipped the Update demo, right?

18:50.000 --> 18:53.000
No! I have six more minutes.

18:53.000 --> 18:59.000
I'm not trying to get both demos in, but I think the Update demo is more amazing.

18:59.000 --> 19:01.000
So, I just talk about Composability.

19:01.000 --> 19:04.000
We're an image-based distribution.

19:04.000 --> 19:07.000
We are immutable. We don't have any package management.

19:07.000 --> 19:12.000
It's actually really hard to add something on the operating system level, and it's meant to be that way.

19:12.000 --> 19:15.000
However, there may be things that you want to add.

19:15.000 --> 19:23.000
Maybe Kubernetes, maybe Wazm, maybe an alternative container runtime like Podman.

19:23.000 --> 19:28.000
And since we don't want to provide any options because that would make the lights switch weird,

19:28.000 --> 19:33.000
there's this thing called SystemDISOSX, relatively new technology.

19:33.000 --> 19:40.000
It's been around for three years now, and this thing called disease based on file system images that are immutable,

19:40.000 --> 19:42.000
which kind of resonates strongly with us.

19:42.000 --> 19:50.000
So, the way this works is you take your Sussex image, which only contains certain binaries when you ship for one application,

19:50.000 --> 19:57.000
and then it's being merged at runtime or at boot time with your operating system tree,

19:57.000 --> 19:59.000
and it results in basically a merge tree.

19:59.000 --> 20:04.000
So, if you look at your operating system, then you'll see all of the binaries in there.

20:04.000 --> 20:06.000
Building is easy as well.

20:06.000 --> 20:09.000
You just have a local directory that resembles a root file system,

20:09.000 --> 20:13.000
and then you run makeswashfs or makeoz on that local path,

20:13.000 --> 20:19.000
and you end up with a file system image that you can use with Sussex.

20:19.000 --> 20:24.000
Yes, the update's there, and I'm going to show you the update. It worked.

20:24.000 --> 20:28.000
So, it now tells you that we need to reboot, right?

20:28.000 --> 20:32.000
Before we do that, let's check out what we're running.

20:32.000 --> 20:39.000
We're running an alpha version, 3815, clearly I'm a developer, because I run alpha,

20:39.000 --> 20:46.000
and we're running the 6.1 kernel, and our application is happily running on this.

20:46.000 --> 20:50.000
Let's reload again, and now I reboot.

20:50.000 --> 20:53.000
Ah, Zudo will make me a sandwich.

20:53.000 --> 21:01.000
And this will reboot into the new partition, where the new update is staged,

21:01.000 --> 21:08.000
and there's kind of a, like, a settle time that the update tool will wait,

21:08.000 --> 21:13.000
and after that time concluded, it'll mark the new partition as the default partition.

21:13.000 --> 21:15.000
You always boot into the new partition.

21:15.000 --> 21:19.000
If you have, like, important applications that absolutely need to run,

21:19.000 --> 21:22.000
you would make the update service depend on those applications,

21:22.000 --> 21:28.000
and if they don't start, the update service will never mark the new partition as the default partition.

21:28.000 --> 21:34.000
And then you only need to reboot in order to fall back in the old, known good state.

21:34.000 --> 21:37.000
All right. So, what did this buy us?

21:37.000 --> 21:43.000
We've upgraded from 3815 to 3850,

21:43.000 --> 21:48.000
and we upgraded from kernel 6.1 to kernel 6.6,

21:48.000 --> 21:51.000
and our container is happy.

21:51.000 --> 21:54.000
Didn't notice anything, which is the whole point.

21:54.000 --> 21:57.000
All right. Image composability.

21:57.000 --> 22:04.000
So, you take your SUSEX images, and you can either pre-bake them into existing operating system images out there,

22:04.000 --> 22:07.000
and then you have, like, a combined image, and you can provision that,

22:07.000 --> 22:14.000
or you can just use the declarative configuration that we provide to download those SUSEX provisioning time,

22:14.000 --> 22:20.000
and then they're there. You can use system, the SUSEX update, to update those SUSEX independently of the operating system,

22:20.000 --> 22:26.000
because if you have a Kubernetes SUSEX, there are no shared binaries of the operating system,

22:26.000 --> 22:30.000
and you might want to update that in your own pace.

22:30.000 --> 22:35.000
And with Kubernetes particularly and Cluster API, we have a need-proof concept running.

22:35.000 --> 22:40.000
So, for the Cluster API phones, they have pre-baked images with operating system and Kubernetes in it.

22:40.000 --> 22:43.000
It is very, very hard for them to update any OSPET.

22:43.000 --> 22:48.000
So, what they do is they delete nodes, and they basically provision new nodes that has a new stack in it.

22:48.000 --> 22:55.000
That's nice. It works for most workloads, but for some it doesn't, and there SUSEX comes in.

22:55.000 --> 23:03.000
So, SUSEX allows you, and we demo that for the OpenStack provider, to just update your Kubernetes in place,

23:03.000 --> 23:06.000
and, yeah, be happy about it.

23:06.000 --> 23:13.000
We're trying to, you know, invest more in that work and get more Cluster API providers interested.

23:13.000 --> 23:17.000
And this is the point where I need to skip the SUSEX demo for you.

23:17.000 --> 23:24.000
It's basically just showing you a wasn't-time binary appearing and disappearing after I merged the SUSEX and unmatched the SUSEX,

23:24.000 --> 23:28.000
but I have one and a half minutes left.

23:28.000 --> 23:34.000
So, I'm going to skip and continue with how you can work with us.

23:34.000 --> 23:38.000
If you're interested, there are a few flyers out there with a few QR codes on them.

23:38.000 --> 23:43.000
Just grab one when you go out, and you have a few pointers how to interact with us.

23:43.000 --> 23:48.000
We are very active on metrics and Slack. We're basically there all of the time.

23:48.000 --> 23:53.000
You will always find and maintain either on our metrics channel or on a Slack channel.

23:53.000 --> 23:57.000
We have Office OS that are user-centric. If you have a question about FlatCard,

23:57.000 --> 24:03.000
if you want to discuss something and you are a user, every second Tuesday of the month at 3.30 pm UTC.

24:03.000 --> 24:08.000
There's a DevSync. If you're interested in developing with FlatCard

24:08.000 --> 24:16.000
or if you just want to see how we plan our work, the day-to-day stuff, how we plan roadmap and how we discuss security stuff,

24:16.000 --> 24:21.000
that's the sync to join. All of our work is public.

24:21.000 --> 24:24.000
All of our planning is public. Roadmap is not public.

24:24.000 --> 24:27.000
And all of those are on GitHub. Those are GitHub boards.

24:27.000 --> 24:31.000
You can just review them and you see what we're up to.

24:31.000 --> 24:38.000
If you want to contribute or join as a developer, we're trying to make this very low bar.

24:38.000 --> 24:43.000
So we have a software development kit that is a container because obviously it is.

24:43.000 --> 24:53.000
And it just takes those seven steps. You just run those seven steps and you will fully rebuild that release from scratch

24:53.000 --> 24:59.000
and you will fully run all of our scenarios locally on a very small QEMO cluster.

24:59.000 --> 25:04.000
And it's equally easy to make modifications and to build your own images.

25:08.000 --> 25:14.000
To just wrap things up, and we have five minutes left for questions after that,

25:14.000 --> 25:18.000
so I skipped the SysX demo for a good reason to be open for your questions.

25:18.000 --> 25:23.000
If you leverage the isolation that container runtimes, container ups have inherently,

25:23.000 --> 25:26.000
and you look at this from the operating system point of view,

25:26.000 --> 25:31.000
there's a lot to win in terms of interchangeability and replaceability.

25:31.000 --> 25:37.000
If you use the declarative configuration before provisioning time and focus on the business logic

25:37.000 --> 25:43.000
for configuring your instances and to abstract all of the unnecessary complexities

25:43.000 --> 25:47.000
without taking away the option to fiddle with the bits,

25:47.000 --> 25:52.000
but with taking away the requirement of absolutely having to in order to get to something workable,

25:53.000 --> 25:56.000
you eliminate config drift.

25:56.000 --> 26:00.000
Atomic automated updates gives you that kind of contract,

26:00.000 --> 26:04.000
that abstraction that you get with container runtimes.

26:04.000 --> 26:10.000
And if you still need to change something on the operating system layer that absolutely cannot go into a container,

26:10.000 --> 26:13.000
you can compose it in the SysX.

26:13.000 --> 26:18.000
Fully community driven, we've submitted the project to the CNCF as an incubating project.

26:18.000 --> 26:21.000
The process is ongoing.

26:21.000 --> 26:23.000
Keep your fingers crossed.

26:23.000 --> 26:26.000
And that's it.

26:26.000 --> 26:29.000
Thank you.

26:34.000 --> 26:37.000
Any questions?

26:37.000 --> 26:40.000
Thank you.

26:49.000 --> 26:52.000
A lot of, a lot of please.

27:07.000 --> 27:12.000
How would you run this in production?

27:22.000 --> 27:25.000
I don't think I got that.

27:25.000 --> 27:28.000
How do you run debugging tools?

27:28.000 --> 27:33.000
The obvious first answer is you shouldn't because they are debugging tools

27:33.000 --> 27:38.000
and they just, you know, open your nodes wide up to all kinds of strangenesses.

27:38.000 --> 27:43.000
But if you absolutely have to, there's a development container that we have for flat car.

27:43.000 --> 27:48.000
That is usually used in automation, like when you, at provisioning time, or reboot,

27:48.000 --> 27:52.000
if you need to build some special kernel modules, then you would use that.

27:52.000 --> 27:56.000
So if you absolutely have to run DevTools, that's the way. You can do it.

27:56.000 --> 27:59.000
What's the DevTools in a container?

27:59.000 --> 28:03.000
They already are. We are shipping them with all of the releases.

28:03.000 --> 28:06.000
So that's the thing.

28:19.000 --> 28:23.000
You speak very, very loud. The mic is...

28:29.000 --> 28:33.000
If there's a standard for...

28:33.000 --> 28:37.000
Yes, there are multiple.

28:37.000 --> 28:41.000
They're actually pretty well defined.

28:41.000 --> 28:45.000
Where is the... You should probably ask the person a little bit behind you.

28:45.000 --> 28:49.000
I've seen it and I know it's smiling there.

28:49.000 --> 28:53.000
But anyways, so if you check out the dev.com,

28:53.000 --> 28:58.000
you can see the dev.com, you can see the dev.com, you can see the dev.com,

28:58.000 --> 29:05.000
if you check out uapi-group.org,

29:05.000 --> 29:10.000
then you'll find, like this is pretty dry, but you'll find

29:10.000 --> 29:17.000
some specifications on, like, how those images are supposed to work.

29:17.000 --> 29:22.000
And there's a number. You could use WashFest images.

29:22.000 --> 29:27.000
You could even use, like, complete file system images with multiple

29:27.000 --> 29:31.000
runtimes for multiple architectures in them. And it's all specified here.

29:31.000 --> 29:35.000
If you want to hands-on examples,

29:39.000 --> 29:43.000
we have in the FlacCa project,

29:43.000 --> 29:47.000
we have something we call the SusEx Bakery.

29:51.000 --> 29:54.000
And that... Did it load? Yeah.

29:54.000 --> 29:58.000
So that has some hands-on examples on deploying your custom docker,

29:58.000 --> 30:02.000
on deploying wasn'time,

30:02.000 --> 30:06.000
on deploying Kubernetes, and also on making itself updateable.

30:07.000 --> 30:11.000
Sorry?

30:22.000 --> 30:25.000
Sorry, I need to ask you this.

30:25.000 --> 30:29.000
Reproducible builds are something that are

30:29.000 --> 30:33.000
pretty much on our radar and our roadmap.

30:33.000 --> 30:37.000
We even produce, as a build artifact,

30:37.000 --> 30:42.000
salsa provenance in the 0.2 revision, I believe.

30:42.000 --> 30:45.000
So there's initial work there.

30:45.000 --> 30:49.000
You can use that to basically build on, but there are a number of things that we need to solve,

30:49.000 --> 30:54.000
like, you know, the right compiler usage and so on.

30:54.000 --> 30:58.000
So FlacCa is actually making heavy use of Gentoo in the back end.

30:58.000 --> 31:02.000
So if you use that for Gentoo, you can reuse it on FlacCa. It's very easy.

