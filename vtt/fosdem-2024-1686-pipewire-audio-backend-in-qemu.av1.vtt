WEBVTT

00:00.000 --> 00:10.800
Hi everyone, my name is Dorian Dabasi and I work at Red Hat.

00:10.800 --> 00:16.120
I currently work on enabling the audio stack and other features in the automotive team.

00:16.120 --> 00:20.680
And today I'll be talking about the Piperia Audio Backend in QEMO.

00:20.680 --> 00:23.760
So just a brief overview about what Piperia is.

00:23.760 --> 00:28.240
Piperia is a multimedia service for handling booths audio and video.

00:28.240 --> 00:34.200
But in this presentation I'll be focusing on the implementation that was done in QEMO

00:34.200 --> 00:40.880
and I will also focus on its use cases in the embedded platforms.

00:40.880 --> 00:44.160
So for a start, what's the QEMO's audio backing?

00:44.160 --> 00:49.600
It's a software component that's responsible for managing the audio streams and also providing

00:49.600 --> 00:54.640
audio functionality to the emulated platform and in our case QEMO.

00:54.640 --> 00:59.440
And it's also responsible for handling the audio imputes and the outputs on your virtual

00:59.440 --> 01:02.320
machine that's running on QEMO.

01:02.320 --> 01:07.200
The Piperia Audio Backend, it provides an interface that would allow the sharing of these audio

01:07.200 --> 01:12.640
streams from the guest operating system to your host using the Piperia native APIs and

01:12.640 --> 01:14.520
libraries.

01:14.520 --> 01:18.080
So how does it work?

01:18.080 --> 01:21.800
Here this is an illustration of what the stack looked like.

01:21.800 --> 01:25.680
First we have the application that's running in the guest is sending the audio data to

01:25.680 --> 01:32.760
Piperia through MMFD and DM above and we have the Piperia Daemon that's communicating with

01:32.760 --> 01:37.280
the session manager and it's also responsible for handling the media routing and in charge

01:37.280 --> 01:41.840
of talking and talking to the AOSA driver on the guest channel.

01:41.840 --> 01:48.200
And in QEMO you can see that we have the emulated sound card driver which could either be AC97

01:48.200 --> 01:50.520
GOS or Intel HDA.

01:50.520 --> 01:56.280
It's providing the audio software emulation and then like any other host application that's

01:56.280 --> 02:01.360
running in your host user space, QEMO with the native Piperia Backend would be playing

02:01.360 --> 02:05.640
these audio streams to the host Piperia Daemon directly.

02:05.640 --> 02:09.800
So now it's playing those audio streams to the Piperia Daemon in the host user space

02:09.800 --> 02:14.440
but it's not going through any of the post audio compatibility layer or in the case of

02:14.440 --> 02:18.000
maybe AOSA, the AOSA plugin.

02:18.000 --> 02:25.320
So the Piperia Daemon process would now handle the media routing to the corresponding libraries

02:25.320 --> 02:33.120
like AOSA library and routed to the sound driver that's in the host channel.

02:33.120 --> 02:39.720
So after many iterations of the patches, the Piperia Audio Backend was merged in QEMO

02:39.720 --> 02:46.440
in May 2023 and it was added to the QEMO version 8.1 release and currently it supports

02:46.440 --> 02:53.280
Piperia version 0.36 although there's been a latest release in Piperia which is a Piperia

02:53.280 --> 02:58.800
1.0 release and that's like a huge milestone for Piperia.

02:58.800 --> 03:04.760
So after the QEMO 8.1 release there was some improvements that were made to the backend

03:04.760 --> 03:11.120
thanks to McHundry and Volca for their support while optimizing this backend.

03:12.120 --> 03:18.480
QEMO, it has a number of audio backends and you can find the latest audio backend which

03:18.480 --> 03:21.760
is Piperia among the list of available audio drivers.

03:21.760 --> 03:26.560
So depending on your architecture, the Audio Dev Help option should show you the list of

03:26.560 --> 03:29.400
audio drivers and you can see Piperia there.

03:29.400 --> 03:34.880
The Piperia Audio Backend, it uses similar structures like the other audio backends although

03:34.880 --> 03:43.160
the difference is that it's being implemented with the Piperia native C libraries.

03:43.160 --> 03:49.480
So these are the Piperia Audio Backend properties that can be configured on your command line.

03:49.480 --> 03:57.240
So first we have the Audio Dev command and we specify the Piperia backend that we want

03:57.240 --> 04:04.160
to use and also specify the ID of the Piperia backend and then you can specify the name

04:04.160 --> 04:09.720
of the Piperia backend, the stream name and the latency for the output stream and you

04:09.720 --> 04:15.520
could also specify the same for your input stream depending on the latency that you want.

04:15.520 --> 04:21.160
So this is a description of what the QAPI schema looks like for the Piperia Audio Backend.

04:21.160 --> 04:27.400
You can see the name there which is the Piperia Key Target Object and it's used to specify

04:27.400 --> 04:32.360
the target object to link to although it's not necessary if you do not specify an object

04:32.360 --> 04:34.360
name to set.

04:34.360 --> 04:38.720
And for the stream name, this parameter is a stream media name that is being used when

04:38.720 --> 04:44.800
you're creating a new stream and if you don't set a stream name, it should use the ID of

04:44.800 --> 04:47.960
the Piperia backend which is a PW sound.

04:47.960 --> 04:54.360
For the latency, in order to set your desired latency, you could set anyone that you want

04:54.360 --> 05:01.400
although the default latency is 46 milliseconds for Piperia Audio Backend.

05:01.400 --> 05:05.960
So there are other parameters like the mixing engine, the frequency, the channels and the

05:05.960 --> 05:06.960
formats.

05:06.960 --> 05:12.800
These parameters are common to the other audio backends like Paul Sodu, Jack and Ausa.

05:12.800 --> 05:20.480
But one thing to note is that in QEMU, it currently supports just one channel or two

05:20.480 --> 05:21.480
channels.

05:21.480 --> 05:22.840
That's money or stereo setup.

05:22.840 --> 05:27.400
So you can configure either one or two channels and in Piperia, when you're using a single

05:27.400 --> 05:36.280
channel, the content of your buffer is basically S1 samples, S2, S3 and each of the samples

05:36.280 --> 05:39.440
would be the buffer samples.

05:39.440 --> 05:45.000
Now when you have two channels, the format would be expecting the buffer to be like one

05:45.000 --> 05:49.560
sample on the left and one sample on the right and like continuously like that.

05:49.560 --> 05:55.360
So each of the samples that's the one on the left would be going to the left speakers and

05:55.360 --> 05:58.200
then the samples on your right would be going to the right speakers.

05:58.200 --> 06:05.480
So in the case of two channels, the sum of the samples would be the sum of the left samples

06:05.480 --> 06:08.520
and the right samples which is the stride.

06:08.520 --> 06:13.800
And then the buffer size there, it's specified in microseconds just in case you want to configure

06:13.800 --> 06:15.680
a buffer size.

06:15.680 --> 06:20.800
And the default format that can be used is S16, although the Piperia Audio Backend has

06:20.800 --> 06:23.800
a range of formats that it supports.

06:23.800 --> 06:30.080
And for frequency, you could set a default frequency of 44.1 milliseconds.

06:30.080 --> 06:37.920
So to use the Piperia Audio Backend, you need an audio device and this audio device is an

06:37.920 --> 06:39.720
emulated sound card.

06:39.720 --> 06:45.480
It's a legacy PCI device that's been plugged directly into the PCI ExpressRid bus.

06:45.480 --> 06:51.420
So this is an example of how the audio device is being configured in the command line.

06:51.420 --> 06:58.020
So first we have the device option which we're specifying an Intel HD device and we'll specify

06:58.020 --> 07:06.140
a codec option like HD Duplex for streams from your host speakers and your host microphone.

07:06.140 --> 07:12.620
And maybe if you wanted to only allow access to just your speakers, you could use the HD

07:12.620 --> 07:19.580
output option or if you just want access only to your microphone, you could use the HD

07:19.700 --> 07:21.140
microphone option.

07:21.140 --> 07:27.620
So here you can see that when specifying the sound card device to use, I used the ID that

07:27.620 --> 07:29.700
was specified in the Piperia Backend.

07:29.700 --> 07:34.820
That's you telling the sound card device to use the Piperia Backend.

07:34.820 --> 07:39.180
So this is how the properties of the Intel HD Audio Device are being declared inside

07:39.180 --> 07:40.260
the code.

07:40.260 --> 07:45.460
You can look up how the properties of the other devices like GOS and AC97 are being

07:45.460 --> 07:53.580
declared inside the code.

07:53.580 --> 07:59.940
So Quemo allows you to configure multiple audio backends and this is very useful in embedded

07:59.940 --> 08:01.820
platform development.

08:01.820 --> 08:07.340
So let's say for example that I'm emulating an infotainment system using Quemo and I want

08:07.340 --> 08:12.180
to configure a stream only for notifications on the mono channel and then I want to configure

08:12.300 --> 08:16.100
another stream only for music on two channels.

08:16.100 --> 08:20.700
This multiple audio backend configuration, it will allow you to specify different parameters

08:20.700 --> 08:22.980
for each of the created stream.

08:22.980 --> 08:27.260
So this is a visual representation of what the backend would look like with two Piperia

08:27.260 --> 08:28.740
Audio Backends.

08:28.740 --> 08:34.020
So you can see that each node in the guest is representing a created stream and you can

08:34.020 --> 08:40.940
see that the nodes which are the colored boxes and you can also see the host speaker nodes.

08:40.940 --> 08:47.060
So for playback, the output ports of the Quemo node which is on the right, the output

08:47.060 --> 08:51.540
ports for the Quemo node which is on the right, I've been routed to the speaker nodes

08:51.540 --> 08:56.900
on the host and then the input ports that's coming from the host microphone, I've been

08:56.900 --> 09:00.180
routed to the input ports on the guest.

09:00.180 --> 09:05.100
So this is also very useful when maybe you want to isolate the audio that's coming from

09:05.100 --> 09:09.900
different processes that are running in your guest.

09:09.940 --> 09:16.260
So now we'll take a technical deep dive into how the Piperia Audio Backend works.

09:16.260 --> 09:18.420
So what happens in playback?

09:18.420 --> 09:24.060
For playback, we first activate the stream and using this Piperia Streamset Active API

09:24.060 --> 09:29.260
call, it will set the stream mode into streaming and then next we call the buffer get free

09:29.260 --> 09:30.260
function.

09:30.260 --> 09:33.940
This function is used to know in advance the available number of bytes for writing data

09:33.940 --> 09:39.780
to the buffer and this also improves the playback latency by a factor of two.

09:39.980 --> 09:43.700
And later I will show you some latency measurements.

09:43.700 --> 09:47.420
So next what we want to do is to lock the tread loop because I'm using the tread loop

09:47.420 --> 09:52.820
mechanism and this mechanism ensures that we are doing the Piperia API calls only from

09:52.820 --> 09:54.700
one single tread at a time.

09:54.700 --> 09:58.580
So you don't want to be accessing this Piperia resources from multiple threads because it

09:58.580 --> 10:01.980
could cause a risk condition.

10:01.980 --> 10:07.660
So next what we want to do is to get the number of bytes that are available for writing data

10:07.660 --> 10:09.260
to the buffer.

10:09.260 --> 10:13.020
How we get this value is that we subtract the number of bytes that are actually inside

10:13.020 --> 10:17.660
the ring buffer from the effective Piperia Backend buffer size.

10:17.660 --> 10:22.300
And to get these bytes that are inside the ring buffer, we use the sparring buffer get

10:22.300 --> 10:25.420
rights index API call.

10:25.420 --> 10:33.020
So now what we do next is to use the sparring buffer write data to actually do a mem copy

10:33.020 --> 10:38.540
of buffer data from the source audio device to a temporary buffer with the index being

10:38.540 --> 10:42.700
the offset and then we update the write pointer.

10:42.700 --> 10:48.020
So here at this point there is the possibility of buffer on the run sometime occurring and

10:48.020 --> 10:53.420
although this happens in very rare cases, this is like a situation where the audio buffer

10:53.420 --> 10:58.620
levels has dropped below a certain threshold and it sometimes cause audio distortion or

10:58.620 --> 11:04.100
stuttering and like we cannot really guarantee that okay this guest would be producing the

11:04.100 --> 11:06.020
audio samples fast enough.

11:06.020 --> 11:12.220
So in Piperia we had a robust solution to fix this issue which was to handle this buffer

11:12.220 --> 11:14.580
on the runs by plain silence.

11:14.580 --> 11:20.020
You can look up the code on how we handle the buffer on the runs.

11:20.020 --> 11:25.620
So next what we do is to get a buffer that can be filled for the playback stream and

11:25.620 --> 11:30.620
then we copy this audio data from the temporary buffer to the Piperia buffer using the sparring

11:30.620 --> 11:32.420
buffer read data API call.

11:32.820 --> 11:36.820
Although I'm just giving you a summary of what the sparring buffer read data API call

11:36.820 --> 11:40.780
does but it does much more than that.

11:40.780 --> 11:46.700
And then next we queue the buffer for playback and this continue to happen in a loop until

11:46.700 --> 11:51.420
all the buffers have been played.

11:51.420 --> 11:54.540
So for the capture side what happens?

11:54.540 --> 12:00.620
It's more or less the opposite of what's happened in the Piperia backend and in this

12:00.700 --> 12:04.780
case like it's kind of similar but not because it's the opposite.

12:04.780 --> 12:09.900
So in this case what we do first is that we like activate the stream and then we use the

12:09.900 --> 12:15.660
buffer get refunction to know in advance the available number of bytes that we can write

12:15.660 --> 12:20.660
and then we use the tread loop lock again to ensure that we're just doing the API calls

12:20.660 --> 12:24.580
from one single tread at a time.

12:24.580 --> 12:29.300
But the difference here is that this time around instead of using the sparring buffer

12:29.300 --> 12:33.940
write data API call we're using the sparring buffer read data call and this time we're

12:33.940 --> 12:39.340
doing a mem copy of buffer data from the temporary buffer to the source audio device.

12:39.340 --> 12:45.780
So with the index being the offset we would update the read pointer afterwards.

12:45.780 --> 12:51.060
Then what we want to do next is to get a buffer that can be consumed for the capture streams

12:51.060 --> 12:56.580
and then we copy the audio data from the Piperia buffer to the source audio device using the

12:56.660 --> 13:03.180
sparring buffer write data API call.

13:03.180 --> 13:08.940
Next we queue the buffer for capture and then this continues in a loop until all the buffers

13:08.940 --> 13:14.900
are being consumed for capture.

13:14.900 --> 13:21.020
So as regards to the volume controls in order to be able to adjust your volumes through the

13:21.020 --> 13:27.380
virtual machine to be effective on the host we use the Piperia volume control API calls

13:27.380 --> 13:33.380
and this volume control code would allow for purpose synchronization of the volume changes

13:33.380 --> 13:36.580
that are made on the guests to be effective on the host.

13:36.580 --> 13:41.980
So when this volume changes have been applied on the node output monitor parts of the guests

13:41.980 --> 13:44.740
it will synchronize with the host.

13:44.740 --> 13:50.420
So for Piperia I use the Piperia stream set control API call and this is used to set the

13:50.420 --> 13:52.140
effective volume.

13:52.140 --> 13:58.740
Although one thing to note in Quemo was that it had volume levels of 0 to 255 and in the

13:58.740 --> 14:06.500
back end because the Piperia API has volume levels from a floating range of 0 to 1 where

14:06.500 --> 14:12.460
0 is the silence and 1 is representing without attenuation I had to do a linear conversion

14:12.460 --> 14:20.820
of these levels so did a linear conversion of 255 levels to Piperia floats in range of

14:20.820 --> 14:25.140
0 to 1.

14:25.140 --> 14:30.500
So regarding the features of the Piperia back end these features are not like the features

14:30.500 --> 14:36.300
of Piperia in general it's not only limited to its design to handle multimedia processing

14:36.300 --> 14:42.140
on the Linux but it also transcends to applications that have been built with the Piperia C API

14:42.140 --> 14:46.780
of which a use case now is the Piperia audio back end in Quemo.

14:46.780 --> 14:53.340
So on into the Piperia low latency features the Piperia back end has been developed to

14:53.340 --> 14:59.580
significantly reduce the latency in many ways and one of the ways is by setting the Piperia

14:59.580 --> 15:05.900
keynote latency property and we set it in the back end to be 75% of the time period for

15:05.900 --> 15:13.440
faster updates and the other way in which we reduced latency was to use the buffer get

15:13.440 --> 15:18.980
free function which improved the latency by a factor of 2.

15:18.980 --> 15:24.980
And I think to note about the latency is that the Piperia back end latency is mostly determined

15:24.980 --> 15:30.580
by a combination of the buffer size and the sample rate of the back end and this is usually

15:30.580 --> 15:33.820
called the quantum.

15:33.820 --> 15:41.500
So another feature of the audio back end is that it's providing a reduced footprint and

15:41.500 --> 15:46.740
also reduced dependencies in comparison to the other audio back ends that we have in

15:46.740 --> 15:53.740
Quemo and for the native Piperia back end we get to benefit from Piperia features such

15:53.740 --> 16:01.180
as the less CPU usage and the memory as well.

16:01.180 --> 16:06.220
So here I made some benchmarkings of the round triple latencies for the different audio back

16:06.220 --> 16:12.500
ends and all these latencies were being measured with a jack Iodili and a loopback cable.

16:12.500 --> 16:18.300
So listed here are the round triple latencies as reported by jack Iodili and the sample

16:18.300 --> 16:23.740
rate of the device that I used is set to 44.1 kilohertz.

16:23.740 --> 16:29.580
So as you can see there, yeah, I have to agree that jack is like topping the charts in low

16:29.660 --> 16:32.820
latency as expected but that's not my focus.

16:32.820 --> 16:40.460
You can see that the low latency that Piperia offers is quite low and then next we have

16:40.460 --> 16:47.820
the pulse audio and SDL competing with each other.

16:47.820 --> 16:54.700
So about debugging, while I was working on this audio back end, the GDB was very useful

16:54.700 --> 17:00.220
because in case you want to examine the state like registers and memory and you want to

17:00.220 --> 17:04.860
maybe set break points and watch points you could use it and you could also leverage the

17:04.860 --> 17:07.820
Quemos internal tracing infrastructure.

17:07.820 --> 17:12.660
So I added a couple of Piperia audio back end trace events that you can use.

17:12.660 --> 17:16.940
So these trace events you can configure it on the command line and example of these trace

17:16.940 --> 17:20.500
events is the Piperia writes trace events.

17:20.500 --> 17:25.540
When you set it, it will show you the length of bytes that's to be written to the buffer.

17:25.540 --> 17:30.020
It will also show you the available number of bytes that can be written.

17:30.020 --> 17:35.140
And one thing to note here with Quemos is that this, when if you use this, if you enable

17:35.140 --> 17:40.100
this Piperia write trace event, it produces a lot of outputs given that we are copying

17:40.100 --> 17:41.980
bytes every millisecond.

17:41.980 --> 17:46.020
So you should expect to have a very big log file in case you want to enable those trace

17:46.020 --> 17:47.380
events.

17:47.380 --> 17:52.780
And then there is the other tool that is very handy, the Piperia debug login.

17:52.780 --> 17:58.060
You can use it to set different debug levels from 0 to 5 and these levels would help you

17:58.060 --> 18:04.580
to see and have control of the behavior of the Piperia back end or if you were maybe

18:04.580 --> 18:11.180
using like debugging your own Piperia application.

18:11.180 --> 18:14.540
So here I added some helpful links.

18:14.540 --> 18:20.340
The first one which is my blog about the Piperia back end and its usage in Quemo.

18:20.340 --> 18:25.300
The next one was Get Hoffman's blog about the sound configuration changes that were

18:25.300 --> 18:27.180
made in Quemo.

18:27.180 --> 18:32.180
And then we also have like the Quemo invocation that's in case maybe you want to see and know

18:32.180 --> 18:35.820
how to use these audio back ends or maybe some other audio back ends, you could look

18:35.820 --> 18:39.460
up the Quemo invocation and how to use it.

18:40.020 --> 18:45.060
Then I also added the Piperia's wiki page on performance measurements.

18:45.060 --> 18:50.220
So it includes scripts that you could use to measure the latency in the different audio

18:50.220 --> 18:53.740
back ends like Piperia, Paul, Sodja and Jack.

18:53.740 --> 18:59.340
And you could also use it to measure the context switches and the CPU cycles, etc.

18:59.340 --> 19:04.180
So at this point here I would like to give a shout out to Intema and the Piperia maintainer

19:04.180 --> 19:09.740
who assisted me while I was working on this back end.

19:09.740 --> 19:10.740
Thank you.

19:10.740 --> 19:12.740
Do you have any questions?

19:12.740 --> 19:13.740
Any questions?

19:13.740 --> 19:14.740
Questions?

19:14.740 --> 19:15.740
What was?

19:15.740 --> 19:16.740
Oh yeah.

19:16.740 --> 19:41.740
I'm not curious what applications you tested with the incubators.

19:41.740 --> 19:46.700
So you're asking what applications in Quemo that I tested with this and how they behave.

19:46.700 --> 19:52.100
Okay, you could test a couple of applications that's like trying to play audio which maybe

19:52.100 --> 19:55.900
you're watching YouTube on your guest.

19:55.900 --> 20:02.580
But I mostly use the loopback cable and the Jack Iodili tool to measure latency.

20:02.580 --> 20:06.300
At least that's very effective because you could use it to measure like the CPU cycles

20:06.300 --> 20:07.300
as well.

20:07.300 --> 20:13.300
The latency and you could also measure other like features that you're interested in.

20:13.300 --> 20:17.140
Any other questions?

20:17.140 --> 20:21.440
Thank you very much.

20:21.440 --> 20:22.940
Thank you.

20:22.940 --> 20:24.000
Thank you.

