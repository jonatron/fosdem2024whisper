WEBVTT

00:00.000 --> 00:12.520
Thank you so much guys.

00:12.520 --> 00:17.560
So is my audio, can everyone hear my voice?

00:17.560 --> 00:18.560
Perfect.

00:18.560 --> 00:22.400
So I would be talking about external Rooksef cluster.

00:22.400 --> 00:27.080
So people might not be aware like what's Rook, what's Sef and what thing I'm talking

00:27.080 --> 00:28.080
about.

00:28.080 --> 00:31.960
So I will tell you to the intro first about what topic is this and then we'll deep dive

00:31.960 --> 00:35.400
into this topic.

00:35.400 --> 00:43.480
So I am Parth Arora and I work for IBM Storage as a software developer and closely with Rook

00:43.480 --> 00:50.120
operator and I'm one of the lead approver of that repo.

00:50.120 --> 00:56.800
So anybody like might be storing data applications, creating applications through Kubernetes,

00:57.800 --> 01:03.520
but everyone knows like Kubernetes is designed in terms like which doesn't talks much about

01:03.520 --> 01:09.080
data but anyhow any application we can talk about there is storage for that and we need

01:09.080 --> 01:11.640
to store data in it.

01:11.640 --> 01:17.120
And for now if someone wants to store the data that is coming from Kubernetes application,

01:17.120 --> 01:22.840
we just talk about cloud providers and talking to the cloud and they might be doing anything

01:22.840 --> 01:25.240
we don't know at the back end.

01:25.240 --> 01:30.520
So why not to bring the storage into your data center and traditionally like there are

01:30.520 --> 01:35.120
some limitations also with cloud providers like number of nodes, how we can scale up

01:35.120 --> 01:36.600
the different AZs.

01:36.600 --> 01:42.200
So why not to have a native solution with your own Kubernetes cluster.

01:42.200 --> 01:47.840
So and also like to manage that cluster how natively your Kubernetes application manages

01:47.840 --> 01:52.000
and also manage that storage in the same way.

01:52.000 --> 02:02.720
So here comes the most trusted platform CIF that can like make the storage available to

02:02.720 --> 02:08.920
you in the form of Kubernetes, how it will be but first I will talk about CIF, why CIF.

02:08.920 --> 02:19.360
So it's like it has a lot of enterprise already enterprises using from like trusted platform

02:19.360 --> 02:23.400
from past 10 years and it provides all the storage at the same time the block file and

02:23.400 --> 02:30.120
object you name it and it provides and its open source and it has so many features its

02:30.120 --> 02:37.360
resilient, it's configurable, it's more disaster proven and it's consistent and provide like

02:37.360 --> 02:44.040
like give your data a safe point of view.

02:44.040 --> 02:49.440
And so CIF was designed even back when Kubernetes was introduced.

02:49.440 --> 02:54.600
So to bring the storage to the Kubernetes world we have designed Rook and this is how

02:54.600 --> 02:58.160
it was born to bring the CIF to the Kubernetes.

02:58.160 --> 03:03.480
It's the management layer so it helps in installing and managing the state.

03:03.480 --> 03:07.560
It's not just like it's there are Python scripts that just install it but it actually

03:07.560 --> 03:09.520
manage the state of it.

03:09.520 --> 03:17.600
How it does it's a Kubernetes operator which works how a Kubernetes actual desired and

03:17.600 --> 03:23.560
actual state works and we can manage to that and how we can define what we need to give

03:23.560 --> 03:25.920
the state so that's the CRDs.

03:25.920 --> 03:35.640
We can give any definition to that and that thing can be configured and like we can give

03:36.000 --> 03:41.760
I need monitoring, I need the block storage so these types of things we can provide through

03:41.760 --> 03:44.600
CRDs.

03:44.600 --> 03:48.960
And so this is how the architecture of Rook CIF looks like, Rook is the management layer

03:48.960 --> 03:57.680
which installs CIFs, the CIFC driver which helps in mounting the port, like the storage

03:57.680 --> 04:03.800
to your application port, how it's the native Kubernetes we have CIFC or CIF and CIF being

04:03.880 --> 04:06.880
the data layer.

04:06.880 --> 04:14.840
And Rook works in two mode, so first is what I talked about bringing the storage into your

04:14.840 --> 04:20.200
application cluster only, the same cluster that would be the Rook Converse mode.

04:20.200 --> 04:26.120
It's recommended if like we just have one to the single cluster but when we need external

04:26.120 --> 04:30.040
cluster that I would be talking about.

04:30.040 --> 04:33.120
So first of all what an external cluster is.

04:33.120 --> 04:39.200
So external cluster means you already have a CIF cluster installed in which like there

04:39.200 --> 04:45.520
are CIF domains, these are the CIF terminologies, the mon, OSTs, RGWMDS, these are some domains

04:45.520 --> 04:49.200
running that helps your storage to like store the data.

04:49.200 --> 04:55.160
So it's already been there and now comes the part like I'm in Kubernetes world, I need

04:55.160 --> 04:59.080
to connect this CIF to the Kubernetes, so how will you do that?

04:59.120 --> 05:04.640
So there would be a separate Kubernetes cluster running in which there would be a Rook operator

05:04.640 --> 05:11.640
and this FCS earlier and we need to provide data of external CIF cluster to Kubernetes,

05:12.360 --> 05:17.440
so how that magic does, so I will be taking to that part.

05:17.440 --> 05:24.440
So first of all I will tell why you need external cluster, in what cases like I told you like

05:25.000 --> 05:31.000
you can have this same cluster in your native application, so why you need external cluster.

05:31.000 --> 05:38.000
So you might be having a big enterprise where you have like different domains of Kubernetes

05:39.520 --> 05:44.160
cluster running, like there is an admin cluster that is there and there is a finance cluster

05:44.160 --> 05:49.000
department Kubernetes cluster there and you need isolation between them, they shouldn't

05:49.000 --> 05:54.000
talk with each other and underline storage you want the same, so you can make use of

05:54.040 --> 05:57.160
external cluster in that way.

05:57.160 --> 06:01.360
As you see like there is a standalone external CIF cluster that has been connected to different

06:01.360 --> 06:07.400
Kubernetes cluster and have isolation of what kind of data is stored and only access that

06:07.400 --> 06:14.400
data only, that is internal algorithm that maps the secret and puts your data safe.

06:15.200 --> 06:21.280
So this is how you can, like this is one use case when you can use external CIF cluster.

06:21.280 --> 06:25.160
So you see like for example you are native, you are already using CIF but you want to

06:25.160 --> 06:29.960
come into the Kubernetes world, so that would be one of the use case and the third would

06:29.960 --> 06:35.960
be like if you need complete isolation of data, like someone wants to keep the data totally

06:35.960 --> 06:39.800
separate, so then you can use that.

06:39.800 --> 06:43.480
So but external cluster provides all the three types of data, block file object, there is

06:43.480 --> 06:47.040
no restriction on that, so you can make use of that.

06:47.080 --> 06:52.240
So this is why external CIF cluster now comes apart how we can install it and how it internally

06:52.240 --> 06:53.400
works.

06:53.400 --> 06:58.000
Like I showed you like in the diagram we have to grab the information from the standalone

06:58.000 --> 07:02.240
cluster and give it to the Kubernetes cluster, so this is how it's been done.

07:02.240 --> 07:06.400
So there is a provider cluster that standalone where demons are running, so cluster admin

07:06.400 --> 07:12.120
will come, it will scrape the data from it and once scraping the data it will provide

07:12.120 --> 07:16.280
it to the consumer cluster and this is the Kubernetes cluster that is running.

07:16.320 --> 07:25.320
So that data it will create some secrets config map and after we have the details what all

07:25.320 --> 07:30.640
it has been first, like it will give the IP address and other details that we needed

07:30.640 --> 07:36.160
and once it's done it will give it to the Roku operator and Roku operator will perform

07:36.160 --> 07:43.120
in the external mode this time and will create the Roku resources and the storage class and

07:43.120 --> 07:46.560
then we can create PVCs and mount that PVCs to your application ports.

07:48.840 --> 07:55.080
So this is the working and how this scraping is done, so there is a Python script we have

07:55.080 --> 08:02.200
to give some CLI flags to it and this should be like user defined what kind of flags they are,

08:02.200 --> 08:08.320
like it's RBD data pool name or RGB endpoint, so if we give like the specific name of the

08:08.320 --> 08:12.480
pool then we run some self commands with this Python script and it will fetch the data.

08:13.160 --> 08:20.120
Like the monitoring endpoints and other things and give it to the Kubernetes cluster and once

08:20.120 --> 08:26.240
we have the JSON data then we run the import script for it to import it to the Kubernetes

08:26.240 --> 08:31.520
cluster and deploy the manifest, these are the CRDs, the definitions, how we want to configure

08:31.520 --> 08:36.240
our external cluster. So I will be showing this into them in the demo how this works.

08:37.240 --> 08:44.320
And once it's done then we will verify the connection, like we have the cluster running

08:44.320 --> 08:49.840
and so how we can check it, we can just get the self cluster and we can see it's in the

08:49.840 --> 08:55.880
connected and the health okay state. And then goes like we can go and create the storage class,

08:55.880 --> 09:03.000
pool and create PVC, on depending like what kind of storage you need, you need Cephaphase or RBD

09:03.000 --> 09:13.040
or RGW. And now comes the time for the demo. Everything breaks so I have recorded the demo.

09:21.040 --> 09:31.040
So is it visible? So this is the standalone self cluster where we can see the health is okay

09:33.040 --> 09:41.120
and now there are some pools already been there, RBD pools, so these are some self native

09:41.120 --> 09:46.880
commands that we can run by this Cephaphase, we list these Cephaphase file system pools.

09:46.880 --> 09:53.760
Now there is a Python script, I'm giving using the CLI flags and you can see I run the Python

09:53.760 --> 09:59.840
script with some flags and I got the exported data. So this was like external cluster, I

09:59.880 --> 10:05.120
export this data, then I will take this to the Kubernetes cluster that's the second cluster,

10:05.120 --> 10:18.560
we'll copy this and so in this terminal like there was the Minicube running and I have exported it

10:18.560 --> 10:26.720
and now I will run the import script that will use this exported values and create the config

10:26.720 --> 10:36.520
and I have to see grids for the Kubernetes. And after that I have to install the manifest,

10:36.520 --> 10:43.440
the CRDs, the definitions, I am using the example folder of Rook in which there are already defined

10:43.440 --> 10:50.340
some configurations so I am using that only for now. So there would be CRDs.ML, there would be

10:50.820 --> 10:56.220
common.ML which have the Rbex, the specific permissions to it, what permissions I need to

10:56.220 --> 11:09.420
give and the operator.ML, the Rook operator file and the cluster external, this is the

11:09.420 --> 11:18.740
Ceph extender cluster that I will create in Rook, in the Kubernetes side and there is also

11:18.740 --> 11:23.100
common external which creates the separate namespace if we want to keep the Ceph cluster in

11:23.100 --> 11:28.540
different namespace. So in this I am using a separate namespace for the Ceph cluster and for

11:28.540 --> 11:32.940
the Kubernetes cluster I am using the separate namespace in which the Rook operator resides.

11:32.940 --> 11:42.020
And now I am checking, I have created all the manifest, now I am waiting like to get the Ceph

11:42.140 --> 11:50.740
cluster up and running. So the Ceph cluster is created and so this is the Ceph cluster,

11:50.740 --> 12:03.180
internals, YAML if you want to see and now I will describe it and you can see like it is

12:03.180 --> 12:11.540
ready. Now the main thing is I have to wait for the Rook operator to get started. So this is how

12:12.060 --> 12:17.140
all the parts that will be get started, there is a Rook operator, there are some Ceph CSI

12:17.140 --> 12:26.260
plugins, demons that will use to mount the data and yeah that's all. And other demons are already

12:26.260 --> 12:34.700
running in external Ceph cluster, the Mons, OSDs in which our data will resides. So the

12:34.700 --> 12:39.100
connection is good what I have shown in previous slides, now I have created the storage class,

12:40.060 --> 12:46.220
the storage class will be also be created, we can create some others if we needed. Now in this

12:46.220 --> 12:57.260
demo I will tell you like how we can create a RBDPVC and store data and how it's disaster proven or

12:57.260 --> 13:05.700
how it's how Ceph internally replicates and make our data safe. So now the demo starts for that.

13:06.700 --> 13:15.460
So once we have the storage class the Ceph RBD or the Rook Ceph block these two belongs to the

13:15.460 --> 13:26.460
block pool and now I am creating this MySQL example in which it will create a MySQL pod and a PVC

13:26.460 --> 13:34.020
and a service to it. So what I will do is making use of that Ceph RBD storage class,

13:34.020 --> 13:44.780
I will create this PVC and mount this PVC to the MySQL pod and there is a service if you want to

13:44.780 --> 13:55.740
see that MySQL service outside the mini-cube you can make use of that service. So the PVC is created,

13:56.020 --> 14:05.220
you have the pod, I will wait for the pod, PVC is bounced it and pod is still getting created,

14:05.220 --> 14:14.700
the word press MySQL and the service is created. Once the pod is created, it's up and running.

14:14.700 --> 14:21.900
Now I will go inside the pod and I will go to the mount path that I have written and I will create a

14:21.900 --> 14:29.900
file in it. So this is actual application for example MySQL that might be your actual application

14:29.900 --> 14:42.180
there and what I am doing right now here. So this is I am telling like if you want to use a

14:42.180 --> 14:49.780
node pod service like this is something that is needed if you want to see are like I am on mini-cube

14:49.900 --> 14:55.620
if you want to see the cluster outside whatever you can use this node pod service but the main thing

14:55.620 --> 15:04.140
is the data so focusing on that. So I am inside the pod using this command, escape command.

15:11.500 --> 15:17.700
So I am inside it now I will go to a certain path that's the mount pod where the PVC is

15:17.700 --> 15:35.180
been mounted that is where so while creating the pod I have given this path so I am using that only

15:35.180 --> 15:43.900
and now in this part where PVC will write the data so I am creating a demo dot TXT where I will

15:43.940 --> 15:55.940
say I will be persisted because of CIF replication and now I will go so this is inside the mount

15:55.940 --> 16:03.780
pod and I will go and delete this pod. So this is file has been created I mean so it the pod now

16:03.780 --> 16:09.020
I will delete the pod for example there is a disaster this pod has been deleted I will delete

16:09.060 --> 16:16.100
this and as it is a deployment it will create again in some other node as the pod is deleted and

16:16.100 --> 16:26.060
now I can see the pod is recreated by Kubernetes it is 9 second ago and I will go inside it again to

16:26.060 --> 16:31.780
the exact path the CD where live mySQL and I can see that that file should re-exist there

16:32.780 --> 16:47.060
where live mySQL and if I cat it so there is still demo dot TXT even if the pod was gone but the

16:47.060 --> 16:56.220
file was still exist you can see that and that's all with the demo and quickly back to the slides

16:56.220 --> 17:03.980
so these are some new features we have added the RIDOS namespace the IPv6 support and the

17:03.980 --> 17:12.220
RGW multisite and we are also going to add some new more features the replica one and support for

17:12.220 --> 17:19.220
the topology awareness and also to improve the documentation and these are some community

17:19.220 --> 17:25.620
links and thanks for that this is my LinkedIn logo if you want someone wants to connect and yeah

17:25.620 --> 17:35.980
thanks for that any questions anyone have

17:35.980 --> 17:46.260
what credentials do you need to abstract after get all the information from the subcluster do you

17:46.260 --> 17:53.680
need to be admin user or can you also be a regular user so if you don't want to expose

17:53.680 --> 17:58.460
adding credentials to a user so can you do actually do this as a user with enough self

17:59.060 --> 18:10.220
to make an export of all these information and bring it into your own okay so the question is what

18:10.220 --> 18:16.580
all permissions we need when we actually the export the data the client admin is there like

18:16.580 --> 18:22.220
which actually gets the data and give it to the Kubernetes cluster so what all permissions we needed

18:22.220 --> 18:29.260
so we keep the permissions minimum we can give the admin key ring but we just give the minimum

18:29.260 --> 18:37.280
permissions to it to just read and write because we don't allow ROOP to create anything on this

18:37.280 --> 18:43.340
subcluster we just allow to read the data and manage it so that's the minimum permissions that

18:43.340 --> 18:58.980
was required to CFCSI so we expose that only anyone else okay I guess thanks thank you all

