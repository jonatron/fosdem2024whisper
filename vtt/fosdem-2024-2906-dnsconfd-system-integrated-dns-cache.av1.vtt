WEBVTT

00:00.000 --> 00:07.000
My next speakers are Thomas and Peter who will tell us about DNS Confit, which is new

00:10.000 --> 00:12.000
to me from quite curious.

00:12.000 --> 00:18.000
So, hi everyone, my name is Tomas Korbars, this is my colleague Petro Manchik, we work

00:18.000 --> 00:22.000
at Red Hat and today we've come to talk to you about our new project that is called

00:22.000 --> 00:29.000
DNS Confit. So let's start a bit with a motivation behind this project. Last year we received

00:30.000 --> 00:35.000
a request from a user that required us to make possible for Unbound to be used as a

00:35.000 --> 00:41.000
local DNS cache and to be able to consume configuration from the network manager. In

00:41.000 --> 00:46.000
the past we had DNSsectorrigger package for this, but we dropped that in rail 9. So should

00:46.000 --> 00:53.000
we reintroduce it? We thought about implementing a debuts API into Unbound just as DNSMiles

00:53.000 --> 00:58.000
has and then implementing a network manager plugin just as DNSMiles has. But then we realized

00:58.000 --> 01:04.000
that if some similar request came in the future for different service we'll be doing the same

01:04.000 --> 01:09.000
over again. So we thought about creating a new project that would serve as a conduit

01:09.000 --> 01:16.000
between network manager and local DNS caching services. This project is the DNS Comp. Our

01:16.000 --> 01:23.000
requirements for it would be to be able to easily exchange the DNS cache, underlying cache,

01:24.000 --> 01:30.000
and to add more services in the future without too much work. We need to be able to support

01:30.000 --> 01:39.000
split DNS configuration. We need to be able to support split DNS configuration and then

01:39.000 --> 01:46.000
we need to be able to auto configure without manual interaction from the user. Also, we

01:46.000 --> 01:52.000
would like it to use already present system configuration and defaults and security features

01:52.000 --> 01:58.000
that are already built in and we maintain inside of our distribution. The behavior needs to be

01:58.000 --> 02:05.000
configurable enough so you can change handling of corner cases and you are not caught of guard

02:05.000 --> 02:08.000
by the behavior that you would not expect.

02:08.000 --> 02:25.000
Okay. Let's get a bit back in the past and tell something about why Fedora 33 introduced

02:25.000 --> 02:36.000
DNS cache and what it brings to us was a possibility of multiple simultaneous VPN

02:36.000 --> 02:43.000
connection at the same time. And that's great. It also made possible to configure global

02:43.000 --> 02:54.000
servers but reach some names which are accessible only on local network, which is nice for DNS

02:54.000 --> 03:02.000
over TLS but that was not enabled yet and still isn't. And it brought us excellent configuration

03:02.000 --> 03:10.000
presentation by Resolve's CDL command compared to what we had before. That was clearly better.

03:10.000 --> 03:18.000
And it also introduced well-documented bus interface for both configuration changes, for

03:18.000 --> 03:29.000
configuration displaying and also name resolution. They have nice article but that's not our job

03:29.000 --> 03:39.000
here. So what do we mean by 3DNS for here? When you connect to VPN without some smart

03:39.000 --> 03:50.000
solution like this, you send all name queries just a single VPN and use only your primary

03:50.000 --> 03:57.000
connectivity to deliver traffic to VPN server and that consumes everything you use. At that

03:57.000 --> 04:04.000
time, you cannot use any other connection interfaces you have on your laptop or mobile

04:04.000 --> 04:15.000
phone or something else because you use just one DNS or set the DNS that VPN knows. With

04:15.000 --> 04:24.000
split DNS, you can send different name queries to different set of servers provided by different

04:24.000 --> 04:31.000
networks. You are connected at the same time and most current devices today are capable

04:31.000 --> 04:45.000
of connectioning to different networks at the same time including multiple VPNs. All you

04:45.000 --> 04:54.000
need to have is non-coflicting names for them. So for example here, names are different and

04:54.000 --> 05:03.000
if some names in those domains provide some useful networks, you can access them at the

05:03.000 --> 05:11.000
same time. And we could end it here and thank SystemDGuys if everything worked great but

05:11.000 --> 05:19.000
sadly that was not the case entirely. I have listed few issues I think are important and

05:19.000 --> 05:30.000
still aren't fixed sufficiently but there were more bugs in the meantime somewhere fixed,

05:30.000 --> 05:40.000
some are still not. For example, it prevents any usage of DNS on the host which is where it

05:40.000 --> 05:48.000
is enabled by default configuration both Ubuntu and on our Fedora because it just doesn't

05:48.000 --> 05:57.000
forward DNS-enabled bit set in queries received. So any library which is quite capable of using

05:57.000 --> 06:07.000
DNS-sec cannot use it even if infrastructure, your network provides capability for it. Also,

06:07.000 --> 06:16.000
at least for Fedora and Ubuntu desktop I think, you would be quite surprised this top level

06:16.000 --> 06:26.000
domains often that does not exist because it sends top names without dot just a local

06:26.000 --> 06:36.000
interface over multicast protocol and if it doesn't find something which usually it doesn't,

06:36.000 --> 06:45.000
it just returns no that does not exist. So com domain does not exist but github.com domain,

06:45.000 --> 06:56.000
surprise it does and even on server edition when I think this is really unwanted. And also

06:56.000 --> 07:06.000
strange response is when a response fails because of DNS-sec validation fail, it still

07:06.000 --> 07:15.000
might contain a valid answer in the response which is unexpected and no other implementation

07:15.000 --> 07:26.000
I know does it this way. So DIC plus short DNS-sec failed org even with DNS-sec enabled in system

07:26.000 --> 07:38.000
DreslD gives you very nice address and I've listed just few issue numbers. So lessons we take

07:38.000 --> 07:48.000
from this is we want split DNS functionality auto configured and we want possibility to DNS over

07:48.000 --> 08:03.000
TLS and also that we want nicer front end than we had but system D people are very good at

08:03.000 --> 08:10.000
expertise in system integration and they are quite good engineers and I know it but they lack

08:10.000 --> 08:21.000
expertise in DNS protocol area and I am afraid it is visible and at the same time DNS resolvers

08:21.000 --> 08:30.000
people are excellent in DNS protocol area but their integration into system is often very

08:30.000 --> 08:40.000
limited or at least done and we think only the integration is missing and that is what we are

08:40.000 --> 08:51.000
trying to provide. So we want to reuse existing functionality. We want to provide some common

08:51.000 --> 09:02.000
interface to set forwarding to different servers so it doesn't change much and we want to provide

09:02.000 --> 09:13.000
nicer front end for showing what is configured regardless of use DNS cache in the end. So what

09:13.000 --> 09:23.000
we need for split DNS we need some local address which receives queries from applications that

09:23.000 --> 09:31.000
usually localhost we need ability to configure different domains to be forwarded to different

09:31.000 --> 09:40.000
sent of servers and of course some default for root servers to be forwarded to global default and we

09:40.000 --> 09:48.000
also want ability to reconfigure the service without stopping it and flashing entire cache as

09:48.000 --> 10:01.000
starting it again and from this is list of servers we have in Fedora and I think all of them are able to

10:01.000 --> 10:11.000
provide split DNS functionality most of them are also able to provide DNS over TLS functionality but

10:11.000 --> 10:23.000
only DNS mask except there is of D have some D bus capability and that is quite limited and DNS mask has

10:23.000 --> 10:33.000
own issues. So our approach is use what already exists provide just front end and components

10:33.000 --> 10:43.000
coordination do not reinvent the wheel. We do not want to handle DNS queries ourselves in our

10:43.000 --> 10:56.000
service we want proper services to do it and we just provide configuration for them and I have already

10:56.000 --> 11:11.000
shown almost every open source resolver has that ability and because we are not handling queries we just want to

11:11.000 --> 11:23.000
try single thread application and we written just our prototype in Python to verify this would work. What we

11:23.000 --> 11:36.000
also want is to reconfigure ETC resolve confile only when we verify basics that service is running and restore it when

11:36.000 --> 11:48.000
our service is stopped. I really hate what a result is when you uninstall it you have to fix it by hand. And we

11:48.000 --> 11:59.000
want to have stand alone demon because not everything is primary configuration we think should be done in network

11:59.000 --> 12:12.000
manager so there is some unified way to configure it whether it is used system be resolved D or our demon it should

12:12.000 --> 12:25.000
not change it should be just implementation detail. And we think the common part is the biggest one and just very

12:25.000 --> 12:37.000
small cash specific module is required to implement different caches what we plan to support is what we have in

12:37.000 --> 12:49.000
the RL that is primary unbound and also bind and DNS mask. And we want to provide basic compatibility for services

12:49.000 --> 13:01.000
using system D or the API directly because something already uses that but we do not want to implement every aspect

13:01.000 --> 13:07.000
of what they already implemented because we do not think that is necessary.

13:11.000 --> 13:18.000
So right how does the flow of configuration look right now network manager receives its list of DNS servers from either

13:18.000 --> 13:27.000
DHCP or the connection profile and then it pushes the configuration through the bus API into DNS confi D. DNS confi D

13:27.000 --> 13:35.000
then translates this configuration into some internal representation that we think is general enough for most underlying DNS caches

13:35.000 --> 13:44.000
and then we use specified module to transform this into the specific configuration that is used by the specific underlying

13:44.000 --> 13:55.000
service. For example for unbound it is a list of four borders. How does the system integration look like now. DNS confi D

13:55.000 --> 14:06.000
uses already existing unbound service that we ship and support so it respects its defaults security features and configuration

14:06.000 --> 14:16.000
that we ship. We inherit the system D result D debuts API so we work as an in place replacement as of now. You use the

14:16.000 --> 14:26.000
default system configuration that is provided and then we watch the underlying changes of the DNS cache so you are not caught

14:26.000 --> 14:37.000
off guard by the sudden inability to resolve the domain names. Here's the life cycle of our program that I talked about. DNS confi D

14:37.000 --> 14:48.000
itself is implemented as a system service so you can inspect it as you would inspect normal system service and it is started

14:48.000 --> 14:56.000
either on boot when it is enabled or it is started when configuration is pushed through because it is implemented by the bus

14:56.000 --> 15:07.000
and system D triggers us upon the configuration. After we start we start the underlying DNS cache. We look whether it is ready or not

15:07.000 --> 15:15.000
because there is some polling right now needed and we wait for the configuration that is provided by network manager. After that we watch

15:15.000 --> 15:27.000
for status changes and perform actions as are needed. Here are some memorable issues that we've encountered. The first one is a war for

15:27.000 --> 15:37.000
resolve confile because network manager finds out that a system D result D is running or not by checking existence of some symbolic links

15:37.000 --> 15:46.000
in system and we cannot own them because they are owned by the system D result D package and if they are not present on the system then

15:46.000 --> 15:59.000
network manager tried always to override our modifications of the resolve conf. We got a run by that by implementing a command that pushes lines

15:59.000 --> 16:10.000
into the configuration of network manager and we stop it from touching the resolve conf. We argued about whether it is better to execute the underlying

16:10.000 --> 16:19.000
service as a sub process or a system service because sub process approach provides easier way to monitor whether it is running or not

16:19.000 --> 16:32.000
but then I was persuaded by Peter that the system service is better because we use things that we already have in place. There is the issue whether unbound is truly up or not

16:32.000 --> 16:47.000
because the start job was finished but the command channel was not open yet so we faced some instability during testing but we got around that by pulling a few times

16:48.000 --> 16:59.000
and we need to update only zones that were updated in configuration so we hold current state that is set into unbound and we update only zones that are required

16:59.000 --> 17:12.000
and we thought that implementing this in D bus would be easier than it proved it really was. We've created a way, we are using of testing this

17:13.000 --> 17:27.000
we are using TMT test management tool with containers that allow us to simulate some network behavior in a way that verifies the actions of DNS conf. If you'll ever want to contribute

17:27.000 --> 17:38.000
set of these tests will verify that you won't change behavior that is already in place or you will be able just to show us where we are wrong and you want us to change the thing.

17:43.000 --> 18:00.000
Okay so what is working already? I admit we wanted much more to present here but it proved not so simple so split DNS configuration as you from network manager already works

18:01.000 --> 18:17.000
ETC Resolve Conf is changed just when our demon is running and is restored when it is stopped. Unbound support is the only one we have at this moment

18:18.000 --> 18:40.000
and implementation uses only D bus interfaces of system D Resolve D and at this moment also only its D bus name so it can be running just DNS Conf or Resolve D but not both

18:40.000 --> 19:07.000
and we reused network manager system D Resolve DNS plugin for now because it pushes configuration over D bus but in the future we want to get rid of it and make our own or use more parameters from just IP address

19:08.000 --> 19:25.000
and that is what we would like to use unlike the opportunistic way which system D Resolve D used because these RFCs were not defined at that time and we think this is correct way

19:25.000 --> 19:40.000
and support multiple cache is running at the same time is not necessary usually but it would be very helpful for some kinds of testing.

19:41.000 --> 20:02.000
We would like to have ability to forward over DNS over HTTPS but there is problem not any DNS cache we have in RL supports that and in further there are only few similar with DNS over Quick

20:03.000 --> 20:24.000
and auto configuration of DNS sec would be nice we would like to have some successor and better implementation what was once attempted with DNS sec trigger but maybe better accept it

20:25.000 --> 20:49.000
and maybe if its time sometime in the future rewrite into Rust and reduce memory required memory for our interfaces that would be all for us so if there are questions please now is the time

20:49.000 --> 20:59.000
and if we can't answer them please use these mails or file issue on the project

20:59.000 --> 21:04.000
Definitely stick around to the next speaker we will talk about the Rust domain craze

21:04.000 --> 21:06.000
and thanks for the call

21:11.000 --> 21:13.000
Questions, stay phone

21:14.000 --> 21:20.000
Would be helpful for inbound to have a D bus connection where it says when it's ready

21:20.000 --> 21:40.000
No I don't think it needs to be D bus connection I think we need to correct LIP system D notify event which it kinds of supports but I think last time we try to enable in federal it start crashing so it's not built in

21:40.000 --> 21:59.000
but some kind of support is there we just need support to inbound to tell us I'm the service I think I'm ready and there's system D API for that we need to use that whenever possible it doesn't have to be D bus

21:59.000 --> 22:01.000
Visek?

22:29.000 --> 22:40.000
If you only want to communicate over DNS local servers you need to crash

22:40.000 --> 22:46.000
I understand that you want to drop the MSS Resolve bridge

22:46.000 --> 22:52.000
So how do you want to overcome this? This is part of the question

22:52.000 --> 22:59.000
Second part of the comment is that we talk about D bus but D bus is something everyone can relate

22:59.000 --> 23:15.000
Actually now it's a series of D bus in parallel which means that we can have a resolution since any book before the D bus server is up

23:15.000 --> 23:22.000
Which is why it's always useful so we had a plan to add the private interface

23:22.000 --> 23:33.000
The second question was do we plan to add running interface? No I don't think we want that

23:33.000 --> 23:40.000
First question was get other info API

23:40.000 --> 23:48.000
How can you send the additional information for example about multiple interfaces over DNS projects?

23:48.000 --> 24:06.000
How can I send from which interface comes the query or how to request query just for selected interfaces over DNS?

24:06.000 --> 24:13.000
We don't want to because in what cases this is needed

24:13.000 --> 24:18.000
I think network manager needs that just to verify the connection works

24:18.000 --> 24:30.000
I think we might have different service which just will query us please

24:30.000 --> 24:39.000
Tell me address resolved on this interface and we will send the query just to correct addresses

24:39.000 --> 24:49.000
Because we know which addresses are used for that interface but that would be not served by the local cache

24:49.000 --> 24:53.000
Because that is not yet configured for that

24:53.000 --> 24:59.000
Can I make more sense to take this separately?

24:59.000 --> 25:02.000
To do this after session? Because it seems quite specific

25:02.000 --> 25:04.000
Yes, yes it might

25:04.000 --> 25:07.000
Any other questions?

25:07.000 --> 25:09.000
No thank you again

