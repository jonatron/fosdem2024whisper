WEBVTT

00:00.000 --> 00:08.000
Okay, can you hear me?

00:08.000 --> 00:10.640
Excellent. Thank you.

00:10.640 --> 00:12.480
So it's a pleasure to be here.

00:12.480 --> 00:15.200
I'm on goal this amazing speakers today.

00:15.200 --> 00:16.480
So I'm Thanos.

00:16.480 --> 00:18.800
I'm a search fellow at the University of Manchester.

00:18.800 --> 00:20.960
I'm part of the Tornado VM team.

00:20.960 --> 00:26.240
And today I will talk about polyglot language implementations,

00:26.880 --> 00:31.200
which enable programming languages like Ruby, Python,

00:31.200 --> 00:34.960
and to run on top of the JVM, along with Java, of course.

00:34.960 --> 00:37.760
And I will try to make a step forward

00:37.760 --> 00:42.240
and show you how they can harness GPU acceleration from the JVM.

00:44.560 --> 00:47.040
I'll start a little bit with the polyglot programming,

00:47.040 --> 00:49.440
which has been here for many years,

00:49.440 --> 00:53.360
but in a sense it has been reignited by the advent of

00:54.320 --> 00:56.800
the Truffle framework from Graal VM.

00:57.520 --> 01:00.720
And in a sense it enables multiple programming languages

01:00.720 --> 01:03.680
to run on top of the JVM and interoperate.

01:03.680 --> 01:08.800
So that means that one Java class file can invoke a Python function

01:08.800 --> 01:11.920
and the Python program can invoke a Java method.

01:13.520 --> 01:14.720
Well, this is very interesting.

01:14.720 --> 01:16.480
It comes with many advantages.

01:17.680 --> 01:19.600
But what about GPU programming?

01:19.600 --> 01:21.280
Well, GPUs from Java.

01:21.280 --> 01:22.800
Well, this is not a thing yet.

01:24.080 --> 01:26.800
That's why we have been motivated at the University of Manchester

01:26.800 --> 01:30.560
and we have done all this research in the past eight years

01:30.560 --> 01:32.960
and we have created Tornado VM.

01:33.760 --> 01:36.240
Here is a link to the resources of Tornado VM

01:36.240 --> 01:39.920
with all the presentations that explain the programming model.

01:39.920 --> 01:42.400
Because my goal today is not to go very deep,

01:42.400 --> 01:44.960
to dive into the Tornado VM very deep,

01:44.960 --> 01:46.960
but to present the interoperability

01:46.960 --> 01:48.400
with the other programming languages

01:48.400 --> 01:52.560
and how they can use GPU acceleration from the JVM.

01:54.160 --> 01:56.960
So Tornado VM is an open source

01:56.960 --> 02:00.560
plug-in to existing JDK distributions.

02:00.560 --> 02:03.920
It is compatible with JDK 21, as you will see later.

02:04.560 --> 02:07.520
And it has some very cool features.

02:07.520 --> 02:09.840
So it has a platform agnostic API.

02:09.840 --> 02:14.000
So developers, they don't need to know GPU programming,

02:14.000 --> 02:15.120
FPGA programming.

02:16.960 --> 02:19.760
It comes with an optimizing compiler.

02:19.760 --> 02:22.400
So we extend GRAL with new phases

02:22.480 --> 02:24.640
that they can take Java methods

02:24.640 --> 02:26.640
and compile them to GPU code.

02:28.960 --> 02:31.920
We have a feature of dynamic reconfiguration at runtime,

02:31.920 --> 02:36.160
which means that the method execution can be migrated

02:36.160 --> 02:38.320
from a GPU back to the JVM

02:38.320 --> 02:42.400
and then go to the FPGA if it is appropriate.

02:43.440 --> 02:47.040
And with the latest release 1.0,

02:47.040 --> 02:50.240
we have enabled support for off-heap data types.

02:50.240 --> 02:53.520
So data can be allocated off-heap

02:53.520 --> 02:56.160
with a foreign function and memory API.

02:56.800 --> 03:01.680
And this is the API that Mauricio described earlier today.

03:01.680 --> 03:05.280
So feel free to follow Tornado VM in Twitter

03:05.280 --> 03:09.280
to engage with the website and of course to fork

03:09.280 --> 03:12.400
and try our examples which are open sourcing GitHub.

03:14.400 --> 03:16.800
So I spoke a little bit about off-heap data types,

03:16.800 --> 03:19.360
so I'll give an introduction, an example,

03:19.360 --> 03:23.440
because I'm not going to dive very into the API.

03:23.440 --> 03:25.760
So here we see two snapshots of code.

03:25.760 --> 03:29.200
On the left side, we see a main method

03:29.200 --> 03:32.000
that contains the allocation of float array

03:32.800 --> 03:34.160
by using primitive types,

03:34.960 --> 03:38.800
but is allocated as an object, in a sense, on-heap.

03:38.800 --> 03:41.520
So to migrate from such an allocation

03:42.240 --> 03:46.320
to the new allocation API that's exposed by the Tornado API,

03:46.320 --> 03:48.720
we have created the float array object

03:48.720 --> 03:51.600
that inside it can allocate memory

03:51.600 --> 03:54.960
by using the memory segment of the foreign function API.

03:55.840 --> 03:58.320
And it will allocate this memory off-heap.

03:58.320 --> 04:03.840
So this memory segment could be used directly from the GPU

04:04.880 --> 04:08.000
without the need to worry about GC collections and this stuff.

04:08.560 --> 04:11.760
And the cool part is that even if you don't use GPU programming,

04:11.760 --> 04:14.240
even if you don't want to execute on GPUs,

04:14.240 --> 04:18.320
you can still use this API to allocate memory off-heap.

04:19.120 --> 04:23.680
And here is a link that explains more.

04:24.720 --> 04:27.840
I hope it's visual from your side.

04:27.840 --> 04:32.320
If not, you will find my slides online in the Fosdome webpage.

04:33.040 --> 04:34.960
So the motivation for today is that

04:35.520 --> 04:39.280
Graal VM enables interoperability between programming languages

04:39.280 --> 04:42.560
like Ruby, JavaScript, and other programming languages.

04:43.680 --> 04:47.040
And Tornado VM enables hardware acceleration for Java.

04:47.600 --> 04:50.320
So what if we can combine them together

04:50.320 --> 04:54.240
and harness GPU acceleration from all these programming languages

04:54.240 --> 04:55.840
that are running on top of Trafl?

04:58.000 --> 05:00.160
Let's have a dive into the tech flow.

05:00.160 --> 05:04.480
So in this slide, I present a software stack

05:04.480 --> 05:06.720
from Graal VM for Trafl.

05:06.720 --> 05:10.160
So on the top, we see the Trafl framework

05:10.160 --> 05:13.360
and many implementations of polyglot runtimes

05:13.360 --> 05:16.880
like Graalpy, Graal.js, Trafl Ruby.

05:16.960 --> 05:20.240
And others because Trafl enables also programming language

05:20.240 --> 05:23.200
implementers to create their own programming languages

05:24.480 --> 05:26.240
by using the Java API.

05:26.240 --> 05:30.080
So I have grouped Python, Ruby, JavaScript, and Node.js

05:30.720 --> 05:32.240
in this side of the slide.

05:33.440 --> 05:37.360
And then beneath them, there is the Graal VM Zit compiler,

05:37.360 --> 05:39.280
so an optimizing compiler from Graal.

05:40.320 --> 05:44.240
So Java is also running on top of the JVM, of course.

05:44.240 --> 05:46.320
And all these languages, they start

05:46.880 --> 05:49.760
in the interpreted mode, and once they reach a hot state,

05:49.760 --> 05:52.480
then the optimizing compiler kicks in.

05:53.360 --> 05:57.520
And the cool part with such a polyglot implementation

05:57.520 --> 05:59.360
that enables polyglot programming

05:59.360 --> 06:03.360
is that there is, for the compiler enthusiasts,

06:03.360 --> 06:05.680
there is one Graal IR.

06:06.400 --> 06:09.680
So the nodes, at runtime, they are rewritten.

06:09.680 --> 06:11.920
That means that it can adjust.

06:11.920 --> 06:14.960
So if we kick in a Python function,

06:14.960 --> 06:17.120
then the node can be rewritten,

06:17.120 --> 06:20.800
and the Graal compiler will take a shape

06:20.800 --> 06:24.240
and will emit at the assembly code that will run on the CPU.

06:25.440 --> 06:29.200
So this solution offers the interoperability

06:29.200 --> 06:32.400
and offers the execution among different CPU

06:32.400 --> 06:33.840
instruction set architectures.

06:34.560 --> 06:38.720
But what if we have this heterogeneous hardware,

06:38.720 --> 06:41.680
like GPUs, FPGAs, which are available

06:41.680 --> 06:43.120
in some systems and servers?

06:43.760 --> 06:45.840
Well, then we'll have Tornado VM

06:45.840 --> 06:51.200
that enables Java methods to be compiled for GPUs, FPGAs, etc.

06:51.760 --> 06:54.480
Tornado VM has its own JIT compiler,

06:54.480 --> 06:57.680
which is an extension, a superset, I would say, of Graal,

06:57.680 --> 07:02.720
the Graal compiler, that it is enhanced with new phases

07:02.720 --> 07:05.680
in the compiler to automatically specialize

07:05.680 --> 07:10.720
the code from a method for GPU acceleration and FPG acceleration.

07:10.720 --> 07:12.320
So at the backbone of the compiler,

07:12.320 --> 07:14.240
we have three backends at the moment.

07:14.240 --> 07:17.440
We have OpenCL backend, CUDA, and SPV.

07:18.960 --> 07:22.560
And such a solution would enable many things.

07:22.560 --> 07:25.920
So if you want to learn more about the APIs,

07:25.920 --> 07:28.640
you can scan this QR code.

07:29.360 --> 07:33.840
And the code that is implemented with Tornado VM,

07:33.840 --> 07:38.320
it can harness besides the off-hip data types,

07:38.320 --> 07:41.920
it can also harness the execution with a Tornado VM profiler.

07:41.920 --> 07:45.440
If you want to learn more about the characteristics

07:45.440 --> 07:47.920
of your application, you can see how many data

07:47.920 --> 07:49.840
will be copying in the GPU memory,

07:50.720 --> 07:52.800
how expensive is the IEO maybe,

07:52.800 --> 07:55.120
because this could be very critical

07:55.120 --> 07:56.560
for the performance of the system.

07:57.520 --> 07:59.840
And you can customize even how many,

07:59.840 --> 08:02.400
how the data transfers will be performed.

08:02.400 --> 08:05.440
Because, for example, if you have a method

08:05.440 --> 08:07.520
that consumes redoneally data,

08:07.520 --> 08:10.160
then maybe you need to copy the data once,

08:10.160 --> 08:13.200
instead of copying the data every time you execute the kernel.

08:15.680 --> 08:18.080
Okay, so let's jump to the deployment.

08:19.760 --> 08:21.680
As I said, Tornado VM is compatible

08:21.680 --> 08:24.000
with different JDK distributions,

08:24.000 --> 08:29.600
so it's not a JVM, it is a plugin for JDK distributions.

08:30.160 --> 08:32.720
So it can be seen as a library, in a sense,

08:32.720 --> 08:34.800
because it offers an API in Java.

08:35.520 --> 08:38.880
And it is compatible with all these distributions.

08:39.760 --> 08:42.880
And on the other side, we have the compiler backends

08:42.880 --> 08:44.640
that makes it compatible with

08:45.360 --> 08:47.600
different heterogeneous hardware accelerators.

08:48.160 --> 08:52.160
We can emit vectorized code for multi-core CPU execution

08:52.160 --> 08:53.120
through OpenCL.

08:53.920 --> 08:57.200
We can run with different GPUs and FPGAs.

08:57.920 --> 09:00.720
In this particular talk, I will focus on GraVM,

09:00.720 --> 09:02.480
because we want to leverage polyglot,

09:03.120 --> 09:06.160
and NVIDIA GPUs, because I have created Docker images

09:06.160 --> 09:08.480
that they run on the NVIDIA GPUs.

09:10.720 --> 09:13.520
Now, regarding the GraVM deployment,

09:14.720 --> 09:16.720
I will focus in this slide in GraL Python,

09:16.720 --> 09:19.920
which is one implementation of polyglot runtime.

09:21.040 --> 09:26.000
This is shipped in two different standalone versions, releases.

09:26.000 --> 09:27.760
So we have the native standalone,

09:27.760 --> 09:31.520
which comes with the native image.

09:31.520 --> 09:34.400
And then we have the JVM standalone that enables

09:34.400 --> 09:38.160
the execution of Python programs on top of the JVM,

09:38.160 --> 09:40.560
and it has also the JVM compiler.

09:41.600 --> 09:45.200
The version that we tested is the 23.1,

09:45.200 --> 09:48.400
because tornado VM is compatible with this version of GraL.

09:49.200 --> 09:52.640
And here you can see that we have downloaded the community,

09:52.640 --> 09:54.320
and that's JVM.

09:54.320 --> 09:58.640
So we have the JVM standalone version downloaded.

09:59.520 --> 10:02.720
Well, we need the JVM standalone,

10:02.720 --> 10:04.880
because we want to run with tornado VM,

10:04.880 --> 10:08.080
and tornado VM will extend the GraL VM compiler.

10:08.080 --> 10:09.440
So this is the reason.

10:10.160 --> 10:12.080
The problem is that we tried it,

10:12.080 --> 10:16.080
and the JVM distribution is shipped with the JVM standalone,

10:16.080 --> 10:19.920
with a compiler built that it is built with libgral.

10:20.480 --> 10:25.600
So this comes with not many compiler modules,

10:25.600 --> 10:28.400
and that breaks the consistency for tornado VM.

10:28.400 --> 10:29.200
When we tried it.

10:30.160 --> 10:32.320
And this is because they wanted the image,

10:32.320 --> 10:34.640
the footprint to be lower, which makes sense,

10:34.640 --> 10:36.880
but it broke the compatibility with tornado VM.

10:37.680 --> 10:41.200
The good part on this story is that GraL is very active.

10:41.200 --> 10:44.000
The GraL community is very active in Slack workspace,

10:44.000 --> 10:47.840
so we managed to figure out what was the problem.

10:49.360 --> 10:52.400
On the bad side is that the solution was to build

10:52.400 --> 10:54.560
a GraL Pi and GraL VM from source,

10:54.560 --> 10:56.400
which was quite painful.

10:57.040 --> 11:00.320
And in order to avoid this pain for anyone

11:00.320 --> 11:01.840
who wants to try this work,

11:01.840 --> 11:04.240
we decided to build a Docker image

11:04.240 --> 11:08.640
that has inside GraL Pi, tornado VM,

11:08.640 --> 11:11.040
and we have also added the NVIDIA driver.

11:11.040 --> 11:14.800
So if you have a Linux machine or any machine

11:14.800 --> 11:16.640
that has an NVIDIA GPU,

11:16.640 --> 11:20.320
and you have also the NVIDIA container toolkit in this machine,

11:20.320 --> 11:23.040
then you will be able to run this image.

11:23.600 --> 11:27.520
The Docker file, the image is open source in GitHub.

11:28.560 --> 11:31.360
And on the other side,

11:31.360 --> 11:34.160
you can see the QR code that has the acceleration library.

11:34.160 --> 11:36.800
So the code that we have implemented

11:36.800 --> 11:39.760
in the examples module of tornado VM

11:41.200 --> 11:44.000
for the computation part that we will upload on the GPU,

11:44.000 --> 11:47.520
like K-means, matrix multiplication,

11:47.520 --> 11:48.720
and those are the examples.

11:48.720 --> 11:50.720
But there are also other compute examples

11:50.720 --> 11:52.240
that we have in the GitHub.

11:53.520 --> 11:56.160
And you can also pull the Docker image from Docker Hub.

11:58.800 --> 12:01.360
So we will jump into the examples.

12:03.280 --> 12:04.560
So as you see here,

12:04.560 --> 12:08.800
we have the Python and Java with tornado VM.

12:08.800 --> 12:12.880
So we have the Python program that imports Java,

12:12.880 --> 12:17.440
and then it loads the class from the compute examples class

12:17.440 --> 12:20.320
of the tornado VM repository.

12:21.040 --> 12:24.080
And then we have in this Java class that we have loaded,

12:24.080 --> 12:28.000
we have two methods that can be accessed by the Python program.

12:28.560 --> 12:30.240
The first one is the set inputs

12:30.240 --> 12:32.960
that set the actual data points

12:32.960 --> 12:36.240
and the number of packets that will be used for K-means.

12:36.240 --> 12:38.880
And the second one is the run with GPU.

12:38.880 --> 12:43.520
So this will invoke the actual GPU compilation

12:43.520 --> 12:45.440
for GPUs and the GPU execution.

12:46.960 --> 12:48.080
And on the other side,

12:48.160 --> 12:50.160
we have the Java tornado VM,

12:50.160 --> 12:53.520
where we use Java and the tornado VM API

12:53.520 --> 12:57.040
to create these parallel implementations of K-means.

13:00.880 --> 13:04.400
In this slide, you see, well, the steps,

13:04.400 --> 13:09.200
how to clone the repository that contains this Python program.

13:09.200 --> 13:12.640
And we see also the Python program, the K-means.py.

13:13.360 --> 13:18.880
So we see here beneath that we have the invocation

13:18.880 --> 13:24.480
of the actual method functions, Java methods, sorry.

13:26.160 --> 13:29.760
And here is the link for the Java implementation of K-means.

13:31.360 --> 13:33.520
And now if we jump into the Java part,

13:33.520 --> 13:37.040
which contains the computation that will be offloaded on the GPU.

13:37.040 --> 13:43.280
No, before we jump to the computation,

13:43.280 --> 13:46.560
we have the set inputs and I wanted to make a connection

13:46.560 --> 13:49.040
to reflect on the off-heap data types.

13:49.040 --> 13:52.320
So with these two, with a new vector float,

13:52.320 --> 13:56.800
this is an API type that is exposed by tornado VM

13:56.800 --> 14:00.480
and can allocate data vector types off-heap.

14:01.520 --> 14:03.760
And then we'll have the create matrix of clusters

14:03.760 --> 14:07.520
that does perform some initialization of the objects

14:07.520 --> 14:11.680
and also allocate some other data, like the clusters,

14:11.680 --> 14:15.040
which are going to be allocated off-heap as well.

14:16.320 --> 14:19.680
And now we are ready to move into the actual computation part.

14:19.680 --> 14:24.000
So on the left side, you see the run with Java implementation

14:24.000 --> 14:24.800
of this method.

14:25.440 --> 14:28.240
And on the right side, you see the accelerated one

14:28.240 --> 14:31.040
with the tornado VM API.

14:31.360 --> 14:35.040
So as we see here, the actual computation has been

14:35.040 --> 14:37.920
in this method, has been performed by this method.

14:37.920 --> 14:39.360
So they assign clusters.

14:40.480 --> 14:42.800
And the corresponding one on the right side,

14:42.800 --> 14:48.240
that is the tornado VM implementation, is this one.

14:48.240 --> 14:53.040
So in this one, I would like to focus on two parts.

14:53.040 --> 14:55.840
So you can see the task graph implementation.

14:55.840 --> 14:59.440
Task graph is an object exposed by the tornado VM API.

15:00.400 --> 15:03.680
In a sense, task graph enables you to define

15:03.680 --> 15:06.160
what code will go to the GPU.

15:06.160 --> 15:08.480
So what's going to be the actual computation

15:09.120 --> 15:12.800
and what data should be used on the GPU.

15:12.800 --> 15:15.200
So the input data and the output data.

15:15.200 --> 15:18.160
So in a sense, the task graph enables programmers

15:18.160 --> 15:22.240
to define what is going to go to the GPU for execution.

15:23.440 --> 15:26.000
And the second API, once we have done this,

15:26.000 --> 15:28.960
as you can see here, we can define also the data

15:29.200 --> 15:32.240
transfer mode, how often we want data input,

15:32.240 --> 15:35.760
input data or output data to be copied back and forth

15:35.760 --> 15:37.040
from the GPU.

15:37.040 --> 15:39.920
And once we have defined that, we can move to the second part,

15:39.920 --> 15:41.760
which is the execution plan.

15:41.760 --> 15:44.160
So the execution plan is another object

15:44.160 --> 15:46.160
that enables programmers to define

15:46.880 --> 15:49.120
how the execution will take place.

15:49.120 --> 15:52.080
So it could be, for example, with the profiler enabled,

15:52.080 --> 15:55.520
without the profiler enabled, with a custom grid size,

15:56.560 --> 15:58.560
which is defined by the programmer.

15:58.640 --> 16:03.600
And once we have defined how the execution will be done,

16:03.600 --> 16:09.040
will be performed, we are able to execute the actual task graph.

16:09.040 --> 16:12.160
So with execution.execute, it is this part

16:12.720 --> 16:16.320
that enables the actual execution of the code

16:16.320 --> 16:17.440
and the GIT compilation.

16:18.480 --> 16:24.320
So the second time that we will execute the assigned clusters,

16:24.320 --> 16:26.000
well, this is going to be the second time

16:26.000 --> 16:29.600
that we invoke the actual execute of the execution plan.

16:30.720 --> 16:33.840
And the second time that we will invoke the execution plan,

16:33.840 --> 16:35.840
the execution of the execution plan,

16:35.840 --> 16:38.880
this is going to be the time that the code will not be GIT

16:38.880 --> 16:40.320
because it is already GIT.

16:40.320 --> 16:43.280
So the code, the OpenCL code or the CUDA code

16:43.920 --> 16:48.160
will be all retrieved from the code cache of Tornado VM.

16:50.720 --> 16:54.160
So now we can move to the actual example to run.

16:54.960 --> 16:58.880
I have recorded a video that enables the execution

16:58.880 --> 17:00.960
of K-Means and MathExfoom.liblication

17:00.960 --> 17:03.440
because on my MacBook, I don't have an NVIDIA GPU.

17:04.080 --> 17:08.880
So we will fork the actual repository with examples.

17:11.440 --> 17:14.320
And now that we have forked, we will go inside,

17:15.040 --> 17:16.720
we check out the FOSDEM branch.

17:21.520 --> 17:23.680
And this is the Python code that we saw earlier.

17:24.160 --> 17:25.280
So it has these three.

17:25.920 --> 17:29.040
First, we load the class, and then we are able to invoke

17:29.040 --> 17:30.720
the Java code from Python.

17:32.560 --> 17:36.000
And here we will run, first, the Java implementation

17:36.000 --> 17:39.440
and then the GPU accelerated implementation.

17:40.560 --> 17:44.000
We can also pull the Docker image that we have created.

17:48.480 --> 17:51.440
And here in the repository, we have a launcher script

17:51.440 --> 17:53.040
that enables to run.

17:53.040 --> 17:56.480
So at first, we will try the Tornado devices

17:56.480 --> 18:00.240
to query how many NVIDIA GPUs exist in the system.

18:01.840 --> 18:07.040
And here it is the 2000 GPU that exists in my machine at home.

18:08.880 --> 18:11.440
And once we have done this, we will run with Truffle,

18:11.440 --> 18:12.400
the Python program.

18:12.400 --> 18:16.320
So Tornado Truffle, the Truffle flag and Python,

18:16.320 --> 18:19.120
will be able to run the actual Python program.

18:19.760 --> 18:23.360
And we will see here that at first,

18:24.080 --> 18:26.240
it will bring Hello World from Python.

18:26.240 --> 18:28.800
And then we run the Java implementation,

18:28.800 --> 18:31.680
which is a sequential, that I'm with Java.

18:31.680 --> 18:33.840
And then they run with GPU method.

18:34.400 --> 18:37.600
And as we see here, they take the first one, one second,

18:37.600 --> 18:40.720
and the second one, 140 milliseconds.

18:45.200 --> 18:48.400
So here we will try the same example,

18:48.400 --> 18:52.880
but with the thread info, which will enable the printing

18:52.880 --> 18:56.080
of the actual threads that have been used on the GPU.

18:56.080 --> 18:59.200
So as we see here, we have the number of data points

18:59.200 --> 19:01.200
that we passed with the set input.

19:01.200 --> 19:04.320
It has been the number of the global thread size

19:04.320 --> 19:05.840
that is uploaded on the GPU.

19:06.560 --> 19:08.480
And now we move to the second example,

19:08.480 --> 19:12.000
which is the matrix multiplication with Tornado VM.

19:12.000 --> 19:15.120
So in this example, we run five times

19:15.120 --> 19:16.800
the matrix multiplication.

19:16.800 --> 19:19.040
So we see here the execution time

19:19.040 --> 19:21.760
of matrix multiplication on the GPU.

19:21.760 --> 19:24.560
So the first time it was half second,

19:24.560 --> 19:27.840
and then it has moved to three milliseconds.

19:27.840 --> 19:30.160
This is because the first execution,

19:30.160 --> 19:33.200
it involves also the GIT compilation, which is expensive.

19:33.760 --> 19:35.840
Then the second time, third time,

19:35.840 --> 19:38.400
the execution time has been saturated

19:38.400 --> 19:41.040
because it is the actual launching of the code.

19:42.000 --> 19:45.840
Okay, I have showed you example of Python with Gralpy,

19:45.840 --> 19:47.920
but this is not the only one.

19:47.920 --> 19:51.840
We have also the key images for the other programming languages

19:51.840 --> 19:53.520
for JavaScript, Ruby,

19:53.520 --> 19:56.080
and you can find more details in those links

19:56.080 --> 19:58.320
where we have a blog post.

19:58.320 --> 20:02.080
And we explain also the polyglot programming

20:02.080 --> 20:04.080
from Tornado VM.

20:06.560 --> 20:10.480
So now we will try to find the other examples

20:10.480 --> 20:12.960
so now I will jump to the summary of my talk.

20:14.560 --> 20:15.840
So as key takeaways,

20:15.840 --> 20:18.880
I would like to emphasize that GralVM and Traffl

20:18.880 --> 20:22.080
enable Java interoperability with other programming languages

20:22.080 --> 20:23.680
that run on top of the JVM.

20:24.400 --> 20:29.040
Tornado VM afflows Java methods on GPUs, FPGAs,

20:29.040 --> 20:30.160
and multicore CPUs,

20:30.160 --> 20:33.360
so you can create parallel implementations.

20:34.320 --> 20:37.440
And that Tornado VM offers a Java API,

20:37.440 --> 20:40.320
so programmers, they don't need to know GPU programming.

20:40.480 --> 20:44.000
It is a Java API, a Java way to express parallelism.

20:44.560 --> 20:47.680
And we have also new off-hip data types.

20:49.040 --> 20:51.440
So finally, yes, it is possible to create

20:51.440 --> 20:54.720
high-performing implementations of code

20:54.720 --> 20:56.880
for data science libraries in Java,

20:56.880 --> 20:59.840
and reuse them by other programming languages.

21:03.120 --> 21:06.320
This is a slide that summarizes everyone

21:06.320 --> 21:08.160
who has contributed as a research staff

21:08.160 --> 21:10.880
for students at the University of Manchester,

21:10.880 --> 21:12.880
and these images are from our campus.

21:14.080 --> 21:17.680
And this is a surprise that it was taken and it was not raining.

21:20.160 --> 21:22.560
So I would like to invite you to join our community,

21:22.560 --> 21:24.320
follow us in GitHub,

21:24.320 --> 21:26.960
join us in the Tornado VM Slack space

21:26.960 --> 21:28.080
if you have questions,

21:28.080 --> 21:31.680
or if you want to interact with a team for discussions,

21:32.320 --> 21:34.400
and also to try our examples in GitHub.

21:34.720 --> 21:36.240
And in my last slide,

21:36.240 --> 21:39.360
I would like to acknowledge all these research funds

21:39.360 --> 21:43.120
that have supported their work at Tornado VM,

21:43.120 --> 21:46.560
like Elegant and Crip, Tango, Iro and InCode.

21:47.200 --> 21:49.600
So with that, I conclude my talk,

21:49.600 --> 21:52.800
and I think we have time for one or two questions.

21:52.800 --> 21:54.000
Okay, I've got the mic here,

21:54.000 --> 21:55.840
but first, I lived in Manchester for five years,

21:55.840 --> 21:57.360
and it doesn't always rain.

21:57.360 --> 21:58.560
Just mostly.

21:58.640 --> 21:59.520
Just mostly.

22:08.080 --> 22:09.120
Thanks for a great talk.

22:09.680 --> 22:13.120
Like one of the first pictures you had showed Tornado VM

22:13.840 --> 22:18.400
in parallel to the GrowlJIT using the JVMCI.

22:18.400 --> 22:24.080
So do you interact directly with JVMCI for generating code?

22:24.080 --> 22:25.040
Correct, yes.

22:25.120 --> 22:29.200
So the JVMCI enables other JIT compilers

22:29.200 --> 22:30.640
to be hooked in the JVM,

22:30.640 --> 22:33.600
and that's how we run, because we extend.

22:33.600 --> 22:38.560
So do you work with the standard JVMCI in upstream or open JDK,

22:38.560 --> 22:41.920
or you need the lab JDK with the latest JVMCI changes?

22:41.920 --> 22:43.280
Because the GrowlJIT compiler,

22:43.280 --> 22:46.400
as far as I know, requires the lab JDK with latest changes.

22:47.040 --> 22:49.520
We work with the standard JVMCI, yes.

22:50.080 --> 22:50.480
Thank you.

22:55.280 --> 22:55.920
Thank you.

23:06.240 --> 23:09.440
So when you write the kernel code in Java,

23:09.440 --> 23:13.760
then is it usually high-level code that you write,

23:13.760 --> 23:16.400
or do you try to write optimized code in Java?

23:16.400 --> 23:19.120
Like usually when you write, let's say, Qtacode,

23:19.120 --> 23:22.160
then you try to write a very specialized,

23:22.160 --> 23:24.320
use warp intrinsics and that kind of stuff.

23:24.400 --> 23:26.880
Is that something that is like in scope for turn out of VM,

23:26.880 --> 23:27.840
or not so much?

23:27.840 --> 23:29.440
No, that's a great question.

23:29.440 --> 23:32.080
Well, to answer this question, we do both.

23:32.080 --> 23:35.360
So we have two APIs.

23:35.360 --> 23:38.960
One is created for Java programmers.

23:38.960 --> 23:43.600
We will have, let's say, a computation that has four loops.

23:44.160 --> 23:46.800
So this is something that you can paralyze

23:46.800 --> 23:48.800
if you don't have data dependency.

23:48.800 --> 23:51.920
So we expose an annotation in this case,

23:51.920 --> 23:53.280
similar to OpenMP.

23:53.280 --> 23:56.640
So you can do add parallel in the four loop

23:56.640 --> 23:58.720
in order to give a hint to the compiler

23:58.720 --> 24:00.720
that this can run in parallel

24:00.720 --> 24:04.160
and will create parallel implementations in OpenCL or CUDA.

24:05.200 --> 24:10.000
And the second part is that if you are familiar with OpenCL

24:10.000 --> 24:14.400
and CUDA and you want to have access to low-level intrinsics,

24:14.400 --> 24:18.080
like, for example, use barriers or local memory,

24:18.080 --> 24:19.600
allocate local memory,

24:19.600 --> 24:23.120
then we'll have a second API, which is called kernel API.

24:24.240 --> 24:28.240
And with that, you can pretty much access every interesting

24:28.240 --> 24:31.920
that exists in OpenCL and CUDA programming from Java.

24:31.920 --> 24:34.880
So personally, I have used the second API

24:34.880 --> 24:39.280
to port existing OpenCL kernels in Java with Tonedo.

