WEBVTT

00:00.000 --> 00:16.880
Hi, I'm here to talk about what we have done to, you talk about what we are doing at my

00:16.880 --> 00:26.480
work to, does this work?

00:26.480 --> 00:28.560
I'll just change slides here.

00:28.560 --> 00:32.000
Good.

00:32.000 --> 00:41.560
We are, we are an ISP in Copenhagen and we do fiber to the building type networks where

00:41.560 --> 00:50.040
we do the high concentration of customers, we can give some quite good rates.

00:50.040 --> 00:57.720
Then we currently have a little bit more than a hundred gig going through our network and

00:57.720 --> 01:07.560
if we look a bit at how a network can be simplified, then we have some external connections,

01:07.560 --> 01:16.560
PNIS, ISPs, transit connections and they are all going in to a router that is, DLC that

01:16.560 --> 01:22.800
means default free zone, so it doesn't have a default route, it only knows all the million

01:22.800 --> 01:30.080
different routes to all the networks connected to the internet and then from there traffic

01:30.080 --> 01:38.680
going into the network goes on to a free switch where we have OSPF cloud which knows all of

01:38.680 --> 01:44.240
the internal routing and then we have end users connected to that and we have some internal

01:44.240 --> 01:53.000
streaming servers so that we don't have to get any unicast stream from Amsterdam or

01:53.000 --> 01:54.320
elsewhere.

01:54.320 --> 02:06.360
We used to do this with an old Cisco big chassis router back when we had 10 gig, it was a very

02:06.360 --> 02:14.240
simple time and then we upgraded to something like this when we changed it to 40 gig and

02:14.240 --> 02:21.040
these both operators router and stick but I'll come back to that later and then in 2020 we

02:21.040 --> 02:29.440
changed, we just run it on a 2U server with 4, 10 gig ports changed together so still 40

02:29.440 --> 02:37.680
gig and then in 21 we upgraded to 100 gig with some of these cuts and like the router and stick

02:37.680 --> 02:43.800
I talked about before is basically when you have a single port on your router and then

02:43.800 --> 02:53.240
you just use reland tags to differentiate between different external connections and

02:53.240 --> 02:59.720
you start doing that because router ports are expensive so it's a lot cheaper just to

02:59.720 --> 03:08.480
aggregate it with relands on a switch and it's also an easy way to scale without using

03:08.480 --> 03:12.360
too much money.

03:12.360 --> 03:20.360
So now we get into what TC flow is, it's an API extension to the kernel originally I think

03:20.360 --> 03:33.120
developed by Melanux but it's mainly marketed as just for making open v-switch more efficient

03:33.120 --> 03:37.600
and bypassing the host system.

03:37.600 --> 03:46.360
However, apart from just being used in this sense, it can also be used to do forwarding

03:46.360 --> 03:50.760
or other shenanigans.

03:50.760 --> 03:57.560
Here in the bottom you can see that on the network card there is an embedded switch chip

03:57.560 --> 04:06.840
so that it can have virtual machines represented as a port in the NIC itself and that way the

04:06.840 --> 04:13.080
offload rules can forward traffic directly into a virtual machine.

04:13.080 --> 04:17.920
Still not used for that however but this is what you can find marketing wise from all

04:17.920 --> 04:24.800
the vendors, they generically just call it something with OBS offload.

04:24.800 --> 04:31.680
A TC flow is part of TC which is Linux traffic control.

04:31.680 --> 04:37.320
It operates with chains and priorities and then you can have some different things you

04:37.320 --> 04:43.640
can do through packets, you can change them, you can drop them, you can redirect them or

04:43.640 --> 04:51.640
go to another chain and continue there or you can trap them whereby they go to the CPU.

04:51.640 --> 04:56.160
In the hardware offload part of TC there's a few other modules other than flow that can

04:56.160 --> 05:04.720
also do hardware offload but generically you have the rules of skip software or skip hardware

05:04.720 --> 05:09.680
so it's kind of the other way around where if you want something only installed in hardware

05:09.680 --> 05:16.920
you say skip software and wise versa.

05:16.920 --> 05:23.040
It's kind of wind diagnostic but you really have to read deep down the drivers for the

05:23.040 --> 05:28.920
different devices to see what do they support or just have the hardware tested.

05:29.880 --> 05:36.600
A lot of the hardware can do many of the operations that we need for this project but we have only

05:36.600 --> 05:42.360
been able to test it with Kandidex cards at this point.

05:42.360 --> 05:49.080
So these chains and priorities and rules in it is basically that the packet goes into

05:49.080 --> 05:55.080
chain zero and then it takes the first matching rule with the lowest priority and then it does

05:55.080 --> 06:00.920
whatever is there, it could be the go to and go chain one and then continue and it could

06:00.920 --> 06:09.640
do this number sequence wherever you can recognize it but this limit to 256 go to set max.

06:09.640 --> 06:13.640
But then they can drop the packet, they can trap it or redirect it and redirect can go

06:13.640 --> 06:19.480
out any other port in the better switch chip we saw before.

06:19.880 --> 06:23.880
I'm going a bit quickly because it's a short time flood.

06:23.880 --> 06:33.080
If we know what does it take to actually forward a packet in hardware, we need the

06:33.080 --> 06:39.480
wheeler to be different when it goes back out of the port and we need the magnetizers to

06:39.480 --> 06:42.920
have changed in order to prevent routing loops.

06:42.920 --> 06:48.920
We need to determine the detail or hub limits and then before there's also checks on but

06:49.320 --> 06:53.080
we don't actually need that rule in hardware, the hardware does it automatically.

06:54.760 --> 07:00.120
If we didn't suffer we would need that rule and then push it back out of the port that came in.

07:01.160 --> 07:04.360
Even on dual port nicks we cannot push it out of the other port.

07:05.560 --> 07:14.280
They are at least on the Kandidex cards, they are just two separate nicks on the same PCB.

07:15.240 --> 07:18.600
Not that they cannot talk to each other.

07:20.600 --> 07:24.120
So if we do this then it would look like this.

07:24.120 --> 07:25.640
If you just use a TC command.

07:27.480 --> 07:33.720
So what does here is that it says that it's an Ingress rule and it has a chain and preference

07:34.440 --> 07:41.880
and then it says it's a VLAN packet and it's a skip software and it's IPv4 this one

07:42.440 --> 07:48.200
and then modifies the VLAN as an action and goes on to modify the magnetizers and then

07:48.200 --> 07:54.760
decommends the detail and updates the checksum and then it pushes it back out again with the redirect.

07:58.200 --> 08:01.800
If we then show this rule with the show command afterwards

08:02.680 --> 08:06.760
then we get output like this. This is a bit edited to fit on the slide.

08:07.720 --> 08:11.880
So the action order here is actually a little bit longer.

08:12.680 --> 08:19.000
We can see that it sets these values with these masks when it changes the magnetizers

08:19.720 --> 08:24.040
and then the decommends detail is just a masked overflow.

08:27.560 --> 08:31.640
If you then use the dash S option then you get statistics.

08:31.640 --> 08:37.640
We can see that a few packets and bytes got pushed in hardware and never saw this off a path.

08:39.880 --> 08:46.200
So we used this for a few years. We just did it statically for all inbound traffic because for

08:46.200 --> 08:54.760
inbound traffic as shown in the diagram earlier it always just goes to the layer 3 internal network

08:54.760 --> 09:00.600
so that part is static if that site has power that will always work otherwise it wouldn't even

09:00.600 --> 09:05.960
advertise our address space. That will always work so therefore we could do that statically

09:05.960 --> 09:14.840
for some time but now we also wanted it to work for some high traffic outbound prefixes

09:14.840 --> 09:23.480
but for that we needed to work with BDP but yeah more than a bit later.

09:24.440 --> 09:33.240
So in the static case we basically have this chain and priority design where we have two rules

09:33.240 --> 09:42.120
and change the rule where it just spits it up in IPv4, IPv6 and then in chain one or two

09:43.160 --> 09:51.240
we first say all packets with an expiring detail needs to go visit the CPU so that the CPU can send

09:51.320 --> 10:03.560
back an IP packet saying that this packet exploded and then it matches some link net so that our

10:03.560 --> 10:13.320
BDP sessions don't die because we still need BDP to be able to so the packet for internal traffic

10:13.560 --> 10:20.760
link nets in our own address space we need to have rules specifically for that so that that

10:20.760 --> 10:27.880
actually gets to the CPU and then we just match the inbound destinations and go to the

10:27.880 --> 10:37.160
chain 4 and 6 where we have the big rule changing the backdressers and VLAN tag and so on so we ran

10:37.160 --> 10:47.400
this for a few years until we needed something more dynamic and this is basically how it looks when

10:47.400 --> 10:58.040
you have amount of where you place the rules so I wasn't able to generate enough packets in a small

10:58.040 --> 11:07.640
test setup that I could actually get the nick to hit the limit in the beginning but from 15 packets

11:08.520 --> 11:15.240
and 15 rules and more you can see that it decreases with the more rules I add to the hardware that

11:15.240 --> 11:26.200
needs to check the slower it gets at the moment we have around four million packets going through

11:27.080 --> 11:33.800
each of these routers so we had like this is the worst case where every single packet

11:34.520 --> 11:40.280
needs to go through this many rules so the numbers is a lot better if it's a very traffic pattern

11:40.920 --> 11:50.200
this is the worst case scenario then in order to make this dynamically and get all this these

11:50.200 --> 11:58.680
rules put in dynamically based on bdp changes we have made some software that just has an event

11:58.680 --> 12:09.240
loop with some netlink sockets and talks with the network stack in the kernel gets all the routes

12:09.240 --> 12:18.520
and links and neighbors and and then generates the tc rule set based on that and dynamically updates

12:19.080 --> 12:24.040
and then we have bird feed in all the rules into a separate routing table

12:25.880 --> 12:33.640
and and then it automatically gets notified there because we have a monitoring session

12:34.360 --> 12:42.440
in that link and then updates the rule set dynamically so that we can use the cpu on the

12:42.440 --> 12:51.400
long tail of all the non-offloaded prefixes and in the bird side we just have a kernel protocol

12:52.680 --> 13:00.200
to have an extra kernel table where we then have some type protocols copying from the full

13:00.200 --> 13:08.520
routing table select prefixes like all our own address space but also some select cdn's and other

13:08.520 --> 13:19.240
stuff that we then offload and in the future we need to make the configuration a bit more

13:19.240 --> 13:25.720
flexible so we can also handle directly connected paths so if someone wants to use this for their

13:25.720 --> 13:31.560
home connection if someone has 5.7 or something like that where they get 25 gig connections

13:31.560 --> 13:38.520
then you can we want to use this in our hacker space as well where we have 10 gig connection so

13:39.160 --> 13:47.160
like expand it a bit make it more flexible so it's not only our use case and then we need some

13:47.160 --> 13:55.480
kernel support on mtu and ecmp support but it's easier to do that after we've presented this

13:56.200 --> 14:02.360
and then we need to test if we can also do this with binding the port together so that we can use

14:03.160 --> 14:07.480
both ports in a dual port nick and just install the rules in both of them

14:10.600 --> 14:19.000
we've also tested how much power do we need to do a small embedded setup so with a dual port

14:19.640 --> 14:28.680
connected f5 we can run that on around 15 watt so that's a quite efficient solution but

14:29.720 --> 14:35.000
that's more for the hacker space scenario than for real world isp stuff

14:37.800 --> 14:44.040
yeah that was the end of it i think that was also the time yeah we are on time yeah

14:44.920 --> 14:46.920
yeah

14:48.520 --> 14:52.920
any question

15:01.240 --> 15:10.200
hi there yeah hi there so you've shown us performance graph of the worst case scenario

15:10.280 --> 15:16.200
yeah is it better addressed in software than in hardware in that case or

15:17.880 --> 15:27.480
it's uh that is also right now some performance issues in in the kernel side but that'll probably

15:27.480 --> 15:36.040
get fixed soon but the graph was the worst case in the hardware only because it had an amount of

15:37.000 --> 15:41.720
non matching rules followed by the one matching rule for the test traffic that i generated

15:42.920 --> 15:49.640
and if you would let's say execute the same scenario in software only would that lead to better

15:49.640 --> 15:58.280
performance or no it had like at the hundred rules it matches the software performance

16:01.080 --> 16:05.640
so equal performance right yeah after a hundred rules okay thanks

16:06.040 --> 16:14.200
so it's way better performance than software until you have a hundred rules in hardware

16:15.560 --> 16:15.800
thanks

16:19.720 --> 16:24.520
well actually the kind of got answered by your last question i was just wondering and do you do like

16:25.080 --> 16:29.480
net flow analysis or something to choose the rules that the choose the routes that you're going to

16:29.560 --> 16:35.800
use those 100 yes we we have some flow analysis to do that and select the prefixes

16:36.520 --> 16:40.680
and what is what's your normal level of how many rules you do populate

16:43.000 --> 16:50.280
at the moment we we only have around 30 or something like that but we need to do it a bit more

16:50.280 --> 17:00.520
dynamically and do it a little bit more clever so that so that we put related routes that are

17:00.520 --> 17:08.360
overlapping out in a separate chain so that it's it can check on the on the largest prefix

17:09.080 --> 17:17.400
and then only if it matches largest prefix go into the subchain so we need a little bit more

17:17.400 --> 17:22.680
of those kind of optimizations but currently all the ground functionality is there for the basic

17:24.120 --> 17:31.480
use case and all of the reference counting internally and chaining the primitives from the

17:31.480 --> 17:41.240
kernel together to have only have the same destination mapped in as one target so that

17:41.240 --> 17:47.240
mobile routes can go to the same next hub by having the same chain that they all jump to

17:49.400 --> 17:54.200
since you found some traffic to the cpu how do you protect your network from pathological

17:54.200 --> 18:02.440
traffic creating a denial of service we don't have enough of it to really do a lot right now

18:02.680 --> 18:11.400
but it does happen occasionally but we have so much capacity that it's rare that it's anything

18:12.680 --> 18:19.080
significant enough to want a lot of extra time spent on it we have first initial customers

18:20.040 --> 18:24.760
where it's mostly meta gaming and stuff like that that leads to details we don't have

18:25.400 --> 18:28.200
hosting and stuff like that where web shops are more targeted

18:28.840 --> 18:35.880
yeah excuse me i did not get how many rules you can offload to the hardware

18:36.920 --> 18:44.120
you can offload many rules to it but the chain of rules you can check right now

18:45.640 --> 18:51.320
drops in performance when you are above 15 rules and then it gets worse from there

18:51.320 --> 18:56.840
but that was in the worst case scenario where all the packets will go through that many rules

18:56.840 --> 19:02.840
where half of our traffic is inbound traffic and therefore all the inbound traffic

19:03.800 --> 19:06.600
to our own address space we can check that in the first few rules

19:07.480 --> 19:14.520
it looks a very low number of rules yes it would be nice if numbers were higher

19:15.160 --> 19:21.320
on the amount of rules but it just means you need to be more clever in designing the how the chains

19:21.320 --> 19:27.560
are constructed okay and were you able to experiment with other hardware from those

19:29.080 --> 19:34.280
we would like to do that as well we would like to test with other hardware as well

19:34.280 --> 19:43.400
but at the moment we we have some connex 4 as well and connex 4 might actually be but i only

19:43.400 --> 19:51.800
discovered that after i left for here but connex 4 is mentioned in some documents that it might

19:51.800 --> 19:57.640
support it but it also depends on the firmware versions because a lot of the supports for instance

19:57.640 --> 20:03.160
for TTL document and stuff like that depends on the new firmware release okay but they have

20:03.160 --> 20:12.360
some documents saying it's connex 5 onward it's a feature called asap2 that is this feature

20:14.360 --> 20:22.040
yeah so if you look in data sheets for asap2 from nvidia and melanox then it's it has this feature

20:23.000 --> 20:28.680
but they have some of the initial documents on it say that connex 4 should work as well

20:30.760 --> 20:31.720
yeah sure thank you

20:35.320 --> 20:36.120
okay thank you

20:43.400 --> 20:44.300
you

