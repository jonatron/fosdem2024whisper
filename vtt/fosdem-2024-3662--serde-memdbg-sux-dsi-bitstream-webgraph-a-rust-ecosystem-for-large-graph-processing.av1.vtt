WEBVTT

00:00.000 --> 00:16.000
Hi everybody, we're just about to have our next talk, who will be Sebastiano Vigna, who

00:16.000 --> 00:19.960
will be talking about a Rust ecosystem for large graph processing.

00:19.960 --> 00:20.960
Sebastiano?

00:20.960 --> 00:21.960
Thank you.

00:21.960 --> 00:22.960
Okay.

00:22.960 --> 00:23.960
Okay.

00:23.960 --> 00:28.120
How many Rust programmers here?

00:28.120 --> 00:29.840
Well, some.

00:29.840 --> 00:34.240
How many Rust programmers who handle large data structures, like those of gigabytes?

00:34.240 --> 00:35.240
A few.

00:35.240 --> 00:36.240
Okay.

00:36.240 --> 00:38.800
The first group is reasonably interested.

00:38.800 --> 00:40.680
The second group is more interested.

00:40.680 --> 00:41.960
The rest of the people can't sleep.

00:41.960 --> 00:42.960
I'm not offended.

00:42.960 --> 00:43.960
You can use the computer.

00:43.960 --> 00:46.360
It will be very, very boring.

00:46.360 --> 00:47.840
So okay, let me introduce why.

00:47.840 --> 00:48.840
Okay.

00:48.840 --> 00:52.760
What I'm doing is just announcing a few crates we are distributing that do very specific

00:52.760 --> 00:56.760
thing related to large-scale data analytics.

00:56.760 --> 01:01.320
And the original of this is a framework for graph compression that has been around for

01:01.320 --> 01:03.400
around 20 years.

01:03.400 --> 01:08.200
And that's being used by the community around the WWW, the web conf, the largest conference

01:08.200 --> 01:13.880
on the web in general, academic conference.

01:13.880 --> 01:17.720
For the last 20 years, there are many data sets that are distributed in this format that

01:17.720 --> 01:19.200
are utilised and so on.

01:19.200 --> 01:21.200
There are a lot of journals.

01:21.200 --> 01:26.080
And in 2011, it was used to measure the degrees of separation on Facebook, if you remember

01:26.080 --> 01:27.320
it, maybe you're too young.

01:27.320 --> 01:31.520
But it was quite a feat at that time because, I mean, it was for 15 years ago and Facebook

01:31.520 --> 01:33.320
was still rather large.

01:33.320 --> 01:38.800
But we were able at that time to represent the entire Facebook graph in just 211 gigabytes,

01:38.800 --> 01:44.880
which made it possible to run some pretty nice algorithm to compute this and distribution.

01:44.880 --> 01:48.560
Maybe in this community, I should mention that in the late, I started to do free software

01:48.560 --> 01:50.560
in the late 80s on the Amiga.

01:50.560 --> 01:51.560
Okay.

01:51.560 --> 01:55.520
So nobody remembers what it is, but I have some history with the free software movement

01:55.520 --> 01:56.680
as well.

01:56.680 --> 02:02.160
So at some point, we decided to move to Rust for the obvious reasons, like it's a high-performance,

02:02.160 --> 02:03.160
safe language.

02:03.160 --> 02:05.480
But, okay, all I said is in Java.

02:05.480 --> 02:09.160
It was written in Java, started in the 80s and of the 90s.

02:09.160 --> 02:12.000
And at that time, it seemed a very good idea.

02:12.000 --> 02:13.000
Okay.

02:13.000 --> 02:17.120
Then things happened like arrays are at most two billion elements.

02:17.120 --> 02:21.840
And if you have graphs with 50 billion elements, you cannot even index the notes, which gets

02:21.840 --> 02:24.720
very, very annoying.

02:24.720 --> 02:28.280
And today, anything this size is done using memory mapping.

02:28.280 --> 02:33.320
I mean, if you go to Facebook, Google, whatever, all the large structures are there in memory,

02:33.320 --> 02:37.360
but usually they're just memory mapped because you don't want to start up time.

02:37.360 --> 02:42.840
If you load in memory a graph that is half a terabyte, you wait minutes, whatever the

02:42.840 --> 02:43.840
platform you are on.

02:43.840 --> 02:48.000
But if you can memory map it, this time is amortized along the visit of the graph, for

02:48.000 --> 02:49.000
instance.

02:49.000 --> 02:50.000
Okay.

02:50.000 --> 02:52.880
And we actually need to represent very large graph.

02:52.880 --> 02:59.080
If you ever use Java, the access to memory mapping facility, I will not say words because

02:59.080 --> 03:04.600
they would not be proper in this particular situation.

03:04.600 --> 03:06.520
There are really lazy iterators.

03:06.520 --> 03:09.560
If you're written in Java and iterator, you know what I mean.

03:09.560 --> 03:14.760
And okay, so we, to do this, we needed to port a number of ideas from a Java library

03:14.760 --> 03:16.280
and to develop a few new things.

03:16.280 --> 03:20.280
So the first thing is absurdity, weird name.

03:20.280 --> 03:24.280
So it's a framework from epsilon copy, serialization, deserialization.

03:24.280 --> 03:29.600
So you might know what is zero copy, serialization, deserialization, means that you serialize

03:29.600 --> 03:34.920
something and then you use the memory, actually in the state it is, to represent the object

03:34.920 --> 03:35.920
internally.

03:35.920 --> 03:36.920
Okay.

03:36.920 --> 03:38.080
So there is no deserialization.

03:38.080 --> 03:39.760
You don't build a new object.

03:39.760 --> 03:43.080
The piece of memory is directly used as it is.

03:43.080 --> 03:48.560
And this is how things work, as I said, in all these organizations that have large indices,

03:49.000 --> 03:50.440
Facebook, Amazon, whatever you want.

03:50.440 --> 03:54.120
I mean, the index is on disk, it's memory mapped as it is.

03:54.120 --> 03:57.120
It's not deserialized in any proper sense.

03:57.120 --> 04:05.120
There are a few frameworks like abomination that do this kind of things in Rust, but they

04:05.120 --> 04:06.880
all have problems for us.

04:06.880 --> 04:12.400
The first one is the oldest one by Frank McSherry, writes into the serialized object.

04:12.400 --> 04:16.800
So if you want to memory map a file, that's out of question.

04:17.160 --> 04:20.400
You might know it is from the people that do the internalization library.

04:20.400 --> 04:23.880
Nice idea, but it has a huge impact on performance.

04:23.880 --> 04:27.920
It does some kind of runtime resolution of the access to vectors.

04:27.920 --> 04:32.520
And then there is Archive, you might be familiar with, which too does some relative memory

04:32.520 --> 04:33.760
that is differentiation.

04:33.760 --> 04:38.120
And also the structure you deserialize is completely different from the one you serialize.

04:38.120 --> 04:42.200
So you have to delegate all the methods and then each time you change one, you have to

04:42.200 --> 04:43.600
change the other.

04:43.600 --> 04:44.960
Not very practical.

04:44.960 --> 04:49.600
So what we did was develop this framework, which requires a little bit of collaboration

04:49.600 --> 04:51.080
from the underlying struct.

04:51.080 --> 04:56.280
But the basic idea is that you serialize something and then you epsilon copy deserialize it.

04:56.280 --> 05:01.800
So you access it, you allocate a very small amount of memory and then the rest comes directly

05:01.800 --> 05:04.160
from the disk without any intervention.

05:04.160 --> 05:08.600
And the way we do it, we remap vectors essentially.

05:08.600 --> 05:13.680
You build a structure with a vector, but when you deserialize it, it has a reference to a slice.

05:13.720 --> 05:19.400
In this way, we just have to allocate the actual struct that you want to deserialize,

05:19.400 --> 05:24.360
but then anything that is a pointer inside just point to the original memory.

05:24.360 --> 05:28.360
So epsilon copy, the idea is that it's not a zero copy because we did a little bit of

05:28.360 --> 05:31.720
copying, epsilon copy, a very small amount.

05:31.720 --> 05:35.080
But the advantage is that now you have exactly the structure that you serialize.

05:35.080 --> 05:37.680
It's exactly that structure with all its methods.

05:37.680 --> 05:42.240
The only thing you have to do, if you have vectors, there must be a type parameter and

05:42.240 --> 05:46.760
you must write the access method for as a left to a slice.

05:46.760 --> 05:50.320
Of course, when writing, you write for a vector, but when you read, you read it from

05:50.320 --> 05:51.320
a slice.

05:51.320 --> 05:52.720
This is the collaboration you need.

05:52.720 --> 05:58.080
But then, completed transparently, like you can do it with basic type.

05:58.080 --> 06:01.240
You store a vector and then you memory map it and that's it.

06:01.240 --> 06:05.280
And what you get in T is a reference to a slice.

06:05.280 --> 06:09.960
More precisely, something that derives to a slice, to a reference to a slice.

06:09.960 --> 06:15.760
And again, you work essentially transparently with respect to the framework.

06:15.760 --> 06:21.040
Unlike the other cases, and since there is nothing intervening, resolving the pointers,

06:21.040 --> 06:25.160
there is no dynamic resolution, everything is done at this realization time, zero impact

06:25.160 --> 06:26.160
on performance.

06:26.160 --> 06:28.800
The performance is exactly the one of the original structure.

06:28.800 --> 06:34.120
We use this to map massive immutable data structure like representation of sequences

06:34.120 --> 06:39.920
of sets and so on that are like those of gigabytes, 100 gigabytes on disk directly on memory,

06:40.880 --> 06:42.040
without any load time.

06:42.040 --> 06:47.840
So if you handle large immutable data structures, that could be for you.

06:47.840 --> 06:51.840
Memdology, that's a very small crate, but it's a problem we had.

06:51.840 --> 06:57.480
Okay, it's a high performance memory occupancy detector, which sounds ridiculous when you

06:57.480 --> 07:02.920
say it because, well, it does as to measure the memory occupied.

07:02.920 --> 07:09.840
It's not so easy because if you use the one that are around, so it is like a large vector

07:09.960 --> 07:12.760
and few other things, this is the amount of a located memory.

07:12.760 --> 07:17.840
These are the three more common frameworks, sorry, crates that do that, and this is the

07:17.840 --> 07:22.360
amount of time that they take, and this is the amount of time we take.

07:22.360 --> 07:27.880
So the reason is that without some infrastructure similar to the one of absurdity, you have

07:27.880 --> 07:31.840
to iterate through collections to measure the space occupied.

07:31.840 --> 07:36.080
And if you iterate through a billion element collection, it will take a lot of time.

07:36.080 --> 07:41.280
So we routinely measure the space and occupancy of things that are like 50 gigabytes, it will

07:41.280 --> 07:42.520
take eight minutes.

07:42.520 --> 07:46.920
So we develop this if you need to measure the actual occupation memory, not stack occupation,

07:46.920 --> 07:51.480
the actual occupation in memory of something large, try MDBG.

07:51.480 --> 07:56.960
Also, as a nice, it does you a print out of the structure with the old memory occupancy.

07:56.960 --> 08:01.680
It's important for us because we do all the time this succinct data structure that have

08:01.680 --> 08:04.720
various components and we need to know the relative size.

08:04.720 --> 08:07.720
So this is only if you have very large data structures.

08:07.720 --> 08:11.560
They are small, you can iterate, no problem.

08:11.560 --> 08:15.480
Succ is an ongoing problem, ongoing problem, yeah, it's an ongoing problem.

08:15.480 --> 08:19.760
I won't say an ingrate, but it's actually kind of an ongoing problem.

08:19.760 --> 08:26.360
And it's a part of an existing C++ project and Java project about succinct data structures.

08:26.360 --> 08:27.520
You might know what they are.

08:27.520 --> 08:32.440
If you don't, no problem, you don't need this crate, but they're very fashionable now.

08:32.440 --> 08:37.760
There is one crate at least that does this, but we wanted to have something more sophisticated.

08:37.760 --> 08:42.680
So if you're interested in Elias Fano representation of monotone sequences, ranking, selection,

08:42.680 --> 08:44.320
and so on, please have a look.

08:44.320 --> 08:51.360
This is really getting to existence, but we like to have feedback.

08:51.360 --> 08:57.920
Fungal piece bit streams, very, very high performance bit stream with read and write

08:57.920 --> 09:06.000
by word and support for big and little Indian files and a lot of instantaneous code, gamma,

09:06.000 --> 09:07.400
gamma, delta, go long, and so on.

09:07.400 --> 09:12.560
This is kind of cosy you'd like in MPEG or so on, but we use it to do graph compression

09:12.560 --> 09:19.760
and we spend a lot of time to optimize every single shift and move and also to give you

09:19.760 --> 09:27.200
scripts to just run and we massively test all parameters you can configure on your architecture

09:27.200 --> 09:32.520
so you can choose how to optimize the speed of the coding and the coding specifically

09:32.520 --> 09:35.720
on your architecture.

09:35.720 --> 09:41.960
Like which word size to use to pick up stuff from memory, using the coding tables or not,

09:41.960 --> 09:42.960
and so on.

09:42.960 --> 09:48.920
And this comes from quite a long experience in doing this with web graph.

09:48.920 --> 09:52.480
So if you're interested in writing this instantaneous code for compression, you should have a look

09:52.480 --> 09:57.920
at this IBS stream just to tell you a gamma code is ready in less than two thousand seconds.

09:57.920 --> 10:01.000
So I think this is pretty nice.

10:01.000 --> 10:07.560
Okay, the last piece which is probably the more specific, so you might less be interested

10:07.560 --> 10:09.280
in is web graph.

10:09.280 --> 10:14.880
So web graph is a framework to represent very large graphs in a compressed form.

10:14.880 --> 10:20.000
So typically snapshot of the web are represented in about one to two bits per link.

10:20.000 --> 10:24.400
The software heritage graph which is a graph with about half a trillion edges, it's three

10:24.400 --> 10:29.000
bits per link, Wikipedia costs 10 bits per link, it depends on the structure of the graph.

10:29.000 --> 10:34.680
But usually in particular the graph is redundant, you can represent data in 10, 20, even 50

10:34.680 --> 10:41.600
times less than you do with a redundant version.

10:41.600 --> 10:46.440
It's a rough sport of the Java version and of course we use the SIB stream for instantaneous

10:46.440 --> 10:50.760
code and sucks for pointers in the big stream.

10:50.760 --> 10:55.920
And just to give you a very simple example, the software heritage graph is 34 billion

10:55.920 --> 11:01.680
nodes and a little bit more than half a trillion arcs and you can do a BFV visit single thread

11:01.680 --> 11:04.160
in three hours.

11:04.160 --> 11:05.320
It's very nice.

11:05.320 --> 11:11.200
Okay, you have to notice half a trillion edges.

11:11.200 --> 11:14.760
The ergonomics of the whole thing is incredibly better than Java.

11:14.760 --> 11:19.960
Just having real iterators changes completely the game because it's much more natural that

11:19.960 --> 11:21.480
what we had.

11:21.480 --> 11:27.280
And this is all the others are crates that you can download and use that are pretty stable.

11:27.280 --> 11:31.480
This is still on GitHub because it's a lot of code, a lot of optimization.

11:31.480 --> 11:37.720
We just merged into main the last big chunk of modification, the API should be stable

11:37.720 --> 11:38.920
by now.

11:38.920 --> 11:40.040
But this is very specialized.

11:40.040 --> 11:45.040
I mean unless you have graphs with hundreds of billions, half a trillion arcs, for instance,

11:45.040 --> 11:51.760
this biologist did this huge data set with a trillion protein-protein similarity edges

11:51.760 --> 11:55.320
and they did it with web graph because if you need a trillion edge and you need to distribute

11:55.320 --> 12:01.120
it and analyze it on a standard hardware, not a massive supercomputer, you do it using

12:01.120 --> 12:03.320
compression.

12:03.320 --> 12:07.440
There is also support for labels on the edges that you can enumerate and it's much better

12:07.440 --> 12:12.880
in the new version than in the old one.

12:12.880 --> 12:17.760
And one thing that we had to fight a lot against is lenders.

12:17.760 --> 12:22.160
So if you're familiar, I don't feel familiar with a lender idea.

12:22.160 --> 12:26.920
It's generally an idea and a number of crates for Rust.

12:26.920 --> 12:34.200
Lenders are iterators whose return object depends on the iterator itself.

12:34.200 --> 12:39.240
So iterators in Rust are thoughts that give you values and you can take the values and

12:39.240 --> 12:40.240
use them.

12:40.240 --> 12:45.360
But in all this kind of batch processing for graphs, you iterate on the graph and you cannot

12:45.360 --> 12:47.320
look at two nodes at the same time.

12:47.320 --> 12:51.720
There is a sequential iteration which goes through a file or a sorting of labels.

12:51.720 --> 12:57.000
So you need to be able to say, okay, this is the next batch of successor, use it, but I

12:57.000 --> 13:00.960
won't give you the next one until you finish with this one.

13:00.960 --> 13:05.400
To do this, you need to use essentially generic associated type.

13:05.400 --> 13:06.400
Not really that.

13:06.400 --> 13:08.800
We use higher order trade bounds.

13:08.800 --> 13:16.160
But you need to impose that each call to next can be made only when the previous one went

13:16.160 --> 13:17.160
out of scope.

13:17.160 --> 13:20.320
So you cannot do two calls to next in a row.

13:20.320 --> 13:22.800
And this is called a lender.

13:22.800 --> 13:28.840
There are a few crates that implement lenders now which have, say, almost feature parity

13:28.840 --> 13:36.200
with iterator, but the fact is that presently they work because of bug in the borrower

13:36.200 --> 13:37.200
checker.

13:37.200 --> 13:42.880
So the borrower checker doesn't check certain things that if fixed would make all these

13:42.880 --> 13:45.000
lender crates not work.

13:45.000 --> 13:50.360
And at that point, we would be in really deep shit because we have no idea how to do this

13:50.360 --> 13:52.440
other than the way we're doing it.

13:52.440 --> 13:59.920
In fact, we're even in a situation where we have a chain of an iterator returning iterators

13:59.920 --> 14:02.800
and the final value depend on state on the initials thing.

14:02.800 --> 14:08.560
So there is a propagation of bounds of on lifetime that goes through two different types.

14:08.560 --> 14:11.440
And that gives me headache each time I look at it.

14:11.440 --> 14:12.720
And in fact, I didn't even invent it.

14:12.720 --> 14:16.480
I asked on Rust forum and they said, I have this completely crazy situation.

14:16.480 --> 14:17.480
What can I do?

14:17.480 --> 14:23.520
And a very nice guy wrote a type like this with 25 different implied type bounds and now

14:23.520 --> 14:24.520
it works.

14:24.520 --> 14:26.720
But let's hope it continues to work.

14:26.720 --> 14:32.280
But this is just to say we need a little bit more borrowing in Rust than there is now to

14:32.280 --> 14:36.920
make this work properly because it has been a little bit of a pain to get something like

14:36.920 --> 14:43.960
an iterator in which the return value depend on the iterating object.

14:43.960 --> 14:50.680
In the last thing, if anybody know how to get one thing done, index get.

14:50.680 --> 14:57.880
Since 2015, it's been sitting in the issues of Rust to have an index trait that gives

14:57.880 --> 15:00.280
you a value, not a reference.

15:00.280 --> 15:01.560
Because index give you a reference.

15:01.560 --> 15:04.760
Now, index give you a reference is fine.

15:04.760 --> 15:10.160
But if you do compress, succinct, any kind of implicit data structure, index giving you

15:10.160 --> 15:12.440
a reference is a pain in the ass.

15:12.440 --> 15:13.800
Because you don't have the data.

15:13.800 --> 15:14.800
They are implicitly represented.

15:14.800 --> 15:21.400
You need the trait that giving two nice square brackets will give a value, not a reference.

15:21.400 --> 15:25.680
And then you can enter the world of modern implicit data structure.

15:25.680 --> 15:30.280
So if you know anybody who can implement this, convince someone in compiler team to get done

15:30.280 --> 15:31.720
with this, you please do it.

15:31.720 --> 15:32.720
I'm over.

15:32.720 --> 15:33.720
Thank you.

15:33.720 --> 15:45.720
Thank you.

