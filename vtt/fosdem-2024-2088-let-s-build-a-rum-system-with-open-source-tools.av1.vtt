WEBVTT

00:00.000 --> 00:13.760
Hello, everyone. Today I'm going to be talking about building a real user monitoring system

00:13.760 --> 00:20.640
with open source tools. And before I dive in, a bit more info about me. My name is Tvetan

00:20.640 --> 00:31.040
Stojchev. I work on MPALS in Akamai. MPALS is a real user monitoring system. It's a commercial one

00:31.040 --> 00:39.760
and it serves, I think, thousands of customers, Akamai customers. And my hobby project is basic

00:39.760 --> 00:46.560
run, which will be the focal point of this presentation. And really before I dive in,

00:46.560 --> 00:51.920
I would like to share a bit more about some of my other personal activities. Every December,

00:52.800 --> 00:58.000
I make an attempt to publish at least one blog post on the web performance calendar.

00:58.000 --> 01:04.080
That's the best place for the web performance to see us in the year. And the other thing is,

01:04.960 --> 01:11.040
sometimes I do a street art. So that's my safety net plan. If chat GPT takes over the world,

01:11.040 --> 01:18.160
I still will have something creative to do. Yeah. So let's now move on to the important part of

01:18.160 --> 01:25.840
the presentation. And let's take a look how, in general, how a real user monitoring system

01:25.840 --> 01:33.360
would look like. So we will need to have something in the browser, ideally a JavaScript agent,

01:33.360 --> 01:40.240
that will read some data and it will send it to a server. We will store it somewhere in a storage

01:40.240 --> 01:46.080
and later we will analyze this data. And here we just see the most trivial piece of JavaScript.

01:46.080 --> 01:53.040
This is the bare minimum that will do the job in the browser. So this piece of JavaScript will

01:54.400 --> 02:02.720
read what is the current page URL. And it will create a one by one image,

02:03.680 --> 02:10.880
one by one pixel image. It will append it to the HTML and this actually will create,

02:11.600 --> 02:19.760
will force the browser to create a request to this endpoint. And here is a really very simple

02:21.040 --> 02:27.920
code snippet on the server side, how the code will look like when we need to intercept this

02:27.920 --> 02:35.520
data and to store it somewhere. So here is our route where the browser will hit this route.

02:35.520 --> 02:39.760
We will read the query parameters, headers, headers, and even we will put a timestamp in

02:39.760 --> 02:45.280
the structure and then we'll save it to JSON on the file system and we will return back to the

02:45.280 --> 02:54.960
browser a transparent GIF. And eventually we will, on the next stage, when we want to analyze the data,

02:55.600 --> 03:02.560
we will go through all the files and we can create a summary for the page visits. And for

03:02.560 --> 03:08.800
example here in this example we can see that category four was the most visited page with 427

03:10.480 --> 03:20.080
page visits. So that's the theory. And in 2019 I started as a hobby basicram and that's the

03:20.080 --> 03:29.120
initial version and the components that I used to build basicram. So on the browser side I started

03:29.120 --> 03:35.360
using an open source library called boomerang.js which collects a bunch of interesting metrics

03:35.360 --> 03:44.160
from the browser and sends them to a server. On the server side I used nginx and some PHP

03:44.160 --> 03:54.960
application code. And for storage I used mysql and for analyzing the data I still used php and

03:54.960 --> 04:02.320
for reading the data and serving it to a frontend and on the frontend I used plot.ly.js for visualizations.

04:04.080 --> 04:10.320
And I ended up with something like this. It actually, it's really interesting after five years it's

04:10.320 --> 04:15.360
still running. So if you want to give it a try this is the first version of basicram. You can

04:15.360 --> 04:27.120
visit demo.basicram.com and you can play with this UI. Now about boomerang.js. Boomerang.js was

04:27.120 --> 04:33.600
started 2011 in Yahoo by Philip Tellis who happened actually to be now a colleague of mine.

04:34.480 --> 04:41.360
And currently the library is maintained by MPAL's engineering team in Akamai.

04:42.160 --> 04:46.240
And as I mentioned the library collects a bunch of interesting metrics like the

04:46.240 --> 04:54.400
interesting ones for core web vitals, lcp, cls, fid. It also can track some session data. It can also

04:54.400 --> 05:03.040
help users of the library to create a timeline of all the clicks over the page like cycle of a visitor.

05:03.920 --> 05:10.240
And also it has more modern ways to send the data to the server like more modern

05:10.240 --> 05:19.360
JavaScript APIs fetch, XHR and send beacon. And it can be found on GitHub in akamai slash boomerang.

05:21.360 --> 05:27.600
On the back end side that's again like very theoretical but what actually was happening I

05:27.600 --> 05:32.960
still was every request that I was getting to my server I was saving it in a file.

05:34.000 --> 05:40.080
And then periodically I was running a cron job which here I just marked as a that's kind of a

05:40.080 --> 05:45.600
too much overhead and you understand why later. But I was running a cron job that was reading all

05:45.600 --> 05:52.480
these collected files and I was creating one big batch and I was inserting this data in my SQL.

05:52.880 --> 06:04.160
I also ended up with a database model that's very biased. My previous background was I was building

06:04.160 --> 06:10.880
Magento online shops and if somebody ever worked with Magento we'll probably recognize some patterns

06:10.880 --> 06:16.880
about all these foreign key relationships and this main table that's in the center of everything.

06:17.840 --> 06:26.800
I had to put bunch of indexes here and again this created a bit too much overhead for I would say

06:26.800 --> 06:31.600
also on the code level like on the application level but also for me as a maintainer. So I had to

06:31.600 --> 06:38.960
take care about again every time when I wanted to introduce some new dimension I had to create a new

06:38.960 --> 06:44.720
table and to put a bit more code for inserting the data and it was just too much maintenance for me.

06:45.360 --> 06:52.720
Also I had to take care about not duplicating some of the data here and this is because of the nature

06:52.720 --> 07:01.280
of PHP. PHP is kind of a stateless so every request is independent from the other request

07:01.280 --> 07:05.680
so I couldn't keep some things in memory. If I could keep some references in memory I probably

07:06.000 --> 07:15.120
could optimize some things here. And actually question to the audience do you have an idea what

07:15.120 --> 07:23.280
this query actually would produce? What's the idea behind this query? Maybe.

07:23.440 --> 07:24.640
I can say that.

07:30.800 --> 07:36.640
Bucketing? Yeah it's a bucketing for a histogram and I also had to write

07:38.240 --> 07:47.440
a lot of kind of queries that are in the data scientists type of queries which also was for

07:47.440 --> 07:56.080
me introduced a bit of a learning curve but the system had really had coded in itself such type of

07:56.080 --> 08:02.320
queries and this here represents a histogram of the time to first byte. Like we can see that the

08:02.320 --> 08:12.800
median is around 1.8 seconds. It's a bit skewed. And with the help of plotly the JavaScript library

08:12.800 --> 08:23.200
for visualization I could create such panels for distributions for operating systems and

08:23.200 --> 08:28.720
mobile operating systems and I also could write such bar charts that were showing kind of the

08:28.720 --> 08:36.160
relationship between the first byte and start render time. And yeah reference to the plotly

08:36.160 --> 08:42.400
it's a really cool library really rich and you can create a bunch of panels with it.

08:43.440 --> 08:53.920
But I found myself like having difficulties and probably not focusing at the right place. So

08:55.040 --> 09:01.920
as I say when you build a real user monitoring system you need to change your mindset and your

09:02.000 --> 09:09.360
queries should be more like in data scientist style. And the PHP were out and the ORM that I was

09:09.360 --> 09:16.560
using I was using doctrine. It's not really meant for writing complex queries from this fashion. So

09:16.560 --> 09:24.000
I found myself writing my own query builder and using doctrine when convenient and using my

09:24.000 --> 09:29.120
query builder when convenient but this was again too much maintenance for just for a single

09:29.200 --> 09:36.480
maintainer of a project. I also wanted to introduce user management and permission system

09:37.200 --> 09:41.760
but again with my limited time and working from time to time on the project during the weekends

09:41.760 --> 09:46.560
this was just again too much it was not the right focus. I wanted just to show some meaningful data.

09:47.680 --> 09:55.840
And yeah I really love plotly but I just ended up with large blobs of JavaScript here and there

09:55.840 --> 10:00.880
and it was more like more and more plotlier. I wanted to see data not writing JavaScript.

10:03.440 --> 10:14.480
So I took a break I believe half a year and I focused on my main job but from time to time I was

10:14.480 --> 10:21.840
doing research and I was reading some other articles about time series databases and I started

10:21.840 --> 10:29.120
exploring some of the open source available open source systems for visualization. So I

10:30.400 --> 10:39.040
kind of rebuilt the complete backend. I still kept boomerang but I rewritten the server site so I

10:39.040 --> 10:47.040
completely removed nginx and PHP and I used golang. I replaced my SQL with click house

10:47.680 --> 10:52.240
and I replaced all the custom code all the PHP and plotlier with grafana.

10:53.920 --> 10:59.360
And again if you would like to play with the current version of basicram that's what I ended up with

10:59.360 --> 11:07.280
that's actually a let's say a bit of rebranded version of grafana with the specific basicram

11:07.280 --> 11:13.920
dashboards and settings. So if you would like to play with it just visit this address and write

11:13.920 --> 11:24.880
calendar calendar as a username and password. So where golang was really useful, golang it's

11:24.880 --> 11:31.520
just different paradigm it's a different idea compared to PHP. Golang you can compile a single

11:31.520 --> 11:37.280
binary that and in this single binary everything that I needed was packaged inside the binary so

11:37.280 --> 11:42.960
it's just a process that you run on the server and it has everything inside and this allow me to

11:43.600 --> 11:49.280
replace the actually to get rid of nginx because golang has a package for built in

11:49.280 --> 11:58.160
htp server and yes PHP also has a package for PHP for htp server but you need to do a lot of

11:58.160 --> 12:02.880
work arounds to make it working because just not native in this is not native in PHP.

12:04.720 --> 12:12.880
I also could leverage the existing click house package in golang for interacting with the

12:13.840 --> 12:18.960
click house database and I took advantage of asynchronous insert which saved me a lot of

12:20.880 --> 12:26.880
I could get rid of some code that I had in the PHP version of the basicram.

12:28.800 --> 12:33.680
Also in golang it was very easy to create a backup mechanism for all the data that was

12:33.680 --> 12:38.880
flowing through the system because in golang I could easily keep stuff in memory I didn't have

12:38.880 --> 12:44.400
to write each request to the system on a file and later to batch it and bundle it. I was just

12:44.400 --> 12:52.160
keeping these data points and requests in for example in memory for 10 minutes and I could

12:52.160 --> 12:56.800
just flush them on the hard drive and compress them and this was again really really easy few

12:56.800 --> 13:04.160
lines of code and just natively coming in golang and also for some cases where I needed encryption

13:04.240 --> 13:10.640
again in golang there is a let's encrypt package it's a third-party package but I could easily just

13:10.640 --> 13:15.120
spin a server and say okay I want to use let's encrypt and I was getting secure connection to

13:15.120 --> 13:20.400
this server with it it really reduced the operation the effort on the operation site.

13:22.560 --> 13:31.040
I also took advantage of a gip lookup library which is using the maxmind database and why

13:31.040 --> 13:36.000
I needed this because in a real user monitoring system you would like to see from which city a

13:36.000 --> 13:41.360
visitor visited the website or from which country visited the website this is really helpful when

13:41.360 --> 13:47.120
you want to create a report and when you want to figure out maybe in which country is your website

13:47.120 --> 13:55.600
is slow. I also took advantage of another library about user agent parsing so this library helped

13:55.600 --> 14:02.720
me to extract important information about the browser name the operating system and the user

14:02.720 --> 14:15.040
agent family and I also started using my new favorite database Clickhouse. So you remember

14:15.040 --> 14:20.640
where I say that I was doing a lot of work when I was like batching and bundling everything and

14:20.720 --> 14:28.000
inserting these big batches in MySQL. Clickhouse comes with a really cool feature called asynchronous

14:28.000 --> 14:35.120
inserts so Clickhouse allowed me every time when a request reaches my back end to immediately to

14:35.120 --> 14:41.760
create an insert to Clickhouse and Clickhouse was internally batching this and it was deciding

14:41.760 --> 14:48.480
where it needs to insert in the database so this was not this helped to like not reach some performance

14:48.480 --> 14:57.600
botonics. Another thing that I could do with Clickhouse so here you see I have seven tables

14:57.600 --> 15:04.240
in the old setup with MySQL but in Clickhouse I actually end up with two tables and I actually

15:04.240 --> 15:10.080
could I actually could have one table but I needed this table for showing the host names in the filters

15:10.080 --> 15:16.640
in Grafana and just Clickhouse or in general when you work when we work with time series

15:17.920 --> 15:24.000
the main idea is that here the the the data is normalized I try to really build

15:25.760 --> 15:31.680
to build a user monitoring system in the fashion of a webshop right which is really the wrong idea

15:33.360 --> 15:38.480
but when we use time series database the idea is that the data you can just throw your data into

15:38.480 --> 15:44.800
this database you you have one large fat table and you throw a lot of data and you don't really

15:44.800 --> 15:51.840
need to consider duplication of the data for example here we have this filter's device type

15:52.560 --> 15:57.760
and I don't have a foreign key here to another table where I keep references to all the device types

15:57.760 --> 16:04.000
I just can insert and insert the same string over and over again desktop desktop desktop and this

16:04.000 --> 16:09.840
database will be completely fine with it it will compress the data internally and I won't experience

16:09.840 --> 16:18.640
any performance bottlenecks when I filter by this field and here is my other favorite feature in

16:18.640 --> 16:26.000
Clickhouse it's called it's called low cardinality data type and this data type is really convenient for

16:26.880 --> 16:37.600
columns where the distinct values in this column some less less than 10 000 because this it's

16:37.600 --> 16:43.840
optimizing eternally and it's the the where conditions and the filters in this case are much

16:43.840 --> 16:51.920
much faster when we use low cardinality we if if we have more than 10 000 distinct values we probably

16:51.920 --> 16:57.680
need to go again to something like this and to start introducing additional dimension tables

17:01.040 --> 17:08.640
also so here in left is really uh I would say insane I even don't know how I created this I still

17:09.440 --> 17:16.160
I'm really surprised with myself and you we cannot zoom in here but this was a process where

17:16.560 --> 17:22.160
it included querying my my secure database and I had some application code and I had bunch of

17:22.160 --> 17:28.160
cron jobs and this was trying to guess and to find out all the sessions that bounced and what was

17:28.160 --> 17:34.880
the duration of the sessions it was just really complex and for example to to calculate the bounce

17:34.880 --> 17:43.280
rate with my new setup in Clickhouse I just could use such a query again I got a bit help with this

17:43.920 --> 17:50.880
query I don't completely understand it but it does it actually it works and it's much more

17:50.880 --> 18:00.720
simple and much much more it makes my it makes basic run much much easier to maintain and with

18:01.440 --> 18:07.920
with this query I could actually create easily this correlation between bounce rate and epic

18:08.240 --> 18:11.680
and metric and in our case this is time to first bite

18:13.840 --> 18:19.600
also I want to say that open source is not only about how great is the open source

18:20.160 --> 18:24.160
product that you work with but also the community is very important and that's why

18:24.160 --> 18:31.040
I also stick to Clickhouse they have really great slack community and every time when I ask a question

18:31.040 --> 18:37.040
I I can say that in the matter of a few hours I get really a good response for example here I'm

18:37.040 --> 18:42.800
asking hey I I wrote this query but I feel that it's not optimal I'm not a SQL expert and here

18:42.800 --> 18:48.640
another expert actually suggested a better way how to write this query it's it's shorter and it's much

18:48.640 --> 18:57.680
more performant and also probably this is the first and probably the last database channel

18:58.480 --> 19:02.960
YouTube channel that I will be subscribed but I'm actually subscribed to the Clickhouse YouTube

19:02.960 --> 19:08.560
channel and they have really really good videos like they have every month they have like a release

19:08.560 --> 19:15.600
party video where the the Clickhouse team is showing the new features and there are a lot of

19:15.600 --> 19:23.280
good tutorials so it's it's really welcoming for for beginners and they say you get support from

19:23.280 --> 19:31.040
the community and there is really good there are really good materials out there so now let's look at

19:31.040 --> 19:37.840
the user interface Grafana earlier I mentioned that I was about to start in my in the first

19:37.840 --> 19:44.640
version of basicram I was about to start implementing my own my own user management and login and

19:44.640 --> 19:50.320
authentication and Grafana this comes out of the box so it's much easier to add new user to give them

19:50.320 --> 19:57.120
different permissions and again this is just the code that I would never want to write again

19:57.600 --> 20:07.600
right and in this repository I bundle the basicram version of Grafana it has some customizations

20:09.600 --> 20:16.960
also another benefit of Grafana is it's very easy to model the data and what you want to see

20:16.960 --> 20:23.520
in the in the visualization panels so for example here we have we can define filters

20:24.480 --> 20:31.600
we can have a preview of our data we can also configure different things for example here I'm

20:31.600 --> 20:36.800
just showing how I can configure different colors for the different thresholds and also there is an

20:36.800 --> 20:45.360
SQL editor so when I write the SQL here this Grafana uses this SQL to fetch the data from Clickhouse

20:45.600 --> 20:54.400
and here are other panels that I took advantage of here is the world map so I could it was really

20:54.400 --> 20:59.600
literally plug-and-play I just configured few stuff and I say it from where to read the data about

20:59.600 --> 21:09.120
the countries Grafana also has a third-party plugin for plotly so I still there were scenarios

21:09.120 --> 21:16.160
where I wanted to build some more complex panels and with this panel I could actually build this

21:16.160 --> 21:23.040
one which is showing how the device the screen size is the width of the screen size is distributed

21:25.280 --> 21:31.440
yeah time series this is the kind of the most the default view in Grafana and also I could present

21:32.240 --> 21:35.760
the data in a table this is very good when you want to explore your own data

21:35.920 --> 21:44.480
also Grafana comes with different data sources and of course Grafana needs to know how to talk to

21:46.240 --> 21:52.880
Clickhouse in my in basic realm I'm using a data source developed by company called

21:52.880 --> 21:59.840
Altinity but there is also another one developed by actually official by Clickhouse right

22:04.880 --> 22:12.160
yeah and just to say that all these things that I'm showing all these dashboards that

22:12.960 --> 22:15.520
are built in in the basic version of Grafana

22:16.240 --> 22:25.040
everything there is actually under version control so it's not just that I created a dashboard in

22:25.600 --> 22:30.640
Grafana instance and exported it and save it somewhere this I have this repository where I have

22:31.200 --> 22:36.720
the configuration for each of the panels that I'm maintaining and then this makes makes it much

22:36.720 --> 22:42.400
easier when I need to change something or to add a new panel and I can go through the history and I

22:42.400 --> 22:48.960
can understand what actually change if something has to be reverted yeah for example here we are

22:48.960 --> 22:56.320
seeing how I keep this row as it's a templated SQL but this is how it's presented then when we look

22:56.320 --> 23:07.360
in Grafana and again out of all this source code configuration that I keep for the dashboards

23:07.440 --> 23:14.320
I'm building a docker image where we here we have a bit of branding work just removing

23:14.320 --> 23:21.040
some things from the default or rewriting some things from the default Grafana image here we are

23:21.040 --> 23:26.080
installing the plugins that we need for our setup and here we are importing all the configurations

23:26.080 --> 23:34.400
for the dashboards and the data sources and what I found over time when I spoke to different people

23:34.400 --> 23:41.200
who asked me about three user monitoring systems very often the conversation was just ending when

23:41.200 --> 23:45.440
when I was explaining yeah you need to run this component on this server and you need to run this

23:45.440 --> 23:50.640
component on this server and you don't need to run this component on this server and it looks like

23:50.640 --> 23:56.640
their use case the use case of the people that I spoke to was actually not requiring them to scale

23:56.640 --> 24:03.920
they had pretty small websites or web shops and I work on something a bit more monolithic

24:04.560 --> 24:10.960
it's called basicrum o in one and the idea is that probably again probably it sounds from

24:10.960 --> 24:14.640
engineering point of view a bad practice but it actually could be really practical thing

24:16.960 --> 24:23.520
the idea is to run everything on one big box and I believe for 20 euro a month this could be actually

24:23.520 --> 24:32.800
hosted somewhere and I tested it it can handle 1.5 million page views a month and the idea here is

24:33.440 --> 24:42.480
we introduced traffic which is a proxy it stays in front of this folder components and it's helping

24:42.480 --> 24:48.960
me for SSL termination and routing request because some of the request needs to go to the data

24:48.960 --> 24:56.320
collection part and other request needs to go to the grafana to the part where we analyze the data

24:57.360 --> 25:01.920
so this is really convenient it's really easy for people if you just want to give it a try

25:03.760 --> 25:10.720
and a few takeaways I just have to say that a real user monitoring system is fairly complex system

25:10.720 --> 25:17.040
and you need to learn to train yourself you want to develop one you need to you need to

25:18.320 --> 25:23.040
learn more about on the data collection site where how the data is collected from the browser

25:23.920 --> 25:30.000
how to visualize the data and it will be a bonus if you learn about how time series databases work

25:30.240 --> 25:36.080
again choosing the right database to solve the right problem is the key

25:37.520 --> 25:42.240
and it's great when when you can transfer a problem from the application on the database

25:42.960 --> 25:50.000
layer it just saves a lot of time and yeah grafana could save a lot of time and effort even I

25:50.000 --> 25:56.480
recommend it even if you still want to build your own front end maybe just start with grafana to

25:56.480 --> 26:03.760
play with the data and to display something it literally will save a lot of time and I got a

26:03.760 --> 26:09.840
signal that I run out of time but you can catch me up all right I can take one question

26:26.960 --> 26:35.280
so in this project we don't really keep any IP addresses so for example that I guess that's

26:35.280 --> 26:45.360
what we consider like user data or yeah so the backend doesn't store any personal data

26:46.160 --> 26:51.120
in this case so by default it's using the IP address only to identify the country and the

26:52.080 --> 27:00.880
the city but it's not storing the IP address after that and I know that on the data collection site

27:00.880 --> 27:07.360
from the boomerang library I'm not sure if it's on the boomerang library has also like

27:07.360 --> 27:14.160
part of the boomerang source code is private but I know that for PCI compliance reasons it has

27:14.160 --> 27:21.200
special parts that try to avoid collecting stuff around the user sometimes the user may

27:21.200 --> 27:26.080
put for example a credit card number and this could be actually collected by mistake so this

27:26.080 --> 27:40.880
library also tries to avoid collecting critical user information do you mean to consent

27:41.760 --> 27:42.080
the cons

27:48.000 --> 27:53.920
so the library comes with a special snippet that's a loader snippet so you can have your own

27:55.120 --> 28:01.600
callback so you can you can call this loader snippet only after a cookie consent so it's possible

28:10.880 --> 28:12.100
you

