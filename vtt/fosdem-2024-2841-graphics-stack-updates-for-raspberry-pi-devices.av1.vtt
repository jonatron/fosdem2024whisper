WEBVTT

00:00.000 --> 00:10.080
So, as I said, thank you to attend this talk about the

00:10.080 --> 00:13.960
acceptance update of the graphics stack in the Raspberry Pi device.

00:13.960 --> 00:18.280
Thanks also to the organizers of the 4 of them and especially to the people

00:18.280 --> 00:20.280
organizing this DAPLOO.

00:20.280 --> 00:21.280
Which is great.

00:21.280 --> 00:24.360
So, let me introduce ourselves.

00:24.360 --> 00:26.680
My name is Juan and with me is my colleague,

00:26.680 --> 00:28.400
Czema Kacenova.

00:28.400 --> 00:33.360
We work in the Alia and the graphics team and we are working on the Raspberry Pi graphics

00:33.360 --> 00:34.680
stack.

00:34.680 --> 00:38.880
So, what is it to come out?

00:38.880 --> 00:44.880
It's basically cover the change that happened in the graphics stack since the release of

00:44.880 --> 00:49.480
Raspberry Pi OS Bullseye Edition was in November 2021.

00:49.480 --> 00:52.840
Up to the latest version, which is Boogworm.

00:52.840 --> 00:57.520
It was released several months ago in October 2023.

00:57.520 --> 01:06.240
So, I mean, this is for people that are not used to with, primarily with MISA.

01:06.240 --> 01:11.920
We have like five devices, Raspberry Pi devices, well, there are more, but are like variations

01:11.920 --> 01:13.680
of those devices.

01:13.680 --> 01:18.280
The Raspberry Pi 1, 2, and 3 use the GPU from Brathcom.

01:18.280 --> 01:20.720
It's called VideoCore 4.

01:20.720 --> 01:24.680
And the name of the MISA driver is called VC4.

01:24.680 --> 01:32.480
And then for the Raspberry Pi 4 and 5, they use the VideoCore 6 and 7.

01:32.480 --> 01:36.840
And the name of the driver changes like V3D for the OpenGLES.

01:36.840 --> 01:42.240
In this case, they are support for the Vulkan driver, which is called V3DV.

01:42.240 --> 01:44.320
So, what things happened?

01:44.320 --> 01:49.920
Well, probably the most exciting one is the release of the Raspberry Pi 5.

01:49.920 --> 01:54.280
This is evolution of the GPU from Raspberry Pi 4.

01:54.280 --> 02:01.120
It's an architecture, but with more benefits, how it really means like.

02:01.120 --> 02:04.640
It has like a higher clock rate, so it's faster.

02:04.640 --> 02:07.360
It supports up to eight render types.

02:07.360 --> 02:14.120
It has better support for subgroups operations, which is interesting for Vulkan.

02:14.120 --> 02:25.440
And that provides a lot of changes at the institutional level, so it allows to have more compact shaders

02:25.440 --> 02:27.440
which run faster.

02:27.440 --> 02:35.160
Drawback is that it has a bit of less register, so it suffers a bit of more pressure.

02:35.160 --> 02:38.800
And this is the support.

02:38.800 --> 02:49.800
It's integrated with V3DV-divis, and it was submitted for review almost the same day the Raspberry Pi 4 was announced.

02:49.800 --> 02:56.800
And now it's released in MSI 23.3, and that's in the current 6.8, which is required.

02:56.800 --> 03:05.000
As I said, this is more or less the evolution of the GPU, so the GPU front has the Raspberry Pi 4.

03:05.000 --> 03:12.000
So nowadays the features are more or less the same in terms of the driver implementation.

03:12.000 --> 03:22.000
So it supports the OpenGL ES 3.1 and Vulkan 1.2, and that supports a non-conformant version of OpenGL 3.1.

03:22.000 --> 03:25.000
We will see that at this moment.

03:25.000 --> 03:34.000
So from the point of view of the drivers in the MSI, the OpenGL driver, well, one of the important things was that

03:34.000 --> 03:41.000
we promote from OpenGL 2.1 to OpenGL 3.1 with some caveats, I'll explain later.

03:41.000 --> 03:50.000
I think this is quite important because at the end the Raspberry Pi is intended to be used as a desktop PC in most cases.

03:50.000 --> 03:56.000
So targeting the OpenGL desktop apps is quite interesting.

03:56.000 --> 04:03.000
So there are some applications that require OpenGL 3.0 something, and now they come on the Raspberry Pi.

04:04.000 --> 04:15.000
The upgrade from Bullside to Google allow us to expose 35 new extensions from OpenGL, OpenGL and OpenGL ES.

04:15.000 --> 04:24.000
And I was saying before, the driver is not fully compliant of 3.1 because there are some missing features in the hardware.

04:24.000 --> 04:33.000
For instance, this version requires 8 radio turrets. This is fixed in Raspberry Pi 5 but not in Raspberry Pi 4.

04:33.000 --> 04:35.000
It doesn't support 4.

04:35.000 --> 04:43.000
And then the hardware itself does seamless QMAP filtering, and the OpenGL spec requires no seamless.

04:43.000 --> 04:46.000
And then some other formats that are not supported.

04:46.000 --> 04:50.000
But all in all, probably these are not the most easy features.

04:50.000 --> 04:52.000
So we support anything else.

04:52.000 --> 05:01.000
So from a practical point of view, probably any application that uses OpenGL 3.1 will work in the Raspberry Pi.

05:03.000 --> 05:10.000
Then in the Vulkan driver, we move from Vulkan 1.0 to 1.2.

05:10.000 --> 05:17.000
So this is Vulkan 1.0, 1.1, and then 1.2, which meant exposing like 80 new extensions.

05:18.000 --> 05:23.000
It will compare both versions of the driver from Bullside to Vulkan.

05:23.000 --> 05:25.000
So there are a lot of new extensions.

05:25.000 --> 05:33.000
Some, I mean, I mentioned like extension dealing with sub-drops, as I said, which is very interesting for Vulkan.

05:33.000 --> 05:36.000
Extension dealing with geometry shaders.

05:36.000 --> 05:42.000
But I think the probably the most important work done was improving the performance.

05:42.000 --> 05:49.000
So when Vulkan 1.0 was released, the target was just having a performance driver.

05:49.000 --> 05:53.000
So we didn't spend any time on making it fast.

05:53.000 --> 05:59.000
And during this lifetime, we were working a lot on making it more performant,

05:59.000 --> 06:09.000
specifically in the shader compiler to reduce the liminal analysis and make strategies to make the shader smaller and faster.

06:09.000 --> 06:14.000
The good part is that the shader compiler for the Vulkan driver is actually shared with the OpenGL.

06:14.000 --> 06:18.000
So both the OpenGL and Vulkan share the same compiler.

06:18.000 --> 06:25.000
So all the improvements in the driver in the shader compiler also affect the OpenGL.

06:25.000 --> 06:29.000
So basically the improvements are both for Vulkan and for OpenGL driver.

06:29.000 --> 06:36.000
Another thing relevant to mention is that now Think, which is the driver that supports OpenGL over Vulkan,

06:36.000 --> 06:39.000
works with the V3D driver.

06:39.000 --> 06:44.000
So it means that you can use the Think to open up the applications.

06:44.000 --> 06:51.000
And then that's how we, well, we know Roman Stratenko was working on that in support for Android.

06:51.000 --> 06:58.000
So now you can run Android in Raspberry Pi 4 with the Vulkan driver.

06:58.000 --> 07:03.000
And now my colleague will continue with the workbook in the kernel.

07:06.000 --> 07:17.000
Okay, Sam.

07:17.000 --> 07:24.000
Well, continuing with our work on Vulkan on the Raspberry Pi,

07:24.000 --> 07:29.000
we need to implement several features that were not available in the hardware.

07:29.000 --> 07:33.000
We need to create what we call CPU jobs.

07:33.000 --> 07:38.000
That is part of the behavior that is not available in the hardware support for the GPU.

07:38.000 --> 07:43.000
So we implemented that in the Vulkan driver in the user space.

07:43.000 --> 07:51.000
But that implied, that was affecting mainly to some queries about performance counters,

07:51.000 --> 07:58.000
time-stun queries, and compute-shaded in this patch.

07:58.000 --> 08:06.000
So this caused issues because when we were submitting the different command buffers to the GPU,

08:06.000 --> 08:12.000
we need to start the driver of the GPU submissions, do the work in the user space,

08:12.000 --> 08:14.000
and then continue after having the result.

08:14.000 --> 08:19.000
So one of the improvements that we have just recently landed upstream in the kernel

08:19.000 --> 08:21.000
was these kernel CPU jobs.

08:21.000 --> 08:27.000
So we moved this operation that are already known to the kernel space.

08:27.000 --> 08:31.000
So when we are creating in the Mesa driver the submission,

08:31.000 --> 08:39.000
we are currently going to handle that so we don't stop the submission of GPU jobs.

08:39.000 --> 08:44.000
That was quite an interesting improvement in terms of performance.

08:44.000 --> 08:48.000
Because before this, there were a lot of stalls in the submission.

08:48.000 --> 08:54.000
Another feature that was quite interesting for the users at the end

08:54.000 --> 08:59.000
was to know if they were just really the GPU when they were running the different applications.

08:59.000 --> 09:01.000
It would happen for a lot of developers.

09:01.000 --> 09:04.000
I don't know if this is really working with the GPU.

09:04.000 --> 09:08.000
So we implemented GPU stats.

09:08.000 --> 09:18.000
We suppose these users' stats per process using the standard way of doing it in DRM.

09:18.000 --> 09:21.000
And we also suppose global stats.

09:21.000 --> 09:26.000
So this way some application just, if you want to know the global status of the GPU,

09:26.000 --> 09:28.000
just check that the value of the percentage of usage.

09:28.000 --> 09:31.000
Because in other case, you need to go to every process,

09:31.000 --> 09:38.000
check each process which amount of GPU has been used, and do the complete sum up.

09:38.000 --> 09:43.000
So because of using the standard interfaces, we can run application like GPU top.

09:43.000 --> 09:47.000
That is really nice because it works for several drivers.

09:47.000 --> 09:56.000
And at the end for the global stats, as we, there is no, no a common defined interface to expose that.

09:56.000 --> 09:58.000
We are currently using a CFS.

09:58.000 --> 10:09.000
So the hardware lacks some features to provide the stats as other drivers are using,

10:09.000 --> 10:11.000
in the case for something in Intel.

10:11.000 --> 10:14.000
So we are, we go to a simple approach.

10:14.000 --> 10:20.000
It is just, we put in the DRM schedule, when we submit a job to the GPU,

10:20.000 --> 10:24.000
we just get the time stop and the job ends, we get the finished time.

10:24.000 --> 10:31.000
As we are only processing one job on each queue, we have the information about how much the GPU was used.

10:31.000 --> 10:40.000
So we can show here, for example, it's on the top right of the, there is a graph with a widget that the users can check

10:40.000 --> 10:41.000
and the GPU users.

10:41.000 --> 10:48.000
And in the task manager, we already have the information about the GPU users.

10:48.000 --> 10:53.000
For example, in this screen, the main user of the GPU is Chromium,

10:53.000 --> 10:57.000
and the second one is the compositor, in our case it's Wayfire,

10:57.000 --> 11:05.000
because it's compositing all the different windows and surface that we already have there available.

11:05.000 --> 11:11.000
So, well, that's the highlights of the modifications from the kernel.

11:11.000 --> 11:25.000
And one of the main important changes that we did from Bullside to Bookrow Raspberry Pi OS was the change from the default desktop.

11:25.000 --> 11:31.000
Previously, in Bullside, we were running for the Raspberry Pi 4 devices,

11:32.000 --> 11:37.000
matter with Xserver, and it was being OK.

11:37.000 --> 11:44.000
And for the previous generation hardware, Raspberry Pi 1, 2 and 3, we were running the previous desktop we had,

11:44.000 --> 11:48.000
that it was an open-boss with Xserver.

11:48.000 --> 11:51.000
Matter was too heavy for the generation of hardware.

11:51.000 --> 11:55.000
And when we have this release of Bookrow,

11:56.000 --> 12:00.000
now all the Raspberry Pi's that we started, the public e-mails,

12:00.000 --> 12:03.000
they get a Wayland desktop using Wayfire.

12:03.000 --> 12:10.000
That was, and for Raspberry Pi 5, it was just a digital, it's the default one.

12:10.000 --> 12:14.000
For previous generation, we still maintained the open-box and Xserver,

12:14.000 --> 12:19.000
but I want to commend on this, now this is the last part of the talk.

12:20.000 --> 12:24.000
So, well, Wayfire is using OpenGL for doing the composition.

12:24.000 --> 12:27.000
It's based on WLroute's backend.

12:27.000 --> 12:30.000
We use the OpenGL, but it's quite tight for OpenGL.

12:30.000 --> 12:35.000
So, all the plugins are implemented there using the OpenGL API.

12:35.000 --> 12:43.000
One of the most important things we did in this transition from Bullside to Bookrow

12:43.000 --> 12:49.000
was that the user's experience don't change a lot.

12:49.000 --> 12:57.000
So, as we can see, Simon Long from the Raspberry Pi has a lot of effort here.

12:57.000 --> 13:00.000
So, it's difficult if you don't see the change of the background

13:00.000 --> 13:05.000
to figure out what are the differences between the previous version and the new one.

13:05.000 --> 13:10.000
That is Bullside and this is Bookrow running.

13:10.000 --> 13:20.000
So, all has been rewritten, the panel, the theme, because there are different compositors.

13:20.000 --> 13:26.000
And, well, now we go to the desktop on the previous generations of hardware.

13:26.000 --> 13:31.000
Well, we are still using the Xserver with Openbox.

13:31.000 --> 13:33.000
It's the file, the file we have.

13:33.000 --> 13:38.000
This has been the same way since Bullside, we didn't try to see it in two matters.

13:39.000 --> 13:47.000
The main cause of still using this is that we need to use sober composition.

13:47.000 --> 13:58.000
We use the CPU to render the desktop because the hardware limitations are supposed to have a memory limit

13:58.000 --> 14:05.000
that is 256 megabytes by default.

14:05.000 --> 14:15.000
The problem is that we don't have control when the GPU memory that is using the CMA, Continuous Memory Allocator, runs out.

14:15.000 --> 14:25.000
So, at the moment, we'll answer a new Chromium Brows tab that uses CMA memory.

14:25.000 --> 14:32.000
If we run out of the memory, next application that can do the following allocation could be the Xserver

14:32.000 --> 14:35.000
and it can crash or the compositor.

14:35.000 --> 14:47.000
So, the solution it has been there during all the time is on these devices, all we are using is CPU sober composition.

14:47.000 --> 14:52.000
So, GLAMOR has been off all the time and there is no hardware support.

14:52.000 --> 14:55.000
You can run a full screen application.

14:55.000 --> 14:58.000
All has been, you can enable it, but it's not the default.

14:58.000 --> 15:01.000
You can enable GLAMOR and you get hardware acceleration.

15:01.000 --> 15:05.000
But you are supposed to crash in your desktop at any moment.

15:05.000 --> 15:16.000
And there are a lot of hungry applications like the browser that can kill you if you open six tabs, you are completely frozen the desktop.

15:16.000 --> 15:30.000
So, during the previous development cycle on both sides, we wanted to make the possibility of enabling the hardware accelerated applications.

15:30.000 --> 15:41.000
So, if you want to launch your GLX-DRS, the GLX-DRS is not using a low beam pipe and it's using the driver for the hardware.

15:41.000 --> 15:44.000
So, we managed to do that.

15:44.000 --> 15:57.000
We enabled the hardware acceleration on the four applications while we were still doing sober composition for the rest of the desktop.

15:57.000 --> 16:03.000
So, in case you run out of memory, what is going to crash is just application.

16:03.000 --> 16:15.000
You are not supposed to the Xserver crashing or matter crashing or whatever application because they are not prepared for when you do a memory location, it fails.

16:15.000 --> 16:18.000
We assume that all the time it has been working.

16:18.000 --> 16:23.000
This was implemented modifying the mod-setting driving index server.

16:24.000 --> 16:31.000
We implemented the support for DRI-3 in this case, but without the need of using Glamour.

16:31.000 --> 16:36.000
How it is currently written is just Glamour enables the DRI-3.

16:36.000 --> 16:40.000
So, on crash-by-devices, we can use DRI-3.

16:40.000 --> 16:45.000
Even we don't have open GL during the composition of the general browser.

16:45.000 --> 16:51.000
There is a request for the server, but there is now too much interest in integrating that.

16:51.000 --> 16:55.000
We understand because Xserver development is stopped at the end.

16:55.000 --> 17:00.000
But we have been using that downstream for almost a year.

17:00.000 --> 17:04.000
It was a huge improvement for the users.

17:04.000 --> 17:10.000
With these changes, we avoid the problem of the GPU memory subsystem.

17:11.000 --> 17:20.000
When we were about to release Book World, the idea is that we are transitioning to Wayfire as the stock compositor.

17:20.000 --> 17:24.000
What can we do for the older generation devices?

17:24.000 --> 17:32.000
We need to rethink again how we solved the previous problems with Xserver, now with Wayfire.

17:32.000 --> 17:39.000
We need to do the software rendering composition using the CPU.

17:39.000 --> 17:45.000
We would like to allow again hardware-obscleted applications.

17:45.000 --> 17:53.000
The problem with using Wayfire to do the software composition is that Wayfire is quite tight to use OpenGL.

17:53.000 --> 17:56.000
It is using WR-ROOT's backend.

17:56.000 --> 18:02.000
As you have seen in parts of the code, mainly in the plugins, doing the different effects,

18:02.000 --> 18:06.000
we are doing calls to OpenGL API.

18:06.000 --> 18:10.000
We don't want to do that.

18:10.000 --> 18:15.000
The first thing is WR-ROOT already has a Pismand backend that is working.

18:15.000 --> 18:19.000
You can just transition and the parts of WR-ROOT that are using Wayfire,

18:19.000 --> 18:24.000
just do small changes to use the Pismand backend and it works.

18:24.000 --> 18:32.000
The next part is we need to reimplement all the parts that were tied to OpenGL in the different plugins

18:32.000 --> 18:35.000
that we are going to use in the distribution.

18:35.000 --> 18:39.000
There are some that are quite complex that we didn't need, so we didn't implement the change,

18:39.000 --> 18:42.000
to use the Pismand rendering logic.

18:42.000 --> 18:53.000
This way we managed to get all the rendering done by Wayfire to be done using CPU rendering.

18:53.000 --> 18:58.000
The problem is that if you do that and you start doing blending operations,

18:58.000 --> 19:03.000
in architecture they become really slow.

19:03.000 --> 19:11.000
Reading from the even buffer when they are doing the blending, assuming that we have synchronous memory,

19:11.000 --> 19:16.000
all the changes are flashing at the moment, it is terrible.

19:16.000 --> 19:25.000
We experimented with enabling for the buffers, we used for doing the run buffers,

19:25.000 --> 19:27.000
to use non-coherent memory.

19:27.000 --> 19:32.000
That makes that if you write on the CPU and then you put it on the display,

19:32.000 --> 19:36.000
maybe there is not coherency.

19:36.000 --> 19:43.000
So you start needing to flash in some places, you need to handle that.

19:43.000 --> 19:50.000
Some things happen funny because in 32 bits, IRM is different to 64 bits.

19:50.000 --> 19:57.000
Things that work in one place, in 32 bits you can just, I'm going to flash the memory before putting it on the display,

19:57.000 --> 20:00.000
and it works. In 62 bits, it doesn't work.

20:00.000 --> 20:04.000
At the end the flash is not doing anything in architecture.

20:04.000 --> 20:09.000
So we need to handle the synchronization but in the compositor to do that work.

20:09.000 --> 20:15.000
The difference of that change is that everything runs fast enough.

20:15.000 --> 20:23.000
The problem is that when you enable non-coherent buffers, you only want to do that for the compositor,

20:23.000 --> 20:25.000
but not the rest of the application.

20:25.000 --> 20:30.000
So that is complex because some applications don't work on the core and buffers

20:30.000 --> 20:35.000
and we are dealing with that, maybe enabling it with a parameter,

20:35.000 --> 20:41.000
creating a new IOCTL for the getting non-coherent buffers or what.

20:41.000 --> 20:46.000
So well, and for the other part, that was quite fast as we already know,

20:46.000 --> 20:51.000
the part about getting hardware-assisted applications because we already have the knowledge of doing this on the X-server,

20:51.000 --> 20:56.000
so at the end we need to handle in WOL routes to pass in the pieceband backend,

20:56.000 --> 21:03.000
to pass modifiers with the memory buffers, and it was already working.

21:03.000 --> 21:11.000
So I'm going to show this is the current working progress we have with this work.

21:11.000 --> 21:16.000
This is our Raspberry Pi 3 running the desktop.

21:16.000 --> 21:18.000
It's using the non-coherent buffers.

21:18.000 --> 21:21.000
In other case, you will see how the programs are moving.

21:21.000 --> 21:23.000
The performance is quite good.

21:23.000 --> 21:28.000
Some of the more complex things that are most expensive is the shadow calculation.

21:29.000 --> 21:32.000
You cannot imagine doing this in the CPU.

21:32.000 --> 21:36.000
Every time you scale a window, it's complex.

21:36.000 --> 21:41.000
We are seeing that these DLX-servers are using the hardware acceleration

21:41.000 --> 21:47.000
and it's not the best thing that we can do because there are possibilities of having another different plane

21:47.000 --> 21:56.000
to show that there is no display, but we are bleeding this by the compositor.

21:57.000 --> 22:00.000
We have enough time, I think.

22:00.000 --> 22:03.000
So we are going to see several plugins working.

22:03.000 --> 22:14.000
So as a conclusion, we are on the point of maybe thinking about putting this for the users,

22:14.000 --> 22:17.000
but it's still not ready.

22:17.000 --> 22:23.000
One of the things that Raspberry Pi devices have is trying to maintain all the generation of hardware

22:23.000 --> 22:29.000
because you can run the last Raspberry Pi OS with the Raspberry Pi 1 and it will work.

22:29.000 --> 22:33.000
We have already tested it with the Raspberry Pi 1 that has slower memory.

22:33.000 --> 22:37.000
Juan was doing that this and was good enough.

22:37.000 --> 22:42.000
It's comparable with the results we are having with the X-server.

22:42.000 --> 22:47.000
We are seeing Chrome running, that is using hardware acceleration.

22:47.000 --> 22:53.000
The good thing is that as we are not spending memory of the CPU, we can run more applications.

22:53.000 --> 22:58.000
But you can crash Chrome in just open, I think it's eight apps.

22:58.000 --> 23:03.000
You will crash Chrome in, but only in some cases, only one window.

23:03.000 --> 23:06.000
This is the Zoom working, this is all software composition.

23:08.000 --> 23:13.000
And I think that's all from us.

23:14.000 --> 23:19.000
We already implemented, this is the switcher that has Wi-Fi by default,

23:19.000 --> 23:24.000
we implemented with Pisman and we tried to do a more simple option,

23:24.000 --> 23:27.000
but this one was already working fine at the end.

23:27.000 --> 23:35.000
So we are maintaining the most complex part is doing the transparency and using the alpha channels in software.

23:35.000 --> 23:41.000
So, question, I think we are on time.

23:44.000 --> 23:51.000
What features do you need the CPU to actually get into the job?

23:51.000 --> 23:57.000
And are they used a lot, the applications need them, will that impact the performance?

23:57.000 --> 24:06.000
Well, our colleague, Mayra Canal, has a lot of positive planning in this and they are really out.

24:06.000 --> 24:14.000
The question is, which features in particular are needed to be done in the CPU and cannot be done in the GPU?

24:14.000 --> 24:18.000
I think I already commented, there are some things related to performance counters,

24:18.000 --> 24:25.000
mainly if you want, when you are running the CPU commands, to reset the counters.

24:25.000 --> 24:28.000
These need to be done in the GPU.

24:28.000 --> 24:31.000
No, you need to write in a resistor in particular.

24:32.000 --> 24:39.000
The other one is related to getting the time stamp, because there is no support to get the time stamp from the GPU.

24:39.000 --> 24:43.000
And the other one is indirect computer shader dispatch.

24:43.000 --> 24:49.000
Is that when you are sending several instances of a computer shader,

24:49.000 --> 24:56.000
in this case you need to send in the CPU one by one, because there is no support in the GPU.

24:56.000 --> 25:01.000
So you just submit the buffer to the kernel and the kernel is going to handle that.

25:01.000 --> 25:08.000
In the other case you were in the user space, you send one, wait, and you are going to send one by one.

25:12.000 --> 25:14.000
So, well, time's up.

25:14.000 --> 25:16.000
Thank you very much for your attendance.

25:26.000 --> 25:29.000
you

