WEBVTT

00:00.000 --> 00:13.000
Okay, so thank you for joining. The next talk is still about Collabora, second talk of the

00:13.000 --> 00:19.920
day about Collabora, which is about Collabora online usability optimization. And we still

00:19.920 --> 00:27.040
have Kaelin, and that was in the previous talk, and also Michael that is joining us.

00:27.040 --> 00:30.960
Thank you, Kaelin. Fantastic. This is Kaelin, this is Michael. Good. This is what I'm going

00:30.960 --> 00:37.960
to say. You'll see it as we get there. And yes, fantastic. Kaelin did a very good spiel

00:37.960 --> 00:41.520
earlier on how this thing works. So if you're in the previous talk, you saw something similar

00:41.520 --> 00:45.280
to this, but you have your browser, and then you have a web socket talking to a server on

00:45.280 --> 00:50.520
the back end, C++. And this talks to the Librofiskit over a Unix domain socket, which

00:50.520 --> 00:57.080
does all sorts of beautiful interoperability rendering, tiled goodness. And yes, this fetches

00:57.080 --> 01:03.880
data from an own cloud, an OSIS, a next cloud, a pygmc file, lots of things, any kind of

01:03.880 --> 01:11.800
WAPI share point I think we can use even. Yeah, for the good guys, right? And yes, so

01:11.800 --> 01:15.360
anyway, so this gets the file, this pushes it in here, it renders it, it comes back out

01:15.360 --> 01:21.320
to the browser. And yes, we do all sorts of things to try and cache that. So JavaScript

01:21.320 --> 01:23.680
here, good stuff over there. Anything else on there?

01:23.680 --> 01:27.880
Nope, nope. Seems pretty silly. And I just want to talk a little bit about latencies.

01:27.880 --> 01:31.320
This is an interactive presentation. I'm not going to ask you to put your hands up just

01:31.320 --> 01:36.120
yet. But just here are some timings. And the one I want to time is this human eye blink,

01:36.120 --> 01:40.920
100 milliseconds for a human eye blink, okay? Right, so here we are. How good are you at

01:40.920 --> 01:45.920
blinking? Are you ready? Okay? So I'm going to press a button and we'll start blinking.

01:45.920 --> 01:49.600
And when you see red, stop. But you need to count at the same time, okay? You ready?

01:49.600 --> 01:50.600
Silently.

01:50.600 --> 01:58.160
Silently. Yeah, yeah, here we go. Ready? Ready? Are you ready? Go. How many? How many did you

01:58.160 --> 02:04.400
get? Do you want to try again? Yeah? Okay, so here is reciprocation for beginners, okay?

02:04.400 --> 02:10.900
So this is an advanced topic in maths, okay? If you need help. Anyway, so if you're a falcon,

02:10.900 --> 02:17.660
you've got like 7.7 milliseconds. So that's pretty good. Me, I'm more about here. I don't

02:17.660 --> 02:21.420
know how about you. Six, seven, eight. How many did you get? Do you want to try again?

02:21.420 --> 02:25.500
Okay, we're going to try again. It's like, okay, right? You got the idea now, right?

02:25.500 --> 02:30.780
Okay, ready? Not completely, okay. So I'm going to click and it's going to go green.

02:30.780 --> 02:35.940
Start blinking. And count the blinks you're doing. Blink as fast as you can, right? As

02:35.940 --> 02:39.940
many as you can. I want to get a high score here, right? We're going for the Peregrine

02:39.980 --> 02:45.980
Falcon 153 in a second, right? Okay, ready? Okay, three, two, you've not started yet,

02:45.980 --> 02:53.980
have you? Three, two, one, blink. Okay, that was a second. You had to blink. How many did

02:53.980 --> 03:00.140
you get? Five, six, seven, eight. Yeah, okay, fair enough. So this tells you your score.

03:00.140 --> 03:07.640
And interestingly, in the UK, they say a blink takes between 100 and 150 milliseconds. In

03:07.680 --> 03:13.180
Harvard, it takes between 100 and 400, which tells you something about Americans. Maybe.

03:13.180 --> 03:20.180
I don't know. It's slower pace of life is good for people generally. Anyway, sorry. So here

03:20.180 --> 03:24.880
we are. So actually, the very interesting thing is that when you start looking at some of

03:24.880 --> 03:28.880
these numbers, now on a log scale, so they're a bit more friendly, you know, the blinking

03:28.880 --> 03:32.760
is really quite slow. You can go from the Frankfurt to the US east coast and back again

03:32.800 --> 03:39.800
in the same time, right? So that's pretty good. You know, the 60 hertz frame time, 16,

03:40.680 --> 03:45.960
you know, is also quite long. You can get Frankfurt Milan, Frankfurt London is a similar

03:45.960 --> 03:49.520
time to the time it takes to get something on the screen, particularly when you add the

03:49.520 --> 03:54.320
monitor latency. So you blink faster than you miss. Lots of people are very worried

03:54.320 --> 04:00.480
about latency, and they don't have a good feeling for how long things take. But it's

04:00.960 --> 04:04.960
quite interesting to see some of these things. And also, in terms of typing, you know, like

04:04.960 --> 04:11.560
the average typist is supposed to be like three characters a second, pro 6.6. Yeah,

04:11.560 --> 04:17.560
it's human eye blinkers quicker. But you know, even me typing, not very accurately, it's

04:17.560 --> 04:21.800
like, yeah, quite, quite, and if you mash the keyboard, it turns out you're massively

04:21.800 --> 04:26.600
faster, like you're 10 times faster than the average typist when you mash the keyboard.

04:27.400 --> 04:32.080
It's not, you know, it's not good for it. So yes, there we go. Anyway, I'm going to

04:32.080 --> 04:34.680
hand over to Depp, Aquilon, unless you have anything to add?

04:34.680 --> 04:41.180
No, no, no, no, nothing to add on blinking. But yeah, the fundamental point that networking

04:41.180 --> 04:44.640
is really, really fast and stuff comes from one end to the other and back in a very, very

04:44.640 --> 04:48.120
sharp period of time is great. So, you know, don't generally have to worry too much about

04:48.120 --> 04:54.800
that part of things. Yeah, so what we do is that we have a bunch of demo servers that

04:54.840 --> 05:01.160
are generally publicly accessible. And what I've started, we started in recently is to

05:01.160 --> 05:08.160
use perf to sample once a second and record for an entire week what happens on the public

05:08.160 --> 05:11.760
servers. And at the end of the week, then we generate a single flame graph from all of

05:11.760 --> 05:18.760
that to see what, where, where, where our time is spent over the week generally. That's

05:19.040 --> 05:24.200
the demo servers, multi user testing. We have this once a week called some of the people

05:24.240 --> 05:28.880
present in the room, join us from that from other people, organizations and, and community

05:28.880 --> 05:33.160
members, members. And we just have a general feel as to what it feels like in that little

05:33.160 --> 05:40.160
10, 20, 15 person call for the applications are still responsive or whatever issues arise

05:40.280 --> 05:45.320
in testing that can be checked at that point. And that is also profiled and flame graph

05:45.320 --> 05:50.160
generated, typically one for writer and one for Calc in recent tests, which are all stuck

05:50.160 --> 05:53.360
up in GitHub that you can look at yourselves if you're interested to see the change in

05:53.360 --> 05:57.720
time over what we're looking at. We use it internally in clapper, of course, with the

05:57.720 --> 06:02.800
deployment that is used daily there and the same week long profile that I mentioned for

06:02.800 --> 06:09.800
the demo server is run on the internal one now as well. Yeah, so that's the tooling that

06:09.960 --> 06:13.440
we're looking at there. And then interactive debugging, which you have the clapper online,

06:13.440 --> 06:18.360
you can do yourself. You just go help about and you trip a click on the dialogue there.

06:18.360 --> 06:22.040
And that'll show you up this debugging display that we're looking at here. There's loads

06:22.080 --> 06:27.200
of information in it there. The far right inside the tick box as you check them on, certain

06:27.200 --> 06:31.320
ones will check on display in the bottom left corner to tell you things. But maybe more

06:31.320 --> 06:38.040
interestingly, the one that we're calling the tile overlays. When you type in the documents,

06:38.040 --> 06:43.840
you'll get these flashing areas. And that's the part of the document that has been required

06:43.840 --> 06:48.400
to be redrawn because of your interaction. So what you're really hoping to see, especially

06:48.440 --> 06:51.960
looking at these things is that people are typing and you're hoping to see a small rectangle

06:51.960 --> 06:56.240
around the area of change that they're actually making. If the entire screen starts flashing,

06:56.240 --> 06:59.920
it means that there's a whole reason other piles of things have been redrawn or been

06:59.920 --> 07:06.680
invalidated to be painted to be redrawn later on to avoid that. These are the kind of flame

07:06.680 --> 07:11.400
graphs that we look at and the week and just for the purposes of looking at these things,

07:11.400 --> 07:15.240
the colors don't matter in these flame graphs or most flame graphs. What matters is the

07:15.280 --> 07:19.720
width of the line, the width of the bar, the wider the bar, the more proportionally time has

07:19.720 --> 07:22.760
been spent there. What you want to do is you want to take a quick look at it. You want to see

07:22.760 --> 07:26.400
which is the widest line and see can you make the wider lines narrower. I mean, it's nothing to

07:26.400 --> 07:33.360
the profiling really. It's just make the wide ones narrow. Yeah, so this particular one is in

07:33.360 --> 07:40.240
the widest bar there. This whole gigantic pile of boost, spirit, classic, whatever, which is all

07:40.240 --> 07:45.560
being used to detect if the PDF that people are opening up is a particular type of PDF, the

07:45.560 --> 07:51.080
hybrid PDF that's using LibreOffice where you can embed the LibreOffice document inside the PDF.

07:51.080 --> 07:57.680
So when you open up PDF, you also have the original document. It just takes a ludicrous amount of

07:57.680 --> 08:02.680
time, especially over the course of a week to collect up that information when it can be done in

08:02.680 --> 08:09.200
many orders of magnitude less. Yes. So it's good to see that sort of stuff and disappear off the

08:09.240 --> 08:17.240
profile. You should never optimize before profiling, obviously. Cool. Thanks, Will. Storing previous

08:17.240 --> 08:23.560
tiles. Yeah, so we've done a whole lot of work to improve our tile rendering performance. We store

08:23.560 --> 08:27.040
previous tiles that have been rendered so we can see what the difference is and just send the

08:27.040 --> 08:32.800
difference. That saves a lot of bandwidth and reduces latency too. And we've completely rewritten

08:32.840 --> 08:39.240
this. Well, how this is done in the last six months to a year. So we've already compressed it, so

08:39.240 --> 08:44.600
just a simple run length encoding. Because we're extremely modern, instead of doing stupid stuff

08:44.600 --> 08:50.400
like using byte lengths and this kind of thing, we use bit masks. And you'll see why in a second.

08:50.400 --> 08:55.160
So the bit mask essentially says, is the pixel the same as the previous pixel? So you end up with a

08:55.160 --> 09:01.520
bit mask. We have 1056 square tiles. So in four 64 bit numbers, we can have the whole bit mask for

09:01.840 --> 09:08.200
the row. And yeah, it's pretty easy. This removes a whole load of things. Previously, we stored them

09:08.200 --> 09:12.840
uncompressed. We compared them uncompressed. Turns out to be massively slower. Touch is much more

09:12.840 --> 09:20.240
memory. It uses much more space. And we also did clever things to hash each row as we did that

09:21.360 --> 09:26.080
while we were copying. But it turns out this is far better just to use the bit mask and some of

09:26.080 --> 09:31.600
that stuff. And, Koel and I did this fun thing with AVX2. Why not? You hear about these processor

09:31.600 --> 09:37.240
accelerated things and after shrinking our inner loop down to almost nothing, it's still not as

09:37.240 --> 09:42.520
quick as it could be on the CPU. So this is how we do it. We load a whole load, actually eight pixels,

09:42.520 --> 09:48.760
into a whole single AVX register, which is just kind of nice, right? Eight pixels at a time. And

09:48.760 --> 09:52.600
the problem is we need to compare it with the previous thing. So we shift a bit off the end. We

09:52.800 --> 09:59.680
shove the previous one. We shift it along, although actually it's really a sort of, yeah, it's a

09:59.680 --> 10:05.120
crossbar switch here that you permute to move things. There is no shift in AVX registers that

10:05.120 --> 10:10.520
does that. And then we just compare these guys. And that gives you a whole load of either whole

10:10.520 --> 10:16.800
all ones are all zeroes. And then comes Koel on magic trick. Well, yeah, in AVX, there's the AVX2,

10:16.960 --> 10:22.640
which is like practically available. But AVX512, which is not practically available, has a particular

10:22.640 --> 10:28.000
call that you can do that will compare the two things for you and give you that bit mask, which

10:28.000 --> 10:35.760
is not available in the AVX2. And if you look at what's available, though, you can guess if it was

10:35.760 --> 10:40.520
done in floats, then the number is basically available for you. So you cast it to floats, and

10:40.520 --> 10:46.400
you do this move mask thing brings your top bits in and gives you what you were hoping for in the

10:46.440 --> 10:51.160
first place, which is just an individual bit result for each individual pixel that you've

10:51.160 --> 10:56.200
compared, whether they're true or not. And you can basically so compress, pull the bits you're

10:56.200 --> 11:01.200
looking for out in no time. It's great. Which is pretty awesome. So, you know, you convert this

11:01.200 --> 11:05.000
into a floating point number, and you get the sign out of it. And that's your that's your

11:05.000 --> 11:09.800
orally bit mask. So the nice thing about this is there's no branch, there's no compare. There's

11:09.800 --> 11:14.040
nothing. There's a simple flat loop with about five instructions. At the end of that, we then

11:14.080 --> 11:17.560
have to work out how many pixels to copy because it's all very well saying these the same, but

11:17.560 --> 11:22.760
you need individual copies of those different pixels one after another. So a bit of a pop count

11:22.760 --> 11:29.640
will count the bits in the mask. And then with a clever lookup table, we can also use this. Yeah,

11:29.640 --> 11:34.240
this clever instruction shuffling instructions to shuffle the things in that we need to copy them

11:34.240 --> 11:41.720
out, stack them up. Bingo, twice as fast, which is nice. And hopefully AVX512, you know, will make

11:41.760 --> 11:47.680
it even even faster if you believe that you'll believe anything. So yes, here we go. So this is a

11:47.680 --> 11:53.640
real problem here. And if only we can find the idiot responsible for you. We don't need to

11:53.640 --> 11:59.080
suggest. Yeah, no, what's sometimes interesting is that, while I said earlier narrow was better,

11:59.080 --> 12:06.440
sometimes it can be interesting to see that wider will be better in the sense that when you look at

12:06.440 --> 12:10.920
the flame graph, what you should see is individual threads should all be positioned separately.

12:10.960 --> 12:15.720
So they shouldn't be, you know, combined with the main thread. So if you're not seeing work that you

12:15.720 --> 12:20.840
expect to see happening in a thread on the left hand side, basically, of your flame graph, then it

12:20.840 --> 12:24.920
means the threading isn't being used. So it becomes apparent that while there's this code that attempts

12:24.920 --> 12:31.200
to do this threading for doing this previous delta stuff, there is no existence of the threads and

12:31.200 --> 12:36.560
there's a flaw that needs to be sorted. So when you fix the flaw for the threading and bring it back

12:36.600 --> 12:42.280
in, you see then on the far left hand side, because it's rooted in the threading area, all that work

12:42.280 --> 12:46.960
is put on the left hand side separately in the flame graph. And while it's wider, it now means

12:46.960 --> 12:52.240
it's operating in a separate thread and you've made progress. So it's nice to get twice as fast and

12:52.240 --> 12:56.520
then four times as fast on top of it. That's the right sort of approach. Yeah, I think we're going

12:56.520 --> 13:01.560
to skip through some of these because we're running out of time. But working out where to do the work,

13:01.560 --> 13:05.720
either in the browser or not, and I'm pretty multiplying and the stupidity of the web and

13:05.760 --> 13:13.000
having an RGB, un-premultiplied alpha API. When it's almost certainly going to be premultiplied

13:13.000 --> 13:15.800
underneath its hood. Yeah, underneath the hood, all the hardware, everything is doing

13:15.800 --> 13:19.280
premultiplying because it's so much quicker. You can see the complaints online about people

13:19.280 --> 13:23.360
pushing RGBA into the canvas and getting something out that isn't the same because it's been

13:23.360 --> 13:30.800
premultiplied and then un-premultiplied. Anyway, there you go. The web APIs are awesome. What else?

13:31.760 --> 13:37.760
What should be on your profile? Well, it's very hard to know. This could be okay. Here's a whole

13:37.760 --> 13:42.960
lot of un-premultiplication here. It's a very old profile. It's a time, but hey, there's a lot of

13:42.960 --> 13:47.680
rendering on the profile. Not very much painting, lots of delta ring, so we fixed that. But actually,

13:47.680 --> 13:51.560
it's very hard to know if this is good or bad looking at that. Actually, with lots of bogus

13:51.560 --> 13:56.880
invalidations, you start to see lots of rendering and that's not what you want. So everything

13:56.920 --> 14:01.360
should shrink and you'll end up with a profile that looks the same, but everything feels much

14:01.360 --> 14:05.600
quicker. So we've done lots of work to shrink, I guess. Mr. Enders, do you want to pick a couple

14:05.600 --> 14:11.960
of these now? Yeah, just as you mentioned, with multiple user document tests, we have kind of

14:11.960 --> 14:16.240
basically monitor what's happening. People are joining documents. We got that full document

14:16.240 --> 14:22.360
invalidation we mentioned about happening. Clicking in headers and footers were causing the same

14:22.360 --> 14:26.640
things. I think fundamentally, because the invalidations and redrawing on the desktop has

14:26.640 --> 14:31.240
become so cheap, while in the past, the very distant past, we might have been pretty good at

14:31.240 --> 14:37.000
keeping validations down. In that case, we've become slack in recent decades and now we've

14:37.000 --> 14:41.000
treated it as cheap and that has affected things. So let's kind of have a look at that again and

14:41.000 --> 14:46.360
bring things down to smaller rendering areas and less invalidations. Yeah, and the good news is

14:46.360 --> 14:51.200
that improves LibreOffice, of course, as well. It's more efficient and clean on your PC as well

14:51.240 --> 14:56.360
underneath. So good. We've done lots better latency hiding in terms of more aggressive

14:56.360 --> 15:01.120
prefetching. So the next slide is there before you switch to it. So it's absolutely instant.

15:01.120 --> 15:05.840
Hiding latency in those ways is quite fun, enlarging the area around the view and maintaining

15:05.840 --> 15:10.760
that as tiles and just storing and managing much more compressed tile data in the clients that

15:10.760 --> 15:17.240
we manage much better now. This is a fun one. But we don't have much time for it. Yeah, well,

15:17.280 --> 15:22.360
God, classically, standard list and C++ was always a standard list. And if you wanted to get

15:22.360 --> 15:27.040
the size of it, you had to like pass the entire list from start to finish. That was sorted out

15:27.040 --> 15:32.320
decades ago. But for whatever reason, for compatibility purposes, if you use the particular

15:32.320 --> 15:36.960
Red Hat developer tool chain, then you seem to get the classic behavior or standard list back

15:36.960 --> 15:43.120
again. So when we were assuming that you was cheap and cheerful to get the length of a standard

15:43.160 --> 15:47.920
list, it turns out to be not the case with this particular case. So you have to go back to a

15:47.920 --> 15:52.160
different approach and it appears in your profile like that. But again, it looks normal that it

15:52.160 --> 15:56.600
should take some time to draw things. And it's normal to have a cache to speed that up. But if

15:56.600 --> 16:01.800
the cache has got 20,000 items in it, and you're just walking this list, you know, point it,

16:01.800 --> 16:08.080
chasing anyway. So gone. Oh, fun stuff. Like why not have a massive virtual device in the

16:08.080 --> 16:12.160
background that you could render to instead of the whole document every time you do something? Not

16:12.200 --> 16:16.960
great. Or another one, why not have a benchmark every time you start the document to see how fast

16:16.960 --> 16:22.080
rendering is, allocate a whole load of memory and dirty it, you know? Great. Yeah, trying to cache

16:22.080 --> 16:26.160
images. So we didn't bother catching compressed images because they're compressed, right? So why

16:26.160 --> 16:30.280
bother? They're small. They're good to have memory, except TIFFs not so much compressed, you know,

16:30.280 --> 16:37.080
you eventually have the whole massive chunk of memory there. Using G-Lib C trimming functions

16:37.240 --> 16:45.440
on idle to reduce memory usage. Yeah, trying to get better measurements of various things. Yeah,

16:45.440 --> 16:51.760
this is a fun one. Well, oh, this is the S-Maps word. Yes, yes, yes, we're reading the proc S-Maps

16:51.760 --> 16:57.360
to see how much memory we're using. And the classic S-Maps has got multiple entries in it for many,

16:57.360 --> 17:04.600
many parts of your process. So you just read multiple lines. So there's a relatively new one

17:04.640 --> 17:10.680
that has it all pre-edited for you. ProxMaps roll up, which is exactly what we want. Same code to

17:10.680 --> 17:15.160
read the previous one should work with the new one. Then apparently we're running out of memory,

17:15.160 --> 17:18.720
or it's being reported that we're running out of memory, and it's all very, very bizarre. You

17:18.720 --> 17:23.440
can't proc S-Maps roll up yourself. The numbers are good. There's something very odd, but it turns

17:23.440 --> 17:27.520
out that if you seek back to the beginning and then read again, that the numbers double every

17:27.520 --> 17:31.920
time you do this. There's an actual bug in the original implementation. It's not there in the

17:31.960 --> 17:39.320
kernel, my version 6 kernel, but it is there on version V18 or 16 that the servers were applied

17:39.320 --> 17:44.280
on. So you have to be just the right version for it to appear. So Linus fixed it, thank God.

17:44.280 --> 17:48.040
Quillholt found it. Well, it was fixed before we found it. But it's always nice to know you have

17:48.040 --> 17:52.440
to check your kernel is the right, you know, is the quality kernel before you start asking it

17:52.440 --> 17:57.440
how much memory it's using. Yeah, hunspell in the loop was almost entirely dominated, not by

17:57.480 --> 18:01.800
actually spelling things, but by looking at the time. You know, I'm sure in a bad talk, you know,

18:01.800 --> 18:06.960
it's quite similar. But that's a little bit unfortunate. So yeah, some improvements there.

18:06.960 --> 18:11.480
And lots of other things, graphs showing speedups. We've got to get to usability in the last minute.

18:11.480 --> 18:16.640
Let me whizz through this then. Here we go. Accessibility, dark modes, pretty pictures. This

18:16.640 --> 18:22.160
is going to be fast. Keyboard accelerators. This is all of the good stuff for people. Screen reading,

18:22.160 --> 18:27.420
and all sorts of nice things, videos of that. Better page navigators at the side so you

18:27.420 --> 18:32.500
can see where you're going. And lots of just little bits of usability polish, a nice font

18:32.500 --> 18:37.980
previews. Was this your page number thing? I forget who did that. Making it easier to insert page

18:37.980 --> 18:42.780
numbers so people can see, you know, what's going on easily, better change tracking and showing

18:42.780 --> 18:50.060
changes, AI, depot, stuff, and hey, some some. The good news is there's more opportunity for

18:50.060 --> 18:55.580
performance improvement. So we're still, we're still having fun. You know, hey, come join us.

18:55.740 --> 18:57.180
There's some cool play files to read.

19:00.180 --> 19:01.260
Right. Well, yes.

19:03.740 --> 19:08.020
At the moment, in Calc, when you're typing the entire row and validates beyond the right

19:08.020 --> 19:11.100
hand side of where you're actually typing. So we brought that down to the self in the most

19:11.100 --> 19:15.180
generic case, but it's not done for writer. In the writer case, if you're typing, we are

19:15.180 --> 19:19.420
invalidating all the way to the right hand side of the screen. So we'll bring shrink back back

19:19.420 --> 19:23.500
down again. We have some new metrics that we've included in that debugging overlay thing that

19:23.500 --> 19:27.740
give you an indication of, you know, how much of these updates that are coming through are the same

19:29.180 --> 19:33.740
data as they were before the update came through and the numbers are staggeringly high. So there's

19:33.740 --> 19:38.780
plenty of room for improvement to validate less, send more data down. So what we have now is

19:39.740 --> 19:47.980
fix, uh, approval. Yeah. The moment that's always been troublesome in, uh, Lear Office is the treatment

19:47.980 --> 19:53.020
of the alpha layer. We picked the wrong direction than everybody else does. Everybody else picks

19:54.060 --> 19:58.460
transparency. We picked opacity or vice versa. So we have the opposite direction. Everybody else

19:58.460 --> 20:02.940
would want to actually output something in the real world that handles transparency. We have to

20:02.940 --> 20:07.260
like reverse our transparency. So that's problematic. That's, that's now fixed. That one is fixed.

20:07.260 --> 20:11.900
That one is fixed. But then we've also kept our transparency layer in a separate, uh,

20:12.700 --> 20:17.580
bitmap, a separate buffer than an actual bitmap. And if we put them together someday, that would

20:17.580 --> 20:21.740
make things a lot easier, I believe. Yeah. It's the Windows 16 API decisions that are still with

20:21.740 --> 20:26.780
us. But anyway, we're getting rid of them quickly. That's great. Um, yeah, performance regression

20:26.780 --> 20:30.860
testing with Valgrind, uh, pipeline loading. So at the moment, oh, we got five minutes. Oh,

20:30.860 --> 20:35.660
look at that. Fantastic. I went too quickly. No, you're doing fine. Okay, right. Fine. Excellent.

20:35.660 --> 20:41.340
I think we're nearly the end. Um, so pipeline loading. So at the moment we have, um, we, we

20:41.340 --> 20:45.180
essentially fetch a, fetch a webpage that passes all the credentials we need to check ourselves.

20:45.180 --> 20:51.180
We'd load lots of JavaScript. We open a web socket. Then do we actually see if we can actually load

20:51.180 --> 20:55.580
the document and start checking who the user is? This is really foolish. I'm taking on a first start,

20:55.580 --> 20:59.900
we can be, you know, checking the user, downloading the document, even loading the document ready

20:59.900 --> 21:05.340
to get the web socket and then have a pre-rendered version. So this, this is very substantially

21:05.340 --> 21:10.700
reducing, um, startup time to make it incredibly quick. You already have a huge advantage that you

21:10.700 --> 21:15.420
have a real server at the back end and you're not having to jit, you know, millions of lines of code

21:16.140 --> 21:20.460
in your browser from JavaScript into something or, you know, web assembly into something. Um,

21:20.460 --> 21:25.580
so it should be just amazingly fast. And so this is a great way to, to speed that even further.

21:25.580 --> 21:29.340
And, you know, and a real server, you may have a time share, but you know, when you arrived

21:29.340 --> 21:33.500
your server, it's probably not doing much. In fact, the CPU cost on most of our servers is

21:33.500 --> 21:37.580
extremely low. So, you know, there's suddenly all these threads ready to render your document and

21:37.580 --> 21:42.700
get, get stuff to you quickly. Say some good things. And Valgrind, we've done a whole lot of work to

21:42.700 --> 21:48.380
get, um, it to run nicely under Valgrind with our privilege model and container model. It's a bit

21:48.380 --> 21:54.060
of a problem. Uh, and so we have some code now that turns everything into one process. So you

21:54.060 --> 21:58.460
can load and collaborate on one document and automate that, but you can run it in, in Valgrind.

21:58.460 --> 22:03.980
And why do you want to do performance profiling in Valgrind? It seems like a retro, uh, poly,

22:03.980 --> 22:09.900
right? But the beautiful thing about Valgrind is the simulated CPU. So anybody can run the same

22:09.900 --> 22:15.980
workload on their machine and between two runs, it's the same thing. And Valgrind luckily doesn't

22:15.980 --> 22:21.260
have a simulated thermal management system that randomly throttles your CPU, uh, performance. And

22:21.260 --> 22:24.540
it luckily doesn't have people screwing with your cache memory and running cron jobs in the

22:24.540 --> 22:29.500
background and, you know, thermally recalibrating your disk and all this other stuff. So what you

22:29.500 --> 22:35.020
discover is that between two identical commits, you're getting fractions of a, small fractions of a

22:35.020 --> 22:40.060
percent difference in the Valgrind numbers, which is beautiful because performance tends not to go

22:40.140 --> 22:46.700
away in big jumps. Like we can, it can go in big jumps, but it tends to go slowly downhill. And if

22:46.700 --> 22:52.140
the noise is bigger than the slow downhill, you've no idea where the problem is. So much better to

22:52.140 --> 22:56.780
have a little series of steps going down in one half a percent at a time and go, hey, we get rid of

22:56.780 --> 23:02.140
that and that. And did you realize and, uh, so, so this is really vital. And LibreOffice uses this

23:02.140 --> 23:07.820
on its perf, um, automation has been beautiful web pages with graphs. Um, and we'll, we'll be applying

23:07.900 --> 23:12.300
to, to collaborate online to, to try and avoid regressions. Yeah. Someday soon. Someday soon.

23:12.300 --> 23:16.060
Yeah. Neil, Neil Lazzone, we think probably. Anyway, anything else?

23:16.060 --> 23:22.540
No, I think we've covered plenty. Well, so, and yes, of course, we can't do anything without our

23:22.540 --> 23:26.460
partners and customers that pay for it all, blah, blah, blah, commercial plug. Good. Yes. That's good.

23:26.460 --> 23:32.300
Job done. And conclusions. Yes. So, uh, computers are unbelievably fast. I mean, like this is

23:32.300 --> 23:36.460
something that you should take home. You know, like the quarter of a nanosecond that your four giga

23:36.460 --> 23:41.020
hertz processor takes is just unbelievable in the scale of a hundred milliseconds plus. It takes

23:41.020 --> 23:46.780
you to blink your eye. It's just fantastically speedy in a way you can't explain. Uh, the network

23:46.780 --> 23:52.700
latency to anywhere almost, you know, you can go three times, uh, London to Frankfurt and back

23:52.700 --> 23:56.860
in the time you can blink, right? Like it's, it's unbelievably fast. In fact, you can go,

23:57.580 --> 24:01.580
you know, Frankfurt, Milan faster than your monitor can refresh, right? So, so like,

24:01.580 --> 24:06.060
it's quite amazing when you start looking at the times of things. Um, architecture is really a

24:06.140 --> 24:11.420
bet on CPUs and networks getting faster and cheaper. Has anyone noticed a trend there? I think

24:11.420 --> 24:14.940
there might be something in that. And, and we're basically racing the hardware guys. I mean, you

24:14.940 --> 24:19.020
know, we, we do stupid stuff, obviously, and then we remove it later. But, you know, the hardware

24:19.020 --> 24:22.700
people are also trying to beat us to run stupid stuff quicker. You know, that's their mission.

24:22.700 --> 24:27.900
And, uh, yes. And, and we extremely smooth. Don't get the feeling that it's bad. Try it. You know,

24:27.900 --> 24:32.300
most of these problems, you'll only start to see them when you have 20 plus people collaboratively

24:32.300 --> 24:37.260
editing in document. So, uh, yeah, it's, it's kind of, it's kind of cool. So give it a try and try

24:37.260 --> 24:41.980
the latest version and see, give us some feedback, get involved. And there's lots, lots of fun to

24:41.980 --> 24:48.780
get involved with. I mean, I don't know. Yeah, I'd like us to play two things. As I mentioned earlier,

24:48.780 --> 24:53.660
the profile that we have for Calc and Writers uploaded to GitHub once a week, generic Calc

24:53.660 --> 24:58.380
performance profile, generic writer performance profile, search on the online GitHub issues.

24:58.380 --> 25:02.220
And you can see all of the, the chats that we've mentioned there in the past. And you can even

25:02.220 --> 25:06.300
see with the progress there and the occasional blip during a call where things go horrifically

25:06.300 --> 25:10.140
wrong and get sorted out in the next one. So yeah, plenty to see and see what we're doing.

25:10.140 --> 25:12.940
There's some links in the slide. You can't see to the profiles and get involved in the Libre

25:12.940 --> 25:21.580
Office of Technology. Thank you. That's it. You've been very patient. Thank you.

