WEBVTT

00:00.000 --> 00:06.200
This is nice.

00:06.200 --> 00:08.000
So hello guys.

00:08.000 --> 00:13.920
Today we prepare a presentation about the house engineering in action.

00:13.920 --> 00:20.160
I am Marj Orsak and this is Henrik Srenching and we both work as a software engineers

00:20.160 --> 00:22.120
in Red Hat.

00:22.120 --> 00:25.920
Today we also prepared a quiz.

00:25.920 --> 00:27.040
You can see the QR code.

00:27.040 --> 00:32.120
You can scan it with your phone and if you are quick enough and you get correct answers

00:32.120 --> 00:35.240
you can win a prize.

00:35.240 --> 00:37.880
So over to you Henrik.

00:37.880 --> 00:42.800
Yeah so the content of the presentation is as following.

00:42.800 --> 00:46.840
We will begin with a brief explanation of house engineering.

00:46.840 --> 00:51.840
Then we will describe how the target systems may actually look like in the production.

00:51.840 --> 00:57.440
Then we will turn our focus on disabling house.

00:57.440 --> 01:03.400
Afterwards there will be two brief demonstrations and then a quick conclusion of how to actually

01:03.400 --> 01:06.400
work with the house.

01:06.400 --> 01:12.640
So when we are thinking about system resilience or application resilience we have to think

01:12.640 --> 01:16.400
about all the components which our application depends upon.

01:16.400 --> 01:20.480
That mean other components and other services.

01:20.480 --> 01:25.200
There is also big dependency on the network and infrastructure.

01:25.200 --> 01:30.120
All of these things are mostly visible in the distributed systems.

01:30.120 --> 01:36.720
There are many known fallacies about distributed systems mostly concerned about network and

01:36.720 --> 01:39.480
bandwidth.

01:39.480 --> 01:46.240
When we will then take a look on a system from the viewpoint of many instances and services

01:46.320 --> 01:53.920
which have to communicate with each other in order for the system to work great.

01:53.920 --> 02:00.440
We will come to the problem of complexity and the fact that there is possibly no single

02:00.440 --> 02:08.320
person that can understand the system completely and every state which can the system get into.

02:08.320 --> 02:14.320
So what can happen and what will probably inevitably happen in the system of such magnitude

02:14.400 --> 02:19.080
is the thing that one instance or more will crash.

02:19.080 --> 02:26.080
This is the story about house monkey which I guess some of you may be familiar with

02:26.080 --> 02:33.840
but all we have to know so far is that it is some of first house tools which just randomly

02:33.840 --> 02:40.200
kill some instance in the production and it will force engineers to take proactive action

02:40.200 --> 02:43.280
to make system more resilient.

02:43.280 --> 02:49.920
We can take this step further and bring down not just few instances but availability zone

02:49.920 --> 02:58.360
or cluster or bring any kind of network traffic and get the system into the state we are not

02:58.360 --> 03:02.440
so comfortable in for the production environment.

03:02.440 --> 03:06.640
So we will get to the definition of the house experimenting.

03:06.640 --> 03:11.560
It's experimenting on a system in order to build confidence in the system's capability

03:11.640 --> 03:15.240
to withstand turbulent condition in production.

03:15.240 --> 03:22.120
This may sound weird for us because why would anyone want to bring the house into the production

03:22.120 --> 03:30.600
isn't it something funny which should we actually avoid and the real reason for doing so is

03:30.600 --> 03:33.720
actually it's the time difference.

03:33.880 --> 03:41.800
It's much more easier to solve the problems at 4 a.m. or 4 p.m. rather than 4 a.m. when you are

03:41.800 --> 03:47.400
under the high pressure from the customers to solve the problems.

03:47.400 --> 03:53.320
There are many principles which we have to abide or we should abide in the house engineering.

03:53.320 --> 03:59.960
The first one and most important one is minimal blast radius for each experiment you conduct.

04:00.040 --> 04:05.640
We should imagine some red button for each experiment which should be able to stop it in case

04:05.640 --> 04:07.960
anything goes wrong.

04:07.960 --> 04:14.280
Other principles are mostly focused on the same thing like we would test the thing in the real

04:14.280 --> 04:19.800
life. We want to focus on how it actually works in production. We want to make sure it works

04:19.800 --> 04:24.440
correctly and we want to introduce the problems that may happen in the real life.

04:24.440 --> 04:29.960
The last principle is that it's a continuous run which is basically about running these tests

04:29.960 --> 04:34.280
or experiments each time for as possible and as effortlessly as possible.

04:35.480 --> 04:38.120
Now over to the target system.

04:38.120 --> 04:45.080
This all started with the monolith architecture where we get one box, one backend, one database

04:45.080 --> 04:53.320
and one UI. In terms of the complexity it was quite low. You simply get some users

04:53.320 --> 05:00.200
connection and the server complexity or overwhelming was not so high. Then after some time you add

05:00.200 --> 05:05.960
some customers more and more like let's say not four or five thousand and the load was pretty much

05:05.960 --> 05:13.160
high and the server would immediately crash for instance. So such architecture is really hard to

05:13.160 --> 05:19.000
scale horizontally and one way how to tackle this problem is scale vertically but you can scale

05:19.000 --> 05:20.680
vertically all the time.

05:21.400 --> 05:27.640
The second point was that the fault-or-ency of such architecture is really bad. You just

05:27.640 --> 05:33.000
target one node and the server just immediately crash and the users will be really sad because

05:33.000 --> 05:41.720
they don't get any response. So then Dockercams with the microservice architecture where all these

05:41.800 --> 05:52.440
previous improved we got portability isolation. We somehow get better horizontal scaling but in

05:52.440 --> 05:57.160
case when you have like thousands of instances it would be quite hard to manage all of these

05:57.160 --> 06:02.680
containers. On the other hand also the complexity here increased because of the network trickle

06:02.680 --> 06:11.320
and more. And so Kubernetes came to solve scalability in terms of the horizontal and

06:12.440 --> 06:19.000
in the Kubernetes you easily if you want to have one replica of the system you just type in the

06:19.000 --> 06:27.960
semi-ammo file, apply it and the Kubernetes will do it. Then if you see your server crashes or

06:27.960 --> 06:32.840
somehow is overloaded with the request you just simply set it to free and the Kubernetes will

06:32.840 --> 06:39.320
do it. The same with the fault-or-ency where you just I don't know inject some disruptions or

06:40.040 --> 06:45.320
something else in the pods. The one will still be up if you only target two of them.

06:46.840 --> 06:56.200
But still complexity increased again. And so we are in the operator stage where no one

06:57.080 --> 07:03.800
can entirely grasp the system in terms of the behavior. And I want to present one of the such

07:03.800 --> 07:13.400
operators is the StreamZ. StreamZ is basically Apache Kafka on its core with encapsulation in

07:13.400 --> 07:22.200
the Kubernetes system. On top of that you got some operators which simplifies some upright

07:22.200 --> 07:27.400
dynamic configuration. It is tracing more security involved also Grafana dashboards.

07:28.120 --> 07:34.040
And it is part of the cloud native computing foundation. But it's quite tough too much

07:34.040 --> 07:40.120
unknowns right? So let's break this down. So Apache Kafka it has a lot of buzzwords as you can see

07:40.120 --> 07:47.080
public subscribe model it is messaging system and so on but still this doesn't help right?

07:48.040 --> 07:55.080
So let's move to the basic of the Kafka. We got some producers not these ones but

07:56.600 --> 07:59.960
we get some clients right? So these clients sending messages to the broker.

08:00.920 --> 08:06.440
They are happy because the connection is up. We could also set system scale. We could create

08:06.440 --> 08:14.040
another Kafka broker set up some listeners and another one. We got a second set of the clients

08:14.040 --> 08:19.560
which are called consumers and they are simply receiving this data. So we got this really

08:20.840 --> 08:25.720
example of the system where you have producers and consumers but we need also some representation

08:25.720 --> 08:34.600
of the data which is Kafka topics. Also each Kafka broker has his own configuration and you can

08:34.600 --> 08:41.800
basically set up versions set up in center replicas but this is not important for this talk. So

08:42.280 --> 08:48.280
we got a lot of buzzwords as you can see but unfortunately or maybe fortunately we don't

08:48.280 --> 08:54.600
have time for it. So we could stick with this model now. So we got the producers we got the

08:54.600 --> 09:01.880
consumers we got some brokers which are the servers and what if we encapsulate system

09:02.600 --> 09:08.200
in the Kubernetes? Now top of that we add some operators managing the Kafka ecosystem

09:08.600 --> 09:17.640
and on top of that we have the operators and this is basically Stream Z. Really complex right?

09:20.040 --> 09:29.640
So here we can see an example of the deployment of the Stream Z where you got a lot of connections.

09:30.200 --> 09:36.040
These components are not like really important now. The main idea here is that even with this

09:36.200 --> 09:43.720
low deployments you get a lot of things where you can inject the hairs. So

09:45.400 --> 09:51.480
now I want to say that if we go to the production one of such production is the

09:51.480 --> 09:56.760
scale job and before I dig into it I want to thank these guys because without them we would be

09:56.760 --> 10:04.200
unable to run such hairs in such a massive survival scale. So as I said a scale job is

10:04.200 --> 10:09.480
the production environment for Stream Z and other projects and there are a lot of

10:11.960 --> 10:16.680
technologies involved such as I don't know Tecno pipelines, Teno's Promi to use, Grafana,

10:16.680 --> 10:26.600
Loki logs and more. And here you can see a basic example of how we basically induce the hairs here.

10:26.600 --> 10:31.720
Here we have some Kafka clients, Streams produces consumers with some databases which are

10:32.120 --> 10:37.480
communicating with Kafka Connect. We have some middle maker which is transfer data from Kafka A

10:37.480 --> 10:44.680
to Kafka B but still this is not intention of this slide. There are a lot of connections.

10:45.560 --> 10:53.400
So I think over to you, Henrik. Thanks. So the point of these slides were actually to show

10:54.200 --> 11:00.040
or somehow explain that when we come to the system and take a first look it may look quite

11:00.040 --> 11:06.600
MSC and quite complicated. We may not understand the whole underlying technological stack or every

11:06.600 --> 11:15.080
single components and we are in the position when we want to talk about how the system actually

11:15.080 --> 11:19.720
behaves when we introduce house whereas we are not even sure how it should behave normally.

11:21.480 --> 11:29.480
That's even increased by the fact that the system doesn't behave how it is in paper but in

11:29.480 --> 11:36.280
actuality there are countless of instances and connections, operators, clients, network traffic.

11:36.840 --> 11:40.680
We need to have some sort of observability and some intuition in the system.

11:41.960 --> 11:47.720
Like in other presentation that were before us there were already some mentions about Prometheus

11:47.720 --> 11:54.360
and Grafana. They are quite famous for their purpose. So we will be using them as well.

11:54.760 --> 12:02.840
As mentioned we need to have some intuition about the system and how it behaves. Without that it is just a mess.

12:04.360 --> 12:10.280
So we actually want to introduce some chaos into the system so we start a search for the

12:11.080 --> 12:15.800
problematic parts of the system or where we actually what we actually want to focus on.

12:16.360 --> 12:22.920
It is a simple process when we take a basically simple look on the system. Take a look what is

12:23.000 --> 12:29.240
critical component, where are some possible bottlenecks, are some part of the network really

12:29.240 --> 12:35.240
critical here, are there some real world events that can cause my system to be vulnerable for

12:35.240 --> 12:41.160
some time like some rolling updates or some notes we start in the cloud and things like that.

12:42.200 --> 12:46.760
What would be really helpful is to collaborate with all the people involved in the system.

12:46.760 --> 12:52.520
Like we definitely need some input from the devs. We need to know about at least some basic

12:52.520 --> 12:57.480
information about architectural component. What we may come up with is some simple document

12:57.480 --> 13:05.560
describing all important parts or things that may occur there or protocols that are included

13:05.560 --> 13:10.920
and we will naturally come to the important configuration parameters and maybe even some

13:10.920 --> 13:17.800
proposal for simple chaos that could be included. So the output of this in reality is some first

13:17.880 --> 13:23.720
look on part of the system which may be actually targeted for simple chaos. Now that we know

13:25.080 --> 13:30.520
at least have some first insight what could be first, what could be our first guess to start

13:30.520 --> 13:39.000
with the chaos, we may focus on concrete chaos and we may start with some simple experiments.

13:39.880 --> 13:45.560
Now how to actually formulate some kind of hypothesis or some sort of experiment when we

13:45.560 --> 13:54.760
will take a look on specific thing. We will take a look on just part of the system or few components.

13:55.320 --> 14:05.000
Now we will decide to make sure that our core part of the system is actually capable to withstand

14:06.040 --> 14:12.920
some instances being lost or have some failures. Because this is still a production environment

14:12.920 --> 14:20.280
and although it was even in the main principles of the chaos, we don't want to start with chaos in

14:20.280 --> 14:28.600
the production environment. I guess everyone here knows why because first intern will try to introduce

14:28.600 --> 14:35.800
some chaos, he will bring down all instances, service will not be available for for holiday and

14:35.800 --> 14:42.200
good luck explaining that to your boss. Now we will probably start in a smaller scale,

14:42.200 --> 14:48.680
in a stage environment with much smaller traffic, with much smaller stakes like let's say there will

14:48.680 --> 14:58.840
be some some client maybe just random random fraction. We will have some few instances and

14:58.840 --> 15:06.280
few controllers. We will start by making sure that system is in a steady state. We have our

15:06.280 --> 15:12.520
instances up and running. When we are sure about it we can introduce the chaos. When we introduce the

15:12.520 --> 15:18.840
chaos, instances goes down and afterwards the system stabilizes by again bringing down the

15:18.840 --> 15:24.840
instance. During all the time we are observing all important metrics and parameters about the system.

15:24.840 --> 15:30.840
For example it could be messages per second. Now that all that is set and done we can actually

15:30.840 --> 15:39.080
implement our chaos. What can be really helpful for this are chaos tools. We will not be describing

15:39.080 --> 15:46.280
all of them but simply mention like there is chaos mesh, Kraken, or it moves or some other choices.

15:47.640 --> 15:52.920
They will help with definition, evaluation, execution and all the other stuff. We will end up

15:53.000 --> 16:02.120
with very simple email files to be executed. Now we can actually execute our chaos and see

16:02.120 --> 16:06.760
everything went as expected. There was small decrease in the traffic but overall the system

16:06.760 --> 16:13.560
got to the desired state after a while. Okay this was first experiment in the stage. Everything

16:13.560 --> 16:19.800
went great. We've got the good feeling of resilience being confirmed in our system

16:20.520 --> 16:26.200
but what we are supposed to do now is to repeat the experiment, scale it a bit up,

16:26.200 --> 16:32.520
go into the production, really try, really make sure that it is this production environment where

16:32.520 --> 16:38.440
we will get the confidence. What may happen is that it will not at all go according to the plan.

16:39.080 --> 16:45.080
It will fail miserably and this is also the reason why we should scale these experiments a bit slower

16:45.080 --> 16:50.920
and this is also the reason why we eventually want to make them in production because we want to

16:50.920 --> 16:55.880
really make sure that this environment which is so important for us is actually able to handle that

16:55.880 --> 17:07.400
problem. So as I said no reason to despair just keep on and try in the stage and make it and start

17:07.400 --> 17:14.600
slowly definitely. So to the demos. Okay so guys today we prepared two demos for you.

17:15.400 --> 17:23.800
The first one is the broker's failure. Here we will target the Kafka Pots. We have seven replicas

17:23.800 --> 17:30.040
of the Kafka and we will be targeting the three of them. The observability or the metric would

17:30.040 --> 17:37.560
gather would be like throughput, CPU memory and the traffic in the Kafka Pots. Then we will also

17:38.440 --> 17:45.160
define some steady state which is basically that all brokers and client replicas would be like okay

17:45.160 --> 17:52.360
and the communication throughput will be stable even we inject the chaos and if we define the

17:52.360 --> 18:00.360
hypothesis it would be like we will be eliminating three of the Kafka Pots and this would not

18:02.360 --> 18:08.440
eventually do some cascading failure and we will be okay like user will be not affected with this.

18:08.920 --> 18:16.040
So yeah and also we will have some checks on the producers and Kafka Pots. So let's move on the

18:16.040 --> 18:24.760
demo and hopefully it will somehow work. So okay so here we got some setup. We have Kafka cluster.

18:24.760 --> 18:34.120
We have some notes. We have producers. Here is defined Pots hails. We have ModFix which we

18:34.120 --> 18:40.200
targeting the value of three. Three means that we will be targeting three Pots which will be unable

18:40.200 --> 18:47.000
to run and duration for this would be three minutes. So let's try with our script to inject the hails.

18:49.320 --> 18:56.760
Yeah so now we are injecting the hails and we see that all Pots that three of them would be not

18:56.760 --> 19:02.680
running. We would move to the graph on a dashbox where we have some metrics. Here's some really

19:02.760 --> 19:11.000
simple not production ready messages per second as you can see. Now you can see here at the

19:11.000 --> 19:17.000
decrease immediate decrease of connections. There will be also decreased the average of the messages

19:17.000 --> 19:26.440
but Kafka would recover even when the Pots are down. So here's the decrease but after a time we

19:26.440 --> 19:37.240
would see that it eventually recover somehow. Yeah and as we can see also we got some brokers on line

19:37.240 --> 19:45.480
four. It's correct now. There are also some under replicated partitions. Yeah Kafka is okay now

19:47.640 --> 19:53.800
and after this experiment will be done. I think it could be

19:56.600 --> 20:04.120
done. Yeah so now we are do the checks. We are checking the stream Z Pots at Kafka which are

20:04.120 --> 20:13.240
just internal custom resource of stream Z and now the Kafka Pots are ready. We're completed and also

20:13.240 --> 20:19.720
in the Gavana dashboards we will see that brokers will go online. The under replicated partitions

20:19.720 --> 20:31.240
we all go to the zero. I think so in the yeah and here it is. Okay so this was the first demo

20:32.520 --> 20:34.520
and we got also the second one.

20:37.160 --> 20:47.720
Yeah and this is basically a worker node node crash and to quickly somehow describe it the topology

20:47.800 --> 20:52.280
is that we have the producer we have the Kafka AI Kafka B with some consumer and in the middle

20:52.280 --> 20:57.320
there is Kafka mirror maker which just basically transfers data from Kafka A to Kafka B.

20:59.080 --> 21:04.360
The steady state again is that all services are fully available and ready to accept traffic.

21:05.400 --> 21:11.240
We made the hypothesis that eliminating one of the Kubernetes worker spools will not bring any

21:11.240 --> 21:18.280
down services and also the producers and consumers will be not affected. They will be simply

21:19.240 --> 21:26.120
sending some messages without any harm. So let's move to the demo two. I will show you the important

21:26.120 --> 21:32.760
things. So we got source Kafka cluster, target Kafka cluster, mirror maker, we have some work

21:32.760 --> 21:43.000
nodes, we inject the chaos. We would also create the continuous clients, person, consumer and

21:43.000 --> 21:49.880
that's for the correctness that all messages are sent and also receive without any harm. There will

21:49.880 --> 21:58.600
be no connection refuse or something like that. So we now reset or crash the worker node. We will see

21:58.600 --> 22:06.360
that the worker node will move from the ready state to not ready. Here it is, it's not ready,

22:06.360 --> 22:10.680
but clients are successfully and happy with the sending and receiving messages.

22:13.800 --> 22:18.760
The script is just checking that worker node is still not ready and we are waiting for recovery.

22:20.520 --> 22:25.720
It would take some time. I think it should be a worker.

22:29.240 --> 22:35.720
And so now worker nodes just move to the ready state. We can see that all containers which

22:35.720 --> 22:42.280
were affected on the specific node would be creating again, producing consumers still sending and

22:42.280 --> 22:53.400
receiving messages. We do some checks on again, again, the stateful sets. Yes, this is okay. We

22:53.400 --> 22:59.640
target cluster, cluster, recovered also. We're also doing checks for mirror maker.

23:02.440 --> 23:09.960
And the script just runs successfully and we are happy.

23:13.000 --> 23:22.520
Okay. So I think that was two of the demos. And last words over to Henry.

23:23.480 --> 23:32.840
Yeah. So as you could see in the demonstration, the benefits of the chaos or execution of the chaos

23:32.840 --> 23:42.920
was a bit different from testing we are used to. There was quite a big hype about the house

23:42.920 --> 23:50.520
engineering and possible benefits it can bring to our organization. Yes, it can definitely reveal

23:50.520 --> 23:56.920
bugs in the production. You can drastically improve the actual performance there or the

23:56.920 --> 24:05.880
situation in the cluster regarding the resilience of the system. But what is the main benefit

24:05.880 --> 24:12.920
of doing such a thing is getting confidence in the system, finding the misconfiguration. Those of

24:12.920 --> 24:19.720
you who have tried running application in Kubernetes know how important it is to have all volumes,

24:19.720 --> 24:25.640
all liveness and readiness checks set correctly and overall infrastructure set in place.

24:27.960 --> 24:36.520
The actual greatest benefit is actually is in fact getting experience and new knowledge about the

24:36.520 --> 24:45.640
system and really understand how it is supposed to work. This is not a holy grail as I said and it

24:45.640 --> 24:52.680
can be a bit disappointing for some. But if we think about the house engineering as natural a step

24:52.680 --> 25:02.840
above the other testing and not their replacement, we can see a great benefit in it. So how we can

25:02.840 --> 25:09.960
actually embrace it in our organization. The very well-known concept is game days when we will

25:09.960 --> 25:16.280
put together a lot of roles and a lot of people from our organization, introduce some kind of chaos

25:16.280 --> 25:22.040
and let them handle it in some reasonable manner where they can all communicate, all contribute

25:22.040 --> 25:28.840
and fix the problem in reasonable time. So that's why I do a friendly way how to start with it.

25:29.800 --> 25:35.160
Know your tools. I know it can be overwhelming. You could see it even in the demo that we have

25:35.160 --> 25:42.440
to introduce quite a lot of tools in order to run even simple experiments. But once you know the

25:42.440 --> 25:50.600
basics and have some confidence in it, you can really start to make some kind of chaos. We can

25:50.600 --> 25:57.080
really recommend some great books about house engineering, Kafka if you want, but still there

25:57.080 --> 26:03.240
are a lot of tools and it's what is the most important due to that is to definitely start small

26:04.200 --> 26:11.160
don't be afraid to set up some stage environment where you can actually practice and confirm your

26:11.160 --> 26:18.520
hypothesis before you will actually go into the production and start doing mayhem. Thank you for

26:18.520 --> 26:20.280
your attention. Really appreciate it.

26:30.440 --> 26:31.000
Questions?

26:33.240 --> 26:35.880
No time? One question.

26:35.880 --> 26:38.520
Question? Yeah?

26:56.520 --> 27:03.640
Yes, there are. It actually depends. It mostly from practical terms. It mostly start to make

27:03.640 --> 27:08.520
sense only when we are talking about not some kind of monolithic application, but we are actually

27:08.520 --> 27:15.160
deployed on a cloud. It's some kind of microservices architecture. I would say that it does not

27:15.160 --> 27:22.520
depends as much on the size of the system as the fact how much you depend on a customer experience

27:22.520 --> 27:30.840
in a sense. That when will it really be decremental for your system to get into the

27:30.840 --> 27:40.360
chaotic condition. But yeah, thank you as well.

