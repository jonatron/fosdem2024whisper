WEBVTT

00:00.000 --> 00:11.200
We are welcoming David who will talk to us about training OpenSeal Accelerators LLNs

00:11.200 --> 00:13.000
on your phones.

00:13.000 --> 00:25.500
Hello everyone, thank you for coming for the talk.

00:25.500 --> 00:27.280
I will start who I am.

00:27.280 --> 00:28.480
I'm David Heidelberg.

00:28.480 --> 00:38.480
I work on Mesa3D as a developer and currently I'm contracted by Colobora to work on CI.

00:38.480 --> 00:47.600
In free time I work on mobile embedded Linux and Alpine and Debian and PostmarketOS contributor.

00:47.600 --> 00:49.560
Let's go for the topic.

00:49.560 --> 00:57.000
So content, just quick introduction, what these things are about, how it looked before

00:57.000 --> 01:01.600
OpenSeal and these standards we have these days.

01:01.600 --> 01:08.280
What we have now, how it works, I will show you an example with running PostmarketOS with

01:08.280 --> 01:14.120
OpenSeal and what we can expect in the future.

01:14.120 --> 01:22.760
So first, who ever heard about OpenSeal here?

01:22.760 --> 01:23.760
Like most of you.

01:23.760 --> 01:37.720
So OpenSeal allows you to run your C or C++ code on CPU or GPU or specific DSP or FPGA

01:37.720 --> 01:41.800
or whatever you can find.

01:41.800 --> 01:43.040
This is one thing.

01:43.040 --> 01:46.480
Second thing, Mesa3D is part of GPU drivers.

01:46.480 --> 01:53.720
You have all the Linux systems or Linux computers and it also works on Android and Windows.

01:54.680 --> 02:00.040
I think Haiku or something exotic.

02:00.040 --> 02:07.160
And last thing which I use for my talk is TinyGrad framework which allow you to run

02:07.160 --> 02:14.240
stuff like GPT or stable diffusion or other interesting projects which you can run usually

02:14.240 --> 02:16.800
on GPU.

02:16.800 --> 02:24.520
So how it looked before the compute we know right now like QDA, ROKM, OpenCL.

02:24.520 --> 02:32.960
So you could do the computation or CPU but CPU has high overhead because it's meant to

02:32.960 --> 02:41.720
run classical computer programs and not highly parallelized software.

02:41.720 --> 02:50.480
Before you can use OpenGL and with OpenGL you could squash computations into OpenGL workloads

02:50.480 --> 02:58.120
but it was a big hack and work around and some scientists did it but not widely used

02:58.120 --> 03:01.640
between people.

03:01.640 --> 03:03.520
So currently we have options.

03:03.520 --> 03:11.400
We have already CPUs with multiple cores and multiple threads but the overhead is still

03:11.440 --> 03:12.960
here.

03:12.960 --> 03:20.960
We have GPUs which are much faster and much more easily parallelized but they still have

03:20.960 --> 03:28.880
some overhead and then we have smaller units like NPUs which you can find in new phones

03:28.880 --> 03:37.400
or new devices, new dev boards and these are optimized to run machine learning or AI workloads.

03:37.400 --> 03:48.440
But usually in the hardware you get like Linux phones you still don't have these accelerators

03:48.440 --> 03:49.720
in the place.

03:49.720 --> 03:53.800
For new phones and new devices you have them already.

03:53.800 --> 04:04.400
So probably all of who of you used OpenCL or any acceleration to run something?

04:05.400 --> 04:10.160
Okay, much less than the knowledge of OpenCL.

04:10.160 --> 04:15.760
So you can do all the stuff with this image processing and these days language models

04:15.760 --> 04:19.640
are most popular I would say.

04:19.640 --> 04:27.720
So this is motivation and what we have like in open source world.

04:27.720 --> 04:34.000
In general you can use multiple technologies but I will talk mainly about OpenCL because

04:34.000 --> 04:37.920
OpenCL gives you one thing which nothing else can.

04:37.920 --> 04:43.960
You can have CUDA or ROKM but these are usually vendor specific.

04:43.960 --> 04:51.400
So if you are going to write let's say software for your phone and then like you want something

04:51.400 --> 04:57.360
which will run everywhere and you cannot achieve that with CUDA at all because it's proprietary

04:57.360 --> 05:00.240
and closed.

05:00.240 --> 05:06.720
You can eventually do that with ROKM but it's only AMD for AMD cards.

05:06.720 --> 05:10.920
And so only alternative which remains is OpenCL.

05:10.920 --> 05:18.080
And in 2012 we had Clover implementation in Mesa and I was very excited about that and

05:18.080 --> 05:23.560
I was like wow now everything will be faster maybe it's associated with me running Gen2

05:23.560 --> 05:29.640
back then and like wanting everything be compiled and much faster than on the BN or anything

05:29.640 --> 05:31.440
else.

05:31.440 --> 05:33.200
But for me it didn't work.

05:33.200 --> 05:38.280
I lost interest pretty quickly because to be honest with Clover nothing was working for

05:38.280 --> 05:39.600
me.

05:39.600 --> 05:52.960
So I gave up but in 2022 there came a rustical implementation to Mesa 3D and it supported

05:52.960 --> 05:55.560
latest OpenCL standard.

05:55.560 --> 05:59.840
It gained support for Intel and multiple drivers.

05:59.840 --> 06:06.240
You can see these supported drivers are Intel, AMD, Mali which is for example present on

06:06.240 --> 06:15.920
Pineform Pro and of course you can run on CPU or you can run over Vulkan with Zinc.

06:15.920 --> 06:21.760
There is work in progress for multiple drivers including Asahi, Qualcomm Adreno which is

06:21.760 --> 06:27.240
for example present in OnePlus 6 which is pretty popular Linux phone and which I also

06:27.240 --> 06:36.160
own and Vivante which is used in for example LibreM 5 and Raspberry Pi everyone knows I

06:36.160 --> 06:38.560
assume.

06:38.560 --> 06:46.960
So here is example how it looks like if you run a LLM model on OnePlus 6.

06:46.960 --> 06:48.120
It runs.

06:48.120 --> 06:50.840
It's not that slow.

06:50.840 --> 06:57.720
It's not that fast but on other hand it runs which is amazing.

06:57.720 --> 07:05.120
This is GPT2 as you can see from the answer it's not very clever but on other hand it

07:05.120 --> 07:08.640
runs and this is general purpose model.

07:08.640 --> 07:19.000
So currently I tried run some GPT3 based models some minimal ones but it's not there yet.

07:19.000 --> 07:26.320
But GPT2 works for me and so it means for every app you developing or you thinking about

07:26.320 --> 07:33.840
you can consider using some smaller models and already run it for example on the phones.

07:33.840 --> 07:39.960
And what is interesting is performance of course because if you run the model without

07:39.960 --> 07:50.320
GPU so you run on CPU you get load on 8 cores 100% to 4 week cores for strong cores get

07:50.320 --> 07:59.080
fully utilized but if you switch to open CL you get hardly 2 cores at 20% of usage and

07:59.080 --> 08:01.640
it's much faster as you can see.

08:01.640 --> 08:06.920
So and we talking about small phone.

08:06.920 --> 08:12.720
Here is the slide if anyone wants to try and you have post market OS on your phone so you

08:12.720 --> 08:19.040
can apply these simple steps to get in the place where it will be running.

08:19.040 --> 08:24.240
I guess you will not try right now but after you see the slides you can click through it

08:24.240 --> 08:28.720
and install the stuff.

08:28.720 --> 08:35.320
And in general where to open CL and compute heading.

08:35.320 --> 08:42.080
You can do a lot it's kind of widely supported already there is a lot of progress done on

08:42.080 --> 08:46.160
Rusty CL on Linux.

08:46.160 --> 08:52.280
So I believe in one year you can assume on every device you will be able to run some

08:52.280 --> 08:54.360
CL.

08:54.360 --> 08:59.600
So you can use it for your workloads for example what I heard just today before the talk that

08:59.600 --> 09:06.680
lib camera which is used for processing input from Linux cameras on phones considering using

09:06.680 --> 09:10.120
open CL workloads for processing.

09:10.120 --> 09:14.600
So it will get popular soon I hope so.

09:14.600 --> 09:21.200
And so what I'm thinking is the most interesting part you can start relying on it because for

09:21.200 --> 09:30.400
today applications like blender or GIMP are able to use open CL but it's not something

09:30.400 --> 09:32.320
you can count on.

09:32.320 --> 09:36.480
But that's hopefully change very soon.

09:36.480 --> 09:43.440
And another thing to talk about is what's going to happen next because open CL 3 is pretty

09:43.440 --> 09:50.480
good specification but if you compare to CUDA from NVIDIA it's really lacking.

09:51.240 --> 09:56.320
Best we have right now but it's not that amazing.

09:56.320 --> 10:03.920
It's amazing enough to be fast to provide a float which is good quality but it's not

10:03.920 --> 10:07.080
that great compete with CUDA.

10:07.080 --> 10:16.320
So I recommend for example David early talk about future of open CL and about some standardization

10:16.320 --> 10:22.360
which would fit all vendors so it wouldn't end up like one vendor trying to push his

10:22.360 --> 10:27.520
technology and this allow other vendors to contribute or use it.

10:27.520 --> 10:33.560
So something like open CL 4 we will see what happen.

10:33.560 --> 10:40.400
So this is eventually future is a little bit unclear but so far the open CL looks pretty

10:40.400 --> 10:41.400
good.

10:41.760 --> 10:47.840
Clover implementation and Mesa which was from 2012 is pretty dead because no one use it,

10:47.840 --> 10:49.040
no one maintains it.

10:49.040 --> 10:56.440
We dropped it from CI approximately half year ago so it's not counted it's just waiting

10:56.440 --> 10:58.880
for deletion.

10:58.880 --> 11:05.560
And last thing I want to point out is like even the low power device as a phone can provide

11:05.560 --> 11:10.520
some nice acceleration with open CL and it's really visible difference.

11:11.640 --> 11:19.600
So few credits Carol Herbst for bringing Rustical alive because that was very nice project

11:19.600 --> 11:27.080
and he was integrating Rust based software into Mesa which is C and C++ based.

11:27.080 --> 11:30.120
So that's pretty challenging task.

11:30.120 --> 11:38.240
Rob Clark for working of free Drano because are there are here any one plus six users?

11:39.240 --> 11:45.400
Okay right so you know the GPU works pretty well on these devices so this is lot of his

11:45.400 --> 11:49.960
work and Dimitri Baryshko.

11:49.960 --> 11:52.360
I hope I read the name right.

11:52.360 --> 12:00.560
Good for preparing to merge request for free Drano on Rustical and it's not merged yet

12:00.560 --> 12:09.480
but it's pretty close needs some polishing and for the tiny grad and GPT2 it works well

12:09.480 --> 12:14.400
enough and of course many others who contributed.

12:14.400 --> 12:18.840
So thank you for your attention and fire the questions.

12:30.560 --> 12:41.920
Thanks for the talk question regarding your comparison of the workload.

12:41.920 --> 12:48.000
Did you check the load on the GPU when you were running this model?

12:48.000 --> 12:53.280
So when you compare the eight cores versus GPU?

12:53.280 --> 13:02.200
I haven't checked the workload on GPU but I assume it was pretty high but yeah it's

13:02.200 --> 13:08.640
what I forgot to mention is it's not yet optimized and no one tried to profile it to give a good

13:08.640 --> 13:16.680
performance and tiny grad project is also meant mostly for the powerful AMD GPUs or

13:16.680 --> 13:22.480
let's say NVIDIA even Intel is not that popular.

13:22.480 --> 13:28.120
So it means these results which you've seen are still highly unoptimized software so it

13:28.120 --> 13:36.360
has probably lot of chances for improvements and better performance.

13:36.360 --> 13:37.360
Thank you.

13:37.360 --> 13:40.760
Hi thank you.

13:40.760 --> 13:47.120
Since newer for example Qualcomm SoC also have an actual NPU in the SoC are you aware

13:47.120 --> 13:53.400
if it's possible to run OpenCL on this and what could be missing and if anybody is working

13:53.400 --> 13:55.960
on this or something?

13:55.960 --> 14:05.760
Yes the new hardware has NPUs and for example one of former colleagues Tomoe Wissoso working

14:05.760 --> 14:13.840
on etnavif acceleration for NPUs so recently in Mesa was integrated part of the Teflon

14:13.840 --> 14:20.960
framework which allows interaction between etnavif which is vivante GPUs like in it's

14:20.960 --> 14:29.160
newer generation than this in LibreM 5 and you can run TensorFlow networks on NPU directly

14:29.160 --> 14:39.320
but so far only one vendor at vivante and one device is supported I think.

14:39.320 --> 14:42.400
How's the RAM usage?

14:42.400 --> 14:48.880
I think the model I used is like 500 megabytes so it's pretty nice like nothing serious the

14:48.880 --> 14:58.240
phone has 8 gigabytes so anyway for the RAM usage it would be more interesting with GPT-3

14:58.240 --> 15:07.680
models but I think on the phones the language models are useful only if they are specialized

15:07.680 --> 15:16.680
and cut down to appropriate power which the phone offers because of course you cannot

15:16.680 --> 15:30.440
run full Lama GPT model which requires 8 gigabytes of RAM or VRAM on phone which has 8 gigabytes

15:30.440 --> 15:36.680
of RAM.

15:36.680 --> 15:41.920
Do we have any more questions?

15:41.920 --> 15:42.920
That's it then.

15:42.920 --> 15:50.920
Another big round of applause for David.

15:50.920 --> 15:52.320
Next talk is in 15 minutes.

