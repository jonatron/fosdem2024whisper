WEBVTT

00:00.000 --> 00:05.000
Hello, welcome.

00:05.000 --> 00:09.000
My name is Peter Sabini from Canonical.

00:09.000 --> 00:11.000
I'm a software engineer there.

00:11.000 --> 00:17.000
I work on various CEP stuff and I'm very excited to present Microsoft with the tagline

00:17.000 --> 00:23.000
Get CEP up and running in minutes, unlike my slideshow.

00:23.000 --> 00:25.000
So problem statement.

00:25.000 --> 00:27.000
Microsoft packages CEP.

00:27.000 --> 00:34.000
This is a big complex system with distributed configuration, distributed components,

00:34.000 --> 00:40.000
complex bootstrapping, procedure and complex operations.

00:40.000 --> 00:43.000
It also has non-trivial hardware requirements.

00:43.000 --> 00:49.000
It's not just like you can download a package, install it on my notebook and be ready to go.

00:49.000 --> 00:57.000
It also has impact uptake and adoption among users.

00:57.000 --> 01:06.000
So if you're, for example, a famous physics research organization with thousands of nodes in your storage cluster,

01:06.000 --> 01:10.000
you probably have trained staff on hand 24-7.

01:10.000 --> 01:13.000
So you're good, you don't need Microsoft.

01:13.000 --> 01:22.000
If not, if you don't have a team of trained experts on hand, maybe Microsoft is something for you.

01:22.000 --> 01:23.000
So what is Microsoft?

01:23.000 --> 01:25.000
Microsoft is a single package staff cluster.

01:25.000 --> 01:29.000
Everything is in one file.

01:29.000 --> 01:38.000
We're designed it to be a simple setup so you can get a running staff cluster for command lines.

01:38.000 --> 01:40.000
And it runs on your notebook.

01:40.000 --> 01:45.000
You just need one node with obviously one hard disk.

01:45.000 --> 01:50.000
So simple possible staff cluster you can do is install Microsoft,

01:50.000 --> 01:56.000
putstrap the cluster, add some simulated OSDs, disk drives.

01:56.000 --> 01:59.000
So this is loop files in this case.

01:59.000 --> 02:01.000
No extra block devices required.

02:01.000 --> 02:08.000
And then wait a few minutes and your staff cluster should be ready to go.

02:08.000 --> 02:09.000
How did you do this?

02:09.000 --> 02:12.000
Microsoft is a snap package, as you might have guessed.

02:12.000 --> 02:18.000
Snap packages have the benefit that you're completely isolated from the host system.

02:18.000 --> 02:23.000
All the user land is in separate namespace.

02:24.000 --> 02:32.000
You just need a kernel, network devices, block devices, hardware, etc. to get up and running.

02:32.000 --> 02:40.000
This gives you a good isolation from the host system and gives a consistent environment across different operating systems.

02:40.000 --> 02:48.000
Some other goodies, it's isolation from the host system also means its access is isolated.

02:48.000 --> 02:53.000
The snap package just cannot do anything it wants on the host system,

02:53.000 --> 02:56.000
which is good for safety, security and robustness reasons.

02:56.000 --> 02:59.000
And you have standardized risk levels.

02:59.000 --> 03:06.000
So if you want to install release candidates, etc., there's a standardized way to do this.

03:06.000 --> 03:10.000
A little bit of overview of the Microsoft architecture.

03:10.000 --> 03:15.000
You have a service management demon that manages the standard CEP components

03:15.000 --> 03:23.000
and has a distributed database, a DITS proposal for storing configuration and no topology.

03:23.000 --> 03:31.000
Also included in the snap package is a CLI that talks to the service management demon via an API.

03:31.000 --> 03:40.000
All this is just a standard Ubuntu devian packages, no special binaries here involved.

03:41.000 --> 03:48.000
I mentioned the service API, so everything in Microsoft happens via this service API.

03:48.000 --> 03:57.000
Things you can do with the API, like listing block devices, adding or removing nodes, adding and removing disks.

03:57.000 --> 04:04.000
Everything works via the API and the included CLI is just another client for this API.

04:04.000 --> 04:09.000
So this is obviously great for integrating it in other systems.

04:09.000 --> 04:14.000
Some more internals. Microsoft is built on the micro cluster library,

04:14.000 --> 04:23.000
which provides this distributed configuration database, which is using RAVT for consensus.

04:23.000 --> 04:28.000
It also provides cluster membership and API framework.

04:30.000 --> 04:35.000
I already talked a little bit about scaling down, so single node systems work.

04:35.000 --> 04:42.000
One important component here is that we automatically manage the crush rules from CEP.

04:42.000 --> 04:50.000
So this means that as you start up with a single node, you get a failure domain of OSD,

04:50.000 --> 04:55.000
so in effect your single node clusters work out of the box,

04:55.000 --> 05:02.000
but if you add more nodes, your resiliency and your failure domain gets scaled up automatically.

05:03.000 --> 05:08.000
It's also possible to provide custom crush rules.

05:08.000 --> 05:12.000
This is important, for instance, if you go for larger failure domains,

05:12.000 --> 05:17.000
for instance, if you have a failure domain of rooms or racks, you can implement this.

05:17.000 --> 05:21.000
Microsoft itself doesn't know about your rooms or racks,

05:21.000 --> 05:26.000
but it won't step on your toes if you provide a custom crush rule set.

05:27.000 --> 05:33.000
Microsoft is famously scalable to thousands of nodes.

05:33.000 --> 05:39.000
Microsoft's scalability upwards is primarily bound by the RAVT algorithm used in the VQLite database.

05:39.000 --> 05:49.000
For performance, I would like to note that we're not sitting in the data path anywhere for CEP operations.

05:49.000 --> 05:54.000
You get the standard CEP performance behavior, also with Microsoft.

05:57.000 --> 06:04.000
Some integrations. Microsoft is the back storage back end for a number of projects in Canonical,

06:04.000 --> 06:08.000
for instance Sunbeam, MicroCades, MicroCloud and LXT.

06:08.000 --> 06:17.000
There's also, if you're running Juju models, there's a charm available currently in beta to integrate Microsoft into your Juju Clouds.

06:17.000 --> 06:27.000
Last but not least, there's a nice little GitHub action that we provide to integrate Microsoft into your GitHub CI workflow.

06:27.000 --> 06:35.000
So if you need, for instance, a S3 endpoint for your testing pipeline, this is an action that would help you with that.

06:36.000 --> 06:39.000
Okay, on for demos.

06:40.000 --> 06:48.000
I prerecorded these demos for time reasons and also because I'm very bad at talking and typing at the same time.

06:50.000 --> 06:52.000
So let's see how this goes.

06:56.000 --> 06:59.000
So this is the single node setup we talked about before.

06:59.000 --> 07:01.000
I'm going to install the single node.

07:01.000 --> 07:07.000
Microsoft Cluster gives it some simulated disks and enable a Rata's gateway,

07:07.000 --> 07:09.000
which would give you an S3 endpoint.

07:11.000 --> 07:13.000
Yeah, installation.

07:13.000 --> 07:16.000
We have the standard stable risk level here set.

07:17.000 --> 07:20.000
So this is what you get by default.

07:21.000 --> 07:24.000
You see my capital DSL connection here.

07:25.000 --> 07:28.000
Yeah, so we bootstrap the cluster.

07:31.000 --> 07:33.000
This is done pretty quickly.

07:34.000 --> 07:38.000
We can see now that we have a few services running already, but no disks.

07:39.000 --> 07:41.000
Then we add some simulated disks.

07:41.000 --> 07:42.000
These are just loop files.

07:42.000 --> 07:45.000
This is useful for lab environment or for testing.

07:45.000 --> 07:47.000
Don't use it for production.

07:47.000 --> 07:50.000
For production, you would use separate block devices, obviously.

07:50.000 --> 07:54.000
But if you want to get going on your laptop, that's the way to do it.

07:55.000 --> 07:57.000
We enable Rata's gateway.

07:57.000 --> 08:00.000
You can see it is active here now.

08:01.000 --> 08:03.000
We create a Rata's gateway user.

08:03.000 --> 08:05.000
This is just a standard safe way to do it.

08:05.000 --> 08:08.000
It's a little ugly command line here.

08:08.000 --> 08:10.000
And yeah, and we're done.

08:10.000 --> 08:15.000
We can use our favorite S3 client to access our new Rata's gateway endpoint.

08:16.000 --> 08:20.000
So just to prove that it works, we are creating a bucket here

08:23.000 --> 08:27.000
and put some image up in this bucket.

08:31.000 --> 08:35.000
Yeah, so that's for this demo.

08:35.000 --> 08:38.000
So this is the simplest possible case.

08:45.000 --> 08:48.000
Let's do something a little bit more complicated.

08:48.000 --> 08:51.000
Say we have got a few extra nodes now.

08:51.000 --> 08:55.000
We want to in an expander cluster and provide it with real block devices.

08:55.000 --> 08:57.000
This is the way we will do it.

08:57.000 --> 09:02.000
I'm now using the candidate risk level because I want to use some features from Microsoft

09:02.000 --> 09:08.000
that didn't make it to the stable risk level yet.

09:09.000 --> 09:15.000
So to cluster Microsoft, you need to get the token from the bootstrap node.

09:15.000 --> 09:19.000
So the first node that we provided, like this.

09:19.000 --> 09:23.000
Name the node you want to add and get the token for it

09:23.000 --> 09:30.000
and provide that token to the node that you want to add here.

09:30.000 --> 09:34.000
So, and yeah, small typo.

09:34.000 --> 09:36.000
These have happened as well.

09:36.000 --> 09:40.000
And yeah, and now all our nodes are clustered.

09:40.000 --> 09:43.000
Let's check Microsoft status.

09:43.000 --> 09:49.000
We can see all our new services here, but all the new nodes don't have any disks yet.

09:49.000 --> 09:51.000
So let's add some disks.

09:51.000 --> 09:56.000
So what I'm going to do here is add a user feature that comes from the release candidate

09:56.000 --> 10:00.000
that is automatically pro for empty block devices.

10:00.000 --> 10:04.000
So anything that's not mounted is clean.

10:04.000 --> 10:09.000
We take as a block device here with this switch.

10:09.000 --> 10:12.000
Let the thing settle a little.

10:12.000 --> 10:16.000
You can see there's lots of virtual disks from Kime.

10:16.000 --> 10:20.000
And we have a lot of disks in our cluster.

10:20.000 --> 10:25.000
So the safe cluster is still setting a little bit,

10:25.000 --> 10:29.000
but we suddenly have a lot more space available.

10:29.000 --> 10:34.000
So one thing we can do is provide a second radius gateway endpoint.

10:34.000 --> 10:38.000
Now we can see that the data we put in before is still here.

10:38.000 --> 10:40.000
So that's reassuring.

10:41.000 --> 10:53.000
And what we'll try to do now is we put in another OSD on the first node,

10:53.000 --> 10:55.000
but this time we want to make it encrypted.

10:55.000 --> 10:58.000
So full disk encryption is something we provide here.

10:58.000 --> 11:01.000
It relies on the dmcrypt kernel module.

11:01.000 --> 11:04.000
Not all kernels have that, so that's a little bit of a gotcha.

11:04.000 --> 11:07.000
You need to make sure it does.

11:07.000 --> 11:13.000
And also, this goes by so fast,

11:13.000 --> 11:17.000
also this is something that the snap is not allowed by default to do.

11:17.000 --> 11:23.000
You need to connect the dmcrypt module explicitly to make this happen.

11:23.000 --> 11:30.000
But once you do, it will give you an encrypted OSD device.

11:30.000 --> 11:32.000
That's the one up there now.

11:32.000 --> 11:35.000
Just to prove that this is a dmcrypt device,

11:35.000 --> 11:39.000
there's a setup here.

11:39.000 --> 11:46.000
Well, we all have the loop file for OSDs from the first node that we originally installed.

11:46.000 --> 11:47.000
Let's remove that.

11:47.000 --> 11:56.000
We have plenty of block devices now, so that our cluster has real disks.

11:56.000 --> 12:00.000
So as a last step for our production cluster,

12:00.000 --> 12:04.000
something that snaps to by default is auto refresh.

12:04.000 --> 12:07.000
This is something you don't typically want for your self cluster.

12:07.000 --> 12:11.000
You want to control updates for your self cluster.

12:11.000 --> 12:18.000
And that is a step you do, is hold all the snaps and prevent auto refresh

12:18.000 --> 12:26.000
so that you can refresh or update your software to your own leisure.

12:26.000 --> 12:33.000
So, yeah, so that's what's for the demos.

12:33.000 --> 12:36.000
Short outlook, what comes next.

12:36.000 --> 12:41.000
We want to make the clustering experience a little smoother still.

12:41.000 --> 12:42.000
No passing around of tokens.

12:42.000 --> 12:48.000
So one thing we could do with this or we planned to do is on the local network

12:48.000 --> 12:51.000
use MDNS to determine new nodes.

12:51.000 --> 12:55.000
Another thing that we want to do in the near future is provide built-in

12:55.000 --> 13:05.000
HAA and load balancing for other gateway endpoints and also RBD mirroring support.

13:05.000 --> 13:11.000
So that was it for the demos and for the...

13:11.000 --> 13:15.000
Thank you.

13:15.000 --> 13:16.000
Any questions?

13:16.000 --> 13:18.000
I don't know if we have time for questions.

13:18.000 --> 13:22.000
One question maybe.

13:22.000 --> 13:24.000
Otherwise, I'll be outside.

13:24.000 --> 13:28.000
Just talk to me and I'll be happy to answer your questions.

13:28.000 --> 13:30.000
Oh, sorry.

13:30.000 --> 13:31.000
Here you go.

13:31.000 --> 13:35.000
What architecture do you support with CPL architecture?

13:35.000 --> 13:37.000
Can you repeat the question please?

13:37.000 --> 13:40.000
What CPL architecture do you support?

13:40.000 --> 13:43.000
So snaps are pretty flexible.

13:43.000 --> 13:48.000
We develop on AMD64, but ARM is tested.

13:48.000 --> 13:51.000
I don't know if the top of my heart had...

13:51.000 --> 13:58.000
But ARM, AMD64, power, PT and risk, I believe...

