WEBVTT

00:00.000 --> 00:12.360
Thank you.

00:12.360 --> 00:13.360
Hello everyone.

00:13.360 --> 00:15.840
Thank you so much for being here.

00:15.840 --> 00:22.400
In today's session we are going to do an introduction to a driving observability on operations in

00:22.400 --> 00:24.800
cloud edge system.

00:24.800 --> 00:26.920
First let me introduce myself.

00:26.920 --> 00:28.960
My name is Victor Palma.

00:28.960 --> 00:33.920
I'm a cloud engineer at Supernevola Systems.

00:33.920 --> 00:42.920
I come from Madrid, Spain and I've been working for Supernevola for more than two years.

00:42.920 --> 00:46.920
So let's move on to the presentation.

00:46.920 --> 00:55.000
First I would like to start with some initial context in order to introduce later what we

00:55.000 --> 00:57.360
are going to see here.

00:57.360 --> 01:02.600
So first what is observability?

01:02.600 --> 01:08.400
Observability is the ability to understand and analyze the internal behavior of a system

01:08.400 --> 01:12.040
by collecting and analyzing relevant data.

01:12.040 --> 01:15.200
That is the dictionary definition.

01:15.200 --> 01:24.080
But in other words it's just the ability to transform data into information in something

01:24.080 --> 01:27.000
that could be useful for us.

01:27.000 --> 01:39.640
So we can have a lot of system logs or of data or number but if we don't provide a

01:39.640 --> 01:42.520
meaning to them it's useless.

01:42.520 --> 01:57.200
So observability has multiple, sorry I just got blank.

01:57.200 --> 02:06.040
It has some advantages like anomaly detection that allows us to identify anomalies or bugs

02:06.040 --> 02:08.360
in our system.

02:08.360 --> 02:16.320
This also provides the ability to do a performance analysis.

02:16.320 --> 02:23.720
So we can identify areas for improvement in our system and finally it's very useful for

02:23.720 --> 02:25.440
decision making.

02:25.440 --> 02:35.320
So we can see the impact of the change that we made in our system in a very easy way.

02:35.320 --> 02:39.320
As the saying goes information is power.

02:39.320 --> 02:44.360
So observability is very, very important in nowadays.

02:44.360 --> 02:53.120
But now I would like to talk about AI because AI in nowadays is everywhere.

02:53.120 --> 02:57.280
You know, a marketing guy's fault.

02:57.280 --> 03:04.160
But it's really useful for observability.

03:04.160 --> 03:10.160
It's really useful for the quick answer is yes, socially.

03:10.160 --> 03:25.440
AI provides the capacity to create more enhancing data analysis, create automated anomaly detection,

03:25.440 --> 03:27.680
create dynamic scaling for our cloud.

03:27.680 --> 03:36.640
For example, if we have more workloads that the usual we can automatically create new

03:36.640 --> 03:46.080
notes or deploy new VMs in our cloud in order to provide more services to our customers.

03:46.080 --> 03:55.720
And finally we can create predictive analysis, analytics in order to predict how our system

03:55.720 --> 04:01.800
is going to behave in the future.

04:01.800 --> 04:10.000
After finishing this part, I also would like to talk about data sorrentia and open source

04:10.000 --> 04:22.680
because I think the most important concept about AI, currently it's the data sorrentia

04:22.680 --> 04:25.400
or the information.

04:25.400 --> 04:33.280
Many organizations truce sensitive data to third parties providers.

04:33.280 --> 04:43.280
Currently these providers are based outside of Europe and we need to wait in order to

04:43.280 --> 04:49.440
bring back the data to our servers to Europe and be more transparent in this way.

04:49.440 --> 04:56.880
So as a solution, the open source is a very good solution for that problem.

04:56.880 --> 05:09.760
And provide more transparency for the cloud and helps reducing the vendor locking in our

05:09.760 --> 05:10.760
infrastructure.

05:10.760 --> 05:22.080
So we are not tired to a specific vendor and we can migrate within vendors every time we

05:22.080 --> 05:24.320
need it.

05:24.320 --> 05:27.600
So what's next?

05:27.600 --> 05:33.680
How can we address all these challenges?

05:33.680 --> 05:43.440
The answer is the one AYOPS framework, the open source solution for eye driving observability.

05:43.440 --> 05:54.360
One AYOPS framework combines open Nebula as virtualization and cloud management tool,

05:54.360 --> 06:06.080
Prometheus and Grafana as metrics and visualization solution and some AI and ML algorithms to predict

06:06.080 --> 06:11.280
and analyze all the behavior of infrastructure.

06:11.280 --> 06:17.800
All the three technologies together creates the one AI AYOPS framework that we are going

06:17.800 --> 06:20.040
to see here today.

06:20.040 --> 06:27.080
So let's go step by step and first we are going to see what is open Nebula.

06:27.080 --> 06:37.120
Open Nebula is the open source cloud platform solution in order to create your own cloud.

06:37.120 --> 06:45.640
It provides the ability to deploy virtual machines in your own private data center,

06:45.640 --> 06:49.920
in your public cloud or even in the edge.

06:49.920 --> 06:57.440
But you not only can deploy virtual machines but also application containers, micro VMs

06:57.440 --> 06:59.840
or even Kubernetes clusters.

07:00.840 --> 07:08.360
As I've said before, one of the features of open Nebula since it's open source and it's

07:08.360 --> 07:16.240
oriented to provide a truly, truly flexibility to the cloud is that avoids the vendor locking

07:16.240 --> 07:26.720
so you can migrate your workloads with between different providers in a very, very easy way.

07:26.720 --> 07:36.520
Open Nebula has a lot of integration with party tools like Terraform, like Kubernetes,

07:36.520 --> 07:38.560
Ansible or Docker.

07:38.560 --> 07:49.640
It also has a built-in tools like Sandstone that is the web user interface and you can

07:49.640 --> 07:57.280
handle all of your infrastructure from there or from the Celi-I and deploy a built-in machines

07:57.280 --> 08:07.880
based on the where, on KBM, LXD or micro VMs in Fightcracker.

08:07.880 --> 08:15.840
Finally one of the most important features of open Nebula is the possibility to expand

08:15.840 --> 08:21.440
your cloud to the multi-cloud or to the hybrid cloud.

08:21.440 --> 08:33.200
You can create on demand resources on the edge in Amazon, Google Cloud, Equinids, just

08:33.200 --> 08:38.280
clicking a button or automatically if you configure that.

08:38.280 --> 08:45.280
So you can migrate workloads to your on-premise data center to on-edge data center of the

08:45.280 --> 08:51.240
public cloud in a very strife way.

08:51.240 --> 08:57.720
So you can deploy any infrastructure with a uniform management for all this infrastructure

08:57.720 --> 09:03.960
and you can run any application in your cloud.

09:03.960 --> 09:10.800
For open Nebula doesn't occur if the host is located in Equinix or the edge or in your

09:10.800 --> 09:12.600
private data center.

09:12.600 --> 09:21.200
The only thing that open Nebula occurs is what is, what VM is running the workload and how

09:21.200 --> 09:24.480
can you access to that VM?

09:24.480 --> 09:27.640
So very handy.

09:27.640 --> 09:35.240
The next section of the one AI ops is the integration of open Nebula with Prometheus.

09:35.240 --> 09:48.280
That integration is based on the Prometheus Sportex like the Prometheus Node Sportex

09:48.280 --> 09:55.640
that is installed in every open Nebula server.

09:55.640 --> 10:05.120
It's also installed in the hypervisor nodes and it combines with the open Nebula Libreth

10:05.120 --> 10:06.120
exporter.

10:06.120 --> 10:14.160
It's a Q-Stume exporter created by open Nebula in order to extract and collect information

10:14.160 --> 10:18.560
about the KVM machines.

10:18.560 --> 10:28.760
And we also combine this information with the, the, our metrics of open Nebula that

10:28.760 --> 10:31.280
it's created itself.

10:31.280 --> 10:38.520
And this metric is, is gathered by the 1D Demon, it's the main demon of open Nebula.

10:38.520 --> 10:47.840
And then it's exported to the Prometheus server through the open Nebula server exporter.

10:47.840 --> 11:00.960
So the, so the next thing is the AI that we, sorry, that we add to the formula.

11:00.960 --> 11:09.960
So we create a bunch of machine learning algorithms and some decision algorithms and implemented

11:09.960 --> 11:14.760
in the, as a, as an exporter to Prometheus.

11:14.760 --> 11:27.120
So gathering all the metrics that open Nebula produce and the exporter produce, we implement

11:27.120 --> 11:36.120
this algorithm in order to predict and to, to, to get how, how improve the performance

11:36.120 --> 11:39.080
of, of your cloud.

11:39.080 --> 11:48.400
So in summary, the feature and capabilities of when one AA ops are the CPU usage prediction

11:48.400 --> 11:52.200
of the VMs of your cloud.

11:52.200 --> 11:59.920
One A ops come predicts the individual VM CPU prediction per hour, the general CPU prediction

11:59.920 --> 12:03.000
for usage for, for your host.

12:03.000 --> 12:11.440
The accuracy of that prediction, a very important value in, in terms of a feasibility of your,

12:11.440 --> 12:13.320
of the, of the system.

12:13.320 --> 12:23.200
And then when AI ops come also suggests where you can place a VM.

12:23.200 --> 12:31.440
So based on that prediction, one A ops maybe tell you to migrate a VM from one server to

12:31.440 --> 12:37.800
another in order to improve the performance there.

12:37.800 --> 12:43.040
Three main policies configuring when, when AI ops.

12:43.040 --> 12:47.040
The first one is the load balancing, load balancing.

12:47.040 --> 12:55.080
It's just the name said balancing the load of, in all your notes is very useful when

12:55.080 --> 13:02.560
you have on premise or private data center and you want to use all your hosts.

13:02.560 --> 13:05.680
The next policy is the resource contention.

13:05.680 --> 13:16.360
So very useful for public cloud environments when you want to use a, a few number of, of

13:16.360 --> 13:17.360
hosts.

13:17.640 --> 13:21.440
And the last one is reduce migration.

13:21.440 --> 13:29.280
That policy very useful when you want to, to avoid migrating VMs between hosts.

13:29.280 --> 13:37.560
The, this, this scenario is very useful for a edge environment where the migration of

13:37.560 --> 13:45.160
people machines between edge nodes sometimes it's kind of a bit done.

13:45.160 --> 13:52.280
So here you can see the architecture of the one AI ops.

13:52.280 --> 13:58.040
It's based on the already existed open nebulizer architecture.

13:58.040 --> 14:03.800
So everything at the bottom, it's already what's open nebulize.

14:03.800 --> 14:13.080
And the layer at the, at the top of the picture is the new one AI ops architecture layer.

14:13.080 --> 14:21.720
So here you can see the modules that we have implemented in order to provide this, this

14:21.720 --> 14:32.800
prediction and then the, all the virtualize infrastructure orchestrated.

14:32.800 --> 14:39.040
So let's do a demo in order to show you how this works.

14:39.040 --> 14:50.760
First we are going to, to go to the, to the open nebulizer system portal in order to show

14:50.760 --> 14:54.560
you, wait, sorry.

14:54.560 --> 15:17.920
Thank you.

15:17.920 --> 15:19.440
I'm so sorry.

15:19.440 --> 15:23.200
Get me a minute.

15:23.200 --> 15:25.840
Okay.

15:25.840 --> 15:36.200
Well, this is the main dashboard of open nebula, user graphic interface.

15:36.200 --> 15:44.520
Here you can see the, the principal information about your cloud, like how many machines we

15:44.520 --> 15:51.840
have or the images or the built on network or the usage of the host.

15:51.840 --> 15:57.720
We have currently, here, sorry.

15:57.720 --> 15:59.520
In this, this is a demo environment.

15:59.520 --> 16:12.160
So we only have two, two hosts with some workload and that these are the, the VMs that we have

16:12.160 --> 16:13.800
running in these environments.

16:13.800 --> 16:21.040
So this is solid to see that we already have some workload in this environment and this

16:21.040 --> 16:23.560
workload is fully random.

16:23.560 --> 16:30.120
So maybe it's consumed a certain CPU depending on, on the time.

16:30.120 --> 16:39.800
So when we install the one AI ops framework and we have a documentation for that, if we

16:39.800 --> 16:45.360
go to Grafana and import the one dashboard, we can see this.

16:45.360 --> 16:51.000
This is the results that we want AI ops generates.

16:51.000 --> 17:02.280
So we can see here at the left, the average CPU predicted per host.

17:02.280 --> 17:11.320
Here we can see the average CPU, the usage per VMs and here the, the, the real usage.

17:11.320 --> 17:17.920
So as you can see, it's closely one value to, to the other and the accuracy of the prediction,

17:17.920 --> 17:23.200
in that case, a 92%.

17:23.200 --> 17:30.640
Here we can see the suggestions that one AI provides to the user.

17:30.640 --> 17:34.440
The first policy is the core optimization policy.

17:34.440 --> 17:43.240
So it's going to, to, to reduce, it's going to try to reduce the number of hosts to the

17:43.240 --> 17:44.480
minimum.

17:44.480 --> 17:48.480
Here you can see that all the VMs, we have five VMs in this demo.

17:48.480 --> 17:51.400
The four VMs are in one host.

17:51.400 --> 17:59.240
Since this host is fully and not a more VMs entering inside this host, the, we have here

17:59.240 --> 18:06.760
another VM, but it tries to concentrate the VMs in the, in a few hosts.

18:06.760 --> 18:14.280
Here you can see the migrations that one AI suggests for, for that, for, for achieve

18:14.280 --> 18:15.400
this distribution.

18:15.400 --> 18:23.600
So it's to get to us to move the VM with ID three to the host with ID one.

18:23.600 --> 18:28.240
It's very, very, very easy to, to follow the instructions.

18:28.240 --> 18:34.680
And then we have the other policies, the load balancing optimization that as you can see,

18:34.680 --> 18:41.520
we have the, the VMs distributed in the two hosts that we have in our environment.

18:41.520 --> 18:44.960
And then the final policy that is the immigration optimization.

18:44.960 --> 18:54.600
And in this case, no migrations are suggested because no, no optimization are found.

18:54.600 --> 18:55.600
But what?

18:55.600 --> 19:03.080
In case one AI produce something, it, it should be up here.

19:03.080 --> 19:04.880
And what?

19:04.880 --> 19:10.360
Returning to the, to the slides.

19:10.360 --> 19:14.760
Well this is the demo that we have just seen.

19:14.760 --> 19:21.600
And closing thoughts on the next step of, of this project.

19:21.600 --> 19:28.720
So the next steps and challenge that we, we are facing in, in one AI ops.

19:28.720 --> 19:37.440
First is implement the virtualization operations in order to apply the suggestion automatically.

19:37.440 --> 19:44.120
Currently the operations are only suggested but not performed in the, in the cloud system.

19:44.120 --> 19:50.880
Then we would like to improve the AI ops distribution as part of the OpenEvola software.

19:50.880 --> 19:54.400
And you need to install it separately.

19:54.400 --> 20:02.240
And finally we will, we will like to expand the functionality to provide anomaly detections,

20:02.240 --> 20:06.720
allocation based on memory prediction and network traffic.

20:06.720 --> 20:11.920
Because we only provide currently CPU usage prediction.

20:11.920 --> 20:18.640
And based on the result of the, of the tool, creates alerts and warnings.

20:18.640 --> 20:21.680
This project is totally open source.

20:21.680 --> 20:29.120
So you can go to the repository on GitHub and collaborate and suggest new features and,

20:29.120 --> 20:32.080
and, and changes.

20:32.080 --> 20:43.120
And finally I would like to, to encourage you to join to the OpenEvola community.

20:43.120 --> 20:51.440
So you can visit the forum and participate in, in discussion with other, with other OpenEvola

20:51.440 --> 21:01.320
users and, and learn and help together in the cloud community that we have created here.

21:01.320 --> 21:08.920
As closing a slide, I would like also to, to, to say that this project is, is founded by

21:09.040 --> 21:15.680
the European Union as part of the Horizon Europe Research and Innovation Program.

21:15.680 --> 21:18.520
So this project is called CONNIS.

21:18.520 --> 21:24.040
I, I recommend you to take a look in, in this URL.

21:24.040 --> 21:28.080
I can espel you if you want, COG and IT.

21:28.080 --> 21:29.080
COGNET.

21:29.080 --> 21:32.080
It's, it's very, very interesting project.

21:32.080 --> 21:33.680
Well, that's all.

21:33.680 --> 21:34.960
Thank you very much for your attention.

21:39.920 --> 21:44.920
So, questions?

21:44.920 --> 21:45.920
Yeah.

21:45.920 --> 22:03.920
Yeah, we use a linear models and a, and a, and a, and a, and a, and a, and a, and a, and

22:04.640 --> 22:07.640
and a, and a, and a vasilian models tool.

22:07.640 --> 22:09.740
By, by a sund.

22:09.740 --> 22:10.660
Sorry.

22:10.660 --> 22:12.340
By a sund model save any models.

22:12.340 --> 22:16.260
And would you be able to share the slides in the forms of website as teÅ¼ in the child.

22:18.860 --> 22:30.480
You can also find it in the repositories as, as yeah here in this repository all the data

22:30.480 --> 22:33.440
models and, algorithm applied are, are explained.

22:33.440 --> 22:35.440
Okay, thank you.

22:35.440 --> 22:36.440
You're welcome.

22:36.440 --> 22:38.440
Any more questions?

22:38.440 --> 22:39.440
Yeah.

22:39.440 --> 22:42.440
I think I'm basically, it was the same question as the last one.

22:42.440 --> 22:48.440
If you could quickly go back to, because you did actually have the model, and then in like a couple of slides before.

22:48.440 --> 22:49.440
Here.

22:49.440 --> 22:53.440
So here, does it explain where the Bayesian are we in here?

22:53.440 --> 22:58.440
It's not in there, because that was the bit I was wondering how the model worked.

22:59.440 --> 23:00.440
Okay, thank you.

23:00.440 --> 23:03.440
That's a question near our side.

23:03.440 --> 23:04.440
Okay.

23:04.440 --> 23:05.440
Thank you.

23:05.440 --> 23:06.440
Perfect.

23:06.440 --> 23:07.440
Any more questions?

23:11.440 --> 23:12.440
You're welcome.

23:12.440 --> 23:14.440
Ah, it's here.

23:14.440 --> 23:20.440
Are you optimizing for CPU utilization or not recorded?

23:20.440 --> 23:27.440
So also, is it that possible to also say, okay, optimize for availability or network throughput?

23:27.440 --> 23:40.440
Okay, he asked about if we optimize for CPU or for other attributes like network or memory.

23:40.440 --> 23:48.440
Currently, in the current state of the project, we are only make suggestions and predict the CPU usage.

23:48.440 --> 24:03.440
But the idea is to implement a prediction based on the network, on the memory, and other keystone attributes that you want to add to the tool.

24:05.440 --> 24:10.440
The idea is that you can change the prediction, the configuration.

24:10.440 --> 24:14.440
But really, it's just a prototype.

24:14.440 --> 24:16.440
So, for storage?

24:17.440 --> 24:18.440
For storage prediction.

24:18.440 --> 24:21.440
Yeah, we are also considering that.

24:24.440 --> 24:29.440
Sometimes, optimality is changing based on the cloud service provider.

24:29.440 --> 24:35.440
Do you also consider this or is this only based on regular hardware?

24:35.440 --> 24:38.440
Or what is your optimization?

24:38.440 --> 24:42.440
Yeah, we are considering that too.

24:42.440 --> 24:49.440
I mean, the way to optimize based on the location of the VM.

24:49.440 --> 24:56.440
It's not the same half a VM in your on-premise cloud than on the public cloud or on the edge.

24:56.440 --> 25:03.440
So, it's based on different policies that we are currently defining.

25:03.440 --> 25:05.440
Yeah, we are considering that.

25:08.440 --> 25:09.440
Any more questions?

25:09.440 --> 25:13.440
Okay, thank you so much.

