WEBVTT

00:00.000 --> 00:17.140
Okay, this is going to be interesting.

00:17.140 --> 00:21.720
We are relying on the Wi-Fi bit here as well.

00:21.720 --> 00:24.560
So it would actually help if you turn off your Wi-Fi.

00:24.560 --> 00:26.560
I know that's a big ask.

00:27.520 --> 00:29.680
Consider that for the next half an hour.

00:29.680 --> 00:31.680
That would be really helpful.

00:31.680 --> 00:35.000
So Vanessa is live here through a video call.

00:35.000 --> 00:37.000
Give us away, Vanessa.

00:37.000 --> 00:39.000
We can...

00:39.000 --> 00:41.000
Well, can you try speaking?

00:41.000 --> 00:43.000
What's up, folks?

00:43.000 --> 00:45.000
Sorry, I'm not working.

00:45.000 --> 00:47.000
Is that working?

00:57.000 --> 00:59.000
Try again?

00:59.000 --> 01:01.000
Still what's up, son of them?

01:01.000 --> 01:03.000
Okay, that's really better.

01:03.000 --> 01:05.000
Nice.

01:05.000 --> 01:09.000
So we'll start your recording, Vanessa,

01:09.000 --> 01:11.000
and then we'll try and do live Q&A at the end.

01:11.000 --> 01:13.000
Sounds good.

01:13.000 --> 01:15.000
I have some answers for the previous Q&A, too,

01:15.000 --> 01:17.000
so we can talk a little bit about that.

01:17.000 --> 01:19.000
We can try.

01:19.000 --> 01:21.000
We can try.

01:21.000 --> 01:23.000
By the way, Vanessa is also the one who designed

01:23.000 --> 01:25.000
the HPC social logo.

01:25.440 --> 01:27.440
So you should thank her for that

01:27.440 --> 01:29.440
and take some stickers when you leave.

01:29.440 --> 01:31.440
Thank you.

01:31.440 --> 01:33.440
Thank you.

01:33.440 --> 01:35.440
All right, here comes the talk.

01:39.440 --> 01:41.440
Hi, folks.

01:41.440 --> 01:43.440
I'm Vanessa Socket,

01:43.440 --> 01:45.440
and today we're going to be talking about

01:45.440 --> 01:47.440
Kubernetes and HPC,

01:47.440 --> 01:49.440
the bare metal bros.

01:49.440 --> 01:51.440
So I thought I would open this talk

01:51.440 --> 01:53.440
by putting two words on the slide

01:53.880 --> 01:55.880
and then I'll go to the next question.

01:55.880 --> 01:57.880
So, what is the question

01:57.880 --> 01:59.880
that you guys have been asking

01:59.880 --> 02:01.880
or very anxious?

02:01.880 --> 02:03.880
Those words are cloud and HPC.

02:03.880 --> 02:05.880
So probably the question on everyone's mind

02:05.880 --> 02:07.880
is what does the future look like?

02:07.880 --> 02:09.880
I'm going to answer this question

02:09.880 --> 02:11.880
by posing a question back to you.

02:11.880 --> 02:13.880
Where is the money going?

02:13.880 --> 02:15.880
So we can look at polls

02:15.880 --> 02:17.880
from Gartner and Hyperion Research

02:17.880 --> 02:19.880
that suggests that cloud

02:19.880 --> 02:21.880
is projected to reach

02:22.320 --> 02:24.320
$40 billion by 2026

02:24.320 --> 02:26.320
with a smaller CGR of 6.4%.

02:26.320 --> 02:28.320
So very superficially speaking,

02:28.320 --> 02:30.320
the money is going to cloud.

02:30.320 --> 02:32.320
Now, we can also then follow up on this question

02:32.320 --> 02:34.320
like, okay, that's great,

02:34.320 --> 02:36.320
but who's going to get left behind?

02:36.320 --> 02:38.320
We can look at a paper

02:38.320 --> 02:40.320
from Reed Gannon and Degar from 2023

02:40.320 --> 02:42.320
that identified some really interesting trends.

02:42.320 --> 02:44.320
For HPC, it suggested

02:44.320 --> 02:46.320
that the way that we design our system

02:46.320 --> 02:48.320
will not be a problem

02:48.320 --> 02:50.320
because we're not going to be

02:50.760 --> 02:52.760
able to design our system

02:52.760 --> 02:54.760
will not continue to work.

02:54.760 --> 02:56.760
We cannot depend on dentered scaling

02:56.760 --> 02:58.760
and Moore's law.

02:58.760 --> 03:00.760
There's increasing rising costs

03:00.760 --> 03:02.760
for improved semiconductors.

03:02.760 --> 03:04.760
This is going to make it harder

03:04.760 --> 03:06.760
and increasingly more expensive

03:06.760 --> 03:08.760
and laborious to deploy new systems.

03:08.760 --> 03:10.760
And they define something called NREs

03:10.760 --> 03:12.760
or Non-Reoccurring Engineering Costs

03:12.760 --> 03:14.760
that we are incurring for every new system.

03:14.760 --> 03:16.760
Now, cloud, on the other hand,

03:16.760 --> 03:18.760
is leading the space of innovation.

03:19.200 --> 03:21.200
As we know, there's this massive expansion

03:21.200 --> 03:23.200
of large-scale commercial clouds.

03:23.200 --> 03:25.200
They are not depending

03:25.200 --> 03:27.200
on software vendors or hardware vendors.

03:27.200 --> 03:29.200
They're making their own stuff in-house.

03:29.200 --> 03:31.200
And guess what?

03:31.200 --> 03:33.200
They're hiring away and attracting the talent pool.

03:33.200 --> 03:35.200
And they made a really interesting analogy

03:35.200 --> 03:37.200
with temperature.

03:37.200 --> 03:39.200
They described HPC at endothermic

03:39.200 --> 03:41.200
requiring the absorption of heat

03:41.200 --> 03:43.200
for survival.

03:43.200 --> 03:45.200
And cloud is exothermic

03:45.200 --> 03:47.200
and really giving off of heat.

03:47.640 --> 03:49.640
And we know that, folks,

03:49.640 --> 03:51.640
we're not talking about heat here.

03:51.640 --> 03:53.640
We are talking about money.

03:53.640 --> 03:55.640
But to continue the heat analogy,

03:55.640 --> 03:57.640
you'll know that if you've ever been

03:57.640 --> 03:59.640
out in the snow

03:59.640 --> 04:01.640
in a cold environment,

04:01.640 --> 04:03.640
you are much more likely

04:03.640 --> 04:05.640
going to be wanting to give off heat

04:05.640 --> 04:07.640
to survive.

04:07.640 --> 04:09.640
So who gets left behind?

04:09.640 --> 04:11.640
Well, the person that needs

04:11.640 --> 04:13.640
to constantly absorb heat

04:13.640 --> 04:15.640
that's probably going to run out

04:16.080 --> 04:18.080
is the person that needs to absorb heat.

04:18.080 --> 04:20.080
And that's the reason

04:20.080 --> 04:22.080
that we're all here.

04:22.080 --> 04:24.080
It's because we need to ensure

04:24.080 --> 04:26.080
that the needs of our science

04:26.080 --> 04:28.080
are represented in this new environment.

04:28.080 --> 04:30.080
And guess what?

04:30.080 --> 04:32.080
The success of our science,

04:32.080 --> 04:34.080
the reason that we're all here,

04:34.080 --> 04:36.080
really depends on our ability

04:36.080 --> 04:38.080
to be collaborative

04:38.080 --> 04:40.080
in this space.

04:40.080 --> 04:42.080
And so this is really kind of

04:42.080 --> 04:44.080
the manifesto of Converge Computing.

04:44.520 --> 04:46.520
So if we bring them together,

04:46.520 --> 04:48.520
we get this new technology space

04:48.520 --> 04:50.520
where we have the best of both worlds.

04:50.520 --> 04:52.520
So where do we start?

04:52.520 --> 04:54.520
Well, here is how the talk

04:54.520 --> 04:56.520
is going to proceed today.

04:56.520 --> 04:58.520
We're going to start with models for convergence,

04:58.520 --> 05:00.520
talking about patterns

05:00.520 --> 05:02.520
for bringing together traditionally disparate environments.

05:02.520 --> 05:04.520
We're then going to move into strategies

05:04.520 --> 05:06.520
for convergence.

05:06.520 --> 05:08.520
So designs that I've noticed

05:08.520 --> 05:10.520
allow for easy movement

05:10.520 --> 05:12.520
between the spaces.

05:12.960 --> 05:14.960
So let's start with

05:14.960 --> 05:16.960
those models for convergence.

05:16.960 --> 05:18.960
Now, if you've looked in paper land,

05:18.960 --> 05:20.960
you've probably seen many different models.

05:20.960 --> 05:22.960
There's many different ways

05:22.960 --> 05:24.960
to take HPC and cloud

05:24.960 --> 05:26.960
and put them together.

05:26.960 --> 05:28.960
I'm going to talk about the high-level patterns

05:28.960 --> 05:30.960
and from the perspective of someone

05:30.960 --> 05:32.960
that's maybe deploying a system.

05:32.960 --> 05:34.960
So let's say that's me,

05:34.960 --> 05:36.960
and let's say I want my cloud and HPC,

05:36.960 --> 05:38.960
I'm going to take my limited set of resources

05:38.960 --> 05:40.960
and I'm going to try to split them

05:40.960 --> 05:42.960
into two steps.

05:42.960 --> 05:44.960
So I spend a ton of money and I do this,

05:44.960 --> 05:46.960
and then,

05:46.960 --> 05:48.960
I chose poorly.

05:48.960 --> 05:50.960
No one's using half my resources,

05:50.960 --> 05:52.960
and oh my god, so four years later

05:52.960 --> 05:54.960
I come back and I'm like, all right,

05:54.960 --> 05:56.960
I want cloud, X or HPC exclusive or HPC.

05:56.960 --> 05:58.960
I understand I can't have my cake and you to choose,

05:58.960 --> 06:00.960
so I am just going to choose one.

06:00.960 --> 06:02.960
We've used HPC for all these years,

06:02.960 --> 06:04.960
red and butter, this is why you've always done things.

06:04.960 --> 06:06.960
I choose HPC.

06:06.960 --> 06:08.960
Great, six months later, someone comes into my office.

06:10.960 --> 06:12.960
Are we dinosaurs?

06:12.960 --> 06:14.960
You know, everyone over there is using YAML and automation

06:14.960 --> 06:16.960
and we have this old setup and

06:16.960 --> 06:18.960
ah, so you go back in your office,

06:18.960 --> 06:20.960
you contemplate your life choice and you're like,

06:20.960 --> 06:22.960
oh right, no, it's okay, I'm not going to wait another four years.

06:22.960 --> 06:24.960
I'm going to sneak it in.

06:24.960 --> 06:26.960
So this is where you see all of these ideas,

06:26.960 --> 06:28.960
like bursting, multi-cluster,

06:28.960 --> 06:30.960
and these are generally referring to this idea

06:30.960 --> 06:32.960
of having some home base of resources

06:32.960 --> 06:34.960
and reaching out to get more.

06:34.960 --> 06:36.960
And the problem with this approach as I see it

06:36.960 --> 06:38.960
is that the complexity of these approaches

06:38.960 --> 06:40.960
often reflects the complexity of the systems.

06:40.960 --> 06:42.960
So they tend to be snowflake,

06:42.960 --> 06:44.960
they tend to be complex, and this is why there hasn't been

06:44.960 --> 06:46.960
like a single leader that has emerged in the space.

06:46.960 --> 06:48.960
So here is a different idea

06:48.960 --> 06:50.960
that's less common because it doesn't

06:50.960 --> 06:52.960
superficially kind of make sense.

06:52.960 --> 06:54.960
I want cloud or HPC, meaning I want to be able to run HPC,

06:54.960 --> 06:56.960
or cloud,

06:56.960 --> 06:58.960
or at the same time,

06:58.960 --> 07:00.960
or something together that's more converged,

07:00.960 --> 07:02.960
like what the heck

07:02.960 --> 07:04.960
am I talking about,

07:04.960 --> 07:06.960
don't I?

07:06.960 --> 07:08.960
Am I talking about,

07:08.960 --> 07:10.960
don't worry, we'll talk about it.

07:10.960 --> 07:12.960
Let's first talk about strategies for convergence.

07:12.960 --> 07:14.960
So these strategies

07:14.960 --> 07:16.960
I need to point out, these are not just about the technology,

07:16.960 --> 07:18.960
they are also about the people

07:18.960 --> 07:20.960
which is often harder.

07:20.960 --> 07:22.960
The first is common goals.

07:22.960 --> 07:24.960
In order to get two different communities working together,

07:24.960 --> 07:26.960
they have to care about the same things.

07:26.960 --> 07:28.960
You can't get around that.

07:28.960 --> 07:30.960
The second is modularity.

07:30.960 --> 07:32.960
So the degree to which your application or infrastructure

07:32.960 --> 07:34.960
can be modular,

07:34.960 --> 07:36.960
is that you can use things interchangeably

07:36.960 --> 07:38.960
and swap them, be very creative.

07:38.960 --> 07:40.960
The third is integration.

07:40.960 --> 07:42.960
This is consumption of an entire thing

07:42.960 --> 07:44.960
in another thing by way of different strategies.

07:44.960 --> 07:46.960
So let me give you some examples.

07:46.960 --> 07:48.960
For goals,

07:48.960 --> 07:50.960
the best overlap of goals I've seen

07:50.960 --> 07:52.960
is with respect to batch workloads.

07:52.960 --> 07:54.960
So a few years ago, the Kubernetes community

07:54.960 --> 07:56.960
started the batch working group,

07:56.960 --> 07:58.960
and this was because this new need to have

07:58.960 --> 08:00.960
AI ML workloads in Kubernetes.

08:00.960 --> 08:02.960
Traditionally, Kubernetes is where you run services,

08:02.960 --> 08:04.960
you keep something running.

08:04.960 --> 08:06.960
And there wasn't this concept of starting something

08:06.960 --> 08:08.960
and having it complete, but all of a sudden

08:08.960 --> 08:10.960
there was this new need, and guess what?

08:10.960 --> 08:12.960
We have been doing that in HPC land

08:12.960 --> 08:14.960
for like a couple of decades now.

08:14.960 --> 08:16.960
Modularity, a really great example,

08:16.960 --> 08:18.960
is actually with Kubernetes and Flux Framework.

08:18.960 --> 08:20.960
So you may think of Flux as just like

08:20.960 --> 08:22.960
this workload manager,

08:22.960 --> 08:24.960
but actually it's called a framework

08:24.960 --> 08:26.960
because we assemble

08:26.960 --> 08:28.960
many different components together

08:28.960 --> 08:30.960
to assemble into the workload manager

08:30.960 --> 08:32.960
known as Flux.

08:32.960 --> 08:34.960
Kubernetes is the same,

08:34.960 --> 08:36.960
different set of components,

08:36.960 --> 08:38.960
and there is going to be a creative way

08:38.960 --> 08:40.960
that we can kind of use these interchangeably.

08:40.960 --> 08:42.960
So the final example of integration,

08:42.960 --> 08:44.960
the best technologies I can provide

08:44.960 --> 08:46.960
are containers and language bindings.

08:46.960 --> 08:48.960
Container technologies are literally this vehicle

08:48.960 --> 08:50.960
to let you move between spaces,

08:50.960 --> 08:52.960
and language bindings are going to let you take it traditionally

08:52.960 --> 08:54.960
like C++ HPC project

08:54.960 --> 08:56.960
and extend it into a language

08:56.960 --> 08:58.960
that is native to the language

08:58.960 --> 09:00.960
and extend it into a language

09:00.960 --> 09:02.960
that is native to cloud.

09:02.960 --> 09:04.960
So for example, Go.

09:04.960 --> 09:06.960
Alrighty, let's get into some examples

09:06.960 --> 09:08.960
just like eggs three ways.

09:08.960 --> 09:10.960
Here are some projects that we've actually been working on

09:10.960 --> 09:12.960
at the lab. The first is Fluids.

09:12.960 --> 09:14.960
As I alluded to, this is the Flux scheduler,

09:14.960 --> 09:16.960
swapped with Coop scheduler.

09:16.960 --> 09:18.960
The next is the Flux operator,

09:18.960 --> 09:20.960
the entirety of Flux Framework

09:20.960 --> 09:22.960
implemented inside of Kubernetes.

09:22.960 --> 09:24.960
And then the namesake

09:24.960 --> 09:26.960
of this talk about air battle grows,

09:26.960 --> 09:28.960
Flux and Kubernetes working side by side.

09:28.960 --> 09:30.960
So let's start

09:30.960 --> 09:32.960
with the Flux scheduler within Kubernetes.

09:32.960 --> 09:34.960
You may be familiar with Kubernetes when you launch a job.

09:34.960 --> 09:36.960
You ask for a certain number of resources

09:36.960 --> 09:38.960
that's given to the scheduler.

09:38.960 --> 09:40.960
The scheduler says, okay, here are four pods.

09:40.960 --> 09:42.960
Have a nice day.

09:42.960 --> 09:44.960
So what we're going to do is bring in Fluents.

09:44.960 --> 09:46.960
So our C++ package, FluxSched,

09:46.960 --> 09:48.960
that is mapped with Go bindings

09:48.960 --> 09:50.960
into a custom scheduler plugin.

09:50.960 --> 09:52.960
We're going to swap it.

09:52.960 --> 09:54.960
And so you're basically going to be asking

09:54.960 --> 09:56.960
for the same amount of resources,

09:56.960 --> 09:58.960
but the scheduling is going to be done

09:58.960 --> 10:00.960
by FluxSched.

10:00.960 --> 10:02.960
How does this do?

10:02.960 --> 10:04.960
Well, we find that the workflows run three times faster.

10:04.960 --> 10:06.960
So what you're seeing here is Coop scheduler

10:06.960 --> 10:08.960
on the top, Fluents on the bottom.

10:08.960 --> 10:10.960
You see a lot of randomness with respect to

10:10.960 --> 10:12.960
how Coop scheduler places jobs.

10:12.960 --> 10:14.960
What this leads to is a pathological scheduling pattern.

10:14.960 --> 10:16.960
So anywhere you see a red box on there,

10:16.960 --> 10:18.960
that is a startup delay.

10:18.960 --> 10:20.960
And what that means in practice is though,

10:20.960 --> 10:22.960
is that although the workloads themselves

10:22.960 --> 10:24.960
run in similar times,

10:24.960 --> 10:26.960
we have a lot of outliers.

10:26.960 --> 10:28.960
We have a lot of jobs that take a really long time

10:28.960 --> 10:30.960
to get started.

10:30.960 --> 10:32.960
And so Fluents improves upon us.

10:32.960 --> 10:34.960
So Fluents is a really great example

10:34.960 --> 10:36.960
of modularity because we're taking

10:36.960 --> 10:38.960
an HPC technology

10:38.960 --> 10:40.960
and we're literally swapping it.

10:40.960 --> 10:42.960
And the modularity of the software allows for that.

10:42.960 --> 10:44.960
It's also a great example of integration.

10:44.960 --> 10:46.960
Because we have those Go bindings,

10:46.960 --> 10:48.960
we can speak the language

10:48.960 --> 10:50.960
of the cloud need of communities.

10:50.960 --> 10:52.960
Alrighty, next project, the Flex Operator.

10:52.960 --> 10:54.960
Super cool.

10:54.960 --> 10:56.960
All the gophers in Flexland are pretty cool.

10:56.960 --> 10:58.960
Alright, so the Flex Operator

10:58.960 --> 11:00.960
is implementing the entirety of Flex

11:00.960 --> 11:02.960
framework inside of Kubernetes,

11:02.960 --> 11:04.960
your own HPC cluster.

11:04.960 --> 11:06.960
This happens by way of a custom resource

11:06.960 --> 11:08.960
definition of CRD, where you basically

11:08.960 --> 11:10.960
give all the parameters that you want

11:10.960 --> 11:12.960
for your cluster, whether that's a single job

11:12.960 --> 11:14.960
or whether you want an interactive cluster.

11:14.960 --> 11:16.960
This creates what we call the mini cluster,

11:16.960 --> 11:18.960
which, you know, Flex Operator

11:18.960 --> 11:20.960
is a mini cluster, which, you know,

11:20.960 --> 11:22.960
Flux doesn't know the difference that it's running

11:22.960 --> 11:24.960
in Kubernetes versus on bare metal.

11:24.960 --> 11:26.960
There's a lead broker that's connected

11:26.960 --> 11:28.960
to several follower brokers.

11:28.960 --> 11:30.960
So here you have one pod for one physical node.

11:30.960 --> 11:32.960
The tree based overlay network

11:32.960 --> 11:34.960
within each pod or node,

11:34.960 --> 11:36.960
you have Flux that's added on the fly to your application.

11:36.960 --> 11:38.960
And the Operator is just going to basically

11:38.960 --> 11:40.960
reconcile until the state

11:40.960 --> 11:42.960
that you need for your cluster matches

11:42.960 --> 11:44.960
the actual state of the cluster.

11:44.960 --> 11:46.960
How well does it do?

11:46.960 --> 11:48.960
We added it to the best in the space last year.

11:48.960 --> 11:50.960
The MPI Operator and the Flux Operator

11:50.960 --> 11:52.960
consistently outperformed the MPI Operator

11:52.960 --> 11:54.960
we believe because of the 0MQ bootstrap.

11:54.960 --> 11:56.960
So the Flux Operator

11:56.960 --> 11:58.960
is a beautiful example of integration

11:58.960 --> 12:00.960
because we're taking the entirety of Flux

12:00.960 --> 12:02.960
framework and implementing it inside of Kubernetes.

12:02.960 --> 12:04.960
Bro, bro, bro, is it time

12:04.960 --> 12:06.960
for the bare metal bro?

12:06.960 --> 12:08.960
Yeah!

12:08.960 --> 12:10.960
Okay, so, warning.

12:10.960 --> 12:12.960
I've been saying bare metal,

12:12.960 --> 12:14.960
but nobody's going to give me bare metal.

12:14.960 --> 12:16.960
Let's be frank about that.

12:16.960 --> 12:18.960
So I was using virtual machine.

12:18.960 --> 12:20.960
We're using virtual machine as a proxy

12:20.960 --> 12:22.960
for bare metal.

12:22.960 --> 12:24.960
So just a warning.

12:24.960 --> 12:26.960
So what's different about this picture?

12:26.960 --> 12:28.960
The orange is on the outside.

12:28.960 --> 12:30.960
So we actually have Flux framework on the outside

12:30.960 --> 12:32.960
spinning up a Kubernetes cluster

12:32.960 --> 12:34.960
and notice that we actually still have

12:34.960 --> 12:36.960
compute running on bare metal alongside Kubernetes.

12:36.960 --> 12:38.960
How's that possible?

12:38.960 --> 12:40.960
Don't worry, I'll tell you.

12:40.960 --> 12:42.960
So why do we need this in the first place?

12:42.960 --> 12:44.960
As you know, also,

12:44.960 --> 12:46.960
there are increasingly more

12:46.960 --> 12:48.960
complex heterogeneous workloads

12:48.960 --> 12:50.960
that are coming to HPC.

12:50.960 --> 12:52.960
So this means not just, you know,

12:52.960 --> 12:54.960
embarrassingly parallel stuff,

12:54.960 --> 12:56.960
but also adding in services,

12:56.960 --> 12:58.960
databases, task queues.

12:58.960 --> 13:00.960
Ah!

13:00.960 --> 13:02.960
Okay, so I was...

13:02.960 --> 13:04.960
This slide is not wrong.

13:04.960 --> 13:06.960
I was going to give you an example

13:06.960 --> 13:08.960
of such a workload, and apparently this slide

13:08.960 --> 13:10.960
is giving you this warning that I'm a bad scientist

13:10.960 --> 13:12.960
and I'm not wrong, but I will point out

13:12.960 --> 13:14.960
that my example is actually a very good example

13:14.960 --> 13:16.960
that is a prototype for this kind of design.

13:16.960 --> 13:18.960
Let's talk about that.

13:18.960 --> 13:20.960
So let's say that we're running simulations.

13:20.960 --> 13:22.960
We're training examples

13:22.960 --> 13:24.960
one through N, whatever, doesn't matter,

13:24.960 --> 13:26.960
and we want to send them

13:26.960 --> 13:28.960
to a machine learning server, a specific endpoint

13:28.960 --> 13:30.960
to do the training.

13:30.960 --> 13:32.960
We then want to wait till some metric of goodness

13:32.960 --> 13:34.960
or perhaps a number of samples,

13:34.960 --> 13:36.960
and then we want to flip it around.

13:36.960 --> 13:38.960
We want to run simulations again,

13:38.960 --> 13:40.960
but we want to instead give this

13:40.960 --> 13:42.960
to our machine learning server

13:42.960 --> 13:44.960
without the actual values,

13:44.960 --> 13:46.960
then we're going to have a vector of the true values

13:46.960 --> 13:48.960
and the predictions, and we're going to see how well we did.

13:48.960 --> 13:50.960
Now, very superficially,

13:50.960 --> 13:52.960
if we match this to HPC

13:52.960 --> 13:54.960
versus Kubernetes, this is how we do it.

13:54.960 --> 13:56.960
We would expect that the simulations

13:56.960 --> 13:58.960
would run better on bare metal,

13:58.960 --> 14:00.960
and the service thing would run better

14:00.960 --> 14:02.960
in user netties or Kubernetes.

14:02.960 --> 14:04.960
This is way to be...

14:04.960 --> 14:06.960
We need to prove to ourselves first.

14:06.960 --> 14:08.960
So a lot of you are probably out there like,

14:08.960 --> 14:10.960
user net, like, Kubernetes?

14:10.960 --> 14:12.960
Like, in user things, are you nuts?

14:12.960 --> 14:14.960
I'm not nuts.

14:14.960 --> 14:16.960
There's actually something called user netties.

14:16.960 --> 14:18.960
It's a Kubernetes enhancement proposal

14:18.960 --> 14:20.960
or CUP proposal in 2022

14:20.960 --> 14:22.960
by a very talented developer named

14:22.960 --> 14:24.960
Akihiro Sudo.

14:24.960 --> 14:26.960
Akihiro must point out

14:26.960 --> 14:28.960
won the top maintainer award

14:28.960 --> 14:30.960
for KUKON last year.

14:30.960 --> 14:32.960
He's an incredibly talented developer.

14:32.960 --> 14:34.960
If you've used any of these technologies,

14:34.960 --> 14:36.960
he's the one behind it.

14:36.960 --> 14:38.960
Hats off to Akihiro.

14:38.960 --> 14:40.960
So last year, at the beginning of the year,

14:40.960 --> 14:42.960
user netties was really a hodgepodge

14:42.960 --> 14:44.960
with kind of bash grips. It was really hard to use.

14:44.960 --> 14:46.960
So I engaged with Akihiro

14:46.960 --> 14:48.960
and we released Generation 2

14:48.960 --> 14:50.960
of user netties in September.

14:50.960 --> 14:52.960
And guess what?

14:52.960 --> 14:54.960
It is using containerization, which is really great.

14:54.960 --> 14:56.960
It has these components that we'll go into

14:56.960 --> 14:58.960
in more detail.

14:58.960 --> 15:00.960
So what does it mean in practice?

15:00.960 --> 15:02.960
Well, it means when you're building a virtual machine,

15:02.960 --> 15:04.960
you need to have C groups version to enable.

15:04.960 --> 15:06.960
I recommend LIMA or Linux virtual machines

15:06.960 --> 15:08.960
if you're prototyping this for the first time.

15:08.960 --> 15:10.960
It also means that you need

15:10.960 --> 15:12.960
to enable these kernel modules.

15:12.960 --> 15:14.960
So very generally speaking, the RNet filter

15:14.960 --> 15:16.960
is going to allow you to apply IP tables, rules, bridge traffic.

15:16.960 --> 15:18.960
VXLan is going to allow you to connect VXLan devices

15:18.960 --> 15:20.960
on different hosts to a standalone bridge.

15:20.960 --> 15:22.960
This is important because we actually have

15:22.960 --> 15:24.960
different physical nodes.

15:24.960 --> 15:26.960
Now it's going to use RULE stocker.

15:26.960 --> 15:28.960
This isn't such a crazy idea anymore.

15:28.960 --> 15:30.960
Many clusters have podmin these days.

15:30.960 --> 15:32.960
And so what does it mean?

15:32.960 --> 15:34.960
Actually, when you bring out these VMs,

15:34.960 --> 15:36.960
it means that you're going to run a make up command

15:36.960 --> 15:38.960
that has two contexts.

15:38.960 --> 15:40.960
So both of them are going to build and start a base image

15:40.960 --> 15:42.960
that is using kind, kubernetes,

15:42.960 --> 15:44.960
and Docker with CNI plugins.

15:44.960 --> 15:46.960
And then the two contexts are the control plane

15:46.960 --> 15:48.960
and the worker.

15:48.960 --> 15:50.960
The control plane is going to install Flano,

15:50.960 --> 15:52.960
run kubernetes, and admit.

15:52.960 --> 15:54.960
This makes a joint command which is basically a token

15:54.960 --> 15:56.960
that you give to the workers,

15:56.960 --> 15:58.960
and then the togers can authenticate and join the cluster.

15:58.960 --> 16:00.960
And so that's what they do.

16:00.960 --> 16:02.960
They're just like, I'm ready to serve.

16:02.960 --> 16:04.960
All right, so we created this garbage cluster

16:04.960 --> 16:06.960
small and mighty using Overt and Ansible.

16:06.960 --> 16:08.960
It is small and mighty because each has

16:08.960 --> 16:10.960
eight cores and 30 MBs RAM

16:10.960 --> 16:12.960
and a 10-NVVD iterate.

16:12.960 --> 16:14.960
And I want to point out that we have

16:14.960 --> 16:16.960
seven nodes here because generally speaking,

16:16.960 --> 16:18.960
we're going to have six that we run

16:18.960 --> 16:20.960
things with compute on and one's going to be

16:20.960 --> 16:22.960
an admin node or control plane.

16:22.960 --> 16:24.960
Again,

16:24.960 --> 16:26.960
warning, not bare metal,

16:26.960 --> 16:28.960
you get the deal.

16:28.960 --> 16:30.960
All right, so what's in these VMs when we bring them up?

16:30.960 --> 16:32.960
We have a complete system

16:32.960 --> 16:34.960
install a flux,

16:34.960 --> 16:36.960
singularity on bare metal for reasons I'll tell you a little bit.

16:36.960 --> 16:38.960
Lamps installed on bare metal

16:38.960 --> 16:40.960
and of course user netties ready to be brought up.

16:40.960 --> 16:42.960
So once I shell into these VMs,

16:42.960 --> 16:44.960
my flux cluster is ready to go.

16:44.960 --> 16:46.960
I can do flux resource list and I can see all my nodes.

16:46.960 --> 16:48.960
And user netties, again,

16:48.960 --> 16:50.960
that administrative node is also a control plane.

16:50.960 --> 16:52.960
So we technically have six nodes to work with.

16:52.960 --> 16:54.960
And then we have a user netties.

16:54.960 --> 16:56.960
So we technically have six nodes to work with.

16:56.960 --> 16:58.960
And we can still see them with

16:58.960 --> 17:00.960
coop control get nodes.

17:00.960 --> 17:02.960
Here's what we're working with.

17:02.960 --> 17:04.960
User netties and flux running side by side

17:04.960 --> 17:06.960
the bare metal bros.

17:06.960 --> 17:08.960
All right, bro, bro, what experiments

17:08.960 --> 17:10.960
do we want to run all of them, bro?

17:10.960 --> 17:12.960
All right.

17:12.960 --> 17:14.960
So we first need to sanity check

17:14.960 --> 17:16.960
that what I said earlier about the bare metal

17:16.960 --> 17:18.960
and lamps and the simulations is actually true.

17:18.960 --> 17:20.960
We need to look at

17:20.960 --> 17:22.960
application performance between flux

17:22.960 --> 17:24.960
and user netties.

17:24.960 --> 17:26.960
So the way we're going to do that is by running a few things.

17:26.960 --> 17:28.960
We're first going to run lamps on bare metal with flux.

17:28.960 --> 17:30.960
We're then going to do the same thing

17:30.960 --> 17:32.960
but in a singularity container.

17:32.960 --> 17:34.960
And I did this just to demonstrate that you don't

17:34.960 --> 17:36.960
lose anything by using containers.

17:36.960 --> 17:38.960
Here's great.

17:38.960 --> 17:40.960
We're then going to run lamps in user netties

17:40.960 --> 17:42.960
with the flux operator.

17:42.960 --> 17:44.960
And then finally we're going to repeat cases one and two,

17:44.960 --> 17:46.960
but with user netties running in the background

17:46.960 --> 17:48.960
to look to see if there's any overhead of that.

17:48.960 --> 17:50.960
And I need to pause for a second

17:50.960 --> 17:52.960
because I know how incredibly cool this third case is.

17:52.960 --> 17:54.960
We have

17:54.960 --> 17:56.960
flux on the outside.

17:56.960 --> 17:58.960
Flux is running

17:58.960 --> 18:00.960
user netties.

18:00.960 --> 18:02.960
Within that we are launching the flux operator

18:02.960 --> 18:04.960
which is bringing up another instance of flux

18:04.960 --> 18:06.960
and inside there is where lamps is running.

18:06.960 --> 18:08.960
So folks, like I know Thanksgiving is over

18:08.960 --> 18:10.960
but this is the ultimate

18:10.960 --> 18:12.960
production.

18:12.960 --> 18:14.960
And we expect lamps to be slower in user netties

18:14.960 --> 18:16.960
because as we know it makes MPI collective calls.

18:16.960 --> 18:18.960
User netties are using

18:18.960 --> 18:20.960
something called SERP 4.NET NS

18:20.960 --> 18:22.960
which requires additional processing of packets

18:22.960 --> 18:24.960
with a tap device. I have a great paper I can share

18:24.960 --> 18:26.960
if you're interested in learning more about that.

18:26.960 --> 18:28.960
So drumroll the results

18:28.960 --> 18:30.960
as we expected the

18:30.960 --> 18:32.960
well actually maybe we didn't expect but guess what

18:32.960 --> 18:34.960
the bare metal case is the singularity container

18:34.960 --> 18:36.960
is very comparable to actual bare metal.

18:36.960 --> 18:38.960
I was very surprised by this.

18:38.960 --> 18:40.960
So user netties does not

18:40.960 --> 18:42.960
add a lot of overhead.

18:42.960 --> 18:44.960
And this is what we'd expected that guy up there

18:44.960 --> 18:46.960
running in user netties

18:46.960 --> 18:48.960
is about twice as slow as running

18:48.960 --> 18:50.960
on bare metal.

18:50.960 --> 18:52.960
So what did we learn?

18:52.960 --> 18:54.960
Well, we learned that for a setup like this

18:54.960 --> 18:56.960
the network sensitive stuff

18:56.960 --> 18:58.960
probably should be run on the HPC.

18:58.960 --> 19:00.960
But I'll point out

19:00.960 --> 19:02.960
there's opportunity for improving this in user netties.

19:02.960 --> 19:04.960
If you have experience with networking

19:04.960 --> 19:06.960
I'd like you to go over to the GitHub

19:06.960 --> 19:08.960
right now and I'm just going to wait a lot for the talk

19:08.960 --> 19:10.960
and engage with that to hear it to work on this problem.

19:10.960 --> 19:12.960
Now the next thing we want

19:12.960 --> 19:14.960
to look at is distributed machine learning

19:14.960 --> 19:16.960
specifically two cases

19:16.960 --> 19:18.960
one distributed to across six nodes

19:18.960 --> 19:20.960
and then the second on one node

19:20.960 --> 19:22.960
so the distributed case network is a

19:22.960 --> 19:24.960
variable and for the one node obviously

19:24.960 --> 19:26.960
network is not a variable.

19:26.960 --> 19:28.960
Drum roll results same thing

19:28.960 --> 19:30.960
it's about twice as fast

19:30.960 --> 19:32.960
on bare metal or

19:32.960 --> 19:34.960
twice as slow I guess on user

19:34.960 --> 19:36.960
netties.

19:36.960 --> 19:38.960
And interestingly when you look at just a

19:38.960 --> 19:40.960
single node these are really

19:40.960 --> 19:42.960
comparable so there's no issue with running

19:42.960 --> 19:44.960
something on a single node in user netties

19:44.960 --> 19:46.960
in and of itself it's really when you bring

19:46.960 --> 19:48.960
in the networking that it becomes a variable.

19:48.960 --> 19:50.960
So it's a network right

19:50.960 --> 19:52.960
well let's sanity check one more thing

19:52.960 --> 19:54.960
here's I per thing we did one bit of

19:54.960 --> 19:56.960
transfer for each node as a client to each node

19:56.960 --> 19:58.960
as a server we see bit rate and give you bits

19:58.960 --> 20:00.960
per second is between 10 and 30 for bare metal

20:00.960 --> 20:02.960
user netties with like

20:02.960 --> 20:04.960
non detectable closest here are really really terrible

20:04.960 --> 20:06.960
we can look so we can see the same patterns

20:06.960 --> 20:08.960
for transfer gigabits per second

20:08.960 --> 20:10.960
and so yes

20:10.960 --> 20:12.960
it's the network we're pretty confident

20:12.960 --> 20:14.960
for the setup it's the network.

20:14.960 --> 20:16.960
All right can we do the fun workflow now

20:16.960 --> 20:18.960
we absolutely can so guess

20:18.960 --> 20:20.960
what I actually prototyped this kind

20:20.960 --> 20:22.960
of workflow because I was really excited about it

20:22.960 --> 20:24.960
and so what we're going to do is we're going to

20:24.960 --> 20:26.960
be launching a batch job with flux

20:26.960 --> 20:28.960
batch this means the flux instance

20:28.960 --> 20:30.960
that's only by the running user it's going

20:30.960 --> 20:32.960
to scope resources using hw lock

20:32.960 --> 20:34.960
in this backshot where we can

20:34.960 --> 20:36.960
basically bring up and tear down all

20:36.960 --> 20:38.960
of user netties.

20:38.960 --> 20:40.960
We're going to take that workflow that I mentioned before

20:40.960 --> 20:42.960
we're going to map it into our star track cluster

20:42.960 --> 20:44.960
space so we're going to run simulations with lamps

20:44.960 --> 20:46.960
randomly selecting the problem

20:46.960 --> 20:48.960
sizes predict well time we're then

20:48.960 --> 20:50.960
going to bring up a machine learning server a

20:50.960 --> 20:52.960
special server I made using river a few years

20:52.960 --> 20:54.960
ago and then we're going to

20:54.960 --> 20:56.960
basically do the test cases we're going to run

20:56.960 --> 20:58.960
lamps again but we're going to leave

20:58.960 --> 21:00.960
out the actual well time and we're

21:00.960 --> 21:02.960
going to ask our models what it is

21:02.960 --> 21:04.960
and we're going to do

21:04.960 --> 21:06.960
a thousand training samples and 250

21:06.960 --> 21:08.960
testing samples.

21:08.960 --> 21:10.960
How do we do?

21:10.960 --> 21:12.960
I put no thought into these

21:12.960 --> 21:14.960
particular models but I did

21:14.960 --> 21:16.960
three kinds of regression

21:16.960 --> 21:18.960
the Bayesian and sampling from a probability distribution

21:18.960 --> 21:20.960
didn't do super well but for the

21:20.960 --> 21:22.960
first two there's an actual

21:22.960 --> 21:24.960
kind of pattern between the predicted and the

21:24.960 --> 21:26.960
actual time and so although

21:26.960 --> 21:28.960
I put no thought into this I was really pleased

21:28.960 --> 21:30.960
with this result to see that the general prototype

21:30.960 --> 21:32.960
this idea of having bare

21:32.960 --> 21:34.960
middle simulations running alongside a service

21:34.960 --> 21:36.960
there is something

21:36.960 --> 21:38.960
here we can do science

21:38.960 --> 21:40.960
this way with actual real scientific questions

21:40.960 --> 21:42.960
and I'll point out that there are

21:42.960 --> 21:44.960
real heterogeneous workloads out in the wild

21:44.960 --> 21:46.960
and you this capability here's Moomi

21:46.960 --> 21:48.960
the massively parallel multi-stale machine

21:48.960 --> 21:50.960
learn model infrastructure

21:50.960 --> 21:52.960
and this is basically simulating biological systems

21:52.960 --> 21:54.960
the interact between proteins and plasma

21:54.960 --> 21:56.960
membrane I'll also point out that the

21:56.960 --> 21:58.960
Moomins are what it's based on

21:58.960 --> 22:00.960
the name the finished book comic book series

22:00.960 --> 22:02.960
with really cute hippos with often yellow

22:02.960 --> 22:04.960
spiky hair very awesome

22:04.960 --> 22:06.960
so this is the perfect example

22:06.960 --> 22:08.960
the bare metal rows of coexistence

22:08.960 --> 22:10.960
adopting technologies to make it possible

22:10.960 --> 22:12.960
to go to coexist and

22:12.960 --> 22:14.960
continuing to improve upon them so that

22:14.960 --> 22:16.960
for example with networking

22:16.960 --> 22:18.960
this environment can get even better

22:18.960 --> 22:20.960
so what should you

22:20.960 --> 22:22.960
remember from this talk if you take nothing

22:22.960 --> 22:24.960
else away the first is looking out

22:24.960 --> 22:26.960
for opportunities for collaboration

22:26.960 --> 22:28.960
look for that alignment of goals between

22:28.960 --> 22:30.960
spaces that's an opportunity

22:30.960 --> 22:32.960
the second is providing handles for your

22:32.960 --> 22:34.960
components so you don't have the bandwidth

22:34.960 --> 22:36.960
to look for opportunities add some

22:36.960 --> 22:38.960
go bindings to C++ projects because

22:38.960 --> 22:40.960
someone else could find you

22:40.960 --> 22:42.960
the third is engagement

22:42.960 --> 22:44.960
we need to show up at the table

22:44.960 --> 22:46.960
we need to go to working groups, conferences

22:46.960 --> 22:48.960
places that you haven't traditionally been

22:48.960 --> 22:50.960
to engage in to find these opportunities

22:50.960 --> 22:52.960
for collaboration and possibly

22:52.960 --> 22:54.960
the most important is this

22:54.960 --> 22:56.960
mindset we've had this mindset

22:56.960 --> 22:58.960
of cloud versus HPC

22:58.960 --> 23:00.960
that one has to win

23:00.960 --> 23:02.960
but they're different for so long

23:02.960 --> 23:04.960
we need to throw that away

23:04.960 --> 23:06.960
and get rid of the adversarial thinking

23:06.960 --> 23:08.960
and have a more collaborative

23:08.960 --> 23:10.960
mindset this

23:10.960 --> 23:12.960
is the vision that we have for the future

23:12.960 --> 23:14.960
for converge computing

23:14.960 --> 23:16.960
and we hope that you like to join us

23:16.960 --> 23:18.960
so thank you

23:18.960 --> 23:20.960
that's how to reach me my email

23:20.960 --> 23:22.960
and social networks and here's some interesting links

23:22.960 --> 23:24.960
for the flux and the various projects

23:24.960 --> 23:26.960
I think I will take some questions

23:26.960 --> 23:28.960
virtually now

23:36.960 --> 23:38.960
okay we can take a couple of questions

23:38.960 --> 23:40.960
it seems like the wifi is stable enough

23:40.960 --> 23:42.960
to let Vanessa answer them

23:42.960 --> 23:44.960
do we have any questions

23:44.960 --> 23:46.960
okay

23:48.960 --> 23:50.960
so Vanessa we may have to repeat

23:50.960 --> 23:52.960
a question for you we'll see how that works

23:56.960 --> 23:58.960
hi Vanessa amazing talk

23:58.960 --> 24:00.960
congrats

24:00.960 --> 24:02.960
so I was wondering

24:02.960 --> 24:04.960
if your architecture

24:04.960 --> 24:06.960
can support sidecars

24:06.960 --> 24:08.960
because one of the nightmares I had

24:08.960 --> 24:10.960
when I was trying to do something

24:10.960 --> 24:12.960
similar was that in order

24:12.960 --> 24:14.960
to get the sidecars running

24:14.960 --> 24:16.960
I had to spin up a second

24:16.960 --> 24:18.960
network stack and that created a lot of overhead

24:26.960 --> 24:28.960
so

24:28.960 --> 24:30.960
no

24:48.960 --> 24:50.960
no just one is on

24:52.960 --> 24:54.960
okay did you get the question Vanessa

24:54.960 --> 24:56.960
no I didn't hear the question at all

24:58.960 --> 25:00.960
neither did I

25:00.960 --> 25:02.960
yeah maybe that's better

25:02.960 --> 25:04.960
okay let's do it like this

25:04.960 --> 25:06.960
you'll come up front and ask it here

25:06.960 --> 25:08.960
yeah that's perfect that'd be great

25:08.960 --> 25:10.960
I can hear you great

25:14.960 --> 25:16.960
hi there

25:16.960 --> 25:18.960
hi

25:18.960 --> 25:20.960
so I was wondering if your architecture

25:20.960 --> 25:22.960
can support sidecar containers

25:22.960 --> 25:24.960
because as I was saying

25:24.960 --> 25:26.960
when I was trying to do something similar

25:26.960 --> 25:28.960
when I

25:28.960 --> 25:30.960
tried to create the sidecars I had to

25:30.960 --> 25:32.960
create a second network stack

25:32.960 --> 25:34.960
within singularity so the

25:34.960 --> 25:36.960
network overhead was

25:36.960 --> 25:38.960
amazingly high

25:38.960 --> 25:40.960
so absolutely a flux

25:40.960 --> 25:42.960
operator actually uses a sidecar container

25:42.960 --> 25:44.960
on a net container which is similar

25:44.960 --> 25:46.960
in concept to add flux on the fly

25:46.960 --> 25:48.960
as a view what's going on

25:48.960 --> 25:50.960
in Kubernetes is sort of a different

25:50.960 --> 25:52.960
thing than the networking

25:52.960 --> 25:54.960
issue so the short answer is yes

25:58.960 --> 26:00.960
to kind of add to that though I'm not sure

26:00.960 --> 26:02.960
that singularity and Kubernetes

26:02.960 --> 26:04.960
singularity as the container

26:04.960 --> 26:06.960
runtime for Kubernetes would work

26:06.960 --> 26:08.960
I have never tried that but it

26:08.960 --> 26:10.960
doesn't sound like it would work

26:10.960 --> 26:12.960
yeah it needs to be done

26:12.960 --> 26:14.960
yeah exactly

26:16.960 --> 26:18.960
hi Vanessa thank you

26:18.960 --> 26:20.960
hi it was the most fun presentation

26:20.960 --> 26:22.960
on the post then so far

26:22.960 --> 26:24.960
thank you

26:24.960 --> 26:26.960
so when you were saying that

26:26.960 --> 26:28.960
the main difference between

26:28.960 --> 26:30.960
performance between EBM and bare metal

26:30.960 --> 26:32.960
workloads was related to network

26:32.960 --> 26:34.960
was that the case

26:34.960 --> 26:36.960
also for distributed

26:36.960 --> 26:38.960
training and if that's the case

26:38.960 --> 26:40.960
were you using infini band or not

26:40.960 --> 26:42.960
so this we did

26:42.960 --> 26:44.960
not have infini band and you make a really

26:44.960 --> 26:46.960
good point that this kind of setup would

26:46.960 --> 26:48.960
need to be tested with an actually great

26:48.960 --> 26:50.960
network and that is still a very big

26:50.960 --> 26:52.960
challenge even for cloud

26:52.960 --> 26:54.960
so for example if you use AWS you can bring

26:54.960 --> 26:56.960
the elastic fiber adapter which will give you

26:56.960 --> 26:58.960
great networking performance but if you go

26:58.960 --> 27:00.960
to other clouds and I don't have to name this specifically

27:00.960 --> 27:02.960
you tend to only get really good networks

27:02.960 --> 27:04.960
when it comes to using like TPUs

27:04.960 --> 27:06.960
or GPUs the exception

27:06.960 --> 27:08.960
though is Azure which has a lot of really great

27:08.960 --> 27:10.960
HPC stuff kind of

27:10.960 --> 27:12.960
built in

27:12.960 --> 27:14.960
so absolutely

27:14.960 --> 27:16.960
you can get that setup with infini band

27:20.960 --> 27:22.960
Hi thank you for your talk

27:22.960 --> 27:24.960
I had a smile on my face the whole time

27:24.960 --> 27:26.960
thank you for having such high energy

27:26.960 --> 27:28.960
at the end of the day

27:32.960 --> 27:34.960
what was I going to say

27:36.960 --> 27:38.960
oh yeah so probably in my workloads

27:38.960 --> 27:40.960
I can reduce the

27:40.960 --> 27:42.960
network traffic by a very

27:42.960 --> 27:44.960
large margin if I can

27:44.960 --> 27:46.960
constrain certain jobs to

27:46.960 --> 27:48.960
specific nodes because

27:48.960 --> 27:50.960
then large files don't have to be moved

27:50.960 --> 27:52.960
for certain jobs to

27:52.960 --> 27:54.960
across the network is that something that you

27:54.960 --> 27:56.960
could keep in mind

27:58.960 --> 28:00.960
so if you remember the very quick

28:00.960 --> 28:02.960
machine learning experiment that we showed when we're running something on one node

28:02.960 --> 28:04.960
and you're not using the network

28:04.960 --> 28:06.960
there's no issue so if you're just running

28:06.960 --> 28:08.960
something on one node in user netties

28:08.960 --> 28:10.960
you won't have an issue in a degree to

28:10.960 --> 28:12.960
which you can reduce anything that uses

28:12.960 --> 28:14.960
network so moving data

28:14.960 --> 28:16.960
MPI etc etc you will get

28:16.960 --> 28:18.960
similar performance at least from this small

28:18.960 --> 28:20.960
prototype experiment that we've seen

28:20.960 --> 28:22.960
as you would on bare metal

28:22.960 --> 28:24.960
I have to do this because it wasn't really bare metal

28:28.960 --> 28:30.960
thanks

28:32.960 --> 28:34.960
one more question

28:34.960 --> 28:36.960
hey Vanessa that's Danny

28:36.960 --> 28:38.960
I'm gonna die my hair

28:38.960 --> 28:40.960
soon so you won't recognize me again

28:40.960 --> 28:42.960
I really liked your framing

28:42.960 --> 28:44.960
actually I thought I was going to

28:44.960 --> 28:46.960
sort of being adversarial and then I actually

28:46.960 --> 28:48.960
realized what you were saying and I really appreciated it

28:48.960 --> 28:50.960
however though regarding the

28:50.960 --> 28:52.960
adversarial framing I have

28:52.960 --> 28:54.960
some experience with for example

28:54.960 --> 28:56.960
cloud tools and cloud environments

28:56.960 --> 28:58.960
being used as

28:58.960 --> 29:00.960
platforms for vendor lock-in

29:00.960 --> 29:02.960
I think that you described especially with your converged computing

29:02.960 --> 29:04.960
kind of the way that you can push back against

29:04.960 --> 29:06.960
scientific labs aren't

29:06.960 --> 29:08.960
kind of in-depth to corporations

29:08.960 --> 29:10.960
I actually think that you kind of

29:10.960 --> 29:12.960
made a really useful example of

29:12.960 --> 29:14.960
one way to do that in your talk so

29:14.960 --> 29:16.960
again I actually was very very

29:16.960 --> 29:18.960
impressed by the way you kind of explained that

29:18.960 --> 29:20.960
I would like to know in the more general

29:20.960 --> 29:22.960
sense how can labs and

29:22.960 --> 29:24.960
potentially RSEs make use of

29:24.960 --> 29:26.960
cloud tools without

29:26.960 --> 29:28.960
getting locked in or

29:28.960 --> 29:30.960
becoming beholden again to a corporate environment

29:30.960 --> 29:32.960
and again by the way I think that you effectively did that in this talk

29:32.960 --> 29:34.960
so I'm more looking for a general kind of

29:34.960 --> 29:36.960
thought about that

29:36.960 --> 29:38.960
You're totally correct that vendor lock

29:38.960 --> 29:40.960
is an issue and when you tend to see

29:40.960 --> 29:42.960
many sort of niche APIs in different

29:42.960 --> 29:44.960
clouds and then you built your entire thing around them

29:44.960 --> 29:46.960
you do face that as an issue

29:46.960 --> 29:48.960
but the great thing about Kubernetes is

29:48.960 --> 29:50.960
that it is this open source project

29:50.960 --> 29:52.960
that is available across clouds

29:52.960 --> 29:54.960
there are subtle differences but if you make a workload

29:54.960 --> 29:56.960
that can run on Kubernetes

29:56.960 --> 29:58.960
you're going to have an easier time to move it between clouds

29:58.960 --> 30:00.960
and that's you know speaking from my lab

30:00.960 --> 30:02.960
we work on flux framework and one of our goals with

30:02.960 --> 30:04.960
flux is to make things

30:04.960 --> 30:06.960
portable not just between clouds

30:06.960 --> 30:08.960
but between cloud and HPC

30:08.960 --> 30:10.960
that's also something like

30:10.960 --> 30:12.960
user netties running actually

30:12.960 --> 30:14.960
Kubernetes on bare metal alongside HPC

30:14.960 --> 30:16.960
is so important because all of a sudden

30:16.960 --> 30:18.960
you have the same workload

30:18.960 --> 30:20.960
and it runs in all the places

30:20.960 --> 30:22.960
that is sort of like the vision we don't

30:22.960 --> 30:24.960
we want to make sure that the scientific

30:24.960 --> 30:26.960
workloads that we're running today can run

30:26.960 --> 30:28.960
in all places not just to

30:28.960 --> 30:30.960
one niche specific cloud

30:30.960 --> 30:32.960
not just one niche specific center

30:32.960 --> 30:34.960
just convergence

30:34.960 --> 30:36.960
TLDR

30:36.960 --> 30:38.960
that is very exciting and I really appreciate that response

30:38.960 --> 30:40.960
thank you so much

30:40.960 --> 30:42.960
okay that's all we have time for

30:42.960 --> 30:44.960
this workout great Vanessa I hope you agree

30:46.960 --> 30:48.960
yeah it was really fun if anyone has further questions

30:48.960 --> 30:50.960
and stuff please reach out to me

30:50.960 --> 30:52.960
I love chatting it was a pleasure chatting

30:52.960 --> 30:54.960
with you and I hope you have a great rest of your fun

30:54.960 --> 30:56.960
then

30:58.960 --> 31:00.960
thank you

31:02.960 --> 31:04.960
and the best way

31:04.960 --> 31:06.960
to reach out to Vanessa is via HPC

31:06.960 --> 31:08.960
social so don't forget to grab a sticker

31:08.960 --> 31:10.960
and you walk out please consider

31:10.960 --> 31:12.960
doing a small donation in the box as well

31:12.960 --> 31:14.960
to help cover the costs and if you're leaving

31:14.960 --> 31:16.960
please check if you see any trash

31:16.960 --> 31:18.960
around please take the trash with you

31:18.960 --> 31:20.960
bottles anything

31:20.960 --> 31:22.960
anything you clean up we don't have to clean up

31:24.960 --> 31:26.960
thanks a lot Vanessa this was great

31:26.960 --> 31:28.960
bye

