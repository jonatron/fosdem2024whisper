WEBVTT

00:00.000 --> 00:08.240
Thank you everyone for coming.

00:08.240 --> 00:12.960
So I'm going to do a quick talk on Porty Software at RISV.

00:12.960 --> 00:15.720
So thank you very much for attending.

00:15.720 --> 00:20.480
So just to quickly introduce myself, I'm a software engineer and team leader at RIVOS.

00:20.480 --> 00:25.000
We are a hardware company doing RISV CPUs.

00:25.000 --> 00:31.000
I work on the management time team, where on the management time system arrays and profiling team.

00:31.000 --> 00:38.600
So our scope is we work a lot on OpenJDK, Python, Go, system libraries like OpenBLAS,

00:38.600 --> 00:42.920
Sleaf, like Math, stuff and everything profiling.

00:42.920 --> 00:47.920
I also have a hat at the language and time working group at RISE.

00:47.920 --> 00:49.320
So what is RISE?

00:49.320 --> 00:54.960
It's a collaborative effort to accelerate the development of open-source software for the RISV architecture.

00:54.960 --> 01:00.880
So we are basically a consortium of companies who are interested in porting software to RISV.

01:00.880 --> 01:02.520
We're investing a lot in that.

01:02.520 --> 01:07.520
I'm going to get back to it, but we're doing a lot and we would love to have you involved as well.

01:07.520 --> 01:14.400
The focus of this working group is on OpenJDK, Go, Python, .NET, Android, Runtime and V8.

01:14.400 --> 01:19.560
Most of them already support RISV to varying degree like we're also going to see after.

01:19.560 --> 01:25.480
The focus is really on the compilers of these different runtimes, on the runtime themselves,

01:25.480 --> 01:28.880
like the libraries, the base class libraries and everything, but also on the ecosystem.

01:28.880 --> 01:34.680
Make sure that the most used Java libraries, most used .NET libraries, most used Python libraries and everything

01:34.680 --> 01:37.520
are well supported on RISV.

01:37.520 --> 01:43.360
Also, my last hat is also part of Adapter Working Group, where we are distributing Java.

01:43.360 --> 01:49.200
We're making sure that there is a Java distribution available for 11, 17, 21.

01:49.240 --> 01:53.240
It's in progress. We're getting close, but you should soon have a distribution,

01:53.240 --> 01:58.640
like a rendered distribution of Java on RISV.

01:58.640 --> 02:00.640
Let me just increase the size here.

02:00.640 --> 02:04.040
Who is the intending audience of this talk?

02:04.040 --> 02:08.640
The talk is really for people who have some experience in RISV and who want to get more involved,

02:08.640 --> 02:13.680
but also for people who have very little experience with RISV or no experience with RISV.

02:13.680 --> 02:15.120
But it sounds exciting, right?

02:15.120 --> 02:16.120
And it is really exciting.

02:16.120 --> 02:19.160
There's a lot of work to do and it's a lot of fun.

02:19.160 --> 02:21.880
So I will not talk assembly. Don't get scared.

02:21.880 --> 02:25.720
And if you don't know a concept or words, please ask.

02:25.720 --> 02:28.120
I would love to have a bit of interaction.

02:28.120 --> 02:31.080
Also, the target is application system.

02:31.080 --> 02:35.280
So anything like smart phone, laptop, desktop, servers, and HPC.

02:35.280 --> 02:37.080
We're not going to talk about embedded.

02:37.080 --> 02:38.920
We're not going to talk about microcontrollers.

02:38.920 --> 02:42.080
That's not the topic of this talk.

02:42.080 --> 02:45.760
So first of all, I want to give a huge shout out to Unsexyful.

02:45.760 --> 02:50.440
They made a lot of things a lot easier to port to RISV.

02:50.440 --> 02:55.800
So there have been years of investment in porting a lot of software to RISV.

02:55.800 --> 02:59.280
And that just makes a path easier for RISV.

02:59.280 --> 03:03.480
There's a lot of libraries out there, for example, which support X86, RISV, PPC, and

03:03.480 --> 03:04.980
F390X.

03:04.980 --> 03:09.320
So adding support to RISV to that is very straightforward.

03:09.320 --> 03:13.720
Because well, adding RISV is just like one more flag, one more configuration somewhere,

03:13.720 --> 03:14.720
and it's pretty easy.

03:14.840 --> 03:16.600
There's already all the if-desfs.

03:16.600 --> 03:18.080
There's already the compiler support.

03:18.080 --> 03:22.120
There's also already the cross-compilation support, CI setup, and everything.

03:22.120 --> 03:24.360
So RISV is just like one more thing.

03:24.360 --> 03:29.600
For libraries and projects which only support X86, though, the work is a bit more involved.

03:29.600 --> 03:35.720
Obviously, we have to support, we have to add support to the build systems to support,

03:35.720 --> 03:37.440
for example, cross-compilation.

03:37.440 --> 03:40.760
For the sources, we have to teach it that well, not everything is X86.

03:40.760 --> 03:43.200
There is some assumptions about X86.

03:43.200 --> 03:46.400
And so you want to make sure that you root out all of these issues.

03:50.080 --> 03:57.320
In terms of resources, in the RISV ecosystem, the kind of the reference is the RISV GitHub

03:57.320 --> 03:58.320
organization.

03:58.320 --> 04:02.720
It's spread out, but it's really the most complete.

04:02.720 --> 04:08.120
For anything related to Scala instructions, you have like the RISV ISA manual.

04:08.120 --> 04:10.560
For the vector instructions, you have the vector spec.

04:10.560 --> 04:13.160
For the vector crypto instructions, you have the vector crypto.

04:13.160 --> 04:17.840
For the vector spec, and for the vector intrinsics, you have the vector intrinsics spec.

04:17.840 --> 04:19.920
So there is a bunch of documents.

04:19.920 --> 04:23.720
They are a bit spread out, but they really are the reference.

04:23.720 --> 04:26.760
The GitHub organization is really where the work happens.

04:26.760 --> 04:28.720
There is work happening on mailing lists.

04:28.720 --> 04:30.280
There is work happening in meetings.

04:30.280 --> 04:35.800
But in the end, all the spec, all the documents, all the result documents of all the discussions

04:35.800 --> 04:37.520
is on GitHub.

04:37.520 --> 04:40.320
So GitHub really is the reference.

04:40.320 --> 04:47.520
It's quite unique compared to, for example, X86 or ARM or other architectures.

04:47.520 --> 04:48.800
It's just very open source.

04:48.800 --> 04:50.200
It's very easy to access.

04:50.200 --> 04:52.200
So it's easy to look for.

04:52.200 --> 04:55.240
It's not easy to find, but it's easy to look for things.

04:55.240 --> 04:58.000
Also, watch out for peer releases.

04:58.000 --> 05:02.760
Things will change, may change in between the peer release and the release.

05:02.760 --> 05:10.000
It will just be weird because, for example, for the vector spec, there was the 0.7 release,

05:10.000 --> 05:13.720
which some hardware vendor have adopted for some of the boards.

05:13.720 --> 05:18.600
But the encoding of some instructions or even instructions have completely changed, meaning

05:18.600 --> 05:22.800
that if you are coding against vector 1.0, it's just not going to work on any board which

05:22.800 --> 05:23.800
implements vector 0.7.

05:23.800 --> 05:25.320
And it's just not going to work.

05:25.320 --> 05:27.120
It's not, sometimes it's going to fail.

05:27.120 --> 05:29.000
No, not going to work.

05:29.000 --> 05:35.920
There is also this very nice resource I found recently, the RISV Intrinsic Viewer.

05:35.920 --> 05:43.240
So it just allows you to look through the vector Intrinsics implemented in GCC and Clang.

05:43.240 --> 05:45.160
There are 14,000 Intrinsics.

05:45.160 --> 05:48.960
So it's very nice to have a nice thing to look through.

05:48.960 --> 05:50.880
A lot of them are very complicated.

05:50.880 --> 05:54.440
Like for example, there is like 5,000 about vector load and stones.

05:54.440 --> 05:59.480
It's just because of the combinatorial explosion of what kind of load you want, what are the

05:59.480 --> 06:02.280
sizes you want to load, what are the element sizes you want to load.

06:02.280 --> 06:03.280
There's a lot of repetition.

06:03.440 --> 06:09.280
There's just a lot of them, so it's easy to look through them.

06:09.280 --> 06:13.960
So the first question you should ask yourself when you are wanting to receive is, what am

06:13.960 --> 06:14.960
I targeting?

06:14.960 --> 06:18.360
As you may be familiar, RISV has a concept of extensions.

06:18.360 --> 06:22.080
And so you kind of want to understand, OK, I'm writing software, but who is going to

06:22.080 --> 06:23.320
be able to run it?

06:23.320 --> 06:25.720
Which boards are going to be able to run it?

06:25.720 --> 06:30.920
The most basic one is Rv6 for GCC, for application servers or application processors.

06:30.920 --> 06:33.000
That's really the most basic ones.

06:33.040 --> 06:38.280
Even the first board, like high-five-enished support set.

06:38.280 --> 06:41.080
Can of the second step is bit-manip.

06:41.080 --> 06:50.920
So anything like ZBA, ZBB, ZBS, which allows you to do some bit manipulation, like rotate.

06:50.920 --> 06:51.920
I don't have all of them in mind.

06:51.920 --> 06:55.640
But like bit manipulation stuff, like where you want to look at one bit of a word or thing

06:55.640 --> 06:59.480
like this, this is going to be in bit-manip.

06:59.480 --> 07:00.840
A lot of boards implement that.

07:00.840 --> 07:05.400
I think from the high-five-enmatch, which was like the second board released in the

07:05.400 --> 07:07.240
market, it supports it.

07:07.240 --> 07:09.800
So it's very, very, very common.

07:09.800 --> 07:10.800
Then you have Vector.

07:10.800 --> 07:12.520
Again, please use Vector 1.0.

07:12.520 --> 07:15.200
Please don't target 0.7.

07:15.200 --> 07:17.280
Today there is one board that supports it.

07:17.280 --> 07:18.280
It has one core.

07:18.280 --> 07:20.240
It's not very fast, but at least it supports it.

07:20.240 --> 07:22.680
But nearly nothing supports it.

07:22.680 --> 07:24.880
And Vector Crypto, don't want to support it yet.

07:24.880 --> 07:27.600
It's probably going to come like one, two years down the line.

07:27.600 --> 07:30.600
So that's really, we expect that to be the future.

07:30.600 --> 07:35.280
But obviously it's been ratified two months ago or something.

07:35.280 --> 07:38.160
So nothing implemented yet.

07:38.160 --> 07:44.760
To kind of simplify things, there is the concept of profiles, which is a RISV international

07:44.760 --> 07:49.400
concept with RVA 20, RVA 22, RVA 23.

07:49.400 --> 07:51.640
There is RVA 24, there is discussion.

07:51.640 --> 07:54.800
There's going to be RVA 25 and et cetera.

07:54.800 --> 07:59.280
You can find the spec at this link as well.

07:59.280 --> 08:06.440
But the idea is to kind of define a set of extensions that allow software to target a

08:06.440 --> 08:12.840
specific profile and how we're to say we are RVA 22 compatible, we are RVA 23 compatible,

08:12.840 --> 08:14.440
we are RVA 24 compatible.

08:14.440 --> 08:18.440
And just like makes it easier than having to support a hundred different extensions and

08:18.440 --> 08:20.680
you don't really know what to target.

08:20.680 --> 08:28.000
So what's certain, what you can target today with very good certainty is RV64GC plus Bitman

08:28.000 --> 08:31.080
Ip and that hardware probe for vector and vector crypto.

08:31.080 --> 08:34.040
I'm going to talk about how we'll provide after.

08:34.040 --> 08:38.440
What's putting my crystal ball, like using my crystal ball, I can say that the expectations

08:38.440 --> 08:42.600
for the future is RV23 plus vector crypto.

08:42.600 --> 08:43.840
Please don't quote me on that.

08:43.840 --> 08:45.760
It's just a crystal ball.

08:45.760 --> 08:47.960
That's the expectations from looking at.

08:47.960 --> 08:51.240
But let's see how the future evolves, but that's the expectation.

08:51.240 --> 08:54.840
Knowing that in RV23, vector would be mandatory.

08:54.840 --> 09:00.840
So that would basically be RV64GC, Bitman, vector, vector crypto would be the future.

09:00.840 --> 09:05.160
I don't know when it's going to happen either, but that's the expectations.

09:05.160 --> 09:06.960
So in all of that, hardware probe is your friend.

09:06.960 --> 09:08.520
So what is our probe?

09:08.520 --> 09:13.240
It's a Linux kernel syscall that allows you to check for hardware capabilities.

09:13.240 --> 09:18.320
For example, it's going to allow you to say, well, do my machine support ZBA?

09:18.320 --> 09:19.320
Great.

09:19.320 --> 09:20.320
It's going to tell you.

09:20.320 --> 09:21.560
Do my machine support V?

09:21.560 --> 09:22.560
Great.

09:22.560 --> 09:23.560
It's going to tell you.

09:23.560 --> 09:30.000
Do my machine support ZVKNHA, which is like SHA256 extension.

09:30.000 --> 09:31.480
It's going to tell you.

09:31.480 --> 09:33.440
It's very rapidly evolving.

09:33.440 --> 09:40.040
For example, in the last Linux release, which coming out like 6.6 or 6.8, I don't remember,

09:40.040 --> 09:44.000
they added like 15 or 20 extensions which are probed as part of the syscall.

09:44.000 --> 09:50.040
So it's always better to be in the last version on Linux to use that.

09:50.040 --> 09:54.160
And once you know what you want to target, you want to look at your compilers and run

09:54.160 --> 09:55.320
times and libraries.

09:55.320 --> 09:58.080
What is supported on RISV?

09:58.080 --> 10:03.080
So there is always support for RISV in many compilers and run times.

10:03.080 --> 10:08.000
For example, GCC, LVM, OpenJDK, Go, Python, .NET, V8, and RISV.

10:08.000 --> 10:10.280
And anymore, do support RISV.

10:10.280 --> 10:16.320
There's various degrees of quality and support of this RISV.

10:16.320 --> 10:22.000
For example, I think kind of red of the curve, this GCC and VN OpenJDK support very well

10:22.000 --> 10:23.000
RISV.

10:23.000 --> 10:27.240
Bit back of the wave, I'm not going to cite them.

10:27.240 --> 10:32.280
But it's, for example, they will only support RV64GC, which is functional, but it's not

10:32.280 --> 10:33.920
very fast, but at least it's functional, right?

10:33.920 --> 10:36.560
You can actually test it.

10:36.560 --> 10:39.200
It's also very rapidly evolving.

10:39.200 --> 10:44.760
For example, like with the team, we contribute regularly to the OpenJDK and there is like,

10:44.760 --> 10:47.560
for every release, there is like dozens of commits improving.

10:47.560 --> 10:49.600
Like for example, hey, now we support this extension.

10:49.600 --> 10:51.800
Hey, now we actually are editing with these instructions.

10:51.800 --> 10:53.280
So it's going very fast.

10:53.280 --> 10:56.160
So it's important to really be on the latest and greatest.

10:56.160 --> 10:59.320
Like for example, I said, I talked about the kernel before.

10:59.320 --> 11:00.320
Same thing.

11:00.320 --> 11:02.760
Vector was like six months ago or something.

11:02.760 --> 11:06.720
So before that, you didn't have a Linux kernel release with support for Vector.

11:06.720 --> 11:10.520
So it's very important to be on the latest ones.

11:10.520 --> 11:12.040
Also it's more and more libraries.

11:12.040 --> 11:14.240
Support RISV.

11:14.240 --> 11:19.600
That's where most of the outcome work I expect because it's great for example that GCC and

11:19.600 --> 11:26.600
LVM support RISV, but if any of your 20 data pens don't support RISV, like it's not going

11:26.600 --> 11:27.600
to work, right?

11:27.600 --> 11:31.560
You need all of the libraries to be supporting RISV.

11:31.560 --> 11:39.240
So RVI is maintaining this very nice page, which it's not very full, very complete, but

11:39.240 --> 11:43.720
it's a good reference, which kind of highlights some of the projects we have which are supporting

11:43.720 --> 11:44.720
RISV.

11:44.720 --> 11:45.720
Sorry.

11:45.720 --> 11:52.760
If you project to RISV and don't sit on this page, please report it to our VDI.

11:52.760 --> 11:57.000
I think they will be very happy to add it.

11:57.000 --> 12:01.040
Also in that, huge shout out to all the contributors.

12:01.040 --> 12:04.680
I don't know you, but thank you very much for all the work you're doing.

12:04.680 --> 12:06.360
Without you, all of that would not be possible.

12:06.360 --> 12:08.560
And also, many of them are doing it on their free time.

12:08.560 --> 12:11.720
So thank you very much.

12:11.720 --> 12:16.200
So what are some of the difficulties and gotchas that we ran into?

12:16.200 --> 12:19.720
So the main one is like hereby assumptions.

12:19.720 --> 12:22.960
A lot of code has been written for a long time for RISV.

12:22.960 --> 12:25.800
For example, who assumes that the page is 4K, right?

12:25.800 --> 12:34.080
So luckily, only five pages are 4,000 bits, but some architecture is in the past after

12:34.080 --> 12:35.960
it, hey, what if a page is 16K?

12:35.960 --> 12:37.240
Who knows what's going to happen?

12:37.240 --> 12:39.760
Why don't things break?

12:40.000 --> 12:44.240
So something specific to RISV, the vector length specific code.

12:44.240 --> 12:49.080
Some architectures, like for example XA6 and Arm Neon, for example, they say a vector

12:49.080 --> 12:52.120
is going to be 128 bits or 256 bits.

12:52.120 --> 12:55.840
And the software engineers who write code is going to know that's the vector length

12:55.840 --> 12:58.040
and I'm going to write my code for that.

12:58.040 --> 13:03.640
RISV decided to be smart and decided that there should be no, everything should be vector

13:03.640 --> 13:04.880
length and elastic.

13:05.320 --> 13:10.400
Well, it makes it a bit harder to write software because a lot of software assumes that vector

13:10.400 --> 13:15.880
are 256, 512, 128 bits and they just don't know what it means to be vector length and

13:15.880 --> 13:16.880
elastic.

13:17.880 --> 13:20.880
Also, can you know connans?

13:20.880 --> 13:28.480
Whenever you try to represent a nan, in XA6, it's basically any value of nan is valid and

13:28.480 --> 13:36.120
if you do a multiply of 1 by a nan, any value of nan is going to return you this value of

13:36.120 --> 13:37.120
nan.

13:37.120 --> 13:38.600
So for example, it's going to keep the sign.

13:38.600 --> 13:40.880
That's kind of the main issue.

13:40.880 --> 13:46.480
But in RISV, if you do 1.0 times a nan, or minus nan, it's going to return you connecal

13:46.480 --> 13:47.480
nan.

13:47.480 --> 13:48.480
That's part of the spec.

13:48.480 --> 13:50.600
That's how hardware should behave.

13:50.600 --> 13:54.880
That means that if you multiply 1 by a negative number, it's going to return you a positive

13:54.880 --> 13:55.880
number.

13:55.880 --> 13:56.880
Right?

13:56.880 --> 13:58.040
It's not a number.

13:58.040 --> 14:01.480
So what should be the behavior there?

14:01.480 --> 14:04.600
Well, RISV says it should be positive.

14:04.600 --> 14:08.400
So if you try to do each sign of this result on C++, it's going to return you something

14:08.400 --> 14:10.680
different on XA6 and RISV.

14:10.680 --> 14:12.680
So that's gotches.

14:12.680 --> 14:16.000
Well, we will have to look out for that.

14:16.000 --> 14:18.240
Something else, memory model.

14:18.240 --> 14:21.800
It's stronger on XA6, weak on RISV.

14:21.800 --> 14:26.160
I think I'm 64 degree up there as well of teaching people that weak memory model are

14:26.160 --> 14:30.760
different and they're worth looking out for.

14:30.760 --> 14:32.880
Something else that we are going to have to look at.

14:32.880 --> 14:34.880
Also the vector spec simplicity.

14:34.880 --> 14:36.920
It's very simple to program.

14:36.920 --> 14:40.560
Hard to implement in hardware.

14:40.560 --> 14:44.400
You have some instructions that can do something very complex.

14:44.400 --> 14:49.680
And so to implement it efficiently in hardware, it can be very difficult or even impossible.

14:49.680 --> 14:52.240
Meaning that sometimes you have your stream instructions.

14:52.240 --> 14:55.920
Something takes like five cycles, 15 cycles, maybe 30 cycles.

14:55.920 --> 15:00.720
And then suddenly one instructions cannot be executed by hardware, has to be emulated

15:00.720 --> 15:03.280
and takes a thousand cycles.

15:03.280 --> 15:05.480
And you don't see it as like your programming user space.

15:05.480 --> 15:06.480
You just don't see it.

15:06.480 --> 15:07.480
It just works.

15:07.480 --> 15:11.600
But then you are like, oh, I vectorize my code and it's 50 times slower.

15:11.600 --> 15:12.600
What happened?

15:12.600 --> 15:13.600
Like I don't understand.

15:13.600 --> 15:14.720
It should be eight times faster or 16 times faster.

15:14.720 --> 15:15.720
It's slower.

15:15.720 --> 15:16.720
What happens?

15:16.720 --> 15:18.560
Well, that's going to be the problem.

15:18.560 --> 15:20.160
So solutions here.

15:20.160 --> 15:21.160
Test it.

15:21.160 --> 15:22.760
Figure out what's happening.

15:22.760 --> 15:24.440
Use profiling tools.

15:24.440 --> 15:26.120
Figure out what can go wrong.

15:26.120 --> 15:31.920
And eventually refer to vendor specific information like optimization manual that may tell you

15:31.920 --> 15:36.800
this instruction is slow on the hardware because X20.

15:36.800 --> 15:37.800
So great.

15:37.800 --> 15:39.800
Now you are developing, compiling and testing.

15:39.800 --> 15:43.800
Like, you know, how do you test things basically?

15:43.800 --> 15:47.080
So here, QMU is with friends, also membo.

15:47.080 --> 15:50.360
So functionally, QMU I think is the most complex.

15:50.360 --> 15:57.040
Purely because when people want to make a new extension, they implemented on QMU to

15:57.040 --> 15:59.000
test what it would mean.

15:59.000 --> 16:01.360
So basically they prototype it on QMU.

16:01.360 --> 16:03.960
And then whenever it's modified, it's like, well, it's there, right?

16:03.960 --> 16:05.120
We prototype everything.

16:05.120 --> 16:06.120
So it's just there.

16:06.120 --> 16:09.040
So it's vector crypto, for example.

16:09.040 --> 16:11.040
QMU was the first one because it was prototyped.

16:11.040 --> 16:12.040
So it was easy.

16:12.040 --> 16:13.040
Yes, please.

16:13.040 --> 16:20.040
You just talked about how you need to measure the example of X instructions.

16:20.040 --> 16:21.040
Yes.

16:21.040 --> 16:26.280
How well does that translate to QMU?

16:26.280 --> 16:27.640
Going to talk about it after.

16:27.640 --> 16:28.640
Yes.

16:28.640 --> 16:33.200
So the question was, how do we measure performance on QMU?

16:33.200 --> 16:34.200
That's next slide.

16:34.200 --> 16:37.240
Oh, second, next slide.

16:37.240 --> 16:42.640
So user space simulation is also easy enough on the Ubuntu or Debian based.

16:42.640 --> 16:44.920
You do add getting stored QMU static.

16:44.920 --> 16:47.960
And then you can just run a Docker image with RISC-5 stuff.

16:47.960 --> 16:49.640
Like you literally run RISC-5.

16:49.640 --> 16:51.600
It's Ubuntu and RISC-5.

16:51.600 --> 16:52.600
It just works.

16:52.600 --> 16:55.080
So it's very easy to test out.

16:55.080 --> 16:57.040
It's great for most testing.

16:57.040 --> 16:58.200
There's some leak abstractions.

16:58.200 --> 17:02.400
For example, on QMU, Parat is 7.0.

17:02.400 --> 17:04.880
Proxy.info would read the host one.

17:04.880 --> 17:07.920
So you're on a RISC-5 environment, RISC-5 binary.

17:07.920 --> 17:10.800
You read proxy.info and tell you support AVX2.

17:11.800 --> 17:12.800
Why?

17:12.800 --> 17:13.800
Like, not in the right thing.

17:13.800 --> 17:14.800
What's happening?

17:14.800 --> 17:18.800
It's also not particularly, yes, please.

17:18.800 --> 17:19.800
No.

17:19.800 --> 17:21.800
I was wrong.

17:21.800 --> 17:22.800
OK.

17:22.800 --> 17:26.800
Post 7.0, that specific thing is emulated.

17:26.800 --> 17:28.800
So you don't have this problem anymore.

17:28.800 --> 17:30.800
But also QMU is not particularly fast.

17:30.800 --> 17:34.800
It can be like five to 10 times slower, like on single-core performance.

17:34.800 --> 17:39.800
So if you want to test, have a machine with 64 cores, things go faster.

17:40.800 --> 17:43.800
Also linking for executable or libraries,

17:43.800 --> 17:47.800
like large ones can take a long time because linking is usually single-core.

17:47.800 --> 17:49.800
So it can be slow.

17:49.800 --> 17:53.800
Also debugging can get pretty complicated because you attach GDB-grade.

17:53.800 --> 17:56.800
But do you attach it to the X86 QMU process?

17:56.800 --> 18:00.800
Or do you attach it to the RISC-5 that QMU is trying to emulate?

18:00.800 --> 18:01.800
And yes.

18:01.800 --> 18:04.800
QMU can act as a database server.

18:04.800 --> 18:05.800
Yes.

18:05.800 --> 18:08.800
But that's a bit broken in some ways sometimes.

18:08.800 --> 18:09.800
And it's, yeah, sorry.

18:09.800 --> 18:13.800
So the question was, GDB can act as a GDB server?

18:13.800 --> 18:14.800
QMU, yes, sorry.

18:14.800 --> 18:16.800
QMU can act as a GDB server.

18:16.800 --> 18:19.800
It works most of the time, sometimes it doesn't.

18:19.800 --> 18:23.800
That's where it can get a bit complicated and sometimes it's just like...

18:23.800 --> 18:27.800
So that brings me to the next point of, well, the other way of doing it

18:27.800 --> 18:30.800
is just cross-compilation and testing on deathboards.

18:30.800 --> 18:34.800
So obviously you have faster build-up times because cross-compilation,

18:35.800 --> 18:39.800
like your compiler are native, so it's just faster.

18:39.800 --> 18:43.800
Per-MEs, well, first of all, does your project support cross-compilation?

18:43.800 --> 18:45.800
That's not a given.

18:45.800 --> 18:48.800
Also, today's boards have a mutation.

18:48.800 --> 18:49.800
They don't support everything.

18:49.800 --> 18:52.800
For example, I mentioned before, vector, vector, crypto.

18:52.800 --> 18:53.800
Vector only one board supports it.

18:53.800 --> 18:55.800
Vector, crypto, no one supports it.

18:55.800 --> 18:57.800
So you cannot test any of these algorithms.

18:57.800 --> 18:59.800
You are stuck to QMU for that.

18:59.800 --> 19:01.800
Also, hardware bugs.

19:01.800 --> 19:04.800
Not every board just behaves perfectly.

19:04.800 --> 19:07.800
I'm not going to name names, but you have, for example,

19:07.800 --> 19:09.800
some boards have atomic bugs.

19:09.800 --> 19:11.800
So you do an atomic operation.

19:11.800 --> 19:13.800
It's going to say success.

19:13.800 --> 19:17.800
It's going to say failed if it did succeed.

19:17.800 --> 19:21.800
So mutex can be a bit complicated to implement like that.

19:21.800 --> 19:23.800
So you have bugs.

19:25.800 --> 19:27.800
So on that, retries, again, is your friend.

19:27.800 --> 19:29.800
If it fails once, try a second time.

19:29.800 --> 19:32.800
If it succeeds, and it succeeds always in another board

19:32.800 --> 19:34.800
that you know doesn't have the bug,

19:34.800 --> 19:37.800
probably just do the hardware bug.

19:37.800 --> 19:39.800
So for CI, QMU is your friend.

19:39.800 --> 19:41.800
Again, on GitHub Actions, for example,

19:41.800 --> 19:44.800
which is quite nice, a lot of free time,

19:44.800 --> 19:45.800
it's a one-liner.

19:45.800 --> 19:47.800
You do like use Docker setup QMU action.

19:47.800 --> 19:50.800
And suddenly you have QMU setup on your machine.

19:50.800 --> 19:52.800
It takes like three or five seconds to set up.

19:52.800 --> 19:54.800
So it's also very fast.

19:54.800 --> 19:55.800
You don't even need Docker.

19:55.800 --> 19:58.800
You can just use QMU and D prefix and the C straight

19:58.800 --> 20:01.800
to kind of help you have like a RISV file system

20:01.800 --> 20:04.800
like slash ETC, slash user, slash home, slash everything.

20:04.800 --> 20:08.800
And QMU is going to take care of loading the file from there

20:08.800 --> 20:10.800
rather than the host X86 stuff.

20:10.800 --> 20:11.800
And then it just works.

20:11.800 --> 20:15.800
Like you have a RISV machine on GitHub Actions for free.

20:15.800 --> 20:17.800
Slow, but it's functional.

20:17.800 --> 20:20.800
Also, you can tweak the available CPU options.

20:20.800 --> 20:23.800
For example, you can say, I want ZBA, ZBB, ZBS,

20:23.800 --> 20:25.800
but I don't want vector.

20:25.800 --> 20:27.800
For example, I'm working on my fancy library,

20:27.800 --> 20:28.800
which I added vector.

20:28.800 --> 20:30.800
I want to make sure it's still going to run on the board

20:30.800 --> 20:32.800
that don't support vector and it's not going to crash.

20:32.800 --> 20:34.800
Well, I can do that.

20:34.800 --> 20:37.800
Or I want to run with vector lengths of 128 bits.

20:37.800 --> 20:38.800
Great.

20:38.800 --> 20:39.800
You do.

20:39.800 --> 20:40.800
You just specify V-Linux 128.

20:40.800 --> 20:41.800
It just works.

20:41.800 --> 20:43.800
256 bits, same thing.

20:43.800 --> 20:47.800
You can go up to 16,000 if you want, but it's poor of two.

20:47.800 --> 20:50.800
Basically, it makes it very easy to test a lot of different

20:50.800 --> 20:54.800
configuration for free on CI.

20:54.800 --> 20:57.800
So that's where I come to the question about performance.

20:57.800 --> 21:00.800
So performance measurement is not different.

21:00.800 --> 21:02.800
It is not psycho-accurate.

21:02.800 --> 21:05.800
It does not even try to be psycho-accurate.

21:05.800 --> 21:06.800
It's just not what's made for.

21:06.800 --> 21:09.800
Usually, psycho-accuracy measure, there is some open source one,

21:09.800 --> 21:14.800
like Gem5, but the vector-specific ones are extremely secret,

21:14.800 --> 21:16.800
obviously, because it contains a lot of information

21:16.800 --> 21:18.800
about the macro-architecture.

21:18.800 --> 21:19.800
So don't ask me.

21:19.800 --> 21:23.800
I will not say any information about our psycho-accuracy.

21:23.800 --> 21:26.800
What you can do, eventually, is interaction count.

21:26.800 --> 21:29.800
The problem is it's very inaccurate.

21:29.800 --> 21:32.800
If you go from 10 instructions to five, but if instruction takes

21:32.800 --> 21:35.800
10 times the latency, yes, you're going to use less instructions

21:35.800 --> 21:37.800
but it's going to take longer.

21:37.800 --> 21:40.800
So that's not perfect.

21:40.800 --> 21:42.800
Better than nothing, but not perfect.

21:42.800 --> 21:44.800
Second bad thing is bolts.

21:44.800 --> 21:47.800
The problem is imaging, optimizing for high-end servers,

21:47.800 --> 21:51.800
like Crashing and Fall, sorry, on the Raspberry Pi.

21:51.800 --> 21:54.800
The preference profile doesn't really match.

21:54.800 --> 21:57.800
Like the bolts we have today, all in order CPUs,

21:57.800 --> 22:00.800
there's few cores, scalability is not very great.

22:00.800 --> 22:03.800
For example, if you go to 64 cores on the board,

22:03.800 --> 22:05.800
which was meant to be four cores, memory accesses are going

22:05.800 --> 22:09.800
to be slow as soon as you load it a bit, so it can be limited.

22:09.800 --> 22:11.800
Also, only one board again supports vectors,

22:11.800 --> 22:15.800
so that's the KNMV K230.

22:15.800 --> 22:17.800
So you cannot even really test vector.

22:17.800 --> 22:20.800
You cannot really test vector monthly-thirty either.

22:20.800 --> 22:23.800
So it's getting better, slowly, but it's getting better.

22:23.800 --> 22:24.800
Yes, please.

22:24.800 --> 22:27.800
I think the vector version is 0.7.

22:27.800 --> 22:31.800
On the KNMV K230, it's the first one that supports vector 1.0.

22:31.800 --> 22:34.800
It's, for example, the Leachie Pi with a 3.910

22:34.800 --> 22:39.800
that only supports vectors 0.7.

22:39.800 --> 22:41.800
And also the optimization manual,

22:41.800 --> 22:43.800
so that's something that we're working on at RISE.

22:43.800 --> 22:45.800
It should be coming in the next few days,

22:45.800 --> 22:47.800
but stay tuned.

22:47.800 --> 22:50.800
And it's kind of a guide of how to write performance codes

22:50.800 --> 22:55.800
with the input of companies who are actually making chips,

22:55.800 --> 22:58.800
and so who knows, okay, this kind of thing is going to work

22:58.800 --> 23:00.800
well on our chip, and so we came together

23:00.800 --> 23:03.800
and we said, okay, these are common guidelines

23:03.800 --> 23:07.800
for writing efficient RISE 5 code.

23:07.800 --> 23:11.800
So I think, South, it's fun, never too late.

23:11.800 --> 23:12.800
Please get involved.

23:12.800 --> 23:15.800
There is so much more work to do.

23:15.800 --> 23:17.800
There is more work that you can imagine.

23:17.800 --> 23:19.800
We have enough work for the whole industry

23:19.800 --> 23:21.800
for the next five to 10 years at least,

23:21.800 --> 23:23.800
so please get involved.

23:23.800 --> 23:26.800
Check out Wikidotrizeproject.dev

23:26.800 --> 23:29.800
if you don't really know where to start.

23:29.800 --> 23:32.800
We are trying to outline some of the work

23:32.800 --> 23:35.800
that we're planning to do and we're welcome contributions.

23:35.800 --> 23:38.800
Also, if you have an idea, make a proposal.

23:38.800 --> 23:40.800
We pay money for that.

23:40.800 --> 23:42.800
As in, if you think that there is something,

23:42.800 --> 23:44.800
like some project that should be ported to RISE 5,

23:44.800 --> 23:46.800
we're ready to sponsor it, so it's paid open source work,

23:46.800 --> 23:49.800
which is not that common.

23:49.800 --> 23:53.800
And finally, thank you to all contributors again.

23:53.800 --> 23:58.800
Without everyone, it would just not be possible.

23:58.800 --> 24:05.800
Applause

24:05.800 --> 24:06.800
Yes, please.

24:06.800 --> 24:09.800
So what about running software for the two years

24:09.800 --> 24:11.800
and testing the...

24:11.800 --> 24:14.800
Softon, can you define softon, please?

24:14.800 --> 24:19.800
So basically, I know very low implementation of the...

24:19.800 --> 24:22.800
Any other, you know, any of the compiling the FPG

24:22.800 --> 24:23.800
and running it on FPG?

24:23.800 --> 24:24.800
Yes, OK.

24:24.800 --> 24:26.800
So it tested with very fast and very key Mew,

24:26.800 --> 24:28.800
but you know, not that fast.

24:28.800 --> 24:31.800
So the question is how about running,

24:31.800 --> 24:33.800
like so literally softon on FPG

24:33.800 --> 24:36.800
basically have a very low guarantee on implementation

24:36.800 --> 24:38.800
compared to FPG and run on that.

24:38.800 --> 24:42.800
We are doing that internally for cycle accuracy.

24:42.800 --> 24:46.800
Even that is way slower than QMU.

24:46.800 --> 24:50.800
Like I think in order of speed today,

24:50.800 --> 24:52.800
it's bolts are still a bit faster than QMU.

24:52.800 --> 24:55.800
QMU, as long as you have a lot of calls,

24:55.800 --> 24:57.800
like 30 to 64 calls is ideal.

24:57.800 --> 25:01.800
FPGA, emulators, this kind of stuff,

25:01.800 --> 25:04.800
and software emulation of RTL.

25:04.800 --> 25:06.800
If you reverse that in terms of accuracy

25:06.800 --> 25:09.800
of the results you are going to get in terms of performance,

25:09.800 --> 25:13.800
software emulation, emulator, FPGA are going to be the most accurate

25:13.800 --> 25:15.800
because they are actually trying to be cycle accurate.

25:15.800 --> 25:17.800
QMU is not trying to be cycle accurate.

25:17.800 --> 25:20.800
The bolts are by definition cycle accurate.

25:20.800 --> 25:23.800
But the big advantage of emulators and simulators

25:23.800 --> 25:25.800
and all of that is you can literally have a trace

25:25.800 --> 25:28.800
of this instruction took that many cycles

25:28.800 --> 25:30.800
and this instruction took that many...

25:30.800 --> 25:32.800
like a stream of every instructions executed.

25:32.800 --> 25:35.800
And so you can have a very, very precise and accurate

25:35.800 --> 25:39.800
representation of this is how my application ran.

25:39.800 --> 25:46.800
Obviously you run like something that would take 10 seconds on XC6,

25:46.800 --> 25:48.800
you generate it for like 300 gigs of data.

25:48.800 --> 25:51.800
But you have very precise and accurate information.

25:51.800 --> 25:54.800
Yes, please.

25:54.800 --> 25:59.800
So my question is more on the hardware vendors now, right?

25:59.800 --> 26:03.800
So if somebody starts to take out today, right,

26:03.800 --> 26:07.800
what should he... because of the RTL development

26:07.800 --> 26:12.800
and then the tape out itself takes a lot of time, right?

26:12.800 --> 26:14.800
So this is... you're looking for one or two years.

26:14.800 --> 26:20.800
So what should somebody start with if you said the greatest today is this?

26:20.800 --> 26:24.800
But if I have a product that would go in this time,

26:24.800 --> 26:26.800
I can have some iteration in between, right?

26:26.800 --> 26:30.800
But the problem is again that something changes in the vector library

26:30.800 --> 26:34.800
and then my entire timing constraint gets discovered.

26:34.800 --> 26:36.800
Yes.

26:36.800 --> 26:39.800
Should I only be starting what you suggest now?

26:39.800 --> 26:42.800
Or can I have some kind of a plan that I can update

26:42.800 --> 26:45.800
six months, eight months, if something new comes?

26:45.800 --> 26:47.800
What is your suggestion for this?

26:47.800 --> 26:53.800
So the question is what should be the target?

26:53.800 --> 26:55.800
If today I want to make a new hardware,

26:55.800 --> 26:57.800
what should be the target profile, for example?

26:58.800 --> 27:01.800
And do I understand correctly that you're also

27:01.800 --> 27:07.800
alluding to the cycle timing of interactions and things like this?

27:07.800 --> 27:12.800
Because it will disturb the logic, right?

27:12.800 --> 27:13.800
Yes.

27:13.800 --> 27:16.800
If the number of cycles for the same thing will be different

27:16.800 --> 27:19.800
if I have a better vector crypt or something coming.

27:19.800 --> 27:20.800
Yeah.

27:21.800 --> 27:26.800
If we look at what's being done, for example,

27:26.800 --> 27:29.800
by the companies who are talking about what's happening, right?

27:29.800 --> 27:34.800
There is Sci-5, there is T-Head, there is Ventana.

27:34.800 --> 27:37.800
Well, let's take these three as examples.

27:37.800 --> 27:41.800
T-Head, as the latest announced, was the C908,

27:41.800 --> 27:43.800
which does support vector 1.0,

27:43.800 --> 27:45.800
but they announced it only like a few weeks

27:45.800 --> 27:47.800
or a month after a vector crypto was announced.

27:47.800 --> 27:49.800
So obviously when they announced it,

27:49.800 --> 27:51.800
it's because they knew it was going to tape out.

27:51.800 --> 27:53.800
And so vector crypto is complex spec,

27:53.800 --> 27:57.800
so they didn't have time to implement it.

27:57.800 --> 28:03.800
Ventana has also announced that they would support

28:03.800 --> 28:06.800
in their second gen chip the vector 1.0.

28:06.800 --> 28:10.800
I think that was announced at RVA Summit last November.

28:10.800 --> 28:12.800
But I don't think they are targeting vector crypto

28:12.800 --> 28:15.800
because same thing, the timing was like when it was announced,

28:15.800 --> 28:18.800
vector crypto just came out or was coming out in next week.

28:18.800 --> 28:19.800
So same thing.

28:19.800 --> 28:24.800
Sci-5, same thing, they announced vector 1.0 chip,

28:24.800 --> 28:26.800
but same thing non-vector crypto.

28:26.800 --> 28:28.800
The expectation is that vector crypto chips

28:28.800 --> 28:30.800
are going to be announced like a year from now,

28:30.800 --> 28:33.800
maybe a year and a half, maybe two years.

28:33.800 --> 28:37.800
We all hope it would be two months from now, but things shift.

28:40.800 --> 28:42.800
If you start the chip today,

28:42.800 --> 28:46.800
I sure hope that you are targeting RVA 23 plus vector crypto.

28:46.800 --> 28:48.800
But that's obviously for starting a chip today,

28:48.800 --> 28:50.800
and that's easy for me to say, but how to...

28:50.800 --> 28:52.800
Knowing that the chip takes...

28:52.800 --> 28:55.800
If you look at Intel's timing, a chip takes 5 to 7 years to take out.

28:55.800 --> 28:58.800
So you start today, you deliver it in 2030.

29:01.800 --> 29:03.800
Does that answer your question or...?

29:03.800 --> 29:05.800
Yeah, it's a soft talk.

29:05.800 --> 29:09.800
So what I get that today, if I start,

29:09.800 --> 29:12.800
I can't just move vector crypto.

29:12.800 --> 29:14.800
If you write after today,

29:14.800 --> 29:20.800
I think you can expect to have boards having it in 2025, 2026.

29:20.800 --> 29:22.800
But that's where Harropov is good for you,

29:22.800 --> 29:26.800
because you can check if Harropov is supported by the hardware,

29:26.800 --> 29:29.800
then use this path, else use the other path.

29:30.800 --> 29:32.800
And that's how it's done in OpenJK.

29:32.800 --> 29:35.800
That's how it's done in OpenSSL, that's how it's done everywhere,

29:35.800 --> 29:41.800
because it's funny how we say hardware has a 5-year lead time.

29:41.800 --> 29:43.800
Software also has, it's not just like,

29:43.800 --> 29:45.800
I have my commit and then everyone has it, right?

29:45.800 --> 29:48.800
You need to have a release and then a release is not enough,

29:48.800 --> 29:50.800
because it needs to be shipped in a distribution,

29:50.800 --> 29:52.800
and then hopefully it's a LTS,

29:52.800 --> 29:56.800
and so you easily have 2 to 3 years lead time in software often.

29:56.800 --> 30:01.800
So it's important that if you are doing crypto stuff like OpenSSL,

30:01.800 --> 30:03.800
do vector crypto today,

30:03.800 --> 30:07.800
because the time that it gets into the hands of like Reda Hat 10 customers,

30:07.800 --> 30:10.800
or Ubuntu 24.0 for anything like this,

30:10.800 --> 30:13.800
well you want to have community of things like a year ago, 2 years ago.

30:13.800 --> 30:17.800
So then these hardware vendors are talking to RISE to get this input,

30:17.800 --> 30:22.800
because you are kind of connecting all the open source.

30:22.800 --> 30:24.800
Sci-Fi is part of RISE, Ventana is part of RISE,

30:24.800 --> 30:28.800
Revis is part of RISE, Alibaba is part of RISE.

30:28.800 --> 30:34.800
It's really, like, we want to push the software forward,

30:34.800 --> 30:38.800
because we also understand that software needs to be ready for,

30:38.800 --> 30:42.800
when we get the bold out, we need people to actually have software to run on it.

30:42.800 --> 30:45.800
So here I talk really about a lot of user space stuff,

30:45.800 --> 30:49.800
but RISE also has interest in kernels, in debugger, in firmware,

30:49.800 --> 30:54.800
in OpenSBI, in emulators, in debuggers, in like everything.

30:54.800 --> 30:56.800
Like we want all the software to work,

30:56.800 --> 30:58.800
and even software and a bit of firmware to work, right?

30:58.800 --> 31:01.800
So we need everything to work physically.

31:01.800 --> 31:02.800
Thank you.

31:02.800 --> 31:03.800
Yes.

31:03.800 --> 31:08.800
I'd like to say that I think you're asking as well about knowing what instructions

31:08.800 --> 31:12.800
to use to target performance improvement in software, right?

31:12.800 --> 31:17.800
So how do I today use the best equipment that we know we're going to be able to do?

31:17.800 --> 31:21.800
So the simple answer is,

31:21.800 --> 31:26.800
Maxi, the compiler again is,

31:26.800 --> 31:28.800
the question is, what's the problem?

31:28.800 --> 31:31.800
Are there any already know what patterns are for the compiler?

31:31.800 --> 31:35.800
Or already know about the appropriate pattern

31:35.800 --> 31:36.800
sort of under hardware?

31:36.800 --> 31:41.800
We will get instructed, and I'll just say you need to do the micro-commodization.

31:41.800 --> 31:45.800
You want to go down to the example, or that's by the optimization going on.

31:45.800 --> 31:46.800
Yes.

31:46.800 --> 31:49.800
So general advice for what to do.

31:49.800 --> 31:52.800
It's not going to be perfect for all architect.

31:52.800 --> 31:56.800
Similar to AR64, training the way for what the RISC-5 software.

31:56.800 --> 32:00.800
You have an AR64 implementation, not too difficult, but RISC-5.

32:00.800 --> 32:04.800
If you have a RISC-5 implementation, which is semi-optimal,

32:04.800 --> 32:07.800
you know you're not going to want these useful rocks,

32:07.800 --> 32:10.800
that contribution into an open-source project,

32:10.800 --> 32:14.800
even if it isn't specific to your micro-architect, it's very valuable.

32:14.800 --> 32:18.800
So just to repeat what Kieran said for the people online.

32:18.800 --> 32:21.800
If you're trying to do some optimizations,

32:21.800 --> 32:24.800
the compiler is provided by the vendors,

32:24.800 --> 32:27.800
usually is going to have vendor-specific information.

32:27.800 --> 32:30.800
Usually things that have not landed upstream yet,

32:30.800 --> 32:33.800
or that have landed upstream in the latest, like Trunk, for example.

32:33.800 --> 32:38.800
So it's not released yet, and so they just want to make sure that they have,

32:38.800 --> 32:41.800
like customers can have access to the best compiler for their,

32:41.800 --> 32:45.800
for whoever buys Ventana or ReVos or Sci-Fi stuff.

32:45.800 --> 32:49.800
And then if you're trying to do really handwritten assembly optimizations,

32:49.800 --> 32:52.800
then that's where you start referring to the optimization manuals,

32:52.800 --> 32:57.800
where there is the rise one,

32:57.800 --> 33:00.800
but you also expect ReVos to have some, Ventana to have some,

33:00.800 --> 33:04.800
Sci-Fi to have some, but they should be very specific to the hardware,

33:04.800 --> 33:09.800
and the rise optimization or RISC-5 optimization manual will be very generic,

33:09.800 --> 33:14.800
and for example, how to best use vector instructions,

33:14.800 --> 33:17.800
like what's to avoid, what's not to avoid,

33:17.800 --> 33:21.800
how to basically best use it across most hardware.

33:21.800 --> 33:25.800
So I think like OpenBLAST in that is very interesting,

33:25.800 --> 33:29.800
because they are planarized on X86, right?

33:29.800 --> 33:34.800
They have 20 or something different kernels for X86,

33:34.800 --> 33:38.800
based on the generation of Intel and based on the generation of AMD,

33:38.800 --> 33:42.800
because they know that the timing of instruction is different based on whatever,

33:42.800 --> 33:46.800
so you can compile for Nehalem or you can compile for Sci-Fi,

33:46.800 --> 33:49.800
or you can compile for Xenversion3 or Xenversion4,

33:49.800 --> 33:52.800
and then the kernels are going to be different on this kind of stuff,

33:52.800 --> 33:54.800
and I think OpenBLAST is very specific in that,

33:54.800 --> 33:58.800
because they really try to extract, yes, it's like a linear algebra library,

33:58.800 --> 34:02.800
but it's, I think it's really pushing the thing very, very, very, very far,

34:02.800 --> 34:06.800
but that's kind of the kind of things that is going to be very vendor specific,

34:06.800 --> 34:12.800
and which you will have to refer to the vendor spec or vendor optimization manual for that.

34:12.800 --> 34:14.800
Thank you for the question.

34:14.800 --> 34:16.800
Yes, please.

34:16.800 --> 34:18.800
Two questions.

34:18.800 --> 34:22.800
So one, when you start boarding open source software to RISC-V,

34:22.800 --> 34:25.800
you probably have to talk to the developers or the maintainers as well.

34:25.800 --> 34:26.800
Yes.

34:26.800 --> 34:29.800
How often do you get the reaction, we have no idea what RISC-V is?

34:29.800 --> 34:34.800
So the question is, if you're trying to port project RISC-V,

34:34.800 --> 34:37.800
what is the, like, how often does it happen that the maintainer says RISC-V,

34:37.800 --> 34:39.800
like, you don't know what it is?

34:39.800 --> 34:41.800
Honestly, pretty rarely.

34:41.800 --> 34:44.800
I think Hacker News has done a lot of great job of,

34:44.800 --> 34:48.800
RISC-V is exciting, RISC-V is new, can't involve in RISC-V,

34:48.800 --> 34:51.800
people don't really know, oh yeah, it's a new architecture, right,

34:51.800 --> 34:53.800
it sounds exciting, right, but they don't really know what it is,

34:53.800 --> 34:55.800
but at least they heard about it.

34:55.800 --> 34:58.800
About three years ago, I was in a talk about Flist,

34:58.800 --> 34:59.800
Yes.

34:59.800 --> 35:01.800
like a variant of OpenBLAST,

35:01.800 --> 35:04.800
and I suggested that we should look at RISC-V,

35:04.800 --> 35:07.800
we had no idea what it was, and these are pretty low level people, right?

35:07.800 --> 35:08.800
Yes.

35:08.800 --> 35:10.800
That was three years ago, but I think it's too late.

35:10.800 --> 35:14.800
Yes, so your comment also, for like,

35:14.800 --> 35:17.800
looking at the beliefs of, like, three years ago you said,

35:17.800 --> 35:22.800
people didn't know what RISC-V was, but hopefully today they know.

35:22.800 --> 35:25.800
Hopefully today or when it works in RISC-V?

35:25.800 --> 35:28.800
Today it works, yeah, so, I think, actually the,

35:28.800 --> 35:30.800
also like, fun tidbit for OpenBLAST, like,

35:30.800 --> 35:33.800
Serge in the room just got the RISC-V branch of OpenBLAST

35:33.800 --> 35:37.800
merged into the trunk branch of OpenBLAST,

35:37.800 --> 35:40.800
so there was some generic support for RISC-V in the developed branch,

35:40.800 --> 35:44.800
but there was some, like, vendor-specific optimization

35:44.800 --> 35:46.800
in the RISC-V branch, and we just merged it, right,

35:46.800 --> 35:50.800
so that's also why, even for things as important as the West,

35:50.800 --> 35:52.800
but I've got things that are evolving fast,

35:52.800 --> 35:55.800
that's why, that's what I mean by,

35:55.800 --> 35:57.800
if you don't know how to get involved,

35:57.800 --> 35:59.800
you know that there's a lot of stuff to get involved in,

35:59.800 --> 36:03.800
and look around any library you generally use,

36:03.800 --> 36:05.800
probably one support RISC-V, so please add support RISC-V to that,

36:05.800 --> 36:07.800
it's going to be nice.

36:07.800 --> 36:09.800
And then another question is, there's different aspects,

36:09.800 --> 36:12.800
there's getting it to build, to compile,

36:12.800 --> 36:14.800
there's, you talked about performance a bit as well,

36:14.800 --> 36:16.800
what about running the test suite,

36:16.800 --> 36:20.800
because that's like, there may be 100,000 tests,

36:20.800 --> 36:22.800
there may be five failing on RISC-V,

36:22.800 --> 36:26.800
fixing those may require a lot of work to get it to actually fully back.

36:26.800 --> 36:33.800
Yes, so the, your comment was, yeah, we can compile,

36:33.800 --> 36:37.800
we can test, but what about the five last tests

36:37.800 --> 36:40.800
which are failing on RISC-V specifically?

36:40.800 --> 36:44.800
Yeah, so either it's assumptions which are taken

36:44.800 --> 36:48.800
which don't hold on RISC-V, that's usually the hard one to figure out,

36:48.800 --> 36:53.800
or, yeah, well the code is generally broken,

36:53.800 --> 36:55.800
but again, like, because it's running on X86,

36:55.800 --> 36:59.800
it was just working, that's why I think having support for

36:59.800 --> 37:03.800
M64, F390, XPPC helps, because the projector really knows

37:03.800 --> 37:07.800
that it's not just X86, and so they developed it in a way

37:07.800 --> 37:09.800
that works everywhere.

37:09.800 --> 37:12.800
QMU in that is very interesting, because, well,

37:12.800 --> 37:15.800
if you're in QMU on X86, the memory model,

37:15.800 --> 37:18.800
they don't try to emulate anything, so it's just the host memory model.

37:18.800 --> 37:22.800
So you test on X86 with QMU, so you have your Raspberry,

37:22.800 --> 37:25.800
based on your memory model, it's just going to work.

37:25.800 --> 37:28.800
You go to a bold, suddenly it's going to fail,

37:28.800 --> 37:30.800
and you don't know why, but it works on RISC-V,

37:30.800 --> 37:34.800
or it works on QMU, well, but the thing is it's TSO on QMU,

37:34.800 --> 37:37.800
weak memory ordering on RISC-V, and that's the harm bug to track.

37:37.800 --> 37:38.800
Yes, please.

37:38.800 --> 37:40.800
I just wanted to add the R-M-I to the exact same issue.

37:40.800 --> 37:43.800
Yes, so the comment was R-M-I is exactly the same problem,

37:43.800 --> 37:45.800
and yes, it's exactly the same issue.

37:45.800 --> 37:46.800
Yes, please.

37:46.800 --> 37:51.800
While porting some software to RISC-V,

37:51.800 --> 37:58.800
I had a problem that didn't appear even on ARM64,

37:58.800 --> 38:01.800
so there are even bits that...

38:01.800 --> 38:02.800
Yes.

38:02.800 --> 38:06.800
...that ARM64 doesn't help.

38:06.800 --> 38:11.800
It was that GP register that isn't on...

38:11.800 --> 38:12.800
Possibly.

38:12.800 --> 38:15.800
It's a long story, but it was related to GP register

38:15.800 --> 38:19.800
that worked differently, that is not on ARM64,

38:19.800 --> 38:24.800
and is on RISC-V, so there are even stories like this.

38:24.800 --> 38:26.800
Just to repeat your comment again for online,

38:26.800 --> 38:28.800
it's not because everything works on ARM64

38:28.800 --> 38:31.800
that everything is going to magically work on RISC-V.

38:31.800 --> 38:34.800
Yes, there are some RISC-V specificities, but yeah.

38:34.800 --> 38:35.800
Yes, please.

38:35.800 --> 38:38.800
I'm just curious, you may be outside of the scope of the effort,

38:38.800 --> 38:41.800
but it seems like there's an opportunity to provide

38:42.800 --> 38:46.800
possibilities for the hardware vendors

38:46.800 --> 38:50.800
in terms of the code that's already forwarded,

38:50.800 --> 38:54.800
the prevalence of specific instructions in the app prices

38:54.800 --> 38:58.800
so that the hardware vendors know what to focus on

38:58.800 --> 39:00.800
in terms of the performance.

39:00.800 --> 39:04.800
So the remark was there is opportunity in looking

39:04.800 --> 39:08.800
at the dynamic traces of instructions for hardware vendors

39:08.800 --> 39:11.800
to figure out what do we need to invest in, and yes.

39:11.800 --> 39:12.800
Yeah.

39:12.800 --> 39:15.800
To shun them, yes, there's a lot of opportunity there.

39:15.800 --> 39:18.800
The question usually for that is more what workload

39:18.800 --> 39:20.800
should we focus on?

39:20.800 --> 39:23.800
And that's a very tough question because obviously it depends

39:23.800 --> 39:25.800
which market you're targeting.

39:25.800 --> 39:28.800
Also given the certain market, there is five different software,

39:28.800 --> 39:30.800
five different frameworks, which one are you looking at?

39:30.800 --> 39:32.800
How do you use the frameworks?

39:32.800 --> 39:33.800
Which library?

39:33.800 --> 39:34.800
But yes.

39:34.800 --> 39:35.800
There's a...

39:35.800 --> 39:36.800
Yeah.

39:36.800 --> 39:43.800
Just further on the general subject of not obvious problems

39:43.800 --> 39:47.800
and QMU are analyzing them.

39:47.800 --> 39:52.800
So the single most useful, not obvious thing with QMU

39:52.800 --> 39:58.800
is a mode for alternative and most effective operations

39:58.800 --> 40:04.800
where it will crash the diagnostic components in the register.

40:04.800 --> 40:15.800
So the remark was QMU has a mode where you can trash...

40:15.800 --> 40:18.800
You can trash or cache?

40:18.800 --> 40:19.800
Trash.

40:19.800 --> 40:20.800
Trash.

40:20.800 --> 40:21.800
Okay.

40:21.800 --> 40:26.800
You can trash the vector register length if I understand...

40:26.800 --> 40:32.800
So for masks and for tails of the mask or the elements...

40:32.800 --> 40:33.800
Okay.

40:33.800 --> 40:36.800
...and elements that have been built into that vector...

40:36.800 --> 40:39.800
Yeah, you can basically put garbage into the tail elements

40:39.800 --> 40:42.800
and other kind of things like that to help you test

40:42.800 --> 40:45.800
so that you will actually see that you are using

40:45.800 --> 40:48.800
not what you expected to use in the vector.

40:48.800 --> 40:51.800
And that just makes your life easier for testing

40:51.800 --> 40:55.800
because it's going to crash instead of just failing silently.

40:56.800 --> 40:58.800
Yes, please.

41:07.800 --> 41:10.800
So the question is for precision and compatibility

41:10.800 --> 41:12.800
do we want QMU in ARM64?

41:12.800 --> 41:15.800
So the fun thing is for adoption that I mentioned at the beginning

41:15.800 --> 41:18.800
like to build Java, we are actually using an ARM64 VM

41:18.800 --> 41:20.800
just because it's cheaper on the cloud.

41:20.800 --> 41:22.800
And yeah, I mean it's compilation

41:22.800 --> 41:26.800
but you run the GCC compiler basically and it just works.

41:26.800 --> 41:28.800
Like there's no question.

41:28.800 --> 41:32.800
So you are stress testing GCC but it works.

41:32.800 --> 41:36.800
For general testing, yes, but for example,

41:36.800 --> 41:38.800
GitHub Actions you only have XC6 machines.

41:38.800 --> 41:41.800
So I think it would be better to test on ARM64

41:41.800 --> 41:43.800
because yeah, you have week memory monitoring,

41:43.800 --> 41:46.800
you have all of that, but it's harder to access.

41:48.800 --> 41:49.800
Yes, please.

41:49.800 --> 41:53.800
Yes, so the comment is with ARM64 SV,

41:53.800 --> 41:55.800
which is the Scalemore vector extension,

41:55.800 --> 41:58.800
like vector length agnostic extension,

41:58.800 --> 42:01.800
it will help with five to basically let the world know

42:01.800 --> 42:04.800
that there is not just vector and yes, absolutely.

42:04.800 --> 42:07.800
Like I mentioned in the slide right after,

42:07.800 --> 42:10.800
but there is a project called XCMD

42:10.800 --> 42:12.800
which is a project called XCMD.

42:12.800 --> 42:14.800
And it's a project called XCMD.

42:14.800 --> 42:16.800
And it's a project called XCMD.

42:17.800 --> 42:19.800
There is a project called XCMD

42:19.800 --> 42:24.800
which is kind of abstraction over vector stuff.

42:24.800 --> 42:28.800
And yes, it assumes that the vector is 128, 512, or 256

42:28.800 --> 42:29.800
and you don't have a choice.

42:29.800 --> 42:32.800
So feeling resiving that is a bit painful.

42:32.800 --> 42:35.800
Yes, I think there was.

42:35.800 --> 42:36.800
Yeah, sorry.

42:36.800 --> 42:38.800
Okay.

42:38.800 --> 42:40.800
Any other questions?

42:42.800 --> 42:44.800
Okay, well, thank you everyone.

42:44.800 --> 42:46.800
Thank you.

