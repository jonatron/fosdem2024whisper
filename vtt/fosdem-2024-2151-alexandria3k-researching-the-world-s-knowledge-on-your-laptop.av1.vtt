WEBVTT

00:00.000 --> 00:10.840
Thank you.

00:10.840 --> 00:12.840
Good morning.

00:12.840 --> 00:16.160
How many of you have read the scientific paper in the last month?

00:16.160 --> 00:22.440
Okay, for a number of you, probably why you are here, it turns out that we are churning

00:22.440 --> 00:24.160
ever more scientific papers.

00:24.160 --> 00:26.600
I'll show figures later on.

00:26.600 --> 00:33.200
And for this reason, we are conducting studies on them, so seeing how they accumulate.

00:33.200 --> 00:38.680
Some types of studies that you see here, systematic studies, so a study on previous

00:38.680 --> 00:43.440
science that can be reproduced and done in a specific objective, we're not just picking

00:43.440 --> 00:46.800
out a few papers from a pile.

00:46.800 --> 00:52.200
Scientometric or bibliometric studies where we measure things such as the scientist's

00:52.200 --> 00:55.960
output or work done in a specific field.

00:55.960 --> 00:59.680
And then there's the analysis where we use statistical techniques from existing studies,

00:59.680 --> 01:06.360
say how cancer is related to smoking in order to get better results and other secondary

01:06.360 --> 01:07.360
studies.

01:07.360 --> 01:12.760
And as you see here, such studies have been rising from 1970s onward at an exponential

01:12.760 --> 01:13.760
rate.

01:13.760 --> 01:15.560
Look at the scale on the left, it's logarithmic.

01:15.560 --> 01:21.120
Scale, something that we will see many times in this presentation.

01:21.120 --> 01:26.760
Now they have reached tens of thousands of studies every year.

01:26.760 --> 01:31.680
We've been conducting such studies also in our group, such as looking how various data

01:31.680 --> 01:38.920
papers, open data papers are being used by others or how software engineering research,

01:38.920 --> 01:44.280
thousands of papers that are published on software engineering every year are actually

01:44.280 --> 01:46.920
used in practice.

01:46.920 --> 01:52.200
And lately also looking at how machine learning is associated and used in software engineering.

01:52.200 --> 01:56.280
There have been so many papers in this area that we conducted what is called the tertiary

01:56.280 --> 01:57.280
study.

01:57.280 --> 02:02.880
So we didn't look at all those thousands of papers, but we looked at papers that summarized

02:02.880 --> 02:08.560
those studies, a few dozens of papers.

02:08.560 --> 02:16.640
Research on using existing publication data and building on it, demand quantitative data.

02:16.640 --> 02:20.800
And two scientists are famous for working and establishing the field.

02:20.800 --> 02:27.120
The one you see on the left is Eugene Garfield, a linguist and businessman.

02:27.120 --> 02:33.320
He established the Institute for Scientific Information, which became then the science

02:33.320 --> 02:39.960
citation index, then was bought by Thomson Reuters and now it's a firm called Clarivate.

02:39.960 --> 02:46.200
And on the right, another famous scientist, Derek DeSolla Price, who also worked in this

02:47.120 --> 02:54.440
field, established disciplines together with the Garfield of Scientometrics and Bibliometrics.

02:54.440 --> 02:59.960
We can use and measure scientific output and study it using a variety of services such

02:59.960 --> 03:01.680
as the ones you see here.

03:01.680 --> 03:04.720
How many of you have used any of those in the past month say?

03:04.720 --> 03:06.520
Wow, a fair number.

03:06.520 --> 03:10.360
However, there are problems associated with them.

03:10.360 --> 03:14.040
First of all, there's a lack of transparency, repeatability and reproducibility, a query

03:14.040 --> 03:20.000
that you give today on say Google Scholar will give you, well give completely different

03:20.000 --> 03:21.960
results in a year.

03:21.960 --> 03:27.080
And we have no idea how the results appear here, why the results appear in a specific

03:27.080 --> 03:28.080
order.

03:28.080 --> 03:31.240
Latency can be high and bandwidth can be low.

03:31.240 --> 03:37.960
If you want to run a query on tens of thousands or millions of publications, good luck going

03:37.960 --> 03:40.680
back and forth on an API.

03:40.680 --> 03:43.280
And if this wasn't bad enough, there are also rate limits.

03:43.320 --> 03:50.240
I have the privilege of having been kicked out of various digital libraries for 25 years

03:50.240 --> 03:51.240
now.

03:51.240 --> 03:55.960
There are also the query languages that we use are also proprietary, so each service

03:55.960 --> 03:58.000
has its own query language.

03:58.000 --> 04:01.920
And they can be restricted in what you can do, maybe you can add terms, what you cannot

04:01.920 --> 04:05.360
order them or you cannot search in specific fields.

04:05.360 --> 04:07.120
The coverage may be limited.

04:07.120 --> 04:12.000
Some services contain only a subset of what we want to search.

04:12.000 --> 04:14.800
And finally, there's the issue of availability and costs.

04:14.800 --> 04:19.080
Some are lucky enough to have subscriptions to some of these services.

04:19.080 --> 04:23.240
Others don't and even if you have a subscription, getting access to the full dataset may be

04:23.240 --> 04:24.240
difficult.

04:24.240 --> 04:29.520
Thankfully, two developments are changing the status quo.

04:29.520 --> 04:33.960
The one is the rise in computing power that we have in our hands.

04:33.960 --> 04:36.640
I don't know if you have seen this picture.

04:36.640 --> 04:43.120
We are the group of people delivering an Elliott computer to the Norwich City Council.

04:43.120 --> 04:48.120
And a few decades later, somebody putting a photograph in a Raspberry Pi zero in front

04:48.120 --> 04:50.000
of the same building.

04:50.000 --> 04:53.280
I've compared the two machines.

04:53.280 --> 04:56.040
Interestingly, both are European endeavors.

04:56.040 --> 05:01.400
And there are three to six orders of magnitude increases in power.

05:01.400 --> 05:07.480
So things that we could do in 56 compared to now are a thousand to a million times better

05:07.480 --> 05:08.640
than they are today.

05:08.640 --> 05:10.360
So obviously we can use this power.

05:10.360 --> 05:16.160
Remember, CentroMetrics was established on those decades, past decades, to do a lot more.

05:16.160 --> 05:21.960
The second development is the open availability of datasets.

05:21.960 --> 05:28.720
We are here celebrating openness in software and data and hardware.

05:28.720 --> 05:34.280
And a large number of datasets associated with research have now become available, such

05:34.280 --> 05:40.120
as Crossref, ResearchRid, United States Patents, PubMed, Research Organizational Registry.

05:40.120 --> 05:42.480
I'll go over them.

05:42.480 --> 05:48.360
So alluding with some lack of modesty, I admit to the famous library of Alexandria, how it

05:48.360 --> 05:55.160
should be in the third millennium, I've developed a system called Alexandria 3K that allows

05:55.160 --> 06:00.760
us to perform publication metadata analytics on our desktop.

06:00.760 --> 06:07.400
What it does, it provides us relational access to about two terabytes of data without needing

06:07.400 --> 06:14.040
to have actually this amount of space on your computer, who has two terabytes available

06:14.040 --> 06:16.040
on their laptop.

06:16.040 --> 06:17.280
Exactly.

06:17.280 --> 06:19.760
So you don't need it with Alexandria 3K.

06:19.760 --> 06:24.240
It gives us access to about four billion records, 75, 74 tables.

06:24.240 --> 06:26.640
You install it as a single Python model.

06:26.640 --> 06:32.600
You don't need to maintain a graph database or a cluster to install and maintain.

06:32.600 --> 06:39.120
I know it's not a problem for you, but it is a problem for people who are not in computing.

06:39.120 --> 06:40.760
And it's also super efficient.

06:40.760 --> 06:47.360
So if you run a sample query on the whole dataset, but sample it, it can finish in minutes.

06:47.360 --> 06:52.400
If you build slices of the data to study, it can finish in between five hours in a couple

06:52.400 --> 06:53.400
of days.

06:53.400 --> 06:57.360
But then you can run queries that finish in seconds.

06:57.360 --> 07:02.600
And the space requirements start at about 160 gigabytes for the downloaded data.

07:02.600 --> 07:09.040
You can then process it in compressed form without requiring to decompress it.

07:09.040 --> 07:16.120
What I will show in the next about half an hour is the model that you get access to and

07:16.120 --> 07:18.160
the type of data that you can use.

07:18.160 --> 07:21.080
I will explain how it can be used in practice.

07:21.080 --> 07:26.920
Go deep into how it is implemented, giving you perhaps ideas how you can do similar things

07:26.920 --> 07:32.880
and finish with some issues, limitations on how to move forward.

07:32.880 --> 07:37.200
So what you see here is the scheme of all 75 tables.

07:37.200 --> 07:40.120
They are colored based on the dataset they come from.

07:40.120 --> 07:43.000
So on the top you see the United States patent.

07:43.000 --> 07:44.880
The various yellow ones are cross-off.

07:44.880 --> 07:46.880
This is the main dataset.

07:46.880 --> 07:52.280
It contains details about publications and their authors.

07:52.280 --> 07:58.360
There's also a similar set from PubMed regarding health sciences.

07:58.360 --> 08:03.360
Information about researchers, open access journals, research organizations and some

08:03.360 --> 08:06.280
other stuff I will explain.

08:06.280 --> 08:09.680
As I said, the main dataset is cross-ref.

08:09.680 --> 08:11.440
You see it here.

08:11.440 --> 08:13.720
It contains mainly works.

08:13.720 --> 08:20.400
And then these contain references to their authors, the updates, subjects, funders, licenses

08:20.400 --> 08:28.240
and also the affiliations of the authors and the references in each work.

08:28.240 --> 08:31.040
In numbers these are on top thousands.

08:31.040 --> 08:36.320
So you get about 135 millions of publications.

08:36.320 --> 08:43.240
Not all of them contain a subject or references but you see these numbers diminishing but

08:43.240 --> 08:45.960
have been going up over the years.

08:45.960 --> 08:51.960
About 360,000 million authors and about 1.7 billion references.

08:51.960 --> 08:53.640
So each work has a reference at the end.

08:53.640 --> 08:56.680
You get that many references.

08:56.680 --> 09:04.120
Many of the works are also associated with subjects telling us what area they appear in.

09:04.120 --> 09:06.120
Most of the works are journal articles.

09:06.120 --> 09:12.720
Then come book chapters, proceedings and other elements such as books and post-edentries

09:12.720 --> 09:16.480
and dissertations, far, far, far smaller.

09:16.480 --> 09:21.160
If we look at the publications that appear in the dataset each year, all of these charts

09:21.160 --> 09:25.640
by the way have been derived through Alexandria 3K.

09:25.640 --> 09:31.240
You see that they've been increasing at a very large rate over the years.

09:31.240 --> 09:34.080
If you think it's exponential, you're indeed right.

09:34.080 --> 09:38.760
If we plot it on a logarithmic scale, you see a linear rise which means an exponential

09:38.760 --> 09:40.720
rise in the numbers.

09:40.720 --> 09:43.760
You can even see two dips in the rise.

09:43.760 --> 09:46.240
Any idea why these are there?

09:46.240 --> 09:47.240
What happened?

09:47.240 --> 09:48.240
Wars.

09:48.240 --> 09:49.240
Wars, exactly.

09:49.240 --> 09:56.160
The wars apart from the extreme human tragedy also affected the science of the world wars.

09:56.160 --> 10:01.560
Regarding availability, these are the various lines show which works have an abstract, the

10:01.560 --> 10:07.400
subject, the funder, a researcher identifier, awards associated with the works.

10:07.400 --> 10:10.520
You see that these numbers have been rising in most areas.

10:10.520 --> 10:15.000
The subject is a special case.

10:15.000 --> 10:21.240
Another dataset associated with Alexandria 3K is the open research and contributor ID.

10:21.240 --> 10:23.840
How many of you have such an identifier?

10:23.840 --> 10:24.840
Good.

10:24.840 --> 10:26.840
If you don't have one, go and get one.

10:26.840 --> 10:33.280
If you publish, it helps us, all scientists, associate you with what you have been publishing.

10:33.280 --> 10:39.920
These have some basic details about the persons and then further details associated with distinctions

10:39.920 --> 10:45.520
and education, invited positions, memberships, peer reviews conducted in other resources

10:45.520 --> 10:52.280
associated with researchers who have access to a specific large telescope, for example.

10:52.280 --> 10:58.760
Again the completeness of this data is not uniform, so most people have associated works

10:58.760 --> 11:05.520
with them, but fewer have associated employment, education, personal data.

11:05.520 --> 11:13.920
A similar dataset to the works is the dataset of publications made available through PubMed,

11:13.920 --> 11:21.160
United States government effort, which has publications associated with health sciences,

11:21.160 --> 11:26.520
so health and biomedicine.

11:27.480 --> 11:33.640
Similar to Crossref, but it also contains some more specialized fields, such as pathogens

11:33.640 --> 11:41.000
or a very complete taxonomy of where something belongs or chemical substances mentioned in

11:41.000 --> 11:47.120
a specific paper, so it allows you to do more concentrated and specific research.

11:47.120 --> 11:54.080
Also available are the United States patents for the past 20 years, containing about 5 million

11:54.560 --> 12:01.800
records and a registry containing about 600,000 records of research organization, so the

12:01.800 --> 12:06.840
organization you are associated with, if it conducts research, it should appear there,

12:06.840 --> 12:11.960
it's a taxonomy, so it contains the parent organization of your organization, in many

12:11.960 --> 12:16.200
cases up to the top, maybe the government.

12:16.200 --> 12:22.720
What else is there, some smaller datasets, journal names, so that you can directly associate

12:22.840 --> 12:29.040
them with their ISSN, funder names and also director of open access journal metadata about

12:29.040 --> 12:31.960
19,000 records.

12:31.960 --> 12:38.440
All these are tied together through identifiers, such as the digital object identifiers, DOIS,

12:38.440 --> 12:43.600
the research identifiers, ISSNs, URLs and many other identifiers.

12:43.600 --> 12:49.160
In a way you see here, these are just some representative tables from diverse datasets

12:49.160 --> 12:52.160
linked together.

12:52.600 --> 12:56.480
How are we using Alexandria 3K in practice?

12:56.480 --> 13:02.040
You can use it as a command line tool with a typical nowadays for large tools pattern

13:02.040 --> 13:09.040
of running A3K, the A3K command and then sub-commands such as populate or process or query.

13:12.360 --> 13:17.640
Here's an example of that, I'm running the A3K command asking it to populate a database

13:17.720 --> 13:24.720
called COVID-DOTB from the crossref dataset found in this directory and selecting only

13:25.560 --> 13:29.640
the rows that have a title or an abstract that matches COVID.

13:29.640 --> 13:33.240
I will show you later on how this can be useful.

13:33.240 --> 13:38.480
You can also use it through Python, here's an example, you import it, you create an instance

13:38.480 --> 13:45.480
for the specific data source that you are interested in and then you give a similar,

13:46.040 --> 13:50.400
you call a method that performs a similar function.

13:50.400 --> 13:57.400
Typically the way to use A3K is to download the data for crossref, it's about 160 gigabytes,

13:57.920 --> 14:02.480
can be downloaded through a torrent in about three hours, by the way this gives you plausible

14:02.480 --> 14:06.200
deniability argument of why you are using torrent.

14:06.200 --> 14:11.840
You can then run various exploratory data analytics queries directly on the sample,

14:11.880 --> 14:16.920
this can finish in about two minutes for 1% of the records, no need to uncompress this

14:16.920 --> 14:20.640
to the terabytes required.

14:20.640 --> 14:26.640
Or you can populate a database, a SQL-like database, this can take for four to 20 hours,

14:26.640 --> 14:33.640
the database can be some four to 200 gigabytes in size depending on what you store in it

14:34.320 --> 14:36.080
if you are selective.

14:36.080 --> 14:42.320
And there on the database you can test, define, refine analysis queries, the queries can

14:42.320 --> 14:48.560
run in minutes or in hours if they are very complicated.

14:48.560 --> 14:53.040
Mainly how you can use it, you can run ad hoc SQL queries directly on the uncompressed

14:53.040 --> 15:00.040
data, you can populate SQL-like databases and there you can select elements either horizontally

15:00.800 --> 15:05.800
so you can say I want only the records that match the specific condition, works published

15:05.800 --> 15:12.000
in the last two years, or vertically specific columns, I'm not interested in the abstract

15:12.000 --> 15:17.520
for instance because it takes up a lot of space and I'm not going to use it.

15:17.520 --> 15:22.720
And then once you index the SQL database you can have many queries finishing in seconds

15:22.720 --> 15:26.440
or even on the complete data set.

15:26.440 --> 15:32.080
Here is an example of a query directly on the data set so without creating an intermediate

15:32.080 --> 15:39.080
database measuring how many publications appear each year, the chart I showed you earlier.

15:40.320 --> 15:45.800
Here is another example of a query that performs sampling, it calls a random function of Python

15:45.800 --> 15:52.800
and which returns to when it's less than 0.01 so I get 1% sample from the data set to find

15:53.480 --> 16:00.480
out how many works contain abstracts and how many don't, this is the answer and there is

16:00.520 --> 16:06.920
a chance on the complete data set but sampling it in about two minutes.

16:06.920 --> 16:12.600
Another example here of populating a database in order to extract metrics that I showed

16:12.600 --> 16:19.600
you previously how many works have authors or subjects or abstracts associated with them.

16:20.480 --> 16:27.160
Similarly another population completing the ORC data set showing how many elements are

16:27.160 --> 16:32.400
there in the corresponding query on that data set.

16:32.400 --> 16:39.400
Let's see some more advanced elements. Here there are two papers both written by Nobel

16:39.680 --> 16:45.080
prize winning authors, on the left is one by Colin Schame, a Nobel winning paper used

16:45.120 --> 16:51.400
to establish theorems to develop a method for calculating the structure of the electrons

16:51.400 --> 16:56.920
and on the right another famous paper, you are probably aware of it by Watson and Crick

16:56.920 --> 17:02.680
or Nobel winners introducing a model of the structure of the DNA.

17:02.680 --> 17:08.880
However, the way these papers are associated with science turns out to be very different.

17:08.960 --> 17:15.120
If we look at the papers that cite them, so the red papers here, the two blue ones are

17:15.120 --> 17:22.120
this one and this one, we see that for the left papers that cite the electron paper also

17:22.760 --> 17:29.760
cite previous work, so works cited by this. Whereas papers that cite the DNA paper do

17:30.000 --> 17:37.000
not cite the work published before it. So people have said that the paper on the left

17:38.000 --> 17:44.000
consolidates existing research and advances it whereas the paper on the right, the DNA

17:44.000 --> 17:51.000
paper is a disruptive paper that changes all things and therefore people no longer cite

17:51.760 --> 17:56.200
other works, they cite only this paper and ones that follow it.

17:56.200 --> 18:00.760
There is a measure that you can calculate here, the paper that first established, published

18:00.760 --> 18:07.120
in Nature gave these numbers here. My own calculation on Alexandria 3K gives similar

18:07.120 --> 18:13.240
numbers and the very highly significant statistical measure showing that these methods are equivalent.

18:13.240 --> 18:18.280
The one on the top, the published one is opaque, you cannot reproduce it because the data is

18:18.280 --> 18:23.320
not openly available, the one on the bottom can be run on your laptop.

18:23.320 --> 18:28.320
Here are some other measures, the evolution of scientific publishing after the Second

18:28.360 --> 18:34.360
World War, before that things were completely different, that's why I'm not looking at it.

18:34.360 --> 18:41.360
We see many interesting changes, number of authors per work, reason from 1.5 to 4, works

18:41.440 --> 18:48.440
per author from 1.99 fell to 1.59, references rose from 13 to 46, pages doubled to 12, the

18:52.880 --> 18:57.360
consolidation disruption index fell, so if you think that science is becoming less disruptive

18:57.400 --> 19:04.400
these days, it is a true number of citations, works published, journals, works cited at least

19:04.600 --> 19:11.600
once and factors have all reason exponentially. All these calculated with queries you can

19:11.640 --> 19:16.640
find on the software you can download from GitHub.

19:16.640 --> 19:21.360
Here's another interesting chart showing how the evolution of applications has changed

19:21.360 --> 19:27.160
in specific fields. You can see the big rise in computer science, the purple thing that

19:27.840 --> 19:34.340
has increased substantially over the years and also the relative fall in publications

19:34.340 --> 19:40.480
in arts and humanities, the other purple here that has diminished in this way. The absolute

19:40.480 --> 19:45.680
number has risen, don't be fooled because publications have risen exponentially, but

19:45.680 --> 19:52.480
still it occupies a lot less than it did in 1945.

19:52.520 --> 19:57.520
Here are examples of two other data sets. Here is the evolution of applicants by year

19:57.520 --> 20:03.680
and country of US patents. You see a fall in the number of patents associated with the

20:03.680 --> 20:10.680
United States and Japan and the rise from a low base of patents associated with China

20:10.680 --> 20:17.680
on the bottom of the blue line. Another one associated with replicating a paper looking

20:18.600 --> 20:25.600
at specific software, statistical software used in health science public research. Again

20:25.920 --> 20:32.920
you see with green the original paper and with orange replicated results with Alexandria

20:34.640 --> 20:41.640
3K. This was completed by a TU Delft student in a couple of weeks. Let me show you a more

20:41.960 --> 20:47.960
substantial example of what can be done with Alexandria 3K, proof of principle of concept

20:47.960 --> 20:54.960
study on a specific topic namely COVID-19. What you are seeing this was while checking

20:54.960 --> 21:00.960
the data set, it's a publication I found in the American Journal of Ethics, COVID care

21:00.960 --> 21:07.960
in color, which was published by an author, a nurse working in a Bronx emergency room

21:08.280 --> 21:15.280
team for six years and painting for 25 years. She captioned it as fear of non contagion

21:15.280 --> 21:21.680
is dreadful, especially without proper protective equipment. So at the beginning of the pandemic

21:21.680 --> 21:26.920
when we were conflicting information on how and when to use our PPE we relied on each

21:26.920 --> 21:33.920
other. I created the data set in the way you see here. I populated a vertical and horizontal

21:34.040 --> 21:41.040
slice in an escalate database, selecting some elements with those that matched COVID in

21:43.760 --> 21:49.160
their title or abstract in running nine hours, about three gigabytes of data and the ones

21:49.160 --> 21:56.160
I indexed it, it rose to 3.6 gigabytes. We can see some numbers, half a million of published

21:57.080 --> 22:04.080
articles, 2.6 million authors, imagine the amount of effort that went into there and

22:09.480 --> 22:15.280
eight million references that go more there. What are the topics associated with this research?

22:15.280 --> 22:19.440
Everything you can imagine, you can see education and engineering featuring very high, of course,

22:19.440 --> 22:25.240
after general medicine, but even strategy, management, law, cultural studies, pollution,

22:25.280 --> 22:32.280
anthropology, AI, waste management, ocean engineering, all over the place. Who funded

22:33.120 --> 22:39.880
that research? This is the query I run, National Natural Science Foundation of China, the highest

22:39.880 --> 22:44.840
number of publication, and then National Institutes of Health and National Science Foundation

22:44.840 --> 22:51.080
from the United States, followed by various trusts and councils. However, if we look at

22:51.160 --> 22:56.960
the affiliations associated with COVID publications, these are the queries that established this.

22:56.960 --> 23:01.920
We see that first came the government of the US, then the University of California System,

23:01.920 --> 23:07.720
University of Toronto, and so on. Here I use the research organization registry and I moved

23:07.720 --> 23:13.720
the organizations up to the parent organization, so Berkeley, for instance, and the University

23:13.720 --> 23:20.720
of Southern California rose and UCSD rose to the University of California systems.

23:21.960 --> 23:28.040
Another question is, how quickly could we look and work on each other? So how scientists

23:28.040 --> 23:33.560
publishing COVID research could cite each other? Was it taking too long a time because

23:33.560 --> 23:41.240
things were advancing too fast? And what I have plotted here is publications citing other COVID

23:41.240 --> 23:47.720
publications published each month. And you see that fairly quickly, even on April 2020,

23:47.800 --> 23:54.760
there were thousands of citations to other COVID publications, which rose to hundreds of thousands

23:54.760 --> 24:02.120
late in 2020, early 2021. The number you see at the beginning is an artifact, journals that

24:02.120 --> 24:08.040
got published with a January date, although they appeared a lot later. This shows us we shouldn't

24:08.040 --> 24:13.880
blindly trust our data. I also looked at collaboration. I found some amazing things.

24:13.960 --> 24:19.560
There were articles with thousands of authors authored by thousands of people. So you see

24:19.560 --> 24:27.560
articles by 2,000 or 1,700 authors. I thought this cannot be true, so I looked at it and I saw

24:27.560 --> 24:33.960
that there were indeed most articles had the number of, say, five authors, but there were

24:33.960 --> 24:40.920
articles with 7,000 authors. I looked, couldn't believe my eyes, I haven't seen such thing before,

24:40.920 --> 24:46.600
and it was indeed published here. The number of authors appear in a footnote, pages 20 to 28.

24:48.920 --> 24:53.640
And this is not an isolated case. Through other queries, I found many articles with thousands

24:53.640 --> 25:00.920
of authors showing a way of collaborating in an amazing way. People probably contributed by giving

25:00.920 --> 25:06.360
data from hundreds or thousands of hospitals, and all these were collected on papers.

25:06.600 --> 25:15.080
Let me switch subject. How many of you heard the impact factor? Yeah. What is the impact factor for

25:15.080 --> 25:22.200
those who haven't? It's a measure that tries to see how important a journal is by measuring the

25:22.200 --> 25:30.040
citations articles in this venue appear in a given year divided by the publications in the previous

25:30.040 --> 25:35.320
two years. It has been severely criticized, especially when it's used for measuring scientific

25:36.040 --> 25:41.560
worthiness or productivity of authors. But another problem is that it is opaque,

25:41.560 --> 25:47.560
so clarivate publishes, but we have no idea how exactly it derives those numbers. It publishes

25:47.560 --> 25:53.720
the method, but we cannot replicate it. We can with Alexandria 3K with queries and populations,

25:53.720 --> 26:01.880
such as the ones you see here, and we can get results that rank journals by impact factor with

26:01.960 --> 26:08.200
a very high significance correlation with the ones published by clarivate.

26:11.000 --> 26:16.520
Through this, we can do queries such as find the most cited article in the last two years.

26:18.280 --> 26:25.080
It is this article establishing a method where you can find how matter depends on the properties

26:25.080 --> 26:31.480
of steel based just on facts associated with the atomic structure of that. I found that very

26:31.480 --> 26:38.600
strange, very specialized subject, so I run a query to find what publications are citing this

26:38.600 --> 26:45.480
article, and I got results in the titles such as these at a rate of about eight records per second

26:45.480 --> 26:51.640
over the entire data set. If you cannot understand the titles, this is how people outside computing

26:51.640 --> 26:58.760
view us when we talk shop. I also looked at cited articles over the last two years,

26:59.320 --> 27:05.640
but published in that period, and predictably this was clinical features of patients infected with

27:05.640 --> 27:12.280
the 2019 novel coronavirus in Wuhan, China, the first article reporting on COVID-19.

27:13.560 --> 27:21.160
Another metric of author productivity is the so-called H5 index, so how many papers you

27:21.160 --> 27:27.080
have published in the past five years that have been cited at least that number of times.

27:27.880 --> 27:33.640
I found an author that has an index of 76, which amounts to 15 papers a year,

27:34.360 --> 27:41.160
and many authors having more than 60 papers, an H index more than 60 and 100 more than 38.

27:41.160 --> 27:46.360
This was too large to believe because it's not only what was established previously,

27:47.000 --> 27:52.520
hyperproductive authors that publish papers every one paper every five days,

27:53.400 --> 28:02.120
but also papers that got cited a lot. So how could this happen? Using again a 3k, I looked at

28:02.120 --> 28:08.760
papers that were cited, the papers that those authors cited, and created and looked at those

28:08.760 --> 28:16.120
graphs, and what I found is that those papers were cited a lot more between them than papers of

28:16.120 --> 28:23.240
other authors with high productivity, but not that high. So this seems to suggest that some sort of

28:23.240 --> 28:30.760
clique is working there, authors citing each other, and thereby elevating artificially their H5 index.

28:30.760 --> 28:38.200
Whatever number you use, you can game it as you can understand. Finally, here's another interesting

28:38.200 --> 28:46.600
chart. Here I'm looking at what topics cite other topics, and for instance we can see these are the

28:46.600 --> 28:51.560
50 strongest relationships, and we can see for instance that the cancer research cites a lot

28:52.360 --> 29:00.040
chemistry organic and inorganic chemistry. I will finish with some details regarding the

29:00.040 --> 29:05.640
implementation of Alexandria 3k. I hope that you will get a few tricks that you can use here.

29:05.640 --> 29:12.760
A3k is based on a plugin architecture, so on the top you get the command line interface,

29:12.760 --> 29:18.440
which uses a Python API, which you can also use directly, and below it there are two

29:18.440 --> 29:26.120
sets of plugins. Data sources, the values I showed you, you can create new ones by creating a file

29:26.120 --> 29:32.840
that establishes a new data source, say archive publications, and processes, things that manipulate

29:32.840 --> 29:39.240
data in new ways. For example, matching authors with affiliations, or disambigrating authors with

29:39.240 --> 29:45.400
the same name based on what they publish. And finally, at the bottom is a plugin API that these

29:45.400 --> 29:54.920
plugins use in order to function. The main ideas behind the cross implementation and the other

29:54.920 --> 30:01.640
databases is SQLite and virtual tables. Virtual tables are a feature of SQL that's magical. You

30:01.640 --> 30:08.040
can create tables that don't exist as real in a real database, but appear as tables which you can

30:08.040 --> 30:15.880
access with the select and other SQL statements. It uses a method to partition the data because

30:15.880 --> 30:22.360
large databases come in many, many files, tens of thousands of files for the case of Crossref.

30:22.360 --> 30:29.720
I use the number of each file as an index so that the database, the virtual table implementation

30:29.720 --> 30:35.640
of SQLite does not jump from one partition to the next because the compressing partitions is

30:36.360 --> 30:44.200
expensive. Another trick is to understand once you have written a query or a selection of what

30:44.200 --> 30:50.760
data you are interested in, how to understand what tables or what fields you want from each table.

30:51.320 --> 30:57.960
I don't want to parse SQL, especially with all the specific implementation details and

30:58.840 --> 31:06.680
of the SQLite. So what I'm doing is I ask SQLite to trace the analysis of the query,

31:06.680 --> 31:15.400
and thereby I can understand which columns and tables it touches. I also create vertical slices

31:15.400 --> 31:23.480
of the partitions for the queries in order to run faster. And I use various queries to only look

31:23.480 --> 31:30.600
within a partition in order to populate records. So the Crossref data appears in JSON format,

31:31.240 --> 31:37.160
and in a self-contained file, one of the 26,000 files, it will contain all references to each

31:37.160 --> 31:44.600
work. I don't need to go to other partitions. So here's an example. When you run the query on

31:44.600 --> 31:51.800
the top, what is happening underneath, a virtual table is created, and the query is run on this

31:52.360 --> 31:58.760
very simple table. When, however, you also do joins, what is happening is it is creating

31:59.480 --> 32:07.560
the tables, but as you see here, the tables have a container ID restricted to each container in

32:07.560 --> 32:14.360
turn, one, two, three, up to 26,000, so that each partition is decompressed in turn, and not all of

32:14.360 --> 32:21.320
them are run together, and then this is run on tables that are actually realized. For

32:21.320 --> 32:28.440
population, similar things are happening. So if you populate something with a condition, so I want

32:28.440 --> 32:33.880
only those subjects associated with library information sciences, and you want only some

32:33.880 --> 32:40.680
columns. First of all, tracing establishes the table name and the field that you are interested in,

32:41.240 --> 32:49.720
and then again on partitions, tables are populated, and then queries are run to

32:51.000 --> 32:56.200
fill the data that you want from the populated tables. I found that this is faster than using

32:56.200 --> 33:02.680
virtual tables because of the various joins. A thing called topological sort is used to

33:02.680 --> 33:08.920
establish in which order the joins have happened based on the names of the tables that you want

33:08.920 --> 33:18.840
to join. Similar ideas for populating ORCID, United States Patterns, and so on. Here because this

33:18.840 --> 33:24.040
appears XML records, we can skip the parsing of XML records that we are not interested in,

33:24.040 --> 33:31.800
and thereby gain an additional efficiency advantage. Let me finish with some issues and

33:31.800 --> 33:41.320
limitations of A3K. The coverage of authors is fairly low, so about 17 out of 360 million authors

33:41.320 --> 33:48.280
have an associated ORCID with them. Keep in mind data go back to the beginning, to the second world

33:48.280 --> 33:55.400
war, so ORCID wasn't a thing then. But even now, not all authors have an ORCID. This is improving

33:55.400 --> 34:01.240
because many institutions asked them to, and we're also investigating ways with which we can

34:01.320 --> 34:07.640
disambiguate authors even if they don't have an ORCID through machine learning methods. Also

34:07.640 --> 34:12.040
affiliations, either they are missing or they appear in diverse forms, so the same university,

34:12.040 --> 34:18.840
say the one here, can appear as ULB or the full name of the university. Abstracts are also not

34:19.880 --> 34:25.000
always available. A small number of them have an abstract, but many publications also have a text

34:25.000 --> 34:30.680
mining link that you can use in order to obtain the full text for data mining purposes.

34:31.640 --> 34:36.600
The subjects of publications are based on an identifier established by Scopus,

34:36.600 --> 34:41.000
which is associated with complete journals. So if something appears in a zoology journal,

34:41.000 --> 34:47.320
we assume it has to do with zoology, even if it has to do with, say, biology or informatics.

34:48.280 --> 34:54.840
Again, we're working to use machine learning methods in order to obtain better results here,

34:55.480 --> 34:59.640
and looking specifically at the impact factor calculation, which many

34:59.640 --> 35:04.520
are interested in. Establishing what is a citable item is a tricky,

35:05.800 --> 35:11.400
clarivate uses a proprietary method, where, for instance, an editorial or a letter is not

35:11.400 --> 35:17.960
considered citable. It's difficult to do this automatically. I assume they have people working

35:17.960 --> 35:26.360
on that. Way forward on how we can work as a community to improve a 3-key, first of all,

35:26.360 --> 35:30.200
these are the early days, so I'd be very happy to help the community conduct studies.

35:30.200 --> 35:34.920
If you have an idea for something for a way to use a 3-key, please contact me.

35:35.640 --> 35:39.480
I would like to integrate more open access data. So here are some ideas,

35:39.480 --> 35:46.200
an archive, the DBLP is a database of computer science papers. There is a taxonomy of medical

35:46.200 --> 35:52.360
research called MESH, which is extremely interesting, and the wider one used by the public

35:52.360 --> 35:59.000
library of science that I also think would be worth integrating and associating works with it.

36:00.040 --> 36:04.840
Associated with that is improving the various processes, so things in which we can process

36:04.840 --> 36:11.560
the data, disambiguate authors, there are many John Smiths or Zouz in China, find out which are the

36:11.560 --> 36:17.720
ones that are written in a specific article, and classify the topics of the publications.

36:18.440 --> 36:24.360
And finally, and this is relevant to us here at FOSTEM, evangelize more and better data

36:24.360 --> 36:31.880
availability, more use of ORCID and publication improvements on the published metadata.

36:33.640 --> 36:37.080
With this, I thank you for your interest and attendance here.

36:48.200 --> 36:54.040
I think we should have time for one or two questions. Do we have questions?

37:02.840 --> 37:09.400
Thank you for your talk. About 10 years ago, I participated in a Kangol contest,

37:09.400 --> 37:16.200
which was about disambiguating authors to link papers written by the same authors

37:16.920 --> 37:22.120
in the work you've done. Have you observed the need ambiguity in the different forms?

37:22.120 --> 37:23.960
Sir, can you repeat? I'm not hearing very well.

37:25.320 --> 37:31.800
Sorry. So I was saying that about 10 years ago, I participated in a Kangol contest, where the topic

37:31.800 --> 37:39.000
was finding the different papers written by the same author, because the names had different

37:39.000 --> 37:47.640
variations and formats and so on. In your work, do you also observe that the names of the authors

37:48.200 --> 37:53.720
are written in different ways, and that makes it harder for you to link papers together?

37:54.520 --> 38:00.520
All right. The question is regarding a contest that was run 10 years ago, whether there are authors

38:00.520 --> 38:07.880
that have their names appearing different forms? Absolutely. First names often get abbreviated

38:07.880 --> 38:14.280
to the first letter. Middle names appear and disappear in random order. So it's indeed a problem,

38:14.280 --> 38:22.440
or kid helps. But also efforts such as what you did to develop ways in which to establish

38:22.440 --> 38:28.200
uniquely an author with all their works are helpful and they can be integrated as processes,

38:28.200 --> 38:32.840
either with a pull request on A3K or by using the API and doing it on your own.

38:37.960 --> 38:50.600
Okay. So it's a two-parter. First, how often do you update the dataset? And is there a way to

38:50.600 --> 38:57.880
download the delta? Just get the new stuff. Okay. Two things. How often I update the dataset? The

38:57.880 --> 39:04.920
answer is never in contrast to say to open Alex. A3K is a tool for working with existing data.

39:04.920 --> 39:10.680
So whenever you want, you go and fetch the data. It doesn't come with the data. You use the data

39:10.680 --> 39:18.280
sources from their primary source. I don't pretend to curate the data. A3K allows you to use existing

39:18.280 --> 39:25.640
open data sources. Can you work with incremental updates if you have a way to running a select,

39:26.600 --> 39:30.520
if you have incremental parts, you can populate the data also with the increments,

39:30.520 --> 39:34.120
if you have a database that you populate incrementally.

39:45.400 --> 39:55.880
Thanks. Thanks. So I was in a university 10 years ago and more than 10 years ago and by that time

39:56.040 --> 40:05.080
our articles were published mostly as unstructured text. So is that still a thing? Are there any,

40:06.120 --> 40:09.960
are you aware of any efforts to make articles,

40:14.760 --> 40:18.600
because they're unstructured tests and those are difficult to analyze?

40:19.160 --> 40:26.840
If I understand the question correctly, whether there are efforts to structure the articles in a

40:26.840 --> 40:33.240
way that we can better analyze it, there are tools such as a Grubbit, we heard it yesterday at

40:35.000 --> 40:42.680
another dev room on Open Science that do that and create XML associated with an article's text.

40:43.640 --> 40:50.520
Of course this cannot always be perfect. Ideally we'd want the complete pipeline from authors

40:50.520 --> 40:55.800
to publishers to publication to take this structured account and not reverse engineer

40:56.360 --> 40:59.080
the structure after it has been published.

41:03.640 --> 41:11.240
Well, when I look at the time, we should stop here for Q&A here in this room. Maybe he has some more

41:11.320 --> 41:14.600
time. So maybe some applause.

41:19.640 --> 41:25.800
And first anyone wants to say thank you. Thank you very much.

