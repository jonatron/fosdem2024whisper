WEBVTT

00:30.000 --> 00:31.000
Hello?

00:31.000 --> 00:32.000
Yes.

00:32.000 --> 00:37.000
Ah, okay, let's do it again.

00:37.000 --> 00:40.000
Hello, hello friends.

00:40.000 --> 00:41.000
I'm Roman.

00:41.000 --> 00:50.000
So let's start with an overview.

00:50.000 --> 00:53.000
I'm scratchy too, right?

00:53.000 --> 00:54.000
Okay.

00:54.000 --> 01:00.000
A little overview and motivation why we are doing all this.

01:00.000 --> 01:02.000
So what is Project Lili put?

01:02.000 --> 01:04.000
Let's start at the beginning.

01:04.000 --> 01:07.000
It's an open JDK project.

01:07.000 --> 01:09.000
It's a community project.

01:09.000 --> 01:13.000
We get contributions by Red Hat, Oracle, SAP, Huawei, Alibaba,

01:13.000 --> 01:15.000
Amazon, that's me.

01:15.000 --> 01:20.000
The goal of that project is to reduce memory footprint of Java

01:20.000 --> 01:22.000
object headers.

01:22.000 --> 01:26.000
We have a lot of applications in the Java heap.

01:26.000 --> 01:29.000
It's a side effect of that goal.

01:29.000 --> 01:34.000
We will also see potential CPU and latency improvements.

01:34.000 --> 01:40.000
Specifically, we want to achieve these heap memory reductions by

01:40.000 --> 01:44.000
reducing the size of Java object headers.

01:44.000 --> 01:49.000
To illustrate this, let's consider a hypothetical very

01:49.000 --> 01:54.000
long button, I guess.

01:54.000 --> 01:57.000
So let's look at this hypothetical Java heap.

01:57.000 --> 02:02.000
Each of these squares represents a Java object with different

02:02.000 --> 02:03.000
sizes.

02:03.000 --> 02:10.000
And in such a heap, you would have like everything that is

02:10.000 --> 02:12.000
yellow here is object headers.

02:12.000 --> 02:17.000
This is really just metadata for the VM, like class information

02:17.000 --> 02:19.000
stuff, I will get into this.

02:19.000 --> 02:23.000
And this means we waste quite a lot of memory on just this

02:23.000 --> 02:26.000
metadata in the heap.

02:26.000 --> 02:30.000
With Project Lili put, the goal is, so if we consider each of

02:30.000 --> 02:36.000
these half of those squares, one word, we want to reduce these

02:36.000 --> 02:41.000
cut in half, might end up looking like this, with much less

02:41.000 --> 02:44.000
metadata for each object, and we free up a lot of space there

02:44.000 --> 02:45.000
at the bottom.

02:45.000 --> 02:47.000
So this is the savings.

02:47.000 --> 02:53.000
Another way to look at this is to see the breakdown of

02:53.000 --> 02:57.000
metadata versus actual payload.

02:57.000 --> 03:02.000
And I did some statistics when I started this project, so around

03:02.000 --> 03:06.000
20% of live data on the heap is actually only metadata in the

03:06.000 --> 03:08.000
object headers.

03:08.000 --> 03:10.000
This depends a lot on the workloads.

03:10.000 --> 03:13.000
Some workloads, they have much more of this.

03:13.000 --> 03:16.000
Some workloads, they have much less ratio here, but this is

03:16.000 --> 03:18.000
kind of an average.

03:18.000 --> 03:22.000
So this is the current situation where we have headers that are

03:22.000 --> 03:24.000
12 bytes large.

03:24.000 --> 03:29.000
With Project Lili put, in the first step, we want to go to

03:29.000 --> 03:36.000
8 byte headers, which means that the ratio of metadata here is

03:36.000 --> 03:40.000
only like 13% on average.

03:40.000 --> 03:45.000
My very again, average savings of around 7%, 10% up to maybe

03:45.000 --> 03:48.000
30% or even a little bit more percent here.

03:48.000 --> 03:52.000
The long term goal with Lili put is to have headers that are

03:52.000 --> 03:57.000
only 4 bytes large, and if you achieve this, then only 6% of

03:57.000 --> 04:03.000
live data is object headers and savings up to like 50% compared

04:03.000 --> 04:05.000
to the current situation.

04:06.000 --> 04:10.000
Yeah.

04:10.000 --> 04:12.000
We did that already.

04:12.000 --> 04:15.000
We did most of the work already and we run some services at

04:15.000 --> 04:20.000
Amazon already with Lili put, and this is the heap usage of the

04:20.000 --> 04:23.000
service after GC.

04:23.000 --> 04:27.000
When Lili put was deployed, it dropped by about 30%.

04:27.000 --> 04:32.000
I must admit that this is a bit of a sweet spot case.

04:33.000 --> 04:38.000
Not all workloads look like this, but it shows quite well how

04:38.000 --> 04:39.000
this works.

04:39.000 --> 04:44.000
As a side effect of this, the CPU utilization also dropped by

04:44.000 --> 04:48.000
about 25% here when Lili put got deployed.

04:48.000 --> 04:50.000
Is this still on?

04:50.000 --> 04:52.000
Okay.

04:52.000 --> 04:54.000
This was quite significant.

04:54.000 --> 05:01.000
This has the effect on latency too, latency drops by

05:01.000 --> 05:03.000
about 30%.

05:03.000 --> 05:05.000
Some percent.

05:05.000 --> 05:07.000
So, motivation part.

05:07.000 --> 05:12.000
Helpfully, Alexi did the GLL thing.

05:12.000 --> 05:17.000
So, if you have a workload and want to know how would it look

05:17.000 --> 05:22.000
like when running Lili put, you can generate a heap dump with

05:22.000 --> 05:26.000
your workload and then run it through GLL tool, which is here,

05:26.000 --> 05:31.000
and you can get some nice estimates of your heap utilization

05:31.000 --> 05:35.000
with different configurations, including Lili put.

05:35.000 --> 05:38.000
To tell you, you would say, I don't know, such and such

05:38.000 --> 05:41.000
percent with Lili put, and you can even do the crystal ball and

05:41.000 --> 05:45.000
look into the future and tell you about Lili put too.

05:45.000 --> 05:48.000
That's because the heap dumps don't actually include the

05:48.000 --> 05:49.000
metadata.

05:49.000 --> 05:54.000
The GLL tool needs to do a lot of calculations and estimations

05:54.000 --> 05:58.000
and get up and come up with something, but it's pretty useful.

05:58.000 --> 06:01.000
So, yeah.

06:01.000 --> 06:03.000
At the end of the day, what does it do?

06:03.000 --> 06:05.000
It reduces your hardware or cloud cost.

06:05.000 --> 06:09.000
You can save on instance cost here.

06:09.000 --> 06:14.000
Or you can do the same hardware and drive more load on it.

06:14.000 --> 06:17.000
You can help reduce your energy bills and save your tool for

06:17.000 --> 06:21.000
the climate, but this is all good.

06:22.000 --> 06:24.000
So, yeah.

06:24.000 --> 06:27.000
Let's look into what's going on there.

06:27.000 --> 06:30.000
The object headers.

06:30.000 --> 06:33.000
This is a breakdown of the current situation.

06:33.000 --> 06:37.000
So, we have a one word at the beginning of each object, which we

06:37.000 --> 06:40.000
call the mark words for historical reasons.

06:40.000 --> 06:42.000
It's really just a header that contains stuff.

06:42.000 --> 06:46.000
And we have 64 bits and two bits here.

06:46.000 --> 06:48.000
They are tag bits or lock bits.

06:48.000 --> 06:52.000
They indicate what the rest of those bits mean.

06:52.000 --> 07:00.000
Usually, they mean we have GCH bits here, four bits of GCH 0215.

07:00.000 --> 07:03.000
And then we have hash code bits.

07:03.000 --> 07:06.000
When you call the identity hash code of an object, this will

07:06.000 --> 07:09.000
generate some number and stick it here.

07:09.000 --> 07:12.000
And then you have some unused bits up here.

07:12.000 --> 07:13.000
Okay.

07:13.000 --> 07:17.000
And the next word, the second word of each object contains the

07:17.000 --> 07:18.000
class pointer.

07:18.000 --> 07:21.000
The class, so-called class pointer, points to the internal

07:21.000 --> 07:24.000
structures in hotspot that tells you all about the class of

07:24.000 --> 07:26.000
this object.

07:26.000 --> 07:29.000
Usually, what you get with modern JVMs is the compressed

07:29.000 --> 07:31.000
class pointer, which is 32 bits.

07:31.000 --> 07:33.000
I don't want to talk about the other one.

07:33.000 --> 07:39.000
Thomas is going to talk about a lot of details, how to deal

07:39.000 --> 07:41.000
with that.

07:41.000 --> 07:44.000
But this means that the first two words of each object are

07:44.000 --> 07:46.000
taken by metadata.

07:46.000 --> 07:50.000
For many objects, we can start sticking actual payload in these

07:50.000 --> 07:52.000
other 32 bits here.

07:52.000 --> 07:55.000
For arrays, we can put the area length there.

07:55.000 --> 07:59.000
But what we want to do is you can probably see this already.

07:59.000 --> 08:02.000
We have the class pointer here.

08:02.000 --> 08:03.000
We have free space here.

08:03.000 --> 08:07.000
We can probably stick it in there and then see how to deal

08:07.000 --> 08:09.000
with the other bits there.

08:10.000 --> 08:15.000
Problem is, what we also have is the so-called displaced

08:15.000 --> 08:17.000
headers or overloaded headers.

08:17.000 --> 08:22.000
This happens when locking happens or the GC does some

08:22.000 --> 08:23.000
things.

08:23.000 --> 08:27.000
And then the tag material indicates what the rest of those

08:27.000 --> 08:28.000
bits mean.

08:28.000 --> 08:30.000
And usually, it's a native pointer to some structure.

08:30.000 --> 08:34.000
And this means that those bits up here, they are not

08:34.000 --> 08:35.000
actually always unused.

08:35.000 --> 08:37.000
They are only sometimes unused.

08:37.000 --> 08:39.000
And this is a major problem.

08:39.000 --> 08:42.000
I get into how we want to deal with this.

08:42.000 --> 08:45.000
We still have to compress class pointers there.

08:45.000 --> 08:49.000
As an example, when this happens is when we have a

08:49.000 --> 08:52.000
stack-locked object, then the tag width is 0, 0.

08:52.000 --> 08:54.000
The rest of it is appointed onto the stack.

08:54.000 --> 08:57.000
Then we have objects that are locked by an object monitor.

08:57.000 --> 08:58.000
Then we have those tag widths.

08:58.000 --> 09:02.000
And this is appointed to some object monitor structure that

09:02.000 --> 09:03.000
lives somewhere.

09:03.000 --> 09:07.000
Or the GC can use this for storing forwarding

09:07.000 --> 09:08.000
information.

09:08.000 --> 09:11.000
And it points to then we have this tag with here, 0, 0, 1,

09:11.000 --> 09:12.000
1.

09:12.000 --> 09:14.000
And it points to the forwarded object in somewhere else in

09:14.000 --> 09:15.000
the heap.

09:18.000 --> 09:20.000
This may look like this.

09:20.000 --> 09:22.000
In the example of an object monitor, we have exactly the

09:22.000 --> 09:24.000
situation that I just talked about.

09:24.000 --> 09:26.000
If that happens, where's the original mark?

09:26.000 --> 09:28.000
We'll say we have some hash code here.

09:28.000 --> 09:31.000
And so we need to preserve it somewhere.

09:32.000 --> 09:35.000
Its answer is usually it's in the, for example, in the

09:35.000 --> 09:37.000
object monitor case.

09:37.000 --> 09:42.000
It's displaced in the beginning of the object monitor

09:42.000 --> 09:43.000
structure.

09:45.000 --> 09:46.000
This is fiddly stuff.

09:46.000 --> 09:50.000
So the plan is to get here for the first step that we are

09:50.000 --> 09:52.000
currently working on.

09:52.000 --> 09:54.000
So we would still have to tag width here.

09:54.000 --> 09:56.000
Or maybe not.

09:56.000 --> 10:00.000
Then we have another bit here that says about self-focusing.

10:00.000 --> 10:03.000
Then we have H widths here, some unused stuff.

10:03.000 --> 10:08.000
And the identity hash code and a few bits for the class

10:08.000 --> 10:09.000
pointer.

10:09.000 --> 10:12.000
If you count this correctly, those are not 32 bits like we

10:12.000 --> 10:13.000
had before.

10:13.000 --> 10:14.000
Those are fewer bits.

10:14.000 --> 10:17.000
Thomas is going to talk about how we do this.

10:17.000 --> 10:21.000
The long-term plan for lilyput, we call this lilyput 2

10:21.000 --> 10:24.000
already, is squeeze everything into 32 bits.

10:24.000 --> 10:26.000
We still have those tag widths here.

10:26.000 --> 10:28.000
It says forwarding point that you see H.

10:29.000 --> 10:30.000
It's unchanged.

10:30.000 --> 10:32.000
The hash code will fit into two bits.

10:34.000 --> 10:36.000
This is going to be a very bad hash code.

10:36.000 --> 10:37.000
No, no, no.

10:37.000 --> 10:38.000
We will leave it.

10:38.000 --> 10:41.000
And you still have the class pointer up there.

10:42.000 --> 10:44.000
What's the problem with all this?

10:44.000 --> 10:47.000
So in the current situation, we have two words.

10:47.000 --> 10:50.000
In the first word, it rarely carries any interesting

10:50.000 --> 10:51.000
information.

10:51.000 --> 10:54.000
So it might carry hash code information, but it's only,

10:54.000 --> 10:57.000
usually happens for few objects.

10:57.000 --> 11:01.000
It may carry locking information, but only very few

11:01.000 --> 11:03.000
objects ever get locked on the heap.

11:03.000 --> 11:08.000
So we basically waste this word on stuff that is rarely used.

11:08.000 --> 11:11.000
And in the second word, we have this class pointer, which is

11:11.000 --> 11:14.000
very crucial information for any object, because we need this

11:14.000 --> 11:16.000
for all sorts of things.

11:16.000 --> 11:19.000
If we don't have this, then this object doesn't have

11:19.000 --> 11:22.000
a type identity, which is, yeah, we may not do this.

11:22.000 --> 11:25.000
So this is part of the problem.

11:25.000 --> 11:28.000
In the new world, this class pointer is part of the header,

11:28.000 --> 11:32.000
which means that suddenly this header has crucial information

11:32.000 --> 11:34.000
that we must not lose.

11:34.000 --> 11:38.000
And we always need for things like figuring out the object size

11:38.000 --> 11:39.000
or something like this.

11:39.000 --> 11:42.000
So we must never lose that class pointer there.

11:42.000 --> 11:45.000
And also, this header displacement that I talked about

11:45.000 --> 11:51.000
earlier means that how do we access this information when we

11:51.000 --> 11:54.000
need to follow through the actual mark word?

11:54.000 --> 11:56.000
Those are the three problems.

11:56.000 --> 11:59.000
How to fit everything into fewer bits, how to safely access

11:59.000 --> 12:03.000
this mark word, and how to avoid losing the class pointer

12:03.000 --> 12:05.000
to begin with.

12:05.000 --> 12:08.000
I don't have all that much time, so I can only scratch the

12:08.000 --> 12:10.000
surface of these problems.

12:10.000 --> 12:13.000
I will give a very high level overview of this.

12:13.000 --> 12:15.000
The first is locking.

12:15.000 --> 12:20.000
When locking happens, then, well, let's look at, for example,

12:20.000 --> 12:28.000
stack locking is the most lightweightest locking in hotspot.

12:28.000 --> 12:33.000
It's really just a compare and swap thing on the object header.

12:33.000 --> 12:36.000
Oops, this was too fast.

12:36.000 --> 12:38.000
Let's go back a little bit.

12:38.000 --> 12:42.000
So it coordinates threads by comparing and swapping on the

12:42.000 --> 12:43.000
object header.

12:43.000 --> 12:45.000
I said this.

12:45.000 --> 12:49.000
It doesn't, as soon as contention happens, we inflate this

12:49.000 --> 12:51.000
stack lock to an object monitor.

12:51.000 --> 12:53.000
It doesn't support weight notify.

12:53.000 --> 12:54.000
It doesn't support JNI.

12:54.000 --> 12:57.000
And any of this happens, we inflate this to what we call

12:57.000 --> 12:59.000
a full object monitor.

12:59.000 --> 13:03.000
This is why we have those two different locking modes.

13:03.000 --> 13:08.000
The way that works now is when that happens, the JVM does a

13:08.000 --> 13:12.000
compare and swap on the object header, and it basically

13:12.000 --> 13:16.000
exchanges the current header with a pointer to the stack, and

13:16.000 --> 13:20.000
it sticks the original mark word somewhere on the stack.

13:20.000 --> 13:23.000
And this is how we can basically find the original mark

13:23.000 --> 13:25.000
word and restore it when we unlock.

13:25.000 --> 13:28.000
Then we do the opposite thing and swap it around.

13:28.000 --> 13:33.000
It does answer the question, is this thread locked by object

13:33.000 --> 13:36.000
E, by this object?

13:36.000 --> 13:39.000
It cannot answer the question, which thread is currently

13:39.000 --> 13:45.000
locking this object because it's not really necessary.

13:45.000 --> 13:48.000
In little bit, the way we solved that is, and I have only one

13:48.000 --> 13:52.000
slide of this, which is quite amazing because I basically

13:52.000 --> 13:56.000
rewrote the locking implementation there for this.

13:56.000 --> 14:01.000
But what we do there is instead of putting the original

14:01.000 --> 14:04.000
mark word on the stack and putting a pointer on this

14:04.000 --> 14:07.000
location in the mark word, we basically turn this around

14:07.000 --> 14:10.000
and say, OK, let's have a small lock stack, what would we

14:10.000 --> 14:14.000
call it, lock stack, and we basically only push the object

14:14.000 --> 14:17.000
on that lock stack, and then we still do a compare and swap

14:17.000 --> 14:20.000
to flip a single bit here, but nothing else.

14:20.000 --> 14:23.000
The rest of the mark word stays intact.

14:23.000 --> 14:26.000
It still answers the same question, is this thread T

14:26.000 --> 14:30.000
locking this object and nothing else?

14:30.000 --> 14:33.000
So this is the very scratch-to-service overview

14:33.000 --> 14:35.000
of this stack locking.

14:35.000 --> 14:38.000
I cannot go deeper because time is running out.

14:38.000 --> 14:41.000
I need to hand over to you at this point.

14:42.000 --> 14:44.000
Yeah, let's look at the monitor locking.

14:44.000 --> 14:46.000
It's a very similar situation.

14:46.000 --> 14:49.000
All engineers are currently working on a solution for that.

14:49.000 --> 14:53.000
The basic idea is to, instead of doing the mapping from

14:53.000 --> 14:56.000
an object to the object monitor through the header,

14:56.000 --> 14:59.000
we have a side table and do the mapping there, and then

14:59.000 --> 15:02.000
this means that we don't have this header displacement

15:02.000 --> 15:04.000
anymore.

15:04.000 --> 15:10.000
Also, only scratching the surface on GC forwarding.

15:11.000 --> 15:14.000
Some GCs need to store forwarding information

15:14.000 --> 15:16.000
in the object header.

15:16.000 --> 15:21.000
This is used by those full GCs here, and zero GC,

15:21.000 --> 15:25.000
G1 GC, and Shenandoah, they use the header

15:25.000 --> 15:29.000
to store forwarding information, and this means that

15:29.000 --> 15:32.000
when that happens, we would lose this class pointer

15:32.000 --> 15:35.000
and this crucial information because this is why

15:35.000 --> 15:37.000
this is a big problem here.

15:37.000 --> 15:40.000
For normal operation, it is not such a big deal

15:40.000 --> 15:43.000
because there we have copying GCs where we first create

15:43.000 --> 15:46.000
a copy of the object and then stick the forwarding pointer

15:46.000 --> 15:49.000
only in the old copy, which means that in the new copy

15:49.000 --> 15:52.000
we still have all this information, and we can follow

15:52.000 --> 15:55.000
through to this new copy to get to the class pointer.

15:55.000 --> 15:58.000
The full GC, because it has no space to copy the object,

15:58.000 --> 16:01.000
it slides the objects down to the bottom, and in the process

16:01.000 --> 16:04.000
of this, would lose this class information.

16:05.000 --> 16:08.000
The parallel GC has a different solution here

16:08.000 --> 16:12.000
based on the idea, let's not store this forwarding

16:12.000 --> 16:15.000
information in the header, let's always re-computers

16:15.000 --> 16:19.000
in some clever ways, and this is basically the plan

16:19.000 --> 16:22.000
for what we want to do for the other GCs too.

16:22.000 --> 16:24.000
So that GC doesn't have this problem because

16:24.000 --> 16:26.000
they don't do full GCs.

16:27.000 --> 16:31.000
And with that, well, we have a JAP-450,

16:32.000 --> 16:35.000
we have this new lightweight locking, it's already

16:35.000 --> 16:39.000
in Jadikate 21, the flag to enable this,

16:39.000 --> 16:43.000
when you grab a build from Alexi or some other place,

16:43.000 --> 16:48.000
is this flag, and I think with this, I'm handing over

16:48.000 --> 16:52.000
to Thomas, he wants to talk about the class pointers.

16:52.000 --> 16:53.000
Oh, you mentioned it.

16:53.000 --> 16:54.000
Oh, okay.

16:54.000 --> 16:55.000
Does this work?

16:55.000 --> 16:56.000
Oh, it works, wonderful.

16:56.000 --> 16:58.000
Yeah, okay, let me switch it.

16:59.000 --> 17:01.000
Class pointers, and...

17:02.000 --> 17:03.000
Wait.

17:05.000 --> 17:06.000
I'm gonna start.

17:06.000 --> 17:09.000
Okay, I'm Thomas, I work at Red Hat,

17:09.000 --> 17:13.000
and I'm going to talk about what we do for class pointers

17:13.000 --> 17:16.000
in Lilliput, and this will be a very quick dive

17:16.000 --> 17:18.000
because I only have 10 minutes,

17:18.000 --> 17:20.000
and this is a bit of a challenge.

17:21.000 --> 17:23.000
This is just one, but one of...

17:23.000 --> 17:25.000
Oh, this is cool.

17:25.000 --> 17:29.000
This is just one of many moving parts in the Lilliput project.

17:29.000 --> 17:32.000
Lilliput is really a real community project

17:32.000 --> 17:34.000
in the sense that many companies contribute,

17:34.000 --> 17:38.000
and this is the part we decided to tackle at Red Hat.

17:39.000 --> 17:43.000
And with that, let's start, time's ticking.

17:43.000 --> 17:45.000
So class pointer, the class pointer currently

17:45.000 --> 17:48.000
in the Lilliput project is 32 bits,

17:48.000 --> 17:50.000
and that's way too much, we need it to be smaller.

17:50.000 --> 17:53.000
It takes up like half of the whole space

17:53.000 --> 17:56.000
of the 64-bit Lilliput object header.

17:56.000 --> 18:00.000
And so, okay, that's pretty self-explanatory.

18:00.000 --> 18:02.000
So some background first.

18:02.000 --> 18:05.000
When we load up a class, a Java class,

18:05.000 --> 18:09.000
we build up a whole range of companion data structures

18:09.000 --> 18:14.000
in native memory, and the centerpiece of this group,

18:14.000 --> 18:16.000
and kind of the big boy among them,

18:16.000 --> 18:19.000
is the class structure written with K.

18:19.000 --> 18:23.000
It's a variable-sized structure ranging from 400 bytes

18:23.000 --> 18:26.000
to, I don't know, we saw five megabyte monsters at Amazon,

18:26.000 --> 18:28.000
but that's rare.

18:28.000 --> 18:33.000
And every object's header refers to the class structure of its class.

18:33.000 --> 18:36.000
That's why the shape of this reference really matters,

18:36.000 --> 18:38.000
because for one, obviously, footprint,

18:38.000 --> 18:42.000
and because also we need to be able to dereference it very quickly.

18:42.000 --> 18:45.000
Going from object to class is a hot path.

18:45.000 --> 18:48.000
So we could just store the native class pointer

18:48.000 --> 18:50.000
in the object header, but we usually don't do this,

18:50.000 --> 18:52.000
because it's 64-bit way too big.

18:52.000 --> 18:54.000
So since a long time ago,

18:54.000 --> 18:56.000
we already employ this optimization

18:56.000 --> 19:00.000
where we split the native pointer into two parts,

19:00.000 --> 19:04.000
a 32-bit offset that refers to a 64-bit runtime constant base,

19:04.000 --> 19:05.000
and we only store the offset,

19:05.000 --> 19:09.000
and that's what we call the narrow class pointer.

19:09.000 --> 19:13.000
And that trick only works if we are able to confine

19:13.000 --> 19:17.000
all class structures within a four-gigabyte range, obviously.

19:17.000 --> 19:19.000
And that's exactly what the class base is for.

19:19.000 --> 19:21.000
That's the only reason it exists.

19:21.000 --> 19:24.000
And we also have CDS.

19:24.000 --> 19:27.000
So CDS contains class data sharing.

19:27.000 --> 19:30.000
CDS archives contain pre-baked class structures.

19:30.000 --> 19:34.000
And so what we do usually is we map the CDS archive

19:34.000 --> 19:35.000
very close to the class base,

19:35.000 --> 19:39.000
such that both are engulfed by the class encoding range,

19:39.000 --> 19:41.000
and every class structure in either region

19:41.000 --> 19:44.000
can be reached by a narrow class pointer.

19:45.000 --> 19:48.000
Okay, and so decoding is basically just an addition.

19:48.000 --> 19:51.000
I'm a bit simplifying because of time reasons.

19:51.000 --> 19:54.000
And the nice thing is from the point of the JIT,

19:54.000 --> 19:58.000
the encoding base is a runtime constant.

19:58.000 --> 20:00.000
It's obviously not a constant

20:00.000 --> 20:02.000
because we determine it when the VM start-up.

20:02.000 --> 20:04.000
It's subject to ASLR and some such,

20:04.000 --> 20:07.000
but we can encode it,

20:07.000 --> 20:09.000
the JIT can encode it as a 64-bit immediate.

20:09.000 --> 20:13.000
That is nice, so we already save a load.

20:13.000 --> 20:15.000
And then we have a ton of optimizations

20:15.000 --> 20:19.000
that basically all depend on the base looking good.

20:19.000 --> 20:23.000
Whatever looking good means for the specific CPU,

20:23.000 --> 20:26.000
such that we can load it with just one move.

20:26.000 --> 20:28.000
I won't get into details.

20:28.000 --> 20:31.000
One simple example is if we manage to place

20:31.000 --> 20:33.000
the class base below four gigabytes,

20:33.000 --> 20:35.000
then we can set the base to zero.

20:35.000 --> 20:37.000
This is what we call unskilled encoding.

20:37.000 --> 20:39.000
And now basically every narrow class pointer

20:39.000 --> 20:42.000
is the class pointer, so we don't have to do anything.

20:42.000 --> 20:45.000
And we are now also very good at it.

20:45.000 --> 20:47.000
The one effect of the Lilliput work is that

20:47.000 --> 20:50.000
unless the address base is really populated

20:50.000 --> 20:53.000
or unless the operating system just flatly refuses to do this,

20:53.000 --> 20:56.000
this is likely to happen.

20:56.000 --> 20:59.000
Okay, for Lilliput, 32-bit is still too much,

20:59.000 --> 21:01.000
so we shrink it.

21:01.000 --> 21:02.000
There are some side goals.

21:02.000 --> 21:04.000
We still want to be able to address enough classes,

21:04.000 --> 21:07.000
whatever enough means, that is a complicated question.

21:07.000 --> 21:11.000
And we also want to basically keep using

21:12.000 --> 21:15.000
metaspaces and CDS, because both give us a ton of features

21:15.000 --> 21:18.000
we would need to reinvent otherwise.

21:18.000 --> 21:20.000
And we also decided to, for now,

21:20.000 --> 21:23.000
keep the class base layout as it is.

21:23.000 --> 21:25.000
Now, as a kind of a bar,

21:25.000 --> 21:28.000
we can load about five million classes, give or take.

21:28.000 --> 21:31.000
The class base is artificially kept at three gig.

21:31.000 --> 21:33.000
And I personally believe if you manage to load

21:33.000 --> 21:35.000
five million classes, you are either very patient

21:35.000 --> 21:37.000
or probably not really aware of doing it,

21:37.000 --> 21:39.000
because it's a leak.

21:39.000 --> 21:42.000
And I think this number is really high.

21:42.000 --> 21:44.000
Question still means is how many classes

21:44.000 --> 21:46.000
do we actually need to address.

21:46.000 --> 21:48.000
And this is a very complicated question,

21:48.000 --> 21:50.000
and I don't have much time.

21:50.000 --> 21:55.000
Therefore, we kind of decided to sidestep this question.

21:55.000 --> 21:58.000
We said, okay, we don't reduce the class encoding range,

21:58.000 --> 22:00.000
it's still four gigabyte.

22:00.000 --> 22:02.000
We leave it at that.

22:02.000 --> 22:05.000
And we say anything in multi-million range is probably fine.

22:05.000 --> 22:07.000
We can also, there's still room to reduce it,

22:07.000 --> 22:09.000
but we don't for now.

22:09.000 --> 22:12.000
What we do instead is we increase,

22:12.000 --> 22:15.000
if we manage to store all class structures

22:15.000 --> 22:17.000
at certain aligned addresses,

22:17.000 --> 22:19.000
then we can use this alignment shadow

22:19.000 --> 22:21.000
to save other stuff.

22:21.000 --> 22:23.000
And that's what we do.

22:23.000 --> 22:25.000
We decided on a 10-bit alignment

22:25.000 --> 22:28.000
because one kilobyte statistics.

22:28.000 --> 22:30.000
Even though class is very precise,

22:30.000 --> 22:33.000
it's the vast majority of class structures

22:33.000 --> 22:35.000
is below one kilobyte,

22:35.000 --> 22:37.000
usually larger than 512 byte.

22:37.000 --> 22:39.000
It's a bell distribution,

22:39.000 --> 22:41.000
so it's really much larger,

22:41.000 --> 22:43.000
outliers exist, but are really rare.

22:43.000 --> 22:47.000
And 10-bit give us 22-bit class pointers for now,

22:47.000 --> 22:50.000
and let us address three million classes.

22:50.000 --> 22:53.000
And that's still way enough, I think.

22:53.000 --> 22:55.000
So now something interesting happens,

22:55.000 --> 22:58.000
where before we used to store class structures

22:58.000 --> 23:00.000
in the class space back to back,

23:00.000 --> 23:03.000
we now store them at aligned boundaries.

23:03.000 --> 23:06.000
And this means that the narrow class pointer

23:06.000 --> 23:08.000
now is more like a class ID,

23:08.000 --> 23:11.000
because the class label morphs into a table

23:11.000 --> 23:13.000
of one kilobyte-sized slots,

23:13.000 --> 23:15.000
where every class structure occupies one

23:15.000 --> 23:17.000
or very rarely multiple slots.

23:17.000 --> 23:19.000
And we use the value range of narrow class

23:19.000 --> 23:21.000
much more efficiently, because every value now

23:21.000 --> 23:23.000
means basically another class.

23:23.000 --> 23:25.000
So it's a hypothetical 16-bit class pointer

23:25.000 --> 23:27.000
can address 65,000 classes.

23:27.000 --> 23:29.000
And that's good.

23:29.000 --> 23:31.000
Obviously, that hurts.

23:31.000 --> 23:33.000
We have alignment raised now.

23:33.000 --> 23:37.000
And for time reasons, I can't go into this.

23:37.000 --> 23:40.000
The gist of this slide is that we made metaspace

23:40.000 --> 23:43.000
very good at allocating at aligned boundaries

23:43.000 --> 23:46.000
without footprint loss, so we don't pay for that.

23:46.000 --> 23:49.000
And we still retain allocation performance

23:49.000 --> 23:50.000
as what we pour.

23:50.000 --> 23:53.000
This actually did cost quite a bit of work.

23:53.000 --> 23:56.000
Some supporting statistic, I skipped that.

23:56.000 --> 23:58.000
This is Mark Berthley out now.

23:58.000 --> 24:00.000
But before we had a 30-bit class pointer,

24:00.000 --> 24:02.000
we now reduced it to 22-bit

24:02.000 --> 24:05.000
that allowed us to inflate the hash

24:05.000 --> 24:07.000
back to its former 31-bit size.

24:07.000 --> 24:10.000
The story behind this is when we started with Lilliput,

24:10.000 --> 24:13.000
we had to reduce iHash to 25-bits,

24:13.000 --> 24:15.000
which of course has negative consequences

24:15.000 --> 24:17.000
for applications with large datasets,

24:17.000 --> 24:19.000
hash collisions, and so on.

24:19.000 --> 24:22.000
And the nice thing is we now have four free-bits.

24:22.000 --> 24:25.000
I'm sure we find a use for them.

24:25.000 --> 24:29.000
And there are some kinks still to be ironed out.

24:30.000 --> 24:33.000
Very quickly, hyperaligning class structures,

24:33.000 --> 24:36.000
any structure to sizes larger than cache line size

24:36.000 --> 24:39.000
may have detrimental effects to cache efficiency.

24:39.000 --> 24:43.000
I will need to look into this.

24:43.000 --> 24:46.000
We have mitigations planned if this should happen.

24:46.000 --> 24:47.000
There's also 30-bit.

24:47.000 --> 24:50.000
We are now in the weird situation that 30-bit class pointers

24:50.000 --> 24:53.000
on 30-bit platforms are larger than a 64-bit.

24:53.000 --> 24:56.000
And this is, we can deal with this.

24:57.000 --> 25:00.000
I kind of hope that 30-bit is going away.

25:00.000 --> 25:03.000
Before I have to do this.

25:03.000 --> 25:07.000
Future, 16-bit is possible if we just naively reduce

25:07.000 --> 25:10.000
narrow class pointer to 16-bit.

25:10.000 --> 25:12.000
This would be a severe reduction in the number

25:12.000 --> 25:15.000
of loadable classes and probably not acceptable.

25:15.000 --> 25:18.000
What we can do, however, is we can switch to a model

25:18.000 --> 25:21.000
where we have maybe a variable size tether

25:21.000 --> 25:24.000
where objects of the first 65,000 classes

25:25.000 --> 25:27.000
would benefit from lilyput and have lilyput header

25:27.000 --> 25:30.000
objects beyond that would get the narrow class pointer

25:30.000 --> 25:32.000
appended to the mark word.

25:32.000 --> 25:34.000
This, of course, is a lot more complex

25:34.000 --> 25:36.000
than what we do nowadays.

25:36.000 --> 25:39.000
But this is possible, maybe we have to do this.

25:39.000 --> 25:41.000
Should we ever do 30-bit lilyput headers

25:41.000 --> 25:43.000
as we have planned?

25:43.000 --> 25:45.000
This is basically not my idea.

25:45.000 --> 25:47.000
This is John Rose's idea.

25:47.000 --> 25:50.000
Okay, as a summary, where we are now,

25:50.000 --> 25:53.000
10-bits we freed, restored IH to 31-bit,

25:53.000 --> 25:55.000
which is nice, that had been a blemish

25:55.000 --> 25:58.000
in the current lilyput implementation.

25:58.000 --> 26:01.000
And at the cost of a reduction of the number

26:01.000 --> 26:04.000
of addressable classes from five to three million,

26:04.000 --> 26:06.000
still a completely fantastic number.

26:06.000 --> 26:08.000
And the decoding is now more complex

26:08.000 --> 26:11.000
because now in addition to the addition,

26:11.000 --> 26:13.000
we also need to shift.

26:15.000 --> 26:17.000
There are some side effects trickling down.

26:17.000 --> 26:19.000
How are we time-wise?

26:19.000 --> 26:22.000
Oh, okay.

26:22.000 --> 26:24.000
Some side effects trickling down.

26:24.000 --> 26:27.000
We improved class pointer setup for the stock JVM.

26:27.000 --> 26:29.000
These are improvements that are already

26:29.000 --> 26:31.000
rolling out with JK22.

26:31.000 --> 26:33.000
And yeah, that basically was it.

26:33.000 --> 26:35.000
Thank you very much.

26:35.000 --> 26:37.000
And...

26:37.000 --> 26:39.000
Thank you.

26:39.000 --> 26:41.000
We take questions.

26:41.000 --> 26:43.000
Hello, Ren.

26:52.000 --> 26:55.000
Thank you.

26:55.000 --> 26:57.000
Thank you.

26:57.000 --> 26:59.000
Very nice job.

26:59.000 --> 27:01.000
Just a question.

27:01.000 --> 27:03.000
Is it worth to use some compression

27:03.000 --> 27:05.000
maybe to store these maps?

27:05.000 --> 27:07.000
Oh, sorry.

27:07.000 --> 27:10.000
Is there any gain possible to use some compression

27:10.000 --> 27:13.000
about these external maps or things like that?

27:13.000 --> 27:15.000
Maybe objects are not moving too much,

27:15.000 --> 27:18.000
or maybe you can compress these address or hash maps?

27:18.000 --> 27:21.000
I don't know if it's worth because it...

27:21.000 --> 27:25.000
analyze the pointers that may be big, then you have large,

27:25.000 --> 27:29.500
let's say, remember set of pointers somewhere or flags

27:29.500 --> 27:31.880
and maybe you have some compression here,

27:31.880 --> 27:35.080
may save and, but of course it's a trade off

27:35.080 --> 27:37.440
of performance between memories, so,

27:37.440 --> 27:39.280
depend on the use case.

27:39.280 --> 27:40.120
Thank you.

27:41.360 --> 27:45.020
So the question was, do we have any plans to like,

27:45.980 --> 27:49.220
optimize or address hash map implementations?

27:50.220 --> 27:52.740
You have to get more compact structure

27:52.740 --> 27:55.240
that we can delete from such hash maps.

27:55.240 --> 27:56.240
Ah, okay.

27:56.240 --> 27:59.100
Oh, this is not part of the scope of the report,

27:59.100 --> 28:00.820
but there may be other efforts.

28:01.820 --> 28:03.080
Yeah, but I don't know.

28:10.180 --> 28:11.020
Thank you.

28:13.540 --> 28:14.380
Well, you can go.

28:15.340 --> 28:16.620
Okay, thanks.

28:16.620 --> 28:18.140
I answer this quickly.

28:18.140 --> 28:20.100
The question was, when can we expect to see this

28:20.100 --> 28:22.700
in available release?

28:22.700 --> 28:24.860
Questions, I don't know yet.

28:24.860 --> 28:26.900
We have most of the stuff lined up already.

28:28.940 --> 28:33.940
I'm saying 24, but don't sue me on that.

28:37.260 --> 28:38.980
Can you just upstream this a bit tricky?

28:38.980 --> 28:39.820
Yeah.

28:39.820 --> 28:42.100
It's a whole separate effort.

28:42.100 --> 28:42.940
Yeah.

28:43.900 --> 28:46.860
I had a question whether it was possible,

28:46.860 --> 28:49.900
well, or was there any plan to actually make

28:49.900 --> 28:51.660
those tisings configurable?

28:51.660 --> 28:55.980
My thinking is that I would imagine maybe it's naive,

28:55.980 --> 28:58.380
but in some applications I worked on,

28:58.380 --> 28:59.980
classes are really, really simple

28:59.980 --> 29:01.220
and you don't use many of them.

29:01.220 --> 29:04.540
So you would benefit from having extremely small,

29:04.540 --> 29:07.340
you know, address to, like a small address space,

29:07.340 --> 29:09.340
basically, and smaller points or as even.

29:12.620 --> 29:14.700
I didn't quite, what's the question?

29:14.780 --> 29:18.620
The question is, is it possible to have configuration

29:18.620 --> 29:22.180
ergonomics for different sizes of class pointers

29:22.180 --> 29:24.180
or is it fixed by the JVM?

29:24.180 --> 29:28.260
Oh, possibly we debated that at some point in time.

29:28.260 --> 29:30.700
There are some advantages of keeping them constant

29:30.700 --> 29:32.820
because you get more efficient.

29:35.020 --> 29:37.700
But yeah, I'm not even sure we planned this

29:37.700 --> 29:39.180
as a development switch.

29:39.180 --> 29:40.700
Maybe it's undecided yet.

29:45.700 --> 29:46.540
Okay.

29:48.700 --> 29:49.700
No more questions?

29:49.700 --> 29:51.700
Okay then, thank you.

29:51.700 --> 29:52.700
Thank you.

