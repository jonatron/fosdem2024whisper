WEBVTT

00:00.000 --> 00:10.680
We want to stick to time. We've been doing very well. So we want to keep that up. The

00:10.680 --> 00:16.560
next talk is about, oh sorry, your name is on the slide. That doesn't help me. Tim, yeah,

00:16.560 --> 00:21.920
the next talk is about Tim, about extracting mini apps for HPC software.

00:21.920 --> 00:27.680
Then, thank you for having me back at Fostum. I am Tim. I'm by trade. I'm a compiler guy

00:27.720 --> 00:32.480
and I'm giving a talk to the HPC Dev Room and how that came to be. We'll be part of the talk

00:32.480 --> 00:37.440
extracting mini apps from HPC software for total cost of ownership, optimized system procurement.

00:37.440 --> 00:43.240
And I want to give a quick background about how this project came to be before I actually

00:43.240 --> 00:49.840
start to go into the technical details because this project is part of the NHR Association

00:49.840 --> 00:56.080
funded TCO, so total cost of ownership project. NHR stands for Nationales Hochleistungsregion.

00:56.080 --> 01:01.680
It's the German term for national high performance computing basically. And the NHR is an alliance

01:01.680 --> 01:07.600
of computing centers which all have different specialties, but all have a common admission

01:07.600 --> 01:12.720
process and have a so-called harmonized computing environment, which means basically that all the

01:12.720 --> 01:18.240
clusters at the different locations have very similar scheduling system, very similar file system.

01:18.240 --> 01:22.640
So if you manage to get your app work, your program working on one of the HPC systems,

01:22.640 --> 01:27.200
you probably will be able to get it working on the other systems as well. And this little blue

01:27.200 --> 01:35.760
dot is the is Darmstadt, which is where I'm from. And all those systems are procured at some point.

01:36.640 --> 01:41.120
And whenever you do hardware procurement, there are basically is basically the question, well,

01:41.120 --> 01:46.000
what hardware do you get? And the simple answer is you get the best of everything, right? So you get

01:46.000 --> 01:52.320
the most cores, the fastest RAM, the most efficient power delivering units, fastest most storage,

01:52.960 --> 01:59.600
but this is infeasible for all but the largest of HPC computing centers and even those usually

01:59.600 --> 02:05.600
struggle. So usually what you do is you go ahead and say, well, we want to get the best performance

02:05.600 --> 02:10.080
per dollar we spent. And for this, you need to figure out what's the performance is you get for

02:10.080 --> 02:16.080
the dollars. And so you usually use lin pack and stream, which are benchmarking. One is for basically

02:16.080 --> 02:20.480
stressing your floating point unit and turning your rack system into a space heater. And the other

02:20.480 --> 02:26.000
is memory. And then because you do not want to run only synthetic benchmarks, you use some of the

02:26.000 --> 02:32.640
spec HPC suite benchmark and run this and you figure out how much performance do you get basically.

02:33.360 --> 02:39.440
And recently there was this push to get more power efficient when you do HPC community. And so there

02:39.440 --> 02:44.800
the target to hit is performance per watt. And there again, you basically use lin pack. If you

02:44.880 --> 02:50.720
look at the green 500 list of HPC system, they all publish their lin pack score. But this is not

02:50.720 --> 02:56.320
actually representative of what the system will cost during its lifetime. It's usually just what's

02:56.320 --> 03:02.320
your one time investment cost in procuring the actual hardware. And what you really want to figure

03:02.320 --> 03:06.720
out, especially in the case of this distributed national high performance computing association,

03:07.920 --> 03:13.600
is the money we spend actually well invested for the use cases our users have. And this is where

03:13.600 --> 03:19.280
this total cost of ownership project came to be. Because you do not want to score your procurement

03:19.280 --> 03:24.560
only on performance, but you want to have it be a mix of different factors. Of course, you want

03:24.560 --> 03:29.120
to have the initial hard and software investment cost as part of this, but you also want to figure

03:29.120 --> 03:36.240
in cooling costs because this is one of the main cost drivers today. Put your power in and how to

03:36.240 --> 03:41.280
get the heat away that dissipate the heat that you generate. And usually want to have technical

03:41.360 --> 03:46.720
and administrative stuff for your HPC system to actually work properly. And then the last thing

03:46.720 --> 03:52.160
which is power consumption. And it's not power consumption of your idling system because that

03:52.160 --> 03:58.320
is reasonably low, but it's of the job mix you're running. And this job mix is very essential in

03:58.320 --> 04:05.040
this whole thing because the job mix is a very user dependent metric. It is what is the system

04:05.040 --> 04:11.200
actually being used for. So for example, and this is again referring back to these distributed and

04:11.520 --> 04:17.360
slightly specialized computing centers we have in HR. If you do physics simulation, your application

04:17.360 --> 04:22.560
might benefit more from faster CPUs with more core counts than compared to if you're using AI

04:22.560 --> 04:28.080
workloads where you probably can't get enough accelerator cards for your workload. And what

04:28.080 --> 04:33.760
you do is you monitor how your system is used, which is doable. For example, Slurm can do this.

04:34.320 --> 04:40.080
And then you figure out that your users are running lamps and grommets and open form your

04:40.080 --> 04:46.000
typical HPC software. And then you cannot really give that one to the vendors, can you? Because

04:46.000 --> 04:51.920
if you give a big grommet run to your hardware vendor and say, well, you have 48 hours to run

04:51.920 --> 04:56.560
this code through, then your vendor will probably not do it. And in an even more extreme case,

04:56.560 --> 05:01.440
you have this weird institute like the scientific computing institute where I'm from, which runs

05:01.440 --> 05:08.480
one weird A.out executable. They self-compiled with a custom build script. And you cannot give

05:08.480 --> 05:19.760
those to the vendors as well. But the problem is that all those HPC applications are large and

05:19.760 --> 05:26.720
complex and have different coding and software patterns. But they are the most representative

05:26.720 --> 05:32.400
thing you can get about what is actually running on your clusters. And so the idea is if you have some

05:32.480 --> 05:39.440
very big and complex HPC system like the one you see is simulated on the right, which has

05:39.440 --> 05:45.120
some kind of entry point and then does matrix multiplication and conditioning and heavy output

05:45.120 --> 05:50.880
preparation, the thing that actually spends most of the compute cycles is the one in gray.

05:52.000 --> 05:58.480
So the matrix conditioning matrix solving. And if you have a so-called mini-app, which is just

05:58.480 --> 06:02.960
the gray part and not all the other things around, you might be able to shrink this application

06:02.960 --> 06:10.080
significantly. And this mini-app approach was in one actually pioneered by Jan-Patrick Ler,

06:10.080 --> 06:16.080
which was the guy who gave the talk before me. So talk about coincidence. And the basic idea is

06:16.080 --> 06:21.680
you shrink the size of your application, but keep the computational characteristics. So the

06:21.680 --> 06:26.320
computational kernel where actually most of your compute cycles are spent stays the same. And then

06:26.320 --> 06:31.920
you just need to add some wrapper function that sets the kernel up. And then to finish, you just

06:31.920 --> 06:37.200
need to find some way to graceful terminator program because you can then have time measurements,

06:37.200 --> 06:43.520
power measurements on this little part of the actual big program. Great. And so this is why

06:43.520 --> 06:49.120
they needed the compiler guy to do this because they wanted in this total cost of ownership

06:50.000 --> 06:55.840
project the idea was to have a fully automatic extraction pipeline. And the basic idea for

06:55.840 --> 07:01.840
this pipeline was first you analyze the whole program. For this, we used the MetaCG framework.

07:01.840 --> 07:06.960
And those of you who happened to be there last year when I gave a completely different talk,

07:06.960 --> 07:11.600
we were using MetaCG as well. So it's a tool that's used at our institute quite heavily

07:11.600 --> 07:18.080
and allows you to have a representation of how do functions behave according to each other over

07:18.080 --> 07:23.360
the whole program. So you can get a whole program call graph. Once you know how all those functions

07:23.360 --> 07:27.920
relate to another, you can figure out what is the actual kernel. So where are my compute cycles

07:27.920 --> 07:34.560
spent? For this, the intention was to use Pira. The other idea is you just ask a domain expert

07:34.560 --> 07:38.880
what's the slow part of your program and they will probably tell you. So this is much more easy

07:38.880 --> 07:44.640
usually. And then the actual extraction of the kernel. And for this, we developed the Apex tool,

07:44.640 --> 07:50.800
so the app extraction tool. And it's a clang front and based compiler tool that does source code

07:50.800 --> 07:56.320
manipulation. And the basic idea is you query the so-called AST. You do not need to know how

07:56.320 --> 08:01.520
you get the AST, what an AST is. The only thing that you know an AST is a very, very condensed

08:01.520 --> 08:10.880
and information dense form and representation of a single CPP file. So if you have this CPP file

08:10.880 --> 08:16.720
on the right, which only contains the main function, you get the thing on the right. Admittedly,

08:16.720 --> 08:22.800
this is very much shortened. Where you can then find your record declaration, you find your structs,

08:22.800 --> 08:29.120
and you find your assignments, and you find your function calls. So what you then do is you can

08:29.120 --> 08:35.040
query this AST for your information to figure out how these function behave. So if you want to

08:35.680 --> 08:39.200
track the kernel, so you already know which part of the program you want to extract,

08:40.960 --> 08:45.840
you find all the functions that are used for this kernel. So your kernel might call some

08:45.840 --> 08:51.360
subroutines, you want to extract all those subroutines as well. And sadly, the AST is unable to provide us

08:51.360 --> 08:58.320
this information because we only can extract when we have the definition, so the body of the function.

08:58.320 --> 09:04.320
And as we are only limited to one CPP file with our AST information at a time, we only have to

09:05.040 --> 09:11.360
the declaration in this case, which was part of the header. So the print as function in our example

09:11.360 --> 09:17.360
is only declared, it's not defined, we have no body there. So if we have the whole body of a function,

09:17.360 --> 09:22.560
we get it as a whole text block. If we only have a definition, we remember that we need that one,

09:22.560 --> 09:27.680
and we extract it once we actually find the source file that contains the definition for this function.

09:29.280 --> 09:36.560
What we can do is we can find and extract all the used globals, because you usually rely on some

09:36.640 --> 09:42.080
kind of struct definition, you might even be using global state. And this is where the AST has the

09:42.080 --> 09:47.280
information. The whole definition of our struct S was inside the header files, and the header files

09:47.280 --> 09:53.280
are included by the preprocessor, though they are part when the AST is built. Great. So we just

09:53.280 --> 09:57.600
extract those as a text block, and then we need to find all include statements because include is

09:57.600 --> 10:04.720
the last colored thing in our example. And then we run into a little problem. Because remember how

10:04.720 --> 10:11.120
I told you that it's great that the preprocessor put the header files into our source code,

10:12.080 --> 10:17.280
well, include statements are also handled by the preprocessor. So everything that was specified in

10:17.280 --> 10:22.800
this include statement header is put physically in the AST once it's built, and we do not have

10:22.800 --> 10:28.160
the information anymore. So what we need to, and this is also true for defines and if and defs

10:28.160 --> 10:33.440
and pragmas, all those are resolved by the preprocessor, and we do not have any way to really

10:33.440 --> 10:38.480
figure this out once we get to the AST level. So we do not only need to hook into the AST,

10:38.480 --> 10:43.600
but we also need to hook into the preprocessor. Those of you who have actually worked with a

10:43.600 --> 10:52.320
preprocessor might know that the preprocessor is basically doing copy paste. So it's not context

10:52.320 --> 10:59.760
sensitive, it just takes include files, puts it where the include statement was. So we somehow

10:59.760 --> 11:05.600
need to map this context insensitive analysis results we get from our preprocessor hooks

11:06.480 --> 11:12.640
to figure out how do those relate to the context sensitive information that we get from our AST.

11:14.320 --> 11:19.520
And the only thing that those two share in common is source file locations. The preprocessor knows

11:19.520 --> 11:26.800
in which source file line it currently is when it does its copy pasting, and the AST can map back

11:26.800 --> 11:32.880
to the original source file. So what we do is if we now go to a more realistic example, this is an

11:32.880 --> 11:38.880
excerpt from the Lulish code, you do not, it's heavily shortened so there's no way to figure

11:38.880 --> 11:44.400
out what it does exactly, but we are mostly interested in the things colored yellow, which means

11:44.400 --> 11:49.920
it starts with an open MP, if open MP is available statement and then it includes the actual header

11:49.920 --> 11:54.400
and the preprocessor gives us all this information. Whenever it encounters one of these statements,

11:54.400 --> 12:00.000
we get a callback that tells us we found an if open MP, it goes from line one to line three,

12:00.000 --> 12:06.160
and then we found an include statement in line two, and then it's on us to figure out that line one

12:06.160 --> 12:11.680
to three fully encompasses the statement we found in line two, because again we're only doing text

12:11.680 --> 12:19.280
block extraction. So this is the conflicts that happen inside the preprocessor, but if we go on

12:19.280 --> 12:24.880
the preprocessor also tells us that it found an if open MP statement again in line six to 13,

12:24.880 --> 12:30.880
and seven to nine, and 15, but we also have the knowledge that there is a whole function going

12:30.880 --> 12:36.960
from line five to line 20, so we need to marry those two informations together as well, and

12:37.840 --> 12:45.200
this matching process was one of the challenges that we needed to overcome when we did the kernel

12:45.280 --> 12:54.080
extraction. So when we started this whole process, we had a very good idea how to do single translation

12:54.080 --> 13:01.120
unit C code, and we expanded on this to allow for multi-translation unit C code and C with C++

13:01.120 --> 13:06.240
components like new and delete and classes, and we're currently working on getting codes that

13:06.240 --> 13:13.120
makes heavy use of templates working, because the problem once you come into templates is that

13:13.120 --> 13:19.040
if you think back about our analysis step, we only get information about functions and how those

13:19.040 --> 13:24.480
functions relate to another, and templates are in a compiler speaking sense not necessarily

13:24.480 --> 13:31.440
functions, they are descriptions of how functions will be generated at compile time, so our analysis

13:31.440 --> 13:36.320
is currently not offering us the information about the original template, but only about the

13:36.320 --> 13:41.760
instantiated templates as generated by the compiler, so we are currently working on getting

13:41.760 --> 13:50.960
templates to work, and if you think back about the global usage analysis we are doing, if you have

13:50.960 --> 13:56.960
complex class inheritance and polymorphism, we are currently not able to traverse all possible

13:56.960 --> 14:03.760
diamond inheritance hierarchical implementations that are possible in C++, and lastly the idea is to

14:03.760 --> 14:10.480
also allow for automatic check pointing, so the wrapper calls that need to be generated to set

14:10.480 --> 14:15.680
up the environment for the kernel to run, it is theoretically possible to fully automatically

14:15.680 --> 14:22.320
generate those wrapper calls, we just haven't looked into it, and lastly the thing we are

14:22.960 --> 14:29.760
very skeptical if we are ever able to do it is to just mini extract from every C++ code ever written

14:29.760 --> 14:37.760
ever, because there are so many things you can do in C++ that we can try to achieve this, but

14:37.760 --> 14:44.160
I am very skeptical if we ever will be able to do this, but I don't want to leave you on this

14:45.040 --> 14:52.080
kind of depressing note actually, even in a state like this where the tool cannot fully

14:52.080 --> 14:59.040
handle all templates, even in a state where the tool cannot handle most complex inheritance hierarchies,

15:00.400 --> 15:04.880
tool assisted mini abstraction can still be useful, for example if you are willing to

15:04.880 --> 15:09.920
include the templates manually, because your program won't compile with the templates, you can

15:09.920 --> 15:16.000
just copy paste them, then you can get mini abstraction to work right now, and if you are

15:16.000 --> 15:21.760
interested in doing pinpoint optimizations on your source code, you can extract only those small

15:21.760 --> 15:28.080
snippets of code that you actually intend to work on, optimize those, and then do manual

15:28.080 --> 15:33.120
optimization and reintegrate those easily, so there are uses even for a tool that is not able

15:33.120 --> 15:42.000
to handle every C++ code ever written, and if you know of any HPC code that you think has a kernel

15:42.000 --> 15:48.880
that is identifiable and maybe not using the most and the deepest inheritance hierarchies,

15:48.880 --> 15:54.640
let me know, because I am always interested in figuring out how well my tool performs on other

15:54.640 --> 16:02.080
codes, so with this, thank you for your attention, I hope it was kind of interesting and I am open

16:02.080 --> 16:04.080
for questions.

16:14.880 --> 16:16.880
Any questions?

16:17.120 --> 16:33.120
I am the author of a large sparse matrix library, do you have something similar already in your

16:34.000 --> 16:41.920
catalog or collection? So it would be interesting to apply this tool on a library, but usually when

16:42.800 --> 16:50.080
the thing we are doing is we have the whole HPC software and then you call into the library,

16:50.080 --> 16:55.680
so of course your library is probably doing the heavy lifting and therefore probably doing the

16:56.960 --> 17:04.160
kernel part of the program, but extracting this one, we can look into that, but extracting a call

17:04.240 --> 17:15.200
to a library is relatively speaking very easy, so programs whose basic structure is do some setup,

17:15.200 --> 17:22.400
call an HPC library, get input back, those are basically mini-apps in the sense that we are

17:22.400 --> 17:27.680
talking about because they are not doing most of the heavy computing themselves, but if your library

17:27.680 --> 17:34.480
has internal conditioning or matrix solving capabilities that you know of that your library

17:34.480 --> 17:40.320
struggles to do, then we are talking again, so just let me know the name of your library and I will

17:40.880 --> 17:42.880
try to look into it.

17:47.920 --> 17:49.920
Four questions?

17:50.640 --> 17:52.640
Alex was first.

17:52.960 --> 18:04.080
Hi. I was wondering your mini-apps seem to be focused on compute-intensive parts of the code,

18:04.880 --> 18:11.040
do you also construct mini-apps for storage-intensive applications or something else?

18:13.040 --> 18:18.560
So the automatic identification wire, the PIRRA tool, tries to figure out what is the compute-

18:18.560 --> 18:23.760
intensive part of the program, and this is the only kernel, so to speak, that we are able to

18:23.760 --> 18:29.200
automatically identify, but if you as a domain expert know that this is the part of our program

18:29.200 --> 18:35.120
where we are IO limited, then this is nothing PIRRA can identify, but if you say I want to extract

18:35.120 --> 18:42.160
starting from this function, our tool should in theory be able to extract the IO limited part of

18:42.160 --> 18:48.320
your code. So this is the point where you as a domain expert need to specify this is the part I

18:48.320 --> 18:53.840
want to extract, because the only thing that is very prominently identifiable is compute-limited

18:54.560 --> 18:57.600
parts of the program, but yeah, in theory it should work.

18:57.600 --> 19:03.760
And second question, I might allow, and do you have a library of mini-apps that are ready to use for

19:03.840 --> 19:11.520
others by third parties? So currently we're doing our benchmark, our benchmarks on already

19:11.520 --> 19:16.480
existing mini-apps, so we're doing mini-app extraction from mini-apps because I am profiting from

19:16.480 --> 19:20.400
the small size of those mini-apps to validate that my program actually runs, for example the

19:20.400 --> 19:27.840
Lulash example I showed is a mini-app in itself. If I remember correctly, it's a shock simulation

19:27.840 --> 19:35.120
in fluids, please don't quote me on that, but it's a great code and it's very easy to work with,

19:35.120 --> 19:41.200
so I'm using that for my evaluation, but the idea is to get it to work on larger codes, for example

19:41.200 --> 19:48.000
we're currently looking to the ISSM ice sheet and system model, which is a well ice melting

19:48.000 --> 19:55.040
simulation for large ice sheets, but yeah we're always looking for other codes, and if you have

19:55.120 --> 20:04.000
something that is IO bound then of course tell me. Thank you. Plenty of time for more questions, Chris.

20:06.400 --> 20:11.120
Sorry to be that guy, but how hard would it be to adapt this approach to Fortran?

20:15.040 --> 20:21.360
Fortran tooling in general is something that has been of interest at our institute for a long time,

20:21.360 --> 20:26.640
but the problem with Fortran tooling is that most of our knowledge is coming from the Klang front

20:26.640 --> 20:37.680
and so the C language front end, and I am not sure if Fortran, the current new Fortran LLVM front end,

20:37.680 --> 20:47.040
offers the same analysis and query capabilities as Klang, and the idea to move lower in the hierarchy

20:47.040 --> 20:52.960
towards the LLVM IR, which is more target agnostic, or language agnostic more really,

20:54.880 --> 21:00.720
is that as soon as you go down to IR it's very hard to go back to figure out what was the source

21:00.720 --> 21:08.240
code files that actually made up this IR. So yes we already have in the back of our mind that there

21:08.240 --> 21:16.080
are other languages that are used in HPC systems, and usually if I present this approach I'm getting

21:16.080 --> 21:20.880
asked, well we have some Fortran codes, we have some Python codes, how does your approach work,

21:20.880 --> 21:26.560
but we are sadly limited by the Klang front end's capabilities, so C++, Objective C,

21:27.520 --> 21:32.880
which we have never tried, I'm not saying that we are able to do Objective C, but C and C++ we

21:32.880 --> 21:37.200
have tried and are currently limited to because of our design choice. There is a Fortran front end

21:38.080 --> 21:47.760
for Klang. Yes, but this is a Fortran front end for the LLVM infrastructure.

21:49.280 --> 21:54.560
The C-Lang front end is the part of the LLVM project which takes C code and translates to the

21:54.560 --> 22:01.920
LLVM IR, so I'm not sure if we're talking about the same thing. There is a Fortran front end

22:01.920 --> 22:08.320
for LLVM, yes, but I would be very surprised if there is a way to translate Fortran code into

22:08.320 --> 22:14.000
something that Klang can understand, but I might be wrong, there is a myriad of interesting software

22:14.000 --> 22:21.760
repositories, but currently consider Miscoptic. Then you can use this small part of your code

22:21.760 --> 22:26.720
to stress the floating point unit only of linear hardware. Then this other part is very well

22:26.720 --> 22:33.520
vectorized, so we extract this one and suddenly you are able to use AVX 512 instructions.

22:34.400 --> 22:41.360
The idea is to extract every code intensive part into its own little package and then benchmark

22:41.360 --> 22:49.360
with those separate packages, so at the end you can get an idea about the whole performance of your

22:50.240 --> 22:56.480
as your individual kernel regions. This would be the approach I take.

23:00.800 --> 23:03.760
Maybe one more? In the back, okay.

23:12.560 --> 23:16.720
Great talk, thank you so much. A couple of questions, Chris. I believe there was a paper

23:16.720 --> 23:22.480
about a Fortran mini app extractor some time ago. I can dig that up and send it to you.

23:22.480 --> 23:33.280
If I remember that, if not, shoot me a message. Then the flank front end in the LLVM project

23:33.280 --> 23:39.760
currently is not compatible with Klang, so we do get different ASTs. This approach is actually

23:39.760 --> 23:47.600
working at the AST level, so if it were using the LLVM IR level and then somehow like magically

23:47.600 --> 23:53.520
map back dwarf, then it would work, but there was a different project that did that, it worked kind

23:53.520 --> 24:02.400
of okay. I would have a question about you for the complex inheritance hierarchies. Do you have any

24:02.400 --> 24:09.040
idea on how you could tackle that, approach that, represent this thing across the whole program or

24:09.920 --> 24:16.080
things so, I mean, did you have spent any time so far looking at that or did you say like okay,

24:16.080 --> 24:20.320
that's future me or future someone going to do that?

24:21.200 --> 24:27.280
So thank you for the question. So it's a mix of both. I spent some time thinking about it and decided

24:27.280 --> 24:35.280
it was for future me because I didn't assume it to be very easy, but you already mentioned the

24:35.360 --> 24:41.520
general idea that as soon as you go into more complex inheritance chains, you aren't able to

24:41.520 --> 24:48.400
extract everything from one header file per se, so you need to do the same opportunistic extraction

24:49.120 --> 24:54.880
idea that we do for functions, but now for classes, structs and all their possible inheritance parents.

24:55.520 --> 25:00.800
So this is something that we need to analyze on the whole program scale, so this is something that

25:01.760 --> 25:10.000
in the not near future, but in the foreseeable future, I intend to put as an analysis pass into

25:10.000 --> 25:18.400
the CG collector, which you probably are familiar with. So the idea is that this tool is then able

25:18.400 --> 25:25.840
to annotate all this information as metadata and then once we merge it, we get a very good,

25:25.840 --> 25:33.760
hopefully, impression of how those inheritance chains flow through the whole program. So the idea.

25:35.120 --> 25:46.320
Thanks. Okay, that's all we have time for. Thanks a lot, Tim.

