WEBVTT

00:00.000 --> 00:11.440
Next we have Prashant Rao with Kuzu, a graph database management system for Python graph

00:11.440 --> 00:15.080
data science.

00:15.080 --> 00:20.320
All right.

00:20.320 --> 00:22.160
Good afternoon, everyone.

00:22.160 --> 00:23.320
So my name is Prashant.

00:23.320 --> 00:25.600
I'm an AI engineer at Kuzu.

00:25.600 --> 00:27.760
So I'll be talking about graph databases today.

00:27.760 --> 00:32.200
Just a quick show of hands, how many people have worked with graph databases or heard

00:32.200 --> 00:33.200
of them?

00:33.200 --> 00:34.200
Fair number.

00:34.200 --> 00:35.200
Okay.

00:35.200 --> 00:37.040
So you're in the right room today.

00:37.040 --> 00:39.680
So I'll outline a bit about what I'm going to cover.

00:39.680 --> 00:43.600
I'll start with what graphs are for those who are not familiar.

00:43.600 --> 00:48.680
And then when you need graph modeling, I'll also cover some of the features of a competent

00:48.680 --> 00:52.400
graph database management system and what that means.

00:52.400 --> 00:57.360
And that leads into the vision that Kuzu has, both as a GDBMS or that is a graph database

00:57.360 --> 01:02.600
management system and as the go-to solution for graph data science.

01:02.600 --> 01:07.520
And I'll end with a walkthrough on how Kuzu makes graph data science workflows easier

01:07.520 --> 01:09.960
for the developer.

01:09.960 --> 01:14.880
So the first question we must ask is, what are graphs or networks as they're sometimes

01:14.880 --> 01:15.880
called?

01:15.880 --> 01:21.360
They are an abstract representation of entities and relationships.

01:21.360 --> 01:26.360
Essentially an entity is represented as a node and the way these are connected together

01:26.360 --> 01:30.920
is represented by an edge, which is the relationship shown in this figure.

01:30.920 --> 01:36.000
And as the figure in the bottom shows, these can get pretty complex and reveal really interesting

01:36.000 --> 01:39.600
structures about connected data.

01:39.600 --> 01:42.800
And that's exactly what we see in the real world.

01:42.800 --> 01:46.400
Graphs are actually one of the most natural ways to represent data.

01:46.400 --> 01:50.200
Social networks are of course something we are very familiar with, but graphs are very

01:50.200 --> 01:54.840
prevalent in many other domains, all the way from drug interactions to molecular networks

01:54.840 --> 01:56.720
to traffic networks.

01:56.720 --> 02:02.760
In the world of finance, you analyze transactions for things like fraud and they also are very

02:02.760 --> 02:07.360
common in knowledge graphs that encode factual information about the world.

02:07.360 --> 02:13.040
Kuzu is a graph database management system, which is a class of database management systems.

02:13.040 --> 02:17.520
So I'll start by giving a general overview about GDBMS.

02:17.520 --> 02:21.240
You generally have three components to any database system.

02:21.240 --> 02:25.960
You have the data model, you have the query language, and you have the system implementation.

02:25.960 --> 02:31.240
From the data model perspective, graph data models differ from the conventional relational

02:31.240 --> 02:37.280
data model in the sense that you typically represent the data as nodes and edges.

02:37.280 --> 02:42.520
And you have key value properties on these nodes or edges.

02:42.520 --> 02:46.960
In this example, with this triangle you see here, you have a cyclic relationship of transactions

02:46.960 --> 02:52.760
between people, one, two, and three, where the nodes one, two, and three have property

02:52.760 --> 02:58.600
information on the name and the edges have the amount of the transaction as a property.

02:58.600 --> 03:01.400
So this is called the property graph model of graphs.

03:01.400 --> 03:05.200
And it's very, very common and prevalent in the industry.

03:05.200 --> 03:09.520
But there's also another data model called RDF, resource description framework, which

03:09.520 --> 03:14.920
has a similar concept of subject, predicates, and objects, which represent a triple.

03:14.920 --> 03:19.440
The triple is a basic unit of data in the graph, but it's the same idea as the property

03:19.440 --> 03:23.120
graph model except the implementation is different.

03:23.120 --> 03:27.720
From a query language perspective, every graph database management system needs a high-level

03:27.720 --> 03:32.200
query language that's designed specifically with graph syntax.

03:32.200 --> 03:33.800
And an example of this is shown here.

03:33.800 --> 03:37.400
This is the Cypher query language, which Kuzu implements.

03:37.400 --> 03:42.440
And incidentally, Cypher is the same language that was invented and popularized by Neo4j,

03:42.440 --> 03:44.640
if anybody's used that before.

03:44.640 --> 03:51.600
But what this example query snippet shows is you have node information, A and B, of

03:51.600 --> 03:56.280
type account, and you're matching on those nodes.

03:56.280 --> 04:03.200
And then you're running a query, a joint query equivalent in a way that reminds you a lot

04:03.200 --> 04:04.200
of SQL.

04:04.200 --> 04:08.840
It's very declarative and it's very high-level reminiscent of SQL.

04:08.840 --> 04:13.560
From a system implementation standpoint, universally, I think it's hard to come with

04:14.120 --> 04:20.400
a statement that covers all graph systems, but in general, they implement storage structures,

04:20.400 --> 04:23.320
indices, and operators that are specific to graphs.

04:23.320 --> 04:26.280
One example is the shortest path operator.

04:26.280 --> 04:30.000
These are operators that are not prevalent in relational systems but are very common

04:30.000 --> 04:33.000
in graph systems.

04:33.000 --> 04:36.120
There are many reasons why you might need graph modeling, but I'll cover just a couple

04:36.120 --> 04:39.240
of them in these next two slides.

04:39.320 --> 04:45.840
For an example, let's take this query where we are trying to find direct or indirect possible

04:45.840 --> 04:51.000
sources of money flow into a person's account from a particular location.

04:51.000 --> 04:56.800
So in this example, the person is Alice, represented by node B in this Cypher query,

04:56.800 --> 05:03.320
and we are matching on the owner of that account, which is Alice, but also matching

05:03.320 --> 05:06.840
on account A, whose location is Canada.

05:06.920 --> 05:11.840
The key here is that the middle portion, which is the transfer star, that star syntax

05:11.840 --> 05:15.440
is a high-level general syntax called clean star.

05:15.440 --> 05:21.280
It's used to implement indirect and recursive joins.

05:21.280 --> 05:23.840
As you can see, the query is quite concise.

05:23.840 --> 05:25.320
It's quite readable.

05:25.320 --> 05:30.480
You can do this sort of query in SQL, but it's a recursive query and it's not as easy.

05:30.480 --> 05:34.440
It's going to be a lot more verbose and not that easy to read.

05:34.440 --> 05:38.440
One other example of this would be the shortest path query, which is a lot harder to do in

05:38.440 --> 05:42.640
recursive SQL, but in Cypher, it's very, very straightforward.

05:42.640 --> 05:48.560
It's just an additional clause that you add attached onto the previous query.

05:48.560 --> 05:53.040
Another case where you need graph modeling is in heterogeneous data.

05:53.040 --> 06:00.360
This example here shows an example of Dbpedia, which is a structured version of Wikipedia,

06:00.360 --> 06:04.240
and we're taking this example of the location we're in right now, University, Liberator

06:04.240 --> 06:06.320
and Brussels.

06:06.320 --> 06:13.640
On the left is the way it's stored in structured form, where you have key value properties,

06:13.640 --> 06:16.360
and each of these properties links to other properties.

06:16.360 --> 06:21.000
But on the right, we schematically represent that as a graph.

06:21.000 --> 06:24.040
As you can see, the university is linked to the city of Brussels.

06:24.040 --> 06:28.840
It's also linked to the country of Belgium, what affiliations it has, and each of these

06:28.840 --> 06:33.840
individual resources can be linked to other resources.

06:33.840 --> 06:38.760
This actually expresses the power of a graph model, because doing this with a tabular form

06:38.760 --> 06:42.800
of data would be almost impossible, because that's how Wikipedia is structured.

06:42.800 --> 06:46.720
It's a lot of connected information.

06:46.720 --> 06:50.840
This leads us to the question of what is a feature set of a competent graph database.

06:50.840 --> 06:57.320
We list a few of them here, but I think it's very difficult to go through each of them

06:57.320 --> 06:58.640
in the time we have.

06:58.640 --> 07:02.440
We do have a blog post that covers this in much more detail.

07:02.440 --> 07:07.200
It's called What Every GDBMS Should Do and Division.

07:07.200 --> 07:14.960
But in a nutshell, every GDBMS has to support things like many to many growing joins, recursive

07:14.960 --> 07:20.080
joins on top of heterogeneous data sets, for example, knowledge graphs.

07:20.080 --> 07:24.120
Another thing that we can highlight here is the schema querying aspect, where in this last

07:24.120 --> 07:31.440
example, you have account information and transaction edges, and you're able to query

07:31.440 --> 07:33.480
on the type of the edge.

07:33.480 --> 07:35.920
Let's say you have two different kinds of transactions.

07:35.920 --> 07:40.600
You don't want each of those on either side of the middle node to be the same transaction

07:40.600 --> 07:41.600
type.

07:41.600 --> 07:47.960
You're able to say, you apply a predicate on the edge to say that you don't want nodes

07:47.960 --> 07:49.600
of a particular type.

07:49.600 --> 07:52.080
This is the sort of thing that you can't do in SQL.

07:52.080 --> 07:55.480
You can only do this in a graph model.

07:55.480 --> 08:01.920
The vision of Kuzu as a graph database management system is it aims to represent the state of

08:01.920 --> 08:07.480
the art of how graphs should be stored, indexed, and queried.

08:07.480 --> 08:11.680
It does this by being highly scalable to several terabytes of data.

08:11.680 --> 08:14.920
It's very fast in terms of query speed.

08:14.920 --> 08:18.480
It supports the property graph model, which we described earlier.

08:18.480 --> 08:24.800
It also supports the RDF data model, which is going to be coming in the next release.

08:24.800 --> 08:28.640
It does so via a high-level query language, Cypher.

08:28.640 --> 08:32.320
It's easy to use and uses an embeddable architecture.

08:32.320 --> 08:36.800
We like to think of ourselves as like duck DB or SQLite, but for graphs.

08:36.800 --> 08:41.400
If you ever come across either of the other two relational systems, Kuzu is like a graph

08:41.400 --> 08:43.560
analog to those systems.

08:43.560 --> 08:47.840
I should also note here that Kuzu is based on many years of research at the University

08:47.840 --> 08:49.440
of Waterloo.

08:49.440 --> 08:55.880
It's now being developed in an independent company called Kuzu Inc, which we're from.

08:55.880 --> 08:59.480
The other big vision that Kuzu has from a data science perspective, specifically graph

08:59.480 --> 09:05.320
data science perspective, is to be the go-to back end for graph modeling and data science.

09:05.320 --> 09:11.520
Essentially the vision here is if you look at the bottom half of this figure, you have

09:11.520 --> 09:18.880
a lot of data sitting in disparate sources like data lakes, warehouses, relational databases

09:18.920 --> 09:22.320
all the way from Postgres and many others.

09:22.320 --> 09:26.680
There's a lot of interoperability challenges that you have with these data sources.

09:26.680 --> 09:30.800
Even though you have structured data, in many cases working with them as a graph is quite

09:30.800 --> 09:35.080
challenging because of the movement of data across the systems into a powerful graph database

09:35.080 --> 09:36.080
back end.

09:36.080 --> 09:42.200
Kuzu aims to be a simpler way and sort of an interface to that.

09:42.200 --> 09:49.920
In the upper half, the aim of this is to make graph data science much more accessible

09:49.920 --> 09:56.440
in the sense that we provide zero copy access to the data by writing out the format that

09:56.440 --> 09:57.840
is native to those libraries.

09:57.840 --> 10:00.600
For example, PyTorchumetric, NetworkX.

10:00.600 --> 10:06.080
These are popular graph data science libraries and machine learning libraries in Python.

10:06.080 --> 10:10.920
By being well integrated with the Python data science ecosystem, we believe this makes it

10:10.960 --> 10:13.880
a lot more achievable.

10:13.880 --> 10:19.520
I'll quickly walk through an example of how Kuzu makes graph data science easier in terms

10:19.520 --> 10:21.160
of a workflow.

10:21.160 --> 10:26.960
Let's consider this real world, a simple example, a toy example, where you have two different

10:26.960 --> 10:27.960
data sources.

10:27.960 --> 10:31.120
You have people and the movies that they watched.

10:31.120 --> 10:35.040
You also have people and their friends and where they live.

10:35.040 --> 10:37.160
These are two different data sets.

10:37.160 --> 10:41.320
Your goal is to use the information from this data to build a movie recommender system

10:41.320 --> 10:44.800
where a person who has watched certain movies gets recommended other movies.

10:44.800 --> 10:48.360
There are many ways you can build such a recommendation system.

10:48.360 --> 10:53.560
One we'll cover here is using a graph neural network, specifically using link prediction

10:53.560 --> 10:59.320
where you're trying to predict a recommended edge between a person and a movie.

10:59.320 --> 11:03.360
This is a very simple example where you have data set one which has the persons and the

11:03.360 --> 11:08.440
movies with some additional metadata, could be age or any other attributes.

11:08.440 --> 11:15.680
Then data set two has persons and what friends those persons have and where they live.

11:15.680 --> 11:19.600
For those who have not worked with graph machine learning before, it's a very high level overview

11:19.600 --> 11:26.720
in this slide where the goal of graph machine learning is to embed the nodes and the surroundings

11:26.720 --> 11:29.320
space into a vector space.

11:29.320 --> 11:35.680
The benefit of this is that it incorporates the structure of the graph based on the nodes

11:35.680 --> 11:38.880
and their surrounding neighbors.

11:38.880 --> 11:44.680
The idea is that you perform a computation on the graph nodes and transform the features

11:44.680 --> 11:50.040
of that graph into a feature vector like the array shown there.

11:50.040 --> 11:55.040
The idea is very similar to the kind of vectors that you may have seen in other domains like

11:55.040 --> 11:59.520
computer vision or natural language processing where the only difference is that in those

11:59.520 --> 12:04.800
domains you are considering the similarity between words in a sentence or pixels in an

12:04.800 --> 12:10.120
image whereas in this case you're considering the similarity of the topology of the graph

12:10.120 --> 12:13.280
itself.

12:13.280 --> 12:17.600
All of that is great but when you're working with the data, you're immediately faced with

12:17.600 --> 12:18.600
a problem.

12:18.600 --> 12:21.440
The data you have might exist in different sources.

12:21.440 --> 12:26.120
For example, the movies watch may exist in Postgres and the person friends may exist

12:26.120 --> 12:32.400
in other structured data sources that you export to CSV or Parquet or something similar.

12:32.400 --> 12:35.160
You need to bring them together to form a graph.

12:35.160 --> 12:37.920
Conceptually, this is how a graph would look.

12:37.920 --> 12:40.080
You have nodes that represent the persons.

12:40.080 --> 12:44.960
You have edges that represent the movies that they watched in the first graph and then in

12:44.960 --> 12:49.760
the second one you have edges that represent friendship between people and what cities

12:49.760 --> 12:53.280
they live in.

12:53.280 --> 12:57.280
The moment you do that, you have another problem where you potentially have overlapping data

12:57.280 --> 13:00.440
or duplicate data between these two subgraphs.

13:00.440 --> 13:04.200
In one of them you have the persons and the cities and the other ones you have persons

13:04.200 --> 13:06.400
and movies.

13:06.400 --> 13:08.880
Many of them might be the same people.

13:08.880 --> 13:13.560
There is some deduplication logic that's required where you have to merge people with the same

13:13.560 --> 13:18.080
attributes and there's some custom logic that needs to be put in terms of how you decide

13:18.080 --> 13:20.200
whether something is a duplicate.

13:20.200 --> 13:25.120
Once you do that, you have the final result where you have some nodes that are dangling

13:25.120 --> 13:29.720
in the sense that they have no edges that attach them to other nodes.

13:29.720 --> 13:34.480
These actually don't inform the machine learning model and would need to be removed.

13:34.480 --> 13:38.840
To do all of this is actually quite tedious if you were to write your own custom logic

13:38.840 --> 13:41.480
in your own language of choice.

13:41.480 --> 13:46.160
Where Kuzu comes in and where it's very powerful is the ability to just install an embeddable

13:46.160 --> 13:49.800
library using pip install Kuzu in Python.

13:49.800 --> 13:55.200
Once you have that, you're very rapidly able to run query execution to create the tables,

13:55.200 --> 14:02.320
load the data in and perform deduplication logic and dangling node removal using a high-level

14:02.320 --> 14:07.640
query language like Cypher in a way that scales to the size of the data that you have.

14:07.640 --> 14:11.440
In many ways, you don't have to worry about the scalability problem because now you have

14:11.440 --> 14:16.600
a high-level query language supporting your operations in the middle stages.

14:16.600 --> 14:20.920
Once you have all of the data and the features that are loaded into a graph, you essentially

14:20.920 --> 14:27.320
can walk through this process where you not only have the data in the right form, but you're

14:27.320 --> 14:33.080
able to actually encode the features into the graph and store that on disk.

14:33.080 --> 14:37.400
One of the biggest limitations with PyTorch geometric is if you ever worked with it before,

14:37.400 --> 14:40.600
is it's very memory intensive when you're working with large graphs.

14:40.640 --> 14:45.160
Kuzu helps a lot in this regard by persisting the features onto disk.

14:45.160 --> 14:49.760
That's exactly where I think we want to highlight this point.

14:49.760 --> 14:55.640
I think we are almost out of time, but I'll wrap up by saying the key points to take away.

14:55.640 --> 15:00.720
Kuzu is an in-process analytical graph database system, kind of like DuckTB is in the SQL

15:00.720 --> 15:03.160
world, but for graphs.

15:03.160 --> 15:08.280
It's highly scalable and optimized for multi-core parallelism and very well integrated with

15:08.920 --> 15:14.960
the PyData ecosystem, including NumPy, PyAro, NetworkX, PyTorch, and so on.

15:14.960 --> 15:18.800
It supports both the property graph model as well as the RDF graph model via Cypher,

15:18.800 --> 15:20.760
a high-level query language.

15:20.760 --> 15:24.800
It's embeddable and very easy to use from your application.

15:24.800 --> 15:29.080
It's also accessible with other language bindings, not just Python.

15:29.080 --> 15:33.200
If you come from other languages, those options exist as well.

15:33.200 --> 15:34.200
That's it from us.

15:34.220 --> 15:39.160
Kuzu is an open source, very permissive, licensed, MIT licensed project.

15:39.160 --> 15:43.000
I'd love for everyone to give it a try and reach out to us on Discord.

15:43.000 --> 15:45.340
We're always open to chatting more about graph use cases.

15:45.340 --> 15:45.840
Thank you.

