WEBVTT

00:00.000 --> 00:12.000
Thank you.

00:12.000 --> 00:17.160
My name is Chris Pinn and I'm a PhD researcher at the University of Southampton and we will

00:17.160 --> 00:19.880
talk about safe reinforcement learning.

00:19.880 --> 00:24.360
So the first helpful presentation will be theoretical, we will define what reinforcement

00:24.360 --> 00:27.520
learning is, when is it safe and so on.

00:27.520 --> 00:32.440
In the second half we will talk about practical scenarios which hopefully will sort of clear

00:32.440 --> 00:34.600
things up.

00:34.600 --> 00:38.040
So when do we use reinforcement learning?

00:38.040 --> 00:40.440
When we want to solve control problems.

00:40.440 --> 00:45.600
So you can imagine we have a robotic arm that we want to control with an algorithm or that

00:45.600 --> 00:52.480
we have a space shuttle that should land on a moon and what these scenarios have in common

00:52.480 --> 00:56.680
is that there is an agent that interacts with an environment.

00:56.680 --> 01:03.240
The agent is choosing actions and the environment reacts to those actions by choosing actions,

01:03.240 --> 01:06.080
by choosing states and rewards, sorry.

01:06.080 --> 01:11.560
Mathematically the environment is defined as a set of states, as a mark of decision process

01:11.560 --> 01:18.160
with a set of states, set of actions, reward function and a transition function.

01:18.160 --> 01:24.800
The goal of this process is to find an agent that maximizes an optimality criterion.

01:24.800 --> 01:32.520
The optimality criterion is a long term sum of rewards that come from the environment.

01:32.520 --> 01:36.520
Now the agent's life cycle has two phases.

01:36.520 --> 01:40.120
First is the training phase and then we have the deployment phase.

01:40.120 --> 01:46.560
So in the training phase the agent explores the environment by taking random actions and

01:46.560 --> 01:53.280
over time it starts exploiting what it has learned about the environment to get further.

01:53.280 --> 01:59.360
Now when we are happy that the agent has been trained we deploy it and then the agent only

01:59.360 --> 02:01.520
exploits what it has learned.

02:01.520 --> 02:08.880
It doesn't explore anymore, it doesn't take random actions, it just does the same action

02:08.880 --> 02:12.040
over and over.

02:12.040 --> 02:16.880
Now the training is done with respect to the optimality criterion.

02:16.880 --> 02:21.920
So we want to find an agent that maximizes that long term sum of rewards coming from

02:21.920 --> 02:24.600
the environment.

02:24.600 --> 02:29.200
Open source software is a great enabler of reinforcement learning.

02:29.200 --> 02:38.160
We have gymnasium with a bunch of toy examples, Atari games, physics based games and so on.

02:38.160 --> 02:42.800
We have physics simulators like Mujoko that is used in robotics research but can be used

02:42.800 --> 02:45.360
in reinforcement learning as well.

02:45.360 --> 02:51.760
We can have traffic control and self driven vehicles with Sumo and Carla or multi agents

02:51.760 --> 02:55.840
scenarios, whether agent cooperate or compete.

02:55.840 --> 03:02.160
Now this shows the breadth of applications that reinforcement learning can have and obviously

03:02.160 --> 03:10.160
if your scenarios can match one of these simulators they are open source you can extend them and

03:10.160 --> 03:12.320
use them in your training.

03:12.320 --> 03:16.880
Of course in many scenarios you will have to create your own environment and here is

03:16.880 --> 03:18.560
a simple example on how to do that.

03:18.560 --> 03:24.240
So you would import gymnasium package, extend the environment class and among the things

03:24.240 --> 03:27.520
that you have to define would be a step function.

03:27.520 --> 03:32.440
Now the step function accepts the agent's action and returns a reward and a state.

03:32.440 --> 03:36.120
So basically it defines how the environment behaves.

03:36.120 --> 03:40.320
Similarly you would have a class of agent that accepts the state and returns an action

03:40.320 --> 03:45.040
and so this is the loop we have seen before but in code.

03:45.040 --> 03:48.440
This example is simplified but if you had to do the gymnasium documentation you would

03:48.440 --> 03:51.600
see something very similar there.

03:51.600 --> 03:55.440
Now why do we care about safer reinforcement learning?

03:55.440 --> 04:00.440
Because an agent maximizing an optimative criterion has to be creative and this creativity

04:00.440 --> 04:02.360
can be very dangerous.

04:02.360 --> 04:08.640
Last year we have seen this scenario from US Air Force where they said that their drone

04:08.640 --> 04:14.240
killed a human operator because the operator was stopping it from maximizing an signal.

04:14.240 --> 04:20.800
Now that didn't happen, it was a hypothetical scenario but it's a very good example of why

04:20.800 --> 04:24.080
we want to make reinforcement learning safe.

04:24.080 --> 04:30.080
So safety in this case means controlling the behaviors that the agent can discover over

04:30.080 --> 04:33.600
the training phase.

04:33.600 --> 04:39.520
Now we have two options when we want to include safety in the reinforcement learning process.

04:39.520 --> 04:43.840
Either we modify the optimative criterion that the agent is maximizing.

04:43.920 --> 04:48.360
If this optimative criterion contains some safety information and the agent maximizes

04:48.360 --> 04:52.800
it during the training phase it will become safer as well.

04:52.800 --> 04:57.560
Or we can modify the agent's actions and again if we do it correctly we can prohibit certain

04:57.560 --> 05:02.960
actions from being taken and again increase safety of the agent.

05:02.960 --> 05:06.160
Now I'll give you two examples of this.

05:06.160 --> 05:09.640
Let's start by modification of the optimative criterion.

05:09.640 --> 05:15.160
So we begin with a Markov decision process with a set of states, set of actions, a reward

05:15.160 --> 05:18.200
function and a transition function.

05:18.200 --> 05:23.640
Now we would like to add a function H to it that contains the safety information that

05:23.640 --> 05:26.880
we want to pass to the agent.

05:26.880 --> 05:32.640
This function H is inferred from data in more complicated scenarios and on how to do that

05:32.640 --> 05:37.480
I refer you to the paper but here I'm just going to self-engineery because it's a simple

05:37.480 --> 05:39.480
scenario.

05:39.680 --> 05:45.800
So on the left you see the new optimative criterion.

05:45.800 --> 05:51.960
So now we want to find an agent that maximizes the long-term sum of rewards as well as the

05:51.960 --> 05:57.600
function H. And that's how we can use that's one example of changing the optimative criterion

05:57.600 --> 05:59.920
to include safety information.

05:59.920 --> 06:06.080
So let's imagine that we have a reward function that rewards the agent for reaching the goal

06:06.120 --> 06:09.680
with 100 points and gives it a minus one point otherwise.

06:09.680 --> 06:13.360
This motivates the agent to complete the goal as fast as possible.

06:13.360 --> 06:17.640
And then let's say that we have inferred this function H from data and that it punishes

06:17.640 --> 06:21.480
the agent for going into the water with minus 100 points.

06:21.480 --> 06:27.320
So in a trajectory where the agent avoids all water we get a bunch of minus ones and

06:27.320 --> 06:33.520
a hundred at the end and the function H isn't used in this example but in a trajectory where

06:33.560 --> 06:40.560
we go through water the agent is punished by this function H. And this enables it to

06:40.640 --> 06:45.440
distinguish between these two trajectories and when the training phase is over it will

06:45.440 --> 06:48.680
naturally start avoiding these water tiles.

06:48.680 --> 06:53.040
And this example is obviously very simple but it works in much more bigger and scalable

06:53.040 --> 06:55.480
scenarios than this.

06:55.480 --> 07:00.320
And so some properties is that here we have safety only during the deployment phase not

07:00.320 --> 07:02.080
the training phase.

07:02.080 --> 07:06.080
The agent has to try all the dangerous stuff in order to realize that they are bad and

07:06.080 --> 07:08.880
avoid them doing the deployment phase.

07:08.880 --> 07:14.120
Similarly we have to have the data set of safe behaviors because we needed to infer

07:14.120 --> 07:16.240
the function H from it.

07:16.240 --> 07:22.840
So again these safe behaviors could be easy or difficult to obtain depending on your scenario.

07:22.840 --> 07:27.840
And in this example we don't need to define what safety actually means because it's implicitly

07:27.840 --> 07:29.640
contained in the data set.

07:29.680 --> 07:33.800
And again defining safe behaviors can be actually pretty hard when you think about it.

07:33.800 --> 07:36.720
And if you could have a data set that just contains all the safe trajectories and you

07:36.720 --> 07:40.680
can learn from it that actually is pretty helpful.

07:40.680 --> 07:43.040
As we will see in the next scenario.

07:43.040 --> 07:50.040
So in the second scenario we modify the agent's actions and in this paper researchers use

07:51.600 --> 07:56.760
formal methods and they start with the environment which is Markov decision process and they

07:56.800 --> 07:59.960
turn it into something called a transition system.

07:59.960 --> 08:06.960
Now similarly they have a set of safety requirements that the agent should adhere to and they turn

08:08.000 --> 08:11.040
them into specifications in linear typology.

08:11.040 --> 08:17.400
And then they bring these two things together with reactive synthesis to generate a shield.

08:17.400 --> 08:24.400
And this shield has the formal mathematical guarantees of ensuring that the agent never

08:25.040 --> 08:27.680
does a dangerous action.

08:27.680 --> 08:32.520
And what this means, let's take a look at the first trajectory again.

08:32.520 --> 08:36.600
Now here the shield isn't used and it's exactly the same as before.

08:36.600 --> 08:40.000
So we get a bunch of minus ones and a hundred at the end.

08:40.000 --> 08:46.320
But when we take a look at the second trajectory now when the agent should go through the water

08:46.320 --> 08:49.300
the shield will actually prohibit it from happening at all.

08:49.300 --> 08:53.200
So instead the shield will send the agent in some other direction and the agent will

08:53.200 --> 08:56.800
never experience going through the dangerous state.

08:56.800 --> 09:03.800
And so here we have safety during both training and deployment and compare this with the modification

09:04.240 --> 09:08.960
of the optimality criterion where we only had safety during the deployment phase not

09:08.960 --> 09:10.280
the training phase.

09:10.280 --> 09:16.280
However it comes at the cost of being able to specify the transition system.

09:16.280 --> 09:23.120
So this can actually be pretty difficult if we don't know the Markov decision process

09:23.720 --> 09:28.680
or we can't specify the transition system or if we don't know the safety specifications

09:28.680 --> 09:32.000
we can't use the reactive synthesis to generate a shield at all.

09:32.000 --> 09:33.560
So there is a trade-off.

09:33.560 --> 09:39.680
This approach is much more manual but what it enables us to do is guarantee more safety

09:39.680 --> 09:43.160
during the training and deployment phase.

09:43.160 --> 09:48.160
And yeah, thank you.

09:48.560 --> 09:54.560
Do we have any questions?

09:54.560 --> 09:57.920
Hi, thank you for the presentation.

09:57.920 --> 10:04.920
I had a quick question about the shielding aspect when we're basically blocking in action

10:05.080 --> 10:10.080
because we decided it's unsafe or the like.

10:10.080 --> 10:17.080
In this circumstance when the AI or the reinforcement learner is not able to expand in this direction

10:18.520 --> 10:24.520
because it's been blocked and it doesn't understand that part of the state space, what if down

10:24.520 --> 10:31.360
the line it ends up in a scenario where it like violates the safety because it came across

10:31.360 --> 10:33.480
it from a different trajectory?

10:33.480 --> 10:37.400
How would we be able to train it away from that?

10:37.400 --> 10:40.400
Because it's, I'm not sure if I explained it.

10:40.400 --> 10:44.320
Yeah, that's a good question.

10:44.320 --> 10:51.320
The agent can never violate the property if we specify the transition system correctly.

10:51.960 --> 10:56.560
If we make a mistake there or if we don't understand the Markov-District Process properly,

10:56.560 --> 10:59.720
that means if we don't know the transition function in the Markov-District Process for

10:59.720 --> 11:05.640
example, then obviously we'll get garbage and that's true of all formal methods.

11:05.640 --> 11:09.440
If you don't specify the transition model correctly or the specifications correctly,

11:09.440 --> 11:10.880
then you are not guaranteeing anything.

11:10.920 --> 11:15.760
So it's completely dependent on us having that knowledge and the capability of making

11:15.760 --> 11:17.040
that transition system.

11:17.040 --> 11:20.040
Got time for one more.

11:20.040 --> 11:23.040
There you go.

11:23.040 --> 11:30.040
Hi, in your example the rewards for falling in water of the environment was only minus

11:30.240 --> 11:36.400
one, but we're saying that's deadly, that's unsafe, we need to avoid it.

11:36.400 --> 11:41.400
Why are we not just addressing this by fixing the environment?

11:41.400 --> 11:47.400
Why don't we address this simply by fixing the environment?

11:47.400 --> 11:50.680
That's a good question.

11:50.680 --> 11:56.880
We can't modify the environment because the environment is not in our control.

11:56.880 --> 12:01.080
So the transitions will happen whether we want it or not.

12:01.080 --> 12:04.760
Now the question is how do we train the agents to avoid it?

12:04.760 --> 12:11.640
In this example, we punish the agent with minus 100 points and over time it will realize

12:11.640 --> 12:16.600
in the neural network for example that these trajectories involving water are just not

12:16.600 --> 12:18.920
worth it and it will start avoiding them.

12:18.920 --> 12:22.880
But we can't do anything about the environment, the environment is given and we can't modify

12:22.880 --> 12:23.880
it in any way.

12:23.880 --> 12:29.380
All right, y'all give Crispin another round of applause.

12:34.760 --> 12:35.760
Thank you.

12:35.760 --> 12:36.760
Thank you.

12:36.760 --> 12:37.760
Thank you.

12:37.760 --> 12:38.760
Thank you.

12:38.760 --> 12:39.760
Thank you.

12:39.760 --> 12:40.760
Thank you.

12:40.760 --> 12:41.760
Thank you.

12:41.760 --> 12:42.760
Thank you.

12:42.760 --> 12:43.760
Thank you.

12:43.760 --> 12:44.760
Thank you.

12:44.760 --> 12:45.760
Thank you.

12:45.760 --> 12:46.760
Thank you.

12:46.760 --> 12:47.760
Thank you.

12:47.760 --> 12:48.760
Thank you.

12:48.760 --> 12:49.760
Thank you.

12:49.760 --> 12:50.760
Thank you.

12:50.760 --> 12:51.760
Thank you.

12:51.760 --> 12:52.760
Thank you.

12:52.760 --> 12:53.760
Thank you.

12:53.760 --> 12:54.760
Thank you.

12:54.760 --> 12:55.760
Thank you.

12:55.760 --> 12:56.760
Thank you.

12:56.760 --> 12:57.760
Thank you.

12:57.760 --> 12:58.760
Thank you.

12:58.760 --> 12:59.760
Thank you.

12:59.760 --> 13:00.760
Thank you.

13:00.760 --> 13:01.760
Thank you.

13:01.760 --> 13:02.760
Thank you.

13:02.760 --> 13:03.760
Thank you.

