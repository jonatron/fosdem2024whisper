WEBVTT

00:00.000 --> 00:14.560
Hello. So, yeah, I'm Stefan Graber. I'm the project leader for Linux containers. And

00:14.560 --> 00:19.160
I'm just switching to the right screen here. There we go. And I'm one of the, one of the

00:19.160 --> 00:24.600
in-cast, in-cast maintainers. I was also the former project leader for LexD when I was

00:24.600 --> 00:31.920
working at Canonical. So, gonna go for a tiny bit of history first and then go into more,

00:31.920 --> 00:37.800
you know, what in-cast is and what can you do with it. So, the LXC project itself was created

00:37.800 --> 00:47.960
way back in August 2008 through IBM. That's the original Linux containers run time and was,

00:47.960 --> 00:52.480
has been used kind of everywhere, including the original version of Docker and some other places

00:53.160 --> 00:58.600
at that point. Linux containers itself was created, so the organization was created back in

00:58.600 --> 01:07.040
September 2014 and the LexD project got announced by Canonical in November 2014. Then LexD's been

01:07.040 --> 01:13.960
going on for a while until a lot of things happened in 2023. So, on July 4th Canonical announced

01:13.960 --> 01:18.680
that LexD was gonna be moved out of the Linux containers community project and moved into

01:19.520 --> 01:25.400
the Canonical organization itself. The next day we noticed that all non Canonical maintainers had

01:25.400 --> 01:31.080
lost all privileges on the repository. So, only Canonical employees were left maintaining it at

01:31.080 --> 01:40.600
that point. Then a few days later I left Canonical, so that happened. Then August 1st, Alex Astorai,

01:40.600 --> 01:48.640
who was the open SUSE package for LexD decided to go ahead and fork LexD as a new community

01:48.680 --> 01:54.680
project called InCus. A few days after that we made a decision to include InCus as part of the

01:54.680 --> 02:01.040
Linux containers project, effectively giving it the spot that LexD once had. InCus 0.1 was

02:01.040 --> 02:09.600
released on October 7th and we've had another four releases since then. Lastly, just as a bit of an

02:09.600 --> 02:16.800
early Christmas present, Canonical decided to go ahead and re-license LexD to AGPL as well as

02:16.920 --> 02:22.360
require everyone to sign a CLA to contribute to LexD. The consequence of that for us as a

02:22.360 --> 02:27.080
not-patry-to project is that we cannot look at anything happening in LexD anymore. We can't

02:27.080 --> 02:32.280
take any changes from LexD anymore, so InCus is effectively a hard fork at that point. So,

02:32.280 --> 02:39.080
that's the history. Now, to go back to what is this thing actually all about. So, InCus is a

02:39.080 --> 02:45.520
system container and virtual machine manager. It's image-based, so you've got a pretty large

02:45.600 --> 02:52.120
selection of distros. It's going to be a whole slide about it a bit later. But, yeah, it lets you

02:52.120 --> 02:58.400
effectively kind of cloud-like immediately create instances from any of those images. The system

02:58.400 --> 03:02.480
container part means that we run full Linux distributions. We don't run application containers,

03:02.480 --> 03:06.720
we don't run OCI right now, we don't do any other kind of stuff. The containers are really like a

03:06.720 --> 03:12.040
full Linux system that you then install packages in a normal way. Everything is built around a REST

03:12.080 --> 03:18.920
API with a pretty decent CLI tool. That REST API also has other clients who will go through that in

03:18.920 --> 03:25.280
a tiny bit. InCus got great support for resource limits, so you can pretty easily limit CPU memory,

03:25.280 --> 03:30.920
disk network, I or whatever you want. It's also got extremely good device pass-through to both

03:30.920 --> 03:35.680
containers and virtual machines, so you can do things like passing GPUs or attaching virtual TPMs

03:35.680 --> 03:41.600
or sharing your home directory or doing a whole bunch of different kind of sharing and passing

03:41.640 --> 03:46.880
through devices into containers and virtual machines. It also supports all of the expected stuff.

03:46.880 --> 03:51.200
I mean, it does snapshots, does backups, it's got a variety of networking options, a bunch of

03:51.200 --> 03:59.400
storage options, all of that stuff. It can also create projects as a way to group a bunch of

03:59.400 --> 04:04.240
instances together and effectively even open ID connect, which is cannot go to standard these

04:04.240 --> 04:10.480
days. And for authorization, we support OpenFGA, which is the open fine-grained access control

04:10.480 --> 04:16.960
project. That gets you, as the name implies, pretty fine-grained access control. There's also a

04:16.960 --> 04:21.600
number of web interfaces you can use on top of that. So here you've got one of those, which is

04:21.600 --> 04:28.400
actually the LexD web interface that runs perfectly fine on top of InCus. And yeah, that's one of

04:28.400 --> 04:35.440
the options there. As far as what you can run, well, there are a few options you can see up there.

04:35.760 --> 04:42.240
So InCus is indeed all based around images. We build images for pretty much all of the major

04:42.240 --> 04:50.000
Linux distros and even some of the not-so-major. And we build everything on both X86 and ARM.

04:50.720 --> 04:58.000
The vast majority of them are available for both container or VMs. We've got a number of them that

04:58.000 --> 05:03.520
are just for containers. And then because we do normal VMs, you can also run Windows, 3BSD,

05:03.520 --> 05:13.680
whatever else you want inside of the virtual machine. All right. So let's do a first quick demo

05:13.680 --> 05:24.000
of the standalone InCus experience. So if I switch over there, first thing we'll do is just launch

05:24.800 --> 05:37.360
an ARM Linux container. There we go. So we've got that. Then let's do another one for, let's do

05:37.360 --> 05:45.440
Alpine, the Edge release. So just do that. And this is obviously at risk of blowing up at any

05:45.440 --> 05:54.480
point because I'm on the first Wi-Fi. I think Ubuntu was planning on doing a VM.

05:55.840 --> 05:59.760
So let's do a VM instead of a container. So just tell it you want a VM instead.

06:00.640 --> 06:06.800
That's pretty much all that there is to it. And with that running, so we can see that the two

06:06.800 --> 06:10.240
containers already started, got their IPs and everything. The VM is still booting up, so it

06:10.240 --> 06:16.080
hasn't got its IP yet. It does now. If you want to get into any of them, you can just exact any

06:16.080 --> 06:21.600
commands. You can get a shell into Alpine. You can get a full bash inside of Arch. And you can do

06:21.600 --> 06:26.000
the exact same thing with the virtual machine. So like you don't need to get a console and

06:26.000 --> 06:29.920
log in and everything. Like there's an agent automatically in our virtual machines. You get

06:29.920 --> 06:36.160
to just immediately access them as if they're containers. So that works really well. You can

06:36.240 --> 06:43.440
create snapshots. So if you wanted to snapshot, the opposite snapshot creates the Arch one. If

06:43.440 --> 06:48.640
you don't give it a name, it just picks one for you. So we can see there's now a snapshot that we

06:48.640 --> 06:52.960
can restore or just keep around. There's also the ability to do automatic snapshots with a

06:52.960 --> 06:56.480
chron-type pattern with automatic snapshot expiry. You can do all that kind of stuff.

06:58.880 --> 07:04.880
Now let's create a custom storage volume. So we'll just do storage, volume, create, default.

07:07.120 --> 07:17.680
Let's call it demo. And then we're going to be adding that as a device to, let's say, Arch.

07:17.680 --> 07:24.960
So just call it demo. It's a disk. It comes from the default storage pool. And the volume is called

07:24.960 --> 07:35.760
demo. Configure this. There. And I forgot to do add. There. twice add. Now if we go inside of

07:35.840 --> 07:40.880
that VM, again, we see there's a new entry there. And then empty home. Hey, that's my home die tray.

07:43.440 --> 07:46.720
So that's very nice and easy. It's kind of doing automatically,

07:46.720 --> 07:52.160
VIRTA, UFS, 9p, all that kind of stuff. It talks to the agents to trigger the mounts. And it just,

07:52.160 --> 07:56.720
like our goal is for virtual machines to feel like containers in, like as much as we can.

07:56.720 --> 07:59.440
And having that agent in there really makes that super easy.

08:00.160 --> 08:08.240
And for the last party trick of this demo, let's do launch images. Open suzer,

08:08.240 --> 08:16.880
tumbleweed, desktop KDE as a desktop image. And also tell it that I want to see the VGA console

08:16.880 --> 08:21.920
as soon as it starts. So when I do that, it actually gets me a second window, which I need to drag

08:21.920 --> 08:27.520
over here. And let's try full screen that thing. Maybe. Yeah, full screen doesn't work. Okay.

08:27.760 --> 08:35.280
But we can see it boot. And it's going to get us eventually into a KDE session. Not sure

08:35.280 --> 08:40.720
where the resize didn't work. Oh, okay. Maybe the desktop where? I saw a mouse pointer that was

08:40.720 --> 08:48.240
about the right size. Nope. Okay. So it is starting KDE there. So we even have some desktop images.

08:48.240 --> 08:52.480
We've got an arch desktop image with GNOME. We've got Ubuntu with GNOME. And we've got open

08:52.560 --> 08:59.040
suzer with KDE. We're not building too many more of them mostly because they're actually very

08:59.040 --> 09:04.080
expensive to build as far as like resource, like the build time and distributing pretty large images.

09:04.640 --> 09:08.400
But it's to show that this works. And if you want to run your own, you can totally do that.

09:09.440 --> 09:16.240
All right. So let's just go back to slides. Come on.

09:17.120 --> 09:18.960
There we go.

09:22.080 --> 09:29.840
So other things you can do as this thing is effectively your own local tiny cloud and it's

09:29.840 --> 09:34.320
all built on rest API. It's what it also makes it very easy to integrate with other things. And

09:34.320 --> 09:39.600
other things here mean some of the pretty usual tools you might be dealing with. So Terraform,

09:39.600 --> 09:44.880
OpenTofu, you can integrate with that very easily. We've got a provider to maintain ourselves that

09:44.960 --> 09:51.200
you get to use. Encebal has got a connection plugin that you can use to deploy any of your playbooks

09:51.200 --> 09:56.880
directly against virtual machines or containers. And if you want to build your own images as

09:56.880 --> 10:01.920
derivatives of ours, you can use Packer as a very easy way to take our images and inject whatever

10:01.920 --> 10:06.240
stuff you want in there. There are a bunch of other tools. I mean, LexD especially had a lot

10:06.240 --> 10:10.480
of third-party tools that could integrate with it. A bunch of those are now migrating over to

10:10.480 --> 10:15.360
InCurse or supporting both. So that's very, it's a list that's very rapidly growing.

10:17.200 --> 10:24.080
Other things you can do, well, InCurse exposes an open metrics endpoint to get the details like

10:24.080 --> 10:29.920
the resource consumption and usage and all that kind of stuff of all of the instances running on it.

10:29.920 --> 10:34.160
So you can integrate that with Prometheus to script that data and keep it on the side.

10:35.040 --> 10:42.640
It also supports streaming, logging and audit events to Gryffina low key. So you get to effectively

10:42.640 --> 10:48.080
have your events and your metric in the same spot at which point you can use the dashboard

10:48.080 --> 10:52.720
that we've got in the Gryffina store to get something like that and run on Intel. So that's

10:52.720 --> 10:58.400
pretty convenient as well. If you don't like typing the name of your remote every time, you can switch

10:58.400 --> 11:04.160
to a remote. So you just do a remote switch at which point if I do a list, it goes straight

11:04.160 --> 11:10.560
to that remote and you don't need to type it every single time. That cluster is actually using

11:11.520 --> 11:21.200
a mix of local storage and remote storage. So it's got CF for HDD SSDs and it's got a local

11:22.560 --> 11:28.080
ZFS storage pool as well. And on the network side, it uses oven. So it actually has all of the

11:29.360 --> 11:34.720
stuff in place. And actually if we look at the remote list from earlier, we can see that it uses

11:34.720 --> 11:41.600
OIDC for login. So it's also using authentication bits I mentioned. Now if you wanted to launch,

11:41.600 --> 11:49.600
say, the BN12 instance on that thing, you can do it the perfectly normal way. And that's just

11:49.600 --> 11:55.440
going to instruct the cluster to go and do it. So in this case, thankfully it's running back home

11:55.440 --> 11:59.840
with very fast internet. So I don't need to wait for the first Wi-Fi to download stuff for me.

12:01.600 --> 12:05.600
But it's actually downloading the image and parking it creating the volume on-safe in this case and

12:05.600 --> 12:10.640
then starting the instance. I didn't even tell it whatever I wanted it on. So it just picked

12:10.640 --> 12:16.160
wherever it made sense, which is actually funny because if you use an image and you don't specify

12:16.160 --> 12:20.960
what architecture you want, you're going to get one of the architectures. So in this case, I didn't

12:20.960 --> 12:25.520
tell it I wanted ARM or Intel. There was more capacity on ARM, so I got an ARM instance.

12:27.920 --> 12:32.000
We can go and check that easily. But I know that the server it picked in that list is an ARM

12:32.000 --> 12:43.280
server. So if I go in here and look at the architecture, it's AR64. All right. Let's just look at things here.

12:44.000 --> 12:53.200
And I wanted to just show the dashboard as well. I'm just going to drag that particular window over.

12:54.400 --> 13:01.520
Where is it? It is here. I had it open. I've got way too many windows open on my laptop.

13:02.480 --> 13:14.960
Okay. So it's Grafana. It's loading. It's loading. And in this dashboard. Okay. I'm just making sure it

13:14.960 --> 13:24.320
looks at the right cluster before I show it to you. So there we go. Yeah. So this is actually the

13:24.320 --> 13:29.280
dashboard for the very first I was talking to. So it's SHF, the one I was showing. It's looking

13:29.280 --> 13:33.760
at the demo project. So we can see the top offenders as far as resource usage and that kind of

13:33.760 --> 13:40.160
stuff. We can look at graphs for network, for storage. And we can even kind of go down on the

13:40.160 --> 13:45.520
specific instances and see what they've been doing. So you could expand an instance and go look at

13:45.520 --> 13:51.120
its usage. It also gets all of the events from Loki. So we can see the instance creation and

13:51.120 --> 13:57.280
any comments like that. That shell I got is actually right here. And any error and stuff is also all

13:57.280 --> 14:06.000
captured right there. So that's the metric side of things. All right. So where do you get to run

14:06.000 --> 14:13.520
this thing? Well, quite a few distros have packages now for Incus as well as I meant, as I've mentioned,

14:13.520 --> 14:18.320
Devenin Lubuntu without packages in their next table release. We're also looking at doing a long

14:18.320 --> 14:24.640
term support release for Incus itself. Right now you might see version numbers like 0.4, 0.5 and be

14:24.640 --> 14:31.200
a bit scared about it. You need to remember that this is a derivative of Lexday. So one of our

14:31.200 --> 14:35.600
zero point release is just as stable if not more stable and like a five point something on the Lexday

14:35.600 --> 14:42.240
side. We've just not done anything past zero because we're waiting for the LTS of our other projects

14:42.240 --> 14:47.200
within the next containers, which we will do in March. And that's going to be the LTS of LXC,

14:47.200 --> 14:52.560
like CFS and Incus all at the same time. And we usually try to line up versions. So Incus is going

14:52.640 --> 14:58.640
to jump straight from 0.6 probably straight to 6.0. That's what's going to happen with the LTS.

15:01.040 --> 15:06.880
As far as other features we're looking at adding, with the release of Linux 6.7, we now have Bicash

15:06.880 --> 15:13.440
FS in the Linux kernel. And it's pretty interesting for us on the Incus side because it's very close

15:13.440 --> 15:19.200
to what ZFS or the RFS does, which we already support. So we're looking at adding a Bicash FS

15:19.200 --> 15:24.880
storage driver for people who want to start using that. On the cluster side, I mentioned that we

15:24.880 --> 15:31.200
support Cef right now, which is a pretty good option, but also a bit heavyweight. A bunch of people

15:31.200 --> 15:36.640
could instead do something different, whether it's like using a shared NVMe of a fabric drive or using

15:37.440 --> 15:42.800
some old fiber channel sand they might have gotten on eBay or something like that. So we're looking

15:42.800 --> 15:48.720
at adding distributed LVM as a storage driver, which effectively means if you have multiple systems

15:48.720 --> 15:55.680
that can all see the exact same block device somehow, then you can use LVM on top of that with a

15:55.680 --> 16:00.640
distributed locking manager on top so that all of the different machines in the cluster get to use

16:00.640 --> 16:05.520
that. So that kind of solves the issue of like how do I use my old sand at work or something else,

16:05.520 --> 16:11.200
you can use that. But it can also work in some other cases. I think someone is looking at using

16:11.200 --> 16:17.440
that with DRBD, for example, as an option. We are looking at adding OCI application container support.

16:17.440 --> 16:22.080
So that's potentially a bit of a surprise for some folks. But we feel that these days,

16:23.600 --> 16:28.560
like the application container space has now stabilized enough and we've got enough of our

16:28.560 --> 16:34.240
users who literally just run, like for some reason are running Docker inside of InCast to run a few

16:34.240 --> 16:39.520
specific applications that this particular use case we could support natively. So we're not looking

16:39.520 --> 16:44.800
at like competing with Kubernetes with all of the service mesh, this auto distribution thing.

16:44.880 --> 16:50.400
Like that's crazy stuff. They get to do that. But we would like it to be possible for you to run

16:50.400 --> 16:55.760
like two or three small containers for like your IoT software or whatever. That's kind of what we're

16:55.760 --> 17:03.040
looking at doing there. And on the networking side, we're using OVEN for distributed networking,

17:03.040 --> 17:09.040
which works pretty well. But we're also working now on another feature of OVEN which is called

17:09.040 --> 17:14.640
Interconnects, which then allows for having multiple clusters and then interconnect to the

17:14.640 --> 17:20.560
network. So you can have instances on multiple networks, on multiple clusters, and then connect

17:20.560 --> 17:25.360
those together and can direct them. And you've got 30 minutes with InCast pre-installed in there to

17:25.360 --> 17:29.120
just take it for a ride, play with it for a bit, see if that's something that's interesting to you

17:29.120 --> 17:34.880
and if it is, then you can go and install it for yourself. And that's it. We can try some questions.

17:34.880 --> 17:40.560
We've seen it's a bit difficult. So please, everyone remain quiet if there's any questions. So we

17:40.560 --> 17:45.280
can try and hear them. Is there anything? Oh, you have it there. Okay.

17:48.000 --> 17:55.360
So I'm quite sure some people are interested with the differences from the end there and this too.

17:55.840 --> 18:08.000
So compared to what? Sorry, I didn't catch that part.

18:11.840 --> 18:15.840
Oh, VMware. Okay. Well, so it's a lot cheaper.

18:16.560 --> 18:29.040
Yeah, for anyone who's using VMware professionally and has followed the news recently,

18:29.920 --> 18:38.400
let's say your VMware build is not great right now. So this is a viable alternative in many cases.

18:38.400 --> 18:44.080
It doesn't do, it doesn't have all 50,000 components all around it and all that kind of stuff.

18:44.080 --> 18:48.640
But if you are primarily using it as a way to get a cluster, create a bunch of VMs, maybe create

18:48.640 --> 18:53.840
some containers, run whatever OS you want on there, that will do it just fine. So it's definitely

18:53.840 --> 18:57.520
an option there. I mean, it's kind of in the same vein at that point as compared to like,

18:57.520 --> 19:01.920
you know, a Proxmox or some of those other options, it will work just fine. With the

19:01.920 --> 19:06.880
FireTact, we do have like, it's not a distribution you can install it on any system you want.

19:07.520 --> 19:13.360
It's obviously all open source and yeah, it is a pretty viable alternative and we do have a lot

19:13.360 --> 19:20.160
of people who are using VMware that are very closely looking at this as a potential way out of VMware right now.

19:23.600 --> 19:28.640
So the question here, better understanding terminology, would the LNs find a backdoor

19:28.640 --> 19:33.200
between a system container and a location container? Yeah, so the difference between

19:33.200 --> 19:38.000
application containers and system containers is a system container will run like a full Linux

19:38.000 --> 19:41.840
distro. It will run system day, it's going to have Udev running, it's going to, you'll be able to

19:41.840 --> 19:47.120
access it into it, install packages, reboot it. It's really designed to be like a state full

19:47.120 --> 19:52.960
long running type thing. Whereas your application container is usually, I mean, ideally single

19:52.960 --> 19:57.600
process or a process of some of its children, it's really more designed around delivering a

19:57.600 --> 20:02.000
specific application and most often it's going to be quite stateless with the idea that you can

20:02.000 --> 20:08.960
just nuke the thing and replace it at any point. It's kind of two different concepts. Like some

20:08.960 --> 20:13.440
people like the idea of having a system that they actually get to select what packages

20:13.440 --> 20:18.960
installed exact config and stuff and some people prefer not to care about any of that and just

20:18.960 --> 20:22.080
have something pre-installed and that's what an application container gets you.

20:23.200 --> 20:27.440
That's why having the ability to run some application containers directly on InCus

20:27.440 --> 20:32.640
alongside the system containers I think will be quite interesting because if you just, like if

20:32.640 --> 20:36.800
for a specific application it's easier to just get their pre-made thing then you'll be able to do that

20:37.440 --> 20:39.280
while still being able to run everything else.

20:47.440 --> 20:52.960
Yep, so we do have a bash completion profile. I absolutely hate shell completion for some reason,

20:52.960 --> 20:55.040
so I don't have it on my machine so I can't show you.

20:55.120 --> 21:03.120
System containers provide the ones that are interested in the rights?

21:05.280 --> 21:11.520
Yeah, I mean it is possible to get application container run times to get you a full system

21:11.520 --> 21:15.520
container. I mean nothing prevents you from deciding that the application you run in the

21:15.520 --> 21:21.360
container has been in it. That's definitely possible. It's just not what they were really

21:21.440 --> 21:27.360
meant for so there's a bunch of kind of, it just feels kind of less polished because that's not,

21:27.360 --> 21:34.800
that wasn't their goal. Like things like being able to dynamically pass new files in and dynamically

21:36.240 --> 21:41.920
attach devices, get whatever number of shells you want, be able to interact with the outside

21:41.920 --> 21:46.000
words through like a Unix socket inside of there. Those kind of things don't make too much sense

21:46.080 --> 21:50.960
for application containers just at the beginning and so some of those features will probably

21:50.960 --> 21:58.640
be lacking on that side. I tend, I mean, I was going to say like I usually like, you know,

21:58.640 --> 22:04.240
having one tool for the job and like picking the right tool for the job and effectively if you

22:04.240 --> 22:07.760
really care about running a bunch of application containers use one of the application container

22:07.760 --> 22:12.960
run times whether it's using Podman, Docker or some of those others. One thing that's actually

22:12.960 --> 22:17.200
interesting is that you can totally run Docker or Podman inside of an InCast container.

22:18.000 --> 22:23.760
So that works. You can run your normal Ubuntu, Debian or whatever in Existio inside of an InCast

22:23.760 --> 22:28.640
container and then install Docker or Podman in there and run some containers alongside whatever

22:28.640 --> 22:32.960
else you might be doing in that container. So that's something that works fine. I think we're

22:32.960 --> 22:37.920
probably out of time at this point. So thanks a lot everyone. I'm probably going to just be outside

22:37.920 --> 22:48.880
for a tiny bit if anyone has more questions and things. But yeah, thanks a lot.

