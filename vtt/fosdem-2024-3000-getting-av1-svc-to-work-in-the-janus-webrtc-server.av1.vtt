WEBVTT

00:00.000 --> 00:08.880
Well, welcome everybody.

00:08.880 --> 00:12.120
Lorenzo here needs no introduction.

00:12.120 --> 00:17.760
He brought the crazy contraption to give his presentation with.

00:17.760 --> 00:19.760
It's almost a dangerous demo in and of itself.

00:19.760 --> 00:20.760
Yeah, yeah, easy.

00:20.760 --> 00:25.120
And he'll be telling us all about AV1 as we see.

00:25.120 --> 00:26.880
Let's go for it.

00:27.880 --> 00:29.880
Yeah, you can hear me, right?

00:29.880 --> 00:30.880
Yes, sir.

00:30.880 --> 00:32.880
So thanks so for the introduction.

00:32.880 --> 00:35.880
Yeah, so I'll be talking about specifically AV1 as we see.

00:35.880 --> 00:38.880
I'll go, it's in some technical details.

00:38.880 --> 00:40.880
So it may be boring here and there,

00:40.880 --> 00:45.880
but I really think it's important in order to get a better understanding of how it all works.

00:45.880 --> 00:48.880
And this is just a quick introduction over me.

00:48.880 --> 00:53.880
So I'm one of the co-founders of a small company based in the south of Italy called Miteco.

00:53.880 --> 00:56.880
I'm the main author of Janus, which is an open source for Bouticy server.

00:56.880 --> 01:00.880
And there are some links if you want to get in touch with me or learn more.

01:00.880 --> 01:03.880
And basically what we'll be talking about today is AV1.

01:03.880 --> 01:06.880
And if you're not familiar with what AV1 is, it's a new,

01:06.880 --> 01:11.880
relatively new video codec that was designed within the context of the Alliance for Open Media.

01:11.880 --> 01:13.880
That has a lot of companies behind it.

01:13.880 --> 01:16.880
There's Apple, Cisco, Google, really a ton of them.

01:16.880 --> 01:20.880
And what they really wanted to do was to create an open and royalty free video codec.

01:20.880 --> 01:26.880
And of course emphasis on open and royalty free because we don't want another H264 or H265,

01:26.880 --> 01:33.880
which was specifically designed for real time applications pretty much like Opus was also designed as a codec for the internet.

01:33.880 --> 01:39.880
So that was quite important innovation with support for higher resolution, so for KM Beyond.

01:39.880 --> 01:46.880
And most importantly, it was also conceived to have support for SVC baked in the codec specification itself.

01:46.880 --> 01:50.880
And that's quite important because some other codec support SVC as well,

01:50.880 --> 01:54.880
but many times they come as, let's say, later additions.

01:54.880 --> 01:57.880
So basically codecs are extended to have SVC supported.

01:57.880 --> 02:01.880
In this case, AV1 was conceived with native support for SVC.

02:01.880 --> 02:07.880
So all AV1 implementations are supposed to at least be able to decode an SVC stream, for instance,

02:07.880 --> 02:12.880
which is important when you start working in hardware decoders and stuff like this.

02:12.880 --> 02:18.880
And of course this got me and should get you all very interested because these are all very interesting features

02:18.880 --> 02:21.880
to have for different reasons in WebRTC.

02:21.880 --> 02:25.880
And SVC is important for a few different reasons.

02:25.880 --> 02:27.880
So we all know what CIML Cast is.

02:27.880 --> 02:32.880
You use a single M line to basically carry multiple quality streams,

02:32.880 --> 02:36.880
like you have a high, medium and low quality stream, both sent at the same time,

02:36.880 --> 02:41.880
so that different qualities can be distributed to different participants as needed.

02:41.880 --> 02:45.880
But with CIML Cast, each stream is encoded as a separate stream,

02:45.880 --> 02:50.880
which means that each stream is also decoded independently of others.

02:50.880 --> 02:55.880
But this does mean that you have to encode the same stream more than once.

02:55.880 --> 03:00.880
And the fact that they are decoded independently can also cause some challenges sometimes.

03:00.880 --> 03:04.880
With SVC instead, you still use the same media source, the same M line and so on,

03:04.880 --> 03:09.880
but the different qualities, so high, medium, low, whatever it is,

03:09.880 --> 03:11.880
are all layers of the same thing.

03:11.880 --> 03:16.880
So you have a single video stream that has like an onion, different layers,

03:16.880 --> 03:21.880
that basically make each layer provides more detail if you want to look at it that way.

03:21.880 --> 03:24.880
And so the key difference between CIML Cast and SVC is that with CIML Cast,

03:24.880 --> 03:28.880
since you have different streams, you also have different SSRCs.

03:28.880 --> 03:31.880
Each quality is a separate RTP stream.

03:31.880 --> 03:35.880
With SVC, all layers are the same SSRCs.

03:35.880 --> 03:39.880
So as far as the recipient is concerned, it's just a single stream,

03:39.880 --> 03:43.880
which means that it does require less bandwidth because you can pack some things up

03:43.880 --> 03:45.880
and it's more a layer kind of approach.

03:45.880 --> 03:51.880
It is sometimes more CPU intensive in terms of encoding because that's a bit more tricky,

03:51.880 --> 03:55.880
but it does have some advantages over CIML Cast as a consequence of that.

03:55.880 --> 04:00.880
And an interesting aspect is that CIML Cast, as we know it in WebRTC today,

04:00.880 --> 04:03.880
actually did already make use of SVC somehow,

04:03.880 --> 04:08.880
because when we say, for instance, BPA to CIML Cast, and then we mention temporal layers,

04:08.880 --> 04:11.880
temporal layers are not a feature of CIML Cast.

04:11.880 --> 04:13.880
Temporal layers are a feature of SVC.

04:13.880 --> 04:20.880
So we are basically using a feature of VPA that allows us to use a partial SVC functionality

04:20.880 --> 04:30.880
where we can have different frame rates within over the same RTP stream that we are handling.

04:30.880 --> 04:33.880
And this is just summarizing it from a visual perspective.

04:33.880 --> 04:37.880
So you have CIML Cast sending three different streams and then we can choose which,

04:37.880 --> 04:41.880
an SFU in the middle can choose which stream to send to other participants.

04:41.880 --> 04:45.880
With SVC, we have one big thing that has many layers.

04:45.880 --> 04:51.880
One participant may want to receive them all, another participant may only want to receive the medium layer,

04:51.880 --> 04:54.880
and then another participant may want to receive the lowest layer as possible.

04:54.880 --> 04:58.880
This is just to give you an idea from a visual perspective instead.

04:58.880 --> 05:01.880
And so I was very interested in implementing it in Janus,

05:01.880 --> 05:05.880
and here are a few links if you want to learn more about Janus itself.

05:05.880 --> 05:10.880
And so I started to figure out what we needed to do in terms of what do I need to do in order to get that working.

05:10.880 --> 05:16.880
And so first of all, of course, we need a way to negotiate AV1 and the SDP, and that's of course a given.

05:16.880 --> 05:21.880
It may be helpful also to be able to detect keyframes in the stream,

05:21.880 --> 05:23.880
and that may be helpful for different reasons.

05:23.880 --> 05:29.880
For instance, when you are doing Siemulcast as a server, it helps when you know whether a packet is a keyframe or not,

05:29.880 --> 05:33.880
especially if you want to switch on a keyframe or stuff like this.

05:33.880 --> 05:44.880
It's also important to be able to somehow interpret how the AV frames are spread across RTP packets,

05:44.880 --> 05:47.880
and for us it's especially important for our recordings,

05:47.880 --> 05:51.880
because when we record stuff in Janus, we just record all the RTP packets that we received,

05:51.880 --> 05:53.880
so that we can go through them later on.

05:53.880 --> 06:00.880
And so basically getting a recording in a playable format just means reorder all these RTP packets I received,

06:00.880 --> 06:07.880
get the AV1 frames out of those RTP packets, and then put them into an mp4 file to make an example.

06:07.880 --> 06:13.880
And this means that we need to know how AV1 fits within RTP, and we show how that works later.

06:13.880 --> 06:20.880
For SVC specifically, there is another important thing that is called the dependency descriptor that I'll talk about in a minute.

06:20.880 --> 06:24.880
And so that means that we also need to somehow support that in the server as well,

06:24.880 --> 06:30.880
which first of all means negotiating it, or extensions must be negotiated in order to be used.

06:30.880 --> 06:33.880
We need to know how to parse an extension of that sort,

06:33.880 --> 06:38.880
and then we need to figure out how to use the information that we receive in that extension.

06:38.880 --> 06:43.880
And as we'll see, 0.5 is the one that got me the most in trouble, and then I'll explain later why.

06:43.880 --> 06:46.880
But starting from negotiation is very easy,

06:46.880 --> 06:52.880
so you just negotiate the codec name and the relatively clock rate there, so that's easy.

06:52.880 --> 06:59.880
Detecting keyframes and support m basically being able to extract frames from packets is a bit more complicated,

06:59.880 --> 07:03.880
but that's because we need to start delving a bit deeper,

07:03.880 --> 07:07.880
and so figure out how AV1 is packetized over RTP.

07:07.880 --> 07:10.880
And that's actually something that's true for all codecs.

07:10.880 --> 07:13.880
So for all codecs, you need packetization rules,

07:13.880 --> 07:18.880
and that's especially true for video, because for video, typically you have larger frames,

07:18.880 --> 07:20.880
and RTP packets cannot be that large.

07:20.880 --> 07:23.880
They are usually limited by the MTU size and so on.

07:23.880 --> 07:28.880
And so you need to have some rules that tell you if you have a frame that is this large,

07:28.880 --> 07:33.880
this is how you split it across multiple RTP packets for this codec, this codec, and this other codec.

07:33.880 --> 07:38.880
And usually there are some similarities, but usually each codec has its own rules,

07:38.880 --> 07:42.880
mostly because of the nature of the bit stream, let's see.

07:42.880 --> 07:48.880
And this is an activity that typically the ITF carries on in the AVT core working group,

07:48.880 --> 07:55.880
because basically all packetization rules as RTP and WebRTC are all standards.

07:55.880 --> 08:00.880
Unfortunately for AV1, it did not happen in the ITF, so they came up with their own specification,

08:00.880 --> 08:02.880
which is provided here.

08:02.880 --> 08:07.880
So in this specification, they provide information both on the AV1 aggregation header,

08:07.880 --> 08:09.880
that is those packetization rules that I mentioned.

08:09.880 --> 08:13.880
So how do I split an AV frame over multiple RTP packets,

08:13.880 --> 08:19.880
and how do I get that same frame back when I have access to the RTP packets on the other side?

08:19.880 --> 08:27.880
And it also talks in great detail about this dependency, the scripture, which is a beast of its own, as you can see.

08:27.880 --> 08:31.880
And this is basically how it looks like from a visual perspective.

08:31.880 --> 08:36.880
So with RTP, you typically have an RTP header with all the usual stuff that you all know.

08:36.880 --> 08:42.880
You can have some RTP extensions in there, and this is where the new RTP extension would appear.

08:42.880 --> 08:44.880
And then you have the RTP payload.

08:44.880 --> 08:48.880
And the RTP payload is where this aggregation header plays a role,

08:48.880 --> 08:53.880
because as we mentioned, we cannot just dump an AV frame in there because it may not fit.

08:53.880 --> 08:58.880
And so we need to have some sort of information that tells us how an AV frame is actually split,

08:58.880 --> 09:04.880
or if there are more than one AV frame in the same packet, we need to know that as well.

09:04.880 --> 09:08.880
And the AV aggregation header, the AV1 aggregation header is fairly simple,

09:08.880 --> 09:12.880
because it's just a single byte with a few bits that you can set.

09:12.880 --> 09:18.880
Like, I will not go too much into the detail, not to bore you, but just information about whether these OBO,

09:18.880 --> 09:21.880
and the OBO is basically the equivalent of an AL for AV1.

09:21.880 --> 09:27.880
So if you know what an AL is for RAS264, an OBO is the same thing for AV1, more or less.

09:27.880 --> 09:30.880
So it's basically a unit of a frame.

09:30.880 --> 09:35.880
And then basically these attributes tells you whether or not an RTP packet that you just receive

09:35.880 --> 09:40.880
is a continuation from a previous frame, so that you know that whatever you're receiving now,

09:40.880 --> 09:46.880
you have to append to whatever buffer you had before, whether or not this frame is complete or not,

09:46.880 --> 09:51.880
whether you have to actually wait for something else before passing it to the decoder.

09:51.880 --> 09:55.880
You may have some information about how many OBOs are in place, which is actually optional,

09:55.880 --> 09:57.880
and we'll see why in a second.

09:57.880 --> 10:03.880
And then this bit tells you whether this is the packet that you receive is the beginning of an AV frame,

10:03.880 --> 10:09.880
which is, again, all of these pieces are very important when you have to reconstruct the AV frame when you receive it,

10:09.880 --> 10:14.880
so that AV1 frame when you receive it, so that you know that this is the first thing that you have to put in there,

10:14.880 --> 10:21.880
then you pass this year, this year, this year, eventually you again end up with the complete AV frame.

10:22.880 --> 10:25.880
And basically it looks a bit like this, so in this case, for instance,

10:25.880 --> 10:29.880
we are actually aggregating multiple OBOs in the same RTP packets,

10:29.880 --> 10:33.880
and in this case we are not specifying that there are that many elements,

10:33.880 --> 10:37.880
which means that for each OBO in there, after the aggregation header,

10:37.880 --> 10:42.880
we have a variable size element that tells us how long each OBO is,

10:42.880 --> 10:46.880
so in this case we're just going sequentially, aggregation header, we know there are some packets,

10:46.880 --> 10:50.880
we check the size, then we read exactly this amount of bytes,

10:50.880 --> 10:54.880
and this is the first element, second element we read the size of that,

10:54.880 --> 10:56.880
and we go on and go on and go on.

10:56.880 --> 11:04.880
And the W attribute over here allows us to save a tiny bit of space when you use it,

11:04.880 --> 11:09.880
because if you say that, for instance, there are just two OBOs in this element,

11:09.880 --> 11:14.880
then this means that you do need to provide the size of all the elements except the last,

11:14.880 --> 11:18.880
because then you can read them sequentially by checking the variable size length

11:18.880 --> 11:20.880
until you get to a certain point.

11:20.880 --> 11:24.880
When you get to the last element, you know that all the bytes that are left

11:24.880 --> 11:30.880
are actually associated to that frame, so you don't need that additional variable element in there,

11:30.880 --> 11:35.880
so you save a bit of data, maybe not that much, but in some cases it may be helpful.

11:35.880 --> 11:39.880
And to use the aggregation header, I mean I mentioned that it can be helpful

11:39.880 --> 11:41.880
in a few different cases.

11:41.880 --> 11:45.880
In my specific use case, I basically interpreted that, for instance,

11:45.880 --> 11:51.880
not a continuation and a first packet, I can more or less treat as a key frame.

11:51.880 --> 11:57.880
It's, of course, not really always like that, but it at least gives me the beginning of something,

11:57.880 --> 12:02.880
which is something that is very quick and simple to use when you're actually just routing stuff.

12:02.880 --> 12:05.880
You just read a single byte and make some decisions based on that.

12:05.880 --> 12:10.880
For instance, when you need to do some symbol-cast-related switches, for instance.

12:10.880 --> 12:14.880
For recordings, I needed to do something more complex, because as I mentioned,

12:14.880 --> 12:20.880
we need to traversal the RTP packets, reconstruct an obu frame, and an ap1 frame

12:20.880 --> 12:24.880
before we can put it into an mp4 packet, which means that I had to actually implement

12:24.880 --> 12:28.880
all that de-packetization rules accordingly.

12:28.880 --> 12:33.880
And also I had to implement the parsing of a specific obu in order to get some additional information,

12:33.880 --> 12:37.880
like the video resolution, because if I'm creating an mp4 frame,

12:37.880 --> 12:41.880
I don't need to decode the frames, but at least I do need to know how large it is

12:41.880 --> 12:46.880
so that I can put it into the mp4 header, for instance, or maybe use the RTP headers

12:46.880 --> 12:49.880
to figure out roughly the frame rate, these sort of things.

12:49.880 --> 12:55.880
And all that I've mentioned so far is really all that you need if you want to use everyone normally,

12:55.880 --> 12:59.880
just as a regular codec, so we forecast all streams are independent of each other.

12:59.880 --> 13:06.880
So if I want to go from high to low, I can just move to the SSRC with the low quality stream,

13:06.880 --> 13:11.880
and I don't need to do anything else. The low quality stream is encoded separately from that other one.

13:11.880 --> 13:16.880
I don't need to know anything about that other stream, they're completely independent.

13:16.880 --> 13:20.880
With SSRC, that's not always true, because you may have some dependencies in place.

13:20.880 --> 13:27.880
So if I want to go from, for instance, the highest quality layer, since we are talking about an onion,

13:27.880 --> 13:33.880
will very much likely depend on one or more packets from the medium layer and the low layer,

13:33.880 --> 13:38.880
which means that I may have to forward those two, otherwise the high quality layer will not work,

13:38.880 --> 13:41.880
because that alone is not enough to decode something.

13:41.880 --> 13:46.880
And these are all things that you need to figure out at runtime, because you have a stream that is coming in

13:46.880 --> 13:50.880
and you have to make a decision right away, otherwise you cause delays and stuff like this.

13:50.880 --> 13:55.880
And most importantly, most of the times you may not even be able to parse the payload,

13:55.880 --> 13:59.880
because, for instance, if insertable streams are used and the stream is end-to-end encrypted,

13:59.880 --> 14:03.880
you cannot have a look at the payload to see what is what.

14:03.880 --> 14:06.880
And this is what the dependency descriptor is for.

14:06.880 --> 14:11.880
The idea that you have an external component, so an RTP extension,

14:11.880 --> 14:15.880
that contains all the information related to the packet that you just received.

14:15.880 --> 14:19.880
And this one would not be encrypted as the payload itself,

14:19.880 --> 14:24.880
and so it's something that an intermediary like an SFU can use to do something.

14:25.880 --> 14:30.880
And this is just one example that comes from the RTP specification over there.

14:30.880 --> 14:32.880
There are really a ton of examples.

14:32.880 --> 14:37.880
In this case, this is an example of how L2 T3 dependencies work.

14:37.880 --> 14:44.880
L2 T3 means two different spatial layers that depend on each other and three temporal layers.

14:44.880 --> 14:48.880
So two video resolutions and maybe 30, 20, 10 frames per second.

14:48.880 --> 14:53.880
And this gives you an idea of how the dependencies work as a frame goes by.

14:53.880 --> 14:57.880
So this is the first frame, second, third, fourth, and so on and so forth.

14:57.880 --> 15:00.880
And so you'll see that in this specific kind of approach,

15:00.880 --> 15:05.880
the first packet you'll receive will be related to spatial layer zero, temporal layer zero.

15:05.880 --> 15:09.880
And pretty much everything depends on this packet over here.

15:09.880 --> 15:14.880
And then if I want spatial layer one and temporal layer zero,

15:14.880 --> 15:20.880
I definitely need to relay this packet to otherwise this one will not be able to be decoded.

15:20.880 --> 15:26.880
If I'm interested and basically you follow the arrows and you have an idea of the kind of dependencies that you can do

15:26.880 --> 15:31.880
so that you can choose which packets you can actually drop or not.

15:31.880 --> 15:35.880
And as you can guess, the problem is, as an SFU, how do I know these?

15:35.880 --> 15:40.880
So how do I know that this is what is happening and these are the dependencies that are in place?

15:40.880 --> 15:45.880
And this is basically what the dependency the scripture provides and I'll explain how in a second.

15:45.880 --> 15:48.880
And so continuing from the requirements that I described before,

15:48.880 --> 15:52.880
it means that if I wanted to have SAP or for this component in Janus,

15:52.880 --> 15:58.880
but this is true for each web artist is around there, again, I need a way to negotiate the extension.

15:58.880 --> 16:04.880
I need to somehow parse it so I don't, I need to know how it is encoded so that I can figure out what is in there.

16:04.880 --> 16:06.880
And then I need to find a way to use it.

16:06.880 --> 16:10.880
So for instance, to recover those dependencies there.

16:10.880 --> 16:14.880
And I thought that negotiation was supposed to be the easy part,

16:14.880 --> 16:21.880
but it's actually not that easy because of course you just need to negotiate that extension with that name as an additional X map.

16:21.880 --> 16:24.880
That's how it works for all extensions in the SDP.

16:24.880 --> 16:31.880
But it turned out that I also needed to support the so-called two byte header extensions using X map allow mixed.

16:31.880 --> 16:36.880
And this is because RTP extensions by default are supposed to be quite small.

16:36.880 --> 16:43.880
And so you usually have the so-called one byte header RTP extension where in one byte you provide some information,

16:43.880 --> 16:47.880
which means though that the length of the extension is limited as well.

16:47.880 --> 16:55.880
So since you are using one byte to convey a lot of information, the size of the extension itself cannot be more than,

16:55.880 --> 16:59.880
if I'm correct, more than 16 bytes or something like this.

16:59.880 --> 17:00.880
I don't remember now exactly.

17:00.880 --> 17:04.880
And the dependency, the script though can be much larger than that.

17:04.880 --> 17:09.880
And so you do need to support two bytes extensions with at the time I didn't.

17:09.880 --> 17:14.880
So I needed to implement that first in order to get it to work because when I started testing it,

17:14.880 --> 17:18.880
nothing worked and it turned out that this was the issue.

17:18.880 --> 17:23.880
And then we need, once we have negotiated it and we start receiving the dependency, the script,

17:23.880 --> 17:26.880
as part of our TP packets, we need to figure out a way to parse it.

17:26.880 --> 17:30.880
And this was really a nightmare for me.

17:30.880 --> 17:33.880
This is like therapy for me right now because I'm sharing all this with you.

17:33.880 --> 17:38.880
And I actually run to the about this in a couple of blog posts where you can see the nitty-gritty details.

17:38.880 --> 17:44.880
But just to give you an idea, basically it's, let's say a mess.

17:44.880 --> 17:46.880
I will not say that word because I don't want to be bit.

17:46.880 --> 17:52.880
But basically you can see that this is a specification that was written by somebody who writes codex,

17:52.880 --> 17:57.880
not a network specification because all fields are variable length and often at the bit level,

17:57.880 --> 18:01.880
which makes it really a nightmare to parse sometimes.

18:01.880 --> 18:07.880
And from what we regard the specification itself, it's indeed quite flexible because there are a few

18:07.880 --> 18:12.880
mandatory fields like if this is a start of a frame and end of a frame, the frame number,

18:12.880 --> 18:16.880
and the template ID for those dependencies that we've seen before.

18:16.880 --> 18:22.880
But also everything else is optional, which means that you can either have a dependency

18:22.880 --> 18:27.880
in the scriptural element that describes everything, so the whole context of the SVC

18:27.880 --> 18:30.880
or just something that tells you the scope of the current frame.

18:30.880 --> 18:34.880
And when we look at how a dependency in the scriptural really looks like,

18:34.880 --> 18:39.880
this is a simple parser that I created to basically debug things offline.

18:39.880 --> 18:44.880
And when we receive a keyframe, typically we have a 95 bytes extension,

18:44.880 --> 18:46.880
which if you know RTP, that's a lot.

18:46.880 --> 18:52.880
That's basically almost 10% of the payloads that you have.

18:52.880 --> 18:57.880
So it's really big, but that's because it contains a lot of information.

18:57.880 --> 19:00.880
So if you start parsing it and serializing everything that you receive,

19:01.880 --> 19:06.880
you have information about the different layers that you have, spatial temporal and so on and so forth.

19:06.880 --> 19:13.880
TDI, I don't remember exactly what it was, but this is just the output of that tool.

19:13.880 --> 19:15.880
That's a lot of stuff.

19:15.880 --> 19:25.880
So blah, blah, blah, blah, some more chains, some more stuff, the code layer targets.

19:25.880 --> 19:27.880
I have some stuff about resolutions.

19:27.880 --> 19:29.880
And finally, we're done.

19:29.880 --> 19:35.880
Basically, all the parts that we've seen before were basically the media center telling us,

19:35.880 --> 19:41.880
these are all the information that I used for this specific SVC context.

19:41.880 --> 19:47.880
So in this case, this was an L3T3, so three temporal layers and three spatial layers.

19:47.880 --> 19:54.880
And all those, that huge stuff that you've seen before is all the information related to chain dependencies,

19:54.880 --> 19:56.880
all that kind of very low level stuff.

19:56.880 --> 19:59.880
And so if you want to use it, it's there.

19:59.880 --> 20:05.880
And then at the end, it also tells you the resolution streams of the three different spatial layers.

20:05.880 --> 20:09.880
In this case, it was low because I captured really at the beginning, I think.

20:09.880 --> 20:15.880
And finally, it tells you that for this specific RTP packet, this is a spatial layer zero, temporal layer zero,

20:15.880 --> 20:23.880
and it uses template index number one, which is indeed spatial zero, temporal layer zero.

20:23.880 --> 20:28.880
And this is the information that we need because then having a look at all the stuff that we've seen before,

20:28.880 --> 20:33.880
we know that the resolution for spatial layer zero is, in this case, this multi-mover here.

20:33.880 --> 20:37.880
In practice, it would be something like 320 by something else.

20:37.880 --> 20:38.880
And this is it.

20:38.880 --> 20:42.880
And of course, likely not all dependency descriptors are so long,

20:42.880 --> 20:46.880
only for the meaningful key frame packets, it's usually like that.

20:46.880 --> 20:52.880
And then other dependency descriptors will be much smaller, like only seven bytes,

20:52.880 --> 20:58.880
because they will only tell you, for instance, the temporal index of this specific packet.

20:58.880 --> 21:01.880
In this case, it is a spatial layer zero at temporal layer zero.

21:01.880 --> 21:06.880
But I only know this because I received this before.

21:06.880 --> 21:10.880
So I received somewhere in time this huge chunk of information before,

21:10.880 --> 21:16.880
because if I only receive this and I get temporal index six, what is six?

21:16.880 --> 21:17.880
Six relative to what?

21:17.880 --> 21:18.880
So what does it mean?

21:18.880 --> 21:20.880
I don't even know how many layers there are.

21:20.880 --> 21:26.880
So you do need to have that information first if you want to make sense of all these smaller packets

21:26.880 --> 21:32.880
that you receive later after that, which means that when you start to implement stuff in a server,

21:32.880 --> 21:37.880
it does mean that you start need to keep a state, which is not really true for single cast or other things.

21:37.880 --> 21:40.880
I don't mean it's partly true, but only in a very limited way.

21:40.880 --> 21:44.880
In this case, it does mean that anytime that you receive that huge packet and you parse it,

21:44.880 --> 21:48.880
you need to keep it somewhere so that when you receive packets after that,

21:48.880 --> 21:51.880
you can reference them and use them for something.

21:51.880 --> 21:57.880
And the idea was that once I have a knowledge of those templates and I receive information

21:57.880 --> 22:04.880
and I know that this packet that I just received, this spatial layer X and temporal layer Y,

22:04.880 --> 22:09.880
then as a server, I can decide whether or not I want to relay it or drop it.

22:09.880 --> 22:13.880
And you can do it the relatively easy way or you can do it the hard way.

22:13.880 --> 22:17.880
The hard way is figuring out all of those dependencies that we've seen before.

22:17.880 --> 22:20.880
I went for the easier way, especially right now.

22:20.880 --> 22:27.880
If it is temporal layer 2, then relay everything related to spatial layer 1 and 0 as well,

22:27.880 --> 22:37.880
as long as it's the same or let's say the temporal layer is smaller or equal to the one that I'm receiving.

22:37.880 --> 22:42.880
So I may be relaying more than I should, but at least I know that everything is there.

22:42.880 --> 22:47.880
What's important is that once you use that information so that once you've parsed it,

22:47.880 --> 22:51.880
you cannot drop it. You need to relay it anyway because it's not only helpful to you,

22:51.880 --> 22:55.880
it's also helpful to the subscriber that is receiving that video stream

22:55.880 --> 22:58.880
because they also need to know what is what.

22:58.880 --> 23:01.880
So you need to forward that information as well.

23:01.880 --> 23:07.880
And very important, you also need to update the RTP headers accordingly, including the marker bit,

23:07.880 --> 23:10.880
which is what really drove me nuts the first time

23:10.880 --> 23:13.880
because I actually implemented all this for a long time and it didn't work.

23:13.880 --> 23:18.880
And eventually I figured out that the problem was that I was not updating marker bits as well.

23:18.880 --> 23:20.880
And this is the reason, basically.

23:20.880 --> 23:28.880
So if we have a sequence of RTP packets related to different spatial layers and temporal layers,

23:28.880 --> 23:33.880
this is basically what it looks like from an RTP perspective, including marker bits.

23:33.880 --> 23:36.880
If I am dropping spatial layer 2 because I don't need it,

23:36.880 --> 23:40.880
then what it means is that I'm dropping some packets over here.

23:40.880 --> 23:44.880
So of course, all the packets that I'm dropping, I need to update the sequence number

23:44.880 --> 23:49.880
so that it keeps on growing monotonically because otherwise the recipient will think

23:49.880 --> 23:53.880
that they are missing, losing some packets, but they are not missing them.

23:53.880 --> 23:55.880
They are just dropping them because they don't need them.

23:55.880 --> 24:00.880
So I need to update the sequence number so that this is one, this is two, this is three,

24:00.880 --> 24:02.880
this is four, five, six, seven, etc.

24:02.880 --> 24:06.880
So I need to make sure that they know that they are not really missing anything.

24:06.880 --> 24:11.880
But I also need to update where I'm setting the M equals one marker bit as well

24:11.880 --> 24:15.880
because this is needed for b-decoating, especially from Chrome.

24:15.880 --> 24:21.880
So in particular, you need to set M equals one on the last packet with the same timestamp.

24:21.880 --> 24:25.880
So since the timestamp now is changing on the second packet,

24:25.880 --> 24:28.880
because that's the last packet with that timestamp over there,

24:28.880 --> 24:32.880
I need to set M equals one on that second packet before I forward it

24:32.880 --> 24:36.880
or otherwise nothing works basically.

24:36.880 --> 24:39.880
Sorry, wrong direction.

24:39.880 --> 24:45.880
And basically, if you want to test all these and with Janus or with anything else,

24:45.880 --> 24:48.880
of course you need to have a browser that supports all this stuff.

24:48.880 --> 24:52.880
And the kind of bad news is that at the moment I think only Chrome supports it.

24:52.880 --> 24:55.880
I don't know if other Chrome-based browsers support it too,

24:55.880 --> 24:59.880
but definitely Chrome supports AV1 as a codec.

24:59.880 --> 25:04.880
And you can check that by using the RTP sender get capabilities thing to see.

25:04.880 --> 25:08.880
If you see AV1 in that list, you do support AV1 as a codec.

25:08.880 --> 25:14.880
But you also need to support SBC functionality and most importantly the dependency, the scripture.

25:14.880 --> 25:17.880
And the dependency, the scripture is not offered by default.

25:17.880 --> 25:21.880
So you do still need, I think, to first fill the trial like this.

25:21.880 --> 25:27.880
I don't remember right now if you can just manage the SDP to artificially put the extension in your SDP

25:27.880 --> 25:31.880
in order to make it work anyway, but that I should check, I should double check.

25:31.880 --> 25:36.880
But you may need to launch, for instance, Chrome with that thing over here

25:36.880 --> 25:40.880
so that the extension appears in the supported extensions by the browser.

25:40.880 --> 25:46.880
When you do that, then your browser is capable of encoding AV1 SBC functionality

25:46.880 --> 25:49.880
with dependency and scripture, which is quite important.

25:49.880 --> 25:53.880
And if you want to test this, I also made it very simple because

25:53.880 --> 25:57.880
if you go on the online demos for Janus and you check the eco test demo

25:57.880 --> 26:01.880
you can provide a couple of attributes to, first of all, for AV1 as a codec

26:01.880 --> 26:06.880
and then for a specific flavor of SBC, in this case, for instance, L3T3

26:06.880 --> 26:10.880
to send three temporal layers and three spatial layers.

26:10.880 --> 26:14.880
And when you do some small buttons appear over there

26:14.880 --> 26:17.880
and they allow you to check one thing or the other,

26:17.880 --> 26:23.880
which means that you will send the big AV1 SBC stream to Janus

26:23.880 --> 26:26.880
and Janus will send you back only what you asked for.

26:26.880 --> 26:29.880
So in this case, for instance, spatial layer one and temporal layer two

26:29.880 --> 26:32.880
which is why my resolution is smaller and the bitrate is smaller as well.

26:32.880 --> 26:36.880
So by playing a bit with those things you should see resolution changing,

26:36.880 --> 26:38.880
bitrate changing, if it does, it works.

26:38.880 --> 26:42.880
And the same functionality is also supported in the video room, of course,

26:42.880 --> 26:44.880
which is the SFU to do video conferencing.

26:44.880 --> 26:47.880
So at least in theory you can have a complete video conference

26:47.880 --> 26:52.880
that is based on AV1 SBC as well, even though we haven't tested that much

26:52.880 --> 26:54.880
but it should definitely work.

26:54.880 --> 26:56.880
And I think this is it.

26:56.880 --> 26:59.880
I'm not sure if we have time for questions, but before that,

26:59.880 --> 27:03.880
I also wanted to announce that, I'm sorry, I'm bothering you all,

27:03.880 --> 27:04.880
but JanusCon is back.

27:04.880 --> 27:07.880
So JanusCon is our own Janus conference.

27:07.880 --> 27:10.880
So it's a conference devoted to Janus and WebRTC in general,

27:10.880 --> 27:14.880
which will happen at the end of April in Naples in the south of Italy.

27:14.880 --> 27:18.880
We have a few sponsors already which I'm very grateful for.

27:18.880 --> 27:22.880
And the call for paper ends in about a week.

27:22.880 --> 27:25.880
So if you have anything interesting doing with Janus and WebRTC,

27:25.880 --> 27:28.880
you can feel free to submit a talk there.

27:28.880 --> 27:30.880
Well, tickets are also available for sale as well.

27:30.880 --> 27:33.880
And of course, if your company is interested in sponsoring,

27:33.880 --> 27:35.880
that would be great too.

27:35.880 --> 27:36.880
And that is all.

27:36.880 --> 27:37.880
I don't know if we have time for questions

27:37.880 --> 27:42.880
because I didn't really check how fast I was going, maybe too fast or...

27:42.880 --> 27:48.880
Okay, so are there any questions for anyone at the C part?

27:48.880 --> 27:52.880
I see a couple.

27:52.880 --> 27:54.880
I think slow me with...

27:54.880 --> 28:01.880
Generally, would you say that the SBC is like the generation of simulcast

28:01.880 --> 28:05.880
or if we continue, whether we look at the future of people

28:05.880 --> 28:13.880
on the platform that will replace it or they will need to get the sale by sale?

28:13.880 --> 28:17.880
I mean, in general, if you look at, for instance, if you look at that...

28:17.880 --> 28:18.880
Oh, sorry, sorry.

28:18.880 --> 28:23.880
Slow me was asking, is basically SBC or evolution of simulcast

28:23.880 --> 28:26.880
or does it make sense to have them both at the same time?

28:26.880 --> 28:28.880
Which one will take...

28:28.880 --> 28:30.880
Which one will be more important in the future?

28:30.880 --> 28:34.880
Which one is the technology to invest in in the future, maybe, as well?

28:34.880 --> 28:37.880
And functionally, I mean, they serve the same purpose, if you want,

28:37.880 --> 28:40.880
because I have the same demo for simulcasts

28:40.880 --> 28:43.880
and if you look at the demo for simulcasts, it looks visually the same.

28:43.880 --> 28:47.880
So you have the same buttons to say, I want high quality, low quality and so on.

28:47.880 --> 28:51.880
The difference are really in just how the thing is implemented.

28:51.880 --> 28:56.880
And the main problem, I mean, in general, SBC is supposed to be more advanced,

28:56.880 --> 29:01.880
of course, than simulcast and more resilient as well, probably.

29:01.880 --> 29:08.880
But the main obstacle right now is that it's related to what I was saying before.

29:08.880 --> 29:15.880
So right now, if you want to use AV1 SBC, you have to do a custom flag,

29:15.880 --> 29:18.880
which means that right at the outset, it's really not something

29:18.880 --> 29:20.880
that you can ask your customer to do, for instance.

29:20.880 --> 29:23.880
So for the moment, it's not really something that is production ready.

29:23.880 --> 29:28.880
You can use the SBC flavor of VP9, which provides a similar feature,

29:28.880 --> 29:31.880
which is now available out there.

29:31.880 --> 29:37.880
But still, simulcast is overwhelmingly favored in general for production environments

29:37.880 --> 29:41.880
because it's been battle tested, it's been there since day one.

29:41.880 --> 29:45.880
Everybody supports simulcast, it's easier to work with and so on and so forth.

29:45.880 --> 29:50.880
So for the moment, it doesn't make sense to just use force SBC

29:50.880 --> 29:53.880
in your production environment right away, if not for experimental purposes

29:53.880 --> 29:57.880
and for testing how it works, for dipping your toes in the technology.

29:57.880 --> 30:01.880
But for the future, I definitely think you should pay attention to that

30:01.880 --> 30:04.880
because AV1 will be the code that everybody will adopt,

30:04.880 --> 30:08.880
hopefully because it's better quality, it's royalty free, it's open source,

30:08.880 --> 30:10.880
and it has SBC baked in.

30:10.880 --> 30:14.880
Sooner or later, hopefully Safari will have AV1 as we see,

30:14.880 --> 30:19.880
Firefox will have it, Edge and other browsers will have it as well.

30:19.880 --> 30:21.880
And you definitely want to be ready when that happens

30:21.880 --> 30:24.880
because otherwise you'll be the one stuck with the old codec

30:24.880 --> 30:28.880
and everybody else is taking advantage of the new team.

30:28.880 --> 30:32.880
I think learns that you can munch the SDP to make it work.

30:32.880 --> 30:34.880
For the extension, yeah.

30:34.880 --> 30:36.880
Because we have it working new team.

30:36.880 --> 30:40.880
Tuzlomi, there is one thing that in some environments might be relevant

30:40.880 --> 30:47.880
which is as many hardware decoders don't cope with SBC,

30:47.880 --> 30:51.880
but they do with Samocast because they look like a normal strain.

30:51.880 --> 30:54.880
So if you're in a resource constrained thing,

30:54.880 --> 30:56.880
maybe receiving SBC is no bueno,

30:56.880 --> 30:59.880
but receiving a normal Samocast will be better.

30:59.880 --> 31:03.880
But in theory, these will not be true for AV1

31:03.880 --> 31:05.880
because AV1 was conceived with SBC in mind.

31:05.880 --> 31:09.880
So in theory, all hardware decoders, too, even smaller ones,

31:09.880 --> 31:11.880
will know how to interpret that.

31:11.880 --> 31:14.880
And since it's a single stream, they will be able to decode it.

31:14.880 --> 31:16.880
Of course, it's just theory and...

31:16.880 --> 31:17.880
Ideally they would.

31:17.880 --> 31:24.880
For VP9, for example, Chrome still does not use hardware decoders

31:24.880 --> 31:27.880
when you use SBC.

31:27.880 --> 31:34.880
And I'm not sure because AV1 hardware support is hit and miss yet still.

31:34.880 --> 31:36.880
And there was another question here, yeah?

31:36.880 --> 31:40.880
Yeah, I was wondering what the forward error correction strategy here

31:40.880 --> 31:43.880
is, like, is this patient, if there are...

31:43.880 --> 31:47.880
I'm sorry, if forward error correction is used,

31:47.880 --> 31:49.880
how do you use it with do is I mean...

31:49.880 --> 31:53.880
Yeah, if all the use forward error correction is SBC,

31:53.880 --> 31:59.880
then you are like, helping out some tactics and then it doesn't work.

31:59.880 --> 32:00.880
Yeah, that's a good question.

32:00.880 --> 32:05.880
And it's actually related to one of the doubts that I have related to FBC,

32:05.880 --> 32:11.880
mostly because I mean something like AV1, SBC and CMUCAS as well

32:11.880 --> 32:14.880
only makes sense when you have a server in the middle.

32:14.880 --> 32:18.880
It doesn't really make sense if you are sending something from point A to point B

32:18.880 --> 32:20.880
and point B is the one that is meant to receive it

32:20.880 --> 32:23.880
because in this case you are sending everything anyway.

32:23.880 --> 32:28.880
So unless you are using SBC as some sort of a...

32:28.880 --> 32:30.880
of your redundancy mechanism because you say,

32:30.880 --> 32:34.880
if I lose some packets related to two, I can still display one.

32:34.880 --> 32:36.880
That's one thing, but that's not really what it's meant for.

32:36.880 --> 32:39.880
And so the moment you have a server in the middle,

32:39.880 --> 32:45.880
it also means that you can offload the forward error correction stuff to the server as well.

32:45.880 --> 32:50.880
So which does make sense also because, for instance, when you use FlexFec,

32:50.880 --> 32:54.880
which is the thing that was described in the first presentation from Chrome,

32:54.880 --> 32:58.880
Chrome by default will not put any redundancy information,

32:58.880 --> 33:05.880
so it will not put any FEC packets until the peer tells them that they are losing some packets.

33:05.880 --> 33:08.880
And this is to optimize stuff, so you don't add redundancy unless it's needed

33:08.880 --> 33:13.880
because there's loss reported, which becomes a problem if you're doing something like a video conference

33:13.880 --> 33:16.880
because your uplink find may be perfect,

33:16.880 --> 33:21.880
and then you have subscriber X over here that is experiencing loss

33:21.880 --> 33:25.880
and you don't have any redundancy packets to send them instead.

33:25.880 --> 33:28.880
So the idea and probably the solution to that,

33:28.880 --> 33:31.880
this is something that I'm still brainstorming myself because FEC interests me,

33:31.880 --> 33:36.880
but I have some doubts there, is that probably the forward error correction stuff

33:36.880 --> 33:41.880
is something that the server itself will need to add on each subscriber leg.

33:41.880 --> 33:47.880
So from the server to you, I will have a dedicated FEC channel

33:47.880 --> 33:51.880
where I add some forward error correction stuff from the stream that I'm sending you,

33:51.880 --> 33:57.880
and for the stream that I'm sending you, the layer 2 may not be there,

33:57.880 --> 34:01.880
but I have a consistent stream because packets are in sequence,

34:01.880 --> 34:04.880
and so the forward error correction stuff that I'll be sending you

34:04.880 --> 34:07.880
will be different from the one that I'll be sending to somebody else

34:07.880 --> 34:09.880
who is receiving additional layers,

34:09.880 --> 34:13.880
and that's probably the only way to do this if you don't want to forward FEC

34:13.880 --> 34:18.880
and to end without treating it, which anyway wouldn't be useful at all,

34:18.880 --> 34:22.880
especially if the sender is not providing that information themselves.

34:36.880 --> 34:40.880
Yeah, in my experience, and this may be an implementation choice, of course,

34:41.880 --> 34:46.880
I did have to forward it because otherwise it would not be decoded properly, basically.

34:46.880 --> 34:49.880
And I don't know if this is actually really needed,

34:49.880 --> 34:55.880
like for instance, even the marker bit 1, that's not really needed from a specification perspective

34:55.880 --> 34:58.880
because as a receiver, you do see that the timestamp is changing,

34:58.880 --> 35:02.880
so you do know that it is a new frame and you can decode the previous one.

35:02.880 --> 35:05.880
But it's simply that Chrome expects that marker bit set to 1,

35:05.880 --> 35:08.880
otherwise it will not decode a frame, basically.

35:08.880 --> 35:12.880
So in my experience, you need to forward that information too.

35:12.880 --> 35:16.880
And I guess it makes sense because the recipients themselves

35:16.880 --> 35:20.880
also need to decode possibly differently the video stream

35:20.880 --> 35:23.880
depending on what they are receiving because they need to know

35:23.880 --> 35:28.880
if the resolution must be this size or this size or this size or something like this.

35:28.880 --> 35:32.880
It may all be part of the 81 bit stream, so it may be redundant information

35:32.880 --> 35:37.880
as far as they are concerned, but at least when I made these tests a few months ago,

35:37.880 --> 35:41.880
it was needed, so just relaying it makes sense.

35:41.880 --> 35:43.880
Yeah.

35:43.880 --> 35:49.880
In regard to switching this layer, like saw your previous talk somewhere

35:49.880 --> 35:56.880
was on bandwidth estimation, maybe you can comment on how they do go together

35:56.880 --> 36:00.880
or is there something specific to 81?

36:00.880 --> 36:05.880
Yeah, no, I mean the bandwidth estimation stuff is important for a few different reasons.

36:05.880 --> 36:09.880
And in this case, I'm talking about the bandwidth estimation on the subscriber side.

36:09.880 --> 36:14.880
So from server to recipients, because on the publisher side,

36:14.880 --> 36:18.880
there is transport-wide control CC and basically the browser

36:18.880 --> 36:22.880
themselves are capable of using the feedback to figure out if they need to send less or more.

36:22.880 --> 36:27.880
And so dynamically, you may see that some special layers are not appearing

36:27.880 --> 36:30.880
because the browser doesn't have enough bandwidth for that.

36:30.880 --> 36:34.880
On the subscriber perspective, it's really useful because it allows us to

36:34.880 --> 36:38.880
it helps with the decision. So for instance, right now I just mentioned

36:38.880 --> 36:42.880
just generically whether I want to relay or drop a packet, but this

36:42.880 --> 36:46.880
actually depends on why I should relay it because a user may want to receive

36:46.880 --> 36:50.880
the highest quality possible, a user may want to receive the lowest quality possible,

36:50.880 --> 36:55.880
but this may be because they only want a lower quality because the video is going to appear

36:55.880 --> 37:00.880
in a thumbnail and so they don't need the whole thing and that's an application logic decision.

37:00.880 --> 37:04.880
And now the decision may come from the user doesn't have enough bandwidth for

37:04.880 --> 37:08.880
all of that stuff, so they don't have enough bandwidth for special layer 2 and 1.

37:08.880 --> 37:12.880
Let's just send them special layer 0. And this is where bandwidth estimation helps

37:12.880 --> 37:16.880
because if I'm sending stuff to the subscriber and I'm starting to get information

37:16.880 --> 37:20.880
that congestion is happening, then internally the server can update

37:20.880 --> 37:25.880
which special layer or temporal layer I should send to this specific publisher

37:25.880 --> 37:29.880
dynamically. And so this will impact my decisions to relay or

37:29.880 --> 37:33.880
drop stuff and so it allows me to dynamically

37:33.880 --> 37:37.880
dynamically impact the quality of the subscriber depending on how much

37:37.880 --> 37:41.880
bandwidth they have. And in my experiments right now I've only done this with Siebel

37:41.880 --> 37:45.880
because I haven't hooked it up to SBC yet, but the key principles are really the same.

37:45.880 --> 37:49.880
One minute?

37:49.880 --> 37:53.880
Yeah, just related to that is there a way

37:53.880 --> 37:57.880
or Wipen Web to signal the final cast

37:57.880 --> 38:01.880
of the publisher and the subscribed site?

38:01.880 --> 38:05.880
Yeah, I mean for the final cast or SBC.

38:05.880 --> 38:09.880
Of course, yeah, in Wipen Web do you

38:09.880 --> 38:13.880
with Wipen Web is there any need to signal Siebel cast or SBC as well

38:13.880 --> 38:17.880
and does it make sense? And in general, I mean it's definitely important

38:17.880 --> 38:21.880
that you signal it on Wip because you want to

38:21.880 --> 38:25.880
make sure that the stream that you are ingesting is recognized by the server

38:25.880 --> 38:29.880
as a Siebel cast or an SBC stream so that the server can also

38:29.880 --> 38:33.880
parcel of those dependency descriptors in case it's a one

38:33.880 --> 38:37.880
SBC for instance or in case it's Siebel cast it knows that it needs to

38:37.880 --> 38:41.880
take care of, let's say, three different qualities. On the subscriber's side

38:41.880 --> 38:45.880
for Siebel cast it's really not important because you're

38:45.880 --> 38:49.880
just always, as a subscriber, you're just always going to receive one

38:49.880 --> 38:53.880
video stream and as far as you're concerned it's a consistent

38:53.880 --> 38:57.880
video stream. You don't even know that there is a switch behind the curtains that is happening

38:57.880 --> 39:01.880
from high to low to medium or whatever. You just see a single video stream

39:01.880 --> 39:05.880
so you don't need to be aware of the fact that it's Siebel cast. For everyone

39:05.880 --> 39:09.880
as a SBC it may be important to negotiate the dependency, the scripture

39:09.880 --> 39:13.880
extension as I mentioned because if it's needed for decoding purposes

39:13.880 --> 39:17.880
and you want the browser to be able to

39:17.880 --> 39:21.880
decode things properly then you may want to negotiate that extension as well

39:21.880 --> 39:25.880
on the subscriber's side. But as I was saying before it may or may not be needed

39:25.880 --> 39:29.880
so that's something that we'll have to check. And I think I'm

39:29.880 --> 39:33.880
really out of time now so.

39:33.880 --> 39:37.880
Thank you.

39:43.880 --> 39:47.880
Thank you.

