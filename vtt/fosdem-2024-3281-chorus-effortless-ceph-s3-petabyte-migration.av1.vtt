WEBVTT

00:00.000 --> 00:13.720
Hi everyone, hope I am audible to the larger crowd here, thank you.

00:13.720 --> 00:20.840
So I am Sreesha Gurduru, I am a self engineer working for Klyso and I am here to present

00:20.840 --> 00:29.240
an open source project called KORUS which is responsible for an effortless S3 petabyte

00:29.240 --> 00:35.120
migration slash replication.

00:35.120 --> 00:42.920
So let us talk about why data migration, like when do we come across a data migration scenario.

00:42.920 --> 00:51.840
So lot of the companies and organizations these days have private cloud clusters and hardware

00:51.840 --> 00:58.480
which has certain specifications, queue and it can come to end of life anytime like the

00:58.480 --> 01:03.280
vendor might stop supporting the existing hardware, there might be a new hardware coming

01:03.280 --> 01:04.360
up.

01:04.360 --> 01:11.020
So in that case either there are two options in front of us which is to augment the existing

01:11.020 --> 01:13.960
cluster with the new hardware.

01:13.960 --> 01:19.560
If the specifications and skew are similar to what we have now in the cluster or to build

01:19.560 --> 01:25.560
up, build a new brand new cluster all together and when we build a brand new cluster there

01:25.560 --> 01:31.340
is a high reason for data to be migrated between old production cluster and the new

01:31.340 --> 01:39.840
cluster so that you know the data continues to stay and the operations can happen smooth.

01:39.840 --> 01:42.880
This can be one of the main reasons for data migration.

01:42.880 --> 01:48.640
Let us talk a few woes or difficulties with data migration.

01:48.640 --> 01:55.240
When we are talking about migration of data we are not talking about few bytes or gigabytes,

01:55.240 --> 01:57.920
we are talking about petabytes of storage.

01:57.920 --> 02:02.840
We have lot of data being stored in our storage back ends these days and those have to be

02:02.840 --> 02:07.160
effortlessly migrated to the new clusters.

02:07.160 --> 02:13.560
So the challenges include syncing petabytes of data and the continuous monitoring that

02:13.560 --> 02:20.240
we have to do behind the scenes like we just pick up some tool like RClone in this case

02:20.760 --> 02:25.760
and RClone is a robust synchronization tool, copy tool.

02:25.760 --> 02:31.440
So when we just pick that up we just have to monitor or even if we run that in the background

02:31.440 --> 02:37.480
we keep monitoring the status of the replication and also the time consumed with that huge amount

02:37.480 --> 02:41.440
of data to be copied across the clusters.

02:41.440 --> 02:48.600
Obviously the tools that are used for the migration and continuous changes in the data.

02:48.600 --> 02:51.840
The existing cluster we just do not yet decommission it.

02:51.840 --> 02:58.280
We have an active operation happening on the cluster be it reads, writes, updates.

02:58.280 --> 03:04.120
So that also due to the continuous changes in the data we might see it a bit difficult

03:04.120 --> 03:06.840
to copy or migrate the data.

03:06.840 --> 03:14.340
Let me share one of our experiences with a customer where in the similar scenario they

03:14.420 --> 03:21.420
had their cluster end of life and then we built a brand news cluster for them and the

03:21.420 --> 03:26.060
data to be migrated was around 3 petabytes.

03:26.060 --> 03:32.060
So between the old and new clusters we picked up RClone as the data migration tool.

03:32.060 --> 03:39.060
Migration let alone the data we had to migrate the metadata obviously and there was some

03:39.860 --> 03:46.060
issue with RClone where we could not copy the ACLs and the bucket policies for that

03:46.060 --> 03:53.060
particular bucket and then we had to tweak around and then we eventually got it to working

03:53.060 --> 03:57.500
and then that was a bit of difficult task for us.

03:57.500 --> 03:58.900
Indeed it is a Herculean task.

03:58.900 --> 04:05.900
So this experiment, this encounters with our experiences led to a tool called KORUS which

04:06.020 --> 04:13.020
is an open source tool which is a data application software which is capable of synchronizing

04:14.940 --> 04:23.060
S3 data as of today S3 data between multiple cloud storage back ends.

04:23.060 --> 04:28.700
Let me present you some of the problem statements for our tool.

04:28.700 --> 04:32.900
How to migrate S3 vendor data with reduced downtime?

04:32.900 --> 04:39.300
So I would not say there would not be any downtime but with reduced downtime and the

04:39.300 --> 04:43.740
cluster being operational at the same time and also the data being copied to the new

04:43.740 --> 04:45.660
cluster.

04:45.660 --> 04:52.820
And how to back up S3 data to another S3 in a different region or a different vendor?

04:52.820 --> 04:59.820
Here we might not have the same back end, we might be using storages from different providers

05:00.820 --> 05:07.180
like Amazon, Google, Minayo and we might have our own private clusters.

05:07.180 --> 05:10.940
So it is vendor agnostic.

05:10.940 --> 05:16.940
Like the initial goals of KORUS was to have a vendor agnostic solution.

05:16.940 --> 05:23.180
Like it should be able to support multiple back ends and with a pluggable architecture

05:23.180 --> 05:27.540
that means the components in KORUS are loosely coupled.

05:27.620 --> 05:34.300
Like if I see that one of the layer can be better, it can be replaced with another tool

05:34.300 --> 05:40.740
which is more performant and more efficient, I should be able to replace it.

05:40.740 --> 05:46.140
And then benchmarking of course before we add in any component we benchmark that tool

05:46.140 --> 05:52.700
efficiently so that it will be compatible with all of our, the entire project.

05:52.700 --> 05:55.180
The focus on correctness.

05:55.220 --> 06:01.220
So the data which is present in the source and the back ends, the following back ends

06:01.220 --> 06:08.220
we ensure that the data is correct and in sync with all the storage back ends.

06:08.220 --> 06:14.140
And then migrating big bucket under the load without downtime or with reduced downtime.

06:14.140 --> 06:19.580
So there are two things here, there can be multiple buckets with small amounts of data

06:19.580 --> 06:25.100
and those buckets are easy to be copied because it just takes couple of minutes.

06:25.100 --> 06:32.700
But there is a scenario where a bucket, one bucket has huge amount of data and lot of

06:32.700 --> 06:38.420
clients might be writing to one bucket and that bucket has to be migrated.

06:38.420 --> 06:42.540
So that is a bit of concern.

06:42.540 --> 06:52.580
So overview of chorus is there is one main storage and remaining can be configured as

06:52.580 --> 06:55.500
the follower back ends.

06:55.500 --> 07:00.940
And the users start by inputting the storage credentials in the configuration.

07:00.940 --> 07:10.220
And once the configuration is started and configured the chorus S3 API can be used instead of the

07:10.220 --> 07:11.940
storage backend API.

07:11.940 --> 07:18.380
Like if you are using AWS, if you are using Google, Minio, every backend has its own API.

07:18.380 --> 07:23.300
Instead of using that you can use one backend, one chorus API to communicate with multiple

07:23.300 --> 07:26.820
back ends because they are all S3 based.

07:26.820 --> 07:35.380
And chorus proxies request to the main storage and then eventually the storage is copied to

07:35.380 --> 07:38.780
the followers in the backend.

07:38.780 --> 07:41.100
All the existing data is replicated.

07:41.100 --> 07:48.180
For example when we introduce chorus into our ecosystem we might already have clusters

07:48.180 --> 07:52.460
with certain amount of data which has to be copied to different back ends that we configure

07:52.460 --> 07:53.460
later.

07:53.460 --> 07:58.380
So the existing data can be configured in the background using this tool.

07:58.380 --> 08:03.140
The data replication can be configured, paused, resumed, there are different life cycles for

08:03.140 --> 08:05.820
that particular request, the status.

08:05.820 --> 08:12.780
You can just stop, start, resume at any time you want and the management can be done using

08:12.780 --> 08:18.700
a web UI or a CLI.

08:18.700 --> 08:24.700
So the features of chorus include routing and replication per bucket.

08:24.700 --> 08:32.500
You can configure where to route or the request and where to replicate a bucket.

08:32.500 --> 08:37.820
And then again the same using you can pause and resume anytime.

08:37.820 --> 08:41.580
And then synchronize object or metadata.

08:41.580 --> 08:48.620
So just not the storage, you can also copy the ACLs, bucket policies, tags and everything

08:48.620 --> 08:50.820
using the same tool.

08:50.820 --> 08:57.380
And then as I spoke earlier migrate existing data in the background, track replication lag.

08:57.380 --> 09:02.460
So as of today we might have one set of configuration and the data must be copying to the source.

09:02.460 --> 09:08.420
To the back end, to the follower back ends, we can always track the replication and we

09:08.420 --> 09:11.020
can improve with the configuration options.

09:11.020 --> 09:16.300
We can start to rate limit, we can increase the number of workers.

09:16.300 --> 09:18.220
So we can do that.

09:18.220 --> 09:21.060
And chorus exposes Prometheus metrics.

09:21.060 --> 09:27.660
So we have the entire logging thing and the metrics are sent to Prometheus and the logs

09:27.660 --> 09:29.340
are in JSON format.

09:29.340 --> 09:30.940
Easy to read.

09:30.940 --> 09:34.020
Proxy and Grafana form the monitoring stack.

09:34.020 --> 09:39.380
You can visualize the data of how the bucket is being replicated and at what stage it is

09:39.380 --> 09:43.580
using the visualization stack.

09:43.580 --> 09:48.780
Let me briefly talk about the architecture of this entire chorus.

09:48.780 --> 09:51.660
Chorus is structured around two main web services.

09:51.660 --> 09:54.980
One is the proxy and the other is the worker.

09:54.980 --> 09:57.620
So initially the request comes to the proxy.

09:57.700 --> 10:02.660
We are talking about a flow where the routing policy is there.

10:02.660 --> 10:07.380
So initially the request comes to the proxy and based on the routing policy, if the bucket

10:07.380 --> 10:12.580
has to go to the main storage, which is configured to be Minayo here.

10:12.580 --> 10:18.900
So the request comes to the main storage and the request goes back to the proxy and then

10:18.900 --> 10:20.180
eventually to the user.

10:20.180 --> 10:22.420
That is one of the flows.

10:22.420 --> 10:28.700
The second flow is where the replication scenario is established.

10:28.700 --> 10:35.780
Again the request comes to the proxy and then there is an event or task created in Redis

10:35.780 --> 10:38.540
based on the replication policy.

10:38.540 --> 10:45.260
Like it knows what is the main storage and which storage should the replication be done

10:45.260 --> 10:46.260
to.

10:46.260 --> 10:49.020
In this case it is Ceph for example.

10:49.020 --> 10:55.580
And then the chorus worker reads the requests or tasks from the cache and then that routes

10:55.580 --> 11:02.180
the request to the, reads from the main storage and replicates to the back end.

11:02.180 --> 11:07.020
The chorus worker is accessible using WebBY and CLI.

11:07.020 --> 11:15.780
So this is an overview of the entire flow based on different scenarios.

11:15.780 --> 11:22.620
So chorus also has an initial migration feature like as I told the replication can happen

11:22.620 --> 11:24.260
in the background.

11:24.260 --> 11:29.940
So initially when the replication is happening in the background, all the buckets are listed

11:29.940 --> 11:36.740
from the main storage and then the objects within particular bucket are listed and then

11:36.740 --> 11:42.780
the number of tasks based on the objects are created and it is ensured that every object

11:42.780 --> 11:50.340
is copied to the follower back end using a particular task.

11:50.340 --> 11:58.420
So the worker processes tasks in the background copying the data to the follower back ends.

11:58.420 --> 12:06.980
Here these are the main components of chorus proxy worker admin UI and Redis.

12:06.980 --> 12:14.780
So proxy and worker are written in Golang and admin is written in view or the entire

12:14.780 --> 12:22.860
deployment is done in a containerized fashion on Kubernetes pods.

12:22.860 --> 12:29.180
Let us talk something about the resource requirements for different components in chorus.

12:29.180 --> 12:36.260
For Redis the scaling is done using Redis cluster and the persistence is ensured using

12:37.260 --> 12:46.660
Redis AOF and Redis database and fault tolerance in case of Redis data loss we can always restart

12:46.660 --> 12:52.860
the bucket replication because the state is maintained and memory consumption if there

12:52.860 --> 12:59.700
are around 1 million objects that are to be migrated then it can approximately take 1

13:00.140 --> 13:07.060
million tasks in the queue then approximately 700 MB.

13:07.060 --> 13:12.780
This is all based on our benchmarking it can change with different scenarios and Redis

13:12.780 --> 13:21.860
is assumed to take less CPU and it can be between 100 and 1000 requests per second.

13:21.860 --> 13:28.580
So coming to the proxy it is stateless, it consumes less memory and less CPU but high

13:28.580 --> 13:36.660
network because proxy is the kind of brain it takes in the requests and it routes the

13:36.660 --> 13:43.980
request accordingly based on replication or the actual routing hence it also needs high

13:43.980 --> 13:44.980
network.

13:44.980 --> 13:50.500
Coming to the worker it is again stateless it takes in high memory and high network

13:50.500 --> 13:56.780
but less CPU because worker is the one that routes the reads request from the cache and

13:56.820 --> 14:01.500
routes request to the back ends based on the replication policy.

14:01.500 --> 14:08.100
So worker instance network and memory consumption can be rate limited in the configuration like

14:08.100 --> 14:13.260
in the day when there is huge amount of request coming to our clusters we can just stop the

14:13.260 --> 14:21.140
migration activity for a minute like or we can rate limited to do it at a lesser rate

14:21.140 --> 14:25.620
and then eventually you can increase it when the bandwidth is high.

14:25.620 --> 14:35.140
So yeah so what are our next steps for chorus we want to perform more load tests in case

14:35.140 --> 14:43.260
of larger buckets more data and efficient time consumption and then resource optimization

14:43.260 --> 14:49.540
at various component level like Redis how can we make it better and workers we want

14:49.540 --> 14:56.340
to make the logic more functional and then the API cost.

14:56.340 --> 15:04.420
So the routing policy alternatives since we have multiple storage packets what we want

15:04.420 --> 15:11.380
to do is to route based on object size for example if there is one GB of file you can

15:11.380 --> 15:17.260
configure it to be written to a particular storage packet and then if there is small

15:17.300 --> 15:21.980
number of files you can configure it for one back end based on the quota and lot of other

15:21.980 --> 15:27.860
parameters and load balance read request for replicated data.

15:27.860 --> 15:32.860
Now that we have multiple storage back ends in our hand we can always make an efficient

15:32.860 --> 15:39.860
use of each back end we can load balance the requests like for example if main storage

15:39.860 --> 15:46.140
is busy in taking writes since the storage is data is being copied to the follower back

15:46.140 --> 15:50.900
ends we can always route the read request to any of the back end which is idle so that

15:50.900 --> 16:02.700
logic and so every storage back end is providing a bucket notification and event log so we

16:02.700 --> 16:08.260
can subscribe to those events instead of querying the proxy every time and overloading it instead

16:08.260 --> 16:16.860
we can use proxy to really write data and migrate data so we can use that proxy instead

16:16.860 --> 16:23.260
or to keep polling for the bucket information and then there is we are planning on having

16:23.260 --> 16:30.060
a Swift API compatibility as of today we have a robust S3 API compatibility but we are planning

16:30.060 --> 16:37.900
to have open stack Swift integration and then life cycle policy the API parameters for different

16:37.900 --> 16:44.660
back ends is different so we just want to have good life cycle policy it is being tested

16:44.660 --> 16:50.300
and when a bucket is created with a particular life cycle policy in one storage the similar

16:50.300 --> 16:57.100
should be replicated to the other back ends as well without loss of any configuration

16:57.100 --> 17:02.700
for policy.

17:02.700 --> 17:07.900
Use cases the further use cases that we see for chorus are active transparent proxy post

17:07.900 --> 17:16.660
migration to speak briefly about active proxy migration so that means if the source and

17:16.660 --> 17:22.580
destination are completely copied and if we want to stop using the source anymore so once

17:22.580 --> 17:28.820
the data is moved we should be able to switch the proxy to the to another back end to make

17:28.820 --> 17:35.900
it a main storage instead of configuring it in the configuration file.

17:35.900 --> 17:41.860
Robust backup service so if we have two to DCs two sites then we want to synchronize

17:41.860 --> 17:50.100
data between both sites in both directions so the simple setup is to synchronize data

17:50.100 --> 17:58.060
between prod and the backup site so we want to make this tool efficient enough to be robust

17:58.060 --> 18:02.780
backup service like we can ensure that during disaster recovery even if the primary goes

18:02.780 --> 18:07.900
down we can simply do all the operations from the other back ends that are available by

18:07.900 --> 18:16.300
switching the storage back ends based on the based on how they are configured.

18:16.300 --> 18:23.340
So any questions regarding chorus and its implementation?

18:23.340 --> 18:30.860
One question regarding versioning so if you do replication from a source to a destination

18:30.860 --> 18:36.980
and the source has versioning enabled and there's a couple of versions how does this

18:36.980 --> 18:39.580
integrate into the chorus?

18:39.580 --> 18:46.060
So the question is basically about object versioning so if an object version is configured

18:46.060 --> 18:49.260
in the source how do we replicate it right?

18:49.260 --> 18:57.500
So for example in object versioning those are also defined as objects in a hidden bucket

18:57.500 --> 19:05.380
right so that bucket will eventually also be copied with the metadata so the object

19:05.380 --> 19:11.340
which you create initially before the first object it will have metadata and you configure

19:11.340 --> 19:15.900
versioning on it and there is a hidden bucket where all the versions go and we can restore

19:16.740 --> 19:20.740
it to that version anytime so the entire data is copied to the other back end as well with

19:20.740 --> 19:23.740
the metadata so that's how we can ensure.

19:23.740 --> 19:29.740
Yes I'm picking also about backup use case.

19:29.740 --> 19:34.140
Did chorus manage object log?

19:34.140 --> 19:35.140
Can you repeat that for me?

19:35.140 --> 19:38.820
Did chorus manage object log?

19:38.820 --> 19:39.820
Object log.

19:39.820 --> 19:40.820
Object log.

19:40.820 --> 19:41.820
Yes.

19:41.820 --> 19:42.820
Lock.

19:42.820 --> 19:43.820
Lock.

19:43.820 --> 19:44.820
Yes.

19:44.820 --> 19:59.660
I'm sorry I couldn't get that.

19:59.660 --> 20:07.180
Maybe I'm not so much acquainted with that scenario but can you just elaborate about

20:07.180 --> 20:09.740
what do you mean by object locking?

20:09.740 --> 20:21.780
It's a warm technology just the object is in right one time and also it cannot be modified.

20:21.780 --> 20:22.780
Okay.

20:22.780 --> 20:23.780
Read only.

20:23.780 --> 20:24.780
Read only.

20:24.780 --> 20:25.780
Read only.

20:25.780 --> 20:27.980
It's like a malware protection.

20:27.980 --> 20:30.500
The ransomware thing.

20:30.500 --> 20:36.500
That was one of our features that we want to implement so the question is more about when

20:36.500 --> 20:42.300
an object is locked or when there is an attack on the data.

20:42.300 --> 20:50.780
So yeah the ransomware thing is in one of our discussions where so actively the back

20:50.780 --> 20:58.100
end will be exposed more instead of the main storage so the warm or whatever that is introduced

20:58.100 --> 21:04.860
it will be in the back end and then we I mean that's just in discussions we are not yet

21:04.860 --> 21:11.020
there to implement but please feel free to post your question in GitHub.

21:11.020 --> 21:12.620
You can raise an issue.

21:12.620 --> 21:18.300
You can start a discussion in GitHub and then we can definitely take that as a feature with

21:18.300 --> 21:23.900
more details.

21:23.900 --> 21:25.700
These are the resources.

21:25.700 --> 21:33.700
We have open source the back end and then you can definitely reach out to us on GitHub

21:33.940 --> 21:38.340
and then this is a GitHub link for the chorus project.

21:38.340 --> 21:44.420
I'm sure chorus is more than I cannot speak about chorus so much in this 30 minutes but

21:44.420 --> 21:52.380
definitely it's a more efficient tool and it has a lot of capabilities to be acting

21:52.380 --> 21:55.780
as an orchestration layer for multiple back ends.

21:55.780 --> 22:01.060
Like we need not just use we can use one API to talk to multiple different types of back

22:01.060 --> 22:03.020
ends with different vendors.

22:03.060 --> 22:12.660
So yeah we are looking forward for more improvements more features and you can always talk to us

22:12.660 --> 22:20.380
on GitHub and then we can definitely improve this project together.

22:20.380 --> 22:22.820
Thank you so much for this opportunity.

22:33.020 --> 22:38.020
Can I get a correct answer?

22:38.020 --> 22:43.020
While you're migrating you want to implement a load balancing feature.

22:43.020 --> 22:44.020
Yeah.

22:44.020 --> 22:51.020
So you need to be in state of all the objects that you already migrated and that you still

22:51.020 --> 22:54.020
have to migrate to make an informed decision where you should go.

22:54.020 --> 22:57.020
So do you already have like a database or how do you know?

22:57.020 --> 23:02.020
Yeah we are going to do yeah to make the load balancing so that you the request is sent

23:02.020 --> 23:03.020
to the correct.

23:03.020 --> 23:07.020
Yeah so you can like a faster cluster you can go to the new cluster.

23:07.020 --> 23:08.020
Exactly.

23:08.020 --> 23:13.020
It's time to be more mature to add a presentation for the code but it didn't get in 30 minutes.

23:13.020 --> 23:14.020
Yeah I got it.

23:14.020 --> 23:16.020
It was like down how to restore it.

23:16.020 --> 23:17.020
Yeah sure sure.

23:17.020 --> 23:20.020
But it's still like it's really new we have some people there casting we're talking to

23:20.020 --> 23:21.020
start.

23:21.020 --> 23:22.020
Thank you.

23:22.020 --> 23:23.020
Very nice.

23:23.020 --> 23:28.020
At the moment we just need to connect both.

23:28.020 --> 23:30.020
Yeah that would be great.

23:30.020 --> 23:32.020
And we can set up the next speaker.

