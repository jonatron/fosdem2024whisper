WEBVTT

00:00.000 --> 00:11.000
Let's welcome Rakhiv and enjoy it.

00:11.000 --> 00:16.000
How do you?

00:16.000 --> 00:18.000
Thanks guys.

00:18.000 --> 00:20.000
So, hi all.

00:20.000 --> 00:26.000
Today I'll be presenting a talk on container storage interface add-ons, in short CSI add-ons.

00:26.000 --> 00:33.000
We'll be talking on how it extends CSI specification to provide some advanced storage operations.

00:33.000 --> 00:41.000
I'm Rakhiv, I work at IBM, I'm the maintainer of CESI and CSI add-ons and core contributor at Rook.

00:41.000 --> 00:45.000
You can ping me up if you have any doubts later on.

00:45.000 --> 00:52.000
So, in short, first we'll go through what are containers and container orchestration,

00:52.000 --> 00:58.000
increase storage drivers and come to CSI specification and what is CSI.

00:58.000 --> 01:08.000
Then the deployment, next we'll continue with introduction to CSI and some of its Fung operation which we currently support.

01:08.000 --> 01:11.000
First, let's start with container.

01:11.000 --> 01:20.000
So, for containers we usually whenever we are building code or a software we just build it into an executable and we use that.

01:20.000 --> 01:28.000
But executable needs, it has dependencies which I used to be present in the OS to run.

01:28.000 --> 01:36.000
So, to get rid of that we built containers, which has all the dependencies packed in and you can run it almost anywhere.

01:36.000 --> 01:44.000
So, that is a container and we later on came up with how to manage it and scale it up, deploy and stop.

01:44.000 --> 01:52.000
So, there we have container orchestration which usually just automates the deployment, manages it, scales it, upgrades the versions, etc.

01:52.000 --> 01:55.000
And we have a few platforms that are very popular to do it.

01:55.000 --> 01:59.000
Kubernetes, Docker swarm, Apache and Nomad.

01:59.000 --> 02:02.000
This is for container and container orchestration.

02:02.000 --> 02:09.000
So, containers were you and container orchestration were started with being stateless in mind.

02:09.000 --> 02:13.000
That is they can go down at any point and come up back again.

02:13.000 --> 02:24.000
So, there was not a very need for persistent storage there but then they realized this kind of model actually fits perfectly for other applications.

02:24.000 --> 02:29.000
And then came in the need for persistent storage in such a platform.

02:29.000 --> 02:38.000
So, like we have this separate container orchestration platforms like Kubernetes, MeSOS and there's also storage systems like providers like CES,

02:39.000 --> 02:41.000
Longhorn and Gluster.

02:41.000 --> 02:51.000
But so the main problem was like how do they integrate with the Kubernetes platforms, Kubernetes MeSOS.

02:51.000 --> 03:01.000
At the beginning there were increased storage drivers that is like code for each storage systems within the container orchestration platforms.

03:01.000 --> 03:07.000
So, like Gluster would have its code in MeSOS and Kubernetes and Nomad separately.

03:07.000 --> 03:22.000
But likewise with Ceph but that was not scalable because it had a few advantages like we have the code if there was a bug found like if there was a Ceph bug found in Kubernetes.

03:22.000 --> 03:26.000
They had to wait till next Kubernetes release to fix it.

03:26.000 --> 03:32.000
So, it was very hard for them to scale and like develop independently.

03:32.000 --> 03:44.000
So, there it led to the development of CSI like people from different container orchestration platforms came together and they wanted to bring in a single interface, container storage interface,

03:44.000 --> 03:56.000
which would allow like the storage platforms to build just one plugin so that they could just write one implementation and it runs on all the platforms.

03:57.000 --> 04:16.000
So, yeah this CSI specification mainly that's where it started it just specifies APIs to create, delete, mount and mount volumes plus snapshots and storage vendors just needed to develop one single CSI driver to and it will run on all other platforms.

04:17.000 --> 04:25.000
A usual CSI deployment has two things like a provisional deployment and a node plugin demensate.

04:25.000 --> 04:32.000
Provisional mainly handles the creation, deletion, snapshots and resize of volumes.

04:32.000 --> 04:44.000
So, like all these requests of create, delete, resize and snapshot comes from like a Kubernetes entity or a site car or a like a MeSOS or Nomad.

04:45.000 --> 04:55.000
The CSI driver in this case just handles this request is just a API call for it and then node plugin demensate mainly handles just mounting and unmounting one volume.

04:55.000 --> 05:04.000
So, you have one usually a provisional deployment consists of two containers like two pods right a pod is just a bunch of containers running together.

05:04.000 --> 05:08.000
So, you have like two but one is active at a time like with the leader election.

05:08.000 --> 05:18.000
So, if one goes down other one will become active but node plugin demensate is something that runs per node for each node you will need to mount and unmount volumes.

05:20.000 --> 05:21.000
Let's go ahead.

05:21.000 --> 05:23.000
So, this is how it usually looks.

05:23.000 --> 05:26.000
It's basically in Kubernetes.

05:26.000 --> 05:33.000
So, you have nodes there and Kubernetes service with API control manager and snapshot controller.

05:34.000 --> 05:39.000
So, this is a proper CSI driver provisional pod.

05:39.000 --> 05:55.000
So, you have four containers running one of it the CSI plugin is the actual CSI driver that is the code written by the storage vendor and all of the other site cars basically are written for Kubernetes to like interact with this plugin.

05:56.000 --> 06:11.000
So, similarly these four functions of these four containers will be handled differently under container orchestration platform and then similarly we have node plugin demensate so that it can mount volumes onto the node separately.

06:14.000 --> 06:16.000
Let's move ahead.

06:16.000 --> 06:18.000
So, here comes CSI add-ons.

06:19.000 --> 06:40.000
So, like sometimes just the basic operations of creation deletion snapshot mounting unmounting is not sufficient because like CSI driver becomes like a pathway of interaction between the user operation going on in a container platform and the storage system.

06:40.000 --> 06:47.000
So, you can kind of extend it to handle more scenarios that led to the formation of CSI add-ons.

06:47.000 --> 06:55.000
So, one of the why we were starting this project the main problem was like reclaiming space.

06:55.000 --> 07:09.000
The question was to run FS stream on a file system mounted on a RBD device like the crux is that when you delete a file in a file system the file system doesn't immediately release that space back to the block.

07:10.000 --> 07:13.000
It is done later on.

07:13.000 --> 07:21.000
So, like there will be a storage consumption discrepancy between the file system and the block device.

07:21.000 --> 07:28.000
So, if you run FS stream it basically releases the storage and admin can see exactly how much is consumed.

07:28.000 --> 07:32.000
So, that was one of the starting points of how this project came on to be.

07:32.000 --> 07:37.000
So, we again we have to keep it in the same way as the CSI specification.

07:37.000 --> 07:38.000
We have three things.

07:38.000 --> 07:41.000
First is the specification provides the API's.

07:41.000 --> 07:44.000
So, currently we have four services.

07:44.000 --> 07:47.000
One is identity that is definitely required.

07:47.000 --> 07:56.000
It basically allows the CSI driver to advertise and register itself with the CSI add-ons controller.

07:56.000 --> 08:02.000
And then there is three different abilities which we support currently and which I will discuss about later on.

08:02.000 --> 08:11.000
And the main one CSI add-ons controller this will watch and respond to custom resources.

08:11.000 --> 08:21.000
Custom resources are something in Kubernetes where which allows users to have configs and tell the controller what to do.

08:21.000 --> 08:29.000
So, it is Kubernetes specific but it is a way of giving instruction to the CSI add-ons on what needs to be done.

08:29.000 --> 08:38.000
The controller then connects to the sidecar and then the sidecar in turn forwards the request to the CSI driver finally.

08:38.000 --> 08:48.000
So, sidecar also like kind of advertises itself that it is available and allows the controller to connect to the CSI driver.

08:48.000 --> 08:54.000
So, the new deployment with CSI add-ons kind of looks like this.

08:54.000 --> 09:03.000
So, similarly how we had multiple sidecars from Kubernetes we have one more sidecar here and here and we have the controller.

09:03.000 --> 09:11.000
So, this is the new request flow how it works.

09:11.000 --> 09:23.000
So, coming to the first operation reclaim space is just a configuration giving us the volume details and tell a just so.

09:23.000 --> 09:27.000
There is two kinds of thing in reclaim space.

09:27.000 --> 09:30.000
So, one is online operation and offline operation.

09:30.000 --> 09:34.000
Basically being CSI add-ons we do not tell the CSI driver what it needs to do.

09:34.000 --> 09:43.000
We just forward the request and it the storage vendor will implement what it needs to be when they receive the request.

09:43.000 --> 09:51.000
So, currently for online operation at CFCSI we do FS-trim on the file system more volumes specifically.

09:51.000 --> 09:57.000
Other volume more like block mode we do not do anything we just return success.

09:57.000 --> 10:12.000
And for offline if it is a arbiter volume we do RBD specify it basically punches out zeros like not save continuous zero zeroes in the block device to save memory.

10:12.000 --> 10:18.000
So, it is up to the storage vendor to decide what operation needs to be done in the back end.

10:19.000 --> 10:28.000
So, since the operation is reclaim space we need to do something regarding the storage consumption.

10:28.000 --> 10:35.000
So, next one is network fans this came a bit later on but it is also pretty prominent to one.

10:35.000 --> 10:46.000
So, this allows the user or admin or some kind of automation to tell the storage system that this range of IP or IP ranges.

10:46.000 --> 10:56.000
CIDR needs to be block listed and the clients in this range of IP should not be able to communicate with the storage cluster.

10:56.000 --> 11:04.000
So, this plays a critical role in two scenarios one is the first one metro disaster recovery.

11:04.000 --> 11:12.000
So, it is like you have two clusters which has applications of workloads running accessing a single storage cluster outside.

11:12.000 --> 11:15.000
But let us say the cluster A goes down.

11:15.000 --> 11:24.000
So, then we will just fence it off so that it does not read or write from the cluster and there is no data corruption in the storage cluster.

11:24.000 --> 11:27.000
And only B will still be able to access it.

11:27.000 --> 11:38.000
So, we basically if a cluster A has a goes down we just block it and let B still have access to the cluster or other way around depending on the scenario.

11:38.000 --> 11:46.000
And then one more is node loss you can have just one of the nodes going down that time we can just block list one of the nodes.

11:46.000 --> 11:49.000
Next we have volume replication.

11:49.000 --> 11:58.000
So, here again CSI add-ons enables additional operations between the user and the storage system.

11:58.000 --> 12:08.000
So, volume replication is one of those where we are just forward instructions between the user and the storage system.

12:08.000 --> 12:16.000
Here we are basically telling the CSI driver to enable replication for a certain image.

12:16.000 --> 12:22.000
And while image I mean like volume between one two clusters.

12:22.000 --> 12:32.000
So, here we are setting up replication between cluster primary and secondary and letting the storage system know which state the image needs to be in.

12:32.000 --> 12:40.000
So, primary will be primary state and secondary state primary the image will be active and in use.

12:40.000 --> 12:44.000
If the primary cluster goes down we can activate the secondary.

12:44.000 --> 12:56.000
So, there will be CSI drivers and CSI add-ons on both the clusters and user will need to like push this state at two primary when the cluster goes down.

12:56.000 --> 12:59.000
So, then it will take over all the operations.

12:59.000 --> 13:05.000
So, most currently the operation supported like promote, demote and resync.

13:05.000 --> 13:12.000
So, this has become a pretty critical part in disaster recovery.

13:13.000 --> 13:24.000
And the main consumer of this is Sraman right now which works with OCM open cluster management to handle disaster recovery scenarios with multiple clusters.

13:24.000 --> 13:30.000
And this will soon be also supporting volume snapshots.

13:30.000 --> 13:41.000
So, that you can have version snapshots sync between two clusters and then like you it's automatically backup like better than backup.

13:41.000 --> 13:45.000
So, SEPH only sync the diffs between two images.

13:45.000 --> 13:49.000
So, you can have snapshot one taken today and snapshot two taken today.

13:49.000 --> 13:53.000
Only the diffs between those two will actually go into the second cluster.

13:53.000 --> 14:01.000
And if you lose the first cluster can just restore the second snapshot on the secondary cluster and you start using it.

14:01.000 --> 14:05.000
That's one more scenario that's coming soon.

14:05.000 --> 14:15.000
So, future roadmap will soon have maybe rotation of encryption keys.

14:15.000 --> 14:25.000
This is a bit complicated to implement because we need to make sure the encrypted keys are not lost in the process.

14:25.000 --> 14:30.000
So, that is one of the scenarios and next is volume group application.

14:30.000 --> 14:35.000
The volume application which I talked about earlier just was meant for one of the volumes.

14:35.000 --> 14:38.000
Here we will have it specified as a group.

14:38.000 --> 14:50.000
So, we can replicate a whole bunch of volumes together instead of just one because most of the time applications may be using multiple volumes or like applications may be using multiple volumes.

14:50.000 --> 14:58.000
So, like when we when a bunch of applications is running together they may have dependencies on the each other.

14:58.000 --> 15:04.000
So, it implies that data and the volumes also may be dependent on each other.

15:04.000 --> 15:07.000
The third one is repairing corrupted file system.

15:07.000 --> 15:11.000
Like the CSI driver is the one who mounts the volumes and take care of it.

15:11.000 --> 15:16.000
So, it will have kind of good knowledge on how a file system can be repaired.

15:16.000 --> 15:21.000
And so, we may introduce our resource to actually kind of implement this.

15:21.000 --> 15:24.000
This is the end of the future roadmap.

15:24.000 --> 15:27.000
These are few references.

15:27.000 --> 15:29.000
So, do we have any questions?

15:37.000 --> 15:38.000
Okay.

15:38.000 --> 15:40.000
Then thanks for listening.

15:40.000 --> 15:42.000
Thanks so much.

