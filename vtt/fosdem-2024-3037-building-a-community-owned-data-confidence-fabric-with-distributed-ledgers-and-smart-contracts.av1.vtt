WEBVTT

00:00.000 --> 00:15.080
Okay, so the next topic is building community-owned data confidence fabric with distributed ledgers

00:15.080 --> 00:16.400
and smart contracts.

00:16.400 --> 00:20.200
So let's give them a good welcome.

00:20.200 --> 00:26.120
Hi, everyone.

00:26.120 --> 00:27.120
Thanks for staying.

00:27.120 --> 00:32.880
So today we're going to be talking about data confidence fabrics.

00:32.880 --> 00:34.160
Here's the agenda for the day.

00:34.160 --> 00:41.320
So briefly, first of all, I refer to my colleague Sean here as an academic who works for University

00:41.320 --> 00:43.200
College Cork.

00:43.200 --> 00:49.360
We've been collaborating on this project, so I'm going to start by quickly going over

00:49.360 --> 00:54.600
dense open source support and community contributions.

00:54.600 --> 01:02.480
I'm going to go over Linus Foundation Edge, how it started, and I'm going to be focusing

01:02.480 --> 01:07.160
on Project Alvarium, which is part of Al of Edge.

01:07.160 --> 01:08.840
Then I'm going to hand over to Sean.

01:08.840 --> 01:14.120
He will go over a few use cases of Project Alvarium.

01:14.120 --> 01:20.440
And at the end, we have a short demo that we're going to show you.

01:20.440 --> 01:25.760
So the team I work in, we do research mostly in European research projects.

01:25.760 --> 01:32.880
Most of the projects, all of them are funded by the European Commission, multiple programs,

01:32.880 --> 01:36.640
mostly Horizon Europe program.

01:36.640 --> 01:42.360
We collaborate with a lot of the consortium's basically a lot of partners across Europe,

01:42.360 --> 01:47.000
across all these projects we've collaborated with over 100 partners.

01:47.000 --> 01:54.800
The domains that we work on are mostly edge computing and cloud, storage, the zeroed ledgers,

01:54.800 --> 01:59.560
and quite recently we were working on knowledge graphs.

01:59.560 --> 02:05.320
And a common theme across projects is always energy efficiency and sustainability.

02:05.320 --> 02:11.240
Most of the projects here deal with orchestrating deployment across cloud and edge resource

02:11.240 --> 02:12.880
constraint devices.

02:12.880 --> 02:15.080
I'm currently working in the Clever project.

02:15.080 --> 02:21.400
We're trying to build knowledge graphs that represent Kubernetes clusters

02:21.400 --> 02:27.920
and use these knowledge graphs, try out graph algorithms if we see that it enhance the schedulers

02:27.920 --> 02:32.520
in Kubernetes.

02:32.520 --> 02:38.600
And yeah, the themes are mostly, as I just said, this is just quickly about the projects.

02:38.600 --> 02:42.600
And one thing is that most of the work that we do on these projects, most of the code

02:42.600 --> 02:46.400
is open sourced.

02:46.400 --> 02:52.880
Now Dell and open source, Dell has been working with the Linux Foundation for over the past

02:52.880 --> 02:55.960
decade.

02:55.960 --> 03:01.640
It's a founding member of open program with infrastructure up in HBC, as well as member

03:01.640 --> 03:06.600
of the CNCF up in SSF and you can make for you Sonic and the Yachto projects.

03:06.600 --> 03:11.480
Currently Dell is involved in 43 open source projects, 10 of which are Linux Foundation

03:11.480 --> 03:12.980
projects.

03:12.980 --> 03:17.480
Now in preparation for this, I went to asking colleagues about the individual contributors

03:17.480 --> 03:24.320
across the company, about their contributions, and I learned that currently there's a process

03:24.320 --> 03:30.600
in place to inject open source contributions into my legal, which is our legal compliance

03:30.600 --> 03:35.800
systems, ServiceNow and JIRA, to basically encourage contributions from the company so

03:35.800 --> 03:38.640
that it be logged.

03:38.640 --> 03:45.760
There's also other activities such as events organized within Dell about open source.

03:45.760 --> 03:52.280
One of the colleagues contributed a first time contributing page to just help other basically

03:52.280 --> 03:57.920
contributors to make their first contribution.

03:57.920 --> 04:02.200
And what I learned as well is that the most effective way as of now is across the company

04:02.200 --> 04:03.880
is word of mouth.

04:03.880 --> 04:08.000
So it's driven by interest mostly.

04:08.000 --> 04:17.320
So Dell contributions to the Linux kernel as originally a hardware manufacturer, basically

04:17.320 --> 04:26.200
when Dell needs to develop drivers to make sure that Ubuntu runs properly on a Dell system.

04:26.200 --> 04:28.360
So these drivers are open sourced.

04:28.360 --> 04:29.880
The process goes as follows.

04:29.880 --> 04:37.800
Basically these drivers are basically developed, contributed, and they're pushed back to the

04:37.800 --> 04:43.440
Linux kernel so that they work across all the distributions.

04:43.440 --> 04:49.360
After their first implementation, any patches to make sure they all work properly are also

04:49.360 --> 04:55.760
pushed back to the kernel so that it would work with distributions like mostly working

04:55.760 --> 05:01.520
first on Ubuntu and then as they're pushed back to the kernel so that they work with

05:01.520 --> 05:08.400
Fedora or Debian or Arch or any other distro.

05:08.400 --> 05:16.400
Now the story of ILOF Edge, it started in 2017 when Dell contributed 125,000 lines of code

05:16.400 --> 05:22.920
to Linux Foundation to create AdJax Foundry due to its interest in Edge computing, which

05:22.920 --> 05:33.760
is basically an open source platform for basically processing across the Edge and the Cloud.

05:33.760 --> 05:39.600
This platform basically enables Edge data collection from sensors, communication between

05:39.600 --> 05:48.880
enterprise and cloud and on-prem data centers, and for processing at the Edge.

05:48.880 --> 05:57.200
It has runtimes, messaging substrates and so on.

05:57.200 --> 06:02.880
And it's basically an umbrella organization for other projects.

06:02.880 --> 06:08.840
One project of ILOF Edge is Project Alvarium, which we're going to discuss now, AdJax Foundry,

06:08.840 --> 06:14.320
which what started the whole thing and other projects.

06:14.320 --> 06:20.200
Now to go on to Alvarium, first the problem statement.

06:20.200 --> 06:26.640
So basically the motivation for Alvarium came from the realization that a lot of the projects

06:26.640 --> 06:34.800
that we're involved in, we deal with data sources that are dispersed, the sensors come

06:34.800 --> 06:41.960
from different manufacturers, sometimes sensors are owned by different organizations, and

06:41.960 --> 06:48.920
usually the data is processed as it jumps through the network.

06:48.920 --> 06:57.200
Before it reaches AWS S3, let's say it can be processed at a local server and so on.

06:57.200 --> 07:03.320
So we needed a way to measure the trust of how much we can trust those data points that

07:03.320 --> 07:08.520
are coming from these several data sources.

07:08.520 --> 07:14.560
So this was the origin of the idea of a data confidence fabric.

07:14.560 --> 07:20.320
So the idea is to define what trust is basically.

07:20.320 --> 07:25.200
So as I said, more implications are extensively distributed.

07:25.200 --> 07:27.360
Data traverses across the network.

07:27.360 --> 07:32.000
The idea is to create metadata that attests the verifiability of the data at the origin.

07:32.000 --> 07:35.920
That's the first step.

07:35.920 --> 07:41.880
The second thing is to create metadata describing how the data was processed as it jumps throughout

07:41.880 --> 07:42.880
the network.

07:42.880 --> 07:51.360
At any point the data is touched, we need to inject metadata at what happened here.

07:51.360 --> 07:55.320
And those are the insertion points.

07:55.320 --> 08:00.200
And we need a way to measure or quantify that trust into a floating point.

08:00.200 --> 08:07.160
So we need to put it over 10 and over a certain confidence score so that this confidence score

08:07.160 --> 08:14.200
can be used by the end user to decide how to actuate or do anything with that data.

08:14.200 --> 08:17.640
So to summarize what I just said, there are two things here.

08:17.640 --> 08:22.520
One is a policy defining the measure of trust.

08:22.520 --> 08:29.960
And the other is an implementation that interprets that policy and calculates the trust score.

08:30.960 --> 08:37.720
And the insertion points that I just discussed, so in this figure here we see that first we have the

08:37.720 --> 08:44.280
route of trust, which is the sensor that signs the data when it catches it.

08:44.280 --> 08:51.040
The gateway, the data goes from all these sensors to a gateway where authentication

08:51.040 --> 08:52.840
or authorization happens.

08:53.840 --> 09:02.640
At the gateway, the Alvarium SDK would be used to inject basically the data capture environment

09:02.640 --> 09:05.080
where the data was captured.

09:05.080 --> 09:09.560
Next the data would travel on to an edge server or a distributed storage.

09:09.560 --> 09:16.000
Here like for secure immutable, scalable edge persistence such as IPFS would be there.

09:16.000 --> 09:20.640
And then the fifth injection point would be a ledger where the trust would be registered.

09:21.640 --> 09:27.640
A more concrete example of this or what trust are we talking about those, what are these policies?

09:27.640 --> 09:35.640
So basically at the gateway what we have is if there's TPM on the source device, you get a score of one.

09:35.640 --> 09:39.640
If there's secure boot, you get a score of one.

09:39.640 --> 09:43.640
If the data was registered in a ledger, you get a score of one.

09:43.640 --> 09:46.640
Then at the edge server, more things get added.

09:46.640 --> 09:54.640
If there's an application running on that server that's encrypting the data before it travels, you get an additional score.

09:54.640 --> 09:58.640
And if the signature was verified at this point, you get an additional point.

09:58.640 --> 10:07.640
So this is the idea of injecting, of calculating that trust measure as the data is traveling throughout the network.

10:07.640 --> 10:11.640
And at the end, you would get a score, a confidence score.

10:11.640 --> 10:23.640
If the data matches or basically satisfies all these policies that we set for it, then you would get six out of six.

10:23.640 --> 10:34.640
If there's no TPM on the device, it would get five out of six so that the consumer of the data would kind of know basically that there's something missing.

10:34.640 --> 10:39.640
Or there's something that would lead to a certain issue with the data.

10:39.640 --> 10:42.640
And of course, these are just dummy weights that we're using here.

10:42.640 --> 10:45.640
It was scoring one for every one of these policies.

10:45.640 --> 10:54.640
You could configure this to basically weigh something more than the other and so on.

10:54.640 --> 10:56.640
The different use cases of LVARM.

10:56.640 --> 11:01.640
So one is internal quality and security control.

11:01.640 --> 11:04.640
The second is regulatory compliance.

11:04.640 --> 11:12.640
So to get a very viable percentage of data that it should meet a certain threshold.

11:12.640 --> 11:22.640
Marketplace application, like if you're selling your data or any IoT data, these trust measures would help basically.

11:22.640 --> 11:24.640
Make others trust the data more.

11:24.640 --> 11:34.640
Trusted actuation, if it's a real-time application and your data is going right into a certain actuation.

11:34.640 --> 11:44.640
So you would use this score to know if you trust to do that action based on that measure or that temperature reading, let's say.

11:44.640 --> 11:54.640
And the final one is a trusted ecosystem partner where your metrics could factor into trust ratings by using one's product or service.

11:54.640 --> 12:00.640
There's multiple implementations of LVARM and we'd be happy if anyone's interested to have a look at them.

12:00.640 --> 12:04.640
There's a Go implementation, the Java, Python one.

12:04.640 --> 12:07.640
And Rust one is on the way.

12:07.640 --> 12:09.640
These are the links.

12:09.640 --> 12:13.640
There's the website and all those GitHub repos with examples.

12:13.640 --> 12:15.640
So if anyone's interested.

12:15.640 --> 12:22.640
Now I'm going to hand over to Sean to walk through some use cases.

12:22.640 --> 12:23.640
Thanks.

12:23.640 --> 12:33.640
So I'm Sean O'Murphy from the University College Cork and we're partners with Dell and a whole bunch of other research institutions, universities, industry,

12:33.640 --> 12:42.640
in this European project, the Collaborative Edge Cloud Continuum and the Bettered AI for a visionary industry of the future, which we call Clever because that's too much of a multiple.

12:42.640 --> 12:53.640
Now the idea with Clever is that we're exploring technologies where work is being done down at the edge, up at the cloud, that has been passed up and down,

12:53.640 --> 13:01.640
and decisions being made about what to do with it, and generally the applications are AI and machine learning applications.

13:02.640 --> 13:11.640
So from the open view, we have some use cases with the data confidence that data confidence fabric like Alvarian can give you.

13:11.640 --> 13:18.640
So it allows you to mix and match data from old hardware, new hardware, different firmware versions,

13:18.640 --> 13:30.640
and some of this may be more or less secured, there may be historical information about vulnerabilities that may make you consider some historical data differently than you might if it was up to date.

13:31.640 --> 13:38.640
You also may have an application where you're accepting contributions from a whole mix of different sources.

13:38.640 --> 13:48.640
So rather than a single organization having complete ownership and control over a whole network, you could have accepting public sources,

13:48.640 --> 13:56.640
so you know, Citizen Science, the universe, that kind of thing, where you could have expert users and you could have lay people contributing together.

13:56.640 --> 14:06.640
They could also be using, you know, publicly available consumer sensors mixed with high-end professional grade stuff.

14:06.640 --> 14:14.640
Also, you may be dealing with datasets where you want to be able to be sure that you have permission to use everything within it.

14:14.640 --> 14:22.640
So it could be a case where you're collecting data to train models of it, and you want to make sure that you have the correct licensing for all of the material within it,

14:22.640 --> 14:25.640
so we think annotations could be very useful for that.

14:25.640 --> 14:33.640
So this could save you from getting in difficulties down the line by using things you shouldn't have been using.

14:33.640 --> 14:41.640
So from UCC, we're working with models and data conference for mixed trust applications.

14:41.640 --> 14:46.640
So many applications and models, they like to have a lot of data to work with, so this makes intuitive sense.

14:46.640 --> 14:52.640
You have lots of material, that's more scope to learn things about the domain that you're trying to train on.

14:52.640 --> 14:59.640
However, not all data comes from the same place, so like I said, you'd have different sensors, different firmwares, old hardware, new hardware.

14:59.640 --> 15:06.640
Something might have been installed by a certified engineer or might have been something installed by a home user.

15:06.640 --> 15:08.640
So you can have a mix of all these different sources.

15:08.640 --> 15:18.640
So like I said, some of it you can trust highly, some of it maybe not so much, but a lot of data generally makes for a better model.

15:18.640 --> 15:23.640
So in the illustration here, I just have an example of we have data coming from out of different sources.

15:23.640 --> 15:31.640
Most of it might be low trust score data, so it's the larger red circle, and some could be a little bit more trusted in the yellow,

15:31.640 --> 15:36.640
and then in the green that's maybe the best quality or the most trustable stuff that we are aware of.

15:36.640 --> 15:45.640
Now we could train a model just with the trust score 6 material, which is the smallest, so we could say we're very confident with all the material that goes into this learning.

15:45.640 --> 15:50.640
However, it's the least amount of material compared to the alternatives we have.

15:50.640 --> 15:54.640
So you may get good results or you may get sort of limited results.

15:54.640 --> 15:57.640
And it's dependent on what you're working with and what your application is.

15:57.640 --> 16:04.640
But generally most models will do better when they have more to learn from, but if there is some poor quality or possibly malicious material in your larger set,

16:04.640 --> 16:09.640
you want to do something to avoid that contributing over much to your model.

16:09.640 --> 16:16.640
So we take our trust scores, which the data conference fabric can give you by annotating the data based on its provenance and its history and security characteristics.

16:16.640 --> 16:19.640
And we use this as an additional input into our model.

16:19.640 --> 16:25.640
So most machine-earning AI models, they allow you to weight inputs, either through integers or super sampling or so on.

16:25.640 --> 16:30.640
So depending on your application, you might be more interested in one kind of annotation versus another.

16:30.640 --> 16:38.640
So you might be more interested in stuff that has been signed and gone through like secure socket layer, come from devices with just platform models installed.

16:38.640 --> 16:45.640
Or you may not worry too much about if there might be something like the difference between security and safety, very important.

16:45.640 --> 16:48.640
You want to make sure your material is very good and very trustable.

16:48.640 --> 16:56.640
Something more like optimization, where a catastrophe in optimization is a small loss of money and carbon,

16:56.640 --> 17:00.640
but loss in safety and security could be much more serious.

17:00.640 --> 17:08.640
So we have a case study from our work using data conference as an input and machine-earning.

17:08.640 --> 17:11.640
So we have decided to mix trust.

17:11.640 --> 17:17.640
So we're accepting material from the low trust data set combined with the material from the high trust data set.

17:17.640 --> 17:22.640
But we are using the trust ways to decide how much it contributes to the model.

17:22.640 --> 17:29.640
So we have an experimental set up here where we take an existing data set and a portion of it we poison.

17:29.640 --> 17:37.640
So we actually manipulate it by changing results in order to cause a sort of malicious result in our resulting model.

17:37.640 --> 17:42.640
But because we have annotations to data conference fabrics to work with,

17:42.640 --> 17:50.640
we can make sure that I had a potential to have been altered in this way as a commensurate low trust weighting audit.

17:50.640 --> 17:58.640
So the idea here is to combine the stuff that may have been manipulated or malicious in nature with the stuff that we know is relatively clean high trust

17:58.640 --> 18:03.640
and make sure we have a large data set that has a combination of high trust and low trust,

18:03.640 --> 18:08.640
but we're going to limit how much the low trust stuff contributes without discrediting it entirely.

18:08.640 --> 18:14.640
So our experimental set up here is we have the census data for California Housing in 1990.

18:14.640 --> 18:19.640
And our objective here is can we predict the median value of housing based on the other fields.

18:19.640 --> 18:22.640
So the latitude, latitude, ocean proximity and so on.

18:22.640 --> 18:27.640
And the poisoning here is to adjust the latitude by two degrees northwards.

18:27.640 --> 18:30.640
So the effect of this, if you're a malicious actor and you wanted to wreck a model,

18:30.640 --> 18:33.640
if you applied this to material that was contrary to the model,

18:33.640 --> 18:36.640
what you would do is you would say LA is somewhere in the middle of the desert,

18:36.640 --> 18:39.640
San Diego is up the coast and no man's land and so on.

18:39.640 --> 18:45.640
So the impact on your model is that the predictions for the median values get pretty out of whack.

18:45.640 --> 18:52.640
And so the results here, the blue line is a baseline where it would be the best case where everything was perfect and not manipulated.

18:52.640 --> 18:58.640
And then you can see when we just used the clean set, which is the green result here,

18:58.640 --> 19:03.640
the more and more of your data set that may have been poisoned, the more of it you are disregarding entirely,

19:03.640 --> 19:06.640
which means your clean data set is getting smaller and smaller and smaller.

19:06.640 --> 19:12.640
So once you get past 50%, it starts to degrade because the amount of material it can learn from is getting smaller.

19:12.640 --> 19:18.640
On the other side, then we have the poison data set where we are incorporating the poison stuff with the clean stuff.

19:18.640 --> 19:23.640
This degrades pretty badly as well because the poison stuff is malicious in nature,

19:23.640 --> 19:25.640
it was intended to cause a bad result.

19:25.640 --> 19:28.640
And then the trust result, which is the red triangles here,

19:28.640 --> 19:31.640
these are when we're actually using the trust as an additional input.

19:31.640 --> 19:38.640
So we're accepting the material that may have been poisoned, but it's been weighed more low compared to the clean stuff.

19:38.640 --> 19:44.640
So we get the benefit of all of the clean material, all of the potentially poison stuff,

19:44.640 --> 19:50.640
some of which does have good explanatory power within it because it wasn't necessarily altered,

19:50.640 --> 19:53.640
it could have been no trust but still good quality,

19:53.640 --> 19:58.640
or there are parts of it that hadn't been manipulated that still give you good information.

19:59.640 --> 20:06.640
So from our side in the University College of Cork, we have some future directions in using data conference fabric,

20:06.640 --> 20:09.640
so we think it's pretty interesting technology in a cool area.

20:09.640 --> 20:15.640
So usually in a zero trust environment, you're using this to decide whether or not to use something,

20:15.640 --> 20:19.640
whereas we think there's something interesting to be done in use it a little bit,

20:19.640 --> 20:22.640
or use it as much as is appropriate for the application in mind.

20:23.640 --> 20:30.640
We also think the trust scoring, so we saw you could trust something based on your choice of weightings for certain security features,

20:30.640 --> 20:35.640
but what if those weightings could be something that could be learned on a per application or per organization basis?

20:35.640 --> 20:40.640
There might be some way to do this through iterative modelling,

20:40.640 --> 20:44.640
or feedback loops, something communicating between the edge and the cloud and back again,

20:44.640 --> 20:46.640
based on the actual results of what you're doing.

20:47.640 --> 20:50.640
And we have some interesting in some new types of annotations,

20:50.640 --> 20:56.640
so what you see with a researcher as Khalid is working in this area exploring different kinds of annotations,

20:56.640 --> 20:59.640
maybe there's some new novel approaches we can take here.

21:00.640 --> 21:06.640
And so we have some ideas about the performance of the models themselves being a way to calibrate the weights.

21:07.640 --> 21:13.640
So based on how well your model is doing, is that telling you something about a particular annotation that appears quite a lot?

21:17.640 --> 21:22.640
We have some of our contract demo using Gilverin Harrison.

21:25.640 --> 21:31.640
Yeah, so now I'm going to show a five minute demo.

21:40.640 --> 21:42.640
Okay, so this demo has...

21:42.640 --> 21:44.640
Oh, what happened?

21:56.640 --> 22:02.640
So with this demo I'm using Hedera and Project Alvarium,

22:02.640 --> 22:08.640
so Hedera is a serial ledger that is basically...

22:08.640 --> 22:11.640
It uses the hashgraph, the serial consensus algorithm,

22:11.640 --> 22:18.640
what makes... we chose it because it's consensus in Hedera is much faster than other ledgers,

22:18.640 --> 22:20.640
and because it has public subscribe semantics.

22:21.640 --> 22:28.640
Now, Alvarium works with any messaging, like middleware, like Kafka or Polsar,

22:28.640 --> 22:33.640
or a... let me pause it for a minute.

22:33.640 --> 22:38.640
Okay, it works with anything that supports public subscribe semantics.

22:38.640 --> 22:41.640
In this case we're using a distributed ledger, it doesn't have to be a ledger,

22:41.640 --> 22:43.640
but the ledger here adds some more trust.

22:44.640 --> 22:52.640
So what I just did here is I started a service that created a smart contract on the Hedera ledger,

22:52.640 --> 23:00.640
and it created a new topic on Hedera so that the sensor can publish to that topic.

23:04.640 --> 23:11.640
And the smart contract is used for automated billing of trust services,

23:11.640 --> 23:16.640
or the annotations that are injected into the data as it's flowing,

23:16.640 --> 23:25.640
and the topic ID is for Alvarium SDK basically to publish the annotations that is...

23:25.640 --> 23:27.640
It's adding the data to that topic.

23:27.640 --> 23:29.640
The smart contract that the devices...

23:30.640 --> 23:32.640
So those are the two...

23:34.640 --> 23:39.640
And then what I'll do here is I'll just update the config for the other two apps

23:39.640 --> 23:44.640
to use those contract ID and the topic ID.

23:49.640 --> 23:54.640
Then I will go and run the UI, basically it's a React app

23:54.640 --> 24:07.640
that basically subscribes to that topic to get the annotations and to the smart contract.

24:10.640 --> 24:11.640
Just a second.

24:11.640 --> 24:17.640
So here I'm showing the wallet of the user which has some HBuy,

24:17.640 --> 24:19.640
which is the cryptocurrency used in Hedera,

24:19.640 --> 24:23.640
and the wallet of the trust provider which is that in this case.

24:23.640 --> 24:28.640
And you would have a fee which is registered in the smart contract

24:28.640 --> 24:34.640
that is going to be paid by the customer based on the annotations

24:34.640 --> 24:37.640
that are added to the sensor data that it's using.

24:38.640 --> 24:44.640
And the gas fees are paid to the network for publishing to the topic.

24:46.640 --> 24:51.640
So here what you'll see is the source of the data, the reading,

24:51.640 --> 24:55.640
the auditing, the hash, and the annotations that are added.

25:01.640 --> 25:05.640
So now I'm running a simulated sensor that is generating dummy data

25:05.640 --> 25:08.640
and sending that dummy data to the ledger.

25:08.640 --> 25:15.640
This is the dummy data generated and annotated by the SDK, by Alvarium.

25:16.640 --> 25:18.640
And they're going to be sent over.

25:18.640 --> 25:27.640
And then I'll switch now over to the UI to see them pop up here.

25:30.640 --> 25:35.640
And you would see now that the billing is happening there.

25:35.640 --> 25:38.640
The fee is being automated.

25:39.640 --> 25:46.640
And if you view the annotations there, you would see the scores that are added to the data point.

25:48.640 --> 25:51.640
So I'll view them now and I'll be able to see it.

25:53.640 --> 26:00.640
As you can see here, it's a Boolean where what's the kind of the trust annotation here.

26:00.640 --> 26:05.640
It's TPM first. It's false. You don't get a point for this and so on.

26:08.640 --> 26:15.640
If we look at the wallet, we can see a certain fee transferred from customers account to Dell's account.

26:16.640 --> 26:18.640
So here are they coming in?

26:20.640 --> 26:31.640
I set up another one here where here the billing is happening automatically after every annotation or every data point as it comes in.

26:31.640 --> 26:42.640
But I noticed that the publisher ends up paying a lot of gas fees because every time they're doing a transaction.

26:43.640 --> 26:48.640
So what you can do is pay in bulk in the second small demo.

26:55.640 --> 27:01.640
Basically they start coming in. You don't see nothing going out of the wallet of the publisher.

27:01.640 --> 27:19.640
And then after you fetch a few, you can see the amount you up top and this is how much is due for the pay.

27:19.640 --> 27:26.640
And at a certain point you just pay the bill at once and it's a single transaction.

27:26.640 --> 27:29.640
Account balance has increased by 21.

27:29.640 --> 27:32.640
And that's it.

27:34.640 --> 27:35.640
Thank you guys.

27:35.640 --> 27:38.640
What happens when the button is clicked is...

27:42.640 --> 27:52.640
So if you want to reach out, those are our emails. If you're interested in the project, please reach out.

27:52.640 --> 27:58.640
If you have any interesting project that you think we might be interested in as well, reach out to us.

27:58.640 --> 28:01.640
And if you have any questions, yeah.

28:01.640 --> 28:02.640
Yeah.

28:15.640 --> 28:22.640
So an example of the trust scores, you could also have them not be just zero or one or range to create new ones, right?

28:22.640 --> 28:23.640
Save hang on.

28:23.640 --> 28:30.640
When you were attributing trust scores, you could also configure them to be not just one or zero, but also range in between?

28:30.640 --> 28:33.640
Exactly. Any floating number.

28:33.640 --> 28:44.640
In such a case, how do you verify that the same factors are causing the same amount of changes in trust factors from different sources?

28:45.640 --> 28:54.640
Like if a vulnerability in one source is attributing the same amount of trust as if it's coming from a different source in a different edge node.

28:57.640 --> 29:01.640
So if you're attributing a different number, okay.

29:01.640 --> 29:12.640
If there's one thing that's causing a decrease in trust, but that the decrease is equivalent no matter where the decrease in trust is viewed.

29:15.640 --> 29:17.640
Hello.

29:23.640 --> 29:30.640
My assumption here is that the ways would be defined by the verifier only.

29:30.640 --> 29:31.640
Exactly.

29:31.640 --> 29:32.640
So that is it.

29:32.640 --> 29:34.640
So you have a certain number of policies.

29:34.640 --> 29:35.640
You're coming in.

29:35.640 --> 29:36.640
You want to check that.

29:36.640 --> 29:39.640
So at once, you have all the metadata, all the annotations.

29:39.640 --> 29:45.640
The annotations are just telling me if there's encryption or not, what type of encryption is there.

29:45.640 --> 29:50.640
If there's any other policy there, if the signature was verified and so on.

29:50.640 --> 29:59.640
So at once, I would weight all these based on what I want to weigh them and I would calculate the score.

29:59.640 --> 30:03.640
So the scoring is not happening as it's flowing through.

30:03.640 --> 30:13.640
What is happening as the data, like the points where the data is touched throughout the network, the gateway, the whatever server the data passed through before coming to storage.

30:14.640 --> 30:18.640
All I'm getting is the metadata and at once at the end, I'm scoring it.

30:21.640 --> 30:22.640
Yeah.

30:23.640 --> 30:26.640
So thanks for the talk.

30:27.640 --> 30:37.640
I believe you mentioned that you also use this technology for license compliance and some, I would assume licenses for data sets.

30:38.640 --> 30:41.640
I think this is the first time I hear something about this.

30:41.640 --> 30:42.640
So I'm really interested.

30:42.640 --> 30:45.640
Could you please elaborate a bit on that?

30:45.640 --> 30:48.640
How does it look like, at least on a high level?

30:48.640 --> 30:51.640
So we're very early on on this.

30:51.640 --> 30:58.640
But I think the usual thing with the data conference fabric idea is you can make an annotation for pretty much anything you care about.

30:59.640 --> 31:03.640
So if what you're interested in is I have a data set.

31:04.640 --> 31:09.640
I want to verify which of it has been either say properly licensed.

31:09.640 --> 31:12.640
Well, there's going to be some sort of process to verify those licenses.

31:12.640 --> 31:20.640
And that's a step that if that step is complete at the time that it's requested, that's an annotation that gets added to the history of that piece of data.

31:21.640 --> 31:29.640
And then at a subsequent point, if I'm trying to create and collect the data set from all the material that I have, I can say just give me all the stuff that had that annotation associated with it.

31:30.640 --> 31:35.640
Now I have a data set that I know at the time of Providence that it passed that particular check at that time.

31:36.640 --> 31:37.640
Now, how do you go about that check?

31:37.640 --> 31:42.640
I mean, that's up to the developer, I suppose.

31:43.640 --> 31:49.640
But the idea here is that the data conference fabric gives you the framework to approach things like this.

31:51.640 --> 31:55.640
And so I think this is very important when you're collecting things from a mix of public and private sources.

31:55.640 --> 31:58.640
It could have come from many different paths to where you are.

31:59.640 --> 32:04.640
You want to make sure that everything's above board and you don't get yourself into a sticky situation using things that you shouldn't have been using.

32:05.640 --> 32:13.640
And also we're starting to see tools like Nightshade and Glaze that are attempting to make that stuff worse to work with for very good reasons.

32:14.640 --> 32:18.640
And so you can make sure that you're using good quality material that you have the permission to use.

32:19.640 --> 32:26.640
And that means models that you've been training on that stuff you know as well that these are all above board and fair to use.

32:27.640 --> 32:34.640
Yeah, just something to be clear.

32:35.640 --> 32:39.640
If I well understood, it's not you who are providing the scoring, right?

32:40.640 --> 32:49.640
So the framework gives you a way to have the annotations up in a ledger or a subscription broker or something like that.

32:49.640 --> 33:01.640
Trust scores, they can be generated and added to your ledger or they can be computed at any time that a decision is being made about that piece of data that could be happening anywhere.

33:02.640 --> 33:07.640
So you can request the scores that have been pre-computed or you can make your own trust score algorithm.

33:08.640 --> 33:12.640
And in that case you just request the annotations associated with all the material you're dealing with and produce your own scores.

33:13.640 --> 33:17.640
So there's flexibility there and you can have trust scores based on a per application basis as well.

33:17.640 --> 33:22.640
Okay, and so I have a question about smart contract and Mario in your blockchain use case.

33:23.640 --> 33:31.640
I mean, why to use your contract, your smart contract to only inscribe data on chain?

33:32.640 --> 33:37.640
What is the interest for your database to write the scoring on chain via smart contract?

33:38.640 --> 33:43.640
Why are you using it? You can just like verify in the database.

33:43.640 --> 33:54.640
And also a second question which I think is related if you are really providing verifiable and high level data, can you act as an oracle?

33:55.640 --> 34:05.640
And you can begin an oracle into the blockchain to have secured and already noted annotated data which we can use.

34:06.640 --> 34:10.640
Are you thinking about this type of use case as a oracle?

34:10.640 --> 34:17.640
Okay, so the first question, why are we using a smart contract?

34:18.640 --> 34:26.640
As I mentioned, a ledger and a smart contract are not essential to, it was just the use case here.

34:27.640 --> 34:39.640
So the main purpose of the contract was just to automate billing for any trust services that have been for, if anyone wants to package up this SDK and provide it as a service.

34:41.640 --> 34:50.640
And keep it distributed. They can use a ledger to automate the billing process for the use of those annotate, as per annotation.

34:51.640 --> 34:55.640
So the only reason we're going on chain is to just track those annotations.

34:56.640 --> 35:03.640
But as you said, there's storing on chain is inefficient if there's a bulk of data that's going on chain.

35:03.640 --> 35:14.640
And for the second one as an oracle, to be honest, I'm not quite sure how oracles work. I'm familiar with oracles.

35:15.640 --> 35:24.640
They're also peer to peer, but I'm not quite familiar with how they work. So how, how a very amount would contribute to being an oracle.

35:24.640 --> 35:27.640
So, yeah.

35:38.640 --> 35:42.640
Okay, so no more questions. So again, let's have another round of applause.

35:54.640 --> 35:55.640
Thank you.

