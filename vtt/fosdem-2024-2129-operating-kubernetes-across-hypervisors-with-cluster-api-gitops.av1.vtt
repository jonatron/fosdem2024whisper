WEBVTT

00:00.000 --> 00:10.000
Hi everyone.

00:10.000 --> 00:15.000
Hi everyone.

00:15.000 --> 00:20.000
Welcome to this talk on Cluster API across hypervisors and with GitOps.

00:20.000 --> 00:24.000
So we've got a lot of the hype words in there.

00:24.000 --> 00:26.000
My name's Richard.

00:26.000 --> 00:30.000
I'm a maintainer of various Cluster API providers,

00:30.000 --> 00:37.000
more notably the AWS provider, the GCP provider and the RKE2 provider.

00:37.000 --> 00:41.000
Hey, I'm Alex. I work together with Richard at SUSE.

00:41.000 --> 00:47.000
I'm a KPI operator maintainer and also maintaining RKE2 provider.

00:47.000 --> 00:55.000
So today we are going to talk about Cluster API.

00:55.000 --> 00:58.000
This is only for the stream.

00:58.000 --> 01:00.000
But just to speak louder.

01:00.000 --> 01:05.000
So today we are going to speak about Cluster API, GitOps and couple virtualization providers.

01:05.000 --> 01:09.000
So we'll briefly talk about what is KPI.

01:09.000 --> 01:11.000
There was a previous talk about this.

01:11.000 --> 01:16.000
But just in case you haven't been there, we will repeat something again.

01:16.000 --> 01:22.000
We will tell you and show how Proxmox will integrate with KPI

01:22.000 --> 01:24.000
and how GitOps can be added to there.

01:24.000 --> 01:28.000
And then we'll replicate the same process with KubeVirt

01:28.000 --> 01:35.000
to show that KPI can work with different infrastructure providers.

01:35.000 --> 01:42.000
Cool. So all the demos, well the two demos in this session are available via this repo.

01:42.000 --> 01:44.000
Feel free to take a picture of it.

01:44.000 --> 01:47.000
It's got the full script for it.

01:47.000 --> 01:51.000
So you can actually run this yourselves when you get home.

01:51.000 --> 01:55.000
I'll leave that out for a second.

01:55.000 --> 01:59.000
So who was in the last talk about the intro to Cluster API?

01:59.000 --> 02:01.000
Okay, cool.

02:01.000 --> 02:05.000
So you get the idea that you have a management cluster.

02:05.000 --> 02:07.000
Oh, yep, sorry.

02:07.000 --> 02:09.000
You have an idea that you have a management cluster

02:09.000 --> 02:12.000
and to that management cluster you install Cluster API.

02:12.000 --> 02:16.000
Now Cluster API is made up of the core controllers and a bunch of providers.

02:16.000 --> 02:20.000
And you can mix and match those providers to meet your needs.

02:20.000 --> 02:25.000
So if you have your provision in AWS, you just install the AWS provider.

02:25.000 --> 02:29.000
Once you have that, you then declare the shape of your cluster.

02:29.000 --> 02:33.000
It's fully declarative using Kate's custom resources.

02:33.000 --> 02:35.000
And you apply that to the management cluster.

02:35.000 --> 02:39.000
Then Cluster API does its magic and then it will provision the infrastructure

02:39.000 --> 02:46.000
and then bootstrap Kubernetes on top of that infrastructure.

02:46.000 --> 02:50.000
So we're going to demonstrate how it works on Proxmox.

02:50.000 --> 02:53.000
So just a couple of words about this in case you don't know what it is.

02:53.000 --> 02:55.000
It's a virtualization platform.

02:55.000 --> 03:00.000
It's open source and includes anything you need for your virtualization purposes.

03:00.000 --> 03:07.000
One thing to note is there are two providers for Proxmox if you go out there.

03:07.000 --> 03:13.000
So one requires you to have essentially a template pre-created within your environment.

03:13.000 --> 03:17.000
The other one will essentially just take a bare OS and it will install everything on top.

03:17.000 --> 03:24.000
We are using the one that requires a template.

03:24.000 --> 03:30.000
Yeah, so we made a diagram of how our cluster will look like in terms of cluster API.

03:30.000 --> 03:34.000
So everything you see there are Kubernetes resources.

03:34.000 --> 03:41.000
And all these resources, they represent the cluster, the Proxmox cluster we are going to use.

03:41.000 --> 03:45.000
So we'll have the main entities, of course our cluster.

03:45.000 --> 03:49.000
It will reference the infrastructure and also reference the control planes

03:49.000 --> 03:54.000
and the way how they should look like on the Proxmox environment.

03:54.000 --> 03:59.000
And then another resource is machine deployment, which is used for worker machines

03:59.000 --> 04:05.000
and it also should reference a template of how it's going to look like on Proxmox

04:05.000 --> 04:10.000
and also some configuration for bootstrapping Kubernetes over there.

04:11.000 --> 04:14.000
Cool, so over to the demo.

04:14.000 --> 04:22.000
So we were going to do the demo live, but actually the network is not being nice to us.

04:22.000 --> 04:25.000
So luckily we did record it.

04:25.000 --> 04:29.000
So let me just set this up.

04:29.000 --> 04:35.000
That's what I'm going to do.

04:35.000 --> 04:39.000
Can I do full screen?

04:39.000 --> 04:42.000
Is that a visit?

04:42.000 --> 04:45.000
Yeah, that's what I tried.

04:45.000 --> 04:48.000
Obviously didn't try hard enough.

04:48.000 --> 04:52.000
So hopefully you can see this all right.

04:52.000 --> 04:57.000
So this is just shown initially the repo that that link showed before.

04:57.000 --> 05:03.000
In that repo there are two branches, one for the Kubevert side and one for the Proxmox side.

05:03.000 --> 05:07.000
So we're just going to use obviously the Proxmox branch here.

05:08.000 --> 05:12.000
And in that you can see all of the artifacts that we would have used in a live demo

05:12.000 --> 05:16.000
and if you're going to use this yourself.

05:16.000 --> 05:19.000
So moving on then to the pre-rex.

05:19.000 --> 05:23.000
So as I mentioned, you are required if you're going to do this yourself

05:23.000 --> 05:26.000
to have a template in your Proxmox environment.

05:26.000 --> 05:29.000
So the way that you do this, if you want to do it in an automated way,

05:29.000 --> 05:31.000
so you can use the Kubernetes image builder project

05:31.000 --> 05:37.000
and that has specific make targets that will provision and build that base image for you.

05:37.000 --> 05:42.000
And actually what we should see in a minute is I should change to that window

05:42.000 --> 05:44.000
and you can see it here.

05:44.000 --> 05:49.000
So the Virtual Machine 100 has been built using the Kubernetes image builder project.

05:49.000 --> 05:52.000
So that's got everything on there required to bootstrap Kubernetes.

05:52.000 --> 05:56.000
So it's got versions of KubeADM, et cetera, already baked into that VM

05:56.000 --> 06:00.000
and it's been marked as a template within Proxmox.

06:01.000 --> 06:04.000
Yeah.

06:04.000 --> 06:11.000
Cool. So the basic flow is we're going to create the management cluster.

06:11.000 --> 06:13.000
Sorry, we're going to create the management cluster.

06:13.000 --> 06:18.000
We're going to then install GitOps agent on there

06:18.000 --> 06:21.000
and then we're going to create a cluster.

06:24.000 --> 06:29.000
So I'm just going to fast forward here because this is great.

06:29.000 --> 06:31.000
We're using kind for our management cluster.

06:31.000 --> 06:36.000
So if I just fast forward, just preloading a bunch of images onto there.

06:36.000 --> 06:40.000
The idea being it would have made the demo a lot quicker.

06:42.000 --> 06:46.000
So I'm going to start canines in another environment, another window

06:46.000 --> 06:49.000
so you can see actually what is getting installed.

06:50.000 --> 06:59.000
So this is a plain vanilla Kubernetes cluster at this moment in time.

06:59.000 --> 07:04.000
One thing to note, if you're going through the instructions at a later time,

07:04.000 --> 07:09.000
we've made a slight config change to the cluster cut all utility configuration

07:09.000 --> 07:12.000
so that we can install an IPAN provider.

07:12.000 --> 07:15.000
So probably in the last session you went through the different provider types.

07:15.000 --> 07:19.000
The main ones are the control plane provider, infrastructure provider

07:19.000 --> 07:25.000
and bootstrap provider, but the newer provider types are the IPAN provider

07:25.000 --> 07:29.000
which is especially useful for virtualized and bare metal type scenarios

07:29.000 --> 07:31.000
and also the add-on provider type.

07:35.000 --> 07:38.000
So the way that you create a management cluster is with cluster cut all.

07:38.000 --> 07:41.000
One thing to note here is we're specifying version numbers.

07:41.000 --> 07:45.000
That was purely just to pin the versions so that we could load the images

07:45.000 --> 07:47.000
but you don't have to do that in your environment.

07:47.000 --> 07:52.000
And this will go away and install all of the providers and core Cappy into this,

07:52.000 --> 07:55.000
turning it into a Cappy management cluster.

07:55.000 --> 08:00.000
So if we fast forward a bit, you can see them installed in now

08:00.000 --> 08:07.000
and you can see the IPAN provider at the top there and the Proxmox provider.

08:12.000 --> 08:14.000
So the next step, so we've got a management cluster.

08:14.000 --> 08:16.000
So we want to use GitOps in this scenario.

08:16.000 --> 08:19.000
So you can use whatever GitOps agent you want.

08:19.000 --> 08:23.000
So we're going to be using fleet, but you could equally apply these steps

08:23.000 --> 08:29.000
with slack modifications if you wanted to use flux, Argo CD, whatever your choice is.

08:29.000 --> 08:35.000
But we're using fleet so we just need to quickly install fleet, a couple of Helm commands

08:35.000 --> 08:37.000
and we'll have that there.

08:38.000 --> 08:41.000
So we can fast forward a bit.

08:45.000 --> 08:49.000
So now we have the GitOps agent in our cluster.

08:49.000 --> 08:52.000
We can start using GitOps to provision clusters.

08:52.000 --> 08:58.000
And this is where I guess the mixture of cluster API and GitOps comes really interesting

08:58.000 --> 09:02.000
because you then can create clusters via a pull request,

09:02.000 --> 09:06.000
which opens up to all sorts of different types of scenarios.

09:06.000 --> 09:11.000
And it also means you can perform all of the operations against that cluster via pull requests.

09:11.000 --> 09:13.000
So you have the full history of the changes.

09:13.000 --> 09:16.000
You can roll back and all of those types of things.

09:16.000 --> 09:19.000
If you're using GitOps, you're used to with your applications,

09:19.000 --> 09:23.000
but you can now apply it to your actual clusters in the cluster lifecycle.

09:26.000 --> 09:31.000
So in the repo, you'll see two folders.

09:31.000 --> 09:35.000
Funny enough, the one with the cluster definitions in is in the clusters folder.

09:35.000 --> 09:38.000
It's just got the one cluster definition in there.

09:38.000 --> 09:41.000
So we're going to bring it up now to have a look at what it is.

09:44.000 --> 09:46.000
So it's just pure YAML.

09:46.000 --> 09:49.000
It describes the shape of your cluster.

09:49.000 --> 09:53.000
There's different resources to represent different parts of the cluster,

09:53.000 --> 09:56.000
whether that's the control plane or the worker machines.

09:58.000 --> 10:02.000
And it matches the diagram that we showed before in the presentation.

10:02.000 --> 10:07.000
Basically, this YAML is what you saw in the diagram, but not visualized.

10:08.000 --> 10:10.000
So two things to note here.

10:10.000 --> 10:13.000
Just highlighting the fact that we are using the Proxmox,

10:13.000 --> 10:17.000
so you will have resource kinds dependent on your infrastructure provider here

10:17.000 --> 10:20.000
and likewise for the other type of providers.

10:20.000 --> 10:23.000
So there's a couple of things we want to note there.

10:23.000 --> 10:25.000
So just highlighting some labels here.

10:25.000 --> 10:28.000
If you just remember these labels say CNI Calico,

10:28.000 --> 10:30.000
we'll come back to that in a bit.

10:33.000 --> 10:36.000
And then we just see some various other aspects.

10:36.000 --> 10:40.000
One thing to note, we're also using CubeVip here.

10:40.000 --> 10:45.000
So in this type of environment, you need some sort of load balancer

10:45.000 --> 10:47.000
so that you can get to the API server.

10:47.000 --> 10:50.000
So we're just using CubeVip as an easy way to do that,

10:50.000 --> 10:52.000
and it uses gratuitous ARP.

10:52.000 --> 10:57.000
So if the control plane machine that is currently hosted on crashes,

10:57.000 --> 11:00.000
it will move across and it will start advertising the address

11:00.000 --> 11:02.000
from another control plane machine.

11:02.000 --> 11:05.000
So it's quite a nice setup.

11:05.000 --> 11:08.000
So we can fast forward there.

11:08.000 --> 11:12.000
So here you can just see the shape of the VMs that we want,

11:12.000 --> 11:14.000
the specifications, so this could be whatever you want.

11:14.000 --> 11:18.000
One thing to note is you'll see the template idea at the bottom,

11:18.000 --> 11:22.000
which says 100, so that will have to match the template

11:22.000 --> 11:25.000
that was created via the ImageBuilder process.

11:25.000 --> 11:27.000
If they don't, then things don't work.

11:31.000 --> 11:38.000
So we require a small amount of configuration for Fleet,

11:38.000 --> 11:41.000
and this will be the same for other GitOps agents.

11:41.000 --> 11:44.000
So in this file, we call it the Git repo,

11:44.000 --> 11:48.000
and this just tells Fleet about, hey, go to this source repo,

11:48.000 --> 11:51.000
download everything in there and apply it to the cluster.

11:51.000 --> 11:56.000
So you'll just see that the repo URL, the branch that we require,

11:56.000 --> 11:58.000
so we're on the Proximax branch,

11:58.000 --> 12:01.000
and then potentially any paths or secrets that are required

12:01.000 --> 12:03.000
to access that cluster.

12:11.000 --> 12:14.000
Cool, now we've done that.

12:14.000 --> 12:18.000
We've applied that to the cluster,

12:18.000 --> 12:20.000
so it's going to bring all those cluster definitions

12:20.000 --> 12:22.000
into our management cluster,

12:22.000 --> 12:25.000
and then hopefully we start to get virtual machines being started

12:25.000 --> 12:28.000
and that cluster will be formed.

12:34.000 --> 12:36.000
Maybe.

12:49.000 --> 12:51.000
Cool, so you can see now that automatically,

12:51.000 --> 12:55.000
the cluster API has created machines here for you.

12:55.000 --> 12:58.000
So you'll see that there's one machine for the control plane

12:58.000 --> 13:00.000
and one machine for the machine deployment

13:00.000 --> 13:03.000
or the worker nodes, and you can see that the one has started

13:03.000 --> 13:05.000
to move to provisioning.

13:05.000 --> 13:09.000
What that basically means is it's going to provision the infrastructure

13:09.000 --> 13:12.000
and then start to bootstrap Kubernetes.

13:17.000 --> 13:20.000
So what does this mean from a Proximax point of view?

13:21.000 --> 13:24.000
People with really good eyesight will probably see

13:24.000 --> 13:26.000
that there's a new VM starting up,

13:26.000 --> 13:28.000
so you can see it in the events at the bottom there,

13:28.000 --> 13:32.000
a VM 104, and you'll see it on the side in the viewer.

13:32.000 --> 13:36.000
So this is being orchestrated by the cluster API provider.

13:36.000 --> 13:39.000
So it's talking directly to the Proximax API and saying,

13:39.000 --> 13:43.000
hey, create me a VM, I'm going to use it for this control plane machine.

13:45.000 --> 13:47.000
Now this part does take a while,

13:47.000 --> 13:50.000
so we're going to have to skip quite some way through.

13:50.000 --> 13:54.000
We'll just get it to the point where you bring up the console,

13:54.000 --> 13:57.000
so you can see it's using Ubuntu,

13:57.000 --> 14:00.000
and if we fast forward a little bit,

14:01.000 --> 14:08.000
eventually you'll start to see, essentially, cloud init will kick in,

14:08.000 --> 14:11.000
and depending on how you configure the bootstrap providers,

14:11.000 --> 14:14.000
it will use either cloud init or ignition currently.

14:14.000 --> 14:18.000
This is using cloud init, so you'll start to see cloud init run in,

14:18.000 --> 14:22.000
and that will essentially be running the commands

14:22.000 --> 14:28.000
to bootstrap Kubernetes on top of this VM using QVADM in this instance.

14:31.000 --> 14:33.000
Oh, we missed it.

14:34.000 --> 14:36.000
You'll see it, it will come up.

14:36.000 --> 14:38.000
So it does come up, and you can see that.

14:38.000 --> 14:41.000
So essentially what it's doing.

14:41.000 --> 14:46.000
So at that point, we have one control plane machine ready, essentially.

14:46.000 --> 14:49.000
Once one control plane machine is ready,

14:49.000 --> 14:54.000
you can then start to provision the worker machines,

14:56.000 --> 14:59.000
and it always waits until one control plane machine is ready,

14:59.000 --> 15:03.000
and then it will just start provisioning all of the worker machines in parallel.

15:03.000 --> 15:08.000
So we can fast forward that, and you'll see another VMs come up,

15:08.000 --> 15:12.000
and I think you get the point, so it just repeats the same things,

15:12.000 --> 15:14.000
but this will be for a worker machine.

15:17.000 --> 15:22.000
So, well, I just skipped ahead in the top part of the terminal window.

15:22.000 --> 15:26.000
I have just got the cube config for that newly created cluster.

15:26.000 --> 15:30.000
So the cube configs for the newly created child clusters

15:30.000 --> 15:33.000
or the tenant clusters are available in the management cluster,

15:33.000 --> 15:37.000
so you can get that out and then run, and obviously do what you want with it.

15:37.000 --> 15:41.000
In this instance, I'm just showing that stuff is running in there.

15:41.000 --> 15:45.000
So you can see that Calico is running in there,

15:45.000 --> 15:49.000
so we didn't put Calico necessarily in the cluster definition,

15:49.000 --> 15:55.000
but if we go back to those labels on our cluster definition that said C&I is Calico,

15:55.000 --> 15:59.000
that is using a feature of a cluster API called cluster resource sets,

15:59.000 --> 16:04.000
and essentially this enables you to install any type of resources

16:04.000 --> 16:07.000
into a newly provisioned cluster automatically.

16:07.000 --> 16:11.000
So it's really ideal for things like C&I or cloud providers

16:11.000 --> 16:16.000
to be able to do the things that you want as soon as that cluster is being provisioned.

16:16.000 --> 16:19.000
And again, this is all done in a declarative way,

16:19.000 --> 16:22.000
so you don't have to do any special commands,

16:22.000 --> 16:24.000
you just put all of your definitions into Git,

16:24.000 --> 16:27.000
and then the cluster API will do the orchestration.

16:33.000 --> 16:37.000
So this is what is in the second folder, in the repo, in the CRS folder.

16:38.000 --> 16:40.000
You'll see that there's a cluster resource set,

16:40.000 --> 16:42.000
and you'll see that there's a label selector,

16:42.000 --> 16:44.000
so if your cluster matches that label selector,

16:44.000 --> 16:47.000
it will then apply all the resources listed below,

16:47.000 --> 16:53.000
and those resources are essentially just config maps or secrets,

16:53.000 --> 16:57.000
and they contain embedded Kubernetes YAML,

16:57.000 --> 17:00.000
so it will just squirt those into your cluster.

17:01.000 --> 17:06.000
So where are we now?

17:11.000 --> 17:15.000
So we've got one control plane and one working machine,

17:15.000 --> 17:17.000
so I said that you could go into Git,

17:17.000 --> 17:21.000
and you could scale the cluster and do all your operations,

17:21.000 --> 17:23.000
so what we're going to show here is actually,

17:23.000 --> 17:25.000
if you go to the cluster definition in Git,

17:25.000 --> 17:28.000
we're going to scroll down until we get to the machine deployments,

17:28.000 --> 17:31.000
where it will say replica one,

17:31.000 --> 17:34.000
and we're going to, hopefully,

17:34.000 --> 17:36.000
you see the machine deployment there,

17:36.000 --> 17:38.000
change app to two,

17:38.000 --> 17:41.000
commit those changes to the Git repo,

17:46.000 --> 17:49.000
and you can probably guess what will happen now.

17:49.000 --> 17:56.000
Any VM is spun up, Kubernetes is bootstrapped on there,

17:56.000 --> 17:58.000
and that node joins the existing cluster,

17:58.000 --> 18:01.000
and you'll see eventually that it does come up.

18:01.000 --> 18:07.000
So that is the props box demo.

18:27.000 --> 18:31.000
So now we're going to show the same process with Kubert,

18:31.000 --> 18:36.000
and the idea is that you can use cluster API to provision your clusters

18:36.000 --> 18:40.000
and multiple providers in the same operational way,

18:40.000 --> 18:43.000
so the process for different infrastructure providers

18:43.000 --> 18:45.000
is relatively very similar,

18:45.000 --> 18:49.000
with the difference in operating your infrastructure,

18:49.000 --> 18:52.000
I mean, defining how your machine looks like,

18:52.000 --> 18:55.000
but the whole idea is the same,

18:55.000 --> 18:58.000
no matter where you're on your clusters.

19:03.000 --> 19:06.000
So the one major difference with the Kubert provider

19:06.000 --> 19:11.000
is it requires Kubert to be installed in your cluster already.

19:11.000 --> 19:14.000
So before you install the provider for Kubert,

19:14.000 --> 19:16.000
the Kappi provider, you must have Kubert already installed.

19:16.000 --> 19:20.000
So what you're seeing here is we're installing Metal LB

19:20.000 --> 19:23.000
to take the place of providing the load balancers

19:23.000 --> 19:26.000
within this environment.

19:26.000 --> 19:28.000
Then we install Kubert,

19:28.000 --> 19:33.000
and so Kubert works on the basis of you describe your virtual machine

19:33.000 --> 19:35.000
as a custom resource,

19:35.000 --> 19:39.000
and then it will make that happen behind the scenes via QMU.

19:39.000 --> 19:41.000
So this is what we're doing first,

19:41.000 --> 19:43.000
and this is before you get to any of the Kappi stuff.

19:43.000 --> 19:47.000
This is just setting up the Kubernetes cluster.

19:48.000 --> 19:52.000
So you can see the Kubert is starting up.

19:55.000 --> 20:00.000
So we now done the quaternate installing the provider.

20:00.000 --> 20:03.000
We're going to install the GitOps agent here now.

20:03.000 --> 20:05.000
So it's basically the same process,

20:05.000 --> 20:07.000
just slightly modified with different providers

20:07.000 --> 20:09.000
and just with different prerequisites,

20:09.000 --> 20:11.000
the prerequisite being Kubert.

20:13.000 --> 20:14.000
So forward again.

20:14.000 --> 20:16.000
So in this second branch,

20:16.000 --> 20:19.000
you'll see a different cluster definition that uses Kubert,

20:19.000 --> 20:21.000
but essentially the way it's applied to the cluster

20:21.000 --> 20:24.000
is exactly the same via GitOps.

20:24.000 --> 20:29.000
So what you take away from this is the same operational procedure

20:29.000 --> 20:31.000
irrespective of your target infrastructure

20:31.000 --> 20:34.000
or the flavor of Kubernetes that you want.

20:34.000 --> 20:38.000
You just create some YAML that describes the cluster that you want.

20:39.000 --> 20:45.000
So you'll see this is the interesting part.

20:45.000 --> 20:48.000
It will spin up a pod per VM,

20:48.000 --> 20:51.000
and that pod will then do the interaction with

20:51.000 --> 20:56.000
to actually provision the virtual machine on the host.

20:56.000 --> 20:58.000
So you'll see one of these spin up

20:58.000 --> 20:59.000
for each of the virtual machines

20:59.000 --> 21:02.000
that are required for the cluster.

21:04.000 --> 21:07.000
You can look at the boot logs via VNC as well.

21:07.000 --> 21:09.000
So if you use Vert Cuttle in this scenario,

21:09.000 --> 21:12.000
just get the name of the node and use Vert Cuttle,

21:12.000 --> 21:14.000
and you'll see it's using exactly the same

21:14.000 --> 21:18.000
QBADM commands that we saw previously with the Proxmox.

21:24.000 --> 21:28.000
And then we do the same operation.

21:28.000 --> 21:31.000
We scale it, and you get the third machine.

21:31.000 --> 21:35.000
So again, probably the key takeaway from this is

21:37.000 --> 21:39.000
that you can use the cloud environment

21:39.000 --> 21:41.000
to actually use it.

21:49.000 --> 21:51.000
So key takeaways are,

21:51.000 --> 21:54.000
CAPI can be used in many, many different infrastructure environments,

21:54.000 --> 21:56.000
not just like the cloud environments

21:56.000 --> 21:58.000
where a lot of people would naturally think of it.

21:58.000 --> 22:02.000
So virtualized environments, bare metal type environments,

22:02.000 --> 22:04.000
and some really interesting type environments

22:04.000 --> 22:07.000
where you want a control plane as a pod type scenario.

22:07.000 --> 22:10.000
It supports different Kubernetes flavors,

22:10.000 --> 22:12.000
so you might want just pure upstream with QBADM.

22:12.000 --> 22:14.000
You might want something a bit more lightweight,

22:14.000 --> 22:16.000
so you can use K3S.

22:16.000 --> 22:19.000
So it allows you to mix and match all of these things.

22:19.000 --> 22:22.000
And lastly, this is fully declarative,

22:22.000 --> 22:24.000
fully GitOps friendly.

22:24.000 --> 22:27.000
Perform all of your cluster operations via Git.

22:30.000 --> 22:32.000
So yeah, thank you for coming.

22:32.000 --> 22:34.000
Thank you for your question.

22:34.000 --> 22:36.000
Thanks.

23:02.000 --> 23:04.000
Thank you.

23:26.000 --> 23:28.000
Yeah, so the question was,

23:28.000 --> 23:30.000
can you realistically provision a cluster

23:30.000 --> 23:32.000
associated infrastructure like low balances,

23:32.000 --> 23:35.000
et cetera, with the cluster API currently in a hyperscaler

23:35.000 --> 23:37.000
to like AWS as an example?

23:37.000 --> 23:40.000
The answer is yes, definitely for AWS.

23:40.000 --> 23:44.000
I'd say the caveat is it will provision the infrastructure

23:44.000 --> 23:46.000
in an opinionated way.

23:46.000 --> 23:49.000
So it will only provision the infrastructure that's required

23:49.000 --> 23:51.000
for the cluster and nothing more.

23:51.000 --> 23:55.000
And it will provision it in a way that it thinks best.

23:55.000 --> 23:57.000
So you can slightly tweak it if you want.

23:57.000 --> 24:01.000
If you don't like, say you want to use A or Bs instead of

24:01.000 --> 24:04.000
something else, or you want to add security groups,

24:04.000 --> 24:06.000
it does allow you to do that as well.

24:06.000 --> 24:08.000
But there are, I guess, boundaries.

24:08.000 --> 24:11.000
So if you want full flexibility,

24:11.000 --> 24:14.000
then it might need to do something else.

24:14.000 --> 24:17.000
But you can also use things like Terraform

24:17.000 --> 24:19.000
and cluster API together.

24:19.000 --> 24:21.000
It doesn't have to be an either or.

24:21.000 --> 24:23.000
So you might provision the VPC and the network with Terraform

24:23.000 --> 24:25.000
and then get cluster API to do the Kubernetes

24:25.000 --> 24:29.000
and like the day two operations type of stuff on Kubernetes.

