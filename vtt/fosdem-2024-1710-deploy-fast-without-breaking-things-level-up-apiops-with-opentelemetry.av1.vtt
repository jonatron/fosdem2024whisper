WEBVTT

00:00.000 --> 00:10.880
with the topic. It is a very big mouthful of a topic today, but I'm hoping that we're

00:10.880 --> 00:14.400
going to break this down for you today and that you're actually going to learn something

00:14.400 --> 00:20.760
that you can take home back to actually implement yourselves. I'm here just to be talking about

00:20.760 --> 00:26.460
the open telemetry part. Sonya is actually the brains of this operation. She's basically

00:26.460 --> 00:30.380
been planning this whole thing, set everything up and just invited me at the end because

00:30.380 --> 00:37.200
yeah, because I'm pretty. That's basically all that I'm contributing today. So I am hopeful

00:37.200 --> 00:43.380
that a lot of you have had any type of touch with open telemetry and observability in general,

00:43.380 --> 00:48.700
but also that you know the basic DevOps principles and how that is going to be connected with

00:48.700 --> 00:55.280
API Ops. Just an introduction for both myself and Sonya. I am Adnan. I do developer relations

00:55.280 --> 01:02.280
as you obviously might have already figured out. And yeah, Sonya here is a product manager

01:02.280 --> 01:05.000
at Tyche and I would like to hand over the microphone.

01:05.000 --> 01:11.080
Yeah, hi. I'm a product manager at Tyche. So we do API management. We have an open source

01:11.080 --> 01:15.160
API gateway. If you were in the session before that, you have seen it on the screen. It's

01:15.160 --> 01:19.480
an API gateway that's written in Go. It's really fast and has lots of capabilities. So

01:19.480 --> 01:25.480
do check it out. And now we are happy to talk about the topic.

01:25.480 --> 01:31.400
Cool. Just a quick rundown of the agenda for today. We have four main topics for the agenda

01:31.400 --> 01:35.720
today. First and foremost, we're going to talk about API Ops, what it is, how you can

01:35.720 --> 01:41.280
get started. And then from there, we're going to take a closer look into how to do API Ops

01:41.280 --> 01:45.000
hands on. So we're going to start with the Kubernetes cluster. We'll walk you through

01:45.000 --> 01:51.000
how to use Argo CD and Tyche for your API gateway and basically just enable very fast

01:51.000 --> 01:55.960
flows and very fast deployments and release cycles within your APIs. From there, we're

01:55.960 --> 01:59.640
going to move into production environment. So we're going to say, okay, so what do I

01:59.640 --> 02:04.680
need to do to get observability, to get insight into my production APIs? And from there, we're

02:04.680 --> 02:10.560
going to shift left even more and figure out how to integrate the release cycles and make

02:10.640 --> 02:16.320
them have integrated. I'm going to say integration testing as well. So we're shifting left even

02:16.320 --> 02:21.640
more using the production data, so the observability data for testing as well. So that's going

02:21.640 --> 02:27.240
to be, I'm going to say my most favorite part because I'm here from Trace Test and we do

02:27.240 --> 02:33.240
that. But for right now, let's do the API Ops portion first.

02:34.240 --> 02:41.240
Yes, so what is API Ops? Thank you. So you might be familiar with API management and I find that

02:45.400 --> 02:50.480
sometimes in API management, we have too many manual operation. And as you all know, manual

02:50.480 --> 02:55.280
operation, that's a cause for disaster, that's a cause for error, that's a cause for security

02:55.280 --> 03:02.280
problems and we need to speed up things. So my interpretation of what is API Ops and

03:02.320 --> 03:06.680
you might have heard about API Ops and some vendors will try to push their ideas of what

03:06.680 --> 03:12.120
is API Ops. Some would say it's about deploying your API fast. I'd like to bring a bit back

03:12.120 --> 03:18.120
the cultural side of DevOps and say that API Ops is the offspring of DevOps and API management.

03:18.120 --> 03:24.560
So it's applying the culture of DevOps to your API management lifecycle. And why? Because

03:24.560 --> 03:31.560
you want to deliver value fast without disrupting your users. So if we think back about the

03:31.560 --> 03:36.000
DevOps culture, the DevOps principle that originally came from before we started to have

03:36.000 --> 03:41.440
lots of vendor trying to sell off things that are DevOps applied, it's about fast flow.

03:41.440 --> 03:47.440
I want to be able to commit and have it used by user to have feedback. So to have that

03:47.440 --> 03:52.320
culture of having feedback loops. And it's also about enabling that culture of learning.

03:52.320 --> 03:57.040
I want to understand what's going on. I want to learn fast, fail fast and be able to provide

03:57.040 --> 04:01.840
value to my users. And we're here today to tell you that we think that observability is

04:01.840 --> 04:09.200
a key enabler for all that in API management or API Ops. So let's take a look at how to

04:09.200 --> 04:18.160
implement API Ops in modern Kubernetes environments to have fast flow. So typically you will have

04:18.160 --> 04:23.280
a developer that's building a service. You will have things like open API specification

04:23.280 --> 04:28.440
along the way. So we had a talk in this room earlier about open API. I'm not going to go

04:28.440 --> 04:33.440
more into details, but it's definitely a space, a place that you have to take into your CI,

04:33.440 --> 04:37.920
into your continuous integration, making it all automated. Today we're going to talk now

04:37.920 --> 04:42.000
a little bit more on the deployment side. That's why we haven't added it, but of course things

04:42.000 --> 04:47.000
like linting and generating documentation. All that should be part of your process. So

04:47.000 --> 04:51.920
once the developer commits something, it goes to the CI, continuous integration, and the

04:51.960 --> 04:57.600
result might be a Docker container. So it gets published. And now we want to deploy that. We

04:57.600 --> 05:02.280
want to deploy that new version of that service. We want to deploy it with an API specification.

05:02.280 --> 05:09.840
And for that in Kubernetes, the new way of doing continuous deployment is to use GitOps.

05:09.840 --> 05:17.960
There are projects like AgroCD or Flux that are able to do GitOps. What does it mean? GitOps?

05:18.000 --> 05:30.120
You're lucky you're really pretty. Okay. So the main thing about GitOps is you don't have a

05:30.120 --> 05:34.520
continuous pipeline that pushes the things and deploy to your server. That's the Kubernetes

05:34.520 --> 05:41.120
cluster with something like Agro. Pull the information and deploy it itself. So how does

05:41.120 --> 05:46.840
it look like? You have then at the end of your CI pipeline, you have to make a change into your

05:46.880 --> 05:51.880
deployment repository. You have a code artifacts for all your changes, all the configuration. And

05:51.880 --> 05:59.000
you might have a new version that is placed into staging. And AgroCD on your Kubernetes cluster

05:59.000 --> 06:05.240
can be configured to automatically pick it up and deploy it. So all automated. Now there's

06:05.240 --> 06:11.160
another thing that you need is to expose an API is an API gateway. So in that example, we are using

06:11.160 --> 06:17.640
tag API gateway to use the authentication, verification, monitoring. So we add an API

06:17.640 --> 06:22.440
gateway, open source API gateway to that. And that's going to be interesting also for the

06:22.440 --> 06:29.760
observability part later. So an API gateway helps you to centrally manage your APIs to use

06:29.760 --> 06:34.800
authentication, authorization, weight limiting, all this capability that you need in operation.

06:35.760 --> 06:41.720
How do you add that? The Kubernetes and GitHub way. Typically we focus on resource definition

06:41.720 --> 06:47.600
like it's the way in Kubernetes. So you can add things. And that's a very, very simple where you

06:47.600 --> 06:52.480
can say which protocol it use. You could define things like weight limiting, like security policy,

06:53.360 --> 06:59.360
which service is proxying on your cluster. And again, it's configuration as code. So it's again

06:59.440 --> 07:06.840
central repository. And when you make changes to it into your deployment configuration

07:06.840 --> 07:16.200
repository, something like ArgosCD will track it and will apply it automatically. So what we see

07:16.200 --> 07:21.560
at the end in your ArgosCD application, you see, okay, all my application definitions, all my

07:21.600 --> 07:31.720
application are synchronized automatically with whatever I put into my Git repository. So now we

07:31.720 --> 07:37.040
have the first step, right? We have automation for fast flow. We are preventing configuration drift.

07:37.040 --> 07:43.520
We have enhanced security. All is automated. No manual error. We are more efficient. We also

07:43.520 --> 07:48.560
have an audit trail. So we see exactly what was changed in the deployment of your APIs. And we

07:48.560 --> 07:52.000
have better collaboration and visibility on what's happening.

07:55.720 --> 08:04.480
Wonderful. And obviously, as the slide says, that is not enough. So we're getting the automation part

08:04.480 --> 08:13.160
down. What do we do next? Step three in the whole process is to get additional feedback into your

08:13.200 --> 08:20.080
feedback loops so you can connect both ops and dev correctly. So what this means is that the ops

08:20.080 --> 08:26.120
team needs to enable the dev team to fix issues by exactly knowing what the issue is, so that the

08:26.120 --> 08:32.000
dev team doesn't need to spend useless cycles trying to figure out what the problem is. And we do that

08:32.680 --> 08:38.920
by using OpenTelemetry and using Yeager, which are observability tools within our API ops pipelines.

08:39.800 --> 08:49.120
Now, this is what we exactly don't want. We don't want to see gears turning and hoping it's all fine

08:49.200 --> 08:55.040
because it's not really fine. You don't know what your users are seeing. So we don't really know if our

08:55.040 --> 08:59.360
users are happy. We just kind of know it works. And then you kind of do prayer driven development, as I

08:59.360 --> 09:04.840
like saying, that's not really what we want. We want to use observability to infer the internal state of

09:04.840 --> 09:10.240
our system by getting telemetry out of our system to understand what's actually happening. And then we

09:10.240 --> 09:14.480
can figure out whether our users are happy. Because this is something that we can see by using

09:14.520 --> 09:20.200
observability with distributed tracing. When our API is exposed telemetry, we can actually see, oh, okay,

09:20.960 --> 09:26.160
obviously something is wrong because we have breaking APIs. So it's pretty obvious that our users are

09:26.160 --> 09:31.760
unhappy because we can obviously see things breaking for them. And this is a particular view that you

09:31.760 --> 09:37.680
get by using Jaeger. Now, let's get to the fun part of actually showing you how it all works and how you

09:37.680 --> 09:44.600
can set it up yourself. Now, the way you do it is you use CNCF observability tooling. So tooling from the

09:44.600 --> 09:50.400
CNCF tracing landscape, more specifically open telemetry and Jaeger. Open telemetry is an incubating

09:50.400 --> 09:56.680
project. Jaeger is a graduated project. So they're all fully open source supported by the CNCF. Now, the

09:56.680 --> 10:03.080
specifics are that you use open telemetry as the open standard, we're very focused on open standard for the

10:03.080 --> 10:07.800
whole dev room today. So once again, it's an open standard to generate, collect and export your

10:07.800 --> 10:14.280
telemetry. Remember that part, it's a bunch of libraries and APIs that help you generate, collect and

10:14.280 --> 10:21.880
export telemetry. Now, where do you export it to? Well, you export it to Jaeger, which is a tracing

10:21.960 --> 10:29.000
backend, which is just like a data store for your distributed tracing. And then you use Jaeger for all of

10:29.000 --> 10:33.200
your production monitoring troubleshooting and whatever else you need to do in your production

10:33.200 --> 10:42.560
environment. Now, from this, one of the bigger issues is that open telemetry is quite hard to implement if

10:42.560 --> 10:49.520
you're new to it. So some vendors like to bake it in into their systems. One such vendor is

10:52.640 --> 10:59.160
there was a lot of suspense, right? Yeah. Yeah. So one thing that we did in tech is to add support,

10:59.160 --> 11:04.520
native support for open telemetry, because we know that people that works in the API space, they use

11:04.520 --> 11:09.760
API's to proxy multiple services, and the developers might not yet have implemented open

11:09.760 --> 11:16.200
telemetry. But we know they need one where to report the data on all the APIs have really visibility on

11:16.200 --> 11:22.680
what's happening. And so we added support, native support for open telemetry in tech to enable our

11:22.680 --> 11:29.400
user to export this data and to capture them automatically for older APIs. So that's need a couple

11:29.400 --> 11:34.280
of settings. This is settings for our hand charts. So where do you need to enable it in tech? You need

11:34.280 --> 11:39.400
to say where do you want to send the data to an open telemetry collector could be also directly to an

11:39.400 --> 11:46.600
observability backend. And this is what you get. So for every API request, you get a distributed

11:46.600 --> 11:53.240
trace for what's happening at the gateway and till the upstream service. So you can see, first of all,

11:53.240 --> 11:58.120
you can see any error that's happening already at the API gateway level, authentication error,

11:58.120 --> 12:03.240
wait limiting. We see sometimes people only monitor what's happening on the service, but they don't

12:03.240 --> 12:07.440
realize they're already missing a lot of people having issue with the authorization, authentication,

12:07.680 --> 12:13.680
wait limiting. And then you see what's happening in the upstream. So you can very, very quickly catch

12:13.680 --> 12:20.240
up errors, understand not only the timing text, the HTTP response code, but really what's happening

12:20.240 --> 12:25.120
if there's an error, if something is slow, where is it happening? Is it on the API gateway, is it on

12:25.120 --> 12:31.600
the upstream service? What are the details of the transaction that enables a team to better troubleshoot

12:31.600 --> 12:40.880
the issue? And with that, we have now achieved feedback from production. So we have healthy

12:40.880 --> 12:48.880
development lifecycle with feedback loop between Dev and Ops. If there's an issue, then the Ops team

12:48.880 --> 12:53.920
can report it, can take a look. So it's not only an error on a metric that goes up, it's really a

12:53.920 --> 12:59.040
trace where you understand where's the problem, you know, which team needs to act on. And it enables

12:59.040 --> 13:05.920
you to provide a better user experience, fix the issues earlier. Again, what have achieved, feedback

13:05.920 --> 13:11.600
from production, we no longer relying on user reporting feature, no longer somebody that calls

13:11.600 --> 13:15.680
support and say, oh, I have a problem, something is done, no, you see it, you see it all, so you can

13:15.680 --> 13:21.120
be proactive. You understand the API performance, you understand really what's happening, where the

13:21.200 --> 13:24.080
error is happening, and you can solve issues faster.

13:26.880 --> 13:33.760
And with that suspendsful mic switch, again, it's not enough. So we need to introduce another layer

13:34.400 --> 13:41.920
of, actually this one, no, we need to introduce another layer of protection. Because right now,

13:42.720 --> 13:49.280
we want, we're only stopping bugs after our users are seeing them. So we exactly know that a user saw

13:49.280 --> 13:54.640
problem that broke our API, and then we're now rotating back to fix it. We need to be more

13:54.640 --> 14:00.640
proactive and figure out how to stop the bugs before they even reach our users. Now, so this is a

14:00.640 --> 14:05.040
shift left even more approach, but actually for you guys, it's shift left even more approach.

14:06.160 --> 14:10.480
Because we want to add observability to our release cycles as well. So not just our production

14:10.480 --> 14:18.960
systems. So the way we're going to go through that a bit is by doing this little squiggly in between,

14:19.040 --> 14:25.520
as well. So this basically means that you need to implement something called trace-based testing,

14:25.520 --> 14:31.840
which is also called observability driven development. If you like honeycomb and their CTO,

14:31.840 --> 14:42.000
it's a term that they coined. Okay. Anyway, the way that you use trace-based testing is you quite

14:42.000 --> 14:46.960
literally using the distributed tracing that your observability, like open telemetry exposes,

14:46.960 --> 14:53.360
and then you're running tests on those actual data points from your infrastructure. So that

14:53.360 --> 14:58.720
means that even though we can see that we have our gears turning, that's awesome. But my initial

14:58.720 --> 15:04.800
connection to that API gateway is returning 200. But how do I know this is not broken? How do I

15:04.800 --> 15:08.880
know if this is on fire or not? This is an external service. I don't like I don't manage this.

15:10.240 --> 15:14.640
So this is something that easily breaks and that you don't really have a lot of control over.

15:15.280 --> 15:19.760
Now, let me show you how you can actually get to that state where you can do your testing against

15:19.760 --> 15:24.960
the distributed trace itself. This is a screenshot from Trace-Test, which is also a

15:26.080 --> 15:32.160
CNCF tracing landscape tool. You can build your test by getting the trace itself from Jaeger,

15:32.160 --> 15:37.920
and then you're writing your test specs directly against trace data. So you're not using any mocking,

15:37.920 --> 15:41.920
you're not using any faking or whatever the word is nowadays with kids use, I don't even know.

15:42.880 --> 15:47.360
You're literally getting the actual data back and running your test against that data.

15:48.240 --> 15:51.920
Now, the magical part here is that you can quite literally test against anything that's

15:51.920 --> 15:56.160
exposing telemetry. It can be an API gateway like TIC, it can be databases like Postgres,

15:56.160 --> 16:00.960
it can be caches like Redis, it can be pretty much anything that you have instrumented to

16:00.960 --> 16:06.400
export traces. Now, this is a really cool use case for authentication as well, but also for

16:07.120 --> 16:11.120
GraphQL. Now, for authentication, you have a very good example.

16:13.440 --> 16:18.080
Yeah, something like Off-Flow where you have multiple service taking to each other and getting

16:18.080 --> 16:24.560
the request, that's one of the really cool, useful examples. And also something that I've

16:24.560 --> 16:30.560
noticed as well is for GraphQL. So one thing for GraphQL is that it often returns a 200,

16:30.560 --> 16:35.680
even though it's failing because the actual error is within the response. So you don't really know,

16:36.000 --> 16:40.000
it's very intricate to test that. One thing you can do with trace-based testing is you can drill

16:40.000 --> 16:46.000
down to the actual middleware that handles that in your API gateway, find the exact error that

16:46.000 --> 16:51.840
happened, and then you can run your test spec on that exact value. So with all of this, we're

16:51.840 --> 16:56.800
getting step one, which is functional testing. So we can actually functionally validate our

16:56.800 --> 17:01.600
behavior of the system by using all of the telemetry that you've implemented in the prior

17:01.600 --> 17:07.120
step to make your production environment reliable. Now, but it doesn't really stop there. We also

17:07.120 --> 17:12.560
have step two, which is performance testing, because every span has a duration. You can quite

17:12.560 --> 17:17.840
literally go in and say, I want my duration for this span to be less than whatever value of 200

17:17.840 --> 17:22.480
milliseconds or something, which means that if you have external services, external APIs, upstream

17:22.480 --> 17:28.560
APIs that you're not in charge of, if their performance is bad, you can validate that and

17:28.640 --> 17:33.280
you know exactly what part of your system is misbehaving. So this is the performance aspect as

17:33.280 --> 17:40.640
well. So you're getting basically two things from one, I'm going to say exercise. Now the way you

17:40.640 --> 17:44.880
do it, I'm going to walk you through quickly. You do this shifting left with trace test, which is,

17:44.880 --> 17:50.640
as I said, open source part of the CNCF tracing landscape as well. And what it does, it is quite

17:50.640 --> 17:57.360
literally giving you the infrastructure by actually the distributed system architecture by looking at

17:57.360 --> 18:01.600
the trace data. And then you can both get the overview of what your system is doing,

18:01.600 --> 18:06.400
and you can run tests against exactly what's happening in your system. So those are two

18:06.400 --> 18:11.040
powerful things because as engineers, it's very hard to know what the system is doing if it's

18:11.040 --> 18:16.000
highly distributed with a lot of microservices, especially if you're a new person on a team,

18:16.000 --> 18:22.480
it's just, it's a pain to do that. But with trace test, I want to show you how you can implement

18:22.480 --> 18:29.920
these integration tests in your Argo CD, like right here. So this is what an integration test in a

18:29.920 --> 18:36.560
post sync hook would look like. You have a API that you're deploying, you have your integration test,

18:36.560 --> 18:43.600
which basically runs a Kubernetes job from Argos, from the Argo CD sync hook, then it runs a few

18:43.600 --> 18:47.360
integration tests. If they, if they're failing, awesome, you know that they're failing, if they're

18:47.360 --> 18:52.400
passing, even better, you see that they're passing, but doesn't really stop here. The thing that you

18:52.560 --> 18:59.360
get with this is also every test that fails, you have a URL to go to that particular test

18:59.360 --> 19:06.080
to actually see precisely which part of that transaction failed within your API, within your

19:06.080 --> 19:11.680
API microservices. And I really like that part because this is not just, oh, yo, this failed,

19:12.400 --> 19:16.400
this is actually, this failed, here's exactly how, where, and what happened.

19:16.400 --> 19:23.680
And with that, we're actually getting to a stage where we're validating our production, but we're

19:23.680 --> 19:29.920
also using that effort we put into our production reliability to validate pre-production as well.

19:29.920 --> 19:35.680
So you're basically getting the exact same overview graph that Sonya just showed you, but

19:35.680 --> 19:40.560
instead of using your end users, you're running tests with trace test against the API Gateway

19:40.560 --> 19:46.000
platform, then you're getting the traces back from your Yeager or Grafana or whatever you're using,

19:46.080 --> 19:53.120
and then that info goes back to the API developer that can then fix the issues that were found.

19:54.800 --> 20:00.800
Now, with this, I'm just going to wrap up everything that we learned from this last section,

20:00.800 --> 20:05.120
which is that we got functional testing and we got performance testing. So you can both

20:05.680 --> 20:10.400
validate your behavior, or actually the behavior of your system, so all upstream and downstream

20:10.400 --> 20:15.680
services, API transactions, both the ones that you manage and don't manage, you can

20:15.760 --> 20:21.680
actually test database performance, you can test cache, you can also test the size of an HTTP

20:21.680 --> 20:26.160
response and request, but you can also do very intricate performance testing by validating

20:26.160 --> 20:33.600
the duration of every part of your API. And with that, I have a saying where I'm from. We say

20:34.960 --> 20:39.040
you're swatting two flies with one swing because I think that's more friendly than killing birds

20:39.040 --> 20:44.640
with stones. So yeah, with that, I think that this is the closest we can get to be bounty

20:44.640 --> 20:51.360
hunters because we're bug hunters. That was very lame. Anyway, so that's a CU space cowboy reference

20:51.360 --> 21:02.880
if somebody can. Thank you for making this. So, and just before we close, I want to say if this is

21:02.880 --> 21:07.840
a topic that's interesting for you, we're running an online API observability conference in February.

21:07.840 --> 21:12.080
It's going to be called LEAP because it's going to be on the LEAP there. So if that's the topic

21:12.080 --> 21:16.320
that's interesting to you, make sure to register. We have lots of people from the API space and

21:16.320 --> 21:21.920
observability space that will be coming. We also have a GitHub project about all the screenshot

21:21.920 --> 21:26.800
that we showed to you today. We were working on it as a GitHub example. We don't have a link for

21:26.800 --> 21:33.920
it, but if you're interested, just reach out to us. Those are LinkedIn. Yeah, I don't like Twitter

21:33.920 --> 21:40.480
anymore. So make sure to send a connect and we're happy to send you a link to a GitHub project. You

21:40.480 --> 21:45.680
can try it all by yourself at this combination of open source projects. Thank you so much.

21:57.840 --> 22:00.960
So we have some time for questions. Yeah, there is one over there.

22:03.360 --> 22:04.640
Questions down. Questions down.

22:10.480 --> 22:12.640
Go ahead with one customer. Yeah.

22:40.480 --> 22:58.320
Okay, so the question is, I have to repeat for the video, the question is, if I have a service that

22:59.120 --> 23:04.960
can be accessed by multiple customers, do I want to have one to send the data to different places

23:04.960 --> 23:10.240
so to split them out or do I want to have just one year, one open telemetry? And as always,

23:10.240 --> 23:16.000
it depends. And on what does it depend? It depends on do you want to give access to those data to

23:16.000 --> 23:21.040
your customers somewhere? Do you want to have strict regulation on the data of your customer

23:21.040 --> 23:24.400
where you may need to split them by location? But yeah.

23:34.960 --> 23:40.240
Yeah, yeah.

23:44.080 --> 23:44.320
Yeah.

23:46.400 --> 23:50.960
Yeah, that's a very, very, very good question. So the question is, how do I monitor the service

23:50.960 --> 23:56.240
level for every customer? So typically you have for every customer, they have, they are authenticated.

23:56.240 --> 24:01.920
So you have maybe something like a token. Yeah, yeah, but in production, yeah, yeah. So they're

24:01.920 --> 24:08.080
authenticated. So when they come to you, you can put a tag on an information on the trace,

24:08.080 --> 24:12.160
and tag will do it automatically if you're using the authorization or authentication from

24:12.160 --> 24:20.720
tag, tag. The API, yeah, it's tag. Tag. Yeah, no worry. And so on the traces, we put the information

24:20.720 --> 24:26.960
on who is going to API. And with open telemetry, you can then use the data to create your own report

24:26.960 --> 24:33.120
based on that information. Yeah. So we add that information on the API call so that you can reuse

24:33.120 --> 24:40.400
it for your report. Yeah, it's directly exposed. Yeah. That's a very good question. It's really

24:40.400 --> 24:46.000
important to monitor per customers because you want to, some customers have different usage,

24:46.000 --> 24:51.200
different patterns, and you want to make sure that every one of them is happy and not just like an

24:51.200 --> 24:53.600
average where you don't really understand whether problems.

25:04.960 --> 25:07.600
Also, the question is whether Trace Test notifies on errors.

25:09.840 --> 25:15.040
No, Trace Test is just a testing tool. You would then need something to automate the test,

25:15.040 --> 25:19.840
like Argo, and then you need something to alert on failures as well. And then you can pick the

25:19.840 --> 25:25.440
alerting tool that you want. Whatever you're using right now, you can automate within your CI,

25:25.440 --> 25:30.880
so you can build your CI within Argo or within whatever you can use Tecton. You can do basically

25:30.880 --> 25:36.720
whatever CI tool you're using, and then you're sending errors on that. So think just integration

25:36.720 --> 25:40.960
testing. You just get works, doesn't work, then you do whatever else you want to do. Yeah.

25:41.440 --> 25:43.600
Yeah. Another question.

25:55.600 --> 25:59.600
Observability data for APS, I can take that one. So,

26:02.080 --> 26:07.440
yeah, so the question is how do you deal with data privacy? And because in the observability

26:07.440 --> 26:12.880
data, they can land a lot that could be considered privacy data. So first, you have to be very

26:12.880 --> 26:17.360
aware of that, that observability data could potentially have some data that in your country,

26:17.360 --> 26:23.120
in your own regulation could have some impact. OpenTelemetry has a lot of tool for that. In the

26:23.120 --> 26:28.080
OpenTelemetry collector, there are kind of plugins that you can define using Yamal and say,

26:28.080 --> 26:34.320
that arguments, that thing I want to filter out, I don't want to register it. So you're very flexible

26:34.320 --> 26:38.080
in your observability pipeline, but that's something that you have to take care of to make

26:38.080 --> 26:41.840
sure that your developers haven't added something that you don't want to store.

26:44.560 --> 26:46.640
Sorry. I'll go for it. Go for it.

26:46.800 --> 26:58.960
Jack, when I use the data to send the data to the OpenTelemetry, this data is made only on HB8.

26:59.920 --> 27:11.360
HB8, the status only. So like a 100, 500 message, the status of the response of the

27:12.640 --> 27:23.440
HB8 request. Yes. All on another way is to analyze the response of the request.

27:23.440 --> 27:29.920
So the question is, what do we track or what kind of data do we expose with tech?

27:54.080 --> 28:09.520
So, yeah, so in tag the gateway, when it's being called, you will get the answer, but the traces,

28:09.520 --> 28:14.480
it will export using OpenTelemetry will contain all the data, all the steps, the traces that we

28:14.480 --> 28:19.440
saw in Yeager. And you can also extend them. So we have a plugin mechanism where you could,

28:19.440 --> 28:24.800
that you could load into there and add even more data if that's more open, extend your OpenTelemetry

28:24.800 --> 28:38.480
traces. The question is, where is the effort? So tech make it easier for you because it captured

28:38.480 --> 28:43.520
the starts up to the call to the upstream service and it tell you how long it took.

28:44.240 --> 28:48.400
And but if you want to get even more details, what happens after that, then it's where you

28:48.400 --> 28:54.720
need to instrument your services using OpenTelemetry. And then the beauty of it is when all the services

28:54.720 --> 28:58.560
speak the same observability language, they all send the data to the same place,

28:58.560 --> 29:03.760
then you have the full picture and that's kind of the operational dream. Thank you.

29:04.400 --> 29:08.720
Yeah. You suggest to run that on a trade production?

29:08.720 --> 29:11.040
It's right.

29:39.040 --> 29:52.400
Correct. Correct. So you wouldn't use trace this in this point of view for your production,

29:52.400 --> 29:56.160
you would use it in pre production, where you need sampling to be at 100%.

30:00.320 --> 30:07.040
Yeah, yeah, we can also just stand. We'll just wait so you can come by and chat with us. So because

30:07.120 --> 30:14.320
yeah, we don't have time. Don't follow up on questions. Yeah. So yeah, yeah, we'll be here. Come here. Yeah.

30:14.960 --> 30:16.960
Cool. Thank you.

