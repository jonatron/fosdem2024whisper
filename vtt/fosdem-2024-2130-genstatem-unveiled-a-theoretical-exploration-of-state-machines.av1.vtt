WEBVTT

00:00.000 --> 00:10.880
especially state machines and how they are handled in Ireland and also from a theoretical

00:10.880 --> 00:20.200
point of view. So, it's up to you. Thank you.

00:20.200 --> 00:24.720
All right. Yes, he said, like, I'm relatively young but I know a school guy, so I code in

00:24.720 --> 00:33.960
V-man and use Ireland. So, this went too fast already. I work in Erlang Solutions. We do

00:33.960 --> 00:39.200
like Erlang stuff, so concurrency, scalability, the useful things that most of you would be

00:39.200 --> 00:44.480
hopefully familiar and we also contribute a lot to open source. This talk is going to

00:44.480 --> 00:50.760
be about state machines, as you heard. First, a question of protocols. What are protocols?

00:50.760 --> 00:54.600
I wanted to make a survey and ask you and so on, but we have limited time, so I'm going

00:54.600 --> 01:04.040
to answer the question already. System of rules. A few examples. Okay, I need to point

01:04.040 --> 01:10.240
here for this to work. Protocol defines the system of rules for syntax, semantics of the

01:10.240 --> 01:17.000
project, the program that you want to write. Some examples, the usual ones are TCP for

01:17.000 --> 01:22.560
network communication, is connection oriented, stream oriented, messages are ordered and

01:22.560 --> 01:29.400
they are acknowledged. Another common example, TLS for privacy, integrity and authenticity,

01:29.400 --> 01:38.120
encryption, very important. I hope that everybody has HTTPS enabling the browsers by default.

01:38.120 --> 01:44.520
Some other examples are file formats or markup languages. Parsers for them can also be implemented

01:44.520 --> 01:51.800
as state machines. The two classic examples, XML and JSON. XML is particularly interesting

01:51.800 --> 02:00.000
to me because I work in XMPP messaging server written in Erlang, of course. If you saw our

02:00.000 --> 02:06.720
talk in CodeBeam, for those that are following CodeBeam, Pablo and me, we talked about the

02:06.720 --> 02:13.000
state machine re-implementation in Mongo's IM. This is a bit of a continuation to that.

02:13.200 --> 02:20.560
Some more complex protocols can be implemented as state machines like HTTP and as I mentioned,

02:20.560 --> 02:26.440
XMPP, which is my specialty, which is extensible, that's the X and my favorite part of the whole

02:26.440 --> 02:32.240
thing, it's an instant messaging protocol that also has presences, the green bubble,

02:32.240 --> 02:36.560
whether your friend is connected or not and it also does contact list maintenance on the

02:36.600 --> 02:44.840
core protocol, 500 extensions and build your own. This is the state machine diagram for

02:44.840 --> 02:55.480
the protocol. Much like a flow chart on steroids, I really like that analogy. With the state

02:55.480 --> 03:00.240
machines, we are like the usual thing, how you think about state machines, you draw the

03:00.240 --> 03:05.120
state with some arrows, the arrows have tags about how you transition to the next state.

03:06.120 --> 03:11.920
Finest state machines give you a way to visualize a system that can be very complex.

03:11.920 --> 03:22.520
Why state machines? State machines can be seen as a model. We want to model the behavior of

03:22.520 --> 03:29.640
protocol that can be very complex like TLS or HTTP, most of you will be familiar, XMPP,

03:29.680 --> 03:38.080
my specialty. Let's talk a little bit quickly about state machines in particular. A few

03:38.080 --> 03:45.680
formalities. I studied mathematics in university, I'm excited by these weird symbols, but some

03:45.680 --> 03:53.560
people can find them off-putting, so I will try to make it pleasant. A few terminologies,

03:53.880 --> 04:00.480
we define an alphabet, terminologies, you use Greek symbols, mathematicians, which are the

04:00.480 --> 04:08.280
input symbols, zeros and ones, or ASCII characters, UTF-8, or complex symbols treated as a single

04:08.280 --> 04:16.200
element, half, and you can do equivalences. One of the weakest ones is the regular grammars,

04:16.360 --> 04:23.160
it's how you do regexes. A regex, this thing that right ones are never read, but very powerful,

04:23.960 --> 04:32.200
is theoretically equivalent to a state machine. Again, this is jumping too fast. Something a

04:32.200 --> 04:36.760
little bit more powerful is the partial automata, I'm not going to focus on this one too much,

04:36.760 --> 04:41.640
use a key difference, then nothing else parsed, now it has one more thing, it's the same thing

04:41.720 --> 04:46.920
before, plus a stack, and the stack behaves as you would expect. The function that used to take

04:46.920 --> 04:54.120
the state and the input symbol also takes the stack and the output of the function is whether

04:54.120 --> 04:59.880
you pop something from the stack or you push something on the stack. It's safe to consume a

04:59.880 --> 05:05.880
string that you give to this PDA as it arrives to one of the final states with an empty stack.

05:05.880 --> 05:11.800
There are equivalent definitions, not all definitions require the empty stack, but I choose

05:11.800 --> 05:19.640
that one. They are equivalent to context-free grammars, parsers, but not compilers. Why a compiler?

05:20.440 --> 05:26.040
So in tree, the thing about being context-free is that it doesn't remember symbols that were

05:26.040 --> 05:34.440
defined before. So for a compiler, for example, the usual regex compiler for C that needs to remember

05:34.920 --> 05:42.680
the definition when you say int e and then you use e later below, parser doesn't remember that,

05:42.680 --> 05:51.880
you need symbol tables, parser only builds the syntax tree. And the fancy one, the computer,

05:51.880 --> 05:56.680
theoretically, Turing machines, which is again the same thing, but nothing else is supplanted by a

05:56.680 --> 06:03.240
tape that is infinite. It is equivalent whether it's finite in one side and infinite in the other,

06:03.240 --> 06:07.480
all of those are equivalents, whether it has two tapes is also equivalent, will arrive to that.

06:07.480 --> 06:13.320
The function takes the tape and the action go one to the left and write something, go one to the

06:13.320 --> 06:18.280
right and write something. Very similar, a Turing machine is said to consume a string

06:19.560 --> 06:26.840
when the next step is undefined. When it holds, you have all heard of the holding problem.

06:26.920 --> 06:33.240
There is no way to know whether a Turing machine will hold. That is important. They are equivalent

06:33.240 --> 06:37.400
to interested grammars, compilers in the Chomsky hierarchy that are like four levels.

06:37.400 --> 06:41.400
The three things that I describe are zero, one and four, there is something in the level three

06:41.400 --> 06:50.040
that is not directly useful for the moment. So I skip that. So how do they compare? This goes very

06:50.040 --> 06:56.680
fast sometimes. So that's the power that they have. A Turing machine can do all the others.

06:57.480 --> 07:03.640
PDA can do the one over there. So that's the power that they can do. They contain the power of each

07:03.640 --> 07:11.960
other. Two FSMs running together has still the same theoretical power, the same thing that a PDA

07:11.960 --> 07:19.240
with a finite buffer or a PDA with a finite state machine is still as powerful as one PDA.

07:20.040 --> 07:25.000
Turing machines, whether it's multi-tape, tape one banded on one side, they are all equivalent

07:25.000 --> 07:32.120
again. A Turing machine doesn't get more powerful by giving it 100 tapes. It gets maybe more

07:32.120 --> 07:41.720
performant theoretically, but the problems that it can solve are all the same. And a PDA with two

07:41.720 --> 07:45.720
stacks is really a Turing machine when you know you can just go in both directions. So when you

07:45.720 --> 07:52.920
give the PDA two stacks, you build a Turing machine. So conceptually, finite state machines can keep

07:52.920 --> 07:59.960
track of one thing, the state. The push-down automata can keep track of two things, the state and the

07:59.960 --> 08:06.280
top of the stack. And a Turing machine can keep track of infinite things. When I was going through

08:06.280 --> 08:11.960
the mathematics and I came to this conclusion, I found this funny for a completely unrelated reason.

08:12.520 --> 08:16.360
In the European languages, I mean to human languages,

08:16.360 --> 08:21.560
used to have the concept of dual as something different to singular and plural.

08:28.120 --> 08:32.920
The function that it computes depends on one thing to things or an infinite number of them.

08:35.320 --> 08:40.280
The function that was defined before. So in the European languages, as I said,

08:40.360 --> 08:47.320
they had this special concept of the dual. And I found it very funny how informal human languages

08:47.320 --> 08:52.600
used to have such a thing as a dual, as a different grammatical category than one and infinite.

08:53.400 --> 08:58.040
When you build the declinations, they had a different thing. Why do I know this strange thing

08:58.040 --> 09:07.240
about languages? Because I live in Poland. So Slavic languages have some remnants of that

09:07.320 --> 09:11.720
dual concept. So there is this famous joke of in Polish you have like 100 ways to

09:12.360 --> 09:16.360
declinate number two. And you have more ways to declinate number two than you have number three

09:16.360 --> 09:22.840
because of that all dual. So two is special. I live in Poland, but I'm not Polish. It's challenging.

09:24.120 --> 09:31.880
So do FSMs produce output? Let's go move slowly to what is useful here.

09:32.040 --> 09:40.040
We can define finite state transducers, which same thing than before and then nothing else is

09:40.040 --> 09:47.640
supplanted by another output alphabet. The function takes the state and the input and decides the

09:47.640 --> 09:54.600
next state and a symbol for the output. It's a to consume a string the same and they are also

09:54.600 --> 10:00.040
equivalent to regular grammars. When it comes to the problems they can solve, again, they're all

10:00.040 --> 10:05.160
equivalent. You get fancier tools, but there are properties that are going to be all the same.

10:06.200 --> 10:11.960
You will see in a second there are many, but let's focus on two ways of defining transducers,

10:11.960 --> 10:18.920
the milley machines and Moore machines, whether the output, I have a laser, yes, whether the output

10:18.920 --> 10:26.840
symbol depends on the input and the previous state or only on the previous state. There is a way to

10:26.840 --> 10:32.920
define Moore machines from a milley machine, but not the other way around, so milley has a

10:32.920 --> 10:40.120
bit more powerful. Now something a bit more useful, how do they compare? They are still

10:40.680 --> 10:46.440
the same than the FSM machines, but this can be composed. We are getting into a bit of engineering.

10:46.520 --> 10:54.680
We are almost there. Not that much. This is a thing, laser. Yes, oh god. Come on,

10:56.360 --> 11:03.560
sometimes. So given three sets of states, three alphabets, one machine goes from one state and

11:03.560 --> 11:07.640
one alphabet to the next state and the other alphabet. The second machine uses the same

11:07.640 --> 11:13.560
the output of the previous as its input, so you can define the composition as a state machine

11:13.560 --> 11:19.720
that takes the first alphabet and the first set of inputs and gives you the third alphabet and set

11:19.720 --> 11:28.440
of inputs. Composition, cool. Why? Because you can implement all these things as state machines and

11:28.440 --> 11:37.880
the output of one is the input of the next. So my stack on XMPP, you can implement TCP as a state

11:37.880 --> 11:45.640
machine. Have you heard of the Erlang socket, the new socket? It's implemented in TCP on top of

11:45.640 --> 11:52.840
gain state them. If you go to the source code. So I have the output of one state them, throwing

11:52.840 --> 11:57.160
into the output of the next state them. TLS is also implemented as a gain state them,

11:57.160 --> 12:05.560
throwing output to my thing, to the XML parser that throws its output to the XMPP protocol.

12:06.280 --> 12:13.320
So we are composing things. One last theoretical thing. The unions of FSMs that is

12:14.520 --> 12:20.760
uniting all the states and strings, it's also an FSM intersection, so the states and its input

12:20.760 --> 12:27.000
symbols in common gives you a very small FSM. It's also an FSM reversing, still an FSM,

12:29.080 --> 12:35.240
empty, so no states and no input is also an FSM that when you do union and concatenation with

12:35.240 --> 12:41.800
another FSM does nothing and homomorphism, so a function that transforms alphabets and

12:42.520 --> 12:50.360
states into other alphabets and states preserves the structure of an FSM. So FSMs are a semi-ring.

12:50.920 --> 12:58.120
This is an algebraic structure. Why is it useful to have search algebras? To prove things

12:58.760 --> 13:03.480
that you cannot prove with Turing machines because they do not form an algebra.

13:06.040 --> 13:14.920
So now let's do something engineering, state them. So as I said before, it's a Melly machine.

13:14.920 --> 13:23.400
It gets the input and the alphabets, it produces the states and alphabets, it produces the next,

13:23.400 --> 13:29.240
you follow, I hope. We can consider that the input are the messages in the mailbox and the output

13:30.040 --> 13:34.040
symbols are side effects, like for example sending messages to another mailbox.

13:36.360 --> 13:46.840
Gain state them. I'm a big fan. I love it, but I know that people sometimes don't use it because

13:46.840 --> 13:53.320
maybe it's confusing or I don't know, complicated. So I'm going to try to explain one thing that is

13:53.880 --> 14:01.800
very useful here. An extended mailbox. This is a discussion that the OTP team, when they put the

14:01.800 --> 14:07.880
pull request for gain state them, there is a big discussion with over a thousand messages that was

14:07.880 --> 14:11.960
probably forgotten, but when they discovered gain state them and I liked it, I went to the source

14:11.960 --> 14:19.000
and I read that super long thing. And there are useful things said there. A way to visualize a

14:19.080 --> 14:26.360
gain state them. Imagine that it has one queue, that is something more than the process mailbox,

14:26.360 --> 14:32.920
with like three pointers. The head pointing at the oldest event and the tail pointing at the youngest

14:32.920 --> 14:42.360
and current is where I am now. You can move where current is with some of the actions that gain

14:42.360 --> 14:48.440
state them gives you, for example postponing an event. Postponing an event means that current moves

14:48.440 --> 14:52.360
to the next, but the event is not forgotten. There is a different action that will put current

14:52.360 --> 14:58.440
again in the head. Not postponing and you consume it is removed from the queue. When the state changes,

14:58.440 --> 15:03.560
current goes again to head. Next event inserts things where current is and not at the tail.

15:04.360 --> 15:12.440
And timeouts inserts things at the tail. So the engine, the gain state them implementation

15:12.440 --> 15:19.320
allows you to extend the inputs that your formal state machine is going to get. How does it work?

15:19.320 --> 15:24.440
Imagine that we are here, we have event one and we decide to postpone it. What happens?

15:24.440 --> 15:31.880
It's still on the mailbox. We just are now going to deal with event two. Now we decide to do some

15:31.880 --> 15:37.160
stuff and then go to the next state. So that has been processed and current because we changed the

15:37.160 --> 15:45.880
state goes back to the previous. Now we are again going to handle event one and this time we decide

15:45.880 --> 15:54.600
to not change the state, but we generate new inputs as if this process has received a message.

15:54.600 --> 16:01.320
But this event A, which is ad hoc, we just created it, is inserted where current is. So it's the next

16:01.320 --> 16:06.520
event that we are going to handle. We can decide to postpone it. Now we are going to handle event

16:07.480 --> 16:11.800
three. With event three we do some stuff, but we don't generate events. Imagine that there is

16:11.800 --> 16:17.400
middle code here doing. So event three has been dealt with. Now you go to event four and you decide

16:17.400 --> 16:24.600
to postpone event four, but also insert and event B. So event four goes behind, you insert and event

16:24.600 --> 16:31.160
B, you get the idea. So the engine gives you a way to extend the process queue. What am I doing with time?

16:31.960 --> 16:40.600
Oh, one more important power. I'm not going to have time for everything. One more useful power of

16:40.600 --> 16:45.800
the state machines. Managing accidental complexity. There is a talk that I want to recommend. It's

16:45.800 --> 16:50.920
quite an old one, maybe something like 10 or 15 years ago by Ulf Rieger, where he was complaining

16:50.920 --> 16:58.280
about some limitations of GANFSM, but even GAN server that we all use. Very useful talk and I have

16:58.280 --> 17:05.160
one tiny answer to that with the new GAN state that didn't exist back then. Typical state on, off,

17:05.160 --> 17:11.800
but you can imagine that you're switching a light, but your switch talks to a

17:13.720 --> 17:20.040
through a cable protocol to the light machine. So when the user says on, this is a GAN server,

17:20.040 --> 17:25.560
you say and the state is off, you send a request to on, you wait for the answer on, it's on, vice versa,

17:26.200 --> 17:32.760
relatively intuitive code. Now imagine that that request through the cable protocol was not

17:32.760 --> 17:38.520
synchronous and imagine that the switches cannot block. It needs to do other stuff. So you send

17:38.520 --> 17:43.960
an asynchronous request to the light, hey, turn on yourself and continue doing other things, but then

17:43.960 --> 17:51.400
the user sends more off and on. What do you decide to do here? It's not part of the protocol. The

17:51.400 --> 17:58.600
events are now asynchronous and out of order. There is no global ordering. So there are some

17:58.600 --> 18:10.520
questions like you need to choose what to do. Sometimes this, this is the, so we can use a

18:10.520 --> 18:15.720
state machine. They use all the way. The name of the function is the name of the state and you can

18:15.720 --> 18:19.800
postpone things if you are already running a request, you postpone it and when if the user

18:19.880 --> 18:24.440
press on like a hundred times, by the time they like says on, then you have changed the state and

18:24.440 --> 18:30.360
you're going to handle all those. It's already on, so just do nothing. But the code is terribly

18:30.360 --> 18:35.560
symmetric. It feels repetitive. So problems, there is no ordering when things are asynchronous.

18:37.080 --> 18:41.560
Tying yourself to the ordering of events leads to accidental complexity. This is the

18:41.560 --> 18:47.000
point of Ulfiger when the order changes, the whole implementation changes. It grows relative to

18:47.000 --> 18:50.360
the number of states. This is super simple. It's a like that goes on and off. But imagine

18:50.920 --> 18:56.280
complicated protocols and for example a middle layer between a very talkative protocol and a

18:56.280 --> 19:07.480
like one and code reuse. So I really like the handle event way of handling things. It's a single

19:07.480 --> 19:17.800
function callback that gets a simple the state and the data. By the way, it's very confusing

19:17.800 --> 19:24.120
because we are used to the state of the process for the server thing. But in the state, the state is

19:24.120 --> 19:29.080
the state machine state. So the other thing where you save like, I don't know, the socket, for example,

19:29.080 --> 19:36.200
is called data. So just confusing terminology. This, you can just pattern match whether you're

19:36.200 --> 19:42.280
in the same state and the previous function that was terribly repetitive is now in a single

19:42.920 --> 19:50.120
function head. This is, I believe, a way to answer to the problem that Ulf raised and now I'm

19:50.120 --> 19:59.000
exactly on time. One more slide. A way to answer to that problem and in a way that you can reuse

19:59.000 --> 20:04.680
code, that you can decide the order of events because you can postpone things and you can also

20:04.680 --> 20:13.640
insert things. Quickly here, why I use on the XMPP, we had like this implementation. There is only

20:13.640 --> 20:19.160
one thing that I really like here. The composing. As I said before, you have the TCP state machines

20:19.160 --> 20:24.680
that go to TLS that goes to XML that goes to messaging. So if we want to implement this on a

20:24.680 --> 20:31.560
single process, this can be, for example, this is a simplification on my data. I have a parser and

20:31.560 --> 20:37.000
the crypto library that when I get that TCP payload, this is how we do it in Mongoose. I am not

20:37.000 --> 20:44.600
TCP, TCP we just use TCP to complicate it. So it's a separate process. But crypto and XML parsers,

20:44.600 --> 20:52.600
we implemented on the spot. There is a C code that the parsers, part of the XML, for example,

20:52.600 --> 20:59.400
it gives you a new parser with a buffer and the XML structure that then you can use to generate

20:59.400 --> 21:07.880
the events that my protocol cares about, the XML payloads. That's one use case that we have.

21:09.240 --> 21:15.320
That's me. You can find me by that picture in all the places. Those are some of the projects I

21:15.320 --> 21:23.000
work in and I was going to say questions, but we are one minute late. Thank you.

