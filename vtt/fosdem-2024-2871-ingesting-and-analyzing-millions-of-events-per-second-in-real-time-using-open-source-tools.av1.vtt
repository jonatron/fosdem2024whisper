WEBVTT

00:00.000 --> 00:14.480
Hi, I'm Javier here.

00:14.480 --> 00:17.360
This is going to be, oh, is it too loud?

00:17.360 --> 00:19.240
Okay, yeah, sorry.

00:19.240 --> 00:22.560
So yeah, I'm Javier.

00:22.560 --> 00:26.880
If you can find me on Mastodon or Twitter, I don't really use Mastodon, but it's Fosdome,

00:26.880 --> 00:28.320
so I have to put it there.

00:28.320 --> 00:32.200
But Twitter is better.

00:32.200 --> 00:34.840
It's not better, it's just where I hang around.

00:34.840 --> 00:38.760
Anyway, this is already too difficult.

00:38.760 --> 00:39.760
I don't have any slides.

00:39.760 --> 00:47.040
I have this gist that I will be scrolling, but this is going to be mostly a demo-driven

00:47.040 --> 00:48.040
talk.

00:48.040 --> 00:53.240
So I'm going to be speaking today about a template I've created, so you can start doing

00:53.240 --> 00:57.680
a stimuli that is using different open source components.

00:57.680 --> 00:59.040
I'm not a tab vocate.

00:59.040 --> 01:01.960
I've been working with data for a long time.

01:01.960 --> 01:07.000
10 years ago, it was actually my first Fosdome.

01:07.000 --> 01:08.520
I have the teaser to prove it.

01:08.520 --> 01:11.560
I was speaking at the time about FAST data.

01:11.560 --> 01:13.600
I was speaking about credits.

01:13.600 --> 01:16.200
And today I'm going to be speaking again about FAST data.

01:16.200 --> 01:22.400
And last year, I organized the FAST and streaming data at the room, so I really like data,

01:22.400 --> 01:26.720
and I wanted to share with you some of the things I've learned about working with data.

01:26.720 --> 01:31.160
So 10 years ago, it was difficult to work with streaming data.

01:31.160 --> 01:37.760
I put in the presentation that you can work with millions of events per second, and we

01:37.760 --> 01:39.720
have some users doing that actually.

01:39.720 --> 01:44.000
I work for a company that developed an open source database, but I wanted to speak about

01:44.000 --> 01:45.240
all the things today.

01:45.240 --> 01:50.240
So some people really need to ingest millions of events per second.

01:50.240 --> 01:52.000
Most often you don't have to do that.

01:52.000 --> 01:57.000
You are happy with a few thousands, hundreds, tens of events per second, whatever.

01:57.000 --> 02:02.680
But 10 years ago, it was really not that easy to work with streaming data, because actually

02:02.680 --> 02:06.920
many of the technologies that you have to see with FAST data, it didn't exist at the

02:06.920 --> 02:10.520
time.

02:10.520 --> 02:14.080
So for example, 10 years ago, you have Cassandra and Redis.

02:14.080 --> 02:15.080
They were already available.

02:15.080 --> 02:17.160
They were just FAST databases.

02:17.160 --> 02:21.280
But even Apache Kafka, I will be speaking about Kafka today in case you don't know it.

02:21.280 --> 02:23.440
Kafka was only three years old.

02:23.440 --> 02:29.040
And pretty much every other technology that you will consider FAST or real time today,

02:29.040 --> 02:33.880
in 2014, it was either not a 16 or just being born.

02:33.880 --> 02:39.560
Things like Spark Streaming or Apache Flink, which are super cool today.

02:39.560 --> 02:42.840
Grafana, I'm going to be presenting about Grafana also today.

02:42.840 --> 02:45.760
QuestDB, which is the database where I work.

02:45.760 --> 02:49.600
At the time, it has a different name.

02:49.600 --> 02:51.240
But it didn't exist.

02:51.240 --> 02:59.760
Even large proprietary people like Google Cloud or Amazon Kinesis at the time were not offering

02:59.760 --> 03:00.760
streaming data.

03:00.760 --> 03:05.120
So what I'm trying to say here is that 10 years ago, working with streaming data, it

03:05.120 --> 03:06.120
was not really a thing.

03:06.120 --> 03:07.120
Some people will do it.

03:07.120 --> 03:11.360
Twitter, for example, was doing streaming data at the time with very interesting technologies.

03:11.360 --> 03:13.880
Facebook was doing streaming data.

03:13.880 --> 03:17.080
But it was not so useful to work with streaming data 10 years ago.

03:17.080 --> 03:19.960
But you are thinking, that was 10 years ago.

03:19.960 --> 03:21.600
Now we are in 2024.

03:21.600 --> 03:22.600
It should be this year.

03:22.600 --> 03:24.680
Well, it should be this year.

03:24.680 --> 03:26.360
But it's not always the case.

03:26.360 --> 03:28.280
So let me just for a second.

03:28.280 --> 03:34.480
10 years ago, if you were doing streaming data, you would do like micro-batches of data.

03:34.480 --> 03:38.040
The streaming data platform would be a database.

03:38.040 --> 03:39.040
It would be Postgres.

03:39.040 --> 03:40.840
10 years ago, maybe it was MySQL.

03:40.840 --> 03:41.840
Sorry, Postgres.

03:41.840 --> 03:44.160
But 10 years ago, MySQL was kind of...

03:44.160 --> 03:46.800
But it would be Maria de Veo, Postgres, or Cassandra.

03:46.800 --> 03:51.800
But you will have the data platform was a database and a CSV file.

03:51.800 --> 03:52.800
That's it.

03:52.800 --> 03:53.800
Okay?

03:53.800 --> 03:56.840
But it's not really a pipeline.

03:56.840 --> 03:59.000
And a CSV file was not super cool.

03:59.000 --> 04:01.480
So then we had some innovation.

04:01.480 --> 04:05.600
So we added an extra step.

04:05.600 --> 04:08.080
It was basically adding Excel.

04:08.080 --> 04:09.200
We reinforced them.

04:09.200 --> 04:12.520
So maybe OpenOffice, LibreOffice, whatever.

04:12.520 --> 04:16.400
But basically, the data platform was a database and a spreadsheet.

04:17.000 --> 04:18.320
And this was the data platform.

04:18.320 --> 04:21.480
I was doing reports for like, you know, interesting people.

04:21.480 --> 04:22.920
And that's how you would do things.

04:22.920 --> 04:26.120
It was not really a data platform at all.

04:26.120 --> 04:28.080
But then, this is not working.

04:28.080 --> 04:29.080
That's fine.

04:29.080 --> 04:30.720
I can do it with this.

04:30.720 --> 04:32.720
But then, at some point, I will start doing the demo.

04:32.720 --> 04:33.720
Don't worry.

04:33.720 --> 04:37.840
But at some point, we decided, and that was the first thing I wanted to talk about today,

04:37.840 --> 04:43.320
we decided that sending all your data directly to your database, it might not be the best

04:43.320 --> 04:44.320
idea.

04:44.320 --> 04:49.160
At some point, we decided that it could be a good idea to decouple the ingestion of

04:49.160 --> 04:53.440
the data with the storage analysis of the data.

04:53.440 --> 04:54.440
For many reasons.

04:54.440 --> 04:58.760
For example, because if you send your data all the time to your database and you need

04:58.760 --> 05:02.640
to restart the server for any reason, you have a problem.

05:02.640 --> 05:06.080
You cannot really stop the database if you are sending data all the time.

05:06.080 --> 05:09.360
Or maybe your database is not super fast, or maybe it is.

05:09.360 --> 05:11.720
But you know, you can have something in between.

05:11.720 --> 05:15.520
Or maybe you want to send your data to your database and to somewhere else.

05:15.520 --> 05:19.120
And it will be cool to have something in between to send the data.

05:19.120 --> 05:21.800
So we started seeing things like Apache Kafka and so on.

05:21.800 --> 05:24.000
I will present about that today.

05:24.000 --> 05:26.240
But that was the first step.

05:26.240 --> 05:32.040
Starting sending data first to some buffer and then to the database.

05:32.040 --> 05:37.560
Then we started seeing dashboards, not really Excel, but dashboards like Tablo, Grafana,

05:37.560 --> 05:39.320
other things to present the data.

05:39.320 --> 05:41.000
That was already interesting.

05:41.000 --> 05:45.240
And at this point, we spent a few years with this architecture.

05:45.240 --> 05:51.400
You have something like Apache Kafka or Rabbit NQ or whatever to have data in gestion.

05:51.400 --> 05:56.360
You will have a database, could be a PostgreSQL database, could be a different database specialized

05:56.360 --> 06:00.480
in working with faster data like Cassandra or other databases.

06:00.480 --> 06:02.040
And then you will have some business dashboards.

06:02.040 --> 06:04.160
And that was cool.

06:04.160 --> 06:08.720
But then it was the time of more real time of more advances.

06:08.720 --> 06:13.400
And it was like, now that I can analyze data, I also want to predict the future.

06:13.400 --> 06:15.440
I don't want to understand only what's happened.

06:15.440 --> 06:18.880
I want to understand why it's happened and what's going to happen next.

06:18.880 --> 06:20.720
Spoiler, it doesn't work.

06:20.720 --> 06:27.480
But anyway, we presented walks and today I will show you some examples of time series

06:27.480 --> 06:28.480
forecasting.

06:28.480 --> 06:30.200
But that's the idea.

06:30.200 --> 06:32.240
So it was not enough already with doing analytics.

06:32.240 --> 06:34.560
It will be analytics and some machine learning.

06:34.680 --> 06:38.680
And by the way, the dashboards got real time.

06:38.680 --> 06:41.080
You didn't have to be reloading with S5.

06:41.080 --> 06:48.760
I had at some point, and this is true in my browser, I had an extension that the only

06:48.760 --> 06:52.120
thing it did, it was reloading the page every five seconds.

06:52.120 --> 06:54.280
So we could have real time dashboards.

06:54.280 --> 06:57.720
No, that's the thing because they didn't reload.

06:57.720 --> 06:59.320
So you had that.

06:59.320 --> 07:02.360
So then it came people like Grafana or maybe Looker.

07:02.360 --> 07:05.240
And you look at it because it's not open source.

07:05.240 --> 07:08.720
And then you started seeing real time analytics.

07:08.720 --> 07:09.920
And that was pretty cool.

07:09.920 --> 07:13.560
And it's like, okay, this is looking something interesting.

07:13.560 --> 07:18.360
But yeah, then we have, of course, observability, monitoring, because if you want to go to

07:18.360 --> 07:22.680
production, you need to make sure things are working, not only on your machine, but in

07:22.680 --> 07:23.680
real time.

07:23.680 --> 07:24.680
And that's kind of the thing.

07:24.680 --> 07:26.920
We started in more and more things.

07:26.920 --> 07:31.800
And if you want to start today to ingest streaming data and less streaming data, you have all

07:31.800 --> 07:34.720
the components, but there are a lot of components.

07:34.720 --> 07:36.400
And they have to work together.

07:36.400 --> 07:40.000
And if you have never done it before, it can feel overwhelming.

07:40.000 --> 07:43.920
So that's what I want to be talking about today, how we can build a platform that can

07:43.920 --> 07:46.120
do all of these things.

07:46.120 --> 07:49.920
So this is what it looks like, like everything everywhere, all of the ones, like that movie.

07:49.920 --> 07:53.200
So that's basically what it looks like.

07:53.200 --> 07:58.440
Today you want to have an IT platform at the very least, very likely you have a dating

07:58.440 --> 08:00.400
system layer.

08:00.400 --> 08:06.400
The data from that layer needs to go somehow to your analytics database.

08:06.400 --> 08:08.960
It will probably have some listeners and application.

08:08.960 --> 08:14.040
You might want to send data also directly without the buffer, because your application

08:14.040 --> 08:15.920
doesn't support it, whatever.

08:15.920 --> 08:20.080
Then you have your database, your machine learning data science workflow, your monitoring

08:20.080 --> 08:22.080
workflow, your dashboard.

08:22.080 --> 08:24.040
And this is kind of the thing.

08:24.040 --> 08:28.760
So as I said, I call this the everything everywhere, all of the ones.

08:28.760 --> 08:33.320
And if it's the first time that you are working with streaming data, this might look to you

08:33.320 --> 08:36.440
the same way, look in the movies, like, what's this?

08:36.440 --> 08:39.640
Like, I don't know, this doesn't look natural.

08:39.640 --> 08:41.520
It looks like very weird.

08:41.520 --> 08:46.480
So that's why I decided, after working with streaming data for a while and suffering these

08:46.480 --> 08:54.000
things, I decided it was time to develop some easy example that you could deploy and

08:54.000 --> 08:58.720
if you don't have it, if you know about real analytics, don't use this.

08:58.720 --> 09:03.520
But if you have no idea about real analytics, if you've never done it before, this template

09:03.520 --> 09:07.520
gives you everything you need, everything.

09:07.520 --> 09:15.680
It gives you a few components you can use to capture real analytics end to end.

09:15.680 --> 09:21.520
Turning gestion to dashboard with monitoring with everything you need, with data science

09:21.520 --> 09:25.200
to do real analytics, and everything is open source in this template.

09:25.200 --> 09:27.800
So this is what I want to talk about today.

09:27.800 --> 09:29.280
What is in the template?

09:29.280 --> 09:33.400
And basically, I want to do this for a demo.

09:33.400 --> 09:34.560
I don't want to be speaking on it.

09:34.560 --> 09:36.160
I've been speaking on it for ten minutes.

09:36.160 --> 09:37.160
That's it.

09:37.160 --> 09:40.160
I mean, I want to be speaking now, but it's going to be with things moving on, with code

09:40.160 --> 09:41.160
and everything.

09:41.160 --> 09:44.200
So this is what the template, it looks like very blurry.

09:44.200 --> 09:49.480
But basically, this template is going to give you an Apache Kafka.

09:49.480 --> 09:52.160
I will tell you now about Kafka and why I chose it.

09:52.160 --> 09:58.120
But Apache Kafka, which is the ingestion layer, I have KafkaConnet, which is a component for

09:58.120 --> 10:01.840
getting data out of Kafka and into other places.

10:01.840 --> 10:06.080
The data I'm going to put it into QSDB, which is a fast and serious database, it's my employer.

10:06.080 --> 10:07.640
So I'm biased here.

10:07.640 --> 10:10.560
But I think they pay me.

10:10.560 --> 10:12.440
But it's Apache 2.

10:12.440 --> 10:13.760
So it's open source.

10:13.760 --> 10:14.760
You can use it for free.

10:14.760 --> 10:16.720
You don't have to pay to use it.

10:16.720 --> 10:18.840
But if you pay, it's better because then I get...

10:18.840 --> 10:22.720
Anyway, so I'm going to be using a database here.

10:22.720 --> 10:25.760
Mom, maybe I'm looking at this.

10:25.760 --> 10:27.360
I know I speak too much.

10:27.360 --> 10:32.440
Anyway, and then I'm going to be using Jupyter Notebooks, which are...

10:32.440 --> 10:36.520
It's Python on the browser, which is pretty cool for doing that science.

10:36.520 --> 10:42.160
And I will do some interactive exploration and some forecasting modeling.

10:42.160 --> 10:44.680
We'll have real time, that's what's with Grafana.

10:44.880 --> 10:51.360
On top of that, I'm using Telegraph to collect metrics and store them again on a database.

10:51.360 --> 10:53.880
So that's everything in the template.

10:53.880 --> 10:57.000
And this is what I'm going to be talking about today.

10:57.000 --> 10:59.960
So the template lives here.

10:59.960 --> 11:03.720
Until yesterday, it was on my user on GitHub.

11:03.720 --> 11:04.720
But that's of yesterday.

11:04.720 --> 11:08.880
We move it to my organization because they pay me.

11:08.880 --> 11:12.080
And we are open source and we are cool because FOSDEM.

11:12.560 --> 11:13.560
Yeah.

11:13.560 --> 11:16.960
So basically, I'm the single contributor.

11:16.960 --> 11:18.640
So basically, it lives here.

11:18.640 --> 11:21.000
Time series, stimuli, this template.

11:21.000 --> 11:23.520
The entry point is a Docker compose.

11:23.520 --> 11:28.080
So I know some people prefer other things, but Docker compose is cool.

11:28.080 --> 11:32.200
So this Docker compose, if you want to see what it's doing, feel free.

11:32.200 --> 11:35.720
But basically it's going to start all the things that I've told you before.

11:35.720 --> 11:37.920
And I'm going to show you what this looks like.

11:38.080 --> 11:42.000
So Docker compose up.

11:42.000 --> 11:44.440
If you're familiar, you all know Docker.

11:44.440 --> 11:47.760
Docker compose, in case anyone doesn't know Docker compose, I know, you know,

11:47.760 --> 11:48.720
better than me.

11:48.720 --> 11:53.480
But Docker compose allows you to start several containers in one go.

11:53.480 --> 11:59.240
And since networking Docker is a mess, the cool thing about Docker compose is

11:59.240 --> 12:02.320
that all the containers in the same compose file, they can talk to each other,

12:02.320 --> 12:03.120
which is cool.

12:03.120 --> 12:04.160
It's convenient.

12:04.160 --> 12:07.880
So if I do Docker compose up, look, looks.

12:08.120 --> 12:09.800
Things moving.

12:09.800 --> 12:12.200
So yeah, thank you for coming to this talk.

12:12.200 --> 12:14.400
So yeah, things are starting now.

12:14.400 --> 12:17.800
In my laptop, it starts fast because all the images are already here.

12:17.800 --> 12:20.400
You start from scratch, it needs to download a lot of things.

12:20.400 --> 12:25.120
It's going to download about one gigabyte of data.

12:25.120 --> 12:27.040
No, one gigabyte of data.

12:27.040 --> 12:28.880
But there's a lot of images here.

12:28.880 --> 12:32.120
But basically, I don't have any custom image.

12:32.120 --> 12:35.880
All the images in Docker compose are the standard,

12:35.920 --> 12:38.200
you can't know book image, standard Kafka.

12:38.200 --> 12:39.600
I didn't add anything.

12:39.600 --> 12:42.600
So it downloads everything in a wire connection.

12:42.600 --> 12:47.640
It takes about one minute or so to download everything and start up from scratch.

12:47.640 --> 12:53.480
So once you have this ready, we already have something up and running.

12:53.480 --> 12:54.000
Not much.

12:54.000 --> 12:55.200
It doesn't look like much.

12:55.200 --> 12:58.360
But since I'm moving there, and I want to show you what we have.

12:59.680 --> 13:03.800
So the first thing we have, I opened the other browser.

13:03.800 --> 13:06.360
Not you, Chrome, not today, not today.

13:08.400 --> 13:10.040
It's my default browser, sort of like that.

13:11.720 --> 13:14.560
I feel terribly bad because actually,

13:14.560 --> 13:17.480
Firefox was giving free cookies before and I took one.

13:17.480 --> 13:21.560
So thank you Firefox, I'm using Firefox for the demo.

13:21.560 --> 13:24.200
So the first thing you have here, it's,

13:24.200 --> 13:28.840
this is Jupyter notebooks.

13:28.840 --> 13:33.880
I'll show you to run Python, also the things, but Python from the browser.

13:33.880 --> 13:38.760
So the thing I have here to see like easily, I have created a script,

13:38.760 --> 13:43.320
which is going to read data from GitHub because open source.

13:43.320 --> 13:46.000
It's going to be reading public data from GitHub and

13:46.000 --> 13:48.400
it's going to be sending data to Kafka.

13:48.400 --> 13:50.480
And it's going to go through all the steps.

13:50.480 --> 13:52.920
And it's going to give me a nice task board.

13:52.920 --> 13:56.160
So if I execute this from the browser, it's going to be,

13:56.160 --> 14:00.400
it's going to be calling the GitHub API and every 10 seconds,

14:00.400 --> 14:02.640
it's going to be getting new data.

14:02.640 --> 14:05.240
I do it every 10 seconds because there are some great limits.

14:05.240 --> 14:08.480
I wanted to make sure I'm within the limits, but every 10 seconds,

14:08.480 --> 14:10.080
I get data from GitHub.

14:12.080 --> 14:15.240
It's going to go through all the components and

14:15.240 --> 14:17.240
eventually it will read the data.

14:17.240 --> 14:20.840
I'm going to tell you now all the things.

14:20.840 --> 14:25.640
And here in this dashboard, if everything was fine,

14:25.640 --> 14:29.400
I cannot see any data, that's not looking good.

14:29.400 --> 14:32.000
Let me ask for a second.

14:32.000 --> 14:36.800
We still have data, yes?

14:36.800 --> 14:41.320
Yeah, we do have data, actually, I guess.

14:41.440 --> 14:44.440
Table doesn't exist, table doesn't exist.

14:44.440 --> 14:47.440
Not you, Chrome, sorry.

14:49.440 --> 14:50.440
That's why I don't like it, man.

14:50.440 --> 14:53.440
It's always there, it's always trying.

14:53.440 --> 14:57.440
So, okay.

14:57.440 --> 15:02.440
Let's just for one second to check if I have data.

15:02.440 --> 15:05.440
I'm going to go through all the steps.

15:05.440 --> 15:07.440
I'm going to go through all the steps.

15:07.560 --> 15:13.560
Just one second to check if I have data coming into Kafka.

15:17.560 --> 15:21.560
It's funny because I was testing it earlier, like right here.

15:21.560 --> 15:24.560
Let me for a second what I have.

15:24.560 --> 15:28.560
I will tell you what I'm doing once I know what I'm doing myself.

15:28.560 --> 15:33.560
I mean, I cannot be more, okay.

15:33.560 --> 15:35.560
So, just for a second.

15:35.680 --> 15:39.680
What are you doing here, Javier?

15:39.680 --> 15:41.680
Topic?

15:41.680 --> 15:44.680
Can I see which topics I have created?

15:44.680 --> 15:46.680
I have...

15:46.680 --> 15:48.680
I will tell you in one second what I mean.

15:48.680 --> 15:51.680
The moment it works, I will tell you everything about it.

15:51.680 --> 15:55.680
But, okay, the topic is here.

15:55.680 --> 15:57.680
And...

15:57.680 --> 16:02.680
So, basically now I'm trying to see if I have some data

16:02.800 --> 16:08.800
entering into my Kafka platform or not.

16:08.800 --> 16:11.800
Yeah, data is coming here.

16:11.800 --> 16:15.800
So, let me go to the dashboard again.

16:15.800 --> 16:17.800
Oh, yeah.

16:17.800 --> 16:20.800
So, I don't know exactly what happened.

16:20.800 --> 16:22.800
I started this grid from the command line.

16:22.800 --> 16:25.800
But, basically, this is what should happen from the beginning.

16:25.800 --> 16:27.800
I have data which is going to Kafka.

16:27.800 --> 16:30.800
And I have a dashboard that is refreshing.

16:30.920 --> 16:32.920
No hands here.

16:32.920 --> 16:34.920
And in five seconds, it's going to have new data.

16:34.920 --> 16:36.920
That's kind of the idea, okay.

16:36.920 --> 16:40.920
So, this is, you know, this is like high level overview.

16:40.920 --> 16:43.920
What I want to do now is telling you all the different steps

16:43.920 --> 16:45.920
and how we can see what's happening

16:45.920 --> 16:47.920
at a different point in time.

16:47.920 --> 16:50.920
So, the first thing is...

16:50.920 --> 16:54.920
I told you already, I have here a script

16:54.920 --> 16:56.920
which is sending GitHub events to Kafka.

16:56.920 --> 16:59.920
So, Kafka, it's a message broker.

17:00.040 --> 17:03.040
It means you send messages, events, what...

17:03.040 --> 17:05.040
A message can be anything.

17:05.040 --> 17:07.040
In this case, I'm sending JSON because it is here.

17:07.040 --> 17:09.040
But you could have, like, Abro.

17:09.040 --> 17:12.040
You could have, like, anything, plain strings, whatever.

17:12.040 --> 17:15.040
You send events to Kafka on one end.

17:15.040 --> 17:18.040
And then, different consumers can read those events.

17:18.040 --> 17:21.040
It sounds super simple, but it's very powerful.

17:21.040 --> 17:24.040
First, because Kafka is very scalable.

17:24.040 --> 17:27.040
You can use Kafka at any scale.

17:27.040 --> 17:29.040
If you have a lot of data, you can add many servers.

17:29.160 --> 17:32.160
And, you know, it scales horizontally.

17:32.160 --> 17:34.160
It works pretty well.

17:34.160 --> 17:36.160
In Kafka, you don't have tables.

17:36.160 --> 17:38.160
You have something called topics.

17:38.160 --> 17:40.160
So, when you send a message, you publish the message

17:40.160 --> 17:42.160
into a topic.

17:42.160 --> 17:44.160
And then you can have any number of consumers

17:44.160 --> 17:46.160
reading messages from that topic.

17:46.160 --> 17:48.160
And they can read messages in different ways.

17:48.160 --> 17:50.160
You can choose to have each consumer

17:50.160 --> 17:53.160
is going to see all the messages from the topic.

17:53.160 --> 17:55.160
You can have them...

17:55.160 --> 17:57.160
You can create a consumer group.

17:57.280 --> 17:59.280
They read collaboratively.

17:59.280 --> 18:01.280
So, basically, you can have a topic

18:01.280 --> 18:03.280
in which consumers are reading data,

18:03.280 --> 18:05.280
and all of them are going to see all the data.

18:05.280 --> 18:07.280
You can have a topic in which

18:07.280 --> 18:09.280
consumers are going to be reading in parallel,

18:09.280 --> 18:11.280
different parts, so they can collaborate.

18:11.280 --> 18:13.280
You can have a topic in which

18:13.280 --> 18:15.280
some consumers are reading from the beginning,

18:15.280 --> 18:17.280
the others are reading from the other part.

18:17.280 --> 18:19.280
You define a retention period

18:19.280 --> 18:21.280
for each topic, and you can replay

18:21.280 --> 18:23.280
the stream of events

18:23.280 --> 18:25.280
at any point until that retention period.

18:25.400 --> 18:27.400
So, you want to replay what happened.

18:27.400 --> 18:29.400
You can replay again.

18:29.400 --> 18:31.400
So, it gives you a lot of possibilities,

18:31.400 --> 18:33.400
and it's a very good way of decoupling the ingestion layer.

18:33.400 --> 18:35.400
So, when you are working with Kafka,

18:35.400 --> 18:37.400
basically, what you do

18:37.400 --> 18:39.400
is like you use any Kafka library.

18:39.400 --> 18:41.400
And as I showed you here,

18:41.400 --> 18:43.400
all you have to do is

18:43.400 --> 18:45.400
you create a producer

18:45.400 --> 18:47.400
and you send your events,

18:47.400 --> 18:49.400
in this case, in JSON,

18:49.400 --> 18:51.400
to a topic from that producer library.

18:51.400 --> 18:53.400
On this notebook,

18:53.520 --> 18:55.520
I have the code only on Python,

18:55.520 --> 18:57.520
but in the template,

18:57.520 --> 18:59.520
if you defer the languages,

18:59.520 --> 19:01.520
you have the ingestion in multiple languages.

19:01.520 --> 19:03.520
I did this

19:03.520 --> 19:05.520
in all the languages

19:05.520 --> 19:07.520
that chatGPT could help me with.

19:07.520 --> 19:09.520
I did it in Python,

19:09.520 --> 19:11.520
and then I told chatGPT,

19:11.520 --> 19:13.520
do this in Node.js.

19:13.520 --> 19:15.520
Didn't work. Hey, I get this error.

19:15.520 --> 19:17.520
Anyway, so it works.

19:17.520 --> 19:19.520
So, basically here, you have the same code

19:19.520 --> 19:21.520
in different languages.

19:21.640 --> 19:23.640
You can do the ingestion using Node,

19:23.640 --> 19:25.640
you can do the ingestion using Java,

19:25.640 --> 19:27.640
using Rust,

19:27.640 --> 19:29.640
which is basically right now

19:29.640 --> 19:31.640
from the command line,

19:31.640 --> 19:33.640
and here sending from Rust, actually,

19:33.640 --> 19:35.640
they tie into the topic.

19:35.640 --> 19:37.640
So, right now, we ingest data

19:37.640 --> 19:39.640
into a Kafka topic.

19:39.640 --> 19:41.640
But Kafka itself is not doing anything

19:41.640 --> 19:43.640
with not-you-chrome.

19:43.640 --> 19:45.640
And this is...

19:45.640 --> 19:47.640
No, sorry, it's my brain.

19:47.640 --> 19:49.640
It's where, like...

19:49.760 --> 19:51.760
In the process in Kafka,

19:51.760 --> 19:53.760
this is the first step in the process here.

19:53.760 --> 19:55.760
From GitHub, we send to Kafka.

19:55.760 --> 19:57.760
Okay?

19:57.760 --> 19:59.760
But from Kafka, the messages don't go anywhere.

19:59.760 --> 20:01.760
Kafka, by design,

20:01.760 --> 20:03.760
is not pushing messages.

20:03.760 --> 20:05.760
There are some message brokers that use the push model.

20:05.760 --> 20:07.760
In Kafka, we use the pull model,

20:07.760 --> 20:09.760
which means if you want to read messages,

20:09.760 --> 20:11.760
you need to tell the broker.

20:11.760 --> 20:13.760
You need to tell the server in Kafka,

20:13.760 --> 20:15.760
hey, give me the following batch of messages.

20:15.760 --> 20:17.760
The people in Kafka decided to do it that way,

20:17.880 --> 20:19.880
because if you're going to have multiple consumers,

20:19.880 --> 20:21.880
it's a better way to work.

20:21.880 --> 20:23.880
If you are pushing data all the time

20:23.880 --> 20:25.880
and you have a low consumer

20:25.880 --> 20:27.880
and you're pushing data,

20:27.880 --> 20:29.880
the consumer eventually is going to be overwhelmed,

20:29.880 --> 20:31.880
it's not going to work.

20:31.880 --> 20:33.880
So in Kafka, you need to be pulling data.

20:33.880 --> 20:35.880
So you will need some application

20:35.880 --> 20:37.880
calling Kafka

20:37.880 --> 20:39.880
and saying, hey, you have new messages in the topic,

20:39.880 --> 20:41.880
give me the messages,

20:41.880 --> 20:43.880
I want to store them somewhere.

20:43.880 --> 20:45.880
For the first years, that would be how you work with Kafka.

20:45.880 --> 20:46.880
But that was annoying.

20:47.000 --> 20:49.000
So Kafka created something called KafkaConnect,

20:49.000 --> 20:51.000
which is part also of the open source Kafka.

20:51.000 --> 20:53.000
In Kafka, there is a

20:53.000 --> 20:55.000
like in pretty much in many projects,

20:55.000 --> 20:57.000
you have a part which is fully open source

20:57.000 --> 20:59.000
and a part which is proprietary.

20:59.000 --> 21:01.000
The things I'm showing today

21:01.000 --> 21:03.000
are only open source, of course.

21:03.000 --> 21:05.000
So KafkaConnect is part of open source Kafka.

21:05.000 --> 21:07.000
And what it does,

21:07.000 --> 21:09.000
it gives you connectors

21:09.000 --> 21:11.000
to read data from Kafka

21:11.000 --> 21:13.000
and write into multiple destinations.

21:13.000 --> 21:15.000
In the audience today,

21:15.120 --> 21:17.120
we have my colleague, Yaramir,

21:17.120 --> 21:19.120
I don't know exactly where he is,

21:19.120 --> 21:21.120
but Yaramir actually wrote the connector

21:21.120 --> 21:23.120
to write data from KafkaConnect into QSDB.

21:23.120 --> 21:25.120
I wouldn't know how to do that, but you know.

21:25.120 --> 21:27.120
So basically what I have here,

21:27.120 --> 21:29.120
I have a connector

21:29.120 --> 21:31.120
which is

21:31.120 --> 21:33.120
sending data from Kafka

21:33.120 --> 21:35.120
into a database.

21:35.120 --> 21:37.120
This KafkaConnect,

21:37.120 --> 21:39.120
it doesn't have any web interface.

21:39.120 --> 21:41.120
It has just an API.

21:41.120 --> 21:43.120
But I can tell KafkaConnect

21:43.240 --> 21:45.240
from the notebook.

21:45.240 --> 21:47.240
I can tell KafkaConnect, which plugins

21:47.240 --> 21:49.240
you have available.

21:49.240 --> 21:51.240
By default, you have only a few plugins.

21:51.240 --> 21:53.240
In this compose,

21:53.240 --> 21:55.240
Docker compose, I've included

21:55.240 --> 21:57.240
the driver, the GAR file

21:57.240 --> 21:59.240
to send data to QSDB.

21:59.240 --> 22:01.240
You could be sending data to, I don't know,

22:01.240 --> 22:03.240
to Amazon S3,

22:03.240 --> 22:05.240
to Hadoop file system,

22:05.240 --> 22:07.240
to Clickhouse, to Postgres, to whatever.

22:07.240 --> 22:09.240
In this case, I'm sending messages to another Kafka.

22:09.240 --> 22:11.240
So in this case, I'm sending messages

22:11.360 --> 22:13.360
to the KafkaConnect.

22:13.360 --> 22:15.360
So I have this connector available.

22:15.360 --> 22:17.360
And then what I have to do,

22:17.360 --> 22:19.360
I have to configure

22:19.360 --> 22:21.360
if I want to send data,

22:21.360 --> 22:23.360
I have to tell Connect

22:23.360 --> 22:25.360
where to get the data from

22:25.360 --> 22:27.360
and where it's going to send the data to.

22:27.360 --> 22:29.360
So in this case, I have a configuration

22:29.360 --> 22:31.360
that says

22:31.360 --> 22:33.360
I'm going to be reading data

22:33.360 --> 22:35.360
from a topic called GitHub Events,

22:35.360 --> 22:37.360
which is the topic in which I'm sending data.

22:37.360 --> 22:39.360
I'm going to be sending the data,

22:39.480 --> 22:41.480
I'm going to be sending data to this

22:41.480 --> 22:43.480
host and port.

22:43.480 --> 22:45.480
And then its different connector

22:45.480 --> 22:47.480
is going to have different parameters.

22:47.480 --> 22:49.480
At the very least, you will want to configure

22:49.480 --> 22:51.480
the format of the messages.

22:51.480 --> 22:53.480
In this case, I'm saying that we are going to be getting

22:53.480 --> 22:55.480
JSON messages in JSON

22:55.480 --> 22:57.480
and I want to output strings.

22:57.480 --> 22:59.480
So different connectors will have different options.

22:59.480 --> 23:01.480
And in this case, I'm also telling

23:01.480 --> 23:03.480
the timestamp name,

23:03.480 --> 23:05.480
I want to rename it to create it.

23:05.480 --> 23:07.480
So those kind of things, basically.

23:07.600 --> 23:09.600
But you know, it's connected with different options.

23:09.600 --> 23:11.600
But in the end, what I'm doing here

23:11.600 --> 23:13.600
is configuring how Kafka Connect

23:13.600 --> 23:15.600
is going to be reading data from a topic

23:15.600 --> 23:17.600
and how it's going to be writing the data

23:17.600 --> 23:19.600
at the destination.

23:19.600 --> 23:21.600
Are you still with me here? Yes?

23:21.600 --> 23:23.600
Cool.

23:23.600 --> 23:25.600
So destination is a database.

23:25.600 --> 23:27.600
And that database is called QuestDB,

23:27.600 --> 23:29.600
as I told you earlier.

23:29.600 --> 23:31.600
QuestDB is my employer.

23:31.600 --> 23:33.600
So, you know, I only have

23:33.600 --> 23:35.600
good things to say about QuestDB

23:35.720 --> 23:37.720
because it's actually pretty cool.

23:37.720 --> 23:39.720
So, in this,

23:39.720 --> 23:41.720
this is the instance

23:41.720 --> 23:43.720
running in Docker.

23:43.720 --> 23:45.720
And you can see here, it has multiple tables.

23:45.720 --> 23:47.720
Because I'm writing also monitoring.

23:47.720 --> 23:49.720
But basically, the tabling

23:49.720 --> 23:51.720
with Wayne-Gestin data is this one.

23:51.720 --> 23:53.720
GitHub Events.

23:53.720 --> 23:55.720
If I do something like

23:55.720 --> 23:57.720
select count

23:57.720 --> 23:59.720
from GitHub Events,

23:59.720 --> 24:01.720
we have only, like, you know,

24:01.720 --> 24:03.720
a thousand events,

24:03.840 --> 24:05.840
which is not too much.

24:05.840 --> 24:07.840
But anyway, it is what we have right now.

24:07.840 --> 24:09.840
So,

24:09.840 --> 24:11.840
this is what it looks like.

24:11.840 --> 24:13.840
We have, like, push events

24:13.840 --> 24:15.840
from different repositories,

24:15.840 --> 24:17.840
from different people, at different time stamps.

24:17.840 --> 24:19.840
And this database, this QuestDB thing,

24:19.840 --> 24:21.840
it's an open source database

24:21.840 --> 24:23.840
which is specialized on time series.

24:23.840 --> 24:25.840
Time series basically means

24:25.840 --> 24:27.840
I have data points with a time stamp

24:27.840 --> 24:29.840
and I want to do aggregations by time.

24:29.840 --> 24:31.840
So, we speak SQL,

24:31.960 --> 24:33.960
but a cool thing,

24:33.960 --> 24:35.960
I believe it's a cool thing in QuestDB,

24:35.960 --> 24:37.960
you can do things like this.

24:37.960 --> 24:39.960
I can say, I want to have

24:39.960 --> 24:41.960
the count of

24:41.960 --> 24:43.960
how many messages

24:43.960 --> 24:45.960
I'm getting in intervals

24:45.960 --> 24:47.960
of, for example, I don't know,

24:47.960 --> 24:49.960
every five seconds.

24:49.960 --> 24:51.960
So, I can say this.

24:51.960 --> 24:53.960
And what I get here is like,

24:53.960 --> 24:55.960
for each five seconds,

24:55.960 --> 24:57.960
I'm running this aggregation.

24:57.960 --> 24:59.960
So, basically, this is like a group buy.

25:00.080 --> 25:02.080
But instead of grouping

25:02.080 --> 25:04.080
by, you know, any dimension,

25:04.080 --> 25:06.080
I'm grouping in time intervals.

25:06.080 --> 25:08.080
So, this is the cool thing about a time series database.

25:08.080 --> 25:10.080
It gives you a lot of things for, you know,

25:10.080 --> 25:12.080
working with time.

25:12.080 --> 25:14.080
It gives you a lot of interesting things

25:14.080 --> 25:16.080
for working with time data.

25:16.080 --> 25:18.080
Or you could also say, I want to have this,

25:18.080 --> 25:20.080
but by repository

25:20.080 --> 25:22.080
and time stamp

25:22.080 --> 25:24.080
in intervals of five minutes.

25:24.080 --> 25:26.080
So, here I will have

25:26.080 --> 25:28.080
for this different repository

25:28.200 --> 25:30.200
I'm going to have for each five minutes interval

25:30.200 --> 25:32.200
I'm going to have

25:32.200 --> 25:34.200
the number of events for that repository.

25:34.200 --> 25:36.200
So, you can start to see how this can be interesting.

25:36.200 --> 25:38.200
You can also do things like

25:38.200 --> 25:40.200
join two different tables by approximate time.

25:40.200 --> 25:42.200
Maybe in one table I'm getting

25:42.200 --> 25:44.200
data every few microseconds.

25:44.200 --> 25:46.200
In the other, I'm getting data every two hours.

25:46.200 --> 25:48.200
But I want to join the closest event in time.

25:48.200 --> 25:50.200
Doing that in SQL,

25:50.200 --> 25:52.200
it can be done, but it's like, you know,

25:52.200 --> 25:54.200
a bit query.

25:54.200 --> 25:56.200
So, here is like, okay, just as of join.

25:56.320 --> 25:58.320
So, we have this kind of extensions.

25:58.320 --> 26:00.320
And the main thing about QSDB

26:00.320 --> 26:02.320
is that it's built

26:02.320 --> 26:04.320
time series data tends to be fast.

26:04.320 --> 26:06.320
So, it's built for a speed.

26:06.320 --> 26:08.320
In this case, I only have

26:08.320 --> 26:10.320
not even 2000 events.

26:10.320 --> 26:12.320
So, the 1000 events

26:12.320 --> 26:14.320
sounds like nothing, but

26:14.320 --> 26:16.320
if I go, oh, not the most,

26:16.320 --> 26:18.320
if I go to

26:18.320 --> 26:20.320
we have a demo site

26:20.320 --> 26:22.320
and I will stop

26:22.320 --> 26:24.320
speaking about QSDB

26:24.440 --> 26:26.440
in one second, I cannot write.

26:26.440 --> 26:28.440
I cannot even type the name of my

26:28.440 --> 26:30.440
database. Okay, that's better, yeah?

26:30.440 --> 26:32.440
Demo...

26:32.440 --> 26:34.440
Okay.

26:38.440 --> 26:40.440
See, eventually

26:40.440 --> 26:42.440
we'll note, internet here,

26:42.440 --> 26:44.440
I have the...

26:44.440 --> 26:46.440
The Wi-Fi here is not really great.

26:46.440 --> 26:48.440
When it comes back, I will show you

26:48.440 --> 26:50.440
which is totally why I was not getting

26:50.440 --> 26:52.440
any data earlier.

26:52.560 --> 26:54.560
So, in this public site

26:54.560 --> 26:56.560
we have some demo tables

26:56.560 --> 26:58.560
and I have one table

26:58.560 --> 27:00.560
which is a bit

27:00.560 --> 27:02.560
bigger than the one I just

27:02.560 --> 27:04.560
showed you.

27:04.560 --> 27:06.560
So, this table

27:06.560 --> 27:08.560
is not huge,

27:08.560 --> 27:10.560
but it's already

27:10.560 --> 27:12.560
1.6 billion records,

27:12.560 --> 27:14.560
which is not too bad.

27:14.560 --> 27:16.560
1.6 billion records is not too bad.

27:16.560 --> 27:18.560
And a cool thing about QSDB

27:18.560 --> 27:20.560
is that it's designed to be

27:20.680 --> 27:22.680
a good example.

27:22.680 --> 27:24.680
I could do something like

27:24.680 --> 27:26.680
give me the average...

27:26.680 --> 27:28.680
Let me just find any column

27:28.680 --> 27:30.680
in this table.

27:30.680 --> 27:32.680
So, give me the average

27:32.680 --> 27:34.680
fair amount, for example.

27:34.680 --> 27:36.680
From this table

27:36.680 --> 27:38.680
that has 1.6

27:38.680 --> 27:40.680
billion records.

27:40.680 --> 27:42.680
How long should you expect

27:42.680 --> 27:44.680
to do a full scan

27:44.680 --> 27:46.680
over 1.6 billion records

27:46.680 --> 27:48.680
and do an average? We cannot

27:48.800 --> 27:50.800
catch. How fast will you say?

27:50.800 --> 27:52.800
1.6 billion records.

27:52.800 --> 27:54.800
How many?

27:54.800 --> 27:56.800
20 seconds.

27:56.800 --> 27:58.800
20 seconds, that's good.

27:58.800 --> 28:00.800
I like it.

28:00.800 --> 28:02.800
It took 400 milliseconds.

28:02.800 --> 28:04.800
It was not too bad, but it's like

28:04.800 --> 28:06.800
that's the kind of thing.

28:06.800 --> 28:08.800
It's still kind of slow

28:08.800 --> 28:10.800
because QSDB

28:10.800 --> 28:12.800
is optimized

28:12.800 --> 28:14.800
to be very fast

28:14.800 --> 28:16.800
when you have chunks of time.

28:16.920 --> 28:18.920
So, it's like

28:18.920 --> 28:20.920
where...

28:20.920 --> 28:22.920
I don't know.

28:22.920 --> 28:24.920
If you go only for things like

28:24.920 --> 28:26.920
I want to only select

28:26.920 --> 28:28.920
data which is in

28:28.920 --> 28:30.920
one particular year, for example,

28:30.920 --> 28:32.920
this will be way faster

28:32.920 --> 28:34.920
because in the end...

28:34.920 --> 28:36.920
Oh.

28:36.920 --> 28:38.920
That's better.

28:38.920 --> 28:40.920
This will be faster.

28:40.920 --> 28:42.920
It took 200 milliseconds,

28:42.920 --> 28:44.920
which is not too bad considering

28:44.920 --> 28:46.920
that we have a lot of time.

28:46.920 --> 28:48.920
Oops.

28:48.920 --> 28:50.920
Count from trips.

28:54.920 --> 28:56.920
Okay.

28:56.920 --> 28:58.920
That's not...

28:58.920 --> 29:00.920
Let's go for one year with data.

29:00.920 --> 29:02.920
So, yeah, in this case,

29:02.920 --> 29:04.920
I can do the average.

29:04.920 --> 29:06.920
Distance, for example.

29:06.920 --> 29:08.920
And we are speaking

29:08.920 --> 29:10.920
about 170 million records.

29:10.920 --> 29:12.920
And it's still like, you know,

29:12.920 --> 29:14.920
execution is 50 milliseconds.

29:14.920 --> 29:16.920
So, that's the kind of thing.

29:16.920 --> 29:18.920
Time series databases, we are not the only one.

29:18.920 --> 29:20.920
We are the fastest.

29:20.920 --> 29:22.920
If you ask any other database provider,

29:22.920 --> 29:24.920
they will tell you they are the faster

29:24.920 --> 29:26.920
and they are right

29:26.920 --> 29:28.920
because it really depends on the query.

29:28.920 --> 29:30.920
We are super fast for time series queries.

29:30.920 --> 29:32.920
If you try to do other type of queries,

29:32.920 --> 29:34.920
we are not the fastest.

29:34.920 --> 29:36.920
But if you're just about time series,

29:36.920 --> 29:38.920
then we are super fast.

29:38.920 --> 29:40.920
Everything is optimized for that.

29:40.920 --> 29:42.920
Okay.

29:42.920 --> 29:44.920
But I don't have 1.6 billion records.

29:44.920 --> 29:46.920
And that might be it.

29:46.920 --> 29:48.920
But something I don't know if you ever consider

29:48.920 --> 29:50.920
that actually getting

29:50.920 --> 29:52.920
a billion records

29:52.920 --> 29:54.920
is easier than you think.

29:54.920 --> 29:56.920
If you have

29:56.920 --> 29:58.920
500 time series,

29:58.920 --> 30:00.920
maybe 500 cars or scooters

30:00.920 --> 30:02.920
or whatever, maybe 500 machines,

30:02.920 --> 30:04.920
500 users with a phone,

30:04.920 --> 30:06.920
send in a data point

30:06.920 --> 30:08.920
every second.

30:08.920 --> 30:10.920
So you have

30:10.920 --> 30:12.920
a data point per second.

30:12.920 --> 30:14.920
It doesn't sound like much, no?

30:14.920 --> 30:16.920
But then you say,

30:16.920 --> 30:18.920
how many seconds are in one day?

30:18.920 --> 30:20.920
How many seconds are in every week?

30:20.920 --> 30:22.920
How many seconds are in your typical month

30:22.920 --> 30:24.920
of 30.

30:24.920 --> 30:26.920
43, 7 days?

30:26.920 --> 30:28.920
Because some months are different to others.

30:28.920 --> 30:30.920
So this is the amount of seconds

30:30.920 --> 30:32.920
you have in every month

30:32.920 --> 30:34.920
times

30:34.920 --> 30:36.920
500.

30:36.920 --> 30:38.920
So you have 500 devices

30:38.920 --> 30:40.920
sending you a data point every second.

30:40.920 --> 30:42.920
You are going to get

30:42.920 --> 30:44.920
1.3 billion records

30:44.920 --> 30:46.920
in just one month.

30:46.920 --> 30:48.920
So we see users

30:48.920 --> 30:50.920
generating this amount of data

30:50.920 --> 30:52.920
every day or even multiple

30:52.920 --> 30:54.920
times this data every day.

30:54.920 --> 30:56.920
So what I'm trying to say here is that

30:56.920 --> 30:58.920
when you are working with streaming data,

30:58.920 --> 31:00.920
it's pretty simple to get to the point

31:00.920 --> 31:02.920
in which you can actually

31:02.920 --> 31:04.920
have a lot of data to process.

31:04.920 --> 31:06.920
So it's quite interesting to have

31:06.920 --> 31:08.920
some database that can help you with that.

31:08.920 --> 31:10.920
But enough about

31:10.920 --> 31:12.920
speaking about QSDB.

31:12.920 --> 31:14.920
I want to speak about all the things today.

31:14.920 --> 31:16.920
So I told you already

31:16.920 --> 31:18.920
you can

31:18.920 --> 31:20.920
get data directly

31:20.920 --> 31:22.920
to Kafka

31:22.920 --> 31:24.920
into QSDB.

31:24.920 --> 31:26.920
By the way, before I move from this,

31:26.920 --> 31:28.920
I have another notebook because

31:28.920 --> 31:30.920
I told you I wanted to speak about

31:30.920 --> 31:32.920
millions of units per second

31:32.920 --> 31:34.920
in just in just 30

31:34.920 --> 31:36.920
events every 10 seconds

31:36.920 --> 31:38.920
so I have another notebook

31:38.920 --> 31:40.920
which is not getting data from GitHub

31:40.920 --> 31:42.920
but it's generating synthetic data.

31:42.920 --> 31:44.920
So I'm going to show you

31:44.920 --> 31:46.920
another notebook to send events

31:46.920 --> 31:48.920
a bit faster. So I have another notebook

31:48.920 --> 31:50.920
which is generating fake data, IoT data.

31:50.920 --> 31:52.920
And it's going to be

31:52.920 --> 31:54.920
sending, I'm going to be sending

31:54.920 --> 31:56.920
for example, I don't know,

31:56.920 --> 31:58.920
10,000 events per second?

31:58.920 --> 32:00.920
10,000 per second?

32:00.920 --> 32:02.920
15,000 per second?

32:02.920 --> 32:04.920
15,000 per second, you think?

32:04.920 --> 32:06.920
15,000 events

32:06.920 --> 32:08.920
every 100 milliseconds.

32:08.920 --> 32:10.920
15,000 events every 100 milliseconds?

32:10.920 --> 32:12.920
Will you be happy with that?

32:12.920 --> 32:14.920
15,000 events every 100 milliseconds?

32:14.920 --> 32:16.920
So now anyway,

32:16.920 --> 32:18.920
I'm sending 15,000

32:18.920 --> 32:20.920
events every 100 milliseconds

32:20.920 --> 32:22.920
which is still not super fast.

32:22.920 --> 32:24.920
And I have a second dashboard, just for you

32:24.920 --> 32:26.920
to see that this kind of technologies

32:26.920 --> 32:28.920
all together, they really can help you

32:28.920 --> 32:30.920
work with data which is fast.

32:30.920 --> 32:32.920
So right now here,

32:32.920 --> 32:34.920
I'm going to show you.

32:34.920 --> 32:36.920
So we have a dashboard

32:36.920 --> 32:38.920
that's every 100 milliseconds

32:38.920 --> 32:40.920
it's refreshing.

32:40.920 --> 32:42.920
And this is how fast we are seeing

32:42.920 --> 32:44.920
data. So this is not, I mean,

32:44.920 --> 32:46.920
it's not super fast, but you get the idea, yeah?

32:46.920 --> 32:48.920
So Kafka

32:48.920 --> 32:50.920
can ingest literally

32:50.920 --> 32:52.920
hundreds of millions of you in a second.

32:52.920 --> 32:54.920
In QuestDB, we can ingest

32:54.920 --> 32:56.920
up to 4.2 million

32:56.920 --> 32:58.920
events per second.

32:58.920 --> 33:00.920
If you have 16 CPUs,

33:00.920 --> 33:02.920
12 16 CPUs, you can ingest

33:02.920 --> 33:04.920
up to 4.2 million

33:04.920 --> 33:06.920
events per second. But that's the idea.

33:06.920 --> 33:08.920
This already feels real time.

33:08.920 --> 33:10.920
This already feels like, you know,

33:10.920 --> 33:12.920
this already feels like a real time dashboard

33:12.920 --> 33:14.920
in which we really are having data which is moving fast.

33:14.920 --> 33:16.920
I wanted to show you this

33:16.920 --> 33:18.920
because otherwise it feels like cheating.

33:18.920 --> 33:20.920
So this is not real data, but if you

33:20.920 --> 33:22.920
have real data at a speed, you can see

33:22.920 --> 33:24.920
on the planes, you know, this is

33:24.920 --> 33:26.920
random data. So yeah, they are not

33:26.920 --> 33:28.920
crazy is the way it is. But other

33:28.920 --> 33:30.920
thing, I wanted to show you that it really

33:30.920 --> 33:32.920
supports this kind of thing.

33:32.920 --> 33:34.920
So let me

33:34.920 --> 33:36.920
just move on to the next part.

33:36.920 --> 33:38.920
So the next part is I told you

33:38.920 --> 33:40.920
that, okay, it's cool

33:40.920 --> 33:42.920
to send data. It's cool to be able

33:42.920 --> 33:44.920
to run SQL queries and analyze data.

33:44.920 --> 33:46.920
But I also want

33:46.920 --> 33:48.920
to have some way of doing that science.

33:48.920 --> 33:50.920
I want to have some way of predict the future.

33:50.920 --> 33:52.920
So for doing that science,

33:52.920 --> 33:54.920
there are many different tools.

33:54.920 --> 33:56.920
But you get a notebook which is

33:56.920 --> 33:58.920
where I'm running all the demo here

33:58.920 --> 34:00.920
today in the browser. It's

34:00.920 --> 34:02.920
a very popular way of doing interactive

34:02.920 --> 34:04.920
analytics, sorry, interactive

34:04.920 --> 34:06.920
exploration. And I created

34:06.920 --> 34:08.920
here

34:08.920 --> 34:10.920
a notebook

34:12.920 --> 34:14.920
in which I'm using two

34:14.920 --> 34:16.920
different tools, actually three different

34:16.920 --> 34:18.920
tools for doing data

34:18.920 --> 34:20.920
exploration. Even if you are not a

34:20.920 --> 34:22.920
Python person, a Python developer,

34:22.920 --> 34:24.920
you probably have heard about Python

34:24.920 --> 34:26.920
pandas, yeah? Maybe not,

34:26.920 --> 34:28.920
maybe yes. But Python pandas

34:28.920 --> 34:30.920
is one of the most popular libraries for that science.

34:32.920 --> 34:34.920
Some people say it's slow, the latest

34:34.920 --> 34:36.920
version is faster, until recently

34:36.920 --> 34:38.920
pandas wouldn't parallelize.

34:38.920 --> 34:40.920
So you have like a large dataset,

34:40.920 --> 34:42.920
you know, but it has, but

34:42.920 --> 34:44.920
we forgive pandas to be slow,

34:44.920 --> 34:46.920
because first it's not that slow. Second,

34:46.920 --> 34:48.920
it gives you a huge

34:48.920 --> 34:50.920
amount of things to do with your data.

34:50.920 --> 34:52.920
So it's probably the most popular tool

34:52.920 --> 34:54.920
for doing interactive data exploration.

34:54.920 --> 34:56.920
And this is the kind of thing you can do.

34:56.920 --> 34:58.920
The first thing of course will be connecting

34:58.920 --> 35:00.920
to where you have your data.

35:00.920 --> 35:02.920
In pandas you can connect to your data,

35:02.920 --> 35:04.920
I don't know, you could use just

35:04.920 --> 35:06.920
CSV or whatever, in my case,

35:06.920 --> 35:08.920
I have the data in QSDB.

35:08.920 --> 35:10.920
In QSDB we speak the process protocol

35:10.920 --> 35:12.920
for query in

35:12.920 --> 35:14.920
data. So I'm just going to be connecting

35:14.920 --> 35:16.920
to a database

35:16.920 --> 35:18.920
using the process protocol.

35:18.920 --> 35:20.920
And I say,

35:20.920 --> 35:22.920
I tell pandas, pandas,

35:22.920 --> 35:24.920
read this query,

35:24.920 --> 35:26.920
give me the results. Okay, nothing too

35:26.920 --> 35:28.920
interesting. Now, in pandas

35:28.920 --> 35:30.920
everything goes into what is called a data

35:30.920 --> 35:32.920
frame. And data frames have a lot of

35:32.920 --> 35:34.920
methods to do interesting things.

35:34.920 --> 35:36.920
So it's like also if you use

35:36.920 --> 35:38.920
Spark or other tools, you also use

35:38.920 --> 35:40.920
the data in abstraction. So basically

35:40.920 --> 35:42.920
give me the information.

35:42.920 --> 35:44.920
It gives me some, like you know,

35:44.920 --> 35:46.920
some basic data types and so on,

35:46.920 --> 35:48.920
not too bad.

35:48.920 --> 35:50.920
Describe the data set. And it's already

35:50.920 --> 35:52.920
interesting because if I don't have any

35:52.920 --> 35:54.920
idea about my data, this already tells

35:54.920 --> 35:56.920
me, hey, for the, I only

35:56.920 --> 35:58.920
have one numerical column and it's a

35:58.920 --> 36:00.920
time stamp with Santa Poc, so this is

36:00.920 --> 36:02.920
anticlimatic, but it will tell me

36:02.920 --> 36:04.920
which is the minimum value, maximum value,

36:04.920 --> 36:06.920
the percentiles. So I can get an idea

36:06.920 --> 36:08.920
of my data or I can say, hey,

36:08.920 --> 36:10.920
tell me for all the columns that

36:10.920 --> 36:12.920
are not a number, tell

36:12.920 --> 36:14.920
me how many, you know,

36:14.920 --> 36:16.920
how many different values

36:16.920 --> 36:18.920
I have for its different type of

36:18.920 --> 36:20.920
event. And I can see here

36:20.920 --> 36:22.920
that in this data set,

36:22.920 --> 36:24.920
I almost don't have

36:24.920 --> 36:26.920
comments on commits

36:26.920 --> 36:28.920
for the 2000 events I've seen

36:28.920 --> 36:30.920
so far, but most of the

36:30.920 --> 36:32.920
events are push events on GitHub, which

36:32.920 --> 36:34.920
makes sense. But if I

36:34.920 --> 36:36.920
try to do any type

36:36.920 --> 36:38.920
of forecasting, what I see already

36:38.920 --> 36:40.920
is like my data is super bias.

36:40.920 --> 36:42.920
My data is going to train

36:42.920 --> 36:44.920
pretty well on push events because

36:44.920 --> 36:46.920
they are very common, but I only

36:46.920 --> 36:48.920
have two events with commit. I

36:48.920 --> 36:50.920
cannot learn from two events.

36:50.920 --> 36:52.920
You see the idea, yeah? So that's kind of

36:52.920 --> 36:54.920
a thing. Without doing nothing, you

36:54.920 --> 36:56.920
start getting familiar with your data

36:56.920 --> 36:58.920
set. They're like, oh, I only have to say

36:58.920 --> 37:00.920
describe, tell me the things. And it's

37:00.920 --> 37:02.920
very powerful because, you know, you

37:02.920 --> 37:04.920
don't have to worry much about those

37:04.920 --> 37:06.920
things. It's like, oh, show me what it

37:06.920 --> 37:08.920
looks like. This is the set event

37:08.920 --> 37:10.920
that I want to see. And I say, well,

37:10.920 --> 37:12.920
I prefer to see the distribution for

37:12.920 --> 37:14.920
each event. So again, Pandas

37:14.920 --> 37:16.920
gives you very simple ways

37:16.920 --> 37:18.920
to grouping things together,

37:18.920 --> 37:20.920
representing things. But basically

37:20.920 --> 37:22.920
here, for its different type of event,

37:22.920 --> 37:24.920
I'm going to see the count of how many

37:24.920 --> 37:26.920
events per minute I'm seeing

37:26.920 --> 37:28.920
for each of the different types.

37:28.920 --> 37:30.920
And as you can see, like, you know,

37:30.920 --> 37:32.920
you don't have to, I don't want to enter

37:32.920 --> 37:34.920
in details of the code because actually

37:34.920 --> 37:36.920
it's super simple. But it's just for

37:36.920 --> 37:38.920
simple things. And you can do it

37:38.920 --> 37:40.920
very easily. And with no effort,

37:40.920 --> 37:42.920
you can get interesting statistics.

37:42.920 --> 37:44.920
Of course, if you go deeper,

37:44.920 --> 37:46.920
you get more interesting things.

37:46.920 --> 37:48.920
But that's kind of the

37:48.920 --> 37:50.920
kind of the idea. So Pandas is pretty

37:50.920 --> 37:52.920
cool. But, you know,

37:52.920 --> 37:54.920
people have no heart. It was like, oh,

37:54.920 --> 37:56.920
Pandas is slow. Let's do something

37:56.920 --> 37:58.920
new in Rust because, you know,

37:58.920 --> 38:00.920
Python is slow. Rust is faster, blah,

38:00.920 --> 38:02.920
blah, blah, whatever.

38:02.920 --> 38:04.920
You can go to the building there and

38:04.920 --> 38:06.920
you can see the difference between

38:06.920 --> 38:08.920
Python and Rust. I'm not going to.

38:08.920 --> 38:10.920
But basically, Polar is the new

38:10.920 --> 38:12.920
key to the block. It's the library

38:12.920 --> 38:14.920
which is making data science faster.

38:14.920 --> 38:16.920
In some Pandas, it's saying, oh, now

38:16.920 --> 38:18.920
we are going to be faster. So it's

38:18.920 --> 38:20.920
pretty cool because now they are both

38:20.920 --> 38:22.920
trying to compete. But, but online,

38:22.920 --> 38:24.920
many people are using now Polar, not

38:24.920 --> 38:26.920
Pandas. And also the name.

38:26.920 --> 38:28.920
And a Panda, and a Polar beer,

38:28.920 --> 38:30.920
and a koala, it's like whatever, man.

38:30.920 --> 38:32.920
It's like, yeah.

38:32.920 --> 38:34.920
Basically here with Polar, same thing.

38:34.920 --> 38:36.920
You connect to the database and

38:36.920 --> 38:38.920
you have, literally,

38:38.920 --> 38:40.920
the same things you can do.

38:40.920 --> 38:42.920
Give me the account of events,

38:42.920 --> 38:44.920
give me the different things, you get the idea.

38:44.920 --> 38:46.920
I also went with a different

38:46.920 --> 38:48.920
library facets that I

38:48.920 --> 38:50.920
like it for some things. Because

38:50.920 --> 38:52.920
in facets, with very little

38:52.920 --> 38:54.920
code, I can get,

38:54.920 --> 38:56.920
again, a lot of insight from my data.

38:56.920 --> 38:58.920
For example, here, I can just

38:58.920 --> 39:00.920
see, hey, from the data I have already,

39:00.920 --> 39:02.920
so me,

39:02.920 --> 39:04.920
it will, oh,

39:04.920 --> 39:06.920
it's, my, my Wi-Fi here is

39:06.920 --> 39:08.920
super slow. I don't know if you have

39:08.920 --> 39:10.920
the same issue, but my Wi-Fi is super

39:10.920 --> 39:12.920
slow. And this is, this is calling

39:12.920 --> 39:14.920
an external JavaScript.

39:14.920 --> 39:16.920
And since the Wi-Fi is unreliable,

39:16.920 --> 39:18.920
it's not displaying fast, which is

39:18.920 --> 39:20.920
what happened with it, more earlier. It's

39:20.920 --> 39:22.920
what happened also, 1.1.2 QSDB. So

39:22.920 --> 39:24.920
everything is running locally. But this

39:24.920 --> 39:26.920
particular script is using

39:26.920 --> 39:28.920
a script from our place.

39:28.920 --> 39:30.920
So I can, sadly, I cannot show it

39:30.920 --> 39:32.920
to you, but if I reload the notebook,

39:32.920 --> 39:34.920
I should have

39:34.920 --> 39:36.920
the cache version

39:36.920 --> 39:38.920
that I had from the previous

39:38.920 --> 39:40.920
execution. So this, this one

39:40.920 --> 39:42.920
is not live. It's the, a cool

39:42.920 --> 39:44.920
thing about the Python notebooks

39:44.920 --> 39:46.920
is that you can, you can

39:46.920 --> 39:48.920
save the notebook with

39:48.920 --> 39:50.920
the current execution results, and you can

39:50.920 --> 39:52.920
serve with your team. So this is what it looked

39:52.920 --> 39:54.920
like when I had the internet, okay? Sorry

39:54.920 --> 39:56.920
about the internet, but you know, I don't have a

39:56.920 --> 39:58.920
computer here, but basically you have

39:58.920 --> 40:00.920
interesting things. So that's for exploring

40:00.920 --> 40:02.920
the data, but I promise you that we wanted

40:02.920 --> 40:04.920
to predict the future.

40:04.920 --> 40:06.920
And we wanted to have, like, you know, to have

40:06.920 --> 40:08.920
forecasting on time series. And again,

40:08.920 --> 40:10.920
in Python, you

40:10.920 --> 40:12.920
have a lot of interesting

40:12.920 --> 40:14.920
ways of working with time series

40:14.920 --> 40:16.920
data. When you have data in time

40:16.920 --> 40:18.920
series, the logical step

40:18.920 --> 40:20.920
is trying to predict what's going to happen next.

40:20.920 --> 40:22.920
How many,

40:22.920 --> 40:24.920
how, how much

40:24.920 --> 40:26.920
stock do I need to have

40:26.920 --> 40:28.920
for people that are going to be buying

40:28.920 --> 40:30.920
my product? How much energy do I have

40:30.920 --> 40:32.920
to produce?

40:32.920 --> 40:34.920
What's going to be the price

40:34.920 --> 40:36.920
of the Bitcoin in two weeks from

40:36.920 --> 40:38.920
now? Those kind of things are

40:38.920 --> 40:40.920
interesting to predict. If you know that one,

40:40.920 --> 40:42.920
let me know. But those kind of things are

40:42.920 --> 40:44.920
interesting, okay? That's the Holy Grail

40:44.920 --> 40:46.920
time series prediction. And this, of course,

40:46.920 --> 40:48.920
is not trivial, but there are a number

40:48.920 --> 40:50.920
of algorithms, like profit,

40:50.920 --> 40:52.920
or Arima, Arima, or

40:52.920 --> 40:54.920
other algorithms that actually

40:54.920 --> 40:56.920
allows you to do time series

40:56.920 --> 40:58.920
forecasting. Better or worse,

40:58.920 --> 41:00.920
but they allow you to do that. So in this case,

41:00.920 --> 41:02.920
I'm going to be using

41:02.920 --> 41:04.920
a model which is called

41:04.920 --> 41:06.920
profit. It was originally

41:06.920 --> 41:08.920
developed by Facebook. And

41:08.920 --> 41:10.920
profit allows you to do time series

41:10.920 --> 41:12.920
forecasting in a very simple way.

41:12.920 --> 41:14.920
I connect to my database,

41:14.920 --> 41:16.920
and the next thing I'm going to do

41:16.920 --> 41:18.920
is I'm going to tell my database

41:18.920 --> 41:20.920
QSDB to select

41:20.920 --> 41:22.920
all the events

41:22.920 --> 41:24.920
sample per minute, as I saw you

41:24.920 --> 41:26.920
earlier. So basically here,

41:26.920 --> 41:28.920
what I have is the all the events per

41:28.920 --> 41:30.920
minute. I can see some minutes I have

41:30.920 --> 41:32.920
more events, some minutes I have fewer

41:32.920 --> 41:34.920
events, and this is going to be my training

41:34.920 --> 41:36.920
data. And this is all it takes

41:36.920 --> 41:38.920
to train a model.

41:38.920 --> 41:40.920
You don't need to have any fancy

41:40.920 --> 41:42.920
NVIDIA GPU.

41:42.920 --> 41:44.920
You don't have to, for this,

41:44.920 --> 41:46.920
I choose this one profit specifically

41:46.920 --> 41:48.920
and other than having this notebook

41:48.920 --> 41:50.920
because they are very, very, very lightweight.

41:50.920 --> 41:52.920
So you can predict

41:52.920 --> 41:54.920
also we don't have any data. We have only

41:54.920 --> 41:56.920
2000 events, which is super fast. But the

41:56.920 --> 41:58.920
model already learned, and with this I

41:58.920 --> 42:00.920
can already make predictions.

42:00.920 --> 42:02.920
Since I have data only for the past 20 minutes,

42:02.920 --> 42:04.920
what I'm going to try to predict

42:04.920 --> 42:06.920
is only the next 10 minutes of the future.

42:06.920 --> 42:08.920
Predicting 10 minutes into the future is not much.

42:08.920 --> 42:10.920
If I had more data in the

42:10.920 --> 42:12.920
dataset, I could predict the next week,

42:12.920 --> 42:14.920
the next month, whatever. But in this case,

42:14.920 --> 42:16.920
I'm going to predict the next 10 minutes.

42:16.920 --> 42:18.920
And this is what it looks like.

42:18.920 --> 42:20.920
It looks like

42:20.920 --> 42:22.920
we are seeing fewer and fewer

42:22.920 --> 42:24.920
commits over time.

42:24.920 --> 42:26.920
I can't predict the model. Something I

42:26.920 --> 42:28.920
can do.

42:28.920 --> 42:30.920
I can, for example, say

42:30.920 --> 42:32.920
so I'm sending data now

42:32.920 --> 42:34.920
for only one script.

42:34.920 --> 42:36.920
I can open, let me

42:36.920 --> 42:38.920
for a second, I can open

42:38.920 --> 42:40.920
this script and I can start

42:40.920 --> 42:42.920
sending events

42:42.920 --> 42:44.920
every two seconds

42:44.920 --> 42:46.920
instead of every 10 seconds.

42:46.920 --> 42:48.920
So if I

42:48.920 --> 42:50.920
execute now

42:58.920 --> 43:00.920
Yeah, it's

43:00.920 --> 43:02.920
the drawing here, I'm reading events from

43:02.920 --> 43:04.920
GitHub. Since Internet again

43:04.920 --> 43:06.920
is not working, I'm going to disconnect and

43:06.920 --> 43:08.920
reconnect again.

43:08.920 --> 43:10.920
I'm actually going to use my phone to

43:10.920 --> 43:12.920
tether.

43:12.920 --> 43:14.920
I'm going to use my phone

43:14.920 --> 43:16.920
just for a second.

43:16.920 --> 43:18.920
I can

43:18.920 --> 43:20.920
whistle.

43:20.920 --> 43:22.920
I'm trying to use my phone for

43:22.920 --> 43:24.920
the Internet connection.

43:24.920 --> 43:26.920
I can show you this.

43:26.920 --> 43:28.920
Wi-Fi hotspot.

43:28.920 --> 43:30.920
Don't connect to my hotspot now.

43:34.920 --> 43:36.920
Let's see if it opens.

43:36.920 --> 43:38.920
I can't see it.

43:42.920 --> 43:44.920
Okay, so that's my

43:44.920 --> 43:46.920
phone and hopefully let's see

43:46.920 --> 43:48.920
if this is better.

43:50.920 --> 43:52.920
No.

43:52.920 --> 43:54.920
Okay, my phone is also useless.

43:54.920 --> 43:56.920
So I don't have Internet.

43:56.920 --> 43:58.920
Basically what I was trying to do, I

43:58.920 --> 44:00.920
was trying to send more data.

44:02.920 --> 44:04.920
Yeah, that's okay.

44:04.920 --> 44:06.920
I just wanted to send more data.

44:06.920 --> 44:08.920
So you will see

44:08.920 --> 44:10.920
if we get more data.

44:10.920 --> 44:12.920
The prediction will be

44:12.920 --> 44:14.920
that it's going to be, you know, higher because

44:14.920 --> 44:16.920
that's how it works. But anyway,

44:16.920 --> 44:18.920
I have here another model,

44:18.920 --> 44:20.920
linear regression. You can use linear regression

44:20.920 --> 44:22.920
for predicting many things.

44:22.920 --> 44:24.920
In this case, I'm going to use linear regression

44:24.920 --> 44:26.920
for predicting

44:26.920 --> 44:28.920
also time series.

44:28.920 --> 44:30.920
And Senaedia. I trained the model

44:30.920 --> 44:32.920
and now I can predict the future.

44:32.920 --> 44:34.920
And as you can see,

44:34.920 --> 44:36.920
this model is also very pessimistic

44:36.920 --> 44:38.920
about the future. It says that

44:38.920 --> 44:40.920
we are not getting enough data.

44:40.920 --> 44:42.920
This is what we've been seeing and the prediction

44:42.920 --> 44:44.920
is going to go down.

44:44.920 --> 44:46.920
But that's cool. I mean, we have data.

44:46.920 --> 44:48.920
We have ways of predicting

44:48.920 --> 44:50.920
what's going to happen based on the past.

44:50.920 --> 44:52.920
Which is, I don't know if you thought

44:52.920 --> 44:54.920
it was this simple. These models are

44:54.920 --> 44:56.920
super simplistic, but they kind of give you

44:56.920 --> 44:58.920
an orientation, a trend, which is interesting.

44:58.920 --> 45:00.920
So we have that already.

45:00.920 --> 45:02.920
So next step,

45:02.920 --> 45:04.920
and almost running out of time,

45:04.920 --> 45:06.920
next step will be the

45:06.920 --> 45:08.920
real-time dashboard. I already

45:08.920 --> 45:10.920
showed you the dashboard. What I'm using

45:10.920 --> 45:12.920
here is called Grafana.

45:12.920 --> 45:14.920
And I have

45:14.920 --> 45:16.920
a couple of dashboards here.

45:16.920 --> 45:18.920
So Grafana, the way it works, Grafana

45:18.920 --> 45:20.920
is a tool for

45:20.920 --> 45:22.920
dashboard and alerts. And Grafana

45:22.920 --> 45:24.920
has a lot of plugins to connect to

45:24.920 --> 45:26.920
different data sources. In our case,

45:26.920 --> 45:28.920
I created a data source using the

45:28.920 --> 45:30.920
Postgres connector, because we are

45:30.920 --> 45:32.920
compatible with Postgres at the protocol

45:32.920 --> 45:34.920
level. So

45:34.920 --> 45:36.920
I'm connecting to my instance of

45:36.920 --> 45:38.920
QSDB in this docker container.

45:38.920 --> 45:40.920
And once you have a connection,

45:40.920 --> 45:42.920
you can just create dashboards

45:42.920 --> 45:44.920
and alerts on the dashboards.

45:44.920 --> 45:46.920
And the way I do this,

45:46.920 --> 45:48.920
if I go to any of these panels,

45:48.920 --> 45:50.920
the way you do this

45:50.920 --> 45:52.920
is just with SQL.

45:52.920 --> 45:54.920
So basically, each of these

45:54.920 --> 45:56.920
panels, they look very fancy

45:56.920 --> 45:58.920
by behind the scenes. It's just

45:58.920 --> 46:00.920
SQL query. It has

46:00.920 --> 46:02.920
some filters, for example, here.

46:02.920 --> 46:04.920
Here,

46:04.920 --> 46:06.920
this timestamp filter

46:06.920 --> 46:08.920
means that the

46:08.920 --> 46:10.920
filter is going to be the one

46:10.920 --> 46:12.920
that you have

46:12.920 --> 46:14.920
here selected on the dashboard.

46:14.920 --> 46:16.920
So you have some macros, so everything

46:16.920 --> 46:18.920
works together. But in the end,

46:18.920 --> 46:20.920
creating a dashboard in Grafana, just creating

46:20.920 --> 46:22.920
the SQL, putting the charge you want,

46:22.920 --> 46:24.920
choosing the color, choosing you want

46:24.920 --> 46:26.920
to have multiple series. If you want to have

46:26.920 --> 46:28.920
like, you know, how it's going to be the scale,

46:28.920 --> 46:30.920
but that's kind of the thing. That's all you need.

46:30.920 --> 46:32.920
The last component I had in my

46:32.920 --> 46:34.920
template was the

46:34.920 --> 46:36.920
monitoring. So monitoring,

46:36.920 --> 46:38.920
what you usually do

46:38.920 --> 46:40.920
for monitoring, you need to have some kind of server

46:40.920 --> 46:42.920
agent. If you're in Kubernetes, you have

46:42.920 --> 46:44.920
your own agents. In my case,

46:44.920 --> 46:46.920
I'm not running Kubernetes or anything.

46:46.920 --> 46:48.920
So I'm running an agent which is

46:48.920 --> 46:50.920
called Telegraph. It was created by InfluxDB.

46:50.920 --> 46:52.920
And Telegraph allows you

46:52.920 --> 46:54.920
to connect to many

46:54.920 --> 46:56.920
source of metrics.

46:56.920 --> 46:58.920
It allows you to connect to many destinations

46:58.920 --> 47:00.920
for storing the data. One of the destinations

47:00.920 --> 47:02.920
is QSDB. We are at a serious database,

47:02.920 --> 47:04.920
so it's pretty cool for monitoring.

47:04.920 --> 47:06.920
So I'm getting the data

47:06.920 --> 47:08.920
from QSDB and from Kafka monitoring metrics.

47:08.920 --> 47:10.920
The Telegraph agent

47:10.920 --> 47:12.920
is collecting the metrics. In the

47:12.920 --> 47:14.920
agent itself, I'm doing some transformation

47:14.920 --> 47:16.920
and then I'm writing the data back into

47:16.920 --> 47:18.920
my database.

47:18.920 --> 47:20.920
So for example, if I go

47:20.920 --> 47:22.920
to my QSDB installation,

47:22.920 --> 47:24.920
this is what

47:24.920 --> 47:26.920
my metrics look like. So in QSDB,

47:26.920 --> 47:28.920
we're exposing these metrics.

47:28.920 --> 47:30.920
How many items

47:30.920 --> 47:32.920
I'm missing from the cache, number

47:32.920 --> 47:34.920
of queries, number of commits

47:34.920 --> 47:36.920
in the file system,

47:36.920 --> 47:38.920
all those things. In Kafka,

47:38.920 --> 47:40.920
we also have a lot of different metrics that

47:40.920 --> 47:42.920
we can explore. So Telegraph is

47:42.920 --> 47:44.920
pulling all those metrics and we are sending

47:44.920 --> 47:46.920
them back to QSDB. So if I go

47:46.920 --> 47:48.920
to my local instance

47:48.920 --> 47:50.920
here,

47:50.920 --> 47:52.920
you will see I have here a few tables.

47:52.920 --> 47:54.920
Kafka cluster,

47:54.920 --> 47:56.920
Kafka runtime.

47:56.920 --> 47:58.920
If I do, for example, I want to

47:58.920 --> 48:00.920
select all the data from

48:00.920 --> 48:02.920
Kafka

48:02.920 --> 48:04.920
Java runtime,

48:04.920 --> 48:06.920
you see within

48:06.920 --> 48:08.920
collecting data,

48:08.920 --> 48:10.920
this actually is quite small.

48:10.920 --> 48:12.920
If I go to Kafka topic, hopefully in this

48:12.920 --> 48:14.920
one, I should have more activity.

48:14.920 --> 48:16.920
So, yeah, for the time that I've been

48:16.920 --> 48:18.920
writing this demo, we've been

48:18.920 --> 48:20.920
collecting data about the different topics,

48:20.920 --> 48:22.920
how many effects,

48:22.920 --> 48:24.920
how many bytes

48:24.920 --> 48:26.920
per second we were sending

48:26.920 --> 48:28.920
out of the topic, how much

48:28.920 --> 48:30.920
data was entering. You can see

48:30.920 --> 48:32.920
the offset in which

48:32.920 --> 48:34.920
different consumers are reading data.

48:34.920 --> 48:36.920
So every kind of statistics and that

48:36.920 --> 48:38.920
monitoring, we have on the database.

48:38.920 --> 48:40.920
So you could also build your dashboard.

48:40.920 --> 48:42.920
That's kind of the idea.

48:42.920 --> 48:44.920
So,

48:44.920 --> 48:46.920
what I wanted to tell you today

48:46.920 --> 48:48.920
is that if you want

48:48.920 --> 48:50.920
to work with streaming data,

48:50.920 --> 48:52.920
it's hard because streaming data

48:52.920 --> 48:54.920
can get very big. It never stops

48:54.920 --> 48:56.920
or at some point you need to decide when to do

48:56.920 --> 48:58.920
analytics. Isbasti is going

48:58.920 --> 49:00.920
to be sometimes faster, sometimes slower.

49:00.920 --> 49:02.920
At some times it will get late.

49:02.920 --> 49:04.920
At some times you will get new data

49:04.920 --> 49:06.920
with an update for some results

49:06.920 --> 49:08.920
that you already made it. At some point,

49:08.920 --> 49:10.920
the individual points

49:10.920 --> 49:12.920
that you are gathering, you are collecting

49:12.920 --> 49:14.920
that are very valuable for the

49:14.920 --> 49:16.920
immediate analytics. They lose value

49:16.920 --> 49:18.920
but the aggregate data gets more

49:18.920 --> 49:20.920
interesting. So working with this kind of

49:20.920 --> 49:22.920
data, it can be hard, but if you have the right

49:22.920 --> 49:24.920
tools, you can actually start working

49:24.920 --> 49:26.920
with streaming data at

49:26.920 --> 49:28.920
pretty much any speed in

49:28.920 --> 49:30.920
a simple enough way.

49:30.920 --> 49:32.920
At least for starting. Then of course,

49:32.920 --> 49:34.920
everything gets harder.

49:34.920 --> 49:36.920
So, I didn't

49:36.920 --> 49:38.920
tell you many, many things that I don't have

49:38.920 --> 49:40.920
yet. So, I'm going to talk about

49:40.920 --> 49:42.920
the template. The template is a starting

49:42.920 --> 49:44.920
point. So you can start

49:44.920 --> 49:46.920
getting familiar with streaming data

49:46.920 --> 49:48.920
and have some tools that are interesting.

49:48.920 --> 49:50.920
But if you want to go to production,

49:50.920 --> 49:52.920
you will need to support more data

49:52.920 --> 49:54.920
formats. Not that JSON. It's not very

49:54.920 --> 49:56.920
efficient for moving fast data.

49:56.920 --> 49:58.920
You want to have data life cycle

49:58.920 --> 50:00.920
policies. At which point, I'm going

50:00.920 --> 50:02.920
to delete the data. At which point,

50:02.920 --> 50:04.920
if I'm getting data every few milliseconds,

50:04.920 --> 50:06.920
maybe I want to aggregate into data

50:06.920 --> 50:08.920
for a few years, one month, I don't know.

50:08.920 --> 50:10.920
I want to delete the data, I don't know.

50:10.920 --> 50:12.920
So you have to define, maybe you want

50:12.920 --> 50:14.920
to move the data to some cheap

50:14.920 --> 50:16.920
cold storage. So those kind of things you have

50:16.920 --> 50:18.920
to decide. Data quality,

50:18.920 --> 50:20.920
data governance, replication.

50:20.920 --> 50:22.920
In this case, each component

50:22.920 --> 50:24.920
has only one server running.

50:24.920 --> 50:26.920
It's easy to add more replicas for

50:26.920 --> 50:28.920
each of the components, but you have to do it.

50:28.920 --> 50:30.920
So there are many things I didn't cover.

50:30.920 --> 50:32.920
But hopefully, this was interesting for you.

50:32.920 --> 50:34.920
There are the links to all

50:34.920 --> 50:36.920
the tools that I've been using in the template.

50:36.920 --> 50:38.920
The template itself is a patched

50:38.920 --> 50:40.920
to zero. So feel free to do

50:40.920 --> 50:42.920
anything you want with that.

50:42.920 --> 50:44.920
For anything you want, you can contact

50:44.920 --> 50:46.920
me on MasterDone,

50:46.920 --> 50:48.920
but actually, I will not

50:48.920 --> 50:50.920
with it. So contact me on Twitter,

50:50.920 --> 50:52.920
which is easier. Thank you very much.

50:52.920 --> 50:54.920
Anything you want, I hear.

50:54.920 --> 50:56.920
APPLAUSE

