WEBVTT

00:00.000 --> 00:06.240
You can't hear me at the back.

00:06.240 --> 00:10.080
Do not hesitate to ask for me to speak louder.

00:10.080 --> 00:17.240
So I'm Alex and I'm a co-founder of the DeFloor Association, which is a non-profit self-hosting

00:17.240 --> 00:18.240
collective.

00:18.240 --> 00:21.160
We're a member of the Chateau Network in France.

00:21.160 --> 00:26.000
And so what that means is we're doing self-hosting and we're trying to promote self-hosting as

00:26.000 --> 00:34.960
an alternative to putting everything in large data centers and relying on cloud providers.

00:34.960 --> 00:40.320
The thing is actually doing this is relatively hard and mostly it's hard because we want

00:40.320 --> 00:44.600
systems that are reliable, which are available most of the time.

00:44.600 --> 00:48.720
And if you have computers at home, you have a lot of issues.

00:48.720 --> 00:53.720
In particular, the computers we're using at DeFloor are these kind of computers, very

00:53.720 --> 00:55.720
cheap old desktop computers.

00:55.720 --> 01:00.880
They're not meant to be servers and we expect that they could crash at any time.

01:00.880 --> 01:04.200
These are some other examples that we had and those are still used actually.

01:04.200 --> 01:08.160
So these are also old desktop computers and we have some system which is based on only

01:08.160 --> 01:10.520
these kinds of machines.

01:10.520 --> 01:12.400
So they can die.

01:12.400 --> 01:17.040
We also have issues possibly with the internet where the electricity connection because we're

01:17.040 --> 01:19.520
at home so we don't have redundancy.

01:19.520 --> 01:21.880
It can go at any time.

01:21.880 --> 01:28.200
And to alleviate these issues, what we do is that we do distributed systems and we have

01:28.200 --> 01:31.400
a multi-site geo-replicated cluster.

01:31.400 --> 01:35.560
And so in our case, the DeFloor cluster is in three places.

01:35.560 --> 01:39.760
There's some nodes in Brussels here, some nodes in Lille and some nodes in Paris.

01:39.760 --> 01:44.440
And basically the aim is to build a system that makes use of some cheap hardware which

01:44.440 --> 01:49.440
is disseminated in all of these locations and they can basically relay one another when

01:49.440 --> 01:53.720
there's an issue somewhere and the whole thing stays up even if there are issues in

01:53.720 --> 01:55.920
individual locations.

01:55.920 --> 01:59.840
And so this is one of the reasons why I call this a low-tech platform because we're using

01:59.840 --> 02:06.880
what we have at hand, cheap machines and regular internet connections.

02:06.880 --> 02:11.680
One of the main components in this platform is object storage.

02:11.680 --> 02:16.840
And so I will not enter too much into why object storage except that it's very adapted

02:16.840 --> 02:22.520
to flexible deployments which are kind of inspired by what is done in the cloud.

02:22.520 --> 02:29.800
And indeed, Amazon S3 was created as a cloud product and in 2006 was introduced.

02:29.800 --> 02:34.320
And it became since then a de facto standard and many applications are compatible with

02:34.320 --> 02:35.840
this object storage.

02:35.840 --> 02:39.960
And so it makes sense to base our infrastructure on this kind of software because we can just

02:39.960 --> 02:44.440
like plug and play all kinds of various things which are already able to use this kind of

02:44.440 --> 02:48.880
storage layer as a backend.

02:48.880 --> 02:52.600
There were many actually alternative implementations of S3.

02:52.600 --> 02:53.960
MENU is one of the most common ones.

02:53.960 --> 02:57.680
I think CEPH is also an implementation.

02:57.680 --> 03:02.800
What we discovered is actually that these implementations are not very well suited to

03:02.800 --> 03:04.080
geo-distributed deployments.

03:04.080 --> 03:10.240
So deployments where nodes are in remote locations because in such case you will have higher latency

03:10.240 --> 03:16.720
between the nodes and it can cause issues and the system is basically a bit slower.

03:16.720 --> 03:19.480
And sometimes it's even really unusable.

03:19.480 --> 03:23.520
So Garage was made specifically for this use case.

03:23.520 --> 03:28.520
We make use of distributed systems theory, CRDT in particular, which I will talk about

03:28.520 --> 03:29.600
later.

03:29.600 --> 03:35.400
And this is basically the aim is to provide a drop-in replacement for Amazon S3 or S3-compatible

03:35.400 --> 03:41.560
storage systems which is available, possible to run directly on this kind of geo-distributed

03:41.560 --> 03:46.960
cluster and the data will be replicated at several locations and it's kind of transparent

03:46.960 --> 03:51.680
and it's supposed to be reasonably fast, not completely slow down all the replications

03:51.680 --> 03:55.200
which are running on it.

03:55.200 --> 04:01.160
One of the main ways we were able to achieve this outcome was to use CRDT and weak consistency.

04:01.160 --> 04:07.080
So this is a bit theoretical explanation of what is going on in Garage and I will have

04:07.080 --> 04:09.080
another slide talking about this later.

04:09.080 --> 04:14.640
But basically we're trying to avoid so-called consensus algorithms like RAF or PAXOS because

04:14.640 --> 04:19.000
these algorithms have issues and are actually very sensitive to latency.

04:19.000 --> 04:24.360
But just to list the issues in a clear way, the first of them is software complexity.

04:24.360 --> 04:29.280
I think RAF is actually a complex piece of software and it can be implemented badly and

04:29.320 --> 04:34.360
if you do it wrong it can lead to various unacceptable outcomes.

04:34.360 --> 04:40.240
And of course the issue of performance which I've talked about already.

04:40.240 --> 04:44.160
Those algorithms like RAF are using a leader so the leader is becoming a bottleneck for

04:44.160 --> 04:47.080
most requests in the system.

04:47.080 --> 04:51.320
So if you cannot really scale if you have a naive strategy with just one leader in the

04:51.320 --> 04:57.920
system it's also sensitive to higher latency because if the leader happens to be a node

04:57.920 --> 05:03.360
in a very far away location, well everything has to transit from there and then come back.

05:03.360 --> 05:07.040
And so if the leader happens to be the wrong node everything is going to be much slower

05:07.040 --> 05:08.760
in the system.

05:08.760 --> 05:12.760
And also if the system is disrupted and the leader goes down the system will have to take

05:12.760 --> 05:17.240
some time to reconverge and it's actually something that can take a long time especially

05:17.240 --> 05:22.960
if the latency between nodes is high and those are not able to communicate very efficiently.

05:22.960 --> 05:27.400
And so for this reason we made Garage a completely different design which is based entirely on

05:27.400 --> 05:34.520
CRDT internally which kind of solves most of these issues.

05:34.520 --> 05:40.040
Object storage is very likely very similar to basic key value store except that the values

05:40.040 --> 05:44.480
are objects like big blobs of data.

05:44.480 --> 05:49.080
And so here we have an example where we have the key which is there's no notion of a file

05:49.080 --> 05:54.200
system hierarchy so we will just have the entire path in the key with the slash it doesn't

05:54.200 --> 05:56.200
have any specific meaning.

05:56.200 --> 06:00.680
And the value is like some metadata here it's inspired from the HTTP headers because it's

06:00.680 --> 06:05.880
very strongly based on the HTTP semantics and then you have the binary data for each

06:05.880 --> 06:07.960
of your files.

06:07.960 --> 06:12.480
It happens that this semantics key value actually maps very well to CRDT and this is why we

06:12.480 --> 06:15.920
were able to make this work.

06:15.920 --> 06:19.760
So just to convince you in one slide that this is actually a worthwhile trade off this

06:19.760 --> 06:25.160
is one of the best results we have for Garage and it's a performance comparison for Garage

06:25.160 --> 06:27.040
versus Migno.

06:27.040 --> 06:31.200
So it's a simulated deployment where we have nodes which are simulated on a single machine

06:31.200 --> 06:34.720
and we add some artificial latency between the nodes.

06:34.720 --> 06:41.760
And so here we have nodes with 50 milliseconds so pretty long delay between them and so basically

06:41.760 --> 06:47.640
we can see that they take some duration which is a multiple of the round trip time latency

06:47.840 --> 06:52.040
but for Migno it's a very high multiple so some very common requests like remove object

06:52.040 --> 06:56.640
or put object will take like more than one second and for Garage we were able to bring

06:56.640 --> 07:01.400
this down to somewhere between 300, 500 milliseconds.

07:01.400 --> 07:05.480
So quite an improvement.

07:05.480 --> 07:12.720
So the main focus of this talk is to basically discuss recent developments in Garage because

07:12.720 --> 07:16.920
so we were here at Fosdame two years ago and I think maybe lots of people in the room

07:16.920 --> 07:19.760
are already aware of Garage.

07:19.760 --> 07:26.640
So yeah two years ago we were at the beginning of a grant by NGI Pointer and which was the

07:26.640 --> 07:33.360
first grant and it allowed us to bring this version 0.6.0 which was the first like public

07:33.360 --> 07:36.640
beta version that we launched.

07:36.640 --> 07:41.040
So it was like a point where we considered that we had some basic feature which was pretty

07:41.040 --> 07:45.320
good actually and we could ask people to come and actually many people were interested

07:45.560 --> 07:49.000
and this is the point where we started also to have some external contributions to the

07:49.000 --> 07:51.440
project.

07:51.440 --> 07:54.720
So we did Fosdame about at the time.

07:54.720 --> 08:05.080
In April we did version 0.7 and so version 0.7 was so focused mostly on observability

08:05.080 --> 08:08.200
and integration with the ecosystem.

08:08.200 --> 08:13.240
So we added support for metrics and traces using OpenTelemetry which is a standard for

08:13.280 --> 08:16.240
exporting observability data.

08:16.240 --> 08:21.680
We also added some flexibility because while we had originally built the system like supposed

08:21.680 --> 08:25.880
to have three copies of everything so we would expect to have nodes in three different data

08:25.880 --> 08:30.720
centers actually people were also willing to use the system with less copies so we added

08:30.720 --> 08:37.040
one or two copies and we also had some weaker consistency which was useful to like make the

08:37.040 --> 08:41.040
system faster or help recover data in some scenarios.

08:41.040 --> 08:46.000
We also added integration with Kubernetes for discovery of nodes so that the cluster

08:46.000 --> 08:52.200
is able to set up automatically the links between each nodes and we also added an administration

08:52.200 --> 08:55.600
API which is useful to like set up the cluster.

08:55.600 --> 09:01.360
It's basically a very simple REST API where you can create buckets which are stored spaces,

09:01.360 --> 09:06.200
create access keys, give rights to access keys, etc.

09:06.200 --> 09:09.200
I will just show a little bit about the monitoring part.

09:09.200 --> 09:14.720
So this is a graphnet dashboard that we made for Garage and as you can see it's actually

09:14.720 --> 09:16.440
pretty complete.

09:16.440 --> 09:21.480
We can monitor so here is the request going on through the S3 API endpoints and here's

09:21.480 --> 09:26.160
the request going through the web endpoints because Garage supports serving buckets directly

09:26.160 --> 09:32.080
as websites which is something we make heavy use of at Le Fleur.

09:32.080 --> 09:36.720
Here we have the error rate and more interestingly here we have some internal metrics so like

09:36.760 --> 09:42.640
this is the data which is being read and written to the disk on the nodes.

09:42.640 --> 09:47.800
This is some internal metrics for the communications between nodes RPC and these are some queues

09:47.800 --> 09:55.040
so how much data is remaining to be processed and so yeah just quick note here the GCQ is

09:55.040 --> 09:58.760
common points where people are like why is this queue not going to zero?

09:58.760 --> 10:02.120
It's normal that it's not going to zero because items are staying in the queue for 24 hours

10:02.120 --> 10:06.040
before they're processed just for information.

10:06.040 --> 10:10.800
So basically this queue should almost be to zero and this one too and if it's not then

10:10.800 --> 10:14.160
probably your system is under too much load.

10:14.160 --> 10:18.800
And we also have tracing so if you want to go further into like how Garage is handling

10:18.800 --> 10:21.960
a request you can use this feature.

10:21.960 --> 10:26.920
So here we're exporting traces to Yeager and this is a trace of a pretty standard list

10:26.920 --> 10:32.400
objects API call and so we can see that the list objects is first reading some data to

10:32.400 --> 10:36.080
get some access information on the access key and the buckets.

10:36.080 --> 10:40.160
So this is some very fast call because all this information is copied on the node and

10:40.160 --> 10:45.120
it can just read it locally and then it's going to do some actual requesting on remote

10:45.120 --> 10:49.640
nodes for the list of objects that should return and we see here that it's sending a

10:49.640 --> 10:55.200
request to two nodes and the request is taking a bit of time before it completes and then

10:55.200 --> 11:00.720
so yeah I think this is a pretty slow cluster and it's taking 100 milliseconds but on faster

11:00.720 --> 11:05.120
hardware it can be of course much faster.

11:05.120 --> 11:13.960
So this was 0.7 and then we did 0.8 so that was at the end of the NGI pointer grant and

11:13.960 --> 11:22.560
for 0.8 we had a pretty high focus on making the performance better.

11:22.560 --> 11:27.120
So first thing we did was like change the metadata engine because we were using sled

11:27.120 --> 11:31.720
and it had a lot of issues I'll talk about that and we did some various performance improvements

11:31.720 --> 11:40.360
across the board making basically some pretty good improvements in this kind of area and

11:40.360 --> 11:44.240
in terms of features we added CODAS so this is not a feature from Amazon but it's a feature

11:44.240 --> 11:48.880
which you can add on Garage is like limit the size of a bucket to a maximum size of

11:48.880 --> 11:53.240
objects or maximum number of objects and it's pretty useful in a multi-tonnets setup where

11:53.360 --> 12:00.280
we'd like to lend some storage space to someone but have them restrain to some fixed capacity

12:00.280 --> 12:06.280
and of course some regular developments on quality of life improvements etc.

12:06.280 --> 12:10.480
So yeah just to talk a little bit about the metadata engine so we were using sled which

12:10.480 --> 12:16.480
is a metadata key value store embedded key value store which is written in Rust so we

12:16.480 --> 12:20.000
thought yeah it's written in Rust it's pretty good Garage is also written in Rust so let's

12:20.040 --> 12:26.280
just integrate them and at the point when we started Garage sled was like one of the most

12:26.280 --> 12:32.960
popular key value stores for Rust but actually it's not very well maintained anymore and it had

12:32.960 --> 12:38.960
many issues so it was making very large files on disk because it was like just writing and writing

12:38.960 --> 12:43.120
and writing and probably it was some internal way to optimize performance but it was not very

12:43.120 --> 12:49.440
satisfactory for us to have like data files that were 10 times too big. The performance was also

12:49.880 --> 12:56.160
pretty unpredictable on spinning hard drives it was actually very bad and also from a developer

12:56.160 --> 13:01.960
perspective it has some API limitations and this has prevented us from implementing some specific

13:01.960 --> 13:08.720
features in Garage and hopefully when we get rid of sled we can actually do that. So as an

13:08.720 --> 13:15.400
alternative we added LMDB so LMDB is a key value storage which is used I think in OpenLDAP

13:15.440 --> 13:20.600
and some other software and it's a pretty established piece of software at this point so we

13:20.600 --> 13:25.720
consider it pretty stable it has good performance and it maintains a reasonable size of files on

13:25.720 --> 13:33.320
disk so this is the default now and we also have SQLite as a second choice originally we had not

13:33.320 --> 13:38.840
optimized SQLite that much so it was not recommended we had not made another bunch of tests but

13:38.880 --> 13:46.040
probably now it's okay to use as well and just to show some comparison we did some benchmarks

13:46.040 --> 13:52.320
and basically LMDB is much faster pretty much twice as fast as sled not really twice but

13:52.320 --> 13:58.240
actually significantly faster and for all these common API endpoints and SQLite was not optimized

13:58.240 --> 14:07.160
at that time I cannot I do not have the data updated for now. Another optimization we made is

14:07.520 --> 14:14.120
block streaming so the idea here is that Garage will store your data so when it receives an object

14:14.120 --> 14:20.080
it will split the object into pieces of by default 1 megabyte and then store these pieces on data

14:20.080 --> 14:27.400
servers all around the cluster and then when you want to read the data well your API request is

14:27.400 --> 14:32.800
going to go through some Garage node which is going to receive the request is going to look at

14:33.120 --> 14:37.440
the objects the metadata and determine okay we have to get this part this part this part from

14:37.440 --> 14:43.120
these different nodes in the cluster so it's going to do an internal RPC request to the storage

14:43.120 --> 14:50.560
node which has the actual 1 megabyte data block and so this is how it was working before basically

14:50.560 --> 14:55.800
this first node that was receiving the API request it would like just read the 1 megabyte into RAM

14:55.800 --> 15:01.040
and not send anything to the client before so basically here the client is just waiting for

15:01.280 --> 15:05.520
the data to arrive and the data is being transferred here between these two nodes between inside the

15:05.520 --> 15:09.680
cluster and so basically the client is just waiting for some stuff to happen inside the cluster

15:09.680 --> 15:15.280
where it could just have received some data earlier and so the optimization we made was

15:15.280 --> 15:20.280
actually pretty simple but it's pretty big change in the code it was to start sending the data as

15:20.280 --> 15:25.160
soon as it arrives to this intermediate node and so here we just have a small buffer of data which

15:25.200 --> 15:33.840
is received and waiting to be sent back to the client and so by doing this pretty small change

15:33.840 --> 15:40.040
we actually managed to reduce the time to first byte measurement so this measurement is when you

15:40.040 --> 15:46.560
do a request to Garage to receive to get an object you will specify the path of the object send

15:46.560 --> 15:51.560
your HTTP request all the headers etc and then you will wait for the server to reply the server

15:51.600 --> 15:56.320
will give you some headers saying okay the object is coming and then he will start streaming some

15:56.320 --> 16:01.120
data and so this measures the time between you the point where you start sending a request and the

16:01.120 --> 16:08.280
moment where the first actual bytes of the data file are coming back and here we are in a actually

16:08.280 --> 16:13.560
again it's a simulated deployment but we have pretty slow networking so 5 megabits per second so

16:13.560 --> 16:19.120
it's actually very slow and so before the optimization garage was here so we would have to

16:19.320 --> 16:24.240
wait pretty much two seconds before some data was coming because the like a one megabyte file was

16:24.240 --> 16:29.400
being transferred out this very slow connection before it could be returned. Minio has some average

16:29.400 --> 16:34.280
performance here and with the optimization garage is very fast and we're able to return the first

16:34.280 --> 16:39.520
bytes of the data and so this is important because for instance for websites you want to display the

16:39.520 --> 16:44.320
content as fast as possible and even if it's a big file then maybe the first bytes are very relevant

16:44.320 --> 16:49.880
so for an image you can have a preview in the first bytes for an HTML file we can have pretty much

16:49.880 --> 16:56.480
everything and so minimizing this time is very critical to user experience. So I think we pretty

16:56.480 --> 17:01.560
much managed to do this and we also did some other various improvements on the code pass and

17:01.560 --> 17:11.280
garage so on the bottom we have 0.7 then we have 0.8 beta 1 beta 2 here we removed some F-sync and

17:11.320 --> 17:17.480
it's completely optional to have F-sync and we're almost matching so here is like raw throughput

17:17.480 --> 17:22.120
when you're reading and writing big objects continuously to garage the throughput is still

17:22.120 --> 17:28.240
a bit worse than Minio but it's actually getting pretty close so there's still room for improvement

17:28.240 --> 17:33.920
in this domain and it's yeah we haven't done much more work on this but it's definitely something

17:33.920 --> 17:42.640
that could still be optimized I believe. So then it was the end of the NGI pointer grants so we did

17:42.640 --> 17:47.880
a bunch of conferences in France this was not me this was other people from Duffleur and then we

17:47.880 --> 17:59.480
started another grant by NGI 0 through NLNet and this led to the release of 0.9 and so 0.9 was

17:59.480 --> 18:07.800
actually a pretty big release so yeah we had a support for multiple AGDs per node and this is

18:07.800 --> 18:14.160
actually a pretty big feature because now you can have one garage node which is directly talking to

18:14.160 --> 18:18.840
the hard drive and you don't have to do some pooling at the file system level or some RAID system

18:18.840 --> 18:25.520
basically you will just format each of your drives independently as a file system and each of them

18:25.560 --> 18:30.080
has a directory, a mount point and garage will just use all of these mount points and like share

18:30.080 --> 18:35.880
the data between the drives. This is probably the model which allows for the best performance on

18:35.880 --> 18:42.640
the server with multiple drives. We also added some features for S3 compatibility so we added

18:42.640 --> 18:49.360
support for basic lifecycle and lifecycle is a feature where it allows you to clean basically

18:49.360 --> 18:54.160
some stuff which is going on in the bucket and so for instance in S3 you can start uploading

18:54.200 --> 18:59.320
an object using a multi-part upload so multi-part upload means you're initiating the upload at one

18:59.320 --> 19:04.600
point and then you're going to do individual requests to add pieces of the file and then

19:04.600 --> 19:09.560
once you're finished you do a complete request and then the files get uploaded that gets stored

19:09.560 --> 19:14.760
completely in the system and so it could happen that these multi-parts upload they get

19:14.760 --> 19:19.920
aborted in the middle you never get to finish the the the requests and in this case there's

19:19.960 --> 19:24.960
some data that's lying around in the cluster and so if you configure a lifecycle using this is a

19:24.960 --> 19:29.880
very standard S3 API if you support if you configure a lifecycle in your brackets you can

19:29.880 --> 19:34.040
basically get rid of all this tail data after say a delay of one day or something like that.

19:34.040 --> 19:42.120
And another thing we added for S3 compatibility is retries of multi-parts upload and this was

19:42.120 --> 19:47.640
actually because in S3 if you fail a part you can because maybe your network was broken you can

19:47.680 --> 19:52.520
try again this part and you can still complete your multi-part upload and in the first versions of

19:52.520 --> 19:56.320
garage we did not have that and you would have to restart the upload from the beginning now you

19:56.320 --> 20:03.320
can resume only a single part. LMDB is now by default we're deprecating SLED and we have this

20:03.320 --> 20:09.640
new layout computation algorithm which I will talk a little bit about. So as I said garage is meant

20:09.640 --> 20:14.920
to work on geo-distributed clusters so you have nodes which are in different geographical locations

20:14.920 --> 20:20.880
we call them zones in garage so here we have three different zones and the data is going to be

20:20.880 --> 20:26.120
replicated and each file has to be on different zones for optimal redundancy. So here is an

20:26.120 --> 20:30.880
illustration if we have five zones for example the blue file will be in Belgium France and

20:30.880 --> 20:35.160
Switzerland so in three different places and the red file will also be in three different places

20:35.160 --> 20:42.120
not necessarily the same here it's UK France and Germany. And the idea is that we do this using this

20:42.160 --> 20:47.000
kind of pre-computed layout which is a table which will say okay the cluster the data in the

20:47.000 --> 20:52.600
cluster is divided in 256 parts and each of these parts is assigned to a fixed set of three servers

20:52.600 --> 20:58.480
and for each part we have to decide so three servers which are in different places in the cluster

20:58.480 --> 21:05.280
and we have to also balance the quantity of data that is going to go on each server. So basically

21:06.160 --> 21:12.800
for 0.9 we added an algorithm which is able to do this in an optimal fashion so basically this

21:12.800 --> 21:16.480
table is computed once when you set up the cluster or when you add some new nodes and then it's

21:16.480 --> 21:20.560
propagated to everybody and everybody then knows this table and knows where to look for the data.

21:22.320 --> 21:26.480
We actually published a paper if you're interested in the details of the algorithms that we use.

21:26.480 --> 21:37.040
Okay so that was 0.9 and then we went on and worked on 0.10 and 0.10 is actually a beta version and I

21:37.040 --> 21:42.720
think we will not have a stable 0.10 because it's not worth it to like update to 0.10 and then update

21:42.720 --> 21:48.240
again to 1.0 when it's going to be out so I think we will just leave the 0.10 at beta and do the

21:48.800 --> 21:58.000
0.1.0 in May but so I'll just talk a little bit about the 0.10 beta. It's mostly focused on fixing

21:58.000 --> 22:02.720
some consistency issues that would happen like when you were adding some servers in the system or

22:02.720 --> 22:10.240
removing some servers and so I will enter into a bit of distributed system theory to try to explain

22:10.960 --> 22:16.800
why exactly it's an issue and what is the solution that we made. So since I've said that

22:16.800 --> 22:23.760
garage is not based on consensus it means that we have to work with inconsistent primitives

22:23.760 --> 22:30.400
so this means we have to work with conflict-free, replicated data types, CRDTs and so these are

22:30.400 --> 22:36.720
not transactional, they are pretty much very very weakly consistent, very freeform to use

22:36.720 --> 22:42.240
and there's this last-writer wind register which is pretty much the fundamental building block of

22:42.240 --> 22:49.680
garage and so CRDTs alone are not enough to insert consistency so what we add is some read after

22:49.680 --> 22:55.040
write guarantee which is implemented using quorums and I will try to explain, I hope you will

22:55.040 --> 23:00.960
understand how it works, I think it's not so complicated but it's a bit theoretical so yeah,

23:00.960 --> 23:08.960
hold on. So read after write means if a client one is doing an operation right and the system

23:08.960 --> 23:14.320
returns to the client okay your write is saved in the system and then another client is sending a

23:14.320 --> 23:20.080
read for this data after the write is returned okay then the client two will read a value which is

23:20.080 --> 23:27.040
at least the value x that was written or a newer value this is what this means and so in practice

23:27.040 --> 23:31.040
it means that the system is basically evolving between these states so for instance we have

23:31.040 --> 23:35.680
the state here where the system is not storing anything and then we can store some value a or

23:35.680 --> 23:41.040
we can store some value b and if this is like a basic set if you have stored a on one node and b

23:41.040 --> 23:47.360
on another node then when the two nodes like merge together they will have stored a and b okay but

23:47.360 --> 23:53.120
let's do an example here for the writes so these are the three storage nodes and we're supposing that

23:53.120 --> 23:58.880
a node, a client is sending a write operation for value a so the value a is going to be sent to the

23:58.880 --> 24:05.680
network to these three nodes and at some point like maybe the purple node is going to receive the

24:05.680 --> 24:10.720
value a so it's going to move from not knowing anything to knowing the value a then the green

24:10.720 --> 24:15.520
node is also going to move from not knowing anything to knowing a when it receives the messages

24:15.520 --> 24:20.000
and so those two nodes are going to return to the client who did the operation okay I've stored

24:20.000 --> 24:25.520
the value a so at this point the client says so I've received two responses this is two over

24:25.520 --> 24:29.840
three so it's what we call a quorum and at that point the client says okay the data is stored in

24:29.840 --> 24:35.760
the system even if the third node has not received it yet and so this is the point where we can start

24:35.760 --> 24:43.520
a read request and so the read will basically is the client will ask all of the three nodes to return

24:43.520 --> 24:48.400
the value that they have stored and maybe the first node that will return its value is the red node

24:48.400 --> 24:53.600
and the red node has stored nothing so the read will first receive a value of nothing but then it

24:53.600 --> 24:58.320
will wait for another response and the other response will necessarily come from one of these

24:58.320 --> 25:03.840
two nodes and so it will necessarily read the value that was written and so it will just merge these

25:03.840 --> 25:08.880
two so this is why we use CRDTs to do this merge operation and consistency is guaranteed

25:09.840 --> 25:13.840
and maybe at some later point through some synchronization mechanism the red node will catch

25:13.840 --> 25:22.320
up and also receive the value so we have this in algorithmic form but okay and so the issue we have

25:22.320 --> 25:26.880
with this is that we're relying very strongly on these quorum properties so if we have three copies

25:26.880 --> 25:32.800
of data a quorum is at least two nodes of the three but what happens when you remove some nodes and

25:32.800 --> 25:38.080
add some other nodes in the intersystem so we will have some some data which was stored maybe on the

25:38.080 --> 25:43.280
nodes in red here and in the new system the data is being moved and it should be stored on the green

25:43.280 --> 25:48.240
nodes and so now if you do some quorum some right quorum on the red nodes and some some read quorum

25:48.240 --> 25:52.400
on the green nodes there is not necessarily an intersection of one node that has seen the read

25:52.400 --> 25:58.720
and the right and basically the consistency is broken so the question is how do we coordinate

25:58.720 --> 26:02.400
in this situation and how do we ensure that even when the cluster is rebalancing data we

26:02.400 --> 26:08.400
insert consistency and so the solution is a bit complex but basically we need to keep track of

26:08.400 --> 26:13.120
what data is being transferred between the nodes we use multiple right quorum so we're going to

26:13.120 --> 26:18.640
use quorums to write on the old set of nodes and the new set of nodes and switching reads to the

26:18.640 --> 26:23.920
new nodes only once the copy is finished so this is something we implemented for the in the context

26:23.920 --> 26:28.800
of the ngi grants we did some testing using a tool which is called jepsen which is very good for

26:28.800 --> 26:34.720
validating these kind of things and so as you can see in garage 0.9 we had consistency issues in most

26:34.720 --> 26:40.400
of our runs and in point 10 we have all runs are green except one which failed but at least there was

26:40.400 --> 26:45.280
no run where the data was plain wrong and it's actually this is very good result for us

26:47.360 --> 26:53.440
okay so this was point 10 now we're at fosdem and we're going looking forward to making a version

26:53.440 --> 27:00.000
one in april or may basically we're going to focus on security and stability there's a security

27:00.000 --> 27:05.680
audit that is going to be done by radically open security miscellaneous features should be improved

27:05.760 --> 27:10.240
this would be added and improvements may be in the user experience refactoring stuff

27:11.280 --> 27:15.360
and that's it for 1.0 hopefully we'll have that out in april this year

27:16.960 --> 27:21.680
and beyond so we have this survey which is going on in the community right now

27:21.680 --> 27:27.200
and so this is a list of the most requested features by the users of garage and actually

27:27.200 --> 27:31.920
there's a lot of work to do so the first thing is a web interface for cluster management so I guess for

27:32.000 --> 27:36.000
like visualizing the state of the cluster and setting up a new bucket as new access

27:36.880 --> 27:42.720
then it's s3 versioning which is so it's a feature of amazon s3 where you can have a you can save the

27:42.720 --> 27:48.000
historical data in the bucket and it's pretty good for like a backup system where you don't want to

27:48.000 --> 27:52.240
override data accidentally and this is a pretty crucial feature that we would need to have

27:53.440 --> 28:00.720
ACLs are here monitoring and various other things and so this is the point where I'm calling for

28:00.720 --> 28:06.000
help actually because there's a lot of work and I cannot do it myself so if anyone wants to step

28:06.000 --> 28:12.800
in and help us with this please do so we can probably find some some more funding actually we

28:12.800 --> 28:18.000
do have some funding in progress for someone who would like to do a phd on this system in

28:18.560 --> 28:23.280
in relationship with the garage so if anyone wants to do a phd in France working on some stuff

28:23.280 --> 28:28.880
come to us we have this application going on and we also can probably ask some money to nlnet

28:28.880 --> 28:34.320
which have funded us once and nji also once so we can probably get some more money if there's some

28:34.320 --> 28:37.920
specific task that that is planned and we have somebody who is willing to do it

28:40.160 --> 28:46.800
okay and so I will just spend the last few minutes of this talk to explain a little bit about how you

28:46.800 --> 28:52.480
can operate garage for people who have not run it or who are willing to scale their clusters to bigger

28:52.480 --> 28:59.280
systems so this is the basically what I would call the main screen of garage so when you interact

28:59.280 --> 29:04.320
with the cluster just start always by doing garage status and it will tell you if everything is fine

29:04.320 --> 29:08.560
so this is a five node cluster and everything seems to be fine but maybe you will have like failed

29:08.560 --> 29:12.320
nodes so this means that the connection could not be established and something is wrong and you should

29:12.320 --> 29:22.160
fix it garage is made like a some cake of different pieces like this on top we have the s3 api we

29:22.160 --> 29:27.280
also have some custom api which I'm not talking about in this talk and this is three api is actually

29:27.280 --> 29:32.720
implementing using some internal key value store for metadata and some block manager for the actual

29:32.720 --> 29:38.320
data of the big objects and then we have some systems here which maintain consistency in the system

29:39.280 --> 29:44.880
and so maybe to be a bit more specific about what's going on we have these three metadata

29:44.880 --> 29:51.200
data tables here so the first one is like the list of objects in the system the second is the list of

29:51.200 --> 29:55.680
versions of objects and so it's a bit different because an object can have a version which is

29:55.680 --> 29:59.840
currently in the cluster and a version which is currently being uploaded so for the same objects

29:59.840 --> 30:05.520
multiple versions can exist and then this version will also reference a bunch of data blocks so this

30:05.520 --> 30:10.640
is the table which has the reference to actual data blocks and so all of these tables are sharded

30:10.640 --> 30:16.000
across the nodes and in particular for the block reference table if a node has the has the shard

30:16.000 --> 30:20.640
for some references it means it's also responsible for storing the blocks associated with these

30:20.640 --> 30:26.160
references so basically from this metadata table we have a local counter for how many references

30:26.160 --> 30:31.920
for each block and then we have this rescind queue and scheduler which is responsible for ensuring

30:31.920 --> 30:36.800
that the locally stored data blocks are actually matching the number of blocks which have a reference

30:36.800 --> 30:43.600
in the in the store so yeah we have this block rescind for data blocks and this merkle merkle

30:43.680 --> 30:51.520
tree based system for the metadata and so if you do this garage stats command so there's not

30:51.520 --> 30:55.680
status it stats never command you will get some information about the internals of what's going

30:55.680 --> 31:00.560
on so these are the metadata tables and you can see here objects version and block reference

31:01.200 --> 31:05.280
so these are the number of items in the table and there are also the number of items in the

31:05.280 --> 31:10.720
merkle tree which is always a bit bigger and then you have here the number of rc entries for the

31:10.720 --> 31:15.040
block table so the number of blocks which actually have a reference in the system so here we have

31:15.040 --> 31:22.240
42,000 data blocks but we have actually 334,000 block references so this means that blocks are

31:22.240 --> 31:28.880
almost referenced by 10 different objects each on average and then we have some information on the

31:29.520 --> 31:34.320
actual nodes so the partitions here means basically is how many of the lines in the tables are

31:34.320 --> 31:38.640
affected to each of these nodes so if you have more more partitions you're going to use more

31:38.640 --> 31:43.600
storage space basically on that node it's proportional and this is a metric which is

31:43.600 --> 31:48.800
given by the node actually it's it's measuring on disk how much space is available it's not

31:48.800 --> 31:53.360
the use space it's the available space for the data partition and the metadata which is not

31:53.360 --> 31:59.760
necessarily on the same drive and so from all this information garage is able to basically tell you

31:59.760 --> 32:09.200
how much data you can still store on the cluster so here for 600 gigabytes and if you go even further

32:09.200 --> 32:13.760
you can get this list of workers so workers are basically background tasks which are running in

32:13.760 --> 32:18.720
garage all the time and so you have these tasks which are block readings so these are copying

32:18.720 --> 32:25.200
data blocks between nodes when they're missing and these are synchronization tasks for each of the

32:25.200 --> 32:33.520
metadata tables and you can change a bit the parameters of these tasks for so for instance for

32:33.520 --> 32:38.480
the the block re-synchronization you have re-sync tranquility and re-sync worker count

32:39.120 --> 32:44.560
and tranquility is a metric which can be increased to make the system go slower and use less i o if

32:44.560 --> 32:48.880
if it's serring you're saturating your i o you can increase the tranquility and if you want it to go

32:48.880 --> 32:54.000
faster you can just put it to zero and then there's also the worker count so you can set it up to

32:54.000 --> 32:58.000
eight and then you have eight parallel threads which are sending and receiving data blocks in the

32:58.000 --> 33:05.680
network there are some potential limitations if you're running extremely extremely big clusters

33:06.880 --> 33:12.560
probably you cannot run with more than about 100 100 nodes i mean you can but then the the data will

33:12.560 --> 33:17.360
not be very well balanced between the nodes and this is because we're using only 256 partitions

33:18.000 --> 33:21.120
we could probably compile a bigger version in garage but it's currently not the case

33:21.200 --> 33:28.240
and on the metadata side if you have one big bucket which is containing all your objects well you

33:28.240 --> 33:33.280
will have a bottleneck also because the first table the object table is going to store the list of

33:33.280 --> 33:38.400
objects on only three of all of your cluster nodes so if you have lots of data split your data over

33:38.400 --> 33:45.760
different buckets and also on the side on the side of the data blocks so the data is split into

33:46.480 --> 33:49.760
so if you have a hundred megabytes file in your block size is one megabytes your

33:50.240 --> 33:54.720
your file is going to be split into a hundred different files so we will have a lot of small

33:54.720 --> 34:00.160
files on disk you can increase the block size to reduce the number of files and if you have more

34:00.160 --> 34:04.800
files the processing of the queue can also be kind of slow and this is of course also

34:05.360 --> 34:10.960
dependent on your networking conditions and so just some advice for actual deployments

34:11.600 --> 34:16.320
for the metadata if you're going to do a very large cluster we recommend doing some mirroring

34:16.320 --> 34:22.800
on two fast NVMe drives possibly ZFS is a good choice garage itself does not do check summing

34:22.800 --> 34:27.360
on the metadata so it's good to have a file system that does it for you lmdb is the recommended

34:27.360 --> 34:32.400
storage engine and for data block it's a bit different and we have other recommendations we

34:32.400 --> 34:36.880
recommend using an XFS file system because we actually do some check summing for each blocks

34:36.880 --> 34:41.440
because we always compute hashes of the blocks in garage so you do not need to have a file system

34:41.440 --> 34:46.640
which is doing this this check summing again it would be wasteful so just format your partitions

34:46.640 --> 34:52.320
as XFS which is one of the fastest file systems and store your data directly on this if you have

34:52.320 --> 34:56.560
a good network and some nodes with a lot of RAM you can increase the block size to 10 megabytes

34:56.560 --> 35:03.600
at minimum and you can tune these two parameters according to your needs and of course you can do

35:03.600 --> 35:08.560
some more like global tuning split your data over several buckets use less than a hundred

35:08.560 --> 35:14.320
nodes if possible or come to us and we can work out a solution and you can use also gateway nodes

35:14.320 --> 35:21.200
which are good way to like have have nodes which are so have the request go faster because if you

35:21.200 --> 35:25.440
if you have a local gateway on the same server as the client it can basically route the request

35:25.440 --> 35:32.800
directly to the data server and you can possibly avoid run for time we have not made any deployment

35:32.800 --> 35:36.800
bigger than 10 terabytes on the side of the floor but actually some people have as we learned from

35:36.800 --> 35:41.280
the survey and so if some people are in the room it would be great to share your experience

35:42.080 --> 35:48.080
and with this I think I've talked enough garage is available as a open source software on the

35:48.080 --> 35:53.600
website of the floor at switch and in Rust and we have a matrix channel and email you can contact us

35:53.600 --> 36:02.640
and I'm taking some questions

36:02.640 --> 36:06.480
um

36:14.480 --> 36:20.160
so the question was if you store websites on garage can you integrate with dns and basically we copied

36:20.160 --> 36:26.320
the semantics of amazon where you can have a bucket whose name is the the domain of a website

36:26.320 --> 36:31.360
and so garage will route requests to the data according to the host header of the htp request

36:31.440 --> 36:36.720
and basically you just have to to configure your dns server so this is something you have to do as

36:36.720 --> 36:41.440
of at sort of garage but you configure your dns server to write the request to your garage server

36:41.440 --> 36:46.240
and then garage will just select the good bucket with the good content based on the name of the

36:46.240 --> 36:50.320
bucket and you should add some reverse proxy probably in the middle if you want to tell us because

36:50.320 --> 36:56.000
garage does not do tls yeah it's because when one of those website servers goes down then you need

36:56.000 --> 37:01.600
to reroute to some yeah so at the floor we have a solution but it's external to garage so it's more

37:01.600 --> 37:04.160
tooling

37:04.720 --> 37:11.680
yeah so in all the examples you mentioned you have effectively one node for one zone

37:12.720 --> 37:17.360
what if is that by design or can you have multiple nodes per zone or how does that

37:18.160 --> 37:22.240
I think it's uh it's uh so the question was in the examples we have uh one

37:22.960 --> 37:27.040
node on each zone and can we have more than one node and so I think it's yeah it's just the

37:27.040 --> 37:31.360
examples were not very good but yeah of course we can have multiple nodes in a single zone I think

37:31.360 --> 37:37.040
maybe in this in this graph no this is not the good one but there is a there is an example where

37:37.040 --> 37:43.280
we have several nodes in the same zone it's not a problem yeah and if you have let's say everything

37:43.280 --> 37:47.920
else calls and you only have the one zone that's remaining will the node still try to balance the

37:48.240 --> 37:53.200
data between themselves or is that effectively a you're in trouble so the question is how is

37:53.200 --> 37:57.520
data get being balanced between the nodes if you have like one zone where it's have only one node

37:57.520 --> 38:03.920
and maybe the node is smaller and so garage is trying to preserve this property of having three

38:03.920 --> 38:08.320
copies in different places you can you can ask it to have only in two places but by default it's

38:08.320 --> 38:12.880
three places and this means that if you have only three zones and one is a smaller server then you

38:12.880 --> 38:30.320
have smaller capacity of the cluster yes yes so the question yeah so the question is why did we

38:30.320 --> 38:36.080
integrate multiple disk support instead of having multiple nodes in the same zone and I think one

38:36.080 --> 38:40.560
of the most important reasons is that this way you can reduce the total number of garage processes

38:40.560 --> 38:46.080
and entries in this in this table basically because this table has only so many rows and if you

38:46.080 --> 38:49.600
have start having many different nodes it's not going to be well balanced so reducing the number

38:49.600 --> 39:00.880
of nodes helps us be better balanced basically yes I saw many of your design matching the one of

39:00.880 --> 39:08.800
open stack swift and I was wondering if you investigated using it okay so the question is

39:08.800 --> 39:13.200
there's many design points which are matching open stack swift and have we investigated using it

39:14.800 --> 39:20.880
I personally have not used open stack swift and I have not looked so much into it yes

39:30.880 --> 39:35.760
so the question is despite putting this much effort in multi multi node deployments is it

39:35.840 --> 39:44.400
still worth running the system on a single node I think it's it's so many people are doing it and

39:44.400 --> 39:48.320
I think one of the reason people are doing it is because garage is pretty simple to set up and to

39:48.320 --> 39:54.320
use so I think it's definitely possible I think there are also other solutions which are good

39:54.320 --> 40:01.040
for single node setups so yeah try it out and figure what's works best for you and okay so I think

40:01.040 --> 40:04.880
we're done for this talk thank you

