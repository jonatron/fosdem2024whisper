WEBVTT

00:00.000 --> 00:14.120
Good afternoon. Welcome to the DNS Dev Room. Lovely to see a full room once again. And

00:14.120 --> 00:18.280
I would like to just give the word to our first presenter, Thijs, who will talk about

00:18.280 --> 00:25.880
consistent hashing and related stuff. Thank you. Good afternoon. I was already mentioning

00:25.880 --> 00:33.360
like quite a full room for such a niche topic. Anyway, I asked her here because I got too

00:33.360 --> 00:40.280
many slides, as always. My name is Thijs Freyja, I work for KONG. We company open source gateway.

00:40.280 --> 00:44.360
One of the things we do is load balancing. And this talks about like how we implement

00:44.360 --> 00:52.080
consistent hashing in the load balancer, what we run into, or works, what doesn't work.

00:52.080 --> 00:59.800
So consistent load balancing, like what, why, and how. And then like what does DNS have to do with it.

00:59.800 --> 01:06.160
The what is actually fairly simple. You take an outgoing connection, you take a property,

01:06.160 --> 01:11.320
and I'm basing the property, you make sure that every single connection outgoing with that property

01:11.320 --> 01:18.040
ends up on the same back ends system. Which is fairly, I think from the hash bucket perspective,

01:18.040 --> 01:24.800
fairly straightforward. A typical for our situation gateway, typical setup, incoming traffic,

01:24.800 --> 01:35.720
cluster of gateways, and then load balancing towards the back ends, each in one or multiple instances.

01:35.720 --> 01:41.120
When we do consistent hashing, what we'd like to do is, for example, pick a user ID, and

01:41.120 --> 01:46.760
then make sure that that user ID as a property ends up on one specific back end server. And

01:46.800 --> 01:52.280
in this case it would be Harry, John, and Mary would go to instance one, Paul, and Joanne go to instance number two.

01:57.920 --> 01:59.240
What did I touch?

02:02.120 --> 02:02.680
Fat finger.

02:03.640 --> 02:12.760
No. Come on. Yeah, next question. Why? Like I said, it's a bit of a niche topic. It is very specifically

02:13.320 --> 02:19.720
geared towards cash optimization. Consistent hashing is something that many people know,

02:20.920 --> 02:26.200
but this is very specifically to, if you do a lot of hashing, you don't want to have cash misses,

02:26.200 --> 02:32.120
especially when you're scaling systems. And for legacy, we have customers using it for sticky

02:32.120 --> 02:36.440
sessions, but it's like a bit of a mediocre solution to that.

02:36.760 --> 02:48.040
So in this case, yeah, as I already explained, the same users go to the same back end, which means that

02:49.080 --> 02:54.040
the data that a back end application needs to retrieve from a database would only be related

02:54.040 --> 02:59.240
to those specific individuals. So the cash hits will be higher considering the amount of cash that

02:59.320 --> 03:09.480
the system has available. Then hashing. Hashing in general is the basic concept of hash buckets.

03:10.280 --> 03:16.440
You take an input, you throw it into a hashing algorithm. C or C 32 we used before. Now we use

03:16.440 --> 03:23.880
XX hash fast. It needs to be, have a nice consistent distribution. Then you do a modulo,

03:23.960 --> 03:30.040
in this case modulo 10, you end up in one of 10 buckets. That's basically how it works.

03:30.040 --> 03:33.640
So whatever you input, as long as the same input ends up in the same bucket.

03:38.760 --> 03:41.880
Now let's take this, the 10 buckets that we have and let's

03:43.080 --> 03:48.040
extend that and say like we have continuum with how in the print the Kitama library,

03:48.040 --> 03:53.640
which is where this was first implemented, they call it continuum. And we're going to have four

03:53.720 --> 03:59.720
nodes, each one serving 25% of the traffic of the entire continuum. So everything that we get in

03:59.720 --> 04:06.840
and we hash into some bucket falls into one of those in the end four nodes. Now if we now scale

04:06.840 --> 04:15.080
the system, we add a fifth node. Now we see that we have 20% capacity added. So what is that going

04:15.080 --> 04:23.480
to do with our hashing algorithm? From node A, traffic previously going to node A will now end

04:23.480 --> 04:32.760
up, 5% will end up on node B. From B 10% will go to node C and so on. The overall consistency

04:32.760 --> 04:38.680
loss 50%, which means that our backend systems are going to have to fetch a lot of data to catch

04:38.680 --> 04:44.200
over this 50% because they moved systems. That's the thing that we're trying to prevent.

04:45.080 --> 04:49.560
So we have 20% capacity added, 50% of the continuum changed.

04:51.800 --> 04:57.640
Now if you look at what does that mean? At the bottom side we have catch-hate ratio,

04:58.760 --> 05:07.640
take an HTTP server, 70, 80, 90% HTTP servers can catch that, get there. Then you can see that

05:07.640 --> 05:14.760
with a 20% consistency loss, which will be the minimum in our case because we add a server,

05:14.760 --> 05:19.800
20% capacity, so the minimum is always going to be 20%. So if we do that, we have like 50%

05:19.800 --> 05:27.080
extra capacity temporarily. If we don't use consistent hashing, we have 50% loss and we go to

05:27.640 --> 05:33.880
more than 120% pretty much on extra capacity, which is quite a big peak that we get on top.

05:34.520 --> 05:44.680
Now if you lay this out on a ring, and in this case we'll have simplified depiction,

05:44.680 --> 05:50.360
20 buckets, whereas in reality for each expected backend that you need, you would expect to use

05:50.360 --> 05:57.560
like 50 to 100 entries at least. So if I would have a four backends, I would use 400 entries on

05:57.560 --> 06:06.280
this ring. Now if we distribute, if we distribute the five targets, the four targets that we had,

06:06.280 --> 06:12.920
going to do the same example again, the four targets that we had, then we have two relationships.

06:13.560 --> 06:19.720
That's the target, the IP port combination, and it's related to one of the positions on the slot,

06:20.440 --> 06:26.600
one of the slot, different concept, and it's going to be based on weight. Now we have four nodes,

06:27.160 --> 06:33.000
same weight, so each node gets 25% of the available slots, fairly straightforward.

06:34.760 --> 06:40.920
Then there's a slot towards the position on the hash bucket, and that's a one-on-one relationship.

06:40.920 --> 06:49.560
Every slot goes into one bucket, but it's randomized. So if we apply that, this is the way I would get.

06:50.440 --> 06:58.760
So instead of a continuous ranges of AAAA, BBB, CCC, we get a random layout with those nodes.

07:06.200 --> 07:11.480
If we now do hashing, basically we see that every hash ends up nicely in its bucket,

07:12.280 --> 07:20.840
and as a side effect, we'll get if hashing is not available, because the property is not available,

07:20.840 --> 07:26.760
we can do an easy round robin, good by simply walking the ring, which is a side effect of Yawarun.

07:29.080 --> 07:32.920
Now if we move into adding the node again, the same thing that we did before,

07:32.920 --> 07:37.400
we're adding 20% capacity, what we're going to do, we're going to relieve every,

07:38.360 --> 07:42.440
for the slots that we have assigned, we're going to ask every back end to relieve

07:43.160 --> 07:48.760
the amount of slots that are no longer to be assigned to them, in this case they were used from 25 to 20%,

07:49.320 --> 07:56.120
one slot, and we assign them to the new one. Now if we do that, we can see that the distribution

07:56.680 --> 08:04.120
stays the same, and the hashing principle now works, because we have minimal consistency loss.

08:04.840 --> 08:09.480
Now instead of having the 50% consistency loss at the start, we now end up with like

08:09.480 --> 08:13.720
the minimal consistency loss of only exactly the 20% that we added in capacity.

08:18.920 --> 08:24.280
It also has featured that if we have a simple health checker attached to it,

08:24.280 --> 08:29.800
it's quite common with load balancers, is that we can actually move on to the next slot,

08:30.760 --> 08:35.400
and our distribution will not really change. Yes, we'll have an impact on the consistent hashing

08:35.400 --> 08:40.360
principle, and we'll have catch losses, but due to the way it's random layout,

08:41.480 --> 08:46.120
we still maintain that the weights are properly distributed, and the traffic is going to be

08:46.120 --> 08:48.600
distributed about across all your remaining nodes.

08:48.600 --> 09:02.760
Now, that is like the basic concept of how you do the consistent hashing,

09:02.760 --> 09:07.720
and how you minimize this hashing loss. Now when it gets really tricky,

09:07.720 --> 09:12.760
is when you have to do this in a cluster. In a con cluster, the nodes are basically independent,

09:12.760 --> 09:19.000
so there's no shared state, at least we try to minimize, and we just saw that we had a lot of

09:19.000 --> 09:23.720
like random stuff in there, distribution layout, etc., and that's what makes it hard.

09:27.560 --> 09:35.640
So if we have to build a deterministic layout of that ring, we have to make sure that we build

09:35.640 --> 09:39.560
the exact same ring with the exact same layout on every node in the cluster.

09:40.360 --> 09:48.840
And that's where it gets hard. So for the random layout, the way we solve this is by

09:49.560 --> 09:56.200
using the distribution, the random distribution, but not the unpredictable, the unpredictability

09:56.200 --> 10:02.520
of a random number generator by using a separate random number generator, but with a fixed seed,

10:02.520 --> 10:05.960
just a constant, so it generates the same thing over and over again on every node,

10:06.440 --> 10:10.280
because we don't care about it being unique, we only care about the distribution,

10:10.840 --> 10:17.800
that is randomly distributed on the ring. Tracking changes as nodes are being added and removed,

10:18.840 --> 10:23.800
we keep a log history. So whenever we add a new node, if a cluster gets expanded,

10:24.520 --> 10:30.680
we just replay all the changes, so we end up with the exact same state of the cluster,

10:30.680 --> 10:33.560
of the ring, and make sure that the hashing still functions.

10:36.920 --> 10:41.560
And then you think we have it covered, and then comes DNS.

10:43.400 --> 10:50.360
Because initially we had it laid out to do just IP and port, but that's static,

10:51.080 --> 10:55.560
and in all of our customers' environments, it doesn't work. So we have the Kubernetes

10:56.120 --> 11:02.040
OpenShift console, and a common denominator there is DNS. So they do source discovery,

11:02.040 --> 11:05.560
and then exposes through a DNS interface, that's where you get your backhands.

11:11.960 --> 11:18.920
So the solution we had is that we had to expand our balancer to not just take IP at a result,

11:18.920 --> 11:24.440
we had to take hostnames. Now we have to resolve them, and then instead of having a single entry

11:24.440 --> 11:29.000
for a hostname, we could have like 2, 3, 4, 5, depending on what's in the record.

11:32.120 --> 11:45.720
But that hurts. Because if you look at what some of those discovery tools do,

11:47.080 --> 11:51.880
they don't set a truncation flag, which means that every node in the cluster is going to get

11:51.880 --> 11:58.520
a different answer in a different random order. A console, for example, does that, and the core

11:58.520 --> 12:05.000
of it is that those tools try to do load balancing on an infrastructure level by forcing you to renew

12:05.000 --> 12:08.920
the DNS records, and then they're going to give you only the information they want you to have.

12:09.480 --> 12:17.880
So a console is typically quite often we've seen deployment with TTL0 and no truncation flag.

12:17.880 --> 12:22.840
So you get one record, two records, three records, but not all of them. Once we have a

12:22.840 --> 12:27.800
truncation flag, we know we can retry, we can do TCP, we get a whole lot of them, and then we can

12:27.800 --> 12:32.040
check that our balancer stays in sync. Quite often we see that it doesn't happen and the

12:32.040 --> 12:39.160
balancer goes out of sync, and then we have customers complaining, basically. TTL0 means

12:39.800 --> 12:44.760
lots of changes and updates. Every change is an update to the balance of the structure,

12:44.760 --> 12:52.840
which is like really not a good combination. And then of course there's bugs. I was just,

12:53.800 --> 12:59.320
you were commenting on DNS, it's always DNS. And we actually, we had a discussion yesterday and

12:59.320 --> 13:04.680
somebody said actually like, DNS is like the RFC is just like the primer, and then the real

13:04.680 --> 13:09.240
implementation is there are 100 million different limitations out there. That's what DNS pretty

13:09.240 --> 13:17.160
much is. And you have the cater for all of them. So we have customers that set up DNS to only return

13:17.160 --> 13:21.720
a single record ever, and an infrastructure team is refusing to change it despite the fact that

13:21.800 --> 13:29.400
application teams have difficulties with their load balancing. Amazon has Route 53,

13:30.040 --> 13:38.920
occasionally gives you a TTL0, because my guess is it's a rounding bug when the TTL gets too low

13:38.920 --> 13:43.000
to return zero, which it shouldn't be doing. Now if you think all of this is hard,

13:43.720 --> 13:51.400
try filing a bug with Amazon. That's really hard. So in essence, we haven't implemented

13:52.040 --> 13:59.000
implemented it, but to really properly replay everything, we would need to have some central

13:59.000 --> 14:05.080
storage where we have the DNS records over time stored as well. So not only the changes in the

14:05.080 --> 14:09.720
host names, but also at that point in time, how does it resolve what are the entries we're getting?

14:10.360 --> 14:16.600
Because if I add a new node in my cluster later on, rebuild it, and that node gets four entries in

14:16.600 --> 14:22.200
the DNS, where the ones that were started earlier got only three, so we'll get the thing is out of sync again.

14:26.920 --> 14:27.720
Now if you look at the

14:30.520 --> 14:35.400
overall thing, I don't know where we are in time. Oh, we're in 10 minutes left.

14:35.400 --> 14:37.800
10 minutes left. I've been hurrying too much.

14:37.880 --> 14:50.840
So concluding, it is, like I said, it's a niche algorithm and it is good for cache optimizations,

14:50.840 --> 14:56.440
but it is still, its primary concern is still load balancing. It's distributing load. The

14:56.440 --> 15:02.040
consistency is only secondary. And for caching, it makes sense because like with a cache, I mean,

15:02.120 --> 15:08.760
you lose some performance, but you don't lose data. So if you change it, you lose the consistency,

15:08.760 --> 15:14.200
it just rebuilds it and then you're good to go. Minimize the cache disruptions.

15:16.200 --> 15:22.760
The trick is with, it does require some contacts as the hash input. On the TCP level, you could have

15:22.760 --> 15:29.800
like an SNI name maybe or an IP. It's like really rough because it also needs the contacts that you

15:30.680 --> 15:36.280
need into the hash, needs to have enough cardinality to actually make sure that you're going to hit

15:36.280 --> 15:41.400
everything in your cluster. So if you have, I don't know, you say you do on a header,

15:41.400 --> 15:46.120
that either says Android or iOS. That means in your hashing algorithm, you're going to hit only

15:46.120 --> 15:50.760
two backhands ever. So if you scale and you have five backhands, you're still going to hit only two.

15:50.760 --> 15:57.800
If you have to be aware of that, that there's enough cardinality. And as before, DNS is be careful.

15:57.800 --> 16:01.960
Make sure that people set it up correctly. If you have it all in one hand, if you control the whole

16:01.960 --> 16:07.320
lot of it, you can set whether the truncation flag is set or not, whether it results everything,

16:07.320 --> 16:12.440
it gives you all the nodes, all the entries. That's good. If you're not, make sure you check with

16:12.440 --> 16:18.040
your other teams that they actually make sure you get the data to make sure that they're not fighting

16:18.040 --> 16:28.200
each other. Then there was the comparing algorithms. Like the first two are actually

16:28.200 --> 16:34.600
like the more generic ones. The way to round around like everybody knows. It's a good distribution.

16:35.640 --> 16:39.480
It doesn't care about caching. So you hit everything with everything you got.

16:40.600 --> 16:45.080
These connections are the same, but at least it takes into kind of like long lift connections,

16:45.800 --> 16:49.800
which not depending on what you're running or what service you're running might make a good

16:49.800 --> 16:59.400
distinction. Really different. Consistent hashing, cache optimization, and it works wonders under

16:59.400 --> 17:08.280
the right circumstances. There's a catch to really catch. Least latency, we have that.

17:08.520 --> 17:18.520
It's also niche, I'd say. I even wrote it. If you have high variance CPU loads,

17:19.160 --> 17:25.560
so take GraphQL, where you can have simple queries and very big queries, then this makes sense.

17:26.280 --> 17:30.280
But one thing to be careful of is that you need to have equal network latencies.

17:30.920 --> 17:36.040
So if you have all your nodes in the same network, your back ends, you have the same network

17:36.040 --> 17:41.160
latency, then it works. But if you have one close by and another one a couple of hops away

17:41.160 --> 17:47.640
that has more latency, then the least latency one will actually push the close by ones into starvation.

17:48.840 --> 17:52.680
Because they have lower latency because of the network, so we're going to push more load,

17:52.680 --> 17:57.000
and by the time the latency of those systems goes up, you're basically pushing them into

17:57.000 --> 18:02.600
resource starvation, either CPU memory or whatever, and then they become slower. And that's not an

18:02.600 --> 18:06.680
efficient way to run those servers, probably. So there's a catch to it.

18:11.560 --> 18:18.280
And I got it really in time. Questions? Thank you, Dyer.

18:18.520 --> 18:23.960
Thank you.

18:25.960 --> 18:33.480
Plenty of questions. You said that some DNS servers do not send and truncate bits.

18:33.480 --> 18:41.320
Would it be possible to just do TCP? Always? To always do TCP? Yes, that's an option.

18:41.880 --> 18:47.080
That's an option. If you can configure your client, not all clients can be configured. In our case,

18:47.080 --> 18:51.480
the client doesn't, we work with OpenResty. OpenResty underlying DNS client doesn't,

18:53.720 --> 18:59.320
it has a bug that prevents us from using it, actually. It doesn't do retries, which is,

18:59.320 --> 19:04.840
but that indeed is a good solution to making sure you get the entire, the entire record,

19:04.840 --> 19:12.040
all the entries. Yeah. I have a related question, which is, how often do you actually see enough

19:12.040 --> 19:17.560
data in a response to truncation of a content issue? It seems like you get hundreds of address

19:17.560 --> 19:23.960
port pairs and transfer before you know it. Depends on, depends on DNS implementation and to use

19:23.960 --> 19:32.760
the eyes of, of, yeah. So how many, how many times do you actually see truncation happening

19:32.760 --> 19:38.920
in those records? I don't have data. I guess it must have happened or you wouldn't know.

19:38.920 --> 19:43.880
Yeah. I don't have the data. I know that console does it. Console by default will only report three

19:45.080 --> 19:50.680
and it's UDP packet size. And I think it is UDP packet size. I don't know what it exactly is,

19:50.680 --> 19:55.960
but I would assume like four or five maybe and then truncation happens. Depends also depends on

19:55.960 --> 20:01.480
like name sizes and everything because it has to fit in a single packet. Oh, maybe console just

20:01.480 --> 20:06.360
arbitrarily truncates. Yes. Not to do with packet size. Yes. But that's, that is because it's a service

20:06.360 --> 20:12.280
discovery tool and it tries to pull the load balancer rule towards it and force you to only

20:13.000 --> 20:17.960
hit the nodes that it wants you to hit. Okay. So it has, I think it has for example things like

20:18.600 --> 20:22.680
data center awareness. So depending on where the, where the request comes from,

20:22.680 --> 20:26.360
it will give you a different set of answers than in another data center. So you're going to hit

20:26.360 --> 20:39.800
different backends. Yeah. Sorry, can you repeat the question? For the log synchronization.

20:41.720 --> 20:46.280
What algorithm are we using for the log synchronization? Which one do you mean across the cluster

20:47.240 --> 20:50.040
or inside a single balancer?

20:53.320 --> 20:59.240
Let's go with the balancer. I'll give you answer both. First one on the cluster, we don't have it

20:59.240 --> 21:05.960
because we, in the end, we have no synchronization over this shared state of this entire balancer.

21:05.960 --> 21:10.280
We don't share it. The only thing we have is the order in which nodes have been added

21:11.240 --> 21:17.320
and that's synchronized through a database basically, a control plane. Then inside the

21:17.320 --> 21:24.680
balancer algorithm itself due to the way the open rescue works, you basically don't need a log on

21:24.680 --> 21:32.680
this. The one thing is you need to be careful that you do not yield in between operations that

21:32.680 --> 21:37.640
actually are modifying the data structures. As long as you do that in an atomic operation,

21:37.720 --> 21:40.840
you're good and you don't need a log. So I answer the question.

21:44.360 --> 21:51.720
Maybe this, you could share the state of the various nodes in the cluster by just

21:52.280 --> 21:58.200
sharing the state instead of assuming you get the same input data and computing a state.

21:59.160 --> 22:07.880
So then you would of course introduce interdependencies because it would guarantee that all nodes

22:10.920 --> 22:16.600
at least use the same tables. So the question is can you use shared state

22:17.640 --> 22:25.400
instead of rebuilding it in every cluster separately? Yes, you can. The conflusters

22:25.400 --> 22:29.640
are basically independent. The data planes are on their own and they do not share state.

22:29.640 --> 22:35.800
They get their instructions from a control plane. That's it. Yes, there is one, there is the

22:35.800 --> 22:40.840
alternative option. We haven't implemented it yet. I don't think we will implement this.

22:41.400 --> 22:47.160
In reality, we see too little cases where we actually see the things go out of sync and cause

22:47.160 --> 22:54.360
issues. Usually you can tweak it by using a longer TTL so there's going to be less updates

22:54.440 --> 22:57.800
and you don't need it. So I don't think we will be implementing it in the end.

23:02.120 --> 23:06.840
Have you considered using rendezvous hashing instead of consistent hashing?

23:07.960 --> 23:12.040
Have we considered rendezvous hashing instead of consistent hashing? No.

23:15.640 --> 23:21.480
No. Frankly, I don't know it. I wrote this stuff actually quite some years ago.

23:22.200 --> 23:28.680
It was updated by a colleague later on. But I will be looking into rendezvous hashing. Thank you.

23:31.720 --> 23:32.520
Any more questions?

23:36.360 --> 23:39.000
Check. Matrix. No questions.

23:44.520 --> 23:47.320
Almost, almost, almost on the minute.

23:51.480 --> 23:52.940
You

