WEBVTT

00:00.000 --> 00:07.000
All right.

00:07.000 --> 00:14.000
So next up we're going to have Aiden who is going to be talking to us about multi-image

00:14.000 --> 00:16.000
and container.

00:16.000 --> 00:18.000
All right.

00:18.000 --> 00:20.000
Ready?

00:20.000 --> 00:21.000
Okay.

00:21.000 --> 00:22.000
All right.

00:22.000 --> 00:23.000
Hi, everyone.

00:23.000 --> 00:25.000
I'm Aiden McClelland.

00:25.000 --> 00:29.000
I work for a company called Start 9.

00:29.000 --> 00:34.000
So this project here is a little bit of a work in progress,

00:34.000 --> 00:41.000
but it is something we are trying out because we have a little bit of a less common use case

00:41.000 --> 00:45.000
for our containers, and we decided to try something a little different.

00:45.000 --> 00:47.000
So first some background.

00:47.000 --> 00:51.000
We develop an operating system called Start OS.

00:51.000 --> 00:57.000
The purpose of this operating system is to allow end users without technical expertise

00:57.000 --> 01:03.000
to run their own home servers.

01:03.000 --> 01:09.000
So the idea being like trying to bring the desktop experience to home server administration,

01:09.000 --> 01:18.000
and that way we can bring a lot of these self-hosted applications to a wider variety of people

01:18.000 --> 01:25.000
on their own hardware without them having to learn everything you need to learn about Docker

01:25.000 --> 01:33.000
and the hosting tools that we're all familiar with.

01:33.000 --> 01:40.000
So as part of this, we do have a little bit of a different use case than is generally intended

01:40.000 --> 01:46.000
for things like Kubernetes or Ansible or a lot of these tools that are designed

01:46.000 --> 01:49.000
for deploying corporate infrastructure at scale.

01:49.000 --> 01:57.000
We're really looking at like a single host machine that the user wants very low touch with.

01:57.000 --> 02:05.000
They don't want to spend a lot of time configuring their applications at a granular level.

02:05.000 --> 02:15.000
So we decided, you know, like a lot of these applications, they come with these Docker-composed setups, right?

02:15.000 --> 02:21.000
You have a main image that has your application code and then you have things like databases

02:21.000 --> 02:24.000
and reverse proxies, etc.

02:24.000 --> 02:32.000
And commonly we deploy this as a Docker-compose file, and what this does is it creates a bunch of containers

02:32.000 --> 02:38.000
that now have to be managed by the OS and by proxy by the user, right?

02:38.000 --> 02:48.000
So what we've always tried to do with Start OS is we've maintained this idea of one container, one service.

02:48.000 --> 02:57.000
And what this allows us to do is it reduces a lot of the complexity of the management of a bunch of different containers

02:57.000 --> 03:04.000
and also provides a single IP address and virtual interface on which the application is running.

03:04.000 --> 03:11.000
So when you're doing all of your network mapping, all of that can be mapped to a single virtual IP address

03:11.000 --> 03:22.000
that can then be viewed either from within the subnet within the device or is then exported through the host.

03:22.000 --> 03:31.000
This also means that you can define resource limits on a single container basis as opposed to having to do a group of containers

03:31.000 --> 03:38.000
and managing that as a group, a C group with subgroups, right?

03:38.000 --> 03:49.000
Another final reason that we did this is that our package maintainer scripts, we prefer to run inside the contained environment

03:49.000 --> 03:53.000
and these package maintainer scripts are run in JavaScript.

03:54.000 --> 04:03.000
So we run a service manager in the container that reads the package maintainer scripts

04:03.000 --> 04:12.000
and then is able to set up all of our subcontainers, our sub file systems from there, and execute our actual binaries.

04:12.000 --> 04:21.000
Okay, so the question is why do people want multiple containers at all, right?

04:21.000 --> 04:26.000
Like oftentimes you can take a single Docker image, a single application image

04:26.000 --> 04:35.000
and install all of the software you might need, but in practice this is not as easy for the service developer, right?

04:35.000 --> 04:43.000
A lot of times we have people coming to us asking for, hey, I want to be able to use an off-the-shelf Postgres image,

04:43.000 --> 04:50.000
I want to use an off-the-shelf Nginx image, I don't want to have to use like the package manager for the distribution of my container,

04:50.000 --> 04:54.000
to install that and manage it.

04:54.000 --> 04:58.000
So that's like the number one use case that we have for that.

04:58.000 --> 05:07.000
It also allows you to run applications, like say you have one in Debian, one in Alpine, run all of them together.

05:09.000 --> 05:16.000
Then, you know, the other reason that you might want multiple containers is you can isolate the subcomponents of an application away from each other

05:16.000 --> 05:21.000
and also do resource limits on individual application subcomponents.

05:21.000 --> 05:29.000
If anybody has additional reasons why you might want to do separate containers as opposed to a single container for an application,

05:29.000 --> 05:33.000
I would love to hear them, but these are the reasons we came up with.

05:36.000 --> 05:42.000
So our solution, we cover this first use case using trutes.

05:42.000 --> 05:51.000
Number two, as far as we can tell, works for the most part, but that is remaining to be teased out.

05:51.000 --> 05:57.000
This does not allow us to isolate the subcomponents of our application from each other or create resource limits on individual applications.

05:57.000 --> 06:07.000
Subcomponents as easily, those will have to be managed by manual tuning of resource limits within the prokates of the container.

06:07.000 --> 06:20.000
So, yeah, we've ultimately decided that those last two components aren't really necessary for our use case.

06:20.000 --> 06:25.000
Ultimately, a single application is where we define our sandbox.

06:25.000 --> 06:33.000
So sandboxing separate parts of an application from each other, like has some security benefit,

06:33.000 --> 06:38.000
we've decided isn't worth the complexity.

06:38.000 --> 06:41.000
So we decided to do this with LXC.

06:41.000 --> 06:45.000
Why do we do LXC as opposed to something like Docker or Podman?

06:45.000 --> 06:49.000
LXC is a lot more composable.

06:49.000 --> 06:59.000
It allows us to pop the hood on a lot of the very subcomponents of container technology and manage it more manually.

07:00.000 --> 07:05.000
So we can, for example, easily manipulate the container root FS at runtime.

07:05.000 --> 07:15.000
So even with an unprivileged container, that unprivileged container can communicate with the host and modify its root file system very easily.

07:15.000 --> 07:24.000
We use our shared mount propagation for our root FS, which allows the host operating system to easily manipulate that file system.

07:24.000 --> 07:37.000
And then it's also unlike some other container tools, you can perform commands like shrewt and mount from inside an unprivileged container,

07:37.000 --> 07:42.000
which is not allowed on a lot of other technologies.

07:45.000 --> 07:50.000
So to put together a service, an application,

07:51.000 --> 07:58.000
we have effectively a single root FS image that all of our applications share.

07:58.000 --> 08:10.000
This root FS image is just a base image that we use for all of our containers that has a, like, we use Alpine right now,

08:11.000 --> 08:24.000
but it loads a Node.js application that runs the package maintainer scripts and then launches the various actual demons inside their trues.

08:26.000 --> 08:31.000
It communicates with the host using a JSON RPC API over a Unix domain socket.

08:32.000 --> 08:39.000
So there's bi-directional communication between the host and the service manager in the container,

08:39.000 --> 08:45.000
and then, yeah, it can perform the actual application code inside the shrewts.

08:46.000 --> 08:58.000
So the host API, what it does for the container is it can perform some manipulation of the root file system of the container,

08:58.000 --> 09:04.000
and this allows creating overlaid images in the same way you might be creating a container.

09:04.000 --> 09:13.000
All we do is we create a root FS image with an overlay file system and attach it to the container in a way that they can trude into it.

09:14.000 --> 09:21.000
And then we also have a bunch of other APIs that these packages can interact with,

09:21.000 --> 09:26.000
mostly for integration with the end user experience,

09:26.000 --> 09:33.000
and integration with other services and applications on the host in a way that the user might have to intermediate.

09:34.000 --> 09:39.000
And then we also have a set of APIs designed for hassle-free networking.

09:39.000 --> 09:46.000
If you have, you know, some application bound to a port, you can now attach that port to a Tor address,

09:46.000 --> 09:56.000
to a clearnet address, or to just a LAN address so that you can be accessed by your local area network.

09:56.000 --> 10:01.000
And the host OS manages all of the certificate management, either through Let's Encrypt,

10:01.000 --> 10:07.000
or through a host root CA for the LAN communication,

10:07.000 --> 10:11.000
because obviously you can't get a Let's Encrypt certificate for a .local.

10:12.000 --> 10:23.000
Okay, so then the service itself, it runs a very basic API that receives commands from the hosts.

10:23.000 --> 10:29.000
So when the application is running, it can receive like an initialization command,

10:29.000 --> 10:34.000
it can start or stop the service, and then shut down the service entirely in order to kill the container.

10:35.000 --> 10:44.000
And then it also invokes all of the various package maintainer scripts, such as editing user configuration,

10:46.000 --> 10:48.000
installing the service, or updating the service.

10:48.000 --> 10:55.000
All of those perform various package maintainer scripts that get called from the host.

10:56.000 --> 10:59.000
Okay, so when we actually launch a binary,

11:01.000 --> 11:13.000
the package developer defines in some JavaScript, we have some well-typed TypeScript APIs for this to describe this structure,

11:13.000 --> 11:21.000
but it defines what binaries to launch, what images to launch each binary in,

11:22.000 --> 11:24.000
where to mount its persistence volume.

11:24.000 --> 11:28.000
So we have a series of persistence volumes that are mounted to the container,

11:28.000 --> 11:33.000
and can be attached to any path within these sub-file systems,

11:33.000 --> 11:38.000
and then it defines any environment variables or arguments in any standard way that you would launch a program.

11:40.000 --> 11:44.000
And then for each command that you have,

11:44.000 --> 11:47.000
when you just similar to how you would define a system deservice file,

11:47.000 --> 11:56.000
you can define all of these arguments and then any dependencies or health checks associated with your service.

11:56.000 --> 12:12.000
And then for each of these commands, the in-container service manager will mount an overlaid image for the requested image ID to the container.

12:13.000 --> 12:21.000
It will then take our special directories, proxys, dev, and run, and bind them inside the container.

12:21.000 --> 12:25.000
So all of the containers share the same proxys, dev, and run.

12:27.000 --> 12:30.000
And then it will run the command in the true.

12:31.000 --> 12:36.000
Okay, so here is an example I have of a package maintainer script.

12:36.000 --> 12:41.000
I don't know if that's actually visible to everyone.

12:41.000 --> 12:44.000
Is that, are you guys able to see that?

12:44.000 --> 12:45.000
Okay.

12:46.000 --> 12:51.000
Well, I suppose I can just talk about it.

12:51.000 --> 13:00.000
But effectively, you have a fairly simple JSON configuration where you define your image ID, your command, your arguments,

13:00.000 --> 13:07.000
and then some health checks defining when is this thing ready, as well as some dependencies.

13:07.000 --> 13:12.000
So like if you don't want to launch a various demon until another service is ready,

13:12.000 --> 13:16.000
you can just specify that and then it won't launch until its health check passes.

13:18.000 --> 13:22.000
So all of this is available on the GitHub if you want to check it out.

13:23.000 --> 13:34.000
This particular example is in GitHub's start9labs slash hello world startOS.

13:34.000 --> 13:38.000
There should be a link on the talk.

13:38.000 --> 13:44.000
So time to do a little demo of what I have working so far.

13:47.000 --> 13:51.000
Let's see if I can get my shells over here.

13:53.000 --> 13:55.000
All right.

13:58.000 --> 14:03.000
So here I have an instance running, hold on.

14:05.000 --> 14:06.000
There we go.

14:08.000 --> 14:12.000
Here I have an instance running startOS.

14:12.000 --> 14:15.000
I've already installed a package.

14:15.000 --> 14:18.000
This package in this case is NextCloud.

14:18.000 --> 14:21.000
This NextCloud package contains two images.

14:21.000 --> 14:26.000
It's got the NextCloud base image, which also contains the Nginx server

14:26.000 --> 14:30.000
because it's running the PHP for NextCloud.

14:30.000 --> 14:42.000
And then we have Postgres, which is our database persistence layer for NextCloud.

14:42.000 --> 14:47.000
So what we're going to do, so we've attached into this container,

14:47.000 --> 14:51.000
and then I'm going to go ahead and just inject,

14:51.000 --> 14:59.000
basically run a REPL inside the JavaScript engine here.

14:59.000 --> 15:05.000
And I'm going to go ahead and do my imports here as well.

15:07.000 --> 15:13.000
And what this has done is it has connected us to our JSON RPC APIs,

15:13.000 --> 15:16.000
both the hosting of the container and the container into the host.

15:17.000 --> 15:22.000
And then we're going to create a couple of overlay images.

15:22.000 --> 15:26.000
So first we're going to do our Postgres image.

15:28.000 --> 15:33.000
And so what this is going to do is it's going to tell the host,

15:33.000 --> 15:36.000
hey, I want to mount this Postgres image to the container.

15:36.000 --> 15:38.000
It says, okay, here you go.

15:38.000 --> 15:40.000
Here's the path at which I have attached it.

15:40.000 --> 15:44.000
I'm going to do the same thing for the main image.

15:47.000 --> 15:52.000
And there we are.

15:52.000 --> 15:57.000
I'm going to go ahead and define a couple environment variables.

16:17.000 --> 16:20.000
Okay.

16:20.000 --> 16:24.000
So I have a set of temporary hacks that I've put in

16:24.000 --> 16:33.000
that will later be managed by the actual container service manager.

16:33.000 --> 16:38.000
But it's mainly around permissions of the container.

16:38.000 --> 16:41.000
I still need to get Shift FS working properly.

16:41.000 --> 16:45.000
Because LXC, what it does is it maps the UIDs

16:45.000 --> 16:48.000
within the unprivileged container to UIDs on the host.

16:48.000 --> 16:51.000
And so when we mount stuff to the container,

16:51.000 --> 16:54.000
we also need to perform that same mapping.

16:54.000 --> 16:56.000
So we're not doing that yet,

16:56.000 --> 17:04.000
but I have a set of ownership changes that will manage that.

17:04.000 --> 17:08.000
And then all we have to do is go ahead and launch our application.

17:08.000 --> 17:12.000
So I'll go ahead and launch Postgres first.

17:12.000 --> 17:13.000
And here we go.

17:13.000 --> 17:20.000
We have Postgres running inside a tru, inside the container.

17:20.000 --> 17:22.000
And it looks like it's ready.

17:22.000 --> 17:25.000
And then now I can also launch.

17:25.000 --> 17:27.000
Next slide.

17:39.000 --> 17:45.000
So here we have, now both of these applications are running

17:45.000 --> 17:49.000
within the same process namespace, the same C group, the same container.

17:49.000 --> 17:52.000
But they're running from completely separate images.

17:52.000 --> 17:56.000
And that's all I have to show you guys.

17:56.000 --> 18:00.000
I think we can open up for Q&A.

18:09.000 --> 18:11.000
Thank you.

18:24.000 --> 18:27.000
So we have considered the idea.

18:27.000 --> 18:32.000
Right now we actually haven't found it necessary yet.

18:32.000 --> 18:37.000
Like the tru seems to be sufficient for the sandboxing we need to do.

18:38.000 --> 18:43.000
As far as we can tell, the technology is at a point where it wouldn't be too difficult

18:43.000 --> 18:50.000
to do containers and containers, but realistically we haven't found it necessary.

18:50.000 --> 18:52.000
That's all.

19:07.000 --> 19:20.000
So I think you're asking as a package developer how we distribute your application.

19:20.000 --> 19:25.000
So if you have a service that you want to distribute to our users,

19:25.000 --> 19:33.000
to people who are running on StartOS, we have our own, like the company Start9 runs a marketplace.

19:33.000 --> 19:37.000
But we just have a very standardized package format.

19:37.000 --> 19:42.000
In this package format, you could host on any website.

19:42.000 --> 19:47.000
If you want to charge for it, you can charge for it.

19:47.000 --> 19:55.000
But ultimately the APIs are generic enough that you can run your own marketplace

19:55.000 --> 20:02.000
to offer whatever services you want using whatever protocols you'd like to

20:02.000 --> 20:05.000
to gate access to those S9PKs.

20:05.000 --> 20:12.000
So as a service developer, in general, if you're publishing to our official registry,

20:12.000 --> 20:18.000
that means that you have a free and open source project that you're looking to distribute for free.

20:18.000 --> 20:24.000
But that does not stop you from running your own paid marketplace.

20:32.000 --> 20:34.000
One more question.

20:52.000 --> 20:54.000
I'm sorry, I couldn't hear that.

20:55.000 --> 21:01.000
Other resources for our application?

21:01.000 --> 21:08.000
Yeah, so the resources are managed on the scale of the entire application

21:08.000 --> 21:18.000
using the configuration of the outer LXC container that everything runs inside of.

21:18.000 --> 21:21.000
So you can just modify that LXC config.

21:21.000 --> 21:29.000
Well, we modify that LXC config automatically based off of the host APIs.

21:48.000 --> 21:50.000
Thank you.

