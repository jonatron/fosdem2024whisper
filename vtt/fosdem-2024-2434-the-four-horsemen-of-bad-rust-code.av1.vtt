WEBVTT

00:00.000 --> 00:13.880
Let me do a quick survey. Who has a JavaScript background? Okay, maybe like 10%. Who has

00:13.880 --> 00:22.000
a C background? C++? Holy hell. It's like 80% for the people on stream. Who has a Python

00:22.000 --> 00:29.080
background? What are you, Paulie Glotz? What's going on? 70% or so. Any other languages?

00:29.080 --> 00:36.360
Just scream out. I heard something like, it was something like, oh, but I can't really

00:36.360 --> 00:47.200
remember. Does anyone own this book? I found this book on my attic and it was kind of peculiar

00:47.200 --> 00:55.880
because it had some arcane cantations in it and it looked like magic, but it certainly

00:55.880 --> 01:02.080
had something to do with Rust. And I was really excited. I was really enticed by this book.

01:02.080 --> 01:07.040
This is why I want to talk about that book. It was pretty old. There was one section in

01:07.040 --> 01:12.000
there which I really liked and it was called the Four Horsemen of Bad Rust Code. This is

01:12.000 --> 01:18.240
what this talk is about. Before we get into what the Four Horsemen are, I would like to

01:18.320 --> 01:26.040
introduce myself. I'm Matthias. I live in DÃ¼sseldorf in Germany. I've been doing Rust

01:26.040 --> 01:35.520
since around 2015. I do Rust for a living as a consultant. I did a Rust YouTube channel

01:35.520 --> 01:41.120
a long, long time ago called Hello Rust. Only 10 episodes, but well, what can you do? And

01:41.120 --> 01:46.280
lately I started a podcast called Rust in Production. If you like what I say in this

01:46.320 --> 01:53.000
talk, maybe you also want to subscribe to the podcast later on. That's it for the advertisement,

01:53.000 --> 02:00.240
going back to the Four Horsemen. I thought about this title a lot. Why would you talk

02:00.240 --> 02:08.800
about Bad Rust Code? I think from my experience as a Rust consultant, I see patterns evolving

02:08.800 --> 02:13.960
over time. I see people doing the same things in Rust that they do in other languages. They

02:14.000 --> 02:18.960
repeat the same mistakes and I saw that no one really talked about those problems. That

02:18.960 --> 02:24.320
is an issue when you come from a different language and you try to learn the rustic way,

02:24.320 --> 02:29.840
the idiomatic way to write Rust code. This is what this talk is about. Let me present to you

02:29.840 --> 02:38.040
the antagonists. While I do that, try to picture yourself. Imagine who you are and what you think

02:38.080 --> 02:45.320
your role would be in this talk. The first horseman is this. Actually, let me show all

02:45.320 --> 02:56.320
of them. And the first one is ignorance. What is ignorance? Magical little term. We will

02:56.320 --> 03:02.360
get to that in the next slide. And we have excessive abstraction, premature optimization,

03:02.360 --> 03:09.480
and omission. Of course, you could add your own personal Rust horseman. And these are just

03:09.480 --> 03:15.960
very subjective, but these are the things that I see in the real world. Now that we

03:15.960 --> 03:22.760
introduced the antagonists, let's go through their anti-patterns and what they are famous

03:22.760 --> 03:31.280
for one by one, starting with ignorance or ignorance. The horseman that is behind this

03:31.320 --> 03:39.120
pattern is someone that uses stringy type APIs. You have seen it before. Someone uses a string

03:39.120 --> 03:45.240
where they could have used an enum or they don't really embrace pattern matching. And

03:45.240 --> 03:50.720
that makes APIs brittle. You are in a situation where if you refactor something, you might

03:50.720 --> 03:56.720
run risk of forgetting that you changed something or maybe you make a typo and then your string

03:56.760 --> 04:03.760
is incorrect. And so it doesn't represent what you want to represent. They also freely

04:03.760 --> 04:09.480
mutate variables. They go and say, yeah, this is state and I can change it. Rust has the

04:09.480 --> 04:14.640
mud keyword for this, but they do that liberally across the entire code base, which makes reasoning

04:14.640 --> 04:21.640
on a local scope very, very hard. They also use bad or no error handling. We will get

04:22.280 --> 04:26.440
to that in a second. They use unwraps a lot and they don't really think about the error

04:26.440 --> 04:33.200
conditions of your application. They also have a lack of architecture in their applications.

04:33.200 --> 04:39.800
And they use a general prototype style language of writing Rust code. And where do they come

04:39.800 --> 04:46.400
from? Usually those are people that were administrators before or they write shell scripts or they

04:46.400 --> 04:50.680
come from other languages like scripting languages. And this is what they know. Nothing wrong

04:50.720 --> 04:57.720
with that, but they haven't fully embraced what Rust is capable to offer. How do you discover

04:59.040 --> 05:04.960
that you belong to this group in the code? Well, if you do things like this, you have

05:04.960 --> 05:09.600
highly imperative code. You go through the code and then you tell the program, hey, do

05:09.600 --> 05:15.400
this, do that, do this, do that, instead of using, for example, a declarative way of describing

05:15.400 --> 05:21.360
what the stage should be. They also use magic return values like minus one or an empty string

05:21.360 --> 05:27.480
to represent a certain special value instead of using errors. Everything is a string. Unwrap

05:27.480 --> 05:34.480
is used freely. You clone all the things and you use the mod keyword. Why is cloning a bad

05:34.520 --> 05:40.880
thing? I don't think it is. But the problem with clone is that you maybe don't buy into

05:40.880 --> 05:46.560
the Rust model of ownership and borrowing. And that means that you bring what you learned

05:46.560 --> 05:52.080
from the past from other languages to Rust and at some point you run into issues with

05:52.080 --> 05:57.080
your architecture which you cannot easily resolve anymore. And this is why clone is kind

05:57.080 --> 06:03.480
of a stop sign. It's not a warning sign, but it should make you think for a moment.

06:03.480 --> 06:08.560
It's an indicator of structural problems in your code, if you like.

06:08.640 --> 06:16.360
Okay. With that out of the way, let's make it a little more practical. How could we maybe

06:16.360 --> 06:25.040
put this into practice and improve our code step by step? Imagine you wanted to calculate

06:25.040 --> 06:31.320
prices for different cities for a bunch of hotels that you have in these cities. For

06:31.320 --> 06:37.880
example, imagine this was a map. This is an actual map, by the way. Africa does not look

06:37.880 --> 06:45.040
like this. And also, Jerusalem is not the center of the world. I mean, we can debate

06:45.040 --> 06:51.000
about that, but certainly geographically there are some issues with this map. Imagine your

06:51.000 --> 07:00.920
input looked something like this. It's a CSV file. You get a hotel name, a city, a date,

07:00.920 --> 07:08.080
a room type, and a price. And you go through this file line by line and you try to parse

07:08.080 --> 07:15.040
it into something that looks like that. For Brussels, you have a minimum hotel price of

07:15.040 --> 07:23.000
40 bucks, a mean price of 80, and a maximum price of 150. Fun fact, I arrived yesterday

07:23.000 --> 07:30.040
not having a hotel room because I thought I booked a hotel, but it was last year. So

07:30.040 --> 07:36.640
I was in the upper range here. Thanks, Walshbeng, by the way, for sharing your room with me.

07:36.640 --> 07:42.400
Otherwise, they would have been a nightmare. If you wanted to parse the input file and

07:42.400 --> 07:49.320
create a result like this, all you have to do is write this code. That's the entire code.

07:49.320 --> 07:55.760
Nothing really big going on here. There are some peculiarities, but this is usually what

07:55.760 --> 08:02.240
someone would write who would say Rust is not their first language. Maybe they just

08:02.240 --> 08:10.040
try to port whatever they had in another language to Rust. This is code that I see them doing.

08:10.040 --> 08:16.440
What you do is you read the CSV file, then you create a hash map of cities, then you iterate

08:16.440 --> 08:22.560
over each hotel, you try to parse the data by splitting each line, you extract fields

08:22.560 --> 08:28.160
from it, you parse the price, and then you update the city. Updating the city happens

08:28.160 --> 08:39.360
somewhere in the lower end. At the end of it, you print the mean, the max, and the minimum.

08:39.360 --> 08:45.400
That's it. That's the entire code. You know, it's working. Technically, you could run this

08:45.400 --> 08:51.200
code and it will produce the result that you expect. Prices for different cities, we're

08:51.200 --> 08:59.960
done, right? Unless we think about the bigger picture and the demons and the monsters that

08:59.960 --> 09:05.400
are out there out in the ocean, and they can haunt us and bite us. There's dangerous beasts

09:05.400 --> 09:12.000
out there, killer animals. I think what you want to do is improve that code a little bit.

09:12.000 --> 09:17.000
How can we make this code a little more idiomatic? This is the same code. Now, let's look at

09:17.000 --> 09:25.400
some parts that I personally wouldn't want to have. Consider this block. There's some

09:25.400 --> 09:32.000
things going on, but overall, it's a very manual way, a very imperative way of going

09:32.000 --> 09:39.520
through the list of hotels. We literally have a couple if conditions here. If price is smaller

09:39.520 --> 09:45.080
than city data zero and so on, we update the price, yada, yada, yada. There are patterns

09:45.160 --> 09:51.040
that make that a little nicer to read in Rust. This is the same code. It's just something

09:51.040 --> 09:58.560
very similar, but we kind of manage to shrink it down a little bit. In comparison to what

09:58.560 --> 10:06.440
we had before, we get city data and then we use some sort of tuple extraction to get the

10:06.440 --> 10:11.240
mean at a minimum and the max. That makes things a little easier. We can suddenly talk

10:11.240 --> 10:18.080
about mean instead of city data zero, for example. That's not the major problem with

10:18.080 --> 10:26.160
this code. There's unwraps too in here. Well, for a first prototype, that might work fine,

10:26.160 --> 10:31.040
but later on, maybe you don't want to have that. What if you cannot open the hotel's

10:31.040 --> 10:36.840
CSV file? What if you cannot parse a price? In this case, the entire program just stops.

10:37.080 --> 10:42.480
A question of design, but I would say if there's a single line that is invalid, you probably

10:42.480 --> 10:48.520
don't want to stop the execution right away. Another problem is that we index into the

10:48.520 --> 10:56.360
memory right away. Who tells us that a line has that many entries, five entries? It might

10:56.360 --> 11:01.720
have three. It might have zero. Who knows? But if we index into something that doesn't

11:01.760 --> 11:06.960
exist, the program will panic and that is kind of a bad thing. The underscores mean

11:06.960 --> 11:11.000
that the variables are not used, so we can remove them. We have a little bit of a cleaner

11:11.000 --> 11:16.520
structure and a simple way to check that a line is valid would be to just have this manual

11:16.520 --> 11:22.000
check in there. I know it's not very sophisticated, but it helps us along the way. Now we check

11:22.000 --> 11:28.360
if the hotel data length is five and if it is not, we just skip the entry. Let's look

11:28.400 --> 11:34.360
at parsing for a second. How do we want to handle parsing? I said that maybe we don't

11:34.360 --> 11:39.720
want to stop the execution when we run into an issue and we can do that in Rust by matching

11:39.720 --> 11:46.160
on the parse result. A very simple way to do that would be to say match price dot parse

11:46.160 --> 11:49.960
and if we have an okay value, we take it and if we have an error, we don't really care

11:49.960 --> 11:55.200
about the error. We just print an error on standard error and then we continue with the

11:55.240 --> 12:02.240
rest of the parsing. Looking at the input, one thing we can do as well is apply a similar

12:03.200 --> 12:10.200
pattern and introduce a result type. Now we use a box for representing a result type.

12:12.680 --> 12:18.640
This is because you don't need anything, any external library to have a result type that

12:18.640 --> 12:23.360
has an error type which can be literally anything. So it can be a string, anything that implements

12:23.360 --> 12:28.320
error, the error trade. In this case, it's a very simple way to improve your Rust code.

12:28.320 --> 12:34.820
It's a good first step. What we do instead now is we say read to string and then we map

12:34.820 --> 12:41.320
the error in case we have an error to something that a user could understand and act on. Then

12:41.320 --> 12:48.320
yeah, the code is already a little cleaner. We handled a few error cases already and this

12:49.320 --> 12:56.320
is something that might pass a first iteration of a review cycle. Now of course there are

12:56.920 --> 13:03.120
certain other issues with this code. For example, CSV handling. CSV is tricky. Proper handling

13:03.120 --> 13:09.040
of delimiters is very hard. For example, you might have an entry which has semicolons like

13:09.040 --> 13:14.760
on the left side here or you have something that has quotes around a semicolon and you

13:14.800 --> 13:21.800
probably want to handle that. So a simple string split does not suffice. Same with encodings.

13:22.160 --> 13:28.160
On what platform are we operating on? Do we know the encoding right away? Does the CSV

13:28.160 --> 13:33.200
file contain headlines or no headlines? And there's many, many caveats like that. If you're

13:33.200 --> 13:38.400
interested, there's a talk called stop using CSV. I don't say you should stop using CSV,

13:38.400 --> 13:44.960
but I say you should start watching this talk because it's really good. Right. How can

13:44.960 --> 13:50.480
we introduce types? I talked about types a lot and Rust is great with types. We should

13:50.480 --> 13:55.800
use more of them. Here's a simple way. I already talked about the result type and in the first

13:55.800 --> 14:00.960
line we just create an alias for our result and we say it's anything that has a T where

14:00.960 --> 14:07.960
T is generic and the error type is of type box dÃ¼n stet error. And then we can use the

14:08.480 --> 14:15.480
result in our code to make it a little easier to read. As well, we introduce a hotel struct

14:15.840 --> 14:20.320
and we have a couple fields, just strings and floating points at this point. But this

14:20.320 --> 14:25.520
helps us make the code a little more idiomatic already. We will combine those things on the

14:25.520 --> 14:32.520
next slides. But first let's look at the CSV parsing. There's a CSV create. I advise you

14:32.960 --> 14:38.520
to use it. It's pretty solid. And what you can do is you create a builder and a builder

14:38.520 --> 14:45.520
pattern allows you to modify a struct and add members or modify members dynamically.

14:45.960 --> 14:51.960
And in this case we decide that our CSV file has no headers and the delimiter is a semi

14:51.960 --> 14:58.960
colon. And the way you can use it is like this. You now say for hotel in hotels deserialize.

14:59.960 --> 15:06.960
No more strings splitting. And now we match on the hotel because this returns a result.

15:07.480 --> 15:11.840
And now we need to make sure that the hotel that we parse is in fact correct. And after

15:11.840 --> 15:17.000
the step we don't have to deal with edge cases anymore because we know that the struct is

15:17.000 --> 15:23.440
valid. That means it has the required amount of fields and prices are also floats. Which

15:23.440 --> 15:30.040
is great makes the code much more readable already. And it was very simple to do so.

15:30.040 --> 15:36.960
Now I want to quickly talk about this part. There's a cities hash map. It has a string

15:36.960 --> 15:42.120
which is the city name. And then it has three floats which are the mean, the min and the

15:42.120 --> 15:48.240
max price. I don't think this is particularly idiomatic. The way it was used before was

15:48.240 --> 15:55.240
something like this. And we kind of managed to work our way around it. But a better way

15:55.280 --> 16:00.400
I would say would be to introduce a type for this as well. Because if we're talking about

16:00.400 --> 16:05.400
prices and pricing seems to be something that is very central to what we do in this application

16:05.400 --> 16:10.960
maybe we should have a notion of a price. It's very simple to do that. You just introduce

16:10.960 --> 16:15.360
a price type. Now you might be confused why we suddenly don't have a mean anymore. But

16:15.360 --> 16:20.400
instead we have a sum in account. And the reason being that when we parse the files

16:20.400 --> 16:27.400
we update the sum and later on at the end we can calculate the mean. Which has some mathematical

16:27.440 --> 16:33.680
properties which are favorable because now we don't really have, we don't run into rounding

16:33.680 --> 16:39.320
issues anymore. This is an aggregation that we can do whenever we want to get kind of

16:39.320 --> 16:45.760
a mean on the fly. And at the same time we have a default. Now the default is not really

16:45.760 --> 16:52.360
idiomatic too I would say. But the great part about it is that we can later reuse it and

16:52.360 --> 16:58.360
make our code a little more readable. In this case we set the min price to the maximum float.

16:58.360 --> 17:03.480
But then whenever we introduce a new price it will overwrite the maximum because I guess

17:03.480 --> 17:09.600
by definition it's smaller than the maximum or smaller or equal. And same for the max

17:09.600 --> 17:16.320
and some in account are kind of set to zero to begin with. And just before we bring it

17:16.320 --> 17:22.160
all together here's one more thing that we should do which is have a notion of a display

17:22.160 --> 17:27.960
for price. In this case we implement the display trade and we say yeah if ever you want to

17:27.960 --> 17:32.640
print a price this is the structure that you should use. The min, the mean and the max.

17:32.640 --> 17:38.200
And then this way we can make our code way more readable. Now you can see that instead

17:38.200 --> 17:45.400
of using a tuple or floats here we use a price. And when we update the prices we can talk

17:45.400 --> 17:50.400
about this object. We can tell the object hey update your min for example. Here we say

17:50.400 --> 17:57.080
price.min.min holds a price and we automatically get the min price as well. We update those

17:57.080 --> 18:07.400
price fields and yeah we can even introduce a price.add method. I don't show it here but

18:07.400 --> 18:12.920
technically why not. We can add a new hold up price. Prices could be added over time.

18:12.920 --> 18:20.480
Now that depends on I guess your taste, your flavor of rust. This is the entire code. It's

18:20.480 --> 18:24.720
a little longer but you saw all the parts. And now you have something that I would say

18:24.720 --> 18:34.680
isn't a workable state. It's not great but we did one thing. We considered rust. We thought

18:34.680 --> 18:41.880
the ignorance. We started to embrace the rust type system. We started to lean into ownership

18:41.880 --> 18:48.080
and borrowing which are fundamental concepts in rust. We lean into design patterns and we

18:48.080 --> 18:54.600
learn how to improve our architecture. And I would also say if you want to improve this

18:54.600 --> 18:59.360
part try to learn a different programming paradigm. Rust is not the only language. Try

18:59.360 --> 19:04.240
rock or try a functional language like Haskell. It might make you a better rust programmer

19:04.240 --> 19:10.880
too. This is how you fight ignorance. Now if you see that none of these horsemen fit

19:10.880 --> 19:15.120
to you by the way just think of your colleagues how you would want to introduce them to rust

19:15.120 --> 19:18.920
because this is the code you have to review and also probably maintain in the future. So

19:18.920 --> 19:24.320
it's time well invested. If you want to learn more about idiomatic rust specifically there

19:24.320 --> 19:31.560
is a website. I just put it there. It's an open source repository. It has some resources.

19:31.560 --> 19:37.360
This is a rendered version of it. You can sort by difficulty so that's your experience

19:37.360 --> 19:42.160
and then you can sort by interactivity if you want to have a workshop or not. For example

19:42.200 --> 19:47.760
there are free resources on there and paid resources too. Right let's go on and look

19:47.760 --> 19:54.480
at the next horsemen. Excessive abstraction. Everyone in this audience knows someone like

19:54.480 --> 20:00.760
that. They try to over engineer solutions because rust leans into that. It allows you

20:00.760 --> 20:05.600
to do that. It's a nice language to write abstractions. Everyone likes to do that. But

20:05.600 --> 20:10.000
then you add layers of indirection that maybe people don't necessarily understand if they

20:10.040 --> 20:15.200
come from a different background. They use trade successively and generics and lifetimes

20:15.200 --> 20:21.720
and all of these concepts are great in isolation. The combination of which makes the programs

20:21.720 --> 20:26.840
hard to read and understand for newcomers. Now if you find yourself in this camp try

20:26.840 --> 20:34.720
to fight this as well. Common symptoms of this are things like this where you have a

20:34.720 --> 20:40.720
file builder which takes a t as ref of str and a lifetime of a and this makes sure that

20:40.720 --> 20:47.160
you can pass any type and that it has no allocations that are not visible because of the lifetimes.

20:47.160 --> 20:53.360
So this might be fast and it might also to some extent be idiomatic but it is something

20:53.360 --> 20:58.160
that your colleagues also have to understand. Another thing is I might use this again. Let's

20:58.160 --> 21:06.160
make it generic or trades everywhere. And how do you get to that mindset? It's very

21:06.160 --> 21:12.600
simple. After you wrote your CSV parser it's natural that you want other parsers too. Of

21:12.600 --> 21:17.800
course you want to chase on. Of course you want to read and write into a database. You

21:17.800 --> 21:24.120
start thinking that you'll need all of those formats at some point and this is the part

21:24.120 --> 21:30.960
that is important at some point. And then you end up with something like this. It's

21:30.960 --> 21:36.960
a trade definition for a hotel reader and it has a single method called read and it

21:36.960 --> 21:43.040
takes a self that's why it's a method but it also takes a read which implements the

21:43.040 --> 21:48.800
read. That means you can pass anything that implements the read trade and it returns a

21:48.800 --> 21:57.040
box of iterator of item equals result hotel with a lifetime of A. No allocations except

21:57.040 --> 22:04.240
for the box but the iterator itself is a very idiomatic way to say a result of hotel so

22:04.240 --> 22:09.080
parsing errors are considered and it's very applicable for all of the reader types that

22:09.080 --> 22:13.680
you could possibly want. Let's say you wanted to use that trade and implement it for our

22:13.680 --> 22:20.120
hotel reader. Now suddenly we blow up the code to something that is harder to understand

22:20.120 --> 22:26.680
or if it is easy for you to understand please reconsider if your abstractions are too much.

22:26.680 --> 22:33.000
Maybe you ain't going to need it. Right. So we have a hotel reader and it owns a reader

22:33.000 --> 22:41.320
builder and inside of our new method we initialize the CSV hotel reader and we implement hotel

22:41.320 --> 22:48.320
reader down here. The single method called read and we say self.reader builder this is

22:48.320 --> 22:53.800
the code that we saw before we just put it here this is our CSV parser the initialization

22:53.800 --> 23:00.760
of it and then we return a reader.into the serialized hotel map and this is where we

23:00.760 --> 23:10.440
map the errors. Right. Does it look great? I don't know depends on someone's nodding.

23:10.440 --> 23:19.000
We need to talk but it's certainly nice to use I guess. Now we can say for hotel in hotels.read

23:19.000 --> 23:28.200
file. Should hotels know about files? Maybe not. But it's great if you go one step further

23:28.200 --> 23:33.640
and you implement iterator on it and now you can say for hotel in hotels. Alright we're

23:33.640 --> 23:38.840
getting somewhere from a user's perspective that is really great. But remember we're talking

23:38.840 --> 23:43.200
about application code. There's probably code that you earn money with. It's not a

23:43.200 --> 23:49.240
library function that is used by thousands of people. It's your simple CSV parser and

23:49.240 --> 23:54.360
now we just blew it up into something that is harder to understand. Do you really need

23:54.360 --> 24:05.640
this? Well I don't think so. I don't know what this person on the bull does but it certainly

24:05.640 --> 24:12.760
looks confusing to me and this is what people think when they see the top signature. I know

24:12.760 --> 24:19.520
kind of you wanted to optimize it a bit but at what cost? Right whenever you sit here

24:19.520 --> 24:25.760
and you think oh I should implement JSON support and you don't do it for fun. Start thinking

24:25.760 --> 24:34.360
if you really need those subscriptions because they can haunt you. Most of the time they

24:34.400 --> 24:41.280
don't have no need of it. I don't know what sort of animal this is. Is it a lion cat or

24:41.280 --> 24:47.520
something but it's kind of strapped to a cannon and it doesn't look too happy to me.

24:47.520 --> 25:02.640
I don't want this. Probably you're not going to need it. As a side note another thing probably

25:02.720 --> 25:07.600
you shouldn't do too often are macros. There are traits out there that excessively use

25:07.600 --> 25:14.600
macros. What do I mean by macros? Macro rules but also macro derives and these are great

25:14.600 --> 25:21.040
but they come at a cost and the cost could be compile times. Just yesterday I talked

25:21.040 --> 25:26.680
to Daniel Kerkman who I don't know is he here? He's not here. But thanks for the tip.

25:26.680 --> 25:32.280
He has a situation at work where compile times just blow up because of macros and for you

25:32.400 --> 25:37.320
it might be easy to write but for other people it might be hard to use. Maybe you want to

25:37.320 --> 25:44.160
prefer traits over macros if you can. That was the second horseman fighting excessive

25:44.160 --> 25:50.840
abstraction. How can it be done? If you find yourself in this situation keep it simple.

25:50.840 --> 25:56.800
Avoid unnecessary complexity. Just think that the person that will maintain the code is

25:56.840 --> 26:03.440
not a mass murderer but your best friend. Do you treat friends like this? Watch newcomers

26:03.440 --> 26:10.000
use your code. That can be humbling. Ensure that abstractions add value. Yes you can add

26:10.000 --> 26:17.000
a layer of abstraction. Does it add value? That's up to you. Decide and don't add introductions

26:17.520 --> 26:23.000
that you might need in the future. Add them when you need them. Right. Two off the list

26:23.040 --> 26:28.480
we have two more to go. Next one is premature optimization. This is for a lot of people

26:28.480 --> 26:34.600
in here because you are C and C++ programmers. I'm looking at you right now because 90%

26:34.600 --> 26:41.600
of you raised your hand. I see a lot of people from C and C++ come to Rust with this mindset

26:41.960 --> 26:48.960
with these patterns. What are the patterns? They optimize before it's necessary. This

26:49.520 --> 26:56.520
is important different from adding too many layers of abstraction. Optimization in this

26:56.800 --> 27:03.800
case means profiling is not done but instead you kind of try to outsmart the compiler and

27:04.400 --> 27:09.480
you think about performance optimizations way too early before you even need it. Did

27:09.480 --> 27:13.560
I even tell you how big that CSV file was in the beginning? How many entries does it

27:13.600 --> 27:20.360
have? You don't know. Maybe you should not optimize for it right away. They use complex

27:20.360 --> 27:27.360
data structures where simple ones would suffice. For example we saw the hash map with the three

27:27.840 --> 27:32.960
tuple elements. These are things that are kind of unravel and then it ends up being

27:32.960 --> 27:39.960
a mess not very idiomatic and arguably not even faster. And they also have a tendency

27:39.960 --> 27:46.960
to neglect benchmarks. Some red flags. Quotes you might have heard. Without a lifetime this

27:47.800 --> 27:54.320
needs to be cloned. Ignore that. If you know that you have a performance problem then you

27:54.320 --> 28:01.160
can think about lifetimes. It's fine to clone. Let me help the compiler here. The box is so

28:01.160 --> 28:07.840
much overhead. I use B3Map because it's faster than hash map. No need to measure I've got

28:07.880 --> 28:14.880
years of experience. They love the term zero cost abstraction or zero copy. Actually it

28:15.600 --> 28:21.600
should be zero cost in here. And they hate allocations. Whenever they look at an allocation

28:25.600 --> 28:32.600
they feel terrified and they bend over backwards to make that program faster. So whether this

28:33.160 --> 28:40.160
is the developer or the compiler and vice versa is up to you. I've been in both situations.

28:44.040 --> 28:51.040
They turn a completely simple hotel struct with a couple string fields which are owned

28:53.320 --> 28:57.840
yes they live on the heap. Do something that lives on the stack and has a lifetime. And

28:57.920 --> 29:04.040
every time you use a hotel you have to carry on the weight of the lifetime. Well does it

29:04.040 --> 29:08.800
matter for this one particular case? Probably not. But then you look at other places of

29:08.800 --> 29:15.080
the code base and you see that they kind of reverted your changes. They made what you

29:15.080 --> 29:21.160
introduced your hard won knowledge about the abstractions and they took them away. Now we

29:21.160 --> 29:27.040
start to index into our data structure again. We use string split again. We go backwards.

29:27.080 --> 29:34.080
We've been there before. It is super fragile. Again we are going backwards. Now let me play

29:35.560 --> 29:41.440
a little game here. Since there are so many C and C++ programs in here I expect you to

29:41.440 --> 29:48.440
answer this. What is the bottleneck? This is a very famous medieval game who wanted to

29:49.440 --> 29:56.440
be a millionaire. What is the bottleneck? Is it CSV parsing? The DC realization of our

29:57.440 --> 30:04.440
entries. Is it string object creation after we DC realized it? We put it into a hotel

30:04.880 --> 30:11.000
struct. Is that the bottleneck? Is it floating point operations when you parse the price?

30:11.000 --> 30:18.000
Or is it hash map access? Who's for A? Some shy hands? Don't be shy. Who's for B? Okay.

30:18.000 --> 30:25.000
Nice. Who's for C? No one. And who's for D? The hash map. Nice. The correct answer is

30:37.560 --> 30:44.560
you forgot to run with release. How do you find the actual performance improvements?

30:44.560 --> 30:51.560
There's just one correct answer and it is measure. Profile. Use the tools. Cargo flame

30:52.560 --> 30:59.560
graph. Cool thing. You will see that in a second. Use benchmarks. There's criteria on

31:00.560 --> 31:07.560
Nick still in the room? Nicolet? No. His benchmarking tool. Divan. Pretty great. Use it. Okay. I

31:08.560 --> 31:15.560
will give you one example. Let's look at a flame graph of our initial program. The one that a

31:16.560 --> 31:23.560
junior developer could write in two hours. What is the bottleneck? There is no bottleneck. This

31:24.560 --> 31:30.560
is the setup of our flame graph itself. This is the profiler setup. The code itself is

31:30.560 --> 31:36.560
negligible. Negligible, I guess. And why is that? Again, because I didn't tell you how big the

31:37.560 --> 31:41.560
fire was, do you think I can come up with thousands of alliterations for hotels? No. So I added

31:42.560 --> 31:48.560
100 entries. There is no bottleneck here. Okay. You might say, but okay. What if the fire grows?

31:49.560 --> 31:57.560
Let's add a million entries. Okay. Oh, this is still 120 records. So let's add more. This is a

31:57.560 --> 32:06.560
million. You probably ain't going to read it. Let's increase it to 10 million. And indeed, deserialization

32:07.560 --> 32:14.560
of the struct takes most of our time. Okay. If we look a little closer, it says,

32:15.560 --> 32:21.560
serde deserialize deserialize struct. Okay. We have some memory movement going on. Let's take a baseline.

32:22.560 --> 32:31.560
That is our baseline. This is what it takes. 34 seconds. Okay. Now, let's say we kind of want to prove our

32:32.560 --> 32:36.560
C and C++ developer wrong. Does this other abstraction that we added for the hotel struct really add that much

32:37.560 --> 32:43.560
overhead? No. It's the same. It's like 34 seconds still. Oh, actually, this is the part where we remove the

32:43.560 --> 32:53.560
unnecessary fields. But we can go further. We can say, yeah. Here we have a little safer version. We don't index,

32:54.560 --> 33:05.560
but we say nth.1. And we have 32 seconds. Now, our bottleneck is append string. String appending. Okay. I think there's

33:05.560 --> 33:18.560
something that we can fix. Well, okay. Maybe this is not really that readable. But what we do is we split now by a

33:19.560 --> 33:26.560
string. And instead of doing an allocation where we append to our string over and over again, we use this pattern

33:26.560 --> 33:37.560
matching here. And this reduces the runtime by 30% already because we save on allocations. Now, if we try to profile this

33:38.560 --> 33:47.560
code again, where's the bottleneck now? Read until. Okay. What is that about? We have a lot of memory movement going on.

33:48.560 --> 33:57.560
And now we reach a point where the disk becomes the bottleneck. We can use an M-map for this. Now, remember, we are talking about

33:58.560 --> 34:05.560
performance and maybe you should not do those optimizations, but prove a C and C++ program were wrong and they are in tuition. And then you see that

34:06.560 --> 34:12.560
the bottleneck might be solved elsewhere. Now we are at 30 seconds by changing like four or five lines from the entire program, not the

34:12.560 --> 34:22.560
entire thing. We can keep using our abstractions. That's the main point. Here we use an M-map. That's a memory map in the kernel. We save on allocations.

34:23.560 --> 34:35.560
30 seconds. Okay. What if we wanted to do more? It's hard to read, but now we reach the point where in fact the hash map is the bottleneck. And one more step to improve the

34:35.560 --> 34:45.560
code would be to split it up into multiple chunks. You can use rayon. You can now finally use a better hash map like a hash map. And we are down to 3.5 seconds.

34:46.560 --> 34:56.560
And we did that not by guessing, but by profiling. Now if we want to run a profile, it looks different again. Very different. These are the individual chunks that we managed to split up.

34:56.560 --> 35:09.560
We went from 40 seconds to three or four seconds in a couple slides and with few changes. And the point is don't guess, measure. This is the worst part that C developers bring into Rust.

35:10.560 --> 35:22.560
They think everything is a performance overhead. And if this challenge, by the way, looked very similar to the one billion row challenge, this is why it was inspired by it. And it is very similar. Read it up. It's kind of fun.

35:23.560 --> 35:37.560
We did something similar for hotel data. But the more important point here is how can we fight premature optimization? Measure, don't guess. Focus on algorithms and data structures, not micro-optimizations.

35:38.560 --> 35:49.560
More often than not, if you change from a vector to a hash map, this will be way, way more efficient than if you remove your little struct. And if you add lifetimes everywhere.

35:50.560 --> 36:06.560
You can get carried away pretty quickly and Rust encourages you to do so, but it also has the tooling to fight it. Be more pragmatic. Focus on readability and maintainability first and foremost. Use profiling tools to make informed decisions.

36:07.560 --> 36:33.560
You covered all of that. Your code is idiomatic. It is fast. You didn't overdo it. What is missing? Well, the entire rest. Do you have tests? Do you have documentation? Is your API too large? Does your code lack modularity and encapsulation?

36:34.560 --> 36:47.560
These are things that I see from people that are like the lone wolf coders. They know all about Rust, but what they are not really good at is the rest. Explaining the differences to their code maintainers.

36:48.560 --> 36:55.560
And writing documentation. Not about the what, but not about the how, but the what. What does your program do?

36:56.560 --> 37:11.560
Some things they say. It compiles. My work is done here. The code is documentation. Let's just make it all pop. I'll refactor that later, which never happens. Let's look at that code again. This is our first version junior programmer. Three hours.

37:12.560 --> 37:19.560
Okay. How do we test that? It's kind of impossible because this is one big binary, one main. How would we test that?

37:20.560 --> 37:30.560
Well, I guess the question is what do we want to test? Well, first off, I would say let's add a test for parsing the entire thing can be a very simple, true test.

37:31.560 --> 37:43.560
But if we refactor it such that we have a function that parses cities, now we can start to introduce a path here and do the parsing. And this is where the parsing logic is, by the way.

37:43.560 --> 37:52.560
We split it up into a main and the parsed cities. Great. This is our first test. Very crude, but we get to a point where suddenly we can test our changes.

37:53.560 --> 38:00.560
We create a temporary directory. We have a path and then we write into a file and that's it. The parsing is done. Great.

38:01.560 --> 38:09.560
If we wanted to make it a little better, instead of passing in a path, we pass in something that impels read. Now we don't need to create files like here.

38:09.560 --> 38:19.560
Instead, we can have our input as a binary blob. And these are simple things. Add some documentation, add some tests. It's not that hard.

38:20.560 --> 38:38.560
And in order to fight a mission, what you need to do is write more documentation, write unit tests, use tools like Clippy and cargo UDAPs, set up CI CD so that you can handle your changes, create releases, use release please, Marco, greetings go out to you, and keep a change lock of what you changed.

38:40.560 --> 38:50.560
Right. We're getting towards the end. We have seen the anti patterns. You know them now. I hope that you will be able to, you know, see them in your code.

38:51.560 --> 39:05.560
If you want to learn more, there are some other talks that were given here at FOSSTEM and other places. You might want to check them out. Maybe I can put the slides somewhere. And that is all I have to say. Thank you.

39:09.560 --> 39:10.560
Thank you.

