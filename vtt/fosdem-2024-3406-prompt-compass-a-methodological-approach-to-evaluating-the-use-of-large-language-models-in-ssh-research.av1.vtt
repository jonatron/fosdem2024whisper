WEBVTT

00:00.000 --> 00:09.640
Good morning everybody. I am Eric Borra. I am an assistant professor in journalism and

00:09.640 --> 00:14.120
new media at the University of Amsterdam. I have a background in artificial intelligence

00:14.120 --> 00:20.440
and I have been a tool maker with the Digital Medicine Initiative for about two decades now.

00:20.440 --> 00:25.720
And one of the reasons why I make tools is to understand new technologies. And today

00:25.720 --> 00:31.280
I will talk about large language models and particularly their use in social science and

00:31.280 --> 00:39.120
humanities research. So who of you has used chatGPT? I think most of the Western world actually

00:39.120 --> 00:45.920
has used chatGPT in the last year. ChatGPT is based on a large language model where you ask a

00:45.920 --> 00:51.600
question and get an answer. But social science and humanities researchers have also found that

00:51.600 --> 00:58.400
you can just send it instructions. And you can instruct it to do all kinds of things such as

00:58.400 --> 01:04.000
sentiment analysis. So you prompt. It's the way of interacting with large language models. You

01:04.000 --> 01:09.360
specify a prompt and you say classify this tweet. You input a tweet and it will give you a nice

01:09.360 --> 01:16.120
classification of that tweet. You can also extract entities and actors. You can include topics and

01:16.200 --> 01:22.760
teams. So you have the prompt I just showed. You enter a New York Times article and it will

01:22.760 --> 01:26.760
extract all these things for you. It will extract country names, organization names, people names,

01:26.760 --> 01:34.520
specific teams and topics. And it's actually pretty good at this. At least chatGPT is. And I'll

01:34.520 --> 01:41.560
discuss other models in a second. And it's not just named entity extraction or simple classification.

01:42.200 --> 01:52.520
It also, like, somehow extracts teams and it can abstract from the text. And researchers have been

01:52.520 --> 01:58.920
using this, for example, to extract narratives from posts. So here's a very complex prompt, which you

01:58.920 --> 02:06.120
can see in the slides after the talk. But there they use this prompt to go through many, many Reddit

02:06.120 --> 02:14.040
posts, hundreds of thousands, to find out whether there were any conspiracy theories in there. And

02:14.040 --> 02:22.440
they devised a prompt to draw out these narratives. And they actually found that LLMs worked really

02:22.440 --> 02:28.520
well in extracting conspiracy theories as well. So researchers have been using this and they have

02:28.520 --> 02:33.880
been looking at all the tasks that are typically done in social science and humanities and are

02:33.880 --> 02:43.400
starting to test whether LLMs can help in doing these tasks. And there has been a lot of research

02:43.400 --> 02:48.840
in the last year, especially 2023. This is just a really small snippet of this research.

02:51.160 --> 02:59.560
But this research also comes with problems, which I'll touch upon in a bit. But their use is also

02:59.560 --> 03:04.280
understandable, the use of large language models in social science and humanities research, because

03:05.720 --> 03:11.240
they seem to ease and speed up previously difficult and laborious tasks, such as classification,

03:11.240 --> 03:17.320
extraction summarization, and so forth. And they're actually employed as junior research assistants.

03:19.400 --> 03:26.840
Now, while this may seem useful somehow, a lot of people seem to be using chat GPT. Actually,

03:26.840 --> 03:32.440
all the papers I've just shown, they're based on chat GPT. And chat GPT comes with problems,

03:32.440 --> 03:38.120
because it's a platform service. And platform services, as I guess all of you know, are volatile

03:38.120 --> 03:43.480
black boxes. You don't know what's going on in the back end when they update their model, when they

03:44.440 --> 03:50.200
align something differently or sensor or whatever, whether it's getting dumber or not. You basically

03:50.200 --> 03:57.240
don't know. Chat GPT is also very expensive. If you're using the API, you pay for a request.

03:58.600 --> 04:05.720
This is one research project by Miguel Escobar of Arela, who calculated that to process the one and

04:05.720 --> 04:15.880
a half million news items he had in his corpus. He'd need $150,000. It's just too expensive.

04:16.600 --> 04:23.000
There are, of course, also privacy concerns with chat GPT and other platform models, whereby

04:23.000 --> 04:29.320
with chat GPT we know that whatever you input into chat GPT is also used as training data for the

04:29.320 --> 04:37.800
LLM. Users have also found personal and private information resurfacing from other users, etc.

04:38.520 --> 04:45.320
So if you think in terms of open science, replicable research and ethical

04:46.200 --> 04:51.800
research with privacy concerns, you basically cannot use these models, even though you can go to

04:51.800 --> 04:58.120
privacy.openai.com and state that you don't want your inputs to be used as training data.

04:59.880 --> 05:07.240
Well, so how to deal with this? Can we use LLMs in social science and humanities research?

05:08.520 --> 05:15.560
Fortunately, the answer is yes. Chat GPT is not the only model available. You probably heard of

05:15.560 --> 05:21.960
Google Bart or Gemini. You may have heard of Clodes, which are other platform models, but there are

05:21.960 --> 05:27.000
also a lot of publicly available models. All the yellow ones highlighted here, and this is only,

05:28.280 --> 05:33.880
I think, until the second quarter of 2023. Since then, a lot of new models have appeared,

05:33.880 --> 05:42.440
most notably Mistral, for example, the French model, or Mistral, the 8x7B model of Mistral,

05:42.440 --> 05:48.920
which are really good and are catching up on the performance of chat GPT. Publicly available,

05:48.920 --> 05:55.000
however, doesn't mean that it's open, that it's open source, that it's free, because there is this

05:55.000 --> 06:01.960
whole infrastructure and apparatus to train models, to fine tune models, to use models in your own

06:01.960 --> 06:09.560
work. And you see all the orange and red here. Most of these models aren't open,

06:12.760 --> 06:19.640
or have different licenses, etc. So it's not, yeah, you can't just use another one. You need to think

06:19.640 --> 06:26.280
about these things. Two other considerations before I go to the actual tool. If you use

06:27.240 --> 06:32.840
the same prompt in different models, you'll get different results. And this is actually

06:33.960 --> 06:41.400
the same prompt in a series of image models, but you can visually see how results may differ.

06:42.840 --> 06:50.600
This is something to take into account. And last but not least, there are a few technical

06:50.680 --> 06:58.680
parameters in using LLMs. And one way to control differences or variability in the output of LLMs

06:58.680 --> 07:03.800
is through the so-called temperature parameter. If you set the temperature to zero, you'll always

07:03.800 --> 07:08.840
get the same result. The most probable or most likely outcome was if you increase the temperature,

07:09.960 --> 07:19.000
there is a chance of less likely outputs to be included in the results. But again, all these

07:19.000 --> 07:26.120
papers, none of them mention temperature, whilst it's a very important parameter. Last but not

07:26.120 --> 07:32.280
least, this is work I've been doing with Maichieu, small syntactic differences in a query that's

07:32.280 --> 07:37.720
semantically the same may lead to different results as well. So you need to test your prompt for

07:37.720 --> 07:45.720
robustness or consistency. So summarizing, open AI or platforms like chat GPT are volatile black

07:45.720 --> 07:50.360
boxes that cost a lot of money. There are issues of privacy and security. There are different models

07:50.360 --> 07:56.120
which have different licenses, which have different results. LLMs are not deterministic and small

07:56.120 --> 08:01.480
changes in prompts may lead to different outputs. So we need research interfaces where we can

08:01.480 --> 08:07.640
control for such things. We want to be able to do open science with LLMs. So how do we take into

08:07.640 --> 08:14.360
account the volatility of platforms, the robustness of research and its replicability and explainability?

08:15.320 --> 08:21.560
And this is where I started tinkering with a tool I called prompt compass, or actually I had chat

08:21.560 --> 08:29.960
GPT call it prompt compass. And it's a research interface. It's not a chat interface. It allows

08:29.960 --> 08:36.680
you to take into account all these considerations that I've put up. So you can choose various local

08:36.680 --> 08:42.280
models. It has default parameters for replicability. It contains a library of research prompts,

08:42.360 --> 08:47.880
allows for batch processing user input, and allows you to evaluate prompt model combinations.

08:47.880 --> 08:57.080
Do I still have some time to demo this? Cool. Let's do that. So prompt compass is available

08:57.960 --> 09:06.360
on GitHub. We also run it at one of our servers. The design doesn't really shine on this beamer,

09:06.360 --> 09:13.720
but anyway. Here you can select various models which are loaded from hugging face. You can easily

09:14.360 --> 09:23.080
add a new model and select one of these. You can find out more by clicking on the model card

09:23.080 --> 09:28.280
and then see what the model was made for, how it was trained and so forth. All these models are

09:28.280 --> 09:37.000
loaded from hugging face, which is like the GitHub for language models, but we can also

09:38.120 --> 09:44.600
choose GPT for where you and then or any other model of open AI or platforms and then

09:45.160 --> 09:51.960
enter your API key and go over that. You can go into the settings which are default sets to

09:52.760 --> 09:59.960
replicability. There's a little explanation of it. There are a lot of prompts extracted from the

09:59.960 --> 10:10.680
literature and from actual research. And you can input your own prompts like this. You can or you

10:10.680 --> 10:20.600
can adapt existing prompts. And then you can provide user input either line by line or upload a CSV.

10:21.400 --> 10:29.640
And then if you click submit, the selected model will be loaded. And each of the lines will be run

10:29.640 --> 10:36.920
through the model with the indicated prompt. So in this time we chose sentiment analysis,

10:37.800 --> 10:41.320
which says you're in advanced classifying AI, you're tasked with classifying the sentiment of a

10:41.320 --> 10:47.880
text, which can be either positive, negative or neutral. And this is where we'll input or loop

10:47.880 --> 10:53.480
over our inputs. So in this time the user is happy, it's classified as positive. When user is just

10:53.480 --> 10:58.440
a user, it's classified as neutral and the other user is a liar, it's classified as negative.

10:58.440 --> 11:03.400
And this tool is not the end all go all tool for working with LLMs, but it is a way to test

11:03.960 --> 11:09.320
models, to test parameters, to test prompts, to test the robustness of prompts and to

11:10.920 --> 11:14.680
get all this into easily digestible outputs CSV.

11:18.440 --> 11:25.560
So far for the demo. The technology is used, it's really simple. I'm not like a hardcore coder,

11:25.560 --> 11:33.080
but I'm like more of a tie some stuff together coder. Streamlit is a Python interface for

11:33.800 --> 11:40.840
making easily making web applications of machine learning tasks. Lang chain is a very bloated

11:40.840 --> 11:47.240
way to easily connect LLMs and to work with LLMs and prompts and Huckingface is the place where

11:47.320 --> 11:55.640
all these LLMs are stored. We run this on a 24 gigabyte GPU, which is a bit expensive,

11:55.640 --> 11:59.080
but it's not very expensive. Like each research group should be able to get one.

12:01.000 --> 12:07.960
And I mean, yeah, so to get back to my rent against platforms, making LLMs

12:08.600 --> 12:16.200
locally accessible makes them stable and replicable. But we cannot run the biggest models unless we

12:17.000 --> 12:22.200
have access to bigger infrastructure, which we sometimes have. But this is really meant for

12:22.920 --> 12:30.280
researchers that want easy access to local models. I made a video tutorial,

12:30.280 --> 12:35.080
tutorial which you may want to watch. And maybe there's still room for questions.

12:35.480 --> 12:37.800
Just three minutes.

12:57.320 --> 13:04.360
Atlas TI is a rather big and well-known software package for qualitative coding, right?

13:05.720 --> 13:14.600
I'm not sure why they chose to only use Chatchi PT. But yeah, I mean, we've had experience with

13:14.600 --> 13:19.560
local LLMs that you can actually also do similar things with extraction and coding.

13:21.160 --> 13:27.800
So I would definitely be in favor of actually using local LLMs. On the other hand, if you have

13:27.800 --> 13:34.680
proper validation procedures such as intercoder reliability and F1 scores, etc.,

13:34.680 --> 13:40.040
you can get a long way with Chatchi PT because human coders are also fallible and may also

13:40.040 --> 13:46.600
be different today than they were a few weeks ago. So it's not that it's not possible or not

13:46.600 --> 13:51.400
usable at all, but you should be prudent, I think.

13:51.880 --> 14:02.120
You said it's mostly for testing the models, but how big of an input file do you think?

14:03.400 --> 14:10.520
We've run this on more than 100,000 lines of CSV, I think even more. So in the digital methods

14:10.520 --> 14:15.960
winter school and past summer school, we actually run a lot of prompts through it.

14:16.760 --> 14:26.680
And it seemed to work. Sorry. It was asked how big of CSV files it could handle,

14:27.800 --> 14:33.000
and I answered more than 100,000. So it's actually also used in production for

14:33.640 --> 14:38.680
relatively small-scale qualitative research, but it's not limited to things you could do manually

14:38.680 --> 14:41.960
anyway. So let's switch.

