WEBVTT

00:00.000 --> 00:05.000
The stage is yours.

00:06.920 --> 00:08.720
Thank you very much, Martin.

00:08.720 --> 00:12.880
So as Martin told already to you, I'm Andrea

00:12.880 --> 00:16.320
and I'm a software engineer at Genetio.

00:16.320 --> 00:19.040
We are trying to build the cloud platform

00:20.000 --> 00:21.880
for web applications.

00:21.880 --> 00:25.320
So our user should build web and mobile applications

00:25.320 --> 00:27.880
and deploy them to us.

00:28.880 --> 00:31.680
Just before starting, I want to set some expectations.

00:31.680 --> 00:35.320
So first of all, this talk is going to be a talk

00:35.320 --> 00:38.280
of the challenges and solutions we've come up with

00:38.280 --> 00:42.120
by running real node applications on Unicernals.

00:42.120 --> 00:44.760
And secondly, I really hope that this presentation

00:44.760 --> 00:46.600
will spark some discussions.

00:46.600 --> 00:49.240
I know that the Unicernal community is active

00:49.240 --> 00:53.280
and creative and me and my colleagues are going to be around

00:53.280 --> 00:57.000
after the presentation for us to speak

00:57.040 --> 01:00.320
if you have use cases, questions, challenges

01:00.320 --> 01:05.320
that you've tackled and you want to chat with us about them.

01:05.440 --> 01:07.440
So let's get down to business.

01:11.320 --> 01:14.600
Okay, so before starting to build a platform,

01:14.600 --> 01:17.840
we really needed to put down some guidelines

01:17.840 --> 01:22.440
and the vision that we want to implement with that platform.

01:22.440 --> 01:24.080
So is this sound okay?

01:24.080 --> 01:26.240
Because it sounds weird to me.

01:26.520 --> 01:27.360
Okay.

01:27.360 --> 01:28.840
It sounds good, but that's not the Unicernal.

01:28.840 --> 01:30.880
Okay, okay, thanks.

01:31.800 --> 01:33.560
So back to vision.

01:33.560 --> 01:36.920
So first of all, we really want to optimize resources

01:36.920 --> 01:39.480
like power consumption, memory, CPU

01:39.480 --> 01:41.520
and costs for our users.

01:42.520 --> 01:47.520
Secondly, we don't want to throw away response time

01:47.760 --> 01:51.080
for that optimization that we did.

01:51.080 --> 01:55.000
So we want to have a snappy response for our clients.

01:55.040 --> 01:59.080
So response time is very important to us.

01:59.080 --> 02:02.480
And lastly, we want an easy to use, secure

02:02.480 --> 02:04.360
and auto scaling platform.

02:04.360 --> 02:07.840
That means that we want to take the burden

02:07.840 --> 02:11.040
from the developer to where I deploy this application.

02:11.040 --> 02:15.000
How should I scale it and stuff like this?

02:15.000 --> 02:17.040
This is the job of the cloud platform

02:17.040 --> 02:19.800
and we want to provide that to the end user.

02:21.480 --> 02:23.240
So to take all the boxes,

02:23.240 --> 02:25.120
we decided to try Unicernals

02:25.120 --> 02:27.960
in a function as a service environment.

02:27.960 --> 02:30.160
Function as a service essentially means

02:30.160 --> 02:33.000
that when we have an incoming request,

02:33.000 --> 02:37.040
we spawn a VM that will be bounded in time.

02:37.040 --> 02:40.680
So it will leave for a few moments, for a few minutes, sorry.

02:40.680 --> 02:44.000
It will handle that request and send back the response.

02:44.000 --> 02:46.720
And we are doing this with the Unicernals.

02:46.720 --> 02:51.720
So essentially we have our orchestrator down here

02:51.880 --> 02:54.360
and then for the request,

02:54.360 --> 02:57.360
we are spawning the Firecracker with OSV,

02:57.360 --> 02:59.440
that is the Unicernal that we are using,

02:59.440 --> 03:02.280
with Node.js and at the top level,

03:02.280 --> 03:04.140
we have the user application.

03:06.120 --> 03:09.720
But still this has some challenges,

03:09.720 --> 03:13.520
unique set of challenges than the classic Linux container

03:13.520 --> 03:15.440
and classic long-living server.

03:15.440 --> 03:18.560
So our challenges are reducing the call starts

03:18.560 --> 03:21.300
because we are spawning up many VMs,

03:21.300 --> 03:22.900
essentially every few minutes,

03:22.900 --> 03:25.860
we are spawning the VMs if we have incoming requests.

03:25.860 --> 03:29.300
We have to somehow reduce the boot time for these VMs.

03:29.300 --> 03:33.940
So we have to boot it very quickly

03:33.940 --> 03:36.340
or give it a try to another mechanism

03:36.340 --> 03:37.900
to improve this call start.

03:38.860 --> 03:41.420
Secondly, because we are a multi-tenant platform,

03:41.420 --> 03:44.180
we want to somehow be able to upgrade the Unicernal

03:44.180 --> 03:47.940
or do patch, patches it at the Unicernal level

03:47.940 --> 03:50.980
without really iterating through each image

03:50.980 --> 03:55.380
for each of our users and redeploy everything.

03:55.380 --> 03:57.380
And lastly, of course, we want security

03:57.380 --> 03:58.900
and we want process isolation.

03:58.900 --> 04:01.300
That means we don't want application one

04:01.300 --> 04:04.220
to be able to access resource from application two.

04:05.420 --> 04:08.140
So we've tackled first of all the call start,

04:08.140 --> 04:11.180
which is a big problem in the function of the service world.

04:11.180 --> 04:15.100
So to tackle that, we leverage snapshots,

04:15.100 --> 04:17.020
as everybody does probably,

04:17.020 --> 04:19.180
and we did snapshots as following.

04:19.180 --> 04:21.460
We are booting up the Unicernal.

04:21.460 --> 04:23.740
We are starting the Node.js process

04:23.740 --> 04:26.860
and immediately after we start the Node.js process,

04:26.860 --> 04:30.100
we pause and at that moment we are creating a snapshot

04:30.100 --> 04:32.140
and store it for later.

04:32.140 --> 04:34.660
When we want to spawn a new VM,

04:34.660 --> 04:37.220
what we do essentially is we start the firecracker,

04:37.220 --> 04:40.700
which helps us load the snapshot

04:40.700 --> 04:45.260
and then we are attaching a new disk with the user code.

04:45.260 --> 04:48.700
We are mounting it and we are importing it

04:48.700 --> 04:51.020
into the Node.js code that was already started

04:51.020 --> 04:52.020
in the snapshot.

04:52.020 --> 04:55.860
This will help us with the second challenge that we have,

04:55.860 --> 04:57.580
the upgrading the Unicernal

04:57.580 --> 04:59.260
and you'll see in a moment how.

05:01.380 --> 05:04.620
To optimize even further, what we are doing

05:04.620 --> 05:08.700
is that we are not really waiting for requests to come

05:08.700 --> 05:11.740
and to then start the firecracker load the snapshot

05:11.740 --> 05:12.580
and so on.

05:12.580 --> 05:16.340
We have already a pool of warmed VMs that are started

05:16.340 --> 05:18.820
but they are not scheduled on the CPU.

05:18.820 --> 05:21.060
They are just logged in the memory

05:21.060 --> 05:23.260
and then we have an incoming request.

05:23.260 --> 05:26.540
We just take such one VM from the pool,

05:26.540 --> 05:28.880
we attach the user code, we handle it

05:28.880 --> 05:31.420
and then we give back the response to the client.

05:33.460 --> 05:35.980
Going back to upgrading the Unicernal image.

05:35.980 --> 05:38.900
So as I told you, we are a multi-tenant platform

05:38.900 --> 05:42.700
so we expect to have thousands of user applications

05:42.700 --> 05:44.540
that are running on our platform.

05:44.580 --> 05:48.580
So somehow we don't want to embed the user code

05:48.580 --> 05:51.660
into the Unicernal image because if we do that,

05:51.660 --> 05:55.780
we will have to rebuild each image of each of our user

05:55.780 --> 05:59.540
once we do a Unicernal upgrade or a patch or a bug fix.

05:59.540 --> 06:01.420
So what we are doing is that we are creating

06:01.420 --> 06:05.780
a single snapshot with just with the Unicernal

06:05.780 --> 06:08.820
and with Node runtime and as I already told you,

06:08.820 --> 06:10.660
we are attaching the user code later.

06:10.660 --> 06:12.860
So essentially when we are doing an upgrade,

06:12.860 --> 06:15.340
we just have to upgrade that base image

06:15.340 --> 06:18.060
and each VM that is spawned afterwards,

06:18.060 --> 06:20.700
it will reference to that base image.

06:20.700 --> 06:24.820
So this is why we are mounting user code afterwards

06:24.820 --> 06:28.700
and this is how we can enable OSV and Unicernal upgrade

06:28.700 --> 06:31.980
without really redeploying everything on our platform.

06:33.900 --> 06:36.420
And lastly, we are doing security and isolation

06:36.420 --> 06:38.820
using Firecracker Jailer.

06:38.820 --> 06:42.180
This helps us running each process sandbox,

06:42.180 --> 06:44.540
Firecracker Jailer allows us to have

06:44.540 --> 06:46.740
different network namespaces in such a way

06:46.740 --> 06:49.820
that we are not using the same network interface

06:49.820 --> 06:51.740
with each process.

06:51.740 --> 06:53.860
We have different file systems

06:53.860 --> 06:56.020
and different process namespaces.

06:56.980 --> 07:00.860
And also Jailer allows us to limit resources

07:00.860 --> 07:03.780
to make sure that we have some kind of fairness

07:03.780 --> 07:07.340
between VMs because we don't really want one single VM

07:07.340 --> 07:10.500
to eat up the whole CPU or the whole IO bandwidth.

07:10.500 --> 07:12.420
So we can control the IO throughput

07:12.420 --> 07:14.180
and CPU time for each VM.

07:16.660 --> 07:19.540
Because we are running things with real world stuff,

07:19.540 --> 07:22.180
we also find out some bugs,

07:22.180 --> 07:26.540
especially in the first one is in the Node.js V8 compiler,

07:26.540 --> 07:29.540
it used the pop-up instructions incorrectly

07:29.540 --> 07:30.980
and in the privilege mode,

07:30.980 --> 07:34.500
which is the single mode in Unicernal,

07:34.500 --> 07:38.620
with this it enables interrupts and that was not,

07:38.620 --> 07:41.180
we cannot run any Node code essentially,

07:41.180 --> 07:44.340
it was all Node code was affected by this bug.

07:44.340 --> 07:47.540
And also we fixed some bugs in OSV

07:47.540 --> 07:49.460
and we made some contribution upstream.

07:49.460 --> 07:52.500
Those are related to using two file systems

07:52.500 --> 07:54.900
because we need the first file system for the Unicernal

07:54.900 --> 07:56.780
and the second one is the user code

07:56.780 --> 07:58.660
that we are attaching later.

07:58.660 --> 08:03.060
And also we found out some notepossicks compliant

08:03.060 --> 08:06.060
functions in P-treads library.

08:06.900 --> 08:11.060
Okay, now let's talk about metrics.

08:11.060 --> 08:14.540
So as I told you, the most important thing for us

08:14.540 --> 08:17.300
is the request response time,

08:17.300 --> 08:19.900
because we want to make sure that the end user

08:19.900 --> 08:21.900
of our clients are getting their requests

08:21.900 --> 08:23.260
as fast as possible.

08:23.260 --> 08:25.180
So they have a snappy feeling

08:25.180 --> 08:27.140
when they're using the applications.

08:27.140 --> 08:30.620
So we actually benchmark that to see if using

08:30.620 --> 08:33.380
the Unicernal versus Linux container is as fast,

08:33.500 --> 08:38.500
is there is a difference or if there is an improvement.

08:39.940 --> 08:43.260
So first of all, we did a bit of setup.

08:43.260 --> 08:46.300
We are using a client, so essentially the browser,

08:46.300 --> 08:49.460
let's say, the client is in Asia.

08:49.460 --> 08:52.900
And then we are comparing with the standard

08:52.900 --> 08:55.500
functions and service solution from AWS.

08:55.500 --> 08:59.660
And also we have our servers that are running

08:59.660 --> 09:01.980
on the GeneXia infrastructure.

09:01.980 --> 09:03.780
So these are the three actors.

09:03.780 --> 09:07.020
We are comparing AWS Lambda with OSV,

09:07.020 --> 09:08.900
which is a Unicernal solution,

09:08.900 --> 09:11.980
and with a classic Linux container to have

09:11.980 --> 09:16.180
a full comparison, to have a full picture of the problem.

09:16.180 --> 09:17.620
This is the code that we are running.

09:17.620 --> 09:19.220
Essentially it's a Hello World code.

09:19.220 --> 09:20.720
It's more of a ping.

09:20.720 --> 09:21.940
We are just sending a request

09:21.940 --> 09:24.380
and we are getting back a Hello World string.

09:26.100 --> 09:28.140
And these are the actual numbers.

09:28.140 --> 09:30.980
So for a cold call, a cold call means

09:30.980 --> 09:33.380
that there is no VM pre-warmed for us.

09:33.380 --> 09:36.060
So at the moment when we are handling the request,

09:36.060 --> 09:38.580
we also have to wait for the boot time.

09:38.580 --> 09:41.260
And in orange we have Lambda

09:41.260 --> 09:44.860
that was getting us the request back in 300 milliseconds.

09:44.860 --> 09:47.060
And in purple we have the OSV

09:47.060 --> 09:49.060
and in blue we have the Linux container

09:49.060 --> 09:52.860
that are performing like 60 milliseconds.

09:53.820 --> 09:55.860
And then we also have the worm call,

09:55.860 --> 09:59.460
which is for the Lambda is around 60 milliseconds.

09:59.460 --> 10:04.100
And for OSV and Linux is around 30 milliseconds.

10:04.100 --> 10:05.940
So as we can see first of all,

10:05.940 --> 10:10.380
is that the OSV and Linux are mostly performing the same.

10:10.380 --> 10:12.900
And the first question that comes into mind

10:12.900 --> 10:15.460
is why use a Unicernal and why bother with it

10:15.460 --> 10:17.540
if the Linux container is just as good.

10:18.580 --> 10:20.220
But what we cannot see on this graph

10:20.220 --> 10:22.700
is the Linux kernel footprint.

10:22.700 --> 10:26.020
So the Linux kernel takes up much more space in storage

10:26.020 --> 10:27.620
when we are creating the snapshot,

10:27.620 --> 10:28.980
much more space in memory

10:28.980 --> 10:33.260
when we're using the pre-warmed pool VMs and so on.

10:33.260 --> 10:36.140
So the reason we can use Unicernals

10:36.140 --> 10:37.820
is that we are optimizing resources,

10:37.820 --> 10:39.740
even memory and storage and so on.

10:41.900 --> 10:43.460
Next steps for us, first of all,

10:43.460 --> 10:46.020
is to integrate many more canals.

10:46.020 --> 10:47.860
So for now we are just using OSV

10:47.860 --> 10:50.860
because it was the mature project

10:50.860 --> 10:52.460
at the point that we started,

10:52.460 --> 10:56.100
but we want to use many more Unicernals

10:56.100 --> 10:59.180
that are just developing in the community.

10:59.180 --> 11:01.300
And then we also want to add more support

11:01.300 --> 11:03.060
for more programming languages.

11:03.060 --> 11:06.100
We just went full on for Node.js

11:06.100 --> 11:07.740
because it was very popular.

11:07.740 --> 11:09.460
But we also want to add more support

11:09.460 --> 11:13.900
in a way that every web programmer can deploy

11:13.900 --> 11:16.180
its backend code on Unicernals.

11:18.100 --> 11:21.580
As a last call, I want to stay in touch

11:21.580 --> 11:23.980
with the people that are interested

11:23.980 --> 11:25.420
in this kind of project.

11:25.420 --> 11:27.500
I think that this community of Unicernals

11:27.500 --> 11:30.380
is very active and is flourishing

11:30.380 --> 11:32.620
from the contributions that we are all making.

11:34.220 --> 11:35.060
That's all.

11:35.060 --> 11:44.780
Okay, again, time for questions here.

11:44.780 --> 11:45.620
I'll be there.

11:56.060 --> 11:56.820
Hello.

11:56.820 --> 11:57.660
Thank you for the talk.

11:57.660 --> 11:58.500
Very, very interesting.

11:58.500 --> 12:00.020
Just a very simple question.

12:00.020 --> 12:02.420
Can you give me an eyeball on how big

12:02.420 --> 12:04.940
your Unicernal base image for Node.js is?

12:07.740 --> 12:09.220
I don't really have the numbers.

12:09.220 --> 12:12.500
So how big is the Unicernal image?

12:12.500 --> 12:13.340
Yes, so...

12:13.340 --> 12:14.180
For Node.

12:14.980 --> 12:15.820
So the last one?

12:15.820 --> 12:16.820
For Node.

12:16.820 --> 12:17.660
For Node.

12:17.660 --> 12:19.220
I mean what you used in the demo.

12:19.220 --> 12:22.580
So you have some megabytes, the dependencies,

12:22.580 --> 12:24.900
or so adding up to the gigabytes.

12:24.900 --> 12:27.740
To be honest, I didn't look into megabytes.

12:27.740 --> 12:28.660
Under 100.

12:28.660 --> 12:31.620
So I just received the answer in the headphones.

12:31.620 --> 12:33.100
It's megabytes.

12:33.100 --> 12:33.940
Okay, perfect.

12:33.940 --> 12:34.780
Thank you.

12:34.780 --> 12:35.620
Thank you very much.

12:42.380 --> 12:44.260
I may have missed it in the presentation,

12:44.260 --> 12:47.220
but in the benchmark that you showed us,

12:47.220 --> 12:49.460
obviously Lambda was running on the Amazon hardware,

12:49.460 --> 12:53.300
but what was Genesio running on top of?

12:53.300 --> 12:55.860
Genesio is running on top of a bare metal

12:55.860 --> 13:00.860
in another cloud deployment called Host.

13:01.980 --> 13:04.820
So basically we are running, I think, on ARM,

13:04.820 --> 13:06.860
on an ARM server that is bare metal.

13:06.860 --> 13:09.340
So we are building everything up on the ground.

13:09.340 --> 13:10.820
Okay, so there may have been a difference

13:10.820 --> 13:12.780
also in the hardware that was provided by Amazon

13:12.780 --> 13:14.020
and the hardware.

13:14.020 --> 13:16.420
That might be true because the Avisunda

13:16.420 --> 13:18.540
is running Graviton and we don't have

13:18.540 --> 13:21.340
that kind of hardware, of course.

13:21.340 --> 13:22.180
Okay, thanks.

13:22.180 --> 13:23.020
Yeah.

13:23.660 --> 13:27.340
So, more questions, yes.

13:38.740 --> 13:40.860
A bit of access for me.

13:40.860 --> 13:41.780
Thank you very much.

13:41.780 --> 13:43.380
So thank you for the presentation.

13:43.380 --> 13:45.940
I have a kind of a question,

13:45.940 --> 13:49.300
like from someone considering to do serverless

13:49.300 --> 13:51.620
and knowing that there is AWS Lambda,

13:51.620 --> 13:54.300
what is basically your selling point?

13:54.300 --> 13:58.300
Why would one use Genesio versus Lambda?

13:58.300 --> 14:00.500
Given that AWS is a big company

14:00.500 --> 14:03.460
and that Lambda has been running for a long time.

14:03.460 --> 14:05.060
Oh, I see, so there are a lot of reasons

14:05.060 --> 14:08.340
why you would use Genesio over Lambda.

14:08.340 --> 14:12.580
So first of all, we provide a much more easy tooling to use.

14:12.580 --> 14:17.220
So it's more targeting to have a very low learning curve.

14:17.220 --> 14:21.020
So you can just pick up Genesio in the meters of minutes

14:21.100 --> 14:23.940
than AWS Lambda, which is a bit hard to understand

14:23.940 --> 14:25.500
for a first time user.

14:25.500 --> 14:28.780
And secondly, AWS Lambda is not really interested

14:28.780 --> 14:30.940
in resource optimization, at least

14:32.660 --> 14:34.220
at what we tested right now.

14:34.220 --> 14:37.380
So with Unicorus, we want to provide even more

14:37.380 --> 14:39.700
resource optimization and lower costs.

14:42.540 --> 14:43.580
Yeah, hi.

14:43.580 --> 14:44.860
Hello.

14:44.860 --> 14:47.300
Thank you for the presentation.

14:47.300 --> 14:50.220
One of the things that's interesting about Lambda's now

14:50.260 --> 14:52.660
is they have a snapshot mechanism for Java.

14:53.860 --> 14:55.900
Is that something which you're looking to do as well

14:55.900 --> 14:58.700
is to have snapshots for different platforms

14:58.700 --> 15:03.700
and different kind of run times on top of your framework?

15:04.740 --> 15:07.740
Yeah, so we are planning to use the same mechanism

15:07.740 --> 15:09.780
that we use for Node.js.

15:09.780 --> 15:12.820
So basically every program language that we want to support,

15:12.820 --> 15:15.940
it will also have snapshots and it will benefit

15:15.940 --> 15:19.780
from the same mechanism that we use.

15:21.220 --> 15:25.220
Okay, surprisingly, we have still time for questions.

15:26.220 --> 15:27.060
Okay.

15:30.220 --> 15:31.220
Bring them on.

15:32.220 --> 15:33.220
Yeah, I'm gonna kill it.

15:38.220 --> 15:40.220
Very nice optimization.

15:40.220 --> 15:44.220
I mean, with the preheating and the pooling and the others,

15:44.220 --> 15:48.220
are you targeting or are you planning to target also

15:48.220 --> 15:50.220
kernel internals because I mean,

15:50.220 --> 15:52.220
I'm getting this experience from Unicraft

15:52.220 --> 15:55.220
but also I guess in OSB, you could do a lot of fine tuning

15:55.220 --> 15:56.220
there.

15:56.220 --> 15:58.220
Are you looking into that or are you linking only on

15:58.220 --> 16:02.220
how to optimize it via, let's say, external means

16:02.220 --> 16:04.220
because what you showed there is I have this VM,

16:04.220 --> 16:07.220
I pre-warm it, is it something in internals?

16:07.220 --> 16:11.220
For example, bootloaders, boot times are dreaded,

16:11.220 --> 16:13.220
even in your major Unicornals.

16:13.220 --> 16:17.220
Firecracker can be also configured to ditch a lot of devices,

16:17.220 --> 16:20.220
all those sorts of stuff can be fine tuned and optimized

16:20.220 --> 16:21.220
for different use cases.

16:21.220 --> 16:24.220
If you're using Node, some items may not be required,

16:24.220 --> 16:26.220
so you can do, I'm not sure, some postponing.

16:26.220 --> 16:27.220
Are you looking into that as well?

16:27.220 --> 16:29.220
I mean, more of kernel internals?

16:29.220 --> 16:31.220
Yeah, so we are also looking into that.

16:31.220 --> 16:35.220
As I know, we didn't really look into the bootloaders,

16:35.220 --> 16:39.220
but for example, we looked on how to improve the network stack

16:39.220 --> 16:42.220
because it takes up a lot of time to boot up the network stack.

16:42.220 --> 16:43.220
Valdic, is it LWIP?

16:43.220 --> 16:45.220
What is the networking stack of OSB?

16:46.220 --> 16:49.220
The networking stack of it, is it also LWIP

16:49.220 --> 16:51.220
or what is it for OSB?

16:51.220 --> 16:52.220
The next stack.

16:52.220 --> 16:53.220
I'm not sure.

16:53.220 --> 16:55.220
The networking stack, what is it?

16:55.220 --> 16:56.220
I don't think it's the best one.

16:56.220 --> 16:58.220
Okay, that's a good one.

16:58.220 --> 17:01.220
I mean, LWIP is dreadful, but BSD is a better one.

17:01.220 --> 17:02.220
Gotcha?

17:02.220 --> 17:04.220
Yeah, so I know that we are looking into that,

17:04.220 --> 17:07.220
but right now, the things that we already have implemented

17:07.220 --> 17:10.220
are just treating OSB as a black box.

17:10.220 --> 17:11.220
Okay.

17:11.220 --> 17:15.220
Yeah, the next step for us would be to also optimize

17:15.220 --> 17:18.220
the Unicom and the bootloader part.

17:18.220 --> 17:19.220
We need a kind of site for that.

17:19.220 --> 17:21.220
No, no, for sure, for sure, because by many,

17:21.220 --> 17:23.220
Unicom also would take doing a different approach,

17:23.220 --> 17:25.220
but I think that's a very nice spot

17:25.220 --> 17:27.220
and also very challenging to look into.

17:27.220 --> 17:30.220
And Unicom will provide this because you are able to actually

17:30.220 --> 17:31.220
optimize the application itself,

17:31.220 --> 17:34.220
optimize the kernel because of the way it's running.

17:34.220 --> 17:38.220
Did you, I'm not sure, did you use SMP support for this?

17:38.220 --> 17:40.220
What is it currently running single core?

17:40.220 --> 17:42.220
It's running single core.

17:42.220 --> 17:43.220
No.

17:43.220 --> 17:44.220
No?

17:44.220 --> 17:45.220
We got different machine sizes.

17:45.220 --> 17:48.220
No, no, no, I mean the deployment of VM.

17:48.220 --> 17:51.220
Can a VM run multi-threaded with different trends

17:51.220 --> 17:52.220
on different cores?

17:52.220 --> 17:53.220
Yes.

17:53.220 --> 17:54.220
Okay.

17:54.220 --> 17:57.220
So when you deploy something, you can choose the machine

17:57.220 --> 17:59.220
size, you can get inside the size,

17:59.220 --> 18:01.220
you can get on the graph, that's all.

18:01.220 --> 18:04.220
And they have different CPUs now.

18:04.220 --> 18:06.220
Okay, for an actual Unicom only instance.

18:06.220 --> 18:08.220
Okay, awesome.

18:09.220 --> 18:14.220
Okay, let me have a question as well.

18:14.220 --> 18:19.220
I was quite taken by surprise that you don't optimize the image,

18:19.220 --> 18:24.220
which sort of goes in my opinion against the benefits

18:24.220 --> 18:27.220
of Unicom that you really optimize down the image

18:27.220 --> 18:31.220
just specifically to the workload and the APIs and whatever

18:31.220 --> 18:33.220
that the client is using.

18:33.220 --> 18:37.220
So why do you do this trade-off?

18:38.220 --> 18:43.220
Do your customers really think that rebuilding the image

18:43.220 --> 18:46.220
is so cumbersome?

18:46.220 --> 18:53.220
So that means that we have to let our clients tinker

18:53.220 --> 18:55.220
with the Unicom image, right?

18:55.220 --> 18:58.220
So we are actually targeting users that are not that much

18:58.220 --> 19:01.220
into the Unicom stuff, so they are much into writing back

19:01.220 --> 19:04.220
and codes and so on, and they have knowledge into that

19:04.220 --> 19:05.220
direction.

19:05.220 --> 19:09.220
So we'll kind of try to abstract that, so maybe this is why

19:09.220 --> 19:13.220
we did choose to create the base image for them

19:13.220 --> 19:17.220
without really asking them how can we improve it even more.

19:17.220 --> 19:22.220
Okay, so basically you are saying that you are automating it

19:22.220 --> 19:25.220
so you don't want to push that burden to the clients, right?

19:25.220 --> 19:26.220
Exactly.

19:26.220 --> 19:28.220
Okay, that makes sense.

19:28.220 --> 19:33.220
So maybe one final question?

19:33.220 --> 19:34.220
Okay, nothing?

19:34.220 --> 19:35.220
Anyway, thanks for the talk.

19:35.220 --> 19:36.220
Thank you.

