WEBVTT

00:00.000 --> 00:08.240
So, thank you so much for joining me.

00:08.240 --> 00:10.840
My name's Jessica Rose.

00:10.840 --> 00:13.840
And I work on the Common Voice team.

00:13.840 --> 00:19.520
I am very, very annoying about at least four things.

00:19.520 --> 00:24.520
And one of them is I'm really, really excited about human languages and the web.

00:24.520 --> 00:29.520
So when I sat down and said, oh, I'd really like to submit to speak, immediately I thought,

00:29.520 --> 00:33.480
I'd like to give a 45 minute talk about Common Voice.

00:33.480 --> 00:37.800
And then I thought, it's not especially helpful for people.

00:37.800 --> 00:43.240
So I thought I'd go ahead and pull that back slightly and talk a little bit more broadly

00:43.240 --> 00:50.840
about where our data comes from when we talk about AI models and AI outputs.

00:50.840 --> 00:55.200
Given that we're at a Fostum audience, I'm reasonably sure that folks have a general

00:55.200 --> 01:02.200
vibe about what's going on with AI, that you have a set of data, models are applied

01:02.200 --> 01:08.280
to it, and that some kind of machine generated output comes out of that.

01:08.280 --> 01:12.800
The focus of this talk is going to be about where data sets are sourced.

01:12.800 --> 01:16.040
We're not going to be talking a lot about models.

01:16.040 --> 01:22.880
And I do fear that the older I get, the more my thinking and the more my speaking is about

01:22.880 --> 01:25.520
the philosophy that we bring to our work.

01:25.520 --> 01:29.880
It is a fantastic opportunity for those of you who are watching remotely, if you're not

01:29.880 --> 01:34.960
super excited about consent and data models to escape, for those of you who are here in

01:34.960 --> 01:43.840
the room, you can pointedly look at your watch and escape and you're safe.

01:43.840 --> 01:51.040
AI is increasingly in the news, and you may have noticed that it's not a bubble this time.

01:51.040 --> 01:57.600
It's definitely not hype that every single headline about AI is grounded and focused

01:57.600 --> 02:01.120
and deadly serious.

02:01.120 --> 02:06.480
But that a lot of the headlines we're seeing are both the people saying, the tool makers

02:06.480 --> 02:13.080
behind AI saying, AI is absolutely going to save us, with their other hand saying AI,

02:13.080 --> 02:17.480
which I'm making, which I'm selling, which I'd like some funding for, could also kill

02:17.480 --> 02:19.600
us all.

02:20.520 --> 02:26.760
Assumingly, kidding, Sam Aitman, the head of OpenAI said, AI will most probably lead

02:26.760 --> 02:32.080
to the end of the world, but in the meantime, they'll be great companies.

02:32.080 --> 02:33.880
Very relaxing.

02:33.880 --> 02:39.480
Elon Musk generated a robust amount of AI hype.

02:39.480 --> 02:44.640
I liked this quote best because it is equally true of bears.

02:44.640 --> 02:49.760
There's some chance above zero that AI will kill us.

02:49.760 --> 02:54.480
Very relaxing coming from someone working on AI who presumably likes some more money

02:54.480 --> 02:57.080
for AI tooling.

02:57.080 --> 03:03.080
But both of the quotes before, both of these gentlemen, are also signatories to the AI

03:03.080 --> 03:11.040
safety statement on AI risk saying mitigating the risk of extinction from AI should be a

03:11.040 --> 03:20.400
global priority alongside other societal scale risks such as pandemics and nuclear war.

03:20.400 --> 03:24.760
How relaxing, how calming.

03:24.760 --> 03:31.840
One thing that I genuinely enjoy about being me is I'm not as smart and not as confident

03:31.840 --> 03:37.000
as these people who would both like to sell us a future where AI fixes everything and

03:37.000 --> 03:40.200
a future where AI kills us all.

03:40.200 --> 03:46.080
I'm very, very happy to scale down my concerns about AI.

03:46.080 --> 03:48.520
This is very much not the company take.

03:48.520 --> 03:55.880
This is my own personal take, which is that both the incredibly positive AI as our saviour

03:55.880 --> 04:01.320
and the very scary AI like bears could eliminate us all.

04:01.320 --> 04:06.120
This kind of hype distracts from something that's very, very real, which is the things

04:06.120 --> 04:11.440
that are wrong with AI today and the things that are likely to become more calcified with

04:11.440 --> 04:13.080
time.

04:13.080 --> 04:20.920
One of my largest concerns around these personally is that AI from tool makers and AI from individual

04:20.920 --> 04:27.080
projects are taking the sort of commons of what people have created and turning them

04:27.080 --> 04:33.160
into individual company generators for money that you're living in a future, we're all

04:33.160 --> 04:34.160
living in a future.

04:34.400 --> 04:38.320
Oh, where sometimes things don't load.

04:38.320 --> 04:42.320
That's fine and everything's going to be fine.

04:42.320 --> 04:47.120
We're living in a future where these large AI models come from data.

04:47.120 --> 04:54.680
If anybody's interested, this was a screenshot about where chat GPT's data comes from.

04:54.680 --> 04:58.880
And I'm going to paraphrase this badly, please don't sue me.

04:58.880 --> 05:01.200
If I recall correctly, they were very, very proud.

05:01.240 --> 05:03.920
A lot of the AI projects are a little hand-wavy.

05:03.920 --> 05:05.640
They say, oh, don't worry about it.

05:05.640 --> 05:07.960
We find our data.

05:07.960 --> 05:15.400
The Facebook AI Lama model said, our pre-training data is collected generally in accordance with

05:15.400 --> 05:16.920
open sourcing practices.

05:16.920 --> 05:20.320
It's a very long sentence.

05:20.320 --> 05:27.480
The chat GPT said, hey, we find this data publicly on the internet.

05:27.480 --> 05:29.920
This is all publicly available data.

05:30.880 --> 05:32.800
That's fantastic.

05:32.800 --> 05:38.040
We all know that if we find an image online, we can reuse this on our websites no matter

05:38.040 --> 05:39.240
what.

05:39.240 --> 05:44.160
If you accidentally pick the wrong cartoon mouse off the internet and reuse that, it

05:44.160 --> 05:46.640
won't ruin your life.

05:46.640 --> 05:55.440
The idea that AI models can scrape the internet at large, build data sets of our own work,

05:55.440 --> 06:04.120
and give us a future where we've duplicated slides is very, very relaxing.

06:04.120 --> 06:05.120
Fantastic.

06:05.120 --> 06:07.640
Everything's fine.

06:07.640 --> 06:12.400
Open AI's large language models, including chat GPT, cool, cool, cool, are developed

06:12.400 --> 06:14.760
using three primary sources.

06:14.760 --> 06:17.120
And I love this.

06:17.120 --> 06:20.520
Information that is publicly available on the internet.

06:20.520 --> 06:23.080
Have any of you put writing or art?

06:23.520 --> 06:27.520
Y'all actually, who has not put your stuff on the internet?

06:27.520 --> 06:29.400
Amazing.

06:29.400 --> 06:33.360
Who is super happy about chat GPT taking your stuff?

06:33.360 --> 06:37.800
Those two of you, it was like, yeah, the future.

06:37.800 --> 06:40.520
Information that we license from third parties.

06:40.520 --> 06:42.320
I'm a lot chiller about this.

06:42.320 --> 06:45.680
And I think this one's really exciting.

06:45.680 --> 06:49.040
Or stuff that people gave us.

06:49.040 --> 06:53.200
How chill.

06:53.200 --> 06:54.200
How cool.

06:54.200 --> 06:57.200
You can just find stuff online or people.

06:57.200 --> 06:59.120
And I like the human traders.

06:59.120 --> 07:02.760
This is especially menacing.

07:02.760 --> 07:07.760
And the really exciting, the really terrifying part of this, I think, for me is if you write

07:07.760 --> 07:14.320
for a living, if you make art for a living, if you perform for a living, we're slowly

07:14.320 --> 07:20.600
seeing a future where you're being asked to compete with models trained against your

07:20.600 --> 07:22.840
past work.

07:22.840 --> 07:27.960
That when we're looking at an environment where scarcity of work comes together, it's

07:27.960 --> 07:37.400
including the opportunity to compete against your past selves.

07:37.400 --> 07:42.080
The question of whether or not scraping is theft.

07:42.080 --> 07:47.440
Is something I will leave to people who are, again, much more opinionated than I am.

07:47.440 --> 07:54.040
This picture of someone stealing has nothing to do with anything.

07:54.040 --> 07:55.520
But what are our options?

07:55.520 --> 08:05.040
If we want to make AI now, what can we do that's not taking publicly available information?

08:05.040 --> 08:10.360
What can we do that's not having our human traders give us what they want?

08:10.360 --> 08:14.360
This question, non-rhetorical, I appreciate it.

08:14.360 --> 08:20.720
I am naturally biased, but I do work with the common voice team.

08:20.720 --> 08:28.160
And this is a open source, crowdsourced, multilingual voice speech data set.

08:28.160 --> 08:34.520
We've taken, and this is a very, very 2001 way to say it, the most YOLO approach to licensing

08:34.520 --> 08:35.520
possible.

08:35.520 --> 08:37.840
I'm very old.

08:37.840 --> 08:43.440
And what we've done is we've said that there are more than 7,000 languages in the world.

08:43.440 --> 08:52.760
Right now, most voice assistants are very, very chill, about 20 languages.

08:52.760 --> 08:56.920
If any of you have used common voice assistants, which are going to be some of the more common

08:56.920 --> 09:02.360
speech AIs you're likely to encounter, you absolutely know that these work 100% of the

09:02.360 --> 09:09.720
time, 100% perfectly, as long as you sound exactly like I do, and as long as you speak

09:09.720 --> 09:12.560
English with a very, very standard cadence.

09:12.560 --> 09:17.760
For those of you who don't, very chill, good luck with your other 19 languages of the current

09:17.760 --> 09:20.120
time.

09:20.120 --> 09:25.840
What we've got is we've got, this is a lie, we've got 118 languages right now.

09:25.840 --> 09:27.680
And people donate their voices.

09:27.680 --> 09:34.920
They read clips online, and we have about 28,000 recorded hours.

09:34.920 --> 09:38.600
One of my favorite questions, and you can yell, nobody on the other thing will hear you,

09:38.600 --> 09:40.760
can you guess what language we have the most of?

09:40.760 --> 09:44.040
Like most clips, most data?

09:44.040 --> 09:52.480
People, they all said the correct answer immediately, which is Katalon.

09:52.480 --> 09:59.960
They're watching remotely, they get what they get.

09:59.960 --> 10:02.120
So we're really, really excited about this.

10:02.120 --> 10:05.360
We've asked people, can you donate your voice to us?

10:05.360 --> 10:09.200
We'd be so excited, and they do.

10:09.200 --> 10:12.480
And we release this under a CC0 license.

10:12.480 --> 10:18.320
For those not familiar with the CC0 license, this is the most have a good day license you

10:18.320 --> 10:19.320
can do it.

10:19.320 --> 10:24.400
You can, we ask that you don't identify individual voices from the data set, because that's

10:24.400 --> 10:28.080
incredibly creepy and weird, and y'all wouldn't.

10:28.080 --> 10:30.040
But you can do whatever you want with it.

10:30.040 --> 10:33.000
It's free, you can build stuff with it now.

10:33.000 --> 10:37.720
We've had people do academic research, we've had people make weird art.

10:37.720 --> 10:43.920
I love all of it, and the stuff I don't love, I think try not to think too much about.

10:43.920 --> 10:50.160
There's a ton and ton of stuff available, but this is, you can tell, I'd go on forever.

10:50.160 --> 10:52.080
This is not the only way to do things.

10:52.080 --> 11:00.720
So our first model, ask people for their data, and give it out as unlimited as possible.

11:00.720 --> 11:05.680
I asked four different people tightly connected to the Wikimedia Foundation for help with

11:05.680 --> 11:06.680
this.

11:06.680 --> 11:08.640
I'm still going to get this wrong.

11:08.640 --> 11:11.680
Y'all can email me when I'm wrong.

11:11.680 --> 11:18.200
The Wikimedia Foundation has an incredible data set they've built, which was not necessarily

11:18.200 --> 11:23.960
aiming to be used in AI data, but is being picked up by many data sets with being publicly

11:23.960 --> 11:26.000
available on the internet.

11:26.000 --> 11:34.040
They have users generate original text data, which is being released at a Commons by attribution

11:34.040 --> 11:43.080
license, where at the same time they've got Wikimedia Commons data, video, audio, photographs,

11:43.080 --> 11:47.320
being ingested under a network of different licenses.

11:47.320 --> 11:53.280
They've got tight internal regulations about what kinds of license data they can take in,

11:53.280 --> 11:59.920
and the data that they give back continues to exist under this license.

11:59.920 --> 12:08.760
It's CC by attribution and GFDL for their contributed, generated text and work for Wikimedia.

12:08.760 --> 12:16.040
They do an incredibly complex, one of the experts I talked to said, our licensing system is

12:16.040 --> 12:20.880
incredibly simple, and the other three said, no, no.

12:20.880 --> 12:25.560
But they're governed internally by a network of internal policy guidance, legal guidance,

12:25.560 --> 12:31.960
and volunteer support and community debate that keeps their licensing in check.

12:31.960 --> 12:37.640
The two things we've done so far are ask people for their stuff and give it to anybody, or

12:37.640 --> 12:42.560
ask people for their stuff under a licensing network and give it back out under that licensing

12:42.560 --> 12:46.560
network.

12:46.560 --> 12:50.600
Data trusts get complicated quickly.

12:50.600 --> 12:57.360
This is saying we don't want to give out our data under set guidelines that are inflexible.

12:57.360 --> 13:03.600
We want to have ideally humans, but we want to go ahead and in the current inception take

13:03.600 --> 13:09.920
individually contributed data and have a board of directors or have a board of guidance give

13:09.920 --> 13:11.800
access to this.

13:11.800 --> 13:15.360
Some of this can be under specific open source guidelines.

13:15.360 --> 13:22.120
Some of this can be for profit saying, hey, we only give this out under these contexts.

13:22.120 --> 13:29.280
And unfortunately, right now, the only data trusts I've seen who are operating in large

13:29.280 --> 13:35.920
data sets in meaningful ways tend to be operating in a individual contributor data comes in,

13:35.920 --> 13:40.040
and that's handled as a large block.

13:40.040 --> 13:44.320
One thing that I see again and again that I'm always excited for, that I'm always rooting

13:44.320 --> 13:49.720
for is every three or four years there's a startup promising that they're going to bring

13:49.720 --> 13:58.520
individual choice around data sales and data access to individuals through data trusts.

13:58.520 --> 14:02.320
There was one two years ago, there was one five years ago, there will be one six months

14:02.320 --> 14:04.240
from now.

14:04.240 --> 14:08.600
I'm really, really excited to see if this is the future, but haven't seen this launch

14:08.600 --> 14:11.760
yet.

14:12.720 --> 14:18.640
While I'm very, very excited, both professionally and personally about open source access for

14:18.640 --> 14:26.160
data and open source data sets, one option for AI models and AI companies for getting

14:26.160 --> 14:30.560
your data is literally just to pay for it.

14:30.560 --> 14:37.040
Right now, people's work is being scraped at large from the web, and we live in a reasonably

14:37.040 --> 14:40.200
carefully constructed capitalist system.

14:40.200 --> 14:46.720
We could, people working on AI, pay people instead of just finding it publicly available

14:46.720 --> 14:51.280
on the web.

14:51.280 --> 14:56.800
So far, and this is not the exhaustive list of where we could get consensually sourced

14:56.800 --> 15:04.920
data, we're going to see a very, very easy, lazy similarity between all of these.

15:04.920 --> 15:08.920
We can ask for contributions and pass that data back out freely.

15:08.920 --> 15:14.080
We can ask for and offer contributions under a range of existing licenses.

15:14.080 --> 15:17.360
We can create data trusts for controlling.

15:17.360 --> 15:25.520
We can ask to buy their data, but really, all of these futures involve asking for consent

15:25.520 --> 15:33.760
for people's data being ingested.

15:33.760 --> 15:37.440
This doesn't really look like the direction we're heading in right now, though.

15:37.440 --> 15:41.400
We need a couple different things for this to work.

15:41.400 --> 15:46.440
For somebody looking to start a project around open source data or even closed source data

15:46.440 --> 15:52.120
that's based around consent, looking at governance models and internal structures that build

15:52.120 --> 15:59.240
the external policy, build the consent pipeline for asking for and disseminating data, and

15:59.240 --> 16:03.000
policing that to make sure things go okay.

16:03.000 --> 16:09.280
There was recently a data trust in the United Kingdom that handles health data where they

16:09.280 --> 16:12.880
had promised again and again, this is only going to be for science.

16:12.880 --> 16:15.160
This will only ever be for science.

16:15.160 --> 16:16.400
Can we have your health data?

16:16.400 --> 16:20.280
We will only ever use this for science.

16:20.280 --> 16:26.440
Earlier last year, it turned out that science did actually include insurance companies in

16:26.440 --> 16:29.600
a very relaxing and trusting way.

16:29.600 --> 16:38.640
Looking at how do we set up our internal policies for data projects that increase trust?

16:38.640 --> 16:41.920
Creating these external facing structures as well.

16:41.920 --> 16:43.200
How are we going to do this?

16:43.200 --> 16:46.160
Are we going to ask for data and give it under CC0?

16:46.160 --> 16:51.360
Will we have an exhausting and delightful legal framework of licenses?

16:51.360 --> 16:57.720
Or are we going to build a board?

16:57.720 --> 17:05.080
Regulation and oversight is an incredibly spiky question when we look at where we're

17:05.080 --> 17:07.760
going with AI in the futures.

17:07.760 --> 17:12.960
A lot of the larger AI tool makers right now have said, you know what?

17:12.960 --> 17:19.520
We absolutely welcome regulation, but you can't have a subject to copyright.

17:19.520 --> 17:20.960
That would really hold us back.

17:20.960 --> 17:27.040
We welcome regulation, but not any of the regulatory pathways that would hurt our businesses.

17:27.040 --> 17:30.800
We welcome regulation, but none of that regulation.

17:30.800 --> 17:37.240
Unfortunately, if we're hoping for a future where our data is only sourced consensually,

17:37.240 --> 17:43.120
we would need some kind of framework where folks asking, folks seeking consent in our

17:43.120 --> 17:53.200
data aren't in turn having their stuff scraped by folks not interested in consent.

17:53.200 --> 17:54.640
Community enforcement as well.

17:54.640 --> 17:59.320
I was so excited to come to speak with Tafastham about this today because I'd really, really

17:59.320 --> 18:04.160
love to leave the folks who are going to be working on AI tooling, the folks who are interested

18:04.160 --> 18:09.320
in AI tooling, and the folks who are going to be using AI tooling to be asking ourselves

18:09.320 --> 18:12.240
as we pick it up, oh, this is so fun.

18:12.240 --> 18:13.400
Oh, this does this.

18:13.400 --> 18:19.040
Oh, where does the data for this come from so that we can start to make really, really

18:19.040 --> 18:27.760
principled choices or, I can't tell you how to guide your ethical futures, well-considered,

18:27.760 --> 18:33.720
somewhat principled choices that meet our needs, thinking about what we will and won't work

18:33.720 --> 18:39.000
on, what we will and won't use, and what we will and won't pay for is something I'd

18:39.000 --> 18:43.240
really like to leave everyone with.

18:43.240 --> 18:50.560
It's very, very corny, but if we believe that an AI future where our stuff just gets ingested

18:50.560 --> 18:58.160
and we can compete against it our own past selves, if we're lucky, isn't a very hopeful

18:58.160 --> 19:00.320
future.

19:00.320 --> 19:05.000
If we're going to try to do anything that's not lie down and wait, we need some level of

19:05.000 --> 19:06.760
hope.

19:06.760 --> 19:08.440
And hope is fantastic.

19:08.440 --> 19:10.320
Hope seems so glimmery.

19:10.320 --> 19:13.160
Hope seems so soft.

19:13.160 --> 19:17.160
I think hope is also part of a righteous determination.

19:17.160 --> 19:21.760
You all work incredibly hard on the things you build, on the things you write, and the

19:21.760 --> 19:23.480
things you code.

19:23.480 --> 19:30.680
I'd love for you to be just as determined as you are in your work with keeping the intellectual

19:30.680 --> 19:34.760
rights to your work consensually given.

19:34.760 --> 19:40.880
Human-source means, often that we give it to the world, but that gift, that giving has

19:40.880 --> 19:42.800
to be optional.

19:42.800 --> 19:47.800
Thank you so much.

19:47.800 --> 19:51.600
Yes, coming.

19:51.600 --> 19:59.920
Can you help me pass the mic please?

19:59.920 --> 20:02.640
Thank you for the representation.

20:02.640 --> 20:08.760
In the Common Voice Project, you have people submitting samples and people verifying samples.

20:08.760 --> 20:18.160
I did some sample verification and I heard a lot of samples pronounced by the same persons.

20:18.160 --> 20:25.000
And if I understand correctly, to make a good AI, you need samples from varied sources.

20:25.000 --> 20:31.680
Have you ever had to worry about people with good intentions contributing too much, actually?

20:31.680 --> 20:36.840
I love this question because we have, especially with some of our less commonly represented

20:36.840 --> 20:44.400
languages, we've got a relatively small number of super contributors where we say, oh, wow,

20:44.400 --> 20:45.560
this is a little bit less.

20:45.560 --> 20:53.520
And all of the voice contributions are tagged by hash ID to identify the same speaker in

20:53.520 --> 20:55.720
the training sets.

20:55.720 --> 21:01.560
But we absolutely do know that we've got specific language communities where we've got

21:01.560 --> 21:04.160
individual speakers overrepresented.

21:04.160 --> 21:08.280
And both saying, cool, if you're doing this because you're a language nerd, if you're

21:08.280 --> 21:10.840
doing this because you're studying, that's fantastic.

21:10.840 --> 21:14.240
If you're doing this because you want to train models better and better, you can probably

21:14.240 --> 21:16.320
take the summer off.

21:16.320 --> 21:20.200
Our conversations we've absolutely had with people in the past.

21:20.200 --> 21:21.200
Yeah.

21:21.200 --> 21:25.040
But thank you, both for contributing and for the question.

21:25.040 --> 21:30.960
Oh, I should add, and this is not a question but a plug, if anybody speaks a language that's

21:30.960 --> 21:35.160
not currently on common voice, please come, I used to be a teacher.

21:35.160 --> 21:36.960
I was like, please come and see me after class.

21:36.960 --> 21:42.120
I'd love, love, love to see about getting new stuff on.

21:42.120 --> 21:45.800
I'll ask you nine excited language questions.

21:45.800 --> 21:52.520
Can you, oh yes, on the other side.

21:52.520 --> 21:54.640
I'm at 5K steps.

21:54.640 --> 21:57.080
I'm looking forward for 10.

21:57.080 --> 21:58.080
Don't let me down, 10K.

21:58.080 --> 21:59.080
We're doing this.

21:59.520 --> 22:03.280
Thank you.

22:03.280 --> 22:04.880
Thank you for the presentation.

22:04.880 --> 22:09.760
I was wondering if there is a difference when you are trying to, for example, make sure

22:09.760 --> 22:15.000
that the speech recognition is accurate for persons with a different accent, let's say,

22:15.000 --> 22:20.840
because they come from maybe they're Swedish Iranians and they speak Swedish in a certain

22:20.840 --> 22:28.040
way, or if let's say a person has a hearing loss and then they might speak, like people

22:28.080 --> 22:33.120
have different types of hearing loss and therefore the accent, I believe, could be more varied

22:33.120 --> 22:37.920
and I'm wondering if the approach would be different when you are trying to make it accurate

22:37.920 --> 22:41.760
for these different types of groups.

22:41.760 --> 22:44.440
Oh I could have just talked about speaker recognition this whole time.

22:44.440 --> 22:45.640
I'm so excited.

22:45.640 --> 22:52.440
So one thing we do is we allow people to include optional metadata and that includes accent.

22:52.440 --> 22:56.240
And we've got a drop down of the most common accents we've seen for each language, but

22:56.280 --> 22:58.480
that's a blank text field.

22:58.480 --> 23:04.400
There's a researcher named Kathy Reed down at ANU who's doing research about language

23:04.400 --> 23:07.520
identities and how people self describe.

23:07.520 --> 23:12.440
So we have seen people saying, oh I speak Swedish with an Iranian accent.

23:12.440 --> 23:18.080
We've seen people say a huge string of descriptors saying, oh I'm in my 20s and I have a little

23:18.080 --> 23:22.720
bit of a Zoomer accent but I used to live in Massachusetts and I said okay.

23:22.720 --> 23:23.720
It's very specific.

23:23.720 --> 23:25.560
I like that.

23:25.560 --> 23:29.240
But we do also have folks who are talking to us about say, oh I'm a bit hard of hearing

23:29.240 --> 23:31.840
and this is the accent related to that.

23:31.840 --> 23:38.840
So really, really free text accent descriptors has resulted in some incredibly interesting

23:39.320 --> 23:43.880
metadata just in how people describe their own accents.

23:43.880 --> 23:47.000
I do have a question myself now building on that.

23:47.000 --> 23:52.400
Do we use that metadata in a way when I will go and approve or listen to someone to say

23:52.400 --> 23:54.040
yes this is correct.

23:54.040 --> 23:55.720
Would I get any of the inputs?

23:55.720 --> 24:00.440
You know like yes we are targeting this with an accent or we are targeting as clear as

24:00.440 --> 24:02.840
possible so I would be.

24:02.840 --> 24:08.920
So if you're validating common voice clips, we won't show the accent data associated with

24:08.920 --> 24:09.920
it.

24:09.920 --> 24:14.200
The general guidance is if you can understand it, if they're saying what it's saying, I'm

24:14.200 --> 24:16.640
generally pretty happy to accept it.

24:16.640 --> 24:22.320
That's great.

24:22.320 --> 24:30.240
If I give consent but want some attribution, is something like that possible?

24:30.240 --> 24:36.880
So with common voice, I'm so sorry that it's CC0 so there's no attribution associated with

24:36.880 --> 24:37.880
it.

24:37.880 --> 24:44.440
One thing that is super exciting is the platform is all open source as well.

24:44.440 --> 24:49.720
So we have seen language communities that said hey, we're not really comfortable with

24:49.720 --> 24:51.160
the CC0 aspect.

24:51.160 --> 25:00.160
We're going to go ahead and fork the platform and create parallel language corpora.

25:00.160 --> 25:03.240
Plural corpus, I always flub it.

25:03.240 --> 25:07.720
So they're parallel language corpora under different licenses or even we've seen individual

25:07.720 --> 25:17.480
users create their own speech corpora based on what they wanted to do with their own voices.

25:17.480 --> 25:20.920
So under common voice, no, I'm sorry.

25:20.920 --> 25:29.960
Under other collection projects, often yes.

25:29.960 --> 25:34.560
It's a hard decision where to go, which side to go.

25:34.560 --> 25:38.320
Yeah, yeah, yeah.

25:38.320 --> 25:41.960
This is open source.

25:41.960 --> 25:44.520
Thank you.

25:44.520 --> 25:51.560
So when you talk about the CC0, I also understood I remember that there was some issue of concern

25:51.560 --> 25:54.680
because it could have been exploited by some other company.

25:54.680 --> 26:01.880
What I wonder is if exist or if I've ever evaluated a license, let's say this is CC0

26:01.880 --> 26:06.000
except from massive machine learning.

26:06.000 --> 26:11.560
So on this very, very controversial question at Fostum as well.

26:11.560 --> 26:13.240
I've heard other people say them like this.

26:13.240 --> 26:14.640
It's not how I would describe them.

26:14.640 --> 26:23.360
I've heard these licenses described as Franken licenses sometimes or open source alike licenses.

26:23.360 --> 26:28.880
There are a ton of, so I think I want to separate out my personal opinions from the company

26:28.880 --> 26:31.760
opinion because I don't know what that is.

26:31.760 --> 26:37.400
As an individual speaking for myself, I think these are really exciting and interesting.

26:37.400 --> 26:39.840
They're not true open source.

26:40.840 --> 26:45.600
But for people to be evolving licenses and to be evolving how we think about permissions

26:45.600 --> 26:52.440
as we evolve projects with data, even the ones that don't work are just such exciting

26:52.440 --> 26:54.440
experiments.

26:54.440 --> 26:58.400
Yeah, they don't always have to be something I use in my projects.

26:58.400 --> 27:01.400
They don't always have to be pure open source.

27:01.400 --> 27:05.920
But I love mess and they're all very exciting mess.

27:05.920 --> 27:08.360
They're often not all.

27:08.400 --> 27:11.040
So there's some really interesting ones out there.

27:11.040 --> 27:15.240
There's an ethical, I can't recall what it's called.

27:15.240 --> 27:22.480
There's a open source alike license based around, you can only use this for ethical

27:22.480 --> 27:27.880
uses which immediately splits into 90 different, what is ethics?

27:27.880 --> 27:30.320
What's a big company?

27:30.320 --> 27:34.880
And it's just a very, very exciting street to go down.

27:34.880 --> 27:36.960
Thank you so much.

27:36.960 --> 27:38.560
Thank you.

