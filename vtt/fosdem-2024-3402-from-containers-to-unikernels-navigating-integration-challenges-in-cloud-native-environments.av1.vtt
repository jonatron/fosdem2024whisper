WEBVTT

00:00.000 --> 00:13.000
So, thank you.

00:13.000 --> 00:20.000
My name is Ion Blackus and in this presentation we're going to talk about the Unikernels and

00:20.000 --> 00:24.000
their integration challenges in cloud native environments.

00:24.000 --> 00:30.000
So, first we're going to set the scene, talk about containers and some books mechanism

00:30.000 --> 00:37.000
and then we're going to read the Unikernels and talk about our own OCI compatible container

00:37.000 --> 00:44.000
runtime, Uran-C. And after that some demos and evaluation results sections.

00:44.000 --> 00:49.000
So, yeah, we're a team of researchers with mixed industrial and academic background,

00:49.000 --> 00:55.000
mostly focused in virtualization, container untimes and hardware acceleration.

00:55.000 --> 01:02.000
So, yeah, containers is the standard right way to deploy and package your application.

01:02.000 --> 01:05.000
They're portable, they can be run both in cloud and net.

01:05.000 --> 01:11.000
They're easy to scale with the support of a wide ecosystem and they have super fast

01:11.000 --> 01:18.000
phone times. But they come with major risk when it comes to multi-tendent scenarios.

01:18.000 --> 01:24.000
Multiple containers serve the same kernel and rely on software components for their

01:24.000 --> 01:35.000
isolation. So, yeah, in a malicious workload scenario, in a previous escalation, container

01:35.000 --> 01:41.000
impact the entire host. So, what about vendors did was to either use software-assisted

01:41.000 --> 01:47.000
solutions with tools like SACOM or a Parmore or hardware-assisted solutions, VM type solutions

01:47.000 --> 01:56.000
with tools like Farcracker and Deviser. So, yeah, the thing becomes like this.

01:56.000 --> 02:03.000
We have now, we deploy our application inside the container, inside the VM, totally isolated

02:03.000 --> 02:11.000
from the last system and we get to keep actually the benefits of container, the portability

02:11.000 --> 02:18.000
and the scalability and we also kind of resolve the isolation issue. But of course, this comes

02:18.000 --> 02:26.000
with side effects, since higher overhead because of CPU and memory provisioning for the VM

02:26.000 --> 02:36.000
and the much more complex system stack that needs to be maintained. By taking a step back,

02:36.000 --> 02:42.000
we can see that the application does not need all these parts, not run. It just needs the

02:42.000 --> 02:49.000
run time, the libraries and some parts of the OS, like the drivers. And, yeah, this kind

02:49.000 --> 02:57.000
of modeling is enabled by technology called the Unikanal. So, what's Unikanal? It's a

02:57.000 --> 03:03.000
specialized single-hatter space that contains exactly the necessary parts for the application

03:03.000 --> 03:11.000
to run. So, yeah, this leads to reduced attack surface and faster boot times, which is especially

03:11.000 --> 03:20.000
crucial in serverless scenarios where responsiveness matters. And, yeah, but Unikanals are not

03:20.000 --> 03:29.000
widely adopted yet. And why is that? We identified too many issues. The first one is

03:29.000 --> 03:35.000
packaging and, yeah, Unikanals should look like an OS image in order to utilize because

03:35.000 --> 03:42.000
this is support. And the second one, the more, more than, is deployment and the execution of

03:42.000 --> 03:48.000
Unikanals. Container run times need to be extended. Additional logic is needed in order to

03:48.000 --> 03:54.000
execute Unikanals. And with this, I would like to give a floor to your viewers to talk about

03:54.000 --> 04:04.000
your ANSI. Thank you, Yanis. Hello, everyone. So, to solve the deployment challenge of Unikanals,

04:04.000 --> 04:12.000
we introduce URAN-C, which is a Unikanal container runtime. It is fully CRI-compatible. It's written

04:12.000 --> 04:19.000
in Go. Actually, it's a CLI tool which makes use of interconnected Go packages to actually

04:19.000 --> 04:26.000
spawn the Unikanals. It reads Unikanals as processes. So, in a way, it directly manages the

04:26.000 --> 04:33.000
application and not the system in which the application runs. The images, the Unikanal images

04:33.000 --> 04:40.000
required to run these Unikanals are typical OSI artifacts. And in order to actually spawn these

04:40.000 --> 04:48.000
Unikanal VMs, we make use of underlying hypervisors. So, first, let's take a look at how Unikanal

04:48.000 --> 04:54.000
images looks like. First of all, they are standard OSI images, so they can be managed via

04:54.000 --> 05:01.000
standard tooling and can be distributed using already existing registries. But there is one

05:01.000 --> 05:08.000
differentiating factor. URAN-C needs some specific annotations to function. These annotations are

05:08.000 --> 05:14.000
the Unikanal binary path inside the root of S, the Unikanal type, the hypervisor type, the command

05:14.000 --> 05:20.000
line that we need to pass to the Unikanal. And optionally, if we are using IneterD, the path to

05:20.000 --> 05:28.000
the IneterD file. So, to facilitate the packaging of the Unikanal, we created a simple image

05:28.000 --> 05:36.000
builder called BIMMA, which uses simple Docker file like syntax to create the images. As you can see, it's

05:36.000 --> 05:45.000
pretty typical for anyone who has used Docker. It's practically the same thing. So, now we have seen how

05:45.000 --> 05:53.000
an OSI Unikanal image looks. Let's take a closer look at how Unikanal actually spawns a Unikanal.

05:53.000 --> 06:01.000
First of all, container.dc invokes URAN-C create. URAN-C create then sets up a new network name space, create a

06:01.000 --> 06:09.000
new name space, sets up a pseudo terminal if it's required, and spawns URAN-C reexec process inside that name

06:09.000 --> 06:17.000
space. The reexec process then notifies the parent process that had started. Then, URAN-C, the URAN-C

06:17.000 --> 06:26.000
create, the original process, saves the state, the PID of the reexec process, etc. Executes any create runtime hooks, and

06:26.000 --> 06:37.000
then sends an OK IPC message to the reexec process, executes any create container hooks, and then exits. Then,

06:37.000 --> 06:45.000
container.dc invokes URAN-C start, which sends an IPC message to the reexec process, executes post start hooks, and

06:45.000 --> 06:54.000
exits as well. So, now it's the most interesting part. The reexec process actually sets up any necessary network and storage

06:54.000 --> 07:05.000
components, for example, the W device, etc. Executes any start container hooks, and actually spawns the Unikanal VM. So, as you

07:05.000 --> 07:15.000
can see, this is a pretty typical life cycle for any container runtime, with just some minor adjustments to facilitate the Unikanal

07:15.000 --> 07:28.000
execution. So, to actually spawn the Unikanals, we use hypervisors. We made it really easy to integrate any new hypervisors you want to

07:28.000 --> 07:34.000
implement in the system. So, I'm going to show you how to do this with the Unikanal VM. So, in this case, you can just

07:34.000 --> 07:41.000
implement this interface, which is mostly just the exec V function. So, it's really easy. Currently, we have support for

07:41.000 --> 07:49.000
Solo 5, Kimu, and Firecracker. For storage, we have support for block device via the map or

07:49.000 --> 07:59.000
snapshotter. We have support for InitrD, which is packed inside the image. And we also have support for shared effects coming soon.

07:59.000 --> 08:13.000
In the diagram, you can see how an image looks like, the layers look like. So, for the network part, we followed the every simple

08:13.000 --> 08:23.000
approach that is also used by sandboxed runtimes, like Cata containers. We create a new top device inside the container network

08:23.000 --> 08:36.000
loading space. And then we breed all the data, all the traffic to the VF endpoint provided by CNI. We do this using traffic control.

08:36.000 --> 08:47.000
To integrate Unikanals in Kubernetes, we had the Asphalt Challenge. That's because we need to actually spawn non-Unikanal

08:47.000 --> 08:58.000
containers inside the same pod. For example, the POS container or any other side-con containers. To achieve this, we use RunC to spawn the

08:58.000 --> 09:09.000
generic containers. And then, RunC handles the Unikanal containers inside the network namespace of the pod. So, there are some really

09:09.000 --> 09:21.000
interesting use cases, for example, KN80, in which we need to have intrapod Unikanal container communication. In KN80, for example, the QPROXY

09:21.000 --> 09:31.000
container needs to be able to communicate with a user function, which is Unikanal. To achieve this, we implement a static network

09:31.000 --> 09:48.000
configuration. We provide the static IP to the top device. So, we handle it that way. So, now, let's see RunC in action. We will see a simple

09:48.000 --> 09:58.000
deployment using an HDL. We will pull the image from the registry. And using an HDL, we will actually spawn an NGNX Unikanal inside

09:58.000 --> 10:23.000
VM. So, as we can see, there are no containers running right now. We pull the image from our registry. Okay, it's already existing. And now, we can run it using

10:23.000 --> 10:40.000
an HDL. We have to define the runtime. So, we do that. Okay, it spawned. And now, we can see that it started six seconds ago. It was created. Perfect. So, now, we can

10:40.000 --> 10:56.000
inspect the container to find the IP address. Okay. And if we curl it, we can see that it's an NGNX server built using Unicraft. Pretty typical. So, now, we can see the actual

10:56.000 --> 11:09.000
run. And we can see that it's running. And the container is in the RunC process. That's also running. Okay. And now, with that, I will give the floor back to

11:09.000 --> 11:38.000
Janis to show you a more elaborate example with K-nate. Okay. So, now, just... Okay, that's bad.

11:38.000 --> 11:51.000
Now, let's deploy a serverless workload with RunC. So, what we first do here is that we see that we have another RunC process running in the

11:51.000 --> 12:09.000
cluster. And after this, what we need to do is to define the RunC class for the K-H cluster. You can see here, we apply the RunC class. And then, it's time to define the K-native

12:09.000 --> 12:25.000
service. We can see that your RunC container around them is specified. And a simple HTTP-reply-server workload is used as the workload of the serverless function. We apply the

12:25.000 --> 12:44.000
K-native service. And then, we will retrieve back the URL endpoint, which by triggering it with a simple HTTP request, a simple HTTP get, essentially, we start the execution of the serverless workload. So, here, we can see the

12:44.000 --> 13:10.000
curve. And after this, the pods are going to be running. And underneath, there will be the RunC process with the K-mode hypervisors with the sandbox workload. So, yeah. That's it. And...

13:10.000 --> 13:37.000
So, the evaluation section. In order to evaluate your RunC, we convert with other container runtimes, such as divisor and other containers. And, yeah, in that process, we utilize the tool called K-perf responsible for generating and triggering K-native services via HTTP request, as we saw in the demo. And also, responsible for reporting the service latencies.

13:38.000 --> 14:01.000
So, yeah, the scale from zero, evaluation scenarios like this, for a number of iterations, we scale. It's a K-native service. And we report at the end of the number of executions, we report the responsible latency. We do this for every other container

14:01.000 --> 14:27.000
runtime. So, these are the results. We can see on the X axis the different container runtimes used for that process. And on the Y axis, the service response latency, seconds. And, yeah, of course, lower is better. So, on the blog post, with the experiment setup and all the parameters setting for K-perf. That's all. Thank you.

14:31.000 --> 14:33.000
Thank you.

15:01.000 --> 15:23.000
Okay. So, the question is about memory benchmarking, right? Yeah. Memory benchmarking is not yet on our work, but we have plans on that also. Yeah. Something that we can do.

15:32.000 --> 15:34.000
Sorry.

15:39.000 --> 15:44.000
So, the question is if we have run in Germany, AWS, I don't know.

15:44.000 --> 16:05.000
Actually, this experiment was on the Prime Service. We have not yet experienced any big lab vendors and deployments. So, hopefully, maybe the next evaluation will be also part with major vendors.

16:15.000 --> 16:17.000
That was the end.

16:20.000 --> 16:22.000
Okay.

16:36.000 --> 16:43.000
So, okay. I heard something about paravirtualization, right? But, yeah.

16:44.000 --> 16:46.000
Okay.

16:55.000 --> 16:57.000
I'm not sure that that's.

17:14.000 --> 17:16.000
Okay.

17:44.000 --> 18:04.000
So, I think that's it. Thank you. Thank you so much.

