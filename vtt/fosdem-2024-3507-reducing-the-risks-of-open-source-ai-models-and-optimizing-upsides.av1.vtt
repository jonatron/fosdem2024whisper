WEBVTT

00:00.000 --> 00:15.120
45 minutes session today. We want to go over some of the risks of open source models and

00:15.120 --> 00:23.640
then cover like what AI governments can do about this. So the first part will be a presentation

00:23.640 --> 00:28.120
so that we have the same common background information on what we're talking about, what

00:28.120 --> 00:34.160
kind of models, what does the technology look like. This isn't very in depth on the technical

00:34.160 --> 00:39.880
part but people who do have technical questions feel free to ask them and will leave a lot

00:39.880 --> 00:45.440
of time for audience participation questions and answers. So about 10 minute presentation,

00:45.440 --> 00:52.000
15 minute initial panel discussion and then interaction with the public so that you can

00:52.000 --> 01:01.520
give us your thoughts and inputs on AI governments open source models. So let's start just with

01:01.520 --> 01:07.400
AI safety. How many in this room already have heard this term or have read about AI safety?

01:07.400 --> 01:15.840
Can you raise your hands? I'm seeing about half of you, thank you. So an interdisciplinary

01:15.840 --> 01:20.960
field concerned with preventing accidents, misuse or other harmful consequences that

01:21.040 --> 01:30.680
could result from artificial intelligence AI systems. This is not just technical talk or

01:30.680 --> 01:37.080
social thing, it has to both require the expertise to understand what machine learning systems are

01:37.080 --> 01:42.240
doing, how they work, what are their problems. So that's what I'll be going through in the first

01:42.320 --> 01:51.240
10 minutes, 5 minutes of the talk and then figure out how society can adapt and how the

01:51.240 --> 01:59.080
economy can adapt to this new technology. Okay, part one, I'll just go briefly over deep learning,

01:59.080 --> 02:05.160
what's different, what's different with open source with relating to deep learning models as

02:05.280 --> 02:13.320
opposed to classic open source software and then part two, quick introduction, my personal thoughts

02:13.320 --> 02:17.360
on AI governance which do not represent the rest of the panel and then the panel discussion.

02:17.360 --> 02:27.320
Why is there something different with deep learning models? This is not the same as usual

02:27.320 --> 02:33.480
software where you can see the code and you can reason about what the model is doing. In deep

02:33.560 --> 02:40.560
learning models, there is huge pile of weights randomly initialized, updated into its succincts and

02:40.560 --> 02:46.360
there is a field called interpretability which tries to figure out how do these models achieve

02:46.360 --> 02:53.920
what they do and this is not always something that we actually can do for the largest models. So

02:53.920 --> 03:00.880
if I'm talking about GPT-4, chat GPT that you've interacted with, it's not sure how it's able to

03:01.080 --> 03:08.400
get the information that it has and we have some amount of uncertainty about what tasks it can do.

03:08.400 --> 03:16.640
When it was trained, we did not predict what strategies GPT-4 could use to reason and we

03:16.640 --> 03:23.160
continuously discovered techniques like chain of thought prompting and iteratively like tree of

03:23.160 --> 03:30.320
thoughts and this brings the particular difficulty where you can't scope exactly what actions,

03:30.320 --> 03:38.560
what kind of text a GPT-4 like model can do. There are a bunch of vulnerabilities and I'm

03:38.560 --> 03:46.880
going to focus on text and image generative AI. Accidents is when the developer did not

03:46.920 --> 03:53.440
intend the particular use and yet something happened. So specification gaming specifically

03:53.440 --> 04:00.640
that you optimize the AI to succeed at a certain objective like in video games and then it uses

04:00.640 --> 04:07.360
an exploit to gain maximum score instead of actually doing what you wanted to do and there are cases

04:07.360 --> 04:15.120
so for generative AI where we wanted to train an LLM to do text that was agreeable or that users

04:15.240 --> 04:20.920
found quite pleasant and once this was trained with the reinforcement learning system, it started

04:20.920 --> 04:25.280
talking about birthday cakes all the time because the particular machine learning system that was

04:25.280 --> 04:35.240
rating it kept rewarding this. So even as developers, you might train an ML system and then it will

04:35.240 --> 04:41.040
have unwanted behavior. You could also give it perfectly correct data sets and yet it could

04:41.080 --> 04:48.480
misgeneralize if it's learning labels that are easier to learn than more complicated data in your

04:48.480 --> 04:58.000
images. And then hallucinations, we use LLMs to give us some information and yet some of the time

04:58.000 --> 05:05.440
this information is incorrect. I'll not go too long on the adversarial part because this is more,

05:05.680 --> 05:11.200
this is less about how we train systems but how you make sure that while they're in deployment,

05:12.160 --> 05:18.960
you are still safe to use them. But there's specifically like prompt injections where particularly

05:18.960 --> 05:27.920
crafted input can cause your LLM systems to change behavior. Sometimes leak information about what

05:27.920 --> 05:38.000
prompts you are using before. And I mentioned Trajan's because this is a unsolved problem if in

05:38.000 --> 05:45.520
your data set that you downloaded online, if someone created a small percentage of this data to have a

05:45.520 --> 05:53.120
particular relationship, they can make that upon a particular trigger, the machine learning system

05:53.200 --> 05:58.960
will behave differently. So particular token inputs which suddenly cause the LLM to be much

05:58.960 --> 06:05.600
more willing to reveal prompt information or to follow different instructions than what you

06:05.600 --> 06:15.920
thought you find too needed to do. And this discussion like Fosdum is specifically about open

06:15.920 --> 06:21.200
source and open weight models are not the same thing as open source software. It's not because

06:21.200 --> 06:27.520
that you have the weight that you can actually know what the model is doing. Backdolls currently

06:27.520 --> 06:38.400
cannot be identified systematically. And you can't manually update the models. I mean, we don't have

06:38.400 --> 06:47.040
the, I guess humans, we can't update the model weights directly to rewire its behavior, though we

06:47.040 --> 06:53.760
can fine tune it which is like do new training runs. So to have actually open source deep learning

06:53.760 --> 06:59.920
systems, you need to have control of the full stack, both the data set and the training code,

06:59.920 --> 07:05.120
and you actually also need computing power to run this. If you don't have these things, you are not

07:05.840 --> 07:11.680
actually in control of the model that you have. Even if you do train this in our system yourself,

07:11.680 --> 07:19.360
you have the problems that I evoked before earlier. And so some really high level approaches.

07:21.120 --> 07:28.560
If you're going to use these models, don't use them in critical operations. And otherwise, well,

07:28.560 --> 07:33.200
be clear on what your model can and can't do by examining the model card or creating one when

07:33.200 --> 07:43.120
you do yourself. Misuse risk, you cannot, you can't choose how users will retrain your model

07:43.120 --> 07:50.000
after it's deployed. So Lama had particular safeguards so that it doesn't produce illegal content or

07:50.560 --> 07:56.320
illegal instructions. And yet fine tuning can remove those safeguards. So at the fundamental

07:56.320 --> 08:01.280
level, the only way you can get a model to never be dangerous is for it to not be capable to be

08:01.280 --> 08:08.000
dangerous. And if you're going to release your model in the wild, please evaluate your model's

08:08.000 --> 08:14.800
capabilities before release. This corresponds notably to can your model help people in illegal acts

08:16.320 --> 08:22.640
and others? Okay. Part two, AI governance and open source.

08:22.640 --> 08:35.280
Let's just get some context for where we're going. Like the current capabilities appeared in the last

08:35.280 --> 08:41.200
few years and we've had an exponential increase in the amount of training compute put behind

08:41.200 --> 08:46.080
these deep learning models, which is some of the reason why we don't, we're not able to understand

08:46.160 --> 08:52.720
them in that much detail. And not just training compute, the algorithmic efficiency of these models

08:52.720 --> 08:59.840
has been rising. So for a given year, for a given data set, we can now train models that can more

08:59.840 --> 09:07.760
efficiently recognize the image. It takes less amount of compute to train the models for, to

09:07.840 --> 09:16.960
still succeed at getting 63% performance and image in it. And so this leads to a question of like,

09:16.960 --> 09:23.200
are we going to predict, like this is leading to more and more powerful AI. And I just want to open

09:23.200 --> 09:29.760
this question, like leading ML scientists say that mitigating the risk of extinction from AI

09:29.760 --> 09:35.200
should be a global priority alongside other societal scale risks such as pandemics and nuclear war.

09:36.080 --> 09:44.800
So what are they talking about? I just don't have the time in two minutes to go over the whole

09:44.800 --> 09:52.480
field of AI existential risk and like forecasting governance, AGI governance. So I've listed these

09:52.480 --> 09:58.640
three papers for people who will be more interested to go into those details, but we can talk about

09:58.640 --> 10:06.560
some of these in the Q and A if you're interested. There's also pushback, like some ML leaders have

10:06.560 --> 10:12.080
signed this letter. Some others have said that it's ridiculous. Yanlokin's tweets, I think, is

10:12.080 --> 10:18.000
particularly relevant because it pushes back on both sides systematically. LLMs are not superhuman

10:18.000 --> 10:23.680
in all ways right now, but they're not useless parrots. The hallucinations are not the end of all.

10:24.240 --> 10:30.880
And scaling is not sufficient to do AGI, yet we will still get more success with deep learning.

10:32.240 --> 10:39.520
And he pushes back notably that AI doesn't exist and never will. He does think that we will create

10:39.520 --> 10:47.760
AGI or artificial general intelligence, but he has sort of different estimates as to when that

10:47.760 --> 10:55.520
will happen. And I guess I also listed Grady Butch's tweet. There are clear and present real

10:55.520 --> 11:00.560
harms that we must address now to worry about some future fantasy existential risk is a dangerous

11:00.560 --> 11:07.520
opportunity cost. So because there's so much polarization, there are so widely differing views

11:07.520 --> 11:16.000
on AI, AGI and existential risk. This is one of the reasons we're organizing this panel discussion

11:16.000 --> 11:25.120
and the participation of the audience. Before we get into the panel, I just mentioned two key

11:25.120 --> 11:31.280
concepts that I found quite important to frame this is that we have a choice between which

11:31.280 --> 11:37.920
technologies to advance. There are technologies which allow us to better understand the deep

11:37.920 --> 11:43.280
learning models we have to better control the risk that they have. And we can think about

11:44.240 --> 11:49.600
even if this is a complex environment where we don't know exactly what results of what policies

11:49.600 --> 11:56.000
are good or not, there are some keys to success. And one of the reasons I'm doing this here again

11:56.000 --> 12:01.760
is that a wider understanding of AI safety among AGI developers, among users and policymakers,

12:01.760 --> 12:08.080
seems to be one of this overwhelmingly like net good, net positive action.

12:08.080 --> 12:18.080
All right. So I'll hand over the mic to Stefan Adele-Pret. Yeah.

12:18.080 --> 12:25.120
Yeah. Well, thank you, Jonathan. And first of all, big applause for Jonathan for having a short introduction.

12:28.720 --> 12:36.080
And also being so able to introduce yourself. So I will do for you because he's a great researcher,

12:36.160 --> 12:41.120
actually a founder of the European Network for AI Safety, where different research and

12:41.120 --> 12:47.280
collaborators can go together. And also a teacher in the machine learning for a good

12:48.400 --> 12:55.040
bootcamp that has been funded by Aarabens Plus. And I can say for the first time, it's a very great

12:55.040 --> 13:02.480
panel, very great course. And with us, we also have a Felicity Reddell, and she's been involved also

13:02.480 --> 13:10.800
in connection with the Dutch Ministry of Internal Interest and extensive knowledge in AI policy,

13:10.800 --> 13:17.360
especially for the International Center for Future Generation. And with us, we also have

13:17.360 --> 13:24.080
Alexandra Xalidis that is involved in the Future All Life Institute that I personally have a big

13:24.080 --> 13:31.120
pen, have been a researcher in Harvard also for the ethical part of AI and extensive knowledge of

13:31.200 --> 13:37.840
AI safety as well in terms of policy. And in this panel, we have one to tackle, especially this

13:37.840 --> 13:46.400
knowledge. And the first question to have for all of you is what are in your opinion the risks to have

13:47.200 --> 13:53.840
the development of this advanced AI unrestricted through policy? So what are the risks to don't

13:53.840 --> 13:59.600
have any policy in place right now to really reinforce what could be a more stable and a

13:59.600 --> 14:09.680
safety environment? I'll hand it to you first. Thank you. Yeah, so I would like to start with

14:09.680 --> 14:16.400
saying that obviously open sourcing has a lot of consequences that can be very positive,

14:16.400 --> 14:24.160
such as accelerating innovation or decentralizing access. Just at the same time, there can be notable

14:24.160 --> 14:30.080
risks from open sourcing AI, especially as we talk about increasingly capable models.

14:30.800 --> 14:37.280
And so now I'm going to go briefly into those a little bit. I think the risk there stems mainly

14:37.280 --> 14:45.200
from two kinds of things. One is that these capable systems can have quite the misuse potential.

14:46.000 --> 14:52.320
It can make it a lot easier for bad actors, for malicious actors to access model weights

14:52.880 --> 14:58.080
to modify them and then basically to exploit the work of open source developers that didn't

14:58.080 --> 15:03.680
intend any harm. And they can, I mean, you've heard about this, they can poison the information

15:03.680 --> 15:09.680
ecosystem, but also lead to a lot of personal harm, like in the form of scams, for instance, or

15:10.640 --> 15:17.600
adult content without consent and online bullying. But also it can make it easier to

15:17.840 --> 15:24.720
create harmful substances or obviously like the whole story of cyber threats that are being made

15:24.720 --> 15:31.520
easier. So that was the first point, like there's the potential for misuse. And the second point,

15:31.520 --> 15:37.200
Jonathan also mentioned already a little bit, it just makes it a lot harder once the weights are

15:37.200 --> 15:44.320
open sourced to control the whole situation. You cannot really monitor how these models are

15:44.880 --> 15:48.400
used. You cannot roll them back. You cannot shut them down.

15:51.440 --> 15:57.280
Yeah, so I think Felicity Rout lined almost everything. And obviously we can divide the

15:57.280 --> 16:04.480
harms in terms of misuse, but also models going beyond our control. Those are more speculative

16:04.480 --> 16:09.520
in nature, but there are still harms that I think we consider at this stage. What I would like to

16:09.520 --> 16:16.400
focus on right now though, as someone with a policy law background, is that with open source,

16:16.400 --> 16:22.160
it's very difficult to attribute responsibility. So if something does go wrong in the sense of

16:22.160 --> 16:27.200
any of these risks with misuse materialized, and they're much more likely to materialize the

16:27.200 --> 16:32.960
minute that something is online and available with model weights, with data, with the code,

16:33.840 --> 16:40.880
as a policymaker or as let's say the lawyer representing whatever party has been harmed in

16:40.880 --> 16:45.680
that situation, it would be very difficult to identify who has caused that harm and who's

16:45.680 --> 16:53.600
responsible. And for me, that's the scariest part in the sense that in a traditional release,

16:53.600 --> 16:59.600
you would have a company, an entity, which is responsible, which you can therefore keep

17:00.560 --> 17:06.480
accountable for downstream harms. I think I really hope we can discuss this at some point in the

17:06.480 --> 17:13.680
panel, but I also think that there's a lot of misconceptions with regards to what risks developers

17:13.680 --> 17:21.360
take on when they use models released by big tech companies. Not many of us have read the terms of

17:21.360 --> 17:26.560
conditions. I have, and there's nothing in there, or there's very little in there that will protect

17:26.640 --> 17:34.880
downstream developers for future harms, because when a company like Meta open sources their models,

17:34.880 --> 17:41.440
the first thing that they will do is close themselves off from responsibility. Now that

17:41.440 --> 17:46.560
can be a very unfair balance because a lot of the time they might keep information which is crucial

17:46.560 --> 17:52.560
for making those models safe. So you end up with a dynamic where the company holds the levers

17:52.640 --> 17:57.840
necessary to make a model safe, but then attributes all the responsibility to the developer that ends

17:57.840 --> 18:04.240
up putting that downstream application on the market, or just releasing some other form.

18:07.760 --> 18:15.360
Okay, thank you. And on the positive side, if we want to look what will look like to have

18:16.320 --> 18:26.640
governments of AI that is actually working good, are any of your vision can be on your timeline,

18:26.640 --> 18:38.160
what you expect to become? And I guess, yes, what are good governments of open source AI will look like?

18:38.880 --> 18:40.320
Yeah, we can start.

18:46.320 --> 18:53.600
Yeah, so I think on a high level, the good news is that we don't probably need a lot of special

18:53.600 --> 18:58.800
governance rules, because it's the same kind of system, so we can probably have somewhat of the

18:58.800 --> 19:05.840
same requirements. It just makes it a lot easier to comply if you don't have the ways available

19:05.920 --> 19:11.280
for the reasons that were mentioned before. You can avoid it, that guard rates are being stripped,

19:11.280 --> 19:15.760
and you can monitor much more. You can realize when something goes wrong, and you can adjust the system.

19:17.280 --> 19:25.120
And I would also like to stress on a meta level that I think it makes sense to focus a lot of

19:25.120 --> 19:32.240
the requirements on the models that are the most advanced. It's kind of in line or similar to what's

19:32.240 --> 19:36.400
happening at the AI Act, for example, with the risk-based approach, you want to focus your

19:36.400 --> 19:41.200
efforts on where most of the risk stems from, and for general purpose AI systems, the tiered

19:41.200 --> 19:50.560
approach is the analogy of that. But yeah, to come to some concrete examples, I think one good

19:50.560 --> 19:56.800
approach could be to have offered access, restricted access for research and testing. So that could

19:56.800 --> 20:02.000
be vetted researchers from academia, but also from civil society. And I think also the open

20:02.000 --> 20:06.400
source community, there could be ways that independent people from the open source community

20:06.400 --> 20:12.960
could qualify and contribute there as well. And like that, you could get a lot of the benefit of

20:12.960 --> 20:17.280
open sourcing, namely that a lot of more eyes can look at the system and see where something might

20:17.280 --> 20:25.520
be wrong without a lot of the risk of just anybody having access, and as well as bad actors making

20:25.520 --> 20:29.440
it really easy for them to misuse the system. I'm not sure how much time there is.

20:31.120 --> 20:40.160
Just a quick question. Who's read the final text of the AI Act? Okay, cool. So we have,

20:41.040 --> 20:51.200
and what they have essentially compromised on is any model with a 10 to the 25

20:51.920 --> 21:00.080
flaw ratio would end up falling in the highly capable category or a model with systemic risk,

21:00.880 --> 21:06.480
and would therefore be subject to extra precautionary measures. I think that's a relatively

21:06.480 --> 21:14.480
decent threshold to have arrived at. It's not perfect, but I think we can agree that not all

21:14.480 --> 21:22.800
models are as dangerous as some of the most highly capable ones with systemic risk. So

21:22.800 --> 21:29.040
we're not really concerned about open sourcing models which have next to no potential to cause

21:29.040 --> 21:34.640
harm. And I think that's a misconception about the AI safety community versus the open source

21:34.640 --> 21:41.680
community. What we're really concerned with are the most capable models, like the models that are

21:41.760 --> 21:46.880
leading in the field, that are often produced by the largest companies. So again, for me,

21:46.880 --> 21:52.000
it comes back to any type of governance that we arrive at has to have some sort of mechanism for

21:52.000 --> 21:59.040
tracking who is accessing these models. Because not having that tracking mechanism is again in the

21:59.040 --> 22:05.360
interest of these companies. The less information they have about the downstream developer,

22:05.360 --> 22:09.280
the less responsible they are themselves. Because the minute that they know something

22:09.280 --> 22:13.280
is going on or that there's some sort of suspicion that harm could be caused in the future,

22:13.280 --> 22:21.520
they become liable. So I think a know your customer provision would be a minimum for any type of

22:21.520 --> 22:29.600
governance scheme. Thank you. I think I'd like to highlight again this specialization based on

22:29.600 --> 22:36.320
capability. The governance of open source models will depend a lot on what kind of model you're

22:36.320 --> 22:42.400
doing. A lot of models really do benefit from having open weights, having being able to clean

22:42.400 --> 22:48.000
the data sets. And I think this is specifically true for a lot of narrow AI. If you're doing

22:48.000 --> 22:54.400
classification models, then using open source classification models that were trained on data

22:54.400 --> 23:00.080
sets that a lot of people have looked at, pruned and sort of understand, even have done interpretability

23:00.080 --> 23:06.080
on to be able to know what's going on. I think this does significantly reduce risk compared to if

23:06.080 --> 23:12.560
the leading classification models are closed source and sort of don't know what's going on. So I would

23:12.560 --> 23:21.680
encourage like empowering open source to do narrow AI quite well. On the other hand, when we talk about

23:21.680 --> 23:28.880
the, yeah, there's this idea about front chair models who can do more, a wider array of tasks like

23:28.880 --> 23:35.040
coding and where people are trying to make them more and more autonomous by putting them in frameworks

23:35.040 --> 23:42.080
like auto GPT, which was is one of these open source frameworks. And this is why I think the open

23:42.080 --> 23:53.200
source community can sort of at least have some self governance. Like I believe that by, as long as

23:53.200 --> 23:59.040
the developers of these frameworks care enough about their safety, they will do much more secure

23:59.040 --> 24:06.960
products and not generally release products that by default harm the users who sort of just plug in

24:06.960 --> 24:12.640
the model, they let it autonomously write their code and actually it's sort of like use their credit

24:12.640 --> 24:22.000
cards to buy 10,000 euros of stuff. Yeah. Yeah, thank you for your point, you. And before I open up to

24:22.000 --> 24:30.080
your question, I have a last questions. Over last year, I was able to have the occasion to talk about

24:30.080 --> 24:37.840
these topics in a Python community, Linux community, also understanding how Mozilla is getting on board.

24:39.600 --> 24:45.600
So the open source community seems very interesting in these topics. So in your opinion, what the

24:45.680 --> 24:54.640
open source community can do and how it could be best interacting in a way that could be safe for all of us. And

24:54.640 --> 25:00.800
personally, from my experience, that lead for very interesting conversation about the impact, how we can

25:00.800 --> 25:07.600
develop in a safe way. So I'm very interested in your inputs as well. What do you want to kickstart

25:07.680 --> 25:18.560
the conversation? I see mainly three things where the open source community can contribute to AI safety.

25:18.560 --> 25:24.640
One would be participate in existing approaches or like stress testing, red teaming, finding

25:24.640 --> 25:30.960
vulnerabilities, all those kind of things. The second one would be to try to work out more

25:31.920 --> 25:38.800
existing approaches and or develop new approaches that handle the safety issues. So for instance,

25:38.800 --> 25:47.200
finding ways towards robust water marking. And the third one, which might be the most important one,

25:47.200 --> 25:55.360
is to raise awareness and adjust the own behavior and norms. So I mean, awareness of the risks that

25:55.440 --> 26:01.280
we've talked about, and again, especially for increasingly advanced systems, but also to the

26:01.280 --> 26:06.800
distinction that Jonathan mentioned before, that we talk about open source, but it's quite a different

26:06.800 --> 26:12.080
story if you talk about traditional software versus the most capable advanced AI systems.

26:13.200 --> 26:19.120
So for instance, for Linux, the open source community can find a bug and send them in and

26:19.200 --> 26:27.520
can be fixed. But if your AI system tries to talk you into suicide, what do you do? Please

26:27.520 --> 26:34.240
retrain the model. You can't do so much. And other things could also be about the openness.

26:34.240 --> 26:41.200
For instance, we talk about openness, open source as a very binary thing, kind of like

26:41.200 --> 26:48.640
bullying. You just say either it's fully open or it's fully closed, but it's more of a multi-dimensional

26:48.640 --> 26:54.000
vector of different things that you can make available or you can choose making less available.

26:55.760 --> 27:04.240
Yeah, let's go. Yeah, I don't have much to add beyond that, except that working in civil society,

27:04.240 --> 27:11.440
we're always open to hearing from the open source community. We are not the companies. We are

27:11.440 --> 27:16.960
representing people who are concerned about AI safety in general for everyone, for the world.

27:17.280 --> 27:24.960
So, yeah, so we're always open to collaboration in that sense. And there is a future where you

27:24.960 --> 27:30.160
can have, you can reap those benefits of open source that developers benefit from at the same

27:30.160 --> 27:35.440
time as ensuring there's some reasonable guardrails to prevent what we all don't want, which is,

27:35.440 --> 27:44.480
you know, absolute catastrophe. Thank you. I'd like to highlight the work particularly done by

27:44.560 --> 27:50.800
Illyuther AI. Illyuther AI, who originally were some of those who trained again, like some of the

27:50.800 --> 28:00.000
first open-weights, large language models, replications of GPT-2, and who have pivoted in the

28:00.000 --> 28:06.400
last two years to also contribute to interpretably research and fundamental scientific research to

28:06.400 --> 28:13.520
understand deep learning models. And so the open source community has this advantage of having

28:13.520 --> 28:19.520
highly motivated people who are interested in the technical aspects and who will, like, go quite

28:19.520 --> 28:26.720
into detail even just for the passion of it. And we're seeing very good papers being published by

28:26.720 --> 28:32.400
teams like Illyuther AI's. And that's not the only org that works with open source

28:32.400 --> 28:41.760
large language models. So in terms of the scientific understanding, a lot of, like, the current level

28:41.840 --> 28:46.880
of large language models are already a fascinating artifact to study. And I'd encourage open source

28:46.880 --> 28:54.880
community to keep contributing to the advancements of interpretability and control, controlling

28:54.880 --> 29:01.200
the whole class of engineering systems, or how do we monitor what the machine, like, what is this

29:01.200 --> 29:07.360
model actually doing? Can we understand? Can we make it go to particular branches rather than not?

29:07.360 --> 29:11.760
Even, like, a lot of the prompting techniques which are known today have been discovered by people

29:11.760 --> 29:18.720
tinkering with their systems. So furthering our understanding keeps being a good thing. And I'd

29:18.720 --> 29:24.240
encourage the open source community to keep on, like, doing research and interacting in this way

29:24.240 --> 29:32.160
with these models. Also, another recent development regarding the AI Act is that now they're looking

29:32.160 --> 29:37.200
at setting up the AI office. So if you're someone who enjoys tinkering with these types of models

29:37.200 --> 29:44.240
and is interested in safety in general, there's a lot you can contribute to the field through an

29:44.240 --> 29:52.480
institutional group like the AI office, which is looking for highly capable people that have a technical

29:52.480 --> 29:59.920
background. Thank you. I hope you took notes. I took mentally notes, but I'll see you again

29:59.920 --> 30:06.960
in the recording of this. Do you have any questions for the public right now? Otherwise, I think, okay,

30:06.960 --> 30:11.920
I'll come there. Okay. I can't hear. Yes.

30:12.160 --> 30:31.120
Hi. Yeah. So we've spoken a lot about the harms that could come from bad actors. I wanted to ask about

30:31.120 --> 30:38.400
potentially harms that happen with big tech or large powerful organizations having access to

30:39.120 --> 30:44.000
behind closed doors, having access to this technology, and whether you believe that there's

30:44.000 --> 30:48.400
a need for access to their source code or from some kind of regulator or something like that?

30:55.120 --> 30:58.800
Coming. I think I want to...

30:59.040 --> 31:08.960
Yeah, I think that's right. There's definitely risks from that centralization of power. And I think

31:09.840 --> 31:17.200
it's very important how exactly we tackle that. So for instance, if you just require them to make

31:17.200 --> 31:23.440
everything open source, I don't think... I mean, then you don't have... You have that risk a bit

31:23.440 --> 31:29.440
reduced that they do things behind closed doors that are not visible. But I think if you do something

31:29.440 --> 31:35.280
like vetted researcher access, then you can approach it in a much safer way and kind of get

31:35.280 --> 31:43.280
the better deal in terms of balancing the risks and the benefits. This is a very crude analogy, but

31:44.080 --> 31:49.040
it's sort of the same thing with, let's say, if you had a bio weapon, right? Let's say we were

31:49.040 --> 31:57.040
developing this poisonous gas that has a capability to poison a lot of people all at once. It's pretty

31:57.040 --> 32:02.000
bad if there's a couple companies developing it and we have no idea what's happening behind closed

32:02.000 --> 32:06.320
doors, because again, they're companies, they're not state actors, there's no checks and balances

32:06.320 --> 32:12.480
in that sense. And until very recently, there was no regulation to have any access to what they're

32:12.480 --> 32:17.760
doing. On the other hand, we also don't want everyone to have their personal poisonous gas

32:18.480 --> 32:26.000
in the sense that suddenly the risk has been magnified by the fact that everyone has it at

32:26.000 --> 32:34.240
the same time. It's not like a nuclear nonproliferation system where multiple actors having the weapon

32:34.240 --> 32:42.080
would reduce the risk overall. So I think in general, as Felicity said, transparency is important,

32:42.160 --> 32:47.600
but also checks and balances are important. And there are mechanisms for democratizing the

32:47.600 --> 32:53.520
technology in a way that allows us to keep an eye on it and it doesn't just proliferate.

32:54.960 --> 33:01.920
Can I add one quick thing to that? So this narrative of democratizing AI, right? I think

33:01.920 --> 33:11.440
it's one that Big Tech tries to push and talks a lot about that. And I think it's kind of interesting,

33:11.440 --> 33:17.760
because democratizing means kind of like moving towards democracy, right? And that means kind of

33:17.760 --> 33:23.920
making decisions together of like, how do you want to govern a specific technology, for instance?

33:23.920 --> 33:30.160
But within AI, how it's usually used, it means availability, like access to the weights. But

33:30.160 --> 33:35.520
that is not really making a shared decision of how we want to use it. It's more like, like Alex said,

33:35.520 --> 33:40.800
like giving it to everybody. It's more like, yeah, not governing it at all. Anybody can do

33:40.800 --> 33:47.440
whatever they want to do with it. Maybe anarchization would be a better change. I'm not sure, but

33:47.440 --> 33:54.320
it's just like interesting how this is called, but it's not actually that, but it gives it a very

33:54.320 --> 34:05.360
positive spin to it that can be quite misleading, I think. So there's a question from the online

34:05.360 --> 34:12.320
audience, which I'm going to read out loud. So Frank, is there a reason for confidence that a

34:12.320 --> 34:19.280
smaller than 10 to the power 25 flop AI model cannot be trained or tuned for just as much harm as

34:19.280 --> 34:27.200
larger general ones? So, yeah. No, there is no confidence, but we fought tooth and nail to get

34:27.200 --> 34:34.960
it at that level and not a higher one. So this is basically the lowest threshold that was able to be

34:35.120 --> 34:41.040
achieved politically. So no, it wasn't a threshold that was determined through scientific, you know,

34:41.040 --> 34:48.320
studies. But and we have plenty of evidence that a model at 10 to the 24 would be harmful and

34:48.320 --> 34:54.880
potentially dangerous as well. But but yeah, the answer is just this is the best we could get

34:54.880 --> 35:00.160
politically. And thankfully within the AI act, there are other mechanisms for determining,

35:00.240 --> 35:05.600
for classifying a model as having systemic risk besides this threshold. So this is sort of a

35:05.600 --> 35:12.320
presumption. If you fall above it, then you're presumed to have systemic risk. But then the AI

35:12.320 --> 35:17.680
office, for example, will have the discretion to also monitor and determine whether a model has

35:17.680 --> 35:22.720
systemic risk through a much more detailed criteria that gives them that flexibility.

35:23.440 --> 35:32.720
I'd also like to highlight how this value will change in the future, because I gave I showed you

35:32.720 --> 35:41.040
the algorithmic efficiency graph where you need less flops to achieve a particular level of capability

35:41.680 --> 35:51.280
as our science of like how to do the training runs evolves. And so this is a maybe a stopgap

35:51.280 --> 35:56.560
measure where we think we're pretty sure we don't understand how these models work above 10 power

35:56.560 --> 36:02.880
25. And we want to do more science. But also we want people to spend the time to actually analyze.

36:02.880 --> 36:08.320
Like if you're going to do something that's so hard to analyze, please put more effort,

36:08.320 --> 36:15.760
comparatively more effort to analyze it, because it does have in general more unknown capabilities.

36:16.720 --> 36:25.920
And so having there are different terms for having evaluations that depend on how competent the model

36:25.920 --> 36:33.680
is. And in the future, we can imagine more targeted laws that allow you to see, well, what kind of data

36:33.680 --> 36:40.320
are you training on and what kind of capabilities you have, and actually having a risk factor which

36:41.040 --> 36:46.960
depends more on the training data. But it isn't like these things have moved so fast

36:48.240 --> 36:55.040
that governance, governments, sorry, were not able or generally are not able to follow at that kind

36:55.040 --> 37:02.320
of pace of progress. And so I do understand it more of a stopgap than as the best way to govern

37:02.320 --> 37:11.200
these models in the future. And so as institutions are being constructed in different governance,

37:11.200 --> 37:18.800
there are institutions called the AI Safety Institute in the UK, in the US, and you mentioned

37:18.800 --> 37:27.280
one for the EU, the AI office. Now, I think joining, like people working at these organizations will

37:27.280 --> 37:33.120
be able in the future to more closely monitor what it is that makes a model dangerous or doesn't,

37:33.840 --> 37:39.840
capabilities like autonomous replication, and then we'll have maybe more sensible

37:40.880 --> 37:49.040
regulation in that approach. I'll quickly answer a second question from the text, and then we'll

37:49.040 --> 37:54.320
send it back to the room again. It's just a detail where someone asks, where does the

37:54.320 --> 37:58.480
European Network for AI Safety get its funding? And the answer is that we have received a grant

37:58.480 --> 38:05.600
from an organism called Lightspeed Grounds. Yeah, so does someone in the audience again have a question

38:06.240 --> 38:14.080
or comment about these? Is someone up high in the room? Sorry, can you raise your hand again?

38:15.760 --> 38:20.480
But we'll be happy to stay also afterwards outside to catch more questions.

38:21.200 --> 38:27.920
A lot of the safety problems are related to the data and where the data comes from, and obviously

38:27.920 --> 38:34.960
Big Tech will never expose their weight because that's their IP. I'm working on a data provenance

38:34.960 --> 38:42.400
solution, trying to build one, and I'm getting actually really insightful thoughts from this

38:42.400 --> 38:49.200
discussion. I believe that a couple of things in tech are out there like ZKML that can contribute

38:49.200 --> 38:54.320
to this whole thing, where Big Tech should not disclose their weights, but through ZKML,

38:54.320 --> 39:00.800
they can expose that the weights they used were safe, and the proof on chain would improve that

39:00.800 --> 39:08.240
that model is actually safe. Is that a thing you believe in or is that rubbish? Thank you.

39:19.440 --> 39:23.920
Sadly, the acoustics are not so good and so I didn't totally get it.

39:32.400 --> 39:34.640
Can you shout again just the core of your question?

39:37.040 --> 39:43.360
Yeah, let's talk afterwards. Like you mentioned the importance of the data and the

39:43.440 --> 39:50.160
capabilities in data. I think that's quite true, but like for specific technical discussion,

39:50.160 --> 39:57.680
let's talk after. So thank you again for the Jonathan Felicity and Alessandra,

39:57.680 --> 40:00.640
and we're here to talk and continue the conversation. Thank you.

40:08.560 --> 40:12.320
Awesome, y'all. We're going to be speaking or we're going to start our next talk here in about

40:12.320 --> 40:18.320
five minutes. It'll be right here in a second.

