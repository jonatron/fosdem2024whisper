WEBVTT

00:00.000 --> 00:11.400
Hello everyone. My name is Marcus. I would like to share some lessons that I learned while

00:11.400 --> 00:17.360
working on the Firefox Profiler. So yeah, I work at Mozilla. I'm on the Firefox Performance

00:17.360 --> 00:23.800
Team and I work on Firefox itself and on the Profiler and I also have my own Profiler called

00:23.800 --> 00:30.280
Sample which you can use on macOS and Linux to profile your own applications. And I'll give you

00:30.280 --> 00:34.560
a brief overview of the Profiler. So this is what it looks like when you have a profile loaded.

00:34.560 --> 00:41.040
You have a timeline at the top. You have a call tree and you have a sidebar here in the call tree.

00:41.040 --> 00:47.880
It's very, very small down there. I'll zoom in a little bit. In the call tree you can see which

00:47.880 --> 00:55.440
function called which function. You can see how much time each function spent running. So let's say

00:55.440 --> 01:02.240
this function here dispatch event. Firefox Profiler is a sampling Profiler. So it interrupts the

01:02.240 --> 01:07.880
thread at a given interval like usually one millisecond every one millisecond. It checks what's on

01:07.880 --> 01:15.040
the stack and then it accumulates that into this call tree. So one thing that I want to call out

01:15.080 --> 01:20.440
here is the category breakdown in the sidebar. So here we have a bunch of categories. User is

01:20.440 --> 01:27.280
just regular code. Ion here means this is JavaScript code that was jitted into the IonMonkey

01:27.280 --> 01:35.080
subengine of our JavaScript engine. And yeah, there are a bunch of other categories. And you

01:35.080 --> 01:40.320
can select in the timeline and as you draw your selection, oops, as you draw your selection,

01:40.680 --> 01:47.280
the category breakdown updates in the sidebar. So we can also zoom in, see something a little

01:47.280 --> 01:53.160
smaller. So here we have more code in Ion, more code in the user category. It also has a flame

01:53.160 --> 02:00.960
graph. So zoom back out. And flame graph, you're probably familiar with flame graphs. They're a

02:00.960 --> 02:07.080
different representation of the same information. Like you have a call tree, you have nested functions,

02:07.640 --> 02:14.840
the width of each box is the time that is spent in that function. And we also have a tooltip here

02:14.840 --> 02:21.080
in the call tree, which again gives us a category breakdown. I'm emphasizing the category breakdown

02:21.080 --> 02:25.520
so much because we're going to implement our own profiler in a minute, which, and we're going to

02:25.520 --> 02:32.280
focus on calculating this category breakdown. So here we see it's a bit sluggish as you move the

02:32.320 --> 02:38.640
mouse around, because it actually needs to iterate over all the samples in the timeline. It checks

02:38.640 --> 02:44.640
for every sample is the stack inside of the function that you're hovering. If so, check the

02:44.640 --> 02:49.800
category that the CPU is spending its time on for that sample, accumulate that into a map of

02:49.800 --> 02:58.080
categories to counts, and yeah, do that for all the samples. And we can see here at the root node,

02:58.160 --> 03:05.640
we have about 500,000 samples in this profile. So what I didn't tell you is this is actually the

03:05.640 --> 03:15.800
version from last July. And I fixed this performance problem here. So this is what's live now on

03:15.800 --> 03:22.600
profile.farfax.com. Hovering these boxes is no instant. And it's still doing the work. It's still

03:22.640 --> 03:28.360
going through all 500,000 samples every time you move your mouse. So I want to talk a bit about

03:28.360 --> 03:38.240
how we can crunch through all that data in in very short time. Wrong direction. So yeah, even with

03:38.240 --> 03:46.120
lots of samples, we can now have a fast UI. And I made an example project just for this talk

03:46.880 --> 03:52.920
called mini profiler. It is on GitHub. It's also live on Netlify. You can try it out in your browser

03:52.920 --> 04:02.640
if you want to. And this is what it looks like. It has a very reduced feature set, but it also has

04:02.640 --> 04:09.360
this timeline. You can select parts of the timeline and it calculates this category breakdown. So

04:10.320 --> 04:18.640
yeah, let's say here, we spent 30% in Ion Monkey Jitter JavaScript code. At the same time, it also

04:18.640 --> 04:25.680
calculates the heaviest stack. The heaviest stack is the stack that we spend the most samples in.

04:25.680 --> 04:36.240
All right. So yeah, mini profiler features. There's only two features. You select the range, and it

04:36.320 --> 04:42.560
gives you a category breakdown and a heaviest stack. So how does it calculate that? We have an input

04:42.560 --> 04:49.200
JSON, which describes the profile contents. The JSON is a list of samples. Every sample has a

04:49.200 --> 04:52.400
time, a weight, and a stack. Every stack is an array of frames. Every frame has a name and a

04:52.400 --> 05:01.760
category. I'll show you that here in an example. So as I said, a list of samples, every sample has a

05:01.760 --> 05:07.840
time property, a stack property, a weight property. The stack is an array. Each stack frame has a name

05:07.840 --> 05:19.520
and a category. To calculate the category breakdown, we take in the profile. We take in a range of the

05:19.520 --> 05:26.080
indexes of the samples that are in the selection. Then we iterate over this range. We get each sample.

05:26.640 --> 05:32.240
Whoops. We get its stack and its weight. We get the top frame from the stack. We get the category

05:32.240 --> 05:39.600
from the frame. And then we check. Does our map have this category already? If so, get the current

05:39.600 --> 05:46.480
value. Otherwise, default to zero. We add the weight of the current sample. We put the sum back into

05:46.480 --> 05:56.480
the map. And then this map is what gets used by this spelt component. For the heaviest stack, it's

05:56.480 --> 06:03.200
somewhat similar. We again iterate over all the samples in the selected range. For each sample,

06:03.200 --> 06:10.320
we again get the stack and the weight. And now we need to check if this stack has been used by

06:10.320 --> 06:17.760
multiple samples. And how do we find two samples with the same stack? Well, the stack is an array,

06:17.760 --> 06:22.400
and you can't really check them for equality easily. So what I'm doing here is I'm stringifying the

06:22.400 --> 06:28.640
stack into a JSON string. I'm using that as the map key. And then here is a similar issue to what

06:28.640 --> 06:33.680
we had with the category breakdown. We check. Do we have an entry in the map for this stack?

06:33.680 --> 06:39.440
If so, take its current value. Otherwise, default to zero. Add the weight. Put that back into the map.

06:40.880 --> 06:47.520
And if this stack is the heaviest that we've seen so far, we remember it, and then at the end,

06:47.520 --> 06:53.600
we return it. So these are the two main algorithms in this mini-profiler. Category breakdown,

06:53.600 --> 07:00.560
heaviest stack. Both of them have to iterate over all the samples. So how fast is it?

07:02.800 --> 07:07.840
So if I select here, it's reasonably fast. If I make the selection bigger, it starts getting a

07:07.840 --> 07:15.440
little janky. I'm computing some throughputs down here. So 100 nanoseconds per sample is how

07:15.440 --> 07:22.000
long the algorithm for the category breakdown takes. And 30,000 something nanoseconds per sample

07:22.000 --> 07:25.760
for computing the heaviest stack. Because, yeah, we saw the heaviest stack algorithm,

07:25.760 --> 07:31.600
it was really inefficient. It used JSON stringify. It looked up this gigantic string in a map.

07:31.600 --> 07:36.640
It needs to hash the entire big string and so on. So this is obviously not the way to go. But this

07:36.640 --> 07:45.200
is just a place to start so that we understand what's going on. So this is the throughput here.

07:46.400 --> 07:52.720
The nanoseconds per sample might not tell you much. But what you can think about is, how does it

07:52.720 --> 07:57.600
limit the size that you can handle while still being responsive? So let's say you have 100,000

07:57.600 --> 08:04.640
samples. In this example here, we just had 1,600 something samples. What if you have 100,000?

08:04.640 --> 08:09.120
Then you get 10 milliseconds for computing the category breakdown and 3.6 seconds for computing

08:09.120 --> 08:14.800
the heaviest stack. 3.6 seconds per update, that's not acceptable. So we need to do something.

08:16.000 --> 08:21.280
And also the JSON file, because it has all those repeated stacks, it's just massive.

08:22.960 --> 08:30.320
So let's use a different format, different JSON format. Here I made a V2 format. It still has

08:30.320 --> 08:35.520
samples, but instead of having the stacks right in the sample, it just has an index. And this

08:35.520 --> 08:43.520
index now goes into a stack list. Each element in the stack list has a frame index, which goes

08:43.520 --> 08:48.880
into the frame list. Each frame has a name and a category index, which goes into the category list.

08:50.400 --> 08:54.400
So I hope that's not too overwhelming. We just have a bunch of indexes now. Instead of nested

08:54.400 --> 09:01.280
objects, we just have some side-by-side lists and we index into them. And also here the stacks

09:01.280 --> 09:07.040
are a little special because of this parent stack index here. So if, for example, a sample

09:07.040 --> 09:13.520
refers to stack number two, then this is the frame at the top of the stack. Then we go to the parent

09:13.520 --> 09:20.800
stack, find this frame, that's the next frame on the stack, find this stack, put this frame on the

09:20.800 --> 09:23.920
stack, and then the parent here is null. So that means we're at the end of the stack.

09:27.280 --> 09:34.560
Hope I haven't lost anyone yet. So let's go back to the compute-heavy stack algorithm. So we were

09:34.560 --> 09:41.600
iterating over the samples. We were stringifying the stack arrays and we were checking the JSON

09:41.600 --> 09:47.360
string. Now we don't need to do that anymore. Now we have an index. If two samples have the same

09:47.360 --> 09:52.320
stack index, that means they have the same stack. So we just use the stack index now here and we

09:52.320 --> 09:56.880
don't need the JSONification. We don't look up big strings. And this is like a massive performance

09:56.880 --> 10:06.080
improvement. So 300 times faster. The category breakdown is also affected by the new format

10:06.080 --> 10:13.440
changes. So now instead of getting the stack and the frame directly from inside the sample,

10:14.080 --> 10:17.440
we instead get a stack index. We look up the stack index in the stack array,

10:18.080 --> 10:22.320
which gives us a frame index. We look that up again, get the category index, look that up again,

10:22.320 --> 10:27.840
get a category name. This is a string. Put that in the map or add up the weight.

10:32.160 --> 10:38.800
This string here, this is kind of unnecessary. We know if two samples have the same category index,

10:38.880 --> 10:44.960
we can use that as the key. So I made an optimization here to remove this name lookup.

10:44.960 --> 10:50.720
And now we're just accumulating category index weights in this map here. There needs to be

10:50.720 --> 10:57.760
some process, post-processing afterwards to make sure we get these names here in the category

10:57.760 --> 11:05.440
breakdown again. But that's outside of our algorithm. All right. So here I had selected

11:05.440 --> 11:13.200
the small profile for the format version one. Let's switch to the same profile in format version two

11:14.480 --> 11:20.720
and do the selection again. And now we can see, we can select to the full width and it's still

11:20.720 --> 11:28.800
very responsive. So here's our throughputs. So how fast is it now? 47.1 nanoseconds per

11:28.800 --> 11:33.040
sample for the category breakdown is what I measured, 51 for the heaviest stack.

11:33.520 --> 11:41.920
Okay. So that's much better. Let's see how far we can go. We want to see if there's more we can do

11:41.920 --> 11:49.120
here. So we use a profiler. I am going to start the profiler. Oh, what I didn't show you is how to

11:49.120 --> 12:00.240
use the profiler. Well, let me do that really quick. So if you use Firefox and you go to

12:01.200 --> 12:06.080
profiler.firefox.com, you can click this big button here, which gives you a toolbar button.

12:06.960 --> 12:13.440
And then if you click that toolbar button, it starts recording. So let's record our current

12:13.440 --> 12:19.440
implementation. Do a little bit of this, capture a profile and see where the time is spent.

12:22.080 --> 12:23.440
Well, where is the time spent?

12:26.800 --> 12:27.440
One second.

12:30.240 --> 12:35.840
Let's try that again.

12:50.000 --> 12:51.440
Let me refresh this page.

13:01.120 --> 13:11.520
Ah, I can tell you for this time spent. It is so fast that it barely shows up in the profiler

13:13.600 --> 13:17.920
because we are still using the small profile size. So let's do that again.

13:19.680 --> 13:25.920
Capture profile. The local host here, there's barely any CPU usage.

13:26.000 --> 13:32.800
You would see more yellow in here. So let's switch to a bigger profile. We still have just the

13:33.360 --> 13:42.240
1600 samples. Let's switch to the medium profile. So here, yeah, it still works okay.

13:43.040 --> 13:48.880
It gets a little bit janky towards the edge here. So again, we're going to start the profiler,

13:49.520 --> 13:52.640
select, play around a little bit so that we get lots of samples.

13:53.600 --> 13:59.920
Capture the profile. And there we go. This is what I was expecting. So now we have lots of yellow

13:59.920 --> 14:05.360
in here. I'm going to show just this thread. I'm going to switch to JavaScript only. I'm going to

14:05.360 --> 14:10.560
switch to the flame graph. And now what we can see here is we are spending time in compute category

14:10.560 --> 14:17.440
breakdown with string key map and compute heaviest stack with map. And what we see here is that we

14:17.440 --> 14:23.040
are spending some time in map.prototype.set, both over here and over there. That makes sense. We're

14:23.040 --> 14:31.360
assigning things to a map. So can we not use a map? Wrong direction here. So we're seeing the time

14:31.360 --> 14:37.920
in map prototype set. We have the map here. For the category breakdown computation, we're getting

14:37.920 --> 14:43.040
the category index out and putting it back in. But we know these are integers. They're integers

14:43.040 --> 14:47.520
into the category list. The category list doesn't have lots of elements. We can just use an array

14:47.520 --> 14:53.040
here instead. I'm going to use a float 64 array here. Because the weights are floats,

14:54.480 --> 14:59.760
using a typed array means I know that the maximum number of elements is already preallocated. It's

14:59.760 --> 15:04.720
initialized to zero. I don't need to check if there's something in it already. I know that it

15:04.720 --> 15:12.240
starts with zero. I can just add the weight. And that's it. We can do the same modification to

15:12.240 --> 15:18.320
getting the heavier stack, the seriously compute heavy stack algorithm. It was also using a map.

15:19.040 --> 15:23.440
We can use a float 64 array because we know how many stacks there are. Here the key is the

15:24.000 --> 15:31.920
index into the stacks array. We use that key as our index into the map array. And then it should

15:31.920 --> 15:37.440
work as before. And what we see down here, it is three times faster to skip the map to use a typed

15:37.520 --> 15:43.760
array instead. Let's try that out. Here I'm going to switch from the basic implementation to the

15:43.760 --> 15:51.280
integer keys for category breakdown. No, sorry, to the typed arrays instead of maps implementation.

15:51.840 --> 15:59.600
And now I'm going to select, and it's very smooth through the entire profile. And we have 500,000

15:59.600 --> 16:06.080
samples now here. And we are still responsive. And let's see if we get an even bigger profile.

16:06.080 --> 16:12.640
This one here has two million samples. How responsive are we? It's okay. It gets a little

16:12.640 --> 16:22.560
janky towards the end here. It's mostly okay. So where are we now? Let's just take some,

16:23.680 --> 16:28.160
take some recap. We've addressed the obvious load ons. We've done what the profile told us.

16:28.160 --> 16:33.680
We fixed the hotspots. We changed the format so that comparing stacks is cheap, we changed

16:33.680 --> 16:39.920
two maps into typed arrays. Got us a 3x perf boost. In the heaviest stack case,

16:41.360 --> 16:47.840
the map or the amount of memory we're using might be a bit bigger now because we're allocating an

16:47.840 --> 16:53.200
array where we have an element for every single stack index, even if no sample references that

16:53.200 --> 17:01.520
stack index. So maybe some extra memory, but we have a performance boost. And so we have the

17:01.600 --> 17:10.240
throughput here. Yeah. So for the medium profile, our throughput is like 16 nanoseconds. Or

17:10.960 --> 17:16.480
let's see, sometimes it goes up and down a little bit. Yeah, let's say 16 nanoseconds for the category

17:16.480 --> 17:21.440
break down, 40 nanoseconds for the heaviest stack. I was seeing some other numbers when I was trying

17:21.440 --> 17:28.800
this at home. So it's pretty impressive. Modern computers are pretty fast, but maybe we can do

17:28.800 --> 17:37.680
even better. So let's try better. Let's go back to the category breakdown algorithm. We are taking

17:37.680 --> 17:43.200
these two values out of every sample. The sample is an object. It has three properties. We're

17:43.200 --> 17:47.760
ignoring the time property. We're getting these two properties out. So what does that mean at a byte

17:47.760 --> 17:57.200
level? So how are arrays of objects stored in memory? Well, it depends a little bit on which

17:57.200 --> 18:03.680
JS engine you're using, how you're allocating the object, if you happen to be on a fast path or not.

18:04.320 --> 18:10.880
But in SpiderMonkey, this is what you might expect. So we have a samples array, which is backed just

18:10.880 --> 18:18.320
by a list of pointers. Every pointer takes up 8 bytes on a 64-bit system, and it points to a JS

18:18.320 --> 18:24.640
object. So let's say here, the first entry in our samples array points to this JS object here.

18:25.280 --> 18:31.680
The JS object starts with a header. SpiderMonkey takes up 24 bytes on a 64-bit machine.

18:32.400 --> 18:38.240
Then if we're lucky, we have the fields inline just after the header. We might not be lucky,

18:38.240 --> 18:43.120
but let's say we're lucky. We might also have a bit of padding here at the end, because the inline

18:43.680 --> 18:48.560
slots might be only sized to four or eight, and we're using three properties here, so there might

18:48.560 --> 18:55.120
be a bit of extra memory used up by that. So this is just one representation that we could have.

18:55.120 --> 19:00.800
It varies a lot by engine. For example, Chrome has pointer compression, so these things here might

19:00.800 --> 19:06.560
be four bytes each, but then the time field might be an extra pointer, because in Chrome,

19:06.560 --> 19:12.560
sometimes the floating point values are a separate heap allocation. The padding could vary, the

19:12.560 --> 19:17.120
object header size could vary. These fields here could be behind another pointer if they're stored

19:17.120 --> 19:23.280
out of line, and so on. But anyway, what it comes down to is we wanted to get these two fields here,

19:24.720 --> 19:31.520
16 bytes in total, but what we ended up with is all of these other not-so-useful bytes

19:32.320 --> 19:38.640
clogging up our cache. So when the CPU wants to get those bytes, it gets them in 64-bit chunks.

19:38.720 --> 19:49.840
Cache line is 64 bytes. So if you're getting this value here, you're getting the other bytes that

19:49.840 --> 19:54.480
are in the vicinity, even if you don't need them. Well, here we do need the JS object header,

19:54.480 --> 19:59.360
because the JIT needs to check that the object is of the right shape, and so on. But we really just

19:59.360 --> 20:07.440
want those values here. So can we do anything about that? We want to improve our cache line

20:07.440 --> 20:14.560
utilization, and we want to reduce the indirection. Maybe we can. Let's do something radical.

20:15.360 --> 20:22.560
Let's turn everything on the side. So we have this array of objects. What we could do instead

20:22.560 --> 20:27.760
is to have an object of arrays, or struct of arrays, where we have a column,

20:28.880 --> 20:35.280
or where we have just one key for the time column with a big array that has just the time values,

20:35.280 --> 20:39.920
one for the stack index, just the stack index values, the weight, just the weight values,

20:40.480 --> 20:44.400
and a length stored on the side. These arrays must all have the same length.

20:46.880 --> 20:52.720
So now everything's backwards. If we want to access the weight in the past, we had samples i.weight.

20:52.720 --> 20:56.000
Now it looks a bit weird, because we have the sample table.weight column,

20:56.560 --> 21:03.520
and then we get the ith element of that. But let's do it. Let's see where it goes. And so what

21:03.600 --> 21:08.720
we end up with here is a new profile format again. Now we have a sample table, a stack table, a frame

21:08.720 --> 21:15.840
table. The calories are still a list, because it's just some strings. And same thing as before,

21:15.840 --> 21:19.360
the stack index goes into the stack table, the frame index goes into the frame table.

21:19.920 --> 21:22.320
We just need to access the properties differently.

21:24.880 --> 21:30.960
So what does it do for the computation of the heavier stack? Here we were getting the stack

21:30.960 --> 21:34.800
index and the weight property from an object. Now we just get them from separate columns.

21:35.680 --> 21:39.520
And already we're seeing a 2x performance improvement.

21:41.360 --> 21:45.440
For the category breakdown, similar story. Instead of getting the properties from objects,

21:46.000 --> 21:53.440
we get the column first, access the ith element, and get that. This here is even faster, like 3.5x

21:53.440 --> 22:01.360
faster. Let's see that in practice. So we're switching to format v3 now, struct of arrays.

22:02.080 --> 22:09.440
Let's get the medium, medium sized profile. And now it just flies. It's just responsive all the way.

22:10.160 --> 22:14.240
4.5 nanoseconds per sample, that's really not a lot of time. This is super fast now.

22:14.960 --> 22:19.120
Let's get an even bigger profile. Still super responsive.

22:24.160 --> 22:32.320
So when we think about the memory model, or the memory, how it is represented in memory again.

22:33.040 --> 22:35.840
We're accessing these columns now. We're accessing them in order.

22:36.720 --> 22:43.200
And what happens is that our cache lines are now fully utilized. We don't have object headers

22:43.200 --> 22:46.400
clogging up our cache anymore. We just have the numbers that we wanted.

22:46.640 --> 22:53.360
But yeah, it's just super efficient now. We get all the stack indexes, we got all the weights.

22:54.080 --> 22:58.160
The time column is now pretty much irrelevant. It was clogging up our cache before,

22:58.160 --> 23:02.480
but now we're not accessing the time column at all. So it just doesn't bother us anymore.

23:06.320 --> 23:12.080
Okay, so let's recap quickly. We have a struct of arrays. Some people call it parallel arrays,

23:13.040 --> 23:19.680
commonly used in game engines, databases, and so on. It has a few drawbacks. It looks a bit

23:19.680 --> 23:23.760
backwards if you read it. Sometimes when you want to pass around an object, you need to manually

23:23.760 --> 23:30.880
materialize it because you don't just want to pass around an index. But it also means that the

23:30.880 --> 23:35.120
type system, at least in TypeScript, is now less of a help. We can introduce mistakes that it

23:36.000 --> 23:44.000
wouldn't catch. So for example, if we build up our arrays and we end up not putting our values in

23:44.000 --> 23:50.400
every one of the columns, we end up with mismatched lengths, and that is hard to catch at the type

23:50.400 --> 23:55.360
level. Also, when we pass around indexes, sometimes, yeah, you get a number, you don't really know,

23:55.360 --> 23:59.360
is this an index into the stack table, into the frame table? I don't know. The type system,

23:59.360 --> 24:02.960
at least in TypeScript, I don't think is well set up to catch these kinds of mistakes.

24:03.600 --> 24:07.520
But it's much more cache efficient. It's easier on the garbage collector. You need to traverse

24:07.520 --> 24:13.040
fewer objects. Some engines skip the contents of arrays and numbers, so it should speed up that too.

24:13.040 --> 24:20.720
Less memory overhead from object headers and padding. And we can just treat columns separately.

24:20.720 --> 24:26.000
Like sometimes we want to make a change to one column. Let's say we want to shift the entire

24:26.000 --> 24:31.760
profile by some time delta. We can change just the time column. The other columns stay untouched.

24:31.760 --> 24:39.840
We don't need to recreate any objects. And it also gives us a little more control over sizes and how

24:39.840 --> 24:44.960
compact our integers or our numbers are stored. We can pick with our typed array. We could pick

24:44.960 --> 24:50.560
an int32 array. We could pick an int16 array. If we know what the domain of our values are,

24:50.560 --> 24:56.720
we can store things more compactly and we get back in control of the representation.

24:57.680 --> 25:04.000
Okay. I want to make it even faster. So if we look back at our category breakdown,

25:04.000 --> 25:07.520
we're getting the stack index, we're getting the frame index, but it's all just to look up the

25:07.520 --> 25:11.040
category from the frame table. We're not really interested in the stack of the frame. We just

25:11.040 --> 25:17.920
want the category for a sample table, for a sample. So what if we just got the categories for

25:17.920 --> 25:24.720
each sample and use that instead of here, stack, frame, category, just go category, boom. Well,

25:25.360 --> 25:29.360
it would be great if we had this column. Where does it come from? Well, we can compute it here

25:29.360 --> 25:37.760
with the get sample categories method. We iterate over all the samples. We do the stack frame category

25:37.760 --> 25:43.440
conversion here. We cache that in the sample categories column. We pass that to our existing

25:43.440 --> 25:49.440
function, but we only want to do this once, not on every call. So we need to cache it somewhere.

25:49.440 --> 25:55.040
We can use memorization for that. So here's a memorized call. We get the profile. We only run

25:55.040 --> 26:00.000
this once. So if we call this multiple times with the same profile, let's say our profile is immutable,

26:00.000 --> 26:04.080
we have it cached from last time. And we can make the caching even more precise.

26:04.800 --> 26:10.800
If we memorize a function which takes just the columns that we need, then we get it. We wrap

26:10.800 --> 26:15.360
this into the existing get sample categories function, which takes the profile, but then it

26:15.360 --> 26:20.640
takes out the categories. Sorry, it takes out the columns we want, passes those separately to the

26:20.640 --> 26:28.640
memorized function, and that makes the caching even more or even tighter. If you touch a column

26:28.640 --> 26:36.320
that is not involved, you don't invalidate your cache. And did it work? Yes, it did. Oops,

26:36.320 --> 26:44.320
wrong direction again. Memorized sample categories. We're now down to three nanoseconds.

26:45.440 --> 26:56.400
So I'm basically done with the talk. Let's just look at the graph here at the end. This V1 graph

26:56.400 --> 27:01.360
is off the charts like this. It's way higher than this. But we made it faster with every change here.

27:02.320 --> 27:08.320
And this last step of caching the sample, the categories for each sample, it looks like it's

27:08.320 --> 27:14.880
not much, like 25% on these nanoseconds. But what it actually means is we can handle more data.

27:15.360 --> 27:22.640
We can handle a higher count of samples in, let's say, a 16 millisecond interval. And like 25%

27:22.640 --> 27:33.600
more data, that's massive. Okay, I want to say really quick, what is data-oriented design? It's

27:33.600 --> 27:38.480
a mindset and it's a collection of techniques. The main technique here is structure of arrays.

27:39.440 --> 27:44.320
The mindset is more about how you think about it. The shape of the data determines the algorithm

27:44.320 --> 27:49.520
and its performance. You need to know which things are small, which things are big. We might have

27:49.520 --> 27:53.920
seven elements in this array and 100,000 in that array. If you keep that in mind, you're better

27:53.920 --> 27:59.200
set up to write fast code. And if you also think about cache line utilization, you're even better

27:59.200 --> 28:05.120
set up. The rest is not that important. Thanks, everyone.

28:05.120 --> 28:22.880
You can find me in the Firefox Profiler channel. You can check the Firefox Profiler online. Happy profiling!

