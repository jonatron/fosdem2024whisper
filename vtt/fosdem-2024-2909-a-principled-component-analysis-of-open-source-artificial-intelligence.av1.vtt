WEBVTT

00:00.000 --> 00:09.740
As Julia is getting herself up and running, I'd like to introduce her to you.

00:09.740 --> 00:14.800
She is from Seattle, so she's another American here.

00:14.800 --> 00:23.640
She is a social technical, I can't say that, systems nerd and a huge fan of Lego, so that's

00:23.640 --> 00:27.720
important to know.

00:27.720 --> 00:32.480
You used to have a Scottish accent despite being an American and never being to Scotland.

00:32.480 --> 00:35.440
Now that is fascinating, that truly is.

00:35.440 --> 00:37.840
And her favorite joke is just her own humor.

00:37.840 --> 00:40.680
Julia, take it away.

00:40.680 --> 00:46.760
I think that one landed is exactly how I intended to, so thank you.

00:46.760 --> 00:55.120
Hi everyone, it's great to be here today to talk about what I am calling a principled

00:55.120 --> 01:00.000
component analysis of open source AI.

01:00.000 --> 01:02.880
So who am I?

01:02.880 --> 01:05.120
I'm Julia.

01:05.120 --> 01:13.480
I've been focused primarily on open source resilience and the software supply chain for

01:13.480 --> 01:16.480
the past five, ten years.

01:16.480 --> 01:21.240
I feel like a little bit of an open source hipster because I've been talking about some

01:21.240 --> 01:24.520
of these things before they were cool.

01:24.520 --> 01:34.560
But I've been working in and around open source AI since undergrad, so I'm not going

01:34.560 --> 01:38.320
to leave that to your imagination.

01:38.320 --> 01:43.880
Some things will probably give it away though.

01:43.880 --> 01:47.360
So in case you were wondering, that isn't a typo.

01:47.360 --> 01:49.680
It is a pun.

01:49.680 --> 01:56.200
It is a good slash bad pun, and I hope you are ready for more.

01:56.200 --> 02:02.400
I couldn't make a pun out of support vector machines though, so if you have one, please

02:02.400 --> 02:05.280
come talk to me later.

02:05.280 --> 02:06.280
That would be good.

02:06.280 --> 02:11.920
It would be much appreciated.

02:11.920 --> 02:16.520
So open source AI is not new.

02:16.520 --> 02:24.800
I've been doing open source AI for a while now, and I threw a bunch of stuff up on this

02:24.800 --> 02:32.760
slide, mostly copying and pasting from former poster presentations, but I have a chapter

02:32.760 --> 02:34.480
in this lovely book.

02:34.480 --> 02:39.480
I believe there is only one left in stock.

02:39.480 --> 02:42.400
They haven't had much of a call to reprint it.

02:42.400 --> 02:51.480
So you too could own constrained clustering, not about neural networks.

02:51.480 --> 03:02.160
And in that chapter, we had a very interesting approach to exploring information with user

03:02.160 --> 03:07.080
feedback using a variant of the K-means algorithm.

03:07.080 --> 03:10.240
So that was the basis of our chapter.

03:10.240 --> 03:17.360
We used this fantastic open source machine learning library called Weka.

03:17.360 --> 03:20.040
Is anyone familiar with Weka here?

03:20.040 --> 03:22.120
My people, hello.

03:22.120 --> 03:23.840
Excellent.

03:23.840 --> 03:25.440
Weka is wonderful.

03:25.440 --> 03:30.720
It has so many great machine learning algorithms in it.

03:30.720 --> 03:36.480
When I first started using Weka, I went to this website called SourceForge and downloaded

03:36.480 --> 03:37.640
it.

03:37.640 --> 03:38.640
And I was just entranced.

03:39.040 --> 03:41.840
It was my first experience with open source.

03:41.840 --> 03:45.880
I was entranced by this idea that I could go and see the code.

03:45.880 --> 03:48.800
I could go and modify it.

03:48.800 --> 03:58.200
And in fact, some researchers at the University of Texas had done this and modified it and

03:58.200 --> 04:00.200
redistributed it.

04:00.200 --> 04:06.880
Just this magical thing that I could then go and use in my own research, knowing that

04:06.920 --> 04:12.520
somebody who is much better at math than I am had validated all the algorithms.

04:12.520 --> 04:24.040
I also built this lovely autonomous robot, aerial autonomous robot, that won the dubious

04:24.040 --> 04:31.120
award of innovative hardware design from AAAI, which I think means we don't know how you

04:31.160 --> 04:34.440
got this to lift off the ground.

04:34.440 --> 04:43.680
But lift it did, mostly because we made it a giant dodecahedron to lift all of the camera

04:43.680 --> 04:46.560
components.

04:46.560 --> 04:54.240
And a few years ago, I used machine learning to tackle one of the world's hardest problems.

04:55.240 --> 05:00.240
Determining whether or not you should hug something.

05:00.240 --> 05:10.920
So I trained a little model when Fed and Image would tell you if it was a good idea to hug

05:10.920 --> 05:12.920
it or not.

05:12.920 --> 05:21.320
I am sad to say I am not huggable, apparently.

05:21.400 --> 05:23.400
I am not as mathematically proven.

05:23.400 --> 05:29.400
So, you know, maybe I should take my picture again, try it out.

05:29.400 --> 05:31.400
So that's me.

05:31.400 --> 05:38.400
So the bad news here is that I don't have any answers for any of you in this presentation.

05:38.400 --> 05:41.400
I only have open questions.

05:41.400 --> 05:50.400
We are not going into the deep specifics of models, algorithms, or approaches.

05:50.480 --> 05:56.480
This is going to be probably, for some of you, a little bit too high level.

05:56.480 --> 06:00.280
And for some, probably a little too low level.

06:00.280 --> 06:07.840
We are exploring this new area of technology that has ballooned kind of seemingly overnight.

06:07.840 --> 06:09.840
It feels like that to me.

06:09.840 --> 06:12.480
I don't know if that feels like that to you.

06:12.480 --> 06:18.240
And we are facing some really interesting challenges when talking about the advances

06:18.280 --> 06:25.680
that we have seen in artificial intelligence and how it intersects with open source if

06:25.680 --> 06:27.680
you are not using Weka.

06:27.680 --> 06:29.680
If you are using Weka, you are set.

06:29.680 --> 06:31.680
It's great.

06:31.680 --> 06:33.680
They are not paying me for this.

06:33.680 --> 06:35.680
I promise.

06:35.680 --> 06:41.920
So level set, like AI draws from a lot of different fields.

06:41.920 --> 06:47.080
If you go back to your AI 101 course, you are going to probably get a little bit of a survey

06:47.120 --> 06:53.120
overview of all of these different fields, from ethics to philosophy.

06:53.120 --> 06:57.120
Philosophy plays a big part in AI.

06:57.120 --> 07:03.120
Economics, my favorite part of AI is the formal logic side.

07:03.120 --> 07:10.120
But that's because statistics was never really my strong suit, which is why I love computers.

07:10.160 --> 07:19.160
So there are a lot of different considerations when it comes into building AI systems, AI

07:19.160 --> 07:26.160
technologies, and looking at new approaches for things both as practitioners and as researchers.

07:26.160 --> 07:28.160
I'm hearing a lot of echo.

07:28.160 --> 07:30.160
Is that like everyone?

07:30.160 --> 07:32.160
Okay.

07:32.160 --> 07:35.160
It will say everything twice.

07:35.160 --> 07:37.160
It's fine.

07:37.200 --> 07:49.200
So at a very high level, this is one of the slides I show people when they ask me why I don't use the phrase AI.

07:49.200 --> 07:58.200
It's because generally speaking, when people are talking about artificial intelligence, they are not talking about the entire field of artificial intelligence.

07:58.200 --> 08:01.200
They are talking about machine learning.

08:01.240 --> 08:07.240
So we can break it down into roughly two camps.

08:07.240 --> 08:17.240
And I call them camps because people are really settled in one or settled in the other, and they usually don't switch back and forth.

08:17.240 --> 08:19.240
That's been my experience anyway.

08:19.240 --> 08:24.240
So we've got symbolic artificial intelligence, the logic, logical AI.

08:24.240 --> 08:27.240
It's also referred to as logical AI.

08:27.280 --> 08:29.280
How do people think?

08:29.280 --> 08:36.280
How do we teach machines to think in ways that are similar to how people actually think?

08:36.280 --> 08:40.280
So this is where cognitive science really comes into play.

08:40.280 --> 08:53.280
And then the much bigger circle up there is what we're mostly concerned with these days is machine learning, the math.

08:53.320 --> 09:01.320
And this is what I tend to characterize as thinking is hard.

09:01.320 --> 09:10.320
We can probably build a model that comes close and then we'll do some math and we want to get stuff done.

09:10.320 --> 09:17.320
So we're just going to use the data that we've got and cross our fingers.

09:17.360 --> 09:25.360
You can argue with me about that opinion, all you like.

09:25.360 --> 09:27.360
I'm cool with that.

09:27.360 --> 09:39.360
So while I do have this, as I mentioned, the deep abiding love for symbolic AI, we're focusing primarily on machine learning here.

09:39.360 --> 09:44.360
Unless anybody wants to talk about slime.

09:44.400 --> 09:48.400
Not that slime.

09:48.400 --> 09:55.400
So some elements of AI, or machine learning, see?

09:55.400 --> 10:00.400
I'm also getting hit by the AI means machine learning bug.

10:00.400 --> 10:11.400
So some possible elements do include things like what data do we have that go into training the system?

10:11.440 --> 10:13.440
How do we actually train the system?

10:13.440 --> 10:17.440
How do we evaluate the system?

10:17.440 --> 10:22.440
All of the different elements, is there a model as an output?

10:22.440 --> 10:28.440
Is there a user interface as a way of interacting with machine learning?

10:28.440 --> 10:36.440
Now, not all of these are going to be present in every machine learning system.

10:36.480 --> 10:45.480
It kind of blows some people's minds to realize that you can have machine learning without a model.

10:45.480 --> 10:53.480
Or you can have machine learning without a task or prompt.

10:53.480 --> 10:55.480
But it's true.

10:55.480 --> 11:00.480
And we have to account for that when thinking about open source machine learning.

11:00.520 --> 11:08.520
And when we're looking at all of these different components, it gets a little bit hard to reason about.

11:08.520 --> 11:19.520
But if we reduce the dimensionality, PCA pun, we see roughly four buckets emerge.

11:19.520 --> 11:23.520
We've got the data, which is pretty familiar to us.

11:23.520 --> 11:25.520
We know what data is.

11:25.560 --> 11:32.560
We've got a good understanding of what might be training data, what's validation tasks, et cetera.

11:32.560 --> 11:36.560
We've got code, also a well-worn path for open source.

11:36.560 --> 11:40.560
And then we have what I call the other stuff.

11:40.560 --> 11:47.560
Because one of my skills is not naming things.

11:47.560 --> 11:51.560
And then finally, we've got output.

11:51.600 --> 11:58.600
So by doing a rough grouping with K equals four, we wind up with these four buckets.

11:58.600 --> 12:12.600
And I think that by thinking of them in this schematic, it makes it much easier to tackle the challenges that we face one by one.

12:12.600 --> 12:16.600
Now, some elements might appear in multiple buckets.

12:16.600 --> 12:20.600
Not on this slide, because simplicity.

12:20.640 --> 12:23.640
And some might not appear at all.

12:23.640 --> 12:26.640
But it's a starting point.

12:30.640 --> 12:36.640
So let's first talk about data.

12:41.640 --> 12:49.640
So when it comes to machine learning and data, we have some interesting problems.

12:49.680 --> 12:51.680
Some of them are known.

12:51.680 --> 12:55.680
Some of them are unknown.

12:55.680 --> 12:58.680
We have a lot of data out there.

12:58.680 --> 13:03.680
Machine learning research has been going on for what, since the 50s?

13:03.680 --> 13:06.680
Right? Ish?

13:06.680 --> 13:16.680
And that means that a lot of the data that has been used in this research doesn't have known provenance.

13:16.720 --> 13:22.720
So we don't actually know where some of the data came from.

13:22.720 --> 13:27.720
And if we're talking about things, I'm not going to talk about licenses, by the way.

13:27.720 --> 13:29.720
I forgot to mention that.

13:29.720 --> 13:31.720
I'm not talking about licenses.

13:31.720 --> 13:38.720
I'm just going to talk about the challenges in building open machine learning systems.

13:38.760 --> 13:48.760
But when there's data without known provenance, we don't know if it is truly game for us to use.

13:48.760 --> 13:58.760
If we have data that we don't know how it was collected, we don't know if it's truly game for us to use.

13:58.760 --> 14:02.760
Or if it suits our purpose.

14:02.800 --> 14:10.800
It could be an incredibly flawed data set, and we have no way of knowing.

14:12.800 --> 14:21.800
In terms of privacy, we've got a few big challenges with de-identification and anonymization.

14:21.800 --> 14:27.800
There's a lot of really interesting work that can be done in machine learning,

14:27.840 --> 14:32.840
but can't necessarily be done in an open way.

14:32.840 --> 14:38.840
And I don't actually know how to solve that particular beast,

14:38.840 --> 14:45.840
because I feel like I'm kind of splitten, too, on that.

14:45.840 --> 14:53.840
On the one hand, I do think that having full access to the training data,

14:53.880 --> 15:01.880
validation data, test, et cetera, is really important for building open systems.

15:01.880 --> 15:07.880
I don't think it can come at the expense of people's safety.

15:07.880 --> 15:19.880
And so if you are training something based on data that includes personally identifiable information,

15:19.920 --> 15:22.920
we kind of have to weigh that.

15:22.920 --> 15:30.920
There is that question of, should that be an open source system in the first place?

15:30.920 --> 15:34.920
I know that's a very controversial thing to say at FOSDEM.

15:34.920 --> 15:39.920
So I can leave now, if you like.

15:39.960 --> 15:47.960
For systems that also incorporate user feedback as part of the training data,

15:47.960 --> 15:52.960
because that's a fun place to get into,

15:52.960 --> 16:01.960
how do you build in, again, that de-identification and anonymization?

16:01.960 --> 16:06.960
And there are things like, okay, well, how are we actually going about splitting the corpus?

16:07.000 --> 16:11.000
If we are splitting the corpus into training and validation data,

16:11.000 --> 16:15.000
it's actually important to know what proportions we're using

16:15.000 --> 16:21.000
and how we're sampling in order to do that splitting.

16:21.000 --> 16:29.000
But again, some of these may not be applicable for all machine learning systems.

16:29.040 --> 16:38.040
So one of the things that I kept asking myself is,

16:38.040 --> 16:47.040
are these things required to recompile, for some definition of recompile, a model?

16:47.040 --> 16:56.040
So if I wanted to create the model from scratch and build it myself,

16:56.080 --> 17:02.080
what do I need?

17:02.080 --> 17:04.080
The entire dataset?

17:04.080 --> 17:10.080
If you want to show hands and tell me if you think it's required,

17:10.080 --> 17:14.080
I'm fascinated to know, but there's no obligation to.

17:14.080 --> 17:21.080
So is the entire dataset required to recompile a model?

17:21.120 --> 17:27.120
Would a description of the data suffice?

17:27.120 --> 17:33.120
How about a datasheet?

17:33.120 --> 17:43.120
Who thinks that you need to know about how the data was collected?

17:43.120 --> 17:45.120
Well, thank you.

17:45.120 --> 17:47.120
I appreciate you.

17:47.120 --> 17:49.120
I appreciate all of you.

17:49.160 --> 17:53.160
So I kept coming back to this question,

17:53.160 --> 17:55.160
is it required to recompile a model?

17:55.160 --> 18:05.160
And so you'll see that question as we go through some of these other sections.

18:05.160 --> 18:11.160
I do take the stance that you need the entire dataset and the methodology,

18:11.200 --> 18:19.200
but that introduces some big problems and big as in dollar signs,

18:19.200 --> 18:22.200
because these corpus are not small.

18:22.200 --> 18:26.200
Hosting them is expensive.

18:26.200 --> 18:39.200
So if we want to make open source machine learning open to all of the people who are interested in participating in it,

18:39.240 --> 18:44.240
how do we break down the cost of doing so?

18:44.240 --> 18:51.240
How do we make it available?

18:51.240 --> 18:56.240
The methodology, publishing the methodology for how the data was collected,

18:56.240 --> 19:02.240
helps with transparency if you trust it.

19:02.240 --> 19:04.240
But we're open source.

19:04.240 --> 19:08.240
We try to trust each other mostly.

19:08.280 --> 19:10.280
Except for a few of you.

19:10.280 --> 19:12.280
You know who you are.

19:12.280 --> 19:20.280
And there's some open questions about attribution.

19:20.280 --> 19:25.280
If the data also needs attribution.

19:25.280 --> 19:29.280
I'm not talking about from a legal sense or a licensed sense,

19:29.280 --> 19:33.280
just for transparency and for credit,

19:33.280 --> 19:37.280
because we appreciate giving credit where credit is due.

19:37.320 --> 19:42.320
And if somebody wants to opt out of having their data in a corpus,

19:42.320 --> 19:50.320
how do we handle that as well?

19:50.320 --> 19:54.320
Lots of unknown problems with data.

19:54.320 --> 19:58.320
But code gets a little bit easier.

19:58.320 --> 20:04.320
This is going to be the second time I make a Jurassic Park joke this week.

20:04.360 --> 20:08.360
But we know how to do open source software.

20:08.360 --> 20:12.360
This is code. We know how to do this.

20:12.360 --> 20:17.360
And despite what we've been hearing,

20:17.360 --> 20:22.360
most of machine learning fits solely in this camp.

20:22.360 --> 20:28.360
It fits solely in the camp of open source software.

20:28.360 --> 20:30.360
Job well done.

20:30.360 --> 20:31.360
Great.

20:31.400 --> 20:35.400
We've got Weka. What else do we need?

20:35.400 --> 20:43.400
So it does, they are governed by the same requirements as normal open source software.

20:43.400 --> 20:45.400
No special casing needed.

20:45.400 --> 20:48.400
Cool.

20:48.400 --> 20:55.400
One of the unique things, though, about this type of software

20:55.400 --> 20:59.400
is that it may actually produce one of those things we don't yet know how to deal with.

20:59.440 --> 21:01.440
The model.

21:01.440 --> 21:07.440
It also might involve an interface, how to interact with whatever system it produces,

21:07.440 --> 21:11.440
which may or may not be a model.

21:11.440 --> 21:18.440
And it does intersect with the data and some interesting problems that go along with that.

21:18.440 --> 21:26.440
So one of the things that we do when we process data is we clean it.

21:26.480 --> 21:30.480
So does that code need to be open source?

21:35.480 --> 21:41.480
Alongside with cleaning is some interesting value judgments that we have.

21:41.480 --> 21:46.480
We may say, okay, deprioritize this feature a little bit.

21:46.480 --> 21:49.480
Let's increase the priority on this one.

21:49.480 --> 21:54.480
And in that way, we are actually making moral and ethical judgments,

21:54.520 --> 21:57.520
and we're encoding them.

21:57.520 --> 22:02.520
Now, the great thing about code is that it's very easily inspected.

22:02.520 --> 22:07.520
For some definition of very and some definition of easily.

22:07.520 --> 22:10.520
That's an exercise for the reader.

22:10.520 --> 22:17.520
But if we dig into it, we can see where those value judgments are made.

22:17.560 --> 22:23.560
The other stuff, this is my favorite part.

22:23.560 --> 22:28.560
If somebody has a better name for it, let me know.

22:28.560 --> 22:37.560
So our hardware specifications required to recompile a model.

22:37.600 --> 22:43.600
How about disclosure of training time?

22:43.600 --> 22:49.600
How long it was trained for?

22:49.600 --> 22:54.600
A definition of correctness.

22:54.600 --> 23:01.600
So all of these do impact what comes out of the data.

23:01.640 --> 23:09.640
All of these do impact what comes out of your machine learning algorithm.

23:09.640 --> 23:14.640
I was doing like a one-day course,

23:14.640 --> 23:18.640
brushing up some knowledge,

23:18.640 --> 23:22.640
and there was a bit of a competition,

23:22.640 --> 23:26.640
ordinarily I hate competitions in classwork.

23:26.680 --> 23:32.680
But the idea was, okay, let's...

23:32.680 --> 23:34.680
Here are all the concepts.

23:34.680 --> 23:38.680
Do some fine-tuning, play around,

23:38.680 --> 23:45.680
and see who can have the highest accuracy for some random task.

23:45.680 --> 23:49.680
And I thought I did a pretty good job.

23:49.680 --> 23:51.680
The story of my life.

23:51.680 --> 23:53.680
I thought I did a pretty good job.

23:53.720 --> 24:01.720
But there was one person who achieved nearly a perfect score.

24:01.720 --> 24:05.720
So of course, the question is, would you do?

24:05.720 --> 24:13.720
And they said, oh, well, I just ran the training for, you know, two days.

24:13.720 --> 24:17.720
I'm like, oh, okay.

24:17.720 --> 24:19.720
Yeah, this was a one-day course.

24:19.720 --> 24:21.720
I didn't think it was a two-day investment.

24:21.760 --> 24:25.760
So the training time does absolutely affect the quality

24:25.760 --> 24:29.760
and the output of the resulting system,

24:29.760 --> 24:33.760
as does the hardware.

24:33.760 --> 24:39.760
So they are required to recompile a model.

24:39.760 --> 24:45.760
But similar to data, we also have the question of access.

24:45.760 --> 24:48.760
Access to equitable compute.

24:48.800 --> 24:54.800
Access to the hardware itself.

24:54.800 --> 25:01.800
And again, we have a problem of attribution.

25:01.800 --> 25:05.800
So finally, output.

25:05.800 --> 25:12.800
And since we are focusing on models and machine learning models,

25:12.800 --> 25:17.800
we've got matrices.

25:17.840 --> 25:26.840
I don't really know how to make that work in an open way.

25:26.840 --> 25:29.840
Yes, I can inspect a matrix.

25:29.840 --> 25:32.840
Can I make sense of it in isolation?

25:32.840 --> 25:35.840
Not so much.

25:35.840 --> 25:43.840
So if we do a litmus test, if this is all we have,

25:43.880 --> 25:47.880
can we do arbitrary machine learning tasks with just this?

25:47.880 --> 25:49.880
Probably not.

25:49.880 --> 25:55.880
How about just the code?

25:55.880 --> 25:57.880
Probably not.

25:57.880 --> 26:01.880
You still need some data.

26:01.880 --> 26:06.880
Just the hardware? Maybe. I'd like to see that.

26:06.920 --> 26:13.920
Just the model?

26:13.920 --> 26:15.920
I'm going to say no.

26:15.920 --> 26:18.920
That's my... I'm putting my foot down there.

26:18.920 --> 26:24.920
So what do we really need to make a transparent machine learning system?

26:24.920 --> 26:27.920
We need all of it.

26:27.920 --> 26:29.920
It all needs to be there.

26:29.920 --> 26:33.920
It all needs to be open and available.

26:33.960 --> 26:39.960
And that might mean that some things are not suitable for being open.

26:43.960 --> 26:47.960
So some other questions that I'd love for you to think about

26:47.960 --> 26:51.960
as you're thinking about open source and machine learning

26:51.960 --> 26:56.960
is what does contribution to a model look like?

26:56.960 --> 27:01.960
What does correctness of a contribution look like?

27:03.960 --> 27:14.960
How do we actually verify the openness of these systems

27:14.960 --> 27:21.960
in a way that doesn't require a huge amount of investment

27:21.960 --> 27:24.960
that only a select few have?

