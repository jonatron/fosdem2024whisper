WEBVTT

00:00.000 --> 00:19.000
Okay, it's 10.30, time for the next talk.

00:19.000 --> 00:24.200
So we're going to have another talk about Apache Airflow, but by Tatiana Alschwe.

00:24.200 --> 00:34.200
She works for Astronomer and she's going to tell us about DBT, which is a tool that basically you can write SQL and then have that executed in a templated way.

00:34.200 --> 00:38.200
How to integrate that into Apache Airflow. Thank you.

00:38.200 --> 00:45.200
Thank you. Hi, good morning everyone.

00:45.200 --> 00:53.200
So I'm really glad Yarek spoke before, so I don't need to get into the details of what Airflow is.

00:53.200 --> 00:59.200
Many of you know what DBT is. Could you raise your hands? Amazing.

00:59.200 --> 01:13.200
So initially I didn't have any clue of what DBT was and I was working for the BBC where we had very good software engineering principles and lots of things.

01:13.200 --> 01:23.200
And one day I went to support a SQL analyst who was analyzing the results of the A-B test and there was a bug.

01:23.200 --> 01:27.200
So the results of the A-B test were not very consistent.

01:27.200 --> 01:32.200
We were trying to figure out which machine learning models were performing better than the others.

01:32.200 --> 01:36.200
And then I said, okay, let's just see how you're doing things.

01:36.200 --> 01:44.200
And then I checked in his laptop and he had a Word document with a bunch of SQL statements.

01:44.200 --> 01:54.200
And then the procedure he was using was he would use this Word document to write the SQL statements, copy and paste that, like saying, Snowflake and other data warehouses,

01:54.200 --> 01:58.200
export to spreadsheets, then try to join some information.

01:58.200 --> 02:02.200
So the process altogether looked extremely error-prone.

02:02.200 --> 02:10.200
So the principles we had like testing, versioning, some repo, anything, nothing was in place.

02:10.200 --> 02:20.200
We eventually figured out what was the issue, but I really thought, well, we should be able to apply software engineering tools to any process.

02:20.200 --> 02:25.200
And the tools should be as easy as any person with any skill.

02:25.200 --> 02:29.200
And sometime after I came across Djibiti.

02:29.200 --> 02:42.200
So the idea of Djibiti is really to empower people who may not have experienced software development to use good software development practices while they write transformations,

02:42.200 --> 02:44.200
SQL transformations.

02:44.200 --> 02:48.200
So there is Djibiti Core, which is a quite stable project.

02:48.200 --> 02:53.200
It has around 250 contributors, 6000 commits.

02:53.200 --> 02:57.200
It's quite popular on GitHub, over 6000 stars.

02:57.200 --> 02:59.200
And the focus is on transformations.

02:59.200 --> 03:04.200
So you can define your SQL files into text files.

03:04.200 --> 03:08.200
It encourages users to deploy those to Git so you can have versions.

03:08.200 --> 03:11.200
It allows you to define tests.

03:11.200 --> 03:15.200
So let's say you would like to check some columns so you don't have no values.

03:15.200 --> 03:18.200
So it really makes easy all this practice.

03:18.200 --> 03:26.200
And it's an amazing tool which has really helped improving and avoiding the process I've seen in the past.

03:26.200 --> 03:31.200
And then many people may ask, okay, but what is the relation of Djibiti and Airflow?

03:31.200 --> 03:35.200
Aren't analysts happy enough running those scripts locally?

03:35.200 --> 03:39.200
And why using both of them?

03:39.200 --> 03:45.200
Yarek Raja explains what Airflow is, but it's a very mature orchestrator tool,

03:45.200 --> 03:48.200
which can allow you to run things in parallel.

03:48.200 --> 03:53.200
And it has lots of flexibility where you actually run things.

03:53.200 --> 03:57.200
So it does have a few things.

03:57.200 --> 04:02.200
In one hand, we have Airflow where you can write pipelines in Python.

04:02.200 --> 04:03.200
It is flexible.

04:03.200 --> 04:10.200
As Yarek shown, we have hundreds of operators from multiple providers to integrate with

04:10.200 --> 04:16.200
uncountable data warehouses, tools, LLM tools and so on.

04:16.200 --> 04:18.200
But it's quite complex.

04:18.200 --> 04:24.200
So the interface can be a bit overwhelming with lots of colors, lots of boxes.

04:24.200 --> 04:28.200
It can be hard troubleshooting and getting to understand.

04:28.200 --> 04:31.200
If you want to run Airflow locally, there is Airflow standalone,

04:31.200 --> 04:36.200
but you will have to be running a web server, a scheduler, and eventually a worker node.

04:36.200 --> 04:39.200
So there is complexity to it.

04:39.200 --> 04:45.200
On the other hand, Djibiti, people can write workflows just using SQL, transformation workflows.

04:45.200 --> 04:52.200
It is quite specialized SQL data warehouses, but it has a very simple UI.

04:52.200 --> 04:55.200
So Airflow is quite good.

04:55.200 --> 04:59.200
Djibiti is quite good in specifying tests in a simple way.

04:59.200 --> 05:01.200
It is very dependent on management.

05:01.200 --> 05:07.200
You can use Jinja templates to reference other tables created with other modules in Djibiti via SQL files.

05:07.200 --> 05:11.200
It is quite easy to define schemas.

05:11.200 --> 05:15.200
But it only focuses on transformations on the T side of ETL.

05:15.200 --> 05:22.200
Airflow, on the other hand, gives you flexibility to do anything you can do with Python, which is a lot.

05:22.200 --> 05:25.200
But it is quite complex to run.

05:25.200 --> 05:32.200
You could achieve anything you do with Djibiti with Airflow, but you may have to write more code.

05:32.200 --> 05:35.200
So I don't think we need to compare.

05:35.200 --> 05:41.200
I think what many companies decide to do is let's just try to use both tools.

05:41.200 --> 05:47.200
What this presentation is about is how we can use these tools together in an efficient way.

05:47.200 --> 05:52.200
So this is how a Djibiti pipeline looks like.

05:52.200 --> 05:55.200
So on one hand, you have several files.

05:55.200 --> 06:00.200
Inside the models, each of those files represents a table, and you have a transformation.

06:00.200 --> 06:03.200
And then Djibiti allows you to render the pipeline.

06:03.200 --> 06:07.200
So let's say you have some raw data and customers, orders, payments.

06:07.200 --> 06:13.200
You then transform those, and then you aggregate those to be able to generate reports and send it to Tableau or something.

06:13.200 --> 06:17.200
This is what a Djibiti project looks like.

06:17.200 --> 06:21.200
It's similar to what Airflow does since Djibiti interacts with databases.

06:21.200 --> 06:28.200
It has a way of defining how those connections look like, which secrets, credentials you have to use.

06:28.200 --> 06:33.200
Those are done via Djibiti profiles, via YAML files.

06:33.200 --> 06:37.200
And then the question is, okay, so there is similarity.

06:37.200 --> 06:45.200
Djibiti and Airflow can generate DAGs, allow you to create workflows, but how to connect them.

06:45.200 --> 06:54.200
So we thought, okay, what are the options if we had a translation tool to convert from one to the other?

06:54.200 --> 07:02.200
If you check into Djibiti DAGs, what they say is, if you're using Djibiti Cloud, which is a managed version of Djibiti,

07:02.200 --> 07:08.200
you can just deploy things, and there is an official Airflow provider to deploy there.

07:08.200 --> 07:12.200
But currently, they changed the pricing model, and if you have a team,

07:12.200 --> 07:16.200
you'll probably be spending proportional to the amount of developers you have in your team.

07:16.200 --> 07:19.200
So it can get quite expensive.

07:19.200 --> 07:25.200
Another strategy you can use that Djibiti suggests is you can just use the Airflow Bash operator,

07:25.200 --> 07:28.200
which allows you to run any Bash commands.

07:28.200 --> 07:32.200
So the same way you run Djibiti commands, on the command line, you could just trigger those to be run.

07:32.200 --> 07:39.200
Or you could also use Kubernetes operator, delegating from the Airflow worker into Kubernetes pod to run those commands.

07:39.200 --> 07:44.200
So this is an example of how the DAG would look like if you were using Djibiti Cloud.

07:44.200 --> 07:50.200
So in this case, you have just some operator to declare the beginning and the end.

07:50.200 --> 07:52.200
This is an old pattern many people use.

07:52.200 --> 07:55.200
You can check the code in this link below.

07:55.200 --> 08:02.200
But with the recent setup and cheer down features of Airflow, you don't really need to have those dummy operators.

08:02.200 --> 08:09.200
But anyway, in this case, it just starts the job and triggers the job to run in Djibiti Cloud.

08:09.200 --> 08:17.200
The challenge with this is, can someone tell by looking at this pipeline, what are the actual models and transformations going on?

08:17.200 --> 08:20.200
Let's say you had a project with a thousand models.

08:20.200 --> 08:22.200
How could you spot where the problem was?

08:22.200 --> 08:26.200
Worse than that, let's say you have a pipeline that takes 12 hours to run.

08:26.200 --> 08:31.200
And one of those tasks that takes five hours past, but a few others didn't.

08:31.200 --> 08:35.200
How would you just re-trigger the jobs after the failed job?

08:35.200 --> 08:43.200
So this approach is quite limited and doesn't give much visibility from Airflow on what is going on in Djibiti.

08:43.200 --> 08:50.200
Another approach people use is just to trigger Djibiti Commons via batch operators.

08:50.200 --> 08:58.200
So in this case, you again define, let's say, Djibiti seeds where you load from CSV to the database.

08:58.200 --> 09:02.200
Then you can trigger transformations and then you can run Djibiti tests.

09:02.200 --> 09:12.200
In this case, again, you are grouping the Djibiti nodes by type and running one Airflow operator, one Airflow task for each of those steps.

09:12.200 --> 09:18.200
You don't have much visibility and control of what is actually going on, but it can do the job.

09:19.200 --> 09:29.200
Then another approach many people have done in the industry was I will write my own parser of Djibiti projects and render that somehow into Airflow.

09:29.200 --> 09:37.200
In a way that we can parse what are the nodes of the original graph and then render them in Airflow having some granularity on the process.

09:37.200 --> 09:41.200
So there are codes for all these examples I'm sharing.

09:42.200 --> 09:46.200
Each of these approaches has their own pros and cons.

09:46.200 --> 09:51.200
From one perspective, the two first approaches are quite trivial to parse the DAG.

09:51.200 --> 09:57.200
So they are cheap to do the processing every time Airflow tries to parse DAGs and trigger tasks.

09:57.200 --> 10:05.200
On the other hand, it can be harder to troubleshoot, to retry and just to do the work.

10:06.200 --> 10:12.200
In the last case, you can trigger independent tasks, which is quite powerful.

10:12.200 --> 10:23.200
So we've seen many people in the community implementing their own solutions to do this conversion of expanding the Djibiti DAG into an Airflow DAG.

10:23.200 --> 10:29.200
During Airflow Summit last year, Djibiti was one of the most discussed topics and there were several approaches.

10:30.200 --> 10:38.200
Some people use Djibiti manifest, which is an artifact that represents the topology of the Djibiti DAG.

10:38.200 --> 10:49.200
Some people would use dynamic tasks, which Iaric also showed in his presentation, where you can paralyze a sort of map-produce approach with the Airflow.

10:49.200 --> 10:57.200
Some people just generate a static DAG and then they don't use actually Djibiti to run the transformations.

10:57.200 --> 11:08.200
They convert into native Airflow operators, which can be asynchronous operations, so you wouldn't need to have the Airflow worker node blocked while executing the transformations.

11:08.200 --> 11:20.200
And then many people decide to delegate the jobs, so the Airflow worker node isn't necessarily doing the Djibiti commons or the transformations and just delegate to Kubernetes.

11:21.200 --> 11:23.200
So those are some approaches.

11:23.200 --> 11:38.200
And then at Astronomers, since we have several Airflow users and customers trying to do this integration, during a hackathon, some team members developed Astronomers Cosmos, which was a tool to try to help in this conversion.

11:38.200 --> 11:47.200
So the idea was really to have a sort of Rosetta Stone, which could help and simplify everyone's lives.

11:48.200 --> 11:53.200
It is under Apache license, so it's open source.

11:55.200 --> 11:59.200
And this is how Cosmos translates the DAG.

11:59.200 --> 12:04.200
So you can see the original Djibiti project, and that's how it looks in Airflow.

12:04.200 --> 12:12.200
So you have really a mapping one-to-one of how the DAG looks like, and the names are quite close to the original names as well.

12:13.200 --> 12:23.200
This is how the DAG is, so that's all someone with a Djibiti project would have to write to be able to have their project fully translated into Airflow.

12:23.200 --> 12:24.200
So it's quite simple.

12:24.200 --> 12:26.200
We have some configuration.

12:26.200 --> 12:29.200
These first lines relate to Profile Config.

12:29.200 --> 12:30.200
They are optional.

12:30.200 --> 12:40.200
It's a feature of Cosmos which allows you to only define your credentials to access the database using Airflow connections, and then we convert those into Djibiti Profile.

12:40.200 --> 12:44.200
So you don't have to define things in both places.

12:44.200 --> 12:48.200
But if you prefer, you can always give a Profile.channel, and that's it.

12:48.200 --> 12:53.200
So the code of your DAG would be pretty much this hlivesofcode below.

12:55.200 --> 12:59.200
It uses task groups which I also spoke about.

12:59.200 --> 13:02.200
So we allow several ways of running tests.

13:02.200 --> 13:04.200
One of them is to actually group with the model.

13:04.200 --> 13:12.200
So usually you would define test-based models, and you can have task groups where you would have both the execution of the task and then the test.

13:12.200 --> 13:21.200
Because we assume, depending on your configuration, if the tasks for a model don't pass, then it wouldn't make sense for you to continue processing the next transformations.

13:21.200 --> 13:23.200
Then there is a demo.

13:23.200 --> 13:24.200
Let's see.

13:24.200 --> 13:26.200
Okay, we're okay in time.

13:26.200 --> 13:29.200
So this is the Airflow UI.

13:29.200 --> 13:31.200
I'm running this Airflow instance locally.

13:32.200 --> 13:34.200
We have a few DAGs.

13:34.200 --> 13:36.200
Some of them passed, some failed.

13:36.200 --> 13:44.200
And then this is an example of how Djibiti Converted DAG using Cosmos looks like.

13:44.200 --> 13:49.200
With the task groups, the tasks, you can see the code for this here.

13:49.200 --> 13:51.200
It's super simple.

13:51.200 --> 13:56.200
And then, like I say, we would like to trigger this workflow so we could run all these things.

13:56.200 --> 13:58.200
We can click here on the trigger button.

13:58.200 --> 14:00.200
You could be using the API as well.

14:00.200 --> 14:02.200
You could be using the comment line.

14:02.200 --> 14:06.200
And then you can see the tasks being scheduled.

14:06.200 --> 14:11.200
Since it's in my local computer with a single worker, it's much particularly quickly.

14:11.200 --> 14:12.200
But there you are.

14:12.200 --> 14:19.200
You can see the tasks running, executing, and then we can see the ones that are green here.

14:19.200 --> 14:20.200
See?

14:20.200 --> 14:22.200
The ones that are gray.

14:22.200 --> 14:27.200
They are waiting to be scheduled, and then the green ones succeeded.

14:27.200 --> 14:28.200
This is queued.

14:28.200 --> 14:30.200
These are running.

14:30.200 --> 14:32.200
And then there you are.

14:32.200 --> 14:37.200
And then one of the things Cosmos allows is for you to actually check, okay, for this task,

14:37.200 --> 14:41.200
what was the actual SQL statement as a queued?

14:41.200 --> 14:46.200
So you can see the rendered template, and you can understand, oh, this was the transformation.

14:46.200 --> 14:48.200
And it can help troubleshooting.

14:50.200 --> 14:53.200
So that's the demo.

14:53.200 --> 14:56.200
Some of the key features of Cosmos.

14:56.200 --> 15:03.200
So it easily allows you to bring GB2 projects into Airflow.

15:03.200 --> 15:10.200
You can assume people who are used to write the workflows in SQL can remain writing the workflows in SQL.

15:10.200 --> 15:16.200
You can either render as a task group or as a DAG, depending on your granularity.

15:16.200 --> 15:24.200
It gives you flexibility if you want to run tasks in Kubernetes or Docker or in the Airflow Docker worker node.

15:24.200 --> 15:33.200
And recently there is a PR even to delegate to some, I think, Azure container service.

15:33.200 --> 15:38.200
So you could write your own way of executing the GB2 task with Cosmos.

15:38.200 --> 15:46.200
You can override how we do the conversion, depending on the GB2 node types into Airflow tasks.

15:47.200 --> 15:55.200
So there are a few different ways we support doing this parsing of GB2 into Airflow as well.

15:55.200 --> 15:57.200
You can use data sets.

15:57.200 --> 16:05.200
So each, I had a convention to that Airflow.to introduce data set to our scheduling.

16:05.200 --> 16:13.200
So in the past, you could only schedule pipelines in Airflow using Chrome job expressions or same daily or so on.

16:13.200 --> 16:18.200
And the cool thing with data-aware scheduling is let's say you have a pipeline.

16:18.200 --> 16:26.200
Let's say you have a machine learning pipeline where you're processing, let's say, some video metadata in one part, user activity in the other.

16:26.200 --> 16:34.200
And then you would like to aggregate those two pipelines to be able to train your model and fine tune and do whatever.

16:34.200 --> 16:46.200
So now, since Airflow introduced data-aware scheduling, you can define outputs of your DAGs that those data sets are ready.

16:46.200 --> 16:49.200
Let's say the program metadata was processed.

16:49.200 --> 16:53.200
And then you can trigger another DAG when that data becomes available.

16:53.200 --> 16:56.200
And with Cosmos, you also have this.

16:56.200 --> 17:11.200
So you can do conversions of GB2 DAGs and make sure that after transformations is executed, other pipelines that depend on that transformation output will be triggered without having to depend on a time schedule.

17:11.200 --> 17:24.200
And then since Airflow and GB2 core are open source, you can run this by as many developers as you want without having to pay proportionally to a developer, the amount of developers.

17:24.200 --> 17:28.200
And we have a growing active open source community.

17:28.200 --> 17:37.200
So here are just some more details on how you could configure the types of operators we're using within Cosmos.

17:37.200 --> 17:42.200
So Python operator, V2 end operator, Docker, Kubernetes operator.

17:42.200 --> 17:52.200
Initially, we were thinking about since both those are written in Python, our first strategy was why don't we use GB2 code to do this parsing and conversion

17:52.200 --> 17:54.200
when we get to GB2 project.

17:54.200 --> 17:59.200
But then we realized there are many conflicts between versions of Airflow and GB2.

17:59.200 --> 18:07.200
So we ended up not really using the GB2 as a library, but we used other approaches to do this parsing.

18:07.200 --> 18:12.200
So one of the approaches is to use the manifest file, which is one of the outputs from GB2.

18:12.200 --> 18:19.200
Assuming you have a CICG, you could just output that and you have the DAG topology in a JSON file.

18:20.200 --> 18:29.200
However, this method doesn't give all the filtering and excluding flexibility to GB2 offers when you want to just select a subset of nodes to be run.

18:29.200 --> 18:38.200
Then we implemented GB2.ls, which would be a way of displaying selected nodes using GB2 itself.

18:38.200 --> 18:41.200
But the performance isn't particularly good for this.

18:41.200 --> 18:44.200
So we implemented a version with caching.

18:44.200 --> 18:56.200
We have our own custom parser and we usually by default try to depend on the user configuration automatically define a way of parsing the GB2 project.

18:56.200 --> 19:00.200
So we support select and exclude.

19:00.200 --> 19:04.200
There are several different approaches rendering test nodes as well.

19:04.200 --> 19:12.200
We allow users to do this conversion of connection from Airflow into GB2 profiles or to give their own profiles the ammo.

19:12.200 --> 19:19.200
We try to give as much information for users to troubleshoot GB2 pipelines within Airflow itself.

19:19.200 --> 19:21.200
This is the adoption.

19:21.200 --> 19:29.200
So in the last month we had around 242,000 downloads and growing number of stars in GitHub.

19:29.200 --> 19:38.200
Some of the next steps are exposing GB2 documentation from the project within Airflow as well, improve the performance,

19:38.200 --> 19:42.200
and then improve the open lineage integration and a few other things.

19:42.200 --> 19:49.200
We also noticed many people have GB2 projects in different repos than the Airflow one.

19:49.200 --> 19:54.200
So we're looking to ways of optimizing the synchronization of the things.

19:54.200 --> 20:00.200
And there's the one user asked for GB2 Cloud integration, so that's something that may come.

20:00.200 --> 20:11.200
This is a PR from, results of a PR from the community where we actually just render GB2 documentation within Airflow using Cosmos.

20:11.200 --> 20:18.200
We don't have as many commuters or contributions as Airflow, but I think we're quite in a good spot.

20:18.200 --> 20:26.200
So as an example in November we had 20 authors contributing and only three of those were within Astronomer.

20:26.200 --> 20:31.200
We have a growing number of contributors and we're promoting community members into commuters.

20:31.200 --> 20:37.200
We know there's a lot of work to be done and we really appreciate the community support.

20:37.200 --> 20:47.200
The discussions, we currently use Airflow GB2 Slack channel in their flow Slack workspace and we have lots of daily interactions.

20:47.200 --> 20:55.200
And the community is each time developing and supports each other, which is super exciting to see.

20:55.200 --> 21:04.200
There are a few references there. The slides are on the POSDEN website, so you can just click on those if you would like to see more information,

21:04.200 --> 21:09.200
more detailed material, examples of how to run things, and that's it.

21:09.200 --> 21:12.200
Thank you very much and I think we have four minutes for questions.

21:12.200 --> 21:25.200
Thank you very much. That was a very, very interesting talk and it looks like a very, very good project.

21:25.200 --> 21:32.200
Any questions? No questions. I have one question.

21:32.200 --> 21:40.200
Are you only doing the integration from DBT into Airflow or also from Airflow into DBT back again?

21:40.200 --> 21:55.200
At the moment we're doing from DBT into Airflow, but would you see, because the tricky thing is the features DBT offers are a subset of what Airflow offers.

21:55.200 --> 22:04.200
So the conversion in the other way may not be feasible at all, depending on which operators and tasks you would be defining in Airflow.

22:05.200 --> 22:16.200
And also, Astronomer has a managed Airflow, right? So our interest is bringing people into Airflow and not necessarily sending them away over Airflow.

22:16.200 --> 22:28.200
And maybe a follow-up question to that. Is it possible to continuously migrate from DBT to Airflow so that people can continue working in DBT and then you automatically get the data into Airflow?

22:28.200 --> 22:39.200
Yes. So with Cosmos, the current version of Cosmos, it expects the DBT files to be available to Airflow somehow, but this can be done in multiple ways.

22:39.200 --> 22:49.200
So if you're deploying Airflow using a Docker container, you can make sure as part of your RCI CD, you would be fetching that during the image build.

22:49.200 --> 23:06.200
We also saw, I think, British Airways that are using this, they were actually thinking, let's say from GCS, they uploaded the DBT project into GCS, and then the first step of their DAG was actually to download those files and then use that.

23:07.200 --> 23:17.200
So some people may want to have some NFS or volume mounted with the DBT project and make sure that synchronized with the latest.

23:17.200 --> 23:24.200
So there are several ways, and depending on the parse emojis with Cosmos, those will be updated in real time.

23:24.200 --> 23:35.200
Okay, excellent. More questions? We have time for maybe one or two more questions? No? Well then, thank you very much again.

23:35.200 --> 23:38.200
We have a short break now.

