WEBVTT

00:00.000 --> 00:14.560
So, good afternoon everyone and thank you for joining me today.

00:14.560 --> 00:18.480
My name is Bello and I'm a software engineer at Red Hat.

00:18.480 --> 00:24.160
And over the past year I have the opportunity to be part of the forklift team and take it

00:24.160 --> 00:25.160
for a spin.

00:25.240 --> 00:30.680
So, today I'm about to share with you our recent journey and without further ado let's jump in.

00:33.560 --> 00:39.000
So, in today rapidly involving both of IT we can observe an increase in moving away from

00:39.000 --> 00:44.600
traditional virtualization environments towards more hybrid cloud solutions.

00:44.600 --> 00:48.600
And with Red Hat RNOT just observing this trend we're actively participating in it.

00:49.560 --> 00:56.040
So, recently we had the opportunity to go on a journey and migrating from a

00:56.040 --> 00:59.480
virtual established environment to a newer solution.

01:00.200 --> 01:06.360
And today I'm going to share with you some of the inside challenges and benefits of such a transition.

01:09.400 --> 01:12.760
So, let's start by discussing these two very different solutions.

01:13.480 --> 01:16.760
So, picture you on a journey through the IT computing landscape.

01:17.400 --> 01:19.320
Our first step is Ovid.

01:19.320 --> 01:22.520
It's like an older reliable train that's been running for years.

01:23.160 --> 01:30.600
So, Ovid is an open source product based on KVM technologies and it's offering

01:31.880 --> 01:36.360
cost efficient way for enterprise to managing their virtual workloads.

01:37.800 --> 01:40.840
It's an alternative to vSphere.

01:41.480 --> 01:43.000
But our journey does not end there.

01:43.560 --> 01:45.960
We then continue to the world of OKD.

01:45.960 --> 01:49.720
So, picture it as a high speed train whisking us to the future of cloud computing.

01:50.280 --> 01:57.160
So, OKD is also an open source project based on Kubernetes and it's providing us with

01:58.280 --> 02:04.360
cloud computing capabilities alongside enhanced Kubernetes features such as added security,

02:04.360 --> 02:06.760
automation and user friendly interface.

02:06.760 --> 02:10.200
And it supports both containers alongside virtual machines.

02:10.760 --> 02:15.160
So, when considering such a transition it's important to take into account how it can be done.

02:15.160 --> 02:19.480
So, there are several path we could take each with its own set of advantages and challenges.

02:19.480 --> 02:21.880
But today I would like to focus on main three.

02:23.480 --> 02:28.440
So, first we can't reprovisioning all the virtual workloads and start from scratch.

02:29.320 --> 02:35.560
Even though this solution may be sound pretty straight forward it's both costly and time intensive.

02:36.360 --> 02:41.960
And for complex workloads it's not always possible without risking some data integrity

02:41.960 --> 02:43.560
and operational disruptions.

02:45.560 --> 02:49.560
Next, we can migrate all our virtual workloads into containers.

02:49.560 --> 02:53.560
So, with the use of conveyor project we can really reduce the cost here.

02:53.560 --> 02:55.560
But it's still not an easy task.

02:55.560 --> 02:59.560
And again we have the same issues before not all workloads can be containerized.

02:59.560 --> 03:03.560
So, while this may be a good solution for certain types of applications,

03:03.560 --> 03:05.560
it's not suitable for everyone.

03:05.560 --> 03:10.360
And finally, which seems to be the best one is keeping our migration workloads,

03:10.360 --> 03:12.360
our virtual workloads as they are.

03:12.360 --> 03:16.360
And with the use of forklift tool migrate them to the new environment.

03:16.360 --> 03:19.960
So, by that we don't have to worry about any data lozage.

03:19.960 --> 03:24.360
And with the use of this tool we can have simple and smooth transition.

03:24.360 --> 03:26.360
So, what is it forklift?

03:28.360 --> 03:32.360
So, forklift is a tool designed in a system that is designed to be a virtual environment.

03:32.360 --> 03:38.360
Design in assisting migrating from traditional virtualization environments

03:38.360 --> 03:42.360
to Kubernetes-based environments.

03:42.360 --> 03:48.360
And it's taking care of the entire migration process for us.

03:48.360 --> 03:52.360
It's working alongside with another project named CubeVirt.

03:52.360 --> 03:56.360
And CubeVirt providing us the virtualization capabilities

03:56.360 --> 04:00.360
on top of Kubernetes-based environments.

04:00.360 --> 04:05.360
And once forklift migrating the virtual workloads, they will be placed on top of CubeVirt.

04:05.360 --> 04:10.360
So, forklift as a versatile tool supports a variety of source providers,

04:10.360 --> 04:14.360
source environments as you can see here in this list.

04:16.360 --> 04:21.360
So, now I would like to take a deeper look at forklift high-level functionalities.

04:21.360 --> 04:27.360
So, forklift supports two types of environments, KVM-based and VMware-based.

04:27.360 --> 04:31.360
And for both of them, it's taking care for the entire migration process.

04:31.360 --> 04:35.360
That means creating the disk, copying the data, and for VMware-based product,

04:35.360 --> 04:40.360
converting the virtualization stack to match CubeVirt requirements.

04:40.360 --> 04:46.360
And, of course, finally creating the VM itself with its original setup to run on top of CubeVirt.

04:46.360 --> 04:52.360
So, the use of this tool will make easier and smoother transition to the new environment.

04:53.360 --> 04:59.360
So, now that we finish discussing these different solutions and approaches,

04:59.360 --> 05:04.360
let's dive in into the specifics of our own migration from OVirt to OKD,

05:04.360 --> 05:10.360
where forklift used as a crucial tool in facilitating this migration.

05:12.360 --> 05:16.360
So, I would like to start with a little bit background on why we decided to go ahead

05:16.360 --> 05:19.360
and proceed with this transition in the first place.

05:19.360 --> 05:23.360
So, our OVirt environment being in use for more than a decade,

05:23.360 --> 05:27.360
supporting hundreds of virtual machines with diverse usage,

05:27.360 --> 05:31.360
some for production while others for developing and testing.

05:31.360 --> 05:35.360
While the fact that OVirt reaching its end-of-life zone wasn't the main reason

05:35.360 --> 05:40.360
we decided to go on this transition, it certainly matches in this direction.

05:40.360 --> 05:46.360
And, moreover, we wanted to take this opportunity and reallocate some of our resources

05:46.360 --> 05:52.360
and remove underutilized workloads while causing as minimum interference to the users as possible.

05:52.360 --> 05:59.360
So, taking all this into account, the shift to OKD seems to be the most reasonable fitting choice.

06:02.360 --> 06:09.360
So, as any successful story, planning is always essential, and our migration wasn't exception.

06:09.360 --> 06:15.360
So, we started our journey by having in-depth analysis of our current environment

06:15.360 --> 06:24.360
and just understand what the migration requirements and what we need from this transition exactly.

06:24.360 --> 06:28.360
We then continued to having some resource evaluation.

06:28.360 --> 06:32.360
That means we had to make sure that our target environment will have enough resources

06:32.360 --> 06:37.360
to accommodate the incoming workloads in terms of compute, storage and network.

06:37.360 --> 06:42.360
And finally, we had to create a clear timeline to make sure that each step of the way is well known

06:42.360 --> 06:47.360
and everyone involved from users and IT teams are in the loop of this transition.

06:50.360 --> 06:56.360
So, now we would like to zoom in even more into the preparation step and focus on the resource allocation.

06:56.360 --> 07:00.360
So, we had to start by finalizing our VM list for migration.

07:00.360 --> 07:06.360
And when we thought about what going to be the criteria for VM to be eligible for this transition,

07:07.360 --> 07:16.360
we decided to proceed with actively used VMs only and had to have close conversation with their owners to understand their specific needs.

07:16.360 --> 07:24.360
After that, we had to calculate the storage and IP addresses of all the VMs in this list

07:24.360 --> 07:28.360
to make sure that our target environment will have enough resources.

07:28.360 --> 07:31.360
This step was more than just technical preparation.

07:31.360 --> 07:38.360
It was essential to ensure that once the migration is started, we won't have additional downtime to lack of resources.

07:38.360 --> 07:47.360
And last, we had to come up with a way to reflect our original ownership and access mode from the overt environment to OKD.

07:50.360 --> 07:55.360
So, with a well-planned and tool like forklifts at our disposal,

07:55.360 --> 07:59.360
you might think this migration is going to be a walk in the park, right?

07:59.360 --> 08:05.360
Well, not quite. As we started our journey, we discovered that the path ahead of us is going to be quite challenging.

08:05.360 --> 08:09.360
So, now I would like to share with you some of the obstacles we encountered

08:09.360 --> 08:13.360
and how we tackled each of them to keep our migration on track.

08:15.360 --> 08:18.360
So, the first challenge was regarding the VM selection.

08:18.360 --> 08:22.360
So, as I mentioned earlier, we wanted to continue with only actively used VMs.

08:23.360 --> 08:31.360
That required from us to analyze the VM usage patterns

08:31.360 --> 08:36.360
and understand which VMs were actively used during specific time period,

08:36.360 --> 08:38.360
tasks that proven to be quite challenging.

08:38.360 --> 08:44.360
Then we had to gather the information about these VMs, such as disk size, network and ownership.

08:45.360 --> 08:52.360
And that task appeared to be quite demanding as well, both in matters of complexity and in time intensive.

08:54.360 --> 09:00.360
And the first, our two environment had different provisioning models.

09:00.360 --> 09:06.360
Our overt was more admin-driven and our OKD was more user-driven.

09:06.360 --> 09:11.360
And we had to come up with a way to bridge this gap somehow.

09:14.360 --> 09:21.360
So, in order to overcome these challenges, we went ahead and developed Python script

09:21.360 --> 09:24.360
specific for the migration process.

09:24.360 --> 09:26.360
And they can be broken into two categories.

09:26.360 --> 09:32.360
The first one, based on OvitasDK, was mainly used for finalizing the VM list for migration

09:32.360 --> 09:37.360
and two data gathering, such as the disk size, IP allocation and ownership.

09:37.360 --> 09:41.360
The second sort of scripts were based on Kubernetes API

09:41.360 --> 09:45.360
and they were used for creating the namespace on the target environment

09:45.360 --> 09:49.360
and for assigning the appropriate role for the users.

09:50.360 --> 09:56.360
We also uploaded the script to our GitHub region, so they can be used as a blueprint

09:56.360 --> 09:59.360
if anyone wants to take a look, you're more than welcome.

10:02.360 --> 10:06.360
So, now I would like to focus into a specific issue we had

10:06.360 --> 10:10.360
and just walk you through the different stages that we took to solve it.

10:10.360 --> 10:15.360
So, as I mentioned earlier, our two environments had different provisioning models.

10:15.360 --> 10:19.360
So, our Ovitas environment were more centralized models,

10:19.360 --> 10:26.360
where admin had full control of the environment and managed all the resources and created new VMs.

10:26.360 --> 10:31.360
Our OKD environment, on the other hand, is more user-driven,

10:31.360 --> 10:36.360
where the user have freedom to manage and create their own resources within their namespace.

10:37.360 --> 10:41.360
The namespace resources are set by predefined quotas.

10:41.360 --> 10:49.360
So, to bridge this gap, we decided to go ahead and create new namespaces on the target environment

10:49.360 --> 10:54.360
and place in each one of these namespaces all the shared VMs by the same users.

10:54.360 --> 11:01.360
And by giving them an admin access, we made sure that each user will remain

11:01.360 --> 11:05.360
with the original permissions.

11:05.360 --> 11:08.360
So, let's clarify it with an example.

11:08.360 --> 11:14.360
So, let's say after we finished finalizing our VM list, we ended up having four VMs for migration.

11:14.360 --> 11:19.360
So, as you can see, on the new environment, we created three new namespaces,

11:19.360 --> 11:22.360
and in each one of them placed all the shared VMs by the same user.

11:22.360 --> 11:26.360
So, Bob and Alice, who shared two VMs, now will have shared namespaces

11:26.360 --> 11:29.360
with both having admin access to it.

11:29.360 --> 11:33.360
And Bob ended up having three projects assigned to him,

11:33.360 --> 11:38.360
which really reflects the diverse usage on the original setup.

11:41.360 --> 11:46.360
So, now I would like to guide you through the script we used for this mapping process.

11:46.360 --> 11:51.360
So, the first part is based on Ovid SDK, and we did the following.

11:51.360 --> 11:57.360
So, we started by creating a list that mapped between all the VMs and the users from the system.

11:57.360 --> 12:03.360
Then, based on information from another script, we removed all the admin and system users from that list.

12:03.360 --> 12:09.360
Then, we created a dictionary that mapped between sets of VMs and all their corresponding users.

12:09.360 --> 12:15.360
And based on this dictionary and Kubernetes API, we created a YAML file.

12:15.360 --> 12:20.360
So, here we can see a set of actions for one set of VMs.

12:20.360 --> 12:25.360
So, we started by creating the new namespace on the target environment.

12:25.360 --> 12:32.360
Then, we created an admin role that gave full permissions on all the resources under this namespace.

12:32.360 --> 12:40.360
And finally, we created a role binding that bind between a specific user and the role, the admin role.

12:40.360 --> 12:45.360
And by that, we made sure that each user will retain its original access to its resources.

12:47.360 --> 12:52.360
So, now that we finished with the planning and preparation phase,

12:52.360 --> 12:56.360
let's dive into the migration execution.

12:59.360 --> 13:01.360
So, our first step was to deploy Forklift.

13:01.360 --> 13:08.360
Forklift can be installed from the operator hub, and it's managed by an operator lifecycle manager.

13:08.360 --> 13:12.360
In our case, we decided to install it on the same cluster as the target one,

13:12.360 --> 13:16.360
but it also can be deployed on a remote different cluster.

13:16.360 --> 13:21.360
Next, we had to create a new namespace that will hold all the migration resources,

13:21.360 --> 13:25.360
including providers, different mappings, and the plans themselves.

13:25.360 --> 13:28.360
It's important to know that the user used to create the namespace

13:28.360 --> 13:32.360
should have sufficient permissions on the migration resources.

13:33.360 --> 13:36.360
Next, we had to create the target and source provider.

13:36.360 --> 13:40.360
So, each provider represents the environment we're migrating from and to.

13:40.360 --> 13:45.360
Once we deploy Forklift, a new tab named migration will appear in the console,

13:45.360 --> 13:51.360
and from there, we can manage all of our migration resources, including the addition of new providers.

13:52.360 --> 13:57.360
So, we started by creating the source provider, and here we chose Redhead Virtualization,

13:57.360 --> 14:00.360
which is the downstream name for Ovid.

14:00.360 --> 14:06.360
We then had to fill in all the information about this environment, so Forklift will be able to connect it.

14:06.360 --> 14:12.360
Here, it's important to use users that have sufficient permissions on the VMs where about to migrate,

14:12.360 --> 14:14.360
or else the migration will fail.

14:14.360 --> 14:19.360
In our case, since we were dealing with scale migration, we went ahead and used Administrator account.

14:21.360 --> 14:24.360
Next, we created the target provider.

14:24.360 --> 14:30.360
So, here we chose OpenChief Virtualization, which is the downstream name for OKD.

14:30.360 --> 14:36.360
Here, we only need to fill in the name, and all other information is automatically filled in.

14:38.360 --> 14:41.360
Next, we had to create our network and storage mapping.

14:41.360 --> 14:47.360
Once the migration starts, Forklift needs to know how to redirect the incoming workloads in terms of villains and storage class.

14:47.360 --> 14:52.360
This mapping will tell him how to handle the incoming workloads.

14:52.360 --> 14:59.360
So, here we can see our network mapping, and we can see the new villains we created for our migration needs.

15:00.360 --> 15:13.360
Here, we can see the storage mapping and the storage class used for accommodating our incoming workloads.

15:15.360 --> 15:19.360
Finally, with the use of script, we had to create our migration plans.

15:19.360 --> 15:26.360
So, each plan holds inside of it all the VMs that are about to be migrated to do the same namespace.

15:27.360 --> 15:30.360
This means used by the same users.

15:30.360 --> 15:35.360
Once we were ready, we triggered, again, with the use of script, all the migration, and the migration started.

15:35.360 --> 15:42.360
As you can see, it also can be triggered from the console, but since we were handling with scale migration, we automate this process.

15:45.360 --> 15:51.360
Now, I would like to have a quick overview of the steps we had and add some additional information.

15:51.360 --> 15:56.360
So, we started by deploying Forklift and setting up all the costume resources for migration.

15:56.360 --> 16:04.360
Then, with the use of script, we automated all the plans and the migration.

16:04.360 --> 16:07.360
In our case, we decided to go with cold migrations.

16:07.360 --> 16:16.360
That means that during the transition, the VM is going to be shut down, because it best suited our needs.

16:16.360 --> 16:22.360
We're migration, on the other hand, keeping the VM operational during the migration,

16:22.360 --> 16:28.360
but it's leading to longer migration time, because we need constantly backing up the data to keep the VM operational.

16:28.360 --> 16:35.360
So, during this transition, we also monitored and troubleshoot the entire process just to make sure that we're on track.

16:35.360 --> 16:44.360
And once the migration was over, we chose some randomly VMs and tested to see their up and running, and then waited for some user feedback.

16:46.360 --> 16:52.360
So, although eventually we had a successful migration, we did encounter some issues during it.

16:52.360 --> 16:57.360
So, the first two issues related to the fact that we had a lot of simultaneously migration running at once.

16:57.360 --> 17:05.360
That caused both storage and network strain, and eventually led to longer migration times than we originally anticipated.

17:05.360 --> 17:12.360
Another issue we encountered caused some of the migration to fail, and after we had some investigation,

17:12.360 --> 17:17.360
we realized it was related to some bug in our codebase.

17:17.360 --> 17:26.360
After that, we released a fix, and with the use of that fix, we were able to migrate all the VMs, and it was included in the next version of Forklift.

17:26.360 --> 17:38.360
And finally, since the downtime was involved, we had to keep a clear communication and make sure everyone in the loop of what's happening.

17:38.360 --> 17:47.360
So, once we started receiving user feedback from the field, it was clear that we still have some issues to solve in order to make this transition fully successful.

17:47.360 --> 17:51.360
So, the first one was related to boot order issues.

17:51.360 --> 17:56.360
So, VMs with multiple disks were not booting from the right one.

17:56.360 --> 18:06.360
So, we addressed this issue manually, and later we discovered it was caused by another bug in our codebase that was fixed in the next version of Forklift.

18:06.360 --> 18:09.360
The second issue was related to the new VLAN we used.

18:09.360 --> 18:15.360
That caused our FQDN names to change, and the workload inside the VMs were no longer accessible.

18:15.360 --> 18:24.360
So, we had to update our DNS records, and the user had to adjust their FQDN names inside the workload to use the new ones.

18:24.360 --> 18:31.360
And after that, all the workloads were accessible again.

18:31.360 --> 18:40.360
So, as we're reaching the end of today's journey, I think it's a good point to reflect and draw some conclusions.

18:40.360 --> 18:43.360
So, overall, we had a successful migration.

18:43.360 --> 18:49.360
We were able to migrate more than 100 VMs and copy 12 terabytes of data.

18:49.360 --> 19:08.360
We mainly were able to achieve this result through thorough in-depth preparation and planning, and we realized how much it's crucial for a successful migration.

19:08.360 --> 19:14.360
Another thing is that we understand that each migration process can be different and held between different environments,

19:14.360 --> 19:21.360
but we do see some common ground and best practice that can be used to similar journeys.

19:21.360 --> 19:29.360
And finally, and probably the most important, is that even though Forklift is a really powerful tool and it gives us great capabilities,

19:29.360 --> 19:43.360
it cannot facilitate migration on its own, and additional steps are required, such as the use of scripts and thorough preparation.

19:44.360 --> 19:51.360
So, as we're wrapping up today's session, I would like to extend my biggest gratitude for each and every one of you.

19:51.360 --> 19:59.360
I hope that the session today will be valuable for people that want to go on the same journey.

19:59.360 --> 20:09.360
I wasn't able to cover into details all today topics, but we post a blog post about this.

20:09.360 --> 20:14.360
So, whoever wants to get another information, you're more than welcome to take a look.

20:14.360 --> 20:18.360
And that's it. Question and some insights.

20:18.360 --> 20:20.360
Thank you.

20:29.360 --> 20:30.360
Yeah.

20:30.360 --> 20:43.360
How did you handle notifying the VM owners during the process you automated the fanning with the old automated notifications?

20:43.360 --> 20:47.360
Yeah, we had... Can you repeat the question?

20:47.360 --> 20:49.360
No, you should get the question.

20:49.360 --> 20:50.360
Sorry.

20:50.360 --> 20:53.360
For the streaming, so maybe people are watching from the audience.

20:53.360 --> 20:58.360
So, did we automate the process of notifying the VM owners?

20:58.360 --> 21:10.360
So, in our case, we had a VM list that all the owners on this environment were included in, and we said as spreadsheet that all the VMs that were eligible for migration were included.

21:10.360 --> 21:21.360
And then we asked the owners to let us know if they want to migrate their VMs, because there was people that decided to continue to different environments or didn't need the VM at all.

21:22.360 --> 21:29.360
And based on this information, we also built our migration, our final migration list.

21:29.360 --> 21:32.360
So, yeah.

21:32.360 --> 21:34.360
Yes.

21:34.360 --> 21:41.360
Kai, could you please give us some example of what issues you had during the migration steps?

21:41.360 --> 21:48.360
Yeah, so I will give an example about some boat issue we had after the migration.

21:48.360 --> 21:57.360
So, we had a lot of VMs with multiple disks, and when you're trying to boot from the disk that doesn't have the operation system on it, the boot will fail.

21:57.360 --> 22:03.360
So, you see just like a black screen, and the OS is not found.

22:03.360 --> 22:09.360
So, we understood that it's probably not booting from the right disk, because we saw there is another one.

22:09.360 --> 22:13.360
And once we manually changed that, we really saw that it's solving the issue.

22:13.360 --> 22:18.360
So, we adjust this manually for all the VMs in the migration list.

22:18.360 --> 22:22.360
And after that, as I said, we released the fix in our next version.

22:25.360 --> 22:26.360
Yeah.

22:26.360 --> 22:27.360
Hi.

22:27.360 --> 22:28.360
Hi.

22:28.360 --> 22:32.360
Is this tool also performing some kind of a preferred check over the plane?

22:32.360 --> 22:33.360
I don't know.

22:33.360 --> 22:42.360
It's checking that you have enough space on the target storage class, or it's checking that the VM you selected is not exposing particular devices.

22:43.360 --> 22:46.360
But can make the middle of the end?

22:46.360 --> 22:58.360
We do have set of, so the question was if we did some verification to make sure that we have enough space on our target environment or in devices like in compute.

22:58.360 --> 23:06.360
So, we do have set of validations on our plans, but this one are not included.

23:06.360 --> 23:15.360
We're checking more things like names that match Kubernetes and more security things, not something like that.

23:17.360 --> 23:18.360
Yes.

23:18.360 --> 23:22.360
You transferred, you mentioned 12 terabytes of data.

23:22.360 --> 23:31.360
I was in the presentation yesterday about talking about PCCOP, and then we're talking about validating that all the data was named correction over a large database migration.

23:32.360 --> 23:33.360
Did you do some things?

23:33.360 --> 23:35.360
Because I was saying quite a hard problem.

23:35.360 --> 23:38.360
A lot of days you might get a crack or off.

23:38.360 --> 23:44.360
So, the question is if we're doing some validation on the data, if it's copying correctly.

23:44.360 --> 23:56.360
So, it's depend on the source environment, but we do use some external tools for that, and these external tools supposed to make sure that all the data copied correctly.

23:56.360 --> 24:03.360
So, it's really depending on the source environment you're using because there's different flows between the different environments.

24:03.360 --> 24:11.360
But the tools that we're using, for VMware for example, we're using VIRT V2V, so we're taking care for this check.

24:11.360 --> 24:19.360
For Ovid and OpenStack, we're using ImageIO, so it's taking care of under this tool.

24:19.360 --> 24:29.360
Okay, so if anyone want to ask any specific question, feel free to approach outside. Thank you.

