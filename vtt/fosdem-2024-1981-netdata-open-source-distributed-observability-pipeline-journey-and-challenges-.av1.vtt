WEBVTT

00:00.000 --> 00:12.240
So the next presentation is from Costa about net data and open source distributed observability

00:12.240 --> 00:14.640
pipeline journey and challenges.

00:14.640 --> 00:16.140
A plus.

00:16.140 --> 00:18.280
Hello guys.

00:18.280 --> 00:24.040
So how many people use the data in this room?

00:24.040 --> 00:26.680
Oh, a lot.

00:26.680 --> 00:28.880
Okay.

00:28.880 --> 00:37.800
So guys, I have to admit something when I when I get pissed off, I write code.

00:37.800 --> 00:41.680
I know others go to the gym.

00:41.680 --> 00:45.880
But for me, this is how this thing was born, actually.

00:45.880 --> 00:51.320
So I was migrating some infrastructure from on prem to the cloud to one provider.

00:51.320 --> 00:53.080
Doesn't matter.

00:53.080 --> 00:55.680
And we had a lot of problems.

00:55.680 --> 01:01.400
It turned to be a cloud provider bags, not our bags.

01:01.400 --> 01:07.080
But after spending a couple of million euros in observability, building a team of eight

01:07.080 --> 01:11.200
people installing every tool that exists out there.

01:11.200 --> 01:14.400
This was back in 2014.

01:14.400 --> 01:19.840
I concluded that everything that I had built, I was a level executive back then to a FinTech

01:19.840 --> 01:21.280
company.

01:21.280 --> 01:24.800
I concluded that everything that I had built is an illusion.

01:24.800 --> 01:31.240
So the entire everything every observability tool that I tried is an illusion.

01:31.240 --> 01:33.400
Is that make me feel happy?

01:33.400 --> 01:36.480
It doesn't actually troubles with anything.

01:36.480 --> 01:41.520
So I started thinking, OK, why they did it like that?

01:41.520 --> 01:44.760
Why they are not real time?

01:44.760 --> 01:49.560
Why cardinality or granularity is such a problem?

01:49.560 --> 01:54.440
Why, you know, you have to set up everything.

01:54.440 --> 02:01.560
By bit, you need months and months of preparation in actually to build something useful.

02:01.560 --> 02:02.560
And I started experimenting.

02:02.560 --> 02:10.920
I started writing code to see can it be done differently and what this different would

02:10.920 --> 02:12.640
look like.

02:12.640 --> 02:19.800
So I ended up, I started the first committee in 2014, it is in 2014, it is in GitHub.

02:19.800 --> 02:26.080
After a couple of years of experimentation, I released it and boom, people loved it.

02:26.080 --> 02:28.480
So what is the data today?

02:28.480 --> 02:36.400
Today, the data is a monitoring tool that you install on a server and it is a monitoring

02:36.400 --> 02:38.160
in a box.

02:38.160 --> 02:43.800
So it has all the functions of a monitoring pipeline.

02:43.800 --> 02:48.600
It discovers metrics, collect metrics, stores metrics, machine learning for metrics, to

02:48.600 --> 02:54.120
learn the behavior, alerts check, health checks for metrics.

02:54.120 --> 02:57.760
It exports metrics other times on databases.

02:57.760 --> 03:04.880
It streams metrics and replicates metrics between the data agents, of course visualizes,

03:04.880 --> 03:06.760
query engines, everything.

03:06.760 --> 03:09.000
So it's an application.

03:09.000 --> 03:17.720
You install on all your servers and each application is a monitoring.

03:17.720 --> 03:24.080
So if you install the data today on an empty VM, an empty, just go to AWS, get an empty

03:24.080 --> 03:25.080
VM and install the data.

03:25.080 --> 03:26.080
What are you going to get?

03:26.080 --> 03:28.160
We are going to get this.

03:28.160 --> 03:33.280
200 dashboard charts, 2,000 unique time series.

03:33.280 --> 03:38.520
Five unique alerts monitoring, more than 100 components.

03:38.520 --> 03:44.080
As a viewer, as a logs explorer for system to journal, a network explorer for all your

03:44.080 --> 03:49.520
connections, unsupervised anomaly detection for everything so it will learn the behavior

03:49.520 --> 03:53.560
and trigger anomalies, detect outliers.

03:53.560 --> 03:59.000
You are going to get about two months of retention using half a gigabyte of disk space and all

03:59.000 --> 04:08.120
of these run with one presence CPU of a single core, 120 megabytes RAM and almost no disk

04:08.120 --> 04:10.960
IO.

04:10.960 --> 04:18.880
Now when you install an agent, you understand that since each one of them is standalone,

04:18.880 --> 04:23.200
you have to switch, go one by one to see what's happening.

04:23.200 --> 04:25.040
But then we build this.

04:25.040 --> 04:28.920
So the same software can become apparent.

04:28.920 --> 04:31.200
So when it becomes apparent, it's easy.

04:31.200 --> 04:34.400
You go to the others and say, stream there.

04:34.400 --> 04:37.960
And you give the IP of the other thing and then up it again.

04:37.960 --> 04:45.600
So the moment this happens, the parent now can provide alarms, machine learning, dashboards

04:45.600 --> 04:50.000
and everything for all the infrastructure that is behind it.

04:50.000 --> 04:51.720
But we didn't stop there.

04:51.720 --> 04:55.840
You can have as many layers as you want of this.

04:55.840 --> 05:02.720
So you can have data centers that they have apparent cluster and then regions, you know,

05:02.720 --> 05:08.720
a bureau for something that has another cluster of parents, et cetera, et cetera.

05:08.720 --> 05:11.360
This scales well.

05:11.360 --> 05:19.160
So we tested the data apparent with 500 nodes and 40,000 containers.

05:19.160 --> 05:20.160
Against Prometheus.

05:20.160 --> 05:24.320
We did the same with Prometheus in parallel.

05:24.320 --> 05:32.440
The data used one third less CPU, half the memory, 10% less bandwidth, almost no disk

05:32.480 --> 05:34.120
IO.

05:34.120 --> 05:37.960
And in the same store as footprint, we gave them three terabytes.

05:37.960 --> 05:43.680
We managed to have up more than a year of retention.

05:43.680 --> 05:44.680
More than a year of retention.

05:44.680 --> 05:48.920
Of course, it is in tiers.

05:48.920 --> 05:54.240
Now in December, the University of Amsterdam did research about the most energy efficient

05:54.240 --> 05:55.240
tool.

05:55.240 --> 05:58.080
They tested, of course, Prometheus and the data, et cetera.

05:58.080 --> 06:04.200
The data, they said, being the most energy efficient tool.

06:04.200 --> 06:06.880
And what happens in CNCF?

06:06.880 --> 06:16.200
The data actually leads the observability category in CNCF in terms of user's love.

06:16.200 --> 06:22.000
Actually we don't want to incubate in CNCF because of their policies.

06:22.000 --> 06:26.680
We believe that the data should be something different.

06:26.680 --> 06:30.080
So we never applied.

06:30.080 --> 06:35.000
So to build this, we had a number of challenges.

06:35.000 --> 06:42.680
The first challenge is that we wanted everyone, everyone using the data, to be from zero to

06:42.680 --> 06:45.640
hero to day.

06:45.640 --> 06:47.160
So you don't have observability.

06:47.160 --> 06:48.160
That's okay.

06:48.160 --> 06:53.280
You're installing a data and you have observability today.

06:53.280 --> 06:56.040
How we did that?

06:56.040 --> 07:03.120
The first is an observation that we all have the same kind of infrastructure.

07:03.120 --> 07:04.800
It's a finite set.

07:04.800 --> 07:08.960
We use some visual servers or physical servers.

07:08.960 --> 07:14.400
We use packets, applications, databases, web servers, message brokers.

07:14.400 --> 07:17.720
All these are standard components.

07:17.720 --> 07:20.640
We use similar disks, similar interface.

07:20.640 --> 07:21.840
Everything is similar.

07:21.840 --> 07:26.320
Of course, the Lego we built is different.

07:26.320 --> 07:28.160
Each company is unique.

07:28.160 --> 07:33.440
But the components are pretty much the same.

07:33.440 --> 07:38.200
So what will happen if we start monitoring the components?

07:38.200 --> 07:45.440
What will happen if the monitoring solution knows when a postgres is healthy?

07:45.440 --> 07:49.320
When a Kafka is healthy?

07:49.320 --> 07:52.720
When an NGNX, an HA proxy?

07:52.720 --> 07:58.320
What if the monitoring system itself knows if the networking stack of the Linux of this

07:58.320 --> 08:01.000
Linux is okay?

08:01.000 --> 08:05.480
And how to monitor it and how to present it and how to visualize it?

08:05.480 --> 08:14.840
So the observation that we have a lot of common actually allowed us to think a little bit

08:14.840 --> 08:15.840
different.

08:15.840 --> 08:20.120
So how many work with Prometheus here?

08:20.120 --> 08:22.000
Okay.

08:22.000 --> 08:24.600
So did you know the Prometheus, the OpenMetrics format?

08:24.600 --> 08:29.840
So you have a metric and then you have some labels and the value that changes over time.

08:29.840 --> 08:31.720
That's the thing.

08:31.720 --> 08:36.880
What we did there is actually to say, wait a moment, not all labels are the same.

08:36.880 --> 08:44.040
The labels that depict, for example, that this is reads or writes on a disk I.O. are

08:44.040 --> 08:45.240
not labels.

08:45.240 --> 08:47.120
They should not be labels.

08:47.120 --> 08:49.480
They should be something different.

08:49.480 --> 08:53.200
And we came up with this needle framework.

08:53.200 --> 08:55.720
This needle framework says that there is context.

08:55.720 --> 08:59.640
The context is the metric name in Prometheus, exactly the same, the metric name that has

08:59.640 --> 09:02.320
before the labels.

09:02.320 --> 09:04.200
We have instances.

09:04.200 --> 09:06.840
This does not exist in Prometheus.

09:06.840 --> 09:14.000
Instances are all the labels except the ones that get time series values.

09:14.000 --> 09:20.080
So if we're discussing about disks, it's the make of the disks, the type of the disks.

09:20.080 --> 09:21.480
This is the number of the disks.

09:21.480 --> 09:26.880
All these are labels to enrich an instance.

09:26.880 --> 09:31.800
The reads and writes of its performance are time series.

09:31.800 --> 09:35.600
So what this allows us to do is the following.

09:35.600 --> 09:43.480
The first thing is that it allows us to go and configure test boards at the context level.

09:44.360 --> 09:46.520
So we don't care about the metrics anymore.

09:46.520 --> 09:51.760
In order to provide a meaningful dashboard, we only care about the context.

09:51.760 --> 09:59.160
Then it allows us to configure alerts per context which are applied to all instances.

09:59.160 --> 10:03.760
So we can say, hey, apply this alert to all NVMe disks.

10:03.760 --> 10:08.160
Apply this alert to all Postgres databases.

10:08.280 --> 10:13.800
Apply this alert to all Postgres databases before version X or Y.

10:13.800 --> 10:24.560
So the first thing is that this thing allows us to have a fully automated visualization.

10:24.560 --> 10:26.600
You don't need to configure visualization with that data.

10:26.600 --> 10:28.400
It comes up.

10:28.400 --> 10:29.400
The same with alerts.

10:29.400 --> 10:30.520
They come up.

10:30.520 --> 10:34.960
We see 344 of them.

10:34.960 --> 10:42.320
So the moment something happens, boom, an alert from the data will come up.

10:42.320 --> 10:43.480
I think this is not important.

10:43.480 --> 10:48.560
It's how if you see it from the disk perspective or from the container perspective or from

10:48.560 --> 10:52.880
the database perspective, how the labels are matched, it's not that important.

10:52.880 --> 10:55.520
And this is internal information for us.

10:55.520 --> 11:06.240
So this allowed us to have, as I said, fully automated all of this.

11:06.240 --> 11:10.720
And the data is the only monitoring tool that you can install mid-crisis.

11:10.720 --> 11:13.680
So you have an incident happening now.

11:13.680 --> 11:15.600
You don't have observability.

11:15.600 --> 11:18.360
You install the data, alarms will fire up.

11:18.360 --> 11:20.800
The dashboard will be there for you to investigate.

11:20.800 --> 11:28.600
The second challenge was, can we get rid of the query language for slicing and dicing

11:28.600 --> 11:31.280
the data?

11:31.280 --> 11:38.800
So in order to do this, we provide, you are going to come on a metadata board and suddenly

11:38.800 --> 11:40.600
you are going to see 2,000 metrics.

11:40.600 --> 11:42.520
And you are not aware of them.

11:42.520 --> 11:44.600
You just installed it.

11:44.600 --> 11:50.520
So how, what can we do so that you can make sense of this?

11:50.520 --> 11:52.960
You can see the dashboard and say, OK, I understand.

11:52.960 --> 11:55.160
I understand what I need to do.

11:55.160 --> 11:58.120
So this is a data chart.

11:58.120 --> 12:00.680
On the charts, we did the following.

12:00.680 --> 12:01.880
Of course, there is an info.

12:01.880 --> 12:03.240
What is this?

12:03.240 --> 12:10.520
But then we added a number of items on the chart to allow you slice and dice and see anomaly

12:10.520 --> 12:15.760
rates, you know, make sense of that thing.

12:15.760 --> 12:20.520
Now each menu that you see here has an analysis.

12:20.520 --> 12:25.920
And this analysis allows you to see, for example, for the nodes, allows you to see all this

12:25.920 --> 12:30.120
information that is described below from the same view.

12:30.120 --> 12:32.080
This is not another query.

12:32.080 --> 12:37.640
The biggest change that we did is that the data calculates this for every query, for

12:37.640 --> 12:38.640
every, so it's a cube.

12:38.640 --> 12:40.360
Think of your data as a cube.

12:40.360 --> 12:43.080
You can see them in different ways.

12:43.080 --> 12:48.000
What we do is that we provide a time series for one of the views, one of the sides of

12:48.000 --> 12:57.640
the cube, but we calculate some numbers for every possible pivot that you can do.

12:57.640 --> 13:06.520
So that by just browsing the menu, you can understand, you can grasp what the data say.

13:06.520 --> 13:08.120
The same is with group by.

13:08.120 --> 13:11.240
So if you want to slice them, we do exactly the same.

13:11.240 --> 13:16.640
We give you all the possible options that you can slice by just dropping a drop down

13:16.640 --> 13:23.600
menu, two clicks, boom, different cube, different view on the cube.

13:23.600 --> 13:27.920
And the data, and this is how the initial problem, do you remember, that was the migration

13:27.920 --> 13:29.160
I had?

13:29.160 --> 13:35.400
The migration I had was that the bug at the service provider was that they were stopping

13:35.400 --> 13:45.280
the VMs to perform some of their updates, but they were stopping the VMs in little increments.

13:45.280 --> 13:51.560
So suddenly, one Friday, all the VMs were going in slow motion because they were stopping

13:51.560 --> 13:54.640
starting, stopping, starting, stopping, starting like this.

13:54.640 --> 14:00.120
With the data, I was able to find it because every data collection that is missed in the

14:00.120 --> 14:02.960
data is a gap.

14:02.960 --> 14:06.080
So it is not in the chart.

14:06.080 --> 14:11.040
Grafana, for example, told the other tools, smooth it out because there is no bit.

14:11.040 --> 14:12.160
In the data, there is a bit.

14:12.160 --> 14:13.760
I have to collect this every second.

14:13.760 --> 14:15.480
If I miss one, it's a gap.

14:15.480 --> 14:20.400
I didn't collect it.

14:20.400 --> 14:26.360
So another of the challenges, I am going fast because I have only five minutes, guys.

14:26.360 --> 14:28.880
Machine learning.

14:28.880 --> 14:33.560
Google in 2019 announced this.

14:33.560 --> 14:35.080
So they said, come on.

14:35.080 --> 14:39.280
We cannot make machine learning work for observability.

14:39.280 --> 14:41.080
So all our ideas are bad.

14:41.080 --> 14:47.240
And as they say, Todd says here, we should feel bad about it.

14:47.240 --> 14:50.960
So the data does the following.

14:50.960 --> 14:57.520
Trains for every metric collected, trains 18 different models.

14:57.520 --> 15:04.120
These models cover the last few days of the metric behavior.

15:04.120 --> 15:10.040
So for the last few days, the data knows how the metric behaves.

15:10.040 --> 15:12.000
We machine learning.

15:12.000 --> 15:18.560
Now what we do is that every time that we get a new sample, we check these 18 models.

15:18.560 --> 15:25.560
And if all of them agree that this is an anomaly, we say, OK, this is an outlier, guys.

15:25.600 --> 15:27.080
This is an anomaly.

15:27.080 --> 15:28.080
OK?

15:28.080 --> 15:30.080
Five minutes, yeah, I know.

15:30.080 --> 15:31.080
OK?

15:31.080 --> 15:32.800
So what this allows?

15:32.800 --> 15:35.760
I'm going fast because there is a scoring engine inside the data.

15:35.760 --> 15:37.880
So scoring engine is an engine.

15:37.880 --> 15:39.600
You don't query for time series.

15:39.600 --> 15:42.440
You say, hey, I want to find the similarities in the metrics.

15:42.440 --> 15:45.960
I want to know the trends in metrics.

15:45.960 --> 15:48.680
So can you score them all for me, please?

15:48.680 --> 15:49.680
All.

15:49.680 --> 15:52.440
It doesn't matter how many I have.

15:52.440 --> 15:55.880
So to give you an example, this is a data dashboard.

15:55.880 --> 15:58.360
There is a menu where we categorize everything.

15:58.360 --> 16:00.160
So it's an infinite scrolling dashboard.

16:00.160 --> 16:01.160
One chart below the other.

16:01.160 --> 16:04.240
Of course, there are many tabs with different views.

16:04.240 --> 16:08.160
And you press the AR button on the top right.

16:08.160 --> 16:10.480
And there is an anomaly rate on every section.

16:10.480 --> 16:15.880
So you can see now where your anomalies are.

16:15.880 --> 16:19.040
And we have a host level anomaly.

16:19.040 --> 16:20.760
So look what happens here.

16:20.760 --> 16:26.760
When anomalies happen in our systems, they happen in clusters.

16:26.760 --> 16:27.760
Look there.

16:27.760 --> 16:28.760
It's five nodes.

16:28.760 --> 16:30.200
It's 10 nodes.

16:30.200 --> 16:32.200
It happened together.

16:32.200 --> 16:34.160
This every line is a node.

16:34.160 --> 16:36.560
Look there.

16:36.560 --> 16:38.440
They happen together.

16:38.440 --> 16:45.280
And not only that, this shows the percentage of metrics in a node being anomalous together

16:45.280 --> 16:48.760
concurrently.

16:48.760 --> 16:55.040
So you understand that NEDA can detect not only ifs a metric is anomalous, but here we

16:55.040 --> 17:02.200
can see that it can tell you the spread of the anomaly inside your systems, inside each

17:02.200 --> 17:03.720
system, et cetera.

17:03.720 --> 17:08.480
And then you highlight the area of interest.

17:08.480 --> 17:16.320
NEDA gives you a sorted list of all metrics that are anomalous within this region, this

17:16.320 --> 17:17.560
time frame.

17:17.560 --> 17:19.880
So the idea here is that you're a ham moment.

17:19.880 --> 17:23.920
Or what DNS did that?

17:23.920 --> 17:25.720
Storage did that?

17:25.720 --> 17:32.960
The ham moment to be on the top 20, 30 items.

17:32.960 --> 17:34.560
Another mission accomplished.

17:34.560 --> 17:37.560
Two minutes I know.

17:37.560 --> 17:40.000
Logs, I will pass through.

17:40.000 --> 17:41.160
NEDA is a fund.

17:41.160 --> 17:46.840
We base everything on system in the journal D. We have built our own logs management solution

17:46.840 --> 17:48.960
like log key and elastic and the likes.

17:48.960 --> 17:49.960
We zapped it.

17:49.960 --> 17:50.960
We zipped it.

17:50.960 --> 17:51.960
Zipped it.

17:51.960 --> 17:52.960
How is this story?

17:52.960 --> 17:55.080
Don't use it anymore.

17:55.080 --> 17:58.760
And what we do is we rely on system D journals.

17:58.760 --> 18:05.000
Now system D journal has a lot of unique features guys, especially if you are developers.

18:05.000 --> 18:11.960
The ability to index all fields independently of the cardinality.

18:11.960 --> 18:12.960
Independently.

18:12.960 --> 18:13.960
It doesn't matter.

18:13.960 --> 18:17.360
A thousand fields, different values on every log line.

18:17.360 --> 18:18.360
Indexed.

18:18.360 --> 18:19.360
Okay?

18:19.360 --> 18:20.760
This is unique.

18:20.760 --> 18:24.360
You can make troubleshooting amazing like this.

18:24.360 --> 18:29.680
We have a UI similar to Grafana, similar to Kibana.

18:29.680 --> 18:34.720
And actually while all these tools you see on the screen, it's 15 million entries.

18:34.720 --> 18:35.720
The title.

18:35.720 --> 18:37.520
15 million entries.

18:37.520 --> 18:38.520
Logs.

18:38.520 --> 18:42.880
All the others will sample 5,000 at the end.

18:42.880 --> 18:45.760
We stop sampling at a million.

18:45.760 --> 18:46.760
It's fast.

18:46.760 --> 18:52.640
We stop sampling at a million.

18:52.640 --> 18:55.960
We submitted patches to system D to make it faster.

18:55.960 --> 19:03.200
We managed to bypass all the deficiencies of system D for old systems.

19:03.200 --> 19:05.000
You will give me two minutes.

19:05.000 --> 19:06.000
Okay.

19:06.000 --> 19:07.480
It was 50 seconds.

19:07.480 --> 19:08.600
I have another one.

19:08.600 --> 19:09.600
So, okay.

19:09.600 --> 19:10.760
We did a lot of work.

19:10.760 --> 19:17.280
We provided a tool, look to journal that can be used to feed to system D journal.

19:17.280 --> 19:21.920
Your HA proxy logs, your engine logs or whatever, log FMP logs.

19:21.920 --> 19:24.200
If you have a go application, something look like this.

19:24.200 --> 19:25.200
Boom.

19:25.200 --> 19:29.280
Push it everything in.

19:29.280 --> 19:30.880
And of course there is a weakness.

19:30.880 --> 19:32.480
It's a system D journal.

19:32.480 --> 19:35.880
And this is mainly because it's reliable.

19:35.880 --> 19:37.640
It's fault tolerant.

19:37.640 --> 19:39.240
This is what has been designed.

19:39.240 --> 19:42.200
It's secure so it seals the logs.

19:42.200 --> 19:44.880
They cannot be tampered, etc.

19:44.880 --> 19:48.680
The key problem is this storage space that it needs.

19:48.680 --> 19:54.280
It needs a lot more compared to Loki or I don't know with elastic, but for Loki that

19:54.280 --> 19:57.000
we tested, 10 times more.

19:57.000 --> 20:03.720
And you can make it a little bit better, 4 times more if you use a compressed file system.

20:03.720 --> 20:05.960
So another mission accomplished.

20:05.960 --> 20:06.960
That's the last one.

20:07.680 --> 20:09.200
Wait, I didn't see what it was.

20:12.320 --> 20:16.080
Yes, and that's another beauty that the data provides.

20:16.080 --> 20:23.840
So observability is more guys than metrics, logs and traces.

20:23.840 --> 20:29.080
So the moment you run your thumb infrastructure and there is an issue, metrics,

20:29.080 --> 20:32.880
logs and traces usually are not enough.

20:32.880 --> 20:36.400
You may want to see the slow queries the system has.

20:36.400 --> 20:37.760
Database has.

20:37.760 --> 20:43.000
You may want to see the socket that an application is currently connected.

20:43.000 --> 20:47.440
You may want to see different kinds of data.

20:47.440 --> 20:49.640
Most of the observability solutions give up.

20:49.640 --> 20:51.440
They say, OK, open the console.

20:51.440 --> 20:52.440
What can we do?

20:52.440 --> 20:55.160
Go to the console and figure it out.

20:55.160 --> 20:59.360
What we did is this.

20:59.360 --> 21:03.480
So every data collection, this isn't a data.

21:03.480 --> 21:05.880
It has plugins, data collection plugins.

21:05.880 --> 21:09.160
Every data collection plugin exposes functions.

21:09.160 --> 21:17.120
It says, hey, I am Postgres collector and I can expose, give you slow queries.

21:17.120 --> 21:23.400
Hey, I am a network viewer and I can give you the list of circuits.

21:23.400 --> 21:27.800
So we built the protocol in such a way so that you are there.

21:27.800 --> 21:34.120
You go to the grand, grand parent and you ask for something from this node,

21:34.160 --> 21:36.320
this function to be run.

21:36.320 --> 21:39.600
It does.

21:39.600 --> 21:44.160
So actually the whole slogging system that we have, system-less system in journal,

21:44.160 --> 21:45.040
is built like this.

21:45.040 --> 21:48.240
There is a system in journal plugin there that exposes logs.

21:51.320 --> 21:53.360
And this is a network viewer.

21:53.360 --> 21:55.360
What we did here is very simply.

21:55.360 --> 21:56.040
You have sockets.

21:56.040 --> 22:02.640
As you can see, it's 30,000, 31,000 TCP4 sockets.

22:02.640 --> 22:06.320
The key problem was how you make sense.

22:06.320 --> 22:08.240
So what we said it's OK.

22:08.240 --> 22:09.800
From there is the internet.

22:09.800 --> 22:11.640
From here are private IPs.

22:11.640 --> 22:15.040
Above are clients, down are servers.

22:15.040 --> 22:19.040
The position plays a role.

22:19.040 --> 22:22.760
So you can immediately see what connects to the internet.

22:22.760 --> 22:23.720
And this is live.

22:23.720 --> 22:30.840
So if this was not a screenshot and you SSH the server, you would see the SSHD boom, moving.

22:30.880 --> 22:31.400
This is live.

22:36.400 --> 22:38.360
OK.

22:38.360 --> 22:41.600
Our monetization strategy, the data is open source.

22:41.600 --> 22:44.320
This is what we sell just for clarity.

22:44.320 --> 22:47.600
So we sell infinite horizontal scalability.

22:47.600 --> 22:50.200
We sell role-based access.

22:50.200 --> 22:53.800
Access from anywhere because it's on-prem and on-prem solution.

22:53.800 --> 22:58.360
Mobile app for notifications and dynamic configuration and persistent

22:58.400 --> 23:02.680
customizations that we have a SAS offering that does it.

23:02.680 --> 23:09.600
Of course, the same SAS offering is offered on-prem for a price.

23:09.600 --> 23:10.600
That's it.

23:10.600 --> 23:13.600
Thank you very much.

23:13.600 --> 23:14.600
Again.

23:20.600 --> 23:23.360
Questions.

23:23.360 --> 23:24.600
Thanks, Costa.

23:24.600 --> 23:25.840
Do we have any questions here?

23:29.360 --> 23:31.960
I think that is fantastic.

23:31.960 --> 23:36.720
I've been using it for a while and I really, really enjoyed the experience.

23:36.720 --> 23:40.560
A while ago, I was working on a massive experiment.

23:40.560 --> 23:45.480
I had 700 machines that had to throw gigabytes of information.

23:45.480 --> 23:52.760
So I started to look at NetBe and basically went to the grandfather solution.

23:52.760 --> 23:56.800
And then the monitoring was something that wasn't there.

23:56.800 --> 24:00.320
I mean, it's a SAS service.

24:00.320 --> 24:01.760
It's not open source that part.

24:01.760 --> 24:04.280
Do you think that you're going to open source also that part?

24:04.280 --> 24:06.240
Wait.

24:06.240 --> 24:14.080
The first thing is the open source agent and the SAS offering that we have have the same UI.

24:14.080 --> 24:15.560
It's the same thing.

24:15.560 --> 24:17.760
It's on the different UI.

24:17.760 --> 24:23.640
What the data cloud can do is instead of collecting all the data, if you go to data log,

24:23.640 --> 24:25.920
you push all your data to data.

24:25.920 --> 24:27.000
We don't do that.

24:27.000 --> 24:31.360
So your data are inside your data inside your servers.

24:31.360 --> 24:38.600
What the data cloud does is that it can provide you a unified view by querying all the relevant

24:38.600 --> 24:39.960
in the data.

24:39.960 --> 24:45.920
So it queries them, gets the data, aggregates them, and gives you the same view a parent

24:45.920 --> 24:47.600
would give you.

24:47.600 --> 24:48.680
Okay.

24:48.680 --> 24:50.200
Without having the data.

24:50.200 --> 24:55.040
We rely on your agents being alive for this.

24:55.040 --> 24:56.360
This is why we have the streaming.

24:56.360 --> 25:01.680
So when you have a femoral service, a Kubernetes cluster, the node may vanish.

25:01.680 --> 25:08.920
What you do there is you need a node outside this, outside the Kubernetes cluster to have

25:08.920 --> 25:10.920
the P2P parent for your data.

25:10.920 --> 25:15.240
This is where your data will be eventually.

25:15.240 --> 25:19.920
This allows you to have very small retention on every server, even disable a mail and health

25:20.320 --> 25:20.920
checks, etc.

25:20.920 --> 25:25.920
Disable them on your production systems and have a cluster of parents.

25:25.920 --> 25:32.680
Parents can be clusters so you can have two or three or four or how many you want in a

25:32.680 --> 25:37.200
chain so that you can loop them, replicate one to another.

25:37.200 --> 25:42.440
And they will find out if something is missing, they will find out by themselves.

25:42.440 --> 25:45.440
Okay.

25:45.440 --> 25:51.880
We have time for one, another question.

25:51.880 --> 25:52.880
Thank you.

25:52.880 --> 26:00.080
I would like to ask you which issues have you experienced when scaling up on several systems

26:00.080 --> 26:04.200
or centralized things?

26:04.200 --> 26:10.880
Which issues have you experienced when you scale up to several systems or other centralized

26:10.880 --> 26:11.880
things?

26:12.320 --> 26:18.560
As with every software, there is a maximum vertical scalability that can sustain.

26:18.560 --> 26:20.680
The data as you show is better than Prometheus.

26:20.680 --> 26:26.040
Actually, you can be in almost two times in some cases in terms of memory, one third in

26:26.040 --> 26:27.840
terms of CPU, etc.

26:27.840 --> 26:30.080
But still, there is a gap.

26:30.080 --> 26:33.280
The power of the machine, you can give to it.

26:33.280 --> 26:34.280
Okay.

26:34.280 --> 26:46.960
Now when you need then to scale out, what we did is that we split a query, a number of

26:46.960 --> 26:50.640
calculations, has to do a number of stuff in order to give you this little chart there

26:50.640 --> 26:52.640
with the drop downs and the likes.

26:52.640 --> 27:00.200
What we do is that we have the agents to have the work and then we have a central component

27:00.200 --> 27:02.280
that does the rest of the work.

27:02.280 --> 27:09.280
So we managed to split the query engine in two parts, one that runs at the edge and one

27:09.280 --> 27:11.480
that runs centrally.

27:11.480 --> 27:12.480
Still there are limits.

27:12.480 --> 27:17.720
If you go and put, I don't know, 10,000 and a data agents in one room and you want one

27:17.720 --> 27:22.320
chart with 10,000 queries, you understand it's going to 10,000 queries are going to

27:22.320 --> 27:23.320
happen.

27:23.320 --> 27:25.440
So it may take some time.

27:25.440 --> 27:32.240
But still what we found out is that the combined power of your infrastructure is better in

27:32.240 --> 27:36.960
most of the cases than having a very big server.

27:36.960 --> 27:42.720
So even if you query 10,000 servers, there is some latency due to the round trip delays,

27:42.720 --> 27:43.720
et cetera.

27:43.720 --> 27:48.000
But the horsepower that you have, each server will do just a tiny thing.

27:48.000 --> 27:49.640
Oh, this is my part taken.

27:49.640 --> 27:51.480
This is my part taken.

27:51.480 --> 27:56.720
But this means that the heavy lift of the query is spread to 10,000 servers.

27:56.720 --> 27:57.720
You get it?

27:57.720 --> 28:02.520
So it becomes more scalable, faster at the end of the day.

28:02.520 --> 28:04.040
All right.

28:04.040 --> 28:05.080
Thank you.

