WEBVTT

00:00.000 --> 00:05.000
Hello everyone.

00:05.000 --> 00:08.000
You have two more minutes.

00:08.000 --> 00:10.000
My name is Andre.

00:10.000 --> 00:12.000
I work for...

00:12.000 --> 00:16.000
Okay.

00:16.000 --> 00:20.000
I've worked for Red Hat for several years now as quality engineering.

00:20.000 --> 00:24.000
And today's talk is really about performance testing,

00:24.000 --> 00:26.000
but it's not about the testing itself.

00:26.000 --> 00:30.000
It's more than why we should do it.

00:30.000 --> 00:32.000
You have six minutes in class.

00:32.000 --> 00:33.000
Okay.

00:33.000 --> 00:35.000
You're starting early, just so you know.

00:35.000 --> 00:37.000
Yeah.

00:37.000 --> 00:39.000
Okay.

00:39.000 --> 00:46.000
So it's more why we should do it and why there are benefits in it,

00:46.000 --> 00:49.000
even if you do it wrongly or imperfectly.

00:49.000 --> 00:54.000
So that's the main point of the talk for today.

00:54.000 --> 00:58.000
So first thing first, why should we do it?

00:58.000 --> 01:03.000
What are the benefits for us doing the performance testing,

01:03.000 --> 01:08.000
even if we don't have isolated environment and all this kind of stuff?

01:08.000 --> 01:14.000
Well, for me, the main benefit is that even if you don't have the environment you would want,

01:14.000 --> 01:19.000
you can still find the bottlenecks in your application or whatever you are testing.

01:19.000 --> 01:24.000
And you still can optimize it even if you don't have everything ready

01:24.000 --> 01:29.000
because the truth is that the performance testing is quite expensive.

01:29.000 --> 01:33.000
And for the good one, I don't think that there are the companies

01:33.000 --> 01:37.000
that will give you the resources that you need to do it perfectly.

01:37.000 --> 01:43.000
So that's for me the main option or the main reason why to do it.

01:43.000 --> 01:50.000
And for me, the second most important is that you will gain knowledge that you need

01:50.000 --> 01:54.000
or you will obtain the knowledge about the product itself

01:54.000 --> 01:59.000
because you will suddenly see things that you cannot see,

01:59.000 --> 02:02.000
even if you normally deploy things and everything.

02:02.000 --> 02:06.000
You will see the little thingies that are happening here and there.

02:06.000 --> 02:12.000
And the information you gain are quite nice to get.

02:12.000 --> 02:17.000
So that's probably the most, the points that you should look for

02:17.000 --> 02:20.000
if you are thinking about the performance testing.

02:20.000 --> 02:25.000
This is actually what it will gain from it.

02:25.000 --> 02:32.000
So this is only in my opinion, like you will see a lot of papers about the performance testing

02:32.000 --> 02:36.000
and all the things that you have to take care of.

02:37.000 --> 02:42.000
Like on the GitHub, I know about two or three papers that have like 40 pages

02:42.000 --> 02:46.000
about the performance testing and all the criteria that you have to fill.

02:46.000 --> 02:53.000
In my opinion, there are like two variants of the performance testing.

02:53.000 --> 02:57.000
And first is measurement and second is testing.

02:57.000 --> 03:02.000
For me, the measurement is really the thing that you are looking for numbers.

03:02.000 --> 03:06.000
And you need those numbers for, I would say, legal reasons

03:06.000 --> 03:10.000
or anything what you have to declare for your customer.

03:10.000 --> 03:15.000
You will say that, for example, for us, I work on the division.

03:15.000 --> 03:21.000
Like if we would want to say that this connector is actually able to do 30k per second,

03:21.000 --> 03:28.000
we would need some kind of a proof that we can do it.

03:28.000 --> 03:32.000
And getting this proof is like, it's very complicated

03:32.000 --> 03:37.000
and you have to do it in very specific ways.

03:37.000 --> 03:44.000
And even if you have everything, it's not quite, it's not always acceptable.

03:44.000 --> 03:48.000
But the second part or second variation is just testing.

03:48.000 --> 03:53.000
And for me, the testing, the testing is really just finding the bottlenecks in your product.

03:53.000 --> 04:01.000
And I think that's even more, like the testing is even more important

04:01.000 --> 04:08.000
because there you will find all the bottlenecks and you can optimize your application really.

04:08.000 --> 04:13.000
And you can see the flaws in your call because you cannot see these things

04:13.000 --> 04:18.000
when you run it regularly and you don't have everything around the application tuned up

04:18.000 --> 04:21.000
so you don't throttle your application to the maximum.

04:21.000 --> 04:28.000
These things usually happen when you go over the top or near the maximum, near the max.

04:28.000 --> 04:34.000
So, yeah, this is in my opinion two ways how to do the performance testing

04:34.000 --> 04:38.000
or two variants of the performance testing.

04:38.000 --> 04:44.000
What is not really optimal about the testing, which I was talking about,

04:44.000 --> 04:48.000
just finding the bottlenecks, not the number, you need massive monitoring

04:48.000 --> 04:54.000
and I will say more about it in the talk, but that's like main disadvantage of it

04:54.000 --> 05:01.000
that most of the time you will go around tracing, monitoring, metrics

05:01.000 --> 05:08.000
and you will find some stuff that will really give you hard times figuring it out

05:08.000 --> 05:14.000
because you are going for performance and you are speaking in milliseconds, yeah?

05:14.000 --> 05:23.000
But most of the things that are used for monitoring are not really prepared to handle you for one minute second.

05:23.000 --> 05:29.000
They think that it's okay to scrape, for example, metrics by 10 seconds

05:29.000 --> 05:35.000
and like this will give you massive headaches during the time.

05:35.000 --> 05:43.000
I think I have already somehow gathered those, but the goal of the performance testing is,

05:43.000 --> 05:52.000
as I said, find the bottlenecks, but they are much more to it because, for example, the load types, you know,

05:52.000 --> 06:00.000
even if someone of you had crossed the performance testing, the main point all of the guys are talking about

06:00.000 --> 06:06.000
is what kind of load we are going to do, how we are going to do it to make it reproducible, you know?

06:06.000 --> 06:12.000
Because that's like a problem because for some application you put constant loads to them.

06:12.000 --> 06:19.000
Let's say you are going 10k per second, some requests for the API for one hour.

06:19.000 --> 06:29.000
And like that could be fine, but we all saw that some websites or, for example, the systems that you are buying tickets for concerts,

06:29.000 --> 06:36.000
these kind of things need peak loads. You know, you are going low for 5k per second

06:36.000 --> 06:42.000
and suddenly you spin it up for 100k per second or something like that.

06:42.000 --> 06:50.000
So you really need something that will generate the load for you and do it reproducible.

06:50.000 --> 06:57.000
You need to have the same load so you can repeat the testing for a couple of times

06:57.000 --> 07:09.000
and be consistent in that because otherwise you will find all the other things except the flux in your cold.

07:09.000 --> 07:14.000
So the main problems during the performance that you will find.

07:14.000 --> 07:18.000
I have said that you don't need the isolated environment to do it.

07:18.000 --> 07:25.000
And that's true. We don't have isolated environment in our team when we do performance testing.

07:25.000 --> 07:31.000
But if you don't have entirely isolated environment, you need to know your environment pretty well.

07:31.000 --> 07:37.000
You have to know your latencies, you have to know all the hardware specs and these kind of things.

07:37.000 --> 07:48.000
You really need all of the information because if you see some very specific things happen during the test which are not common,

07:49.000 --> 07:55.000
you then can somehow put the puzzle together with all the information you get from the environment

07:55.000 --> 08:07.000
and you can somehow at least, I would say, decrease the number of stupid mistakes that you will have to gather around.

08:07.000 --> 08:16.000
The next thing that is very important is to have the monitoring which I have said already.

08:16.000 --> 08:26.000
You need all the metrics you can get because before that you don't even know what kind of the information would be valuable for you when you start doing it,

08:26.000 --> 08:28.000
but you will need all of it.

08:28.000 --> 08:38.000
If you can get rid and you will surely use it, if you don't gather those metrics and then you will figure out you need it,

08:38.000 --> 08:41.000
you cannot get them from the past.

08:41.000 --> 08:47.000
So really the thing is gather everything you can and it will be fine.

08:47.000 --> 08:56.000
And the last thing for this is you need to tune up all the systems that you are dependent on.

08:56.000 --> 09:06.000
For us, we are working on databases and we cannot really throttle up our product if the database isn't optimized to the hardware it's running

09:06.000 --> 09:10.000
because if the database is not throttling, we are not throttling.

09:10.000 --> 09:20.000
So you basically need to have everything on the high spec there to not bottleneck your application.

09:20.000 --> 09:29.000
So that's one of the main points because on some things it's quite problematic to tune it up.

09:30.000 --> 09:40.000
I quite like this quote because it's all about the metrics and if you have them, it's fine and it's nice.

09:40.000 --> 09:42.000
If you don't have them, it's massive problems.

09:42.000 --> 09:47.000
So I think that's really the quote that you should be looking for.

09:50.000 --> 09:52.000
So again, monitoring.

09:52.000 --> 09:57.000
I have already talked about the problem with the scraping.

09:57.000 --> 10:08.000
So let's say we have used Prometheus mostly and the maximum what you can get from Prometheus is one second scraping.

10:08.000 --> 10:20.000
So that's fine for information causes but not for the performance metrics because the things happen during the milliseconds.

10:20.000 --> 10:33.000
Maybe 10 milliseconds would be enough but one second is really you are losing a lot of information and later on I have the example of what you could see when the scrapers are not fast enough.

10:33.000 --> 10:43.000
And there's like massive problem because not every scraper is or I would say there are no scrapers that can do it really fast.

10:43.000 --> 10:47.000
So you probably have to implement it and we are working on that actually.

10:47.000 --> 10:50.000
So that's the main problem.

10:50.000 --> 11:03.000
And the second problem is that you will end up with having a lot of the systems in the field because you need hardware metrics.

11:03.000 --> 11:06.000
You need JMX metrics and I don't know what other.

11:06.000 --> 11:08.000
It really depends on your application.

11:08.000 --> 11:14.000
But for us, we needed hardware metrics, JMX metrics and some metrics from our test suite.

11:14.000 --> 11:19.000
And these three things actually all the different outputs.

11:19.000 --> 11:24.000
You know, we have used net data for the hardware metrics.

11:24.000 --> 11:28.000
It's like really nice tool open source fast everything nice.

11:28.000 --> 11:32.000
But you cannot import JMX metrics to net data.

11:33.000 --> 11:45.000
And you net data has also one problem that you cannot import like anything would happen in the past because it's strictly hard coded for now.

11:45.000 --> 11:51.000
So that's the problem, you know, and then you will say yes.

11:51.000 --> 11:52.000
Okay.

11:52.000 --> 11:55.000
So I cannot have JMX metrics to the net data.

11:55.000 --> 11:59.000
So I'll add Prometheus, you know, that's fine.

11:59.000 --> 12:04.000
Okay. So you have now net data and Prometheus.

12:04.000 --> 12:05.000
Sorry.

12:12.000 --> 12:20.000
Well, and then you will continue because you still have all at least in our expertise.

12:20.000 --> 12:28.000
We still need that someplace to store the metrics from our test suite and you can just import it everywhere.

12:28.000 --> 12:38.000
So then you happen that you will deploy the Postgres because you can use Postgres as backup for the Prometheus for the data storage.

12:38.000 --> 12:45.000
So now we have net data, Postgres and also Prometheus.

12:45.000 --> 12:57.000
And last but not least, you will add the Grafana because you need to visualize it, you know, and getting all those things in shape that you have like massive monitoring.

12:57.000 --> 13:00.000
You know, everything can go wrong.

13:00.000 --> 13:18.000
So if you can use the least amount of the tools that you can use, it's better because once you have too many, it's nightmare to somehow get that in shape for the whole time.

13:18.000 --> 13:19.000
Yeah.

13:19.000 --> 13:24.000
So with the performance testing, you are not really looking for the numbers.

13:25.000 --> 13:33.000
Numbers are not important in this case because you don't want to see the throughput is like this or like that.

13:33.000 --> 13:44.000
You need to see the trends in the graphs because there you can see if you are constantly slowing down or if you are going like optimizing your way.

13:44.000 --> 13:51.000
So really you have to look for the patterns in graphs and the trends in the graphs.

13:51.000 --> 13:58.000
I have the example for our testing that I will show you the patterns which we have found.

13:58.000 --> 14:02.000
But before that, just our system under test was the BZM.

14:02.000 --> 14:17.000
I don't know if you guys know the BZM, but it's effectively changed that I capture streaming, which means that we sit on the top of the database and we scan the transaction logs and sense all the events that happen there to the Kafka.

14:17.000 --> 14:26.000
We are effectively running in Kafka Connect runtime, which makes the performance testing even more juicy, I would say, because the runtime is not ours.

14:26.000 --> 14:29.000
So it's a little tricky.

14:29.000 --> 14:33.000
So that's our system under test.

14:33.000 --> 14:38.000
And this is the first example that I have put on.

14:38.000 --> 14:44.000
The image on the graph on the top is basically our process duration.

14:44.000 --> 14:48.000
And there are two things that you can see on the graph.

14:48.000 --> 14:56.000
We are effectively most of the time we are oscillating between some values around 200 or 170 to 220.

14:56.000 --> 14:58.000
And that's entirely fine.

14:58.000 --> 15:02.000
That's actually what we want to see if you are looking for some responses.

15:02.000 --> 15:08.000
You need to oscillate around some value like a sinus graph or something like that.

15:08.000 --> 15:13.000
But what is not actually nice is this on the star.

15:14.000 --> 15:19.000
Where we are constantly getting slower and slower and slower.

15:19.000 --> 15:29.000
And we have some peaks there, which have these peaks are don't have a reason to be there because the data are the same all the time.

15:29.000 --> 15:31.000
So this is most likely the flaw in the code.

15:31.000 --> 15:34.000
There is something happening, what shouldn't be happening.

15:34.000 --> 15:38.000
And it can be the database flashing out to the score.

15:38.000 --> 15:42.000
It can be basically anything, but you know that there might be a problem.

15:42.000 --> 15:44.000
You have all other metrics.

15:44.000 --> 15:49.000
You can have metrics from the databases that will show you that flashing was happening or anything.

15:49.000 --> 15:52.000
But this is what you have to look for.

15:52.000 --> 15:57.000
These, it will certainly be different for your application than ours.

15:57.000 --> 16:01.000
You will have to define what you are looking for on the star.

16:01.000 --> 16:03.000
But that's the main thing.

16:03.000 --> 16:06.000
And the more funny example is this.

16:06.000 --> 16:07.000
No, here.

16:07.000 --> 16:11.000
This is Jmx metrics from our, from Libyseum.

16:11.000 --> 16:15.000
And it shows you the size of the queue.

16:15.000 --> 16:18.000
Internal queue of the Bizm.

16:18.000 --> 16:24.000
And that basically means that from the database you are reading to internal queue.

16:24.000 --> 16:29.000
So once the queue is zero, you are not reading.

16:29.000 --> 16:32.000
You know, but we are still processing, right?

16:32.000 --> 16:35.000
So there should, there is some mistake.

16:35.000 --> 16:39.000
And this is actually the problem with the scrapers.

16:39.000 --> 16:46.000
Because if the scraper takes each second, the database is pretty fast and it can empty up the,

16:46.000 --> 16:49.000
it can empty the queue during that time.

16:49.000 --> 16:53.000
And if the scraper hits the right time, it will give you zero.

16:53.000 --> 16:59.000
You know, so from this until the end, the graph is all wrong.

16:59.000 --> 17:00.000
It's not true.

17:00.000 --> 17:03.000
And it's all because of the speed of the scraper.

17:03.000 --> 17:06.000
Because it basically hit the wrong time.

17:06.000 --> 17:16.000
So that is the thing that you have to be worried about because this will happen surely.

17:16.000 --> 17:19.000
And these are some other graphs.

17:19.000 --> 17:21.000
These are, I would say, more wild.

17:21.000 --> 17:27.000
I would say it's from the start of our playing with the performance.

17:27.000 --> 17:31.000
But the top one is also, it's pretty cool.

17:31.000 --> 17:35.000
It's not that constant as it was for the previous one.

17:35.000 --> 17:43.000
But it's still in some borders, you know, we are somehow oscillating, but there is not really clear way.

17:43.000 --> 17:45.000
But the queue size is okay now.

17:45.000 --> 17:49.000
You can see because you have some data there, but not zero.

17:49.000 --> 17:54.000
So there's an issue with the scrapers, as I said.

17:54.000 --> 17:59.000
And this is actually the thing that you will have to look for the patterns.

18:00.000 --> 18:02.000
Really important in graphs.

18:02.000 --> 18:07.000
You can see all the different ones and there are a lot of papers on the Internet that you can find.

18:07.000 --> 18:10.000
What to look for on your specific application.

18:10.000 --> 18:14.000
So, yeah, but not look to the numbers.

18:14.000 --> 18:16.000
Numbers don't say you anything.

18:16.000 --> 18:23.000
You can usually get the higher numbers if you change up the hardware that you are running on.

18:23.000 --> 18:33.000
But if you can optimize it on the some hardware that you have, you will surely get the big numbers on almost anywhere.

18:33.000 --> 18:38.000
So some tips and tricks for me.

18:38.000 --> 18:46.000
During the way that we have started playing with the performance, we have developed a lot of tools.

18:46.000 --> 18:55.000
First is the database manipulation tool, which is effectively giving you a Json API.

18:55.000 --> 19:03.000
And just with the Json API, you can create DML for almost any databases.

19:03.000 --> 19:07.000
We have now probably MySQL, Postgres, and Oracle there.

19:07.000 --> 19:13.000
So it's just you don't need to have a lot of different JDBC connectors in your code.

19:13.000 --> 19:16.000
We'll just deploy this and it will take care of it.

19:16.000 --> 19:22.000
We have also implemented some kind of the load generator that can generate you.

19:22.000 --> 19:27.000
Load for, I would say, constant load, P-codes and all this kind of stuff.

19:27.000 --> 19:33.000
We have also some automation and the other, like MySQL auto-tune, that's it.

19:33.000 --> 19:44.000
We are pretty proud of that because it can basically tune your MySQL to the whole VM that you are running it or physical machine.

19:44.000 --> 19:48.000
And you would say that it's easy, but it's not.

19:48.000 --> 19:55.000
You know it's hard when you look on the seven or eight page of the Google.

19:55.000 --> 20:02.000
You know, in this phase, you know, we are probably not on the good shape and this is one of the things.

20:02.000 --> 20:06.000
So please take a look if you are working on MySQL.

20:06.000 --> 20:10.000
We have some counting of the parameters for the database there.

20:10.000 --> 20:15.000
And it will save you a lot of time if you want to tune up your database.

20:15.000 --> 20:17.000
We have spent the time for you.

20:17.000 --> 20:23.000
And secondly, we have implemented the metadata to Prometheus Creper.

20:23.000 --> 20:29.000
So you can get rid of one, one struggle point in your monitoring environment.

20:29.000 --> 20:38.000
And we are also starting working on the fast Creper for the, for your monitoring, for our monitoring stack.

20:38.000 --> 20:42.000
But it's not done yet because it's quite more complicated than before.

20:42.000 --> 20:46.000
So, but yeah, please take a look.

20:46.000 --> 20:49.000
I will have the links on our GitHub and everything.

20:49.000 --> 21:00.000
It's all open source. So you can just also add some code there if you want.

21:00.000 --> 21:03.000
Yeah, so, so I have started quite early on than I should.

21:03.000 --> 21:05.000
So I have some time now then.

21:05.000 --> 21:11.000
But okay, we can just summarize everything now.

21:11.000 --> 21:18.000
And I hope for some discussion before you guys who done performance testing.

21:18.000 --> 21:22.000
So for me, don't be scared around the performance testing.

21:22.000 --> 21:25.000
He's not like some, some monster.

21:25.000 --> 21:29.000
People are mostly just like creating the monster from it.

21:29.000 --> 21:33.000
But if you don't need that for some legal options or anything, it's fine.

21:33.000 --> 21:35.000
You can play with it. It's funny.

21:35.000 --> 21:39.000
You will gain a lot of knowledge about your, about your product on that.

21:39.000 --> 21:47.000
And especially if you are QE, I mean, a lot of QE folks don't have the

21:47.000 --> 21:49.000
necessary knowledge about the product itself.

21:49.000 --> 21:53.000
And this helps a lot to get through everything because in the end,

21:53.000 --> 21:57.000
you will, you will just go through the code and look for, for the mistakes or

21:57.000 --> 22:03.000
something like that. So that really helps a lot.

22:03.000 --> 22:07.000
So, gather all the metrics you can.

22:07.000 --> 22:13.000
And well, we are also writing our blog and all the repositories will be on the

22:13.000 --> 22:16.000
other side before the two, before the two links.

22:16.000 --> 22:21.000
So I would be happy to hear from you, you know, like repository or organization

22:21.000 --> 22:23.000
before the two links.

22:23.000 --> 22:27.000
And yeah, that's probably it for, for my talk.

22:27.000 --> 22:29.000
As I said, I have started a lot earlier.

22:29.000 --> 22:34.000
So thank you very much for listening to me.

22:34.000 --> 22:44.000
Please do have some questions.

22:44.000 --> 22:48.000
Yeah.

22:48.000 --> 22:52.000
So my question would be, so what kind of experience do you have in your

22:52.000 --> 22:56.000
complex system? And then you see something happens there.

22:56.000 --> 22:59.000
And say, okay, here, here's the latest EP or something like that.

22:59.000 --> 23:04.000
Which experience do you have with, let's say, find the cause of the problem?

23:04.000 --> 23:07.000
Cause, so when something happens randomly, you will see it with wrap and say,

23:07.000 --> 23:10.000
okay, something happens there. And that's maybe annoying, especially when it

23:10.000 --> 23:12.000
happens, happens randomly.

23:12.000 --> 23:15.000
So what kind of strategies you are using then when you know, okay, there is

23:15.000 --> 23:20.000
something to find cause of the problem in the complex chain.

23:20.000 --> 23:24.000
Yeah, so this is, okay.

23:24.000 --> 23:30.000
So, so the question was actually, if there are some, some changes in the

23:30.000 --> 23:35.000
environment, some like latency things or everything, how we can deal with that

23:35.000 --> 23:39.000
and how we can find the causes of the problems.

23:39.000 --> 23:44.000
Yeah, so surely this is the main problem of the whole performance testing

23:44.000 --> 23:50.000
outside of the like isolating and, you know, well, you need the metrics

23:50.000 --> 23:58.000
from everything because then you somehow at least it will help you to, to get

23:58.000 --> 24:03.000
all the things in the right timeline and you can see the need and picks what

24:03.000 --> 24:10.000
could happen. But if it's like something that is really bad, you can find it

24:10.000 --> 24:16.000
usually because it will mostly, it will just disappear in all the logs because

24:16.000 --> 24:20.000
it can be something like if you have the smaller machines, I have to write

24:20.000 --> 24:25.000
once on the, some microchips, you know, it was funny thing that you fill up your

24:25.000 --> 24:30.000
TCP queue. You cannot find that anywhere in the world.

24:30.000 --> 24:36.000
So at that case, you will just repeat the testing, even if it's take long

24:36.000 --> 24:42.000
and you will see if same thing happens or not. I don't have any other like

24:42.000 --> 24:46.000
recommendation for that because this is like really main problem if you are

24:46.000 --> 24:52.000
doing it outside of the ideal environment. This surely will face this, but

24:52.000 --> 24:58.000
mostly it's not happening that often, I would say, because you can have

24:58.000 --> 25:03.000
observation for and tracing for a lot of things and most of the times you can

25:03.000 --> 25:09.000
like colorate those things together. So you exactly know what is wrong,

25:09.000 --> 25:19.000
especially for the network. You can get a lot of network traffic, like, how is

25:19.000 --> 25:25.000
the word? You can see all the traffic and what is going on, especially on one

25:25.000 --> 25:32.000
line. So then you can usually put those graphs together and you know at the time.

25:32.000 --> 25:35.000
If that is okay, answer for you.

25:35.000 --> 25:38.000
Yeah, yeah, yeah.

25:42.000 --> 25:50.000
Thanks for the talk. I was just wondering that how to use the traces,

25:50.000 --> 25:56.000
analyzing the traces, because I've seen that you mentioned metrics and traces.

25:56.000 --> 25:59.000
Sorry, can you speak more louder?

25:59.000 --> 26:01.000
Oh, yeah. Can you hear me now?

26:01.000 --> 26:02.000
Yes.

26:02.000 --> 26:08.000
Yeah, I was wondering how do you use the traces for performance testing,

26:08.000 --> 26:11.000
because when you collect the traces, how do you deal with the sampling of the

26:11.000 --> 26:16.000
traces? And if you miss something because the sampling is bad or you are not

26:16.000 --> 26:22.000
sampling everything, maybe you have to infer something from the metrics and the

26:22.000 --> 26:27.000
traces, I was wondering how do you deal with the traces? And if you use distributed

26:27.000 --> 26:34.000
tracing in a large project like collecting all this kind of stuff.

26:34.000 --> 26:39.000
I'm not sure. I understand the question.

26:39.000 --> 26:44.000
The question actually is, I've seen that you're using collecting the metrics and then

26:44.000 --> 26:47.000
you are analyzing the metrics.

26:47.000 --> 26:48.000
Yeah.

26:48.000 --> 26:50.000
And what about the traces?

26:50.000 --> 26:58.000
Yeah, so, well, the business does not have really that amount of traces that we could

26:58.000 --> 27:05.000
get from it. We have mostly like JMX metrics, you know, from the Java environment.

27:05.000 --> 27:12.000
So that's for us what we analyze. And I'm not sure how can I answer more for the

27:12.000 --> 27:16.000
questions. So I'm sorry. We can discuss it later.

27:16.000 --> 27:31.000
I'm coming to you.

27:31.000 --> 27:38.000
Okay, so my question is about the long running tests. Sometimes the performance

27:38.000 --> 27:44.000
validation is visible only after long run, for example, one week, couple of weeks and

27:44.000 --> 27:50.000
sometimes even more. So how do you address this in your process or how do you recommend

27:50.000 --> 27:52.000
to address this problem?

27:52.000 --> 28:01.000
Yes. So for this, actually colleagues of mine as part of our open source organization,

28:01.000 --> 28:07.000
they are also developing the long running cluster environment, something like that is

28:07.000 --> 28:14.000
like, because, you know, having a long running thing is complicated on itself because you

28:14.000 --> 28:20.000
have to manage it a lot, especially on OpenShift or Kubernetes or these kind of things is like

28:20.000 --> 28:24.000
little problematic before the upgrades and this kind of stuff.

28:24.000 --> 28:30.000
So we have not dealt with that yet, but we are planning that once we are okay, that we

28:30.000 --> 28:37.000
manage that we have everything prepared for like databases and everything, we want to

28:37.000 --> 28:44.000
get the up running on the long running clusters and like regularly doing the performance tests

28:44.000 --> 28:50.000
over, I would say a month or something like that or a week, it usually is enough, even

28:50.000 --> 28:58.000
especially when you put all the numbers to the low ranges for the retention for the memory

28:58.000 --> 29:03.000
and all this kind of stuff. It doesn't take too long to fill everything when you will

29:03.000 --> 29:08.000
start to see the retention and flashes and everything. So yeah, that's our plan, but we

29:08.000 --> 29:15.000
haven't done it yet. So yeah, but if you are interested in that, you should definitely

29:15.000 --> 29:25.000
look in the hub that we have in the repository because it could be useful.

29:25.000 --> 29:32.000
Do you have any tips for running performance tests in the cloud? Because for me that's quite

29:32.000 --> 29:37.000
the opposite of dedicated test runners. But when the software runs finally in the cloud

29:37.000 --> 29:40.000
you should probably also performance test it there.

29:40.000 --> 29:49.000
It's a problem. A big one. We have tried it and it is so inconsistent. The results are

29:49.000 --> 29:56.000
so all over the place. If you run, you have two same clusters, Kubernetes, OpenShifting

29:56.000 --> 30:01.000
doesn't matter actually, and you run the tests on the same clusters at the same time,

30:01.000 --> 30:08.000
clusters are in different zones on AWS, you will get entirely different results because

30:08.000 --> 30:16.000
all the load balancers and these kind of stuff, it really, you know, if you have only

30:16.000 --> 30:21.000
internal communication on the cluster, we found anything from the outside. It could be

30:21.000 --> 30:28.000
actually doable, I think, but if you have any communication during the test that is going

30:28.000 --> 30:33.000
outside the cluster and it has to go through the load balancers and these kind of stuff,

30:33.000 --> 30:41.000
I think that's not doable in any way because it's like you don't know what latency we will

30:41.000 --> 30:49.000
have for this kind of the request and travels. So I think that that would be like really

30:49.000 --> 30:56.000
problematic, but if you could mock up the external communication with just some internal

30:56.000 --> 31:04.000
endpoint, it should be quite okay-ish, I would say, but you will not get like really good

31:04.000 --> 31:10.000
results from that, I think, even if you try more and more. But I think there are some

31:10.000 --> 31:19.000
Kubernetes, some special Kubernetes builds that should be used for these kind of measurements,

31:19.000 --> 31:26.000
but I have never actually tried it, so I cannot recommend it when I try it, when I

31:26.000 --> 31:34.000
try it, but yeah, this is definitely a problem. Okay, so I have more questions?

31:34.000 --> 31:41.000
No, we have a few more minutes for questions. Come on. Otherwise, I'm going to ask you to, you know,

31:41.000 --> 31:45.000
to move your seats. Wait, wait.

31:45.000 --> 31:51.000
You said you would want to have a very small statement developed, so in the milliseconds

31:51.000 --> 31:57.000
ago. Yeah. So doesn't that create problems on their own, something like noisy neighbors and so on?

31:57.000 --> 32:01.000
Yes, it does. It does. Right, but...

32:01.000 --> 32:04.000
How big of a problem that is?

32:04.000 --> 32:11.000
Well, that's the thing. We are really thinking about writing some scraper that is fast enough for this,

32:11.000 --> 32:18.000
and yes, you will probably generate some problems during the way, especially if you would

32:18.000 --> 32:25.000
like to send the metrics directly to the Prometheus every millisecond. You will probably fill up the

32:25.000 --> 32:32.000
network line or the TCP stack or whatever, because it's really fast. So you will...

32:32.000 --> 32:39.000
It will strongly depend on the machine that you are running and if you have space there, if you have a

32:39.000 --> 32:47.000
lot of RAM, you could actually batch all the metrics and send it like one package after the test is done.

32:48.000 --> 32:56.000
But yes, that's actually what we are now fighting with and we are trying to figure out how we are going to

32:56.000 --> 33:09.000
aim for this. But mostly we are thinking that we will do somehow a configurable scraper that will either batch the

33:09.000 --> 33:15.000
request or send them or something like that. I cannot say you because we haven't tried it, actually, what

33:15.000 --> 33:23.000
problems it makes, especially with batching, because I have counted it up and metrics aren't small, actually.

33:23.000 --> 33:31.000
So it will take a lot of place in the memory. So we will have to try it and somehow figure it out.

33:31.000 --> 33:39.000
But before the fast scraper, it will give you really headaches because you will try to find something and

33:39.000 --> 33:46.000
fight something and you will spend 20 times debugging it and then you will find that the scraper hit it the wrong

33:46.000 --> 33:58.000
time every time. So we have to deal with this in some way. But it will be hard and problematic.

34:01.000 --> 34:03.000
I think we have time for one last question.

34:10.000 --> 34:11.000
No one?

34:12.000 --> 34:13.000
Tough crowd.

34:13.000 --> 34:14.000
Thank you very much.

