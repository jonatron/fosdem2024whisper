WEBVTT

00:00.000 --> 00:05.000
So, hello.

00:05.000 --> 00:07.000
Let's start.

00:07.000 --> 00:13.000
I think 50 minutes and all, so I will hurry up here.

00:13.000 --> 00:19.000
And the previous slides and presentations, we saw the overall picture somehow, the crit thing.

00:19.000 --> 00:23.000
And the last slide, we dig into one system somehow.

00:23.000 --> 00:28.000
And in this slide, we also want to dig a little bit more in the details,

00:28.000 --> 00:31.000
how to analyze the power consumption.

00:31.000 --> 00:36.000
What we saw also in the previous presentation, there was an...

00:36.000 --> 00:39.000
Sure, sure, sorry.

00:39.000 --> 00:45.000
What we saw in the last presentation was that we saw the power consumption of one system,

00:45.000 --> 00:54.000
a little bit similar to power supply that we saw this task consumes and what's whatever.

00:54.000 --> 01:04.000
But the question what you often have is after this data, how you can optimize your load for your server,

01:04.000 --> 01:06.000
for your embedded product somehow.

01:06.000 --> 01:13.000
What are the causes why the application runs too often, the system runs too often,

01:13.000 --> 01:19.000
cannot go into deep states, peace states and such things, right?

01:19.000 --> 01:28.000
At the end, it's the hardware that consumes the power and you can save power if you put things into deep sleep states

01:28.000 --> 01:31.000
or consumes the frequency somehow.

01:31.000 --> 01:35.000
And this is really important to save energy.

01:35.000 --> 01:42.000
And what we did in the past was writing scripts to optimize your workload

01:42.000 --> 01:47.000
and get things what are the causes that an application runs too often, right?

01:47.000 --> 01:49.000
It runs often, cannot go into deep sleep states.

01:49.000 --> 01:52.000
This is important to do the power optimization.

01:52.000 --> 02:00.000
And what I provide in the next couple of slides here is an application that helps you to optimize your workload

02:00.000 --> 02:05.000
and makes this visible somehow.

02:05.000 --> 02:10.000
So what we are talking about is a perf script and extension to perf.

02:10.000 --> 02:14.000
So it's, if it is mainline, it's not yet mainline.

02:14.000 --> 02:21.000
I will send this script to Analo in the mailing list and hopefully it gets merged quickly somehow.

02:21.000 --> 02:25.000
But when it's merged then it's really usable, easy usable.

02:25.000 --> 02:29.000
It's just an up-get install and everything works out.

02:29.000 --> 02:32.000
And also for Yachter and Buildroot, it's really easy to use these things after.

02:32.000 --> 02:37.000
It's also important that it can be used in embedded systems and everywhere.

02:37.000 --> 02:39.000
How does it work?

02:39.000 --> 02:47.000
It's just a record call where you record your workload with the workload separator, like every time.

02:47.000 --> 02:53.000
And here I record for 60 for one minute, workload on all the CPUs.

02:53.000 --> 02:56.000
And then you record everything one minute fine.

02:56.000 --> 03:02.000
And then you start with the report, the power analyzer, and it's have different modes.

03:02.000 --> 03:07.000
Because I have just 10 minutes, I show one mode here,

03:07.000 --> 03:12.000
but there are different modes for different optimization, analysis, and things.

03:12.000 --> 03:14.000
So what are the modes?

03:14.000 --> 03:18.000
There are several modes that can be activated and used.

03:18.000 --> 03:24.000
And you just activate or use the mode you want to focus and dig into the details, right?

03:24.000 --> 03:26.000
This is how things work.

03:26.000 --> 03:32.000
And what's also important, every mode has different trace points in the kernel.

03:32.000 --> 03:39.000
So usually you record only the trace points you require for the particular analyzer.

03:39.000 --> 03:46.000
Because if you record everything, every trace point, you get a lot of huge data and things.

03:46.000 --> 03:49.000
So normally you limit the data.

03:49.000 --> 03:51.000
How does it work?

03:51.000 --> 03:53.000
So there's the per script.

03:53.000 --> 03:59.000
As always, you can write a recorded data, as we saw for one minute here.

03:59.000 --> 04:03.000
And it just records all the trace points that are required.

04:03.000 --> 04:09.000
But on the other hand, you can also record the data that are required for your analysis.

04:09.000 --> 04:12.000
This is documented, what the trace points are required.

04:12.000 --> 04:14.000
Then you have the data.

04:14.000 --> 04:20.000
And then you start the script, the reports, and here outputs all the analyzers for this.

04:20.000 --> 04:22.000
You start it here with a timer.

04:22.000 --> 04:24.000
So what are the timer events somehow.

04:24.000 --> 04:29.000
And then because there's a lot of data coming out of this, you usually can use this data already

04:29.000 --> 04:35.000
and see, here's something that's not working well, too much timer interaction, for example.

04:35.000 --> 04:44.000
But what is it also can do is some post-processing and to create data graphs somehow or filter things afterwards

04:44.000 --> 04:47.000
because it's a lot of data.

04:47.000 --> 04:53.000
And here, just a showcase, this is one image that's created.

04:53.000 --> 04:56.000
You see the time and you see a workload.

04:56.000 --> 04:58.000
It's a logarithmic scale here.

04:58.000 --> 05:00.000
How much time, timers are working.

05:00.000 --> 05:09.000
Timers are one course that triggers from a deep C state to an active state to a zero state somehow.

05:09.000 --> 05:11.000
Timers are not that good.

05:11.000 --> 05:16.000
Often you see, if you begin analyzing things on your desktop CZ, you see here,

05:16.000 --> 05:19.000
I think this is the kitty, my terminal I use here.

05:19.000 --> 05:22.000
It has just wake-ups all the time.

05:22.000 --> 05:24.000
Why are there wake-ups here?

05:24.000 --> 05:28.000
And then you see often some buggy applications, clip-out things.

05:28.000 --> 05:34.000
They are constantly triggering your system and this prevents to going into a deep C state.

05:34.000 --> 05:37.000
This is the causes that prevents this.

05:37.000 --> 05:39.000
So it's really important.

05:39.000 --> 05:45.000
And here you see a workload I started and you see all the timers that are correlated with starting a workload.

05:45.000 --> 05:50.000
Here you see a lot of kernel timers and then you can start optimizing things.

05:50.000 --> 05:55.000
This is just a focus for the timer events, but there are a lot of other events as well.

05:55.000 --> 06:01.000
This is other sub-sequence analyzes also just for the timer events.

06:01.000 --> 06:07.000
You see here for a tick-less system, normally if there is no load,

06:07.000 --> 06:13.000
kernel can really go in a deep sleep state.

06:13.000 --> 06:17.000
And then it shuts down the timer tick altogether.

06:17.000 --> 06:20.000
But does it really stop the timer tick?

06:20.000 --> 06:25.000
You will see it here in these images and you can analyze things and optimize things.

06:25.000 --> 06:28.000
What are the kernel timers that trigger your systems?

06:28.000 --> 06:33.000
If you look at the graphs a little bit, the resolution is not that good,

06:33.000 --> 06:37.000
but you see that there are timer ticks all the time,

06:37.000 --> 06:42.000
and the network interrupts, timers are working here and you can optimize this.

06:42.000 --> 06:50.000
If you see this and you know what's happened, what we see here in this graph is the timers that are working for each particular task.

06:50.000 --> 06:52.000
So you can optimize for your task as well.

06:52.000 --> 06:54.000
How many timers are there?

06:54.000 --> 07:01.000
I often see in the production environment that the timer has done all the time somehow and not correlated.

07:01.000 --> 07:07.000
What you can also do there are system calls for the granularity that the timer can optimize things.

07:07.000 --> 07:13.000
For example, the kernel which the introduction of the HR timers, the resolution timers, you can align timers

07:13.000 --> 07:19.000
so that timers are not really spaced there and they're exactly triggered at a particular moment in time,

07:19.000 --> 07:22.000
which is a simple system knob.

07:22.000 --> 07:26.000
You can also say, oh no, it's not so important that the timer is triggered at this time

07:26.000 --> 07:34.000
so that the kernel aligns timers at a particular time and allows a deeper sleep state again, something.

07:34.000 --> 07:41.000
This knowledge can be combined with the knowledge of this, what you see here, for example.

07:41.000 --> 07:43.000
Where are the timers?

07:43.000 --> 07:46.000
Right, CPU 0 is somehow special.

07:46.000 --> 07:47.000
There are the timers.

07:47.000 --> 07:57.000
Can you move, for example, tasks to CPU 1 so that this other CPU cores can go in a deeper sleep state, for example, right?

07:57.000 --> 08:02.000
All this is important to do an optimization there.

08:02.000 --> 08:04.000
There are some general options.

08:04.000 --> 08:07.000
Some others are not always required.

08:07.000 --> 08:10.000
This can be turned on with this particular flag.

08:10.000 --> 08:16.000
There's CPU, often you want analysis on a particular CPU so you can limit the data.

08:16.000 --> 08:25.000
And there's a file out option so if you want to do a post-processing, as we saw in the images, somehow the data is not put it on the standard out,

08:25.000 --> 08:28.000
so it's put it on the file and you can use this there.

08:28.000 --> 08:37.000
And the data is also written in a day and sanitized that you can trust through use partners here to read the CVS data.

08:38.000 --> 08:41.000
And for the post-processing, it's really easy.

08:41.000 --> 08:45.000
But there are multiple modules there provided.

08:45.000 --> 08:51.000
This is just a sneak peek on the timer module, but there are a lot of other modules as well.

08:51.000 --> 08:54.000
You can use them later on.

08:54.000 --> 09:00.000
But to the time limit, I just highlighted this timer module.

09:00.000 --> 09:06.000
But one last sneak peek here, for example, is the governor.

09:06.000 --> 09:16.000
The governor is the component within the kernel to do the processing and commanding of disease deep-stakes.

09:16.000 --> 09:18.000
This is the governor.

09:18.000 --> 09:20.000
You can select a different governor.

09:20.000 --> 09:22.000
It's normally the menu governor.

09:22.000 --> 09:24.000
There are other governors as well.

09:24.000 --> 09:31.000
And here what you see, how often is which C state is commanded here?

09:31.000 --> 09:34.000
And what is also analyzed is, was this good or not?

09:34.000 --> 09:38.000
Because the kernel doing a guess working, right?

09:38.000 --> 09:44.000
So here the things are the next time in 10 milliseconds, there's a workload because the timer will trigger.

09:44.000 --> 09:49.000
So it puts a processor in a particular C state.

09:49.000 --> 09:53.000
But was this the right decision or sleep is too narrow, too shallow?

09:53.000 --> 09:55.000
And so this is also important somehow.

09:55.000 --> 09:59.000
And here you can debug the governor.

09:59.000 --> 10:03.000
A student of mine also discovered a bug for the AMD stuff.

10:03.000 --> 10:06.000
It's for one particular C1 state.

10:06.000 --> 10:10.000
It's switched all the time to the wrong state.

10:10.000 --> 10:15.000
But I think this will be released in the next couple of weeks somehow.

10:15.000 --> 10:17.000
So it's really also important for you.

10:17.000 --> 10:22.000
If you see, does the governor does the right job here?

10:22.000 --> 10:29.000
This is visible with another analysis, but there are multiple other post processing steps.

10:29.000 --> 10:32.000
And yeah, that's all.

10:32.000 --> 10:38.000
I hope this will be integrated in the mainline next couple of weeks.

10:38.000 --> 10:45.000
But if you want, you can use this kernel tree and this particular branch to use this.

10:45.000 --> 10:49.000
It's just a perf script, really easy also to use out of the tree.

10:49.000 --> 10:53.000
And this post processing scripts cannot be shipped with the kernel.

10:53.000 --> 10:55.000
That's not how the kernel somehow works.

10:55.000 --> 11:00.000
This Python scripts and there will be always available here based on this.

11:00.000 --> 11:03.000
And at the end, good documented, hopefully somehow.

11:03.000 --> 11:07.000
So yeah, that's all questions.

11:07.000 --> 11:08.000
Yeah, perfect.

11:08.000 --> 11:14.000
Questions.

11:14.000 --> 11:21.000
I'm always getting a question.

11:21.000 --> 11:24.000
Process of coverage, just x86.

11:24.000 --> 11:26.000
What's the coverage of you got?

11:26.000 --> 11:29.000
I mean, now look, I've got an M1 Apple thing.

11:29.000 --> 11:33.000
Would I be able to run it off there if I run Linux on that hardware?

11:33.000 --> 11:40.000
Yeah, this script will work on ARM x86 for Intel and AMD.

11:40.000 --> 11:48.000
There are differences in the P state tracking because P state tracking is the introduction of Skylake and HWP with hardware tracing.

11:48.000 --> 11:52.000
So it's will be not visible, but it will be visible on ARM CPUs.

11:52.000 --> 11:59.000
For example, some as a sample work, some will not work, but it's just Linux and all the major.

11:59.000 --> 12:04.000
And some are more software, the analysts of scheduling events.

12:04.000 --> 12:09.000
Somehow it will always run, but more hardware like analysis will not work somehow.

12:09.000 --> 12:12.000
But yeah.

12:12.000 --> 12:14.000
Just a follow up for previous question.

12:14.000 --> 12:20.000
Will it work for like, Graviton, all this kind of cloud proprietary processors?

12:20.000 --> 12:21.000
Yeah.

12:21.000 --> 12:24.000
It would generally run there.

12:24.000 --> 12:29.000
If it at least Linux ARM and the processor and it will just be the same.

12:29.000 --> 12:31.000
So no difference there.

12:31.000 --> 12:33.000
Yeah.

12:33.000 --> 12:39.000
So I think it's going to be a good idea to run it on the same hardware.

12:39.000 --> 12:40.000
Yeah.

12:40.000 --> 12:41.000
And there.

12:41.000 --> 12:46.000
If it at least Linux ARM and the processor and it will just be the same.

12:46.000 --> 12:51.000
So no difference there.

12:51.000 --> 12:53.000
Another question.

12:53.000 --> 13:00.000
If not later on we can install a script at your PC and test it.

13:00.000 --> 13:04.000
Hi Aaron.

13:04.000 --> 13:06.000
That's just a follow up on the previous question.

13:06.000 --> 13:11.000
There's actually an extra library, LibOpen CSD, which gives you a whole lot of extra stuff on most ARM cores,

13:11.000 --> 13:21.000
but not necessarily apples and Amazon's ARM cores, but any that actually come from standard designs.

13:21.000 --> 13:24.000
So a Turing design here.

13:24.000 --> 13:27.000
One goal was that it runs everywhere somehow, right?

13:27.000 --> 13:29.000
It must be general.

13:29.000 --> 13:35.000
And I don't, we don't skip going into the EPPF world.

13:35.000 --> 13:39.000
So there are advantages to do things in the kernel to aggregation in the kernel.

13:39.000 --> 13:45.000
So but this has sometimes problems with on specific ARM and PSOX and embedded products.

13:45.000 --> 13:48.000
So the design was really that runs everywhere.

13:48.000 --> 13:51.000
It's easy to use and generally available somehow.

13:51.000 --> 13:58.000
Somehow EPPF working with EPPF things to in the kernel and process unwanted data there out has some advantages, right?

13:58.000 --> 14:01.000
But you need a tool train then on an embedded product.

14:01.000 --> 14:04.000
So it's not that great somehow.

14:04.000 --> 14:11.000
And this everything I told you was somehow the idea on the design somehow or extra library.

14:11.000 --> 14:15.000
Keep it a bit of minimal stuff which works everywhere somehow.

14:15.000 --> 14:20.000
If you want to do more and often you want to do more if you analyze your particular task,

14:20.000 --> 14:25.000
how is the scheduling behavior you need more and you need more custom scripting as well somehow.

14:25.000 --> 14:28.000
But this is not here. I think it's a lot of data already there.

14:28.000 --> 14:30.000
Easy available somehow.

14:30.000 --> 14:36.000
But if you want to do more, you need more scripting and things like that and libraries you want to use.

14:36.000 --> 14:38.000
Sure. It's a compromise.

14:41.000 --> 14:43.000
Maybe a question for me.

14:43.000 --> 14:48.000
Can you give us a few insights about the community?

14:48.000 --> 14:53.000
How many developers, how many people contribute?

14:54.000 --> 14:57.000
Currently I'm the main developer.

15:01.000 --> 15:06.000
But at the end it's just in the Python script so it's not really the rocket science.

15:06.000 --> 15:11.000
And there are students also working on this, help things and looking at the details.

15:11.000 --> 15:14.000
But yeah, it's not that magic somehow.

15:14.000 --> 15:19.000
It's just keeping things putting together and make them easy usable.

15:20.000 --> 15:29.000
The trace points and Steven Roslatt and all the things, the infrastructure that the kernel provides are the main drivers.

15:29.000 --> 15:31.000
That this is possible, right?

15:31.000 --> 15:33.000
So just in script.

15:36.000 --> 15:38.000
Thank you so much.

