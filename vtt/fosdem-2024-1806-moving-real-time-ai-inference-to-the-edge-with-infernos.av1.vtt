WEBVTT

00:00.000 --> 00:10.720
Hi everybody, thanks for the wait, sorry, a bit of technical challenges, but now we can,

00:10.720 --> 00:16.240
this is the last talk related to AI, to AI staff, and I think it's also a nice continuation

00:16.240 --> 00:21.960
from Saou's first presentation, so we'll have Maxime coming from all the way from Vancouver,

00:21.960 --> 00:28.560
I think probably the farthest participant here today, so please go ahead, thank you.

00:28.560 --> 00:36.280
Okay, thank you, thank you Lorenzo, thanks everyone for attending my presentation.

00:36.280 --> 00:45.880
So let's get started, today I'm going to talk about a little bit some of our newest work on

00:46.840 --> 00:52.560
the eyesight of things, we've been a little bit about myself, I'm born and raised in Ukraine,

00:52.560 --> 00:59.280
I have masters in physics and radio physics from Key State University, I live in Vancouver for about

00:59.280 --> 01:11.280
20 years now, father of three, and I've been involved in CIPA in various forms and projects

01:11.280 --> 01:19.280
and whatnot since about 2003, so basically since that first day I give a little bit of

01:19.280 --> 01:28.440
background of what me and open source, today I discovered free BSD in my university at some

01:28.440 --> 01:37.160
like second year, got very curious, started exploring and eventually submitted patches

01:37.160 --> 01:45.280
and all that, got accepted as developer, and then found CIPA Express router which is kind of

01:45.280 --> 01:55.200
later became came earlier in OpenCIP, added some modules, we use it extensively, I also created

01:55.240 --> 02:05.200
Pproxy, which people who use CIPA probably know about, and yeah, I get busy with various

02:05.200 --> 02:14.720
open source projects to this day. Speaking of machine learning, I started in about 2010,

02:15.240 --> 02:27.920
read this really nice book, it's actually free book, it's at 99 sort of age, but surprisingly got

02:27.920 --> 02:35.160
pretty good basic introduction into neural networks and all that, I already got a little bit

02:35.200 --> 02:45.880
curious about that, and then as time went on, it became more available, I trained a little toy

02:45.880 --> 02:53.760
model just trying to figure out if I can detect DTMF from 729 encode frames essentially, because

02:53.760 --> 03:03.640
729 is just a bunch of floating point of coefficient, so you can feed it to the model and get sort of

03:04.000 --> 03:15.960
DTMF detection that way, it was just kind of weekend type of fun project. I also got installed

03:15.960 --> 03:27.400
a little AI powered ADAS called Coma 2, and it's open source project also, the user AI model to

03:27.480 --> 03:34.960
drive your car, you can really like install it yourself, you can hack it, and it's pretty nice,

03:34.960 --> 03:43.280
a good community, I participated in some events there. Also played with DeepFake lab, which kind of

03:43.280 --> 03:51.160
DeepFake framework, which allows you to work faces, also involves some training, so it was curious,

03:51.960 --> 04:00.720
and also lately I've been in San Diego, we've been doing like building a little from scratch,

04:00.720 --> 04:08.400
we've been building model which will drive a little robot across the office, also was quite fun,

04:08.400 --> 04:17.200
lately I've been also playing with Mu Zero support, first for the game of Digger, which I maintain,

04:18.200 --> 04:26.840
Linux port, it was pretty fun to sell highly interested, this is just the device that drives

04:26.840 --> 04:35.640
my car, and it runs open source AI model inside that little chip. So anyway, back to the main point

04:35.640 --> 04:44.000
of this talk, we're basically looking at that from our perspective, which is trying to build a

04:44.000 --> 04:52.640
system which is, we basically have a lot of customers who route calls with us, so we try to

04:52.640 --> 04:59.320
build a model that can be scaled out for provider level, not a model but software which can run

04:59.320 --> 05:07.360
those models. So basically the idea behind this project is that we try to figure out how to scale

05:07.400 --> 05:17.320
those models on reasonably priced hardware, so as the problem with the area right now,

05:17.320 --> 05:24.040
we didn't kind of vacuum tube error, as I call it, machine learning, because we have very expensive

05:24.040 --> 05:35.000
hardware for, you know, to run something mixed trial, you need, you know, like 64 gigabytes of

05:35.040 --> 05:42.160
GPU, and that is very easy, you need several of them or you need a very expensive one, but

05:42.160 --> 05:51.680
eventually all this stuff will get more affordable, so we're trying to kind of work towards that

05:51.680 --> 06:01.680
goal. So right now there are like two major frameworks which people use, which torsion

06:01.680 --> 06:10.360
and tether flow, and they are also pretty heavy as well, it's like hundreds of megabytes,

06:10.360 --> 06:17.560
if not gigabytes, to get stuff running, but it looks like at least from my perspective,

06:17.560 --> 06:24.880
in a few years we'll see some changes, already, see people working on alternative frameworks

06:25.560 --> 06:34.000
that are expected to be more lightweight and more flexible in terms of environment,

06:34.000 --> 06:44.720
because Python is not very easy to scale and integrate into something like CAPIs, although

06:44.720 --> 06:52.880
doable, but anyway. So we started earlier this year with the project, and the original idea was

06:52.880 --> 07:00.000
to just, for starters, we tried to implement text-to-speech, we already took our SIP stack,

07:00.000 --> 07:08.680
which is conveniently Python based, so that was pretty easy, pretty much like 20 lines of code

07:08.680 --> 07:18.560
to get SIP endpoint implemented RTP generation thread and tried to run those models. Essentially

07:18.560 --> 07:32.160
my, yeah, so basically we started with this guy, and it's like four gigabytes, cart, Nvidia 1050,

07:32.160 --> 07:46.480
and I was able to run like one channel essentially on it, of text-to-speech, and then obviously

07:46.480 --> 07:53.760
the next thing was like how to scale it, oh no, hold on, yeah, a little bit into how this works,

07:53.760 --> 08:02.560
essentially. So text-to-speech is, at least with transformers, you basically take your text,

08:02.560 --> 08:11.440
you send it to your model, and then it runs multiple iterations, so you basically have all your stuff

08:12.400 --> 08:19.360
through the model, and then it spills out something like male spectrum, which you put through

08:19.360 --> 08:31.360
Valkoder, and get the audio out. And then the first problem we run out into is that it basically

08:32.240 --> 08:44.720
required this run for the whole duration of the audio, so essentially on my small GPU it would take

08:44.720 --> 08:54.320
quite a lot of time to actually produce audio. So I had to modify it's a sph-t5 model, it's like

08:54.320 --> 09:00.720
not the latest, but one of the pretty good ones from Microsoft a few years ago, they released it,

09:01.440 --> 09:08.480
so I had it to modify it. I mean I re-wrote the model itself, not the model itself, but the pipeline,

09:08.480 --> 09:18.880
so instead of processing the whole audio in one go, I made it so it spits out those smaller chunks.

09:19.520 --> 09:25.360
The unfortunate problem came with that is it started like clicking, because Valkoder probably

09:25.360 --> 09:31.280
was not trained for this mode, so I tried to retrain the Valkoder, it did not go well, very well,

09:31.280 --> 09:38.880
so it did not produce a good result, so I had to build a little kind of post Valkoder scene, which

09:40.560 --> 09:48.160
smooths out essentially and fixes those clicks. And now it sounds pretty well, I will maybe play

09:48.160 --> 10:00.320
some examples when I get finished with the stuff. And then how I tried to scale it, so I got

10:01.360 --> 10:08.640
kind of normal size GPU, I would say, so it's like 16 gigabytes card, and I expected to get like

10:08.640 --> 10:15.760
maybe 10 times, 20 times performance, just looking at the spec, but to my surprise I only got like

10:15.760 --> 10:24.320
two times more performance, so I can only like with this model, I can only run two real-time threads

10:24.320 --> 10:32.000
of TTS on the bigger card. So I started looking into why this is happening and how to improve,

10:32.000 --> 10:41.440
because theoretically it has much more performance. Turns out that in order to get good performance

10:41.440 --> 10:50.560
out of those models you need to use batch inference, so instead of generating like each

10:50.560 --> 10:58.400
prompt, each audio in each one session you batch your prompts that you need to voice out and

10:59.680 --> 11:08.320
submit it into the model, and then it generates all those streams at the same kind of cost.

11:08.880 --> 11:18.800
Because my main problem with using GPUs is that they are pretty widely computationally,

11:18.800 --> 11:24.640
but they are pretty expensive to send, so it's like you're operating through very slow network

11:26.640 --> 11:36.080
with a very fast device, so you need to load a lot of, well not a lot, but several jobs to it.

11:36.800 --> 11:50.000
And so I considered like several ways how to batch it, so my first idea was that maybe I can just

11:51.520 --> 11:57.440
vary the size of the batch, so I run it in continuously, but then as sessions come and go

11:57.440 --> 12:04.800
I can add or remove them to this running batch. Unfortunately this does not work with the sequence

12:04.880 --> 12:11.120
to sequence model, because internally it kind of clocks itself, so you cannot really add another

12:11.120 --> 12:17.840
session of it, so they should be running all of them at the same time, so essentially you need

12:17.840 --> 12:24.160
to do something like this, so you batch a bunch of sentences and that you need to generate,

12:25.440 --> 12:32.880
pump it up and wait for them, all of them to finish, and then at the time you can collect

12:33.440 --> 12:42.880
a new request, then you batch them up and repeat again, and obviously if you have like

12:43.680 --> 12:49.600
pretty powerful GPU you can probably run a few of them, or if you have several GPUs you can

12:50.720 --> 12:58.240
improve latency by running on multiple of them.

12:59.200 --> 13:07.360
Yeah, so that part kind of works. The next thing I'm right now working on

13:08.640 --> 13:16.720
kind of in the other way, so we need to have something like Whisper to do the other way around,

13:18.400 --> 13:27.520
and that one already supports batching, so basically should be pretty scalable there as well,

13:27.520 --> 13:34.800
so right now I have on that $300 card I can do 50 sessions of text-to-speech

13:35.920 --> 13:42.000
in real time at the same time, so it's pretty good result because this is all running locally,

13:42.720 --> 13:50.480
so I don't use any anything pretty much and I can run it on a reasonably small device.

13:51.200 --> 14:00.640
And yeah, I guess the last thing that I played recently was

14:03.920 --> 14:11.040
there is a framework called Ray. It's basically when you can build a little cluster of

14:12.640 --> 14:19.760
machines with different kind of, well maybe the same GPU, maybe different hardware, and

14:20.960 --> 14:29.760
distribute your training or inference work over them, so that's just me running

14:31.200 --> 14:40.400
and not 20 probably games of digger. It's basically, all of this is a model doing inference,

14:42.320 --> 14:48.480
just looking at the screen doing inference saying where this should go basically, and

14:51.120 --> 15:00.480
kind of training itself to win at some point of time. So yeah, Ray,

15:02.000 --> 15:11.440
yeah this is like a CCH training, kind of improving a little bit, maybe, but anyway,

15:11.440 --> 15:18.880
a good interesting part of it, I figured out how to use that Ray, so it's kind of useful

15:19.840 --> 15:27.840
open source framework that you can kind of scale, use to scale up your AI project, so I'll probably

15:28.560 --> 15:37.920
use some of it to distribute. Yeah, there are some links and I guess I have probably a few minutes

15:37.920 --> 15:56.320
for questions. Yeah, yeah, yeah we can do video technically because it's all about,

16:00.240 --> 16:06.480
you know, as soon as we have like the whole mechanism set up, we can do video as well,

16:07.200 --> 16:15.040
so yeah.

16:28.640 --> 16:36.160
Right now I'm using PyTorch, but oh okay, the question is what kind of model can they run?

16:37.200 --> 16:44.640
Right now I'm running with this existing code, I'm using PyTorch, but I also played with

16:45.760 --> 16:52.960
TinyGrad, so I might use some of it as well because as I said, it's kind of very lightweight,

16:52.960 --> 17:03.840
so the whole goal of the guy who wrote it is to keep like usable framework in like 5,000 lines

17:03.920 --> 17:13.760
of Python code, so it's kind of very interesting from that perspective, but yeah, it's not really

17:13.760 --> 17:25.520
limited, it could use anything, right? I think another question, no?

17:25.920 --> 17:28.080
I have one.

17:31.440 --> 17:39.200
If you have another question, please give a round of applause.

