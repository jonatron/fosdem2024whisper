WEBVTT

00:00.000 --> 00:16.400
Hello everybody, welcome to the TNS Dev Room, if you just came in.

00:16.400 --> 00:22.040
Our next speaker is Stefan, who will be telling us about the NSSEC PSK algorithm roll over

00:22.040 --> 00:27.720
for .NL, which normally isn't a very exciting thing, but I trust Stefan, but I've made

00:27.720 --> 00:32.440
it a boring situation. That's still fun to talk about.

00:32.440 --> 00:40.440
Yeah, thank you Peter. Welcome, my name is Stefan Udink, I work for SIDN, the .NL registry,

00:40.440 --> 00:48.720
and I'm talking about the KSK algorithm roll over we did in July last year for .NL.

00:48.720 --> 00:54.880
So why did we do this? What did we do for preparations for this change? What was the

00:54.880 --> 01:01.480
planning like? How did we execute it? And what did we measure on the internet on our

01:01.480 --> 01:09.160
change? So why would we want to change the algorithm?

01:09.160 --> 01:15.800
Yeah, the algorithm we used before was algorithm eight, and that's an RSA algorithm, and we

01:15.800 --> 01:22.160
wanted to use a SAVER algorithm to keep up with the new standards, because since June

01:22.200 --> 01:33.200
2019, the new recommendation from the RFCs is to use an ECDSA algorithm for the NSSEC

01:33.280 --> 01:41.280
situations, and there's currently enough support in Resolvers to do this. As you can

01:41.400 --> 01:51.400
see in the drafts, both RSA and ECDSA are supported equally for most Resolvers.

01:52.600 --> 02:00.400
And a plus side is also that the NSSEC answers we are giving are smaller than the RSA answers,

02:00.400 --> 02:06.920
which gives us less impact when we are hit by reflections attacks. So it's better for

02:06.920 --> 02:17.920
the internet. So in way two, the algorithm roll over, we already replaced our HSMs.

02:18.120 --> 02:27.120
We used for the signing of the zone with new HSMs from Talis, which could do 20,000 signatures

02:27.120 --> 02:36.120
per second, which was a big increase from our previous HSMs. And we started with doing

02:36.240 --> 02:42.960
a test run on our test environment without any changes to see how does this work, how

02:43.040 --> 02:50.040
much time does it take, because there are a lot of things you can change which would

02:50.160 --> 02:57.160
change the time used for some steps in the roll over process. And a normal run took about

03:05.240 --> 03:12.240
three weeks without any changes. To be able to do it efficiently, we also made a test

03:13.240 --> 03:19.720
lab policy for OpenDNSSEC, which rolled very fast to be able to see what changes were done

03:19.720 --> 03:26.720
and to be able to create some scripts to follow everything that is done in the environment.

03:28.120 --> 03:35.120
And we also used the local DNS viz installation to see if a Resolver for our setup, it was

03:36.120 --> 03:43.120
inbound, could indeed resolve the new situation. And for that, we also created a fake root.

03:46.560 --> 03:53.560
So we could play root operator to change everything and we could validate everything to see if

03:54.720 --> 04:01.720
everything worked without any issue. That went quite well. And then we went to our acceptance

04:01.720 --> 04:08.720
environment in which we used a daily copy of the public NL zone and that has 6.4 million

04:12.640 --> 04:18.200
records or at least two main names in it, much more records. And then we had a memory

04:18.200 --> 04:25.200
issue. We used our old 128 gigabytes of memory, no swap usage, but still the system holded

04:26.080 --> 04:33.080
on something. And after we added swap to the system, it ran again, so it continued. It

04:36.240 --> 04:43.240
was not broken. Everything went where it left off. It was strange, but it helped us.

04:45.040 --> 04:52.040
So we could prevent this issue in production. Another thing was that normally we generate

04:52.960 --> 04:59.960
a full zone every half an hour and in a normal run, it took about 24 minutes to generate

05:02.000 --> 05:09.000
the zone, sign it and publish it, including validation. After adding the ECDSA keys, we

05:13.480 --> 05:20.480
did a run and then it took 45 minutes. That's not what we wanted because if you want to

05:21.040 --> 05:26.800
publish every half an hour, you cannot take 45 minutes to publish something. So we had

05:26.800 --> 05:33.800
to find a way to make it less than 30 minutes to do both RSA and ECDSA. And we saw that

05:36.240 --> 05:43.240
mainly the validation part costed a lot more time because ECDSA is harder on the validation

05:43.520 --> 05:50.520
part than on the generation part. And we made some things in parallel. So we compiled the

05:52.880 --> 05:59.880
zone with bind to raw format. We validated with valid NS and all those things we did

06:01.480 --> 06:08.360
in parallel and we added parallelization to valid NS. At least it was already available,

06:08.360 --> 06:15.360
but we used the switch to do that as well. So we are using all cores on our systems to

06:16.000 --> 06:23.000
do the validation and then we got to about 27 minutes of generation. So that's under

06:25.680 --> 06:32.680
30 minutes. Very good job for us. And so we were able to continue with the new zone generation.

06:33.120 --> 06:40.120
So how do we plan this thing? We were in June and we know that it took some time. We saw

06:41.800 --> 06:47.800
that we might have a ZSCA roll over. We didn't want to do a June ZSCA roll over because

06:48.000 --> 06:53.000
Dan Dezon would increase even more because of extra signatures. So we had to plan it

06:53.000 --> 07:00.000
and we had also some data that we could use to do the validation. So we had to do a lot

07:02.800 --> 07:07.080
of things. And people in the organization that had to approve at IANA that we are going

07:07.080 --> 07:14.080
to change the yes in the route. So we expected that the IANA change would take three days

07:14.680 --> 07:21.680
and so we came up with this plan. And with all holidays for people, et cetera, we were

07:24.680 --> 07:31.680
able to do this plan. And as you can see, we have some asterisks next to some dates.

07:31.680 --> 07:38.680
And that's because these are dependent on the IANA change. And if the IANA would take

07:39.080 --> 07:46.080
more time than we expected, then those dates would change. And this is something we couldn't

07:48.800 --> 07:54.800
predict. But even though we thought three days should be normal and should be okay.

07:55.080 --> 08:02.080
And luckily for us, we did a blog post about this change and we were telling the people

08:02.600 --> 08:07.880
we are going to do this change. So if something breaks, you know we are doing this and you

08:07.880 --> 08:13.840
have these dates to see if everything is going according to plan and we will update this

08:13.840 --> 08:20.120
if there's some issue or the dates will change. But it was all good and we planned it very

08:20.160 --> 08:27.160
good because all those dates mentioned here were the dates that were used. So we did it

08:29.200 --> 08:36.200
according to plan. So on executing a plan, it's good to have written commands just to

08:38.080 --> 08:42.760
copy and paste them when you need them. You only have to check, yes, I'm doing this on

08:42.760 --> 08:49.080
the correct system. Yes, it's all written correctly, but you don't have to think about

08:49.120 --> 08:56.120
it anymore. So during the execution, we did continuous checking with the script we wrote

08:58.800 --> 09:05.800
and we did some DNS viz runs on the public DNS viz site to show the people that we are

09:06.120 --> 09:13.120
changing and we have some records that we can show. I will show the DNS viz pictures

09:17.360 --> 09:24.360
lighter. As I mentioned before, there would be an increase in file size for the zone and

09:25.480 --> 09:32.480
before it was 4.5 gig in size during 4.6 and afterwards with smaller signatures, we only

09:32.560 --> 09:39.560
had 2.3.7 gigs and that's very nice. Of course, we have a go no go moment because if we would

09:43.160 --> 09:50.160
have double signatures, we can still go back without any disruptions, et cetera, and if

09:51.680 --> 09:58.680
we would go forward, then we wouldn't be able to go back as easily. So we have to do a

10:03.160 --> 10:10.160
bit of a check and that went well. So some pictures of the algorithm 8 situation. During

10:12.000 --> 10:19.000
the policy change where you see an addition of the EC, we added the algorithm 13 to the

10:19.720 --> 10:26.720
root and removed the algorithm 8 from the root and afterwards we stopped using algorithm

10:27.120 --> 10:34.120
8 and then this is a new situation with only ECDSA. During all this time, we also did some

10:35.480 --> 10:41.840
measurements and a colleague of mine, Moritz MÃ¼ller, did most of the measurements. He

10:41.840 --> 10:48.840
wrote a Rolo from Moon Mon quite a few years back and did it again. We messaged some items

10:49.560 --> 10:56.560
mentioned on the slide. I want to mention that we only messaged two root servers because

10:58.480 --> 11:05.480
all root servers should say the same answers but we didn't want to measure it all 13.

11:08.880 --> 11:14.440
What might be interesting is that you see a lot of numbers in this graph and that was

11:14.720 --> 11:21.720
a bug in the Rolo from Moon software. You also might notice that there are multiple lines

11:22.800 --> 11:29.880
at the top and at the bottom and that's something like this. It's a measurement issue that was

11:29.880 --> 11:36.880
caused by using of a small buffer size and still trying to get key IDs from the answer.

11:37.880 --> 11:44.880
That's why you see a lot of changes and we saw what we were seeing. This is not correct.

11:46.040 --> 11:51.600
What's happening? Because if I do manual checking, everything is fine. What's happening

11:51.600 --> 11:58.600
here? Finally, we were able to find the issue and we were able to fix it. Another interesting

11:59.600 --> 12:06.600
thing is that during the change, we looked at the response sizes we sent and in this table

12:08.280 --> 12:15.280
it's only ns1.dns.nl and other systems have similar, not the same answers because the

12:19.800 --> 12:25.720
sizes might differ based on the implementation of the nameserver that's used because of the

12:25.720 --> 12:32.720
protocol compacting. Another interesting thing here is that the nx domain and dns keys are

12:33.600 --> 12:40.600
increasing during the rover and the ns set is not increasing. It's less and that's because

12:43.280 --> 12:50.280
the R6 and the set are in the additional section and during the rollover the section gets increased

12:50.280 --> 12:57.280
a lot more so only the R set for ns1.dns.nl is in the answer and the R6 but not for all

13:05.600 --> 13:12.600
the nameservers that are in our zone. If we look at traffic, normally we have about

13:12.600 --> 13:19.600
one percent of TCP traffic and during the rollover we have about five percent TCP traffic

13:21.080 --> 13:28.080
and after the rollover it's back to normal again. Here you see a graph with a logarithmic

13:28.080 --> 13:35.080
y-axis and you see that TCP is increasing a lot. It's about eight times more TCP traffic

13:36.080 --> 13:42.080
and especially there is no different state in the internet. It might change and so here

13:42.080 --> 13:49.080
you see that the KSK is back in the logistic direction and it's not going to change. So

13:58.080 --> 14:05.080
we have a level again. So in global we have no measured impact at all as far as we know.

14:09.840 --> 14:16.840
I don't know of any trust issues people had or something and you can see on the left picture

14:16.840 --> 14:23.840
that the adding of the ECDSA key and afterwards the removal of the RSA key and the right picture

14:34.280 --> 14:41.280
is the trust chain that is constant for the resolvers and that's my talk already. Are

14:41.280 --> 14:48.280
there any questions? I've got two questions. The first one on slide 17. You mentioned that

14:48.280 --> 14:55.280
during the rollover the NS size becomes smaller. Yes. I will ask you a question, complete question.

14:55.280 --> 15:02.280
So you said the NS set is getting smaller, yes and the question is? The question is there

15:03.880 --> 15:10.880
is no difference between the two. So I think that's a good question. I think that's a good

15:10.880 --> 15:17.880
question. I think that's a good question. I think that's a good question. I think that's

15:18.560 --> 15:25.560
a good one. So you said the NS set is getting smaller, yes and the question is? The question

15:25.560 --> 15:32.560
is there is an RFC out there that says glue is mandatory. If the size of the response

15:33.680 --> 15:40.440
is getting smaller because you're not including glue you have to set the TC there. Did you

15:40.440 --> 15:47.440
measure for that? So I'll repeat the question. There is an RFC that glue is not made, that

15:47.880 --> 15:51.880
blue is mandatory and did we measure some things about this?

15:51.880 --> 15:58.680
What I know about this situation is we looked at the dns-vis information that we got and

15:58.680 --> 16:05.760
for the measurement for ns1.dns.nl the glue is available but only for that name.

16:05.760 --> 16:12.320
So not the glue for the other nameset records.

16:12.320 --> 16:23.120
And I don't know if we looked at if the TC flag has been set and if there has been acted on.

16:23.120 --> 16:24.680
So and the second question?

16:24.680 --> 16:32.840
Yeah, my second question is I noticed you switched to talus.

16:32.840 --> 16:38.680
With regards to support from the talus company, did you test that if you got proper support,

16:38.680 --> 16:40.680
if you needed it?

16:40.680 --> 16:42.680
I will repeat the question.

16:42.680 --> 16:51.680
He said we switched to talus and did we test the support we had with talus before doing this transition?

16:51.680 --> 17:05.680
No, we did not test the support beforehand and technically we did not switch to talus as in we used to have lunas which was taken over by talus.

17:05.680 --> 17:13.680
So we continued with the same lunas, hsm products as we before.

17:13.680 --> 17:31.680
We had contact with talus before we switched the hsm's but we did not before the rollover try again to contact them to see how support would handle questions from us.

17:31.680 --> 17:34.680
Which might be a very good idea as well.

17:34.680 --> 17:36.680
Thank you for that.

17:36.680 --> 17:38.680
I'm asking for a friend.

17:38.680 --> 17:40.680
Yes.

17:40.680 --> 17:42.680
Relate question to that.

17:42.680 --> 17:50.680
The tank tank we had in the beginning, did you have any rollback plans in case something went bad?

17:50.680 --> 17:53.680
The question is do we have any rollback plans?

17:53.680 --> 17:59.680
I mentioned we had a go no go to see if everything is okay, we go forward.

17:59.680 --> 18:03.680
If everything is starting to fall we go backwards.

18:03.680 --> 18:11.680
After going forward we had some thoughts about how to continue but that might have impact.

18:11.680 --> 18:17.680
So the decision what to do when, depended on the situation at that moment.

18:17.680 --> 18:33.680
And we didn't write out everything, every possible scenario because that would be too much especially based on our testing and our acceptance and test environment that we had confidence that it would all go correctly.

18:33.680 --> 18:42.680
And we would have to look at the situation at the moment to see what the next step would be if something went wrong.

18:42.680 --> 18:44.680
Does that answer your question?

18:44.680 --> 18:46.680
Yes.

18:46.680 --> 19:03.680
If you had choice to redo your procedure, do you think it's worth it to have met HSM at all regarding the other complexity and risk of losing your key in case of backups are not here?

19:03.680 --> 19:09.680
Father Van Aving, a hidden signer that is aggabbed by your words for example.

19:09.680 --> 19:17.680
If I understand your question correctly is about, did you have anything about backups or?

19:17.680 --> 19:36.680
Are you happy with having an HSM reserve having an aggabbed linux that have the casket on the disk and do the signer and just the DNS update that's going on in the world?

19:36.680 --> 19:47.680
I don't know. I hope I understand the question, but if I want to answer your question, I'm thinking about we do not have an aggabbed system.

19:47.680 --> 19:51.680
We do have regularly backups of all the HSM keys.

19:51.680 --> 20:01.680
So in that way we do have an HSM that is aggabbed because the backup unit is an HSM and that we can use to restore keys if necessary.

20:01.680 --> 20:04.680
Do you think it's worth it?

20:04.680 --> 20:05.680
Worth it.

20:05.680 --> 20:30.680
I think it's worth it to have an aggabbed HSM to, it depends on your risk assessment if you want to have an aggabbed system and if you are going to do this thing for instance in public cloud, you might want to have a situation where you have an offline KSK for instance.

20:31.680 --> 20:34.680
So that might be in a setup.

20:34.680 --> 20:44.680
Did you conduct a penetration test on the HSM beforehand and what are your operations in case the security issue gets known in these HSMs?

20:44.680 --> 20:49.680
Did we do a PAN test on the HSMs and the next question was?

20:49.680 --> 20:52.680
What would you do if a vulnerability gets known?

20:52.680 --> 21:17.680
No, we did not do a PAN test and what we would do if a known vulnerability would be known would require our systems to investigate what happens and how can we react on that?

21:17.680 --> 21:24.680
Which information has leaked and how can we recover from that?

21:24.680 --> 21:28.680
Those are not known scenarios at least to me at the moment.

21:28.680 --> 21:30.680
Why not PAN test?

21:30.680 --> 21:32.680
Why not PAN test?

21:32.680 --> 21:36.680
I have no idea.

21:36.680 --> 21:38.680
Yes?

21:38.680 --> 21:44.680
I noticed that an extermination goes up to 14 or two bites.

21:44.680 --> 21:50.680
Did you, I'm curious what was your transportation settings for maximum?

21:50.680 --> 21:56.680
So your question is what is our transportation settings for the UDP size?

21:56.680 --> 22:06.680
We have 1232 as the size of the UDP packets as recommended by an RSE.

22:06.680 --> 22:17.680
Other parties that also provide anycast for .nl have slightly different settings for that.

22:17.680 --> 22:25.680
So that's why we focused here on NS1.DNS.nl because that's something we operate ourselves.

22:25.680 --> 22:27.680
The second question.

22:27.680 --> 22:34.680
The second question was you added the algorithm 13DS keys to the rule zone.

22:34.680 --> 22:37.680
So you've run a dual DS.

22:37.680 --> 22:53.680
What's that to allow removing the algorithm 13DS keys if you had to in a hurry or just as an additional acceptance step before you remove the algorithm 8DS records?

22:53.680 --> 23:02.680
Because during the fairly recent transition of carbon net and the EU cells, they basically just didn't swap.

23:02.680 --> 23:12.680
The question was why did you not remove the algorithm 8DS when you were adding the algorithm 13DS?

23:12.680 --> 23:13.680
Correct?

23:13.680 --> 23:14.680
Yes.

23:14.680 --> 23:21.680
We did that because we wanted to have a solid path and have a possibility to go back without any issue.

23:21.680 --> 23:38.680
So rather than take one big step in that regard, we took two small steps to ensure more stability at least from our point of view and good night's rest for us.

23:38.680 --> 23:40.680
Any other questions?

23:40.680 --> 23:44.680
Maybe not so much a question but a statement if that's allowed.

23:44.680 --> 23:45.680
Yeah.

23:45.680 --> 23:46.680
One.

23:46.680 --> 23:52.680
I think it's incredibly brave for a national top level domain to take a risk, right?

23:52.680 --> 23:56.680
And I think it's very good as a compliment.

23:56.680 --> 24:04.680
But because changing an algorithm is different than changing a key, changing an algorithm is fundamentally hard.

24:04.680 --> 24:14.680
And for SIDN to do this as one of the early adopters, not the first one but one of the early adopters to do this, I think it's very commendable.

24:14.680 --> 24:21.680
And I think you set an example for the rest of the industry for all the other top level domains including ICANN and we're looking at you.

24:21.680 --> 24:23.680
The same I would like to.

24:23.680 --> 24:31.680
And we're looking at you to see what you're doing well and of course we hope nothing goes wrong but we also need to have that information.

24:31.680 --> 24:39.680
And one of your colleagues is working with ICANN to make sure that if we ever do something in the room, that that goes well as well.

24:39.680 --> 24:41.680
So he's part of that group as well.

24:41.680 --> 24:42.680
So yeah, we're looking at this.

24:42.680 --> 24:45.680
We're hoping all the top level domains follow the same example.

24:45.680 --> 24:48.680
And yeah, all my credit goes to you guys.

24:48.680 --> 24:49.680
Welcome.

24:49.680 --> 24:50.680
Thank you.

24:50.680 --> 24:59.680
I want to repeat for the online audience.

24:59.680 --> 25:16.680
Because if you get a compliment like that, the person, Roy Arons from ICANN said that he said, very brave for our .NL or Azure at the end to do this algorithm change in the forefront of the people who are doing the change and are

25:16.680 --> 25:23.680
should be followed by registries to do this change as well.

25:23.680 --> 25:27.680
And we have shown that it's possible and without any incident.

25:27.680 --> 25:30.680
So any other till these please follow us.

25:30.680 --> 25:31.680
Good summary.

25:31.680 --> 25:32.680
Thank you.

25:32.680 --> 25:33.680
Thank you.

