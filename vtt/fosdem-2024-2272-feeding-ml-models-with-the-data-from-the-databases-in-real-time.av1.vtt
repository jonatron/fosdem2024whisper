WEBVTT

00:00.000 --> 00:28.000
All right, it's two. We'll get started.

00:28.000 --> 00:43.000
Hello, my name is Vojta Uranek. I work as a developer at Red Hat and in this short talk,

00:43.000 --> 00:49.000
I would like to discuss one approach, how you can ingest your data from into a machine learning

00:49.000 --> 00:58.000
pipeline from your databases in real time. First, this is how the machine learning pipeline can

00:58.000 --> 01:05.000
look like. And I will be speaking about where the beginning of this pipeline basically here,

01:05.000 --> 01:12.000
how to get the data from your databases into a machine learning pipeline. It can be as complex as

01:12.000 --> 01:20.000
this, but I will simplify. Just imagine you have your application running, you insert data in one or

01:20.000 --> 01:30.000
several databases, and now you want to get advantage of some machine learning or maybe AI and get your

01:30.000 --> 01:38.000
data and do some, for example, prediction. And you want to solve a question how to actually feed the

01:38.000 --> 01:47.000
data from your databases into a model, and especially when some new events are coming, how to do, for

01:47.000 --> 01:55.000
example, online predictions. It might sound simple, you just do some selects from the database, but when

01:55.000 --> 02:04.000
you have quite heavy traffic and you will run the selects in a loop, you will quickly find out that you

02:04.000 --> 02:10.000
will overload the databases with the selects. And if you add more conditions like you don't want to miss any

02:10.000 --> 02:19.000
data, you won't have consistent view of your data, you don't want to do the right and so on, you will

02:19.000 --> 02:27.000
shortly find out that it's a pretty hard problem. There are several ways how to tackle this problem, but I

02:27.000 --> 02:35.000
would suggest maybe in my opinion the best one is to change data capture. What does it mean? As the name suggests,

02:35.000 --> 02:44.000
change data capture is a process of observing some resource for a change, and if there is any change, you

02:44.000 --> 02:51.000
will extract this change and propagate it further, typically into some messaging system. What does it mean

02:51.000 --> 02:58.000
in terms of databases? It typically means to observe transaction log of the database, because transaction

02:58.000 --> 03:05.000
log of the database is typically the source of true for whole database, and all the changes which are

03:05.000 --> 03:12.000
happening to any data in the database are recorded in transaction log. So basically you will observe

03:12.000 --> 03:20.000
transaction log of your database, and if there is any change with some data you are interested in, it can be one

03:20.000 --> 03:27.000
example, whole database, whatever, you will extract this change from transaction log and send it into, for example,

03:27.000 --> 03:39.000
some messaging system. It's pretty, well, probably sounds good, but maybe you have to ask yourself if it's easy

03:39.000 --> 03:48.000
to implement. Fortunately, you don't have to solve these questions. You can use Dibizium, it's an open source

03:48.000 --> 03:59.000
for change data capture. It's pretty major. It has connectors for most of the popular databases, and it

03:59.000 --> 04:09.000
currently comes into two flavors. It originated as a Kafka Connect source connector, when you can deploy the Dibizium

04:09.000 --> 04:21.000
connectors into Kafka Connect, and they will connect to one or several databases of you are using, extract the changes,

04:21.000 --> 04:33.000
and send it to Kafka, where you can use sync connectors to do whatever you want, like, for example, updating

04:33.000 --> 04:40.000
the search index, invalidating the cache. You can, for example, use it also for replicating one database to another

04:40.000 --> 04:49.000
database, but in our case, you will probably want to push it into some data lake pool warehouse, feature store, or maybe

04:49.000 --> 04:59.000
even directly into some machine learning model. If you don't want to use Kafka for whatever reason, you can use Dibizium

04:59.000 --> 05:08.000
standalone, which is standalone process, which basically does the same, but allows you to push the events into whatever

05:08.000 --> 05:17.000
system you like, like Apache Pools, Google Pops up, and it can be even, for example, HTTP endpoint. And if you are

05:17.000 --> 05:26.000
missing any sync of for Dibizium server, it's pretty easy to implement your own. And Dibizium provides some other features,

05:26.000 --> 05:34.000
like, for example, it's capable to do snapshot of your database at any time. It can also transform the records before sending out

05:34.000 --> 05:45.000
into the messaging system and so on. So back to our problem. Unfortunately, this talk is too short to go through the whole

05:45.000 --> 05:57.000
example. On the page of this talk, for them, there is a link to our blog post when we described in details a use case when you

05:57.000 --> 06:08.000
store in the database images of handwritten digits. And whenever the image is stored there, you extract the change using Dibizium,

06:08.000 --> 06:19.000
send it to Kafka and Kafka, send it to TensorFlow, which will recognize what number is written in this picture. So it's a well-known

06:19.000 --> 06:30.000
example. And everything works in real time. As I said, we also have a full example on GitHub. But what I would like to show you here is

06:30.000 --> 06:43.000
that it's really simple. It's basically consists of deploying Dibizium and configuring it. And it's really just one page of JSON config. Here is

06:43.000 --> 06:54.000
you just provide credentials. And more interesting part is here. It's some transformation. Here is one predefined transformation where I

06:54.000 --> 07:07.000
extract only the content of newly inserted image. And because there is some caveat when you use TensorFlow with Kafka because it can

07:07.000 --> 07:18.000
interpret correctly the bytes, I will, I'm transforming the image into string, which is later on parsed in TensorFlow. But I would have to do it

07:18.000 --> 07:31.000
in TensorFlow anyway. So it's no overhead. But I can define my own transformation here. So, and it's just a couple of lines of code which

07:31.000 --> 07:43.000
just converted into string. And on TensorFlow side, it's similar, easy. It's again one page. Here I define the coding function which decodes the string.

07:43.000 --> 07:56.000
And I think it retrieves from Kafka. The most of the code is just defining Kafka endpoint. And it's about three lines to push it into the model which will

07:56.000 --> 08:10.000
recognize what is the number on the image and produce the result which you can consume further. So, as I said, if you are interested in it, please go to our

08:10.000 --> 08:21.000
website, take it up and take a look. And basically that's it. So to sum up, DBZoom is able to do a snapshot of your database and load existing data from your

08:21.000 --> 08:34.000
database into messaging system or directly into TensorFlow. And it can retrieve any change. So it can, once anything is stored into the database, it can

08:34.000 --> 08:51.000
immediately extract this change and send it further to your pipeline so you can do real-time analysis of your data. And basically it works for, you can deploy many databases and do more things with that.

08:51.000 --> 09:08.000
So that's all. If sounds interesting to you, you can try out DBZoom and please share feedback with us on Zulek, or mailing list. We have pretty large Vibran community and we will appreciate your feedback.

09:08.000 --> 09:10.000
Thank you so much for your attention.

09:10.000 --> 09:29.000
Thank you very much, Vojta. We have time for one question. Is this working? One question. Someone come up with one question. Come on.

09:29.000 --> 09:51.000
So if no one, just if you have any questions, switch pop up later on, just hit me on the corridor or elsewhere in the conference.

09:51.000 --> 10:01.000
Thanks for the presentation. My question is, is there any database that's already provided what DBZoom does by default? Any change?

10:01.000 --> 10:03.000
Could we repeat? I don't hear.

10:03.000 --> 10:06.000
Yeah, sure. Is there any data?

10:06.000 --> 10:11.000
Can you please stop moving so we can hear the question?

10:11.000 --> 10:20.000
Thanks for the presentation. My question is, is there any database that does this, what DBZoom does natively already change tracing?

10:20.000 --> 10:28.000
What do you mean natively? Because without any external tool like DBZoom, is there any database that does this already?

10:28.000 --> 10:43.000
Well, it again boils up to what means natively because we leverage typically some native features of database. For example, for Postgres, we use replication slot and we just read replication slot from Postgres.

10:43.000 --> 11:09.000
So you always need something which will need something which reads from database or from Mongo, which is from change stream. So always the database provides usually this natively, but you need something which will read it and translate it into something usable, which will parse, for example, the data you get from the replication slot from Postgres and so on.

11:09.000 --> 11:19.000
So yeah, that was the question actually. Is there any database that does this anyway without using DBZoom? But you said, I think there is no competitor then.

11:19.000 --> 11:22.000
Pardon? Is there any competitors?

11:22.000 --> 11:29.000
Yes, like is database is doing this natively already what DBZoom does?

11:29.000 --> 11:44.000
I'm not aware if there is any database which uses this. I'm aware that some databases provides this, but several of them use DBZoom under the hood as far as I know.

11:44.000 --> 11:54.000
Okay, perfect. Thank you very much, Vojta.

11:59.000 --> 12:01.000
Thank you.

