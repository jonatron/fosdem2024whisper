WEBVTT

00:00.000 --> 00:07.000
Can you hear me?

00:07.000 --> 00:10.000
Okay.

00:10.000 --> 00:16.000
So, today I'm going to talk about a project that we started more or less this summer,

00:16.000 --> 00:20.000
which is FRR Kubernetes.

00:20.000 --> 00:26.000
Some quick words about me. I'm Federico. I work for this networking team at Red Hat,

00:26.000 --> 00:31.000
in charge of making the open-source platform suitable for telco workloads.

00:31.000 --> 00:36.000
And because of that, I managed to touch many network-related projects,

00:36.000 --> 00:43.000
the SRE operator, some C&I plugins, our primary C&I and lately MetaLEDB,

00:43.000 --> 00:46.000
which I'm currently maintaining.

00:46.000 --> 00:51.000
My handles are for the power for Mastodon, Twitter and Gmail.

00:51.000 --> 00:56.000
If you need to reach me out, ask questions, I will try to answer.

00:56.000 --> 01:05.000
So, it's funny because this talk has something to do with the talk that I gave last year here for them,

01:05.000 --> 01:12.000
where I presented how we in MetaLEDB replaced the native Go BGP implementation with FRR.

01:12.000 --> 01:16.000
And first of all, what is FRR?

01:16.000 --> 01:21.000
FRR is a Linux Internet routing protocol suite.

01:21.000 --> 01:27.000
It implements a lot of protocols. It's very stable and super-supported

01:27.000 --> 01:35.000
and supports BGP and BFD, which were a couple of protocols that we were interested into.

01:35.000 --> 01:39.000
What is MetaLEDB? Is anyone using MetaLEDB?

01:39.000 --> 01:45.000
Nice. So, I'm the one to blame if something is not working.

01:45.000 --> 01:53.000
MetaLEDB is the load balancer implementation for Kubernetes cluster using standard routing protocols, including BGP.

01:53.000 --> 01:58.000
BGP allows us to announce our load balancer services across our network.

01:58.000 --> 02:03.000
If you are using Kubernetes on bare metal and you need to expose your application,

02:03.000 --> 02:06.000
there is a good chance that you need something like MetaLEDB.

02:06.000 --> 02:10.000
It's not only alternative, but it's the one that I maintain.

02:10.000 --> 02:13.000
This is more or less the architecture.

02:13.000 --> 02:22.000
We have the Kubernetes API on one side expressed in terms of services and MetaLEDB configuration.

02:22.000 --> 02:27.000
We have some code that takes all these resources, munch them together,

02:27.000 --> 02:34.000
and produces an FRR configuration that an FRR Sidecar container processes

02:34.000 --> 02:38.000
and then handles the BGP implementation.

02:38.000 --> 02:43.000
Last year, in this very conference, I got this question.

02:43.000 --> 02:48.000
Can I run MetaLEDB together my FRR instance on the cluster nodes?

02:48.000 --> 02:53.000
This is something that I keep hearing a lot.

02:53.000 --> 02:59.000
Not only that. What I keep hearing is, hey, but now inside MetaLEDB you have FRR,

02:59.000 --> 03:02.000
so you can also do this and this and this and this.

03:02.000 --> 03:06.000
No, because MetaLEDB is about announcing services,

03:06.000 --> 03:10.000
not for example about receiving routes and injecting them to the node,

03:10.000 --> 03:13.000
which is a common request.

03:13.000 --> 03:17.000
Why is that? On the cloud, everything is easy.

03:17.000 --> 03:22.000
You have one single interface to the node, one default gateway.

03:22.000 --> 03:26.000
You get the client who wants to hit your load balancer service,

03:26.000 --> 03:31.000
get to the node, enters the CNI, goes to the pod, the pod replies,

03:31.000 --> 03:36.000
and then the reply goes to the node and then exits to the default gateway

03:36.000 --> 03:39.000
and reaches the client. All is good.

03:39.000 --> 03:44.000
But on bare metal, we have users that want to have different networks

03:44.000 --> 03:47.000
for different class of traffic, for example,

03:47.000 --> 03:50.000
and you have the clients that are not on the same subject.

03:50.000 --> 03:55.000
So what has happened in this scenario is that your client reaches

03:55.000 --> 03:58.000
your secondary network and guess what?

03:58.000 --> 04:01.000
The traffic will try to exit via the default gateway

04:01.000 --> 04:06.000
and will not reach your client, or even worse, you will be beaten by RPF

04:06.000 --> 04:09.000
and you'll have a bad time trying to debug it.

04:09.000 --> 04:12.000
I've been there a couple of times.

04:12.000 --> 04:14.000
So this was more or less the request.

04:14.000 --> 04:20.000
How can I have something that is able to configure FRR running on my node

04:20.000 --> 04:22.000
together with MetaLEDB?

04:22.000 --> 04:24.000
There are a few alternatives.

04:24.000 --> 04:30.000
The easiest one, at least the easier for me, was run to FRR instances on the node.

04:30.000 --> 04:33.000
So I don't have to do anything on MetaLEDB.

04:33.000 --> 04:39.000
The user can configure its own FRR instance, but that comes with a few issues.

04:39.000 --> 04:45.000
You have duplicate sessions, you have duplicate resources consumed on the node.

04:45.000 --> 04:51.000
You have to use custom parts to let the router know how to connect to MetaLEDB

04:51.000 --> 04:55.000
and how to connect to those custom parts, to the other FRR.

04:55.000 --> 05:01.000
The other option is using two FRR instances in Cascade.

05:04.000 --> 05:11.000
This might work, but FRR wasn't able to peer with localhost until recently.

05:11.000 --> 05:19.000
It limits the flexibility of MetaLEDB because MetaLEDB has a lot of per node configuration knob.

05:19.000 --> 05:24.000
You can say I want to peer to this BGP peer only from this subset of nodes.

05:24.000 --> 05:29.000
In this case, these will affect only this session, which is useless.

05:29.000 --> 05:33.000
And also, what about the BFD implementation in MetaLEDB?

05:33.000 --> 05:36.000
It will establish BFD only through this path.

05:36.000 --> 05:42.000
So the next one, which is the one that I'm going to talk about today,

05:42.000 --> 05:46.000
is to have a shared instance between MetaLEDB and the rest of the world.

05:46.000 --> 05:49.000
So the extra configuration can scale.

05:49.000 --> 05:53.000
We can have something that is distributed across all the nodes.

05:53.000 --> 05:57.000
We don't waste resources because across the same BGP session,

05:57.000 --> 06:02.000
towards the router, we can do what MetaLEDB needs to do, but also other stuff.

06:05.000 --> 06:08.000
The cons were this was a lot of work.

06:08.000 --> 06:11.000
And getting the right API was tricky.

06:11.000 --> 06:15.000
It wasn't clear how to handle conflicts, how to merge all this stuff together.

06:15.000 --> 06:20.000
But eventually, this became a design proposal in MetaLEDB,

06:20.000 --> 06:23.000
and it converged, and we started working on it.

06:23.000 --> 06:26.000
And this is how this new project was born.

06:26.000 --> 06:28.000
It's Kubernetes-based,

06:28.000 --> 06:33.000
the one set that exposes a very limited subset of the FRR API

06:33.000 --> 06:35.000
in a Kubernetes compliant model.

06:35.000 --> 06:38.000
I wrote this description, so it's nice.

06:40.000 --> 06:42.000
This is the new architecture of the new thing.

06:42.000 --> 06:45.000
It's basically we stole what we had in MetaLEDB.

06:45.000 --> 06:48.000
We have this new FRR configuration resource,

06:48.000 --> 06:54.000
and it does basically what I already described about MetaLEDB before.

06:54.000 --> 07:01.000
But now we have a different API and a different way to configure this thing.

07:03.000 --> 07:04.000
How to deploy it?

07:04.000 --> 07:07.000
It can be deployed as a standalone thing,

07:07.000 --> 07:09.000
and this is something that I want to stress.

07:09.000 --> 07:11.000
We can use it together with MetaLEDB,

07:11.000 --> 07:16.000
and we just released a MetaLEDB version that uses this one as a backend,

07:16.000 --> 07:19.000
but you can also deploy it as a standalone component.

07:19.000 --> 07:22.000
So you can use it for your own purposes,

07:22.000 --> 07:26.000
regardless of the fact that you are using MetaLEDB or not.

07:28.000 --> 07:31.000
Now I want to talk a bit about the API.

07:32.000 --> 07:35.000
There was a good amount of discussion on this,

07:35.000 --> 07:42.000
like we were not sure whether we should have exposed the raw FRR configuration

07:42.000 --> 07:47.000
to the external world or having something that was higher level.

07:48.000 --> 07:50.000
Because there were some issues in this.

07:50.000 --> 07:52.000
How do we merge configurations?

07:52.000 --> 07:56.000
How do we allow two configurations,

07:56.000 --> 08:02.000
produced by two different actors, to become the same FRR configuration?

08:02.000 --> 08:04.000
How to intercept the configuration conflicts?

08:04.000 --> 08:07.000
If it was the raw configuration, that would be our Royal mess.

08:08.000 --> 08:13.000
And also the way MetaLEDB configures FRR is very opinionated.

08:13.000 --> 08:15.000
It gives some certain names to route maps,

08:15.000 --> 08:19.000
it gives some certain names to prefix lists,

08:19.000 --> 08:22.000
and if we wanted to extend that with a raw configuration,

08:22.000 --> 08:25.000
that would have become part of the API,

08:25.000 --> 08:28.000
and it would have been something that we couldn't change.

08:29.000 --> 08:34.000
Eventually we ended up with something high level in terms of CRD,

08:34.000 --> 08:37.000
which is FRR configuration.

08:37.000 --> 08:40.000
And this is how a configuration looks like.

08:40.000 --> 08:42.000
It has a BGP section in the spec,

08:42.000 --> 08:47.000
because we are anticipating that we might need other protocols.

08:47.000 --> 08:51.000
We have our outer section, we support multiple routers,

08:53.000 --> 08:56.000
but they need to live in different Linux VRFs.

08:57.000 --> 08:59.000
We can configure the neighbors,

08:59.000 --> 09:04.000
and we can say what prefixes we want to advertise or to receive from those neighbors.

09:05.000 --> 09:07.000
And this is how advertising looks like.

09:07.000 --> 09:12.000
We can say, I want to advertise all the prefixes that I configured in my router,

09:12.000 --> 09:15.000
or only a subset of them.

09:15.000 --> 09:19.000
And the same is more or less for the receiving part.

09:19.000 --> 09:24.000
We can say, from this peer, I want to receive only the prefixes

09:24.000 --> 09:27.000
that are matching this selector.

09:27.000 --> 09:30.000
Or we can say, I want to receive all of them.

09:33.000 --> 09:36.000
And we have an old selector, so you can say this specific configuration

09:36.000 --> 09:40.000
applies only to a subset of the nodes, which is always useful.

09:41.000 --> 09:45.000
And of course, because we know that there will be a lot of configuration

09:45.000 --> 09:49.000
that we don't cover, we also allow for experimenting

09:49.000 --> 09:52.000
or for covering special needs, our configuration,

09:52.000 --> 09:56.000
and there is a priority field where basically this gets appended

09:56.000 --> 10:01.000
to what is rendered inside the configuration from the API.

10:02.000 --> 10:06.000
And of course, we have BFD, communities, local preferences,

10:06.000 --> 10:10.000
and all the stuff that Metal.ed is currently exposing.

10:11.000 --> 10:13.000
It's covered in this API.

10:14.000 --> 10:19.000
And now I'm going to talk a bit about how multiple configurations are merged,

10:19.000 --> 10:21.000
because this was a pain point.

10:21.000 --> 10:25.000
You have multiple actors throwing configurations at the cluster,

10:25.000 --> 10:31.000
and those needs to be merged together in order to produce one single FRR configuration.

10:31.000 --> 10:34.000
And there were some guiding principles into this.

10:34.000 --> 10:37.000
We wanted a given configuration to be self-contained,

10:37.000 --> 10:41.000
meaning that you can have prefixes on one side

10:41.000 --> 10:46.000
and saying that you want to advertise those prefixes on another resource.

10:46.000 --> 10:50.000
A configuration can only add to an existing one,

10:50.000 --> 10:52.000
meaning that you can add neighbors,

10:52.000 --> 10:55.000
but you can't say, I want to remove this neighbor,

10:55.000 --> 10:57.000
applied by this other configuration,

10:57.000 --> 11:01.000
because that would steal the work to other actors.

11:01.000 --> 11:05.000
And a more permissive configuration can override a less permissive one,

11:05.000 --> 11:08.000
meaning that if you have received none,

11:08.000 --> 11:12.000
you can have received some, or a receivable will override the received some.

11:13.000 --> 11:19.000
And this is how we can merge to different configurations.

11:19.000 --> 11:23.000
We have one neighbor on one side, we have two neighbors on the other.

11:23.000 --> 11:26.000
These two configurations are compatible,

11:26.000 --> 11:30.000
and then on one side we advertise only a set of prefixes,

11:30.000 --> 11:33.000
and on the other side we advertise all of them.

11:33.000 --> 11:38.000
And these are two compatible configurations that can be merged together.

11:39.000 --> 11:43.000
Another thing is you apply all the configuration,

11:43.000 --> 11:46.000
and nothing is working. It happens a lot.

11:46.000 --> 11:49.000
We have validation webbooks,

11:49.000 --> 11:53.000
but given that the configuration is composed by multiple configurations,

11:53.000 --> 11:57.000
we know how Kubernetes work, and some things might slip.

11:57.000 --> 12:00.000
So we are exposing the status.

12:00.000 --> 12:02.000
We have three fields.

12:02.000 --> 12:05.000
One is the last conversion result,

12:05.000 --> 12:09.000
which means that if you have multiple incompatible configurations

12:09.000 --> 12:13.000
that makes to the controller and the conversion will fail,

12:13.000 --> 12:16.000
this is where you will see the error.

12:16.000 --> 12:21.000
This is the status of FRR loading the generated configuration,

12:21.000 --> 12:24.000
and this is the configuration running inside FRR.

12:24.000 --> 12:30.000
So it's something that can be used to inspect the status of the thing.

12:30.000 --> 12:34.000
With Metal LB, again, now with this new implementation,

12:34.000 --> 12:37.000
we have the same Kubernetes API on one side.

12:37.000 --> 12:40.000
Metal LB will generate an FRR configuration.

12:40.000 --> 12:45.000
It's going to be read by this new demo, which will talk to the router.

12:45.000 --> 12:52.000
And this is how a Metal LB configuration gets translated into this new one.

12:52.000 --> 12:58.000
So we have the routers, we have the neighbors, and we have a selector.

12:58.000 --> 13:06.000
So each speaker will generate a configuration for the node where it's running on.

13:06.000 --> 13:09.000
Yeah, this is what I just said.

13:09.000 --> 13:11.000
And this is when we add the service.

13:11.000 --> 13:18.000
So we will start advertising those prefixes related to the load balancer service,

13:18.000 --> 13:21.000
and things eventually will work.

13:21.000 --> 13:25.000
And of course, this is something that can be expanded,

13:25.000 --> 13:33.000
providing your own FRR configuration that gets merged to the one generated by Metal LB.

13:33.000 --> 13:37.000
I have a very quick demo.

13:37.000 --> 13:41.000
It's my first time on a live demo, so fingers crossed.

13:41.000 --> 13:44.000
Very quickly, the demo environment is a kind cluster.

13:44.000 --> 13:46.000
We have the demo running on each node.

13:46.000 --> 13:51.000
We have an external FRR container that represents more or less the external router.

13:51.000 --> 13:54.000
And now I'm going to...

13:58.000 --> 14:07.000
Okay, so here I have the kind cluster and a bunch of configuration.

14:07.000 --> 14:11.000
We have here the external container.

14:11.000 --> 14:16.000
It's paired, or it will want to pair with each of the cluster nodes.

14:16.000 --> 14:21.000
And also, it will try to advertise a couple of prefixes.

14:21.000 --> 14:28.000
And I can go on the configuration side and look at this.

14:28.000 --> 14:31.000
This is what I just stated.

14:31.000 --> 14:35.000
We want to advertise only one prefix.

14:35.000 --> 14:38.000
I'm going to apply it.

14:40.000 --> 14:42.000
And hope.

14:47.000 --> 14:55.000
Okay, so the session is up with the whole three nodes.

14:55.000 --> 15:04.000
And we have the single prefix advertised by the three nodes.

15:04.000 --> 15:14.000
And now I can look at this other one, which says advertise all.

15:14.000 --> 15:18.000
And I can apply it directly.

15:18.000 --> 15:24.000
And it's going to be merged, hopefully, to the other one.

15:24.000 --> 15:30.000
And then now we have two prefixes advertised.

15:30.000 --> 15:32.000
So it's working.

15:32.000 --> 15:37.000
We have CI, so it shouldn't be a surprise.

15:37.000 --> 15:43.000
Now I can do something on the receiving side.

15:45.000 --> 15:55.000
Here we want only one service out of the two that the external container is publishing.

15:55.000 --> 15:59.000
And this is a session inside the node.

15:59.000 --> 16:13.000
And eventually, yeah, here we have the last one is the route that is published by the external container.

16:13.000 --> 16:15.000
Yeah, what else can I show?

16:15.000 --> 16:18.000
I have five minutes.

16:18.000 --> 16:20.000
Oh.

16:25.000 --> 16:31.000
I can do this.

16:31.000 --> 16:37.000
So this is a pod running into the node.

16:37.000 --> 16:42.000
And if I try to ping it from outside, it's not going to work.

16:42.000 --> 17:01.000
For example, what I can do is try to put that prefix.

17:05.000 --> 17:07.000
No pressure.

17:08.000 --> 17:13.000
And it pings.

17:13.000 --> 17:17.000
So again, another nice example.

17:17.000 --> 17:20.000
Thank you.

17:20.000 --> 17:22.000
Okay.

17:22.000 --> 17:31.000
So I also have other examples, but I think I stressed my luck already enough.

17:31.000 --> 17:35.000
And we have still five minutes.

17:35.000 --> 17:37.000
Okay.

17:37.000 --> 17:39.000
So what's next?

17:39.000 --> 17:42.000
There are these, I don't know what's next.

17:42.000 --> 17:48.000
FRR provides a lot of opportunities.

17:48.000 --> 17:57.000
This is more or less a subset of what Metalaby offers plus something that was asked by a lot of Metalaby users.

17:57.000 --> 18:08.000
But of course, you can come and provide feedback, suggest the new features, open issues, or even contribute to the project, hopefully.

18:08.000 --> 18:18.000
The good thing is we have a framework that we can expand and grow on implementing new FRR features.

18:18.000 --> 18:20.000
A few resources.

18:20.000 --> 18:23.000
We try to keep the documentation aligned.

18:23.000 --> 18:26.000
So we have an upstream Redmi.

18:26.000 --> 18:28.000
We have the Metalaby documentation.

18:28.000 --> 18:34.000
There is the Metalaby channel on the Kubernetes Slack, which is where I live daily.

18:34.000 --> 18:45.000
And of course, the FRR community is super vibrant, super helpful, and always open to provide feedback and give help to us.

18:45.000 --> 18:51.000
And with that, if you have any questions, I'll be happy to answer.

18:56.000 --> 19:01.000
Thank you.

19:01.000 --> 19:07.000
Why did you keep using the FRR configuration files, which are quite painful to merge, as you said, instead of using the North One APIs?

19:07.000 --> 19:09.000
Can you raise your voice a bit?

19:09.000 --> 19:11.000
Why? Is it better?

19:11.000 --> 19:12.000
Yeah.

19:12.000 --> 19:21.000
Why do you use FRR configuration files, which are, as you said, quite easy to merge, instead of using the North One APIs, which have a NetCon thing?

19:21.000 --> 19:26.000
Because at the time, that was declared as experimental.

19:26.000 --> 19:31.000
I don't know if things changed in the meantime, but like, okay.

19:31.000 --> 19:33.000
So then we can...

19:33.000 --> 19:34.000
You should.

19:34.000 --> 19:35.000
Okay.

19:35.000 --> 19:36.000
Yeah.

19:36.000 --> 19:40.000
But like, we had all this mess already in place, so it was easy at the time to recycle it.

19:40.000 --> 19:46.000
But yeah, if there is a proper API, I'd be happy to start moving to that.

19:46.000 --> 19:48.000
Thank you.

19:48.000 --> 19:50.000
Any other questions?

19:50.000 --> 19:51.000
Okay.

19:51.000 --> 19:52.000
Thank you.

