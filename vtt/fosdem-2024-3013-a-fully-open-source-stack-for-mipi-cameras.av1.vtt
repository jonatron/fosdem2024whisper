WEBVTT

00:00.000 --> 00:07.000
The guy in the green.

00:07.000 --> 00:09.000
Look up.

00:09.000 --> 00:11.000
The guy from the whole group besides.

00:11.000 --> 00:13.000
Yeah, exactly. The same guy.

00:13.000 --> 00:14.000
The same guy.

00:14.000 --> 00:18.000
So you have the team fan and part of it.

00:18.000 --> 00:20.000
Nothing else in the N.A.

00:20.000 --> 00:25.000
So make sure that you have six slides.

00:25.000 --> 00:27.000
Oh, no.

00:27.000 --> 00:29.000
That's hard.

00:29.000 --> 00:31.000
That's a good one.

00:31.000 --> 00:35.000
I can't help but be sure that I'm not going to be able to do that.

00:35.000 --> 00:42.000
I'm going to try to do this on the phone.

00:42.000 --> 00:46.000
Okay, that's just not your thing.

00:46.000 --> 00:48.000
I'm just not sure if you need a copy of this.

00:48.000 --> 00:50.000
No, I'm not going to upload it.

00:50.000 --> 00:52.000
It's just how it is.

00:52.000 --> 00:59.000
Hello?

00:59.000 --> 01:01.000
Hello?

01:01.000 --> 01:03.000
Oh, it's you.

01:03.000 --> 01:05.000
I know.

01:05.000 --> 01:07.000
I know.

01:07.000 --> 01:09.000
I know.

01:09.000 --> 01:11.000
Final four?

01:11.000 --> 01:13.000
Final four.

01:13.000 --> 01:15.000
Final four.

01:15.000 --> 01:17.000
Final four.

01:17.000 --> 01:19.000
Final four.

01:19.000 --> 01:21.000
Final four.

01:21.000 --> 01:23.000
Final four.

01:23.000 --> 01:25.000
What's the other side?

01:25.000 --> 01:27.000
What's the other side?

01:27.000 --> 01:29.000
Okay.

01:29.000 --> 01:33.000
As the way he says.

01:33.000 --> 01:36.000
So, hello.

01:36.000 --> 01:38.000
Welcome to Brussels again.

01:38.000 --> 01:40.000
Still.

01:40.000 --> 01:45.000
Myself and Hans here will be doing a talk on nippy cameras.

01:45.000 --> 01:48.000
Here's some nice logos.

01:48.000 --> 01:50.000
Let's go.

01:50.000 --> 01:52.000
So, introductions.

01:52.000 --> 01:54.000
We'll do this.

01:54.000 --> 01:56.000
Go.

01:56.000 --> 01:58.000
Yeah, so I'm Hans de Hude.

01:58.000 --> 02:00.000
I work for Red Hat.

02:00.000 --> 02:03.000
It says it's on without this.

02:03.000 --> 02:08.000
Now it's on.

02:08.000 --> 02:10.000
Okay.

02:10.000 --> 02:12.000
So, hello everyone.

02:12.000 --> 02:14.000
My name is Hans de Hude.

02:14.000 --> 02:17.000
I work for Red Hat as a software engineer in the laptop hardware

02:17.000 --> 02:19.000
enablement team.

02:19.000 --> 02:22.000
I'm also the upstream kernel subsystem maintainers for drivers

02:22.000 --> 02:26.000
platform x86, which has mostly laptop related drivers.

02:26.000 --> 02:30.000
And, well, new Intel laptops are now using nippy cameras

02:30.000 --> 02:35.000
instead of old USB UBC cameras, which is a problem or a challenge.

02:35.000 --> 02:40.000
So, that's why I'm here and what this talk is about.

02:40.000 --> 02:43.000
So, my name is Brian O'Donohue.

02:43.000 --> 02:45.000
I see I don't have my second name here.

02:45.000 --> 02:47.000
And that's spelled with a Y, not an I,

02:47.000 --> 02:50.000
because I had a granny from Scotland and from Ireland,

02:50.000 --> 02:53.000
which I'm going to give you my standard spiel,

02:53.000 --> 02:55.000
is that not in the UK.

02:55.000 --> 02:57.000
It is in the EU.

02:57.000 --> 02:59.000
We do use the URL.

02:59.000 --> 03:01.000
Thank you.

03:01.000 --> 03:03.000
Thank you.

03:03.000 --> 03:05.000
But we're not in Schengen, so you have to show your passport

03:05.000 --> 03:07.000
when you come to my country.

03:07.000 --> 03:10.000
Yeah, I'm a kernel engineer with the Lamarrow.

03:10.000 --> 03:12.000
I work in the Qualcomm landing team.

03:12.000 --> 03:14.000
And, I suppose about a year ago,

03:14.000 --> 03:21.000
I inherited KMSS, which is the Qualcomm camera subsystem driver

03:21.000 --> 03:24.000
for the Lamarrow team.

03:24.000 --> 03:27.000
Here's my cold Lamarrow place.

03:27.000 --> 03:30.000
They dump all of my in-progress kernel work,

03:30.000 --> 03:36.000
my GitHub, and various IRC channels I'm on.

03:36.000 --> 03:38.000
So, we're going to kind of divide this up a little bit.

03:38.000 --> 03:41.000
There's a full topic.

03:41.000 --> 03:45.000
And it turns out that your I is more sensitive to green.

03:45.000 --> 03:47.000
I didn't know that.

03:47.000 --> 03:51.000
And particularly here, at 550 nanometers,

03:51.000 --> 03:55.000
I kind of a yellow bit of a yellow green.

03:55.000 --> 04:01.000
And so, actually, that impacts how we capture light.

04:01.000 --> 04:04.000
It's like an ADC for light.

04:04.000 --> 04:08.000
And the guy called Bruce Beyer,

04:08.000 --> 04:11.000
working at Eastman Kodak in 1974,

04:11.000 --> 04:15.000
came up with an encoding algorithm for how we capture light.

04:15.000 --> 04:18.000
Because basically the sensors are monochrome sensors.

04:18.000 --> 04:22.000
What we do is we put a filter on top of the sensors

04:22.000 --> 04:26.000
to capture a particular bit of the visual spectrum.

04:26.000 --> 04:29.000
And that's called Bayer encoding.

04:29.000 --> 04:31.000
And it gives us a lot of green again,

04:31.000 --> 04:34.000
because your eye likes green.

04:35.000 --> 04:38.000
And here's a paper, actually.

04:38.000 --> 04:40.000
I thought this was pretty cool.

04:40.000 --> 04:41.000
It just rolls it out here.

04:41.000 --> 04:44.000
So it's green, red, green, red, green, red, green, red.

04:44.000 --> 04:45.000
Blue, green, blue, green.

04:45.000 --> 04:47.000
You see we have a different Bayer pattern here.

04:47.000 --> 04:52.000
But they all conform to a similar layout.

04:52.000 --> 04:54.000
And they look like that.

04:54.000 --> 04:57.000
So, you can't really look at this.

04:57.000 --> 04:59.000
You'll see it a little bit later on.

04:59.000 --> 05:02.000
The picture that comes out in a Bayer pattern,

05:02.000 --> 05:06.000
it looks like anosaic from something from Roman times.

05:06.000 --> 05:08.000
Actually, it looks pretty cool.

05:08.000 --> 05:12.000
But it's not like what you take a picture and you do a selfie.

05:12.000 --> 05:15.000
You don't want to look like you're at Pompeii.

05:15.000 --> 05:17.000
So this is a problem.

05:17.000 --> 05:19.000
We need to interpolate that data.

05:19.000 --> 05:23.000
We need to interpret it and reasonably recover

05:23.000 --> 05:26.000
the original data that we took.

05:26.000 --> 05:29.000
So there are various methods to do that label.

05:29.000 --> 05:31.000
It's called deep-biering.

05:31.000 --> 05:34.000
Funny enough, we have the bi-erring called an image

05:34.000 --> 05:37.000
and we're going to deep-bier it to RGB.

05:37.000 --> 05:40.000
So we have label, nearest, by linear,

05:40.000 --> 05:42.000
and malar, hay, kuttler.

05:42.000 --> 05:44.000
I can't even say this.

05:44.000 --> 05:47.000
And as you go up, as you go down the list here,

05:47.000 --> 05:50.000
the computational overhead goes up.

05:50.000 --> 05:54.000
But the quality that you get out of it similarly goes up.

05:54.000 --> 05:56.000
Unsurprisingly enough.

05:56.000 --> 06:00.000
So here's a great paper on this, actually,

06:00.000 --> 06:05.000
by a guy called Morgan McGuire from about 2009, I think.

06:05.000 --> 06:08.000
And this shows us, you can see the mosaic pattern here.

06:08.000 --> 06:12.000
You can see here on the right, what he's calling graem-truth.

06:12.000 --> 06:14.000
So the original picture.

06:14.000 --> 06:17.000
This is an approximation because actually what happened is

06:17.000 --> 06:20.000
he took a picture with a Bayer sensor

06:20.000 --> 06:22.000
that has a particular resolution.

06:22.000 --> 06:26.000
But for the purposes of the talk, we'll call this graem-truth.

06:26.000 --> 06:31.000
So you can see graem-truth, raw image, label, it's kind of crap.

06:31.000 --> 06:35.000
Nearest, so you approximate based on the nearest pixel.

06:35.000 --> 06:37.000
Hands knows more about this now.

06:37.000 --> 06:42.000
I hope you give a better description of the recovery process

06:42.000 --> 06:44.000
than I will by linear.

06:44.000 --> 06:46.000
So in that case, we do line by line.

06:46.000 --> 06:48.000
And then this one here is much more complex.

06:48.000 --> 06:50.000
It does like kind of a bit in the middle

06:50.000 --> 06:52.000
and then other bits around it.

06:52.000 --> 06:56.000
But far more computationally costly.

06:56.000 --> 07:01.000
So every time you take a picture, if you think about it,

07:01.000 --> 07:05.000
your camera is performing at least this operation.

07:05.000 --> 07:09.000
At least one of these debiring operations

07:09.000 --> 07:13.000
and does so in microseconds.

07:13.000 --> 07:19.000
So how does it do that?

07:19.000 --> 07:22.000
It's not magic, believe it or not.

07:22.000 --> 07:25.000
It uses a thing called a hard ASP.

07:25.000 --> 07:30.000
So a hardware image spectrum, image signal processor, pardon me.

07:30.000 --> 07:37.000
Which typically, especially on modern systems,

07:37.000 --> 07:45.000
entails firmware running inside of the camera component of the SOC.

07:45.000 --> 07:47.000
So if you think about it, you take a picture,

07:47.000 --> 07:50.000
you start to bring data in.

07:50.000 --> 07:54.000
It's locally in memory, quite physically close

07:54.000 --> 07:56.000
to where the stream comes in.

07:56.000 --> 07:58.000
And you immediately want to process it there.

07:58.000 --> 08:03.000
Every time you kick up the chain closer and closer to main memory,

08:03.000 --> 08:08.000
you're going to pay computational costs for whatever you do up there.

08:08.000 --> 08:11.000
And so therefore, we have these hardware ASPs

08:11.000 --> 08:16.000
that have a silicon block and a firmware block

08:16.000 --> 08:20.000
that typically interacts with the silicon.

08:20.000 --> 08:24.000
And it's based on the principle of data locality.

08:24.000 --> 08:27.000
And here's some basic, so the 3As,

08:27.000 --> 08:30.000
the three most basic things that you would do

08:30.000 --> 08:36.000
with the image apart from debiring.

08:36.000 --> 08:39.000
Auto-focus, auto-white balance, auto-exposure.

08:39.000 --> 08:43.000
So bringing the exposure up or down, balancing.

08:43.000 --> 08:47.000
But you can see an example here of how the white hasn't been balanced properly.

08:47.000 --> 08:50.000
And on the right, it has been balanced properly.

08:50.000 --> 08:53.000
And the left is under-exposed.

08:53.000 --> 08:55.000
The middle is ground truth.

08:55.000 --> 08:58.000
And then on the right is over-exposed.

08:58.000 --> 09:02.000
I kind of like the right image myself, but I don't know.

09:02.000 --> 09:07.000
According to what we're calling ground truth, that's over-exposed.

09:07.000 --> 09:12.000
But hardware vendors consider all of this stuff secret sauce.

09:12.000 --> 09:15.000
Secret sauce. What's the definition of secret sauce?

09:15.000 --> 09:21.000
It's the goop that McDonald's puts on the patty before you stick it in your mouth.

09:21.000 --> 09:26.000
So the very simple sensors, the ice-ward sea sensors,

09:26.000 --> 09:28.000
is probably worth saying.

09:28.000 --> 09:36.000
The mid-sea sensors have ice-ward sea bolses that allow you to configure them.

09:36.000 --> 09:39.000
They're pretty cheap, I suppose, as sensors.

09:39.000 --> 09:44.000
But the tunings for those sensors, setting up the PLL,

09:44.000 --> 09:48.000
putting it into any configuration is considered secret sauce.

09:48.000 --> 09:54.000
And typically, if you look in the kernel, what you'll find is we have these big tables with magic numbers.

09:54.000 --> 09:56.000
And the magic number represents a...

09:56.000 --> 09:59.000
And the mirror confuses it. It doesn't know what's happening.

09:59.000 --> 10:02.000
So it thinks it's three different people.

10:02.000 --> 10:05.000
I thought that was a funny feature.

10:06.000 --> 10:11.000
So what's the problem that we're solving?

10:11.000 --> 10:13.000
What is the problem that...

10:13.000 --> 10:17.000
This is where the narrow and red hat come to the camera,

10:17.000 --> 10:20.000
and the problem that we're trying to solve.

10:20.000 --> 10:22.000
So like I say, I'm doing the Qualcomm stuff.

10:22.000 --> 10:25.000
You're doing the X86 stuff.

10:25.000 --> 10:29.000
And the commonality there is the sensors.

10:29.000 --> 10:34.000
So the silicon vendors will not release enough information

10:34.000 --> 10:37.000
to switch on their hardware ASP.

10:37.000 --> 10:44.000
And so what we have for MIPI cameras is raw data coming in,

10:44.000 --> 10:46.000
just Bayer encoded data.

10:46.000 --> 10:52.000
And for quite a while on the Qualcomm side, that's just what we've been delivering.

10:52.000 --> 10:57.000
We deliver you Bayer data upstream, we say, good luck.

10:57.000 --> 10:58.000
Have a nice day.

10:58.000 --> 11:03.000
Which is completely useless if I want to have a Zoom call.

11:04.000 --> 11:10.000
So the question then becomes where and how can we fix it?

11:10.000 --> 11:13.000
And the answer really clearly is in the live camera.

11:13.000 --> 11:17.000
So Laurent and Kieran and Jack-O-Paul and these guys here

11:17.000 --> 11:20.000
have a great project really.

11:20.000 --> 11:25.000
If you've ever tried to use the Video for Linux stuff,

11:25.000 --> 11:29.000
you'll find that the camera, even just hooking up your own camera,

11:29.000 --> 11:31.000
can be quite difficult to do.

11:31.000 --> 11:35.000
So live camera really isolates you from having to know anything about that.

11:35.000 --> 11:38.000
You can just run it, you can reuse the library,

11:38.000 --> 11:40.000
and actually I find it very easy to use.

11:40.000 --> 11:43.000
And I love it. Thanks for the t-shirt.

11:45.000 --> 11:50.000
So what we want to do is we want to do this.

11:50.000 --> 11:53.000
This is an example of a high level.

11:53.000 --> 11:56.000
This is quite similar, I suppose, to how the Raspberry Pi

11:56.000 --> 11:58.000
and the other hardware ASPs work,

11:58.000 --> 12:01.000
is that you have an ISP component here,

12:01.000 --> 12:03.000
sorry, I keep sending the way to the camera,

12:03.000 --> 12:06.000
an ISP component and then an IPA component.

12:06.000 --> 12:08.000
So the three As live inside of the IPA,

12:08.000 --> 12:12.000
and the other stuff, the stitching up,

12:12.000 --> 12:16.000
the pipeline happens in the ISP.

12:16.000 --> 12:21.000
And so when we approached the live camera and said,

12:21.000 --> 12:24.000
hey, we'd like to get something better than Bayer data,

12:24.000 --> 12:27.000
Edward, Edward, camera's stacked, why not?

12:27.000 --> 12:29.000
We do this all day long.

12:29.000 --> 12:32.000
We might as well show something for the jobs we're doing.

12:32.000 --> 12:35.000
They said, please, please implement something like this.

12:35.000 --> 12:37.000
So we started.

12:37.000 --> 12:41.000
My colleague, Andre, kind of love,

12:41.000 --> 12:43.000
I'm terrible with the Russian names,

12:43.000 --> 12:45.000
didn't also the code.

12:45.000 --> 12:48.000
I've been sitting in on the meetings

12:48.000 --> 12:51.000
and kind of arriving up and saying,

12:51.000 --> 12:53.000
here's the way I think it should work.

12:53.000 --> 12:56.000
And then I guess about three or four months ago.

12:56.000 --> 12:58.000
Yeah, something like that.

12:58.000 --> 13:08.000
So I need to join Linairo in doing this

13:08.000 --> 13:11.000
because we had a similar issue with the IPA 6,

13:11.000 --> 13:14.000
that Intel is actually working upstream

13:14.000 --> 13:18.000
to get the NIPI data receiving the CSI receiver going.

13:18.000 --> 13:20.000
And then we have all Bayer data,

13:20.000 --> 13:22.000
but Intel is currently doesn't really have a plan

13:22.000 --> 13:24.000
on how to get their secret source algorithms upstream

13:24.000 --> 13:27.000
because upstream doesn't want secret, they want open.

13:27.000 --> 13:29.000
So that's an issue.

13:29.000 --> 13:33.000
On the other hand, we have users who wanted their cameras to work.

13:33.000 --> 13:36.000
So the conclusion was that we needed a software ISP too.

13:36.000 --> 13:40.000
So we joined up with Linairo instead of doing our own stuff.

13:40.000 --> 13:44.000
And, well, that's starting to work pretty okay now,

13:44.000 --> 13:46.000
as you'll see in the demo.

13:46.000 --> 13:49.000
It needs more work on image quality, but it gives a picture.

13:50.000 --> 13:52.000
But that brings us to the next problem,

13:52.000 --> 13:58.000
which is how do applications access cameras in this new world

13:58.000 --> 14:00.000
where we have NIPI cameras, at least new for,

14:00.000 --> 14:03.000
not for smartphones, but for smartphones for an Android,

14:03.000 --> 14:05.000
but new for Linux on the desktop.

14:05.000 --> 14:08.000
So these NIPI cameras have a pipeline,

14:08.000 --> 14:11.000
be it with a software ISP or a hardware ISP,

14:11.000 --> 14:13.000
and this pipeline needs to be set up and configured.

14:13.000 --> 14:17.000
This is pretty complex stuff, which we don't want to do in a kernel.

14:17.000 --> 14:20.000
In a kernel, we just want to say, hey, here's a bunch of hardware blocks.

14:20.000 --> 14:22.000
Good luck. Just figure out how you chain them together

14:22.000 --> 14:27.000
and tell us, as the kernel, a NIP camera takes care of this for applications,

14:27.000 --> 14:30.000
but this means that currently Firefox and Chrome,

14:30.000 --> 14:33.000
assuming most people use their camera for video conferencing,

14:33.000 --> 14:35.000
and that happens in a browser, or Zoom,

14:35.000 --> 14:37.000
they all directly open the dev video note,

14:37.000 --> 14:39.000
and they expect to just be able to say there,

14:39.000 --> 14:42.000
give me a list of resolutions which you support.

14:42.000 --> 14:44.000
Oh, I want that resolution go.

14:44.000 --> 14:46.000
And it's no longer that simple.

14:47.000 --> 14:52.000
Which means that the applications will need to move to a different way

14:52.000 --> 14:54.000
of accessing cameras.

14:54.000 --> 15:00.000
At the same time, there are initiatives to move Linux on the desktop

15:00.000 --> 15:06.000
to more of a fixed operating system with applications in an app store model,

15:06.000 --> 15:11.000
mostly because building your own distribution from packages is...

15:11.000 --> 15:14.000
You get a lot of variations which can lead to instability,

15:14.000 --> 15:19.000
so having a fixed read-only base image and separate applications is desirable.

15:19.000 --> 15:21.000
These applications also run sandboxed,

15:21.000 --> 15:25.000
which I personally think is a really good idea for something like a browser

15:25.000 --> 15:27.000
because that's often a security hole.

15:27.000 --> 15:31.000
So we try to basically solve both problems at once

15:31.000 --> 15:33.000
by saying we have PipeWire.

15:33.000 --> 15:35.000
PipeWire is already used for screen capture into the browser,

15:35.000 --> 15:39.000
so it supports video transport, so let's also use that for cameras.

15:39.000 --> 15:42.000
So this will solve the sandboxing problem

15:42.000 --> 15:46.000
because PipeWire is on the sandboxed binary boundary with a portal to access it.

15:46.000 --> 15:49.000
And it also solves by using a PipeWire...

15:49.000 --> 15:51.000
a NIP camera plugin for PipeWire.

15:51.000 --> 15:54.000
It also solves the whole how do we access the camera issue.

15:55.000 --> 15:58.000
A colleague for mine has been working on this.

15:58.000 --> 16:03.000
Actually, first, I think PenguTronics started on this.

16:03.000 --> 16:07.000
They did webRTC work, which is like the shared webRTC framework

16:07.000 --> 16:09.000
between Chromium and Firefox.

16:09.000 --> 16:12.000
Then a colleague of mine picked up the integration in Firefox.

16:12.000 --> 16:16.000
This actually landed in Firefox 122, which was released like a week ago.

16:16.000 --> 16:21.000
So the Firefox, which we'll be demoing, is actually just from the Fedora repos.

16:21.000 --> 16:23.000
It's not a custom build.

16:25.000 --> 16:31.000
So with how do we access the camera problem solved

16:31.000 --> 16:34.000
and sort of having a proof of concept which we'll demo,

16:34.000 --> 16:37.000
the question becomes then what do we want to do in the future?

16:37.000 --> 16:40.000
Well, we want to do better as an image quality.

16:40.000 --> 16:44.000
We want to do it faster as in use less CPU

16:44.000 --> 16:47.000
and we want to do it cheaper as in use less energy

16:47.000 --> 16:50.000
because doing everything on the CPU is not good for your battery.

16:51.000 --> 16:55.000
So Brian actually has been experimenting with GPU acceleration.

16:55.000 --> 16:57.000
I have.

16:57.000 --> 16:59.000
And that still doesn't work?

16:59.000 --> 17:00.000
It semi-works.

17:00.000 --> 17:01.000
Oh, you have.

17:01.000 --> 17:05.000
I can change the background green on purpose or as.

17:05.000 --> 17:07.000
Or red, but I can't render.

17:07.000 --> 17:10.000
So I need to flip buffers or something.

17:10.000 --> 17:15.000
I need to go Google and just mash the keyboard until it works.

17:15.000 --> 17:20.000
So we're looking into GPU acceleration starting with OpenGL.

17:20.000 --> 17:26.000
Mostly because we already have OpenGL debiring support in a test app

17:26.000 --> 17:28.000
in Lib Camera called QCAM.

17:28.000 --> 17:30.000
So we already have a set of GL shaders

17:30.000 --> 17:34.000
and it's useful to start with sort of non-working base for the shaders.

17:35.000 --> 17:40.000
In the future, maybe we'll also do OpenCL

17:40.000 --> 17:45.000
also because some ARM SOCs only support or Vulkan.

17:45.000 --> 17:49.000
Some ARM CPUs only support Vulkan like the Imagination GPU.

17:50.000 --> 17:52.000
So yeah, that's another option.

17:52.000 --> 17:56.000
So GPU acceleration should do the faster and cheaper bit,

17:56.000 --> 17:58.000
which would also use less energy.

17:58.000 --> 18:01.000
Then we have a whole list of image quality enhancements

18:01.000 --> 18:03.000
which we would like to work on.

18:03.000 --> 18:06.000
These are also things which are actually done by the hardware ISP,

18:06.000 --> 18:09.000
but we didn't put them on the hardware ISP slide

18:09.000 --> 18:11.000
because the slide was already full.

18:11.000 --> 18:16.000
I'm going to skip this because I would like to use the last five minutes for questions.

18:20.000 --> 18:21.000
So.

18:21.000 --> 18:24.000
APPLAUSE

18:24.000 --> 18:26.000
Hey, you doing demo?

18:26.000 --> 18:27.000
Yeah, that's good.

18:27.000 --> 18:29.000
Questions on the demo?

18:29.000 --> 18:30.000
That's a good point.

18:30.000 --> 18:32.000
I'll give it a go around.

18:32.000 --> 18:33.000
Please.

18:36.000 --> 18:38.000
OK, nobody from that, please.

18:38.000 --> 18:39.000
Everybody from the front.

18:39.000 --> 18:41.000
So here you see permission dialogue,

18:41.000 --> 18:45.000
and then hopefully if I join this meeting, we'll get...

18:45.000 --> 18:46.000
Look.

18:46.000 --> 18:47.000
This is all...

18:47.000 --> 18:49.000
APPLAUSE

18:54.000 --> 18:56.000
So yeah, this is our current image quality

18:56.000 --> 18:59.000
and actually with this lightning it's not too bad.

18:59.000 --> 19:02.000
But if you have a really low light condition,

19:02.000 --> 19:05.000
then it sort of sucks which we need to work on.

19:05.000 --> 19:06.000
Do we have time for questions?

19:06.000 --> 19:08.000
I think we do.

19:08.000 --> 19:09.000
Really quick one.

19:09.000 --> 19:12.000
You mentioned OpenGL and OpenCL.

19:12.000 --> 19:15.000
Meepie is pretty common on embedded systems

19:15.000 --> 19:17.000
that usually don't have OpenGL,

19:17.000 --> 19:20.000
they usually have EGL or GLES.

19:20.000 --> 19:22.000
Yeah, so GLES is what we mean.

19:22.000 --> 19:23.000
Oh, sweet.

19:23.000 --> 19:25.000
OK, thank you.

19:30.000 --> 19:36.000
So what AAA algorithms have you actually got?

19:36.000 --> 19:40.000
So it's a more than just you've got white balance exposure.

19:40.000 --> 19:44.000
Do you have ones that people can play around with,

19:44.000 --> 19:48.000
implement their own versions of them?

19:48.000 --> 19:52.000
We would definitely welcome people to look at

19:52.000 --> 19:53.000
what we have at the moment.

19:53.000 --> 19:54.000
It's in a separate repo.

19:54.000 --> 19:58.000
I don't know if you edit that in the references.

19:58.000 --> 19:59.000
I probably did not.

19:59.000 --> 20:02.000
It has to be, you have to go look at the branch.

20:02.000 --> 20:06.000
So I might have been sure if I gave the branch for the ISP.

20:06.000 --> 20:08.000
I'm a terrible person.

20:08.000 --> 20:11.000
Look, the patches are on the upstream mailing list

20:11.000 --> 20:13.000
and the cover lever also has a link to the branch

20:13.000 --> 20:16.000
where you can check out the branch.

20:16.000 --> 20:18.000
And we definitely welcome people to experiment

20:18.000 --> 20:20.000
with more algorithms, better algorithms.

20:20.000 --> 20:23.000
Only please keep in mind, we're running this in the CPU.

20:23.000 --> 20:26.000
This is actually full HD, 30 FPS,

20:26.000 --> 20:29.000
which is currently like a 40 percent FPU load.

20:29.000 --> 20:32.000
I spend a lot of time optimizing this.

20:32.000 --> 20:35.000
So we need to do this in real time.

20:35.000 --> 20:37.000
That's important to keep in mind.

20:37.000 --> 20:42.000
I think it's probably worth saying that you won't really be

20:42.000 --> 20:46.000
able to use this on an IMX8 unless we get the GPU going.

20:46.000 --> 20:49.000
So there's a cutoff point of computational power.

20:49.000 --> 20:52.000
It's around a bit A53, I suppose.

20:52.000 --> 20:57.000
It's just too much work for those processes at that point.

20:57.000 --> 21:02.000
Hi. As far as Vulkan support,

21:02.000 --> 21:06.000
have you looked at a wrapper like Zinc

21:06.000 --> 21:10.000
to make the same functionality work on Vulkan-only socks

21:10.000 --> 21:13.000
like you have on the ones that do have OpenGL?

21:13.000 --> 21:17.000
No, we have not looked into Vulkan at all.

21:17.000 --> 21:21.000
We're currently at the stage where we're trying to get GLES shaders to work

21:21.000 --> 21:23.000
and Vulkan will come later.

21:23.000 --> 21:25.000
There's lots of stuff which will come later,

21:25.000 --> 21:28.000
like more image quality improvements.

21:28.000 --> 21:31.000
I think it might be more productive to look at OpenCL

21:31.000 --> 21:34.000
because then you don't care what you're talking to.

21:34.000 --> 21:37.000
You're talking to Vulkan, you're talking to a GPU,

21:37.000 --> 21:39.000
you're talking to a CPU.

21:39.000 --> 21:42.000
It kind of doesn't matter from the live camera perspective.

21:42.000 --> 21:45.000
So if we were going to spend more time

21:45.000 --> 21:48.000
rather than choosing between APIs and the GPU,

21:48.000 --> 21:53.000
it might be better to choose between different compute targets.

21:56.000 --> 21:59.000
You mentioned that there's lots of secret source algorithm

21:59.000 --> 22:02.000
which companies don't like upstreaming.

22:02.000 --> 22:05.000
What is the reason for this?

22:05.000 --> 22:07.000
Is it just companies being cagey,

22:07.000 --> 22:10.000
or is there an actual algorithm and all the magic number tables

22:10.000 --> 22:13.000
that for some reason they don't want public?

22:14.000 --> 22:17.000
Personally, I think it's a mix.

22:17.000 --> 22:20.000
On one part it's just companies being secretive

22:20.000 --> 22:23.000
because they're afraid of competition.

22:23.000 --> 22:25.000
Well, I don't think that's really necessary.

22:25.000 --> 22:28.000
This is something which we put together pretty quickly

22:28.000 --> 22:30.000
and it's really basic algorithms.

22:30.000 --> 22:32.000
It already gives a pretty decent picture.

22:32.000 --> 22:35.000
On the other hand, I think that the more advanced stuff

22:35.000 --> 22:38.000
really does have company or trade secrets in it.

22:38.000 --> 22:42.000
It's a mix, at least in my personal opinion.

22:42.000 --> 22:44.000
That was one more question you said.

22:44.000 --> 22:46.000
We're at a time of trade.

22:46.000 --> 22:49.000
Thank you very much for that talk.

22:49.000 --> 22:51.000
That was fantastic.

