WEBVTT

00:00.000 --> 00:08.000
Hello.

00:12.000 --> 00:16.000
I want to start. Hi, everybody. So my name is Foyker Simonis.

00:16.000 --> 00:22.000
Hi, guys and girls.

00:22.000 --> 00:28.000
So my talk, my slides and my examples are on GitHub.

00:28.000 --> 00:34.000
I will show this link one more time at the end of the talk, so you can take a picture if you want.

00:34.000 --> 00:40.000
I'm currently having some fun at the Amazon Coroeta team working on OpenJK

00:40.000 --> 00:46.000
and I did the same along for quite some time in the past at the sub-JVM and submachine team.

00:46.000 --> 00:55.000
Today I want to tell you some details about running Java in containers and in different containers.

00:55.000 --> 00:59.000
One is Cue and the other is Firecracker.

00:59.000 --> 01:03.000
So what is Cue? Cue is checkpoint and restoring user space.

01:03.000 --> 01:10.000
That's functionality in Linux which allows you to serialize a whole process tree to the file system,

01:10.000 --> 01:17.000
basically to an image or a set of images and then it can be later restored from this image

01:17.000 --> 01:22.000
and run at the same state where it was checkpointed.

01:22.000 --> 01:27.000
It only saves anonymous pages of the process so it's quite efficient.

01:27.000 --> 01:33.000
It doesn't save the shared pages. We will see what impact that has.

01:33.000 --> 01:39.000
And correct, that's a coordinated restore and checkpoint. It was mentioned before in several talks.

01:39.000 --> 01:45.000
That's a project in the OpenJK which has basically two goals.

01:45.000 --> 01:50.000
One is to create a user land checkpoint and restore notification API

01:50.000 --> 01:57.000
which allows it applications to react and take actions based on a snapshot or restore event.

01:57.000 --> 02:04.000
So before the snapshot they can do certain things like zero out, secret memory or stuff like that

02:04.000 --> 02:10.000
and then to restore for example they can restore network connections which they tear down at snapshot, things like that.

02:10.000 --> 02:17.000
And gaining quite some traction in the community, the new versions of the popular frameworks

02:17.000 --> 02:22.000
like Spring, Micronaut, Quarkus or even AWS Lambda, they support this API

02:22.000 --> 02:30.000
so if you write applications or code for this frameworks you can already use this API.

02:30.000 --> 02:37.000
The second part of the goal of the correct project is to make the JDK itself SNAP safe.

02:37.000 --> 02:44.000
So the JDM as well as the JDK. This means that it uses this notification API for example in the JDK classes

02:44.000 --> 02:48.000
to take the actions I just talked about.

02:48.000 --> 02:58.000
And this is sometimes useful or even required to make, to react appropriately not only on checkpoint, on restore

02:58.000 --> 03:03.000
but also on clone because once you've checkpointed an application you cannot only restore it,

03:03.000 --> 03:09.000
you can basically restore it many times which I call cloning and then it's important for example if you have

03:09.000 --> 03:19.000
UUIDs or secrets to as I said to either wipe them out or reset them or recreate them.

03:19.000 --> 03:28.000
And Quark is using CRIU as a tool to do the actual checkpoint and restore process

03:28.000 --> 03:37.000
but as I said the API can be used without CRIU itself and we will see how that can be used with Firecracker for example.

03:37.000 --> 03:42.000
So let's dive into a quick demo.

03:42.000 --> 03:49.000
So I will use Pet Clinic as an example here.

03:49.000 --> 03:54.000
Oh this is the wrong window. So this is for CRIU.

03:54.000 --> 03:59.000
So I just start Java with some default settings which I pick up from Java options.

03:59.000 --> 04:10.000
It's basically Java 20 or 22 I think running with 512 max of memory, running the REST version of Spring Pet Clinic.

04:10.000 --> 04:19.000
And it takes about 10 seconds to initialize and then I use URL to access it just to make sure that it works.

04:19.000 --> 04:24.000
And yes you see it works, it really works.

04:24.000 --> 04:35.000
Now we use PMAP to look at RSS of the Java process. It's about almost 450 megabytes as you can see.

04:35.000 --> 04:45.000
And we can now use CRIU to dump this process.

04:45.000 --> 04:48.000
Oh I think it's hard to see.

04:48.000 --> 04:59.000
Yeah I will scroll it up. I just start to dump.

04:59.000 --> 05:08.000
So this was just the command line here to dump the Java process into a directory.

05:08.000 --> 05:20.000
And once we've done that we can take a look to see how much memory that used.

05:20.000 --> 05:28.000
And you see that the image itself is smaller than the footprint of the process itself.

05:28.000 --> 05:34.000
That's because what I said the image only contains the private and the dirty pages of the process,

05:34.000 --> 05:39.000
the shared pages from the mapped files for example.

05:39.000 --> 05:45.000
And we can now restore this process.

05:45.000 --> 05:56.000
So we use CRIU restore from the same directory and it works like in about 200 milliseconds.

05:56.000 --> 06:09.000
And if we use PMAP again you see that it uses, after we store it uses less memory, about 20 megabytes less than before.

06:09.000 --> 06:12.000
So why is that the case?

06:12.000 --> 06:17.000
Again that's because of shared pages.

06:17.000 --> 06:28.000
This is the diff of the whole PMAP output for the initial process before it was snapshotted and after we store.

06:28.000 --> 06:40.000
And you see the basic difference here is that for a lot of libraries like system libraries, LAP-NSS for example,

06:40.000 --> 06:51.000
we used 140 kilobytes for startup but this memory, these pages are not required anymore after we restore the process.

06:51.000 --> 07:00.000
So CRIU has still recorded that the process can access this memory but until it doesn't touch these pages they won't be paged in.

07:00.000 --> 07:08.000
So that's why after we store the process uses less memory which is a nice side effect.

07:08.000 --> 07:14.000
Okay so what other possibilities do we have?

07:14.000 --> 07:32.000
We start the application once again and it always takes about 10 seconds.

07:32.000 --> 07:37.000
So it works again.

07:37.000 --> 07:44.000
Now there is a feature called Trim-NATIFIP which was introduced by Thomas, my former colleague Thomas StÃ¼ffer,

07:44.000 --> 07:52.000
which basically frees the Maloch, the G-Lipsy Maloch buffers.

07:52.000 --> 08:04.000
And this can have quite a significant impact on the footprint of the process.

08:04.000 --> 08:13.000
So we see that the G-Lipsy Maloch cache used about 60 megabytes.

08:13.000 --> 08:24.000
And if we run now P-Map again we see that the RSS is much slower now, much slower now, just about 450 megabytes.

08:24.000 --> 08:33.000
And we can now...

08:33.000 --> 08:39.000
I also experimented with the new option which zeroes part of the heap.

08:39.000 --> 08:48.000
So it basically does a system GC and all the unused parts of the heap will be zeroed.

08:48.000 --> 08:56.000
If we do that and look at the memory footprint of the process we will see that the memory footprint got bigger

08:56.000 --> 09:03.000
because now parts of the heap which weren't mapped before get paged into the memory but they contain only zeros.

09:03.000 --> 09:19.000
And I have a pull request for the QIO project which such as QIO can recognize zero pages and ignore them while dumping.

09:19.000 --> 09:27.000
If we check point now with this zero option, it's basically the same like before.

09:27.000 --> 09:36.000
We just used the skip zero flag which is not standard until now but I hope it will be integrated soon.

09:36.000 --> 09:55.000
And if we take a look at image size we see that the image size now gets considerably smaller.

09:55.000 --> 10:07.000
So it's just 200 megabytes because all the pages which contain just zero bytes are replaced by reference to the kernel zero page.

10:07.000 --> 10:10.000
So basically it's a sparse file, the image file.

10:10.000 --> 10:20.000
And when we restart the process the memory footprint will be smaller as well.

10:20.000 --> 10:37.000
So we restart now from the new directory and when we take a look at the P-map output you see again it's just 270 megabytes.

10:37.000 --> 10:43.000
This is a little cumbersome so why not using the crack release itself.

10:44.000 --> 10:57.000
And the good thing is that crack basically does all what I've showed you what you basically can manually do with a normal JDK and with a normal QIO release.

10:57.000 --> 11:02.000
This is basically built into a crack version of crack build of the open JDK.

11:02.000 --> 11:06.000
We use the option crack check point two and give it a directory.

11:06.000 --> 11:30.000
So we run the application and then once it initializes we see it works and then instead of using the QIO command directly we can use a J command to check point.

11:31.000 --> 11:49.000
So I scroll it up here so we just scroll J command with the PID of the pet cleaning application and we execute the JCMD JDK checkpoint and that killed the checkpoint and also killed the process.

11:50.000 --> 12:07.000
And we can now restart that again with the help of Java by using the second crack option which is crack restore from and then give it the directory where the file was saved to.

12:08.000 --> 12:24.000
And this takes just a few milliseconds again and we see it works and again the memory footprint is like before it's like 280 after the first restore so it's considerably smaller because the heap was shrink.

12:24.000 --> 12:42.000
So what crack is actually doing it's not zeroing the memory but it's unmapping all the unused parts of the heap and I also recently added the feature to call into Tomas Trim native memory functionality to also free the JLPC buffers.

12:42.000 --> 13:03.000
So to summarize like in for a spring pet clinic application it has about memory footprint of a good 500 megabytes and after restore it's a little smaller because it doesn't have to restore all the shared pages.

13:04.000 --> 13:07.000
Image size is about 500 megabytes.

13:08.000 --> 13:29.000
If we zero out all the heap unused heap and use the skip zero flag of crew the RSS goes up just before the checkpoint but instead we get a much smaller image size and also a smaller footprint when we restore.

13:29.000 --> 13:46.000
And that's the same with crack because it basically it doesn't zero but it unmapped the memory and it has the same effect so it would wonder why do we need the zeroing at all then and not just use crack so I hope that will get clear in my next example.

13:47.000 --> 13:53.000
So for the next example I will use firecracker which is a KVM based virtualization.

13:54.000 --> 14:04.000
It's basically QMU on steroids it's a stripped down version of a virtualizer it has only a restricted set of network block device network device.

14:05.000 --> 14:25.000
It's rest base configuration it's written in Rust and it's open source under Apache 2 license and if you ever used AWS Lambda or Fargate for example that's the technology which drives this offering so every Lambda function is running in its own KVM based firecracker container.

14:26.000 --> 14:31.000
This is a diagram of how it works but I think we don't have time to go into the details today.

14:31.000 --> 14:34.000
It said I want to show you how this works practically.

14:35.000 --> 14:38.000
So I have another prepared another window here.

14:38.000 --> 14:48.000
So I use a script which which basically starts firecracker and inside firecracker it then starts again the pet cleaning application.

14:48.000 --> 15:17.000
If you take a close look this basically boots its own kernel which is 6.0 this here Linux 6.1.7 so it boots its own kernel in a virtual machine and then inside the kernel it starts firecracker and now if you see a rail this virtual machine has its own network address so we cannot use localised anymore.

15:17.000 --> 15:31.000
So we have to use the IP address of the virtual machine running on our host system but apart from them it works exactly the same and we have to look for two footprints now.

15:31.000 --> 15:39.000
We want to know we have to look at the footprint of the firecracker VM itself which is about 670 megabytes.

15:39.000 --> 15:59.000
Slightly bigger than that of the whole process and we can also look at the size of the JVM inside the guest and we see the JVM size inside the firecracker guest is about the same like when you run it locally which is basically clear.

16:00.000 --> 16:06.000
And we can now snapshot the whole firecracker container.

16:06.000 --> 16:25.000
Again that takes just a few seconds to this directory and if we want to see how big it is it's like 670 megabytes about the size of the whole firecracker container had in memory when it ran.

16:26.000 --> 16:35.000
So just to demonstrate how it works now we can restore from the snapshot.

16:35.000 --> 16:54.000
This again basically spins up the whole virtual machine in about 200 milliseconds and we can check how much memory it takes and you see it takes very few memory because it takes only 570 megabytes.

16:55.000 --> 17:04.000
Because it only pages in the pages which are really required to start the virtual machine.

17:04.000 --> 17:12.000
Crew paged in all the pages from its page file into the newly created process.

17:12.000 --> 17:18.000
Whereas firecracker does this lazily that's why initially it needs so few memory.

17:19.000 --> 17:36.000
The funny thing is that if you look inside the container by SSH and to the container and to a P map the Java process within the firecracker container still basically needs 500 megabytes but the VM itself only has paged in like 50 megabytes of memory.

17:37.000 --> 17:56.000
And what we can do now is, yeah we wanted to see, we already saw how big the, yeah we are sorry, we just do a request.

17:57.000 --> 18:15.000
So you see it's still working after we store the network devices are restored interfaces and it works and if we look at the image size of firecracker after the restore you see it gets bigger like 270 megabytes which corresponds mostly to what the crew process used.

18:15.000 --> 18:28.000
So that's actually the crew restore Java process so that's about this 270 megabytes seems to be required in order to process this request in pet clinic.

18:30.000 --> 18:39.000
So now how can we get this smaller the image size of the firecracker container because 690 megabytes is quite big.

18:39.000 --> 19:08.000
So again we run firecracker and you can see that I started a starter firecracker process with the checkpoint option so I with the crack checkpoint option so I can actually use the J command version now to check point.

19:09.000 --> 19:15.000
So we have the Java process in the KVM guest.

19:17.000 --> 19:29.000
Again we use SSH to SSH into the firecracker container and then inside the container we execute J command to checkpoint.

19:29.000 --> 19:57.000
This is a special version of checkpoint where we doesn't make sense to use crew within the firecracker container because anyway we will snapshot the whole firecracker container so instead we just use the special version of crack which only executes all the callbacks and thus all the optimizations but doesn't really call crew.

19:58.000 --> 20:10.000
So that's where we have to restart it inside and when we look at the memory.

20:11.000 --> 20:29.000
Inside the the Docker container we see that it's about 290 so it was it went down the SS but unfortunately the container it's like firecracker process itself still uses that much memory and if we.

20:30.000 --> 20:33.000
If we snapshot it.

20:40.000 --> 20:41.000
That that works.

20:44.000 --> 20:45.000
But.

20:47.000 --> 20:49.000
Let's take a look at the size.

20:50.000 --> 20:59.000
It's still 600 megabytes so that's why I called why I choose the title like that that's what I call the semantic.

21:00.000 --> 21:18.000
Gap between the guest and the host like even if I free memory in the guest container the host colonel does not cannot know that these pages are not used anymore by the guest system and they are still dirty from his perspective so if I if I.

21:19.000 --> 21:20.000
Snapshot.

21:21.000 --> 21:28.000
The container the whole VM it's it has to save them to this which makes it inefficient.

21:30.000 --> 21:34.000
So there are different possibilities to cope with this.

21:35.000 --> 21:36.000
One is to use.

21:38.000 --> 21:51.000
The the trim native image and the the zero heap options are showed you before because then firecracker has the chance to wipe out these pages from the image which make the image.

21:52.000 --> 21:53.000
Sight smaller.

21:55.000 --> 22:07.000
So I have I've summarized this here in this table so initially the firecracker process needs about six or almost 700 megabytes of RSS the JVM inside like before 500.

22:08.000 --> 22:16.000
Snapshot is about 600 megabytes after we store 50 megabytes and after the first request again 266.

22:17.000 --> 22:33.000
If you run this crack and do the checkpoint we can minimize the memory size within the VM to about 290 but the but the image size that the snapshot size itself stays at at at 600.

22:34.000 --> 22:49.000
If we do the trim native and zero unused heap the the memory consumption of the of the of the of the virtual machine goes up because again we we touch all the pages in order to zero them.

22:50.000 --> 23:05.000
But we get a much smaller image size because now the virtual machine manager again replaces these pages by the kernel zero page so we get a much smaller image and faster start up time.

23:06.000 --> 23:15.000
There's another possibility and that's called in it of in it on three that's a kernel option so the kernel usually when you give when you am a page and give it back to the kernel.

23:16.000 --> 23:25.000
The kernel doesn't do anything with this memory the kernel zeros the memory when you allocate it when you am up on your page the kernel will give you a zero page a page only containing zeros.

23:26.000 --> 23:32.000
But there is an option called in it on three which does this in the other other way around so it's and it's.

23:33.000 --> 23:41.000
Just for example in security critical application where people want to make sure that once they release memory this memory is immediately zeroed out.

23:42.000 --> 23:54.000
The thing with this is that the initial memory size of the container goes up because when the kernel boots up it was zeros the whole memory so it touches all pages so the footprint of the of the.

23:55.000 --> 24:00.000
Firecracker process is like one gigabyte which is what I gave him for the for the guest on the other side.

24:00.000 --> 24:03.000
When we snapshot this.

24:05.000 --> 24:10.000
We we get down to a four hundred twenty megabytes which is already quite nice.

24:12.000 --> 24:21.000
Last feature which I wanted to mention briefly is ballooning that's a special device inside the guest which can allocate memory can sing of it like a file cache.

24:21.000 --> 24:32.000
And then it has a means to communicate is back to the KVM manager and the host and tell the host that the host can now reuse this part of the memory so with this.

24:33.000 --> 24:44.000
If we inflate the balloon we can decrease the the footprint of the whole virtual machine but unfortunately the snapshot again gets bigger because from the.

24:45.000 --> 25:02.000
Host site this page is still look tainted so we have to combine ballooning within it on three then we get like all the benefits small very small footprint of the running KVM process and the smallest image size so with that I came to the end of my talk there are some references here.

25:03.000 --> 25:11.000
And I linked to the to the examples I showed you and this is where you find the presentation so thanks a lot.

25:14.000 --> 25:15.000
Thank you.

