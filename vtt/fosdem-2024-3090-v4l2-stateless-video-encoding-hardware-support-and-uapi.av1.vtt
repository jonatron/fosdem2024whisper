WEBVTT

00:00.000 --> 00:05.000
All right.

00:05.000 --> 00:09.000
Hello, everybody, and thanks for being here today.

00:09.000 --> 00:13.000
So I'm Paul Krakowski, and today I'll be talking about VFALL2

00:13.000 --> 00:15.000
Stateless Video Encoding.

00:15.000 --> 00:17.000
We're going to talk about hardware support,

00:17.000 --> 00:21.000
and we're going to talk about the Stateless Video Encoding

00:21.000 --> 00:25.000
new API, and we're going to see why it's actually difficult

00:25.000 --> 00:28.000
to come up with this new API.

00:28.000 --> 00:33.000
So I am now self-employed working in my own company,

00:33.000 --> 00:37.000
which is called SeasBase, so I provide engineering services

00:37.000 --> 00:39.000
on topics related to multimedia and graphics,

00:39.000 --> 00:42.000
so if you're interested, there's my email here.

00:42.000 --> 00:46.000
And let's begin with a very simple question.

00:46.000 --> 00:49.000
Why do we need to encode videos?

00:49.000 --> 00:53.000
So the main reason is really that storing pictures

00:53.000 --> 00:57.000
in digital form is extremely big in terms of size.

00:57.000 --> 00:59.000
So it really, really takes a lot of storage,

00:59.000 --> 01:02.000
and this is really not something that is reasonable.

01:02.000 --> 01:04.000
So pictures are just way too big.

01:04.000 --> 01:06.000
So what do we have to do?

01:06.000 --> 01:08.000
Well, simple. We just have to compress them.

01:08.000 --> 01:10.000
So let's compress videos.

01:10.000 --> 01:12.000
And now what happens when we do that?

01:12.000 --> 01:13.000
They tend to look crappy.

01:13.000 --> 01:16.000
So we compress them too much, and they look bad.

01:16.000 --> 01:20.000
So this kind of brings us to the main topic of encoding,

01:20.000 --> 01:23.000
which is the management of the trade-off

01:23.000 --> 01:26.000
between the size of the video and the quality that we get,

01:26.000 --> 01:28.000
or at least the perceived quality.

01:28.000 --> 01:30.000
So we want things that are not too big,

01:30.000 --> 01:33.000
and that look good, essentially.

01:33.000 --> 01:36.000
So in order to do that, we have a number of techniques

01:36.000 --> 01:39.000
which are implemented as codecs, which are just formats

01:39.000 --> 01:45.000
that specify kind of how we should encode videos.

01:45.000 --> 01:48.000
So a good codec has a good trade-off

01:48.000 --> 01:51.000
between the size and the quality,

01:51.000 --> 01:54.000
and in general, the codecs that we have today

01:54.000 --> 01:59.000
are extremely, extremely performant at this.

01:59.000 --> 02:03.000
Most of them have specifications that can be accessed.

02:03.000 --> 02:05.000
They are often hard to read,

02:05.000 --> 02:07.000
because essentially, if you want to understand the spec,

02:07.000 --> 02:09.000
you need to understand the codec first.

02:09.000 --> 02:11.000
And if you don't know the codec first,

02:11.000 --> 02:13.000
it's quite hard to understand the spec.

02:13.000 --> 02:15.000
So that's the situation,

02:15.000 --> 02:18.000
but at least there are some standards and specifications.

02:18.000 --> 02:20.000
So some require realities to be used,

02:20.000 --> 02:22.000
some others don't.

02:22.000 --> 02:24.000
That really depends.

02:24.000 --> 02:26.000
So there is a bunch of codecs which are hyped now.

02:26.000 --> 02:28.000
There is a lot of talk about AV1,

02:28.000 --> 02:30.000
VP9, HTC5, and HTC6,

02:30.000 --> 02:32.000
so that's the kind of upcoming stuff

02:32.000 --> 02:34.000
that everyone is excited about.

02:34.000 --> 02:36.000
But of course, it really takes a while

02:36.000 --> 02:40.000
before people actually use those codecs in the real world.

02:40.000 --> 02:42.000
But there is a real interest in that,

02:42.000 --> 02:45.000
especially if we're looking to reduce our global power consumption,

02:45.000 --> 02:48.000
because better codecs means less data,

02:48.000 --> 02:50.000
and if we have less data,

02:50.000 --> 02:53.000
less energy consumption for storage and for transmission.

02:53.000 --> 02:56.000
So having good codecs is actually quite important,

02:56.000 --> 02:58.000
and it can really make a difference.

02:58.000 --> 03:00.000
So how do we encode those videos?

03:00.000 --> 03:02.000
There is a number of techniques which are used,

03:02.000 --> 03:05.000
usually one after the other in some kind of chain.

03:05.000 --> 03:08.000
So we have a first class, which is the spatial compression,

03:08.000 --> 03:12.000
which is essentially how we can represent a single static picture

03:12.000 --> 03:14.000
in the most efficient way

03:14.000 --> 03:16.000
by using lots of different compression techniques.

03:16.000 --> 03:18.000
So I've mentioned a few here.

03:18.000 --> 03:21.000
Essentially, we go to frequency domain,

03:21.000 --> 03:23.000
and then we can eliminate some coefficients

03:23.000 --> 03:25.000
from that frequency domain.

03:25.000 --> 03:27.000
This is what we call quantization,

03:27.000 --> 03:30.000
and we have a value that we call QP, the quantization parameter,

03:30.000 --> 03:34.000
which is here to tell us how many of those frequency-based coefficients

03:34.000 --> 03:37.000
we want to keep in order to represent our picture.

03:37.000 --> 03:40.000
And in general, the high-frequency coefficients represent details,

03:40.000 --> 03:44.000
and the low-frequency coefficients represent the rough shape of the picture.

03:45.000 --> 03:48.000
But we also apply temporal compression techniques.

03:48.000 --> 03:51.000
Temporal compression will use the previews

03:51.000 --> 03:53.000
and sometimes the next frames as well

03:53.000 --> 03:56.000
in order to kind of calculate the current picture.

03:56.000 --> 03:59.000
So instead of coding a single picture all the time,

03:59.000 --> 04:02.000
we are actually coding a difference between the previews

04:02.000 --> 04:06.000
and all the next picture to create the current one.

04:07.000 --> 04:09.000
So this is, for example, a representation

04:09.000 --> 04:11.000
of what we call motion vectors,

04:11.000 --> 04:15.000
which will indicate how the kind of pixels change

04:15.000 --> 04:17.000
from one frame to another,

04:17.000 --> 04:21.000
so we have a notion of directivity and direction, and so on.

04:23.000 --> 04:27.000
So when we encode a video, we kind of have to decide on a strategy,

04:27.000 --> 04:30.000
which is exactly what do we want to do when we encode that video

04:30.000 --> 04:33.000
and which kind of trade-off we're going to adopt

04:33.000 --> 04:35.000
between the size and the quality

04:35.000 --> 04:38.000
and how we want that trade-off to evolve over time

04:38.000 --> 04:40.000
and things like that.

04:40.000 --> 04:42.000
So we have very common strategies.

04:42.000 --> 04:46.000
For example, if you just want an average or a constant bitrate

04:46.000 --> 04:48.000
to stay the same for the whole stream,

04:48.000 --> 04:51.000
that's one strategy that an encoder can implement,

04:51.000 --> 04:54.000
but we can also decide that instead we want a constant quality,

04:54.000 --> 04:59.000
so the size might change depending on what we want to achieve.

05:00.000 --> 05:04.000
So it's important to understand that this notion of strategy

05:04.000 --> 05:08.000
for encoding is really at the core of what an encoder is doing,

05:08.000 --> 05:11.000
and this is generally what we call rate control.

05:11.000 --> 05:17.000
So rate control is about dynamically deciding on this trade-off,

05:17.000 --> 05:20.000
so we have to decide on which frame type we want to use,

05:20.000 --> 05:22.000
we have to decide on the quantization parameter

05:22.000 --> 05:25.000
and a number of other settings that we can set in the encoder

05:25.000 --> 05:29.000
to essentially have it adapt to different scenes

05:29.000 --> 05:33.000
and also to be able to react to changes in the scene.

05:33.000 --> 05:35.000
So if you, for example, have a movie

05:35.000 --> 05:37.000
and you're going from one scene to another,

05:37.000 --> 05:40.000
you want your encoder to kind of react to that

05:40.000 --> 05:44.000
and give more details on the first image of the new scene,

05:44.000 --> 05:46.000
for example, things like that.

05:46.000 --> 05:49.000
So the main takeaway from this slide

05:49.000 --> 05:51.000
is that the rate control implementation

05:51.000 --> 05:53.000
is really the key to have good encoding

05:53.000 --> 05:56.000
and to have something that is actually performance

05:56.000 --> 05:58.000
and that gives good results.

05:59.000 --> 06:04.000
So now that we have a brief overview of those compression techniques

06:04.000 --> 06:07.000
and what encoders are supposed to do,

06:07.000 --> 06:10.000
the main topic is how do we accelerate this

06:10.000 --> 06:12.000
because doing all of this on the CPU

06:12.000 --> 06:14.000
is usually a very intensive process,

06:14.000 --> 06:16.000
so it will take a lot of your CPU,

06:16.000 --> 06:18.000
and nowadays we have lots of use cases

06:18.000 --> 06:22.000
where people want super high sizes

06:22.000 --> 06:23.000
and they want high frame rate,

06:23.000 --> 06:26.000
like 60 frames per second on things like 4K,

06:26.000 --> 06:30.000
and there are use cases where we want to be able to encode

06:30.000 --> 06:32.000
just as we are receiving the data.

06:32.000 --> 06:35.000
So for example, a regular camera that you used to shoot,

06:35.000 --> 06:37.000
you want to be able to produce a video

06:37.000 --> 06:41.000
so you want to be able to encode in more or less real time.

06:41.000 --> 06:43.000
So if you really want to achieve that,

06:43.000 --> 06:46.000
you have to use dedicated hardware which will offload this

06:46.000 --> 06:50.000
and will give you some acceleration for this process.

06:50.000 --> 06:53.000
So this is when we start talking about hardware-based video encoding

06:53.000 --> 06:56.000
or hardware-accelerated video encoding.

06:56.000 --> 06:58.000
So those hardware encoders,

06:58.000 --> 07:01.000
not only do they know how to produce the correct

07:01.000 --> 07:05.000
format for the video that we chose for the codec,

07:05.000 --> 07:10.000
they sometimes also have some extra features

07:10.000 --> 07:12.000
like pre-processing, so for example,

07:12.000 --> 07:15.000
they can convert the format of the pictures that you give them.

07:15.000 --> 07:19.000
They can also apply things like antishaking or cropping.

07:19.000 --> 07:22.000
So this is a very common pre-processing pipeline

07:22.000 --> 07:25.000
for an encoder.

07:25.000 --> 07:28.000
Usually they will have the ability

07:28.000 --> 07:31.000
to also encode multiple streams in parallel,

07:31.000 --> 07:33.000
so this, well, not necessarily in parallel,

07:33.000 --> 07:36.000
but at least in a time-shared way.

07:36.000 --> 07:38.000
So you could use the same encoder

07:38.000 --> 07:40.000
and have multiple streams that you want to encode,

07:40.000 --> 07:43.000
and then you encode one frame for one stream

07:43.000 --> 07:46.000
which goes into one sync, one output,

07:46.000 --> 07:48.000
and then you have another stream

07:48.000 --> 07:51.000
that you also want to deal with concurrently.

07:51.000 --> 07:55.000
So it's important that you are able to kind of switch

07:55.000 --> 07:57.000
between different contexts of encoding

07:57.000 --> 07:59.000
because you don't want your encoder to be just dedicated

07:59.000 --> 08:01.000
to one thing, to one task,

08:01.000 --> 08:03.000
so it's a little bit like a GPU

08:03.000 --> 08:07.000
where you want to be able to render multiple things like that.

08:07.000 --> 08:11.000
All right, so when we're talking about hardware video encoding,

08:11.000 --> 08:14.000
there is essentially two types of hardware implementations.

08:14.000 --> 08:16.000
The first one, which is probably the most common,

08:16.000 --> 08:19.000
is what we call the stateful encoders.

08:19.000 --> 08:21.000
So these encoders are somewhat abstracted

08:21.000 --> 08:25.000
and a bit less flexible for, let's say, the end users

08:25.000 --> 08:28.000
because they essentially come with a microcontroller

08:28.000 --> 08:31.000
that will do most of the heavy lifting involved in encoding,

08:31.000 --> 08:34.000
especially implementing the rate control strategies.

08:34.000 --> 08:37.000
So they come with a firmware that implements that,

08:37.000 --> 08:42.000
and it really does a lot, especially the rate control.

08:42.000 --> 08:46.000
And the CPU will usually interact with that encoder

08:46.000 --> 08:48.000
through some mailbox interface,

08:48.000 --> 08:50.000
and it will essentially give it messages

08:50.000 --> 08:53.000
like encode this source with these parameters,

08:53.000 --> 08:56.000
but the parameters are still quite high level.

08:56.000 --> 08:58.000
On the contrary, we have a second type of implementation,

08:58.000 --> 09:00.000
which we call the stateless encoders,

09:00.000 --> 09:02.000
which are really more bare metal,

09:02.000 --> 09:04.000
so they are also more flexible,

09:04.000 --> 09:07.000
and we have more control over exactly the parameters

09:07.000 --> 09:11.000
that we give the encoder and all the, let's say, technical decisions

09:11.000 --> 09:14.000
that are used to create the final bit stream,

09:14.000 --> 09:18.000
the final video codec data.

09:18.000 --> 09:22.000
So in that situation, the CPUs that is driving the encoder

09:22.000 --> 09:24.000
have to do more.

09:24.000 --> 09:27.000
It has to do essentially all that the firmware was doing

09:27.000 --> 09:32.000
in the stateful case, so this is generally more involved.

09:32.000 --> 09:37.000
And, yeah, it means that you have more things to do on your kernel,

09:37.000 --> 09:41.000
and we have, of course, a bunch of others.

09:41.000 --> 09:44.000
For the stateless designs, we have less known examples,

09:44.000 --> 09:47.000
but they are also quite popular and found in lots of chips.

09:47.000 --> 09:50.000
So we have the Hentro from Vericilicon,

09:50.000 --> 09:54.000
which is found, for example, on lots of IMX8s.

09:54.000 --> 09:56.000
We also find it on RockChip platforms,

09:56.000 --> 09:59.000
and some on Alwinner as well.

09:59.000 --> 10:02.000
Alwinner, which is a Chinese chip maker,

10:02.000 --> 10:05.000
has their own video engine implementation,

10:05.000 --> 10:07.000
which also has an encoder.

10:07.000 --> 10:11.000
So that's pretty much what we know about so far.

10:11.000 --> 10:14.000
Oh, MediaTek, I didn't mention it.

10:14.000 --> 10:17.000
So it's stateful, but it's kind of helpful between the two, right,

10:17.000 --> 10:20.000
because you can also drive it stateless.

10:20.000 --> 10:25.000
Okay, stateful encoder, stateless decoder.

10:25.000 --> 10:29.000
Okay, great.

10:29.000 --> 10:30.000
All right.

10:30.000 --> 10:35.000
Okay, so for the stateful case, in Linux,

10:35.000 --> 10:37.000
we have a great API in V4L2,

10:37.000 --> 10:40.000
which is based on the V4L2 memory-to-memory framework,

10:40.000 --> 10:41.000
which works with QQs,

10:41.000 --> 10:44.000
so essentially you're going to submit data from user space,

10:44.000 --> 10:46.000
which is your source picture,

10:46.000 --> 10:48.000
and you're going to get some encoded bitstream

10:48.000 --> 10:50.000
as a result from this API.

10:50.000 --> 10:54.000
We have pixel formats to describe the coded streams,

10:54.000 --> 10:57.000
and we can use some specific controls

10:57.000 --> 10:59.000
to set features of the encoder.

10:59.000 --> 11:01.000
So when there is a technical choice to be made,

11:01.000 --> 11:05.000
we can use those controls and tell it exactly,

11:05.000 --> 11:07.000
well, how we want the video to be encoded.

11:07.000 --> 11:11.000
But again, the rate control is implemented in the firmware,

11:11.000 --> 11:15.000
so all we can do with that is to tell the firmware,

11:15.000 --> 11:18.000
to tell the microcontroller what it should do,

11:18.000 --> 11:21.000
but it's not going to be the kernel side that does it.

11:21.000 --> 11:24.000
On the other side with the stateless encoding,

11:24.000 --> 11:27.000
like I said, we have a lot more to do from the CPU side,

11:27.000 --> 11:31.000
and this is when it gets a bit complicated with V4L2.

11:31.000 --> 11:36.000
So currently we don't have a stateless encoding UAPI.

11:37.000 --> 11:41.000
There is some difficulty that I'm going to mention

11:41.000 --> 11:43.000
in order to do that.

11:43.000 --> 11:47.000
And one of the points that are difficult

11:47.000 --> 11:50.000
is to implement the rate control part,

11:50.000 --> 11:54.000
so to be able to adapt exactly what the video stream looks like

11:54.000 --> 11:58.000
depending on the policy that you want to follow.

11:58.000 --> 12:01.000
And of course we want that UAPI to be hardware agnostic,

12:01.000 --> 12:04.000
so we don't want to just have user space drivers

12:04.000 --> 12:06.000
that will be specific to each encoder.

12:06.000 --> 12:09.000
Instead we want to have a generic interface,

12:09.000 --> 12:12.000
like it's the case for the stateful encoders,

12:12.000 --> 12:15.000
where we have this generic V4L2 API.

12:17.000 --> 12:21.000
But stateless encoding also has significant advantages.

12:21.000 --> 12:23.000
It's a lot more flexible,

12:23.000 --> 12:26.000
and that means that we have more control over what's going on.

12:26.000 --> 12:29.000
So in theory we are able to take better decisions

12:29.000 --> 12:33.000
to produce the best stream that we can,

12:33.000 --> 12:35.000
and that is not necessarily the case

12:35.000 --> 12:38.000
with a stateful, firmware-driven approach.

12:38.000 --> 12:41.000
So user space might actually have a bunch of information,

12:41.000 --> 12:44.000
like knowing that the scene is changing, for example,

12:44.000 --> 12:47.000
things like that, that can really help the encoders.

12:47.000 --> 12:50.000
So it actually makes sense for user space

12:50.000 --> 12:53.000
to want to do its own rate control,

12:53.000 --> 12:55.000
because it can have more information,

12:55.000 --> 12:58.000
it can also implement, let's say, advanced strategies.

12:58.000 --> 13:02.000
For example, nowadays there is talk about machine learning

13:02.000 --> 13:04.000
and how to help encoders achieve better results.

13:04.000 --> 13:07.000
So things like that would make sense in user space.

13:07.000 --> 13:10.000
But we also want to support simple cases

13:10.000 --> 13:13.000
where we don't want user space to have a huge stack

13:13.000 --> 13:16.000
that is very complex, but it would really be nice

13:16.000 --> 13:18.000
to have a simple case that can be covered

13:18.000 --> 13:21.000
without so much logic in user space.

13:21.000 --> 13:24.000
So you can kind of see that there is a little bit of contradiction

13:24.000 --> 13:27.000
between these two things, and this is one of the main,

13:27.000 --> 13:31.000
let's say, topics that make creating this UAPI difficult.

13:32.000 --> 13:34.000
So let's take a look at some existing work

13:34.000 --> 13:38.000
that was already carried out for these state-based encoders.

13:38.000 --> 13:42.000
For the Hentro H1, which is probably the most popular one

13:42.000 --> 13:46.000
in this category, we have some work that was done by RockTip,

13:46.000 --> 13:51.000
which is free software, in a stack called MPP.

13:51.000 --> 13:53.000
So you can find the source code here,

13:53.000 --> 13:57.000
and that's the part where it implements encoding for the H1.

13:57.000 --> 13:59.000
So that's great, but this is not V4L2.

13:59.000 --> 14:03.000
It's a fully user space-based approach.

14:03.000 --> 14:07.000
So we have Google that did a custom V4L2 driver in Chromium OS.

14:07.000 --> 14:11.000
So this is for the Chromebooks that they ship with RockTip SOCs

14:11.000 --> 14:13.000
that have the H1.

14:13.000 --> 14:16.000
So this time it's a V4L2 driver,

14:16.000 --> 14:19.000
but it really is hardware-specific,

14:19.000 --> 14:24.000
so you have a very specific API to drive that encoder.

14:24.000 --> 14:27.000
Now, from this base, which has all the knowledge

14:27.000 --> 14:29.000
of how to drive the encoder,

14:29.000 --> 14:35.000
I was able to write a mainline-based implementation

14:35.000 --> 14:40.000
on V4L2 when I was working at Budlin,

14:40.000 --> 14:44.000
and this one is still hardware-specific.

14:44.000 --> 14:47.000
So we have some custom register configuration

14:47.000 --> 14:49.000
that is pushed to the driver,

14:49.000 --> 14:52.000
and we get some custom feedback as a result

14:52.000 --> 14:54.000
that the user space side can use

14:54.000 --> 14:56.000
in order to implement the rate control.

14:56.000 --> 15:00.000
So in this case, the rate control is done in user space entirely.

15:00.000 --> 15:03.000
Ooh, it's not working anymore.

15:03.000 --> 15:06.000
Okay, and now we have also VPH encoding from Collabora,

15:06.000 --> 15:11.000
which also does the rate control in user space.

15:11.000 --> 15:15.000
So you can find the links to the RFC series there

15:15.000 --> 15:18.000
and the user space implementation in Gstreamer as well

15:18.000 --> 15:21.000
to demonstrate how it works.

15:21.000 --> 15:23.000
So while working on this hardware,

15:23.000 --> 15:26.000
there is a few things that we learned.

15:26.000 --> 15:30.000
For example, the fact that some metadata fields of the bitstream

15:30.000 --> 15:34.000
are actually constrained by the hardware itself.

15:34.000 --> 15:39.000
So it means that there are some fields in the codec

15:39.000 --> 15:42.000
where the specification allows you to choose between different values,

15:42.000 --> 15:45.000
but the encoder actually only works with one of them.

15:45.000 --> 15:49.000
So it means that if you are going to generate those fields,

15:49.000 --> 15:52.000
you need to be aware of which hardware you're running on.

15:52.000 --> 15:56.000
So the lesson learned from that was that the bitstream generation

15:56.000 --> 15:58.000
should really be on the kernel side

15:58.000 --> 16:02.000
because this is how you can really know exactly which choices are valid

16:02.000 --> 16:05.000
or not for this particular hardware.

16:05.000 --> 16:08.000
Sometimes the hardware also has rate control helpers.

16:08.000 --> 16:10.000
So this is some hardware features

16:10.000 --> 16:13.000
that can help you implement better rate control.

16:13.000 --> 16:17.000
It's not necessarily always a good idea

16:17.000 --> 16:19.000
or always required to use them, but they exist.

16:19.000 --> 16:22.000
And in that case, this kind of suggests that the rate control

16:22.000 --> 16:24.000
would make sense to be done in user space,

16:24.000 --> 16:26.000
sorry, in the kernel side,

16:26.000 --> 16:31.000
because again, we don't want user space to be specific to a particular hardware.

16:31.000 --> 16:35.000
We want it to be generic and agnostic.

16:35.000 --> 16:40.000
Now for a second example, which is something I've worked on very recently,

16:40.000 --> 16:42.000
again, at Budlin.

16:42.000 --> 16:46.000
So this is based on some existing work from the Linux Sanctuary community,

16:46.000 --> 16:51.000
which did a lot of research and implemented some user space implementation

16:51.000 --> 16:54.000
for the all-winner video engine encoder.

16:54.000 --> 16:58.000
And I did some follow-up work on some more recent platforms

16:58.000 --> 17:01.000
that also implement H264 encoding,

17:01.000 --> 17:05.000
this time using a proper VFIL2 driver.

17:05.000 --> 17:11.000
And again, because we still didn't have the stateless encoding UAPI,

17:11.000 --> 17:16.000
I decided to use the stateful encoding UAPI more or less directly.

17:16.000 --> 17:20.000
And this made it clear that this API was quite limiting

17:20.000 --> 17:27.000
and that it didn't allow leveraging the full potential of the stateless hardware designs.

17:27.000 --> 17:32.000
So there's a few lessons to be learned from that.

17:32.000 --> 17:37.000
Like I said, stateful API is not really a good fit for these stateless encoders,

17:37.000 --> 17:41.000
so it's not really viable to try and use that.

17:41.000 --> 17:45.000
The Bitstream beta data needs to be produced canalside, like I said,

17:45.000 --> 17:49.000
because we have some hardware constraints that we cannot represent

17:49.000 --> 17:52.000
and let's say forward to user space.

17:52.000 --> 17:59.000
So it has to be the kernel that decides how to generate those Bitstream headers.

17:59.000 --> 18:02.000
And for rate control, it's really unclear

18:02.000 --> 18:05.000
because having rate control on the kernel side

18:05.000 --> 18:09.000
makes the user space quite simple and it makes it really easy to operate it

18:09.000 --> 18:12.000
without having a lot of logic aside.

18:12.000 --> 18:16.000
But on the other hand, having the rate control in user space is a lot more flexible

18:16.000 --> 18:20.000
and it means that you can implement kind of whatever strategy you want,

18:20.000 --> 18:23.000
you can decide on the implementation yourself,

18:23.000 --> 18:26.000
which is a bit less easy when it's on the kernel side.

18:26.000 --> 18:30.000
Of course, it's not impossible, it's all free software and you can change it like you want,

18:30.000 --> 18:35.000
but we still understand that there is interest in both cases.

18:35.000 --> 18:40.000
So the current state of the art for stateless encoding in VFrl2

18:40.000 --> 18:43.000
is that it's in progress, it's a discussion,

18:43.000 --> 18:48.000
so if you have an opinion on that or if you have ideas on how this could be improved

18:48.000 --> 18:51.000
or how this published quantization parameter,

18:51.000 --> 18:57.000
which references we're going to use to generate the frames based on the previews on X-frames.

18:57.000 --> 19:03.000
So having a switch kind of allows user space to choose if it wants to have low-level control

19:03.000 --> 19:07.000
or if it wants to have something simple that works, which is maybe suboptimal,

19:07.000 --> 19:10.000
but that can still be used nicely.

19:11.000 --> 19:16.000
Another way would be that we have rate control implemented in the kernel side,

19:16.000 --> 19:21.000
but instead of applying it to the next frame, it would just provide some suggestion to user space,

19:21.000 --> 19:26.000
so it would be some kind of feedback data that is provided with an indication

19:26.000 --> 19:35.000
of what the next QP or frame type could be to follow the policy that was selected kernel side.

19:35.000 --> 19:41.000
This could also work because then user space could decide to follow this feedback suggestion or not,

19:41.000 --> 19:45.000
so then it could decide to do something completely different.

19:45.000 --> 19:48.000
So in that situation, user space would still have all the low-level control,

19:48.000 --> 19:53.000
but it would have suggestions about which values would make sense

19:53.000 --> 19:57.000
depending on the kernel side rate control implementation.

19:57.000 --> 20:01.000
So that's also something that could work, and we could even have some switch to auto-apply the feedback

20:01.000 --> 20:07.000
so that user space doesn't even have to copy that suggestion into the actual configuration.

20:07.000 --> 20:10.000
We could just have a switch that makes it kind of automatically go,

20:10.000 --> 20:15.000
and after that, user space really doesn't have much to do,

20:15.000 --> 20:18.000
and it can kind of let it handle it by itself.

20:18.000 --> 20:23.000
So that would be also some form of trade-off that allows having something simple for user space

20:23.000 --> 20:33.000
and also allow user space to be able to control things if it wants to.

20:33.000 --> 20:36.000
Another thing that would be interesting is to have some common code

20:36.000 --> 20:39.000
that is shared between these different state-based encoders

20:39.000 --> 20:43.000
because especially for things like the bitstream metadata generation,

20:43.000 --> 20:46.000
there is a lot that is common, of course, because it targets the same format,

20:46.000 --> 20:50.000
so we could have some helpers that are shared between these different drivers.

20:50.000 --> 20:54.000
Again, the state-full encoders don't have to generate that bitstream metadata,

20:54.000 --> 20:58.000
so it's really something specific to the state-less encoders.

20:58.000 --> 21:04.000
Finally, the rate control implementations, if they end up existing on the kernel side,

21:04.000 --> 21:07.000
it would also be nice to be able to share them between the different drivers

21:07.000 --> 21:12.000
instead of having driver-specific implementations for that.

21:13.000 --> 21:20.000
Besides discussing and exchanging ideas and hopefully finding a solution

21:20.000 --> 21:23.000
for what this UAPI should look like,

21:23.000 --> 21:29.000
the next step will be to merge the work done on the Hentro and Cedrus drivers

21:29.000 --> 21:34.000
to bring X264 encoding and VPA encoding for Hentro.

21:34.000 --> 21:38.000
After that, the next step will be to have some G-streamer and FFM-back integration

21:38.000 --> 21:42.000
to use this UAPI, and after that, normally the rest of the world

21:42.000 --> 21:49.000
should be able to use the state-less hardware encoders, which will be great.

21:49.000 --> 21:52.000
So time is up for me. Thanks everybody for listening.

21:59.000 --> 22:02.000
Thank you for a great talk. Unfortunately, we do not have time for questions,

22:02.000 --> 22:06.000
but I really encourage everyone who has a question to just catch the speaker in the corridor.

22:06.000 --> 22:08.000
Thank you. Thanks.

