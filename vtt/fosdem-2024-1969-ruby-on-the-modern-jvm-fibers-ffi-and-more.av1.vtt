WEBVTT

00:00.000 --> 00:10.920
Our next speaker is the esteemed and very famous Charlie Nutter, so let's give him a

00:10.920 --> 00:13.200
round of applause.

00:13.200 --> 00:17.760
Alright, microphone working.

00:17.760 --> 00:19.360
Can you hear me okay back there?

00:19.360 --> 00:20.360
Alright, great.

00:20.360 --> 00:22.240
I got a lot to cover.

00:22.240 --> 00:25.200
This is going to be a retrospective of all the problems that we've had trying to get

00:25.200 --> 00:26.760
Ruby onto the JVM.

00:26.760 --> 00:30.560
And then a little status report along the way about how we're doing on making the JVM

00:30.560 --> 00:33.200
catch up with those needs.

00:33.200 --> 00:35.160
Charles Nutter, that's me.

00:35.160 --> 00:37.240
There's my contact information.

00:37.240 --> 00:40.760
Been working at Red Hat now for I think 12 years.

00:40.760 --> 00:42.160
Before that worked for Ingeniard.

00:42.160 --> 00:45.920
It was a Ruby software as a service company.

00:45.920 --> 00:51.000
And then I was at Sun for the last three years as well.

00:51.000 --> 00:55.960
So I probably won't have time for interactive QA, but if you contact me online or post something

00:55.960 --> 00:58.280
in the Matrix channel, I will definitely get to it.

00:58.280 --> 01:00.480
I want to answer all the questions.

01:00.480 --> 01:05.040
Okay, so a little brief review of JRuby here.

01:05.040 --> 01:09.000
Ruby for the JVM, not too surprising there.

01:09.000 --> 01:13.760
It runs on Java 8 currently, but because of all the cool stuff and because we've ridden

01:13.760 --> 01:20.520
the Java 8 horse into the ground, we are going to be 17 or 21 minimum next release, which

01:20.520 --> 01:22.920
should be this year.

01:22.920 --> 01:28.400
In development for a long time, running Rails since 2006 and probably 2008, we started having

01:28.400 --> 01:29.400
production users.

01:29.400 --> 01:34.680
And we're the only alternative Ruby that's really had production users during that time.

01:34.680 --> 01:39.760
There's been a few other experiments, but nothing's ever really taken as well as JRuby.

01:39.760 --> 01:47.080
Maybe the most successful off-platform language brought to the JVM, Jython and Rhino Nazhorn

01:47.080 --> 01:51.640
might give us a run for our money, but given the maintenance state of those libraries,

01:51.640 --> 01:57.240
I think we're probably currently the most successful and most widely used JVM language

01:57.240 --> 02:01.120
that never was envisioned for this platform.

02:01.120 --> 02:03.160
So we've been chasing Rails all the time.

02:03.160 --> 02:06.920
That's kind of the gold standard for whether we can say we're a Ruby implementation or

02:06.920 --> 02:08.680
not.

02:08.680 --> 02:14.400
And after about two years of good work, we managed to get Rails working back then.

02:14.400 --> 02:19.280
Running Rails tests, running CRuby's tests, running all of the different libraries, suites,

02:19.280 --> 02:21.640
as much as possible.

02:21.640 --> 02:25.240
Compliance testing for Ruby has improved over the years, but we pretty much just run everything

02:25.240 --> 02:29.400
to try and make sure that we really are compatible.

02:29.400 --> 02:33.800
And very quickly, we ran into some serious challenges trying to bring a language like

02:33.800 --> 02:40.000
Ruby to the JVM and make it also usable and perform well.

02:40.000 --> 02:41.040
This is the quick summary.

02:41.040 --> 02:45.120
These are all areas I'm going to cover during this talk, so we will just blow right through

02:45.120 --> 02:46.920
here.

02:46.920 --> 02:50.960
These challenges help us grow both as a platform and as a community.

02:50.960 --> 02:55.320
They open up new worlds to Java developers, to JVM developers.

02:55.320 --> 02:59.520
They open up the potential of bringing new and unusual languages to the platform.

02:59.520 --> 03:04.080
It opens up the entire world of native libraries, native features that are out there that we

03:04.080 --> 03:06.300
don't necessarily have on the JVM.

03:06.300 --> 03:10.680
So we really need to focus on what are these challenges bringing a language like Ruby to

03:10.680 --> 03:16.480
the JVM and how can we make the JVM better to support languages like this in the future?

03:17.040 --> 03:19.480
So we'll start with strings and regular expressions.

03:19.480 --> 03:25.080
Excuse me for a moment.

03:25.080 --> 03:26.080
Okay.

03:26.080 --> 03:32.080
So one of the first things we ran into, JRuby's strings were just based on Java strings and

03:32.080 --> 03:34.400
we used Java's regular expressions.

03:34.400 --> 03:39.400
And at the time, regular expressions were being used in very unusual ways in the Ruby

03:39.400 --> 03:40.600
world.

03:40.600 --> 03:45.040
We ran into a case in an early version of Rails where they were using regular expression

03:45.040 --> 03:52.800
matching to parse HTTP requests that came in and look for, say, a mime header for an

03:52.800 --> 03:54.760
image and pull the image out.

03:54.760 --> 04:00.480
So you'd end up with a regular expression operating against a very large piece of data.

04:00.480 --> 04:04.720
And the built-in Java regular expression engine is implemented in such a way that for certain

04:04.720 --> 04:09.320
types of expressions like an alternation like this, it actually will recurse and recurse

04:09.320 --> 04:10.440
and recurse.

04:10.440 --> 04:14.760
And then very easily you can blow the stack out by feeding it too much data, just giving

04:14.800 --> 04:16.000
it too much data.

04:16.000 --> 04:18.520
To process, we'll blow it up.

04:18.520 --> 04:20.000
So we had to find other options.

04:20.000 --> 04:25.520
JRegix was an early one that worked against Java strings and we ran with that for quite

04:25.520 --> 04:26.880
a while.

04:26.880 --> 04:33.360
But eventually it came to be that the Java string itself was insufficient for us to represent

04:33.360 --> 04:36.720
Ruby's string behavior.

04:36.720 --> 04:39.200
Here's what that exception looks like.

04:39.200 --> 04:40.360
Very simple match here.

04:40.400 --> 04:45.880
It's just 10,000 of the A character followed by a B character with that same regular expression.

04:45.880 --> 04:51.760
It'll blow up on every version of JVM that's out there or anything based on OpenJDK classes.

04:51.760 --> 04:54.920
And I believe this is still an issue.

04:54.920 --> 05:03.080
So as we went forward and had to have a more robust, more robust regular expression engine

05:03.080 --> 05:08.800
that would work with a more custom type string on JRuby that matched CRuby's behavior, we

05:08.840 --> 05:15.840
ported over, or a contributor to JRuby, ported over Ruby's regular expression engine.

05:15.840 --> 05:21.280
So Oniguruma is the C library that Ruby uses for regular expression matching and ours is

05:21.280 --> 05:22.280
Joanie.

05:22.280 --> 05:26.800
It's a byte code based register machine, so there's no stack issues.

05:26.800 --> 05:29.280
It doesn't deepen the stack at all.

05:29.280 --> 05:31.800
It matches against byte arrays.

05:31.800 --> 05:35.120
And that'll be clear in a moment here why we need that.

05:35.120 --> 05:40.560
It also can do byte array matching with pluggable encodings, so regardless of what encoding

05:40.560 --> 05:45.760
those characters are in, and potentially if you want to use a different grammar for regular

05:45.760 --> 05:47.080
expression.

05:47.080 --> 05:54.280
This library was ported to characters and used by Nazhorn to do JavaScript regular expressions.

05:54.280 --> 05:58.680
They had the same sort of problems, and so they used our library but made it specific

05:58.680 --> 06:02.080
to JavaScript.

06:02.080 --> 06:06.320
So you see that I'm matching against byte arrays here and I said that strings were insufficient.

06:06.320 --> 06:11.280
Well the problem is that Ruby's string is not just one encoding, it's not just a blob

06:11.280 --> 06:15.960
of characters, it is represented as a byte array with an encoding.

06:15.960 --> 06:21.320
So within the system you can have many strings that all have different encodings and it all

06:21.320 --> 06:26.240
needs to be negotiated together when you combine them or use them against each other.

06:26.240 --> 06:28.320
So we had to follow suit essentially.

06:28.320 --> 06:34.640
We had to make a new string class for JRuby that used bytes, used a byte array, represented

06:34.640 --> 06:39.320
all the encodings, port all of the encoding logic over and the transcoding logic, which

06:39.320 --> 06:42.520
was a major piece of work.

06:42.520 --> 06:47.560
And essentially we have our own string now and we've had this for over a decade because

06:47.560 --> 06:53.160
Java strings just could not emulate all of the behaviors we needed for Ruby.

06:53.160 --> 06:58.000
This does complicate interop with Java but there are improvements coming there.

06:58.000 --> 07:02.000
So J-codings is the encoding library that we use.

07:02.000 --> 07:07.320
This provides the full set of encodings similar to what's in CRuby and the transcoding from

07:07.320 --> 07:11.240
any encoding to another encoding, which is used internally when we have two different

07:11.240 --> 07:14.600
strings come together and need to negotiate that.

07:14.600 --> 07:16.400
So where do we stand on the JVM today?

07:16.400 --> 07:20.800
Well rather than just having a character array inside strings, we do actually have a similar

07:20.800 --> 07:25.240
model now where there's a byte array but only two encodings are allowed inside that

07:25.240 --> 07:26.240
byte array.

07:26.680 --> 07:34.240
ISO 88591, which is essentially just the 128 bits of ASCII, or UTF-16, the old standard

07:34.240 --> 07:35.640
character.

07:35.640 --> 07:41.160
So this does lower our cost going to and from Ruby and Java when we are just using an ASCII

07:41.160 --> 07:46.720
range of characters, but UTF-8 would be nice to have there because most Ruby strings are

07:46.720 --> 07:51.960
going to be UTF-8 probably with at least one multi-byte character in there.

07:51.960 --> 07:57.000
So that has to all be copied over a lot more, a lot less efficiently.

07:57.000 --> 07:58.920
And Java Util Rejects does still blow the stack.

07:58.920 --> 08:02.000
I would love to see it get replaced at some point, but I don't know if there's any work

08:02.000 --> 08:03.880
being done to do that.

08:03.880 --> 08:10.320
Okay, so the next area that we ran into was that we have a nice runtime, but the performance

08:10.320 --> 08:11.320
wasn't there.

08:11.320 --> 08:16.720
We needed to be able to generate JVM bytecode from Ruby code and have it optimize like regular

08:16.720 --> 08:17.720
Java.

08:17.720 --> 08:19.720
So the interpreter was good.

08:19.720 --> 08:26.560
It was similar to Ruby 1.8 before they moved to their own bytecode runtime.

08:26.560 --> 08:28.960
It was very difficult for the JVM to optimize.

08:28.960 --> 08:32.840
We could walk through this stuff quickly and it was very easy to write as an interpreter,

08:32.840 --> 08:36.680
but you had a lot of polymorphic calls within that AST.

08:36.680 --> 08:39.560
It never really could see the optimization path through there.

08:39.560 --> 08:42.360
So we had to write a JIT.

08:42.360 --> 08:47.080
The reason that we did not just immediately start compiling all Ruby code into bytecode

08:47.080 --> 08:55.200
is because, for example, the Rails library will load into memory thousands of classes,

08:55.200 --> 08:59.320
tens of, or tens of thousands of methods, or tens of thousands of methods.

08:59.320 --> 09:04.500
That's a massive load for us to put onto the JVM when only a few hundred or a few thousand

09:04.500 --> 09:07.000
of those are ever going to be called.

09:07.000 --> 09:11.320
It also was slower for us to go straight to bytecode because the bytecode would end up

09:11.320 --> 09:16.360
being interpreted by the JVM's interpreter, which actually turned out to be slower than

09:16.360 --> 09:19.000
our interpreter after it JITs.

09:19.000 --> 09:23.120
So it made more sense for us to leave it in our interpreter until we saw it really needed

09:23.120 --> 09:29.440
JVM bytecode, and there we ended up with basically the first mixed-mode JIT runtime on top of

09:29.440 --> 09:32.120
the JVM.

09:32.120 --> 09:35.320
Later on, we did move to a more modern compiler design.

09:35.320 --> 09:40.360
We had a compiler engineer, Sabu Sastri, come in and help, and he basically helped us move

09:40.360 --> 09:46.000
a lot of the little P-Pole optimizations I was doing in my JIT up to a more modern compiler

09:46.040 --> 09:47.360
architecture.

09:47.360 --> 09:53.520
So this simplified the JIT, simplified what I had to write as far as emitting bytecode,

09:53.520 --> 09:57.560
which then let me explore performance a lot more in other ways.

09:57.560 --> 10:01.840
And then of course, as we move forward, we got invokeDynamic in Java 7.

10:01.840 --> 10:04.080
It's been steadily improving since then.

10:04.080 --> 10:07.920
It's used incredibly heavily in JRuby.

10:07.920 --> 10:13.760
If you take the bytecode of our JIT output from Ruby code, it's pretty much just stack

10:13.800 --> 10:18.120
moves and invokeDynamics for almost everything that we do.

10:18.120 --> 10:22.040
We will access local variables normally, but everything else has to have some little

10:22.040 --> 10:24.480
dynamic aspect as part of Ruby.

10:24.480 --> 10:28.600
So we use it very heavily and probably more heavily than almost any other project on the

10:28.600 --> 10:31.000
JVM.

10:31.000 --> 10:37.280
This is invokeDynamic performance over time from Java 8 up to 17.

10:37.280 --> 10:40.600
Really happy to see the performance improvements every release.

10:40.600 --> 10:43.200
It gets a little bit better.

10:43.240 --> 10:47.760
Looking at what we're doing with a more numeric algorithm, we get a bigger boost out of it.

10:47.760 --> 10:51.280
With something that's just walking a lot of objects, we're already kind of close to where

10:51.280 --> 10:56.040
Java would be on just walking an object graph, but still seeing that we do get some improvements

10:56.040 --> 10:59.800
from running invokeDynamic, making that more direct.

10:59.800 --> 11:02.960
Really cool is when we plug in a different JIT compiler here.

11:02.960 --> 11:07.400
So this is now using invokeDynamic on the Grawl JIT.

11:07.400 --> 11:12.240
And for a numeric algorithm where we're creating tons of numeric objects, we really see the

11:12.280 --> 11:16.080
impact of partial escape analysis helping us.

11:16.080 --> 11:21.520
And this is now really starting to get to the point of Java level performance for a numeric

11:21.520 --> 11:22.800
algorithm.

11:22.800 --> 11:25.480
This is the cases where it really helps.

11:25.480 --> 11:30.280
But over time, we have not seen that Grawl is generally faster and we don't generally

11:30.280 --> 11:34.840
recommend it unless you have something numeric or something that's doing a massive amount

11:34.840 --> 11:38.600
of allocation of temporary objects.

11:38.600 --> 11:42.160
So where are we today?

11:42.880 --> 11:47.960
One of the problems that we have generating individual methods or compiling at the runtime

11:47.960 --> 11:53.680
is ideally we want that compiled method to go away if the class goes away, or if it's

11:53.680 --> 11:57.920
a one-off generated method that eventually doesn't get used.

11:57.920 --> 12:01.920
So it's a class per method, and the only way to make those garbage collectible is a class

12:01.920 --> 12:04.200
loader per class per method.

12:04.200 --> 12:09.200
So every method that we JIT into the system has both a class surrounding it and an entire

12:09.240 --> 12:14.640
class loader just to work within the confines of the garbage collector.

12:14.640 --> 12:19.000
There's no other way to make garbage collectible classes right now on the JVM.

12:19.000 --> 12:22.480
There is anonymous class loader, but it's a hidden class, and we don't try to access

12:22.480 --> 12:24.280
that right now.

12:24.280 --> 12:26.240
Indie is clearly working very well.

12:26.240 --> 12:30.200
We're going to be doing more advanced call sites where we will have special case code

12:30.200 --> 12:35.040
along one fast path and then a slower dynamic path if it turns out it's not the logic we

12:35.040 --> 12:37.240
expected.

12:37.280 --> 12:41.880
It is a tricky API to use, but we have a lot of tooling that we've built around it.

12:41.880 --> 12:46.000
I've got some links to older talks of mine that go into detail on that.

12:46.000 --> 12:50.560
Okay, I think we're doing pretty good on time here.

12:50.560 --> 12:51.760
I know I talk fast.

12:51.760 --> 12:55.520
Come back to the video and do it at like half speed, and then maybe you'll catch everything

12:55.520 --> 12:56.960
that I'm trying to say here.

12:56.960 --> 13:00.960
So the next big area that we ran into was native interop.

13:00.960 --> 13:06.240
The sea ruby world really lives in a POSIX native sea environment.

13:06.240 --> 13:10.320
It's almost a DSL for writing POSIX code, really.

13:10.320 --> 13:13.600
And originally that's kind of what Mott's the creator wanted.

13:13.600 --> 13:19.520
He wanted something where he could write C, but essentially with a nice API, a nice language

13:19.520 --> 13:21.760
on top of it.

13:21.760 --> 13:28.360
So they are very heavily using JNI-like extensions to the runtime for most of their native access.

13:28.360 --> 13:32.840
This is clearly way too invasive for JRuby.

13:32.840 --> 13:36.160
It calls into internals of their object structures.

13:36.160 --> 13:41.920
It has direct access to the heap, direct access to garbage collector endpoints.

13:41.920 --> 13:46.920
Nothing that we can emulate efficiently in JNI, and we have tried.

13:46.920 --> 13:52.840
So we ended up pushing people more towards using programmatic access, like Project Panama,

13:52.840 --> 13:59.040
like libffi, rather than writing C extensions for C ruby to wrap a library.

13:59.040 --> 14:03.000
Let's just wrap the library by writing a little bit of ruby code.

14:03.000 --> 14:05.560
And so started out with the Java native runtime.

14:05.560 --> 14:11.520
It's basically our API for calling from Java down into native code and native memory.

14:11.520 --> 14:17.160
And then on top of that, porting the ruby ffi layer over with some invoke dynamic magic,

14:17.160 --> 14:21.760
try and make that all as clean and fast as possible.

14:21.760 --> 14:25.240
Java native runtime is actually a set of projects.

14:25.240 --> 14:29.680
Up at the top, jffi is the wrapper around libffi.

14:29.680 --> 14:35.200
That's where we ship about 20 different binaries in the jar for all the base platforms that

14:35.200 --> 14:37.360
we support.

14:37.360 --> 14:42.160
Libffi is in there, and we're just using standard libffi with some extra wrapper logic around

14:42.160 --> 14:43.160
it.

14:43.160 --> 14:46.620
JNR-fffi is kind of the baseline user API.

14:46.620 --> 14:51.560
If you're familiar with JNA, this is that level, where you say, I need a struct that's

14:51.560 --> 14:52.680
laid out like this.

14:52.680 --> 14:57.360
I need a function that takes these arguments, make these calls, allocate this memory.

14:57.360 --> 15:01.040
Then above that, we realize there were a lot of functions and a lot of behaviors that people

15:01.040 --> 15:05.200
were going to be rebinding over and over if we didn't provide them.

15:05.200 --> 15:11.360
So we have JNR-possicks, which is a slowly growing corpus of standard posix functions

15:11.360 --> 15:13.480
bound on top of JNR-fffi.

15:13.480 --> 15:19.480
So you can go in there and you can call things like posixpon or open a file or do native

15:19.480 --> 15:21.200
IO.

15:21.200 --> 15:26.640
You can even call fork, and it's a lot of fun to see what happens when you do that.

15:26.640 --> 15:33.560
JNR-enxio, extended native cross-platform IO, builds on JNR-possicks and provides an

15:33.560 --> 15:37.120
NIO channel that is all native down calls.

15:37.120 --> 15:43.040
So where we can't get selectable standard IO on the JVM, we can't get selectable sub-process

15:43.040 --> 15:51.040
channels, we can use JNR-enxio to have actual interactive control over standard input.

15:51.600 --> 15:53.440
Standard IO and sub-processes.

15:53.440 --> 15:58.680
You can actually use JRuby to spin up a VIM instance and it will have full console control

15:58.680 --> 16:00.720
and work properly.

16:00.720 --> 16:06.280
Basically impossible to do with the standard process builder stuff on Java.

16:06.280 --> 16:10.640
Unix socket, not too surprising, just wraps this other stuff with the Unix socket calls.

16:10.640 --> 16:15.580
And then JNR-process, like I mentioned, we have our own selectable channels for processes.

16:15.580 --> 16:21.660
You can use this as a Maven library, you pull it in and you'll have the same API as process

16:21.660 --> 16:27.180
builder but you'll get channels, selectable channels out of it instead of streams.

16:27.180 --> 16:29.900
So it's available right now for that.

16:29.900 --> 16:33.260
This is a little bit of what Ruby FFI looks like.

16:33.260 --> 16:38.300
Pretty straightforward, we're setting up a structure with particular widths of fields,

16:38.300 --> 16:42.100
attaching a function, get time of day, and then we can call it directly.

16:42.140 --> 16:47.580
Under the covers, this all uses JNR and ideally inlines as much as possible up to the native

16:47.580 --> 16:49.500
down call.

16:49.500 --> 16:52.580
So today, native interop on the JVM.

16:52.580 --> 16:57.580
Of course, we have Panama coming along, so the talk before me, Mauricio's talk, that's

16:57.580 --> 17:02.260
where all the information is about where things are going and we're really excited about that.

17:02.260 --> 17:08.180
I actually wrote the original JEP for Panama, which has now been walked away from many times,

17:08.180 --> 17:13.940
but we've been needing this for over a decade now and had to make our own but don't want

17:13.940 --> 17:15.940
to maintain it anymore.

17:15.940 --> 17:22.460
JNR is pretty much the fastest way outside of Panama to do these native down calls.

17:22.460 --> 17:29.060
In some cases, actually beating JNI because there's extensions to generate a little JNI

17:29.060 --> 17:34.020
function in memory using assembly that can cut out some of that overhead rather than

17:34.060 --> 17:37.580
just doing pure programmatic calling through lib.ffi.

17:37.580 --> 17:42.500
Jextract from Panama is coming along.

17:42.500 --> 17:48.980
We're also hoping that we can use that at runtime as a library to and access those data structures

17:48.980 --> 17:52.380
internally to generate Ruby.ffi code.

17:52.380 --> 17:57.820
This would be kind of the last mile for getting Rubyists to switch from writing C extensions

17:57.820 --> 17:59.300
to using FFI.

17:59.300 --> 18:04.180
If we could generate the Ruby.ffi code the same way we do the Panama code, there'd be

18:04.180 --> 18:07.940
nothing to stop them at that point.

18:07.940 --> 18:12.700
There is back-end work happening right now on JNR to integrate it with Panama.

18:12.700 --> 18:16.340
Michelle at Oracle is working on that and I'm hoping that we'll see something in the

18:16.340 --> 18:18.980
next couple of weeks.

18:18.980 --> 18:21.020
A little more review of some of these ideas.

18:21.020 --> 18:24.940
If we have Jextract that can generate Java code, we should be able to use Jextract to

18:24.940 --> 18:27.380
also generate Ruby.ffi code.

18:27.460 --> 18:33.100
That'll be the next big fun toy to play with is of Java 22.

18:33.100 --> 18:36.940
We also use the existing SQLite JDBC driver.

18:36.940 --> 18:40.540
Rubyists like to use SQLite for local development.

18:40.540 --> 18:42.620
But it's going through a JNI back-end.

18:42.620 --> 18:45.500
You have to make sure it's available for the platform that you're on.

18:45.500 --> 18:48.940
They are also playing with Panama behind the scenes.

18:48.940 --> 18:54.980
Early numbers look like two-ish times faster than the JNI wrapper around SQLite that they

18:54.980 --> 18:55.980
have.

18:55.980 --> 18:57.740
So this is coming along.

18:57.740 --> 19:03.220
We also are integrating a new Ruby parser called Prism, which is a simple C library

19:03.220 --> 19:08.660
that we all the implementations can share so that we are using the same Ruby parser.

19:08.660 --> 19:11.420
That we will integrate through Panama as well.

19:11.420 --> 19:17.100
And use Panama to make it much faster for us to downcall into this library, get our

19:17.100 --> 19:19.380
AST back out, and then proceed.

19:19.380 --> 19:25.420
Interestingly, we're also exploring using Prism as a Wasm-compiled library running on

19:25.420 --> 19:32.500
the chicory Wasm implementation on top of the JVM so that we can parse Ruby code using

19:32.500 --> 19:36.260
a native library even if we're not on a platform it's compiled for.

19:36.260 --> 19:38.180
And that's amazing that it works.

19:38.180 --> 19:39.780
All right.

19:39.780 --> 19:40.780
Moving along here.

19:40.780 --> 19:43.100
So lightweight threading is the next big one.

19:43.100 --> 19:48.380
Around Ruby 1.9, they introduced fibers, a coroutine-like concept, a micro thread

19:48.380 --> 19:50.500
concept.

19:50.500 --> 19:54.900
You would still have your native threads there, but they can bounce around to different fibers

19:54.900 --> 19:57.060
at any given time.

19:57.060 --> 20:03.020
And you get little structured concurrency, structured use of fibers, allows you to do

20:03.020 --> 20:06.300
multiple tasks in the same thread.

20:06.300 --> 20:10.380
There's also been a push toward structured concurrency in the Ruby world now, where fibers

20:10.380 --> 20:14.540
can wait on I.O. or make a blocking call on I.O.

20:14.540 --> 20:18.940
The runtime will see that and schedule another fiber to run in its place while it's waiting

20:18.940 --> 20:19.940
for that.

20:19.940 --> 20:25.620
So you can easily handle tens of thousands, hundreds of thousands of concurrent connections,

20:25.620 --> 20:30.740
for example, without blocking that many threads or having to write your own select loop and

20:30.740 --> 20:33.060
what not.

20:33.060 --> 20:37.740
So fibers on JRuby, without a coroutine API at the JVM level, of course, we've had to

20:37.740 --> 20:39.140
use native threads.

20:39.140 --> 20:43.060
And that clearly only scales up to a certain number of threads.

20:43.060 --> 20:46.620
With the structured concurrency example, we could have potentially thousands of fibers

20:46.620 --> 20:50.900
in the system, and it's almost impossible for us to support that with full, heavy native

20:50.900 --> 20:53.220
threads all along the way.

20:53.220 --> 20:57.260
Ruby also primarily uses internal iteration.

20:57.260 --> 21:01.660
Collections just have to implement an each method of basically a for each.

21:01.660 --> 21:05.740
And all collections in the system then expect you to pass a block of code into it.

21:05.740 --> 21:09.640
Well, how do you turn internal iteration into external iteration?

21:09.640 --> 21:14.740
You have to use a coroutine that can yield values back out while staying inside that

21:14.740 --> 21:15.740
loop.

21:15.740 --> 21:20.380
So now we've got that potential for all sorts of fibers, hundreds of thousands of fibers

21:20.380 --> 21:24.540
all over the system, just because we're iterating collections with an external iterator.

21:24.540 --> 21:31.540
I'm going to kind of blow through this because the next talk will cover fibers a bit more.

21:31.540 --> 21:34.340
The example here of handling requests on a thread.

21:34.340 --> 21:36.640
We've got a thread, a request comes in.

21:36.640 --> 21:39.980
Now it's waiting for more information, the thread's not being used.

21:39.980 --> 21:43.960
Finally we get more data, we can proceed with the rest of our request handling.

21:43.960 --> 21:50.720
With fibers, of course, we can use multiple different fibers handling different connections

21:50.720 --> 21:52.360
on the same native thread.

21:52.360 --> 21:55.760
So the request comes in, this fiber's waiting on IO.

21:55.760 --> 22:00.120
Well let's spin up another fiber that can handle the next request that comes in.

22:00.120 --> 22:03.260
And they can multiplex use of that same thread.

22:03.260 --> 22:06.280
This is what we're starting to see more and more in Ruby, and this is where it will be

22:06.280 --> 22:12.520
critical for us to have lightweight fibers, lightweight coroutines on J Ruby.

22:13.440 --> 22:19.920
Okay, so here is a little benchmark, a little example of trying to test how long it takes

22:19.920 --> 22:24.680
to spin up 100,000 fibers and run them all to completion.

22:24.680 --> 22:29.780
So they are 100,000 live fibers in the system at any given time on this benchmark.

22:29.780 --> 22:33.140
And of course as you would expect this doesn't work.

22:33.140 --> 22:39.720
We can't spin up 100,000 native threads, and it just crashes in horrific ways.

22:39.760 --> 22:44.000
I'd love to see this crash in less horrific ways, but ideally we just move away from this

22:44.000 --> 22:46.200
problem altogether.

22:46.200 --> 22:48.520
And that's where we get project loom.

22:48.520 --> 22:56.240
So JVM Today, as of 21, we now have an official API for lightweight coroutines for essentially

22:56.240 --> 23:01.800
fibers that maps almost perfectly to what we need in the Ruby world.

23:01.800 --> 23:02.880
And we've already got this integrated.

23:02.880 --> 23:07.320
We integrated it a year ago actually, and have only made minor changes along the way.

23:07.320 --> 23:13.600
I'd like to show this just to demonstrate how much work we had to do to switch from our

23:13.600 --> 23:19.760
built in native fiber, native thread fibers to the virtual thread fibers.

23:19.760 --> 23:25.440
I was shocked that this was all it took, and suddenly this benchmark actually could run.

23:25.440 --> 23:28.800
It could actually spin up all of those fibers and run them to completion.

23:28.800 --> 23:33.840
So amazing work on the loom side, and very happy with the results.

23:33.880 --> 23:39.040
Once wise, so here I drop it down to 10,000 so that I can actually try and get the threaded

23:39.040 --> 23:40.880
version to work.

23:40.880 --> 23:46.600
Clearly we're getting significant gains on passing, context switching between different

23:46.600 --> 23:50.960
fibers, because loom is just better at that, and there's a much lighter weight process

23:50.960 --> 23:55.400
for going from one fiber to another on the same thread.

23:55.400 --> 23:56.920
Not quite as fast as C Ruby.

23:56.920 --> 24:03.480
I suspect this is probably due to us relying on a very general purpose scheduler for the

24:03.520 --> 24:08.480
virtual threads behind the scenes, where we really just want to say, this fiber's done,

24:08.480 --> 24:13.480
now run this one, rather than unblock that fiber and wait for the scheduler to pick it

24:13.480 --> 24:14.480
up.

24:14.480 --> 24:17.000
I think we can make up most of this overhead.

24:17.000 --> 24:22.440
Similarly on M1, I don't know if this is general to arm or not, but this is the performance

24:22.440 --> 24:23.440
results we have.

24:23.440 --> 24:25.400
Could not get 10,000 to go on M1.

24:25.400 --> 24:29.000
I got to drop it down to like 2,000 or 3,000.

24:29.000 --> 24:34.800
The impact is a bit more here, but again I'm hoping that as loom evolves, as we use it

24:34.800 --> 24:38.080
better, we'll see improvements.

24:38.080 --> 24:40.880
Five minutes for the last section here.

24:40.880 --> 24:44.600
The classic problem with J Ruby is still startup time.

24:44.600 --> 24:48.600
If we did not have startup time, we probably would have won the Ruby war a long time ago.

24:48.600 --> 24:53.360
It's the number one, two, and three complaint about J Ruby is how much longer it takes to

24:53.360 --> 24:55.320
start up.

24:55.320 --> 24:58.480
The JBM is just not designed to start up quickly.

24:58.480 --> 25:01.840
Most of the core JD code starts in the interpreter.

25:01.840 --> 25:06.040
It takes a long time for that to optimize, and then your application can start getting

25:06.040 --> 25:07.280
fast.

25:07.280 --> 25:11.800
We make it worse because we interpret Ruby code, and then every once in a while we'll

25:11.800 --> 25:16.520
just throw more byte code at the JVM, like okay, now this call site's actually bound

25:16.520 --> 25:20.920
to a byte code method, not an interpreter, and we're just confusing the hell out of it

25:20.920 --> 25:22.640
all the time.

25:22.640 --> 25:28.480
This is one of the reasons we actually do lazy compilation to byte code, because we

25:28.480 --> 25:34.400
want to reduce the amount of overhead we force onto the JIT at the JVM level.

25:34.400 --> 25:37.240
Walk through J Ruby's architecture here quick.

25:37.240 --> 25:43.480
We have our Ruby parser, gives us our Ruby AST, we compile into our intermediate representation,

25:43.480 --> 25:46.560
interpret that for a while, and here's where it becomes mixed mode.

25:46.560 --> 25:50.560
Then eventually we will generate byte code for those methods, and then hopefully the

25:50.560 --> 25:54.920
rest of it all works and optimizes to native code.

25:54.920 --> 25:59.000
One of the early ways that we've tried to improve startup time is basically to turn

25:59.000 --> 26:05.720
most of that off, rather than turning anything into byte code, rather than even running the

26:05.720 --> 26:10.280
C2, the fast JIT in hotspot.

26:10.280 --> 26:15.520
We turn only to C1, we use the simple JIT in the JVM, and we only use our interpreter.

26:15.520 --> 26:18.680
This improves our startup time by about 2x.

26:18.720 --> 26:21.120
By far the best thing we've had so far.

26:21.120 --> 26:25.680
Now, another way that could be potentially a way to fix this would be ahead of time

26:25.680 --> 26:26.680
compilation.

26:26.680 --> 26:32.200
Of course, GrawlVM solves this very nicely for that world, but it completely disables

26:32.200 --> 26:35.040
all of the native things that we want.

26:35.040 --> 26:39.920
General purpose, invoke dynamic, and method handles just simply, essentially doesn't work.

26:39.920 --> 26:44.040
Then beyond that, we would have to pre-compile all of our code to byte code.

26:44.040 --> 26:49.440
We'd have to link it in some way that it could ahead of time compile the native.

26:49.440 --> 26:50.960
This is just not going to work for us.

26:50.960 --> 26:55.880
We're hoping that Layden will actually pick up here with a ahead of time option that can

26:55.880 --> 26:59.360
also do some dynamic stuff at runtime.

26:59.360 --> 27:00.920
Where are we today?

27:00.920 --> 27:04.800
The solutions we're looking at in the short term are mostly surrounding the checkpointing

27:04.800 --> 27:07.080
features.

27:07.080 --> 27:13.000
Checkpoint and restore in user space, the CRIU API on Linux allows us to run JRuby to

27:13.000 --> 27:18.280
a certain point, like just after startup, and then save off a copy of it that we can

27:18.280 --> 27:20.400
start quickly with later on.

27:20.400 --> 27:28.200
This is being standardized in Project Crack, an unfortunate name, but a lovely project.

27:28.200 --> 27:32.120
This is working pretty well with JRuby right now, just experimenting with it.

27:32.120 --> 27:37.160
We are still hoping that Layden with some ahead of time compilation that still enables

27:37.160 --> 27:41.240
the rest of JVM features will be our ultimate solution.

27:41.320 --> 27:47.600
You can see here, this is CRuby on the left side just doing a baseline startup.

27:47.600 --> 27:52.240
JRuby's baseline startup without our dash dash dev flag, which turns off all of the

27:52.240 --> 27:53.680
optimization.

27:53.680 --> 27:58.040
The dev flag here, not quite 2x, but giving us a good boost.

27:58.040 --> 28:00.640
Crack of course, significantly faster than all of those.

28:00.640 --> 28:05.520
We've actually gotten to a point in execution where we can start running Ruby code now,

28:05.520 --> 28:11.160
starting to get competitive with CRuby, which was essentially designed for fast startup.

28:11.160 --> 28:15.440
Same example generating a Rails app, again, getting very close to where CRuby sits on

28:15.440 --> 28:16.640
these numbers.

28:16.640 --> 28:23.760
So, wrapping up in the last minute here, JRuby is a test bed for all of these crazy

28:23.760 --> 28:25.320
JVM things that we're doing.

28:25.320 --> 28:27.240
We're pushing all of these edges.

28:27.240 --> 28:34.280
So whether you care about Ruby or not, we are the best invoke dynamic torture test.

28:34.280 --> 28:39.560
We're going to be hitting Panama extremely hard as it gets integrated into the system.

28:39.560 --> 28:45.680
All of the structural threading will be massively exercised by all of the structured concurrency

28:45.680 --> 28:47.400
stuff coming on the Ruby side.

28:47.400 --> 28:51.760
So if you're interested in helping us integrate any of these features, or if you're an implementer

28:51.760 --> 28:57.240
interested in testing these features at scale, JRuby is definitely something you should look

28:57.240 --> 28:58.880
at.

28:58.880 --> 28:59.880
This is more background.

28:59.880 --> 29:02.480
I'll let you take a quick picture of this if you want.

29:02.480 --> 29:06.640
These are talks I've done in the past that basically cover all of my many complaints about the

29:06.640 --> 29:08.000
JVM.

29:08.000 --> 29:12.600
That list of complaints gets smaller and smaller every year, thankfully.

