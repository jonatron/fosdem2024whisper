WEBVTT

00:00.000 --> 00:07.000
Welcome everyone to our next session.

00:07.000 --> 00:17.000
Thank you very much. Hello. Good afternoon. My name is Luca.

00:17.000 --> 00:27.000
By day I work as a software engineer in the Linux systems group on Microsoft where I am responsible for the operating system that runs on the Azure infrastructure.

00:27.000 --> 00:37.000
By night I am involved in various open source projects. I'm a maintainer in system D, a Debian developer, DPDK, yes maintainer, a bunch of other stuff that I consistently forget about.

00:37.000 --> 00:43.000
So I'm going to talk to you about this new feature we had in system D middle of last year called software boot.

00:43.000 --> 00:54.000
And yes, it's a new type of reboot and we're going to look at how it's implemented first and in the second part of the talk we're going to look at two demos showing that running and how it can work with containers.

00:54.000 --> 01:00.000
So if you were at all systems go, you probably saw the first half of the talk while the second half is new.

01:00.000 --> 01:07.000
So first of all, why? Why do we want a new type of reboot? Don't we have enough already? And the answer is of course is performance.

01:07.000 --> 01:19.000
So rebooting means if you have some services that are running on your system and they're providing some functionality during that window of time they are interrupted and people don't like interruptions.

01:19.000 --> 01:27.000
So that is the main motivation for this. I also know that there are some updates system that require double reboots.

01:27.000 --> 01:31.000
I've been told for example that DNF line upgrades require double reboots.

01:31.000 --> 01:35.000
So by shorting the time it takes to do this we can save something there as well.

01:35.000 --> 01:39.000
But the main use case is the first one for avoiding interruptions.

01:39.000 --> 01:46.000
So when you go from a reboot to a KX, you're saving time because you're cutting away the time it takes to reset the firmware and the hardware.

01:46.000 --> 01:50.000
So the next obvious step was to cut away the kernels at time.

01:50.000 --> 01:56.000
If the kernel is not being updated you don't need to reboot it and do all the device initialization and everything else.

01:56.000 --> 01:59.000
So we came up with the idea of soft reboot and this is what it does.

01:59.000 --> 02:03.000
It just reboots the user space portion of your Linux system.

02:03.000 --> 02:08.000
Again the goal is to minimize disruption as much as possible.

02:08.000 --> 02:12.000
So this pairs very well with image based Linux.

02:12.000 --> 02:16.000
We've been talking about image based Linux systems for a couple of years now.

02:16.000 --> 02:22.000
This works very well with it because in the system you have a single root FS which is usually read only.

02:22.000 --> 02:27.000
And then you have a UKI where your kernel is in VR and these are distinct components.

02:27.000 --> 02:29.000
They are usually updated independently.

02:29.000 --> 02:36.000
And so with a soft reboot when you don't update your kernel you can update just your root FS.

02:36.000 --> 02:40.000
Now this also pairs very nicely with kernel live patching.

02:40.000 --> 02:46.000
So on production system you can fix bugs in your kernel without rebooting by using kernel live patching.

02:46.000 --> 02:52.000
And this pairs nicely with that because you can use the system to update the user space portion of your image

02:52.000 --> 02:55.000
when you have bugs or security problems or whatever.

02:55.000 --> 03:02.000
Again we are replacing the entire user space atomically and moving into a new root file system.

03:02.000 --> 03:04.000
Now it's not only for image based systems though.

03:04.000 --> 03:12.000
This can be used for package based OSs because for example you cannot restart the D-Bus demon or broker on a Linux system.

03:12.000 --> 03:14.000
Your system will explode if you do that.

03:14.000 --> 03:22.000
So by doing a soft reboot you can save some time when your D-Bus has some security problems that needs to be fixed or what not.

03:22.000 --> 03:24.000
So let's look at how it is implemented.

03:24.000 --> 03:28.000
So as far as the kernel is concerned nothing is happening.

03:28.000 --> 03:30.000
Everything is business as usual.

03:30.000 --> 03:32.000
It doesn't see anything.

03:32.000 --> 03:35.000
It's all the same session or the same boot.

03:35.000 --> 03:40.000
So for example we have still some problems to solve, some papercasts.

03:40.000 --> 03:45.000
For example if you do journal CTL minus boot minus one you will not see the previous software boot.

03:45.000 --> 03:47.000
You see the previous full reboot.

03:47.000 --> 03:52.000
We have ideas to fix this only to do list but it's one of the fewer papercasts left to solve.

03:52.000 --> 03:55.000
Now as far as user space is concerned everything goes away.

03:55.000 --> 03:57.000
It's a normal shutdown.

03:58.000 --> 04:00.000
So system D goes through the usual phases.

04:00.000 --> 04:07.000
It starts a shutdown target, a software boot target that conflicts with everything else so all the services get stopped.

04:07.000 --> 04:19.000
And then instead of giving control back to the kernel with a Cisco to reboot it just reexact itself into the new root file system by passing the full reboot.

04:19.000 --> 04:21.000
So you can do this in place.

04:21.000 --> 04:26.000
So your software boot is in the same root file system or you prepare ahead of time the new file system.

04:26.000 --> 04:29.000
And the run next route.

04:29.000 --> 04:36.000
And we allow this because usually prepare the new root file system and position all the mounts across and whatnot take some time.

04:36.000 --> 04:42.000
So you can do this ahead of time without having to interrupt all the services by doing it in line.

04:42.000 --> 04:51.000
So you can prepare your next root of s in run next route and then code the software boot so that you transition very quickly to the next root of s.

04:51.000 --> 04:59.000
And again you can prepare your any additional storage you have if you have any encrypted partition for var for example.

04:59.000 --> 05:08.000
You can prepare it ahead of time so you don't need to redo the decryption steps which again takes some time require maybe use an interruption maybe accident tpm or whatnot.

05:08.000 --> 05:11.000
And again the kernel stays the same so no configuration changes.

05:11.000 --> 05:20.000
So in system D 254 we added a new verb system system CTL software boot to do this equivalent the bus API and the next version.

05:20.000 --> 05:28.000
We also had some new signal that tell you yet this is shut down happening and it's off type software boot.

05:28.000 --> 05:33.000
So we are cutting time away from their boot is that all we can do with this.

05:33.000 --> 05:35.000
Not quite we can go further.

05:35.000 --> 05:39.000
So given system D set doesn't exit it's reexec itself.

05:39.000 --> 05:44.000
You can carry over any state we want to the software boot.

05:44.000 --> 05:53.000
So for example the file the script of store is not aware what it is a way to store for the script or inside PID one and then it gives them back to you to your service when it starts.

05:53.000 --> 05:58.000
And by the way all these links are on the slides are used to documentation I will put the slides online.

05:58.000 --> 06:04.000
But basically your service can say hey I have an active TCP connection take the sd for me and keep it there.

06:04.000 --> 06:13.000
And then your service goes down the software would happens you come back and you get back the TCP connection you can pick up from where you left.

06:13.000 --> 06:23.000
Because the kernel just stays running the connection is not interrupted it just buffered and there's some delay of course but it doesn't have to be established for example.

06:23.000 --> 06:33.000
It's not just sockets you can use this for MFD for example for any buffer any state that is expensive to calculate you can store it in a MFD and get it back immediately.

06:33.000 --> 06:41.000
And you can do this for the network stock for example in network D we have these options so that when it goes down it leaves interfaces configured.

06:41.000 --> 06:50.000
And when you go back in the software boot in a new file system you don't have to reconfigure your network interfaces which again can be a bit slow.

06:50.000 --> 07:02.000
And then finally we transition across Zashran as a state pseudophile system or tempfs so that if services have state in Zashran they find it again when they come back.

07:02.000 --> 07:09.000
This is not recursive but and also Zashtemp is reset completely because that's a scratch area.

07:09.000 --> 07:18.000
So by doing this we can accelerate the time that the services need to go back to fully functional after a software boot.

07:18.000 --> 07:25.000
But is that all we can do again and what the hell does any of these have to do with containers is it a container dev room.

07:25.000 --> 07:34.000
So here's an idea now some payloads are completely independent of your router fest for example containers but also portable services.

07:34.000 --> 07:45.000
Now if you don't know what a portable service is I suggest to check it out they're awesome they're a way to attach a system service to your OS that runs from a different root file system.

07:45.000 --> 07:50.000
But it comes with its own image but it's fully integrated with your system services it's quite cool.

07:50.000 --> 07:59.000
But it applies to these but not only this so these these are these services these containers these payloads are independent of the root file system.

07:59.000 --> 08:05.000
So can we let them run during this software boot process the answer is well yes why not.

08:05.000 --> 08:11.000
And the configuration to that is a bit complex it's linked there I want to show it here we show it in a demo later.

08:11.000 --> 08:20.000
But basically you can configure a system service so that system you will not kill it or stop it when the software boot happens.

08:20.000 --> 08:24.000
So is the service keeps running while the router fest is updated under it.

08:24.000 --> 08:30.000
Net or is it accessible we keep it up the current doesn't go away doesn't the conflict devices same thing for the disks.

08:30.000 --> 08:37.000
So for this kind of payloads we go from some interruption to zero interruptions we quite nice.

08:37.000 --> 08:47.000
Of course there's a catch there's always a catch these payloads they really need to have nothing to do with the root file system because for example if you keep anything.

08:47.000 --> 08:55.000
And if I open for the old root file system and you will keep the resource pin and they will be free that you use more memory or whatever else.

08:55.000 --> 09:02.000
So you need to make sure they are disconnected and also other parts of the US are going away for example the bus.

09:02.000 --> 09:11.000
So in the documentation there it shows up but you need to change the way you use the bus via the SD bus library for example to automatically reconnect when it comes up.

09:11.000 --> 09:21.000
It's usually not done because the bus never goes away normally but if you have one of these payloads so virus of the boot you need to change our use the but it's very simple and it's a.

09:21.000 --> 09:23.000
Describing the documentation there.

09:23.000 --> 09:31.000
Now one thing I will look at in the near future is also if we can if we have actual bind parts from the host.

09:31.000 --> 09:38.000
The first into the services if you can automatically refresh them after software boot I'm halfway through that is all done yet.

09:38.000 --> 09:48.000
So let's see this happening with Podman now because I am a coward I did not I'm not doing a live demo I'm showing a recording.

09:48.000 --> 09:58.000
Now this is a dead end image dead end testing and it's running podman some version and so podman has this thing called a quadlet where it generates.

09:58.000 --> 10:08.000
Some system services for your container and now this is not exactly what podman generates though it's a bit different as most stuff here and we see what that is in a moment.

10:08.000 --> 10:16.000
Or you can see down here it runs a very important production use case of sleep infinity that's typical production use case everybody uses.

10:16.000 --> 10:24.000
But to show what the actual difference is because this is a demo to put it together I am not a podman developer or user.

10:24.000 --> 10:32.000
I thought it was cool to make it work and I have it a bit together so podman gives you some some systems service I change it and show you the deep here so.

10:32.000 --> 10:39.000
These settings up here are necessary to make the containers service survive the software boot.

10:39.000 --> 10:51.000
This is a bit of a hack and if this is supported by podman natively it would have to be solved in a better way but basically this ties the container to the root file system to the var directory.

10:51.000 --> 11:02.000
So I have to comment that out so that they are not tied and it doesn't get shut down and then there's four more things down here that are suspicious and we'll see what they are in a moment.

11:02.000 --> 11:08.000
Now which is simple to explain if I start this container this.

11:08.000 --> 11:16.000
Sleep service and it takes a second because it downloads the image in the background and I resolve the complaints that we don't care.

11:17.000 --> 11:18.000
Now.

11:18.000 --> 11:27.000
The way podman works when you run it as part of a system service is works correctly creates some subc groups so there is the payload.

11:27.000 --> 11:38.000
C group node and then there is an additional sidecar control service that runs as part of the same C group and is also a group is dedicated to podman.

11:38.000 --> 11:46.000
Now the reason for this for settings here is because this common binary comes from the root file system.

11:46.000 --> 11:53.000
So we need to make sure if we just do this it will keep the root file system pin that we don't want that.

11:53.000 --> 12:01.000
So my my hack to make the demo work is actually we're running on a different the service runs on a different route image.

12:01.000 --> 12:03.000
So it's another big image with podman inside.

12:03.000 --> 12:11.000
So this binary and the podman binary that runs they come from this image not from the system that way they are independent and they are not tied together.

12:11.000 --> 12:14.000
And then we disconnect a couple of things.

12:14.000 --> 12:15.000
So.

12:15.000 --> 12:23.000
So now we we have that prepared and there's other things so you saw the two C groups there.

12:24.000 --> 12:33.000
Now the way system makes marks a C group for survival of software boot is by setting these extended attribute here.

12:33.000 --> 12:40.000
Now because podman gets a delegation from this C group which is the right thing to do but we don't touch the children.

12:40.000 --> 12:48.000
We do not set these extended attribute automatically for these two payoffs and if podman wanted to support this natively it would have to do that when he sets up the C groups.

12:48.000 --> 12:54.000
Now of course again this is how to gather so I'm doing that by hand just setting the that's a group there.

12:54.000 --> 13:05.000
The extended attribute so that system we won't kill all the these processes when they are running and now we can finally type software boot and we see all the US space going away.

13:05.000 --> 13:14.000
And then shortly thereafter we come back and we get a shell and then we check with us some errors in the C H and we don't care about so just ignore them.

13:15.000 --> 13:24.000
I was too lazy to hide them and then we can see that the sleep is still running and the control monitor as well and it's the same PID is the same processes.

13:24.000 --> 13:28.000
The containers kept running while we shut down all this stuff.

13:28.000 --> 13:35.000
All the system services have been shut down and restarted but the container is just going without interruption.

13:35.000 --> 13:38.000
So yeah again this is very quickly out together.

13:38.000 --> 13:44.000
I am not a podman developer is pondering there interested into supporting this or maybe LXD developers.

13:44.000 --> 13:51.000
I'm happy to help them but this is a have to get a demo I have another one which I think is a bit more interesting.

13:51.000 --> 14:04.000
So as your boost if you're not familiar is the an offload card that is installed in every Azure node so your Azure nodes that run your virtual machines have these arms 64 offloading card that runs the operating system that I work on.

14:04.000 --> 14:27.000
It's called Azure boost and I'm showing here a demo of this recorded in production on an Azure boost that he pulls for a second now we recorded this my colleague Maya to my oh my thanks go for recording this go record this amount ago so far executives and then I asked hey can I show this in public at a conference.

14:27.000 --> 14:53.000
This is never shown before I didn't only in turn on Microsoft stuff super secret and surprisingly they went yes you're going to like what now I have to do it so I had to unfortunately blank out the host names because this is a real node somewhere in the fleet in that it's entering the US and I couldn't show the host name which identifies the node so you will see this blank things I apologize for that but I had to hide those but we are showing here let's start going again.

14:54.000 --> 15:07.000
So in Azure we are running this Microsoft operating system it is just a machine it's arm some version of the kernel 5.10 we have what we call agents these are containers running as portable services.

15:07.000 --> 15:14.000
Some of these are critical for the actual customer VMs if they go away the network is interrupted you cannot make new connections.

15:14.000 --> 15:26.000
The agent is the critical one that goes away network goes away up local is a local one that does some local service so it doesn't matter so we configure the first one that portable service to survive the software boot.

15:26.000 --> 15:43.000
And the second one will will we just go away and disappear now we attach the update agent that does the software boot you can see the portable service is just a touch as a new image so we are moving to a new image here in the background there you can see Sierra console going away.

15:43.000 --> 15:52.000
Now we switch to a new SSH because of course SSH is not a critical service like it went away issue come up in a second.

15:52.000 --> 16:09.000
And we reconnect and we will check and compare the US versions before and after kind of version before and after and check on the status of these containers and see that actually run again so yes the version and the zero three so it was zero one before so we did update the root of S.

16:09.000 --> 16:22.000
It always read only the and very to the fast so we updated as a one block the corner is the same we didn't I didn't cheat and do not show a boot there the current is exactly the same same big than everything so let's check on how these containers are doing.

16:22.000 --> 16:36.000
And we can see this is the critical one the the net agent and we compare the P. I. D. is before and after they are the same so the same process is one nine seven and two zero nine nine they're the same the same process is the same pale.

16:36.000 --> 17:01.000
It keeps running to the software with while we change the the and very to the fast image behind the other one as we started because it's it's just a non critical service so we let that that be a starter so yes this is it for the demo and hope this was interesting this Nick pick at the Azure production machines and running in.

17:01.000 --> 17:07.000
Down in the fleet and we have five meals for questions questions.

17:17.000 --> 17:18.000
Any questions.

17:18.000 --> 17:29.000
I cannot.

17:29.000 --> 17:40.000
So checkpoint restore we don't and that's a very different thing right so checkpoint restore gives you an interruption service.

17:40.000 --> 17:49.000
This doesn't so you check point and then you come back to the same state of the process but you still have an interruption while you do your update this is different this is.

17:49.000 --> 18:02.000
Aim to let us update the root file system with zero interruption for this payloads so it's a bit different and we don't have plans for that at the moment now these are a bit complex payloads so we have a look into CRU at all.

18:03.000 --> 18:04.000
Think there was.

18:05.000 --> 18:06.000
Any other questions.

18:08.000 --> 18:09.000
So I end.

18:11.000 --> 18:13.000
No questions everything clear.

18:17.000 --> 18:19.000
I don't believe that there you go there we go.

18:21.000 --> 18:22.000
I know that guy I'm gonna second.

18:22.000 --> 18:23.000
I'm gonna second.

18:32.000 --> 18:33.000
So.

18:35.000 --> 18:41.000
Excellent question now the demo was recorded in production with a custom image loaded.

18:43.000 --> 18:44.000
Thank you.

18:44.000 --> 18:55.000
The demo we show was on a production node with a custom image with a new feature we are deploying this sometimes this year so it's not yet deployed a scale we will see I'm sure it will explode in horrible ways.

18:56.000 --> 19:05.000
But for now the main thing we found was debas reconnecting to debas was the main thing that broke the services but it's easy to fix that was the main thing for now.

19:07.000 --> 19:08.000
Other questions.

19:08.000 --> 19:09.000
Going.

19:28.000 --> 19:29.000
I can't hear shout.

19:31.000 --> 19:32.000
Shout with the microphone shout.

19:38.000 --> 19:41.000
Yes so the pen is on the local system I showed it before.

19:43.000 --> 19:45.000
You need to prepare them ahead of time.

19:48.000 --> 19:49.000
From here.

19:50.000 --> 19:51.000
It can work it can work.

19:52.000 --> 19:53.000
Thank you.

