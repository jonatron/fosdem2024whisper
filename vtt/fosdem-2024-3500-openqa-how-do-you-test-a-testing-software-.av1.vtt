WEBVTT

00:00.000 --> 00:07.000
I'm going to go to the next one.

00:07.000 --> 00:12.000
It's always interesting to see people here and this group.

00:12.000 --> 00:19.000
A lot of people I've seen before and a lot of people I've seen giving talks before.

00:19.000 --> 00:24.000
Somehow, it never happened that Tina gave a talk here in Boston.

00:24.000 --> 00:29.000
She's been here before but actually she's first of all, thank you for a great welcome.

00:29.000 --> 00:37.000
Hello, hello, can everyone hear me?

00:37.000 --> 00:42.000
The microphone might not be on.

00:42.000 --> 00:46.000
What, is it on? No.

00:54.000 --> 01:03.000
Okay, alright.

01:03.000 --> 01:09.000
So I'm trying my best but remind me if I'm getting too low.

01:09.000 --> 01:17.000
So, yeah, I'm going to do a talk about OpenQA.

01:17.000 --> 01:20.000
Who of you has heard of OpenQA?

01:20.000 --> 01:22.000
Okay, a couple of.

01:22.000 --> 01:26.000
And are you using OpenQA?

01:26.000 --> 01:29.000
Okay.

01:29.000 --> 01:38.000
So I'm doing Perl since 1998 and I'm working now as an engineer through the software solutions.

01:38.000 --> 01:44.000
And I'm in the tools team where we develop OpenQA and it's written in Perl.

01:44.000 --> 01:56.000
And just to give you a little short demo.

01:56.000 --> 02:04.000
So with OpenQA you can test an installation of an operating system and you can start applications.

02:04.000 --> 02:08.000
You can pretty much test everything you want.

02:08.000 --> 02:12.000
So here you see the installation process of Open to the tumbleweed.

02:13.000 --> 02:15.000
It's not real time.

02:15.000 --> 02:17.000
That's a bit fast forward.

02:17.000 --> 02:32.000
But yeah, here it gets more boring.

02:32.000 --> 02:33.000
Okay.

02:33.000 --> 02:41.000
So it's also, it's not only used by OpenSusul but also for Fedora and Debian and actually more.

02:41.000 --> 02:46.000
I think AlmaLinux is using it.

02:46.000 --> 03:00.000
And in this talk I'm going to demonstrate you a little bit the web UI and show some relevant test API functions, the project structure and how we deploy and how we actually develop and test it.

03:11.000 --> 03:26.000
Okay.

03:26.000 --> 03:43.000
I think I'm going to sit down for...

03:43.000 --> 03:44.000
Okay.

03:44.000 --> 03:47.000
Is this readable?

03:47.000 --> 03:53.000
So here you can see all our tests and they are grouped into so-called bills.

03:53.000 --> 04:00.000
And here we have tumbleweed and on AR64, PowerPC.

04:00.000 --> 04:07.000
And then we can click on a build and see all tests of this build.

04:07.000 --> 04:10.000
There are three main states of a test.

04:10.000 --> 04:12.000
It can be passed.

04:12.000 --> 04:18.000
It can be failed or it can be soft failed and soft fail this like, okay, we know about this bug.

04:18.000 --> 04:27.000
And it's not critical for the release but we mark it as we need to look at it later.

04:27.000 --> 04:32.000
And let's look at some actual tests.

04:32.000 --> 04:37.000
So this is a tumbleweed DVD installation.

04:37.000 --> 04:47.000
And you see all these boxes, most of them are screenshots but there can also be informational things.

04:47.000 --> 04:55.000
And here, we can move through those screenshots.

04:55.000 --> 05:01.000
And here we see a screenshot of the installation where you have to choose a time zone.

05:01.000 --> 05:06.000
And we call these screenshots needles actually.

05:06.000 --> 05:10.000
So a needle is something that we want to match for.

05:10.000 --> 05:14.000
And here at the top it says 99% matching.

05:14.000 --> 05:20.000
So that means the screenshot that we got is matching our expectations to 99%.

05:20.000 --> 05:23.000
And why is that?

05:23.000 --> 05:31.000
So we have this bar here and at the left side you can see the actual needle, what we expect.

05:31.000 --> 05:34.000
And on the right side it's the actual screenshot.

05:34.000 --> 05:38.000
And you see that the font has changed a little bit.

05:38.000 --> 05:40.000
And we don't care much about that.

05:40.000 --> 05:42.000
It's still okay.

05:42.000 --> 05:55.000
And that's why we set a threshold of something like 90 something percent and it's still matching and it's okay.

05:55.000 --> 06:03.000
And here's another needle.

06:03.000 --> 06:09.000
And here you can see, so the upper area of the picture is what we want to match.

06:09.000 --> 06:17.000
And you also can see this gray area where the penguin is supposed to run around.

06:17.000 --> 06:23.000
And it's gray because we can actually go into the so-called needle editor.

06:23.000 --> 06:28.000
So we can actually live edit such a needle.

06:28.000 --> 06:33.000
And here you can see the screen area is that what we want to match.

06:33.000 --> 06:39.000
There are also some red areas and I'm sorry it's not colorblind friendly yet.

06:39.000 --> 06:50.000
But you can use some such red areas to exclude certain areas because we don't know where the penguin will be at the time of the screenshot.

06:50.000 --> 06:53.000
So we just exclude that.

06:53.000 --> 06:59.000
And then you can also review the JSON.

06:59.000 --> 07:09.000
So a needle actually consists of a picture and a JSON file that says which areas should be matched.

07:24.000 --> 07:30.000
So here's another needle and it's showing the desktop runner.

07:30.000 --> 07:34.000
And this is actually showing another purpose of a needle.

07:34.000 --> 07:39.000
We don't want to only make sure that we get what we expect.

07:39.000 --> 07:43.000
But we also need this to proceed in a test.

07:43.000 --> 07:50.000
So if I'm in the test and I tell OpenQA to send the shortcut for the desktop runner.

07:50.000 --> 08:00.000
And then immediately type something or tell it to type something then it wouldn't work because it takes a millisecond until that pop-up is actually there.

08:00.000 --> 08:06.000
And the easy way would be to just sleep one second right?

08:06.000 --> 08:10.000
Or maybe to be sure two seconds or maybe rather five.

08:10.000 --> 08:17.000
It can actually sometimes take longer because the worker on the test is running tests in parallel.

08:18.000 --> 08:23.000
So there is this function called a third screen and you can give it a timeout.

08:23.000 --> 08:25.000
So for example 60.

08:25.000 --> 08:31.000
And then it will take a picture every second until it gets this picture what we expect.

08:31.000 --> 08:34.000
And then it knows okay now I can type the command.

08:34.000 --> 08:41.000
So because otherwise if we always sleep five seconds then the test would take a long time.

08:42.000 --> 08:49.000
We can also look at the log files of the test.

08:49.000 --> 08:54.000
And settings.

08:54.000 --> 09:01.000
And here we have all these job groups so you can see what kind of stuff we are testing.

09:01.000 --> 09:07.000
We are actually testing OpenQA itself.

09:07.000 --> 09:18.000
And actually, yeah I don't have any screenshots here.

09:38.000 --> 09:44.000
And here you can see a screenshot of OpenQA inside of OpenQA.

09:44.000 --> 09:49.000
So we use our own software to test ourselves.

09:58.000 --> 10:01.000
Okay so that was the demo.

10:02.000 --> 10:04.000
Here are some code examples.

10:04.000 --> 10:09.000
So here you can see the assert script run call for example.

10:09.000 --> 10:16.000
Which is just sending some text to run and it's asserting that the exit code is zero.

10:16.000 --> 10:20.000
It also has a timeout.

10:20.000 --> 10:23.000
And this is the job group configuration.

10:23.000 --> 10:25.000
We use YAML for that.

10:25.000 --> 10:32.000
The YAML can be huge and so we are actually using the YAML merge key to avoid duplication.

10:38.000 --> 10:41.000
Okay so far about the demonstration.

10:41.000 --> 10:46.000
And yeah these are the test API functions we have.

10:46.000 --> 10:50.000
The most relevant you would probably like to know is you can send a key.

10:50.000 --> 10:53.000
You can also send a key until a needle matches.

10:53.000 --> 10:57.000
So something like enter until you get into the BIOS.

10:57.000 --> 11:03.000
You can have screenshot related functions.

11:03.000 --> 11:05.000
You have the mouse functions.

11:05.000 --> 11:08.000
So mouse drag is a function.

11:08.000 --> 11:10.000
And click and double click.

11:10.000 --> 11:17.000
What we don't yet have is you cannot see the cursor, the pointer moving.

11:18.000 --> 11:24.000
So if you want to use it for demonstration which is actually a very good use case for OpenQA.

11:24.000 --> 11:29.000
Just demo your software with writing a test and having a demo at the same time.

11:29.000 --> 11:32.000
But you don't see the mouse pointer moving yet.

11:32.000 --> 11:38.000
I have proof of concept pull request for that but hasn't gotten in yet.

11:38.000 --> 11:44.000
So and you can even write test modules in Python now.

11:45.000 --> 11:49.000
But that's boring for you because you're in the pull and drag room.

11:49.000 --> 11:52.000
And this is how it would look like.

11:52.000 --> 11:56.000
So we have all these functions like send keys and set var help.

11:56.000 --> 12:00.000
I'm in a Python script, trapped in a pull script.

12:00.000 --> 12:05.000
And okay now to the project structure.

12:05.000 --> 12:07.000
It's split in two parts.

12:07.000 --> 12:11.000
O S O 2 inst which is a name.

12:11.000 --> 12:15.000
I don't like much because it's hard to type and pronounce.

12:15.000 --> 12:18.000
That's the actual code that's running the test.

12:18.000 --> 12:23.000
It started with this project and OpenQA is all the stuff around it.

12:23.000 --> 12:33.000
For viewing the test, configuring, worker schedule, managing assets like ISO files and Q-CAL files,

12:33.000 --> 12:36.000
API and WebSocket.

12:37.000 --> 12:45.000
And it all started in 2009 by Bernhard Wiedemann working at SUSE.

12:45.000 --> 12:50.000
And our code is using Modulicious by now.

12:50.000 --> 12:56.000
Using as the HTTP user agent and for the web server and the classes.

12:56.000 --> 13:04.000
We're using DBIX class and it's really helpful.

13:04.000 --> 13:08.000
We're using now subroutine signatures.

13:08.000 --> 13:14.000
In our tests we use test warnings to avoid that we actually get any unexpected warnings.

13:14.000 --> 13:19.000
We're using test mock module, mock object and mock time.

13:19.000 --> 13:30.000
And for the tidiness we are using pearl critic and pearl tidy and develop cover of course.

13:30.000 --> 13:37.000
But we also have a lot of JavaScript, Python and shell code.

13:37.000 --> 13:42.000
O S O 2 like I said, that's the heart of the software actually.

13:42.000 --> 13:46.000
It's called ISO2Video, the main script.

13:46.000 --> 13:49.000
It takes an ISO and makes a video.

13:49.000 --> 13:59.000
When you develop a test you can actually run it directly if you have an ISO file and some vars.

13:59.000 --> 14:06.000
Then you can actually start VNC viewer to watch what's happening and also change something.

14:06.000 --> 14:12.000
If your test is actually bad and you have to want to try out stuff.

14:12.000 --> 14:15.000
And our deployment is fully automated.

14:15.000 --> 14:23.000
We just merge pull requests and then, okay, with every new commit,

14:23.000 --> 14:31.000
the open SUSE build service will fetch the new commit.

14:31.000 --> 14:40.000
And then we also do a separate update on the web UI regularly and on the workahouse.

14:40.000 --> 14:51.000
And necessary service restarts are happening and database changes will also be done automatically thanks to DBIX class deployment feature.

14:51.000 --> 14:57.000
The open SUSE build service, it's used for all open SUSE packages.

14:57.000 --> 15:09.000
It can build RBM and also other packages.

15:09.000 --> 15:18.000
So here we have all our related packages to open QA.

15:18.000 --> 15:20.000
And how about testing?

15:20.000 --> 15:29.000
So here in open QA we have 98% code coverage and for OS outer hints we have 95.

15:29.000 --> 15:32.000
So how did we achieve such a high test coverage?

15:32.000 --> 15:34.000
We cheated.

15:34.000 --> 15:39.000
Well, at least we do cheat a little bit if you look at that.

15:39.000 --> 15:46.000
So there's this feature about Devil Cover which lets you add this comment, uncover the statement.

15:46.000 --> 15:56.000
And we have a couple of them and most of them are actually just, thank you, in the test directory.

15:56.000 --> 16:03.000
Yeah, but compared to 37,000 lines, I think that's okay.

16:03.000 --> 16:09.000
And here's the coverage trend we get from CodeCov.

16:10.000 --> 16:15.000
And yeah, our general tests are under T and then we have API tests, UI tests.

16:15.000 --> 16:22.000
We are using Selenium currently but we consider use changing to Playwright.

16:22.000 --> 16:27.000
And yeah, the tests are actually included in the coverage.

16:27.000 --> 16:34.000
So, and yeah, we also use open QA to test open QA.

16:34.000 --> 16:36.000
I showed you that.

16:37.000 --> 16:48.000
Some of our tests are forking and yeah, ideally everything should be turned into a unit test where I can, where I don't need to fork,

16:48.000 --> 16:59.000
but still Devil Cover is able to do that if at the end of the process you add this line and then the coverage of the fork will also be collected.

17:00.000 --> 17:06.000
And CodeCov will complain if pull regret adds uncovered code.

17:06.000 --> 17:14.000
It will also complain if the percentage goes below a certain threshold and some directories are actually already marked as fully covered.

17:14.000 --> 17:19.000
So, if there's any line that goes uncovered there, it will also complain.

17:19.000 --> 17:29.000
And since we are using the Merify bot, then nothing can get merged if it's failing some of those tests.

17:29.000 --> 17:36.000
And so you need two approvals and no failing test and then it gets merged automatically.

17:36.000 --> 17:41.000
And that's working quite well.

17:41.000 --> 17:45.000
But checking CodeCov might not be enough.

17:45.000 --> 17:57.000
You know, having 100% CodeCov, which doesn't guarantee you anything, well a little bit, but so pull request authors are encouraged to add new tests for every pull request.

17:57.000 --> 18:03.000
And writing tests is seen as a part of every ticket we work on.

18:03.000 --> 18:14.000
And refactoring is also encouraged and for every regression we encourage people to think what we could do to prevent similar things in the future.

18:14.000 --> 18:17.000
And yeah, I showed you that already.

18:17.000 --> 18:23.000
And okay, I don't know how many times we have a question, but that's it for me. Thank you.

18:31.000 --> 18:34.000
Any questions? One minute.

18:37.000 --> 18:41.000
Okay, no questions? Then, alright.

18:44.000 --> 18:47.000
Thank you.

