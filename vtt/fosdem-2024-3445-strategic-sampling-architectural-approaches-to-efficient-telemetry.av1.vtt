WEBVTT

00:00.000 --> 00:14.640
Yeah, so welcome to this presentation about Open Telemetry and Sampling.

00:14.640 --> 00:16.760
Today with me is Julius.

00:16.760 --> 00:18.760
Hi everybody.

00:18.760 --> 00:21.440
Yeah, so we have to pass around the microphone.

00:21.440 --> 00:22.440
I'm Julius.

00:22.440 --> 00:25.440
I work at Cisco mainly in the telco domain.

00:25.440 --> 00:29.040
I'm doing a lot of infrastructure, cloud related stuff.

00:29.040 --> 00:34.240
Lately I had the task to add Open Telemetry to our stack and yeah, that's why I'm here

00:34.240 --> 00:37.120
talking about Open Telemetry.

00:37.120 --> 00:41.720
One second.

00:41.720 --> 00:43.200
Okay.

00:43.200 --> 00:44.600
Yeah, my name is Pinar.

00:44.600 --> 00:49.080
I work at RATED mainly on observability tools and they also have the chance to contribute

00:49.080 --> 00:51.520
to Open Telemetry.

00:51.520 --> 00:54.640
And what we discussed today is basically a quick recap for those who haven't been in

00:54.640 --> 00:57.560
the room earlier about Open Telemetry.

00:57.560 --> 01:04.160
We discussed traces, why it's important and basically how we should sample our traces

01:04.160 --> 01:06.880
and what is probably not a good idea.

01:06.880 --> 01:11.880
And then also some challenges that may occur when you would like to apply some approaches

01:11.880 --> 01:15.840
to sample your traces.

01:15.840 --> 01:17.680
So what is the Open Telemetry project?

01:17.680 --> 01:24.120
The Open Telemetry project is a project which merged between open tracing and open sensors

01:24.120 --> 01:25.120
a few years ago.

01:25.120 --> 01:32.080
It's a CNCF project which is quite fastly growing and the idea is to provide a vendor

01:32.080 --> 01:38.640
neutral data collection so that you don't get vendor locked in by some agents or you

01:38.640 --> 01:41.600
need to learn the stack always new.

01:41.600 --> 01:44.600
And the project consists out of multiple parts.

01:44.600 --> 01:47.600
There is a specification in API on SDK.

01:47.600 --> 01:52.840
So in case you would like to instrument your applications, there are auto instrumentation

01:52.840 --> 01:57.720
agents that we have seen earlier today as well as a collector which was also shown in

01:57.720 --> 02:03.920
the previous talk which is able to have different inputs, process them, send them to the back

02:03.920 --> 02:06.040
end of your choice.

02:06.040 --> 02:11.840
There are other helpers like Helmcharts and the Kubernetes operator which make our life

02:11.840 --> 02:15.480
on Kubernetes then way easier.

02:15.480 --> 02:20.760
So in this example we see a web shop which is instrumented using the Open Telemetry SDK

02:20.760 --> 02:25.200
and what we get from it, we mainly care here about the traces.

02:25.200 --> 02:28.200
For example, a user will interact with our gateway.

02:28.200 --> 02:33.120
The gateway will then enrich the context and add the tracing context on top.

02:33.120 --> 02:36.800
This one is then propagated to all our different services.

02:36.800 --> 02:41.200
Those services would then report the data that is created, for example, the spans with

02:41.200 --> 02:46.960
some metadata to a database which helps us afterwards to analyze it and visualize it.

02:46.960 --> 02:53.520
So for example, we get then this nice architecture view that we can see on the top as well as

02:53.520 --> 03:01.080
the gun chart like view on the bottom which helps us then to analyze basically how the

03:01.080 --> 03:02.080
request went.

03:02.080 --> 03:05.800
So if we have long delays, if we have some errors in between, we can just find this out

03:05.800 --> 03:09.120
and see it.

03:09.120 --> 03:15.560
So when we have a normal web shop, for example, the majority of traces is probably quite not

03:15.560 --> 03:18.120
really relevant for us.

03:18.120 --> 03:22.640
Traces of interest would be probably, let's say, transactions which have special attributes

03:22.640 --> 03:26.520
or error traces or high delays.

03:26.520 --> 03:32.120
And we should keep an eye on this because if we sample everything, this might turn out

03:32.120 --> 03:34.400
to be a bit more expensive.

03:34.400 --> 03:38.240
Here we have an example from my colleague Julius from their setup.

03:38.240 --> 03:41.920
It produces a decent amount of traces, around one million per minute.

03:41.920 --> 03:46.680
And if you would sample all the traces and directly ingest it, for example, into AWS

03:46.680 --> 03:53.760
X-ray, you would end up with around $250,000 just for sampling the traces with probably

03:53.760 --> 03:58.040
a lot of data that is not really relevant.

03:58.040 --> 04:03.160
If you would bring this down to, let's say, 0.1% or even less because you only are interested

04:03.160 --> 04:08.400
in this special attribute, some errors or some high delays, this becomes way more reasonable.

04:08.400 --> 04:14.640
We go down to $250, and if you have to explain to your manager that you would like to analyze

04:14.640 --> 04:19.080
your system and you would like to spend a quarter million, this is probably not the

04:19.080 --> 04:21.040
best idea.

04:21.040 --> 04:25.280
So the question is now, how can we bring this down to 0.1% or even less?

04:25.280 --> 04:26.440
There are some approaches.

04:26.440 --> 04:28.920
One would be head-based sampling.

04:28.920 --> 04:32.480
Using head-based sampling, the gateway in the beginning would make the decision if a

04:32.480 --> 04:34.280
trace gets sampled.

04:34.280 --> 04:39.160
This information is then propagated in the trace context, and we can then, for example,

04:39.160 --> 04:45.000
define a probability of 10% so that we would like to keep 10% of our traces.

04:45.000 --> 04:50.680
And so this is then usually configured in the SDK.

04:50.680 --> 04:56.920
So the SDK can be configured using some environment variables, which then leads to the point that

04:56.920 --> 05:00.760
we always need to restart our gateway because this is the instrumented part, as so on as

05:00.760 --> 05:04.640
the others, but the first one makes the decision.

05:04.640 --> 05:06.600
There are options to overcome this.

05:06.600 --> 05:08.160
There is the Jega remote sampler.

05:08.160 --> 05:12.280
This was originally introduced in the Jega project and the Jega agent and collector,

05:12.280 --> 05:14.720
which communicate up with each other.

05:14.720 --> 05:20.520
And basically the collector will have a list of configurations for the SDK, and therefore

05:20.520 --> 05:25.280
we can configure the SDK on the fly, which is then quite useful.

05:25.280 --> 05:33.680
There is another way to reduce the amount of traces, so we can sample always and send

05:33.680 --> 05:36.080
the data to a specific collector.

05:36.080 --> 05:40.800
And this collector will use then the probabilistic sampling processor to also bring down the

05:40.800 --> 05:46.480
amount of traces that we then finally ingest into our database or our service that whatever

05:46.480 --> 05:49.880
we take there.

05:49.880 --> 05:52.280
Another approach would be tail-based sampling.

05:52.280 --> 05:53.800
Tail-based sampling is slightly different.

05:53.800 --> 06:01.440
So here we would store the trace and all its spans that are associated to it in the

06:01.440 --> 06:05.280
collector, for example, that will receive all this data.

06:05.280 --> 06:10.720
And this collector will then make the decision if a trace should be sampled or not.

06:10.720 --> 06:13.400
And we will come to this decision later.

06:13.400 --> 06:19.360
You can define certain policies which then basically help you to determine if a trace

06:19.360 --> 06:21.200
should be stored.

06:21.200 --> 06:25.120
The setup is then a bit more complex because you add another component.

06:25.120 --> 06:32.880
And also then you will add extra cost because you need an extra collector which does this.

06:32.880 --> 06:37.280
In general we can say if we have rare events, so like there are sometimes small errors or

06:37.280 --> 06:43.240
sometimes high delays, we can definitely better capture them because we can guarantee that

06:43.240 --> 06:44.760
we have this.

06:44.760 --> 06:49.640
With head-based sampling you lose this information if we have a sampling rate of one percent and

06:49.640 --> 06:51.880
we have an error rate which is way less.

06:51.880 --> 06:55.600
This is not really working.

06:55.600 --> 06:58.360
So tail-based sampling also introduces some overhead.

06:58.360 --> 07:03.640
For this one million traces you need around these resources that are listed there.

07:03.640 --> 07:11.000
So basically an X large or two X large instance on AWS which will add another 130 to 260

07:11.000 --> 07:12.040
US dollars on top.

07:12.040 --> 07:16.800
But still when you compare the price it's still super reasonable.

07:16.800 --> 07:20.800
So the next thing is basically how would this look like in a setup when we introduce the

07:20.800 --> 07:24.600
open-talented collector with a tail-based sampler.

07:24.600 --> 07:28.680
We would have here, what you can see here is the service one which is requesting service

07:28.680 --> 07:30.880
two up to service four.

07:30.880 --> 07:36.040
And the trace context is there propagated and the trace one goes through them.

07:36.040 --> 07:40.080
In service three an error occurs so the span and the traces everything is reported to the

07:40.080 --> 07:42.160
tail-based sampling collector.

07:42.160 --> 07:46.400
And the tail-based sampling collector then will make the decision if this should be sampled.

07:46.400 --> 07:50.240
As we assume we have a policy which says we would like to store errors.

07:50.240 --> 07:54.520
In that case this one would be written to our database and then finally we can observe

07:54.520 --> 07:57.480
this trace.

07:57.480 --> 08:00.800
This is a configuration how it looks like when you deploy the tail-based sampling collector

08:00.800 --> 08:04.200
using the open-talented operator.

08:04.200 --> 08:07.800
On the left we see it's the kind open-talented collector.

08:07.800 --> 08:10.000
But the more interesting part is more in the config.

08:10.000 --> 08:14.240
In the config we define the receiver, processor and exporter.

08:14.240 --> 08:18.360
This is basically the input what we want to modify and then what we would like to export

08:18.360 --> 08:20.400
and where to send it.

08:20.400 --> 08:24.860
In the pipeline section we then glue this together and we can define the OTLP receiving

08:24.860 --> 08:27.240
part is what we would like to ingest.

08:27.240 --> 08:33.480
The processing part we have them there listed there and they will be processed in this order

08:33.480 --> 08:37.000
and then they will be exported in this case to Temple.

08:37.000 --> 08:40.640
On the right side we have more details about the tail-based sampling collector.

08:40.640 --> 08:43.160
And there we see for example the policies that are defined.

08:43.160 --> 08:47.800
There is the error-retained policy and on top without an error so it's an OR.

08:47.800 --> 08:53.080
We have another policy that says we would like to keep 10% of our traces.

08:53.080 --> 08:55.120
There are more options to configure those policies.

08:55.120 --> 08:57.040
I link them here in the slides.

08:57.040 --> 09:00.600
When you go to the GitHub page you see there's tons of.

09:00.600 --> 09:05.960
One interesting would be probably the OTLP, what is it?

09:05.960 --> 09:08.160
Scripting language name.

09:08.160 --> 09:09.160
I forgot it.

09:09.160 --> 09:10.160
Anyway.

09:10.160 --> 09:14.840
You can glue there quite complex policies if you like to.

09:14.840 --> 09:18.400
Another important thing is the numbers of traces.

09:18.400 --> 09:23.280
So you should tune the numbers of traces always together with the memory limit and the resources.

09:23.280 --> 09:27.440
The reason is with the numbers of traces we define how many traces we would like to keep

09:27.440 --> 09:34.280
in memory and the trace can have a different amount of data when so multiply spans, more

09:34.280 --> 09:36.400
metadata, less metadata.

09:36.400 --> 09:42.360
So this is basically a number which isn't useful to define the memory usage.

09:42.360 --> 09:46.000
And yeah, so therefore we should tweak the others too.

09:46.000 --> 09:48.360
And then we have the decision weight.

09:48.360 --> 09:51.880
Since we have tail-based sampling we have no defined end of a trace.

09:51.880 --> 09:56.520
This means we never can make sure that this trace ended.

09:56.520 --> 09:59.200
So there can always be a span that is reported.

09:59.200 --> 10:06.080
We should tweak it in a way that it's reasonable as short as possible but the longest we can

10:06.080 --> 10:11.880
expect or the longest trace that we might expect should be the number so that we don't

10:11.880 --> 10:15.240
end up with inconsistent traces afterwards.

10:15.240 --> 10:20.360
And especially if you have a CACD system that you are tracing, this number is probably way

10:20.360 --> 10:25.080
higher and if we had a 10 seconds value every step would be its own trace so we would then

10:25.080 --> 10:28.440
end up with inconsistency.

10:28.440 --> 10:30.880
The same when we want to replicate our setup here.

10:30.880 --> 10:35.120
You can see it's the same set of as before but now we have two collectors.

10:35.120 --> 10:39.920
What's happened here, the Kubernetes law balancer will send us to collector 1 and collector

10:39.920 --> 10:45.160
2 independently and now both collectors will do their decision on their own.

10:45.160 --> 10:48.880
So collector 1 will go and make the decision after 10 seconds and it will see in service

10:48.880 --> 10:53.720
3 there was an error span so it will sample this and write it to the database.

10:53.720 --> 10:58.920
While collector 2 and collector 4 will send the data to collector 2.

10:58.920 --> 11:05.080
This one will see the policy is not hit and it will just drop it.

11:05.080 --> 11:16.680
So next Julius will show you how to solve this too after we change the microphone.

11:16.680 --> 11:21.840
Like this, can you hear me?

11:21.840 --> 11:29.720
Right, so maybe it wasn't clear on this slide what you see here is actually a problem because

11:29.720 --> 11:34.120
one of our tail samplers decided to sample the trace while the other one decided not

11:34.160 --> 11:38.960
to sample the trace because the span with the error was only propagated to one of the

11:38.960 --> 11:40.600
samplers.

11:40.600 --> 11:47.440
So I will talk about how we can fix this by using smart load balancing in front of it.

11:47.440 --> 11:53.120
What we would like to have is a kind of load balancer which distributes all the spans that

11:53.120 --> 11:57.560
belong to one trace to a single tail sampler so that the tail sampler which is buffering

11:57.560 --> 12:01.880
all the spans can make the right decision in the end.

12:02.000 --> 12:06.600
Additionally, we would also like to create multiple instances of our load balancer so

12:06.600 --> 12:10.360
that we can scale every component because if you have a single load balancer that would

12:10.360 --> 12:12.960
be a single point of failure in our system.

12:12.960 --> 12:18.120
Yeah, so how do we do this?

12:18.120 --> 12:19.960
Introducing the load balancing exporter.

12:19.960 --> 12:25.280
So the load balancing exporter is also part of the open telemetry project.

12:25.280 --> 12:27.720
It consists of three main parts.

12:27.720 --> 12:29.880
The first one is a resolver.

12:29.880 --> 12:33.440
The resolver is responsible for finding all the upstream back ends.

12:33.440 --> 12:38.640
So in this case, this would be our tail sampling processors.

12:38.640 --> 12:40.720
There are multiple ways to do this.

12:40.720 --> 12:43.120
Usually this is done using DNS.

12:43.120 --> 12:47.560
This can be done using the Kubernetes API but it can also be done by just hard coding

12:47.560 --> 12:50.680
a list of back ends and IP addresses in there.

12:50.680 --> 12:55.160
Second, we have to define how to talk to our upstream back ends.

12:55.160 --> 12:56.480
You can define the protocol.

12:56.480 --> 13:02.600
I think at the moment there's only the OTLP supported but it's very easy to add additional

13:02.600 --> 13:07.080
protocols to that and people will do probably.

13:07.080 --> 13:10.240
The last thing we need is to define our routing strategy.

13:10.240 --> 13:16.400
So we have to define what traces go to or what the routing or the load balancing should

13:16.400 --> 13:18.960
be based on like a shouting key.

13:18.960 --> 13:24.800
By default, this is the trace ID but you can also specify it to be the Kubernetes service.

13:25.800 --> 13:27.000
Right.

13:27.000 --> 13:30.360
And the load balancing parts use a method called consistent hashing.

13:30.360 --> 13:35.520
I will not go too much into detail but this makes sure that the two load balancers, they

13:35.520 --> 13:40.800
don't have a state but they still sample or send traces with the same ID and spent with

13:40.800 --> 13:48.160
the same ID always to the identical tail sampling processor.

13:48.160 --> 13:49.960
Let's see how we can define this in code.

13:49.960 --> 13:51.800
It's pretty much what I just said.

13:51.800 --> 13:53.880
You define the routing key.

13:53.880 --> 13:58.800
You define the protocol that you would like to speak to the upstream back ends.

13:58.800 --> 14:05.040
You can define a sending queue which is acting as a buffer between the load balancer and

14:05.040 --> 14:07.800
the tail sampling back ends.

14:07.800 --> 14:09.360
You can define the resolver.

14:09.360 --> 14:11.600
In this case, we're going to use the DNS resolver.

14:11.600 --> 14:15.920
We make use of the headless service which the open telemetry operator is creating for

14:15.920 --> 14:16.920
us.

14:17.920 --> 14:22.120
It contains all the IP addresses of the parts of our tail sampling processor.

14:22.120 --> 14:27.520
And on the side, you can see the different configurations that you could use for static

14:27.520 --> 14:30.160
or Kubernetes-based resolvers.

14:30.160 --> 14:32.680
Right.

14:32.680 --> 14:36.680
So to resolve the problem, we can easily scale.

14:36.680 --> 14:41.960
There are some problems and there are some details that you have to care about when deploying

14:41.960 --> 14:44.760
the load balancing exporter.

14:44.760 --> 14:49.320
Once of all, the load balancer exporter only makes sense if you can export the traces faster

14:49.320 --> 14:51.440
than you receive them, of course.

14:51.440 --> 14:55.200
Because otherwise, the traces and the spans that will pile up and your sending queue will

14:55.200 --> 14:59.200
be at full capacity very quickly.

14:59.200 --> 15:04.560
There is a lot of metrics exported also by the load balancer exporter which you can observe,

15:04.560 --> 15:08.760
like the queue capacity, the queue size, and also the back end latency which you can use

15:08.760 --> 15:14.000
to make sure that the load balancing is happening in an equal fashion across your back ends.

15:15.000 --> 15:19.840
If this is not doing well, it looks like this.

15:19.840 --> 15:22.840
On the top, I'm not sure if you can see this.

15:22.840 --> 15:25.840
Hmm.

15:25.840 --> 15:33.600
It's not possible to turn off the lights, unfortunately, and we didn't have time to make the slides,

15:33.600 --> 15:35.600
like, inverted.

15:35.600 --> 15:37.600
Right.

15:37.600 --> 15:39.200
Okay.

15:39.200 --> 15:44.200
The problem is the queue capacity is always reached and traces will be dropped in that

15:44.200 --> 15:45.200
case.

15:45.200 --> 15:50.120
So it's not enough to make a big buffer between your two connections or between the connection.

15:50.120 --> 15:54.120
You need to make sure that you're actually exporting or writing traces faster than you're

15:54.120 --> 15:57.960
receiving because eventually they will overflow this queue.

15:57.960 --> 16:01.800
In our case, the problem was the frottling on the CPU, basically, because the tail sampling

16:01.800 --> 16:07.880
processors are very CPU intensive and also memory intensive.

16:07.880 --> 16:12.440
The next problem that we have is that we need to load balance the connection to our load

16:12.440 --> 16:13.440
balancers.

16:13.440 --> 16:15.280
So it sounds a bit weird, right?

16:15.280 --> 16:22.520
What you can see on the bottom chart is what happens if you just point your services at

16:22.520 --> 16:26.480
two load balancing exporters and let Kubernetes do the load balancing.

16:26.480 --> 16:30.280
You can observe that one of the exporters is handling, I think, it's 4,000 while the

16:30.280 --> 16:34.040
other one is only doing half of that.

16:34.040 --> 16:38.920
People who saw this before know it's probably GRPC connections, which are very long running

16:38.920 --> 16:43.320
and Kubernetes doesn't like to load balance that.

16:43.320 --> 16:44.320
What can we do instead?

16:44.320 --> 16:47.880
We can use the OTRP HTTP protocol instead.

16:47.880 --> 16:49.800
So this is using HTTP one.

16:49.800 --> 16:53.160
So the load balancing will be working as intended.

16:53.160 --> 16:57.160
However, we lose a bit of the efficiency of HTTP two.

16:57.160 --> 16:58.760
The headers will be recent all the time.

16:58.760 --> 17:01.960
The connection cannot be reused.

17:01.960 --> 17:07.760
As an alternative, we could also use some level seven load balancing, NVoy, basically

17:07.760 --> 17:14.120
instead of just passing the request to a different backend, we can inspect what's going on, take

17:14.120 --> 17:21.960
the individual GRPC packets which are flowing and route them to the different load balancers.

17:21.960 --> 17:22.960
NVoy is a beast.

17:22.960 --> 17:24.760
You need to understand it.

17:24.760 --> 17:25.880
You need people to maintain it.

17:25.880 --> 17:26.880
You need to deploy it.

17:26.880 --> 17:29.880
And so this is a more complex setup.

17:29.880 --> 17:34.760
And the last option we could see here is to deploy open telemetry in a sidecar mode.

17:34.760 --> 17:38.320
So what we saw before is basically running in the deployment mode.

17:38.320 --> 17:40.680
We have a central collector.

17:40.680 --> 17:43.000
Everybody is sending the traces to it.

17:43.000 --> 17:48.120
And in the sidecar mode, we deploy a collector alongside our ports.

17:48.120 --> 17:51.960
So if you have 10 ports, you would also need like 10 collectors which are running.

17:51.960 --> 17:52.960
You see the problem.

17:52.960 --> 17:56.040
You need more capacity on your cluster.

17:56.040 --> 17:58.920
And there's a bit of overhead there.

17:58.920 --> 18:01.880
Right.

18:01.880 --> 18:06.960
Last point we're going to talk about is autoscaling which doesn't exist yet in the open telemetry

18:06.960 --> 18:10.040
framework.

18:10.040 --> 18:13.920
Where I come from, our traffic is very depending on the time of day.

18:13.920 --> 18:18.240
So in the night, nobody is sending SMS, nobody is calling, whatever.

18:18.240 --> 18:21.640
But during the day, we have high peaks of load.

18:21.640 --> 18:27.280
So ideally, we will also like our tail sampling operation to be slower at night while it can

18:27.280 --> 18:31.560
scale up during the day so we can save on the resources which can be pretty enormous,

18:31.560 --> 18:34.400
as you saw.

18:34.400 --> 18:37.040
Right.

18:37.040 --> 18:41.200
And also what we observe is that the errors that we would like to catch usually happens

18:41.200 --> 18:46.320
during the times where there is the highest load because then things go wrong.

18:47.320 --> 18:48.320
Yeah.

18:48.320 --> 18:51.520
And here you can see that the amount of SMS send correlates with the amount of received

18:51.520 --> 18:52.520
spans.

18:52.520 --> 18:54.320
I'm not sure if you can see it.

18:54.320 --> 18:55.640
Okay.

18:55.640 --> 19:03.200
So what me and Bina did is we set out on a mission to kind of build a thing about load

19:03.200 --> 19:07.360
balancing, autoscaling solution that we can tie together.

19:07.360 --> 19:15.640
What we came up with is using some sort of a Kafka intermediate stage between the tail

19:15.640 --> 19:19.160
samplers and our load balancers.

19:19.160 --> 19:22.920
Doing some autoscaling on Kafka is a well-known problem and it's solved.

19:22.920 --> 19:27.200
So that was like a good fit for us.

19:27.200 --> 19:32.200
We had the idea to make the tail samplers whenever they come up, create a topic on our

19:32.200 --> 19:36.720
Kafka cluster representing this tail sampling processor.

19:36.720 --> 19:41.960
And the load balancer, on the other hand, will do list topics on that Kafka cluster,

19:41.960 --> 19:43.840
let's say every five seconds.

19:43.840 --> 19:47.280
So when we create the tail sampler, our load balancer will know about it because it can

19:47.280 --> 19:49.320
see the topic.

19:49.320 --> 19:53.080
And what the load balancer will then do, it will do the same thing.

19:53.080 --> 19:59.400
Instead of routing it to a different HTTP endpoint or different IP address, we will

19:59.400 --> 20:03.920
route the traces and the spans to a different topic on our Kafka cluster.

20:03.920 --> 20:07.320
So you have multiple tail samplers all listening to a single topic while the load balancer

20:07.320 --> 20:11.040
is just rebooting to all those different topics.

20:11.160 --> 20:14.640
Right.

20:14.640 --> 20:20.480
That's basically the written description of the image that you saw before plus the configuration

20:20.480 --> 20:22.960
options which we added in green.

20:22.960 --> 20:28.520
So we added to the load balancing exporter the Kafka protocol so now it can speak Kafka

20:28.520 --> 20:32.320
and send stuff using Kafka.

20:32.320 --> 20:33.320
You can defa...

20:33.320 --> 20:35.320
Sorry, I started in the wrong order.

20:35.320 --> 20:39.120
The first one is the resolver which does the list topics call.

20:39.120 --> 20:45.640
Then we have the protocol and finally on the receiver side, we have a similar topic,

20:45.640 --> 20:47.000
a similar setup.

20:47.000 --> 20:49.200
The only addition here is that we have the create topic call.

20:49.200 --> 20:56.080
So whenever the receiver starts, it will create the topic automatically for us.

20:56.080 --> 21:00.360
So we put this to a test, very basic test, Docker compose.

21:00.360 --> 21:06.240
I think we spun up like one load balancer and two receiver pods, sent some traces.

21:06.240 --> 21:10.760
And what we could see is that the setup is indeed working as intended.

21:10.760 --> 21:17.080
So the load balancing works 50-50, the traffic is split perfectly and we can also see that

21:17.080 --> 21:22.160
the topics are automatically created for us.

21:22.160 --> 21:25.560
If you think this through, you can see that there could be like a lot of problems.

21:25.560 --> 21:27.120
What happens if you want to scale down?

21:27.120 --> 21:28.280
What happens to the topics?

21:28.280 --> 21:29.400
Will there be cleanup?

21:29.400 --> 21:32.600
What if I clean up or delete a topic but there's still stuff in it?

21:33.080 --> 21:37.200
Do I have to think about, I don't know, that letter queues for the traces and spans that

21:37.200 --> 21:38.200
I missed?

21:38.200 --> 21:41.920
So definitely if you want to run this or deploy something like this in production, you have

21:41.920 --> 21:48.720
to put a lot more thought into this than hacking together, I don't know, 500 lines of code.

21:48.720 --> 21:49.720
Right.

21:49.720 --> 21:56.920
Yeah, the slide with the GitHub link is gone so you cannot find the code.

21:57.920 --> 22:00.600
Right.

22:00.600 --> 22:03.240
Which brings us to the conclusion.

22:03.240 --> 22:05.640
Quick recap, traces are valuable.

22:05.640 --> 22:08.400
If you have a complex system, they get even more valuable.

22:08.400 --> 22:10.440
Not all the traces are equally valuable.

22:10.440 --> 22:15.360
If you have a bunch of 200 or case, nobody's really interested in them.

22:15.360 --> 22:19.560
So you have to focus on the traces which are relevant for you or the ones that you're interested

22:19.560 --> 22:21.320
in.

22:21.320 --> 22:23.840
You can use head-based sampling or tail-based sampling.

22:23.840 --> 22:28.200
Head-based sampling is very easy and cost-effective but you cannot put all the configuration that

22:28.200 --> 22:29.840
you can do in tail-sampling.

22:29.840 --> 22:33.880
Tail-sampling helps to focus you even better.

22:33.880 --> 22:38.400
If you do tail-based sampling, you need to think about load balancing at the same time.

22:38.400 --> 22:39.400
Yeah.

22:39.400 --> 22:40.400
Right.

22:40.400 --> 22:45.280
And the last point with the proof-of-concept that we did was just to show it's very easy

22:45.280 --> 22:50.800
to extend the open telemetry framework with minimal code to achieve some custom solutions.

22:50.800 --> 22:56.840
So you have Kafka, you can just bring open telemetry or make it match to whatever you're

22:56.840 --> 22:58.760
running in production.

22:58.760 --> 23:01.000
All right.

23:01.000 --> 23:03.080
Thank you everybody for your attention.

23:03.080 --> 23:11.200
Thank you.

23:11.200 --> 23:12.360
Thanks for the talk.

23:12.360 --> 23:18.280
You mentioned that you could switch from GRPC to HTTP so that you would get a new connection

23:18.280 --> 23:19.280
every time.

23:19.280 --> 23:20.280
Yeah.

23:20.280 --> 23:26.160
So you could switch from the GRPC connection from time to time and let it reconnect.

23:26.160 --> 23:28.040
The GRPC connection should reconnect?

23:28.040 --> 23:29.040
Yeah.

23:29.040 --> 23:35.080
Yeah, you can do that every five minutes, for example, just to mean that the client side.

23:35.080 --> 23:36.080
Right.

23:36.080 --> 23:46.000
So that's something that had to go into the OTLP receiver side, which then, so the exporter

23:46.000 --> 23:48.360
side will then after five minutes just terminate the connection.

23:48.360 --> 23:49.360
Yeah.

23:49.360 --> 23:56.360
I was not aware that you can configure this in open telemetry.

23:56.360 --> 24:02.440
But Kubernetes has this tendency to reallocate the game, the connection in the same node

24:02.440 --> 24:03.440
anyways.

24:03.440 --> 24:04.440
Okay.

24:04.440 --> 24:07.320
If that's true then HTTP doesn't help.

24:07.320 --> 24:10.000
Good point, yeah.

24:10.000 --> 24:16.000
Any other questions?

24:16.000 --> 24:23.720
Yeah, thank you for the talk.

24:23.720 --> 24:26.640
So on the receiver there is the max connection age.

24:26.640 --> 24:32.880
So you can, you know, and I have a question for long running traces.

24:32.880 --> 24:37.560
So there is, you mentioned there is a limit like a timer.

24:37.560 --> 24:42.640
You can set like 10 seconds and then the sampling decision will be made.

24:42.640 --> 24:44.440
Is there a way to do it dynamically?

24:44.440 --> 24:52.360
So let's say I see a tag from, let's say Kafka or something that takes longer to process.

24:52.360 --> 24:59.840
And those traces would be sampled with a timer that is, I don't know, longer than the default.

24:59.840 --> 25:02.640
As far as I know, there's no dynamic timing.

25:02.640 --> 25:04.440
It's just a hard coded value.

25:04.440 --> 25:05.040
It was the number.

