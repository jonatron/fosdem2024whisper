WEBVTT

00:00.000 --> 00:04.900
Hi everyone.

00:04.900 --> 00:07.320
Thanks for that.

00:07.320 --> 00:10.200
Sorry about that delay.

00:10.200 --> 00:14.440
I have a great privilege to introduce Benjamin to us.

00:14.440 --> 00:16.280
Everyone give Benjamin a round of applause.

00:16.280 --> 00:17.520
Thanks.

00:17.520 --> 00:18.800
Okay.

00:18.800 --> 00:26.480
So today I'm going to introduce to you Re-finals which is a micro framework we developed at

00:26.480 --> 00:27.340
Fine Grand.

00:27.340 --> 00:34.340
And I would like also in this talk to give you, to inspire you to, if you're deaf or

00:34.340 --> 00:39.740
you're not into ML or you haven't trained a model before, I think what I'm going to

00:39.740 --> 00:46.220
show you today is a great way to start and get started with ML training.

00:46.220 --> 00:49.620
So why is it so daunting right now to train ML?

00:49.620 --> 00:55.660
I think that's because we're in a phase called, we could call like the foundational models

00:55.660 --> 01:03.180
phase where those huge companies are training big, big models and it's really hard to do.

01:03.180 --> 01:09.020
And the goal of course when training those foundational models will be to reach the AGI.

01:09.020 --> 01:15.980
So I have a very weak and restrictive definition of AGI but I have a 100% scientifically accurate

01:15.980 --> 01:20.780
definition which is we're going to reach AGI when everyone in this room is going to lose

01:20.780 --> 01:25.340
its job and become unemployed engineers.

01:25.340 --> 01:29.900
In the meantime, sorry, what can we do?

01:29.900 --> 01:33.540
So generally we do two kinds of things, right?

01:33.540 --> 01:39.100
So you say you can either become a prompt engineer, you don't touch ML, you're just

01:39.100 --> 01:44.940
relying on an external API or something like that to try to make the foundational model

01:44.940 --> 01:51.980
do what you want it actually to do or you can like do the training and it's very good

01:51.980 --> 01:56.540
for bragging but you cannot build it in open source because it's really costly, you need

01:56.540 --> 01:59.460
GPUs, you need data and it's risky.

01:59.460 --> 02:03.460
Like if you want to actually solve something, you train your MLM, you don't know if it's

02:03.460 --> 02:06.340
going to solve it.

02:06.340 --> 02:12.220
So there is a third way in between prompt engineering and training for a national model

02:12.340 --> 02:15.180
and the idea is the idea of adapters.

02:15.180 --> 02:19.780
So an adapter is just a way to patch a foundational model.

02:19.780 --> 02:26.100
So generally a foundational model or like chains of transformer layers and then for instance

02:26.100 --> 02:34.380
you could just inject some new weights into it and freeze the rest and you train those.

02:34.380 --> 02:42.100
And the advantage is that you require a lot less VRAM and you use the foundation on which

02:42.140 --> 02:48.900
the foundation model is built to train something very powerful and the idea is that you get

02:48.900 --> 02:54.540
a lot more flexibility than using prompt engineering.

02:54.540 --> 02:58.060
So it's something that's really exploding right now.

02:58.060 --> 03:03.060
So for instance that's a list of all adapters that exist for large-rengorge models.

03:03.060 --> 03:05.060
There's a lot.

03:05.060 --> 03:10.980
Also for generative image, there's a lot of adapters that allows you to generate images

03:11.020 --> 03:16.660
exactly like you want and unlock new possibilities that for instance table diffusion, the foundational

03:16.660 --> 03:18.500
model cannot do.

03:18.500 --> 03:22.820
And like every week you get two new papers on that domain.

03:22.820 --> 03:27.020
So it's really, really exploding right now.

03:27.020 --> 03:33.100
Okay, so let's talk a bit about why did we do that?

03:33.100 --> 03:40.380
So most of AI codes today is written in PyTorch, which is an imperative language to write deep

03:40.420 --> 03:41.700
learning model.

03:41.700 --> 03:47.020
It's very convenient because the only thing you have to do is to write your operation

03:47.020 --> 03:52.780
in a procedural way like with the NumPy API and everything works for you.

03:52.780 --> 03:54.140
And that's really great.

03:54.140 --> 04:01.340
But the issue is that when you want to patch or modify your model, you cannot do it because

04:01.340 --> 04:04.140
the code is already written.

04:04.140 --> 04:09.500
So you could monkey patch it, but you get into huge complications when you have multiple

04:09.540 --> 04:14.420
adapters or you want to compose them, et cetera.

04:14.420 --> 04:19.180
So we wrote yet another machine learning framework to solve that.

04:19.180 --> 04:20.900
So maybe that's not a good idea.

04:20.900 --> 04:24.460
So we wrote a micro framework.

04:24.460 --> 04:29.900
And what we mean by that is that Refiners is built on top of PyTorch.

04:29.900 --> 04:32.740
So everything is intercompatible.

04:32.740 --> 04:36.580
So if you train a model with Refiners, it's going to work in PyTorch.

04:36.620 --> 04:42.900
And if you have some elements you have already in PyTorch, it's going to work in Refiners.

04:42.900 --> 04:49.900
So Refiners is based on three key concepts, the chain, the context, and the adapter.

04:49.900 --> 04:55.620
And I'm going to go through each one to show you a bit how it works.

04:55.620 --> 05:01.460
So the idea of a chain is instead of writing just operation by operation in your code,

05:01.460 --> 05:08.060
you write each model as three of different layers.

05:08.060 --> 05:13.260
And what you get is that it's easy to edit dynamically.

05:13.260 --> 05:18.100
And it's completely explicit because when you see the graph of computation, you know

05:18.100 --> 05:19.060
what your model do.

05:19.060 --> 05:20.980
You don't have to look at the actual code.

05:20.980 --> 05:23.380
You just have to look at the model.

05:23.380 --> 05:26.900
So here's a comparison of PyTorch code.

05:26.900 --> 05:30.700
So you define your layers and you write all the operations.

05:30.700 --> 05:38.220
And on the right, what you will do with Refiners, where you just put each layer one after another.

05:38.220 --> 05:41.100
So that seems very basic.

05:41.100 --> 05:47.820
But so what you get is we have a very good representation of everything.

05:47.820 --> 05:52.420
And now you have a lot of helpers to help do some operation on it.

05:52.420 --> 05:56.380
So for instance, you can wrap model, pop them up, add some others.

05:56.380 --> 05:59.540
And then when you look at the rep of the model, everything is explicit.

05:59.540 --> 06:02.300
But you know what everything does.

06:02.300 --> 06:07.900
So even if you change the name of a layer, you still see that it's a chain.

06:07.900 --> 06:12.060
So now the chain is powerful.

06:12.060 --> 06:15.460
But you say, what if I want to do really complex models?

06:15.460 --> 06:19.540
Some models have some data that can pass through different layers.

06:19.540 --> 06:24.140
So we need something to simplify the flow of the chains.

06:24.140 --> 06:31.620
And so we introduce the context API that works a bit like in UI framework where you have a store.

06:31.620 --> 06:36.940
And everything nested down there can have access to that store.

06:36.940 --> 06:42.700
And so the idea is that even if you have a very nested chain, you can set the context.

06:42.700 --> 06:45.980
And then every sub-layer is going to inherit from it.

06:45.980 --> 06:51.540
And even in very complex models, you just add something like deep nested in the model.

06:51.540 --> 06:55.500
You can have access from the outside to any tensor you add to it.

06:55.500 --> 06:59.380
So for adaptation, it's very convenient.

06:59.380 --> 07:02.740
And the third and last concept is the idea of adapter.

07:02.740 --> 07:07.900
And the idea is to have an abstraction that make it easy to perform model surgery.

07:07.900 --> 07:12.580
Because when you're patching, you want maybe to add some parts, remove some parts, and

07:12.580 --> 07:15.020
let everything connected together again.

07:15.020 --> 07:19.340
So obviously, you're not going to do all the operation by hand every time.

07:19.340 --> 07:21.300
And so we have the adapter class.

07:21.300 --> 07:27.340
And the idea is, let's say, for instance, we want to target this linear, add some more logic to it.

07:27.340 --> 07:34.300
Then we can write an adapter that's going to plug itself into it.

07:34.300 --> 07:37.340
In terms of code, it just looks like that.

07:37.340 --> 07:39.460
We have a mixing called the adapter.

07:39.460 --> 07:42.220
And you can just rub it into it.

07:42.220 --> 07:46.300
And you get for free an inject method.

07:46.300 --> 07:48.500
And the inject method is going to do exactly that.

07:48.500 --> 07:50.540
We place the linear by the adapter.

07:50.540 --> 07:52.700
So for instance, this schema, it's

07:52.700 --> 07:56.260
look like the adapter that's called the Lora adapter, which is really common.

07:58.940 --> 07:59.380
OK.

07:59.380 --> 08:03.180
And so now we're using this to train new adapters.

08:03.180 --> 08:05.140
And we're doing it in the open.

08:05.140 --> 08:06.260
So you can have a look.

08:06.260 --> 08:11.740
We have a page called Boonties, where you can come and train adapters.

08:11.740 --> 08:15.940
For instance, the color palette adapter is currently being trained by someone who

08:15.940 --> 08:19.900
hasn't had ML training for monitoring before.

08:19.900 --> 08:25.700
And so if you could come and do some stuff with us, that would be really nice.

08:25.700 --> 08:26.260
Thank you.

08:26.260 --> 08:27.540
Thank you for listening.

08:27.540 --> 08:38.540
Any questions?

08:38.540 --> 08:40.260
No?

08:40.260 --> 08:40.980
Cool.

08:40.980 --> 08:42.620
All right, let's give them another round of applause.

