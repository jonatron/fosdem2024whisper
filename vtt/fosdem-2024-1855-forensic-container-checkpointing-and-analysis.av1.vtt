WEBVTT

00:00.000 --> 00:07.000
Thank you.

00:07.000 --> 00:09.000
Yeah, thank you.

00:09.000 --> 00:12.000
So welcome to my session for forensic container,

00:12.000 --> 00:15.000
check pointing and analysis.

00:15.000 --> 00:17.000
So my name is Adran Reber.

00:17.000 --> 00:19.000
I worked at Red Hat since 2015.

00:19.000 --> 00:22.000
I'm involved in process migration.

00:22.000 --> 00:26.000
What's the basis for container check pointing now?

00:26.000 --> 00:28.000
I guess now 13 years now.

00:28.000 --> 00:31.000
Everything I'm talking about today is about CreeW,

00:31.000 --> 00:34.000
Checkpoint Restore and User Space, a low level tool.

00:34.000 --> 00:37.000
I'm involved there for a long time.

00:37.000 --> 00:42.000
And I'm focusing on container migration since 2015

00:42.000 --> 00:47.000
and forensic container analysis is one use case

00:47.000 --> 00:54.000
of the overall container migration topic.

00:54.000 --> 00:56.000
So this talk will look something like this.

00:56.000 --> 01:00.000
I will give a bit of background about the tools,

01:00.000 --> 01:03.000
who uses Checkpoint Restore currently, who uses CreeW,

01:03.000 --> 01:06.000
how is it used, the use cases.

01:06.000 --> 01:08.000
I will go through a couple of them.

01:08.000 --> 01:11.000
Then I will talk about the title of the talk,

01:11.000 --> 01:12.000
forensic container analysis.

01:12.000 --> 01:15.000
This is basically just a demo.

01:15.000 --> 01:17.000
So maybe it fails.

01:17.000 --> 01:21.000
And then I will talk a bit about the future of Checkpoint Restore,

01:21.000 --> 01:25.000
especially with focus on Kubernetes today.

01:25.000 --> 01:30.000
Okay, so Checkpoint Restore and User Space CreeW,

01:30.000 --> 01:35.000
that's the tool we're using today to do the check pointing

01:35.000 --> 01:39.000
and create the images for the analysis.

01:39.000 --> 01:43.000
And the reason why it's called Checkpoint Restore and User Space

01:43.000 --> 01:46.000
is because Checkpoint Restore is a technology

01:46.000 --> 01:50.000
which exists on operating system and Linux for a long time.

01:50.000 --> 01:54.000
And previous approaches were either in the kernel,

01:54.000 --> 01:57.000
that's why this one is called in User Space,

01:57.000 --> 02:00.000
or they required some preloading.

02:00.000 --> 02:03.000
So you would do an LD preload and then some library

02:03.000 --> 02:05.000
would intercept everything you do.

02:05.000 --> 02:07.000
And then later the Restore,

02:07.000 --> 02:11.000
something would try to create the steps you did before.

02:11.000 --> 02:13.000
CreeW is different.

02:13.000 --> 02:15.000
CreeW is something what you would call

02:15.000 --> 02:18.000
a completely transparent Checkpoint Restore utility.

02:19.000 --> 02:22.000
It doesn't require any preparation of the tool.

02:22.000 --> 02:26.000
You can just point it at any process and you can checkpoint it

02:26.000 --> 02:30.000
if the process is not using any resources CreeW cannot handle.

02:30.000 --> 02:36.000
And then you can restore it on the same or on another machine.

02:36.000 --> 02:43.000
CreeW was developed with the goal to use existing kernel interfaces

02:43.000 --> 02:45.000
as much as possible.

02:45.000 --> 02:48.000
Over the years there were additional kernel interfaces introduced

02:48.000 --> 02:50.000
to support CreeW.

02:50.000 --> 02:53.000
None of these interfaces are specific to Checkpoint Restore,

02:53.000 --> 02:59.000
so there are always multiple different users using those new interfaces.

02:59.000 --> 03:02.000
So the most changes CreeW did to the kernel

03:02.000 --> 03:04.000
are not Checkpoint Restore specific.

03:04.000 --> 03:08.000
Most of the time it's just how to get more information

03:08.000 --> 03:11.000
about the running process out of the kernel.

03:12.000 --> 03:15.000
There are multiple integrations of Checkpoint Restore

03:15.000 --> 03:21.000
in different projects, container run times, container engines,

03:21.000 --> 03:23.000
container orchestrations.

03:23.000 --> 03:26.000
And the first I have to mention here is OpenVz.

03:26.000 --> 03:28.000
It's something I never used personally,

03:28.000 --> 03:30.000
but that's the group behind CreeW,

03:30.000 --> 03:36.000
so they developed CreeW to be able to live migrate their containers.

03:36.000 --> 03:38.000
They were doing containers before it was container,

03:38.000 --> 03:41.000
so it's something which existed for a very long time.

03:41.000 --> 03:45.000
And at some point I'm not sure about the history exactly,

03:45.000 --> 03:48.000
but they came up with CreeW to have a Linux tool

03:48.000 --> 03:52.000
which works for everybody and not just for them.

03:52.000 --> 03:57.000
Another interesting integration of CreeW is in-bored.

03:57.000 --> 04:02.000
This is Google's container engine, what they use in-house

04:02.000 --> 04:04.000
to run all their tasks.

04:05.000 --> 04:10.000
And although the upstream CreeW developers

04:10.000 --> 04:12.000
don't have direct contact with Google,

04:12.000 --> 04:15.000
we know from conferences how Google uses it,

04:15.000 --> 04:20.000
so basically what they do, they can migrate containers

04:20.000 --> 04:25.000
and they mostly do low priority containers.

04:25.000 --> 04:27.000
So if you have a node, there's something running on it.

04:27.000 --> 04:30.000
It needs more resources than before CreeW.

04:30.000 --> 04:33.000
They just killed the low priority container

04:33.000 --> 04:36.000
and restarted the work somewhere else from the beginning.

04:36.000 --> 04:38.000
And with the integration of CreeW,

04:38.000 --> 04:41.000
now they can just move it from one host to another host.

04:41.000 --> 04:42.000
So this is...

04:42.000 --> 04:46.000
And as far as we know, they're using it at least since 2017.

04:46.000 --> 04:51.000
I think that's when we saw the first presentations from Google

04:51.000 --> 04:53.000
how they use CreeW.

04:53.000 --> 04:58.000
Then there's an integration for a long time in CXT,

04:58.000 --> 05:01.000
and I probably have to mention in just today.

05:01.000 --> 05:04.000
It's also integrated there.

05:04.000 --> 05:10.000
Also for a very long time, it's integrated in Dockoff 4.

05:10.000 --> 05:15.000
Also, I don't know, maybe also 2016, something like this.

05:15.000 --> 05:17.000
I've worked for a couple of years

05:17.000 --> 05:20.000
to integrate checkpoint restore support in Portman,

05:20.000 --> 05:22.000
so you can also, using Portman,

05:22.000 --> 05:25.000
checkpoint restore containers migrate them from one host

05:25.000 --> 05:27.000
to another host.

05:27.000 --> 05:30.000
And the thing which I'm currently working on,

05:30.000 --> 05:32.000
which I started around,

05:32.000 --> 05:36.000
people talk to me about how they think to use container migration,

05:36.000 --> 05:38.000
container checkpointing,

05:38.000 --> 05:41.000
and the simplest one is maybe reboot in safe state.

05:41.000 --> 05:45.000
So you have your system running with a container on it,

05:45.000 --> 05:47.000
and you have a blue kernel there,

05:47.000 --> 05:50.000
and it has some problem, and you want to update the kernel.

05:50.000 --> 05:52.000
But your container takes a long time to start.

05:52.000 --> 05:54.000
You're not really happy doing a reboot

05:54.000 --> 05:58.000
because your application is down for a long time.

05:58.000 --> 06:00.000
So with GRI, you can update the kernel,

06:00.000 --> 06:03.000
then you can create a checkpoint,

06:03.000 --> 06:06.000
basically an image, a stateful image of your container,

06:06.000 --> 06:09.000
write it to disk, reboot your host,

06:09.000 --> 06:11.000
and then it comes up with a new kernel.

06:11.000 --> 06:12.000
This time it's green.

06:12.000 --> 06:13.000
You restore the container,

06:13.000 --> 06:17.000
and it's running pretty fast,

06:17.000 --> 06:20.000
much faster than waiting for all the initialization.

06:20.000 --> 06:24.000
So you can quickly do reboot of your systems

06:24.000 --> 06:25.000
using checkpoint restore.

06:25.000 --> 06:28.000
Another one, it's similar to the first one.

06:28.000 --> 06:31.000
And also people have been talking to me about this.

06:31.000 --> 06:33.000
So this is also used in production.

06:33.000 --> 06:35.000
You have a container which takes a long time to start,

06:35.000 --> 06:37.000
the one I've been told about.

06:37.000 --> 06:40.000
It takes like 10 minutes until everything is initialized.

06:40.000 --> 06:44.000
So, and they have a service which they want to sell to customers,

06:44.000 --> 06:47.000
and they want to have the customers fast access to the containers.

06:47.000 --> 06:49.000
They don't want them to wait for 10 minutes.

06:49.000 --> 06:52.000
So what they do is they initialize the container once,

06:52.000 --> 06:55.000
create a checkpoint, write it to disk,

06:55.000 --> 06:59.000
and then they can immediately start in matter of seconds services

06:59.000 --> 07:01.000
from this pre-initialized container,

07:01.000 --> 07:04.000
and their customers don't have to wait 10 minutes.

07:04.000 --> 07:08.000
It's just in 10, 20 seconds, something like this.

07:08.000 --> 07:13.000
The combination of those two use cases is the container live migration.

07:13.000 --> 07:15.000
So we have two hosts.

07:15.000 --> 07:17.000
We have the container on one host.

07:17.000 --> 07:21.000
And it's hopefully stateful because if the container is not stateful,

07:21.000 --> 07:26.000
the whole migration thing of container doesn't make much sense in the end.

07:26.000 --> 07:30.000
For the forensic use case, it can be a stateless container as well

07:30.000 --> 07:32.000
because you can still analyze it.

07:32.000 --> 07:35.000
So what is the same again?

07:35.000 --> 07:38.000
We create a copy of the container, write it to disk,

07:38.000 --> 07:42.000
and then we can create one or multiple copies on the destination system.

07:42.000 --> 07:45.000
And the original container can keep on running or not.

07:45.000 --> 07:50.000
So this is really up to you how you want to use checkpoint restore today.

07:50.000 --> 07:55.000
Another interesting thing people are talking about are spot instances.

07:55.000 --> 07:58.000
Spot instances are usually something which is cheap,

07:58.000 --> 08:00.000
but they go away.

08:00.000 --> 08:03.000
Those VMs, like, I don't know, you have two minutes warning,

08:03.000 --> 08:08.000
and people are using checkpoint restore there in combination with pre-use.

08:08.000 --> 08:10.000
So you get a signal that your VM is going down.

08:10.000 --> 08:12.000
You create a checkpoint, write it somewhere,

08:12.000 --> 08:15.000
and then you can continue to run your workload on another system

08:15.000 --> 08:21.000
without losing any work or without having to do any restart

08:21.000 --> 08:26.000
or long down times or whatever you would like to avoid.

08:26.000 --> 08:33.000
And something which came up recently is people are interested to use it for AI training.

08:33.000 --> 08:38.000
So you have your AI training running somewhere with a GPU,

08:38.000 --> 08:43.000
and for some reason it's aborted, or you have to make space on the node,

08:43.000 --> 08:49.000
and with a combination of checkpoint restore you can create a checkpoint of your container.

08:49.000 --> 08:51.000
In this case it's less about migration.

08:51.000 --> 08:57.000
It's just creating a copy of your state somewhere so you can continue to run it later,

08:57.000 --> 08:59.000
or even migrate.

08:59.000 --> 09:01.000
It really depends on what you want to do there.

09:01.000 --> 09:07.000
The interesting thing here is I mentioned previously that the CRIU cannot handle all resources,

09:07.000 --> 09:11.000
and GPUs are kind of the resources which CRIU cannot always handle.

09:11.000 --> 09:19.000
We are lucky that AMD came up to us and they actually implemented support to migrate

09:19.000 --> 09:23.000
or checkpoint restore applications which are running on the host CPU

09:23.000 --> 09:26.000
and at the same time on the AMD GPU.

09:26.000 --> 09:30.000
For NVIDIA, we don't know if that exists.

09:30.000 --> 09:34.000
We have heard people talking about it.

09:34.000 --> 09:40.000
I think Microsoft mentioned it at some point that they might have been using CRIU

09:40.000 --> 09:45.000
in combination with NVIDIA, but nobody talked to the CRIU Upscreen Project at least.

09:45.000 --> 09:50.000
So we are not aware that people are doing, but we kind of expect that people are using CRIU

09:50.000 --> 09:55.000
in combination with NVIDIA GPUs.

09:55.000 --> 10:01.000
So the next one is then forensic container analysis and my demo.

10:01.000 --> 10:04.000
So my demo is based on a container.

10:04.000 --> 10:06.000
I am using OpenHPC as a base.

10:06.000 --> 10:08.000
So the container is a stateful container.

10:08.000 --> 10:15.000
It is calculating pi and memory which we can hopefully later find in the container.

10:15.000 --> 10:19.000
So to create a checkpoint, there is a complicated way to do it.

10:19.000 --> 10:24.000
So currently checkpoint restoring Kubernetes is only a Q-Bled interface.

10:25.000 --> 10:31.000
Officially, the reason is because checkpoint restore writes your container,

10:31.000 --> 10:34.000
every memory page to disk.

10:34.000 --> 10:41.000
There is the potential, the risk that you now have private keys, random numbers, passwords,

10:41.000 --> 10:43.000
now all written on disk.

10:43.000 --> 10:45.000
The checkpoint is only readable by root.

10:45.000 --> 10:48.000
So the situation doesn't really change because if you root on a machine,

10:48.000 --> 10:51.000
you could also extract the memory, but for now,

10:51.000 --> 10:56.000
because it's not clear how to handle this or how we want to continue in the Kubernetes community.

10:56.000 --> 11:02.000
With this feature, it's just a Q-Bled only interface and it looks like this.

11:02.000 --> 11:07.000
I've also written a QCTL interface.

11:07.000 --> 11:08.000
It looks like this.

11:08.000 --> 11:10.000
It creates also the checkpoint archive.

11:10.000 --> 11:12.000
It's basically doing the same.

11:12.000 --> 11:19.000
It's just wiring all the calls completely to QCTL instead just a Q-Bled.

11:19.000 --> 11:25.000
So now we have a checkpoint and there's a tool called checkpoint control,

11:25.000 --> 11:30.000
which was mainly developed by Google Summer of Code students this year.

11:30.000 --> 11:36.000
So we're very happy for this help, which they did.

11:36.000 --> 11:41.000
And in its simplest form, checkpoint control will give you,

11:41.000 --> 11:45.000
I'm just going to make the font a little bit smaller for a short time here,

11:45.000 --> 11:48.000
and it gives me just some basic information about the container.

11:48.000 --> 11:52.000
I see it's the container's name counter.

11:52.000 --> 11:58.000
It's based on that image, ID, runtime, when it was created, the engine cryo.

11:58.000 --> 12:02.000
Checkpoint size is basically the size of all memory pages,

12:02.000 --> 12:09.000
and root fsdif size is the size of all files which have changed compared to the base image.

12:10.000 --> 12:21.000
So let's unpack the checkpoint archive to see some details.

12:21.000 --> 12:26.000
And it's just a car archive, so it's easy to unpack.

12:26.000 --> 12:28.000
I'm just going to move this to the top again.

12:28.000 --> 12:32.000
And there are a couple of files which now were created by the container engine.

12:32.000 --> 12:34.000
And so we have bind mounts.

12:34.000 --> 12:37.000
This is just some information that is necessary for restore,

12:37.000 --> 12:43.000
because we need to restore all the mounts from the outside of the container to the inside,

12:43.000 --> 12:46.000
and we need to know if it's a file or a directory,

12:46.000 --> 12:54.000
because the container engine doesn't want to remember if it's a container or if it's a directory or a file,

12:54.000 --> 12:56.000
but we need it for the restore.

12:56.000 --> 12:58.000
Configdump has some information.

12:58.000 --> 13:01.000
dump.loc has what cryo tells us.

13:01.000 --> 13:04.000
In this case, it doesn't matter because it works.

13:04.000 --> 13:06.000
Then we have the root fsdif file.

13:06.000 --> 13:11.000
This is all the files which have changed compared to the base image we saw previously.

13:11.000 --> 13:14.000
In the checkpoint directory is the one created by cryo.

13:14.000 --> 13:19.000
So that has the actual process information.

13:19.000 --> 13:23.000
So if we go there, this is the normal thing which cryo does is all,

13:23.000 --> 13:28.000
most of them are protobuf files generated by cryo here.

13:29.000 --> 13:32.000
And cryo comes with a tool called crit,

13:32.000 --> 13:36.000
crit, cryo image tool, and it has a parameter show,

13:36.000 --> 13:39.000
and we can have a look at one of those files.

13:39.000 --> 13:42.000
Let's look at UTS namespace information here.

13:42.000 --> 13:45.000
It basically just tells us the namespace,

13:45.000 --> 13:49.000
the UTS namespace has hostname counters,

13:49.000 --> 13:53.000
but we can also look at a file called pstree.

13:53.000 --> 13:55.000
This is the process tree.

13:55.000 --> 13:59.000
This one, so it starts to get difficult to understand what's going on.

13:59.000 --> 14:02.000
I have a couple of commands prepared.

14:02.000 --> 14:07.000
So I see with this one, we have four PIDs running in our container,

14:07.000 --> 14:09.000
140, 40, 142.

14:09.000 --> 14:13.000
It's important to know this is the view from inside of the PID namespace,

14:13.000 --> 14:18.000
so cryo always remembers the PIDs from within the PID namespace

14:18.000 --> 14:21.000
and tries to recreate those PIDs later.

14:21.000 --> 14:28.000
If I looked at my process, which is maybe still running,

14:28.000 --> 14:33.000
it should, I can see here, basically, it's not hard to read,

14:33.000 --> 14:37.000
but those are the four, where's my mouse?

14:37.000 --> 14:38.000
I don't know.

14:38.000 --> 14:39.000
Oh, there it is.

14:39.000 --> 14:43.000
You see, so this is the PID one of the container,

14:43.000 --> 14:47.000
and this is probably 41, 42, 43, I guess,

14:47.000 --> 14:50.000
and you can see here it has other PIDs on the outside,

14:50.000 --> 14:53.000
because that's the view from the outside of the PID namespace.

14:53.000 --> 14:56.000
So it's important if you ever do an analysis of your checkpoint,

14:56.000 --> 14:59.000
it's always the PIDs from within the PID namespace.

14:59.000 --> 15:04.000
There's also for each process, we have a file called core,

15:04.000 --> 15:08.000
with the core information about the process.

15:08.000 --> 15:10.000
Let's have a quick look at this one,

15:10.000 --> 15:14.000
and it basically has the registers, the value of all the registers,

15:14.000 --> 15:18.000
floating point and much more stuff, and the end you see,

15:18.000 --> 15:21.000
the policies and the name of the process,

15:21.000 --> 15:24.000
and using the name of the process,

15:24.000 --> 15:30.000
I think I can get a list of what processes are running inside of my container

15:30.000 --> 15:34.000
and what they do, and you see the first one is called bash login wrapper,

15:34.000 --> 15:39.000
bash, pi and t, and if I compare this again with what's currently,

15:39.000 --> 15:45.000
I don't know, it's the wrong command, with here, again, I see it's again,

15:45.000 --> 15:51.000
bash login wrapper, bash, the Python code and the t command.

15:51.000 --> 15:56.000
So looking at these files, I can find out everything about the processes here,

15:56.000 --> 15:59.000
so there's a lot of information in here,

15:59.000 --> 16:03.000
and if you're looking for something specific, it might be difficult,

16:03.000 --> 16:06.000
but the information is here.

16:06.000 --> 16:10.000
There are additional files, for example, the tempfs.dev files,

16:10.000 --> 16:12.000
those are maybe also interesting files.

16:12.000 --> 16:17.000
Those are basically, let's have a look at those.

16:17.000 --> 16:21.000
Something like this is probably the right one.

16:21.000 --> 16:25.000
And you see, this is the content of a tempfs,

16:25.000 --> 16:28.000
so every tempfs which is not bind-mounted from the host,

16:28.000 --> 16:30.000
which is native to the container,

16:30.000 --> 16:35.000
KreeU kind of puts it into the image, it's basically just a tar,

16:35.000 --> 16:38.000
so every tempfs which was in your container is now also here,

16:38.000 --> 16:41.000
you can find all the information here.

16:41.000 --> 16:44.000
This looks like this was slash dev.

16:44.000 --> 16:49.000
What else do we have here? Let's have a look.

16:53.000 --> 16:55.000
Yeah, I think that's okay.

16:55.000 --> 17:02.000
And previously, I also wrote some, my secret data into the memory pages,

17:02.000 --> 17:05.000
and I can actually find this memory again here,

17:05.000 --> 17:08.000
this information again in the pages files.

17:08.000 --> 17:11.000
The pages images are, those are not,

17:11.000 --> 17:14.000
protobuf files are raw dumps of the memory,

17:14.000 --> 17:16.000
this is all the memory which was written to disk,

17:16.000 --> 17:20.000
and I can again find the information I've written to memory here.

17:20.000 --> 17:22.000
So if I know what I'm looking for, it's easy,

17:22.000 --> 17:25.000
if I'm looking for a password, then I have to pause it all through

17:25.000 --> 17:28.000
and maybe find a useful string in there,

17:28.000 --> 17:33.000
but this is just to show you, you have access to all memory pages,

17:33.000 --> 17:35.000
and they are now all on disk,

17:35.000 --> 17:41.000
and it can be easily analyzed, or at least look at.

17:41.000 --> 17:46.000
So if, okay, let's, I also wrote a couple of files to my container,

17:46.000 --> 17:49.000
I mentioned this here, the root of sdif tar,

17:49.000 --> 17:51.000
let's unpack that one.

17:53.000 --> 17:56.000
And so now this contains three files,

17:56.000 --> 17:58.000
so these are all files which have changed

17:58.000 --> 18:01.000
compared to the base image of the container,

18:01.000 --> 18:04.000
and this is just really simple, a file which is created,

18:04.000 --> 18:08.000
it just has the, it just contains the name of the file itself,

18:08.000 --> 18:12.000
but it's just to show you, if you want to look at content

18:12.000 --> 18:14.000
which has changed in the container,

18:14.000 --> 18:18.000
you will find it here in this root of sdif tar,

18:18.000 --> 18:20.000
which contains all the changed files.

18:20.000 --> 18:22.000
And if you think this is all too much work,

18:22.000 --> 18:25.000
then I already mentioned checkpoint control before,

18:25.000 --> 18:29.000
and it's even, has even more possibilities

18:29.000 --> 18:32.000
than what I've shown you, most of the things I've done here manually,

18:32.000 --> 18:35.000
the tool, thanks to our Google Summer of Code students,

18:35.000 --> 18:37.000
can at this point do.

18:37.000 --> 18:40.000
So let's have a look at checkpoint control inspect of the,

18:40.000 --> 18:45.000
and the $CP variable is basically pointing to the tar archive,

18:45.000 --> 18:48.000
so the tool is now unpacking the tar archive

18:48.000 --> 18:50.000
and giving us all the information.

18:50.000 --> 18:53.000
And what we see here now is this information we saw before,

18:53.000 --> 18:56.000
so it's just some basic information about the image, where it was,

18:56.000 --> 19:00.000
how big the checkpoint size is, then we see CreeO dump statistics,

19:00.000 --> 19:04.000
this is basically the time CreeO needs to write the checkpoint to this.

19:04.000 --> 19:06.000
You see how many memory pages were scanned,

19:06.000 --> 19:08.000
if they should be written to this,

19:08.000 --> 19:10.000
how many memory pages were actually written to this,

19:10.000 --> 19:15.000
and then we see the full process command line.

19:15.000 --> 19:21.000
We see all the environments variables of all processes running in our container,

19:21.000 --> 19:27.000
and next one even more variables, and more and more,

19:27.000 --> 19:36.000
and at some point there's, I think it even contains the open files,

19:36.000 --> 19:38.000
too many variables here.

19:38.000 --> 19:41.000
You see, now we see the open files, you see the one has open def null,

19:41.000 --> 19:45.000
and then two pipes, and then the working directory,

19:45.000 --> 19:49.000
and open sockets, you even see that that's the socket I've been talking to,

19:49.000 --> 19:52.000
and then we go to the process here,

19:52.000 --> 19:57.000
and then we see all the mounts we need,

19:57.000 --> 20:00.000
this is also important for restoring the process later.

20:00.000 --> 20:03.000
So I guess that's the end of my demo,

20:03.000 --> 20:07.000
so checkpoint control was the tool I was using,

20:07.000 --> 20:13.000
I was using CreeO image tool to have a look at the content of the images,

20:13.000 --> 20:20.000
and then I was using grep to find my secret key from the memory pages.

20:20.000 --> 20:25.000
So one thing I didn't show, you can use, there's a tool in CreeO

20:25.000 --> 20:29.000
which converts the checkpoint images to core dump files,

20:29.000 --> 20:33.000
and then you can use gdb to look at them,

20:33.000 --> 20:37.000
it's basically the same, you see the registers and the call stack and things like this,

20:37.000 --> 20:40.000
might also be interesting for a couple of people,

20:40.000 --> 20:44.000
to what's next, especially with focus on Kubernetes,

20:44.000 --> 20:48.000
so I've shown that I have a kubectl checkpoint kind of working,

20:48.000 --> 20:53.000
that's an open pull request, it's not being actively discussed at this point,

20:53.000 --> 20:57.000
but it's there so if somebody needs it, it can be easily used,

20:57.000 --> 21:03.000
maybe the next step would be to integrate checkpoint for complete parts,

21:03.000 --> 21:06.000
I've implemented this a couple of years ago, it's pretty simple,

21:06.000 --> 21:09.000
we just do a loop over all containers in a pod,

21:09.000 --> 21:12.000
we just create some metadata for the pod and then we can recreate it,

21:12.000 --> 21:14.000
so this is not a technical challenge,

21:14.000 --> 21:19.000
it's just most things at this point are how to get it in a way into Kubernetes

21:19.000 --> 21:23.000
which is sustainable and makes sense,

21:23.000 --> 21:26.000
and then maybe we have something like kubectl migrate,

21:26.000 --> 21:28.000
so we don't have to do it manually,

21:28.000 --> 21:31.000
maybe at some point the scheduler will decide,

21:31.000 --> 21:33.000
let's move this pod somewhere else,

21:33.000 --> 21:38.000
and one thing, so the image format I'm using is currently just a tar file,

21:38.000 --> 21:41.000
I came up with, but it's not a standard,

21:41.000 --> 21:44.000
so container D uses something else,

21:44.000 --> 21:46.000
I looked at the container D format,

21:46.000 --> 21:52.000
it's applicable for what I was looking at,

21:52.000 --> 21:55.000
but the problem was they were using internal protobuf structures,

21:55.000 --> 21:59.000
I didn't thought make sense to have in a public checkpoint,

21:59.000 --> 22:05.000
in theory, checkpointing on container D and restoring in cryo should not be a problem,

22:05.000 --> 22:09.000
but at this point we don't have a common image standard,

22:09.000 --> 22:12.000
I tried to start a discussion here,

22:12.000 --> 22:15.000
but it also didn't continue unfortunately,

22:15.000 --> 22:18.000
so with this I'm at the end,

22:18.000 --> 22:22.000
so I showed you that cryo can checkpoint containers,

22:22.000 --> 22:24.000
I haven't shown the restore part, but it works,

22:24.000 --> 22:27.000
it integrated in different container run times,

22:27.000 --> 22:30.000
it's used in production by different companies at this point,

22:30.000 --> 22:34.000
use cases are things like reboot into new kernel and save states,

22:34.000 --> 22:36.000
multiple copies, container migration,

22:36.000 --> 22:41.000
spot instances, AI learning support for GPUs there,

22:41.000 --> 22:47.000
and this is all available in Kubernetes under the forensic container checkpoint in cap 2008.

22:47.000 --> 22:51.000
So, I'm at the end, thank you, any questions?

22:51.000 --> 22:53.000
Thank you.

23:01.000 --> 23:03.000
Oh, sorry.

23:11.000 --> 23:15.000
Sorry, please be quiet, we cannot hear the questions.

23:15.000 --> 23:17.000
You mentioned GPUs are something you can't handle,

23:17.000 --> 23:20.000
what are the other big resources that...

23:20.000 --> 23:24.000
So basically cryo cannot handle anything that's external to the kernel,

23:24.000 --> 23:28.000
so InfiniBand is one which comes up in high performance computing always,

23:28.000 --> 23:32.000
so everything where you have a state in additional hardware,

23:32.000 --> 23:34.000
you need some way to extract it,

23:34.000 --> 23:38.000
you need to extract the state so you can later restore it, so...

23:38.000 --> 23:40.000
And just create a text in the process,

23:40.000 --> 23:42.000
is that stuff that fails?

23:42.000 --> 23:44.000
Exactly, it fails, Daniel.

24:05.000 --> 24:08.000
So currently the people I've talked to today,

24:08.000 --> 24:11.000
they are just interested in finding out if there has been an attack

24:11.000 --> 24:15.000
or if there is an attack ongoing, things like this,

24:15.000 --> 24:20.000
and then maybe at some point, maybe if you can have a couple of checkpoints

24:20.000 --> 24:23.000
and figure out, okay, this looks like an attack pattern,

24:23.000 --> 24:26.000
maybe detect it automatically using check pointing,

24:26.000 --> 24:28.000
this would be maybe something in the future,

24:28.000 --> 24:32.000
but finding a possible attack is one of the main motivations

24:32.000 --> 24:35.000
for people for the forensic use case.

24:41.000 --> 24:44.000
Thank you.

