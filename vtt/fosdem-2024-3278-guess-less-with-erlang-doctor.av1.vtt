WEBVTT

02:00.000 --> 02:10.000
Okay, I...

02:10.000 --> 02:13.000
Yeah, it switched off to like...

02:13.000 --> 02:16.000
By itself. I didn't touch it.

02:16.000 --> 02:21.000
So yeah, when you debug, for example, your code,

02:21.000 --> 02:24.000
when you're trying to find out why you have a strange error

02:24.000 --> 02:28.000
or something like that, you can use our long tracing.

02:28.000 --> 02:31.000
And it's very powerful, as we said before.

02:31.000 --> 02:34.000
And for example, you can use tools like DBG or Recon

02:34.000 --> 02:37.000
that are using error tracing underneath.

02:37.000 --> 02:41.000
And the first step is to choose which functions

02:41.000 --> 02:46.000
you want to trace, actually, because you don't trace everything.

02:46.000 --> 02:49.000
Although you can trace what you want,

02:49.000 --> 02:52.000
you cannot trace everything at once.

02:52.000 --> 02:55.000
So you choose like, I want this function to be traced

02:55.000 --> 02:57.000
for this bunch of functions.

02:57.000 --> 03:01.000
And then, when you call these functions,

03:01.000 --> 03:03.000
you get your traces being printed out.

03:03.000 --> 03:06.000
So you get the information of this function is called,

03:06.000 --> 03:10.000
these are the arguments, return values, things like that.

03:10.000 --> 03:14.000
You can get it to console, you can get it to a file,

03:14.000 --> 03:16.000
and you can also send it to a network,

03:16.000 --> 03:19.000
and that's what I have been doing for many, many years.

03:19.000 --> 03:22.000
I was just setting up, for example, I said many years, yeah,

03:22.000 --> 03:25.000
for 15 years, I think, with Erlang.

03:25.000 --> 03:28.000
So I was just setting up a special node

03:28.000 --> 03:31.000
that was like collecting traces for all the other nodes.

03:31.000 --> 03:33.000
So you can also send them to the network.

03:33.000 --> 03:39.000
And, well, afterwards, you either read the traces that you collected,

03:39.000 --> 03:43.000
or you can also search them, grab them, parse them,

03:43.000 --> 03:46.000
do some other operations on them if you want.

03:46.000 --> 03:49.000
But these are just text logs, let's say, mostly.

03:49.000 --> 03:53.000
And the problem is that very often you have to repeat the whole process.

03:53.000 --> 03:56.000
That's because you've traced one function,

03:56.000 --> 04:01.000
but you found out that maybe the problem is in another function,

04:01.000 --> 04:04.000
maybe in a completely different module, and so on and so on.

04:04.000 --> 04:06.000
So you do it, so you repeat, repeat,

04:06.000 --> 04:10.000
and that might be kind of a problem.

04:10.000 --> 04:14.000
So this doesn't scale well.

04:14.000 --> 04:20.000
And what I mean by that is if you try to trace a lot of functions,

04:20.000 --> 04:24.000
well, I found out that at least for me,

04:24.000 --> 04:29.000
when I get like 100 to 1000 traces, it becomes difficult to read,

04:29.000 --> 04:33.000
like for a human to read that amount of information.

04:33.000 --> 04:35.000
Okay, but you can search, for example.

04:35.000 --> 04:38.000
And this also has a limit.

04:38.000 --> 04:42.000
So, of course, this is just a rough estimate, let's say,

04:42.000 --> 04:48.000
but for me, usually, when I have like 10 to 100,000 traces,

04:48.000 --> 04:52.000
then it becomes difficult, because even my system can slow down,

04:52.000 --> 04:56.000
IO can slow down, and actually it's quite surprising,

04:56.000 --> 05:01.000
but sending traces to a file or to a network,

05:01.000 --> 05:04.000
it's actually quite expensive.

05:04.000 --> 05:07.000
And it can slow down the system quite a lot.

05:07.000 --> 05:09.000
And it's a heavy operation,

05:09.000 --> 05:12.000
so sometimes I had traces accumulating for three minutes

05:12.000 --> 05:15.000
after I finished traces or something like that,

05:15.000 --> 05:18.000
and the messages were still in the queue, still being processed.

05:18.000 --> 05:21.000
Yeah, so this doesn't scale that well.

05:21.000 --> 05:23.000
Okay, so let's sum up.

05:23.000 --> 05:26.000
Choosing the function to trace is kind of a guesswork.

05:26.000 --> 05:30.000
Not always, of course, sometimes we know precisely,

05:30.000 --> 05:32.000
but most often I don't know.

05:32.000 --> 05:35.000
I know kind of what I'm looking for, but not exactly,

05:35.000 --> 05:39.000
and that's the problem, but I need to somehow know the function exactly here,

05:39.000 --> 05:42.000
to choose it to be traced.

05:42.000 --> 05:46.000
So, possibly many iterations of the process.

05:46.000 --> 05:48.000
This is, for me, this is like ad hoc logging.

05:48.000 --> 05:50.000
This is very much like logging,

05:50.000 --> 05:53.000
but I don't need to have a log statement in my code.

05:53.000 --> 05:57.000
I just choose dynamically, right now, I want to do these functions to be logged.

05:57.000 --> 06:03.000
And what if the trace behaviors have tests that fail every 20 runs, for example,

06:03.000 --> 06:05.000
do I need to repeat this 20 times?

06:05.000 --> 06:08.000
So what? That's the problem, right?

06:08.000 --> 06:13.000
And answer to some of those issues is Erlang Doctor,

06:13.000 --> 06:16.000
at least for me, and for the people who've used that.

06:16.000 --> 06:19.000
So what's the difference?

06:19.000 --> 06:24.000
So you set up the tracing for entire application, not always.

06:24.000 --> 06:26.000
Sometimes it's not possible.

06:26.000 --> 06:28.000
Sometimes you have to trace individual modules,

06:28.000 --> 06:32.000
but usually you can start with just one entire application.

06:33.000 --> 06:37.000
You capture traces, store them in the ETS table,

06:37.000 --> 06:39.000
and you clear the traces afterwards.

06:39.000 --> 06:42.000
And you can repeat this process instead of repeating everything,

06:42.000 --> 06:49.000
because you've collected enough stuff to query and to find out about different functions, for example.

06:49.000 --> 06:53.000
To query, oh, this was this function called, maybe another one, and so on and so on.

06:53.000 --> 06:55.000
You can do this.

06:55.000 --> 06:59.000
And of course, rarely you have to repeat this,

06:59.000 --> 07:02.000
but for me it's only when I, for example, trace the wrong application,

07:02.000 --> 07:06.000
because the problem is not in my code, it's in a library that I've used.

07:06.000 --> 07:09.000
Then I need to trace another Erlang application, right?

07:09.000 --> 07:14.000
But it doesn't occur that often.

07:14.000 --> 07:16.000
This scales much better.

07:16.000 --> 07:18.000
What are the limits?

07:18.000 --> 07:20.000
So on my laptop, for example,

07:20.000 --> 07:23.000
querying becomes slow at about 10 million traces collected,

07:23.000 --> 07:25.000
which is quite a lot,

07:25.000 --> 07:30.000
but it's like tracing a function, a system under heavy load, for example.

07:30.000 --> 07:33.000
And of course it depends on the size of individual traces,

07:33.000 --> 07:38.000
because you can have big arguments being passed or something like that.

07:38.000 --> 07:39.000
Yeah.

07:39.000 --> 07:44.000
System memory becomes the limit at 4 million at about 50 million traces,

07:44.000 --> 07:47.000
but sometimes it's 10 million, sometimes it's 100 million, it depends.

07:47.000 --> 07:51.000
But basically when you have a few million traces, it's probably too much.

07:51.000 --> 07:53.000
So there is a limit, of course.

07:53.000 --> 08:00.000
So to sum up, very few iterations of the whole process, usually one.

08:00.000 --> 08:04.000
This is for me like ad hoc instrumentation instead of ad hoc logging,

08:04.000 --> 08:09.000
because you're gathering structured information in the ETS table.

08:09.000 --> 08:11.000
I will show you details in a moment.

08:11.000 --> 08:15.000
And use cases, for me there are many use cases.

08:15.000 --> 08:18.000
For example, debugging, system exploration.

08:18.000 --> 08:20.000
I often use it to just learn about the system.

08:20.000 --> 08:25.000
I just run the system, do it with the usual stuff when I'm tracing the whole application,

08:25.000 --> 08:30.000
and I'm just querying what the system did, actually, with the traces.

08:30.000 --> 08:39.000
And you can also do some lightweight profiling without the need to set up the profiler for a particular function.

08:39.000 --> 08:40.000
Yeah.

08:40.000 --> 08:43.000
So let's go to the Erlang doctor itself.

08:43.000 --> 08:48.000
How to get it from GitHub for Erlang or for Elixir?

08:48.000 --> 08:57.000
For Elixir it's called Xdoctor, which looks like a former doctor, but it's just a bit funny.

08:57.000 --> 09:02.000
Yeah, so here is a package of Xdox for both of them.

09:02.000 --> 09:07.000
And yeah.

09:07.000 --> 09:08.000
So how to run it?

09:08.000 --> 09:09.000
Three options.

09:09.000 --> 09:13.000
The first one that I'm using sometimes when doing firefighting,

09:13.000 --> 09:22.000
this is when you don't have it in your shell, but you want it right now, like in a system that's misbehaving or something.

09:22.000 --> 09:28.000
In both tools there are snippets that just download it from the Internet and compile it and run it,

09:28.000 --> 09:33.000
which works in this particular case.

09:33.000 --> 09:37.000
It's probably the best option if you just want it right now.

09:37.000 --> 09:47.000
And yeah, all you need is to have the access to the Internet, which is usually the case.

09:47.000 --> 09:58.000
The second option, which I'm using always in development, is just that you set it up in your .erlang or .iex.exe file.

09:58.000 --> 10:05.000
So that it's always available whenever you start any Erlang or Elixir shell, be it in your project or wherever.

10:05.000 --> 10:07.000
And third option, packaging.

10:07.000 --> 10:16.000
You can always include it in your application, in your software, if you think it's that useful.

10:16.000 --> 10:18.000
Okay, so let's move on.

10:18.000 --> 10:19.000
Let's start.

10:19.000 --> 10:26.000
Examples are in Erlang, but they are also available for Elixir in the docs.

10:26.000 --> 10:28.000
You can find them.

10:28.000 --> 10:30.000
The first thing to do is to start.

10:30.000 --> 10:34.000
It needs a GenServer, so it just starts at GenServer.

10:34.000 --> 10:37.000
And a few other examples how you can start them.

10:37.000 --> 10:39.000
You can choose a different ETS table.

10:39.000 --> 10:43.000
You can just have multiple ones if you want, switch between them.

10:43.000 --> 10:45.000
You can limit the size of a table.

10:45.000 --> 10:52.000
Very useful, like in a production environment, if you need to do some tracing, you just set it to like 1,000 or something.

10:52.000 --> 10:57.000
Just the table will never grow bigger, so you will never consume all memory.

10:57.000 --> 11:01.000
And yeah, there is also a start link.

11:01.000 --> 11:04.000
Okay, so let's set up tracing.

11:04.000 --> 11:06.000
I'm just tracing an example module.

11:06.000 --> 11:10.000
It's a test suite, but it contains functions that we can trace.

11:10.000 --> 11:12.000
It's good.

11:12.000 --> 11:15.000
So yeah, I'm just starting that tracer.

11:15.000 --> 11:26.000
And I can also trace a specific function, like provide a module function argument, whole application, multiple applications.

11:26.000 --> 11:28.000
And add a bit more.

11:28.000 --> 11:29.000
You can trace messages.

11:29.000 --> 11:31.000
You can trace specific processes and so on.

11:31.000 --> 11:34.000
There are a few more options.

11:34.000 --> 11:36.000
Capturing the traces.

11:36.000 --> 11:39.000
Okay, so let's call a function from the trace module.

11:39.000 --> 11:42.000
I'm calling just a sleepy factorial of 3.

11:42.000 --> 11:47.000
It's a function that calculates a factorial and just sleeps for 1 millisecond between each step, right?

11:47.000 --> 11:49.000
It will have some time difference.

11:49.000 --> 11:51.000
That's it.

11:51.000 --> 11:53.000
Yeah, very simple.

11:53.000 --> 11:57.000
And yeah, I'm just...

11:57.000 --> 12:00.000
Okay, now we can stop tracing.

12:00.000 --> 12:06.000
It's a good habit because you don't accumulate traces when you don't want them anymore.

12:06.000 --> 12:09.000
And now what can you do?

12:09.000 --> 12:12.000
Because we've accumulated traces, what can you do with them?

12:12.000 --> 12:15.000
So let's read the record definition.

12:15.000 --> 12:21.000
By the way, I'm using records because they are very performant.

12:21.000 --> 12:27.000
And even maps were giving me five times worse performance for some operations.

12:27.000 --> 12:30.000
So yeah, I'm using records.

12:30.000 --> 12:33.000
Yeah, so let's get all the traces.

12:33.000 --> 12:36.000
So I got all the traces and I don't want to talk about everything.

12:36.000 --> 12:38.000
Let's talk about the arguments.

12:38.000 --> 12:41.000
So these are the arguments and these are the return values, okay?

12:41.000 --> 12:44.000
For calls, for returns.

12:44.000 --> 12:48.000
And I will just introduce the other fields as we go.

12:49.000 --> 12:51.000
Arguments are in brackets.

12:51.000 --> 12:55.000
So now trace selection.

12:55.000 --> 12:57.000
You can do a select.

12:57.000 --> 13:02.000
It's a fancy way of doing this ETS select with ETS from 2MS.

13:02.000 --> 13:05.000
And let's get all function calls.

13:05.000 --> 13:08.000
And for each argument, let's just get this argument.

13:08.000 --> 13:10.000
So I'm getting a list of arguments.

13:10.000 --> 13:14.000
And of course, this is a recursive way of calculating factorial.

13:14.000 --> 13:16.000
So it's 3, 2, 1, 0.

13:16.000 --> 13:20.000
And there is also select slash 2.

13:20.000 --> 13:24.000
And this one takes like any term and looks for that term.

13:24.000 --> 13:28.000
So here it found, for example, it has an argument.

13:28.000 --> 13:30.000
Here it found it as a return value.

13:30.000 --> 13:32.000
But there is more.

13:32.000 --> 13:35.000
It can be hidden inside any lists, maps, tuples.

13:35.000 --> 13:42.000
So it will look recursively inside your data structures to find out anything you're looking for.

13:42.000 --> 13:45.000
So for example, you can look for an error message,

13:45.000 --> 13:51.000
even if it's called unknown error, which occurred to me once.

13:51.000 --> 13:53.000
And I just found this unknown error.

13:53.000 --> 14:01.000
I just put unknown error here and I just found the function that that causes, right, instantly.

14:01.000 --> 14:02.000
Okay, there is also filter.

14:02.000 --> 14:04.000
It's similar to select.

14:04.000 --> 14:06.000
But here you can pass any function.

14:06.000 --> 14:09.000
It's a bit slower, but it has more features simply.

14:09.000 --> 14:17.000
So you can, for example, assign a result to a search, to a variable,

14:17.000 --> 14:19.000
and then you can search in that list again.

14:19.000 --> 14:20.000
Oops, sorry.

14:20.000 --> 14:22.000
Then you can search in that list again.

14:22.000 --> 14:24.000
So you can narrow down your search.

14:24.000 --> 14:26.000
You got like two traces.

14:26.000 --> 14:31.000
Now you search in that two traces, but only for calls and you get only one.

14:31.000 --> 14:35.000
So another way to query it.

14:35.000 --> 14:42.000
And the tracebacks are very important for me because I want to, like, know the source where this originated,

14:42.000 --> 14:45.000
this particular, for example, function call.

14:45.000 --> 14:53.000
So here I'm just looking for any return value of one.

14:53.000 --> 15:01.000
And the sleepy factor of one actually matches it and it returned one.

15:01.000 --> 15:05.000
So this is returned, the traceback of this one.

15:05.000 --> 15:07.000
The call itself is first.

15:07.000 --> 15:08.000
It's right here.

15:08.000 --> 15:12.000
Sleepy factor of one and the rest is just the traceback.

15:12.000 --> 15:21.000
And the sleepy factor of zero returned also one, but it skipped because of some skipping logic.

15:21.000 --> 15:26.000
Yeah, it's details, but it helps you, like, limit the output that you get.

15:26.000 --> 15:38.000
Actually, you can disable that and you can get, like, all the traces with output all.

15:38.000 --> 15:45.000
Then you get, then you have no skipping of traceback that include another tracebacks.

15:45.000 --> 15:48.000
And you can limit the number of much traces.

15:48.000 --> 15:51.000
You can reverse call order.

15:51.000 --> 15:56.000
You can search in a different table, in a list, for example.

15:56.000 --> 16:04.000
And you can get only the first traceback if you want, very useful, just shortcut, let's say.

16:04.000 --> 16:17.000
And you can also get it for a particular record or just an index of a trace because there has just this auto-incremented indexes.

16:18.000 --> 16:24.000
Yeah, and similar to a traceback, you have ranges and ranges look inside.

16:24.000 --> 16:33.000
So traceback is like what's the source and ranges give you, like, all the traces starting with a function call until it's returned.

16:33.000 --> 16:35.000
Everything in between, from one process.

16:35.000 --> 16:46.000
And, yeah, so for example, here we are really looking for any traces that are just for function calls that have one as an argument

16:46.000 --> 16:51.000
and you get a range of traces from the call until the return.

16:51.000 --> 16:59.000
A range options, you can get, like, limit call depth is quite interesting and very useful

16:59.000 --> 17:04.000
because by having one you just get a call and return, which is very useful.

17:04.000 --> 17:15.000
And searching in a list of traces is also possible, getting only the first range if there are many, also possible.

17:15.000 --> 17:18.000
And getting the range for a specific trace.

17:18.000 --> 17:20.000
So quite a lot of options.

17:20.000 --> 17:26.000
I've just, you know, added, I've been adding and adding over a few years of development of these two.

17:26.000 --> 17:29.000
So they're all quite useful.

17:29.000 --> 17:33.000
Utilities, two simple utilities they wanted to talk about.

17:33.000 --> 17:35.000
One is to just look up a trace.

17:35.000 --> 17:39.000
Nothing fancy here, ETS lookup, does it, right?

17:39.000 --> 17:43.000
But then you can execute the trace, which is quite useful for me.

17:43.000 --> 17:47.000
So if this was a function call, I can just execute it right now, again.

17:47.000 --> 17:53.000
This is just, for example, let's say I fix the bug and then instead of writing some long code,

17:53.000 --> 17:58.000
I can just execute a trace and see if the result is the same or different.

17:58.000 --> 18:01.000
Or I can trace again, right?

18:01.000 --> 18:04.000
I can start the traces and trace again.

18:04.000 --> 18:06.000
Okay, now a bit of profiling.

18:06.000 --> 18:13.000
So I find this lightweight profiling very useful because it doesn't put as much stress on the system

18:13.000 --> 18:16.000
as F-Prof, for example, the Erlang profiler.

18:16.000 --> 18:19.000
And it's like instantly available.

18:19.000 --> 18:22.000
I don't have to prepare for it in any way.

18:22.000 --> 18:28.000
So call start, it's statistics aggregated by this function.

18:28.000 --> 18:32.000
So I'm aggregating everything under the total atom.

18:32.000 --> 18:36.000
So I'm getting like four calls and this accumulated time and this own time.

18:36.000 --> 18:41.000
These are equal because I'm just accumulating everything.

18:41.000 --> 18:48.000
But if I aggregated by function argument, you can see that there was this one call with each of the arguments.

18:48.000 --> 18:52.000
And this call took the longest time, but actually it's accumulated time

18:52.000 --> 18:56.000
because its own time was the shortest section, right?

18:56.000 --> 18:59.000
You can also do filtering here.

18:59.000 --> 19:04.000
So you can say when N is smaller than 3 and we just skipped one of them.

19:04.000 --> 19:10.000
So you can do that and you can sort them.

19:10.000 --> 19:14.000
Yes, you can sort them and print them as a table.

19:14.000 --> 19:18.000
We had some just nice utilities to have.

19:18.000 --> 19:23.000
And the last feature I wanted to talk about is function call 3 statistics.

19:23.000 --> 19:31.000
I called it like that because let's say we have a function that calculates the Fibonacci sequence

19:31.000 --> 19:36.000
in a suboptimal way, you probably all know that it's suboptimal.

19:36.000 --> 19:42.000
It branches a lot and let's clean up the traces.

19:42.000 --> 19:49.000
Trace again, call fib of 4 which returns 3, which is the correct value and stop tracing.

19:49.000 --> 19:54.000
So we now have different traces in the table and let's do it.

19:54.000 --> 19:57.000
Let's just call this function with default arguments.

19:57.000 --> 20:06.000
So it says that there is a call 3, I mean by that function calls, returns, everything inside repeated twice

20:06.000 --> 20:12.000
because there is this number 2 and it took 10 microseconds that there is no sleep in this example.

20:12.000 --> 20:15.000
So it took 10 microseconds in total.

20:15.000 --> 20:18.000
And this is how the function call 3 looks like.

20:18.000 --> 20:21.000
So you can see that, yeah, indeed, it repeated twice.

20:21.000 --> 20:26.000
So this can help you find out redundant code.

20:26.000 --> 20:27.000
Yeah.

20:27.000 --> 20:32.000
Okay, so this function also has some options but I don't have time to talk about them.

20:32.000 --> 20:35.000
You can just customize them a bit.

20:35.000 --> 20:44.000
And table manipulation, you can get the current table, dump it to a file and load it on a different Erlang node.

20:44.000 --> 20:47.000
And then you can continue analysis on a different Erlang node.

20:47.000 --> 20:50.000
And that's all I wanted to talk about.

20:50.000 --> 20:52.000
And that's me on a mountain bike. Thank you.

