WEBVTT

00:00.000 --> 00:14.880
So I'm Bruno Latulier and I'm working at EDF which is an electricity utility company

00:14.880 --> 00:24.080
in France and we are doing a lot of numerical simulation and of course we need to have a

00:24.080 --> 00:32.080
good verification and validation process and inside the verification process we have to take

00:32.080 --> 00:39.600
care about the floating point error. So I will give more detail about the floating point but I

00:39.600 --> 00:49.120
will be really short because I think almost everybody knows about it. We have in the when we

00:49.120 --> 00:55.040
are doing numerical simulation we design our algorithm with real number is the wonderful

00:55.040 --> 01:01.760
world of the mathematics but we have limited precision and so we have to use float or double

01:01.760 --> 01:10.000
in our code and it means that some usual in our code we have to do one thing and this small

01:10.000 --> 01:16.400
one thing because in double it's 10 to the power minus 16 it's really small this small one thing can

01:16.880 --> 01:24.560
have a huge impact for the final result of for simulation. So we need to be able to

01:25.280 --> 01:30.240
to have an estimation of the difference of the floating point computation and

01:31.440 --> 01:40.240
and the result we expect in the in the mathematical world and usually the developer that's our problem

01:40.240 --> 01:47.760
but usually the developer are able to see that there is a problem because when you modify your

01:47.760 --> 01:55.280
compiler option when you want to add parallelism you change the order of parenthesis and you have

01:55.280 --> 02:02.720
different results and so you see you have a problem but the problem is there and so we need a tool

02:02.720 --> 02:10.160
to be able to do this error estimation for real industrial complex application and it's the

02:10.880 --> 02:21.280
tool we called Vero which has this objective so I'm sorry but I need one slide of mathematics

02:23.360 --> 02:30.960
so it's quite easy we use stochastic arithmetic so usually when we want to debug

02:30.960 --> 02:37.440
we we don't like stochastic but there we are we want to we will use the stochasticity to debug

02:38.400 --> 02:48.240
so we replace the operation by the same operation but with stochastically random

02:49.680 --> 02:57.200
stochastically rounding so I tacadize and it's to the left or to the right with a defined probabilities

03:00.240 --> 03:05.200
I'm doing that for all the operations in my program so it's like a galton ball

03:05.760 --> 03:12.160
each operation I go to the left or to the right and at the end of the program I have kind of

03:12.160 --> 03:19.360
distribution and I use the support of this distribution with the formula there to compute

03:19.360 --> 03:27.120
the number of significant bits or the number of significant digits on this small program one divided

03:27.120 --> 03:33.200
by three followed by a multiplication by three this is the normal execution of the program with

03:33.280 --> 03:41.680
one to the nearest and after I put three execution with random rounding and I use all this result

03:41.680 --> 03:49.200
with the formula to see that I have almost two significant digits so it looks like really easy

03:49.200 --> 03:57.920
like this but if you have to modify all your program all your floating point operation in your

03:57.920 --> 04:07.600
code it will be really tough so the idea is to use valguine a dynamic binary instrumentation

04:09.200 --> 04:18.480
which is help a lot to develop this kind of tool valguine will give me an intermediate

04:18.480 --> 04:26.160
representation and I can just modify the intermediate representation so I don't I'm not able to write

04:26.320 --> 04:34.560
one line of assembly and I will do this kind of tool so valguine is really powerful and so when

04:34.560 --> 04:42.880
there is an operation an integer operation I can replace I can use the same when there is a floating

04:42.880 --> 04:49.840
point operation I can add counter that's easy and I can call my own implementation of the

04:49.840 --> 04:58.640
floating point operation and in this operation I can add the stochastic part of the operation

04:59.360 --> 05:06.400
for the user point of view I need to run the code several times that's the bad part

05:08.000 --> 05:16.560
I need to extract the value of interest I'm computing something I have to know what I want to

05:16.560 --> 05:24.720
compute what is the result the good part it works for all languages so C++, Fortran, Python

05:25.680 --> 05:33.600
yeah and it works with external library when we don't have the source from the valguine developer

05:33.600 --> 05:41.520
point of view it means I have to replace all floating point operation what is nice I there is

05:41.600 --> 05:48.960
no need of shadow memory so it's quite fast and what is really different compared to the other

05:48.960 --> 06:00.000
tools I want to modify the result and so this is the difficult part so I give you a small example

06:00.880 --> 06:10.320
which is called the Mueller suite so it's I compute a suite with recurrence and add some verbo

06:10.320 --> 06:20.240
CT to be like or simulation so there is this kind of result this is the execution with

06:20.240 --> 06:26.960
rounding to the nearest and there is address it's stupid but it will it's it's only a way to

06:28.000 --> 06:34.400
to present something later but it's stupid but I see really often this kind of thing we don't

06:34.400 --> 06:43.520
control or user and so now I run several times with veroo and in red there is all the all the

06:43.520 --> 06:49.280
output which is different so I can see there the result is completely wrong

06:52.960 --> 07:01.200
so when if I say to my colleagues you have a tricky floating point bug somewhere in your

07:01.200 --> 07:10.480
two millions line of code you will get a lot of friends so I need to do something and with one

07:10.480 --> 07:17.600
colleague we we developed something called delta debug which is a trial and error search algorithm

07:18.240 --> 07:26.640
so the user has to provide two script the first one is how I call my program with veroo so it's

07:26.640 --> 07:33.360
very simple because it's only a prefix command where the valguin prefix command and another script to

07:33.360 --> 07:41.760
say if the result is good or not and to say if the result is good or not we are doing a comparison

07:41.760 --> 07:48.160
with the result to the nearest we don't know if the reference is a good one we only know that

07:48.160 --> 07:56.000
there is a difference as is the difference we want to explain and and then we are we are with

07:56.000 --> 08:03.760
this command we we have to say the number of samples we need and at the end you will get the result

08:03.760 --> 08:13.200
there is problem in these two lines of code that's really nice it corresponds to the two

08:13.200 --> 08:23.200
divisions in my example from the valguin developer point of view it means I need to generate the

08:23.200 --> 08:31.920
search space I have to know which line or which symbol in my program where there is floating point

08:31.920 --> 08:37.360
operation so this is the first part and the second part I have to be able to run the program

08:38.320 --> 08:46.320
with a specific configuration I mean a set of function of line which are instrumented and a

08:47.280 --> 08:54.240
set which are where the line are not instrumented so I have to have to introduce

08:54.240 --> 09:01.120
discussion between this tool and valguin but it's not too difficult to do

09:04.080 --> 09:12.640
and it works well on real application but now I will present something which is more experimental

09:13.600 --> 09:20.560
I really go there

09:28.640 --> 09:36.480
I with with the two lines of code sometimes you do you do not have the right information what you

09:36.480 --> 09:41.920
want sometimes is to know the problem happened in the first iteration not in the last one

09:42.800 --> 09:52.000
and this corresponds to temporal localization and so for that I need to to modify the search space

09:52.640 --> 10:00.960
and I use in fact the the output of my program and I can't use directly the output because

10:00.960 --> 10:06.560
from one line to the other I need to to have the same key of all in all execution

10:06.640 --> 10:15.200
so it means I have to wildcard all floating point in the form for the search for the search space

10:15.760 --> 10:22.400
and especially when someone print addresses I have to wildcard addresses because it will be

10:22.400 --> 10:31.120
different in all in in different one and for the users there is nothing else to to modify in it the

10:31.120 --> 10:38.480
result is it happened in begin it and and it is also it's a two first iteration the problem

10:38.480 --> 10:49.680
happened only in the two first iteration so the standard output match without temporal context

10:49.680 --> 10:56.720
I only use the the fact that the user prints the number of iteration

10:57.680 --> 11:04.800
that's very important the user has to pay attention to the bufferization

11:06.000 --> 11:13.840
if you put if you print everything at the end of your program it's unuseful the empty line can

11:13.840 --> 11:22.080
be ignored and I can modify the the output by a filter script and the two last element used

11:22.080 --> 11:27.040
together we are able to to work group iteration we are able to do a lot of things

11:29.280 --> 11:38.560
and from the valgoin developer I have to define a five format to define the interaction between

11:39.200 --> 11:48.080
the IO so this is the standard output or even a file and veroo like client request the idea is to

11:48.080 --> 12:03.280
be able to call client request activated by the IO so my conclusion with veroo we are able to estimate

12:03.280 --> 12:10.800
floating point error it works well we are able to search the origin of floating point error

12:10.800 --> 12:17.040
with delta debuil it works well we are able to search mixed precision configuration it works

12:17.920 --> 12:24.880
well but not so much and we are also able to search where error are amplified

12:26.800 --> 12:37.520
it works sometimes and in my roadmap I want to to be able to to work on new architecture

12:37.520 --> 12:46.720
especially arm 64 I want to add new search space like the bike trace because if someone has

12:46.720 --> 12:52.240
encapsulated the addition it will give me that the addition is unstable nice

12:54.960 --> 13:03.440
and the last point and it will be probably the the most difficult research part is to be able to

13:04.400 --> 13:10.960
to to do error amplification localization without false positive that's the that's the key point

13:11.840 --> 13:20.560
and for the real conclusion it's on github there is documentation there with paper and so if you

13:20.560 --> 13:33.680
want to use it I will be happy

13:41.440 --> 13:47.680
so I imagine quite a few runs are needed because I guess for each floating point operation

13:47.680 --> 13:50.960
you have multiple combinations and can you give some

13:52.080 --> 13:55.920
numbers from experience how many you run one needs to track down bugs

13:59.760 --> 14:06.640
yeah the the question is how many samples we need to be able to do an error estimation

14:08.800 --> 14:15.920
it depends off the accuracy you want if the if your code is unstable and

14:18.640 --> 14:25.200
only to run run with the nearest the first one you have difference you have a bug

14:26.160 --> 14:32.080
so to have a problem it's a really small number if you want to prove that there is no bug

14:33.440 --> 14:37.280
statistically you will need to increase the number of sample

14:39.040 --> 14:46.160
and we have a paper I did there is a paper to say how many number you you need with

14:46.800 --> 14:55.120
with the confidence interval and everything but in practice it depends off the running time

14:55.840 --> 15:03.920
because it's it's always it's always the first question and now we have done the work to be able

15:03.920 --> 15:11.040
to have a number of samples and with theory and nobody use it the reason is is the computational

15:11.040 --> 15:16.000
time which is important

15:30.560 --> 15:39.680
in the interfloor project we are we have collaboration with colleagues of Versailles

15:41.280 --> 15:48.640
near Paris and they are doing all the almost the same work with LLVM infrastructure

15:51.200 --> 15:56.080
and so we are working together and it's a little bit faster

15:57.840 --> 16:03.680
but in fact it's more convenient to use from the binary point

16:11.920 --> 16:18.320
okay question did you at any point contemplate instead of using the stochastic method of

16:18.320 --> 16:22.640
figuring errors actually having your valgrain model of the floating-wave instructions use

16:23.680 --> 16:28.880
interval math that is represent bounds and then propagate the errors then you wouldn't have to

16:28.880 --> 16:32.960
rerun anything you just calculate with an upper and lower bound for each value and

16:34.800 --> 16:40.000
the problem with interval arithmetic what is now first what is really nice with interval

16:40.000 --> 16:50.160
arithmetic right the question is is it better to use interval arithmetic instead of stochastic

16:50.160 --> 16:59.840
arithmetic what is nice with interval arithmetic it don't lie never it say it will say the truth

17:00.800 --> 17:07.440
and it's really nice but the problem is on real industrial application it give the result is

17:07.440 --> 17:14.880
between minus infinity and plus infinity and it's true so that's the problem there is a lot of

17:14.880 --> 17:26.800
false positive in fact I'm I really use I use interval arithmetic when I discover a problem

17:26.800 --> 17:35.440
like that with this tool I'm I extract the problem I work on a small proxy app where I

17:35.760 --> 17:47.360
I'm able to to run interval arithmetic but with with multi precision to increase the precision in

17:47.360 --> 17:55.120
order to reduce the size of the interval as there it's it's really nice because I've I've access to

17:55.120 --> 18:01.120
the guantt's tool but on real industrial application it's it's too difficult

18:25.680 --> 18:38.240
it it works with acc avex avex too but I there was limitation with avex 512 because

18:38.240 --> 18:43.120
it's not implemented in valguine

18:55.600 --> 19:07.120
so this is a question about I think the mathematical library

19:11.840 --> 19:17.840
there is two way to do mathematical to to call mathematical function the first way is sometimes

19:17.840 --> 19:27.120
there is hardware and now I'm able to format wire to instrument fma and sqrt

19:29.840 --> 19:39.280
and the other way is to call the dynamic library of mathematical function and there is way tough

19:39.840 --> 19:48.640
because all the the developer of the mathematical library really know floating point operation

19:48.640 --> 19:55.440
and they are taking into account the fact that they they use one to the nearest operation and so

19:55.440 --> 20:02.880
if you use veroo with stochastic wounding on the mathematical library you can get

20:02.960 --> 20:09.760
stake fault you can get a lot of you you have a lot of problem it means that I have to exclude

20:10.720 --> 20:17.200
from the instrumentation the mathematical library and I have to re-implement myself

20:18.400 --> 20:24.000
the all the mathematical library to add the stochastic part

20:24.720 --> 20:34.320
uh that's quite tricky and uh and in fact I'm using some some thing which are a little bit

20:34.320 --> 20:44.720
limits because I use a reference with the quad mat and uh yeah it there it's okay but if I speak to

20:45.040 --> 20:49.440
to to to specialist of floating point they will kill me

20:53.680 --> 20:59.040
officially we have one minute for one last question but this is the last one

21:02.960 --> 21:05.040
I didn't try so you know what you get

21:05.040 --> 21:16.560
it so I worked in a project once where people had started using this minus f unsafe math in

21:16.560 --> 21:26.080
gcc which basically creates such issues and but they weren't it was a trade-off that they did

21:27.120 --> 21:34.400
performance but they wanted some more reproducibility would this tool work to kind of narrow it down

21:34.400 --> 21:39.920
to certain code paths where you might want to disable the fun safe math

21:43.120 --> 21:53.920
yeah uh the question is uh what what is the the liberty of what kind of freedom can I give to

21:53.920 --> 22:02.240
the compiler to uh to optimize my code in terms of floating point if I understand well and uh

22:02.240 --> 22:10.480
and in fact it's an open question it can help in in in the in that sense um I'm able to um to say

22:10.480 --> 22:21.440
where the code is sensitive but it's related to my test case because it's it's uh I'm not able to

22:21.440 --> 22:31.280
say anything of about the code in general it's the code plus the data set so to help a compiler

22:31.280 --> 22:37.440
it's really tough because the compiler needs to be able to run for its kind of data set

22:40.560 --> 22:48.240
and and the other part is uh or the question is uh which kind of option should we use

22:49.280 --> 22:56.800
for for the for the compiler what I see is a lot of people want to be reproducible and so they use

22:56.800 --> 23:03.200
all zero option and so the only thing they are doing is to be able to reproduce the wrong result

23:04.480 --> 23:13.200
because if you don't know why all zero is better than all three there is a good probability that all

23:13.200 --> 23:23.040
three uh will give you a better result because uh when you are doing a summation you will regroup

23:23.040 --> 23:32.640
it for uh for with the three uh when uh there is fmi there is one error one one

23:32.640 --> 23:44.640
ending error less so it's if if I have an advice you use all the compiler option you can and except

23:44.640 --> 23:52.240
if you know what you what you do uh there are small parts in veroo where I have to take care about uh

23:53.440 --> 24:00.960
floating point error really carefully if you if you use uh error free transformation you have to

24:00.960 --> 24:07.840
take care about floating point error uh so I don't know if you know what is error free transformation

24:08.480 --> 24:16.320
no nobody knows the error free transformation so it's it's a way to when we do a floating

24:16.320 --> 24:22.400
point operation we are able to compute the error on the floating point of of this error

24:22.960 --> 24:29.760
usually is we can represent this number and so we can compute the error and if you

24:30.720 --> 24:37.120
say to a compiler uh the fast math option you will say this error is zero because it's

24:37.120 --> 24:47.360
mathematically is zero and uh it will skip it so if you want to do um tricky algorithm with this

24:47.360 --> 24:54.080
kind of thing uh you have to protect it uh you have to protect your part in fact this part

24:54.080 --> 25:01.280
are used in twasic to be sure that the compiler is not able to to skip it if I make there is a

25:01.280 --> 25:11.120
misconception in floating point real that people want accuracy and reproducibility

25:12.080 --> 25:15.520
and they think they are the same but they are completely different

25:17.040 --> 25:21.200
and you can be inaccurate but they were reproducible or the other way around

25:23.440 --> 25:30.080
and actually in this case the inaccuracy wasn't the problem uh it was more the reproducibility

25:30.640 --> 25:34.480
because the the accuracy wasn't that important that's why people that probably at some point

25:34.480 --> 25:40.320
decided okay I can do so much more in the limit of the amount of time in real time context but let's

25:41.520 --> 25:44.480
see

25:49.360 --> 25:50.880
is that accurate

25:58.720 --> 26:01.020
no

26:01.020 --> 26:04.000
coronavirus

26:04.000 --> 26:08.320
system

26:11.120 --> 26:16.560
Valgrind is not natively possible to run like a ELC or some other end-gain system?

26:16.560 --> 26:29.920
I need Valgrind. In fact, I have less portability than Valgrind because I work for x8664.

26:29.920 --> 26:36.720
This works. I'm working for ARM and this is tough.

26:37.040 --> 26:39.040
This is really tough.

26:39.040 --> 26:44.480
I will need to do patch inside Valgrind for that.

26:44.480 --> 26:50.480
And it's not my... I'm not really confident to do that.

26:50.480 --> 26:57.280
But for x86 it was not easy but it was okay.

26:57.280 --> 27:00.800
Is there no solution for all PLC systems?

27:00.800 --> 27:02.800
Not yet.

27:02.800 --> 27:11.760
That question. Do you have plans to include this into the upstream version of Valgrind?

27:11.760 --> 27:15.760
At some point.

27:15.760 --> 27:20.720
It could be nice. I will like that.

27:20.720 --> 27:28.400
In fact, I think there is still a lot of work because I have only one architecture.

27:28.960 --> 27:37.200
And in the test infrastructure it's quite different because in the Valgrind architecture

27:37.200 --> 27:40.080
we need to do only once a test.

27:41.040 --> 27:44.480
And I have to run several times to be able to compute.

27:44.480 --> 27:50.080
And so I have a completely, I have a separated test infrastructure.

27:50.960 --> 27:55.360
So it couldn't be tested as part of the modern world?

27:56.320 --> 27:59.920
Yeah. It would be difficult I think.

28:01.360 --> 28:06.400
There is work and I think I will need to discuss with the Valgrind team.

28:08.400 --> 28:10.400
It's why I'm there.

28:10.400 --> 28:16.080
I want to begin the discussion with the Valgrind team.

28:16.080 --> 28:24.800
Do you have something about the performance panel?

28:24.800 --> 28:28.800
I was expecting it was the first question.

28:28.800 --> 28:38.800
So with the nearest it correspond to the instrumentation.

28:38.800 --> 28:45.600
And I'm doing a dirty call and I'm doing the same operation.

28:45.600 --> 28:54.240
With float it was not really optimized.

28:54.240 --> 28:55.920
I mainly work with double.

28:56.560 --> 29:01.520
And so it's quite acceptable.

29:01.520 --> 29:08.400
But it's not, the program is a stencil with FMA.

29:08.400 --> 29:15.360
So it's more difficult than a lot of other code.

29:15.440 --> 29:21.200
And if you are running with a code where there is only IO it's really fast.

29:22.320 --> 29:30.720
So but if you want to work with a Blast tree it's worth a lot.

29:31.760 --> 29:38.560
So the one I presented it's only the random part.

29:39.200 --> 29:44.080
And there is a lot of other kind to do randomness between upward and downward.

29:45.600 --> 29:49.200
And the other kind is to over some crazy false positive.

29:52.960 --> 30:00.880
So and FMA only is what I will get in Valgrind when I will be able to modify Valgrind.

30:00.880 --> 30:04.000
Because in Valgrind there is a there is a

30:06.880 --> 30:10.080
the FMA is implemented with the software.

30:10.640 --> 30:14.160
With the software and so there is a lot of operation.

30:14.160 --> 30:18.400
So we can reduce the time of the tool none.

30:18.400 --> 30:21.760
Tool none is yeah I'm faster than the tool none.

30:21.760 --> 30:23.360
So it means that there is a problem.

30:25.520 --> 30:33.760
So I discovered this last week so I'm not able to correct everywhere.

30:33.760 --> 30:41.520
And for me it's important even if I because when I'm doing Delta debug I do not instrument.

30:42.320 --> 30:47.440
I do not instrument some part with FMA and so it's costly.

30:47.440 --> 30:54.320
So I really need to to to modify Valgrind to reduce the performance cost.

30:54.320 --> 31:02.960
Because it's really painful to say I'm doing nothing and it's more costly than to modify the 14.0 behavior.

