WEBVTT

00:00.000 --> 00:18.320
So, hello everyone.

00:18.320 --> 00:19.320
I'm Pavel.

00:19.320 --> 00:25.160
I'm very excited to be here and I will speak about Prometheus and OpenTelemetry and especially

00:25.160 --> 00:30.920
how we can use OpenTelemetry project to scrape Prometheus metrics and what are the challenges

00:30.920 --> 00:33.840
with this setup.

00:33.840 --> 00:37.000
Quickly about myself, I'm Pavel, software engineer at Red Hat.

00:37.000 --> 00:41.160
I mainly work in the distributed tracing space.

00:41.160 --> 00:46.760
I'm contributor maintainer of the OpenTelemetry operator, Grafana tempo operator and the

00:46.760 --> 00:47.760
Yeager project.

00:47.760 --> 00:54.160
If you would like to reach out to me, you can do that on the Twitter, on the CNCF Slack.

00:55.160 --> 01:02.200
So, today I would like to do some introduction into metrics ecosystem so we better understand

01:02.200 --> 01:09.640
what are the projects we can use and then talk about the differences in Prometheus and OpenTelemetry

01:09.640 --> 01:14.400
from the data model perspective, how they do things.

01:14.400 --> 01:20.240
Then we'll talk about what Prometheus components we can find in OpenTelemetry project, both

01:20.320 --> 01:24.640
from the API SDK perspective and in the collector.

01:24.640 --> 01:27.240
The second half will be a live demo.

01:27.240 --> 01:33.240
We will deploy very simple Golang application instrumented with Prometheus client and we

01:33.240 --> 01:36.760
will gather those metrics with OpenTelemetry collector.

01:36.760 --> 01:41.840
All right, so why are we actually here?

01:41.840 --> 01:46.800
We are here because the ecosystem for collecting metrics is fragmented.

01:46.840 --> 01:50.840
There are different projects that provide different capabilities.

01:50.840 --> 01:56.160
So, there is a storage, some projects that can store metrics, some projects that can

01:56.160 --> 02:01.960
only define protocol for something metric data and some projects that can be used only

02:01.960 --> 02:06.720
as an API SDK, something that developers use.

02:06.720 --> 02:13.720
Prometheus sits in between, so it provides kind of end-to-end framework for collecting,

02:13.920 --> 02:19.760
sending, storing, visualizing and alerting on metrics.

02:19.760 --> 02:25.120
Prometheus is very well-adopted, it's very robust and people know how to use it.

02:25.120 --> 02:30.280
On the other hand, there is OpenTelemetry project, which is kind of new and for metrics,

02:30.280 --> 02:36.160
it provides kind of more limited set of capabilities compared to Prometheus.

02:36.160 --> 02:40.480
People still want to use OpenTelemetry for collecting metrics because they can use it

02:40.480 --> 02:47.480
as well for collecting other signals like traces, logs and it's better integrates with

02:49.080 --> 02:54.640
third-party vendors, your SaaS observability solutions.

02:54.640 --> 03:00.640
So the overlap, there is in the API and SDK, Prometheus has clients, OpenTelemetry has

03:00.640 --> 03:05.200
an API and SDK and then there is a protocol.

03:05.200 --> 03:11.480
Prometheus has its own metrics protocol and OpenTelemetry has OTLP protocol.

03:11.480 --> 03:17.000
On top of that, in OpenTelemetry there is collector, which competes with Prometheus agent.

03:17.000 --> 03:23.000
Agent doesn't store metrics, it can just scrape them and send them to Prometheus via OTLP,

03:23.000 --> 03:28.480
not OTLP, but Prometheus remote write.

03:28.480 --> 03:33.440
What I would like to highlight is that OpenTelemetry as well has the auto-instrumentation libraries,

03:33.440 --> 03:36.360
which are not present in Prometheus.

03:36.360 --> 03:42.440
I think it's a great innovation in open source because those libraries, as we saw in the previous

03:42.440 --> 03:47.520
talk, they help you to very quickly instrument your application without any code changes

03:47.520 --> 03:49.960
and a recompilation.

03:49.960 --> 03:56.480
So I think it lowers the bar of adoption of telemetry in your organization.

03:56.480 --> 03:57.840
So that's the ecosystem.

03:57.840 --> 04:03.080
Then we should think about how we can use these systems together because we want to

04:03.080 --> 04:07.360
combine feature set that they offer to us.

04:07.360 --> 04:13.600
So let's take a look before we go into the demo, what are the differences in Prometheus

04:13.600 --> 04:16.960
and OpenTelemetry.

04:16.960 --> 04:20.120
First of all, the most obvious one is how the protocol works.

04:20.120 --> 04:26.480
The Prometheus will pull the metrics from your process and OpenTelemetry, you have to

04:26.600 --> 04:29.480
push the metrics into the collector.

04:29.480 --> 04:30.800
It's not big of deal.

04:30.800 --> 04:34.120
Some protocol might be better for some use cases.

04:34.120 --> 04:38.960
So for instance, the push might be better if you have short-lived processes and you

04:38.960 --> 04:44.040
need to quickly offload the data before the process shuts down.

04:44.040 --> 04:49.160
On the other hand, pool works very well in Kubernetes.

04:49.160 --> 04:53.200
I don't think that's kind of a blocker when using these two systems together.

04:53.440 --> 05:00.240
However, the second point, the data temporality, I think it's kind of a big deal.

05:00.240 --> 05:08.120
The Prometheus uses cumulative temporality, which means that the last observation contains

05:08.120 --> 05:10.720
the previous observations.

05:10.720 --> 05:16.760
So if you have a counter in Prometheus, it will contain the sum, the aggregation of all

05:16.760 --> 05:19.120
the previous values.

05:19.120 --> 05:24.040
In OpenTelemetry, we can use as well cumulative temporality, but we can as well use delta

05:24.040 --> 05:30.320
temporality, which means that the observations that are sent over the wire will be just deltas.

05:30.320 --> 05:36.160
So if people are coming to this room, it will just send one, one, or maybe two if two people

05:36.160 --> 05:38.840
entered at the same time.

05:38.840 --> 05:42.240
And Prometheus cannot ingest delta temporality metrics as far as I know.

05:42.240 --> 05:45.880
So that's a problem.

05:45.880 --> 05:53.360
The second difference, or the third difference, is the histograms, or the exponential histograms.

05:53.360 --> 05:58.040
As far as I did the research, I think they are almost compatible.

05:58.040 --> 06:04.040
However, in the OpenTelemetry, the histogram as well contains min and max values.

06:04.040 --> 06:11.480
So in Prometheus, you can potentially lose some precision of what was observed.

06:11.480 --> 06:14.080
The next difference is the resource attributes.

06:14.080 --> 06:20.000
In OpenTelemetry, when you collect data, there is a resource object that contains information

06:20.000 --> 06:24.480
about the process that is sending the data, which is a pot.

06:24.480 --> 06:32.640
It contains pot label, deployment label, replic acid label, node label, and all those things.

06:32.640 --> 06:35.600
In Prometheus, the concept doesn't exist.

06:35.600 --> 06:39.880
All the labels go to the metric usually.

06:39.880 --> 06:44.280
There is a workaround to put these labels into the target info metric and then do the

06:44.280 --> 06:45.280
joint.

06:45.280 --> 06:49.040
However, it kind of complicates the user experience because you need to do additional

06:49.040 --> 06:52.720
join when querying the data.

06:52.720 --> 06:56.200
The next difference is float versus int.

06:56.200 --> 07:00.360
Prometheus uses floats, and OpenTelemetry can use float and int.

07:00.360 --> 07:04.160
I don't think it's a blocker because with float you can represent very well all the

07:04.160 --> 07:06.600
metrics.

07:06.600 --> 07:15.480
And last major difference is the character set that the system supports for metric names

07:15.480 --> 07:16.960
and label names.

07:16.960 --> 07:22.520
In OpenTelemetry, we can use UTF-8 in Prometheus, only a limited set of characters.

07:22.520 --> 07:30.520
So what happens is that when you are sending hotel labels, they will get corrected to the

07:30.520 --> 07:32.560
form that Prometheus can ingest.

07:32.560 --> 07:39.000
So if there are dots, they will be substituted to underscores, for instance.

07:39.000 --> 07:42.480
So as I said, I was working in the distributed tracing space for a long time and I started

07:42.480 --> 07:43.480
doing some metrics.

07:43.480 --> 07:48.560
And when I did this research, I was even wondering if these systems work, right?

07:48.560 --> 07:54.080
Because there is kind of a lot of things that can go wrong.

07:54.080 --> 07:57.440
And I think the delta temporality might be the biggest one.

07:57.440 --> 08:02.600
So I started looking into how can I solve this problem.

08:02.600 --> 08:12.280
And in the OpenTelemetry SDKs, the OTLP exporter that exports OTLP data, it can be configured

08:12.280 --> 08:18.200
to translate delta temporality metrics to cumulative with this environment variable

08:18.200 --> 08:21.280
that you can see on the slides.

08:21.280 --> 08:28.280
And then as well, you can set it to delta if your metric system supports delta or to

08:28.280 --> 08:31.720
love memory, which will use even more delta.

08:31.720 --> 08:36.080
You may as well ask the question like why we have two temporalities, right?

08:36.080 --> 08:38.280
There is a cumulative and delta.

08:38.280 --> 08:43.240
And as far as I understand, the delta temporality can be more resource efficient when you are

08:43.240 --> 08:49.280
instrumenting your process because the SDK doesn't have to track the summary, right?

08:49.280 --> 08:54.600
They will just quickly send the deltas to the collector or process that is collecting

08:54.600 --> 09:03.520
the data and doesn't have to do that processing that the cumulative metric store is doing.

09:03.520 --> 09:06.080
Okay.

09:06.080 --> 09:08.680
And then the temporality, okay, it's a problem.

09:08.680 --> 09:14.960
And then in the Prometheus exporter in OpenTelemetry ecosystem, it will do some delta to cumulative

09:14.960 --> 09:17.680
temporality translation for you.

09:18.400 --> 09:24.480
However, if you are using Prometheus exporter in the hotel SDKs, they will most likely drop

09:24.480 --> 09:26.000
delta metrics.

09:26.000 --> 09:29.800
So that's something to watch for.

09:29.800 --> 09:30.800
Okay.

09:30.800 --> 09:35.640
So what are the Prometheus components in hotel ecosystem?

09:35.640 --> 09:39.080
In the SDKs, as I mentioned, there is Prometheus exporter.

09:39.080 --> 09:44.600
However, if your metrics are delta temporality, they will most likely be dropped.

09:44.600 --> 09:49.880
As far as I was going through the code and looking at the exporter implementation, maybe

09:49.880 --> 09:54.160
it's not the case in every language, but I was looking, I think, at Golang and Java

09:54.160 --> 09:57.440
and that's what I saw.

09:57.440 --> 10:01.160
In the OpenTelemetry collector, there are three components.

10:01.160 --> 10:04.280
There is Prometheus receiver that we will see in a demo.

10:04.280 --> 10:13.000
Then there is Prometheus exporter that will try to handle temporality correctly.

10:13.000 --> 10:18.360
And then there is remote write, which will most likely drop your delta temporality metrics.

10:18.360 --> 10:20.080
Okay.

10:20.080 --> 10:23.360
So let's try what I prepared.

10:23.360 --> 10:29.600
It's a very simple hello world style application written in Golang, instrumented with Prometheus

10:29.600 --> 10:30.600
client.

10:30.600 --> 10:37.120
And then we will have an OpenTelemetry collector with Prometheus receiver scraping those metrics

10:37.120 --> 10:43.800
and exposing the same metrics on the collector slash metrics endpoint through Prometheus exporter.

10:43.800 --> 10:47.320
So we have receiver and exporter.

10:47.320 --> 10:51.920
And addition to that, we will print the metrics into the standard output of the collector.

10:51.920 --> 10:58.200
And we will compare if they're correctly propagated.

10:58.200 --> 11:00.600
So let me jump back to my console.

11:00.600 --> 11:03.360
I guess it's too small.

11:03.360 --> 11:13.760
I'm not sure I can change the color.

11:13.760 --> 11:17.720
It's better.

11:17.720 --> 11:20.200
Okay.

11:20.200 --> 11:23.120
So just for reference, this is the app.

11:23.120 --> 11:26.280
It's just main class.

11:26.280 --> 11:31.240
Using Prometheus client defines a gauge for tracking the version.

11:31.240 --> 11:37.600
There is a counter for counting requests and histogram for counting the request duration

11:37.600 --> 11:41.520
and some HTTP endpoints.

11:41.520 --> 11:42.840
So the app is running.

11:42.840 --> 11:52.520
I will just forward the endpoints and refresh the make request.

11:52.520 --> 11:54.880
It's a hello world, nothing special.

11:54.880 --> 11:57.520
We're going to see the metrics.

11:57.520 --> 12:05.440
We get a histogram counter and gauge and not many labels.

12:05.440 --> 12:12.680
As a next step, we're going to deploy the collector, which is again a very simple setup.

12:12.680 --> 12:15.280
We are deploying a deployment.

12:15.280 --> 12:20.680
And then we have a Prometheus receiver with a static configuration.

12:20.680 --> 12:24.680
So in a collector config, you can have multiple receivers of the same type.

12:24.680 --> 12:26.560
So I have two Prometheus receivers.

12:26.560 --> 12:29.160
One is called static, one is SD.

12:29.160 --> 12:37.360
We're going to use the static which will scrape the Prometheus example app service.

12:37.360 --> 12:41.600
And as you can see, this config is very similar to what you see in Prometheus.

12:41.600 --> 12:46.560
So you can potentially copy paste your Prometheus config into the collector config for Prometheus

12:46.560 --> 12:50.640
receiver and it should work.

12:50.640 --> 12:53.720
And last step, what we're going to do, we're going to enable the receiver in the metrics

12:53.880 --> 12:58.760
pipeline to make it active.

12:58.760 --> 13:04.000
And now I'm going to deploy it.

13:04.000 --> 13:16.280
As you can see, the collector is up and running.

13:16.280 --> 13:23.000
And I will pour forward again the metrics end points now of the collector.

13:23.000 --> 13:32.280
And we see kind of the same metrics, right?

13:32.280 --> 13:39.680
Here's 18, here's 19 because the Prometheus scraped end points with increased the counter.

13:39.680 --> 13:42.760
And what has changed are the labels, right?

13:42.760 --> 13:48.720
Now I see the instance label, which is the service name and the job which I defined in

13:48.720 --> 13:56.360
the collector config called app job.

13:56.360 --> 14:02.560
And then, yeah, we see the same metrics, the histogram, the version counter and the

14:02.560 --> 14:04.560
direct-quist counter.

14:04.560 --> 14:09.520
Okay, as a next step, we're going to make it a bit more automated.

14:09.520 --> 14:15.160
We're going to use the Prometheus service discovery in the second receiver.

14:15.160 --> 14:18.120
So we need to define the Prometheus as the config.

14:18.120 --> 14:22.720
And in this case, we're going to scrape all the pots that have the label that our app

14:22.720 --> 14:24.000
is using.

14:24.000 --> 14:27.360
Our pot defines this label.

14:27.360 --> 14:34.120
So we're going to enable it by just, you know, overriding the name of this receiver.

14:34.120 --> 14:36.400
It's the same functionality that Prometheus supports, right?

14:36.400 --> 14:41.200
I'm just using it in the open telemetry collector.

14:41.200 --> 14:45.880
It should restart.

14:45.880 --> 14:47.880
It's up and running.

14:47.880 --> 14:55.360
We're going to forward.

14:55.360 --> 14:57.880
And now, again, the same metrics.

14:57.880 --> 15:00.040
What has changed are the labels.

15:00.040 --> 15:02.520
The instance is the pot, right?

15:02.520 --> 15:06.920
Which makes more sense if we are configuring the service discovery for pots.

15:06.920 --> 15:08.560
The job name changed to Kubernetes.

15:08.560 --> 15:10.880
This is what we defined.

15:10.880 --> 15:17.040
In addition to that, now we get the target info, which defines the additional labels

15:17.040 --> 15:19.360
the receiver discovered.

15:19.360 --> 15:25.440
So here I see the namespace, the node name, the pod name.

15:25.440 --> 15:30.560
I think it's readable.

15:30.560 --> 15:35.320
And so what I can do right now, I can write Prometheus query that will do joint and get

15:35.320 --> 15:38.320
all these labels associated to the metric.

15:38.320 --> 15:45.400
Or in the collector, I could write a configuration that will put these labels from the target

15:45.400 --> 15:50.880
info into the metric labels directly, which will simplify the query.

15:50.880 --> 15:54.800
However, it will create more time series in Prometheus.

15:54.800 --> 15:55.800
Okay.

15:55.800 --> 16:07.240
And as the last step, we're going to use the pod monitor for the pod that we deployed.

16:07.240 --> 16:15.080
And we're going to use collector to get this pod monitor, configure the receiver, and scrape

16:15.080 --> 16:16.600
the metrics.

16:16.600 --> 16:25.240
So the way how it works in OpenTenometry operator, we have additional components called target

16:25.240 --> 16:27.280
allocator.

16:27.280 --> 16:33.200
And when you enable it, it will watch all the pod and service monitors in your cluster.

16:33.200 --> 16:35.320
And it can watch a subset of it.

16:35.320 --> 16:38.200
It depends on the label selector.

16:38.200 --> 16:43.880
It will get the scrape targets and then distribute those targets across collectors that you deploy.

16:43.880 --> 16:51.480
So if you deploy 50 collectors, it will distribute the scrape targets into those 50 collectors

16:51.480 --> 16:56.440
so that all the collectors get the same load.

16:56.440 --> 16:57.520
How does it work?

16:57.520 --> 17:02.560
The operator will deploy the target allocator and collector, will change the Prometheus receiver

17:02.560 --> 17:05.920
config with the target allocator service name.

17:05.920 --> 17:11.520
And then collector will connect to the target allocator to get its targets.

17:11.520 --> 17:13.360
Okay.

17:13.360 --> 17:16.320
So we're going to just enable the target allocator.

17:16.320 --> 17:21.720
For that, we need to change the deployment node to stateful set.

17:21.720 --> 17:29.200
Enable the target allocator.

17:29.200 --> 17:35.600
And now we don't have to do any config in the receiver.

17:35.600 --> 17:40.680
We can just leave this empty, the scrape config empty as an empty array.

17:40.680 --> 17:47.040
However, we need to change the Prometheus to, we need to just define a single Prometheus

17:47.040 --> 17:51.160
receiver because the operator will change.

17:51.160 --> 17:56.640
There is a convention that operator will find this receiver and change its configuration.

17:56.640 --> 18:05.240
Okay.

18:05.240 --> 18:09.760
Apply the manifest.

18:09.760 --> 18:11.760
And yeah, it's crashing.

18:11.760 --> 18:14.040
It's a demo.

18:14.040 --> 18:24.600
But it's just waiting for the target allocator to be running and then it will start properly.

18:24.600 --> 18:25.760
Sometimes it just takes some time.

18:25.760 --> 18:26.760
Okay.

18:26.760 --> 18:31.080
It's up and running.

18:31.080 --> 18:39.920
Now, if I refresh the same metrics endpoint from the collector, I see labels again they

18:39.920 --> 18:47.520
changed because now the instance is again the pod IP.

18:47.520 --> 18:52.960
The job name is what the Prometheus receiver uses by default.

18:52.960 --> 18:58.280
And then there's labels like namespace, pod directly on the metric.

18:58.280 --> 19:04.200
However, the target info should as well contain the metadata from Kubernetes, like what is

19:04.200 --> 19:08.080
the pod name, what is the namespace name and so on.

19:08.080 --> 19:09.780
Okay.

19:09.780 --> 19:17.000
So what we saw is that the Prometheus receiver works pretty well.

19:17.000 --> 19:23.240
We can use it to scrape Prometheus metrics.

19:23.240 --> 19:26.680
There shouldn't be an issue and it's as well using the Prometheus configuration.

19:26.680 --> 19:32.120
So if you are familiar with Prometheus, we can just directly copy paste the config into

19:32.120 --> 19:33.680
AutoCollector.

19:33.680 --> 19:42.320
However, what we haven't seen is if the process is instrumented with Auto SDK, then the Delta

19:42.320 --> 19:47.160
temporality metrics will most likely be dropped if you are using Prometheus receiver.

19:47.160 --> 19:55.240
However, if you are using OTLP exporter from the SDK and we set the temporality correctly

19:55.240 --> 19:59.920
to cumulative, then those metrics will be correctly propagated to the collector and

19:59.920 --> 20:02.760
then to Prometheus.

20:02.760 --> 20:06.200
So be careful with the Delta temporality.

20:06.200 --> 20:10.640
The Auto SDK should use the cumulative temporality by default.

20:10.640 --> 20:14.800
So that shouldn't be an issue.

20:14.800 --> 20:21.720
But if you are using something custom, then be careful with those metrics using Delta.

20:21.720 --> 20:25.400
So to wrap up, we saw the Prometheus receiver.

20:25.400 --> 20:29.800
It essentially contains the Prometheus configuration.

20:29.800 --> 20:36.920
However, the dollar signs in the AutoConfig, they are substituted to environment variables.

20:36.920 --> 20:39.560
So you need to escape them with two dollar signs.

20:39.560 --> 20:41.760
That's one difference.

20:41.760 --> 20:46.640
In the open telemetry ecosystem or in open telemetry collector and operator, there is

20:46.640 --> 20:51.200
no support for probe and scrape configs.

20:51.200 --> 20:57.800
And in the service and pod monitors in the AutoOperator, we don't support TLS.

20:58.800 --> 21:01.920
There are limitations.

21:01.920 --> 21:06.320
So where do we want to go with Prometheus and open telemetry?

21:06.320 --> 21:10.360
The Prometheus is planning 3.0 release.

21:10.360 --> 21:13.680
They want to improve the OTLP ingestion endpoint.

21:13.680 --> 21:17.920
So you can now ingest OTLP metrics into Prometheus, which is great.

21:17.920 --> 21:22.800
However, if you are using Delta temporality, those metrics will be dropped and they want

21:22.800 --> 21:28.200
to improve the support for it along other features.

21:28.200 --> 21:34.920
So yeah, feel free to help us to build this thing, to be more robust.

21:34.920 --> 21:40.040
On the open telemetry ecosystem, there is kind of two projects where you could contribute

21:40.040 --> 21:42.560
to improve Prometheus support.

21:42.560 --> 21:46.280
In the collector, there is the Prometheus receiver that we saw, Prometheus exporter

21:46.280 --> 21:47.960
and remote write.

21:47.960 --> 21:52.040
There is a lot of issues on the GitHub where you can help.

21:52.040 --> 21:57.520
And on the operator, sorry, we are planning the next CRD version.

21:57.520 --> 21:59.120
We want Alpha 2.

21:59.120 --> 22:05.360
And we want to create a dedicated target allocator CRD that will expose more Prometheus config.

22:05.360 --> 22:11.280
It's as well something that we are working on and we are very happy to accept your contributions.

22:11.280 --> 22:17.320
Okay, and this is all that I prepared for today.

22:17.320 --> 22:18.320
Thank you.

22:18.320 --> 22:27.440
Do we have any questions?

22:27.440 --> 22:36.520
No questions?

22:36.520 --> 22:38.520
Going longs?

22:38.520 --> 22:40.160
Okay.

22:40.160 --> 22:46.800
Thank you once again.

