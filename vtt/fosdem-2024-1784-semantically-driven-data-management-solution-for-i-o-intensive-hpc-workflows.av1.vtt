WEBVTT

00:00.000 --> 00:27.000
So people can hurry and sit down for the next speaker please.

00:27.000 --> 00:56.000
Okay, thanks for our next talk. We have met in talking about semantically driven data management solution for IO intensive HPC workflows.

00:57.000 --> 01:15.000
Thank you. My name is Metin Chakrachalov. I work at the European Center for Medium Range Weather Forecasts Department, Forecasts and Services Department at ECMWF.

01:15.000 --> 01:32.000
I will talk about the semantically driven data management solution for IO intensive HPC workflows, which the work was funded by the EUR HPC project called IOC.

01:33.000 --> 01:38.000
It is work done by many people.

01:39.000 --> 01:47.000
So a little bit background on the ECMWF, European Center for Medium Range Weather Forecasts.

01:47.000 --> 02:02.000
It is established in 1975 by 23 member states and 12 cooperating states as an intergovernmental organization.

02:02.000 --> 02:21.000
There are three base duty stations with more than 450 people, Redding, Great Britain, in Germany, Bonn and Bologna, Italy.

02:22.000 --> 02:42.000
So ECMWF is both a research institution and 24-7 operational services, producing numerical weather predictions and other data to member states.

02:43.000 --> 03:04.000
There are two big projects that ECMWF is a key player. One is Copernicus. It is the Earth Observation component of the EU's space program.

03:05.000 --> 03:23.000
We provide climate change information, atmospheric composition information and also flooding and fire danger information.

03:24.000 --> 03:33.000
The other big initiative, EU initiative, is the Destination Earth project, which is prototyping digital twins of the Earth.

03:34.000 --> 03:38.000
So the ECMWF's production workflow looks like this.

03:38.000 --> 03:50.000
There are per day 200 million observations, collected acquisitions and fed into the Earth system model.

03:50.000 --> 03:58.000
Those observations and the output from the weather predictions are archived.

03:58.000 --> 04:23.000
Also, these data are used to generate products, which are 300 terabytes of data per day, which then accounts for 65 terabytes of data per day as products disseminated to around 350 destinations to member states and other customers.

04:24.000 --> 04:39.000
So the information system, the data is central. It provides access to data, models and workflows, and the data management is very critical for the operations.

04:40.000 --> 04:48.000
We need transformation of data into information, insights and decisions.

04:49.000 --> 04:59.000
So, semantically driven data management, we have been doing this for a long time.

04:59.000 --> 05:10.000
It means managing data based on its meaningful logical description, rather than just storing data.

05:10.000 --> 05:20.000
We also abstract the backend technologies. We also abstract where and how the data is stored from the users.

05:21.000 --> 05:44.000
So instead of, we try to avoid nested folder structures or UIDs, such as this home user projects ECMWF and blah, blah, blah, or some cryptic UIDs that doesn't make much sense to the user.

05:44.000 --> 05:52.000
Instead, we want to use meaningful, scientifically meaningful metadata to describe the data.

05:52.000 --> 06:01.000
For example, in this case, this project is ECMWF experiment number 42.

06:01.000 --> 06:07.000
The data is 224 parameter pressure and level.

06:07.000 --> 06:19.000
So for that, as part of the IOC project, we developed DAISY, data access and storage interface.

06:19.000 --> 06:28.000
So we provide, we index and identify data using its meaningful description.

06:28.000 --> 06:40.000
And for that also, that allows us to implement optimized algorithms to retrieve archive and retrieve data.

06:41.000 --> 06:54.000
And this is based on the ECMWF, ECMWF object store called FTB, which is also free and open source on GitHub.

06:54.000 --> 07:00.000
And this abstracts, we also abstract the storage technologies behind POSIX.

07:00.000 --> 07:05.000
We support POSIX, DAOS, Moto, and Ceph.

07:05.000 --> 07:18.000
And we provide interfaces and tools as well as CEC and Python APIs.

07:19.000 --> 07:28.000
So the schema, the main complexity is the schema which describes the database.

07:28.000 --> 07:36.000
And it is a collection of rules and each rule is a tree of attributes.

07:37.000 --> 07:51.000
In this example, I have a schema file and inside that I have two rules and each rule is consisting of multiple parameters.

07:51.000 --> 08:04.000
For example, here project experiment date parameter level would translate to a key project ECMWF experiment 42 and so on.

08:04.000 --> 08:17.000
The other rule is event city year and this could translate into event for stem, city is Brussels and year is 2024.

08:17.000 --> 08:27.000
So the rules are, the rules have, the rules are blueprints of the database, how to construct the database.

08:27.000 --> 08:35.000
And they have three levels and they, each level can have multiple attributes.

08:35.000 --> 08:49.000
And to make a rule, it has to be unique and complete to describe the data so that we can identify data from other data.

08:49.000 --> 09:05.000
And we also need to think about the locality where data, where different data is related to it, we would like to store them together.

09:05.000 --> 09:18.000
So each level here, we can think of the first level as directory, the second level file level, and the third level as the indexes in the file.

09:18.000 --> 09:24.000
So the locality would be increased when we go deeper in the level.

09:24.000 --> 09:35.000
The other, we can set daisy, we can set up daisy by the configuration file in YAML configuration file.

09:35.000 --> 09:47.000
We can point to the schema file to find the schema file and we can set the backend storage technology by saying file in this case is reference.

09:47.000 --> 09:57.000
We can also have different parts to the databases. We can have multiple databases. It's called roots.

09:57.000 --> 10:07.000
And we can set multiple behavior to individual roots.

10:07.000 --> 10:14.000
So aside from data, we also need key and we also have query.

10:14.000 --> 10:24.000
The keys would refer to single objects while queries can be any number of objects.

10:24.000 --> 10:42.000
In this case, key defines, identifies, and single objects on the right, I have level as a list of values, 0, 1, and 3.

10:42.000 --> 10:55.000
So it means I make a query for three different data where the differences, the levels are 0, 1, and 3.

10:55.000 --> 11:03.000
So we provide multiple interfaces, command line tools, C, C++, and Python APIs.

11:03.000 --> 11:12.000
But here I present an example for Python API because it's simplest.

11:12.000 --> 11:19.000
So I need, for storing a data by key, I need a key and data.

11:19.000 --> 11:32.000
So data can be anything in this case. I just put a string here, but it can be PNG file or PDF file or any other type of data.

11:32.000 --> 11:38.000
Then I make a key. User is met in project is IOC.

11:38.000 --> 11:43.000
Date is 2023 and city is born.

11:43.000 --> 11:53.000
And I pass this key and data to Daisy and Daisy would archive it.

11:53.000 --> 12:00.000
Then the other main feature is list, searching for data in the database.

12:00.000 --> 12:07.000
I need to make a query, in this case, user met in project IOC.

12:07.000 --> 12:15.000
And in this case, I just want two data objects for two different dates.

12:15.000 --> 12:23.000
And I pass this query to Daisy and it returns me the keys that I need for retrieving.

12:23.000 --> 12:29.000
And in the next example, I have the retrieve getting a key, getting data by a key.

12:29.000 --> 12:35.000
I make a key, user is met in project IOC and so on.

12:35.000 --> 12:39.000
And I pass this key to Daisy and retrieve the data.

12:39.000 --> 12:42.000
So it's very simple.

12:42.000 --> 12:54.000
So to sum it up, we describe data semantically instead of your IDs and nest directories.

12:54.000 --> 13:03.000
And we index and identify data by its meaningful semantic information.

13:03.000 --> 13:12.000
And this also allows us fast and efficient retrieve and search and archive algorithms.

13:12.000 --> 13:18.000
Also, we abstract where how data is stored from the user.

13:18.000 --> 13:24.000
And we make blueprints called rules.

13:24.000 --> 13:30.000
And we make keys to attach to the data and pass it to Daisy.

13:30.000 --> 13:40.000
And Daisy would store and manage the data using multiple different storage technologies.

13:40.000 --> 13:47.000
So more about Daisy, we have, we published, Daisy is free and open source.

13:47.000 --> 13:50.000
We published on GitHub.

13:50.000 --> 13:54.000
We have example C API and Python API.

13:54.000 --> 14:03.000
We also provide binary packages on GitHub for C, C++ as well as Python.

14:03.000 --> 14:10.000
We also have Python packages for Linux, RPM and the beam packages are available.

14:10.000 --> 14:15.000
We also have documentation on read to docs.

14:15.000 --> 14:21.000
And that's all. Thank you for your attention.

14:21.000 --> 14:28.000
APPLAUSE

14:31.000 --> 14:38.000
Thank you. Do you have any questions for Metin?

14:38.000 --> 14:40.000
No? Oh, there is one.

14:45.000 --> 14:50.000
Thank you.

14:50.000 --> 14:58.000
Hey, next presentation. I was wondering if you can specify the type of the values in your SEMA.

14:58.000 --> 15:00.000
You mean integer?

15:00.000 --> 15:04.000
Yeah, yeah, like that. To facilitate the queries.

15:04.000 --> 15:13.000
Yes, attributes can have types. You can set integer, date, string, they can have multiple types.

15:13.000 --> 15:15.000
Okay, thanks.

15:31.000 --> 15:38.000
Hi, thank you for your talk. I was interested in indexes because you mentioned that you index and identify data.

15:38.000 --> 15:50.000
Did some standard type of indexes or you have some format on your own, optimized for this three type of data?

15:50.000 --> 15:58.000
Yes, indexing is based on the rules.

15:58.000 --> 16:10.000
So the rules here, we have three structures which has three levels.

16:10.000 --> 16:17.000
So it has to have three levels which translates to a directory file and data.

16:17.000 --> 16:30.000
And we have in-house mechanism algorithm to that indexes that translates this three into identifying data.

16:30.000 --> 16:34.000
So is it something like gene indexes?

16:34.000 --> 16:36.000
I couldn't hear.

16:36.000 --> 16:40.000
If it is something like gene indexes?

16:40.000 --> 16:44.000
I'm not sure if I understand. Gene index?

16:44.000 --> 17:00.000
Yeah, I'm not the right person, I think, because we use the FTB which has been developed since long time.

17:00.000 --> 17:04.000
And it's a big library.

17:04.000 --> 17:09.000
I cannot answer the question because I haven't worked with that level.

17:09.000 --> 17:11.000
No problem, thank you.

17:11.000 --> 17:30.000
Thank you for the talk.

17:30.000 --> 17:44.000
I would like to know where are these keys stored because you need to query this kind of index and where are they stored for the user?

17:44.000 --> 17:54.000
Yeah, so the indexes would be stored separately but together with the data.

17:54.000 --> 18:00.000
And they would go, for example, in this case, inside the roots.

18:00.000 --> 18:04.000
So each root would be different database.

18:04.000 --> 18:17.000
And if you would look inside the root one, output one, for example, here, you would have index keys inside as well as the data, so together.

18:17.000 --> 18:27.000
Okay, so it's a file system or a database stored inside these two directories?

18:27.000 --> 18:39.000
Yeah, for POSIX, it would be directory, but for object storage, it would be not directly contained or something like that.

18:39.000 --> 18:51.000
Okay, so the way how the index is stored depends on the type of the storage you describe here.

18:51.000 --> 18:57.000
Yes, but we can also have, we have two different abstractions.

18:57.000 --> 19:03.000
One is indexing and we call it catalog and we have a bulk data.

19:03.000 --> 19:16.000
So we can have indexing catalog inside POSIX directory and bulk data on an object store.

19:16.000 --> 19:20.000
Okay, thank you.

19:20.000 --> 19:25.000
Any more questions?

19:25.000 --> 19:29.000
Is the next speaker in the room?

19:29.000 --> 19:31.000
Okay, thank you again, Metin.

19:31.000 --> 19:33.000
Thank you.

