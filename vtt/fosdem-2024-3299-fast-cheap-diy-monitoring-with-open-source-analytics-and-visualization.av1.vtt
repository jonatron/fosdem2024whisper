WEBVTT

00:00.000 --> 00:10.360
Welcome to the next speaker, a big applause for Robert who's going to present on fast

00:10.360 --> 00:15.760
cheap DIY monitoring with open source analytics and visualization.

00:15.760 --> 00:23.120
Okay, thank you.

00:23.120 --> 00:24.480
This is wonderful to see you all here.

00:24.480 --> 00:25.480
Big shout out to FOSTA.

00:25.480 --> 00:28.360
This is the first time I've ever been at FOSTA.

00:28.360 --> 00:29.560
First time I've ever done a talk here.

00:29.560 --> 00:30.560
It's totally awesome.

00:30.560 --> 00:31.560
So thanks a bunch.

00:31.560 --> 00:37.600
All right, we've got 20 minutes to talk about do-it-yourself monitoring with Click House.

00:37.600 --> 00:42.760
And just some intros, just a little bit about my qualifications to talk about this.

00:42.760 --> 00:45.440
My day job is I run a company called Altenity.

00:45.440 --> 00:49.440
We're a server provider for Click House, been around for a number of years.

00:49.440 --> 00:53.080
But more particularly, I've been working with databases for a really long time.

00:53.080 --> 00:56.520
I've been working with Click House as a user for about five years.

00:56.520 --> 01:01.320
Our company also does a fair amount of work in the ecosystem around Click House.

01:01.320 --> 01:07.440
Among other things, we are the maintainers of the community provider in Grafana for

01:07.440 --> 01:08.560
Click House.

01:08.560 --> 01:09.560
That's one of our projects.

01:09.560 --> 01:10.560
It's quite popular.

01:10.560 --> 01:13.000
It has about 14 million downloads.

01:13.000 --> 01:15.480
So let me jump in.

01:15.480 --> 01:16.480
I probably don't.

01:16.480 --> 01:21.520
Pretty much everybody here knows what, how many of you guys run monitoring systems here?

01:21.520 --> 01:24.440
Okay, it's easier to ask who doesn't.

01:24.440 --> 01:26.320
How many people use Grafana?

01:26.320 --> 01:28.080
Okay, excellent.

01:28.080 --> 01:29.600
That'll save some time.

01:29.600 --> 01:30.600
Okay, monitoring.

01:30.600 --> 01:32.160
You say answer questions.

01:32.160 --> 01:33.640
So stuff goes wrong.

01:33.640 --> 01:36.800
You want to figure out quickly what is going on.

01:36.800 --> 01:41.920
Or maybe things are going fine, but you want to do things like capacity planning.

01:41.920 --> 01:43.960
Monitoring helps you answer that question.

01:43.960 --> 01:45.960
I'm kind of old school, so I say monitoring.

01:45.960 --> 01:50.400
Nowadays, people tend to say observability.

01:50.400 --> 01:53.600
But I'll use the word monitoring just because I'm used to it.

01:53.600 --> 01:58.520
So back in the days of old, I've been around for a while, so I was on the last talk, talked

01:58.520 --> 02:01.160
about inheritances from Unix.

02:01.160 --> 02:06.280
When things broke on Unix, you would kind of go in and lay hands upon the machine and

02:06.280 --> 02:10.560
run VM stat or IOS stat and try and figure out what was going on.

02:10.560 --> 02:13.680
Well, those were the bad days.

02:13.680 --> 02:15.640
Nowadays, we tend to do things graphically.

02:15.640 --> 02:17.200
And there's a couple reasons for that.

02:17.200 --> 02:21.720
One, it's way better because if you're trying to find a problem like, hey, somebody is just

02:21.720 --> 02:26.320
like blowing out the CPU, you want to see durations, you want to be able to see a bunch

02:26.320 --> 02:30.440
of metrics together, it's much easier to see these things graphically.

02:30.440 --> 02:34.400
The other thing is that, as you all know, we're now working with systems, for example,

02:34.400 --> 02:38.000
that are based on containers, and you can't actually go in and lay hands on them very

02:38.000 --> 02:39.000
easily.

02:39.000 --> 02:43.840
Moreover, they can restart and then you lose what's on the local file system.

02:43.840 --> 02:48.880
So graphical monitoring, graphical display is critical.

02:48.880 --> 02:51.440
So what we're going to do in this talk is I'm just going to give you a few clues about

02:51.440 --> 02:55.200
how to build one completely from scratch.

02:55.200 --> 02:59.760
And basically, when you're talking about monitoring, you have a number of parts.

02:59.760 --> 03:00.760
This picture shows them.

03:00.760 --> 03:03.400
You start with metrics, logs, and traces.

03:03.400 --> 03:08.120
I probably don't have to explain what those are, but those are standard types of information

03:08.120 --> 03:09.800
that come out of systems.

03:09.800 --> 03:13.760
You're going to ingest them into some sort of database, which you can then run queries

03:13.760 --> 03:16.120
on and it will store them over time.

03:16.120 --> 03:18.800
And then you have some system for doing visualization.

03:18.800 --> 03:21.000
That's thing number one and alerting.

03:21.000 --> 03:23.440
So to tell you when bad stuff is going to happen.

03:23.440 --> 03:28.720
We'll mostly be talking about visualization today, but alerting is another important part.

03:28.720 --> 03:32.320
So the core of these systems is your store.

03:32.320 --> 03:36.280
So something that holds onto these metrics and allows you to do analyses on them of various

03:36.280 --> 03:37.360
kinds.

03:37.360 --> 03:40.320
And for this talk, we're going to talk about Click House.

03:40.320 --> 03:42.720
How many people here have heard of Click House?

03:42.720 --> 03:43.720
Excellent.

03:43.720 --> 03:44.720
Okay.

03:44.720 --> 03:45.720
Great.

03:45.720 --> 03:48.440
By the way, we have some of the core committers here for Click House.

03:48.440 --> 03:53.080
So if you have any questions, I'm sure you'll get answers.

03:53.080 --> 03:58.360
So Click House is kind of like, you could think of it as kind of like, bit like MySQL

03:58.360 --> 04:03.440
or Postgres, but it does analytics in the sense that it runs pretty much everywhere.

04:03.440 --> 04:09.200
It's open source, but it also has all the architectural features designed to read vast

04:09.200 --> 04:11.840
amounts of data very, very quickly.

04:11.840 --> 04:16.320
I won't bore you with this specific kind of marketing things here, but instead show you

04:16.320 --> 04:21.640
a slide that gives you a little better idea of what it is about Click House that makes

04:21.640 --> 04:23.000
it so fast.

04:23.000 --> 04:28.040
So our first thing, like most analytical databases, it stores data in columns.

04:28.040 --> 04:32.360
So you can think of them as being, every column being a big array that's split up in a bunch

04:32.360 --> 04:36.320
of files, but can be read very efficiently.

04:36.320 --> 04:39.480
Click House also allows, can make replicas of data very easily.

04:39.480 --> 04:42.760
So you can, by having more replicas, you have more things to query.

04:42.760 --> 04:49.360
You can handle more, you can handle, you know, more load on the, read load on the system.

04:49.360 --> 04:54.040
Another thing that Click House is extremely good at doing is compressing data and then

04:54.040 --> 04:55.880
reading it in parallel form.

04:55.880 --> 05:01.000
So all of these things taken together allow us to scan data, often, you know, hundreds

05:01.000 --> 05:03.640
of millions of rows, very, very efficiently.

05:03.640 --> 05:07.880
And I'll show you an example of that later in the talk.

05:07.880 --> 05:12.560
So then Grafana and Click House go together hand in glove.

05:12.560 --> 05:17.000
I think, I don't have, we don't have actual stats on this because you just never know

05:17.000 --> 05:18.000
what people are using.

05:18.000 --> 05:19.000
It's all open source.

05:19.000 --> 05:25.200
But Grafana is probably the most commonly used visualization tool with Click House.

05:25.200 --> 05:30.760
It's been available for years and just about everybody uses it, particularly for operational

05:30.760 --> 05:31.920
metrics.

05:31.920 --> 05:32.920
And I love Grafana.

05:32.920 --> 05:38.080
There are many tools out there, but Grafana is just the level of interactivity allows

05:38.080 --> 05:42.760
the fact that you can drill in, sort of look at different time ranges, bounce around between

05:42.760 --> 05:45.120
different metrics easily.

05:45.120 --> 05:47.120
It's really great to use.

05:47.120 --> 05:49.000
All right.

05:49.000 --> 05:55.160
So I'm going to build an example here in about five slides.

05:55.160 --> 05:59.440
So I'm going to just pick VM stat, which I showed in that previous slide.

05:59.440 --> 06:03.440
And what we're going to do is crunch the data, load it into Click House, and then show it

06:03.440 --> 06:05.320
in Grafana.

06:05.320 --> 06:08.000
So the question is how to do that.

06:08.000 --> 06:10.400
Well I'm just going to do it from scratch.

06:10.400 --> 06:16.960
So the first thing I'm going to do is collect VM stat data and turn it into a format that

06:16.960 --> 06:19.000
I can load into Click House.

06:19.000 --> 06:22.800
So this is a little Python program that does it.

06:22.800 --> 06:26.800
The important thing to note is not the details because they're probably not that great.

06:26.800 --> 06:29.320
I'm not a particularly good Python programmer.

06:29.320 --> 06:34.400
But the key thing is there's 14 lines of code here that crunches VM stat.

06:34.400 --> 06:39.160
And every five seconds we'll burp out some JSON.

06:39.160 --> 06:43.640
And what that JSON looks like is this slide right here.

06:43.640 --> 06:46.440
So pretty beautiful, right?

06:46.440 --> 06:47.440
This is data.

06:47.440 --> 06:50.280
This is data that we can get into Click House really easily.

06:50.280 --> 06:51.280
How do we do that?

06:51.280 --> 06:56.360
Well, first of all, we're going to make a table inside Click House to hold it.

06:56.360 --> 07:01.760
And there's relational databases want things to be in tabular forms.

07:01.760 --> 07:07.360
But it turns out Click House has a pretty expansive idea of what constitutes a tabular

07:07.360 --> 07:08.360
form.

07:08.360 --> 07:12.360
In this particular example, I'm going to do it the simplest way, which is I'm actually

07:12.360 --> 07:20.240
going to just create a table with a column for each of my data values.

07:20.240 --> 07:26.480
And so what you see here is a table definition that maps pretty much directly to the values

07:26.480 --> 07:28.560
that you get out of VM stat.

07:28.560 --> 07:34.480
And just a little bit of, you know, if you haven't used analytic databases before or

07:34.480 --> 07:39.360
aren't, you know, like a deep database person, we tend to think of these column values as

07:39.360 --> 07:42.320
being one of two types.

07:42.320 --> 07:47.040
Dimensions, which are characteristics of the thing that we're measuring, or the actual

07:47.040 --> 07:49.000
measurements themselves.

07:49.000 --> 07:55.800
And that's important because generally speaking, when we're scanning the data, we will group

07:55.800 --> 08:00.840
by the dimensions, like collect all the data by host, for example, over a certain period

08:00.840 --> 08:01.840
of time.

08:01.840 --> 08:03.920
And then the measurements of the things we aggregate.

08:03.920 --> 08:04.920
We take averages.

08:04.920 --> 08:05.920
We take max.

08:05.920 --> 08:06.920
We take min.

08:06.920 --> 08:08.160
So on and so forth.

08:08.160 --> 08:14.520
So that's just a quick intro, like a 60-second introduction to data modeling inside an analytic

08:14.520 --> 08:16.840
database.

08:16.840 --> 08:20.160
So the third thing we need to do is we've got the table.

08:20.160 --> 08:21.160
We've got the data.

08:21.160 --> 08:23.440
We need to make the data go to the table.

08:23.440 --> 08:29.680
So ClickHouse is very, very, has a bunch of different ways that you can load data.

08:29.680 --> 08:35.520
One of the simplest ways to do it is to, it has an HTTP interface, and you can simply

08:35.520 --> 08:42.000
push SQL commands and data to go with them up using Curl, which is a great talk on about

08:42.000 --> 08:43.360
two hours ago.

08:43.360 --> 08:45.800
So this is an example of the code.

08:45.800 --> 08:50.640
You can just say, in fact, that top level, if you're familiar with SQL insert commands,

08:50.640 --> 08:52.000
that's an insert command.

08:52.680 --> 08:57.440
ClickHouse has this kind of interesting variation on it where they have input formats.

08:57.440 --> 09:01.440
Instead of reading a bunch of values, which look like tuples, in this case, I'm actually

09:01.440 --> 09:06.400
going to read some data, which will be, for example, if I'm doing a post, it will just

09:06.400 --> 09:08.600
be in the payload.

09:08.600 --> 09:14.320
And it will actually be a bunch of JSON documents, each of which will turn into a row in the

09:14.320 --> 09:15.920
database.

09:15.920 --> 09:21.120
And this thing down here just shows you how to execute exactly that command using Curl.

09:22.080 --> 09:29.920
So, by the way, one of the things about this talk is everything I've done is there's an

09:29.920 --> 09:32.000
example out in GitHub.

09:32.000 --> 09:37.600
If you go ahead and Google, like if you're sitting right here and want to do it, you

09:37.600 --> 09:39.240
could just Google ClickHouse.

09:39.240 --> 09:42.560
Actually, I've got the link at the end.

09:42.560 --> 09:43.560
I'll show it to you.

09:43.560 --> 09:45.040
It's probably not worth it.

09:45.040 --> 09:48.080
So anyway, so we can load that data up.

09:48.240 --> 09:52.200
In the examples, you'll see that I actually wrote a little script that I can just run

09:52.200 --> 09:53.840
and put in the background.

09:53.840 --> 09:57.920
And then it will collect this data on each host that I'm measuring, sticking it into

09:57.920 --> 09:59.120
ClickHouse.

09:59.120 --> 10:01.120
Then what we do is we build a Grafana dashboard.

10:01.120 --> 10:05.680
So everybody, pretty much everybody in the room knows how Grafana works.

10:05.680 --> 10:08.920
What you're going to do is if you haven't done it already, install Grafana.

10:08.920 --> 10:11.560
You will need to add a plugin to talk to ClickHouse.

10:11.560 --> 10:13.440
There are two of them out there.

10:13.440 --> 10:15.280
There's the Altinity plugin that we maintain.

10:15.280 --> 10:18.040
That's the one, the Community plugin that's been around for years.

10:18.040 --> 10:20.440
There's a new one from ClickHouse, Incorporated.

10:20.440 --> 10:25.080
Pick a plugin, install it, put your connection parameters in, and then you can write a few

10:25.080 --> 10:26.080
queries.

10:26.080 --> 10:31.320
And within a few minutes, if you're at all familiar with Grafana, you can create a dashboard

10:31.320 --> 10:32.880
that looks just like this.

10:32.880 --> 10:38.320
This literally took about 15 minutes to create.

10:38.320 --> 10:39.400
And then go crazy.

10:39.400 --> 10:41.520
So you've got data loading.

10:41.520 --> 10:47.560
So using loading it up using Curl, putting a little script around it maybe so that it

10:47.560 --> 10:53.360
can sort of reliably load the data up and dump it.

10:53.360 --> 10:54.440
And then you can go look at it.

10:54.440 --> 10:59.600
But the cool thing here is that once you're in a database, you have this incredible freedom

10:59.600 --> 11:01.720
to use the data any way you want.

11:01.720 --> 11:06.760
Because ClickHouse is not a database like Prometheus, which is basically designed to

11:06.760 --> 11:07.760
hold metrics.

11:07.760 --> 11:11.040
ClickHouse is a general purpose analytic database.

11:11.040 --> 11:16.280
You can ask virtually any question of that data that can be expressed in SQL.

11:16.280 --> 11:17.280
It may go fast.

11:17.280 --> 11:18.000
It may go slow.

11:18.000 --> 11:22.560
But this is an example where we're asking a question like how many machines had a certain

11:22.560 --> 11:28.040
amount of load, like over 25% load for at least a minute in the last 24 hours.

11:28.040 --> 11:29.480
And we can sum the number of minutes.

11:29.480 --> 11:34.600
So this is just an arbitrary question, but it's just a few lines of SQL.

11:34.600 --> 11:39.440
We can get this question moreover if we have a Grafana attached to this, we can turn that

11:39.440 --> 11:47.040
into some sort of display on Grafana that will show it graphically.

11:47.800 --> 11:50.120
You then have something that you can really play around.

11:50.120 --> 11:55.160
And this is just the effects of running some popular commands like stress is a great command

11:55.160 --> 11:57.720
if you want to see something hog memory.

11:57.720 --> 12:02.840
What you can see in this graph here, that big blue part was the OS buffer cache, which

12:02.840 --> 12:06.200
was actually filled with a bunch of pages from previous processes.

12:06.200 --> 12:12.840
You can see that the buffer cache was pretty much blown away when these stress runs began.

12:12.840 --> 12:17.640
Up above you see the result of running sysbench CPU command.

12:17.640 --> 12:23.080
You can see the effect on the CPU usage.

12:23.080 --> 12:27.080
So at this point, this is a very simple example, but you actually have a fair amount of insight

12:27.080 --> 12:31.520
into what's going on these machines.

12:31.520 --> 12:38.160
So the next thing is to take a drink and to scale this up a little bit.

12:38.160 --> 12:39.560
This is a toy that I just showed you.

12:39.680 --> 12:40.560
Anybody can do it.

12:40.560 --> 12:43.560
It's just a few lines of code.

12:43.560 --> 12:48.880
So as you start to think about scaling this system, making it work across a bunch of hosts,

12:48.880 --> 12:54.600
a bunch of different types of metrics, maybe adding logs, the first question that probably

12:54.600 --> 12:59.200
comes up is, hey, I love open source, but do I have to write everything?

12:59.200 --> 13:02.280
And the answer is no, not if you don't want to.

13:02.280 --> 13:06.440
So there's a couple of projects that if you're going to go in and do this, you probably should

13:06.440 --> 13:07.440
be looking at.

13:07.440 --> 13:08.440
One is Fluent Bit.

13:08.440 --> 13:10.720
How many people have used Fluent Bit or Fluent D?

13:10.720 --> 13:12.360
OK, fair number.

13:12.360 --> 13:15.800
We use Fluent Bit quite a bit in our cloud stuff.

13:15.800 --> 13:23.600
So what Fluent Bit does is it basically has a bunch of plugins which will sample different

13:23.600 --> 13:27.360
kinds of metrics, turn them into a data format, and then put them somewhere else.

13:27.360 --> 13:34.840
So for example, you can get the same similar CPU metrics to what I just showed you.

13:34.840 --> 13:39.320
There is an input plugin for Fluent Bit which will grab those, and then you can turn around

13:39.320 --> 13:42.040
and post them to Clickhouse.

13:42.040 --> 13:47.160
And it works as a daemon, so you can just bring it up, let it run.

13:47.160 --> 13:51.240
And so as a result, you don't have to figure out how to parse all these different formats.

13:51.240 --> 13:55.440
Moreover, you don't have to worry about basic things like posting to HTTP.

13:55.440 --> 13:57.080
They take care of it.

13:57.080 --> 14:01.040
So that's a really useful project to look at and one that you should consider.

14:01.040 --> 14:06.400
Second thing is Open Telemetry, which if you were here two talks ago, is basically trying

14:06.400 --> 14:13.480
to create a common data model for all observability data, including metrics, including logs, including

14:13.480 --> 14:15.040
traces.

14:15.040 --> 14:22.160
And that gives you sort of like a universal broker, if you will, that can handle data coming

14:22.160 --> 14:26.880
from all kinds of different sources, maybe Fluent Bit, maybe custom stuff that you build,

14:26.880 --> 14:30.560
and then it will push it into databases like Clickhouse.

14:30.560 --> 14:35.840
And in fact, there is, OTEL as it's often called, has a provider for Clickhouse, which

14:35.840 --> 14:38.000
I believe is in the alpha stage.

14:38.000 --> 14:39.440
Some people are using it.

14:39.440 --> 14:43.400
It still has some performance issues, but one of the things it takes care of is building

14:43.400 --> 14:48.560
the data structures that you use to store your data and doing them in a kind of rational

14:48.560 --> 14:50.360
way.

14:50.360 --> 14:55.760
So in fact, that's to answer question one fully, if you were going to build this out,

14:55.760 --> 14:59.840
you might have a metrics pipeline that looks kind of like the following, where you have

14:59.840 --> 15:06.400
Fluent Bits, perhaps feeding this to an OTEL collector, that's this broker that Open Telemetry

15:06.400 --> 15:10.120
provides, pushing it to Clickhouse and then reading it to Grafana.

15:10.120 --> 15:12.880
I'm not saying this is the only way to do it or even that it's a good thing, that's

15:12.880 --> 15:14.920
what you have to decide, but you can do this.

15:14.920 --> 15:20.200
The pieces are out there and they're all open source.

15:20.200 --> 15:22.880
Second question, this is Fostum.

15:22.880 --> 15:26.480
You yearn to use Postgres or MySQL.

15:26.480 --> 15:27.480
Why not?

15:27.480 --> 15:28.920
Why is it we?

15:28.920 --> 15:30.640
Why don't we just use Postgres for this?

15:30.640 --> 15:34.880
Well, actually for the example I gave, the little one, you could use Postgres and it

15:34.880 --> 15:36.800
would make no difference whatsoever.

15:36.800 --> 15:42.040
But as you start to scale, it becomes really important to pay attention to how the data

15:42.040 --> 15:47.000
is actually represented and what kind of powers of query you have on top of it.

15:47.000 --> 15:52.280
And the key thing to notice here is that Postgres and MySQL are row databases.

15:52.280 --> 15:54.920
So they store the data as rows.

15:54.920 --> 15:58.400
There is a plug-in for Postgres, I guess, that is beginning to change that.

15:58.400 --> 16:02.720
But in general, if you read anything out of a row, if you touch anything in the row,

16:02.720 --> 16:05.000
you'll have to read the entire row.

16:05.000 --> 16:10.000
Whereas with Clickhouse, it's columnar, so if you read columns, say, out of 109 column

16:10.000 --> 16:13.840
table, you only touch the data for those three columns.

16:13.840 --> 16:18.280
Moreover, by putting things into columns, effectively a raise of a single data type,

16:18.280 --> 16:23.400
taking advantages of things like sorting, say time-based sorting, it tends to compress

16:23.400 --> 16:24.680
extremely well.

16:24.680 --> 16:29.160
You can literally compress a lot of these metrics by a factor of 100, so they will

16:29.160 --> 16:32.360
compress down to 1% of their previous value.

16:32.360 --> 16:34.600
Let me just show that graphically.

16:34.600 --> 16:38.320
So what they affect is, is that when you run queries with Clickhouse, they can be easily

16:38.320 --> 16:44.200
a thousand times faster than Postgres and MySQL because of columnar structure, because

16:44.200 --> 16:47.360
of compression, because of parallelization.

16:47.360 --> 16:48.760
And this illustrates it.

16:48.760 --> 16:55.800
This was a sample, literally reading three columns out of 109, scanning 200 million rows.

16:55.800 --> 17:01.080
The amount of data you would read in Postgres or MySQL was 59 gigs, everything.

17:01.080 --> 17:06.000
In Postgres, in Clickhouse, first of all, you read three columns, so that was literally

17:06.000 --> 17:07.000
3% of the data.

17:07.000 --> 17:09.600
You're already up 33x.

17:09.600 --> 17:12.880
Now those columns are compressed, so you're not just up 33x.

17:12.880 --> 17:17.480
The amount of data that you're reading is actually, has been reduced by a factor of

17:17.480 --> 17:19.960
almost 3,000 at this point.

17:19.960 --> 17:23.080
And then you can spread it across, you can spread those reads, say if they're coming

17:23.080 --> 17:28.440
off an SSD, you can be doing them across eight threads, so you could argue that that actually

17:28.440 --> 17:34.440
is going to reduce the amount of work for each thread by approximately 23,000.

17:34.440 --> 17:39.760
So these are, like, not just an order of magnitude, but orders of magnitude less IO.

17:39.760 --> 17:44.920
This is the reason why Clickhouse is great for this kind of problem.

17:44.920 --> 17:47.640
Third thing, how to handle data from a lot of collections.

17:47.640 --> 17:52.240
So for those of you that know Clickhouse, you saw that example and you thought this guy's

17:52.240 --> 17:53.680
an idiot.

17:53.680 --> 17:56.880
He is adding five rows at a time.

17:56.880 --> 18:03.260
And so data warehouses, the flip side of using columns is you touch a lot of files every time

18:03.260 --> 18:04.420
you load data.

18:04.420 --> 18:07.980
So what you want to do is load data, you want to buffer data into big blocks.

18:07.980 --> 18:12.520
So like maybe a million messages at a time.

18:12.520 --> 18:13.880
There are many ways to do this.

18:13.880 --> 18:16.040
Clickhouse has what's called async IO.

18:16.040 --> 18:21.000
But in general for a system like this, if you're starting to receive data from many

18:21.000 --> 18:27.040
collectors, you're starting to get into levels of traffic like 100,000 events, 200,000 millions

18:27.040 --> 18:32.280
of events per second, what you want to consider is introducing Kafka or Red Panda or a similar

18:32.280 --> 18:33.400
event stream.

18:33.400 --> 18:36.640
And Clickhouse, so the collectors write to Kafka.

18:36.640 --> 18:43.920
This then breaks the connection between your producers, which are getting monitoring data

18:43.920 --> 18:45.920
and Clickhouse, which is the consumer.

18:45.920 --> 18:49.040
Moreover, Clickhouse has very good integration with Kafka.

18:49.040 --> 18:53.720
So there's built in, there's multiple types of integration, but the most commonly used

18:53.720 --> 18:56.360
one is Kafka table agent.

18:56.360 --> 19:01.720
So that basically wraps a Kafka topic, able to read it so you can do a select off it and

19:01.720 --> 19:03.640
it basically reads off the queue.

19:03.640 --> 19:08.800
And then there's a trick where you use materialized views to do that read automatically and stick

19:08.800 --> 19:10.560
it on a table in the database.

19:10.560 --> 19:16.160
So as your architecture gets larger, you definitely want to include this so that you can then

19:16.160 --> 19:20.480
read large blocks of data very quickly.

19:20.480 --> 19:21.480
Final thing.

19:21.480 --> 19:24.360
So I talked about this basic mapping of the data.

19:24.360 --> 19:30.920
I modeled it as a table, which is exactly matches the data that's coming out of the JSON, but

19:30.920 --> 19:34.520
that's not the only way that Clickhouse can do this.

19:34.520 --> 19:38.840
So there's actually a number of options.

19:38.840 --> 19:42.720
And one of the most common ones, and one that I was playing around with just as part of

19:42.720 --> 19:49.680
this exercise, was to have a column that just contains the JSON as a string.

19:49.680 --> 19:54.040
And then what you can do is there are functions in Clickhouse to pull the values out one by

19:54.040 --> 19:57.200
one, but they're kind of painful to use.

19:57.200 --> 19:58.520
There's extra syntax.

19:58.520 --> 20:03.760
So what you can also do is basically as the data loads, you can just pull out particular

20:03.760 --> 20:05.840
values and turn them into a regular column.

20:05.840 --> 20:09.840
This is very simple to do with materialized views.

20:09.840 --> 20:15.280
And so what that means is actually if you're doing a demo for this talk, and it's like

20:15.280 --> 20:19.720
3 a.m. in the morning before the talk, what you can do is basically load the entire JSON

20:19.720 --> 20:24.520
document, pull a couple columns out to make your queries work, and leave the rest there.

20:24.520 --> 20:29.240
One of the secret powers of Clickhouse is it does schema management or schema evolution

20:29.240 --> 20:31.480
very, very efficiently online.

20:31.480 --> 20:35.640
So as time goes on and you see more stuff in that JSON string you'd like to read, you

20:35.640 --> 20:37.960
can just pull it out into new columns.

20:37.960 --> 20:41.400
That's a command that can be, those are commands that can be executed instantly.

20:41.400 --> 20:44.280
All right, we're down to zero seconds.

20:44.280 --> 20:46.000
We're almost done.

20:46.000 --> 20:49.360
So there are other options.

20:49.360 --> 20:56.720
So you can, if you have key value pairs, you can do pairs of arrays, like an array of keys,

20:56.720 --> 21:00.800
an array of values, and then Clickhouse has very efficient functions to match those up

21:00.800 --> 21:02.240
and to process them.

21:02.240 --> 21:06.160
You can also use maps, so those are like hash tables.

21:06.160 --> 21:09.520
And these are other ways that you can represent JSON.

21:09.520 --> 21:15.320
Clickhouse has a JSON data type, but it is experimental and actually do for rewrite.

21:15.320 --> 21:19.600
So I just found that there's actually, it's on the schedule, so I just found that out last

21:19.600 --> 21:20.600
night.

21:20.600 --> 21:24.840
So it's going to be implemented, re-implemented in a way better way.

21:24.840 --> 21:29.120
But in the meantime, there are lots of ways to process JSON and Clickhouse has many other

21:29.120 --> 21:33.040
features that make JSON very handy to process.

21:33.040 --> 21:36.640
Okay, where can you find out more?

21:36.640 --> 21:39.920
So lots of sources about this stuff.

21:39.920 --> 21:42.200
I'm listing them here.

21:42.200 --> 21:47.120
The sample code is up in GitHub that shows everything that I did here.

21:47.120 --> 21:52.600
If you just Google GitHub, Clickhouse SQL examples, it will probably be the first thing

21:52.600 --> 21:54.120
that pops up.

21:54.120 --> 21:58.080
And there's a directory in there called open source monitoring.

21:58.080 --> 21:59.080
So you'll see that.

21:59.080 --> 22:03.000
And then, yeah, Clickhouse official docs are very, very complete.

22:03.000 --> 22:07.040
The Altenony Grafana plugin for Clickhouse, Fluentbit, Open Telemetry.

22:07.040 --> 22:13.240
And then we do lots of blogs and YouTube videos about how to use this stuff, including

22:13.240 --> 22:16.440
doing monitoring.

22:16.440 --> 22:18.120
And that's it.

22:18.120 --> 22:28.960
So thank you very much.

22:28.960 --> 22:32.160
And if you want to get in touch with me, you can connect me on LinkedIn.

22:32.160 --> 22:39.720
I'm on Slack, CNCF, Clickhouse, AltenonyDB, Data on Kubernetes, just, you know, or if

22:39.720 --> 22:44.520
you connect, send me email, whatever.

22:44.520 --> 22:46.000
So any questions?

22:46.000 --> 22:48.640
Do we have any questions?

22:48.640 --> 22:58.880
Could you please stay a few minutes for the Q&A so that people understand the questions?

22:58.880 --> 22:59.880
Any questions?

22:59.880 --> 23:09.000
You showed us data coming from a script.

23:09.000 --> 23:12.960
Can we do the same thing with data coming from web applications, for example?

23:12.960 --> 23:14.440
Oh, from a web application?

23:14.440 --> 23:15.680
Yeah, absolutely.

23:15.680 --> 23:21.960
Anything that can generate metrics, you can push them to Clickhouse.

23:21.960 --> 23:26.800
And in that case, if you're generating metrics, I would definitely recommend going and looking

23:26.800 --> 23:32.760
at Hotel, because it has, for example, it has SDKs, which you can embed in your application,

23:32.760 --> 23:36.560
generate metrics that will be translated into a standard form, and then they can get pushed

23:36.560 --> 23:37.560
to Clickhouse.

23:37.560 --> 23:40.880
So did I understand correctly?

23:40.880 --> 23:42.920
There's an SDK for doing this.

23:42.920 --> 23:46.240
Yes, it's part of OpenTelemetry.

23:46.240 --> 23:48.200
That's one, or you can do it yourself.

23:48.200 --> 23:50.680
It's part of Telemetry.

23:50.680 --> 23:53.480
There's multiple solutions for this.

23:53.480 --> 24:05.480
Hi.

24:05.480 --> 24:08.480
So first things for the presentation.

24:08.480 --> 24:14.720
My question is, does it make sense to use Clickhouse for tracing, storing traces, and

24:14.720 --> 24:15.720
such things?

24:15.720 --> 24:17.600
Like traces and logs, other stuff?

24:17.600 --> 24:18.600
Yeah, tracing.

24:18.600 --> 24:19.600
Absolutely.

24:19.600 --> 24:24.680
So, processing and tracing is actually one of the major use cases for Clickhouse.

24:24.680 --> 24:31.200
And what's kind of interesting is you can use databases like a time series database or

24:31.200 --> 24:32.200
something like Prometheus.

24:32.200 --> 24:36.800
The cool thing about putting in Clickhouse is once you get it in, you have this whole,

24:36.800 --> 24:40.880
because you have a full SQL implementation in there, you have much more flexibility about

24:40.880 --> 24:42.720
what you can do with the data.

24:42.720 --> 24:43.720
That's thing number one.

24:43.720 --> 24:48.280
We also have people that we work with, some of our users, that just have standardized

24:48.280 --> 24:52.400
on Clickhouse as sort of the one size fits all.

24:52.400 --> 24:56.640
So that reduces the sort of operational complexity.

24:56.640 --> 25:02.440
Third thing is, Clickhouse is just really good at anything that's time ordered, including

25:02.440 --> 25:10.000
logs, including just practically any kind of data that's emitted over time.

25:10.000 --> 25:13.040
So it's a very powerful engine for this kind of thing.

25:13.040 --> 25:17.840
I focused on metrics because it was just one use case, but traces and logs also work very

25:17.880 --> 25:18.880
well.

25:18.880 --> 25:19.880
Very well.

25:19.880 --> 25:29.880
We have time for one.

25:29.880 --> 25:37.280
Thank you.

25:37.280 --> 25:40.280
I'm looking for a tool to do reprocessing.

25:40.280 --> 25:47.160
So once I ingest a lot of data into Clickhouse, I want to do sort of on demand some fancy

25:47.160 --> 25:53.840
complex calculation that's a bit too complex for a query and then store the data again

25:53.840 --> 25:54.840
in Clickhouse.

25:54.840 --> 25:57.320
Is there a good framework or tool for that?

25:57.320 --> 25:59.480
Yeah, I don't quite understand the use case.

25:59.480 --> 26:02.840
So you're saying you're going to push data in, run a calculation on it, put it somewhere

26:02.840 --> 26:03.840
else?

26:03.840 --> 26:08.880
Yeah, so I gather 100% of data, but I need only 1%, and on that I need to do some complex

26:08.880 --> 26:09.880
business-wide calculation.

26:09.880 --> 26:10.880
Right, okay.

26:10.880 --> 26:11.880
Yeah, there's a couple options.

26:11.880 --> 26:16.280
Within Clickhouse itself, I would recommend you look at materialized views, which are

26:16.320 --> 26:21.080
basically like insert triggers that will run a query and then put the result in another

26:21.080 --> 26:22.580
table.

26:22.580 --> 26:28.580
They can be used, they're normally used to, or the most common use case, is to do pre-aggregation,

26:28.580 --> 26:32.960
but actually they are a transform, just a generalized transform mechanism.

26:32.960 --> 26:38.560
So anything that comes in as an input block, you can run calculations on it using a query,

26:38.560 --> 26:43.600
mix in data from other, you can join on other tables and do anything you want and then dump

26:43.600 --> 26:45.800
it to a target table.

26:45.820 --> 26:47.600
And that's a very efficient mechanism.

26:47.600 --> 26:53.720
It gets invoked every time an insert comes in, the query runs on that block of data.

26:53.720 --> 26:56.040
If you have, it sounds like you have something a little bit different.

26:56.040 --> 26:57.040
Yeah, I can ask you.

26:57.040 --> 26:59.440
Yeah, come by later and we can talk through it.

26:59.440 --> 27:00.440
Thank you.

27:00.440 --> 27:05.440
Okay, I think that's a wrap up.

27:05.440 --> 27:07.320
Yeah, thank you.

