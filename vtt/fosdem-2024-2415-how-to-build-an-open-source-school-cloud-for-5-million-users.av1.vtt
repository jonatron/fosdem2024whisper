WEBVTT

00:00.000 --> 00:13.000
Okay.

00:13.000 --> 00:18.000
Welcome everybody at high noon at Sunday at Fosdem in Jean-Saint.

00:18.000 --> 00:21.000
We'll hear now David Walter.

00:21.000 --> 00:28.000
He will tell us about an on-cloud implementation for schools in Bavaria.

00:28.000 --> 00:32.000
And scaling up for 5 million users.

00:32.000 --> 00:34.000
A warm welcome to David.

00:34.000 --> 00:40.000
Thank you David.

00:40.000 --> 00:42.000
Hi everyone.

00:42.000 --> 00:47.000
Today I will talk about how we did on-cloud implementation

00:47.000 --> 00:52.000
to scale up to 5 million users or will scale up to 5 million users.

00:52.000 --> 00:56.000
And I will talk a little bit about our school project.

00:56.000 --> 00:58.000
So what is our target?

00:58.000 --> 01:02.000
We want to scale up to 5 million users and we want to be better

01:02.000 --> 01:06.000
than every other hyperscaler we know so far.

01:06.000 --> 01:10.000
But before we dive in I will give a quick introduction about

01:10.000 --> 01:12.000
who we are talking about.

01:12.000 --> 01:14.000
My name is David Walter.

01:14.000 --> 01:17.000
I'm project lead of the Bavarian School Crowd project.

01:17.000 --> 01:23.000
And I'm also responsible for the experience,

01:23.000 --> 01:27.000
the customer experience in on-cloud and for the security.

01:27.000 --> 01:32.000
I'm using on-cloud as an open source user administrator since 2014.

01:32.000 --> 01:35.000
So I'm quite familiar with the project.

01:35.000 --> 01:37.000
And yeah.

01:37.000 --> 01:42.000
On-cloud itself launched in 2010 and the on-cloud infinite scale implementation

01:42.000 --> 01:49.000
we are talking today about had general availability in 2022.

01:49.000 --> 01:55.000
Right now we are already hosting more than 2.5 thousand tenants

01:55.000 --> 02:01.000
and we have over 1 million downloads of infinite scale.

02:01.000 --> 02:06.000
And it becomes more and more a backbone for service provider.

02:06.000 --> 02:14.000
As it is already during the press, we got acquired quite recently

02:14.000 --> 02:17.000
by Kiteworks, our new mother firm.

02:17.000 --> 02:27.000
Kiteworks is a security first company and it empowers on-cloud very much.

02:27.000 --> 02:33.000
It helps us to drive security, to drive privacy and to drive compliance

02:33.000 --> 02:38.000
in the same way as they do because Kiteworks is a security first company

02:38.000 --> 02:42.000
which emphasize privacy compliance a lot.

02:42.000 --> 02:48.000
And it also helps us, not only in the project but as a company itself

02:48.000 --> 02:53.000
to provide 24.7 operations and 24.7 support.

02:53.000 --> 02:57.000
There is much more to say but please read that slide afterwards

02:57.000 --> 03:05.000
because I think we are kind of short in time so I uploaded the slides to FOSSTEM

03:05.000 --> 03:08.000
and please read the slide if you are interested.

03:08.000 --> 03:10.000
More in those details.

03:10.000 --> 03:14.000
So just to give a rough idea because I don't know how much you know

03:14.000 --> 03:16.000
about the Federalist State System in Germany.

03:16.000 --> 03:18.000
I want to give you a brief overview.

03:18.000 --> 03:28.000
We are talking about the Federalist State Poverty which has 2.6k schools

03:28.000 --> 03:37.000
compared to Germany with 32 thousand schools and European Union 200 thousand schools.

03:38.000 --> 03:44.000
Because of course we would like to have some kind of prototype

03:44.000 --> 03:51.000
in what we are doing here in Bavaria and to achieve higher goals as you can imagine.

03:51.000 --> 03:56.000
That's why I put here some numbers to give you a rough idea about what we are talking about.

03:56.000 --> 04:00.000
We are talking about 1.6 million pupils.

04:00.000 --> 04:05.000
We are talking about 116 thousand teachers.

04:05.000 --> 04:14.000
Roughly 2 million parents in an overall Federalist State of 13 million inhabitants

04:14.000 --> 04:21.000
which leads us basically to an infinite scale instance of 4.7 million named users.

04:21.000 --> 04:29.000
Why I am focusing on the named users we will come later when we talk about load and performance testing.

04:30.000 --> 04:33.000
So what were our goals?

04:33.000 --> 04:40.000
We have some on a really really high level we have some major topics like data sovereignty,

04:40.000 --> 04:45.000
a most secure system, a most secure fight against shell solution.

04:45.000 --> 04:55.000
We have digital accessibility as of course one of the main targets.

04:55.000 --> 05:00.000
But firstly we need to compete with the hyperscalers

05:00.000 --> 05:08.000
because this project will be always compared to Google, to Microsoft Teams or whatever.

05:08.000 --> 05:18.000
So we have to focus as much as we can on availability, on scale, on reliability as much as we can.

05:18.000 --> 05:23.000
So on a more functional way we talk about maximum integration,

05:23.000 --> 05:29.000
the integration of a messenger and of different other solutions that we will come later on.

05:29.000 --> 05:32.000
We need to scale out without any limits.

05:32.000 --> 05:39.000
We have seen it in the ICDays where we had a partial lockdown in Bavaria recently

05:39.000 --> 05:43.000
that we need to scale out really fast and without any limits.

05:43.000 --> 05:48.000
We have to target updates without zero down times.

05:48.000 --> 05:57.000
And the most challenging when it comes to the project, we had the first 50 schools on board it

05:57.000 --> 06:03.000
after 32 weeks of project duration which was really really challenging.

06:03.000 --> 06:09.000
So let's get on to the base.

06:09.000 --> 06:11.000
What is underneath the project?

06:11.000 --> 06:17.000
When we talk about the infrastructure which is provided by Plus Server,

06:17.000 --> 06:22.000
we are talking about four data centers in Germany.

06:22.000 --> 06:25.000
We are talking about two S3 locations.

06:25.000 --> 06:32.000
We are talking about four metadata locations, 240 Kubernetes clusters,

06:33.000 --> 06:43.000
104 petabytes of S3 storage and up to 60,000 virtual CPUs and 120,000 gigabytes of RAM,

06:43.000 --> 06:46.000
which are quite big numbers.

06:46.000 --> 06:54.000
The project itself or the Kubernetes stack itself is based on the server in CloudStack and Gardner.

06:54.000 --> 06:56.000
That's the logos you see.

06:56.000 --> 07:03.000
The applications the user sees are, as I mentioned already,

07:03.000 --> 07:10.000
of course on Cloud Infinite Scale as its core and the only Office document server

07:10.000 --> 07:20.000
for writing, Excel sheets, presentations and PDF viewer and annotation.

07:20.000 --> 07:27.000
But why did the customer choose, or why did they choose on Cloud Infinite Scale?

07:27.000 --> 07:29.000
Just as a very very rough overview.

07:29.000 --> 07:35.000
The first thing which was very important that we can liberate the user data.

07:35.000 --> 07:42.000
The users have an interface to download their data from cloud to cloud,

07:42.000 --> 07:48.000
from OneDrive, Google Drive, on Cloud Classic, NextCloud,

07:48.000 --> 07:55.000
whatever the schools have already in place or the teachers mostly using their private lives.

07:55.000 --> 07:58.000
We want to liberate the user data.

07:58.000 --> 08:09.000
That's why we have an interface which allows the user immediately to move the data.

08:09.000 --> 08:11.000
The second thing is the spaces.

08:11.000 --> 08:18.000
The spaces is a collaborative workspace which doesn't have an owner.

08:18.000 --> 08:26.000
It has only a manager and it avoids data silos because, as you can imagine, teachers are collaborative people.

08:26.000 --> 08:36.000
So they have those data silos or they have those data silos and when the teacher left or called in sick or whatever,

08:36.000 --> 08:43.000
the material were stuck in his storage so he needed to be sent out.

08:43.000 --> 08:48.000
So we came up with spaces with fixed problem.

08:48.000 --> 08:57.000
Quarter management, as you can imagine, is quite important as well because there is literally a maximum of available storage.

08:57.000 --> 09:02.000
When we are talking about 4.7 million users, there is.

09:02.000 --> 09:04.000
There is just this multiplicator.

09:04.000 --> 09:15.000
The last thing is, it is a technology where I pointed out only one specific thing which makes infinite scales so special.

09:15.000 --> 09:19.000
It got rid of the split brain we had before with OnCloud 10.

09:19.000 --> 09:27.000
Before with OnCloud 10, we always had the database which keeps the metadata and we had the storage which keeps the data.

09:27.000 --> 09:35.000
When one part of it breaks or whatever, you have the problem that you lose more or less all your data.

09:35.000 --> 09:40.000
It is really hard to restore those data depending on the metadata.

09:40.000 --> 09:45.000
I don't know whether you are familiar with what metadata in this perspective means.

09:45.000 --> 09:49.000
It means, for instance, with whom did you share the data?

09:49.000 --> 09:58.000
When you miss this information, because for instance, the database collapsed, you have a real problem.

09:58.000 --> 10:03.000
We don't split this information and we store the metadata right with the data.

10:03.000 --> 10:10.000
The iPad app, just try it out yourself. It is just too good.

10:10.000 --> 10:18.000
I said already we had load and performance testing a lot.

10:18.000 --> 10:24.000
As you can imagine, schools start at 8 in the morning and every school starts at 8 in the morning.

10:24.000 --> 10:31.000
We know we have a huge load peak at 8 in the morning.

10:31.000 --> 10:38.000
We needed to make sure that this really is the software that is capable of dealing with it.

10:39.000 --> 10:48.000
What did we do? We designed two different test scenarios, one was test to fail and one was test to pass.

10:48.000 --> 10:57.000
We know precisely how far can we go and what is our sweet spot to go through.

10:57.000 --> 11:06.000
The target is that 95% of each full request, which means I do an action and as a user,

11:06.000 --> 11:10.000
I feel the action is completed after two seconds.

11:10.000 --> 11:14.000
That is what we are aiming with 95% of the request.

11:14.000 --> 11:17.000
We did it. We did it actually.

11:17.000 --> 11:27.000
The test to pass, the really test was 0.5 seconds of a full painted picture.

11:27.000 --> 11:35.000
Simultaneously, we did X test so we know that the user experience stays the same.

11:35.000 --> 11:38.000
I am running out of time so I am just going.

11:38.000 --> 11:54.000
What I want to emphasize is because we are in FOSSTEM, that onCloud Infinite Scale is not the only product which is in that platform used.

11:54.000 --> 12:01.000
OnCloud Infinite Scale only offers, there are different other softwares in between.

12:01.000 --> 12:13.000
There is a key clock as IDP, there is open LDAP as IDM, there is CLEMAV for the antivirus scan, there is a patch a ticker for the full text search and the OCR.

12:13.000 --> 12:17.000
There is a post fix for sending out the emails.

12:17.000 --> 12:22.000
When it comes to monitoring, also this is Grafana Open Observability Stacks.

12:22.000 --> 12:27.000
This Grafana, that is Prometheus, that is Mimia, that is Tempher and that is Loki.

12:28.000 --> 12:39.000
The operations which is done by our own staff is also mostly driven, more or less fully driven by open source solutions.

12:39.000 --> 12:46.000
We have Ansible with the AVX interface.

12:46.000 --> 12:57.000
We have the OCI by Sonataip, we have the Nginx with the web application firewall, etc.

12:57.000 --> 13:01.000
We have a helm for the package management.

13:01.000 --> 13:13.000
That is quite a big zoo, I would say, when it comes to the software we do use.

13:14.000 --> 13:26.000
We are not alone on the planet with our project, so we need to integrate in the universe which already existed.

13:26.000 --> 13:36.000
The first thing we need to do is to integrate their key clock, their SSO key clock with our key clock.

13:36.000 --> 13:44.000
Why don't we use immediately the same one? Because we need to host our clients, we have to host some other integrations, etc.

13:44.000 --> 13:59.000
The key clock installation went bigger and bigger and bigger, so it was an early decision to say we will host our own key clock and this will be federated with the customers SSO.

13:59.000 --> 14:23.000
Then there is the Metrix messenger by Stewie, which is integrated in a way that you can use the messenger and pick files from onCloud Infinite Scale and send it there or...

15:29.000 --> 15:34.000
...

15:34.000 --> 15:42.000
...

15:42.000 --> 15:44.000
...

15:44.000 --> 15:48.000
...

15:48.000 --> 15:53.000
...

15:53.000 --> 15:57.000
...

15:57.000 --> 16:01.000
...

16:01.000 --> 16:08.000
...

16:08.000 --> 16:12.000
...

16:12.000 --> 16:15.000
...

16:15.000 --> 16:18.000
...

16:18.000 --> 16:23.000
...

16:23.000 --> 16:32.000
...

16:32.000 --> 16:37.000
...

16:37.000 --> 16:42.000
...

16:42.000 --> 16:47.000
...

16:47.000 --> 16:52.000
...

16:52.000 --> 16:57.000
...

16:58.000 --> 17:05.000
I hope I understood your question correctly. You are asking about the autoscaling features we are using?

17:05.000 --> 17:14.000
Yes, because the user's pattern is relatively predictable, so everybody connects a day, you autoscale and downscale when people don't connect.

17:14.000 --> 17:28.000
Basically, this is the Kubernetes horizontal pod autoscaler, which is playing a very important role in it.

17:28.000 --> 17:33.000
There are other elements like resolver porting, etc.

17:33.000 --> 17:40.000
We are using all this information stored in the Helm charts.

17:40.000 --> 17:53.000
Of course, we don't scale to zero because we want to stay HA, high-waivable, at 24.7.

17:53.000 --> 17:56.000
That's why we don't scale it to zero.

17:56.000 --> 18:00.000
Of course, we do the scaling.

18:00.000 --> 18:07.000
To give you an impression, we had one IC day last week or the week before.

18:07.000 --> 18:10.000
Actually, it was three days.

18:10.000 --> 18:18.000
Because of homeschooling at that day, we had a load jump of 60%.

18:18.000 --> 18:24.000
It went smoothly without a second of outage.

18:24.000 --> 18:27.000
Thank you for that question.

18:27.000 --> 18:35.000
I was wondering, can you tell us anything about what sort of user feedback you've received today from teachers, students, parents?

18:35.000 --> 18:43.000
Yes. Thank you for the question. You are asking about the user's feedback.

18:43.000 --> 18:52.000
In the first period of time, we had 50 schools which were monitored very closely with LIME surveys.

18:52.000 --> 19:00.000
They were giving us immediate response to the overall feeling.

19:00.000 --> 19:13.000
Then we had some, I don't know how to call it, black box testing where our product management watched actually the users how they do some tasks.

19:13.000 --> 19:18.000
We improved our user interface and user experience according to it.

19:18.000 --> 19:37.000
Over the time, we have every four or six weeks, I'm not sure, check-ins with some certain key user groups which collect their impressions from the schools and report to us.

19:37.000 --> 19:39.000
Does it answer your question?

19:39.000 --> 19:41.000
Okay, perfect.

19:41.000 --> 19:50.000
You showed us you had 32 weeks between design phase and the first onboarding.

19:50.000 --> 19:51.000
Yes.

19:51.000 --> 20:00.000
Did you do a comparison between your vision and the commercial solutions that were available at the moment?

20:00.000 --> 20:11.000
Yes, but this had been mostly done during the tender phase, as you can imagine.

20:11.000 --> 20:14.000
We were checking on features, of course.

20:14.000 --> 20:21.000
We were checking on as files available on how they do stuff.

20:21.000 --> 20:38.000
At some certain points, we also did our own ways because we do have some insights provided by the customers, like for instance, school start at 8.

20:38.000 --> 20:50.000
I mean, it's common knowledge, but anyway, so we had some insights we could prevent to our platform, provide to our platform.

20:50.000 --> 20:55.000
Right now, we are still learning and getting better every day, right?

20:55.000 --> 21:04.000
Because even though I'm mentioning 5 million users, that's an adoption rate of 100%.

21:04.000 --> 21:18.000
Right now, we are at 950,000 users, but after 10 months after the initial start, the first school,

21:18.000 --> 21:21.000
after the first school, right now, we are 10 months behind.

21:21.000 --> 21:23.000
Okay, thank you.

21:23.000 --> 21:25.000
Any further questions?

21:25.000 --> 21:26.000
Hi.

21:26.000 --> 21:28.000
Hi, David.

21:28.000 --> 21:35.000
So since you store all the metadata with your files, I guess then on S3 or object storage,

21:35.000 --> 21:42.000
is there any sort of database for any sort of metadata, or is that all like user data is stored on Keynote, SSO only,

21:42.000 --> 21:47.000
you have metadata of files, which is together with the files on the object storage?

21:47.000 --> 21:49.000
Yes, thanks.

21:49.000 --> 21:58.000
The only database, or more or less only database we are using is Postgres SQL for Keynote.

21:58.000 --> 22:11.000
The metadata of the files are stored on RWX storage within the Kubernetes world, let's say,

22:11.000 --> 22:21.000
and we have our database to keep the metadata, which is the huge difference between onCloud 10 and onCloud Infinite Scale,

22:21.000 --> 22:24.000
because we needed to get rid of this split brain.

22:24.000 --> 22:29.000
It always is a hassle if you have to connect to the database and to the storage at the same time,

22:29.000 --> 22:36.000
and you cannot synchronize those processes 100%, and that's just impossible, right?

22:36.000 --> 22:43.000
So with this approach, we can make sure that this isn't a hassle anymore.

22:43.000 --> 22:45.000
Does it answer your question?

22:45.000 --> 22:47.000
Yes, just one thing.

22:47.000 --> 22:53.000
But you still store metadata and literally file blobs on different places,

22:53.000 --> 23:03.000
so in theory there's still the possibility of some sort of desync between the metadata volumes and the storage?

23:03.000 --> 23:14.000
In theory maybe, but I would say that's a very scientific question.

23:14.000 --> 23:21.000
I haven't seen and I couldn't imagine a vector in practice. Let's put it this way.

23:21.000 --> 23:22.000
Thank you.

23:22.000 --> 23:25.000
You're welcome.

23:25.000 --> 23:29.000
Hello. Thank you. I'm here.

23:29.000 --> 23:31.000
Great work and congratulations.

23:31.000 --> 23:33.000
Thank you for your presentation.

23:33.000 --> 23:44.000
The question is this, is it possible to try to make all this available for little communities in a little laboratory,

23:44.000 --> 23:52.000
or are there components that works only just on the power of the cloud?

23:52.000 --> 23:55.000
Thank you for asking this question.

23:55.000 --> 24:03.000
This is one of the most important things for us at OnCloud Infinite Scale,

24:03.000 --> 24:12.000
because we come from the community, we had developed this in close collaboration with the Zern,

24:12.000 --> 24:17.000
so this is also something we want to give you back.

24:17.000 --> 24:28.000
There is, when you go to, for instance, to get up to OnCloud.dev or to the documentation site,

24:28.000 --> 24:32.000
there's also available OnCloud Infinite Scale as a single binary,

24:32.000 --> 24:41.000
and you can run it in Docker, in Docker compose, or just as a single binary on your computer.

24:41.000 --> 24:45.000
Please feel free to download.

24:45.000 --> 24:48.000
Thanks for that question.

24:48.000 --> 24:49.000
Hello.

24:49.000 --> 24:50.000
Hi.

24:50.000 --> 25:00.000
I would like to ask, like, if there are any top level administrative tools for managing such a big cluster of Kubernetes nodes.

25:00.000 --> 25:02.000
Sorry, I didn't get a word.

25:02.000 --> 25:07.000
So I would like to ask, if there is a top level administrative tool,

25:07.000 --> 25:14.000
apart from like Grafana, etc., that you used to manage such a big cluster to deploy deployments, etc.

25:14.000 --> 25:19.000
Because it's such a big cluster of nodes.

25:19.000 --> 25:20.000
Yeah.

25:20.000 --> 25:26.000
Well, there is the Gardner provided by Plus Server,

25:26.000 --> 25:35.000
which gives us the opportunity to manage the clusters in a graphical way,

25:35.000 --> 25:39.000
and it also has some CLI, etc.

25:39.000 --> 25:49.000
But our philosophy at that point is more put the genius into the code, into the DevOps code.

25:49.000 --> 26:00.000
So when you're asking for something like a Sousa Ranger or something, we don't have that, to be honest.

26:00.000 --> 26:02.000
Yeah.

26:02.000 --> 26:04.000
Okay, thank you.

26:04.000 --> 26:08.000
I will have to interrupt you because time is over for that slot.

26:08.000 --> 26:12.000
First, I want to thank you with a little swing here.

26:12.000 --> 26:14.000
So that's nice.

