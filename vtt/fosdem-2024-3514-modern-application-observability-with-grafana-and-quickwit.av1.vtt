WEBVTT

00:00.000 --> 00:10.560
Okay, so thanks video team for fixing this.

00:10.560 --> 00:15.780
So that was very helpful and now few minutes late, but we just, you know, do the same talk

00:15.780 --> 00:21.840
a bit later, same 20 minutes slot plus five minutes Q&A is quick wit François.

00:21.840 --> 00:22.840
Welcome.

00:22.840 --> 00:28.840
Should I say it is it sufficient?

00:28.840 --> 00:31.680
Yes, I should be okay.

00:31.680 --> 00:33.680
Hi everyone.

00:33.680 --> 00:35.400
I'm very happy to be here.

00:35.400 --> 00:38.400
Thanks for having me in this room.

00:38.400 --> 00:43.280
We have been working like on observability since three years at quick wit and I would

00:43.280 --> 00:50.440
like to present here what we have done during the three years of outcome of it at least.

00:50.440 --> 00:52.920
First I will introduce myself a bit.

00:52.920 --> 00:53.920
So I'm François.

00:53.920 --> 01:00.040
I'm working on the core engine of quick wit, which is a search engine.

01:00.040 --> 01:04.800
I also the co-founder of quick wit and I also work a lot.

01:04.800 --> 01:12.400
I'm working a lot on the graph and data spring that I will show you in this presentation.

01:12.400 --> 01:22.520
So for this, for the agenda, I would start like with taking a step back, a short one

01:22.520 --> 01:27.240
and then we'll talk about like the problem of cardinality that we can have with metrics

01:27.240 --> 01:35.680
and then I will show you like very briefly the engine of quick wit and how it works.

01:35.680 --> 01:41.640
And finally, I will show you a demo of quick wit working in Grafana for application monitoring.

01:42.640 --> 01:47.240
So let's start by taking a step back.

01:47.240 --> 01:48.680
So I'm showing this graph.

01:48.680 --> 01:49.680
It's not mine.

01:49.680 --> 01:51.640
This diagram is from Ben Siegelman.

01:51.640 --> 01:59.160
This is a Googler who works on the distributed tracing dapper, software dapper when he was

01:59.160 --> 02:05.120
at Google and you co-founded also LightStep, which is a company doing observability and

02:05.120 --> 02:06.880
monitoring stuff.

02:07.000 --> 02:15.120
I kind of like this diagram because it summarizes all the complexities, the intricacies between

02:15.120 --> 02:21.760
monitoring and observability and the different signals that we can get from our applications

02:21.760 --> 02:23.760
or our servers.

02:23.760 --> 02:28.440
So at the bottom, you have the three signals or the three pillars.

02:28.440 --> 02:30.920
It depends on how you call them.

02:30.920 --> 02:32.640
Traces, metrics and logs.

02:32.640 --> 02:37.440
And generally, you store them in different databases, metrics, you store them in time

02:37.440 --> 02:43.360
series databases because you want something optimized for it and it can be very optimized

02:43.360 --> 02:46.680
for this kind of data.

02:46.680 --> 02:49.320
And for trace and logs, it depends.

02:49.320 --> 02:54.200
You can store them in a search engine or you can store them in dedicated storage.

02:54.200 --> 03:01.240
I'm sure you know tempo and loki, loki for logs and tempo for traces.

03:01.240 --> 03:08.720
And on top of it, you try to build your monitoring software with alerts on metrics or could be

03:08.720 --> 03:13.480
on logs or even traces if you can.

03:13.480 --> 03:21.000
So let me talk a bit about how the problem of cardinality here.

03:21.000 --> 03:29.280
At the bottom, you can see that metrics goes always into the TSDB, but you can also put

03:29.280 --> 03:32.440
some traces information into your TSDB.

03:32.440 --> 03:38.200
But in this case, you need to be very careful about what you did because in traces, you

03:38.200 --> 03:44.340
can have a lot of labels and you can be very, very accurate about what's happening.

03:44.340 --> 03:50.880
So that's why I just want to stop there a minute.

03:50.880 --> 03:57.760
When you want to monitor a distributed system and we all generally have that somewhere in

03:57.760 --> 04:05.560
our job, they can fail for various number of reasons, even for a tremendous amount of

04:05.560 --> 04:06.880
reasons.

04:06.880 --> 04:12.840
And so you may want to label everything like if you have a software that is deployed with

04:12.840 --> 04:19.440
different versions, you want to have this version label, same for the host, same for

04:19.440 --> 04:25.760
the customer ID, if you are a SaaS, for example, you can have thousands of them.

04:25.760 --> 04:30.400
And you want also to monitor your services, your endpoints.

04:30.400 --> 04:36.320
In summary, your cardinality will explode and this will be a problem for your time series

04:36.320 --> 04:39.360
database.

04:39.360 --> 04:44.560
It's a problem, it can be either a performance problem or a money problem because if you look

04:44.560 --> 04:51.920
at data dock pricing, for example, you will pay $5 for 100 custom metrics.

04:51.920 --> 04:56.480
So if you want custom metrics because that's it, if you want something very specific to

04:56.480 --> 05:00.440
your business, in this case, it will cost you a lot.

05:00.440 --> 05:05.080
So generally, you don't want to have all those labels on your metrics.

05:05.080 --> 05:10.960
You want to control and just keep a low number of them.

05:10.960 --> 05:14.080
That's not the same for traces.

05:14.080 --> 05:18.880
In general, traces, you want to keep everything so that you can dig into, like, really understand

05:18.880 --> 05:25.040
the full trace, each unit of work in your software, you will be able to understand for

05:25.040 --> 05:30.880
each customer, for one customer particularly, for one request ID or for one user ID, you

05:30.880 --> 05:34.200
will know what happened in your system.

05:34.200 --> 05:40.040
So generally, you keep everything in your traces.

05:40.040 --> 05:44.360
And that's what I will talk about today and that's what QuickWit is for.

05:44.360 --> 05:50.240
So QuickWit is an engine that is storing logs and traces and particular traces.

05:50.240 --> 05:53.720
It does not handle metrics.

05:53.720 --> 06:01.320
And it is a bit different than other search engines in the sense that we decoupled a compute

06:01.320 --> 06:02.320
storage.

06:02.320 --> 06:09.080
So we have the same approach as low key and tempo on this, that chip storage is great.

06:09.080 --> 06:12.560
If you use an object storage, it's also very reliable.

06:12.560 --> 06:18.720
So you don't lose your data and you have all the benefits of decoupling your write

06:18.720 --> 06:21.320
pass, your read pass.

06:21.320 --> 06:27.280
It's really great when you want to scale to a lot of data and that's the case in observability.

06:27.280 --> 06:34.920
And last point is that we worked on the search part a lot so that it can stay sub-second

06:34.920 --> 06:38.760
even if all your data is on object storage.

06:38.760 --> 06:44.320
And I will explain how it works very briefly.

06:44.320 --> 06:49.520
So the engine architecture is quite simple.

06:49.520 --> 06:55.600
It's globally the same for this kind of decoupled compute and storage architecture.

06:55.600 --> 06:58.200
You will find the same for a tempo, for example.

06:58.200 --> 07:02.440
So at the middle, you have your object storage where you store your data.

07:02.440 --> 07:04.800
This is the source of truth.

07:04.800 --> 07:08.400
On the left side, this is the write pass.

07:08.400 --> 07:11.320
And on the right side, this is the read pass.

07:11.320 --> 07:17.200
So on the right pass, you have your incoming, gizand documents, could be traces, could be

07:17.200 --> 07:18.360
whatever.

07:18.360 --> 07:20.960
And you have your indexes that is running.

07:20.960 --> 07:30.280
And each 30 seconds typically or each 15 seconds, it will build what we call a split

07:30.280 --> 07:31.280
in QuickWit.

07:31.280 --> 07:37.240
This is a file where we put all the data structures that are used at search time.

07:37.240 --> 07:47.200
So it's several well-optimized data structures to be searchable on object storage.

07:47.200 --> 07:54.080
So we create them, the indexer creates them, and then upload it to the object storage.

07:54.080 --> 07:59.640
So you will have a bunch of splits that are put on the object storage each 30 seconds,

07:59.640 --> 08:01.200
for example.

08:01.200 --> 08:07.920
And each time you put the split, you also put one row in a meta store.

08:07.920 --> 08:14.920
It could be a PostgreSQL database or it could be just a gizand file stored on the object

08:14.920 --> 08:15.920
storage.

08:15.920 --> 08:21.680
So we will add just the metadata of the split in it.

08:21.680 --> 08:28.560
And once it is inside the meta store, like the metadata of the split that was uploaded

08:28.560 --> 08:34.720
on object storage, on the object storage, then the searcher is able to search it.

08:34.720 --> 08:39.040
So you have this nice decoupling where then if you want more searchers, you just have

08:39.040 --> 08:41.480
to increase your number of searchers.

08:41.480 --> 08:44.800
You can even shut all of them down.

08:44.800 --> 08:48.360
That's not a problem.

08:48.360 --> 08:51.440
So that's for the high-level view.

08:51.440 --> 08:58.520
To understand why QuickWit is fat on object storage, I have to show you also how to do

08:58.520 --> 09:01.400
how a split is made.

09:01.400 --> 09:06.080
This is an interesting part because it shows you also the different data structures that

09:06.080 --> 09:07.560
we are using.

09:07.560 --> 09:13.200
And it will help understand how we can achieve fast search later on.

09:13.200 --> 09:19.440
So you have basically three data structures in the split and one thing that we call a

09:19.440 --> 09:20.440
hard cache.

09:20.440 --> 09:23.520
The first data structure is the doc store.

09:23.520 --> 09:25.560
It's a row-oriented storage.

09:25.560 --> 09:30.960
So if you have a document ID, we will give you the whole jizz and document.

09:30.960 --> 09:34.320
The second data structure is the inverting index.

09:34.320 --> 09:42.320
So in this case, if you are looking for a user ID or a quest ID or a keyword, it's optimized

09:42.320 --> 09:49.040
in the sense that if you give a user ID, we will retrieve immediately the list of document

09:49.040 --> 09:52.760
ID that contains this user ID.

09:52.760 --> 09:56.160
So it is very fast.

09:56.160 --> 10:03.800
And then you just have to retrieve a document from the list of whose document ID is.

10:03.800 --> 10:07.160
The third data structure is the column now store.

10:07.160 --> 10:10.480
So here it's for doing aggregations.

10:10.480 --> 10:18.280
If you want to do analytics on your logs or traces, we will use this column now store.

10:18.280 --> 10:19.920
You can have a lot of columns.

10:19.920 --> 10:21.120
You can have spare columns.

10:21.120 --> 10:24.160
That's optimized for that.

10:24.160 --> 10:33.240
And the last part is what we call it's a split footer that we keep in general in the memory

10:33.240 --> 10:36.000
of a searcher because it's very, very small.

10:36.000 --> 10:42.440
I put it 0.07% of the size of a split.

10:42.440 --> 10:44.080
So it's very, very small.

10:44.080 --> 10:49.960
For that, it's cool because you can always keep it in your cache on your searcher.

10:49.960 --> 10:58.880
And in this hot cache, you will find all small pointers to the other data structures so that

10:58.880 --> 11:04.400
when you make a search request, you will need only to make one or two requests to your object

11:04.400 --> 11:07.360
storage to find the response.

11:07.360 --> 11:15.480
So that's why I said when we optimized QuikWit for object storage, it's because of that because

11:15.480 --> 11:18.640
we optimized those pointers.

11:18.640 --> 11:27.440
We put that in one footer that we can keep in cache.

11:27.440 --> 11:36.920
So just the next part now, I will explain you a bit how spans are stored in QuikWit.

11:36.920 --> 11:38.420
Okay.

11:38.420 --> 11:44.360
I have only eight minutes left, so I need to speed up it.

11:44.360 --> 11:47.440
In QuikWit, you can model things as you want.

11:47.440 --> 11:49.360
You can put any documents.

11:49.360 --> 11:55.720
But for span generally, you want to stick to the open telemetry data model.

11:55.720 --> 12:02.200
So what we did for this demo is that we used the data model based on the open telemetry

12:02.200 --> 12:03.200
data model.

12:03.200 --> 12:09.320
Well, you have a bunch of fields that are always there.

12:09.320 --> 12:13.280
And you have also some dynamic fields like resource attributes and span attributes.

12:13.280 --> 12:15.080
All those are very dynamic.

12:15.080 --> 12:21.480
So I put some random examples here where you can generate random keys and random fields.

12:21.480 --> 12:29.120
And here are the nice things that QuikWit is also shameless so that we can store every

12:29.120 --> 12:31.520
inverting index and the columnar storage.

12:31.520 --> 12:36.800
We can store all those dynamic fields without declaring which fields you have or which you

12:36.800 --> 12:38.200
don't have.

12:38.200 --> 12:44.200
It will index everything in the inverting index and in the columnar storage.

12:44.200 --> 12:50.720
So it's nice when you don't know in advance all your attributes that you have on your

12:50.720 --> 12:53.480
spans.

12:53.480 --> 12:56.520
So it's time for the demo.

12:56.520 --> 13:00.600
So for the demo, I prepared a demo for application monitoring.

13:00.600 --> 13:09.640
So my first problem was generating spans, traces that are understandable for this kind of goal.

13:09.640 --> 13:17.720
So I discovered recently a tool called, which is an extension of K6, which is like a project

13:17.720 --> 13:24.080
from Grafana for testing, for load testing.

13:24.080 --> 13:26.640
And there is a nice extension to generate traces.

13:26.640 --> 13:28.920
I will show you a bit or it works.

13:28.920 --> 13:35.960
And then I deployed a QuikWit cluster on Kubernetes and I did a Grafana instance to show you the

13:35.960 --> 13:38.320
results.

13:38.320 --> 13:43.080
So a word on the XK6 extensions.

13:43.080 --> 13:47.000
It's a nice extension just to, you can declare some spans.

13:47.000 --> 13:51.760
Like here I put some services like shop backend, ethical service.

13:51.760 --> 13:54.080
And you can declare whatever you want.

13:54.080 --> 13:59.760
So it's a template and then you can set the cardinality.

13:59.760 --> 14:02.320
You can set if you want some random attributes.

14:02.320 --> 14:07.760
So it's pretty nice to stress test and see if your engine can handle high cardinality

14:07.760 --> 14:11.560
fields, can handle like many random attributes.

14:11.560 --> 14:16.880
So it's pretty cool.

14:16.880 --> 14:22.040
So let's do the demo because I prepared it's live.

14:22.040 --> 14:23.040
Can you see it?

14:23.040 --> 14:25.000
Maybe I can zoom it a bit.

14:25.000 --> 14:32.640
So here I'm a heating or Kubernetes cluster and the index in which I'm setting traces.

14:32.640 --> 14:38.720
So we have approximately here now 341 million spans.

14:38.720 --> 14:42.080
So if I do a refresh, you will see this number moving.

14:42.080 --> 14:46.760
So now we have 355 millions of spans.

14:46.760 --> 14:50.440
So great.

14:50.440 --> 14:56.400
You can see that the uncompressed size of the document that are ingested in QuikWit have

14:56.400 --> 15:01.680
around 200 and almost 300 gigabytes of size.

15:01.680 --> 15:07.480
And that it's less, we compressed data a lot in QuikWit.

15:07.480 --> 15:15.560
So here it's around you can divide by seven on this example, the size of the data ingested

15:15.560 --> 15:16.840
in QuikWit.

15:16.840 --> 15:24.440
So here the size of Publish Plits is the size that is taken on the JStorage by QuikWit.

15:24.440 --> 15:26.640
So what can we do?

15:26.640 --> 15:32.520
So we are sending a lot of traces just to confirm that it's live.

15:32.520 --> 15:37.280
So here it's just a dashboard based on QuikWit Prometheus Matrix.

15:37.280 --> 15:38.280
So it's live.

15:38.280 --> 15:43.800
I launched it I think six hours earlier today.

15:43.800 --> 15:48.280
So I'm just sending traces at 11 megabytes per second.

15:48.280 --> 15:53.880
That's not huge, but it's already pretty decent because it represents around one terabyte

15:53.880 --> 15:56.400
per day.

15:56.400 --> 16:00.800
And you can see that it is running with only one indexer.

16:00.800 --> 16:08.240
And we have one indexer that is not really doing a lot of things.

16:08.240 --> 16:12.840
It is using between one CPU and two CPU here.

16:12.840 --> 16:18.920
The spikes are due to the compaction that we are doing because QuikWit generates a lot

16:18.920 --> 16:20.040
of splits.

16:20.040 --> 16:23.600
And so we need to merge them.

16:23.600 --> 16:28.480
We need to merge them so that we reduce the number of threads that we will do on the object

16:28.480 --> 16:32.000
storage.

16:32.000 --> 16:33.520
So great, it's working.

16:33.520 --> 16:34.520
It's live.

16:34.520 --> 16:39.360
And I can show you that we can search traces with it.

16:39.360 --> 16:41.360
So let's have a look.

16:41.360 --> 16:45.880
So I'm running a search query on all the documents.

16:45.880 --> 16:47.600
So you have a lot of them here.

16:47.600 --> 16:53.880
You can see that per minute here you have one million spans.

16:53.880 --> 16:55.600
So that's a lot.

16:55.600 --> 17:12.960
And we can focus on maybe I want to remove QuikWit traces.

17:12.960 --> 17:13.960
It should work.

17:13.960 --> 17:20.280
It's service name, I think.

17:20.280 --> 17:21.440
Okay, great.

17:21.440 --> 17:27.480
So here, as we are sending QuikWit traces in QuikWit so that we can monitor our own cluster

17:27.480 --> 17:36.240
with it, so now I'm selecting only the traces that I'm generated for this demo.

17:36.240 --> 17:41.720
So for example, we can look for query articles.

17:41.720 --> 17:53.360
So here, for example, what I can do is focus on one span ID.

17:53.360 --> 18:01.360
So as we have an inverting index, you can look for very accurate attributes.

18:01.360 --> 18:11.040
So here I took a span ID, but I guess we can do this on another.

18:11.040 --> 18:13.120
Let me check.

18:13.120 --> 18:26.080
I need to check article two card.

18:26.080 --> 18:33.880
So now I want to focus on this one because I added on this span particularly a high cardinality

18:33.880 --> 18:36.160
field for the demo.

18:36.160 --> 18:44.440
So you can see here that you have this random attribute with this random value that is here.

18:44.440 --> 18:46.800
So I can focus on it.

18:46.800 --> 18:49.840
And very fast I will get the results.

18:49.840 --> 18:56.360
You can see that this attribute value happens only very rarely.

18:56.360 --> 19:02.840
And it's with a search engine, it's even faster when you have this kind of query.

19:02.840 --> 19:12.440
So that's nice to search through spans, but generally you want to dig into one trace particularly.

19:12.440 --> 19:21.480
So you can use a Yeager plugin that is pointing at QuickWit cluster and it will return all

19:21.480 --> 19:24.040
the spans for the given trace.

19:24.040 --> 19:32.920
So it's easy to go from one span to the whole traces.

19:32.920 --> 19:37.280
But usually you want more from your data.

19:37.280 --> 19:41.320
And by that I mean you want to monitor your services.

19:41.320 --> 19:46.720
Looking at one particular span is very nice, but when you know what you are looking for,

19:46.720 --> 19:53.440
but when you don't know, it's better to build this kind of monitoring dashboard.

19:53.440 --> 20:01.360
So here, okay, I will try to go a bit earlier in the past.

20:01.360 --> 20:03.120
Yeah, good.

20:03.120 --> 20:08.040
So I have set different panels.

20:08.040 --> 20:13.920
The first one is like a data histogram and I'm counting HTTP requests.

20:13.920 --> 20:19.080
So for calling HTTP requests, I'm using span attributes.

20:19.080 --> 20:22.240
I can show you that.

20:22.240 --> 20:26.120
So here I'm saying I want all HTTP requests.

20:26.120 --> 20:29.360
So I'm using like the open telemetry semantic for that.

20:29.360 --> 20:33.040
And I'm saying, okay, I don't want all other spans.

20:33.040 --> 20:39.640
I want only those ones who have at least one HTTP target.

20:39.640 --> 20:45.680
And here in the second panel, I'm doing like a group by by status code.

20:45.680 --> 20:53.720
So I'm using again like a dynamic attribute that is the status code.

20:53.720 --> 21:02.400
And I'm doing a group by on it and building this data histogram.

21:02.400 --> 21:05.160
This one is a bit the same.

21:05.160 --> 21:11.160
So here instead of doing a group by on a status code, I'm doing a group by on target.

21:11.160 --> 21:16.440
So if you want to monitor your endpoints, that's useful.

21:16.440 --> 21:23.320
This one I can show you a bit more because here it's not the count.

21:23.320 --> 21:29.000
We are computing the sum of the duration of each span.

21:29.000 --> 21:34.000
So I'm doing group by on each target and I'm summing all the span duration.

21:34.000 --> 21:39.080
So you have a good feeling about the time taken by each of your target,

21:39.080 --> 21:42.360
your each endpoint.

21:42.360 --> 21:43.720
But that's not enough, right?

21:43.720 --> 21:45.800
You want something that is more useful.

21:45.800 --> 21:50.080
It's nice to see this, but it's not enough to take a decision like is there a problem or not.

21:51.400 --> 21:59.000
The last one is the P95 latency panel is more for that.

21:59.000 --> 22:04.440
So you can see that you have a nice, smooth latency,

22:04.440 --> 22:08.760
a P95 latency on all your services, except for this one.

22:10.600 --> 22:15.600
This is normal because I sent some traces with different latencies.

22:19.400 --> 22:20.680
So we will dig into that.

22:22.040 --> 22:25.640
So we know that maybe there is a problem here.

22:27.400 --> 22:32.200
The panel average latency here is just the average.

22:32.200 --> 22:34.440
I think you understand that.

22:34.440 --> 22:39.960
And this one is also interesting because I'm sending spans from two virtual data centers.

22:39.960 --> 22:44.960
So one is CDG and one is a BIHRU.

22:44.960 --> 22:49.000
And so you can see that most of the spans are coming from CDG here.

22:50.320 --> 22:57.400
So for example, if we want to dig into why there is a problem on the P95 latency,

22:58.600 --> 22:59.800
and I will stop after that.

23:00.360 --> 23:07.000
I will just show you that you can add your query here and say,

23:07.000 --> 23:13.800
okay, I want to look at all the spans that are over one second and one second, 0.1 seconds.

23:16.200 --> 23:21.400
And so you can see that only the endpoint article, so card is problematic.

23:21.400 --> 23:27.400
You can see that maybe there is a problem in the data center because you have suddenly more requests here.

23:27.800 --> 23:35.800
And of course, if you go up to two seconds, then you see that here the color has changed,

23:35.800 --> 23:38.600
but it's coming from this data center.

23:39.400 --> 23:47.800
And if you want to dig into one particular traces, you can open like and have a look at it.

23:49.000 --> 23:50.600
So that's it.

23:52.200 --> 23:55.000
I think I will stop there because time is up.

23:55.400 --> 23:58.600
If you have questions, come and see me.

23:58.600 --> 24:01.000
I will be happy to answer them.

24:01.000 --> 24:05.000
And also I have some stickers and hoodies, so don't hesitate.

24:05.000 --> 24:07.000
I'm happy to give you some.

