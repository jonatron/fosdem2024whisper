WEBVTT

00:00.000 --> 00:10.400
Okay, now it's time for the to introduce the guy that needs to introduce no introduction

00:10.400 --> 00:16.840
so we all know Saul is one of the key members of the the GC team and today is going to talk

00:16.840 --> 00:22.720
about and I hope we'll not get are not first and I'm trying to shoot you down but yeah

00:22.720 --> 00:28.920
it's going to talk about Skynet and AI summaries in GC meet so thank you.

00:28.920 --> 00:33.640
Thanks Lorenzo and thanks everybody for being here. Time for the intro so I don't need to

00:33.640 --> 00:43.240
do that myself. Many of you probably know GC already and GC is where is my cursor. There

00:43.240 --> 00:50.520
we go. It's a video conferencing platform, it's a toolkit to build your own, it's really

00:50.520 --> 00:55.240
a set of open source projects that we combine together to deliver these end-to-end video

00:55.240 --> 01:01.480
conferencing capabilities. It's also a set of APIs and SDKs that you can mix and match,

01:01.480 --> 01:07.400
host it yourself, you know pay us some money and we have a service running or just go to town with

01:07.400 --> 01:13.800
it and it's also a community of people that build more plugins for our platform help each other

01:13.800 --> 01:19.640
and we saw for instance during the pandemic lots of people spinning up instance GC instances

01:19.720 --> 01:26.200
to help other people communicate. It became a lot bigger than the way it started because it's a

01:26.200 --> 01:33.080
project that has been around for a while. GC is 20 years old. It started out as a communicator,

01:33.080 --> 01:40.360
a CIP client then when XMPP Jingle became a thing it became the kind of pivoted to that

01:41.000 --> 01:48.280
and then video was a big focus so multi-party video was an area where a lot of effort was put

01:48.280 --> 01:55.160
together and that kind of came to fruition when WebRTC came out because that made the client was

01:55.160 --> 02:00.120
pushed to the browser and we could run the same software that powered a client on the server

02:00.120 --> 02:06.280
this time and do the multi-party video on the server and that's the GC we have today where last

02:06.280 --> 02:13.320
year I presented how we did 10,000 participants so it went a lot of many transformations over the

02:13.320 --> 02:19.880
years. I think arguably the biggest transformation in all this time was WebRTC and how the desktop

02:19.880 --> 02:29.160
client was in a way left behind and then everything was moved into the browser. Some say that AI is

02:29.160 --> 02:35.880
the next sort of gold rush or the next revolution in this space that will make things change a

02:35.880 --> 02:41.640
little bit. As the old joke goes well in the gold rush era it's not the gold diggers that make the

02:41.640 --> 02:49.000
money it's those selling shovels so I'm hoping to show you some shovels today. Now in 2023

02:50.120 --> 02:57.480
and plus and beyond what's kind of the state of things so AI became huge it has already been there

02:57.480 --> 03:04.680
right we have full played video games with AI characters but in November 2022 something changed

03:05.000 --> 03:12.840
can anybody guess? Open AI, open AI indeed. Open AI released

03:14.200 --> 03:23.320
chat GPT. Now the way I think of it in my head I think the most important part for the end user

03:23.960 --> 03:29.720
of chat GPT because this is vocabulary that is now second nature people have used these words

03:29.800 --> 03:35.560
even though you don't need to know about transformers so to me the more important part is chat

03:36.600 --> 03:43.240
because it's the first time that we could interact with an AI in that way. Before it was always hidden

03:43.240 --> 03:48.200
in some backend server or oh there is AI that enhances these pictures but there is this thing but

03:48.200 --> 03:53.720
you couldn't directly interact with it you couldn't ask it questions get back these prompts you did

03:53.720 --> 03:59.560
not have this ability to interact with it and I think that is what made these new developments

03:59.560 --> 04:06.200
more special than the fact that you can host them yourself of course it's a plus and you know in

04:06.200 --> 04:13.160
GC style this is kind of the path we would try to follow so as I mentioned GC is a collection of

04:13.160 --> 04:19.240
open source projects so it's in order to run GC meet is a platform that's built of different

04:19.240 --> 04:24.360
components and this is like the basic platform where we have the web server the signaling

04:24.360 --> 04:32.360
server which is still based in XMPP the GC video bridge in charge of routing and GCOFO to do the

04:32.360 --> 04:38.440
conference signaling. Now another component not depicted here is GIGACI which allows us to connect

04:38.520 --> 04:49.480
to the PSTN but in this very room sort of in 2017 we presented transcriptions with GIGACI.

04:50.040 --> 04:57.400
It was a project that started out with Google summer of code so we had this building block

04:57.400 --> 05:04.680
already in place and since all of these LLM technologies are text-to-text kind of operations

05:04.680 --> 05:09.800
where you need to feed it text to get some other text you really need to have transcriptions to

05:09.800 --> 05:16.040
start working with it so we had this building block in place and internally we started to prototype

05:16.840 --> 05:24.120
how do we want to leverage these tools are they of use to us so we conducted an experiment where

05:24.680 --> 05:31.320
we built the bots framework our original idea was instead of us building something directly

05:31.320 --> 05:37.080
we're going to build a framework so that these technologies can be integrated externally into

05:37.080 --> 05:45.880
a meeting our rough idea and what we built was to use puppeteer to run Chrome because it's the

05:45.880 --> 05:52.200
richest web RTC endpoint that we can have and I'm going to show why running Chrome on the backend I

05:52.200 --> 05:58.520
guess you could call it was a good idea and we integrated a low-level library so live jitsimid

05:58.520 --> 06:05.240
and you could pretend to be a participant talk to an LLM get the transcript and talk to the user

06:05.240 --> 06:10.760
and with with the script this is what we prototyped and I'd like to show you a couple of videos of

06:10.760 --> 06:24.600
what we built so already hey tutor I've been thinking about the architecture of our to-do web

06:25.320 --> 06:31.880
have you given it any thought yes I've been reading up on it I think we should go with

06:31.880 --> 06:38.680
the restful architecture for scalability and flexibility that's a good idea how about the

06:38.680 --> 06:46.680
database should we use sequel or no sequel I think no sequel it's totally how we better for our needs

06:46.680 --> 06:54.280
it's more flexible and can handle unstructured data yeah agreed and what about the front end

06:54.360 --> 07:03.080
framework we had a robot I was thinking of using react it's fast at the end of the sounds pretty good

07:03.080 --> 07:08.200
how about we get started on building the application let's see if that works sure thing let's do it

07:10.200 --> 07:16.520
all right now we're going to bring John Doe into a meeting and he is arriving late

07:16.840 --> 07:23.400
and the bot just greeted him here in the chat and now he'll get a transcript real quick

07:24.280 --> 07:30.520
of what was said in this meeting and he can just continue the conversation so that's

07:33.640 --> 07:38.360
that was the first thing we wanted to try which is can we use this technology to build something

07:38.360 --> 07:45.000
like that right we had seen many others you know start with oh I get a transcript I do some stuff

07:47.240 --> 07:53.480
but because we had access to real time transcriptions it's like oh can we try a little twist so you

07:53.480 --> 07:58.360
just arrive late you get the summary of what was said already okay and this was done with this

07:58.360 --> 08:03.560
chrome running in the browser now why do you want to run chromium sorry chromium back in or in a

08:03.560 --> 08:08.360
container because then you can do cool stuff like this no not that

08:14.680 --> 08:15.080
hello

08:19.160 --> 08:20.840
I don't know if it's playing or not

08:26.920 --> 08:27.560
that should be

08:34.440 --> 08:36.840
I'll show you when I switch to the files

08:38.920 --> 08:46.040
now but basically it's the fact that we can play audio and use webgl and we kind of do that otherwise

08:46.040 --> 08:53.320
the browser is very smart to end it now what did we learn in this exercise of attempting to use it

08:53.320 --> 08:59.640
this way well first is that JavaScript might not have been the best choice because not all of the

09:00.440 --> 09:10.040
AI libraries are in a different camp also that for our specific application we think that that

09:10.040 --> 09:15.240
instead of going the general route of you know you can ask any question is more specific tasks and

09:15.880 --> 09:21.240
like in this case meeting summarizes something very well defined very well understood and that also

09:21.240 --> 09:27.240
allows us to give some more value to our users through customers so we can only help our users

09:27.240 --> 09:31.800
when they are in a meeting in our software but if we do something like this we can help them even

09:31.800 --> 09:37.560
when they're not there if you can go and check the notes of a meeting that you were not part of

09:37.560 --> 09:42.840
and then it turns out they are useful so you don't really need to be there well that is in and of

09:42.840 --> 09:50.280
itself something that's helpful for you now in terms of running this our idea is to run this

09:50.280 --> 09:55.560
modest model that fulfills the task because running these things I'm going to talk about in a little

09:55.560 --> 10:01.880
bit can be taxing so you want something you know as simple as possible which it still meets the

10:01.880 --> 10:08.760
criteria that you can of course cost can be a problem so when it comes to yeah we run the model

10:08.760 --> 10:14.440
well yeah but that also costs money so some you need to balance balance those things out

10:15.560 --> 10:23.240
and as I said one thing we realized is that writing all of these logic in a bot felt kind of wrong

10:23.320 --> 10:28.920
because you might want to use the same logic you know to apply it in different places so we thought

10:29.480 --> 10:34.760
maybe we should move all that logic to do summaries or to do other interactions with the meeting

10:34.760 --> 10:41.720
to its own dedicated framework and then we can reuse it here or maybe in other places and that's

10:41.720 --> 10:53.080
where the idea of SkyNet came from and we started prototyping that right after so SkyNet is our

10:53.480 --> 11:02.120
core for GC meetings it is designed to support different but specific AI operations our ability

11:02.120 --> 11:07.320
to be horizontally scalable so it could run multiples of the of each of the parts that compose

11:07.320 --> 11:16.040
SkyNet and we currently implemented like three but really two AI services which is summarizations

11:16.040 --> 11:22.280
and transcriptions we get the open AI compatible API for free but at the moment we're not

11:23.240 --> 11:29.240
making extensive use of it per se but focusing on the specific tasks of summarization and

11:29.240 --> 11:35.960
transcriptions and our initial focus as with kind of everything we do in GT was focused on running it

11:35.960 --> 11:44.280
locally using local LLMs so you can run it on your own servers and you don't rely on you know

11:44.280 --> 11:50.440
external cloud services this is how we always build things authentication is also a URL you

11:50.440 --> 11:56.280
plug and you connect elsewhere so it kind of fits our our DNA and personally I was excited

11:56.280 --> 12:00.920
because we were using Python again and I haven't been using it in a few years so that was cool

12:00.920 --> 12:09.080
to go back to it I totally installed this sort of naming from someone's conference at Comcom

12:09.080 --> 12:14.520
couldn't remember his name but I it really resonates with me the idea in the current

12:14.520 --> 12:21.480
AI landscape you have tools that can do speech to text that can text to speech that can do text

12:21.480 --> 12:28.520
to text and then so you have all these transformations right so and then you can combine because if you

12:28.520 --> 12:34.120
want to do a summary you probably have voice first so you need first a speech to text then some text

12:34.120 --> 12:42.280
to text and well then you could summarize that part so we've got our summary's application that

12:42.280 --> 12:50.520
sits on top of long chain and then we run our LLM underneath it and then for transcriptions

12:50.520 --> 12:55.720
we're currently running a whisper I'm going to go into a bit of detail on how we run whisper

12:56.840 --> 13:04.040
and we sort of divided the SkyNet architecture internally to show this this divide because I

13:04.040 --> 13:11.000
think it helps build this mental model of how the data flows from where it begins does it originate

13:11.000 --> 13:17.240
in the speech then goes to text then it ends up being text again so for example the summary's

13:17.880 --> 13:22.680
modules got like two parts to it one is the dispatcher we call it and the other one is the

13:22.680 --> 13:28.280
executor so the only one that needs access to the LLM to actually run inference and get

13:28.280 --> 13:36.360
an output is the executor and the idea here is that we can have multiple dispatchers that will

13:36.360 --> 13:41.800
handle the request then they will prepare the request and they will store you know the content

13:41.800 --> 13:47.960
that needs summarizing in a cache and as a worker becomes available will pick up the work do the work

13:48.920 --> 13:56.360
publish a result and then the frontend scan get back the result to the user these allow us as to

13:57.560 --> 14:04.520
throw in however many executors we can well of course based on how much money it costs us to run

14:04.520 --> 14:13.080
because we need GPUs to do the the inference how much capacity that we need to serve and of course

14:13.080 --> 14:18.760
how many of them we run is a metric of measuring your own how much you want to spend how much you

14:18.760 --> 14:25.800
need to service at a given time and how long can it take how long can the answer take so it's

14:25.800 --> 14:31.400
possible that getting a summary two minutes after a meeting is acceptable maybe you need it in a minute

14:31.400 --> 14:37.240
it all depends and depending on the way you want to go about it you could play with how many of them

14:37.240 --> 14:43.960
you want to run but we built it so that we could run it this way and then we could scale horizontally

14:43.960 --> 14:51.960
based on the on the load that that we needed now as we started playing with this like for real one

14:51.960 --> 15:00.120
thing becomes quickly kind of obvious which is your summary is is like when is a summary a good

15:00.120 --> 15:07.080
summary well first of all it is a good input so the transcription is really critical because if you

15:07.080 --> 15:12.360
have a bad transcript there is no way for you to get the bad summary with a good transcript you can

15:12.360 --> 15:16.120
also get a bad summary but if you have a bad transcript you're definitely going to get bad

15:16.120 --> 15:24.360
summary so uh jigasi as i mentioned before already had transcription capabilities today it has the

15:24.440 --> 15:34.040
ability to connect to world cloud to vosk and now to skynet now the google cloud has changed the models

15:34.040 --> 15:41.480
that they have and the one we have we were using was not really great it didn't give very very accurate

15:41.480 --> 15:47.480
transcriptions and that then showed in the in the summary that we were getting we have not yet played

15:47.480 --> 15:53.240
with the other models that they have but then again sending our audio samples to google is not something

15:54.200 --> 16:01.400
sorry that we're looking forward so we started building the equivalent on top of whisper because

16:01.400 --> 16:09.960
it gave better results the end result is a skynet module that so jigasi will open a web

16:09.960 --> 16:17.240
socket connection towards skynet and it will send audio frames in pcm format and it will get back

16:17.240 --> 16:23.720
the transcript and in this module we're going to run inference leveraging the faster whisper

16:24.680 --> 16:31.480
project faster whisper combines voice activity detection with with an alternate implementation

16:31.480 --> 16:39.320
of whisper to give you these transcripts and you can have near real-time transcriptions in fact

16:40.280 --> 16:45.720
in jigasi you can use this to also show subtitles so they were definitely fast enough for this

16:46.360 --> 16:52.280
for this application that we're interested in and what faster whisper allows us that quote

16:52.280 --> 16:59.800
unquote the og whisper doesn't is the ability to do it in this streaming manner and in real time

16:59.800 --> 17:06.280
so it's way files back and forth is not something we we can use for this application because we want

17:06.280 --> 17:12.040
to get the transcripts in real time and this is this is the way we accomplish it one advantage we

17:12.040 --> 17:20.040
have also from doing it this way is that jigasi will receive the streams from each participant

17:20.040 --> 17:26.280
individually so we already have each participant identified by by their stream you can of course

17:26.280 --> 17:31.560
always do a transcript of a recording but if you have a recording with all the audio mixed in

17:31.560 --> 17:37.800
then you also need to sort of separate it and there can also be mistakes in that now this way

17:37.800 --> 17:43.000
we don't run into that problem because you guys you can clearly identify whose audio it is and send

17:43.000 --> 17:50.440
it as part of the metadata that comes back in that uh web socket channel so once we had all of these

17:50.440 --> 17:58.600
things kind of glued together we had to face reality which is this kind of ai ops so you

17:58.600 --> 18:04.200
can't just throw this in a server and it runs and it's all great because models are kind of big

18:04.760 --> 18:11.560
and that can be a little bit of a problem so deploying this to actual production is a new

18:11.560 --> 18:17.480
headache you need to worry about because you do want when you're as I mentioned if you're

18:17.480 --> 18:23.160
horizontally scaling you do want these new servers that you put in the pool let's say to boot fast

18:23.160 --> 18:29.960
you want them to be able to start working right off the bat so and then you also need you might need

18:29.960 --> 18:36.360
multiple container images because oh this needs this version of CUDA like the version of CUDA the

18:36.360 --> 18:43.080
faster whisper works with is different than the one we need to run you know like lama for example

18:43.080 --> 18:49.800
so you don't need to think about that currently the way we're doing it is we're running OCI virtual

18:49.800 --> 18:58.520
machines and inside each of them we run nomad and skynet in a container and then the VMs have the

18:58.600 --> 19:04.120
models loaded into them in their image so then this way the models are readily available because

19:04.120 --> 19:10.440
the container images are already pretty big and adding the model to them it would make them

19:10.440 --> 19:16.280
unbearably big and you end up with timeouts and yeah things you you don't really you don't really

19:16.280 --> 19:26.120
like I like to show you how this looks like a little bit and since I'm doing pretty good on time I think

19:26.920 --> 19:31.720
let's see where my mouse is so

19:34.360 --> 19:39.000
first come on

19:41.560 --> 19:43.880
okay so

19:48.760 --> 19:52.440
first I would like to show you the video that I couldn't show you

19:53.400 --> 19:55.560
it is this guy

19:58.760 --> 20:04.680
this was the video of the bot that joins the container thing

20:05.560 --> 20:09.400
well maybe that's why it didn't I'm a friendly ah there we go

20:10.200 --> 20:10.760
ah come on

20:11.400 --> 20:24.440
hello I'm a friendly bot this is just an example of what I can do check out other examples for more

20:26.600 --> 20:37.960
so this looks very simple and in a way it is but the way you get here is this little robot that

20:37.960 --> 20:45.240
moves its mouth is a 3d model made with blender animated with webgl and the lips

20:45.240 --> 20:51.960
animate as the audio is being played back the audio has been played back directly in the browser

20:52.600 --> 21:01.400
with we use the service play ht just as a as a test again going forward if we end up needing this

21:02.200 --> 21:08.360
I would love to tinker with Microsoft t5 or mozela's cognigy I think it's what's called

21:09.080 --> 21:17.080
because you can self-host them yourself so that using a browser as your runtime for bots is kind

21:17.080 --> 21:24.600
of nice because it does allow you to do these sorts of things where you can run something like this

21:24.600 --> 21:30.840
and it will be very hard to create a 3d kind of robot that moves their mouth in something else

21:30.840 --> 21:35.720
that's not really a browser and you can animate it so easily with the same library that you use

21:35.720 --> 21:43.880
for for the rest of the stuff so now I want to show you how this thing looks like when you run it

21:45.320 --> 21:53.800
so I committed some stuff so first let's look at for instance the real-time mess of

21:55.400 --> 21:56.600
of the transcriber

21:56.600 --> 22:05.080
let's see oh that's pretty good or is it

22:09.000 --> 22:09.720
are we running

22:14.040 --> 22:15.720
that's correct it's the other one

22:16.600 --> 22:23.240
way beyond so thank you rasvan you can tell who built it so

22:25.480 --> 22:29.480
as we connect and we wait for the interim

22:33.560 --> 22:34.840
hello mr robot

22:38.840 --> 22:39.880
let's look at what's going on

22:42.200 --> 22:44.440
are we running I didn't do anything

22:46.200 --> 22:48.200
let's reboot that

22:53.080 --> 22:53.800
I don't like you

22:59.400 --> 23:03.400
okay

23:04.840 --> 23:05.400
so

23:06.360 --> 23:15.720
okay I think we should be good now

23:21.000 --> 23:21.960
are you there yet

23:26.520 --> 23:27.640
what mic is it using

23:30.520 --> 23:33.800
let's see it is using my mic

23:35.560 --> 23:39.320
well that's unfortunate I'll try to show you later

23:40.680 --> 23:48.520
so this is just this demo is part of the skynet project itself and the way it works is it uses

23:48.520 --> 23:54.520
an audio worklet it will send the audio frames to the skynet server and then it will render them

23:55.480 --> 24:03.560
here in the browser it when it does it it does it in a close to real-time manner and the idea behind

24:03.800 --> 24:09.640
is that you don't need the whole jigasi thing in a jigsabang to test the way it works so

24:09.640 --> 24:14.280
it's like a self-contained demo I'm not sure if it's getting confused with the network or

24:14.280 --> 24:20.680
something else because when I tried yesterday it was working fine and in fact I do see you know

24:20.680 --> 24:27.720
data being received in this thing but I don't see it being rendered here we're going to try one more

24:27.720 --> 24:36.840
time to see hello are you there and if not we're gonna move on so we're gonna move on

24:40.280 --> 24:49.480
yeah well you know experimental technology what can you do so never mind that's what I was thinking

24:49.800 --> 25:02.600
because this network is fun let's see if the other one works so in here I'm now going to load

25:02.600 --> 25:12.280
skynet because we build this thing to be modular I think of it as a modular monolith if you will

25:12.280 --> 25:18.920
which is it's one fat thing but you can disable parts of it and the reason behind it is that you

25:18.920 --> 25:24.200
end up otherwise needing to have simplification you want to have consistent logging you want to

25:24.200 --> 25:28.600
have a lot of things that are common and then this way you can select what do you want to run do you

25:28.600 --> 25:35.720
want to run you want to run only the transcripts do you also want to run the summaries all of that

25:35.720 --> 25:41.400
stuff is separated and you can decide if you want to run it or if you don't want to run it so

25:41.480 --> 25:48.920
I put some text in here of a fictitious conversation that a guy named Tudor and I had

25:50.280 --> 25:56.280
chat gbt is very good at coming up with interesting conversations for you to see how they summarize

25:57.240 --> 26:04.120
we're not necessarily interested in the like I'm pretty sure you can't read the content but I want

26:04.520 --> 26:10.760
comment on on the API a little bit so I'm going to copy this in case this thing goes to shit

26:13.720 --> 26:22.760
because we don't know now we send a post with the data that we want to get summarized to our

26:22.760 --> 26:30.840
summary endpoint hopefully it did get the response yeah and we get back an ID then

26:31.640 --> 26:36.200
this ID we can query in the job ID

26:39.640 --> 26:39.960
thing

26:43.160 --> 26:45.720
and there we go so here we get

26:48.600 --> 26:55.400
so we get the result of our job with the status of success the type is a summary it took 11 seconds

26:56.120 --> 27:02.840
this is a mac with an m1 so it's running accelerated on this very gpu and then yeah

27:02.840 --> 27:07.240
Tudor and seller are discussing the design of the both backend of the web app yada yada yada

27:08.040 --> 27:15.640
so we we built this API that kind of follows a bit of a kind of a polling mechanism

27:17.000 --> 27:23.160
because we found in practice that some you know even within within our workplace like this

27:23.240 --> 27:30.200
stingy proxies and summarizing a long conversation can take a little bit of time and if we had

27:30.200 --> 27:37.240
the request living for too long some proxy in the middle would decide to cut it and also in order

27:37.240 --> 27:43.080
to make it resilient to things falling in the middle and another of these machines taking over

27:43.080 --> 27:48.920
and then like running it again we we decided to build it this way so the idea is you post the

27:48.920 --> 27:54.440
summary you're going to get back an id and then with the with the job polling api you can get

27:55.000 --> 28:03.480
the jobs result as it's done i was looking into using for example event source as another option

28:03.480 --> 28:10.280
so you could have like an ongoing stream that's also a possibility that we'll probably look into

28:10.280 --> 28:17.000
adding to to make it a bit more a bit more palatable but we found that in practice this has been working

28:17.000 --> 28:28.760
well we have processed in the realm of hundreds of thousands of little summaries within within the

28:28.760 --> 28:35.720
company and we're so far happy with how the architecture is is working so we can focus on

28:36.440 --> 28:42.440
what we're going to do sort of next some of the things we we think we should do next are

28:43.080 --> 28:47.080
supporting multiple model runners or backends or however we want to call it

28:48.200 --> 28:54.920
as i said we started out with hosting our own llms but that has different trade-offs and there are

28:54.920 --> 29:02.440
different reasons why people may or may not want to do that so being able to run for example on top

29:02.440 --> 29:09.160
of this example was on top of a seven billion llama too but someone may want to talk to open

29:09.160 --> 29:15.000
ai directly maybe they are not worried about sending their data to open ai or they have a

29:15.000 --> 29:21.000
different deal or whatever or maybe cloud works better for you or maybe you want to actually

29:21.000 --> 29:26.760
not host the model yourself but kind of offload that responsibility to another company like open

29:26.760 --> 29:36.360
router which hosts open source models in that case what sort of thing does like what does this kind

29:36.360 --> 29:41.880
of do in that case well the nice thing is that it can shield you from all of those changes so you

29:41.880 --> 29:48.840
could we're thinking that we can change how you are actually doing the summary or sorry rather

29:48.840 --> 29:53.720
what engine you're using to to run inference but you don't need to change your api you don't need

29:53.720 --> 29:58.520
to modify any of your applications suddenly if we change to a to a model that works better you're

29:58.520 --> 30:03.720
going to just get better results and that is kind of the the path that we're trying to follow

30:03.960 --> 30:10.840
we're going to work on integrating late arrival summaries in in jitsi meet so as i said we'll focus

30:10.840 --> 30:16.120
on making building blocks so we can then plug them together to do this thing right now you can get a

30:16.120 --> 30:22.040
transcript with gigasi and you can use this kind to summarize it we are going to build a way so that

30:22.040 --> 30:27.320
you can glue it directly to jitsi meet to get the late arrival summary straight in the meeting without

30:27.400 --> 30:33.480
needing to have this whole bot thing which i'd be very happy if we go back to but uh yeah it may

30:33.480 --> 30:39.960
take a little while of course more prompt tweaking i think rob did a good job talking about you know

30:40.840 --> 30:46.600
prompting and and a huge prompt he had i'm not sure that ends ever so you always start with

30:46.600 --> 30:53.000
something and then get a tweak it also depends on the context right it's different to summarize

30:53.480 --> 30:59.160
an article than to summarize the conversation and you might want to take the model into a little

30:59.160 --> 31:04.280
journey or steer it into a given direction so i did that's a better job at what it needs to do

31:05.000 --> 31:12.040
and lastly actually this very past week there was a new release of of lava which is the open

31:12.040 --> 31:19.480
source vision model and in the context of a meeting which is kind of our our main focus

31:19.480 --> 31:27.000
it would make sense for instance to summarize slides so if someone because it does help you

31:27.000 --> 31:31.240
capture the context of a meeting if someone is sharing slides they're probably important and

31:31.240 --> 31:36.920
they're probably the central part of the meeting so if we could use lava to get a glimpse of what's

31:36.920 --> 31:43.240
on the slide plus the transcript we think combining all of that would give us a pretty good view of

31:43.240 --> 31:49.880
what happened in that meeting and this way again help our users those who were not necessarily in

31:49.880 --> 31:59.880
the meeting all of this we open source the first scan and version yesterday is now available there

31:59.880 --> 32:05.080
with yeah the full get history so we'll see our mistakes and everything we learned along the way

32:05.320 --> 32:12.280
we're not you know learning machinists here or mad scientists we are learning ourselves

32:12.840 --> 32:19.320
it's a brand new world and i have two people to thank for they are they actually came all the way

32:19.320 --> 32:23.400
here uh ross vanduuder thank you for joining me on this interesting journey

32:24.120 --> 32:35.560
i would definitely not be here telling you this if it wasn't for them so uh very thankful and

32:35.560 --> 32:40.520
it was a very exciting project to to get started and then we can take it further

32:42.680 --> 32:48.200
i'm not sure how am i on time because this thing resetted but that's all i got to tell you today

32:48.840 --> 32:53.880
if there's any questions i'm here right there at back

32:55.240 --> 33:01.160
so you mentioned that you are happy with architecture of your setup right now

33:02.440 --> 33:08.040
how happy are you with the actual summer rates is there much hallucination going on

33:08.760 --> 33:12.360
would be different from a human that is not necessarily

33:14.440 --> 33:16.840
fully aware of all the anti-cellual

33:19.000 --> 33:24.680
good question so ralph was asking um we're happy with the architecture are we happy with the

33:24.680 --> 33:34.680
summaries um so so you know another we are we're trying to support two apis summaries and action

33:34.680 --> 33:44.520
items now a nice thing about about um about this problem application is that if it fits in the

33:44.520 --> 33:49.720
context window right like the lm does not need to invent anything and everything needs to know

33:49.720 --> 33:56.600
is within the data that you give this is the conversation what was said here um and it can

33:56.600 --> 34:03.000
come up with decent summaries at the small model sizes but where small smaller models sort of where

34:03.000 --> 34:08.920
they missed the mark is that capturing what's actually important in a conversation what are the

34:08.920 --> 34:16.040
key points that we should focus on when i'm summarizing this thing and we're trying to find

34:16.040 --> 34:24.280
where is the right balance how high up do we go in the model size um so that we get a better summary

34:25.080 --> 34:30.440
in also in timely manner because of course going bigger will also mean slower inference

34:30.440 --> 34:38.040
and it will also may mean higher cost so uh we we didn't spend a huge amount of time

34:38.040 --> 34:45.160
in improving that yet we have something that you know works okay but it can definitely be improved

34:45.160 --> 34:52.600
so we we are not you know we we can be happier so we we are focusing on making that that better

34:52.600 --> 34:59.560
and our next step in that direction is trying to make this more available for example by sharing

34:59.560 --> 35:04.680
it here and also by sharing it within the company so that people can run their meetings and get them

35:04.680 --> 35:11.800
summarized and this way we're going to you know tweak improve and rinse and repeat all the time

35:14.680 --> 35:21.320
question there have you guys looked into using the language bubble in the browser itself using

35:21.320 --> 35:25.800
for example there's a root implementation blah blah which has wgpu full which means you can

35:26.040 --> 35:29.400
use the browser to you know

35:30.120 --> 35:34.200
interact with the language bubble and I was just wondering if there's been any experimentation

35:34.200 --> 35:42.200
with not doing it centrally on a server so the question is um that there are implementations

35:42.200 --> 35:47.880
that allow you to run the model directly in the browser uh by a rest with web gpu and if we have

35:47.880 --> 35:55.240
thought of running it in the browser no we have not um that said it's a very interesting idea

35:55.960 --> 36:02.280
and that is one that actually fits what I say in the beginning of the the sort of the bots thing

36:02.280 --> 36:09.000
because the browser is such like such a competent beast it can do everything um you could actually

36:09.000 --> 36:14.760
test that by doing it that way because essentially what that was is a script that the browser run

36:14.760 --> 36:22.200
in a container and it had access to everything so I think that would be a like a cool place to test

36:22.360 --> 36:29.320
because basically what our first test did was we used javascript to tell the transcriber hey

36:29.320 --> 36:35.880
give me a transcript in real time and then we would in real time talk to open ai let's say and

36:35.880 --> 36:40.680
get back results so in that sense running it locally would be something that's completely attainable

36:41.800 --> 36:47.640
very interesting I think it's it would be interesting to give it a go I'm not sure if when it comes to

36:48.440 --> 36:54.680
because one of the advantages of model like uh running a centralized thing would be the

36:54.680 --> 37:00.440
fault tolerance for instance so the fact that you get that you send a transcript from the dispatcher

37:00.440 --> 37:05.800
to store it in the cache and even if the inference the whatever node is running inference crashes

37:05.800 --> 37:11.480
another one will pick it up we have a mechanism to uh so that another one picks up if some work

37:11.480 --> 37:17.480
has not been updated in a while and if you push that all the way to the client well a everybody

37:17.480 --> 37:22.600
would need to run their own inference so it feels a bit more wasteful overall if you sum it all up

37:23.640 --> 37:29.160
I think right off the bat and be uh yeah you have the the problem of if it fails with your browser

37:29.160 --> 37:33.880
crashes you're kind of left out in the cold and in the other case the server can take care of it

37:33.880 --> 37:38.600
and send it to you you know by email when it's ready I think but I think is a very interesting

37:39.080 --> 37:42.280
thing right there behind you

37:59.720 --> 38:00.280
yes

38:00.920 --> 38:06.120
possible to connect your other your partners webxs and zooms and what

38:07.080 --> 38:15.800
right there are companies right so the question is sometimes you talk with external organizations

38:15.800 --> 38:21.560
and would you be able to put the bot there so the bot was the experiment that took us where we are

38:21.560 --> 38:28.680
but right now that effort is paused now that bot was a bot for gtc meetings specifically so

38:29.400 --> 38:35.400
um the idea here is the moment anyone from any organization joins one of your meetings you would

38:35.400 --> 38:41.320
be able to transcribe it now if you join their meetings uh yeah we don't have that I know there

38:41.320 --> 38:46.360
are companies that sell proprietary products that integrate with different meeting providers that

38:46.360 --> 38:53.400
can use these so it's it's a little bit of there's companies that use it externally then there is uh

38:53.400 --> 39:00.280
each vendor that is sort of building this up in themselves I'm not sure what the right answer is I

39:00.280 --> 39:07.080
think it feels like to me there's there's a lot of one of the hardest parts of working in this space

39:07.080 --> 39:14.840
has been filtering the signal from the noise there's just so much shit going on but I think this

39:14.840 --> 39:19.960
particular features really blend well with a meeting's product so adding the making them

39:20.040 --> 39:24.840
building so whenever anyone from wherever they are end up in one of your meetings when you record it

39:25.640 --> 39:31.320
in fact this is a change that is coming on the next stable release is that making a recording

39:31.320 --> 39:36.520
will involve making a transcription unless you opt out because at the end of the day the recording

39:36.520 --> 39:42.120
contains everything that's also in transcription the transcription is just making it more palatable so

39:42.120 --> 39:47.880
you can then operate on it um and that's sort of the direction that we're taking at the moment

39:50.200 --> 39:50.920
question here

40:01.240 --> 40:08.360
yes that is a good question um so that is a problem we have not solved yet so we do have

40:08.360 --> 40:14.920
the capability of doing translations but the the uh at the moment the only entity that we

40:14.920 --> 40:21.800
use to do translations is google cloud so that's why sort of out of this picture and also there's

40:21.800 --> 40:27.640
a limitation of the source language so you can get multiple you can get actually subtitles in

40:27.640 --> 40:34.840
different in the language that you want but as long as the source language is English so what we are

40:34.840 --> 40:39.720
looking for which is a bit further in the future is well first of all identifying the languages on

40:39.720 --> 40:45.320
the fly so the source language doesn't matter and then having the ability for the source language to

40:45.320 --> 40:50.840
not be set so to be different so that we can both be in a meeting and I can be talking to you in

40:50.840 --> 40:55.400
English you can be replying in French and I will see subtitles in English you will see subtitles

40:55.400 --> 41:00.760
in French and then we'll get the summary I don't know in Mandarin man because it's going to be fun

41:01.960 --> 41:08.200
but that's you get the idea that that's that's where we're going but more work is needed in that area

41:10.520 --> 41:11.020
um

41:12.840 --> 41:21.400
and that's all you can find me in the whole thank you very much

