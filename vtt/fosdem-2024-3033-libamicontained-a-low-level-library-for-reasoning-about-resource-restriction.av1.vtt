WEBVTT

00:00.000 --> 00:17.760
Hi, so I'm Tycho Anderson. This is my colleague Sebastian Dabdupe and we work. I don't know,

00:17.760 --> 00:24.880
is this, is that, I don't know, we'll see. Oh, did I bump it now? Oh, that off. Okay.

00:24.880 --> 00:32.880
All right, so we work at Netflix on the multi-tenant container platform there. And I want to point

00:32.880 --> 00:40.880
out that the name for this is not in any way related to Jesse Frazell's library we break out.

00:40.880 --> 00:44.880
In fact, I didn't realize that when I cooked up this name that there was that name space collision.

00:44.880 --> 00:53.880
And I think there's some thought that we will probably merge this into a Linux containers project soonish.

00:53.880 --> 01:01.880
So that disclaimer, this won't be this particular name may not be that long lived. So with that,

01:01.880 --> 01:11.880
I just kind of want to go into kind of a basic question. How many CPUs do you have? And this is a question

01:11.880 --> 01:17.880
that it seems very simple. You know, you could call one function that would return an integer and tell you the answer.

01:17.880 --> 01:25.880
But as we'll see in a little bit, people have screwed it up in a large variety of ways. So the way that we do this today is

01:25.880 --> 01:37.880
typically with C groups, I'll go over some of the other interfaces in a bit. But if I just did like a, so I'm in, I'm in this

01:37.880 --> 01:52.880
C group here, I've just created it. There's no limit right now. So this anything tasks in this C group can run on any CPU.

01:52.880 --> 02:12.880
So there's no tasks in the C group right now. So what I'm going to do is put this shell in that C group. So now the things you'll

02:12.880 --> 02:23.880
see the first pit there is the task, the shell and the second one is cat. So now those tasks are in this C group, but there's still no restrictions.

02:23.880 --> 02:41.880
So I'm going to, so now I've restricted this particular shell can only run tasks on CPU zero and one. So this is like every

02:41.880 --> 02:50.880
container engine that you talk to has a way to do this. And it's, this is how they do it. The problem then is if that container itself,

02:50.880 --> 02:59.880
for example, suppose that you were running system D in that container, that can just container may create a sub C group. So now I'm in some sub C group.

02:59.880 --> 03:21.880
And in fact, the container engine will then grant the container the ability to manage C groups. So, so now I'm, now I'm here and I can see that I'm in a sub C group.

03:21.880 --> 03:31.880
And I have CPU set that CPU is that effective that tells me what things in this sub C group can what processes, I'm sorry, what processors they could run on.

03:31.880 --> 03:44.880
But if you look, if I look at the CPU set that CPU's file, which is the one that you use to control stuff with that files empty. So in particular, if you are a naive run time,

03:45.880 --> 04:01.880
you might look at this file and say like, Oh, hey, I can run on lots of CPU's. And the reality is you can't because there's this other file that ends in dot effective that tells you here's what the some total of all of the restrictions up the tree are.

04:01.880 --> 04:16.880
But you have to know to look at this right file. And this is only one of the, the whatever four different interfaces we have. So there's the kernel has a command line you can use to do this.

04:16.880 --> 04:36.880
This is what I still see pieces what people who really care about this stuff like HPC people use. The second one is, honestly, it's what libc's used to use. Then there's prox that prox CPU info all of the system prox of s's are emulated by Alex CFS.

04:36.880 --> 04:55.880
So in some container environments, you will get a view in prox that of the right thing, but in other container environments, you won't. And then on top of all of that, we also have this is called sked get affinity, which gives you some combination of the above results, but does not, for example, containing

04:56.880 --> 05:12.880
I sold CPUs. So if you know, based on all of these interfaces, how to answer this simple question of how many CPUs do I have, you can leave. And if you don't know that's okay, because the whole point of this talk is that we're going to try and fix that for you.

05:13.880 --> 05:28.880
So, so what's missing from this, in particular, you can do stuff like CFS quotas. So you're not even really worried about the number of CPUs you have, you may be able to run on any CPU, but you may have like a very small shared quota.

05:28.880 --> 05:41.880
And so in large multi tenant systems like we have at Netflix, we're trying to move away from assigning specific people to specific CPU sets with the goal being that if there is a CPU that somebody can run on somebody should be running on that CPU.

05:42.880 --> 05:53.880
So even even the original question is not necessarily correct. So it's hard to answer. I'm just going to kind of give you an overview of all the funny stuff that we found.

05:53.880 --> 06:09.880
So TC Malik, if you use non sequential CPU assignments, it will seg fault. That's bad. The JVM's implementation. This is a bug that we filed. So it queries CPU set that CPUs not effective, which was the demo I showed you.

06:10.880 --> 06:20.880
If you just look at the wrong file, you'll get the wrong answer. The other thing is that we care about other resources besides CPU. So we also care about memory and other network things.

06:20.880 --> 06:32.880
And the problem is that this effective file is really nice for CPUs, but it doesn't exist for memory. So if you want to do it for memory, you have to walk the whole C group through yourself as opposed to doing what the kernel already knows.

06:33.880 --> 06:40.880
So answering this question is also kind of annoying for other resources that aren't the, you know, the most obvious one, which is CPUs.

06:41.880 --> 07:04.880
So what we would end up with in production is a two CPU job would allocate 384 gigs of heap, which is half of an R5. And then, you know, that's not very good. So, so this, this kind of annoying. There's a longer explanation that one of my colleagues wrote about, you know, how to compute this correctly in the face of C groups, but again, doesn't take into account I sold CPUs or other things.

07:05.880 --> 07:09.880
So there's more bugs. So G libc used to use.

07:11.880 --> 07:22.880
CIS devices system node, which is a CISFS file. And so we could with LXCFS mask that value, but then they switched to sked get affinity. So now we can no longer do that.

07:23.880 --> 07:29.880
So the reason that it's important to think about what G libc does is lots of people use this CISCFS.

07:30.880 --> 07:37.880
Or if you use the nprox command line tool to do make dash j nprox or whatever that uses G libc, which uses sked get affinity.

07:38.880 --> 07:50.880
So if you are restricting resources in a, in a strange way, you may get the wrong answer and you'll spawn the wrong number of worker threads and you'll context switch into oblivion and get less work done.

07:53.880 --> 08:05.880
So one, one of the G libc maintainers pointed out that this particular problem should be solved by the kernel. That's a bug that he filed that I think nobody from the kernel side at redhead has ever looked at.

08:06.880 --> 08:10.880
Musil for just for completeness does the same thing. Sked get affinity.

08:11.880 --> 08:20.880
So we also saw crashes in lib uv reasoning about this incorrectly, which is no jazz, which is important because that's what serves on Netflix.com.

08:21.880 --> 08:33.880
Even Alex CFS, which was written by some of the people in this room, myself included, I guess maybe nobody else in this room now, but some of the people on the program committee for this containers thing wrote this code and it was wrong.

08:34.880 --> 08:36.880
We have, we found a couple of bugs there.

08:37.880 --> 08:40.880
So this caused crashes in lots of places. That was also bad.

08:41.880 --> 08:43.880
So even the people who are supposed to know how to do this don't know how to do it.

08:43.880 --> 08:57.880
So, yeah, they also though, I mentioned this thing earlier about if you use shares and quota, you know, then you really get the wrong answer. Alex CFS has a solution for this, which is kind of cool.

08:58.880 --> 08:59.880
It's called the CPU future.

09:00.880 --> 09:03.880
So there's a question about where should this computation live?

09:03.880 --> 09:16.880
One of the G-lib C developers said that it should live in the kernel, but the kernel people, you know, haven't really worried about this. They continue to add interfaces for figuring this out.

09:17.880 --> 09:23.880
So, you know, one answer is it should live nowhere. We should just keep allocating large heaps and crashing stuff and whatever.

09:24.880 --> 09:26.880
Unfortunately, my boss doesn't like that answer.

09:27.880 --> 09:30.880
So the next thing is we should fix that in the container runtime.

09:30.880 --> 09:36.880
So this is the traditional way that we did with Alex CFS.

09:37.880 --> 09:44.880
So you bind mounts and file. It's a fuse file system. It looks at when you make a fuse file request, it looks at that process ID.

09:45.880 --> 09:51.880
It goes and looks in the C group tree for that process ID and then tells you it lies to you and it basically says, hey, there's this many CPUs online.

09:51.880 --> 10:04.880
So this is the traditional way it worked for a very long time until libc started switching away from looking at proc and cysfs files and started using cyscalls, which makes sense because parsing proc and cysfs is kind of annoying.

10:05.880 --> 10:09.880
So, you know, if there's a cyscall that can do the thing, they want to use it. So that makes sense.

10:10.880 --> 10:14.880
The kernel people often will say this thing that's mechanism not policy.

10:14.880 --> 10:23.880
So they will give you the mechanism to reason about the thing and it's up to you to make a policy about how many threads or whatever you want to spawn.

10:24.880 --> 10:33.880
And so in principle, they have given us the mechanism because the mechanism is these 40 interfaces that are all can do very different things.

10:34.880 --> 10:37.880
And so if you look at all the right places, you can do it correctly.

10:38.880 --> 10:43.880
So in some sense, the kernel already did it. It's just sort of complicated.

10:44.880 --> 10:46.880
So yeah, there's lots of them.

10:47.880 --> 10:53.880
And the other thing is that there's a new patch series allowing ebpf to do scheduling.

10:54.880 --> 11:01.880
So right now, if you think about cfs and cfs quotas, cfs is hard coded in the kernel, very well understood by lots of people.

11:02.880 --> 11:13.880
If you, the user can load an ebpf program that will now decide which tasks to schedule, the algorithm for determining how many CPU equivalents this thing has is now dynamic.

11:14.880 --> 11:16.880
It depends on the results of this ebpf program.

11:17.880 --> 11:20.880
So the kernel also can't tell you necessarily anymore.

11:22.880 --> 11:31.880
So that's, I guess, the goal of our presentation here today is to have this library exist in user space.

11:32.880 --> 11:35.880
So it's one place so that everybody can go and ask this question in one location.

11:36.880 --> 11:47.880
So it should have no dependencies because Golang doesn't want to link against libc, but also, you know, run times generally don't want to include a lot of other stuff because they're supposed to be small.

11:48.880 --> 11:51.880
If you think about JVM or whatever.

11:53.880 --> 11:54.880
So it should also be correct.

11:55.880 --> 11:56.880
So correct in two senses.

11:57.880 --> 11:59.880
One is, you know, we should give you the right number of CPUs.

12:00.880 --> 12:01.880
So we should do the math correctly.

12:01.880 --> 12:06.880
And the other senses, we shouldn't do like terrible memory corruption or other things like that.

12:07.880 --> 12:11.880
So it should be safe in the programming languages execution model sense.

12:12.880 --> 12:13.880
And with that.

12:14.880 --> 12:19.880
So what we're proposing is this library called live mi contained.

12:20.880 --> 12:29.880
And the idea is that, well, it'll be a container aware API that like calculates like with the resources you have.

12:30.880 --> 12:32.880
In this talk, we're mostly focusing on CPU count.

12:35.880 --> 12:41.880
And it'll be statically linked with the CBI and we're writing it in Rust for, you know, safety guarantees.

12:43.880 --> 12:51.880
And the idea is that it's meant to be used by like language run times and applications instead of like trying to figure out which is the right interface for resources.

12:52.880 --> 12:53.880
And here's a link to our repo.

12:53.880 --> 12:54.880
So.

12:55.880 --> 12:57.880
So why do run times ask?

12:58.880 --> 13:00.880
And like, and we hit all those bugs before.

13:01.880 --> 13:06.880
The idea is that they mostly do this to size their thread pools and specifically like their GC threads.

13:07.880 --> 13:10.880
And they want to size their arenas and allocator threads as well.

13:13.880 --> 13:20.880
And how can this go wrong to give kind of a maybe so much of a sense of what's going on in the future.

13:20.880 --> 13:24.880
So how can this go wrong to give kind of a maybe simplified example.

13:26.880 --> 13:38.880
Let's say you have 10 containers on a host and just to make the math easy that host has 100 CPUs and we assign each container 10% CPU quota.

13:50.880 --> 13:51.880
Okay.

13:59.880 --> 14:00.880
All right back.

14:01.880 --> 14:05.880
So again, 10 containers, 100 CPUs, 10% quota each.

14:06.880 --> 14:17.880
Now what happens is those run times in each container, they're all going to see 100 CPUs and they're going to start 100 threads that they expect to do like a CPU's worth of work.

14:17.880 --> 14:21.880
So what happens they eat through their quota right away.

14:22.880 --> 14:25.880
And you get a ton of starvation right GC pauses.

14:26.880 --> 14:27.880
Everything starts spiking.

14:29.880 --> 14:31.880
So what should they do?

14:32.880 --> 14:33.880
Well, call our API.

14:34.880 --> 14:38.880
So what we have first is kind of your classic.

14:39.880 --> 14:40.880
This is a classic num CPU.

14:40.880 --> 14:46.880
So we're going to call a skedget affinity and this takes into account CPU sets, affinity masks, online CPU sets, affinity masks, online CPU sets.

14:47.880 --> 14:48.880
So we're going to take num CPUs like all this stuff.

14:49.880 --> 14:54.880
But our real value add is this recommended threads calculation.

14:55.880 --> 15:01.880
So this will take num CPUs and constrain it with like quota, for example, and compute.

15:03.880 --> 15:05.880
It's not a rocket science algorithm.

15:05.880 --> 15:09.880
It's basically just quota over a period gives you like a number of like how many threads you should be running.

15:10.880 --> 15:13.880
This is what like system D and Alex CFS do.

15:14.880 --> 15:16.880
So it's like a well known calculation.

15:17.880 --> 15:20.880
And so let's do that example again, but with recommended threads, right?

15:21.880 --> 15:22.880
We've got 10 containers.

15:23.880 --> 15:25.880
We've got our 100 CPU hosts with quota.

15:26.880 --> 15:35.880
Now each one is going to get when they call recommended threads, they're going to get 10 CPUs and problem solved forever.

15:39.880 --> 15:43.880
So what have people done in the past, including ourselves?

15:43.880 --> 15:50.880
So every language runtime implemented themselves, like they call it, you know, pass a container aware flag to JVM.

15:51.880 --> 16:01.880
But usually as we've seen, they get it wrong or sometimes Alex CFS does it does the right calculation, but it can only do the the proc file system mounting.

16:02.880 --> 16:05.880
It doesn't do like doesn't take care of like skedget affinity.

16:05.880 --> 16:14.880
So what we did at Netflix was we use Alex CFS and then using set comp would intercept this skedget affinity to like do this calculation.

16:15.880 --> 16:18.880
So this is kind of our follow up to say we're like, there must be a better way to do this.

16:20.880 --> 16:26.880
And there's lib which is in the Alex project today, but it's not container aware.

16:30.880 --> 16:34.880
So that's actually the we go for and then the next thing we do is we do a set comp.

16:35.880 --> 16:50.880
And the next thing is I wanted to throw out some like some additional like issues we ran into, which is a lot of things assume like static like a static resource size, right?

16:51.880 --> 16:59.880
But in kind of a containers world and you can when you can do you can like edit C groups, like for a running process, we don't think that that's no longer a correct assumption.

16:59.880 --> 17:04.880
Right. So like you're allowed to change C groups on live process and nothing seems to take that into account.

17:05.880 --> 17:09.880
So other things we're thinking about are like, what if thread pools were dynamically sized?

17:10.880 --> 17:15.880
It would periodically check like, Hey, do I still have 10 CPUs or now do I have 20?

17:16.880 --> 17:19.880
You can resize your thread pool that way.

17:21.880 --> 17:22.880
But that's not the way it works today.

17:23.880 --> 17:27.880
So we're that's future work, but more like food for thought.

17:27.880 --> 17:30.880
And that's it. Thank you.

17:58.880 --> 18:01.880
For sure there's overhead.

18:02.880 --> 18:07.880
But the reality is like if you want to lie to people about the right answer to this question, like you have to do something.

18:08.880 --> 18:10.880
And so that's like the best stop gap.

18:11.880 --> 18:20.880
I think like, is there a less over heady way like the real solution writers for just applications to get it right.

18:21.880 --> 18:22.880
So they don't have to worry about it.

18:22.880 --> 18:29.880
And so that's sort of how we arrived at this particular solution is what if people just actually knew how to do this correctly.

18:30.880 --> 18:33.880
The overhead of getting it wrong is probably worse than this.

18:34.880 --> 18:35.880
Right.

18:36.880 --> 18:37.880
Yeah.

18:53.880 --> 18:57.880
I'm not sure.

19:13.880 --> 19:16.880
So checkpoint restore will do C groups.

19:17.880 --> 19:20.880
So like the majority of the way people do this is with C groups.

19:20.880 --> 19:34.880
So I think one problem could be is if you restored an application to a larger or a smaller number of CPUs than originally had today, like mayhem.

19:35.880 --> 19:39.880
So if you if you have 100 CPU application, then you restore it onto a two CPU box.

19:40.880 --> 19:41.880
It's still going to think it has 100 CPUs.

19:41.880 --> 19:52.880
It's going to allocate 100 memory arenas free like one for each physical thread so that it can do, you know, threadless locking to do memory allocations quickly and stuff.

19:53.880 --> 19:56.880
And so 98 of those 100 arenas will be wasted.

19:57.880 --> 20:09.880
So for you guys, if you want to dynamically change CPU size, you have the same problem, but actually on steroids because you really want application run times to resize their thread pools, which is Sebastian's last point.

20:10.880 --> 20:13.880
For us, it's mostly like just get it right the first time, please.

20:14.880 --> 20:15.880
Like let's figure it out later.

20:16.880 --> 20:17.880
Other questions.

20:27.880 --> 20:28.880
It's a good question.

20:29.880 --> 20:30.880
We should right now.

20:31.880 --> 20:35.880
Our set comp stuff is not open source, but there's no reason it could be.

20:35.880 --> 20:38.880
We just has to like turn the crank to do the work.

20:50.880 --> 20:51.880
Just just for the offline folks.

20:52.880 --> 20:53.880
Yeah.

20:53.880 --> 21:04.880
So it's a good question.

21:05.880 --> 21:12.880
So your question is sort of what is the roadmap for this workload like this was basically stop zero of like, do people think this is a good idea?

21:13.880 --> 21:15.880
So it sounds like you think is a good idea.

21:16.880 --> 21:17.880
It would be nice to have your collaboration.

21:17.880 --> 21:22.880
One of the questions I have for the LXC project leaders, are you interested in having this code in LXC?

21:23.880 --> 21:29.880
Because I'm thinking step two would be, you know, we can centralize the code in a place where the container people like it.

21:31.880 --> 21:35.880
And then step three would be, as you say, go hat and hand to each front time.

21:36.880 --> 21:42.880
And it probably involves submitting a pull request and then flying to the Golang conference and the JVM conference and the Node.js conference.

21:42.880 --> 21:46.880
And you know, like it's going to be a long road, but what we have now is bad.

21:47.880 --> 21:48.880
So anyway, to point question to.

21:55.880 --> 21:56.880
Yeah, we talked about it.

21:56.880 --> 22:09.880
So the reason why the thing that exists right now has been developed by all of us is because it could probably make sense to have something that's more generic, that actually covers what needs and that hopefully is bad.

22:10.880 --> 22:17.880
So I think it's a good place to have the community that's working as a project because we're really not famous enough for that.

22:18.880 --> 22:20.880
We've got like CFS with a lot of those things there that are already used by others.

22:21.880 --> 22:22.880
It's a good place for it.

22:23.880 --> 22:27.880
And as I was talking with Tiger in the past about this, this is a long game.

22:28.880 --> 22:31.880
It's something we've been wanting for a lot of decades.

22:32.880 --> 22:36.880
Nobody really did anything about it that we wanted to finally do something about.

22:37.880 --> 22:43.880
So in a lot of decades we can have sites on our great every means instead of having to fill it out.

22:43.880 --> 22:48.880
And in the end, there's going to be the end point in the budget of different things.

22:49.880 --> 22:57.880
So I mean, the obvious one is that for things like the good things, especially the good right time, those kind of things that's almost right, I go to the back of the crowd.

22:58.880 --> 23:04.880
Those definitely need this kind of information because right now they're trying to do it differently, they wouldn't be wrong for different reasons.

23:05.880 --> 23:06.880
So that's what makes sense.

23:07.880 --> 23:09.880
But we can also go to...

23:10.880 --> 23:16.880
... like you know, whatever it's using, it's not taking a lot of resources to use that kind of system.

23:17.880 --> 23:23.880
We've got that very common misconception in an activity that starts, you know, load up, rate, and then something.

23:24.880 --> 23:30.880
Which is really important because once you've got things that are mixed, you can easily create a lot of like 5,000 on the machine.

23:30.880 --> 23:36.880
That's how we did it all, by just creating a secret room that's got almost no resources on the media and then link that to the media.

23:37.880 --> 23:43.880
It's a pretty effective whole in right now, in order to do it, it's a great way to figure out what's actually going on.

23:44.880 --> 23:51.880
You can get some stuff, you can get some stuff on other bases, but if you want the full picture, you can have many different stuff.

23:52.880 --> 23:55.880
I'm getting really tired of having project passing next slide.

23:56.880 --> 23:59.880
It's just extremely hard to approach the material that they want.

24:00.880 --> 24:02.880
Sounds like the answer is yes. You're interested.

24:03.880 --> 24:04.880
Yes.

24:30.880 --> 24:33.880
I was just like, I'm not a project manager.

24:34.880 --> 24:38.880
I'm a group that people call, I don't know what it is.

24:39.880 --> 24:41.880
Because I'm a group that's a student.

24:42.880 --> 24:46.880
Yeah, yeah, memory is actually worse than CPU.

24:47.880 --> 24:48.880
I don't know what it is.

24:49.880 --> 24:52.880
I'm just trying to make it very easy to do in place.

24:53.880 --> 24:58.880
Yeah, so I would say I am certainly not opposed to this.

24:59.880 --> 25:08.880
I think this basically shouldn't live under some Netflix repo, Netflix repo, because nobody will take us seriously.

25:09.880 --> 25:12.880
It's much better to live upstream, whether it's Linux containers, U2 Linux or whatever.

25:13.880 --> 25:19.880
The biggest thing is I'm guessing that U2 Linux people aren't that into Rust.

25:19.880 --> 25:24.880
Rust is like a little bit of a weird thing, because you want to convince people that stuff is safe,

25:25.880 --> 25:28.880
but also adding a Rust tool chain to your build time is kind of painful.

25:29.880 --> 25:32.880
I'd be happy to talk if...

25:33.880 --> 25:34.880
Yeah, I have a question.

25:36.880 --> 25:40.880
He's a computer engineer, he's a employee of my company, so...

25:41.880 --> 25:43.880
Yes, yeah, we should talk after this, for sure.

25:43.880 --> 25:56.880
Yeah, yeah, where it lives is like, I don't want to make another talk in ten years with this same number of bug reports on the top.

26:00.880 --> 26:01.880
Other questions?

26:04.880 --> 26:05.880
Cool, thank you.

26:13.880 --> 26:15.880
Thanks for watching.

