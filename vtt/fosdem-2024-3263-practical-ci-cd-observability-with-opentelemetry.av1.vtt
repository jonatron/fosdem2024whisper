WEBVTT

00:00.000 --> 00:13.760
Hello everyone. Thank you for having us here. It's our first time here, so please be kind.

00:13.760 --> 00:18.760
It's like for both of us, it's the first time here, so we're a little bit nervous. And we're

00:18.760 --> 00:33.600
here to talk about practical CI, CID observability with open telemetry. This is the abstract we

00:33.600 --> 00:37.840
have submitted and of course we don't expect you to go through a whole thing because it's

00:37.840 --> 00:44.680
just enormous. But like if we could abstract the abstract in a way, this talk is about enhancing

00:44.680 --> 00:50.160
your pipeline's ability and performance by bringing observability to every stage of software

00:50.160 --> 00:56.280
delivery. So we're going to answer two questions like how we can identify flakiness and bottlenecks

00:56.280 --> 01:04.360
during our CI-CB process and envision a future of effortless visibility. And again, we're going to

01:04.360 --> 01:12.160
talk about what's the role of open telemetry on that and also how, with the role it plays in

01:12.160 --> 01:18.320
shaping CI-CB's future and explore all the challenges and opportunities ahead. So if I go to the

01:18.320 --> 01:24.200
next slide, a little bit about us, I'm Dimitris, a software engineer at Grafana Labs at the

01:24.200 --> 01:32.600
Platform Productivity Squad. And I'm Giordano, Gio for France. I'm a software engineer for the

01:32.600 --> 01:40.360
X-Port Squad at Grafana Labs. So this is the agenda. As I said, we're going to start by

01:40.400 --> 01:46.480
defining what CI really is and then talk about current issues we have with CI-CB systems. And then

01:46.480 --> 01:53.120
we're going to do a small intro to open telemetry and how we use it really, why it is important to

01:53.120 --> 01:58.600
own our data, and then practical use cases where we are and what's next. So all that being said,

01:58.600 --> 02:04.880
we can proceed to the next slide with a question to you all. So what is CI? We're looking for a

02:05.520 --> 02:18.320
definition of CI here. Anyone there to guess? It's fine if not. Okay, I'll proceed. So, sorry,

02:18.320 --> 02:23.440
say again? Yeah, we're not looking for continuous integration. We're looking for the definition of

02:23.440 --> 02:29.120
that. So CI, I guess, thank you very much, is continuous integration. But the definition of

02:29.200 --> 02:37.080
that is, you know, as some experts have defined it in a couple of books, the thing is that continuous

02:37.080 --> 02:43.000
integration definition can mean different things to different people. It's all a matter of a

02:43.000 --> 02:50.000
perspective where you're looking that from what CI means to everyone. But one thing is for sure,

02:50.000 --> 02:55.480
continuous is the only thing that's going to be there all the time because we're talking about a

02:55.840 --> 03:03.240
never-ending feedback loop, you know, which keeps improving stuff and gives us like visibility

03:03.240 --> 03:11.200
over our CI CD processes. So we can move to the next question, which is what is CI like for real

03:11.200 --> 03:20.120
this time? And I would like again, if someone dares to guess. It's a black box. Yes, right. Yeah,

03:20.440 --> 03:28.920
go on. Yes, running test could be. So again, CI is a list of things like it's a mechanism to,

03:28.920 --> 03:35.000
for example, reduce repetitive manual processes or generate deployable software at any time and

03:35.000 --> 03:40.440
any place. And of course, like, you know, scores, flaky tests and flaky builds and prevent people

03:40.440 --> 03:45.640
from, you know, getting paid at 3am in the morning because we don't want to like spend human hours

03:45.720 --> 03:53.000
during the night. They're really important for us. So the next slide is about, I mean, if you think

03:53.000 --> 03:57.560
that this is complicated, I think not. I think, you know, this happens to this has happened to at

03:57.560 --> 04:03.000
least everyone of us, at least like once, I think. So, you know, it starts from testing, building,

04:03.000 --> 04:09.000
deploying and then waiting for changes or wait for 3am maintenance windows. You can see here

04:09.000 --> 04:15.240
to errors and downtime and panicking. And also when we have resolved all those issues, we all go

04:15.240 --> 04:20.200
to LinkedIn after that and say, you know, I'm a troubleshooting expert or like DevOps expert,

04:20.200 --> 04:28.120
automation expert. So that's what we do because that's who we are. Yes, true. Exactly. So the next

04:28.120 --> 04:35.160
slide is another question. Yeah. So another question, the next slide, what is CI like for real, real

04:35.240 --> 04:41.880
this time? And what we're looking for is a single word. If anyone, there's to guess that single word.

04:43.640 --> 04:48.360
Pipelines, automation. Alertings. Yes, that's what we're looking for. We're looking for

04:48.360 --> 04:55.800
alerting. So CI and alerting serve a common person, a purpose or at least they try to serve a common

04:55.800 --> 05:01.400
purpose. So they work closely together as essential components of continuous automated

05:01.400 --> 05:07.400
monitoring. So you can see that both of them are practically identifying issues or, you know,

05:07.400 --> 05:14.200
we have like continuous system monitoring or like, you know, all those things. And then CI,

05:14.200 --> 05:20.200
we're looking at alerting as the left shift of CI basically, which means that if we have

05:20.200 --> 05:24.600
affecting alerting within CI ensures that, you know, threshold bridges and like potential

05:24.600 --> 05:30.920
problems are going to vanish. And we see I needs to focus on like robust build for new

05:30.920 --> 05:36.600
releases. So together, CI and alerting serve a common goal like prompt problem identification,

05:36.600 --> 05:42.440
fortifying system, reliability and sustainability. As you can see from the picture, they need to be

05:42.440 --> 05:47.960
like holding hands forever because that's what they do. And we'll go to the next slide. So

05:48.840 --> 05:53.640
a few things about continuous integration. We already talked about some of them, but like

05:53.640 --> 05:58.840
continuous integration is the guard in early stages, like we can detect changes, maintain

05:58.840 --> 06:05.960
bills, health and constantly monitor system signals. And like CI is used to catch issues

06:05.960 --> 06:10.280
before they breathe really. So if we go to the next slide, we're having alerting next to that,

06:10.280 --> 06:16.520
but we're not actually comparing those two. We just want to show you how closely, like how

06:16.520 --> 06:23.880
tightly coupled they are together. So alerting is like, you know, is our alerting system,

06:23.880 --> 06:29.000
like for later stages, they identify as problems as well, maintenance, allows and monitor system,

06:29.000 --> 06:35.240
just like CI does. Just but we should see that alerting is a mechanism to be used just in case

06:35.240 --> 06:42.840
CI has something has slipped through CI and we didn't catch it. So when we have alerting in place

06:42.840 --> 06:48.600
and CI in place as well, we need to know that there are not two components running in parallel,

06:48.600 --> 06:55.080
right? They are like CI lanes that lays the groundwork and then alerting response to threats. So they

06:55.080 --> 07:02.440
are like, you know, unstoppable working together to serve the same purpose. So an important thing

07:02.440 --> 07:07.560
to remember about alerting is that every time we need to create actionable alerts. So if something

07:07.560 --> 07:13.480
slips through CI, we need to know, we need to get the alert, have a runbook, have some documentation,

07:13.480 --> 07:17.800
automatically resolve some alerts if we don't think that are important enough to wake up someone in

07:17.800 --> 07:25.160
the middle of the night and all that. So where we are now with CI-CD systems and like, what is this

07:25.160 --> 07:31.000
whole talk amounted to? So observability so far, as you can see here, is about like, you know,

07:31.640 --> 07:38.280
all the all-time classic concepts we know, like from printing here, we have all done this,

07:38.280 --> 07:42.760
like in our early stages, I guess, from printing that, we're still doing that? Yeah, okay, we're

07:42.760 --> 07:49.560
still doing that. And then from paging the platform team or from having three different

07:50.280 --> 07:55.240
platforms, like we can have GitHub or GitHub or Atlassian or Bitbucket, whatever you use and then

07:56.040 --> 08:01.880
find a broken test, go from there to your favorite CI vendor and then go from there to Grafana or

08:01.880 --> 08:08.360
Data Dog or your favorite visualization tool to try and correlate those errors together. So

08:09.320 --> 08:15.320
focusing, as you can see down there, if we focus, if like the sole focus of observability is at the

08:15.320 --> 08:21.080
run part of things, this neglects valuable insights from earlier phases like code review or building

08:21.080 --> 08:28.360
or testing and like incomplete observability across the CI pipeline leads to limited visibility

08:28.360 --> 08:33.000
during, you know, earlier stages. We don't know what happened during the build phase, for example,

08:33.000 --> 08:39.400
or the test phase or we have difficulty in root cause analysis or increased mean time to recovery.

08:39.400 --> 08:43.240
Gio is going to talk to you more about that and how this is related to Dora metrics and

08:43.240 --> 08:48.280
also missed optimization opportunities. Like we know that our CI pipelines take a lot to run,

08:48.360 --> 08:54.840
but we don't actually know what to improve if we want to make them sort of make them faster.

08:54.840 --> 09:01.320
So next question, typical, this is fine meme. You know, we know we deploy something,

09:01.320 --> 09:07.400
everything catches fire, we are happy and what we do basically is that we try to mitigate the fire.

09:07.400 --> 09:13.400
But when the observability part of things is so late in the deployment and development and

09:13.960 --> 09:20.920
life cycle, I think it's too late. So there was no reason to let it last this long and get this bad.

09:20.920 --> 09:29.160
So how we can be more proactive? If we shift our focus a little bit to the left, we can address

09:29.160 --> 09:35.080
issues before they escalate and be proactive. We can enhance the efficiency by catching problems

09:35.080 --> 09:40.600
early in the process. We can have, we can ensure robustness by focusing on like the integrity of

09:40.600 --> 09:45.960
our builds and tests and also be mindful about the cost reduction because this is also a really

09:45.960 --> 09:52.040
important topic and minimize expenses associated with post deployment troubleshooting at downtime.

09:52.840 --> 10:00.920
So the next slide, if we assume that we have focused our shift left, the other, so you know,

10:00.920 --> 10:05.880
things turn the other way around. So instead of having the fire everywhere and then us in the

10:05.880 --> 10:11.000
middle like being agnostic of what's happening is the other way around. So we have a lot of time

10:11.000 --> 10:16.360
to mitigate the fire. We can actually be proactive and as we prioritize observability earlier in

10:16.360 --> 10:22.200
the development process, we are identifying and addressing issues actually before they become

10:22.200 --> 10:28.360
fire. We tried many tools. We tried to find the best way to set up like such a system so we can

10:28.360 --> 10:36.120
proactively like react to all those problems. So the tool we found easier to use and address like

10:36.120 --> 10:42.040
all those issues in CI CD pipeline is open telemetry because it helps us create like standard

10:42.040 --> 10:46.360
patterns and some underconventions. Jir is going to talk to you more about that in a little bit.

10:46.360 --> 10:53.560
So in the next slide, we're going to show how we use open telemetry to get to exactly this point

10:53.560 --> 11:01.080
where it's even if something appears, it's still too early and we can, you know, act and fix the

11:01.080 --> 11:07.480
issue before we wake up people in the middle of the night. So stage is yours. Thank you. Can you hear

11:07.480 --> 11:15.160
me? Okay. Thank you. So first question. What is open telemetry? Does anyone know what to work with?

11:15.560 --> 11:27.160
One? So no one. Okay. A few people. But as a short definition of open telemetry is it's an observability

11:27.160 --> 11:33.800
framework which is designed to manage and create telemetry data such as metrics, logs, traces,

11:33.800 --> 11:40.840
events, whatever. There is of course a more comprehensive definition of open telemetry which

11:40.840 --> 11:47.080
is way longer, way more complex, which you can find on the open telemetry website. For this case,

11:47.080 --> 11:53.720
though, our, what I want to focus on here is two bits of definition which is semantic convention

11:54.360 --> 12:01.400
and owning your own data. Now, semantic conventions, we can think about them as a standard. Like it's

12:01.400 --> 12:06.920
a standard way of naming things, of a standard way of defining attributes for your logs, for your

12:06.920 --> 12:16.680
metrics, for your traces. And I mean, we know, we all know this. If we think about semantic

12:16.680 --> 12:24.040
conventions as standards, we can divide them by two different areas or we can categorize them by two

12:24.040 --> 12:30.760
different, in two different ways. By signal type such as metrics, logs, traces, events, whatever.

12:31.400 --> 12:38.040
And by AIA. Now, by AIA means we have telemetry, there are semantic conventions for databases,

12:38.040 --> 12:43.240
we have semantic conventions for cloud providers, we have semantic conventions for

12:43.240 --> 12:48.040
a lot of different things, really, for log files. Something that is not there yet, though,

12:48.040 --> 12:51.640
is semantic conventions for continuous integration and continuous delivery.

12:52.360 --> 13:00.360
Now, what is important in my opinion? This is important because, I mean, we use some

13:00.360 --> 13:07.400
CI tool, I guess, everyone here uses a different CI tool. But regardless of what we use, we can see

13:07.400 --> 13:13.400
that at the end of the day, the data that is behind its CI tool is the same. Regardless of

13:13.400 --> 13:20.840
whether someone calls it stage, someone calls it job, someone calls it status or outcome or

13:21.480 --> 13:29.240
whatever, the underlying data, such as the job name, the outcome of a CI system is the same.

13:31.000 --> 13:37.080
Now, I'm not extremely familiar with every CI system of there, but at some point I was trying

13:37.080 --> 13:42.760
to figure out why in our CI we had a test that sometimes was taking three minutes to a test,

13:42.760 --> 13:48.040
like a pipeline, that sometimes was taking only three minutes to complete, while some others up

13:48.040 --> 13:52.280
to nine or ten, which, I mean, without any code changes. So, like, if you talk about flakiness,

13:52.280 --> 13:56.760
yeah, that's part of, like, test failing, failing, other parties, why sometimes they take too long.

13:59.400 --> 14:09.960
Easy peasy, I think. I wrote some totally reliable Go code. No? No, okay. It was very good code.

14:10.920 --> 14:18.360
So, what was code was doing was getting stuff out of the ground database and pushing it to

14:18.360 --> 14:26.760
log a template in here for later analysis. It worked great. Worked perfectly. Now, what happened

14:26.760 --> 14:31.480
is that we were able, at the end of the day, even if the code wasn't very good, we were able to at

14:31.480 --> 14:37.480
least look at something outside of our CI system. Why this? Because our CI system didn't provide us

14:38.040 --> 14:43.400
with the UI, with the tools to query for the data we were looking for. So, we were trying to

14:43.400 --> 14:52.120
analyze why something was happening. And our UI wasn't able to do so. So, okay. I share the news

14:52.120 --> 14:59.720
with my team. And, I mean, I guess every one of you has been there at some point in your life.

15:00.680 --> 15:08.760
They got too excited. Ivana wanted us to have the log data on Elasticsearch. Piotr was,

15:08.760 --> 15:13.480
which is not a colleague, wanted to get tracing data from Git action, this is a lot of drone.

15:14.680 --> 15:18.120
And, yeah, no, that code was not good enough.

15:21.400 --> 15:28.600
I mean, back to the drawing board. What happens now? We need to figure out a way of getting data out of

15:29.320 --> 15:37.480
GitHub CI, drone, GitHub actions, whatever. I don't know. What else? Bit bucket. I don't know

15:37.480 --> 15:43.720
if they have a CI system. I'm not sure about. And, we need to push it to every database out there,

15:44.280 --> 15:50.120
from Graphite, Tempo, Elasticsearch, I don't know, whatever. Yeager.

15:52.440 --> 15:58.200
I mean, it sounds like a very silly question. I bet there is no one of us that really uses

15:58.200 --> 16:05.480
10 different databases to match their telemetry data. Also, because then, you know, this is what

16:05.480 --> 16:09.720
was going to happen. We had to write code to get data out of every CI system to push it to every

16:09.720 --> 16:20.440
other database system. And, I say no, like, I wasn't going to do that. But, this, I like some very

16:20.440 --> 16:27.480
important point that's owning your data. When I think about, when I started thinking about

16:27.480 --> 16:33.320
owning my data, what I thought about was mostly owning the hardware in which the data was going

16:33.320 --> 16:39.720
to be stored. So, like, owning the drive or having it stored on one of my machines. I think that

16:39.720 --> 16:47.160
that's not exactly the point we need to make here. I think owning your data means you being able to

16:47.160 --> 16:53.240
decide where the data goes, where and how to store the data. We can very, very well be using a cloud

16:53.480 --> 17:00.200
database provider to store our data. The important bit is that we own, we know, we decide where the

17:00.200 --> 17:05.080
data is going to be stored and we decide and we have the ability to use the data however we want.

17:07.320 --> 17:11.400
So, the reason why open telemetry is important and fits very well with the picture is that by defining

17:11.400 --> 17:16.360
standards and by defining a specification for which data can be transferred and stored, not

17:16.360 --> 17:22.840
stored but transferred, we are able to only take care of the first part of the equation here. We

17:23.640 --> 17:29.720
take data out of the systems and then open telemetry is going to take care about

17:30.120 --> 17:33.480
inverting and sending to the database we need.

17:36.200 --> 17:42.600
What we did was we built an open telemetry collector who, does any of you know what

17:43.800 --> 17:50.440
our collector distribution is? A few. Okay, so an open telemetry collector distribution is basically

17:50.440 --> 17:57.960
a set of pre-built components. It's a binary that you can run, of course, and you can configure to

17:57.960 --> 18:05.560
do things. It consists very reductively of receivers, processors and exporters. Receivers

18:05.560 --> 18:11.000
are the components that allow you to get data in, can be like watching at log files, can be,

18:11.960 --> 18:17.640
I don't know, even listening on Bluetooth 1.0 and check for things over the air,

18:17.640 --> 18:23.000
can be extracting metrics from some running services. There are processors that transfer this

18:23.000 --> 18:28.280
data in the format that you need. They add attributes, they modify attributes or remove them.

18:29.080 --> 18:35.160
And exporters that send this data out to your database of choice. The thing here is that

18:35.160 --> 18:39.640
for those exporters, we didn't write anything. Those are already open source exporters.

18:39.640 --> 18:46.120
There are more for elastic search for whatever, like really Jager or you name it.

18:47.720 --> 18:52.360
So the only bit we had to do was writing a drone receiver, which was getting data out of drone

18:54.040 --> 18:58.920
to pipe into open telemetry and then push it to log in, tempo and prometheus.

19:01.720 --> 19:08.040
There are some practical examples. There is a Jenkins plugin that gets traces data out of

19:09.000 --> 19:18.360
Jenkins and sends it via the OTLP format. Irokinz brought these other

19:19.000 --> 19:25.960
get-up functions that run commands and exports the execution of these commands as trace data.

19:27.000 --> 19:32.600
And of course, our own experiment, which is very complete but very well free to take a look at.

19:33.400 --> 19:40.600
Now, what is unlocked? What is unlocked for us? As I said, first of all, there were these

19:40.600 --> 19:46.680
performance issues with our first test. Second thing, at some point we had this test here

19:48.280 --> 19:55.560
that was a bit flaky. Failing sometimes, sometimes not. Of course, worked on my machine, worked on

19:56.520 --> 20:03.560
my advanced machine probably. But yeah, we couldn't figure out what to do with it because we would

20:03.560 --> 20:09.560
disable it but then when we were going to enable it, if you cannot really reproduce it locally.

20:10.680 --> 20:18.680
Now, by getting this data out of our CI and pushing the build logs into our observability

20:18.680 --> 20:25.080
system into our log instance, we were able to trace back from the build that you see on the right

20:25.560 --> 20:32.200
to the logs for that build, trace back to the first time that failed test in our CI.

20:33.720 --> 20:40.440
And from there, if you look down here, this is an attribute that we thought was valuable.

20:40.440 --> 20:46.920
We had a build number, which is our unique ID for drone, which then pointed out to the first

20:47.880 --> 20:54.440
pull request that introduced that test or that flakiness. With that, we were able to identify

20:54.440 --> 20:58.520
what was causing the actual issue, which was a test which was totally unrelated,

20:58.520 --> 21:03.960
running a different suite. But turns out that was causing the flakiness.

21:05.800 --> 21:11.640
Something else that we were able to do was, so first of all, like one, I don't know, silly thing

21:11.640 --> 21:20.920
that we did but we liked, was to create a custom UI within Grafana to mimic sort of like the UI

21:20.920 --> 21:26.760
that you have when you look at the output of your system. I mean, there is some value maybe

21:26.760 --> 21:31.160
near but the important bit here is that we own the data. We were able to do something which was

21:31.160 --> 21:37.480
funny. We spent maybe one day on it. And yeah, it will look good. The second thing, however,

21:37.480 --> 21:46.040
is more important. Now, in Grafana, we have a very complex release system. Very complex.

21:46.120 --> 21:54.360
We maintain a set of different release branches that need, in theory, should need to be

21:54.360 --> 22:01.960
released at an even time. Of course, like everyone, like for everyone, something things breaks.

22:01.960 --> 22:05.400
Sometimes things break and you don't know why because you are not looking at it. Sometimes

22:05.400 --> 22:11.000
a commit you make in main breaks something else somewhere else because you back ported it but

22:11.000 --> 22:15.800
you didn't really test it. What we were able to do with this was keep getting metrics and

22:15.800 --> 22:22.120
stats out of our system, out of our builds so that we could be the timeline of our deployment

22:22.120 --> 22:27.240
branches. This means that at any given time, we had a single pane of glass to look at what was

22:27.240 --> 22:32.360
the status of our release processes so that our release team could just go here and check

22:33.720 --> 22:39.320
whether something was broken they needed to act upon before trying to do our release.

22:39.960 --> 22:48.600
We also had visibility over the stats over the number of running pipelines or failed pipelines.

22:50.280 --> 22:55.960
We can dig into builds. We can do a lot of different things which we didn't feel like

22:55.960 --> 23:05.480
they were possible in our CI system UI. What is Unlocks? Really anything. The point here is that

23:06.120 --> 23:13.240
we are trying to define standards. We are trying to get into this space. It's a very early stage

23:13.240 --> 23:19.800
concept but what it may unlock given that you own your data, it's really up to you. We can talk

23:19.800 --> 23:29.400
about Dora metrics so having ways of reducing mean time to restoring services, we can talk about

23:30.360 --> 23:37.000
generating red like metrics or requests for your CI. How long did it take? Did something that

23:38.600 --> 23:46.520
happened start at the rate of our failing test? The duration started going up because of something

23:46.520 --> 23:51.240
we did on our environment. That's something that for us it unlocked quite well.

23:51.720 --> 24:00.520
Some other example, caught coverage over time. There is no reason why you cannot export test

24:00.520 --> 24:05.640
results as JUnit maybe and then graph them on Grafana and keep in track of your coverage.

24:07.800 --> 24:12.440
You can do flakiness detection like we did before. You start seeing that the test started

24:12.440 --> 24:17.720
failing at some point. You can detect that. You can create an alert on flakiness.

24:17.720 --> 24:26.520
You can trace back to where the test started flaking. At that point we think it's for us

24:26.520 --> 24:31.640
and we think it was way easier to identify what was the actual root cause of the flakiness.

24:33.000 --> 24:38.120
Then we have security, whatever. Really, the data is yours. You decide what to do with it.

24:40.680 --> 24:46.280
Again, all in all, what is unlocked for us at this point, I think there are three different

24:46.360 --> 24:53.320
CI systems. We are using three different systems for different reasons. All in all,

24:53.320 --> 25:00.440
what is unlocked for us was bringing all the data into there to work with Grafana and to have our

25:00.440 --> 25:09.400
production metrics together with the pre-production metrics. Now, what's next? We have formed an

25:09.480 --> 25:14.760
open telemetry working group about CI security observability. There are more stuff to come.

25:17.240 --> 25:21.960
Join the discussion. If you have your own issue that you want to fix or your own

25:21.960 --> 25:24.360
use case that you want to bring up to the group, please join the

25:25.560 --> 25:31.800
Cloud Native Computing Foundation Slack channel. This is the proposal for the standard.

25:32.760 --> 25:38.600
That's it. If you have any questions.

25:53.560 --> 25:54.920
Any questions? Yeah, up there.

25:55.880 --> 25:56.920
Anything you need?

25:58.680 --> 26:00.680
Yeah, we'll try.

26:12.920 --> 26:13.560
What kind of?

26:16.520 --> 26:17.000
Sampling.

26:25.880 --> 26:35.000
So far, we're not sampling anything. We are collecting a trace for every build that goes

26:35.000 --> 26:42.120
through the CI system. For PRs, it's a bit different because we don't want to create

26:42.840 --> 26:54.680
bad data, like useless data. It costs money. Data costs money. What we do is we generate data only

26:55.080 --> 27:01.160
for pipelines that happen on those branches we care about. So if you make a PR and the PR is okay,

27:02.040 --> 27:08.120
it gets merged into main. After it gets merged, we run another pipeline, the same one before the PR,

27:08.120 --> 27:14.440
and that one we collect data from. That way, basically, we have the flakiness on our list

27:14.440 --> 27:19.080
branch and not on the PRs because in PRs, I mean, it's not flaky. I mean, okay, we can

27:19.080 --> 27:23.960
reflect it as in PRs, but maybe we are doing something and it breaks the build, but maybe it's

27:23.960 --> 27:31.800
not a vital point. Yeah, if we did that for every branch, basically, we would face cardinality

27:31.800 --> 27:37.080
explosion and it's going to be so expensive. So you have to define which branches you're interested

27:37.080 --> 27:41.560
in. For example, in Grafana, we have the main branch, which is like the main branch of our repo,

27:41.560 --> 27:46.440
and then some version branches for all the different versions that Grafana have, for example.

27:46.440 --> 27:52.040
And this is what we're interested in. But again, you're on the data. You can decide to do it all

27:52.040 --> 27:55.560
your way. Any other? Yeah.

27:55.560 --> 28:00.520
How many flaky tests or else have you found by exporting data from this?

28:01.880 --> 28:09.240
What's the case? Yeah, number of unnecessary... Oh, yeah, sorry. Apart from flaky tests,

28:09.240 --> 28:13.240
what other metrics can we get, like useful metrics we can get out of that? So I think...

28:14.040 --> 28:18.360
No, no, no. What other problems have you encountered?

28:18.360 --> 28:22.360
Like you found out that you didn't find what you're just looking at?

28:22.360 --> 28:32.280
For example, stack runners. Runners were stuck in unused repositories. We didn't have a way to know

28:32.280 --> 28:37.560
that there was a runner running all the time. We're getting timeouts and all that. This one problem.

28:37.640 --> 28:44.440
Then another problem is the number of restarts in builds, which is basically related to flaky tests.

28:44.440 --> 28:52.760
But there was no way for us to know how many, like for Geo, for example, went and restarted his

28:52.760 --> 28:56.920
build because it was problematic, because there was a bug, or because there was an actual issue

28:56.920 --> 29:03.240
with runner. It doesn't have to be necessarily code related. So we needed to know how big was

29:03.240 --> 29:09.560
the number of the restarts and then try to find the root cause of what caused this, basically.

29:10.360 --> 29:16.200
There was also something I want to talk about, maybe, is that we are also able to improve a bit

29:16.200 --> 29:22.040
of the performance of our pipeline. And by performance, I mean just allocating more resources.

29:23.000 --> 29:26.680
By doing that, we were also maybe able to reduce the cost of the bit because

29:27.240 --> 29:33.240
the runner where pipelines were running for shorter, there was less queue.

29:34.440 --> 29:39.000
So it's also like improving performance also comes from having the data about

29:40.680 --> 29:49.160
how long they take. And also, last thing is that we also had issues where we used extremely

29:49.160 --> 29:56.200
powerful runners to build docs, for example. And docs builds took, I don't know, a minute

29:56.280 --> 30:00.120
where if the docs build took like five minutes, it was not going to be the end of the world,

30:00.120 --> 30:04.200
because there are docs, they're just small changes, really important changes, don't get me wrong,

30:04.200 --> 30:09.960
but small. So we could move away from really powerful runners to something smaller just to

30:10.680 --> 30:18.040
help with some cost reduction and stuff. Any other questions? Do we have time?

30:18.040 --> 30:25.320
Do we have one up there? Do we have time? No? Come join us at the Grafana booth, please.

30:26.200 --> 30:29.400
Thank you.

