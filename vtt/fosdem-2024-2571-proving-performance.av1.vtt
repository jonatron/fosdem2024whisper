WEBVTT

00:00.000 --> 00:10.600
So now we have Nikolai Vasquez who's come all the way from Atlanta to tell us about

00:10.600 --> 00:16.680
how we can improve performance in our Rust programs and give him your attention and it's

00:16.680 --> 00:18.360
going to be a really good talk.

00:18.360 --> 00:19.360
Take it away.

00:19.360 --> 00:21.000
Thank you very much.

00:21.000 --> 00:22.000
So, yep.

00:22.000 --> 00:24.320
Hi, I'm Nikolai Vasquez.

00:24.320 --> 00:32.960
Some of you might be familiar with my work in the Rust community such as the static assertions

00:32.960 --> 00:40.600
crate or recently Devon which is a benchmarking library that I'll be discussing in this talk.

00:40.600 --> 00:45.720
And so this title I realize is a bit of a misnomer.

00:45.720 --> 00:48.400
You can't really prove performance.

00:48.400 --> 00:53.680
Like there's just various factors that make this impossible such as for example there's

00:53.680 --> 01:01.040
various system conditions that could affect performance depending on machines and you

01:01.040 --> 01:04.120
could be working over different data sets.

01:04.120 --> 01:10.200
And so rather than considering this as proving performance, this is more like getting a vibe

01:10.200 --> 01:15.880
for performance.

01:15.880 --> 01:24.520
And so by show of hands how many people are familiar with measuring performance of their

01:24.520 --> 01:25.520
code?

01:25.520 --> 01:28.080
All right, so the vast majority.

01:28.080 --> 01:29.080
Great.

01:29.080 --> 01:33.760
All right, so you're all experts and you don't need me.

01:33.760 --> 01:40.160
So I know you probably know this but when we discuss what performance means in software,

01:40.160 --> 01:44.560
we're usually talking about how fast it is but to me in broader terms performance is

01:44.560 --> 01:49.400
more about how software uses resources to achieve a desired result.

01:49.400 --> 01:54.920
So along with thinking about the time that's being spent in our software, we should also

01:54.920 --> 01:57.840
be considering the amount of memory that it's using.

01:57.840 --> 02:02.280
I think that's a very important aspect of performance.

02:02.280 --> 02:09.440
And so making good software can be a balancing act of trade-offs between time and space and

02:09.440 --> 02:13.120
so it can be a bit difficult.

02:13.480 --> 02:19.000
As developers, the way that we write code can have a pretty direct impact on the performance

02:19.000 --> 02:20.480
of our software.

02:20.480 --> 02:25.040
So for example, we could be using really inefficient algorithms with a time or space complexity

02:25.040 --> 02:32.360
of O of n squared O of 2 to the n or whatever that yellow line might be.

02:32.360 --> 02:38.960
We might also be repeating computations instead of saving previous results.

02:38.960 --> 02:41.800
We could be choosing to use slower operating system APIs.

02:41.800 --> 02:49.640
So for example, waiting on sockets in Linux with the select system call versus ePoll.

02:49.640 --> 02:56.040
But also, performance can be bad for reasons that are out of your direct control as a developer.

02:56.040 --> 03:01.800
So at a micro level, for example, the CPU might not have cached the data that you're

03:01.800 --> 03:06.440
requesting and instead it will have to reach for main memory.

03:06.440 --> 03:11.880
The CPU might also expect the wrong branch to be taken and it won't speculatively execute

03:11.880 --> 03:18.600
the correct branch as well as the CPU might be waiting on previous computations before

03:18.600 --> 03:22.800
executing subsequent code.

03:22.800 --> 03:29.360
And then at the macro level, looking out, other cases might be that the network has

03:29.360 --> 03:32.280
really poor latency or bandwidth.

03:32.280 --> 03:37.560
Other processes could be using excessive amounts of RAM and that can cause DOS to swap memory

03:37.560 --> 03:43.440
to disk or your storage might be on a slow device like spinning hard drives instead of

03:43.440 --> 03:46.960
SSDs.

03:46.960 --> 03:52.080
So when it comes to performance, why do people choose Rust?

03:52.080 --> 03:57.240
I believe that the central reason to pick Rust for performance is it's in its community.

03:57.720 --> 04:03.720
I find that the community's culture of performance has led to many zero cost abstractions ranging

04:03.720 --> 04:09.920
from async away in the compiler to very fast standard library APIs.

04:09.920 --> 04:12.680
And we also see this culture in third party libraries.

04:12.680 --> 04:17.440
So people will try to make their code work really well and constrain environments in

04:17.440 --> 04:23.640
the embedded space or people will focus their attention on how well they're using time and

04:23.640 --> 04:25.040
space.

04:25.040 --> 04:29.760
So how fast their code is and how much memory it's using.

04:29.760 --> 04:33.680
And as well as the community has developed many tools to measure performance.

04:33.680 --> 04:39.320
So this really does speak to the culture.

04:39.320 --> 04:45.680
And now that we have a sense for what performance is, how do we go about measuring it?

04:45.680 --> 04:51.160
So for the sake of simplicity, I'll only be covering things that can be implemented with

04:51.160 --> 04:52.520
the Rust standard library.

04:52.520 --> 04:54.600
I'm not going to be covering.

04:54.640 --> 04:59.560
For example, hardware performance counters because each operating system has different

04:59.560 --> 05:07.520
APIs for that and usually accessing them requires root privileges and that can be difficult.

05:07.520 --> 05:13.960
So let's consider a simple operation such as allocating a vector of 180 integers.

05:13.960 --> 05:18.040
We could try timing it by using the standard libraries instant type and this is generally

05:18.040 --> 05:20.560
considered an all right approach.

05:20.560 --> 05:22.520
But the results may be surprising.

05:22.520 --> 05:25.200
It might just say zero nanoseconds.

05:25.200 --> 05:27.680
And so why is this happening?

05:27.680 --> 05:32.040
Well, it turns out that the compiler is smart enough to realize that the value wasn't actually

05:32.040 --> 05:35.800
used and so it optimizes out the allocation.

05:35.800 --> 05:41.200
And so when you're benchmarking, you really should pretend or at least trick the compiler

05:41.200 --> 05:45.040
into believing that the value is being used and so the standard library provides a black

05:45.040 --> 05:50.960
box function and you can use that to prevent the compiler from optimizing code that you

05:50.960 --> 05:52.080
want to run.

05:52.080 --> 05:56.920
And I find that a lot of people don't reach for this when they should.

05:56.920 --> 06:02.040
And so now that we're using this, we're actually getting higher timings that are more realistic

06:02.040 --> 06:08.920
and this is evidence that we're actually now measuring the allocation time.

06:08.920 --> 06:10.720
But why 500 nanoseconds?

06:10.720 --> 06:13.560
How consistent or accurate is this timing?

06:13.560 --> 06:18.640
Well, it turns out that if we run our code repeatedly, the times can fluctuate greatly.

06:18.640 --> 06:29.040
So the numbers might vary because of noisy system conditions or some of the things that

06:29.040 --> 06:30.960
I mentioned earlier.

06:30.960 --> 06:37.080
And you might wonder, well, OK, then how can we get a better sense of our code speed?

06:37.080 --> 06:41.520
And you could dive into existing solutions.

06:41.520 --> 06:45.600
What I generally recommend for practicality's sake is you should use an existing library

06:45.600 --> 06:48.600
that implements this correctly.

06:48.600 --> 06:53.480
And so recently I created this library, Devon, that is for exactly this.

06:53.480 --> 06:59.880
I wanted to make a tool that makes it very easy to do correct measurements and be able

06:59.880 --> 07:04.160
to compare various pieces of Rust code.

07:04.160 --> 07:10.080
And so to me, I would say Devon is so easy that it's a comfy bench marking library because

07:10.080 --> 07:14.960
a Devon sofa is like a comfy bench.

07:14.960 --> 07:18.560
And so that's why I named it that.

07:18.560 --> 07:24.360
You can read a bit more about it on the announcement blog post that I have on my website.

07:24.360 --> 07:32.880
But I'll also dive into what Devon can do for us today.

07:32.880 --> 07:36.360
And so I wanted to make Devon really easy to use.

07:36.360 --> 07:40.440
And I wanted the way to register benchmarks to be very simple.

07:40.440 --> 07:46.760
So I came up with this very simple yet powerful attribute macro that behind the scenes will

07:46.760 --> 07:50.360
generate the code that's needed to benchmark a function.

07:50.360 --> 07:56.280
And this might look familiar because this is also how you register unit tests in Rust.

07:56.280 --> 08:01.600
And like unit tests, registering benchmarks with Devon can also be done anywhere, not

08:01.600 --> 08:07.640
just in the same crate as your benchmarking runner.

08:07.640 --> 08:13.400
And you can also, well, I also take advantage of this feature within Devon by measuring internals

08:13.400 --> 08:20.120
of Devon with Devon, which is kind of meta.

08:20.120 --> 08:23.760
And so given the previous benchmark that we wrote, it's pretty straightforward to adapt

08:23.760 --> 08:24.760
it to Devon.

08:24.760 --> 08:27.600
We just stick it in a function and then mark it as bench.

08:27.600 --> 08:30.920
And then Devon will be able to run that.

08:30.920 --> 08:35.360
And after executing our benchmark, Devon presents us with pretty succinct information about

08:35.360 --> 08:37.720
how it ran.

08:37.720 --> 08:42.840
On this, we can see that the best speed was measured at about 70 nanoseconds.

08:42.840 --> 08:50.320
And this realistically represents how fast the function would perform under ideal conditions.

08:50.320 --> 08:54.480
And we also see that the worst case was measured at about 200 nanoseconds.

08:54.480 --> 08:56.920
And so there's various things that could play into that.

08:56.920 --> 09:03.480
It might not be necessarily the code itself, but the situation around the code.

09:03.480 --> 09:08.600
And then we also have median and mean, which represent the average time that the function

09:08.600 --> 09:11.160
took to run.

09:11.160 --> 09:16.680
And we can also see that these values are pretty close to the fastest sample.

09:16.680 --> 09:23.280
So we can be fairly confident that this function will generally perform at this speed, at least

09:23.280 --> 09:26.480
on this machine.

09:26.480 --> 09:33.440
And so to give some insight into how Devon is running this code, we see that it's reporting

09:33.440 --> 09:36.120
the number of samples and total iterations.

09:36.120 --> 09:43.120
And this represents how many timings, samples represents how many timings Devon has measured.

09:43.120 --> 09:50.080
And then iterations is the number of repetitions across all the timings or all the samples.

09:50.080 --> 09:54.640
And if we divide the iteration count by the sample count, we end up getting what I call

09:54.640 --> 09:57.400
the sample size, which is how many iterations per sample.

09:57.400 --> 10:02.840
And so we see that each sample took about 64 iterations.

10:03.120 --> 10:09.440
This is chosen dynamically at runtime based on how much time is spent in earlier samples.

10:09.440 --> 10:16.480
And this number can be higher for faster functions, or it can be as low as just one iteration

10:16.480 --> 10:22.560
per sample for really slow functions.

10:22.560 --> 10:29.160
But if we want to measure not only the time to allocate a vector, or sorry, if we only

10:29.160 --> 10:34.240
want to measure the time to allocate a vector and not the time to deallocate it, then the

10:34.240 --> 10:38.760
way this would work in Devon is you simply return the created value from the benchmark

10:38.760 --> 10:40.080
function.

10:40.080 --> 10:46.960
And this will defer freeing the vector until after the sample is finished being timed.

10:46.960 --> 10:52.080
And since Devon will automatically black box the returned value, we can actually remove

10:52.080 --> 10:54.200
the black box from our function.

10:54.200 --> 10:58.280
And this just makes it a lot easier to read.

10:59.280 --> 11:04.320
And so since we're measuring vector allocation but not deallocation, now our benchmark results

11:04.320 --> 11:13.280
are about half the time that we measured before.

11:13.280 --> 11:20.000
And so far we've only been benchmarking allocating vectors that contain 100 integers, but we

11:20.000 --> 11:23.080
can also benchmark across other cases.

11:23.080 --> 11:33.800
So we can use the args option to measure across one, five, 10, 1,000, you name it, any value

11:33.800 --> 11:36.920
that can be provided as an input.

11:36.920 --> 11:44.840
And this, I find it's generally very good practice to measure across various cases to

11:44.840 --> 11:48.920
get a better sense of how your code's running.

11:48.960 --> 11:57.760
And we can see that generally as expected, as the size increases, the benchmark also slows

11:57.760 --> 11:58.920
down.

11:58.920 --> 12:05.760
But interestingly enough, for cases that are at 10 or smaller, there's not really a difference

12:05.760 --> 12:07.520
in performance.

12:07.520 --> 12:14.560
And so really the differences, I would say, are more like systemic noise because it doesn't

12:14.560 --> 12:21.760
really make sense that creating five values in a vector takes longer than creating 10,

12:21.760 --> 12:26.680
at least not consistently so.

12:26.680 --> 12:31.400
And we also notice that this function really starts to slow down a lot at bigger sizes.

12:31.400 --> 12:39.320
And so that aligns with whatever hypothesis we might have had about this benchmark before.

12:39.320 --> 12:44.800
But we can also compare the performance across multiple types by making the function generic.

12:44.800 --> 12:49.760
And then we can provide a types option to pass all the cases.

12:49.760 --> 12:56.640
So now this benchmark is not only running the standard libraries vector type, but it's

12:56.640 --> 13:02.240
also comparing that against SmallVec using all the same cases as before.

13:02.240 --> 13:06.400
And for those who aren't familiar, SmallVec is a type that's designed to be faster for

13:06.400 --> 13:08.320
smaller sizes.

13:08.320 --> 13:13.720
And it does this by storing values on the stack instead of doing a heap allocation.

13:13.720 --> 13:19.920
But once there's not enough space on the stack, it'll fall back to using the standard libraries

13:19.920 --> 13:27.160
vector, or rather it'll use the heap like the standard libraries vector.

13:27.160 --> 13:32.480
And so to make what's happening a bit clearer, Devon's not actually doing anything special

13:32.480 --> 13:34.000
to the function.

13:34.000 --> 13:37.600
This is just normal generic code that's pretty common to write.

13:38.480 --> 13:43.480
Instead Devon is using the function as is to generate the benchmarking code for each

13:43.480 --> 13:48.120
type that's passed into the attribute.

13:48.120 --> 13:55.720
And so once we run this, we have this nice tree output and table where we see that Devon

13:55.720 --> 14:02.760
has grouped the types as separate trees under the benchmark function's name.

14:02.760 --> 14:07.760
And we can also see from these measurements that, at least for this specific operation

14:07.760 --> 14:13.480
collecting from a range, SmallVec is faster than the standard libraries vector when the

14:13.480 --> 14:17.240
number of items fits on the stack.

14:17.240 --> 14:23.920
However, once a size grows beyond fitting on the stack, once SmallVec needs to do heap

14:23.920 --> 14:28.840
allocations, interestingly enough the standard libraries vector is faster.

14:28.840 --> 14:33.120
And I imagine this is because the standard libraries vector can do nice optimizations

14:33.120 --> 14:38.720
like specialization, which if any of you can make that stable, please.

14:38.720 --> 14:44.400
I've been waiting forever.

14:44.400 --> 14:49.280
But also when we think about software performance, like I mentioned earlier, we shouldn't only

14:49.280 --> 14:52.920
be considering speed and we should also be considering the amount of space that's being

14:52.920 --> 14:54.560
used.

14:54.560 --> 15:01.520
And normally if you're profiling a long running program, keeping track of allocations with

15:01.520 --> 15:07.760
a tool like DHAT, the cost there is relatively low because it gets amortized generally over

15:07.760 --> 15:09.960
the life of the program.

15:09.960 --> 15:15.280
And the nice thing about tools like DHAT is that it'll collect back traces to tell you

15:15.280 --> 15:17.640
exactly where your allocations are happening.

15:17.640 --> 15:21.720
So it does give you a lot of information.

15:22.440 --> 15:28.720
However, in microbenchmarks, when the time spent tracking allocations, like that can

15:28.720 --> 15:30.560
have a noticeable impact.

15:30.560 --> 15:38.520
So taking back traces can take microseconds, whereas the code we want to measure may just

15:38.520 --> 15:40.640
be a few nanoseconds.

15:40.640 --> 15:45.920
And so we would be totally blowing out the timings.

15:45.920 --> 15:52.080
And in a sense, by observing the behavior of our program, we've now also affected our

15:52.080 --> 15:53.880
measurements.

15:53.880 --> 15:59.400
So is it possible to gather insights without affecting measurements?

15:59.400 --> 16:02.680
Is it possible to reduce the amount of time spent here?

16:02.680 --> 16:05.600
Well, I actually managed to do that.

16:05.600 --> 16:11.760
So Devon has a custom allocator that will only track the number of bytes allocated and

16:11.760 --> 16:14.520
the number of allocations during benchmarks.

16:14.520 --> 16:21.400
This applies to allocations, the allocation, reallocation of grow or shrink.

16:21.400 --> 16:28.400
And the way that you use this is you override the global allocator with Devon's allocrofiler.

16:28.400 --> 16:35.040
But you can also pass a custom allocator if in reality you are going to be using a faster

16:35.040 --> 16:37.400
allocator such as meme alloc.

16:37.400 --> 16:39.400
And so it's fairly flexible.

16:42.200 --> 16:48.320
So once we've registered this allocator and we rerun the same benchmarks as before, we

16:48.320 --> 16:54.800
can see which cases are allocating and how many times.

16:54.800 --> 17:00.880
And notice that we are not seeing the allocation listed here because, like I mentioned before,

17:00.880 --> 17:04.360
we're returning the created value from the benchmark.

17:04.360 --> 17:10.320
And so that's being dropped after the sample is run.

17:10.320 --> 17:15.600
And I also want to note that the timings here are the same as before we did any allocation

17:15.600 --> 17:17.480
profiling.

17:17.480 --> 17:24.440
I managed to optimize this to a point that its footprint is pretty indistinguishable front

17:24.440 --> 17:29.840
noise by using thread local storage and then optimizing that further, at least in the case

17:29.840 --> 17:33.640
of Mac OS.

17:33.640 --> 17:41.120
So if we look a little closer, we can see that, yeah, for smaller sizes, indeed, small

17:41.120 --> 17:46.400
back is not going to be performing any heap allocations and is strictly doing its operations

17:46.400 --> 17:51.760
with the stack.

17:51.760 --> 17:56.400
We can also tell Devon the number of bytes we're processing or number of items we're

17:56.400 --> 17:58.000
processing.

17:58.000 --> 18:02.440
And this allows us to get a pretty different perspective.

18:03.440 --> 18:06.640
The way we do this gets a little more complicated.

18:06.640 --> 18:12.360
We change our function to take a venture argument and then we call the counter method on that

18:12.360 --> 18:16.120
and we pass it an instance of bytes count.

18:16.120 --> 18:25.600
In this case, we're saying that we're counting n 32-bit integers and then we pass a closure

18:25.600 --> 18:30.200
to benchmark our function from iterator implementation.

18:33.200 --> 18:45.560
So, we then see that Devon will output the number of bytes being processed in terms of,

18:45.560 --> 18:49.120
in this case, megabytes or gigabytes per second.

18:49.120 --> 18:55.160
And for a lot of people, this might be an easier data point to get an intuition for

18:55.160 --> 18:59.280
the speed rather than just the strict timing numbers.

18:59.280 --> 19:06.040
For some people, saying growing numbers for better performance is just easier to think

19:06.040 --> 19:10.040
about.

19:10.040 --> 19:16.560
So to recap what I just covered, Devon has various features that really set it apart

19:16.560 --> 19:18.920
from existing solutions.

19:18.920 --> 19:24.240
I find that its API is just a lot simpler.

19:24.240 --> 19:28.280
It's easier to remember.

19:28.280 --> 19:37.480
I also really like how the compact output makes it pretty easy to consider various cases.

19:37.480 --> 19:45.680
And as well as because you can parameterize your benchmarks across various cases, you

19:45.680 --> 19:56.080
can really just get a sense for the difference in performance depending on the scenario.

19:56.080 --> 20:04.160
So I also really like that by going with an attribute macro, I realize that, oh, well,

20:04.160 --> 20:09.760
if you make the function generic, you can just pass the types in because you're just

20:09.760 --> 20:13.680
parsing whatever you want as the options.

20:13.680 --> 20:22.280
And so you can have benchmarks over various collections of the standard libraries, so

20:22.280 --> 20:26.680
linked list, VEC, hash set, et cetera.

20:26.680 --> 20:31.120
And you can see how different operations really differ between those collections.

20:31.120 --> 20:35.920
So such operations that are pretty common like clear might be a lot slower on a linked

20:35.920 --> 20:41.680
list whereas on a VEC, it's pretty instant.

20:41.680 --> 20:50.640
And another feature that helps me a lot is thinking of the numbers in terms of throughput.

20:50.640 --> 20:57.720
I find that it tends to just be easier to understand than durations.

20:57.720 --> 21:04.640
As well as something that I find no existing tool out there does is you can track the number

21:04.640 --> 21:11.480
of heap allocations at the same time that you're measuring the time being spent running

21:11.480 --> 21:14.480
your benchmark.

21:14.480 --> 21:19.520
As well as one feature I didn't mention here because I thought it might be a little complex

21:19.520 --> 21:25.200
to cover is you can do some interesting things like run benchmarks over multiple threads

21:25.200 --> 21:31.160
and this allows you to measure the effects of contention on atomics and locks.

21:31.160 --> 21:41.520
So if you're developing a low-level synchronization library, you might find this to be pretty useful.

21:41.520 --> 21:47.680
I also want to cover what motivated me to pursue this.

21:47.680 --> 21:54.840
I found that a lot of existing tools in space were pretty good but their APIs had some, in

21:54.840 --> 22:01.640
my opinion, unnecessary complexity and so I wanted an API that didn't go too far beyond

22:01.640 --> 22:06.680
the complexity of the code that you're benchmarking itself.

22:06.680 --> 22:14.720
And I really appreciated that by trying this new API, open up new possibilities such as

22:14.720 --> 22:20.240
what I mentioned before with benchmarking generic functions, it was relatively straightforward

22:20.240 --> 22:24.120
to implement that which was a bit of a surprise.

22:24.120 --> 22:30.240
So some food for thought if you're developing your own libraries.

22:30.240 --> 22:41.000
And I also found that the default way that some other tools run is pretty slow and I

22:41.080 --> 22:48.720
get why they're trying to do a lot of statistical analysis to remove outliers.

22:48.720 --> 22:57.360
But there are some cases where you do actually want to know when the code was especially

22:57.360 --> 23:03.040
slow because if you're benchmarking over various inputs, it's possible that one case

23:03.040 --> 23:05.840
just happened to create a really large string.

23:05.840 --> 23:12.720
And so you want to be able to get a sense for everything that happened, not just the

23:12.720 --> 23:17.800
best case scenarios in my opinion.

23:17.800 --> 23:23.560
And if you do want to run your benchmarks for longer, have larger sample sizes, more

23:23.560 --> 23:26.360
samples, there's also options to do that.

23:26.360 --> 23:30.720
So you're not restricted.

23:30.720 --> 23:37.520
I also want to mention some other Rust performance measuring tools that I think are very much

23:37.520 --> 23:39.240
worth considering.

23:39.240 --> 23:45.680
So criterion is obviously the current go to Rust benchmarking library.

23:45.680 --> 23:51.160
A feature that I really particularly like about it is its graph output because I'm a

23:51.160 --> 23:52.480
very visual person.

23:52.480 --> 23:56.920
I also do graphic design.

23:56.920 --> 24:01.040
Another newer micro benchmarking library is Tango.

24:01.040 --> 24:08.120
And what I find unique about it is that it has this technique called paired benchmarking

24:08.120 --> 24:13.920
where the execution gets interleaved between benchmarks.

24:13.920 --> 24:22.000
And what this does is it spreads whatever systemic negative conditions evenly across

24:22.000 --> 24:23.520
your benchmarks.

24:23.520 --> 24:28.920
And so this is certainly a feature I eventually want to have in Devon.

24:28.920 --> 24:37.600
Currently my APIs tries to prevent requiring ownership of the closure you're passing in.

24:37.600 --> 24:39.880
I might have to change that to make this work.

24:39.880 --> 24:40.880
I don't know.

24:40.880 --> 24:43.480
I think if we had co-routines, I could make it work.

24:43.480 --> 24:44.840
But I don't know.

24:44.840 --> 24:53.320
Maybe if someone knows how to abuse asynchol weight to get co-routines, please talk to me.

24:53.320 --> 24:56.640
Another very useful tool is flame graphs.

24:56.640 --> 25:00.360
This is more of a general technique that's well known across the industry.

25:00.360 --> 25:03.560
There's plenty of blog posts about this.

25:03.560 --> 25:09.240
But for those who aren't familiar, it's a visualization tool that really helps you find

25:09.240 --> 25:11.880
where to focus your time.

25:11.880 --> 25:18.200
And I think it's very important to find where the bottleneck in your code is before you

25:18.200 --> 25:26.480
actually start picking at specific places to optimize and do microbenchmarks on.

25:26.480 --> 25:35.920
So try to reach for profiling with flame graphs before you do microbenchmarking, if you can.

25:35.920 --> 25:40.040
As well as there's the DHAT crate.

25:40.040 --> 25:47.280
And like I mentioned before, every single time an allocation operation happens, it takes

25:47.280 --> 25:48.320
a back trace.

25:48.320 --> 25:54.800
And so it's able to give you pretty deep insights about where you're allocating memory and how

25:54.800 --> 25:56.240
you're allocating memory.

25:56.240 --> 26:03.920
It's also able to do some other stuff such as tracking max heap usage at a time.

26:03.920 --> 26:08.960
I'm going to try to add that to Devon, but unfortunately it adds a bit of overhead.

26:08.960 --> 26:13.200
So maybe it's possible to subtract that overhead from the timings.

26:13.200 --> 26:16.440
We'll see.

26:16.440 --> 26:24.800
And so some thoughts I want to leave you with is if you're going to be doing reaching for

26:24.800 --> 26:35.680
microbenchmarking tools like criterion, Devon, Tango, really figure out if it's worth microoptimizing

26:35.680 --> 26:44.280
that kind of code, just try to find the meteor performance issues in your program.

26:44.280 --> 26:48.800
So like I mentioned, flame graphs are particularly good for that.

26:48.800 --> 26:54.600
And also rather than having standalone benchmarks, you should be comparing it between different

26:54.600 --> 27:00.800
cases so you can measure across different inputs and implementations.

27:00.800 --> 27:09.640
So like I showed before, with Devon, you can benchmark origin error functions.

27:09.640 --> 27:17.800
And this also, for example, in the case of small vec versus vec, really gives you a better

27:17.800 --> 27:23.440
sense of is it really worth it to optimize your code using unsafe?

27:23.440 --> 27:29.680
And so try to find the specific scenarios where you actually are getting those wins

27:29.680 --> 27:36.720
because no one likes nasal demons.

27:36.800 --> 27:44.360
And also when making your own projects, since I imagine many people here are contributors

27:44.360 --> 27:51.360
to open source and have their own stuff that they're proud of, I really strongly advise

27:51.360 --> 27:55.520
that you don't let perfect be the enemy of good.

27:55.520 --> 28:00.520
Devon has a lot of features that criterion doesn't have, but also vice versa.

28:00.520 --> 28:06.040
Devon doesn't have graphs or machine readable output yet.

28:06.360 --> 28:11.160
I do eventually want to get there, but I didn't let that stop me from publishing something

28:11.160 --> 28:16.720
that I felt was good that people might want to use.

28:16.720 --> 28:24.160
And so try to focus on the features that matter to you most or at least are the most academically

28:24.160 --> 28:25.920
interesting.

28:25.920 --> 28:28.760
Not everything needs to be a full-fledged product.

28:28.760 --> 28:33.560
Definitely try to pursue your interests when making your own projects.

28:33.920 --> 28:37.920
Always remember that you can fill in the gaps later if you want.

28:40.360 --> 28:43.800
So that's all I had for this.

28:43.800 --> 28:53.320
I plan to have questions, but also in the meantime, you can read more about me.

28:53.320 --> 28:56.760
And currently I just have one blog post on there about Devon.

28:56.760 --> 29:03.520
I plan to publish another thing on kind of like std-conditional T in C++, but in

29:03.520 --> 29:10.520
Rust, which is as cursed as it sounds if you're familiar with std-conditional T.

29:10.520 --> 29:18.960
You can also follow me on mastodon or Twitter if I refuse to call it X.

29:18.960 --> 29:21.720
You can check out the code for Devon.

29:21.720 --> 29:23.560
Please give it a star.

29:23.560 --> 29:25.840
Play around with it.

29:25.840 --> 29:31.920
And yeah, if you want to reach out to me, I'm pretty accessible through mastodon.

29:31.920 --> 29:38.480
So there I'm hacky-derm at Nicolai.

29:38.480 --> 29:40.520
So yeah, any questions?

29:40.520 --> 29:48.200
We do have ten minutes for questions, so I'll plant you.

29:48.200 --> 29:49.200
Just raise your hand.

29:49.200 --> 29:50.200
I'm going to come to you.

29:56.440 --> 29:58.240
So Nicolai, thanks for your talk first.

29:58.240 --> 29:59.400
Very nice.

29:59.400 --> 30:01.520
And I have two questions.

30:01.520 --> 30:06.800
The first question is, have you thought about integrating into CI CD, so continuous integration

30:06.800 --> 30:07.800
things?

30:07.800 --> 30:11.920
That like, to me it seemed like this is a very handy tool with that, which I can use if I

30:11.920 --> 30:14.760
have a problem at hand, which I want to analyze.

30:14.760 --> 30:20.040
I quickly can do some benchmark and then dig deeper.

30:20.040 --> 30:24.760
But I think if I have found an issue in a very specific place, I might also want to

30:24.760 --> 30:29.880
have a test case out of it that I can monitor or be alarmed in my CI CD if there is an issue

30:29.880 --> 30:30.880
again.

30:31.240 --> 30:32.840
So that was the first question.

30:32.840 --> 30:37.880
And the second question would be, is it possible to run all those benchmarks in parallel, or

30:37.880 --> 30:44.840
do you have to sequentialize them in order to get the measurements right?

30:44.840 --> 30:46.560
Both great questions.

30:46.560 --> 30:53.640
So right now, what's blocking getting a lot of value out of running your benchmarks in

30:53.640 --> 30:57.040
CI is that Devon doesn't yet have programmatic output.

30:57.040 --> 31:03.640
My plans have JSON output and maybe some other format, if that makes sense.

31:03.640 --> 31:13.640
But yeah, as well as, so if you have programmatic output, then Devon can then consume previous

31:13.640 --> 31:20.360
iterations of that if you're benchmarking across different branches.

31:20.360 --> 31:32.640
As well as the author of Tango was exploring some ideas of using a shared library to compile

31:32.640 --> 31:42.320
it against four different branches and to make that pretty straightforward with get-of-actions.

31:42.320 --> 31:46.360
So yes, I'm definitely very interested in that.

31:47.360 --> 31:50.360
Sorry, repeat the second question.

31:52.360 --> 31:54.360
The second question was regarding the execution.

31:54.360 --> 32:00.360
If you are able to execute more than one benchmark in parallel, and whether there's some impact

32:00.360 --> 32:02.360
on the measurement itself.

32:03.360 --> 32:15.360
Yeah, so while technically you can, I find that putting the current process under more

32:15.360 --> 32:19.360
load could just negatively affect your timings.

32:19.360 --> 32:29.360
And so it didn't seem reasonable to do that, although I haven't actually measured if that

32:29.360 --> 32:32.360
would actually have as big of a negative impact as I would expect.

32:40.360 --> 32:41.360
Thank you.

32:41.360 --> 32:53.360
One question I had is, is there a way to compare the execution time with and without the warm

32:53.360 --> 32:54.360
cache?

32:54.360 --> 32:59.360
That is, the impact of cache on some data structures can be huge.

32:59.360 --> 33:05.360
And sometimes in benchmarking, in micro benchmarking especially, you have the problem that you're

33:05.360 --> 33:08.360
reusing the same lines.

33:08.360 --> 33:11.360
So the second benchmark is going to be faster always.

33:11.360 --> 33:19.360
But maybe your use case is actually the one in which the cache is called, for instance.

33:19.360 --> 33:22.360
Yeah, great question as well.

33:22.360 --> 33:32.360
So I considered having a helper effect function to evict the CPU caches, although I haven't

33:32.360 --> 33:37.360
thought of a good way of documenting when this is best to use.

33:38.360 --> 33:47.360
But in lieu of that, you can apply as a method on the Bencher type.

33:47.360 --> 33:52.360
You can pass a closure to generate inputs for every single iteration.

33:52.360 --> 33:59.360
And so if you wanted to, you could create a new buffer for every single time that function

33:59.360 --> 34:02.360
is being run at your benchmarking.

34:02.360 --> 34:11.360
So since that would be in a different place in memory, the cache effects wouldn't make

34:11.360 --> 34:19.360
the benchmark seem so much faster than it might be in a real world case.

34:19.360 --> 34:22.360
So we have a question from the matrix.

34:22.360 --> 34:23.360
So people are...

34:23.360 --> 34:25.360
Oh, Neo has a question.

34:25.360 --> 34:27.360
People are following online.

34:27.360 --> 34:30.360
So it's a really good topic.

34:30.360 --> 34:31.360
It was a very good talk.

34:31.360 --> 34:33.360
The question is, thanks.

34:33.360 --> 34:39.360
Devan looks very interesting and the API looks much cleaner, simpler than Criterions.

34:39.360 --> 34:43.360
Now Criterion can compare across different runs or implementations and then summarize

34:43.360 --> 34:46.360
whether performance improved or got worse.

34:46.360 --> 34:48.360
Within some confidence interval.

34:48.360 --> 34:52.360
Does Devan have something similar or plan to?

34:52.360 --> 34:55.360
Yeah, so it currently does not.

34:55.360 --> 35:03.360
I found that I kind of shoehorned myself a bit with this output format in that it's

35:03.360 --> 35:08.360
not super easy to add a lot more information to that.

35:08.360 --> 35:14.360
And so it's kind of like has become a bit of a UI problem in a way, which I find interesting

35:14.360 --> 35:18.360
given that's a command line.

35:18.360 --> 35:28.360
But yeah, I would very much like to just directly tell the user that this is faster

35:28.360 --> 35:31.360
or slower than previous runs.

35:31.360 --> 35:40.360
There's also the issue that, for example, if I have my laptop plugged in, now my benchmark

35:40.360 --> 35:41.360
runs faster.

35:41.360 --> 35:44.360
If I don't have it plugged in, then it's slower.

35:44.360 --> 35:45.360
It gets throttled.

35:45.360 --> 35:54.360
So it's not always obvious that there was a change in the implementation that caused

35:54.360 --> 35:59.360
the function to get slower.

35:59.360 --> 36:04.360
And I believe that Criterion's documentation has like a big warning section about this

36:04.360 --> 36:06.360
issue.

36:06.360 --> 36:11.360
But yeah, I do think that is valuable to have and I would like to have it.

36:12.360 --> 36:22.360
And also, if you all are actually very interested in this project, feel free to submit ideas

36:22.360 --> 36:28.360
to the GitHub page for it or better pull requests, implement the features that you'd like to

36:28.360 --> 36:29.360
see.

36:29.360 --> 36:33.360
I'm only one person and only have so many hours in a day.

36:41.360 --> 36:52.360
Yeah, I have two questions.

36:52.360 --> 37:00.360
The first one was you mentioned that some of the flaws or design differences with Criterion

37:00.360 --> 37:08.360
was that it focused a lot on very, I don't know, horrible.

37:08.360 --> 37:20.360
It's a lot in statistics and instead of just giving you the fastest, the average and all

37:20.360 --> 37:21.360
of that.

37:21.360 --> 37:32.360
I was wondering if there is a mechanism in Devon to output, like for example, percentiles

37:32.360 --> 37:34.360
or something like that.

37:34.360 --> 37:45.360
And my second question was when your benchmarking memory, if the function you're benchmarking

37:45.360 --> 37:50.360
the allocates instead of returning all the memory that it allocated, would the output

37:50.360 --> 37:57.360
show the total memory allocated or just the memory remaining when the function returned?

37:57.360 --> 38:10.360
Yeah, so any allocations that happen before the benchmark runs or not, they will or they

38:10.360 --> 38:14.360
will not be visible to Devon in a sense.

38:14.360 --> 38:19.360
It will have recorded that but it won't be associated with the current benchmark.

38:19.360 --> 38:24.360
It will just get cleared before the benchmark runs.

38:24.360 --> 38:30.360
So in that case, you would see that the number of the allocations would be greater than the

38:30.360 --> 38:33.360
number of allocations in the benchmark.

38:33.360 --> 38:39.360
And to answer your first question, when you say percentiles, are you talking about confidence

38:39.360 --> 38:40.360
intervals?

38:40.360 --> 38:48.360
Well, no, I mean, it would be also an option but the first thing that came to mind to me

38:48.360 --> 38:49.360
was percentiles.

38:49.360 --> 39:00.360
Like when you order the outputs, like the timings in the sending order, which for example, I

39:00.360 --> 39:08.360
don't know how to describe it right now, but yes, which was the 95th.

39:08.360 --> 39:15.360
If you did 100 runs, which was the 95th fastest or slowest, for example.

39:15.360 --> 39:18.360
Yeah.

39:18.360 --> 39:30.360
So I would like to have more interesting statistics output.

39:30.360 --> 39:37.360
Right now, I was just focused on having what I considered was the most important for your

39:37.360 --> 39:38.360
average benchmarks.

39:38.360 --> 39:50.360
Like I'd also like to output what the variance is across all the samples.

39:50.360 --> 40:01.360
So again, I kind of painted myself into a bit of a corner in that people usually only have

40:01.360 --> 40:03.360
so many columns in their terminal.

40:03.360 --> 40:12.360
And so this table output will be interesting to see how I add to it.

40:12.360 --> 40:19.360
I think what I'll end up doing is have options for emitting the columns that you're interested

40:19.360 --> 40:23.360
in having and just have certain columns by default.

40:23.360 --> 40:29.360
So when I do end up getting around to having more interesting statistics, that'll probably

40:29.360 --> 40:41.360
lead the way to make a user configurable of whether you end up with a giant table or not.

40:41.360 --> 40:42.360
Okay.

40:42.360 --> 40:44.360
Thank you very much for your talk and your answers.

40:44.360 --> 40:45.360
Thank you.

