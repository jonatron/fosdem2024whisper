WEBVTT

00:00.000 --> 00:10.000
When you didn't know what was going on, I'm looking up at the faces that are blank.

00:10.000 --> 00:12.000
Okay.

00:12.000 --> 00:14.000
So AI.

00:14.000 --> 00:23.000
At one stage, AI was a preserve of data scientists, machine learning folks, and people in universities.

00:23.000 --> 00:28.000
And us developers said, let them have their day.

00:28.000 --> 00:30.000
Okay.

00:30.000 --> 00:37.000
But what I want to look at AI, and I know chart CBT, you know, at the end of November 2022,

00:37.000 --> 00:43.000
throughout and everybody from your parents to your aunts to your uncles,

00:43.000 --> 00:47.000
suddenly know what AI is.

00:47.000 --> 00:54.000
There's kids around the world writing papers and some of the information is right and some of it is wrong.

00:54.000 --> 00:59.000
So there's a lot of teachers out there that are fed up, especially in secondary school or high school,

00:59.000 --> 01:05.000
that are fed up with getting answers from models and from AI.

01:05.000 --> 01:09.000
But on a serious note, what I want to look at today is from the angle of,

01:09.000 --> 01:13.000
I've been a developer for nearly the last 30 years,

01:13.000 --> 01:22.000
and my background is about writing services, applications, mostly back in middle there and earlier in my career in UI.

01:22.000 --> 01:29.000
And because of the growth of AI and because of what chart CBT does,

01:29.000 --> 01:39.000
an awful lot of our leaders in our companies now want us to leverage and consume models in our applications.

01:39.000 --> 01:41.000
So I just want to get a little demographic here.

01:41.000 --> 01:45.000
Hands up, who's the data scientist?

01:45.000 --> 01:47.000
Okay, there's a few of you.

01:47.000 --> 01:49.000
You might be leaving in a couple of minutes.

01:49.000 --> 01:52.000
Any machine learning engineers?

01:52.000 --> 01:54.000
Some more.

01:54.000 --> 01:56.000
Okay, I'm getting a bit nervous now.

01:56.000 --> 01:58.000
Any developers?

01:58.000 --> 02:01.000
Yes!

02:01.000 --> 02:02.000
Sorry for the roar.

02:02.000 --> 02:03.000
I'm safe.

02:03.000 --> 02:06.000
I won't get lynched from here out to the door.

02:06.000 --> 02:09.000
So let's get this show going.

02:09.000 --> 02:12.000
As JJ said, my name is Barton Hickey.

02:12.000 --> 02:15.000
I'm a software engineer working over at IBM.

02:15.000 --> 02:21.000
And I spent around the last eight to ten years in the cloud native space in different communities.

02:21.000 --> 02:31.000
But my key role, I've been very lucky, is contributing to open source communities and trying to drive open source communities forward.

02:31.000 --> 02:33.000
So a little background.

02:33.000 --> 02:36.000
I'm going to do a very small background in AI.

02:36.000 --> 02:38.000
AI folks don't shoot me down.

02:38.000 --> 02:39.000
It's my interpretation.

02:39.000 --> 02:42.000
I'm only dipping my toe in the water over the last couple of months.

02:42.000 --> 02:44.000
So this is how I see it.

02:44.000 --> 02:51.000
But really I want to get into the frameworks and open source in general and how can we use the models using these frameworks?

02:51.000 --> 02:55.000
And then we might have a bit of a demo if I have enough time.

02:55.000 --> 02:58.000
So I'm just going to throw up this definition.

02:58.000 --> 03:02.000
That kind of two sentences on it.

03:02.000 --> 03:11.000
And from what I've looked at and from what I understand, the way I look at the model is I look at it like any other program.

03:11.000 --> 03:13.000
Or library.

03:13.000 --> 03:17.000
Or whatever else that we've often called to use.

03:17.000 --> 03:20.000
So it has an API generally.

03:20.000 --> 03:22.000
We can call it and we can get a result.

03:22.000 --> 03:26.000
We can consume it in our application.

03:26.000 --> 03:37.000
But I suppose the difference here with, for me as a traditional programmer, our traditional programming is, you know, we give it a set of rules and instructions to tell the computer what to do.

03:37.000 --> 03:44.000
And more often than not, we should know how that program is going to work.

03:44.000 --> 03:55.000
With AI models, it's a little bit different here because it's not exactly explicitly programmed for predictor tasks to do or a particular prediction.

03:55.000 --> 03:57.000
It learns with the data.

03:57.000 --> 04:02.000
So there's the big difference from restarting out.

04:02.000 --> 04:06.000
So what is the journey of a model?

04:06.000 --> 04:10.000
So first of all, I suppose it's the building or the prototyping of that model.

04:10.000 --> 04:13.000
And data is the key here.

04:13.000 --> 04:20.000
So as a lot of my North American colleagues would say, garbage in, garbage out.

04:20.000 --> 04:27.000
So the data is really, really important when you're training or creating your model.

04:27.000 --> 04:37.000
And the first part of that is getting that data from reputable sources, reputable domains for the particular operations you want the model to do.

04:37.000 --> 04:49.000
Then it's the preparation of the data because the algorithms that process this data or create the model, they need the data of a certain quality and in certain formats.

04:49.000 --> 04:53.000
So things like errors and omissions need to go out of it.

04:53.000 --> 05:06.000
If there's any duplication, any missing values, and also then maybe converting into a format that the algorithm can use.

05:06.000 --> 05:13.000
There might be some aggregation as well, so that you know what I mean, to normalize values, etc.

05:13.000 --> 05:15.000
Then you choose the algorithm.

05:15.000 --> 05:20.000
The algorithm depends on the operation you want to do.

05:20.000 --> 05:28.000
And it also depends on how you're going to train it, how it's going to function, and then what resources you've available to do it.

05:28.000 --> 05:35.000
So you can see when we started out with the generative models, they were huge, and I'll talk about them in a minute.

05:35.000 --> 05:42.000
But now there seems to be a turn at the start of this year of smaller models and more modular models, etc.

05:42.000 --> 05:46.000
Because of the resources to train them and also to run them.

05:46.000 --> 05:59.000
The key part of the algorithm is to turn that data into a model and produce the model you have that's trained on certain data, which is the next step.

05:59.000 --> 06:13.000
The training data set is very important because you want to see within a certain level of tolerance how exacting is that model.

06:13.000 --> 06:18.000
So what are the results? Are they within a particular tolerance rate that you are going to be willing to accept?

06:18.000 --> 06:27.000
And then at this stage then you say, right, I have a model here, I've trained, what I need to do is the final step, which is validation.

06:27.000 --> 06:44.000
So then, testing the model against data that you didn't use when you were training to see if the model can actually learn when new data comes into it, and our as its predictions or its tasks as you expect them to be.

06:45.000 --> 06:56.000
So a big part of this I think going forward is open source could be the key with models because it's going to come down to trust.

06:56.000 --> 07:00.000
Do we trust the model? Because let's be fair about it.

07:00.000 --> 07:09.000
Even, you know, the people who write the models or the graded data scientists out there are machine learning engineers.

07:09.000 --> 07:14.000
Sometimes they can't even predict what the answers are.

07:14.000 --> 07:21.000
So the key here for transparency and in the open and trust of models is going to be key.

07:21.000 --> 07:24.000
And the last two parts are the parts I like.

07:24.000 --> 07:28.000
Running the model and calling the model, okay?

07:28.000 --> 07:38.000
So, you know, sometimes, you know, back in the day when you used, you know, binary tree library, you just wanted it to give you the answer.

07:38.000 --> 07:43.000
You didn't want to be writing a vanilla binary tree unless you were into that.

07:43.000 --> 07:50.000
So the same with the model. You want the model to do an operation for you, perform a task or make a prediction.

07:50.000 --> 07:55.000
So that's what they call inferencing.

07:55.000 --> 08:00.000
A little bit around genera.vii because that's the buzzword. Everyone's talking about it.

08:00.000 --> 08:05.000
And the key here is that it's a different type of model.

08:05.000 --> 08:12.000
So your tradition in machine learning models were trained on label data.

08:12.000 --> 08:16.000
So data that was specific for specific tasks.

08:16.000 --> 08:24.000
A lot went into knowledge of the domain and knowledge that the data scientists need to know around that particular area.

08:24.000 --> 08:26.000
And it was very intensive in the training.

08:26.000 --> 08:35.000
In this situation with the foundation models, what we're saying here is we're going to train it on a massive data setup on label data.

08:35.000 --> 08:40.000
And then you can use that model and fine tune it.

08:40.000 --> 08:44.000
Or so you can do different tuning like fine tuning where you take the model.

08:44.000 --> 08:52.000
And because these models are deep learning models, there's going to have a lot of layers in them.

08:52.000 --> 08:58.000
You may take one to number of layers off, put your own layers on top, i.e. you're going to train it against your own data.

08:58.000 --> 09:11.000
Or you might use prompt tuning where when you're calling the model, you're going to pass it prompts like examples, what you're looking for, and guide it towards the answer you're looking for.

09:11.000 --> 09:21.000
An example of this is around the large language models which are based on huge language data sets and can generate content from there.

09:21.000 --> 09:33.000
And we can see that in which generative AI. It's about using these models or these generic style models to be able to generate high quality text, video, etc.

09:33.000 --> 09:37.000
So that's the whole idea of the generative where we're going now.

09:37.000 --> 09:45.000
And the idea here that these models, one model can be used for different operations as opposed to be normally trained for one.

09:45.000 --> 09:55.000
Okay, so that's my intro to the AI. And the next part is what I really want to get onto is around the frameworks.

09:55.000 --> 10:04.000
So hands up who's heard of Hoganface. Okay, that's not a bad number.

10:04.000 --> 10:12.000
So Hoganface has built up an AI community which is nice to see based around open source.

10:12.000 --> 10:21.000
It's key things being it is a series of libraries. It has a huge catalog of open source models and data sets.

10:21.000 --> 10:29.000
Now, how is this appealing to you if you want to use open source models or sorry, you want to use AI and models.

10:29.000 --> 10:36.000
The great thing about this is you can pull those models down and you can run and use them locally.

10:36.000 --> 10:44.000
So that if something changes tomorrow morning with Hoganface, you can still use those models. You can use them locally.

10:44.000 --> 10:51.000
Now, they also provide a service where they host the models themselves. So you're directly just calling the models through their API.

10:51.000 --> 10:57.000
But I think it depends on what your setup is and your use cases, etc.

10:58.000 --> 11:02.000
So what I'd like to do with this when I'm looking through this is an example.

11:02.000 --> 11:12.000
And this is an example using the Hoganface Transformer library. And it was an example up on the Ray framework, which I'm going to touch on straight after this.

11:12.000 --> 11:18.000
So the first thing is we're using a model called the T5, which is an encoder-decoder model.

11:18.000 --> 11:25.000
And these types of models are really geared for natural language processing, or NLP as they call it.

11:25.000 --> 11:32.000
So when you're dealing with text of some sort in different languages, etc., it's usually text in, text out.

11:34.000 --> 11:38.000
And the task we want to do here is just a very simple example.

11:38.000 --> 11:45.000
Looking at the code is that what we want to do is just convert a bit of text to French.

11:45.000 --> 11:52.000
And I use French because we're here in Belgium. I couldn't get Flemish or I didn't think of it, so sorry about that.

11:53.000 --> 12:00.000
So when we look at this, we see it's two real calls. The first one is we use the Transformer library, and we call Pipeline.

12:00.000 --> 12:05.000
And in Pipeline, you specify the model and the task you want to do.

12:05.000 --> 12:14.000
And then inside the translate method, we're then going to call the model with the text that we send in.

12:14.000 --> 12:24.000
Very simple Python class. When you run the class, if it's your first time calling that model, it will call out to Hug & Face,

12:24.000 --> 12:29.000
Open Source Cal log, it will pull down the T5 model, and it will put it in cache.

12:29.000 --> 12:35.000
So any call after that for that particular version of the T5, it will get it locally.

12:35.000 --> 12:49.000
Ray. So Ray is a framework for scaling and distributing Python and machine learning applications.

12:49.000 --> 12:56.000
So the capability is it's providing which is batch inferencing on CPUs, GPUs.

12:56.000 --> 13:00.000
So the inferencing being running the models, which we talked about earlier.

13:00.000 --> 13:09.000
But the stuff we're interested in mostly. Obviously the serving of the models, hosting them up, having them ready to go, providing an API in front of them.

13:09.000 --> 13:15.000
Also training of models, large language models. So that's if you have your large language model.

13:15.000 --> 13:22.000
It's not, I suppose it's not giving you results you want for the particular operations you want.

13:22.000 --> 13:25.000
Then you may decide to train that with your own data.

13:25.000 --> 13:29.000
So in that case, you'll create a new version of the model.

13:29.000 --> 13:35.000
And then other operations like reinforced learning, etc.

13:35.000 --> 13:44.000
So if we want to use Ray to now host the model, in this scenario, what's going to happen is Ray has a nice little HTTP server.

13:44.000 --> 13:49.000
And pretty much all you have to do is put an annotation on top.

13:49.000 --> 13:54.000
You can see just below the translator class called serve deployment.

13:54.000 --> 13:58.000
And you can pass args in there if you want.

13:58.000 --> 14:07.000
And the next part that's important is the callback function, which is called underscore underscore call underscore underscore.

14:07.000 --> 14:18.000
So in this situation, once you you have your call, you have your annotation, you can then call this class using.

14:19.000 --> 14:30.000
Using server on. So it then will load it in a HTTP server and it'll provide the interface to it and you can call it as follows.

14:35.000 --> 14:41.000
The next and the final framework I want to look at is the Triton inference server.

14:41.000 --> 14:53.000
And the inference server provides you with support.

14:53.000 --> 15:02.000
For most machine learning applications and frameworks, as well as custom C++ and Python backends.

15:02.000 --> 15:07.000
So you can see the different frameworks that supports there and processes support, etc.

15:07.000 --> 15:15.000
And this time, if you want to wrap it or use it, you need to actually call the class.

15:15.000 --> 15:17.000
I'm going to read it from up here.

15:17.000 --> 15:21.000
It should be either Triton, Python model, which is a bit of a mouthful.

15:21.000 --> 15:29.000
And then you need an execute method. And that's where the HTTP request we're calling to just note here.

15:29.000 --> 15:34.000
It's not using JSON data types. It's using tensor data types.

15:34.000 --> 15:39.000
Okay, so I don't know if you've experienced of using tensors, etc.

15:39.000 --> 15:42.000
PyTorch or anything like that, you'll be used to that.

15:42.000 --> 15:47.000
So you have to convert it in into into Python data types so you can process it.

15:49.000 --> 15:54.000
The other thing you need to do to bootstrap it is you're going to need a configuration file and that config file.

15:54.000 --> 15:58.000
You give it the name of your particular model.

15:58.000 --> 16:00.000
So I'll call it the model you want to host.

16:00.000 --> 16:03.000
So it's going to be the local name that's going to host.

16:03.000 --> 16:07.000
And then you tell it what back end, in this case is Python, and your input output.

16:07.000 --> 16:12.000
And those input outputs are binary tensor flow types.

16:12.000 --> 16:17.000
And the last thing you do then is you need this model directory down here.

16:17.000 --> 16:26.000
So in a minute when we call models, it must be under this with the name of the wrapper model class and your config and your model file.

16:27.000 --> 16:32.000
And finally, to call it then, there's a number of ways you can do it.

16:32.000 --> 16:37.000
One easy way is to use Docker and run its container.

16:37.000 --> 16:46.000
And when you run that then, you then need to copy the artifacts, which is the model directory that we set up into the container.

16:46.000 --> 16:55.000
And then you call the Triton server executable to run it as a HTTP server.

16:55.000 --> 17:04.000
And you can call it in the call request above with that path in the name of your model and in fare.

17:07.000 --> 17:17.000
So before I go on to the demo, I just want to just do a little summary of that.

17:17.000 --> 17:24.000
So I'm after choosing three frameworks and you're probably someone's probably up the back on each and go on.

17:24.000 --> 17:27.000
There's loads more. Yes, there is.

17:27.000 --> 17:33.000
You VLM, you know, which is a nice alternative note to Hogan phase teachers.

17:33.000 --> 17:40.000
And there's just so many of them because this space is just growing phenomenally.

17:40.000 --> 17:45.000
We probably saw it with cloud native over 10 years ago.

17:45.000 --> 17:50.000
And you can see it at the moment that there's more and more frameworks.

17:50.000 --> 17:59.000
And I think the key here will be if we can have these frameworks be open source as well with the models.

17:59.000 --> 18:05.000
And then if companies want to put their value on top, then they can put their sauce on top.

18:05.000 --> 18:13.000
But for most of us here, we would like to be able to choose our framework and see if it'll do what we needed to do.

18:13.000 --> 18:15.000
So load our models and stuff like that.

18:15.000 --> 18:20.000
The other one I'm shown here, I'm just obviously running locally and I'll be running something locally here in a minute.

18:20.000 --> 18:29.000
But the idea here is that you'll deploy this into some system bare metal, a cloud system, whatever, because depending on the model,

18:29.000 --> 18:33.000
it's going to need serious resources to run a lot of the time.

18:33.000 --> 18:43.000
But as I said, there seems to be a shift towards the smaller models and then the plugability in these models where you can have different capabilities in the models.

18:43.000 --> 18:50.000
But that's something for me to learn down the line and for the data scientists to come and tell us about.

18:50.000 --> 18:52.000
Okay, so let's do a quick download.

18:54.000 --> 18:56.000
Yeah, I'll just escape here one second.

18:58.000 --> 18:59.000
You want?

18:59.000 --> 19:01.000
Yeah, if you don't mind.

19:01.000 --> 19:02.000
Thanks very much.

19:08.000 --> 19:10.000
Just before I show this.

19:11.000 --> 19:24.000
So what I'm doing is, I'm just running a framework here called KK.

19:24.000 --> 19:33.000
And why I'm running this is a colleague of mine, Mark Stewart, did a really nice UI using radio, the radio framework, if you know it,

19:33.000 --> 19:40.000
a Python framework for it's very handy for for doing UI, active access stuff and elements and things like that.

19:40.000 --> 19:42.000
And I'm just showing you here that I'm running it.

19:42.000 --> 19:48.000
And what it's running is it's running the AI, sorry, it's running the UI as a HPP server.

19:48.000 --> 19:54.000
And it's also running the back end server with a gRPC and HPP interface.

19:54.000 --> 19:57.000
And the back end server is the host.

19:57.000 --> 20:07.000
So where it's hosting the model and it's and the way to run those models are similar to the other frameworks I've shown by wrapping the model for the particular artifacts that's needed.

20:16.000 --> 20:18.000
Okay, so here's the simple UI.

20:19.000 --> 20:26.000
And what I want to do is just play with models for a minute and then just show, look, it's just code wrapped in the back end.

20:26.000 --> 20:36.000
And as developers, we can just use these models to perform operations that, you know, we might take us ages to write or would be quite difficult to write.

20:36.000 --> 20:38.000
And we can use these off the shelf models, hopefully.

20:38.000 --> 20:41.000
So the first one is putting in a sentence like.

20:49.000 --> 20:53.000
I can.

20:53.000 --> 20:54.000
How's that?

20:54.000 --> 20:55.000
Better?

20:55.000 --> 20:56.000
More.

20:56.000 --> 21:01.000
This is where you were told in class to come down to the front.

21:01.000 --> 21:03.000
Did you hear that?

21:03.000 --> 21:04.000
That's better.

21:04.000 --> 21:05.000
Thank you.

21:06.000 --> 21:07.000
Thank you.

21:07.000 --> 21:14.000
So what I want to do here is I'm using a model that does sentence similarity and I put in the canine is fast is my source.

21:14.000 --> 21:17.000
And then I'm saying the dog is running the cat is asleep.

21:17.000 --> 21:21.000
And if you know your cat from your dogs, a dog is the canine.

21:21.000 --> 21:22.000
A cat is not.

21:22.000 --> 21:25.000
So there we go.

21:25.000 --> 21:27.000
It's telling us 68%.

21:27.000 --> 21:29.000
It thinks the dog is asleep.

21:29.000 --> 21:31.000
So there we go.

21:31.000 --> 21:33.000
It's telling us 68%.

21:33.000 --> 21:37.000
It thinks the dog is running is the best sentence.

21:37.000 --> 21:40.000
But if I change it to say the cat.

21:40.000 --> 21:42.000
Is running.

21:48.000 --> 21:52.000
We now put the cat among the pigeons, so to speak, excuse the pun.

21:52.000 --> 21:58.000
But now we're saying it's a 36% chance that, you know, you're not going to be able to get the cat.

21:59.000 --> 22:02.000
It might be like the canine is running.

22:02.000 --> 22:05.000
So you can see here how you compare on with things.

22:05.000 --> 22:14.000
And because of vector spaces and so forth, where words are aligned, that if you do change some of the words like running, you know, fast, they're going to be near each other.

22:14.000 --> 22:17.000
Dogs and cats are going to be near each other because they're animals.

22:17.000 --> 22:19.000
So yeah, that's what happens there.

22:19.000 --> 22:24.000
So looking again, let's do image classification.

22:24.000 --> 22:27.000
I'm going to choose an image here.

22:29.000 --> 22:31.000
All right.

22:32.000 --> 22:34.000
Yay, puppies.

22:34.000 --> 22:35.000
Okay.

22:35.000 --> 22:43.000
Now, I love dogs, but it's telling me it's a 88% chance that's a golden retriever.

22:43.000 --> 22:46.000
Any person can confirm that?

22:46.000 --> 22:48.000
Okay.

22:48.000 --> 22:49.000
Thank you very much.

22:49.000 --> 22:53.000
I just thought it was a Labrador, but sorry about that.

22:53.000 --> 23:04.000
And the last one I'm going to do is something close to my own heart, which is I've taken an image of a sport we have in Ireland called Hurlin.

23:04.000 --> 23:05.000
Okay.

23:05.000 --> 23:08.000
So Hurlin is players are on the field.

23:08.000 --> 23:09.000
They have a ball.

23:09.000 --> 23:11.000
They have a stick.

23:11.000 --> 23:12.000
Okay.

23:12.000 --> 23:17.000
So when it does the detection here, it detects a sports ball.

23:17.000 --> 23:18.000
That's fantastic.

23:18.000 --> 23:21.000
It detects people, which is fantastic.

23:21.000 --> 23:26.000
But it calls the stick a baseball bat.

23:26.000 --> 23:30.000
Is there anyone in the audience that knows what the name of the stick is?

23:30.000 --> 23:34.000
A hurly.

23:34.000 --> 23:35.000
Yay.

23:35.000 --> 23:37.000
I love to hear.

23:37.000 --> 23:38.000
Okay.

23:38.000 --> 23:43.000
So that's an example of the data the model has been trained with.

23:43.000 --> 23:53.000
What you'd need to do there is either to prompt tune that model or fine tune that model with a data set that tells you more about the game of hurlin.

23:53.000 --> 24:03.000
It's able to detect the field sport, which is which is amazing people and ball, considering it hasn't been trained anything on the sport, but then it lets itself down a little bit.

24:03.000 --> 24:08.000
So why did I show you these things bar showing a really nice UI for my colleague?

24:08.000 --> 24:10.000
The reason is this.

24:11.000 --> 24:18.000
At the end of the day, we're calling libraries like we've always done or we're calling API is like we always done.

24:18.000 --> 24:24.000
And as long as we know the models can be trusted, then that's okay.

24:24.000 --> 24:25.000
All right.

24:25.000 --> 24:29.000
So the first part in here is this is just a great UI code.

24:29.000 --> 24:36.000
And in here, we're just saying there's a series of different classes for the for the different UI tabs.

24:36.000 --> 24:38.000
And we're just calling them here.

24:38.000 --> 24:44.000
And here's an example of one of the UI tabs down here for the image classification.

24:44.000 --> 24:48.000
You can see here the submit button and the other elements that are on the tab.

24:48.000 --> 24:55.000
And in this situation, you can see the submit is going to call into a function called function for that's a great name, by the way.

24:55.000 --> 24:56.000
FN.

24:56.000 --> 24:57.000
I haven't seen that for a long time.

24:57.000 --> 25:02.000
And you can see here in here, then this is where it's going to call the model.

25:02.000 --> 25:03.000
Okay.

25:03.000 --> 25:19.000
Now, why is this a little bit funny because it's a g is the g rpc API is calling and you can see here inside the UI when it starts up, it's going to get it's going to get a handle to the channel of the g rpc server.

25:19.000 --> 25:28.000
And then over on this side, you see here the different tabs classes to handle the UI interaction.

25:28.000 --> 25:32.000
And then finally down here is the wrapper code.

25:32.000 --> 25:37.000
And you can see here it's got similarities to some of the frameworks I showed a while ago.

25:37.000 --> 25:43.000
But in this situation, you use an annotation up here called module, you pass it a task and so forth.

25:43.000 --> 25:52.000
And the key here then is you can see in the init again, we're calling a pipeline, which is the hug and face transformer pipeline up here.

25:52.000 --> 25:54.000
Sorry, I've just jumped.

25:54.000 --> 25:56.000
Yeah, just up here.

25:56.000 --> 26:04.000
If you don't pass a model to it, the default model is going to use is the Google VIT model.

26:04.000 --> 26:06.000
And then down here is the key in the run method.

26:06.000 --> 26:18.000
So when you call the predict, it's going to call the HTTP server and it's going to be redirected in the server to the run method here of this module that's loaded in the server.

26:18.000 --> 26:21.000
And it's going to call the pipe here.

26:21.000 --> 26:27.000
And when it calls the setup pipe and passes the image like it did the last time, then it's going to get the result back.

26:27.000 --> 26:28.000
And that's how you get it.

26:28.000 --> 26:33.000
Okay, so let's jump back to our deck.

26:49.000 --> 26:53.000
All right, to wrap this up, anything's on the screen there.

26:53.000 --> 26:55.000
I'm actually going to talk about that for some reason.

26:55.000 --> 26:57.000
So you can read that if you want.

26:57.000 --> 27:00.000
No.

27:00.000 --> 27:02.000
Why did I do this talk today?

27:02.000 --> 27:08.000
I want to do this talk from the angle of somebody who's been writing code a long, long time.

27:08.000 --> 27:16.000
And I'm afraid that someday someone's going to say you're too old and bald and you've got glasses.

27:16.000 --> 27:18.000
And I'd...

27:18.000 --> 27:20.000
And a beard.

27:20.000 --> 27:21.000
Sorry about that.

27:21.000 --> 27:26.000
No, but when we look at the change, it's probably the biggest change.

27:26.000 --> 27:33.000
If you've been in the industry for maybe 20, 30 years, it's probably the biggest change we've seen the way things are going.

27:33.000 --> 27:40.000
And there's always that phrase, you know, you either adapt or you stand still and everything moves on.

27:40.000 --> 27:48.000
So the ability here to be able to use these models to write applications and improve things is really the key.

27:48.000 --> 27:56.000
And this is something that I suppose to grow out of AI definitely in the last 10 years or plus.

27:56.000 --> 28:07.000
I suppose starting with, or maybe 20 years starting with the Linux Foundation, then into OpenStack, Kubernetes, you know, all the other different great foundations and communities out there.

28:07.000 --> 28:19.000
I think a lot of companies have realized one company or one set of developers cannot keep up with the rate of development, the rate of change, the way technology is going.

28:19.000 --> 28:30.000
And having this ability to get these models eventually out of the grasp of the scientists stuck in the labs that don't want to release anything.

28:30.000 --> 28:33.000
Sorry data scientists, I'm not picking on you.

28:33.000 --> 28:45.000
But you know, to eventually see the light of day and they can go home and tell their families, you know, that thing I was working on for 20 years, they're now using it in fridges, in cars, everywhere.

28:45.000 --> 28:48.000
But the key here is that we need to have it open.

28:48.000 --> 28:52.000
We need to drive forward with our models and be transparent.

28:52.000 --> 29:00.000
We need to have trust in the models because we're asking the model to do something for us or give us a result that we depend on.

29:00.000 --> 29:08.000
And like libraries we use in the old days, you needed to go and do the groundwork and find out can we trust that library?

29:08.000 --> 29:13.000
Is it doing something we don't want it to do? Is it doing something malicious?

29:13.000 --> 29:23.000
So I think there's a big change here from the initial AI maybe in the last 10 years where it was around fraud detection, spam detection, you know, chatbots.

29:23.000 --> 29:27.000
I'd say there's going to be a big proliferation offered the next 5 to 10 years.

29:27.000 --> 29:29.000
So thank you very much.

29:30.000 --> 29:32.000
Questions?

