WEBVTT

00:00.000 --> 00:15.100
Hello everyone, welcome. My name is Eric Dereke. I am the author of Greenfield. Previous

00:15.100 --> 00:20.560
talk we saw how Chromium can run on Wayland. Now we're going to see how Wayland can run

00:20.560 --> 00:32.080
in Chromium. So this is an update about Greenfield. I gave a previous talk in 2019 about Greenfield.

00:32.080 --> 00:39.040
So basically recap what is Greenfield? Greenfield is a Wayland compositor. I'm sure most of

00:39.040 --> 00:46.240
you already know what a compositor is. That runs entirely in your browser. You can control

00:46.240 --> 00:53.600
remote applications with it from different servers if you want. X applications as well,

00:53.600 --> 01:01.200
X Wayland applications. For that, that's also a fun fact. I actually had to port XCB library

01:01.200 --> 01:09.560
to JavaScript, to TypeScript. So I have a Python parse X protocol files and have it output

01:09.560 --> 01:15.560
JavaScript and then you can run it in your browser. Works perfectly by the way. You can

01:15.560 --> 01:23.240
also run Wayland applications directly in your browser. So actual native desktop applications

01:23.240 --> 01:30.680
compiled to WebAssembly and have them talk Wayland protocol to Greenfield inside your browser.

01:30.680 --> 01:36.720
We're going to see some demos at the end of the talk as well. It doesn't have to be WebAssembly

01:36.720 --> 01:42.000
applications. By the way, you can just write JavaScript applications. They just have to talk

01:42.000 --> 01:50.520
Wayland to have their pixels displayed on the screen and to handle input. So that's basically

01:50.520 --> 01:58.480
Greenfield in a nutshell. Why? For God's sake, why would you write this? Well, because I can,

01:58.480 --> 02:05.080
that's basically the crutch of it. No. I had basically an idea. Wouldn't it be cool if we had

02:05.120 --> 02:12.800
a desktop that you could run anywhere? So not tied to a single physical piece of hardware you had

02:12.800 --> 02:21.480
with a single interface for all your applications. So no longer bound to say, yeah, a smartphone or

02:21.480 --> 02:30.000
a desktop or whatever. And yeah, so, and be free basically to run it where you want. So you're not

02:30.320 --> 02:36.720
bound to any SaaS provider or anything. Just have your own server or servers or whatever and have

02:36.720 --> 02:43.840
the application distributed and basically access them anywhere you want, anytime you want without

02:43.840 --> 02:50.360
restrictions purely for you. I thought that would be pretty cool. And it didn't exist. So a set

02:50.440 --> 02:59.560
hell, I'll write it myself. Well, how is Wayland written? Well, in the browser, you're limited to

02:59.560 --> 03:06.080
the JavaScript and WebAssembly. So in the case of Greenfield, most of it is TypeScript that's then

03:06.080 --> 03:13.200
processed to JavaScript. And there's also a bunch of C involved, mostly existing C libraries,

03:13.640 --> 03:22.800
things like lipxkb common is compiled to WebAssembly to handle all the keyboard layouts and keyboard

03:22.800 --> 03:30.280
encodings. There's also lippixman basically to region handling. I'm not going to reinvent the wheel,

03:30.280 --> 03:36.000
I'm not going to write those things in JavaScript. So I took the existing libraries, ported them to

03:36.000 --> 03:42.920
WebAssembly and made them accessible inside the browser. The display part of Greenfield is written

03:43.160 --> 03:50.600
in WebGL to have any kind of decent performance. It's very similar to how a normal or say a native

03:50.600 --> 03:58.440
Wayland compositor would would composite using OpenGL. So in the browser case, it's WebGL. To deal

03:58.440 --> 04:04.880
with the remote applications, that was you need basically a proxy. So the native applications

04:04.880 --> 04:11.000
have to find a socket to connect to a Wayland socket to connect to and have those messages

04:11.080 --> 04:17.960
handled, have the native buffers handled and have the protocol sent to the browser. So I wrote

04:17.960 --> 04:26.440
server in TypeScript. I'm sorry guys. I needed something that was I could prototype fast and

04:26.440 --> 04:31.160
without dealing with seq faults and everything. So I did it in TypeScript and it grew a bit and now

04:31.160 --> 04:37.000
it's a bit bigger. So I guess some point in the future, I promise I will rewrite it and rust

04:37.400 --> 04:44.200
somewhere in the future, I guess. Also the performance critical parts of the whole pipeline

04:44.200 --> 04:50.360
are written in C using Gstreamer as well. So there is a tight integration there. Gstreamer,

04:50.360 --> 04:58.440
really cool project. It does basically everything I needed. Very modular. I could do all kinds of

04:58.440 --> 05:05.320
cool stuff with it. I'll show that a bit later. If there are any Gstreamer faults in the room,

05:05.320 --> 05:12.440
the only thing I missed a bit was color conversion on the GPU. So a hint that would be cool to have.

05:13.240 --> 05:20.520
At some point, I also wrote a Kubernetes implementation basically. So that basically means

05:20.520 --> 05:27.560
that you could use an entire Kubernetes cluster as your desktop. So every application would be its

05:27.560 --> 05:36.120
own part. So you could run 20 performance intensive applications, have them all run on separate

05:36.120 --> 05:42.200
machines, all running smoothly, basically giving you virtually unlimited resources on your desktop.

05:43.000 --> 05:49.080
So that's going in the direction I had in mind. Sadly not open source for the moment. Maybe

05:49.720 --> 05:55.640
someday in the future, if it's gone out of prototype phase, perhaps I can give a talk about it too.

05:56.120 --> 06:03.560
And last but not least, blood, sweat and tears. This is like a side project, a hobby project. So I

06:03.560 --> 06:09.800
was not working on it full time. So at some point, I had to sacrifice a bit of sleep to make some

06:09.800 --> 06:19.160
progress. I do not recommend, by the way. So here we have an overview, a pipeline overview, how frames

06:19.240 --> 06:26.680
are sent from your native site to the browser. So on the top part, we basically have everything

06:26.680 --> 06:32.120
that happens on the GPU. On the bottom part, we have everything that happens on the CPU. So we're

06:32.120 --> 06:39.160
going to start from left to right. On the left, we have an application that renders either on the

06:39.160 --> 06:48.600
GPU or on the CPU using GPU memory buffers or shared memory buffers on the CPU. When an application

06:48.600 --> 06:55.000
submits those pixels to the native compositor, so that's our proxy compositor that I talked about

06:55.000 --> 07:05.960
before, then those buffers are basically handled and they are color split. So we need to get these

07:05.960 --> 07:12.040
buffers to the browser and we need to compress them. Ideally, we do that using video encodings.

07:13.000 --> 07:20.040
But to do that, there are a few issues. Most color buffers that we get from the applications,

07:20.040 --> 07:26.440
they have an alpha channel. This day, we have cool virtual reality headsets that just launched. We

07:26.440 --> 07:31.320
have awesome artificial intelligence. We do not have video codecs that can handle alpha encode.

07:33.720 --> 07:41.320
So I had to split the color into an RGB and an alpha and those are actually two separate video

07:41.720 --> 07:48.600
streams. So the alpha color conversion is handled as a black and white video stream. This is done

07:48.600 --> 07:54.680
on the GPU if possible and there is a fallback to CPU encoding if it's not available.

07:56.120 --> 08:02.280
There's also the color conversion that also always happens on the GPU. And then those two frames,

08:02.280 --> 08:10.120
those two video frames are then sent using a WebSocket server to the browser. A small remark

08:10.120 --> 08:17.160
there, WebSocket server, it's TCP. We're dealing with a real-time application here. Ideally,

08:17.160 --> 08:22.440
ideally, people would use something like WebTransport which uses UDP but it's a bit still

08:22.440 --> 08:28.280
experimental. Not a lot of implementations there for the moment so we're stuck with WebSocket.

08:29.400 --> 08:34.680
The frames arrive on the browser and there we have to decode the video streams, the video frames.

08:35.640 --> 08:42.760
This is done for Firefox only, sadly, using WebWorkers where I ported

08:43.800 --> 08:51.960
FFMPEG to WebAssembly for H.264 as well. To do that, for the other browsers, I used the WebCodec

08:52.600 --> 08:59.720
standard so I can have the browser do all the artwork and have the video frame decoded. And

08:59.720 --> 09:05.480
then last but not least, there is a WebGL shader that puts the colors back together. So we have the

09:05.480 --> 09:12.600
RGB frame, we have the alpha frame, we stitch them back together and we push them to WebGL texture

09:12.600 --> 09:19.960
and paint them nicely on the screen. That was a long trip. This is all how we go from the

09:19.960 --> 09:26.760
application to the compositor. So far so good. But Wayland also has a few mechanisms to tell

09:27.720 --> 09:35.400
the application, hey, I'm done painting your pixels, you can send me the next batch if you were to

09:35.400 --> 09:42.520
please. But there is a bit of an issue here. Wayland is an asynchronous protocol but the

09:42.520 --> 09:47.960
dealing with sending a frame and the compositor telling the application, send me the next frame,

09:47.960 --> 09:53.800
that's a whole slow synchronous process. So what that means is that if this pipeline were

09:53.800 --> 09:59.720
to take no milliseconds, you still have your network latency. Say we have 50 millisecond

09:59.720 --> 10:04.440
network latency, we immediately send the frame to the compositor and then the compositor tells the

10:04.440 --> 10:09.400
application, okay, I'm done, you can send me the next one. You have about 50 milliseconds

10:10.520 --> 10:16.840
between each frame. So that gives you around 20 frames per second. That's not really acceptable.

10:17.720 --> 10:23.080
So we're going to have to be a bit smart and we're going to have to tell the application in advance,

10:23.160 --> 10:30.200
like hey, you can already send the next frame because I think the compositor is probably already

10:30.200 --> 10:37.320
done handling your previous frame. So that is basically the round trip latency problem that we

10:37.320 --> 10:43.400
have. And there are generally two mechanisms in the Wayland protocol. The first one is the one I

10:43.400 --> 10:50.040
just talked about which is the frame callbacks, which is the compositor telling the application,

10:50.040 --> 10:55.960
I'm done painting your pixels. The other one is the sync request. The sync request in the

10:55.960 --> 11:02.840
Wayland protocol is a way for an application to know when all previous requests that have been

11:02.840 --> 11:09.640
sent are done. So when an application sends a request to a compositor, those requests are queued

11:09.640 --> 11:16.920
up. And when an application sends a sync request, then the compositor will simply, once it encounters

11:16.920 --> 11:21.720
that sync request, will simply reply with done. And then the application know, oh, I got a done

11:22.440 --> 11:30.600
response. So that means all previous requests were handled. So to deal with that, as I just

11:30.600 --> 11:37.000
explained, there is the predictive frame callback. This is how we deal with the sync request issue.

11:37.720 --> 11:44.360
So this is a complex picture, a bit out of scope to go too much in detail. But what generally happens

11:44.360 --> 11:52.760
is that the proxy compositor on the native site analyzes all requests that come in. And as soon

11:52.760 --> 11:58.920
as it receives a sync request, it will look at all the previous requests and see, hey, those previous

11:58.920 --> 12:04.760
requests that I just saw, none of them is going to send a reply. So you know what, I'm just going

12:04.760 --> 12:10.280
to send the done event immediately, and I'm not going to wait for the compositor to send a done

12:10.280 --> 12:16.840
event back. And that basically circumvents the whole network around the problem. The only

12:17.800 --> 12:22.760
potential issue you can have there is that you're basically got rid of your throttling.

12:23.320 --> 12:30.200
But there is some intermediate protocol to deal with that between the compositor and the proxy

12:30.200 --> 12:36.840
on the native site. So this is explained in the picture here. On the top, we have the requests

12:36.840 --> 12:41.880
that are coming in in the compositor in a classic Willing scenario. On the top right, we see, hey,

12:41.880 --> 12:47.240
there is a sync request, and it takes a whole network round trip time before the done event is

12:47.240 --> 12:52.600
sent. And the fastest placing, we have the same scenario. We have all the requests coming in.

12:53.480 --> 13:00.520
And at the end, we have our fast sync handled by the proxy. He sees, okay, no, events are going

13:00.520 --> 13:04.920
to be sent. So I don't need to wait for the compositor to send any other events. I'm going to send

13:04.920 --> 13:12.040
the done event immediately. And that makes the whole pipeline asynchronous. It makes the whole

13:12.040 --> 13:17.240
pipeline fast enough. If you have fast encoding, fast decoding, fast enough for gaming as well,

13:17.240 --> 13:22.840
you can do 60 frames per second or more if your hardware is fast enough and your network can deal

13:22.840 --> 13:34.520
with it. So far, the remoting part. Greenfield can do more than just remote applications. As I said

13:34.600 --> 13:42.680
before, you can also run applications directly in your browser. To do that, there is a prototype

13:42.680 --> 13:50.040
Greenfield SDK. It's based on EM scripting because it's the most complete, somewhat post-excompatible

13:50.600 --> 13:59.080
SDK out there that is aimed for the browser. This works fairly well, but it has some disadvantages

13:59.080 --> 14:07.960
as well. Wayland applications are Linux applications, well, almost exclusively. And the EM scripting

14:07.960 --> 14:15.080
SDK aims to be post-excompliant. And that's not Linux compliant. So things like E-Poll are not

14:15.080 --> 14:22.120
implemented in EM scripting. So I had to add those, well, at least E-Poll in the Greenfield SDK

14:22.680 --> 14:31.560
because Wayland requires it as well. There's also the core Wayland protocol is implemented in

14:31.560 --> 14:38.760
Greenfield together with the XG protocol. That works well for desktop applications,

14:38.760 --> 14:44.840
gives you a good standard to work with most office applications. They work out of the box.

14:46.360 --> 14:51.000
In the WebAssembly implementation, there is currently only support for shared memory buffers.

14:52.680 --> 14:59.560
We can theoretically do WebGL if we were to port Mesa to WebAssembly and utilize a

15:00.280 --> 15:08.520
custom WebGL Wayland protocol. The protocol exists. It works. It's simply currently not

15:08.520 --> 15:18.360
implemented inside Mesa itself. That's some future work, I guess, to support that. But we can

15:18.360 --> 15:24.440
perfectly support WebGL. No issue. The work just here needs to be done. So how would this all

15:24.440 --> 15:32.120
look while we have a nice green diagram here to show you how this looks? We have our main page

15:32.120 --> 15:38.760
on the left that loads your composite, that loads Greenfields. Next to it, you will have an

15:38.760 --> 15:45.800
iFrame. The iFrame loads your WebAssembly application, loads it in, and then basically uses

15:46.040 --> 15:56.040
internal iFrame messages to your main thread. Talking 100% pure native Wayland protocol

15:57.160 --> 16:03.240
works fine, works great. Next to it, we have transparently also the remote applications

16:03.240 --> 16:09.000
that are running. For the Greenfield compositor, both applications are just your ordinary Greenfield

16:09.000 --> 16:16.920
applications. He sees no difference. With a small remark that the Way file descriptors are handled

16:16.920 --> 16:24.280
on the protocol is a bit different. In case of not remote applications, the file descriptor is

16:24.280 --> 16:32.040
basically a URL that's passed around and that's opened and transferred and closed whenever needed.

16:32.680 --> 16:40.040
In case of native or rather browser native applications, it's a transferable browser

16:40.040 --> 16:46.040
object that is used. Those two file descriptor discrepancies have not yet been bridged,

16:46.040 --> 16:50.600
so you cannot do copy-paste operations, for example, between a browser application

16:50.600 --> 16:57.480
and a remote application. What you can do, copy-paste between a remote application that works just fine.

16:57.960 --> 17:06.040
That's how it currently works. What would the future look like? There are lots of cool stuff

17:06.040 --> 17:11.480
that can be done, that still needs to be done. There is the issue of sound. There's currently no sound.

17:12.760 --> 17:20.920
I initially left it out because it's a bit out of scope for compositor or Wayland related stuff,

17:21.480 --> 17:28.280
but somebody also already did their master thesis about its implemented sound in Greenfield,

17:28.280 --> 17:35.480
using pipe wire and G-streamer worked really well. Prototype exists and can be implemented,

17:35.480 --> 17:42.200
I guess, pretty easily. There's also the need for a bit more Wayland protocol, so there is just only

17:42.200 --> 17:50.440
the very basic currently implemented. There is no unified file system, again a bit out of scope for

17:50.440 --> 17:56.360
Greenfield, but it doesn't really exist, so it would be nice if we had it. Imagine you run

17:56.360 --> 18:01.640
applications on different servers, they all see their own local file system and you cannot

18:01.640 --> 18:08.440
transfer files between them, so that would be nice to have. There is the port of Mesa, the WebGL,

18:08.440 --> 18:17.000
using Wayland protocol, that would be nice to have. Then last, the hardest part, also the coolest part,

18:17.000 --> 18:24.920
is the whole EM scripting posix issue. It would be nice if you would simply got rid of EM scripting

18:24.920 --> 18:31.720
and just could compile applications directly to WebAssembly and actually have a Linux micro

18:31.720 --> 18:41.240
kernel running in your browser. Somebody else who is also crazy ported Linux kernel to ASMGS.

18:42.200 --> 18:51.000
ASMGS is the predecessor of WebAssembly and that crazy person got the Linux kernel to boot,

18:51.000 --> 18:58.920
up until pit one at least. I tried it to do that myself using WebAssembly, turns out it's actually

18:58.920 --> 19:06.200
really hard to port the Linux kernel to other architecture, no shit, especially if WebAssembly

19:06.760 --> 19:14.520
is not an elf binary. The Linux kernel expects the elf binary to be used, the format to be used

19:14.520 --> 19:20.120
when it's compiled in all kinds of different places and that assumption goes through the window when

19:20.120 --> 19:27.560
you try to compile to WebAssembly. There's also a bit of documentation importing the Linux kernel

19:27.560 --> 19:32.520
to other architectures. There's a ton of documentation about the Linux kernel, but most of it is about

19:33.160 --> 19:39.720
developing drivers. But yeah, I'm not a kernel developer as well, so that might also have to do

19:39.720 --> 19:45.800
something with it. But I'd say think about possibilities you could compile an application

19:45.800 --> 19:51.480
or Linux application to WebAssembly, boot it up in your browser by simply accessing a URL and have

19:51.480 --> 19:58.360
it completely sandboxed, super secure, running inside a desktop that's running in your browser.

19:58.360 --> 20:05.960
That would be really cool I think. So how would say a Linux port look like in WebAssembly?

20:06.520 --> 20:12.840
We have a nice yellow diagram this time. It would simply load access a URL,

20:13.960 --> 20:19.000
it would load the WebAssembly application, the WebAssembly application would then link against your

20:19.960 --> 20:27.640
kernel, which is also a WebAssembly module, bit out of scope for the graphics room, but there are

20:27.720 --> 20:33.960
certain WebAssembly standards that allow you to isolate certain region memories to the application

20:33.960 --> 20:39.800
and to the kernel module and have some regions shared with them. And then we could probably

20:39.800 --> 20:47.960
leverage the Vue.tio stack and have it interact with basically the browser APIs and have your

20:47.960 --> 20:53.320
browser basically be your virtual machine. So that's probably how it would look like.

20:53.560 --> 21:02.120
For the file system some attempts have been made and for now Jusifes seems to be the most

21:02.120 --> 21:11.720
valid candidate. Interesting note here is that Jusifes uses two kinds of databases, one to

21:11.720 --> 21:17.480
store your data itself, one to store the metadata of your file system and the experiment. It was

21:18.280 --> 21:27.560
shown that the metadata database needs to be really fast, so we probably need a locally cached

21:27.560 --> 21:33.720
metadata database which then uses CRTT basically to synchronize between the instances

21:35.560 --> 21:45.480
to make it fast enough. So let's see if we can show some demos. That would be cool I guess.

21:48.040 --> 21:57.560
So in here we have a state-of-the-art green field running, super fancy as you can see.

22:03.160 --> 22:08.120
And I'm going to try a remote application, I hope the wifi holds.

22:08.120 --> 22:24.040
So this one is actually streaming remotely, Doom 3. I noticed it sometimes tends to freeze,

22:24.040 --> 22:28.040
I don't know if it's because it's an old application running on the Nvidia

22:28.600 --> 22:33.720
Wayland drivers. As you can see there is no pointer locking here, so that's still one of

22:33.720 --> 22:38.680
the protocols that needs to be implemented, but we can simply start a new application here.

22:39.240 --> 22:46.360
We have a nice 60 frames per second streaming to your browser. All works fast and fine,

22:47.400 --> 22:50.920
so I wasn't lying when I said it's fast enough for gaming.

22:55.960 --> 22:59.800
You can see it, you can simply walk around here and everything.

23:03.800 --> 23:17.480
There's also, here we have a Western demo applications compiled to a web assembly,

23:18.600 --> 23:27.640
there we go. So this is, I believe it's written using Cairo, I think my pointer is being captured

23:27.640 --> 23:36.840
by the game. So it's using Cairo to draw all everything and G-Lip as well. So this runs inside

23:36.840 --> 23:44.360
an iframe, if you were to inspect the source code here, we have the iframe here that runs the web

23:44.360 --> 23:51.880
assembly application and yeah, talks Wayland protocol, it runs entirely in your browser,

23:51.880 --> 24:02.840
nicely isolated as well. It's all transparently done. Then we have, let's see if we can get this

24:07.560 --> 24:08.520
No, it doesn't want to go.

24:14.360 --> 24:21.320
So this is your web assembly application and of course we can also run desktop applications,

24:21.320 --> 24:33.640
so I have one running locally here. I think this is a cute app, there we go. So this one is running

24:33.640 --> 24:42.520
on my desktop locally and see I can open, so this popup chooser is running in my browser,

24:42.520 --> 24:48.440
it's all Wayland, see I can't move it and you arrive some packet. See it's the fast system of my

24:49.080 --> 24:58.040
my laptop as I said before and you can run it here. So that's that.

25:02.040 --> 25:09.800
So far the demo, let's see if I can move it, yes, can go. So that was it, I hope you enjoyed. If you

25:09.800 --> 25:14.360
have any questions, I guess I have maybe some answers. I'll go from left to right.

25:15.080 --> 25:23.000
How does it work for input events? Yeah, so the question was how does it work for input events?

25:24.200 --> 25:32.440
It uses the browser's input framework. In case of pointer events, it uses the raw pointer event

25:32.440 --> 25:39.400
API if it's available. It's still quite experimental but it removes some lag. So when it does it captures

25:39.400 --> 25:45.720
the browser pointer events and then basically translates this to input events like they were

25:45.720 --> 25:51.560
coming say from from lip inputs and it's then sent using Wayland protocol to the application.

25:51.560 --> 25:59.000
So it's fast but you still have inevitably the network latency that's unavoidable, sadly.

25:59.640 --> 26:05.880
Yes, I have two questions. The first one is does it support KDE's background blur protocol?

26:07.160 --> 26:11.640
The question was does it support KDE background blur? No, it does not. Currently the only

26:11.640 --> 26:16.920
protocols implemented are the core Wayland protocol and basically the XDG desktop protocol.

26:17.480 --> 26:26.040
The second question is vertical synchronization v-sync. Yes, so the vertical v-sync is supported.

26:26.040 --> 26:33.800
So it uses the browser request frame callback API which is basically the v-sync callback that

26:33.800 --> 26:37.960
the browser offers to draw. Next question.

26:56.760 --> 27:06.440
Yeah, so the question was does it use h.265 and is it secure I guess in a legal standpoint?

27:06.440 --> 27:13.880
It uses h.264 in this case. The reason being that the web codec API is from the browsers.

27:14.440 --> 27:19.400
Most of the browsers do not support h.265, at least not at the point I implemented it.

27:19.400 --> 27:26.840
So currently it uses h.264. I currently do not use it for commercial purposes.

27:28.120 --> 27:34.120
So yeah, I hope I'm safe. I'm going to have one more question and then I'm going to stop.

27:35.640 --> 27:40.840
Yes. So obviously the way the application expects Unix socket at the wet level,

27:40.840 --> 27:44.520
do you fix this if they're compiling for WebAssembly? Did you just implement the Wayland?

27:45.400 --> 27:51.320
Yeah, so the question was if you're compiling for WebAssembly and the application is expecting a

27:51.320 --> 28:01.880
Unix socket, how do you implement it? I extended the EM script in SDK so that it basically has

28:01.880 --> 28:09.320
support for Unix sockets on the user level and then also added e-polling to that as well.

28:09.400 --> 28:16.360
So for the application it's purely Unix sockets. It's not entirely so there's only client side

28:16.360 --> 28:20.840
support for connecting to a Unix socket, not for creating a Unix socket because that is done

28:21.960 --> 28:30.360
by the SDK itself. If you have more questions, I'm available. So after the talk, come and find me.

28:30.360 --> 28:41.800
Happy to answer them. Thank you.

