WEBVTT

00:00.000 --> 00:05.000
Welcome back everybody.

00:05.000 --> 00:14.000
We're going to begin a little block of talks with the intersection of AI and real-time communications.

00:14.000 --> 00:20.000
Rob Pickering here is going to take it away with building open source agents using LMS.

00:20.000 --> 00:21.000
Rob, go for it.

00:21.000 --> 00:24.000
Thank you. Hi, can you hear me okay?

00:24.000 --> 00:25.000
Brilliant.

00:25.000 --> 00:26.000
Okay.

00:26.000 --> 00:29.000
So I'm Rob Pickering.

00:29.000 --> 00:38.000
I kind of landed in the real-time communications VoIP industry about 20 years ago after doing a whole load of other internet stuff

00:38.000 --> 00:43.000
and I've never quite really managed to escape.

00:43.000 --> 00:50.000
But what I want to talk about today is the idea of putting LLMs on the end of the telephone

00:50.000 --> 00:57.000
and some work I've been doing for about kind of the last nine, ten months on this whole idea.

00:57.000 --> 01:05.000
But in terms of where I come from on this, what was the kind of the most important thing for a successful project

01:05.000 --> 01:07.000
or a successful open source project?

01:07.000 --> 01:10.000
Come on.

01:10.000 --> 01:13.000
What was the most important thing for a successful project?

01:13.000 --> 01:14.000
Developers.

01:14.000 --> 01:18.000
Developers, yeah.

01:18.000 --> 01:25.000
Well, basically I think it's a fundamental belief in what you're doing that is just completely unassailable.

01:25.000 --> 01:28.000
You just believe in what you're doing and kind of make it work.

01:28.000 --> 01:36.000
I don't have that view actually about AI on telephones or machine voice interfaces generally.

01:36.000 --> 01:42.000
And up to now I've actually been reasonably agnostic about the utility of machine voice interfaces.

01:42.000 --> 01:50.000
I did a bit of work about four years ago right at the height of the COVID pandemic around connecting

01:50.000 --> 01:58.000
dialogue flow type AI, so proprietary conversational AI to phone lines for doing things on assistance lines.

01:58.000 --> 02:05.000
So basically asking a bunch of questions and then feeding the results into a Google spreadsheet.

02:05.000 --> 02:12.000
And I was reasonably convinced at that point that you could kind of make this into a SME product by putting a kind of a front end on it

02:12.000 --> 02:19.000
that allowed it to be self-service where someone could just go to a website, you know, kind of order themselves a machine voice assistant,

02:19.000 --> 02:26.000
connect it to the end of the phone line, feed the results into whatever office automation they use and, you know,

02:26.000 --> 02:29.000
just automate away all their customer contact.

02:29.000 --> 02:36.000
That was probably really quite naive. I didn't think I entirely believed in that, but it actually didn't go so well for that whole thing.

02:36.000 --> 02:47.000
I mean, the technology worked absolutely brilliantly and did a great job, but it took roughly a day basically to onboard the most basic of sort of telephone agents

02:47.000 --> 02:53.000
getting some results back. So kind of before that and ever since really, I've been a little bit agnostic about machine voice interfaces.

02:54.000 --> 03:01.000
I do do the whole, you know, Alexa Google Home thing, but only kind of in the workshop when I'm covered in dust or something,

03:01.000 --> 03:06.000
or in the car. I'm one of those kind of people that prefers kind of pointy-plicky stuff.

03:06.000 --> 03:12.000
From a developer point of view, I think they're painful to train and then they still blow up in your face afterwards

03:12.000 --> 03:17.000
and users either kind of train themselves to, you know, not like that Alexa,

03:17.000 --> 03:24.000
but they're only effectively train themselves to talk to the agent the way the agent expects them to, or they just kind of give up.

03:24.000 --> 03:34.000
Round about sort of this time last year, I started looking when things like GPT kind of went general release or general availability.

03:34.000 --> 03:42.000
I sort of thought, well, actually, could we start using LLMs to do the difficult bit, the intent recognition part of that,

03:43.000 --> 03:47.000
and then effectively kind of feed that into our back-end logic.

03:47.000 --> 03:52.000
I thought, okay, let's give it a go. There was nothing kind of really like that.

03:52.000 --> 03:59.000
OpenAI hadn't released their kind of audio interface, although all the pieces were there with Whisper and everything else.

03:59.000 --> 04:07.000
So I thought, okay, let's kind of give this a go and try connecting an LLM up to the telephone.

04:07.000 --> 04:18.000
So how do you do it? Well, there are quite a few projects that have kind of got most of the moving parts of taking a SIP conversation in

04:18.000 --> 04:23.000
and then turning it into an audio stream that you can then put into a speech-to-text,

04:23.000 --> 04:30.000
send that off to whatever LLM platform or whatever local LLM you're running, and get the results back.

04:30.000 --> 04:36.000
Asterisk, I'd already used actually in the previous project, and there was a nice bit of software actually.

04:36.000 --> 04:39.000
Dan's disappeared off now because I'm talking.

04:39.000 --> 04:48.000
Dan had a nice Asterisk-ARI interface module that effectively allowed you to work with Dialogflow,

04:48.000 --> 04:53.000
and that was fine, send the audio stream up to Google, get the results back.

04:53.000 --> 04:57.000
There's a similar module in FreeSwitch, and then there's Jambones.

04:58.000 --> 05:04.000
Jambones is really quite a nice open source platform, sort of open source UC platform,

05:04.000 --> 05:10.000
that kind of does all this stuff for you from a speech-to-text and text-to-speech point of view.

05:10.000 --> 05:17.000
So it interfaces to, I think it's about 10 different speech-to-text services, multiple text-to-speech services.

05:17.000 --> 05:26.000
It's got a nice API, WebSocket, Vent API, and it's like, okay, let's use Jambones as the piece that we're going to put the SIP calls into

05:26.000 --> 05:31.000
and figure out how to kind of write a bit of middleware that sits in the middle and lets us evaluate this.

05:31.000 --> 05:35.000
So this was my kind of plan for how to work this.

05:35.000 --> 05:41.000
So we have this really imaginatively named LLM agent down here.

05:41.000 --> 05:46.000
It talks to Jambones. It also talks to OpenAI.

05:46.000 --> 05:49.000
So call comes in from a SIP carrier.

05:49.000 --> 05:55.000
Jambones tells us via a WebSocket that that calls come in.

05:55.000 --> 06:01.000
It then hands us the speech-to-text transcription of that call.

06:01.000 --> 06:04.000
We send that off to OpenAI with a prompt.

06:04.000 --> 06:14.000
We get the result back and then we use the text-to-speech engine that also interfaces Jambones to get that out as an audio stream.

06:14.000 --> 06:18.000
And okay, doing it with just one vendor isn't a great idea.

06:18.000 --> 06:25.000
So I added Google Palm 2 onto that as well and put a nice kind of generic interface on it.

06:25.000 --> 06:37.000
We use the Jambones WebSocket API and the way that the agent actually works to get a calling or when we set an agent up,

06:37.000 --> 06:43.000
the idea was to make it quite big and multi-tenant capable so that we could have one instance of Jambones

06:43.000 --> 06:52.000
and one instance of the LLM agent effectively handling multiple inbound calls and multiple inbound call agents

06:52.000 --> 06:56.000
so running effectively multiple LLM scripts.

06:56.000 --> 07:02.000
So what happens when we set a new client up, a new agent within the LLM agent software?

07:02.000 --> 07:05.000
There's a lot of agents in here, aren't there?

07:05.000 --> 07:10.000
We pull a spare number off a pool that we hold inside Jambones.

07:10.000 --> 07:17.000
We nail up an application with a new WebSocket to the agent and we link that to a number

07:17.000 --> 07:28.000
and that then gives us a way of routing a call in into our software that we can then post off the S2T to the LLM.

07:28.000 --> 07:30.000
Inside Jambones it kind of looks like this.

07:30.000 --> 07:39.000
You have a bunch of applications and then we have a bunch of applications that post in to our agent

07:39.000 --> 07:44.000
and then we have a bunch of phone numbers, big lock of them that are routed in from a carrier.

07:44.000 --> 07:52.000
Some of the numbers are allocated to an application that means that they've been linked to an agent and then others are available.

07:52.000 --> 08:02.000
So when we make a REST call into our LLM agent software, we federally nail up an application

08:02.000 --> 08:05.000
and then that connects up to a Jambones phone number.

08:05.000 --> 08:14.000
The whole point really is that the client application that's trying to provide this agent service doesn't see any of that complexity in Jambones,

08:14.000 --> 08:19.000
although it does effectively, is post the prompt to the agent interface.

08:19.000 --> 08:29.000
It gets a phone number back saying I've linked your number to this phone number and then it gets a WebSocket feed of all the events that occur on that agent.

08:29.000 --> 08:46.000
So rather than messing around doing this as PowerPoint where let's set up an agent,

08:46.000 --> 08:57.000
so all that we do is we make a REST call to create an agent.

08:57.000 --> 09:09.000
So in this case I'm going to create a chat GPT, oh sorry, an open AI GPT 3.5 turbo agent with a prompt there.

09:09.000 --> 09:15.000
And then I get a response back. That response gives me an ID and a phone number.

09:15.000 --> 09:30.000
So let's give that phone number a call.

09:30.000 --> 09:49.000
Impossible to stage because the number is kind of pseudo random.

09:49.000 --> 10:10.000
Okay, so that's a simple call on a REST API that allows us to create an agent with a prompt.

10:10.000 --> 10:33.000
I'm just going to tear that agent down again otherwise we will leak phone numbers.

10:33.000 --> 10:48.000
Okay, so that was kind of the first part of it. We built a service that allowed us to run up an agent on the end of a phone number that was using an LLM to fulfill the conversation.

10:48.000 --> 10:55.000
Next part of it I thought was kind of worthwhile was to give it a try with putting a user interface so that we can create a playground for it.

10:55.000 --> 11:02.000
Just allowed us to experiment with it. So that's really kind of the next part of it which is to go back to the playground.

11:02.000 --> 11:09.000
And we do a similar thing here. We can select a model, we'll do a slightly more capable model, hit on create agent.

11:09.000 --> 11:20.000
We did that playing around with voices so I'll still use a UK English voice but let's do something like that.

11:20.000 --> 11:33.000
Yeah, I know the font is quite tiny.

11:33.000 --> 11:43.000
I'd like to buy five different UK flags please.

11:43.000 --> 11:49.000
That's great. Could you please specify the size and material for the UK flags you want to order?

11:49.000 --> 11:57.000
We have sizes of 50 centimetres, one metre and five metres and materials of nylon or canvas.

11:57.000 --> 12:10.000
Could you do me one each of those sizes and each of the material please just so I've got a range of samples?

12:10.000 --> 12:21.000
Sure, just to clarify you would like one UK flag of each size, 50 centimetres, one metre, five metres in both nylon and canvas material.

12:21.000 --> 12:25.000
So that would be a total of six flags. Is that correct?

12:25.000 --> 12:30.000
Yeah, that's right.

12:30.000 --> 12:39.000
Great. Could you please provide your full name and delivery address?

12:39.000 --> 12:51.000
Okay, so actually that was the kind of presentation I was giving in kind of July, August 5 last year.

12:51.000 --> 13:00.000
So that was as we just developed the thing. Let's go back and have a look at some of the fun things we had to try with it.

13:00.000 --> 13:15.000
So I thought this one was quite cool actually.

13:15.000 --> 13:20.000
Rob's Donuts wholesale ordering line. How can I assist you today?

13:20.000 --> 13:25.000
Hello, I need to order some donuts.

13:25.000 --> 13:35.000
Hi there, thank you for calling Rob's Donuts. I'd be happy to help you with your order. How many donuts would you like to order?

13:35.000 --> 13:47.000
Hello, I need to order some donuts. I sell on average 45 donuts a day. What is the best price you can offer me?

13:47.000 --> 14:00.000
Great. Selling an average of 45 donuts is fantastic. Let me calculate the best price for you. Based on our cost price, I can offer you a special price of 40 pence per donut.

14:00.000 --> 14:09.000
So for 45 donuts, the total would be 18 pounds. How does that sound to you?

14:09.000 --> 14:21.000
Thank you for the offer, but I'm afraid that price is too high for me. I need to maintain a margin of 75% on my purchase price in order to make a profit. Can you offer a lower price?

14:21.000 --> 14:31.000
I think Saul will yank me off stage if I let that whole thing run for five minutes. Let's move on.

14:31.000 --> 14:40.000
That was kind of interesting, right? Because that's actually the prompt for the cafe owner. It's not huge.

14:40.000 --> 14:49.000
We basically just set two different agents off, bridged the call together and set these folks. They did actually arrive at a price.

14:49.000 --> 14:59.000
If you find the video online, there is actually a bit of a bug in the middle in that one of the speech-to-text transcriptions gets...

14:59.000 --> 15:08.000
Basically, they get confused about whether they're quoting for the volume of orders per day, and it all goes a bit awry, but then they recover it again.

15:08.000 --> 15:19.000
It's quite an interesting example of... If I was a little cynical at the start of the process, it's certainly a good demo of what the technique can do.

15:19.000 --> 15:28.000
There is kind of a bit of a problem because doing demos like that, it is really, really quite interesting to do that from a one-page prompt.

15:28.000 --> 15:35.000
I literally woke up at four in the morning and thought, it'd be quite cool if we tried that. Wrote a couple of prompts, didn't really tweak them around very much.

15:35.000 --> 15:41.000
I think I posted a video on the internet by about seven in the morning. It's not a complicated thing to do.

15:41.000 --> 15:55.000
But there is a bit of a trap in here because prompts aren't code. They're just an initialization of some AIS state in the expectation or hope that you might get the kind of completions that you're looking for.

15:55.000 --> 15:59.000
But actually, as long as we understand that, we can kind of work with it.

15:59.000 --> 16:11.000
But the problems that we did find while we were playing with these different instantiations of agents based on simple text prompts are certainly hallucination.

16:11.000 --> 16:18.000
All sorts of interesting things happen, especially through the lens of a lossy speech to text.

16:19.000 --> 16:26.000
Bear in mind, we're using these models as text-based models and we're putting speech to text on the front.

16:26.000 --> 16:34.000
So if they get deeply confused about whether we're talking about pounds and pence or donuts per day or pence per donut because of poor transcription,

16:35.000 --> 16:44.000
you get almost a sort of multiplying effect on that hallucination because the randomness that the speech to text can inject,

16:44.000 --> 16:50.000
particularly with real humans on noisy phone lines, kind of multiplies that ability to hallucinate.

16:50.000 --> 16:58.000
There's also prompt injection. I would not encourage anyone to sack all their salespeople and empower an AI to negotiate prices.

16:59.000 --> 17:06.000
Because actually, they don't do maths. I don't know if that's a secret, but they don't do maths.

17:06.000 --> 17:09.000
They replicate previous results effectively.

17:09.000 --> 17:17.000
So that's a problem that if I'm malicious, I can inject malicious text with knowledge

17:17.000 --> 17:25.000
that the underlying LLM is implemented in order for it to give me donuts for 1p or pay me Â£1,000 to buy a donut or something similar.

17:26.000 --> 17:31.000
A particular problem when we're trying to do this with voice on things like telephone connections,

17:31.000 --> 17:41.000
and this is a huge issue because of our expectation we pick up the phone to a human

17:41.000 --> 17:51.000
and you get basically about a second or so to respond before the human on the end gets a bit fed up and either thinks you've gone away or whatever.

17:52.000 --> 18:02.000
So poor latency is particularly a problem, especially if we're using large remote models where we're firing a request into open AIS data centre

18:02.000 --> 18:05.000
and hoping they'll look favourably upon us.

18:05.000 --> 18:15.000
And similarly, there's very definitely a privacy issue around firing off all this data to humongous cloud providers.

18:15.000 --> 18:18.000
A lot of these problems are quite soluble though.

18:19.000 --> 18:26.000
And the model that we came up with which seems to work quite well actually is effectively using containment or gatekeeping.

18:26.000 --> 18:34.000
So rather than just giving the agent one great big prompt right at the start, what you effectively do is micro prompt each stage.

18:34.000 --> 18:42.000
So it's going back really more to that model that I talked about where we use the LLM to do the intent recognition

18:42.000 --> 18:50.000
but then use logic to decide how we as an organisation are going to actually act on that.

18:50.000 --> 18:55.000
And this actually lines up quite nicely with a lot of the current AI safety theory.

18:55.000 --> 19:07.000
If you look at some of Musta Solomon's work on this whole idea of applying guardrails and effectively putting algorithmic controls on what the AI can do,

19:07.000 --> 19:11.000
this idea of gatekeeping and containment actually works quite nicely with that.

19:11.000 --> 19:18.000
So the idea with this is that you allow the LLM full authority over the conversation

19:18.000 --> 19:26.000
but as soon as you then summarise that conversation and once they can act an action on it, anything that's got side effects or changes,

19:26.000 --> 19:29.000
only the gatekeeper logic gets to do that.

19:29.000 --> 19:40.000
So if I'm authenticating a bank account for example, what I don't do is say to the LLM, Fred's password is Tuesday.

19:41.000 --> 19:44.000
Check whether he's authenticated or not.

19:44.000 --> 19:51.000
You effectively use a multi-letter approach where the LLM doesn't have any knowledge of the secret.

19:51.000 --> 20:02.000
It's only sent off to extract and then we give an indicator back to the LLM of either that answer is correct, incorrect or perhaps close enough, drill down.

20:03.000 --> 20:13.000
And what that means is that the LLM then is acting as an agent of the logic which is implementing the algorithm rather than having this kind of,

20:13.000 --> 20:18.000
this autonomy and this ability to go and do things.

20:21.000 --> 20:26.000
I'm probably going to run out of time if I show you how we architected the API to do that.

20:27.000 --> 20:33.000
But fundamentally what you do is the API's got the ability to do that.

20:33.000 --> 20:43.000
What you do is you create your initial agent with your initial prompt and then when a call comes in, we get a call event on the WebSocket

20:43.000 --> 20:50.000
and then each completion or intent recognition that goes through the LLM,

20:50.000 --> 20:54.000
we get an event on the WebSocket that tells us that's happened

20:54.000 --> 21:04.000
and then we get the ability to post an update to the agent on a specific call ID to update the prompt that's being used.

21:04.000 --> 21:11.000
So for example in the banking application context, the user has said, you know, I want to log on.

21:11.000 --> 21:16.000
So we update the prompt to say prompt the user for their secret.

21:17.000 --> 21:25.000
And then when the LLM comes back with a result, the result may be multiple results, right?

21:25.000 --> 21:30.000
It may be, yeah, I got the user secret. No, actually the user doesn't want to know that at all.

21:30.000 --> 21:36.000
They just want to open an account and they want to talk to a salesperson or any one of those intents.

21:36.000 --> 21:45.000
So the gatekeeper then moves the conversation into another of these contexts by putting effectively an update prompt.

21:45.000 --> 21:50.000
So it's a little bit like RAG where well it is RAG effectively.

21:50.000 --> 21:55.000
So what we're effectively doing is that the LLM is doing the intent recognition,

21:55.000 --> 22:00.000
plumbing it into the algorithm and then giving an answer which is generated by an algorithm effectively

22:00.000 --> 22:03.000
and certainly controlling it through logic in an algorithm.

22:03.000 --> 22:09.000
So, you know, that's the 30,000 foot view of the project.

22:10.000 --> 22:17.000
I kind of founded this really as a little side project just because I was kind of interested in what the technology was capable of.

22:17.000 --> 22:26.000
As a result of what I've kind of figured out from doing this, there are a few kind of opportunities for development here.

22:26.000 --> 22:32.000
And the most obvious one which helps solve the latency problem, a whole load of other things, privacy problems as well,

22:32.000 --> 22:36.000
is at the moment we're only interfacing to open AI and Google.

22:36.000 --> 22:42.000
There's an abstraction there that should make it easy to interface to other things, but actually for operational reasons,

22:42.000 --> 22:51.000
we kind of decided to use open AI and Google because this whole thing is available as a playground that you can just go to today

22:51.000 --> 22:54.000
and play with so I'm kind of funding hosting of that.

22:54.000 --> 23:01.000
And the quantization of the hosting cost actually works better with open AI and Google than it does with implementing

23:01.000 --> 23:07.000
an open source model and paying for the compute time to run it to make it available.

23:07.000 --> 23:13.000
But certainly some work on implementing this based on open source models, bringing Mastral and Lama into the mix.

23:13.000 --> 23:17.000
Open source embedded, speech text, text space.

23:17.000 --> 23:21.000
The best game in Taiwan at the moment I think is probably Whisper.

23:21.000 --> 23:27.000
There's been a new release of open AI's Whisper in December and that is an open source product,

23:27.000 --> 23:34.000
unlike some of their other stuff and bringing that into the speech recognition engine,

23:34.000 --> 23:39.000
either doing it here or doing it in JamBones' custom speech-to-text engine,

23:39.000 --> 23:43.000
I think is definitely going to have some interesting results there.

23:43.000 --> 23:46.000
Handling interruptions and async conversations.

23:46.000 --> 23:51.000
Human beings aren't very good at doing conversations in turn like those two agents there were there.

23:51.000 --> 23:53.000
The agent starts talking, human interrupts them.

23:54.000 --> 23:57.000
Yes, you can help me, this is what I want.

23:57.000 --> 24:04.000
So again, being better at handling that and that's something that I think we can do through customization of that speech-to-text engine

24:04.000 --> 24:09.000
that allows us to abort an intent recognition earlier if someone starts talking.

24:09.000 --> 24:15.000
Again, latency is going to be improved by making those models closer.

24:15.000 --> 24:17.000
Function calling.

24:17.000 --> 24:22.000
So open AI's API has got the ability to do function calls.

24:23.000 --> 24:28.000
Some of the other commercial models have, none of them do it in a consistent way

24:28.000 --> 24:31.000
and I've not even thought about how we start to make function calling,

24:31.000 --> 24:37.000
something that the API can kind of abstract away in a sensible way.

24:37.000 --> 24:41.000
Someone came up a while ago with a bot-to-bot API which I think is quite cool

24:41.000 --> 24:44.000
so it's the idea of let's just stop the humans talking to each other at all.

24:44.000 --> 24:49.000
I do not buy and bot, talk, steal them, selling bot and we just don't bother with that anymore.

24:49.000 --> 24:54.000
I like the idea of audio interfaces and this kind of textual log of the conversation

24:54.000 --> 24:57.000
because it's kind of interesting and it means we can read it.

24:57.000 --> 25:06.000
But certainly freeing up humans from conducting it all through the medium of a killer as audio doesn't seem particularly sensible.

25:06.000 --> 25:11.000
And I guess actually a sustainable business model to support the try-out.

25:11.000 --> 25:22.000
The try-out interface which sits at llm.applicay.com's costing $200 or $300 a month to run at the moment in credits

25:22.000 --> 25:28.000
which I guess is a new MacBook every few months so I should probably try and find a way of kind of supporting that.

25:28.000 --> 25:37.000
And most of all a better name because tripping over the name LLM agent in this presentation has been kind of fun.

25:37.000 --> 25:40.000
So that's where the project's going, going forwards.

25:40.000 --> 25:47.000
A bunch of links there. There's my email address, Robert Pickering.org at RobertMatrix.org.

25:47.000 --> 25:49.000
We got time for questions?

25:55.000 --> 26:00.000
So quick question. I'm curious about the integration between OpenAI Whisper and Gambon.

26:01.000 --> 26:04.000
So is this something that you're going to have a little bit of that?

26:04.000 --> 26:09.000
Something you can do, use OpenAI Whisper within Gambon or do you need to do it outside?

26:09.000 --> 26:14.000
Do you know, I've not looked, it seems like an obvious thing for Jambones to do but it's not my project.

26:14.000 --> 26:17.000
It may be they've even done it or someone's done it.

26:17.000 --> 26:22.000
The brilliant thing about Jambones is it has got a custom speech-to-text API on it

26:22.000 --> 26:28.000
so you have got this kind of plug-in availability where you can take a WebSocket audio feed from Jambones

26:28.000 --> 26:31.000
and hand the transcription back in.

26:31.000 --> 26:37.000
But let's say that you want to use OpenAI Whisper because it's open source, it's faster, it's latency.

26:37.000 --> 26:42.000
So I'm curious about how does that interlace with your kind of project.

26:42.000 --> 26:52.000
It's like in the workflow, it's something that you need to do it before using Jambones, in Jambones, after.

26:52.000 --> 26:54.000
I don't know if you have anything about it.

26:54.000 --> 26:57.000
Architecturally I could choose to do that in any place.

26:57.000 --> 27:04.000
It seems pretty obvious that given that I'm handing off speech-to-text and text-to-speech to Jambones,

27:04.000 --> 27:12.000
I would do it there and if someone isn't doing it already then doing it through their custom speech-to-text API

27:12.000 --> 27:14.000
would be the obvious place to do it.

27:14.000 --> 27:20.000
It gets a little bit interesting because of that thing that I talked about with the ability to do interruptions

27:20.000 --> 27:23.000
and terminating the endpoint on a conversation.

27:24.000 --> 27:30.000
I think when setting up, and it's one of the jobs that's kind of been on my to-do list since about October,

27:30.000 --> 27:39.000
when I do get around to doing it, I think I actually may find that doing it within just the straight Jambones custom speech-to-text

27:39.000 --> 27:44.000
interface might be too restrictive because it doesn't allow me to get real-time control of what's going on

27:44.000 --> 27:48.000
in the inferencing that's been handed off to OpenAI.

27:48.000 --> 27:53.000
But I hope not because in theory if we could just throw that into Jambones then that both solves the problem

27:53.000 --> 27:56.000
and is a useful enhancement to that project.

27:56.000 --> 27:57.000
Okay.

27:57.000 --> 27:58.000
Thank you.

27:59.000 --> 28:00.000
Over there.

28:00.000 --> 28:04.000
Yeah, so in the point you mentioned that the language model is not like code,

28:04.000 --> 28:08.000
not going to produce or react like code.

28:08.000 --> 28:09.000
Yeah.

28:09.000 --> 28:10.000
It's not reliable enough.

28:10.000 --> 28:16.000
But there is this thing called the GBNF, the formal grammar in the language.

28:16.000 --> 28:17.000
Yeah.

28:21.000 --> 28:24.000
Okay, so the question was about my statement that prompts on code,

28:24.000 --> 28:27.000
particularly in things like OpenAI models.

28:27.000 --> 28:35.000
And it's my opinion on that impact of by formal notations like GBNF, I think it is in Lama.

28:35.000 --> 28:38.000
GBNF in Lama.

28:38.000 --> 28:45.000
Yeah, I mean that's an alternative approach to the problem is to make the AI more deterministic, right?

28:45.000 --> 28:49.000
So you either constrain the AI by an algorithm kind of outside the box

28:49.000 --> 28:52.000
or you make the AI more deterministic.

28:52.000 --> 28:57.000
And I would argue that actually doing both is probably what you need to do.

28:57.000 --> 28:59.000
Any more questions?

29:03.000 --> 29:06.000
Can I take the orange t-shirt at the back please?

29:06.000 --> 29:07.000
Thank you.

29:07.000 --> 29:09.000
My question was about literacy.

29:10.000 --> 29:14.000
Is that mostly caused by finding what the end of the sentence is

29:14.000 --> 29:19.000
or by the time that it takes to get a response from the LLM?

29:19.000 --> 29:20.000
Okay, that's a great question.

29:20.000 --> 29:24.000
So the question is, I've stated latency is a problem.

29:24.000 --> 29:30.000
Is that primarily caused by the endpoint detection in the speech to text

29:30.000 --> 29:33.000
or is it primarily caused by the latency to the LLM?

29:33.000 --> 29:35.000
And the answer is it's both.

29:36.000 --> 29:41.000
Good streaming speech to text allows me to reduce that latency.

29:41.000 --> 29:47.000
Really bad speech to text, which is kind of heavily blocked.

29:47.000 --> 29:51.000
So I'm having to take a big sample size before I put it into my engine

29:51.000 --> 29:53.000
to get a stream of transcription out.

29:53.000 --> 29:58.000
Obviously slows down my, I can't even send for a completion

29:58.000 --> 30:03.000
until I've got what looks like a meaningful piece of user input.

30:03.000 --> 30:08.000
But I've then got the further delay caused by non-deterministic stuff on the engine.

30:08.000 --> 30:13.000
So you will have noticed actually I chose GPT 3.5 for one of those demos

30:13.000 --> 30:14.000
because it's really quick.

30:14.000 --> 30:18.000
GPT 4, even Turbo, you know, kind of like about half of the latency

30:18.000 --> 30:20.000
is waiting for the transcription

30:20.000 --> 30:23.000
and the other half is waiting for getting the answer back.

30:23.000 --> 30:25.000
But really great question.

30:25.000 --> 30:26.000
How are we doing for time?

30:26.000 --> 30:28.000
I've still got time for another question.

30:28.000 --> 30:31.000
White shirt in the middle there.

30:31.000 --> 30:35.000
Could you screen that a bit by getting the speech to Urbanare?

30:35.000 --> 30:38.000
Because you can both with humans, I don't know how to think about it.

30:38.000 --> 30:41.000
They go, um, okay.

30:41.000 --> 30:45.000
So could you put the voice on to do that?

30:45.000 --> 30:50.000
So the caller hasn't gotten that long silence when they think they've got away.

30:50.000 --> 30:55.000
So the question is can you kind of use human factors

30:55.000 --> 30:59.000
and put some kind of noise, some feedback, some Urbanare's

31:00.000 --> 31:03.000
coming from the agent synthesized to effectively let the human know

31:03.000 --> 31:05.000
that there's still something on the end.

31:05.000 --> 31:07.000
Yeah, you absolutely can actually.

31:07.000 --> 31:14.000
And the LLNA agent code, I think it might have gone away.

31:14.000 --> 31:17.000
But one of the things that I was just put some background noise in there,

31:17.000 --> 31:19.000
so you just put some background office noise,

31:19.000 --> 31:22.000
because everyone is super used to calling call centers

31:22.000 --> 31:24.000
and hearing tappity tappity click clickity click.

31:24.000 --> 31:27.000
And you know, what did you do at the weekend in the background?

31:27.000 --> 31:31.000
So I think I might have found the web file I was using for that

31:31.000 --> 31:35.000
wasn't entirely as unencumbered as I thought it was.

31:35.000 --> 31:37.000
Question down here at the front.

31:37.000 --> 31:39.000
What type of speech model is there?

31:39.000 --> 31:41.000
Because it sounds pretty natural.

31:41.000 --> 31:44.000
That one that I was using there was Google,

31:44.000 --> 31:47.000
which is, you know, still pretty good.

31:47.000 --> 31:49.000
It's old but good.

31:49.000 --> 31:53.000
But it's a really nice streaming speech to text.

31:53.000 --> 31:56.000
The text to speech was again Google.

31:56.000 --> 32:00.000
It's one of the WaveNet Google text to speech.

32:00.000 --> 32:04.000
Sorry, I misunderstood and misrepresented your question.

32:07.000 --> 32:10.000
Is there a lot of interest in the bot interface

32:10.000 --> 32:13.000
where you set the zone up and find and selling bots

32:13.000 --> 32:15.000
which can involve people there?

32:15.000 --> 32:17.000
Which seems to be a big waste of producing power

32:17.000 --> 32:19.000
because you could just have like a json API

32:19.000 --> 32:21.000
and find and sell donuts.

32:21.000 --> 32:25.000
So the question is, is there a lot of interest in a bot-to-bot interface?

32:25.000 --> 32:29.000
And actually, when I was first asked this question,

32:29.000 --> 32:31.000
it was in another presentation, another place.

32:31.000 --> 32:36.000
And I wittily retorted, yeah, we've had EDI for kind of like 30 years

32:36.000 --> 32:40.000
and you know, that's the way that you do these kind of data exchange

32:40.000 --> 32:41.000
and negotiation.

32:41.000 --> 32:44.000
But I think the reality is it is quite interesting

32:44.000 --> 32:47.000
and people have subsequently persuaded me to change my mind on this

32:47.000 --> 32:50.000
because it is quite interesting to be able to see

32:50.000 --> 32:54.000
a human-parsable transcript of a negotiation.

32:54.000 --> 32:56.000
I think it might have legs.

32:56.000 --> 32:58.000
It's well down the feature list.

32:58.000 --> 33:00.000
But it is quite interesting from that point of view

33:00.000 --> 33:02.000
because I can see a transcript of it

33:02.000 --> 33:04.000
rather than seeing, you know, a bunch of XML

33:04.000 --> 33:07.000
or a bunch of JSON saying this is why we arrived at 46p of donut.

33:07.000 --> 33:10.000
What I actually get to see is something that purports to be something

33:10.000 --> 33:12.000
like a kind of human reasoning path.

33:12.000 --> 33:14.000
But I think it's a very good question.

33:15.000 --> 33:17.000
Any more questions?

33:17.000 --> 33:19.000
Oh, waving your arm to the back.

33:19.000 --> 33:21.000
Sorry, I missed you.

33:40.000 --> 33:43.000
Latency is, you know, yeah.

33:44.000 --> 33:50.000
One and a half, two seconds is not a great latency.

33:50.000 --> 33:55.000
Getting that down to 500, 600, 700 milliseconds would be ideal.

33:55.000 --> 33:59.000
And by being able to chunk the speech to text better

33:59.000 --> 34:01.000
so that we can stream it

34:01.000 --> 34:04.000
and also move the inference thing quicker and faster,

34:04.000 --> 34:06.000
we can hopefully get that down.

34:06.000 --> 34:09.000
I mean, through some of the techniques like the background noise,

34:09.000 --> 34:11.000
you can kind of get away with it,

34:11.000 --> 34:14.000
especially if the human being knows they're talking to a bot,

34:14.000 --> 34:16.000
which I, you know, you kind of have to be honest

34:16.000 --> 34:18.000
and let them know that's what's happening.

34:18.000 --> 34:21.000
So I think people are happy to synchronize in that way,

34:21.000 --> 34:23.000
but it could be a lot better.

34:33.000 --> 34:36.000
Okay, so question is what use cases are there

34:36.000 --> 34:40.000
in the domain of customer interaction?

34:42.000 --> 34:45.000
Not out there, but which ones have you tried?

34:45.000 --> 34:47.000
Which ones have we tried?

34:51.000 --> 34:54.000
In terms of trying this on real users

34:54.000 --> 34:57.000
in a real production commercial environment,

34:57.000 --> 35:01.000
I'm going to stick my hands up and say I don't think it's ready yet.

35:01.000 --> 35:06.000
There's some development work that I'm personally doing

35:06.000 --> 35:08.000
that should kind of come to fruition.

35:08.000 --> 35:10.000
August, September this year,

35:10.000 --> 35:12.000
they'll put it into a real trial environment

35:12.000 --> 35:15.000
in a commercial environment,

35:15.000 --> 35:20.000
but right now there's those four or five bullet points on hurdles there

35:20.000 --> 35:24.000
to improve the ability of the system to do stuff.

35:24.000 --> 35:29.000
I wouldn't put this on the end of anything commercially significant

35:29.000 --> 35:33.000
right now in February 2024,

35:33.000 --> 35:37.000
but I think with the right controls in place,

35:38.000 --> 35:43.000
there is so much money to be made out of doing this.

35:43.000 --> 35:47.000
The amount of human endeavor spending its time on the end of headsets

35:47.000 --> 35:50.000
in call centers is going to happen commercially,

35:50.000 --> 35:53.000
so really what we need to do is going to make it work

35:53.000 --> 35:56.000
in the most beneficial way that we possibly can,

35:56.000 --> 35:58.000
and by making it open source

35:58.000 --> 36:01.000
so we can see the moving parts that are inside it,

36:01.000 --> 36:05.000
and having it depend on open source models that can be audited

36:05.000 --> 36:09.000
I think is the best possible way of doing that personally.

36:09.000 --> 36:12.000
I think we're probably done with questions,

36:12.000 --> 36:15.000
I'm certainly out of time, so thank you all very much.

