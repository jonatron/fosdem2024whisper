WEBVTT

00:00.000 --> 00:05.000
To Gaffani the renter

00:09.940 --> 00:29.660
of the

01:29.660 --> 01:36.660
So the thing to watch out is you may have heard with the other speaker.

01:36.660 --> 01:42.660
If when you move, you should be okay.

01:42.660 --> 01:43.660
I think I got it.

01:43.660 --> 01:44.660
He sure was rubbing against it.

01:44.660 --> 01:46.660
That's why it was making so much noise.

01:46.660 --> 01:47.660
Thanks.

01:47.660 --> 01:49.660
Right.

01:49.660 --> 01:52.660
So hello everyone.

01:52.660 --> 01:56.660
Today I'm going to talk to you about the challenges of deploying your network workloads

01:56.660 --> 02:02.660
at different levels of extraction, and particularly from the standpoint of CPU affinity.

02:02.660 --> 02:07.660
So first thing first, I'm Hadi, and I'm currently part of the vector packet processing team at Cisco

02:07.660 --> 02:11.660
alongside Nathan and Hedy who contributed to this presentation.

02:11.660 --> 02:20.660
So we're all contributors to the FDIO VPP project, and Nathan and Hedy are active contributors of the Calico VPP data plane.

02:20.660 --> 02:24.660
So first, I'd like to introduce the concept of network function.

02:24.660 --> 02:28.660
So it's basically any application or network service which processes packets,

02:28.660 --> 02:34.660
and you're probably familiar with physical network functions, which are appliances like routers or switches.

02:34.660 --> 02:45.660
And in these systems, there is a challenge to L4, and also it can be used in many network functions as a very performant data plane component.

02:45.660 --> 02:56.660
So rather than using a scalar packet processing approach, it goes for a vector packet processing approach using an optimized graph of packet.

02:56.660 --> 03:00.660
So also, I'd like to talk about the Calico VPP data plane.

03:00.660 --> 03:05.660
So Calico in itself is a CNI, a container network interface for Kubernetes.

03:05.660 --> 03:12.660
It allows to deploy Kubernetes cluster with additional network and security features,

03:12.660 --> 03:18.660
and it allows for seamless communication between Kubernetes workloads, maybe VM, containers, or legacy workloads.

03:18.660 --> 03:25.660
So what's nice about Calico is that it enables the use of other data planes, in this case VPP.

03:25.660 --> 03:32.660
So Calico VPP allows for the introduction of IPsec tunnels and also wire guard traffic.

03:32.660 --> 03:38.660
And it's also been in GA since December 2023, if you want to check it out.

03:38.660 --> 03:42.660
So first, I want to give an overview of the CPA pinning problematic.

03:42.660 --> 03:53.660
So currently, we have CPU pinning, which is by definition binding a process or thread to a particular CPU or at least a set of CPUs.

03:53.660 --> 04:00.660
And within the scope of network workloads, this allows us to ensure stable and optimal performance for network workloads.

04:00.660 --> 04:07.660
And you may have some workloads which might be with a single thread or they may be multi-threaded, so you want to avoid contention.

04:07.660 --> 04:10.660
And also workloads need to be quite performance.

04:10.660 --> 04:17.660
At some of them, my process hundreds of millions of packets per second and require the most out of your CPUs.

04:17.660 --> 04:20.660
So the question is, how do you select CPUs for pinning?

04:20.660 --> 04:24.660
And also, why is CPU pinning important in the first place?

04:24.660 --> 04:29.660
So of course, there are some concerns concerning the memory architecture of systems.

04:29.660 --> 04:40.660
If, for example, we have one of your network workloads which is pinned on any new node, we'll try to access the memory in another new node and you'll have additional latency.

04:40.660 --> 04:46.660
So the best practice to have is to try to pin your network workloads on CPUs in the same new node.

04:46.660 --> 04:55.660
And of course, distance also for any network interface card that you might have as if it's present on a new node and you're trying to run a network workload,

04:55.660 --> 05:00.660
which will use that device will have additional latency.

05:00.660 --> 05:03.660
So in short, try to be numer-aware.

05:03.660 --> 05:11.660
There are tools if you'd like to get your numer architecture or visualize it directly from the terminal or getting picture outputs.

05:11.660 --> 05:19.660
So some short recommendation would be to avoid pinning on Core Zero as some processes run by default.

05:19.660 --> 05:26.660
And if you can change kernel boot parameter and you want to set up system for maximum performance on benchmarking,

05:26.660 --> 05:32.660
you can try to modify some settings such as ISO CPUs to isolate Core.

05:32.660 --> 05:42.660
You can change also the affinity of some of the IRQs or in reduced kernel noise with no HZ in the score for to remove system cloutics.

05:43.660 --> 05:48.660
So what we attempt to do is see the impact of CPU pinning on one of our example workloads.

05:48.660 --> 05:55.660
So we wanted to test connection between two VPP instances that establish an IP-6 tunnel.

05:55.660 --> 06:04.660
So we run a workload locally using only virtual interfaces and which is each VPP instance using one core only.

06:04.660 --> 06:11.660
And the results were as expected, we managed with proper pinning to get the best performance around 10 gigabits per second

06:11.660 --> 06:20.660
with IP-6 tunnels and the more we went into scenarios like cross-numa or using only two core to pin for process,

06:20.660 --> 06:25.660
the two VPP and the two IPERF, this is where we observed the most performance loss.

06:25.660 --> 06:29.660
So moving into abstraction challenges.

06:29.660 --> 06:31.660
So first of all, the virtual machine.

06:31.660 --> 06:35.660
Virtual machine are popular to employ network workloads in.

06:35.660 --> 06:41.660
They allow to abstract or hardware and have multiple isolated system where we can run our network workloads.

06:41.660 --> 06:55.660
One of the main network namespaces and separate file system, but they also have Linux kernel construct that allow us to manage the resources of our containers, which is C-groups.

06:56.660 --> 07:07.660
So we're directly also able to pin the threads of our network workloads running within container using task command previous command we've seen.

07:07.660 --> 07:20.660
So if you look in particular at one controller of the C-groups, the CPU set controller, it allows us to limit the set of CPUs on the host machine which are available for a container.

07:20.660 --> 07:27.660
And when we do that, we can move towards dedicating some cores to a specific container or a specific workloads.

07:27.660 --> 07:33.660
And of course, it needs to go and turn them with isolation on the host machine of the CPUs that we're going to use.

07:33.660 --> 07:37.660
And you should watch out for difference between C-groups V1 and C-groups V2.

07:37.660 --> 07:45.660
C-groups V2 has been around a long time since 2016, but there are many systems, especially against the system, which still run with C-groups V1.

07:45.660 --> 07:51.660
And this is going to cost a mission on where you can fetch the current CPUs which are available.

07:53.660 --> 07:58.660
So I like to talk about one of the challenges we have with VPP, a concerning CPU set.

07:58.660 --> 08:11.660
So right now if you run VPP and bare metal, what we can do is rather than using task set, we can provide VPP with a list of core we'd like it to pin itself on, to spawn threads and pin itself on.

08:11.660 --> 08:13.660
And bare metal, it works pretty well.

08:14.660 --> 08:21.660
So we're going to ask it to pin itself from 0 to 3 and it's going to pin itself properly and run without any problem.

08:21.660 --> 08:25.660
But here's the challenge. We're going to introduce an abstraction which is containers.

08:25.660 --> 08:29.660
And container in this case will have a limited CPU set.

08:29.660 --> 08:35.660
We're going to ask it to only use CPUs 4 to 7 on the host machine using C-groups CPU set.

08:35.660 --> 08:38.660
And what's going to happen? We're not using task set.

08:38.660 --> 08:42.660
VPP is trying to pin itself on 0 to 3 and this is going to fail.

08:42.660 --> 08:48.660
This is going to fail the P-throw. VPP takes that into consideration and it's mapping.

08:50.660 --> 09:00.660
So again, what we learned from this challenge with the CPU set is that we should be aware of the cores that are exposed by environment.

09:00.660 --> 09:11.660
Our environment is going to expose currently available resources and at the same time we need to be aware of how application that's running within the container instance in this case fetches available cores.

09:13.660 --> 09:20.660
So similarly to our previous use case, we tried to launch an IP-seq tunnel between two VPP instances.

09:20.660 --> 09:26.660
But the twist was to introduce an additional abstraction layer which is container.

09:26.660 --> 09:31.660
In this case we used Calico VPP to route traffic between the two VPP instances.

09:31.660 --> 09:37.660
And we wanted to see if we could expect on the performance side similar performance result with abstraction.

09:38.660 --> 09:46.660
So by adding an additional hop we obtained with our IP-seq tunnels a traffic of around 9 Gbps.

09:46.660 --> 09:51.660
So compared to the 10 Gbps obtained with Burr metal, this is pretty close.

09:51.660 --> 09:58.660
We lost 1 Gbps but we gained the additional feature introduced by Calico VPP which is additional security and isolation.

09:59.660 --> 10:06.660
So to close it up, if you're thinking of switching to performant virtual network function, think about VPP.

10:06.660 --> 10:13.660
If you're thinking of adding a performant data plan to your network function and think Calico VPP if you're using Kubernetes workloads.

10:13.660 --> 10:22.660
And of course be aware of your architecture when configuring especially the layer of abstraction you're currently running your workload on.

10:23.660 --> 10:28.660
And stay tuned if you're interested in seeing more VPP. Pim will have a talk this afternoon.

10:28.660 --> 10:35.660
But we'll talk about how he managed to get hundreds of millions of packets per second with MPLS on an XPC with community hardware.

10:37.660 --> 10:42.660
So this is the test spec we used for our machines. And thank you for your attention.

10:42.660 --> 10:47.660
So right now we're assuming that a set of course is already been made available to a container.

10:47.660 --> 10:51.660
And this is going to be the role of the administrator deploying those containers.

10:51.660 --> 10:56.660
It's going to deploy them on specific human nodes. This is an assumption we need to take.

10:56.660 --> 11:02.660
If for example we have a container with a set of core that are present on a human node and set of core that are present on another human node,

11:02.660 --> 11:04.660
there's not much that can be done with this system.

11:04.660 --> 11:09.660
So we're going to take a set of core that are present on a human node and set of core that are present on another human node.

11:09.660 --> 11:14.660
There's not much that can be done with this system. So there's a need for awareness at different types.

11:17.660 --> 11:21.660
Yes, I was wondering related to kind of big little design CPUs.

11:21.660 --> 11:25.660
Is this a problem as well for pinning or did you look into this?

11:25.660 --> 11:26.660
Sorry.

11:26.660 --> 11:28.660
Like big little CPUs?

11:30.660 --> 11:31.660
No.

11:31.660 --> 11:33.660
Like the ones with performance cores and efficiency cores?

11:34.660 --> 11:39.660
You mean with the new Intel architecture with the yes, with P cores and E cores.

11:39.660 --> 11:44.660
So no, we are not considering this. We're trying to be as agnostic as possible.

11:44.660 --> 11:45.660
Thank you.

11:51.660 --> 11:53.660
This is going to be the last question.

11:56.660 --> 11:57.660
Thank you.

11:57.660 --> 12:10.660
There's the elephant in the room about hyper threading and the fact that the repeat does not perform well if it's scheduled on through on two different hyper thread cores that actually sits on the same physical core.

12:10.660 --> 12:17.660
How do you schedule workload on one core but not on the twin core?

12:17.660 --> 12:18.660
Let's say.

12:19.660 --> 12:24.660
So one of the issues that might arise if you have two cores that are on the same.

12:25.660 --> 12:36.660
If you have two threads are in the same core using hyper threading is that for example, if they're going to deal with the same packets, they might deal with the same packets of cash information and this might create some contention.

12:37.660 --> 12:39.660
Yeah, this will definitely create some contention.

12:39.660 --> 12:41.660
So it's a specific use case.

12:41.660 --> 12:44.660
This is fine if you're only going to read information, same cash line.

12:44.660 --> 12:48.660
But if there are rights, there's definitely going to be some contention and lock and slow down.

12:48.660 --> 12:49.660
Thank you very much.

12:49.660 --> 12:50.660
Thank you again.

12:54.660 --> 12:55.660
Thank you.

13:24.660 --> 13:26.660
Thank you.

13:26.660 --> 13:28.660
Thank you.

13:28.660 --> 13:30.660
Thank you.

13:58.660 --> 14:00.660
Thank you.

14:28.660 --> 14:30.660
Thank you.

14:58.660 --> 15:00.660
I just changed it.

15:00.660 --> 15:04.660
It's a slide also here.

