WEBVTT

00:00.000 --> 00:10.120
Very much. So our next person is Ahel Boroy from Highland that's going to talk to us about

00:10.120 --> 00:13.520
using generative AI and content service platforms together.

00:13.520 --> 00:18.120
Thanks. I was on. I was on.

00:18.120 --> 00:19.280
I was checking the microphone.

00:19.280 --> 00:26.320
Okay, yep. Welcome to everyone. So this is another view on the same topic. So we are

00:26.320 --> 00:33.160
going on the technical side now. It's not like a final feature for a product, but it's

00:33.160 --> 00:38.880
a framework in order to help you to build all the features that we were seeing before

00:38.880 --> 00:45.320
in the context of a content service platform or a document. Okay, so we are going to review

00:45.320 --> 00:52.280
some in a stack. The next step we are going to use that is including also LLN on premise.

00:52.280 --> 00:57.240
We are going to review all the options. We are going also to describe the features we

00:57.240 --> 01:03.040
can build with this stack. And then we are going to review how to integrate that with

01:03.040 --> 01:08.400
your in our case because I'm working for Highland and we are building an open source product

01:08.400 --> 01:12.920
with the name of Fresco that is related to content management. So we are going to see

01:12.920 --> 01:18.920
how to integrate that with that content management platform and also just looking a few to the

01:19.080 --> 01:25.440
future. And obviously I need to include some AI picture because it is what it is. Anyway,

01:25.440 --> 01:32.440
so this GenAI stack that we are using includes mainly three components. So the first one

01:34.360 --> 01:41.360
is Olama. Olama is a service that is able to provide an API to interact with different

01:42.360 --> 01:49.360
LLMs. We are going to see later all the list but you can download your LLN on premise and

01:49.680 --> 01:55.080
this layer is providing the interaction with the LLM. You can even interact with different

01:55.080 --> 02:02.080
LLMs at the same time. The second one is Neo4j. So Neo4j is the vector database when

02:02.760 --> 02:09.760
you are using rack, we augmented reality and so on. Then you need to increase the information

02:10.760 --> 02:17.760
for the LLM. So you are storing all this information in this database. And finally we are using

02:17.760 --> 02:24.760
land change. So this framework is providing land change that is a framework to communicate

02:25.840 --> 02:30.440
all these different elements. So this framework is in Python but if you are not comfortable

02:30.440 --> 02:37.440
with Python there are many other languages that are including this kind of piece. Okay,

02:37.800 --> 02:44.300
so mainly what we have, if someone doesn't like Docker there is no problem so you still

02:44.300 --> 02:51.300
can deploy that without it but it is oriented to services. So you have Olama that is the

02:51.300 --> 02:58.300
one providing the services for the LLM that can be used in GPU or not. So you can, we

02:58.320 --> 03:04.440
are going to run this without the GPU. Just using my regular CPU on my computer. This

03:04.440 --> 03:09.980
is lower. I recommend you to use a GPU but you can do that. And we are piling all the

03:09.980 --> 03:16.980
models that we need so we can just use more than one model. With that we can increase

03:17.840 --> 03:24.840
the information for the operation with that Neo4j database and we can develop an API with

03:25.680 --> 03:32.180
this string LLM, with this framework. Okay, so these are the pieces. You have the project

03:33.180 --> 03:40.180
Docker.gen.a stack. Mainly these pieces is a sample. This sample is oriented to prompting.

03:41.500 --> 03:46.580
You have to reply questions. So we are going to do something a bit different from that

03:46.580 --> 03:53.580
but the first sample you can try is this one. Okay, so all the LLMs that are able to manage

03:53.860 --> 04:00.860
Olama today are these ones. Likely as this is growing every day there are more. But this

04:03.580 --> 04:09.140
was like last week. Okay, so this is what you need to understand. Obviously the larger

04:09.140 --> 04:16.140
the better but you need to take into account your resources. So these are very small. Your

04:16.380 --> 04:22.980
4 gigabytes of RAM and 2 gigabytes of storage. So you can run that on a computer and then

04:22.980 --> 04:28.940
if you want to use something that is better is also larger in resources. And you can even

04:28.940 --> 04:35.940
use these LLMs that require, I don't know, many different computers once I say, okay.

04:37.900 --> 04:44.900
So today we are going to use this kind of LLMs. Also it's relevant to look at the license.

04:46.980 --> 04:52.100
So just was talking before about the license. So this is also something relevant if you

04:52.100 --> 04:56.700
want to build something commercial or something open source or whatever. You need to take care

04:56.700 --> 05:03.700
of the license. Also you can look that there is some weight license there because you have

05:04.220 --> 05:10.140
this LLM2 community license agreement that some people say that this open source, some

05:10.140 --> 05:15.780
other people say that it is not. So it's something different. So better to check if you don't

05:15.780 --> 05:21.780
see a patchy license or something that you can recognize. Better to check the conditions.

05:22.660 --> 05:29.660
So you have a lot of them to choose. We are going to work today on the demo with Mistral,

05:30.660 --> 05:37.660
CementB, that is a French company that is producing this kind of LLMs that are more or less the

05:39.100 --> 05:46.100
same performance as GPT 3.5. So it's good enough. And so what is open source, the LLM is free

05:46.860 --> 05:53.860
to download and to use, but the training data is not free and likely it has some copyright

05:56.700 --> 06:02.700
material on it. We don't know because it's not free. So on the next law ethical AI writing

06:02.700 --> 06:09.700
we have, sorry, yellow. I thought it was orange but it's yellow. Okay. It's more or less fine.

06:10.700 --> 06:17.700
So we are just only missing one. That was for text and for pictures. We know some LLM

06:20.900 --> 06:27.900
with a visual encoder on it. So for this part we are going to use lava. And lava really

06:27.980 --> 06:34.980
is granting all the different requirements. So we are using a green LLM for this other

06:35.220 --> 06:42.220
sample. Okay. Perfect. So all the demo is running on my computer while I'm there in the presentation.

06:45.140 --> 06:52.140
So I have everything running inside is 32 gigabytes of RAM and is AM64 architecture.

06:52.460 --> 06:59.460
So it's not AMD. It's MacBook Pro two years ago, something like that. Okay. As we were

06:59.740 --> 07:06.740
also reviewing the previous version before this GEN AI momentum, we also had some data

07:14.780 --> 07:19.780
section, test recognition, test classification, content analysis. Anyone is using content

07:19.780 --> 07:26.780
analysis for a real use case? Okay. It was not me. So it's something but you saw. But

07:26.780 --> 07:32.580
we have all the things, right? Some kind of automation. But now with the GEN AI, we have

07:32.580 --> 07:39.580
also a power classification. We could classify in the past. But now we can classify better.

07:40.820 --> 07:45.500
We can also, and when I say translate, we are going to see later the demo. Obviously

07:45.500 --> 07:51.500
we can translate. But we can also interact with the LLM in one language and to get the

07:51.500 --> 07:56.860
response in another language. Right? So that is the difference. We can also summarize

07:56.860 --> 08:02.700
a test. This is the most common use case and we can describe a picture. Prompting. Obviously

08:02.700 --> 08:09.700
we can use prompting. We can read that. So we have some new features that we can use in

08:10.340 --> 08:17.340
our documents. Okay. We are going to see some of them implemented. Okay. So what is this

08:17.340 --> 08:24.340
project about? It's not yet. Okay. The project is at some point of the slides. Okay. If not,

08:28.700 --> 08:35.700
I will give you the link. So in this project, what is created is a API by using this, all

08:36.380 --> 08:43.380
these infrastructure in order to provide different services. What we are using is some LLM embeddings.

08:43.460 --> 08:50.460
So we are just trying to avoid hallucinations. Just giving some additional information to

08:50.460 --> 08:54.460
the database from the document. So we are working with a document. Right? So we are

08:54.460 --> 08:59.460
not going with search. We are not going with some other applications of GNI. So we are

08:59.460 --> 09:06.460
focused on features of a document. So we are adding all that information so we can get

09:07.340 --> 09:14.340
a better response and more suitable to the document we are dealing with. And for that

09:16.860 --> 09:23.380
we are using Mistral. And if we are talking about a picture, then we can use the other

09:23.380 --> 09:30.380
LLM that was Java in order, for instance, to describe or classify the picture. We have

09:30.660 --> 09:36.980
also some, so we can choose the LLM. If you want to choose some other LLM than Mistral,

09:36.980 --> 09:42.780
you can do that for text. You can choose some other LLM with a vision and color enabled,

09:42.780 --> 09:49.780
like Java or some other on the list. And we can also choose the language. So we are going

09:49.860 --> 09:56.060
to see that later. We can just drop a document in Japanese and we are getting the summary

09:56.140 --> 10:03.140
in English or in the other side. Right? And also you can choose some numbers like the

10:03.220 --> 10:09.620
summary size or the number of tasks and so on. So these are parameters. Okay. So this

10:09.620 --> 10:16.620
is the API. Right? Pretty cool invocations. But let's see that leave. As always is better.

10:17.620 --> 10:24.620
Can you see the, better? Okay. Okay. So for instance, I'm going to work, let me find the,

10:35.060 --> 10:39.860
I'm going to work with this document. Right? I could be using an English document, but

10:39.860 --> 10:45.860
it should be easier for the AI. So we are using this one. And I'm also going to use

10:47.620 --> 10:54.620
this picture. So for your reference. Okay. Okay. Perfect. So for this document, we are

10:55.780 --> 11:01.780
going to ask for a summary. So give me a summary of this document that is in Japanese. So with

11:01.780 --> 11:08.780
that, if I'm able to. Okay. So this is running on my computer. So I have this ENAI stack

11:09.780 --> 11:16.780
running in this Docker deployment. And I'm getting the request. Okay. And with that, I'm

11:18.220 --> 11:25.220
getting the answer. Okay. So the test, this is a problem with kindergarten, in Japan,

11:27.700 --> 11:32.700
blah, blah, blah. Okay. That's fine. So I'm giving something in Japanese and I'm getting

11:32.700 --> 11:39.700
the summary in English. The second one, come on, note this one. I did it. Okay. The second

11:44.020 --> 11:51.020
one is just to classify. Classify a document that picking a term of a list of terms. So

11:53.260 --> 11:59.460
I want you to classify this document according to Japanese, Spanish or Vietnamese. Again,

11:59.500 --> 12:06.260
it's an easy example. Right. But you can choose whatever list of values. So if I say just

12:06.260 --> 12:13.260
classify this document into one of these three categories, the term is Japanese because the

12:14.500 --> 12:21.500
document is in Japanese. Okay. This is also a Revan for classification. And finally, we

12:21.980 --> 12:28.980
can also make some prompt on the document. What is the name of the zone or this document

12:29.540 --> 12:35.140
in Japanese document? The name of the zone is Musoku. Okay. So three different features

12:35.140 --> 12:42.140
that we can use on this, on a document. You can build more. Again, it's a Python, Python

12:42.380 --> 12:49.300
program with these three specific features, but you can grow up to include something else.

12:49.300 --> 12:55.220
And if we move to the, to the pictures that was for text, but for the pictures, we can

12:55.260 --> 13:02.260
describe this, this picture. We can also extract some, this is a person, this is, but that

13:03.460 --> 13:09.460
was done before. But describing is the, the, the new thing that GNI is providing for us.

13:09.460 --> 13:15.460
This is a bit slower, but in the end, they made so some man posting for the camera. He's

13:15.460 --> 13:22.460
wearing a green beanie, glasses, a black hoodie. And the land yall says air fraked. Well, no,

13:22.460 --> 13:29.460
it's a fresco, but more or less. Okay. The picture was not big enough, but it's fine.

13:31.020 --> 13:38.020
It's, it's something that is, is useful. And it's not that consuming internal resources,

13:38.060 --> 13:44.180
because it's running in, on my machine. So it's, it's fair enough. Okay. Once that we

13:44.180 --> 13:51.180
have all these features, and we have this, Python, just let me show you a bit. So this

13:51.980 --> 13:58.980
is the project, right? You have the Aeboroi, a fresco GNI, and you have the GNI stack,

14:00.380 --> 14:07.380
and mainly it's a Python program. Okay. With all these endpoints described, classified,

14:10.180 --> 14:17.180
prompt, and somebody. Okay. It's no more than that. Okay. If we go back to the original

14:18.180 --> 14:25.180
goal, is to integrate this kind of operations with our, with our product than in our case

14:27.260 --> 14:34.260
is a fresco. So a fresco, we can deploy that also in Docker or whatever you want. And we

14:34.660 --> 14:39.300
have two different APIs. So the first one is the classical press API. And the second

14:39.300 --> 14:46.300
one is a messages API, synchronous and asynchronous. So if we have existing content in the repository,

14:47.300 --> 14:53.660
you have a folder with 100 pictures, and you want to describe that. So you can use the

14:53.660 --> 14:59.980
recipe. Yes, to get the document, apply the operation, and update the document. And that's

14:59.980 --> 15:06.220
fine, because you can make a batch with that. Okay. You have all the operations available.

15:06.220 --> 15:12.780
And if you want to create that like more dynamically, when the people drops the document, yes, perform

15:12.780 --> 15:17.900
the action, then you have the messages API, the asynchronous API. So you can listen to

15:17.900 --> 15:21.580
the event, okay, there is a new picture, and this picture needs to be summarized. I'm going

15:21.580 --> 15:28.580
to summarize the picture, and that's updated. Okay. So these are the two different patterns

15:28.580 --> 15:34.780
we can, we can apply for it. What we are going to see now, again, live, everything is running

15:34.780 --> 15:41.780
on my laptop, just believe me, is something that allows us to classify a document. So

15:43.420 --> 15:50.100
we are going to upload a document. We are creating this rule. The rule is the same just

15:50.100 --> 15:57.100
for you to make the similarity with what is before. So we have a list of languages, Japanese,

15:57.820 --> 16:03.660
Vietnamese, English, whatever. And we are creating a rule to move the document to the right

16:03.700 --> 16:10.700
folder. So you draw a path document, and the document is moved to the right folder. Okay.

16:10.700 --> 16:17.700
Okay. So let's do that. Okay. Let's open a fresco. So there is a folder at some point.

16:34.660 --> 16:41.660
And this folder has a rule that is classifying the documents that I'm dropping on it. Okay.

16:44.900 --> 16:51.900
So if I, for instance, come for classify, no, for classify things, we are going to try

16:53.780 --> 17:00.780
with a Vietnamese one. It has to be a bit creative. Okay. Okay. So at this point, a fresco

17:01.100 --> 17:08.100
is listening to this new document, and it's classifying the document. So it's just selecting

17:10.380 --> 17:17.380
a term from the list of terms, and the document has been updated. So it has been classified.

17:17.820 --> 17:24.820
So if I refresh, what I find is that the document is on the Vietnamese folder, and you can do

17:25.820 --> 17:32.820
that with invoices, with whatever you want. And we can track that it was mistral, the LLM,

17:34.340 --> 17:41.340
that created this classification. Okay. Pretty easy, right? So you can integrate also all

17:41.420 --> 17:48.420
the other operations in that to get some automation. Okay. So I guess that I was running out. But

17:48.420 --> 17:55.420
no problem. So we have more time for questions. So again, this is a simple framework. You

17:57.180 --> 18:04.180
can deploy that on premise. You can choose your LLM. You have an initial REST API for

18:05.700 --> 18:12.700
operations. Public works are welcome. And then you need to integrate that with your product,

18:12.700 --> 18:19.700
with your organization, or whatever. Right. There is also some interesting hackathon with

18:19.820 --> 18:26.820
more use cases. So I presented you some use cases, but you have more of them on this hackathon.

18:26.820 --> 18:33.820
The slides will be, they are available on the, on Foxen. Okay. And also I'm using Olamma,

18:34.500 --> 18:38.620
but there are many other alternatives. You don't need to choose Olamma. So you have GPT4

18:38.620 --> 18:45.620
all locally. This solution is the one used by, by next cloud, second state, high-end

18:47.700 --> 18:54.700
phase is the most known probably. But again, just, this is an initial framework. Take it

18:54.740 --> 19:01.740
as it and try some things with, with the NAA. Okay. That was all. Thanks.

19:01.740 --> 19:08.740
Thank you very much, Angel. Are there any questions?

19:16.340 --> 19:19.340
I'm going to do it in the order of the rule.

19:19.340 --> 19:26.140
Thank you, Angel. It seems to me all these operations are on one picture or one document.

19:26.140 --> 19:31.140
Are you also considering me asking a question on all my documents?

19:31.140 --> 19:37.820
No. So this, this sample is only for a single document or a single picture. But, but that

19:37.820 --> 19:44.820
is as easy as you have the database, the Neo4j database, then you can include as information

19:46.140 --> 19:51.940
as you want for a single document or for a single query. Right. So what I'm doing in

19:51.940 --> 19:58.940
the source code is to remove the previous information. You have to create something that is only for

20:00.020 --> 20:06.300
a single document. But you can modify that in order to add more than one document to

20:06.300 --> 20:13.300
one query. But on the sample is only for a document or a picture.

20:14.620 --> 20:21.620
While summarizing the Japanese PDF, why did you need to provide for context your picture

20:22.300 --> 20:22.940
Sorry.

20:22.940 --> 20:28.940
You showed the summarization of the Japanese PDF.

20:28.940 --> 20:29.940
Yeah.

20:29.940 --> 20:32.420
And then you provided for context the picture.

20:32.420 --> 20:38.060
No, no, the picture was for the last operation. So the three first operations for summarize,

20:38.060 --> 20:43.500
for classify and for prompting were related with the document in Japanese. I could use

20:43.500 --> 20:47.220
some other document. I know, but I love the document because I'm using this for testing

20:47.220 --> 20:53.740
for 15 years, something like that. So it's like my, my precious document. And, and the

20:53.740 --> 20:58.580
picture was there for the last one. It was the description of that picture that is more

20:58.580 --> 21:02.420
or less like, like yours then.

21:02.420 --> 21:06.620
Thank you.

21:06.620 --> 21:12.020
Similar to the previous question that I had, but for a single document, right. So the summarization

21:12.020 --> 21:13.380
for very large documents.

21:13.380 --> 21:14.380
Yeah.

21:14.380 --> 21:21.380
So, the problem is that again, I'm running on my lap. So I cannot use like a very large

21:24.660 --> 21:29.100
document, but I was just trying to summarize, for instance, books. Do you know the Gutenberg

21:29.100 --> 21:33.900
project? On the Gutenberg project, you have all the classics of Alice in Wonderland and

21:33.900 --> 21:40.060
so on. So I was trying to do that with that kind of documents. And it's able to do that,

21:40.180 --> 21:47.180
it takes a while, like minutes on my machine. Again, if instant adjusin, the regular CPU,

21:47.420 --> 21:54.420
you use a GPU, the tiny slide, I don't know, 100 faster, something like that. So I don't

21:55.020 --> 22:00.300
know. I need to make serious test with that. But having the right infrastructure, I guess

22:00.300 --> 22:07.300
that the, the performance is enough. It's not something like very instantaneous, right?

22:08.180 --> 22:11.620
But you can work with it.

22:11.620 --> 22:16.620
Thank you very much. Any other questions?

22:16.620 --> 22:17.620
Yes.

22:17.620 --> 22:24.620
Hi. A follow up on the previous question. Was the insertion into the vector is database

22:24.620 --> 22:30.180
taking a lot of time or was the actual query to the LLM? Because the insertion into the

22:30.180 --> 22:37.180
vector is database has to be done once, whereas the query can be done multiple times if, if

22:37.380 --> 22:39.420
you already vectorize the document, right?

22:39.420 --> 22:44.900
Yeah. So again, I was not trying to deliver a session on how to develop AI, right? It

22:44.900 --> 22:50.860
was just to create a framework. You have the AI track that can reply to you better than

22:50.860 --> 22:56.060
me in relation to that. But yeah, obviously, you can use the database. I'm not, I'm only

22:56.060 --> 23:00.460
using the database for a context of a single document, right? So you can create categories,

23:00.460 --> 23:06.620
you can add more than one document. You can add also the, the links to the response and,

23:06.620 --> 23:13.460
and so on. So yeah, sorry. Maybe I didn't understand you.

23:13.460 --> 23:17.340
Maybe you misunderstood my question. My question was when you added the Alice in Wonderland

23:17.340 --> 23:23.140
book, was it the vectorization that took time or was it the query to the LLM?

23:23.140 --> 23:27.260
No, no, it was vectorization, vectorization of the chance of the document.

23:27.260 --> 23:28.260
Okay. Sorry.

23:28.260 --> 23:30.500
That was the only one question.

23:30.500 --> 23:36.460
I'm not an expert, but I know a bit.

23:36.460 --> 23:42.700
Any other question? Okay.

23:42.700 --> 23:43.700
Thanks. Okay.

23:43.700 --> 23:44.700
One more question.

23:44.700 --> 23:45.700
Last one.

23:45.700 --> 23:51.100
I'll be around. So if someone just wants to, to catch me.

23:51.100 --> 23:56.420
Can you say a bit more about like the biggest use cases you see and if there's any open source

23:56.420 --> 24:01.220
setups of this that are out there for us to look at?

24:01.220 --> 24:06.980
In my opinion, the, the main use case of that is searching.

24:06.980 --> 24:10.420
So but this is a different world with different beasts.

24:10.420 --> 24:16.220
So but for searching AI, it's really quite relevant.

24:16.220 --> 24:25.820
So again, this is just to create a framework and then it's just to apply your imagination.

24:25.820 --> 24:26.820
Thank you very much, Angel.

24:26.820 --> 24:27.820
Thanks.

24:27.820 --> 24:28.820
Thank you.

24:28.820 --> 24:29.820
Thank you.

24:29.820 --> 24:30.820
Thank you.

24:30.820 --> 24:31.820
Thank you.

24:31.820 --> 24:32.820
Thank you.

24:32.820 --> 24:33.820
Thank you.

24:33.820 --> 24:34.820
Thank you.

24:34.820 --> 24:35.820
Thank you.

24:35.820 --> 24:36.820
Thank you.

24:36.820 --> 24:37.820
Thank you.

24:37.820 --> 24:38.820
Thank you.

24:38.820 --> 24:39.820
Thank you.

24:39.820 --> 24:40.820
Thank you.

24:40.820 --> 24:41.820
Thank you.

24:41.820 --> 24:42.820
Thank you.

24:42.820 --> 24:43.820
Thank you.

24:43.820 --> 24:44.820
Thank you.

24:44.820 --> 24:45.820
Thank you.

24:45.820 --> 24:46.820
Thank you.

24:46.820 --> 24:47.820
Thank you.

24:47.820 --> 24:48.820
Thank you.

24:48.820 --> 24:49.820
Thank you.

24:49.820 --> 24:50.820
Thank you.

24:50.820 --> 24:51.820
Thank you.

24:51.820 --> 24:52.820
Thank you.

24:52.820 --> 24:53.820
Thank you.

24:53.820 --> 24:54.820
Thank you.

24:54.820 --> 24:55.820
Thank you.

24:55.820 --> 24:56.820
Thank you.

24:56.820 --> 24:57.820
Thank you.

24:57.820 --> 24:58.820
Thank you.

24:58.820 --> 24:59.820
Thank you.

