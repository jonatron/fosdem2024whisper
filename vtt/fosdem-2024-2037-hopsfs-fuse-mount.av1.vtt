WEBVTT

00:00.000 --> 00:10.640
Good morning everyone. Thank you all for joining the first session of today. You're at the

00:10.640 --> 00:15.280
software defined storage dev room. We'll have storage talks throughout the day until I think

00:16.080 --> 00:25.600
seven tonight. Fabio is here to start with his first talk. Enjoy. Thank you very much.

00:26.560 --> 00:28.080
Thank you.

00:30.560 --> 00:36.480
Yeah, welcome everybody. I'm Fabio. My colleague Salman was supposed to be as well. Unfortunately,

00:36.480 --> 00:41.520
he couldn't make it, which is really a pity because he's the guy who actually built most of

00:41.520 --> 00:47.920
this stuff. So yeah, but now you've got me. So yeah. Well, I'm going to talk about Observice

00:48.000 --> 00:53.680
a few months, what it is and what we built it and how we use it essentially.

00:54.960 --> 00:59.360
Before doing that, I just want to give you a little bit of a context on what is Observice in

00:59.360 --> 01:03.920
general and what it is used. I cannot have a feeling that not a lot of people know about it.

01:04.880 --> 01:13.040
So Observice is effectively a patchy HFS compliant distributed file system. So the code base was

01:13.040 --> 01:18.080
forked out of the patchy HFS and we made a couple of different changes over the course of the years.

01:19.440 --> 01:24.720
Some significant, some less significant, but ultimately there is mostly an architectural

01:24.720 --> 01:30.800
change and also some security differences between what is a patchy HFS and Observice.

01:30.800 --> 01:38.240
Ultimately, the APIs are the same. So the clients are compatible. So if you look at the

01:39.200 --> 01:45.360
actual implementation architecture of Observice, you can actually recognize some of the

01:46.080 --> 01:54.560
processes, some of the entities that you can also find in a patchy HFS. The difference is that

01:54.560 --> 01:58.800
the behave is slightly different. So in HFS and Observice you have the name notes. Those are

01:58.800 --> 02:04.400
responsible for interacting with the clients knowing the state of the file system, responding

02:04.480 --> 02:10.480
to the clients, giving access to the files and so on. The difference between what is a patchy HFS

02:10.480 --> 02:15.360
and Observice is that Observice, we took the metadata information, so the structural file

02:15.360 --> 02:20.400
system directories where the blocks are located, permissions and so on, and we moved it outside

02:20.400 --> 02:25.840
the memory of the of the name note process into the into what we call RONDB, which is essentially

02:26.320 --> 02:33.600
a distributed in-memory database that was forked out of MySQL and DB cluster.

02:35.040 --> 02:39.600
Specifically, MySQL and DB cluster, MySQL and DB cluster, if you're not familiar, is not a cluster

02:39.600 --> 02:44.400
of MySQL, but it's actually a different storage layer as well. So it's basically a distributed

02:44.400 --> 02:49.520
in-memory layer that basically stores the processed data, and you can do like really nice things.

02:49.600 --> 02:54.880
You can do like scaled up and down online. You can do upgrades and stuff like this,

02:54.880 --> 03:03.280
and it's in memory, so it's extremely fast. So moving this metadata information from the

03:03.280 --> 03:08.560
actual names on to the database allows us to basically have effectively stateless

03:09.760 --> 03:13.360
name-node dumps of S. So we can have as many as we want. We can take them down. We can

03:14.000 --> 03:19.840
stuff new ones, allows us to basically be way more flexible in terms of operations,

03:19.840 --> 03:26.560
in terms of management of the entire file system. On the bottom part, we have to say

03:26.560 --> 03:32.000
data management layer. Here we have process like the data nodes, right? It's the same concept in

03:33.040 --> 03:38.080
HFS. Data nodes are responsible for receiving streams of data from clients and storing them

03:38.080 --> 03:43.280
in local file system, and then give it back essentially, right? To deal with that kind of

03:43.280 --> 03:50.160
processes. Here we actually made some notifications, some architectural changes, in a sense that

03:50.720 --> 03:55.760
depending on where you deploy OpsFS, depending on whether you're deploying on-prem or in the

03:55.760 --> 04:02.960
cloud, you can actually decide to store the actual blocks on normal disks, like on the machines,

04:02.960 --> 04:08.320
but you can actually store them also on the cloud object stores. So you can store them on

04:08.880 --> 04:15.440
S3, you can store them on GCS, you can store them on Azure, Block Storage, and so on. So the data

04:15.440 --> 04:20.480
nodes themselves also become stateless essentially, right? And in this case, they act, they have two

04:20.480 --> 04:24.240
roles. They have a role of basically, yeah, not breaking the protocol essentially of the

04:24.800 --> 04:31.200
HFS protocol client, and also interacting as a cache, right? So every time you write something

04:31.200 --> 04:36.720
and then this is flushed down to the storage layer, so to the object stores, but it's also

04:36.720 --> 04:41.200
cached into the data node process themselves, so that if the clients keep writing and reading

04:41.200 --> 04:46.080
the same files, then obviously you don't have to go to the object stores, improving performance quite

04:46.080 --> 04:53.440
a lot. And while the last changes we made around security, and we're going to talk about a little

04:53.440 --> 05:01.120
bit later, but essentially that's OpsFL in a nutshell. Where it is used? Well, OpsFL is

05:01.360 --> 05:07.120
used as part of the broader platform which is called OpsWorx. OpsWorx is essentially a data

05:07.120 --> 05:12.000
platform for machine learning. It allows you to basically provide a bunch of different things

05:12.000 --> 05:16.640
which probably you're not interested in, to basically manage what we call features in machine

05:16.640 --> 05:20.160
learning. So essentially, you take a bunch of data for a bunch of different data sources, you

05:20.160 --> 05:26.400
extract signals to train a model on, and then you're basically storing those signals because

05:26.400 --> 05:31.440
the feature engineering process, the signal extraction process is quite compute intensive,

05:31.440 --> 05:37.360
so you want to store them and reuse them across multiple models and so on. And we have a dual

05:37.360 --> 05:42.320
system architecture, we have offline data for more like training new models and doing like

05:42.320 --> 05:47.600
batch inference, and then we have some online data for doing real-time operations. The OpsFL

05:47.600 --> 05:53.440
part is actually this, it's powering the offline feature store part, which is basically saying

05:53.520 --> 05:59.280
store all the data, historical data across many, many years across more models that is being used

05:59.280 --> 06:05.680
for training additional models and so on. Now, the problem that we're facing and the reason we

06:05.680 --> 06:11.360
started working on the on the on the on the fuels process then for OpsFS is the following.

06:11.920 --> 06:18.560
So the entire platform, the OpsFL feature store platform stands basically in between two different

06:18.560 --> 06:23.120
worlds, should we say, right? So on the other side we have like more traditional data engineering

06:23.120 --> 06:28.320
world where we have like application like Spark, Flink, maybe Beam and so on, which they have been

06:28.320 --> 06:36.960
built from day one essentially, supporting the OpsFS, like the HFS API, supporting the OpsFS

06:36.960 --> 06:42.720
API as well, right? So you can plug it in and they work essentially, right? On the other side,

06:42.720 --> 06:46.800
on the consumer side of the platform, well you have the entire data science world, right? And

06:46.800 --> 06:53.120
that's mostly basically built on top of Python libraries and you know, like they have been built

06:53.920 --> 06:57.920
relying on the fact that it is a log of our system and they don't necessarily interact with these

06:57.920 --> 07:03.120
different systems, whether it's Apache CFS, but also they don't maybe interact even with like

07:03.120 --> 07:07.520
object store or stuff like this, right? They only maybe read data from a local fast system, right?

07:08.160 --> 07:13.040
So we can actually have, if you take the libraries on the left, so if you take the libraries that

07:13.040 --> 07:17.920
are, let's say, data science library and generally the science processes, we can actually have, you

07:17.920 --> 07:24.560
know, two separate scenarios, right? We have a scenario where the libraries actually support

07:24.560 --> 07:34.560
what's called like Liby HFS. Liby HFS is essentially a wrapper for the, it's a JNI wrapper for the

07:35.360 --> 07:41.120
clients for the fast system and that allows us basically to actually interact with the fast system

07:41.120 --> 07:46.000
from a bunch of different libraries like PyArrow, Pandas and a bunch more actually do support

07:47.120 --> 07:54.480
reading from the, reading from HFS or OpsFS through the Liby HFS library. The problem is

07:55.680 --> 08:02.240
that even this scenario still requires access to a JVM, right? And in a world of their science where,

08:02.240 --> 08:07.360
you know, people do, like used to do like pip install a library and that then looks like a bunch of,

08:07.360 --> 08:12.560
like zip file and extracting on your local laptop, you know, configuring the JVM, bringing down the

08:12.560 --> 08:19.200
JVM, bringing down the OpsFS clients or HFS clients, it's quite, it's quite cumbersome, right?

08:19.200 --> 08:25.040
So that, that's kind of, it works but it's not the ideal case scenario, right? The other kind of

08:25.040 --> 08:31.520
libraries or tools that are essentially in the other campus, say, are tools that do not support

08:31.520 --> 08:36.240
Liby HFS at all. So one of the functionalities that you have in OpsFORX is basically, well,

08:36.240 --> 08:41.840
I want to clone a Git repository in OpsFORX so that we can actually run it and, you know, you can't

08:41.840 --> 08:47.440
just run Git on top of HFS or OpsFS, right? So there isn't really like a nice solution there.

08:48.240 --> 08:55.360
And so we started working on implementing a FuseFile system for OpsFS and we actually built it on top

08:55.360 --> 09:00.320
of some of the existing work that is open source and it's out there. All the work that we built on

09:00.320 --> 09:06.480
top is also open source. So the libraries and the entire application is written in Go. So that obviously

09:06.480 --> 09:11.760
removes the need for the JVM and it's built on basically three different projects. The first one

09:11.760 --> 09:19.040
is the FuseLibratesOff, so implemented interface for the FuseFile system. The second one is the

09:19.040 --> 09:26.720
OpsFS Go client, which is actually built on the HFS project from Collymark, which essentially

09:26.720 --> 09:33.600
implements the entire protocol for communicating with the HFS or OpsFS but using Go, right? So you

09:33.600 --> 09:39.120
can actually interact with the file system without the need of the JVM and then basically bring everything

09:39.120 --> 09:46.400
together in the OpsFS Go mount project, which is actually a forked from a project that Microsoft

09:46.400 --> 09:51.840
stopped working on, the HFS mount project. By the time we forked and started working on it,

09:51.840 --> 09:57.360
it was pretty much a POC. I think it was only allowing like read operation or something

09:57.360 --> 10:02.400
and then some on the team kind of expanded really nicely to be able to support much more.

10:03.680 --> 10:09.920
Now, the implementation of this solution actually has a bunch of different challenges

10:09.920 --> 10:15.520
and there are two essentially group of challenges, right? One is the API difference, what is the

10:15.600 --> 10:23.200
API that OpsFS and HFS actually provide and what the API that, you know, the POSIX API that

10:23.200 --> 10:30.160
are required by tools like Git and so on. And then the second one is the different,

10:30.160 --> 10:33.840
complete different security model, right? So we're going to look at how we bridge the gap between the

10:33.840 --> 10:41.440
two in the implementation essentially, right? So the first part is HFS is append only, right? So

10:41.440 --> 10:46.880
when you create a file, you cannot go in the middle of the file and add additional content,

10:46.880 --> 10:52.000
right? You can only append things to the file, right? And so this doesn't work if you open

10:52.000 --> 10:56.160
Veeam and you, you know, editing stuff around, then you can't actually just, you know,

10:57.280 --> 11:03.520
like write directly to HFS OpsFS. As I said, it doesn't support random writes,

11:03.520 --> 11:07.760
it does not support multiple writers. So when you open a file for write, you actually get the

11:07.840 --> 11:12.880
list on the file and you have the only one that can actually write there, right? And nobody else

11:12.880 --> 11:19.440
can actually write, which is not the behavior that you expect on, yeah, like from the POSIX API.

11:20.160 --> 11:26.160
The other aspect is the block size is quite huge. So that's like, you know, the blocks on

11:26.160 --> 11:32.000
OpsFS and HFS are configurable. You can make it as small as possible, but like the default

11:32.000 --> 11:37.520
behavior is to be quite big. It's like 6428 megabytes. Because the axis button and the

11:37.520 --> 11:42.400
write buttons make it that, it's much more performant to do that. And then we're going to talk

11:42.400 --> 11:47.440
about the security model in a second, right? So how does, how does the system work, right?

11:47.440 --> 11:52.160
So we have two scenarios, essentially. We have read scenario, read only mode, or like read and

11:52.160 --> 11:58.560
write scenario. For the read only mode, it's actually quite trivial. What basically happens

11:58.560 --> 12:01.680
actually, I can show you this diagram. I don't know if you can actually see from the back.

12:02.240 --> 12:07.920
But essentially what happens is that the process actually talks to the OpsFS

12:08.880 --> 12:15.360
fuse mount and asks, I want to read this file. Because the API from HFS and OpsFS allow you

12:15.360 --> 12:20.400
to basically, you know, do six and read and stuff like this. They are compatible. We can just forward

12:20.400 --> 12:27.200
the request to the remote storage essentially, right? So you open a file, you open a file,

12:27.280 --> 12:33.600
you read a set amount of bytes from a set amount of position. That is mapped to operations directly

12:33.600 --> 12:39.840
in OpsFS. So that's, yeah, pretty, pretty, pretty trivial to implement.

12:42.080 --> 12:48.320
The writing scenario is a little bit more complicated for the reasons I was mentioning earlier.

12:48.960 --> 12:55.040
So at the i level, what happens is that the remote file gets copied on the local file system.

12:55.040 --> 13:01.280
And we write it in a staging area. And so the processes like the Linux processes are actually

13:01.280 --> 13:09.120
going to write to these, to these like staging replica of the file. And then when either the

13:09.120 --> 13:13.520
file is flushed or the file is closed, then we actually upload the file back into the remote

13:13.520 --> 13:19.920
storage. So it looks like a little bit like this. You open the file and we open the file.

13:20.560 --> 13:25.680
You actually get back, you don't get, we don't download the file immediately, even if you ask for

13:25.680 --> 13:33.200
writing and open the file in writing. And the most, like, the reason is people are lazy essentially,

13:33.200 --> 13:37.280
right? In the sense of like when they open a file, like you say, you want to check out,

13:37.280 --> 13:41.040
you want to look at the file, you know, you might have opened Vim and open a file. And then at that

13:41.040 --> 13:45.760
point, you know, you don't want to write anything, but you have opened it, read and write. Because

13:45.760 --> 13:50.560
file and obfuscation are generally pretty large, we don't want to, you know, keep downloading

13:50.560 --> 13:55.600
random files for absolute, you know, reason, right? So what basically happens is that we delayed

13:55.600 --> 14:01.760
download the file until the first write comes in. When the first write comes in, the write is

14:01.760 --> 14:06.160
intercepted. And we basically, the first thing we do is basically download the file into the,

14:06.160 --> 14:10.560
into the, into the staging area. Well, from there, we can actually do the operations, right?

14:11.120 --> 14:14.560
Now, all the operations that happen, like all the read operation, all the writes operation,

14:15.440 --> 14:20.000
do not go to the, to the, to the remote storage anymore, but they actually act on the local

14:20.000 --> 14:25.440
version of the file. And, you know, this allows us to basically, you know, write random stuff at

14:25.440 --> 14:31.760
a random point in time, so random place in the, in the file, allows us to basically have multiple

14:31.760 --> 14:37.200
writers writing files, multiple readers and so on. So they all write as it was, like a local,

14:37.200 --> 14:41.280
a local file essentially, which, which it is, right? What happens is at some point, someone

14:41.280 --> 14:45.600
was going to call a sync or, you know, call a sync and then close the file. And what's going to

14:45.600 --> 14:51.040
happen is that when the sync happens, then the file is then back uploaded into the, into the

14:51.040 --> 14:57.680
remote storage, right? And that basically allows us to, yeah, add the file in, in OpsFS. When the

14:57.680 --> 15:03.120
last client, the last write client actually closes file, then we can actually remove the file from

15:03.120 --> 15:07.120
the staging directory, right? So if you have like five different clients working on it, and then,

15:07.120 --> 15:11.200
you know, one removes it, then we can let the last one closes it, then we can remove the, the,

15:11.200 --> 15:15.920
the file from the, from the, from the staging, from the staging directory.

15:17.760 --> 15:21.440
Now, the last part, the last difference in terms of like,

15:22.080 --> 15:28.160
compatibility between what the, what the, what the, you know, OpsFS and HFS API are, and the local

15:28.160 --> 15:35.680
file system are. Can I take a question a little bit later? Okay. That's fine. Okay, thanks.

15:35.920 --> 15:43.200
Yeah, so in, in that regard is, yeah, so the way this security works on, on OpsFS is slightly

15:43.200 --> 15:48.960
different than if you're familiar with HFS. You might be familiar with Kerberos. In Opsfs, we

15:48.960 --> 15:55.200
don't use Kerberos, we use certificates. So every user in the platform gets a certificate or actually

15:55.200 --> 16:00.960
gets more than the one, but it doesn't matter. Essentially, in the certificate itself, you know,

16:00.960 --> 16:05.200
we have the information about who the user is, and every time the user needs to do an operation

16:05.200 --> 16:11.920
with the, with the, with the file system and any other service in Opsfs, present a certificate,

16:11.920 --> 16:17.680
and the certificate is obviously verified based on the chain of trust that is established within

16:17.680 --> 16:24.960
Opsfs deployment. So this basically what happens at the high level, right? So every time the Opsfs

16:24.960 --> 16:32.160
fuse mount needs to talk to the, to the Opsfs remote storage, it has to present a certificate

16:32.160 --> 16:38.240
essentially. And this is something that we, we control in the way that we actually use the

16:38.240 --> 16:43.440
Opsfs fuse mount in, in Opsfs works. When we set up the mount point, we actually, you know,

16:43.440 --> 16:48.560
make sure that the mount point is set up with the certificates for that specific user so that,

16:49.280 --> 16:53.920
you know, if I, if I need access to a specific directory from a Python library, Opsfs set up

16:53.920 --> 16:59.280
the mount point and, and provide provision the certificate for, for, for my user so that someone

16:59.280 --> 17:05.680
else cannot, cannot necessarily use the same mount point and the same, and the same, and the same

17:05.680 --> 17:10.240
certificates. So this is controlled at the application level, not the, not necessarily the storage layer.

17:11.360 --> 17:15.120
All the operations that happen on this side, while the authenticated based on the certificate,

17:15.120 --> 17:20.960
the problem happens from, let's say, the Opsfs fuse mount to the, to the user side, to the,

17:21.840 --> 17:27.760
to the, let's say, Linux processes essentially, right? So what happens here is the following,

17:27.760 --> 17:34.480
so there is a, there is a mapping going on between the users in Opsfs and the user on the machine,

17:34.480 --> 17:39.520
right? The problem we, like the problem we had, the problem that, like this setup has is that you

17:39.520 --> 17:44.080
end up having a lot of users on Opsfs, right? My, our deployments might have, like, you know,

17:44.080 --> 17:51.440
5,000 users all on the same deployment that, you know, results in way more groups than, than

17:51.520 --> 17:57.680
than 5,000. And so we cannot spend time and create them all the users, all the, all the, all the groups

17:57.680 --> 18:04.320
in, on the, on the machines. So the way we actually, the way we actually work in this situation is that

18:04.960 --> 18:11.120
when the Opsfs application needs access to, to, to, to, to, to mount point, a provision that mount

18:11.120 --> 18:16.400
point, it also makes sure it doesn't mount the entire file system, it mounts a specific sub-directory,

18:16.400 --> 18:20.640
and when it mounts a specific sub-directory before mounting it, it provisions the users

18:20.720 --> 18:26.560
that are within, that, that, their own files within that specific, the specific sub-directory.

18:26.560 --> 18:31.120
And the way it knows that is because in Opsfs, directories have a specific meaning,

18:31.680 --> 18:35.600
they are organized in a specific way, and so Opsfs application knows which users are,

18:36.400 --> 18:42.080
have access to specific sub-directory. So before actually, you know, mounting the, the file system,

18:42.800 --> 18:46.960
we actually provision all the user, provision all the groups, and then what basically

18:46.960 --> 18:52.400
Opsfs mount does is that, well, you know, a file here is owned by user Fabio, so there is a,

18:52.400 --> 18:58.400
there is a user on the machine, user Fabio with a specific UID, so the user ID of that file is going

18:58.400 --> 19:04.720
to be the, the same essentially. And again, the, the provision of the user is controlled by the,

19:04.720 --> 19:10.000
by the Opsfs application when they, when they, when it, when it's, it's necessary essentially.

19:10.160 --> 19:18.640
Now, there are a couple of, I would say, unsupported capabilities and things that we plan to address

19:18.640 --> 19:23.680
in the futures, more like limitations essentially. One thing I have supported at the moment are

19:24.880 --> 19:30.320
links, both out and soft links. Opsfs has supported for soft links in the background,

19:30.320 --> 19:34.720
but we never really used them in Opsfs, so we didn't have support here in Opsfs mount either.

19:35.200 --> 19:44.160
The challenge we have is around the users of these caches. Yeah, so essentially, you know, if you,

19:44.160 --> 19:48.400
if you're working with a, with a local file system, the kernel, you know, there's a pretty aggressive,

19:48.400 --> 19:54.880
you know, caching of the data and so on. The problem we have is that Opsfs is a multi-tent

19:54.880 --> 20:01.440
platform, so we have multiple users working on the potential of the same files, and so caching the

20:01.440 --> 20:06.000
files becomes a little bit problematic because if you have two users working on the same file,

20:07.200 --> 20:11.600
then like the different mount points are not going to be able to talk to each other, say, A, there is

20:13.280 --> 20:17.280
something that's changed here, and you need to reflect that in your cache. So at the moment,

20:17.280 --> 20:23.200
the caches are kind of disabled, but we're working on, on, on, on some solution to be able to get

20:23.280 --> 20:29.360
notifications and, and, and figure out that files have changed, right? And the reason,

20:30.800 --> 20:34.160
the reason they don't know is because there's different mount points, because each mount point

20:34.160 --> 20:37.920
is not certificate for that specific user, right? So the users are not sharing the same mount point,

20:38.560 --> 20:43.520
so changes, one user making changes is going to talk to a different mount point, and the user making,

20:43.520 --> 20:50.480
making changes essentially. The other thing that happens, if you, if you, if we go back to the

20:50.480 --> 20:58.560
right operation, when we upload the file, we upload the entire file. So there is no concept of,

20:59.680 --> 21:04.880
you know, uploading a specific block. The, the, the HFS API and the OPSF API does not allow you to

21:04.880 --> 21:09.760
basically say, you know, I know that, like a specific block has changed, I'm going to just replace

21:09.760 --> 21:16.160
that specific block, right? So the, this, this operation, if you're working with very, very

21:16.320 --> 21:23.920
hard files, then, then might become a problem. For the use cases that we use OPSFS mount within the

21:23.920 --> 21:28.480
OPSF platform, this is not a problem. Users are working with, like, small Python applications,

21:28.480 --> 21:32.720
or like generally speaking, smaller files, or when they are dealing with bigger files, they are

21:32.720 --> 21:37.920
dealing more, in more like a, in a, in a read process, not necessarily in a, in a writing,

21:39.520 --> 21:44.080
yeah, not going to end up writing a parquet file in beam, like several gigabytes of parquet files.

21:44.880 --> 21:47.760
So that's kind of it. That's kind of where we, where we stand.

21:49.680 --> 21:56.160
If you have any questions, that's, that's, you know, I can take them now. And thank you very much.

22:04.560 --> 22:08.720
Do you and practice have problems with applications ignoring the return value of close?

22:09.120 --> 22:15.680
The return value of close. No, so the question is, if we have problems with

22:16.240 --> 22:21.360
applications ignoring the return value of close, not at the moment, no. We had,

22:22.080 --> 22:27.600
with the way we use OPSFS mount is basically, we, we, we, we, we mount inside of containers,

22:27.600 --> 22:31.120
for instance, when you're doing, if you're running Jupyter notebooks, or if you're running

22:31.120 --> 22:35.520
great applications on, and we shut down the entire process, usually shut down the entire

22:35.520 --> 22:39.600
container, essentially. Right? So we, when, when, you know, the files gets closed and everything

22:39.600 --> 22:44.560
gets closed, um, shut down the entire mount point, essentially. That's because, not necessarily because

22:44.560 --> 22:48.880
it's necessary, but that's because usually, that's the use, like user experience and that people

22:48.880 --> 22:54.800
have in general. But you can't guarantee that the upload actually worked. You can't guarantee,

22:54.800 --> 23:01.440
come again, sorry. The last step. What happens if the upload file fails? If the upload file fails,

23:02.320 --> 23:05.680
try again, and then eventually, yeah, it's a simple, we give up, yeah.

23:07.520 --> 23:07.760
Yeah.

23:07.760 --> 23:13.760
So it seems that the retro machines that try to write open for writing and write through the same

23:13.760 --> 23:20.160
file independently, so obviously one of them wins and takes a lease and does download the file

23:20.160 --> 23:24.640
modification and upload. Yeah. What happens with other machines, or what does the process that

23:24.640 --> 23:28.000
tries to open the file for writing at the same time and write to each and so on?

23:28.960 --> 23:33.680
In this case, the last right wins. Like that's the problem that we have here, right? It's basically

23:33.680 --> 23:38.560
saying if we have, like multiple machines mounted in the same directory, they can't, like, and we're

23:38.560 --> 23:43.600
writing on the same file. I, the mount point doesn't, doesn't know that, like, you are changing the

23:43.600 --> 23:47.680
same, like my mount point doesn't know that you're changing the same file. So we upload the file,

23:47.680 --> 23:52.080
and then you upload the file as well, and your, your, your right space is essentially wins, essentially.

23:53.360 --> 23:54.800
And then, can we get?

23:55.760 --> 24:02.720
No, nobody's waiting for another one. But I'm like, you end up in like, in, you end up in a

24:02.720 --> 24:06.560
weird situation where I might not be able to see immediately your changes, and so I might be able

24:06.560 --> 24:11.440
to, I might re-resave my file and re-re-reprove my file, so then your changes get, get, get

24:11.440 --> 24:15.920
covered written again. So it becomes a little bit, a bit, a little bit like this. Yeah.

24:21.360 --> 24:21.840
Yeah.

24:25.520 --> 24:26.000
Yeah.

24:29.120 --> 24:29.520
Sorry.

24:32.560 --> 24:40.080
Uh, good question. Uh, in general, yes. Um, you might, like, you might not have, like, the security

24:40.080 --> 24:45.680
part of the, we didn't implement the CalBros part of the, the security part. So if, if you have a

24:45.680 --> 24:51.840
secure cluster, then you're probably not going to be able to, to use it. Or, yeah, you, you implement

24:51.840 --> 24:57.120
the CalBros part. Yeah. Yeah. It was a question in the back.

25:04.640 --> 25:10.080
No, there, there, there is, there are processes, yeah, so, yeah, thank you. Uh, the, the question is,

25:10.080 --> 25:14.000
uh, whether or not there are processes sharing the file. Uh, there are processes sharing the file,

25:14.880 --> 25:20.960
but you have, the problem is that you have multiple users writing on, on the same file,

25:21.600 --> 25:23.920
on different processes that do not know about each other.

25:29.920 --> 25:30.240
Yeah.

25:33.040 --> 25:37.840
It's also, you know, it's like the, the, the, the different mount points are, uh, that the,

25:37.840 --> 25:43.280
the question was about the directory metadata. It's also, it's like it's independent with the,

25:43.280 --> 25:44.240
it's independent with each other.

25:51.200 --> 25:58.160
They don't share mount points, no. Yeah. So if you create a file, um, then, yeah, but if you, if you

25:58.160 --> 26:02.880
do a less operation, then we go back to the, to the file system and that is, is, is reflected.

26:03.600 --> 26:07.520
But so, but they're not necessarily relying on the mount point, relying the obfuscates mount,

26:07.520 --> 26:14.000
going back to the, to the, to the, to the remote storage to get the new directory structure and so on.

26:14.880 --> 26:17.440
Yeah.

26:17.440 --> 26:20.960
Um, with this, uh, stage file writing,

26:21.520 --> 26:22.000
yeah.

26:22.000 --> 26:28.160
I mentioned that improves read performance for the client too, right? When it's actually a downloaded

26:28.160 --> 26:34.160
local file and it, do people use that? Like do they do a little write and then just get quick

26:34.160 --> 26:35.200
downloads?

26:35.200 --> 26:40.240
It, it, it does. Um, it does. That's, that's one of the, all that, like that's one of the other,

26:40.320 --> 26:43.440
that's one of the other reasons we kind of, the question, sorry, yeah, sorry.

26:44.720 --> 26:50.160
Uh, the, the question is around the, um, read performance when you're reading a file that you

26:50.160 --> 26:56.640
have, like, uh, like stored locally in the stage directory. Um, it does. Um, we don't have a specific

26:56.640 --> 27:01.840
number, um, but I have like some, like user experience with that. Um, when you have, um,

27:01.840 --> 27:06.080
when you have files that are on, on the remote storage, uh, even if they are, like, especially if

27:06.160 --> 27:10.320
you have smaller files, um, like maybe like a Jupyter notebook, this JSON file, like,

27:10.320 --> 27:15.200
you know, maybe a megabyte maximum or something, um, then it's the, the, the override of going

27:15.200 --> 27:19.120
and fetching it every time. It's, it's quite significant. If you have it locally, it's, it's,

27:19.120 --> 27:21.680
it's much more, uh, you can see it much more reactive.

27:26.960 --> 27:27.520
Yeah, there was a.

27:27.520 --> 27:32.640
I thought you said, uh, you, you delete the station file as soon as you close it.

27:32.640 --> 27:33.120
Yes.

27:34.080 --> 27:40.720
No, but if you, if you, if you, if you, if you have it open, uh, when you, when you keep writing it,

27:40.720 --> 27:44.560
then you, when you read it, you're not going to the, to the remote storage, you're reading from the,

27:44.560 --> 27:52.240
from the local, um, from the local, uh, yeah, from the local, from the local staging directory.

27:52.240 --> 27:58.160
Right. So if you, the only, the only, the only, the only time is, is if you're reading, if you're

27:58.160 --> 28:02.560
doing some read only operations, essentially, then at that point we don't download it, uh, mostly

28:02.560 --> 28:07.200
because, uh, in general, they'll be working with like pretty large files. And so the downloading

28:07.200 --> 28:11.840
process might, might not be necessary, right? Because if you open, let's say a parquet file,

28:11.840 --> 28:15.440
what happens is that you go at the end of the file, you just read the footer to figure out the schema,

28:15.440 --> 28:19.520
figure out where you, you need to go. And if maybe it's like a four gigabytes parquet file,

28:19.520 --> 28:23.840
you don't want to download four gigabytes to read that maybe a couple megabytes of metadata, essentially.

28:25.360 --> 28:25.680
Yeah.

28:28.720 --> 28:28.880
One more.

28:37.600 --> 28:38.320
Thank you very much.

28:38.320 --> 28:44.320
Thank you.

