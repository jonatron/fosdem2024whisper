WEBVTT

00:00.000 --> 00:17.000
Okay, it's 10 a.m. So we can start with the next talk. Next talk is going to be about Apache Airflow.

00:17.000 --> 00:23.000
We have Jarek Potjok tell us about the new features in Apache Airflow 2.0.

00:23.000 --> 00:33.000
Jarek is a PMC member of the Apache Foundation. He's working on this project and he's going to tell us all the details.

00:33.000 --> 00:36.000
It's probably going to be very interesting talk. Thank you.

00:36.000 --> 00:47.000
Thank you for the interaction. Hello everyone. So I'm going to talk about workflows. Who here know about Airflow?

00:47.000 --> 00:54.000
Quite a lot of people. Who uses Airflow on a daily basis? A lot of people know. Good.

00:54.000 --> 01:05.000
So my talk will be mostly to present you the Airflow, what it does and how the new Airflow, like the new workflow orchestrator,

01:05.000 --> 01:15.000
which is the Airflow 2.8 right now, is providing you as users and someone who wants to write workflows in Python.

01:15.000 --> 01:24.000
And that's why we are on a Python track. It provides you with a very modern way of interacting with modern data stack,

01:24.000 --> 01:38.000
processing your data for all the different kinds of users, including the new users like LLM, all the models, all the artificial intelligence.

01:38.000 --> 01:48.000
But first, few words about Airflow. I always refer to my past. I was a choir singer.

01:48.000 --> 01:58.000
I know a lot about music and orchestras and choirs. So Airflow is, if you imagine what Airflow does, because lots of people ask what Airflow does.

01:58.000 --> 02:05.000
Airflow doesn't do much because Airflow is mostly a conductor and orchestrator, someone who tells others what to do.

02:05.000 --> 02:14.000
And as you know, in the modern data processing workflows, mostly you're using a lot of different data processing engines, so to speak.

02:14.000 --> 02:27.000
You just a few of them and you need someone who actually or something which actually tells others what to do when and how to pass data between those different processors of different kinds.

02:27.000 --> 02:37.000
And Airflow does exactly this. On its own, Airflow doesn't do much. Just tells, OK, you do this, you do that, you do this and send that data here and there.

02:37.000 --> 03:01.000
That's basically what Airflow does. And the main thing that Airflow does, and this is why we are here on the Python track, Airflow allows you to define DAG, which is directed acyclic graph of tasks executing the data and dependencies between them.

03:01.000 --> 03:17.000
It allows you to define it in Python code. This is very different from many different orchestrators where you usually define them in YAML or in some declarative way of declaring the dependencies in tasks.

03:17.000 --> 03:27.000
In Airflow, everything is Python. From the beginning to an end, you do everything in Python, including extending Airflow itself, which I'm going to talk about a bit later.

03:27.000 --> 03:36.000
So, like, just a few examples of how it works. You can define DAG with a decorator, a nice Python way. You can also use it in a classic way, which I will show in a moment.

03:36.000 --> 03:46.000
Then you can define a task. And then finally, you can just use the task that you define and you can, yeah, if it works.

03:46.000 --> 03:56.000
And then you can link the task between themselves and make the dependencies between them. And once you define them, Airflow does everything to schedule the task.

03:56.000 --> 04:07.000
As you can see at the top, there is a schedule that the DAG of the task dependencies are executed.

04:07.000 --> 04:23.000
So, once you define it in Python, Airflow does everything for you and executes the tasks that you defined nicely in Python in a way.

04:23.000 --> 04:39.000
There are two ways of defining DAGs. There is a classic way where you define the operators and you use them. And we have about three or four thousand different kinds of operators to talk to all the different kinds of external data processing engines,

04:39.000 --> 04:49.000
which can be either databases or cloud services or pretty much everything that you can imagine. I will show the list later. It's pretty impressive.

04:49.000 --> 04:59.000
Or you can also define the task in a more Pythonic way where you just decorate a Python function as a task and this task gets executed.

04:59.000 --> 05:13.000
That doesn't seem much and probably you thought like myself, I was developing different kinds of workflows over all my career, many years, and tools to run the workflows.

05:13.000 --> 05:23.000
And you would say, okay, that's pretty much all the work or a lot of work that you do is define these kind of workflows and make them easy to run.

05:23.000 --> 05:37.000
And Airflow is one of the most popular orchestrators out there because it does it. I think it hit the spot, the right sweet spot, how this should be defined in the Python, but also nicely executed.

05:37.000 --> 05:54.000
I will show some executed and managed so you can be a Python developer, develop your tasks, and on the other hand, you give immediately someone who operates Airflow a very nice UI and the way how to manage all those workflows you define.

05:54.000 --> 06:07.000
So I will show a few things that many people who already use Airflow and some people here know about Airflow and use it and they might have even not know that things like that are possible.

06:07.000 --> 06:15.000
So those are things which appeared in the few latest versions of Airflow. So like for example, you have a task group.

06:15.000 --> 06:26.000
The task group allows you to group the task together and execute them together and even like rerun them together or make dependency to the whole group until the whole group finishes.

06:26.000 --> 06:35.000
So that's a nice new feature or relatively new features that we have and that we are using in Airflow.

06:35.000 --> 06:51.000
Airflow also has this very nice feature of being able to dynamically create many instances of the same task so you can get like kind of map reduce kind of workflow where you expand the task that to execute like you can execute hundreds or thousands of them and then get the result of that.

06:51.000 --> 07:05.000
So the typical map reduce case and it is very useful in number of cases but as of recently you can also expand the task groups very similar way so you can run groups of tasks in parallel, multiple of them.

07:05.000 --> 07:20.000
Very nice because you can very easily parallelize complex Airflow because by the way, Airflow is not only like you define the task in one place but you can distribute the whole execution of those tasks over a fleet of computers.

07:20.000 --> 07:37.000
Like with Kubernetes, with salary, with different mechanisms. We will not talk about that. There is no time to go into details but this allows you to parallelize your task workflow pretty massively and very complex ones.

07:37.000 --> 07:53.000
Then you have like dynamic task mapping. So that's how it looks like so you get the task, you expand an array of values and then you have multiple tasks running. We have a very nice UI that you can like browse through the list of those tasks.

07:53.000 --> 08:19.000
I just wanted to show it for a reference. Very recent task, very recent feature we have set up in teardown. So when you have a task that requires complex infrastructure to be set up and turned down at the end, we have the whole mechanism to manage those edge cases that comes there when the task executes when and it fails and all the, there are multiple tasks in between set up and teardown.

08:20.000 --> 08:32.000
So this is also nicely handled right now in Airflow. Very recent addition in the last version of Airflow we have integration with FS spec. I don't know who uses FS, who knows FS spec, who uses FS spec.

08:32.000 --> 08:53.000
There are a few people but you should if you haven't used it because it allows you to access the object storage from different storage providers. It allows you to access it in the same way as you would access a local file system using path leap, using slash, using path open.

08:54.000 --> 09:17.000
But also it allows you to integrate with multiple different already existing tools which are using FS spec like pandas, Polar, Spark, DagDB, Iceberg, PyRO, those are just a few that already support FS spec and this means that you can very easily plug in the file that you define from the object storage.

09:18.000 --> 09:36.000
Put it, put like here data frame from pandas to read that and it will read it and it very nicely integrates with the Airflow way how to manage credentials to access that data. So this is really recent features that we have added.

09:37.000 --> 09:53.000
Now this is also connected with data hour scheduling that we have which means that like Airflow by default in the original version was only task based but right now it can also define data sets which are linking the tasks.

09:53.000 --> 10:15.000
So one task can produce data set and another can consume the data set. You can define it and you can use it and those dependencies are automatically taken into account and whenever first task produces the data set the second one is run and we have a lot more features to be added to support that including the FS spec integration and a few others.

10:16.000 --> 10:30.000
A little shout out to Tatiana who will be speaking next from Astronomer. One of the things that we recently added as our LLM operators donated by Astronomer who is one of the stakeholders in Airflow.

10:31.000 --> 10:54.000
So we have the whole set of LLM based operators that you can use to build your LLM workflows because like just training is one thing but you have to prepare the data. You have to process it and then you have to take the results and maybe do some inference and all the other stuff that is not necessarily just teaching and learning.

10:55.000 --> 11:06.000
And this has been implemented. It's not like a theoretical set of operators. It's been implemented in a real implementation of a bot that Astronomer developed.

11:07.000 --> 11:20.000
A few words. So that was about the DAG outtouring. So like this is how you prepare the task and this is the most important for people like you who are like Python developers and want to integrate and implement those tasks in Airflow.

11:20.000 --> 11:38.000
But one important thing is that Airflow provides you out of the box with the modern UI. Like there is a nice graph UI showing you all the dependencies, status of the tasks, how they are working. You can rerun and clear the task from there. You can see the status of that.

11:38.000 --> 11:55.000
This is like great way how you can do your like have your operations people, those who look at those workflows being executed. What's going on with your DAGs? And this is the nice like this is a nice gun chart showing how the DAG is progressing over time.

11:55.000 --> 12:06.000
And you can see the grid view on the left side where you see the history of the task being executed. So the same task would like how it progressed over days or hours or whatever your frequency is.

12:06.000 --> 12:15.000
A very nice integration with logs so you can see what happens when the DAGs fail, when there is a problem. You can diagnose them.

12:16.000 --> 12:35.000
A gun view, a little bit more detail showing you the like how the tasks are progressing while the DAG is being executed. Very nice integrated and very recent addition. You can see the whole overview of the whole cluster of your, as I mentioned, Airflow has the capability of like massive

12:35.000 --> 12:53.000
distribution of the workload among multiple nodes. So you can see that from single place in Airflow as well. But I think the most powerful part of Airflow is that it's not only like you, not only can define the task in Python and execute them. That's rather simple thing.

12:53.000 --> 13:14.000
But Airflow is a platform. Airflow is a platform that has this capability of, and this is the way how we think about platform in the future when we will be developing the next versions of Airflow, that rather than implementing something on our own, we extend the capabilities of Airflow but allowing others to, other

13:23.000 --> 13:51.000
dedicated tools and other solutions to use whatever Airflow produces. An example of that is OpenLinage. OpenLinage is a standard how you can track the provenance of your data across all your flows. So you can know that, for example, this part of the data, this column was a private column that was obfuscated or aggregated so that initially it was related to the privacy.

13:54.000 --> 14:04.000
Then it was not and then it was joined with another data and you can track all this information and OpenLinage allows that and Airflow produces the lineage data and it's fully integrated.

14:05.000 --> 14:25.000
Another thing that we have integrated is OpenTelemetry, which means that Airflow produces the telemetry data that you can use to monitor whatever happens with Airflow, whatever happens with execution of those tasks, using the favorite tools of yours like Datadog or Google Cloud monitoring systems.

14:25.000 --> 14:44.000
All these tools are supporting OpenTelemetry and Airflow produces the data in OpenTelemetry compatible way. Still early day but we already have it. We have integration like, for example, integration with DBT, which Tatiana is going to talk about at the next stop.

14:45.000 --> 15:01.000
So I will not stop here but DBT is one of the most used ways of how Airflow is processing the data or what tasks are executed and we have this nice integration where DBT models can be mapped into the Airflow models and then this comes externally.

15:01.000 --> 15:09.000
This is not something in Airflow but Airflow has the extensibility that allows you to do that and others like Astronomer, they did it.

15:10.000 --> 15:20.000
We have a fully-flashdressed API that allows you to build extensions and those extensions can be of different kinds because you can access all the data inside about Airflow.

15:21.000 --> 15:39.000
Those extensions can be like UIs. We had a discussion yesterday at the dinner. You can build your own UI using those APIs if the UI of Airflow is not enough because we cannot satisfy everyone.

15:40.000 --> 15:45.000
You can do that all with the fully-flashdressed API we have.

15:47.000 --> 15:53.000
A few words also why we are at FOSDEME, like Airflow is fully open source.

15:53.000 --> 15:59.000
Airflow has a 10-year history. This year we have 10th anniversary.

16:00.000 --> 16:09.000
Airflow Summit 2024 which is planned in September in Bay Area. The plan is 4,000 out in this. It is very popular.

16:10.000 --> 16:15.000
Airflow is pretty popular. The last Airflow Summit we had was 500 people in Toronto.

16:16.000 --> 16:23.000
You can see that it has been steadily developed over years. It initially donated from Airbnb.

16:23.000 --> 16:30.000
It became an ASF, Apache Software Foundation project incubation in 2016.

16:30.000 --> 16:33.000
Then in 2019 it became a top-level project.

16:33.000 --> 16:40.000
We released Airflow 2.0 which is this new version of Airflow. We released it in 2020.

16:41.000 --> 16:48.000
Now steadily you can see that we are releasing new versions with new features that are steadily coming.

16:50.000 --> 16:55.000
What is important part of Airflow is open source, permissive license.

16:55.000 --> 17:00.000
The Apache Software Foundation is behind it. Strong governance.

17:00.000 --> 17:08.000
We have really strong stakeholders like Astronomer, Google, Amazon, Microsoft also.

17:09.000 --> 17:13.000
All of them provide Airflow as a service. You can use it in their clouds.

17:13.000 --> 17:23.000
We have a very well-defined security process, release process and maintenance certainty.

17:23.000 --> 17:29.000
You can be quite sure that Airflow is going to be there with the same license.

17:29.000 --> 17:34.000
The license is not going to change as we have heard. It is happening with a number of open source projects.

17:34.000 --> 17:38.000
It is under the Apache Software Foundation umbrella.

17:38.000 --> 17:42.000
It is going to be maintained and you can rely on it being released in the future.

17:42.000 --> 17:44.000
You can rely on it pretty much.

17:44.000 --> 17:50.000
There are some community numbers. It is not a vanity metric.

17:50.000 --> 17:56.000
We have the biggest number of contributors in Apache Software Foundation more than 2700.

17:56.000 --> 18:02.000
61 committers, 32 PMC members. We have 10 years of history.

18:04.000 --> 18:12.000
Those are the different kinds of tools that you can get when you integrate with Airflow.

18:12.000 --> 18:14.000
That should be a little bit before.

18:14.000 --> 18:19.000
You can see this is the community and integration.

18:19.000 --> 18:26.000
All the things that I mentioned before about the extensibility of Airflow allow you to build all the different kinds of extensions.

18:26.000 --> 18:30.000
This is like our community page, tools integration, integrating with Airflow.

18:30.000 --> 18:34.000
The big number of those are there.

18:34.000 --> 18:41.000
A lot of people are developing for Airflow, extending Airflow, adding new capabilities to it.

18:41.000 --> 18:46.000
One thing that I wanted to mention, because this is mostly what I am working on,

18:46.000 --> 18:50.000
we have very solid foundations for Airflow right now.

18:50.000 --> 18:56.000
We defined this public API, public interface that you can rely on when you are working with Airflow.

18:56.000 --> 19:04.000
We can very easily rely on the rest APIs and the number of things that Airflow exposes.

19:04.000 --> 19:09.000
This is the kind of most impressive, one of the most impressive things in Airflow.

19:09.000 --> 19:14.000
I mentioned that before, that we have this many, many different integrations built in.

19:14.000 --> 19:17.000
These are all the integrations that we have.

19:17.000 --> 19:21.000
I am not sure if you can see the names, some of them probably.

19:21.000 --> 19:27.000
There are a lot of different, more than 90 different providers that we have

19:27.000 --> 19:35.000
that allows you to immediately connect to those external services and run Airflow with them.

19:35.000 --> 19:44.000
My big focus is on the security and this is something that comes in the next two years

19:44.000 --> 19:50.000
for every software package that is nearby, near you.

19:50.000 --> 19:55.000
Because we have this CRA Act in Europe and others.

19:55.000 --> 20:02.000
If you haven't seen the kind of public policy and compliance talks here,

20:02.000 --> 20:05.000
you should realize that it's coming.

20:05.000 --> 20:10.000
In two, three years we will all have to follow the security very rigidly

20:10.000 --> 20:16.000
and we have very good functional security team that is handling the security issues

20:16.000 --> 20:19.000
according to the Apache Software Foundation.

20:19.000 --> 20:21.000
We are part of the bounty.

20:21.000 --> 20:28.000
If you find a problem in Apache Airflow, just report it and you can get money for that.

20:28.000 --> 20:33.000
I highly recommend that because we are fixing them fairly quickly.

20:33.000 --> 20:39.000
We have some features like S-BOM and reproducible builds built in Airflow.

20:39.000 --> 20:44.000
As of recently we were working on that for quite some time to make sure

20:44.000 --> 20:49.000
that whatever we deliver is not only useful and nice for the developers,

20:49.000 --> 20:55.000
but also it's secure to deploy and used in your production workflows,

20:55.000 --> 21:02.000
which I think is very important because it means that you can rely on the software.

21:02.000 --> 21:08.000
So summary, I just wanted you to remember from that talk

21:08.000 --> 21:13.000
that Airflow is a modern and solid data orchestrator with really strong foundations.

21:13.000 --> 21:19.000
Something that is going to be developed for next like 10 or 20 or 30 years, you don't know.

21:19.000 --> 21:21.000
The future is pretty bright.

21:21.000 --> 21:25.000
We have new slick ways of interacting with modern data stack.

21:25.000 --> 21:30.000
Even if we were created 10 years ago, right now we have all these new modern ways

21:30.000 --> 21:34.000
of how you can write your workflows and interact with all the external systems.

21:34.000 --> 21:36.000
That's pretty cool.

21:36.000 --> 21:39.000
It's true open source.

21:39.000 --> 21:45.000
And we have huge and supportive community of both the people who develop Airflow

21:45.000 --> 21:48.000
and those who integrate stuff with Airflow.

21:48.000 --> 21:54.000
And that's a sign that this is a really great project to work on.

21:54.000 --> 22:01.000
So I would say if you want to contribute or use, both options are very good for Airflow.

22:01.000 --> 22:04.000
And we continuously evolve.

22:04.000 --> 22:07.000
That would be very short overview.

22:07.000 --> 22:09.000
I didn't have time to go into many details.

22:09.000 --> 22:12.000
So just wanted to touch base on what Airflow does.

22:12.000 --> 22:15.000
If there are any questions, I'm happy to take them.

22:25.000 --> 22:27.000
Very nice talk. Thank you.

22:27.000 --> 22:33.000
My question is, in my group, we haven't been able to agree on whether the Python-based DAGs

22:33.000 --> 22:35.000
are declarative or not.

22:35.000 --> 22:36.000
What do you think?

22:36.000 --> 22:40.000
And do you think distinction matters at all or not?

22:40.000 --> 22:44.000
So the question was about the declarative or not.

22:44.000 --> 22:50.000
I personally think declarative way of defining DAGs or defining the workflows

22:50.000 --> 22:57.000
is very much impaired by the fact that whenever you want to do any kind of complex workflow,

22:57.000 --> 23:02.000
those declarative ways become unmanageable.

23:02.000 --> 23:06.000
In the past, I've developed a number of those workflows, as I mentioned.

23:06.000 --> 23:09.000
Airflow is just last one of them.

23:09.000 --> 23:12.000
Usually what happens when you have these declarative workflows,

23:12.000 --> 23:19.000
when they become complex enough, you start to write Python code to generate these declarative workflows.

23:19.000 --> 23:21.000
Because they are too complex.

23:21.000 --> 23:25.000
And then the benefit of having those declarative workflows is gone.

23:25.000 --> 23:29.000
I would say it's probably better if you start the other way.

23:29.000 --> 23:33.000
You use Python as a way how to define your workflows.

23:33.000 --> 23:38.000
And counter-intuitively, you take your declaration and generate the Python code from that.

23:38.000 --> 23:40.000
This is very easy, actually.

23:40.000 --> 23:42.000
And our customers are doing that.

23:42.000 --> 23:44.000
Our users are doing that, actually.

23:44.000 --> 23:48.000
A lot of users have their own version of declarative way of defining the workflows.

23:48.000 --> 23:52.000
And from that, taking that the airflow is so flexible,

23:52.000 --> 23:56.000
and you can do anything with the Python code,

23:56.000 --> 23:59.000
and you can define the workflows arbitrarily complex,

23:59.000 --> 24:02.000
just mapping it to a Python code that does it,

24:02.000 --> 24:08.000
is usually much simpler than trying to get these declarative workflows to do something complex.

24:08.000 --> 24:11.000
So declarative workflows are great for start,

24:11.000 --> 24:17.000
but when they become complex, the Python way is much better, in my opinion.

24:19.000 --> 24:22.000
Thank you. We have time for one more question.

24:34.000 --> 24:40.000
Is there any interest in airflow to support longer running workflows with events and things like that?

24:40.000 --> 24:41.000
With events.

24:41.000 --> 24:43.000
Blocking operations.

24:43.000 --> 24:45.000
You wait for an event, you wait for a time.

24:45.000 --> 24:46.000
Yes, absolutely.

24:46.000 --> 24:50.000
This is one of the things that I removed for clarity from the presentation,

24:50.000 --> 24:53.000
because previous version of that presentation didn't have that.

24:53.000 --> 24:57.000
So the airflow has so-called deferrable operators,

24:57.000 --> 25:02.000
and this means that when you will have a long running workflows and waiting for something,

25:02.000 --> 25:08.000
then you can defer execution of that operator to something which we call trigger,

25:08.000 --> 25:14.000
which is Async IEL, Python-based event loop, basically.

25:14.000 --> 25:19.000
And this event will wait, or this task will wait on this Async IEL

25:19.000 --> 25:22.000
without taking almost any resources.

25:22.000 --> 25:29.000
So if you have any workflow that, you know, with Async IEL, you can check and trigger when it finishes,

25:29.000 --> 25:34.000
this is the best way, and like, it's absolutely supported by airflow

25:34.000 --> 25:39.000
to have like hours or days running workflows for this kind of waiting for this

25:39.000 --> 25:41.000
without taking too much resources.

25:41.000 --> 25:46.000
That's a relatively new feature, like maybe two years or so.

25:46.000 --> 25:47.000
Thank you.

25:53.000 --> 25:54.000
Okay, thank you very much, Eric.

25:54.000 --> 25:59.000
That was a very interesting talk, and it's certainly a very, very good project to look into.

25:59.000 --> 26:03.000
We'll have another five-minute break now, and then continue with the next talk.

26:11.000 --> 26:14.000
Thank you.

