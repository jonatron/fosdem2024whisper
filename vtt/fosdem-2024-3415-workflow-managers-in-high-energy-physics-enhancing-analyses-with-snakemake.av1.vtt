WEBVTT

00:00.000 --> 00:10.520
Okay, so good afternoon, all both evening, I suppose.

00:10.520 --> 00:14.300
I'm Jamie, so I'm a doctoral student at TU Dartmouth University in Germany and a member

00:14.300 --> 00:17.400
of the LHCB experiment at the Large Hadron Collider.

00:17.400 --> 00:21.800
More importantly for this talk, I'm a user of SnakeMaker's three years and will come

00:21.800 --> 00:27.600
on to what SnakeMaker is and how it's used within our experiment.

00:27.600 --> 00:33.280
To jump straight into the title, so really the first two words of workflow managers,

00:33.280 --> 00:34.600
they do what they say on the turn.

00:34.600 --> 00:39.120
It's tools to manage workflows which is unsurprising from the name.

00:39.120 --> 00:44.240
It's not uncommon in many fields, but particularly in high energy physics, that we have data

00:44.240 --> 00:45.360
of some variety.

00:45.360 --> 00:50.120
We want to apply process, some form of workflow to get some results and ideally a nice paper

00:50.120 --> 00:52.000
out of things.

00:52.000 --> 00:55.040
From this, we can kind of structure our work with a workflow manager.

00:55.480 --> 00:59.640
This involves defining our workflow, organizing or reorganizing the rules in our workflow,

00:59.640 --> 01:04.160
so this kind of flow chart we have running a workflow or miraculously rerunning when

01:04.160 --> 01:08.520
there's a change to the code base or if there needs to be a reproduction of a result, and

01:08.520 --> 01:09.520
also to document a workflow.

01:09.520 --> 01:14.560
So we've had a bit throughout the day about reproducibility, so it also plays into that.

01:14.560 --> 01:16.160
There are a lot of tools on the market.

01:16.160 --> 01:20.760
Here is just a variety for a few different places, some more leaning towards the toolside

01:20.760 --> 01:24.280
of things, some more leaning towards the framework side.

01:24.520 --> 01:26.320
Things like common workflow language.

01:26.320 --> 01:29.760
The focus here today is really going to be around snake make.

01:29.760 --> 01:35.400
So snake make evolved really from the GNU make paradigm, so the workflow is defined from

01:35.400 --> 01:36.920
a set of rules.

01:36.920 --> 01:42.960
These can be related to one another and a directed acyclic graph is generated, which

01:42.960 --> 01:49.440
can just be really thought of as a flow chart linking all of the rules together to get a

01:49.440 --> 01:54.720
set of required target product or results.

01:54.720 --> 02:00.120
We can use wildcards within there that really allows us to create dynamic workflows, thoroughly

02:00.120 --> 02:02.120
encourage having a play around with it.

02:02.120 --> 02:04.920
It's a really fun tool to use, really good tool to use.

02:04.920 --> 02:10.160
It's based on Python, so it's Python compatible, you can in fact use Python functions within

02:10.160 --> 02:11.160
snake make.

02:11.160 --> 02:15.600
This does provide a very shallow learning curve for those already experienced with Python.

02:15.600 --> 02:21.200
It's been in development for a while, it's recently had almost an overhaul of sorts with

02:21.200 --> 02:25.640
version 8 which released at the end of last year and restructured a lot of the functionality

02:25.640 --> 02:28.960
to look more towards the future.

02:28.960 --> 02:33.320
It originally was a large tool in bioinformatics, but within the last five years it's been

02:33.320 --> 02:34.320
picked up in HEP.

02:34.320 --> 02:38.080
Now I've said a lot about HEP and high energy physics, it's probably worth just briefly

02:38.080 --> 02:41.720
touching on what that means, so this really is the physics of the early universe, this

02:41.720 --> 02:45.520
is what we talk about when we talk about particle physics.

02:45.560 --> 02:50.040
For example at the Large Hadron Collider, where the experiment that I work on is based,

02:50.040 --> 02:54.520
we accelerate particles to large fractions of the speed of light and then collide them

02:54.520 --> 02:58.720
to see in these very high temperature, high density environments what the old conditions

02:58.720 --> 03:01.040
of the early universe would have looked like.

03:01.040 --> 03:05.480
There's four main experiments around this ring, which is 27 kilometres of tunnel under

03:05.480 --> 03:08.840
the Swiss French border at Geneva.

03:08.840 --> 03:13.960
By experiment LHCB just in the foreground, other background just next to the airport,

03:14.000 --> 03:18.560
and it's just as easy as looking at the differences between matter and antimatter.

03:18.560 --> 03:22.760
This is our little experiment here, 100 metres under Geneva.

03:22.760 --> 03:27.040
Now analyses in high energy physics really aim to measure some things, so this is a project

03:27.040 --> 03:30.680
all the way through aiming to get a final measurement that might be a massive particle,

03:30.680 --> 03:34.120
the lifetime of a particle or how it decays for example.

03:34.120 --> 03:38.080
We want to contradict ideally the standard model, which is our best understanding of

03:38.080 --> 03:39.440
particle physics.

03:39.440 --> 03:43.200
Any contradictions here would be a sign that some things arise, that there is new physics

03:43.240 --> 03:44.240
out there.

03:44.240 --> 03:49.160
We start off with experimental data, an extractive measurement from that by fitting, selecting

03:49.160 --> 03:50.160
and so on.

03:50.160 --> 03:54.720
There's a number of processes we can apply and these all take place in dedicated scripts,

03:54.720 --> 03:57.360
usually written by analysts.

03:57.360 --> 04:02.920
And since analyses are collaborative projects, these can range from a few authors up to dozens

04:02.920 --> 04:07.400
really in some of the larger analyses, this means that every analysis has a large kind

04:07.400 --> 04:09.520
of shared and dynamic code base.

04:09.520 --> 04:13.160
This may sound familiar, so a software project in many ways has parallels that can be drawn

04:13.160 --> 04:14.160
there.

04:14.160 --> 04:18.440
In terms of what we require from a workflow manager in high energy physics, we can break

04:18.440 --> 04:20.440
it down also like this.

04:20.440 --> 04:24.560
So starting off we need results that need to be reproducible, we can't really afford

04:24.560 --> 04:28.480
to build a second large Hadron Collider, so the best we can do is ensure that our results

04:28.480 --> 04:32.160
from the raw data make sense if we rerun the analysis.

04:32.160 --> 04:36.080
Similarly, if there's a new theoretical input or if we have a change to the scripts, we

04:36.080 --> 04:39.000
may need to rerun this analysis.

04:39.000 --> 04:41.240
Workflow managers really make this very easy.

04:41.240 --> 04:42.840
Excellent, thank you.

04:42.840 --> 04:47.160
The data that we have is often stored remotely and this is because we have absolutely tons

04:47.160 --> 04:48.160
of it.

04:48.160 --> 04:52.720
The scales of data we have are several terabytes per analysis, there can be hundreds of analysis

04:52.720 --> 04:57.400
per experiment ongoing at once, are enormous and this will only get larger.

04:57.400 --> 05:03.680
There's a plot I'll show near the end that shows how much of a daunting case this is,

05:03.680 --> 05:04.920
but we'll come onto that.

05:04.920 --> 05:07.920
So these really can be large scales of data.

05:07.920 --> 05:11.640
The scripts, as I've said, in an analysis can change very frequently and also can be

05:11.640 --> 05:12.640
of very different types.

05:12.640 --> 05:18.840
So these could be in Python, C++, Fortran, it really can range, so you want a flexible

05:18.840 --> 05:23.840
platform where all you need to do is ensure that it runs in the shell and you can then

05:23.840 --> 05:25.880
deploy it within a workflow.

05:25.880 --> 05:28.360
And finally, we need to be very scalable and deployable.

05:28.360 --> 05:33.080
This is both with regards to the amount of data, the amount of authors, and also ensuring

05:33.080 --> 05:37.720
that everyone collaborating on an analysis can contribute to it.

05:37.720 --> 05:41.600
Snakemake actually meets all of these needs, it meets them very well and it's seen a lot

05:41.600 --> 05:46.120
of uptake particularly in my experiment at HCP, to the point where we have a really strong

05:46.120 --> 05:51.720
user base and we've started now to have our internal expertise leading to internal training.

05:51.720 --> 05:57.280
I've linked here a very recent, this took place yesterday, training from a colleague

05:57.280 --> 05:58.280
place.

05:58.280 --> 06:03.080
There's also within our starter kit, which is given to all of our new members, there's

06:03.080 --> 06:04.440
a Snakemake course within there.

06:04.440 --> 06:08.680
So new analysts are trained in this tool so that they can use this right from the off

06:09.040 --> 06:12.600
and since most of them are familiar with Python, that really is getting started from

06:12.600 --> 06:15.080
day one, getting straight into physics.

06:15.080 --> 06:18.720
The features and functionality, suit analysis is really, really well.

06:18.720 --> 06:22.480
So there's interface for HPC resources, the amount of data we have means we can't process

06:22.480 --> 06:27.640
all of it kind of locally, so we do want to make use of the resources we have around the

06:27.640 --> 06:30.120
world and also local clusters.

06:30.120 --> 06:34.440
And also because all of the data is remote, the integration for remote access protocols

06:34.480 --> 06:39.040
within Snakemake enables that in a much more user friendly manner.

06:39.040 --> 06:43.120
So if we start with kind of scalable deployable workflows, so there's a few different ways

06:43.120 --> 06:48.480
that Snakemake approaches this, we can break down our larger workflow into smaller files,

06:48.480 --> 06:54.440
either as wrappers of common snippets or parts of code, or into individual files.

06:54.440 --> 06:58.960
This also helps with maintainability and ensures that one small change in the workflow can't

06:58.960 --> 07:00.480
destroy everything.

07:01.000 --> 07:05.800
Additionally, check pointing within workflows allows for much more flexible definitions,

07:05.800 --> 07:09.680
so if a script is going to produce an unknown number of files, it's not known or it's not

07:09.680 --> 07:16.160
deterministic how the workflow will look from the start, so we can re-evaluate that flow

07:16.160 --> 07:20.000
chart further down the line when that's being produced, so when we know how many files we

07:20.000 --> 07:21.080
have there.

07:21.080 --> 07:24.520
We can also batch the jobs that we have, so if we have a rule that happens many times,

07:24.520 --> 07:29.360
say over many files, we can batch that so we only need to consider as many at time and

07:29.400 --> 07:32.120
that also reduces the overhead locally when running it.

07:32.120 --> 07:37.320
Lastly, just in terms of deployability, so there's integration for specifying package

07:37.320 --> 07:42.840
requirements within Condor so that when you are then using another machine or for another

07:42.840 --> 07:48.160
user that it will run as it does or should run as it does locally.

07:48.160 --> 07:53.720
In terms of distributed computing, so as a large data scale requires large computing

07:53.720 --> 07:59.080
scales, it's not uncommon within analyses even without using workflow managers to use.

07:59.160 --> 08:04.280
Clusters and HPC resources for processing, fitting, it's becoming more and more common

08:04.280 --> 08:08.800
with tools like our data frame which is able to run through Spark.

08:08.800 --> 08:12.680
Snakemake supports common interfaces, so some of the ones just listed on the right hand

08:12.680 --> 08:18.000
side and actually how this is implemented is very straightforward, so a workflow can

08:18.000 --> 08:23.240
be defined exactly as it's done locally and the only additional part is specifying a profile,

08:23.240 --> 08:28.800
so this is typically a job script and a submission script and then in the command line just

08:28.880 --> 08:33.120
an additional flag specifying the profile is given, everything else runs as it would

08:33.120 --> 08:37.160
locally, it's just that the rules then are submitted to the, or the jobs are submitted

08:37.160 --> 08:38.920
to the cluster.

08:38.920 --> 08:41.560
Resource limits can be set, so if you have a job that you're worried is going to need

08:41.560 --> 08:46.160
quite a lot of memory or you're sharing a small number of resources but want to use

08:46.160 --> 08:49.560
central resources, that can help there in a lot of ways.

08:49.560 --> 08:53.200
And finally, if you have a job that maybe you're writing a very small amount of text

08:53.200 --> 08:57.040
or file which you would prefer to run locally where the overhead isn't justified to submit

08:57.080 --> 09:01.480
it to central resources, that can also be specified as a local rule so that regardless

09:01.480 --> 09:06.560
of whether you're running locally or through a cluster, that's, that always runs locally.

09:06.560 --> 09:11.360
And finally in terms of the functionality, so most of our data is stored either on EOS

09:11.360 --> 09:16.080
or this is AtCern or on the worldwide LHC computing grid which is around the, around

09:16.080 --> 09:18.280
the world as in they.

09:18.280 --> 09:23.440
The remote implementation within Snakemake allows for different providers, so we mostly

09:23.560 --> 09:29.920
use X root D, this is the protocol used by EOS and WLCG, but also S3 is becoming a bit

09:29.920 --> 09:32.680
more common in places and that is also supported.

09:32.680 --> 09:38.920
So in both of those cases there's a common, a common implementation where simply add the

09:38.920 --> 09:42.480
provider.remote, excellent thank you.

09:42.480 --> 09:45.480
And this can be wrapped around your path so you just define the rule as you would normally

09:45.480 --> 09:47.680
and add the additional parts on there.

09:47.680 --> 09:51.040
And there's some functionality within there, glob wildcards for if you want to correct a

09:51.120 --> 09:55.000
glob function on these resources rather than having to download all of them in advance and

09:55.000 --> 10:00.880
keep local to avoid repeatedly downloading or streaming files.

10:00.880 --> 10:04.880
This list isn't exhaustive though, I think about eight or nine, possibly more now.

10:04.880 --> 10:10.880
One of the big changes with V8 was to increase the flexibility of the interface there.

10:11.840 --> 10:15.440
Finally just really what do analysts need going forward, so we have this tool that works

10:15.440 --> 10:18.960
very well that we do want to kind of deploy more widely.

10:19.360 --> 10:22.360
There's lots of discussions within the community as to how we do that.

10:22.360 --> 10:26.960
Going forwards, scalability is really probably our biggest issue.

10:26.960 --> 10:30.760
So this is from the Atlas experiment, we are where the cursor is.

10:30.760 --> 10:37.360
In five years time we have five, six times more data than we already have as well as

10:37.360 --> 10:39.920
the data we already do have.

10:39.920 --> 10:43.200
This will keep on going, so we'll have the high luminosity error which is what this large

10:43.200 --> 10:48.240
increase from about 2028 will be where we take enormous amounts of data compared to

10:48.240 --> 10:49.640
what we currently have.

10:49.640 --> 10:52.880
So we need to be able to handle this and we need the tools to handle that.

10:52.880 --> 10:57.920
The experiments are already, LHCB used to be the smallest of the experiments, it's technically

10:57.920 --> 11:01.320
I think still the smallest and sits around a thousand authors.

11:01.320 --> 11:05.920
This grows by about 100 every year, so we need to be able to deploy for that many authors

11:05.920 --> 11:08.040
across kind of analyses.

11:08.040 --> 11:12.080
In terms of usability, analysts typically aren't software devs by trade, so having a

11:12.080 --> 11:16.800
tool that is very user friendly is super helpful in having these implementations that work

11:16.800 --> 11:19.400
very well out of the box adds to that.

11:19.400 --> 11:22.640
So going forward they're kind of continuing the good work there.

11:22.640 --> 11:28.160
And finally in terms of functionality, really what would help is further collaboration and

11:28.160 --> 11:32.440
there's already been quite a bit already between devs and HEP users.

11:32.440 --> 11:38.480
So for example the pull request on GANGA which is a service that we use in LHCB for cluster

11:38.480 --> 11:43.160
interface to implement that as an executor in the same format that was shown on a couple

11:43.160 --> 11:45.300
of slides before.

11:45.300 --> 11:50.380
So to draw some conclusions, workflow managers really are very useful to our research, they

11:50.380 --> 11:53.420
save us a lot of time in making sure that everything's run in the right order, that

11:53.420 --> 11:57.300
nothing is left behind and that we can look back and see what's been run.

11:57.300 --> 12:01.140
The tools do meet our needs, the functionality is there and in fact the use of these tools

12:01.140 --> 12:03.940
will become unavoidable.

12:03.940 --> 12:07.940
Within the next few years the scales of data we have will mean we have to use tools like

12:07.940 --> 12:09.860
Snakemake.

12:09.860 --> 12:14.900
And finally we have a very strong user base, it's a bit spread across the experiments,

12:14.940 --> 12:19.900
not necessarily all in Snakemake, so CMS use Luigi which was on the much earlier slide

12:19.900 --> 12:25.140
and bringing that together to collaborate both across the HEP community and also with

12:25.140 --> 12:29.180
devs really is also crucial in moving that forward.

12:29.180 --> 12:33.420
So I've left a few links on there, the training from earlier and I would otherwise welcome

12:33.420 --> 12:36.420
any questions.

12:36.420 --> 12:43.420
One question I have for you, so a happy user of Snakemake, one thing is sometimes challenging

12:51.420 --> 12:58.420
is the visualization of the data and the dedication flow, how do you handle this especially when

13:00.420 --> 13:04.420
you have thousands of nodes in the data?

13:04.420 --> 13:09.420
So to repeat the question, so within Snakemake there are ways to visualize it, this can be

13:09.420 --> 13:15.420
very difficult with large scales of workflow, so the question is how to approach that.

13:15.420 --> 13:19.620
Often this can be done with grouping, that helps in a lot of cases for very large rules

13:19.620 --> 13:24.420
that then when visualizing the DAG this narrows everything down.

13:24.420 --> 13:28.620
I believe that's both the DAG and pipeline modes, one expands to every job and one just

13:28.620 --> 13:33.420
goes to individual rules, so one is naturally a lot narrower.

13:33.420 --> 13:37.420
Really I would say between batching group, those that have been the ways I've personally

13:37.420 --> 13:42.420
dealt with it, I'm not sure from the outside or elsewhere in the user base, but that would

13:42.420 --> 13:45.420
be how I've gone about it.

13:45.420 --> 13:48.420
Sorry, I think we have to move on to the next talk, so thank you very much for joining me.

