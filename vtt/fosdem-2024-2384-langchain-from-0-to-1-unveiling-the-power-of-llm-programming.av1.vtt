WEBVTT

00:00.000 --> 00:07.000
Hi, y'all.

00:07.000 --> 00:12.240
I have the privilege of introducing you to Stefano.

00:12.240 --> 00:18.000
And he is from Italy, in the middle of the Italian coast.

00:18.000 --> 00:23.040
You've been a Linux enthusiast for 20 years, got me on that one.

00:23.040 --> 00:25.360
And your focus is on VoIP, interestingly enough.

00:25.360 --> 00:32.040
This is his 10th Fosdom, and your favorite animal is you after four beers.

00:32.040 --> 00:33.720
Very appropriate.

00:33.720 --> 00:34.800
Everyone, welcome Stefano.

00:36.400 --> 00:36.900
Thank you.

00:36.900 --> 00:50.240
One of my hobbies was caving.

00:50.240 --> 00:58.440
I spent 10 years going into caves, descending pitches with ropes, crawling into mud, and

00:58.440 --> 01:00.760
doing those awful things.

01:00.760 --> 01:06.120
The reason for doing that is that the very few time I had the chance to be the first

01:06.120 --> 01:10.160
one in an unknown place, it was awesome.

01:10.160 --> 01:18.000
When you are in an unknown place, you face some dangers, but you also have infinite possibilities.

01:18.000 --> 01:20.200
Behind the light of your headlamp, there could be anything.

01:20.200 --> 01:24.840
A river, a beach, kilometers of unexplored passages, who knows.

01:24.840 --> 01:27.600
And I feel the same about the AI today.

01:27.600 --> 01:33.880
And I'd really love to increase the power of your headlamp today.

01:33.880 --> 01:37.400
So I'm going to kick start you into Lang chain.

01:37.400 --> 01:43.280
This is the GitHub page for the talk, where you can find the proof of concept code and

01:43.280 --> 01:44.640
the presentation itself.

01:44.640 --> 01:49.480
It's better if you look at the code during the presentation.

01:49.480 --> 01:54.280
We'll explore Lang chain using one of its notable use case, that is retrieval of met

01:54.280 --> 01:55.280
generation.

01:55.280 --> 02:00.880
And for doing that, we will look at some of its components and concept that are document

02:00.880 --> 02:10.240
loaders, text splitters, embeddings, vector stores, retrievers, prompts and templates

02:10.240 --> 02:16.720
for generating prompts, large-length models, of course, and finally we'll combine some

02:16.720 --> 02:19.280
of those together in a chain.

02:19.280 --> 02:23.480
Then I'll experience the adrenaline of a live demo, and maybe we will take a look at some

02:23.480 --> 02:27.920
other notable use cases.

02:27.920 --> 02:33.560
Let's talk about our main quest first, that is retrieval of met generation.

02:33.560 --> 02:38.840
This cutting edge techniques involves giving additional data to the LLM to enhance its

02:38.840 --> 02:39.840
responses.

02:40.240 --> 02:47.560
It's interesting because when you give additional data to the LLM, the answers become more precise

02:47.560 --> 02:56.560
and relevant, and it's also allowed the citation of sources, and allowed to respond to data

02:56.560 --> 03:03.000
that are not in training data set, that could be even personal data or real-time data.

03:03.000 --> 03:08.480
It's a very discussed topic, and it's an intriguing case for showcasing Lang chain.

03:09.480 --> 03:14.440
This is the scheme of what we want to obtain.

03:14.440 --> 03:17.280
Multiple use cases exist over retrieval of met generation.

03:17.280 --> 03:22.360
We will look at the simple one that is question answering over unstructured data.

03:22.360 --> 03:27.800
We will take some text that is our unstructured data, and we will put it into a storage.

03:27.800 --> 03:34.040
Then we will ask a question and use the data from the storage to help the LLM answer the

03:34.040 --> 03:35.040
question.

03:36.000 --> 03:39.040
Let's look at it in more detail.

03:39.040 --> 03:43.840
We will take data from a transcript from a YouTube video, and we will load it into a

03:43.840 --> 03:45.320
usable format.

03:45.320 --> 03:50.120
Then we will split it into smaller parts and compute a vector representation, also known

03:50.120 --> 03:54.440
as embeddings, of this data.

03:54.440 --> 03:57.240
We will store it into a database.

03:57.240 --> 04:02.040
Then we will ask a question and compute the vector representation of the question, and

04:02.040 --> 04:06.240
use this vector representation to find similar documents.

04:06.240 --> 04:15.240
Then we will put the question and the retrieved documents into the prompt and give it to the

04:15.240 --> 04:19.320
large language model.

04:19.320 --> 04:23.160
If you're thinking that it's complex, I assure you that it's not, and it fits in a few lines

04:23.160 --> 04:25.800
of code.

04:25.800 --> 04:29.960
If you're thinking that it's trivial or worthless, I assure you that it's not the case-hater,

04:30.080 --> 04:33.280
because there are a lot of concepts behind that.

04:33.280 --> 04:35.080
Why using LineChain?

04:35.080 --> 04:39.920
LineChain is a framework for developing LLM-powered applications.

04:39.920 --> 04:44.920
It offers us a lot of ready-to-use of the shelf components and building blocks that make our

04:44.920 --> 04:46.920
life easier.

04:46.920 --> 04:51.680
Should we take our code in production, it also has components that make it easier for

04:51.680 --> 04:58.120
us to do it, and also it has a lot of samples to copy.

04:58.280 --> 05:03.920
It's fun because it has an extreme speed of improvement, and something interesting came

05:03.920 --> 05:09.280
out of its community continuously.

05:09.280 --> 05:16.080
On the other hand, it's very young, and breaking changes may happen, but we like risk.

05:16.080 --> 05:18.080
We are using Python.

05:18.080 --> 05:23.160
LineChain is also available in TypeScript, but that's not make-up-of-tea.

05:23.320 --> 05:28.320
We also have our main requirements that are LineChain, of course.

05:28.320 --> 05:34.320
OpenAI that we will use as embeddings and LLM provider, and TraumaDB as vector store.

05:34.320 --> 05:39.320
Since we're using OpenAI, we will provide an API key.

05:39.320 --> 05:42.320
Okay.

05:42.320 --> 05:46.320
In this part, we prepare and store our data.

05:46.480 --> 05:53.480
We will use four components that are a document loader to retrieve our data, to get our data,

05:55.980 --> 05:57.980
and convert it into a usable format.

05:57.980 --> 06:03.480
A text splitter for divide the document into smaller meaningful units, an embedding function

06:03.480 --> 06:08.480
to compute the vector representation and the vector store to store our vectors.

06:08.480 --> 06:15.480
The document loader is an object that takes from various sources to the data source.

06:16.480 --> 06:23.480
It takes from various sources of data and gives us a transform it into a usable format.

06:23.480 --> 06:25.480
That is a document.

06:25.480 --> 06:32.480
Multiple sources are available, and for instance, we can have files like PDF or text file or

06:33.480 --> 06:40.480
web pages or cloud storage such as Amazon S3 or Google Drive, social media like Reddit,

06:40.480 --> 06:47.480
Twitter, GitHub, and papers, and of course, YouTube transcripts.

06:48.800 --> 06:55.800
It's also very easy to write your own if you don't find something that fits for what

06:55.800 --> 06:57.800
you need.

06:57.800 --> 07:01.440
You can just extend the base loader class.

07:01.440 --> 07:08.440
This is our document loader, and we are using the YouTube loaders from the LineChain community.

07:11.400 --> 07:18.400
And this will take the transcript of our video and put it into the document class.

07:20.600 --> 07:22.600
This is the document class.

07:22.600 --> 07:29.600
It has a page content string that will hold the transcript of our video and a metadata

07:32.200 --> 07:39.200
dictionary that will have a key source with the URL of our video.

07:39.200 --> 07:46.200
Now that we have our document, we want to split it into smaller meaningful units.

07:47.600 --> 07:48.600
Why do we want to split it?

07:48.600 --> 07:50.000
Well, for free reason.

07:50.000 --> 07:57.000
The first one is that the input size of our LLM is limited, so we want to give smaller

07:57.560 --> 07:58.760
pieces.

07:58.760 --> 08:05.760
The second one is that, like me, our LLM tends to be easily distracted, so we want to increase

08:05.960 --> 08:11.160
as much as possible the signal-to-noise ratio and avoid to distract it, giving it useless

08:11.160 --> 08:11.960
information.

08:11.960 --> 08:16.920
So we will choose only the pieces important to answer the question.

08:16.920 --> 08:21.360
And the third reason is that usually we pay per token, so the more we give, the more we

08:21.360 --> 08:23.640
pay.

08:23.640 --> 08:29.440
We can think of five levels of text splitting from simple to complex.

08:29.440 --> 08:35.120
The simple one is splitting just counting charters or tokens.

08:35.200 --> 08:41.200
This is simple and easy, but it has a problem, and the problem is that probably we will end

08:41.200 --> 08:45.120
up splitting in the middle of a word or a phrase.

08:45.120 --> 08:48.800
The second level addresses this problem, and this recursive splitting.

08:48.800 --> 08:55.800
It recursively tries to split text on special charters like new line or punctuation, then

08:57.360 --> 09:04.360
combines those phrases together till the maximum length specified is reached.

09:04.360 --> 09:11.360
The third one, look at the document structure that works for HTML files or markdown or code.

09:12.320 --> 09:18.400
And then there are semantic chunkers that is still experimental on a long chain, and

09:18.400 --> 09:23.040
it's very interesting because it combines phrases together only if they are similar

09:23.040 --> 09:26.120
and use embeddings to compute similarity.

09:26.120 --> 09:33.120
The last one is highly experimental, and it's asking an LLM to split our text.

09:33.480 --> 09:36.320
This is highly experimental and also very expensive.

09:36.320 --> 09:42.560
It probably makes sense only if you are thinking that the cost per token is going to zero.

09:42.560 --> 09:47.320
We are using the recursive charter text splitter, that is the second, and it's a good default

09:47.320 --> 09:48.240
choice.

09:48.240 --> 09:55.240
We can specify the length of the text, and if you want some overlap.

09:56.520 --> 10:01.520
There's not a golden rule about that, so maybe you want to try what works best for you.

10:02.520 --> 10:08.520
Okay, now we have our documents, and we want to compute the embeddings.

10:08.520 --> 10:13.200
The embeddings are a vector representation in a high dimensional space.

10:13.200 --> 10:18.800
That means that we take our data and represent it as a vector.

10:18.800 --> 10:25.800
Each dimension of this vector will reflect an aspect of context or meaning of our data.

10:26.760 --> 10:31.600
There are thousands of those dimensions.

10:31.600 --> 10:37.360
If two pieces of text are similar, they are next to each other in the embedding space.

10:37.360 --> 10:43.200
That means that we can compute the similarity of two pieces of text just measuring the distance

10:43.200 --> 10:46.040
between those vectors.

10:46.040 --> 10:53.040
It seems complex, but for us it's very easy because for us it's just a function that we

10:54.000 --> 10:57.400
use when we create the vector store.

10:57.400 --> 11:00.440
We are using an external provider here, that is OpenAI.

11:00.440 --> 11:06.400
And auto privacy, obviously if you use an external provider to compute embeddings, you

11:06.400 --> 11:11.320
are sending your data to the external provider.

11:11.320 --> 11:15.920
We now have vector representation of our data, and our data is split.

11:15.920 --> 11:17.680
We want to store it into a vector store.

11:17.720 --> 11:24.640
A vector store is a database that is tailored for storing and searching embeddings.

11:24.640 --> 11:28.080
We are using TraumaDB here.

11:28.080 --> 11:30.800
It is open source, it's very easy to set up.

11:30.800 --> 11:34.720
This is the initialization.

11:34.720 --> 11:40.720
And as we said before, we are passing the OpenAI embedding function to it when we initialize

11:40.720 --> 11:42.680
it.

11:42.680 --> 11:48.880
These are the most used vector store in the reports of the state of AI for 2023.

11:48.880 --> 11:54.760
And TraumaDB is at first place, and FACE is also open source, it's from Meta.

11:54.760 --> 11:58.760
And Pinecon is a very popular cloud vector storage.

11:58.760 --> 12:04.760
Okay, we now have hard data into the vector store.

12:04.760 --> 12:07.240
We want to use it.

12:07.240 --> 12:13.400
We will use four main components here that are a retriever to search similar documents

12:13.400 --> 12:20.240
to our question, a prompt that will give the LLM the instruction on the output that we

12:20.240 --> 12:26.600
will give, the LLM that is the heart and lung and brain of our application, and finally

12:26.600 --> 12:29.920
we will combine those three together in a chain.

12:29.920 --> 12:37.160
Okay, the retriever is an object that is responsible for searching documents that are relevant

12:37.160 --> 12:39.120
to answer our question.

12:39.120 --> 12:47.680
The simple retriever does this just computing the vector representation of our question

12:47.680 --> 12:54.320
and search for document that are near to this vector in the embedding space.

12:54.320 --> 12:57.440
This is the simple retriever.

12:57.440 --> 13:03.120
Long chain also offers us more advanced retriever like this one, this is multi-query retriever.

13:03.120 --> 13:11.120
Please use the LLM component to formulate the variation of our question and then use

13:11.120 --> 13:17.320
the embeddings of those variations to search for similar documents, similar and hopefully

13:17.320 --> 13:20.720
relevant to answer our question.

13:20.720 --> 13:28.280
Now that we have similar documents, we can put them into the prompt and the prompt to

13:28.280 --> 13:29.280
give to the LLM.

13:29.280 --> 13:36.360
This is the prompt that we are using and the prompt is just a template with the instruction

13:36.360 --> 13:42.880
for our LLM and two variables in this case that are the context that will be our documents

13:42.880 --> 13:45.680
and the question itself.

13:45.680 --> 13:51.840
I love delving into details because it's just a template and also we can take this prompt

13:51.840 --> 13:53.600
from the long chain app.

13:53.600 --> 13:59.000
Long chain features an app with all the prompts and other objects that we can use, all the

13:59.200 --> 14:03.520
of the shell components that we can use.

14:03.520 --> 14:09.120
We have the prompt, we want to give it to the LLM.

14:09.120 --> 14:14.680
We are using OpenAI SLLM and this is how we initialize it.

14:14.680 --> 14:21.680
I use streaming, the first variable because it really improves the user experience and

14:22.920 --> 14:28.000
temperature zero means that we don't want creativity or hallucination, we just want

14:28.040 --> 14:31.640
precise answers.

14:31.640 --> 14:38.640
Maybe you can argue that I should have used different LLM providers but nobody gets fired

14:40.160 --> 14:45.120
for buying OpenAI so I chose that.

14:45.120 --> 14:49.400
These are the most used LLM providers always from long chain state of AI.

14:49.400 --> 14:56.400
OpenAI is at first place and I'd like to rant a bit about that because CloudAI, the third

14:57.400 --> 15:04.400
on that list, is labeled from almost from everywhere in the world except from Europe.

15:05.560 --> 15:11.920
This week the Italian data protection authority is going against OpenAI over privacy issue

15:11.920 --> 15:13.480
again.

15:13.480 --> 15:19.160
I know that there are a lot of privacy advocates here and I also care about user privacy but

15:19.160 --> 15:26.160
I think that defending the user right shouldn't mean going against going against war against

15:26.760 --> 15:27.400
them.

15:27.400 --> 15:29.880
That's my two cents.

15:29.880 --> 15:34.440
Those are the most used open source providers.

15:34.440 --> 15:38.160
It's interesting because the first three has a very different business model.

15:38.160 --> 15:44.660
The first one rents hardware, the second has a cost per token, paper token and the third

15:44.660 --> 15:50.440
one is for surf hosting.

15:50.440 --> 15:53.840
We now have gathered all the components, we want to put them together.

15:53.840 --> 15:56.960
This is all the components called one after another.

15:56.960 --> 16:02.560
We have our question and we pass the question to the retriever and we get a list of documents.

16:02.560 --> 16:06.520
The list of documents is joined together in the context variable then the context variable

16:06.520 --> 16:13.520
is used in the template to generate the prompt and the prompt is given to the LLM.

16:14.400 --> 16:21.400
It works nice and easy but we can do better and this put everything together using a chain.

16:22.040 --> 16:29.040
A chain is a sequence of components that does a function and it's better than just

16:32.040 --> 16:39.040
calling the component one after another because it has several advantages like it offers sync

16:40.440 --> 16:46.240
and the sync support and that allow us to take our code directly into production without

16:46.240 --> 16:53.240
changing it and also as advantages of observability and it's integrated very well with other

16:54.400 --> 17:02.200
launch chain components that are used to take code in production.

17:02.200 --> 17:09.200
This is the code put together using the LLM expression language LCL that is a new way

17:09.880 --> 17:16.880
of doing those chains. This is an acquired taste and it's quite new.

17:22.520 --> 17:29.520
It's from September but I find it very useful when you get used to it.

17:32.040 --> 17:39.040
Okay, let's see how this works.

17:40.200 --> 17:47.200
This is our code and there are two examples.

17:51.960 --> 17:58.960
One uses the chain, one not, this is the one that doesn't use it and it's just a few lines

18:01.800 --> 18:08.800
of codes. It's very easy.

18:09.200 --> 18:16.200
Okay, I forget the open AI key.

18:31.200 --> 18:38.200
Okay, I forget the open AI key.

18:40.200 --> 18:47.200
Of course it doesn't work.

18:47.200 --> 18:52.200
I'm not connected, you're right.

18:52.200 --> 18:59.200
Okay, I have a backup video.

19:10.200 --> 19:16.200
No, no.

19:16.200 --> 19:23.200
By the way, it's just for giving you an idea of the piece of calling the various components

19:28.360 --> 19:35.360
and the parts that takes the most time is computing embeddings and this is the streaming

19:35.360 --> 19:42.360
output. Okay, I have prepared some questions that are those questions and those are given

19:51.960 --> 19:56.960
too fast, sorry.

19:56.960 --> 20:03.960
I gave the question to the LLM and this is the output of the output of the LLM.

20:06.360 --> 20:13.360
Also, okay, it's nice because this one, the retriever wasn't able to find the answer for

20:21.360 --> 20:28.360
this question and so it wasn't able to give us a response and the LLM told us, I don't

20:28.360 --> 20:31.360
know.

20:31.360 --> 20:36.360
I'm not sure if I can move forward. Maybe I also have it for the LCL.

20:36.360 --> 20:41.360
The LCL version uses the multi-query retriever.

20:41.360 --> 20:47.360
So you will see now that it will ask multiple questions.

20:47.360 --> 20:52.360
Each question is transformed into multiple questions.

20:52.360 --> 20:58.360
This is low, I'm sorry.

21:01.360 --> 21:08.360
Okay, those are the questions and this is the answer that came out.

21:20.360 --> 21:25.360
Okay.

21:25.360 --> 21:32.360
There are also other interesting use cases of luncheon.

21:36.360 --> 21:42.360
We look at the simple one that is question answering over unstructured data.

21:42.360 --> 21:47.360
Also it's very interesting question answering over structured data.

21:47.360 --> 21:53.360
This one uses the LLM component to convert our question into a sequel query that is executed

21:54.360 --> 22:00.360
and the result of the query is used to improve the answer of our LLM.

22:00.360 --> 22:03.360
It's very interesting.

22:03.360 --> 22:06.360
Another one is data extraction.

22:06.360 --> 22:12.360
You just have to provide a JSON schema and then unstructured text and the JSON schema is

22:12.360 --> 22:16.360
automatically filled in with the data from the structured text.

22:16.360 --> 22:20.360
The LLM understands what to put into the JSON schema.

22:20.360 --> 22:25.360
It's interesting because there are people paid for doing that work.

22:25.360 --> 22:34.360
Summarization is very useful and it has a lot of, let's say, problems.

22:34.360 --> 22:36.360
It's an open problem.

22:36.360 --> 22:38.360
It's very interesting and useful.

22:38.360 --> 22:44.360
Then there is a synthetic data generation that is useful if you want to find a model

22:44.360 --> 22:48.360
or maybe if you want to anonymize some data.

22:48.360 --> 22:51.360
It works like data extraction backwards.

22:51.360 --> 22:58.360
You have a JSON schema and the LLM generates a text unstructured that contains data that

22:58.360 --> 23:00.360
will fit into the JSON schema.

23:00.360 --> 23:07.360
Finally, there are agents that is a key concept of luncheon and it's very fun.

23:07.360 --> 23:13.360
With agents, the LLM takes charge of choosing what action to do.

23:14.360 --> 23:18.360
It's worth studying.

23:18.360 --> 23:20.360
It's very interesting.

23:22.360 --> 23:24.360
Okay, that's it.

23:24.360 --> 23:28.360
So, thank you.

23:32.360 --> 23:34.360
Do you have any questions?

23:38.360 --> 23:40.360
I saw his hand first.

23:43.360 --> 23:45.360
Thank you.

23:45.360 --> 23:47.360
Very interesting.

23:47.360 --> 23:49.360
My question is how does this scale?

23:49.360 --> 23:53.360
You showed an example in which we have just one transcript.

23:53.360 --> 23:55.360
What if we had billions of transcripts?

23:55.360 --> 24:00.360
I didn't see any mention to the ranking of the retrieved chunk.

24:00.360 --> 24:03.360
If you can elaborate a little bit on that, it would be very good.

24:03.360 --> 24:05.360
Thanks.

24:07.360 --> 24:11.360
Okay, luncheon helps to take this in production.

24:11.360 --> 24:15.360
This was proof of concept so you can take this in production.

24:15.360 --> 24:18.360
Also, it's out of the scope of this talk.

24:18.360 --> 24:20.360
This was luncheon from zero to one.

24:20.360 --> 24:24.360
So, that scaling is from zero to 100.

24:24.360 --> 24:29.360
You can find a lot of examples on how to take that in production.

24:29.360 --> 24:37.360
If you take a look at the GitHub repository, there is also a link on how people from

24:37.360 --> 24:44.360
luncheon use this in production with the chatbot that helps searching

24:44.360 --> 24:46.360
in the luncheon documentation.

24:46.360 --> 24:52.360
You can find the code and it's very interesting.

24:52.360 --> 24:56.360
If you want to take it in production, it's worth copying that code.

24:56.360 --> 24:58.360
It's the best practice.

24:58.360 --> 25:00.360
Did I answer your question?

25:03.360 --> 25:05.360
I'm sure you'll see this coming.

25:05.360 --> 25:09.360
If I have some money to spend on a hardware and I want to get an LLM,

25:09.360 --> 25:15.360
there is a lot of proprietary intelligence that you use, like the Mbendix in particular,

25:15.360 --> 25:20.360
and also the other part that it's on the query side at the end of the chain.

25:20.360 --> 25:26.360
How difficult it is to do this without using OpenAI?

25:28.360 --> 25:33.360
It's really easy because luncheon allows to swap those components.

25:33.360 --> 25:41.360
I use it here at OpenAI because it's the easy way for having a result.

25:41.360 --> 25:52.360
But if you, for instance, use the Ollama, you can self-host the LLM and ask questions to the LLM,

25:52.360 --> 26:01.360
or maybe with a face you can rent hardware and run your open source model on their hardware.

26:01.360 --> 26:05.360
So it's easy because those components are swappable.

26:07.360 --> 26:10.360
All right, y'all. Let's give Stefano one more round of applause.

