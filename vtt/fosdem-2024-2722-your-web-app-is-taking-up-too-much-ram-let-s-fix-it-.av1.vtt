WEBVTT

00:00.000 --> 00:11.640
Hello everyone. Can you guys hear me properly? Nice, perfect. Yeah, today I wanted to start

00:11.640 --> 00:17.400
your presentation with quite a bold claim. I would say that your web app is taking up

00:17.400 --> 00:26.480
too much RAM and we could fix it. And this like comes from a thing that I noticed recently

00:26.480 --> 00:31.800
and is that if you look at your Chrome browser, you can see that if you hover over the tab,

00:31.800 --> 00:39.280
you will see that the Chrome is now starting, at least in a while, to tell users the memory

00:39.280 --> 00:45.880
usage of your app. Which like, if you look at most applications such as for example g-tub

00:45.880 --> 00:50.760
even while looking at a pretty big diff, the memory usage is not that bad. Yeah, like I

00:50.760 --> 00:58.720
mean, 122 megabytes were a lot in the 2000 but like now it's not as much. But if you look at

00:58.720 --> 01:06.760
other websites that maybe are a bit more expensive such as Airbnb, you can see that if we load a

01:06.760 --> 01:12.960
pretty big page, the memory usage goes way up. Like we're talking about half a gig of RAM being

01:12.960 --> 01:19.920
used by the browser. And what I was wondering like is it our fault? Is it the browser? What's

01:20.080 --> 01:28.600
in that memory that is being used? And we can find out how much of that is actually used by the

01:28.600 --> 01:34.720
JavaScript virtual machine, by our variables, our functions and our code. And the way of doing that,

01:34.720 --> 01:39.680
it's by opening the DevTools and there is a special tab called memory and you can see for

01:39.680 --> 01:44.960
each JavaScript virtual machine that is running, you can see how much memory is it taking up right

01:44.960 --> 01:52.560
now. Which in case of Airbnb, it was like 111 megabytes which is like, it's not much but it

01:52.560 --> 01:59.120
starts to be quite a bit especially when GitHub was like 10 megabytes compared to that. And then

01:59.120 --> 02:06.000
maybe you look at some more extreme examples such as here I propose fully stress test notion by

02:06.080 --> 02:15.360
loading a quite big table and we went into 1.5 gigabytes of RAM used just by

02:15.360 --> 02:21.200
JavaScript variables and that was quite wild because if you think about it, that's a lot. That's

02:21.200 --> 02:28.560
a lot for a web page. And also there are even worse examples or I would say more difficult

02:28.560 --> 02:34.720
examples like the product that I'm currently building and I'm building this web-based tool

02:34.720 --> 02:41.840
called the flux which is a tool for designing electronics on your browser and it is quite

02:41.840 --> 02:49.680
complicated because electronics is made up by a lot of different parts and it's built using

02:51.440 --> 02:58.720
typeskipped React, 3JS, React refiber so we use a bunch of technologies and also a bunch of

02:58.720 --> 03:06.160
abstractions to make our life easier and that had an effect on us. In fact because we wanted to

03:06.160 --> 03:13.680
be able to render very complicated documents with a lot of different shapes and text and everything

03:13.680 --> 03:24.240
has to run at 60 FPS, you can see how holding a big project can take a lot of RAM and that's

03:24.320 --> 03:31.360
something that backfired a bit. Why? Well because originally we focused a lot on performance,

03:31.360 --> 03:37.840
we wanted to have everything load very quickly, we wanted the scroll to be fast and originally

03:37.840 --> 03:43.440
we just optimized for performance, we were like yeah memory is cheap, let's just use whatever

03:43.440 --> 03:51.520
all the memory that we have so we just optimized what the profiler said, not what the memory profiler

03:51.520 --> 04:01.840
said and actually we did this because there was this article that from a while ago that was

04:01.840 --> 04:07.840
talking about yeah if you're building React apps just memorize everything, just cache everything

04:07.840 --> 04:15.200
that you can because that is not going to be an issue in most cases. People did we know that we

04:15.200 --> 04:21.920
were one of those cases and yeah and in fact you can see how like if you load a pretty big

04:21.920 --> 04:31.840
document at least before this talk the app will take too much RAM but they can really hear someone

04:31.840 --> 04:39.360
says well okay I have 16, 32 weeks of RAM on my desktop on my computer, why do I care about

04:39.360 --> 04:46.480
memory users, we're not in 1999 anymore. Well there are still a couple of reasons why we really care

04:46.480 --> 04:54.160
about this now and one of the reasons is out-of-memory crashes. If you're not optimizing memory usage

04:54.160 --> 04:59.440
the browser will limit you. In most cases for example in Chrome if you go over four gigabytes

05:00.320 --> 05:06.160
you will get this, you will get an old snap error code 5 which is an out-of-memory and there is no

05:06.160 --> 05:11.360
way to catch it, no way to solve it, the only thing that you can do is just prevent this from

05:11.360 --> 05:15.680
happening in the first place because here you will need to refresh the page to fix it.

05:16.480 --> 05:22.880
And on iOS it's even worse because on iOS sometimes the limit goes down and no one is really clear

05:22.880 --> 05:31.200
about what the limit is. For example if you are on Safari UIOS sometimes the limit can go as low

05:31.200 --> 05:38.880
as 300 megabytes and this is what you get, you get your browser loading up the page, trying to load

05:38.880 --> 05:43.200
the page then going out of memory, refreshing the page and going in an infinite refresh loop

05:44.400 --> 05:51.760
which you will see your user report and that's when your product manager will come screaming into

05:51.760 --> 05:56.720
your office why is the application not loading on my phone and because you're using too much RAM,

05:56.720 --> 06:03.120
so yeah clients might have a lot of RAM but your browser doesn't care, it will not let you use it.

06:04.080 --> 06:10.640
And another thing is that we also care about the garbage collection performance, if the more that

06:10.640 --> 06:16.640
you allocate the more that you will need to deallocate later and that's a thing that you have to

06:16.640 --> 06:21.600
care about because in some cases the garbage collection connection times can really hurt your

06:21.600 --> 06:26.320
performance. This is a bit of an extreme case that's like one minute of garbage collection

06:26.320 --> 06:33.440
but like this is something that is a bit more realistic. We were debugging an event handler that

06:33.440 --> 06:41.280
was supposed to run on mouse move so something that was totally off path and the major garbage

06:41.280 --> 06:49.360
collector took 0.5 seconds which means that there was a sharp drop in the frame per second just

06:49.360 --> 06:56.080
because the garbage collector had to kick in and so that's another thing that you want to care

06:56.080 --> 07:02.160
if you care about performance. Also memory is part of your performance optimization strategy

07:03.600 --> 07:10.000
and another thing is that as I showed before now Chrome is showing the memory usage of your

07:10.000 --> 07:18.960
website to your users so if your users are using like 12 tabs or if you are insane like in my case

07:18.960 --> 07:24.720
you have 10 browser's opens with a thousand tabs each, yeah that should start closing them maybe

07:24.720 --> 07:32.880
tomorrow. The users will be able to see that it's your website that is taking up their entire

07:32.880 --> 07:41.280
RAM and they will not be happy with you so now they will know which one it is and so yeah we're

07:41.280 --> 07:46.640
into a situation and for example my situation how do we solve this like how we approach this

07:46.640 --> 07:53.200
problem in flux? Well first of all it's important to figure out what is occupying memory and once

07:53.200 --> 08:01.520
you do that like there are multiple strategies that you can use to kill it with fire and then we also

08:01.520 --> 08:07.920
want to make sure we're not doing the same mistake aka we can set up some checks in CI or we can set

08:07.920 --> 08:13.360
up some monitoring even with remote users to check that the memory usage is not that bad right now.

08:14.000 --> 08:20.240
Of course in the talk of today I wanted to focus more on the first point because that's already

08:21.840 --> 08:28.640
a lot of things to talk about. So before going into the tooling I wanted to introduce some

08:30.880 --> 08:37.680
ideas about memory usage so that we know what we're talking about. I like to have those distinctions

08:37.680 --> 08:44.240
while talking about memory usage this is something that I made up in my analysis and like I noticed

08:44.240 --> 08:50.240
that there is a pattern of having either static or transient memory usage. What are we talking about

08:50.240 --> 08:56.080
here? Static memory usage it's when you have variables that are taking up a lot of RAM but they

08:56.080 --> 09:02.720
are long lived, they are global variables, they are state that is staying there and it's not really

09:02.720 --> 09:09.040
changing throughout the run of your application and that's basically what you would find in a

09:09.040 --> 09:16.400
heap snapshot and it is that the easy thing for example the document that loads and it is taking

09:16.400 --> 09:22.480
up a lot of RAM but you don't necessarily have a situation like that sometimes you could have a

09:22.480 --> 09:28.560
transient peak of memory usage which means that for example the user clicks a button and that

09:28.560 --> 09:33.760
button triggers a very quick operation which allocates an array with one million elements

09:34.800 --> 09:41.280
you can see it as a peak in the memory usage at that point and that sometimes can be a bit more

09:41.280 --> 09:46.880
hard to the bug because you want to find that on a heap snapshot because a heap snapshot is just

09:47.440 --> 09:53.440
taking an image of what's in your RAM at that moment and a peak of memory that would be

09:53.520 --> 09:58.720
de-allocated immediately won't show up in that so there are different strategies depending if we

09:58.720 --> 10:07.200
have the first or the second type of memory problem. Another thing that I like to consider is the count

10:08.080 --> 10:15.600
and the size of stuff. Why? Because you might have a very happy situation the kind of analysis wise

10:15.600 --> 10:22.880
in which you're locating a 500 megabyte string or a 500 megabytes array that's very different than

10:23.760 --> 10:31.280
allocating millions of small objects and if you have the first or the second situation you need to

10:31.280 --> 10:37.200
use completely different approach to analyze that because while if you have a giant object it would

10:37.200 --> 10:43.440
just show up in the memory profiler immediately as a very big object if you have millions of small

10:43.440 --> 10:48.560
elements it would be much harder to analyze them because you will need to check what's inside those

10:49.120 --> 10:56.320
those four bytes objects and another thing that I like to bring up is the difference between

10:56.320 --> 11:01.360
shallow and retained size and these are things those are two terms that you will see in the memory

11:01.360 --> 11:10.320
profiler and the reason for that is because in JavaScript everything is a pointer so if you

11:11.040 --> 11:17.440
have an array of strings it's actually an array of pointers to strings so the array itself could be

11:17.440 --> 11:23.200
very small like in order of bytes but the stuff that is pointing to could be giant like it could

11:23.200 --> 11:30.240
be pointing to a lot of one megabyte strings so when we talk about shallow size we talk about the size

11:30.240 --> 11:37.920
of that allocation itself such as the array which is small but that array it's causing other memory

11:37.920 --> 11:46.080
to stay allocated because it's referring to those one megabyte string so the retained size is instead

11:46.640 --> 11:55.520
the total amount of memory that that object or that array is forcing to stay and that is preventing

11:55.520 --> 12:01.040
from being deallocated and there's another last topic that is also quite complicated which are

12:01.040 --> 12:08.800
allocation types and which means that in JavaScript there are multiple things that you can allocate

12:08.800 --> 12:13.680
you have different other types you have code you have strings just array you have typed arrays you

12:13.680 --> 12:22.240
have also closures and each one of those behaves differently in memory and one cool thing that you

12:22.240 --> 12:28.880
can get from this is that for example also functions are something that can take up memory if you're

12:28.880 --> 12:36.240
not careful enough because functions need to save all the variables that are around them so technically

12:36.800 --> 12:43.840
that a function is an object as well in javascript and this means that even for example if you are

12:43.840 --> 12:48.480
creating functions in a loop that could become a memory problem because it's the same thing as

12:48.480 --> 12:57.440
creating an array of objects so like sometimes you can just look up the v8 in Chrome documentation

12:57.440 --> 13:03.040
and find a lot of interesting things about how memory is used internally but that's another

13:03.040 --> 13:08.400
topic for another talk I would say I wanted to instead look into tooling like if we are in a

13:08.400 --> 13:15.680
situation in which we have a lot of memory usage what are some tools that we can use to try to

13:15.680 --> 13:22.880
start to analyze what is going on and how to solve the problem well the most famous one is the

13:22.880 --> 13:27.920
common memory compiler which is that memory tab that you probably saw next to performance

13:28.880 --> 13:34.480
in the Chrome DevTools and it's quite powerful because it can work in three different modes I

13:34.480 --> 13:40.880
think the most interesting ones are the heap snapshot and the allocation sampling which works in

13:40.880 --> 13:47.680
very different ways for different purposes but it is that with the heap snapshot you can take a big

13:47.680 --> 13:54.720
snapshot of everything that it's in your RAM everything that javascript is working with and

13:54.720 --> 14:00.800
like imagine that you created a lot of variables in your code with this you can just save all of them

14:00.800 --> 14:06.720
and look at what's inside of them which is really cool because you can even see the values that you

14:06.720 --> 14:13.440
have there and for each one of those for each allocation that you have you can also see what

14:13.440 --> 14:21.200
is the retainer that means what why is this being a memory who created it and who is holding

14:21.280 --> 14:27.760
references to it and that's useful to determine who is the thing like what is the function that

14:29.440 --> 14:35.680
caused that thing to stay in memory and the heap snapshot are very useful if you want to check

14:36.560 --> 14:42.160
stuff like static memory usage because it takes a snapshot in time if instead you're more interested

14:42.160 --> 14:48.080
into transient memory peaks as I said before there is this other tool called the allocation sampling

14:48.080 --> 14:56.720
which works by accumulating every allocation that happens this means that everything that

14:57.920 --> 15:04.320
is allocated is saved here but you don't get the allocations so which is you can't really measure

15:04.320 --> 15:13.520
how much RAM you're using you can just measure who is creating that RAM that's objects we had

15:13.520 --> 15:19.440
not too many of them but some of them were taking a lot of RAM like 89 megabytes that's a lot and

15:19.440 --> 15:25.680
we had one specific object that was taking a giant amount of memory like 80 megabytes and

15:26.640 --> 15:32.560
which is then by looking at the retainers we were able to immediately figure out what was the function

15:33.120 --> 15:39.840
that was allocating that stuff that was retaining that stuff and that was one of the very first

15:39.840 --> 15:46.400
optimization that we managed to do because this way we went into the code into that function

15:46.400 --> 15:54.800
and realized how we were basically creating a bunch of functions this is react code and a bunch of

15:54.800 --> 16:00.880
string UIDs and saving all of them in a map and apparently that's incredibly inefficient that's

16:00.880 --> 16:05.920
probably not code that you look at the first at the first glance that it seems inefficient but if

16:05.920 --> 16:13.840
you call it thousands of times this is apparently sticking up 80 megs of RAM and so how did we solve

16:13.840 --> 16:24.160
this we refactored it a bit by using a set instead of a map and so it's very experiment based and

16:24.160 --> 16:29.760
with this we were able to have like a 50 percent improvement of memory usage which was huge and

16:29.760 --> 16:36.000
this really made the difference between being able to load some project at all or projects that

16:36.000 --> 16:41.760
would just crush your browser like documents and so that was one of the first big wins that we had

16:42.720 --> 16:48.320
so we were like yeah okay let's continue this eventually we will reach zero megs of memory

16:48.320 --> 16:56.880
use right no immediately after that we hit pretty much a brick wall in which we were taking hit

16:56.880 --> 17:03.600
snapshots and we were seeing that we had two million objects that were taking a lot of space

17:03.600 --> 17:10.000
and it's not that that like we had one big object to optimize each one of them was a couple of bytes

17:10.000 --> 17:16.880
and the heat profiler really doesn't help you in those cases and that's interesting because

17:16.880 --> 17:22.240
that's pretty much the same situation that you will find if you try to profile that same notion

17:22.560 --> 17:28.800
that I've tested it before or even Airbnb as it's actually the same problem and unfortunately the

17:28.800 --> 17:38.640
answer is the problem is react kind of and like we are in the same situation also with notion we have

17:41.040 --> 17:49.360
is it two geeks of ground no that can't be that is just being occupied by a lot of small objects

17:49.440 --> 17:56.400
so yeah we hit a brick wall but what we do now like the heat profiler is very bad and analyzing

17:56.400 --> 18:04.240
those kind of stuff thankfully we can export from it we can export a giant five gigabytes json from

18:05.840 --> 18:11.520
from chrome and then we look at the json and we see that the json it's in a format that is pretty

18:11.520 --> 18:18.160
much unreadable but thankfully there is someone that did work for us the guys at meta and it is

18:19.120 --> 18:25.280
beautiful tool called memelab which it's a toolkit for exploring memory usage

18:26.240 --> 18:31.920
which is very focused on finding memory leaks it has like an entire automation for that but I think

18:31.920 --> 18:38.880
this is even more it's even cooler because it provides you a very powerful API for opening

18:38.880 --> 18:46.160
snapshots from chrome and analyzing them what you can do is that basically you can read the objects

18:46.160 --> 18:55.600
in memory and perform analytics on them for example we wanted to answer this question which type of

18:55.600 --> 19:02.720
objects are taking up the most space out of the two millions that we found in a snapshot well

19:04.560 --> 19:11.280
this is a some code that we wrote I think that we don't have time to go too much into it but I can

19:11.280 --> 19:16.800
publish it the idea is that we can load the snapshot so load the current state of memory

19:16.800 --> 19:21.760
and find all the object types like what are what is the like the type skip type of the object

19:22.320 --> 19:30.720
computed total shallow size for each type and then sort and print results and from these the results

19:30.720 --> 19:37.840
were very cool because the um which is we had the for each object type even including like the

19:38.560 --> 19:44.240
the keys of the object how much memory they were occupying and which is we were able to see that

19:44.240 --> 19:52.320
on the top two we have one object that is called fiber node which is from react and another node

19:52.320 --> 19:59.760
another object that had base q base state memo state what the next q what is that that is not

19:59.760 --> 20:04.320
something that came from our application that's react again that's the data structure that is

20:04.320 --> 20:11.360
used internally for keeping tracks of hooks and so like we went into react to me so that there was

20:11.360 --> 20:17.040
exactly that other structure which in most websites that are using react heavily nowadays is pretty

20:17.040 --> 20:23.680
much the thing that is occupying the most memory with enough so it is we figured out that keeping

20:23.680 --> 20:32.160
tracks of hooks is expensive and but are we supposed to just tear down the 400 000 lines of

20:32.240 --> 20:39.120
react that we have in our app right now like that's a bit too far into the development so we wanted

20:39.120 --> 20:47.440
to know precisely what we need to optimize so we use memlum again this time we uh we see like even

20:47.440 --> 20:53.760
more uh by looking at this fiber node data structure that is used by reactor and we need a lot of

20:53.760 --> 21:00.000
statistics on it to try to figure out what is the react component that is taking up the most

21:00.080 --> 21:05.440
memory so that we can optimize that specific component first and we managed to do this because

21:05.440 --> 21:13.040
this way we were able to divide the odd memory uses by react component and see each hook how much

21:13.920 --> 21:19.760
memory it was using and with this we were able to find out a specific react component that

21:20.640 --> 21:26.400
was using a lot of memory and we cut the memory users down again by 60 percent which was pretty nice

21:26.400 --> 21:32.240
so that's like memlum saved us with this because we were able to make our app properly working

21:33.040 --> 21:42.720
and it also made us possible to answer other questions like as out of all the strings that we

21:42.720 --> 21:49.600
have in our app how many of those are uids should we start optimizing uids and make them numbers

21:49.600 --> 21:55.360
well no because we used memlum to find all the uids and we found out it was like two megabytes in

21:55.360 --> 22:03.200
total so who cares so that's also nice to know what to not prematurely optimize so just to sum up

22:03.200 --> 22:08.800
everything that i said i think that we can all agree that memory analysis is actually difficult

22:08.800 --> 22:13.280
especially because it varies so much between application between framework between browsers

22:14.160 --> 22:22.640
but it's important even if even in a world like nowadays in which we have a lot of a lot of round

22:22.640 --> 22:27.360
because for some apps it really makes a difference it makes the difference between you being able to

22:27.360 --> 22:34.480
use the notion on your phone or the app constantly crashing and never loading your data and

22:36.000 --> 22:41.680
that thing is that the chrome profiler is cool but sometimes it's not enough but thankfully it can

22:41.680 --> 22:49.040
export so that at least you can perform your own analysis externally so thank you for listening to

22:49.040 --> 22:53.440
representation thank you

22:58.000 --> 23:00.320
are there any questions i see a question here

23:02.800 --> 23:09.440
yes you were talking about the shallow size versus retained size yeah when would you ever

23:09.440 --> 23:16.400
be interested in looking at the shallow size sounds like the more interesting one yeah he asked about

23:16.400 --> 23:21.280
when do we care about shallow size when we also have the retained size well i yeah we care a lot

23:21.280 --> 23:25.360
about shallow size in our case it was all about shallow size where to write our own custom

23:25.920 --> 23:32.800
plugging for memblab to just analyze shallow size why because if you are analyzing like very big

23:32.800 --> 23:40.240
objects there are thousands of lines and in that case you have to use tricks like even virtual

23:40.240 --> 23:44.960
scrolling if you know that you can have like instead of allocating all the DOM elements you

23:44.960 --> 23:49.760
keep reusing the same ones and you think about that that's like ejecting from react because you are

23:49.760 --> 23:55.440
creating something just with javascript and the DOM and then you are creating a reactive

23:55.440 --> 23:59.280
wrapper for it so that's another thing that it shows that yeah react is good at orchestrating

23:59.280 --> 24:04.640
stuff but when it comes to the performance critical things that you want to have inside your

24:04.640 --> 24:14.640
application then you need to start optimizing it differently just a small mark or we continue

24:14.720 --> 24:21.760
with the questions so please if there are spaces please try to squeeze and not leave spaces in the

24:21.760 --> 24:29.920
middle as you could see we have hundreds of people waiting outside and here as well and we cannot have

24:29.920 --> 24:36.800
that many people on the sides so please try to squeeze don't let free seats for your jackets or

24:36.800 --> 24:46.080
something put it on your lap thank you and since we're starting to be quite a lot if you're going

24:46.080 --> 24:53.200
to go out please try to go out from the right side and avoid going out from the left side so that

24:53.200 --> 25:02.320
it's easier for everyone thank you we have a question here first i've got more more as a comment

25:02.320 --> 25:06.960
instead of a question so the thing is that with this limitation of four gigabytes for memory

25:07.920 --> 25:14.160
this comes from the fact that like chrome like compresses pointers so that small objects take

25:14.160 --> 25:18.960
less space basically that's one thing second thing is that is this is like a security mitigation so

25:18.960 --> 25:26.080
that when there is some like back in v8 it's harder to exploit it but also i've read on like a

25:26.080 --> 25:32.080
chromium box tracker that there is for example 16 gigabytes limit for fixed arrays so there may

25:32.080 --> 25:37.200
be different limitations for different things like web assembly also has a different limitation

25:37.200 --> 25:45.200
and also supposedly like electron abs doesn't have limits so yeah yeah that that's very cool thank you

25:45.520 --> 26:01.200
i think that firefox has pretty much the same limitations

26:02.800 --> 26:09.200
oh ask me if we're also trying with other browser yeah i'm mostly working on firefox actually and

26:09.200 --> 26:15.920
firefox has very similar limitation and sometimes it's even worse because sometimes we notice that

26:17.040 --> 26:23.440
the upper randomly sometimes takes more memory in firefox for some reason or some things are more

26:23.440 --> 26:30.160
optimized in firefox other things are more optimized in chrome so that that's very complicated to answer

26:30.160 --> 26:35.520
unfortunately because it seems like that the answer is either you look deeply into the source code of

26:35.520 --> 26:42.480
the browsers which is i still haven't reached that point unfortunately or you do try an error

26:47.200 --> 26:53.280
ah no the tooling um you know firefox also has tooling around it which is actually if i remember

26:53.280 --> 27:00.160
correctly more focused around analyzing the memory users of the DOM elements and it also has some

27:00.160 --> 27:09.200
facilities for for analyzing ip snapshots but since like memlab users works with chrome

27:09.200 --> 27:11.600
ip snapshots we went with that immediately

27:16.880 --> 27:18.800
and how do you go about running this in ci?

27:19.680 --> 27:25.840
oh um yeah that's a complicated thing because running in ci it's pure pain

27:26.800 --> 27:33.040
like you can use memlab and run it in ci because it uses playwright i don't remember if it uses

27:33.040 --> 27:40.480
playwright or puppeteer i think puppeteer and with it you can like orchestrate some some tests

27:40.480 --> 27:46.880
that open a page it can even like use some machine learning algorithm to find memory leaks the problem

27:46.880 --> 27:52.560
with doing that is that it's fine if your app is small if your app starts to become bigger then

27:52.640 --> 27:57.680
uh you will need to have a ci machine that is powerful enough to be able to run your app

27:58.240 --> 28:07.360
and the profiler on top of it which for us it meant that the the ci time went like in 30 minutes

28:07.920 --> 28:11.360
which was unacceptable so eventually we removed it but you can do it

28:15.760 --> 28:17.360
are there questions as your question there

28:23.120 --> 28:26.000
so from the browser or something like that?

28:26.720 --> 28:34.320
yeah that's another complicated thing because if you are using chrome i don't think that firefox

28:34.320 --> 28:39.200
allows you that but chrome does you have a specific performance or memory i think

28:40.080 --> 28:46.640
variable that you can use and you can check both the maximum heap allowed size and you can also

28:47.600 --> 28:54.560
read an estimate of the current family usage in our case once we do that we are constantly like

28:55.120 --> 29:02.800
giving data to segment then we analyze in amplitude with which we can like keep track of memory usage

29:02.800 --> 29:07.200
and we are also doing that for like the performance timing the problem is that we notice that

29:07.840 --> 29:16.560
that data who very quickly becomes bogus because it depends a lot on what the user is doing

29:16.560 --> 29:20.880
and when the garbage collector kicks in because the garbage collector sometimes is like it goes

29:20.880 --> 29:26.320
up to four gigabytes and then no problem goes down to 500 megabytes so it's extremely difficult to

29:26.320 --> 29:31.360
capture memory usage because you don't have a precise memory a precise measure on how much

29:32.480 --> 29:37.040
of the total retained memory is active and how much is actually inactive and going to be garbage

29:37.040 --> 29:44.000
collected soon so we try to do it and we have some charts showing how much memory is being used but

29:44.560 --> 29:48.080
it's very hard to make sense of them unfortunately

29:51.200 --> 29:56.080
any other questions you still have around five minutes for questions

