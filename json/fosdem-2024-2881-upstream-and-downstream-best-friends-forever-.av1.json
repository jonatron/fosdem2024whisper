{"text": " to introduce the speaker. Thank you. Hi, folks. Good afternoon. Welcome to the evening sessions. I'm going to turn it over to our next speaker, Franzy, to introduce his set. Just a couple of housekeeping rules if you could make sure phones, et cetera, are on silent. When you're taking a seat, they can be loud. Make sure that you do them gently and try and keep the talking to a minimum. Thank you. Hello, everyone. I'm Franzy Szek. I'm a product owner of the packet project. I will use this project as an example during the talk. Thanks, everyone, for coming. And I would like to also view things from you so don't sneak out with the doors behind. When I was thinking about this talk, I was thinking that if people come here, they already maybe had issues like me and already were thinking about it. So let's use their ideas as well and don't just talk for half of an hour and let them show and share. So I would like you to connect to this URL or just use menti.com and use this number to just connect to the slides so you can also provide some feedback for me or others. I hope it will not break the Wi-Fi or disappear or something, so we'll see how it goes. And this is an example question. Thank you for putting the answers there. That's not only to test it, but also so we know what are you coming from or what are the, what is your background? So let's give you a couple of more seconds. Wow, so many, so many. Okay. Yeah, and also a positive thing is that if you don't see the slides correctly, you can watch them on the screen and, yeah, sorry those who wanted to just fix some bugs in the meantime or check the next session so you need to use the phone or laptop for this. So sorry about that. Okay, so we'll move on. In the title, there was two times mentioned stream, but what do I mean by that? And it's what I mean by that is a stream of code or the program that comes from up, from the developers, down, down, down to the users. So that's the stream I have in mind. And you can have various pieces on the way and anything what goes up to the developer is an upstream. What goes down to the user is downstream. So for example, Fedora is a downstream when looking from the developer point of view or from GitHub, GitHub, but it can be also an upstream for CentoStream for rel. CentoStream is a downstream of Fedora but upstream of rel. So depends always from what place we are looking from. For this talk, when I mentioned upstream, I mean a Gitforge development, the GitHub, GitLab. By downstream, I mean some Linux distribution, for example Fedora in my case. I tried to use upstream developers and downstream maintainers to make it really clear, but just so you know. So just to check, try to show others where do you belong? Are you more a maintainer downstream guy or are you more an upstream developer, maybe curious how you can get to the distribution. So let's show others how we stand here and what I'm talking to. So mostly maintainers and if you are both upstream developer and downstream maintainer, you are somehow in the middle. Okay, so it's not moving much, so I'll continue. Okay, so let's back to the package project I mentioned in the title slide. Something around five years ago with few people around. We were thinking that we will create a new project and as a goal, we said, hey, let's make upstream and downstream closer together. So let's provide some downstream feedback to the development and also for downstream maintainers, let's provide them some connection to the upstream. For example, when they release new code in upstream, so let's get it automatically to downstream. And it was like, yeah, that will be awesome, everyone will be happy. So we started work on that and few months ago, yeah, we came to the upstream developers and said, yeah, we have this federal integration for your project and it's really easy and you will have like new functionality for your project and can be sure that your code will run. The feedback wasn't so positive and we were really surprised because, yeah, we are trying to help you. So what do you think why developers might care about downstream? Why they might even bother why they shouldn't just live on GitHub, GitHub website and live their awesome life and don't care about any distribution at all. So, yeah, hard question. I hope you are typing. Yeah, availability, software option. Wow, wow, so, for me, articles, okay, yeah, many, many reasons. Yeah, without distribution, they might have no users. Yeah, a lot of obvious things. Just to note, after the session, I'll share the results with you so maybe I'll also set up some blog posts but you will have it attached and, yeah, wow, so many things. It looks like it makes sense to care about downstream. So just a couple more seconds. Yeah, people, shitty tools, yeah, revenue maybe also. Yeah, sometimes there is a middleman that you don't need to tackle the video users. Sometimes you just don't want users maybe. Okay, so let's move on. So, we asked maintenance. Hey, maintenance, we have this nice service for you that will automatically send your upstream releases to your service. Always. We were very positive that we helped people. I don't care if they produce new code. It's definitely a new box and more work for me. So, I'm not sure if I want this service. So, same question. Why do you think maintenance should care about upstream? Why there should not be just the upstream that don't produce anything and I can happily leave just rebuild my package every half a year or so when there is a new version of the Linux distribution and live in peace. Yeah, users want new releases, yeah. New features that's related. Missing updates. Similar stuff, similar stuff. Yeah, bug fixes. Yeah. Writing the code is hard and I really don't want to do that and all the patching, yeah. Can you be a maintenance upstim? Looks like not, but yeah, there are a lot of maintenance with dead upstream. But availability, security fixes, stability. So, yeah, we have at least 17 reasons why to do that. So, I think it makes sense to care about upstream. Yeah, if there is no upstream project, then there is no downstream project. So, that makes sense. Okay. So, we really wanted to help people and it was quite a surprise that we were honest on that and that was our goal to bring upstream and downstream closer together. There was nothing hidden, just a really clear goal to help people. So, on the way, after these feedbacks, we also get some positive ones and also there were people that were both upstream and downstream and after all we get some users and also users that provided some feedback and after like these, somehow 40 years, I can say that we are saving some time people and helping them and there are also great people that uses our project. So, it looks like it makes some sense. But it wasn't easy and we are not done definitely and we've collected various feedback and complaints on the way. So, let's pick few typical sentences that we've heard during those years and let's take a look what we can do to help in those situations. So, the first one, when things go wrong, I don't want to look into the logs. I don't understand the downstream logs. There is some build failure and I don't understand it. So, what would you do in this situation? You are providing a build system integration or maybe like you are running RPM builds for the upstream pull requests or testing on Ubuntu or anything like that. Just the downstream feedback for any upstream change and people don't want to tackle with the downstream logs. What, when things go wrong? What would you do? How to help with that? Yeah, reliable mechanism for filling bugs. Yeah, definitely. If the problem is like a packaging problem, anything else what we can do? Okay, so it would be transparent. So, if we need to give the logs, so let's let them suffer as well. Yes, I should have 20 relevant logs to find the one relevant. Yeah, so help with some home combing those. Cool logging libraries. I'm still missing some crucial. Yeah, you can snap or flat peg, yes. But you can get like a failure from creating the snap. So, it's, we can treat like snap or flat peg like this and another distribution maybe if either of the ends but still. Yeah. Okay, I hope that's my job. Okay, I'm still missing one crucial. Like the obvious one. You know that is not possible. Probably that's why the response is not here. So, better logs. Yeah, it's usually not so easily possible. Sometimes yes, sometimes we can do something about it but with these systems, if you've been on the talk like the Dant had an hour ago about all the federal systems we had in place, yeah, we are trying to integrate with all of them and you, for example, copper had multiple logs and all the systems have different logs. So, and they use more or other tools. So, this is layered and we don't have power on all the logs. But maybe we can, as someone mentioned correctly, we can be good in the aggregation or some visualization or we can use AI. Okay, just kidding but a few colleagues and me are actually working on something like that. They are trying to collect various logs with the failures and trying to get like human input what's going on here and how to fix that. So, if you are interested in that, check this out. I really hope this will happen and will produce some really nice data set that can help us to provide some really nice way how to not tackle with hundreds of lines of logs that I don't need to tackle. So, that's just in the beginning but I'm really looking forward to that. Next thing would help us is provide nice notifications and also connect by that, I mean connect people that can help. And it relates also to the last point that we need to set really clear expectations. Who is responsible for what? Who should take a look? Yes, sometimes it's not clear, sometimes it can be really valid back in the code that is sketched quite soon and that's really nice but sometimes it can be downstream issue and sometimes it's something in the middle and so it's really nice when we are introducing these two to make clear expectations. Who will, which like maybe also time-based and with the notifications we can maybe ping people that can help. Okay, so just a single distribution. Why can't you support all of them? So, what would you do? Yeah, there are people from various distributions so that's maybe common that if you want to introduce some CI or anything that, yeah, but if I enable this for Fedora, I want also the BN, SUSE and everything. Can we help somehow? What would you do? Yeah, we can use build systems that support multiple targets like copper or OBS and these. Yeah, snap, flatback, yeah, we can like switch the distribution but when we discuss in the very beginning, we might want to care still in those distributions that we maybe work on. Yes, some distributions, yeah, a lot of distributions are really, really different and it's hard to somehow compare or maybe some obstruction. Yeah, I'm lost in all the good suggestions. I'll read them probably later. So, yeah, it's probably not possible, definitely not all. There are a lot of distributions but as someone suggested, we can maybe try, maybe we can also have open source that someone can contribute, maybe some architecture that can maybe combine various backends so we can maybe share. If you have an open source, so for example, then we'll come from SUSE and say, hey, let's support impact it also OBS and we can maybe collaborate so that's also possible. The tricky one is we are used to do distro specific terms and we need to be really careful about those because when we mention various scratch builds and patches, metadata, bugs, and all the weird terminology, it might be really, it also might be a reason why they are scared and developers don't want to hear about those. So, describe those terms and be careful and also you can hide those somehow so we don't need to speak about co-properties but we can maybe mention RPM builds and these things. So, yeah, what helps us is also we are supporting various functionality types and what helps is to provide easy and reliable testing like infrastructure or so if they can rely on that and they can run their test code or run their tests on this reliable infrastructure we are using for example testing bar project for that so we don't need to do that ourselves. And easy on boarding, I'll probably mention that multiple times but it's crucial because if they hid like the very first problem during the way and yeah with those distribution things it's not easy and we've spoiled it multiple times but it's important. Yes, sorry, I don't want to have more files in the repository. So, CI system that's generated maybe for any upstream CI so yet another config file. Don't be lazy, yeah. Yeah, there are thousands of files in the repository and you don't like yet another one or next to. Yeah. A few interesting things. Anything interesting? You see, okay, so yeah, we can put more complaints. Yeah, yeah. Very interesting things. Yes, I'll probably move on and read those later. It's interesting but so yeah, we might want to stick with the one line if possible, one file if possible because if it is possible but still better one file than multiple files and also not sure why but people rather put a shell script into the JSON or YAML instead of providing a shell script and specify a name of this shell script in the YAML file so we can help them do that. Yeah, if there is more content we can maybe let them link it. We can also enable some custom locations, custom name of or some sub directory so they can maybe hide it a bit and also for example Zoo project uses global configuration if people really don't want to put anything to their git repository just provide a separate git repository they can create a pull request and enable this. It's tricky from the developer point of view like how to do that, how all the messaging should work but yeah. Yeah, I have my own automation works well. So, I have my script or anything and I'm happy with it. How to live? Yeah. Yeah. Use on Civo. Yeah. Okay. Good for you. Yeah, standard protocols. I'll move on. So, this is a generic when we want someone to start using something else even like with the comparable tools we need some killer feature. We don't, having just the same feature set is not enough. They need to have clear motivation to have something extra when they move. Easy onboarding. I've mentioned that this is crucial. We are for example trying to do some online workshop with the gutter platform and various funny stuff to help them. Also, and that can be the killer feature if things break. The people or the users does not need to take a look and fix that and then that can save a lot of time. So, with the maintenance of this automation and we can save them a lot of time and we need to clearly communicate this but we need to also do that. So, if things break we should take a look and just ignore it. And work on the right things. So, listen to the community, listen to the people and try to like don't just assume that you are doing the right features but just ask and get something there. Yeah, your automation can break some rules for example packaging rules. How do you tackle these things? And yes, sometimes when those packaging guidelines were created the automation wasn't such a thing or maybe when they have written that they expected that humans will interact with the packages and maybe standardization, who's rules, yeah. Yeah, we can also tweak the rules. It's not set in stone so we can maybe discuss. Yeah. I trust the both modern and human. Yeah, that's a positive thing on the automation that yeah, it usually does not do like the human like things and be kind and that's I really like. So yeah, be open for suggestion, communicate, don't ignore the issues. Just talk and see what others think about that because sometimes it can be like really valuable feedback and maybe people already have a suggestion how to fix that or how you should behave. Yeah, sometimes you can also let the user decide for example we've talked about an issue if we should upload some archives to the cache if we should do that before it's merged into federal this gate and we were not sure we someone wants the automation someone. Yeah, someone wants just the security so let them decide. Yeah, and for us it helped that we are trying as much as possible to have the provisions as a regular user so we are not kind of special in any way with one slide exception to get less permissions that we need but otherwise we are trying. Yeah, so. Similar to the previous one. You can continue with the voting but I'll skip to two points I had. Yeah, when people think that they you behave differently we can provide some config options but usually bait. There can maybe be maybe people will realize that they don't need it but maybe they will be a new user that will have similar config option or similar feature request so you can maybe combine it for us. For us user defined actions helped a lot because we've done everyone has a different workflow and it was really hard to do securely and well but this was for us a huge and other. Yeah, and respond to the first issue and questions crucial even if it is like a little you weeks thingy that you are showing how you how you treat your users. So that's all from me. This is the project page for Stornon account and if we have maybe two minutes for question if you have any but maybe you can ask the audience. We don't have. Okay, so sorry about that but I think you've shared your opinion on that. So thanks a lot everyone. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.0, "text": " to introduce the speaker.", "tokens": [50364, 281, 5366, 264, 8145, 13, 50464], "temperature": 0.0, "avg_logprob": -0.38697117298572986, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.714923620223999}, {"id": 1, "seek": 0, "start": 3.0, "end": 5.0, "text": " Thank you.", "tokens": [50514, 1044, 291, 13, 50614], "temperature": 0.0, "avg_logprob": -0.38697117298572986, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.714923620223999}, {"id": 2, "seek": 0, "start": 7.0, "end": 10.0, "text": " Hi, folks. Good afternoon. Welcome to the evening sessions.", "tokens": [50714, 2421, 11, 4024, 13, 2205, 6499, 13, 4027, 281, 264, 5634, 11081, 13, 50864], "temperature": 0.0, "avg_logprob": -0.38697117298572986, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.714923620223999}, {"id": 3, "seek": 0, "start": 10.0, "end": 13.0, "text": " I'm going to turn it over to our next speaker, Franzy, to", "tokens": [50864, 286, 478, 516, 281, 1261, 309, 670, 281, 527, 958, 8145, 11, 17288, 1229, 11, 281, 51014], "temperature": 0.0, "avg_logprob": -0.38697117298572986, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.714923620223999}, {"id": 4, "seek": 0, "start": 13.0, "end": 16.0, "text": " introduce his set. Just a couple of housekeeping rules if you", "tokens": [51014, 5366, 702, 992, 13, 1449, 257, 1916, 295, 48033, 4474, 498, 291, 51164], "temperature": 0.0, "avg_logprob": -0.38697117298572986, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.714923620223999}, {"id": 5, "seek": 0, "start": 16.0, "end": 19.0, "text": " could make sure phones, et cetera, are on silent. When you're", "tokens": [51164, 727, 652, 988, 10216, 11, 1030, 11458, 11, 366, 322, 12784, 13, 1133, 291, 434, 51314], "temperature": 0.0, "avg_logprob": -0.38697117298572986, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.714923620223999}, {"id": 6, "seek": 0, "start": 19.0, "end": 22.0, "text": " taking a seat, they can be loud. Make sure that you do them", "tokens": [51314, 1940, 257, 6121, 11, 436, 393, 312, 6588, 13, 4387, 988, 300, 291, 360, 552, 51464], "temperature": 0.0, "avg_logprob": -0.38697117298572986, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.714923620223999}, {"id": 7, "seek": 0, "start": 22.0, "end": 26.0, "text": " gently and try and keep the talking to a minimum. Thank you.", "tokens": [51464, 13073, 293, 853, 293, 1066, 264, 1417, 281, 257, 7285, 13, 1044, 291, 13, 51664], "temperature": 0.0, "avg_logprob": -0.38697117298572986, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.714923620223999}, {"id": 8, "seek": 2600, "start": 26.0, "end": 30.0, "text": " Hello, everyone. I'm Franzy Szek. I'm a product owner of the", "tokens": [50364, 2425, 11, 1518, 13, 286, 478, 17288, 1229, 24699, 916, 13, 286, 478, 257, 1674, 7289, 295, 264, 50564], "temperature": 0.0, "avg_logprob": -0.1794612530580501, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.32635045051574707}, {"id": 9, "seek": 2600, "start": 30.0, "end": 34.0, "text": " packet project. I will use this project as an example during", "tokens": [50564, 20300, 1716, 13, 286, 486, 764, 341, 1716, 382, 364, 1365, 1830, 50764], "temperature": 0.0, "avg_logprob": -0.1794612530580501, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.32635045051574707}, {"id": 10, "seek": 2600, "start": 34.0, "end": 39.0, "text": " the talk. Thanks, everyone, for coming. And I would like to", "tokens": [50764, 264, 751, 13, 2561, 11, 1518, 11, 337, 1348, 13, 400, 286, 576, 411, 281, 51014], "temperature": 0.0, "avg_logprob": -0.1794612530580501, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.32635045051574707}, {"id": 11, "seek": 2600, "start": 39.0, "end": 44.0, "text": " also view things from you so don't sneak out with the doors", "tokens": [51014, 611, 1910, 721, 490, 291, 370, 500, 380, 13164, 484, 365, 264, 8077, 51264], "temperature": 0.0, "avg_logprob": -0.1794612530580501, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.32635045051574707}, {"id": 12, "seek": 2600, "start": 44.0, "end": 50.0, "text": " behind. When I was thinking about this talk, I was thinking", "tokens": [51264, 2261, 13, 1133, 286, 390, 1953, 466, 341, 751, 11, 286, 390, 1953, 51564], "temperature": 0.0, "avg_logprob": -0.1794612530580501, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.32635045051574707}, {"id": 13, "seek": 2600, "start": 50.0, "end": 55.0, "text": " that if people come here, they already maybe had issues like", "tokens": [51564, 300, 498, 561, 808, 510, 11, 436, 1217, 1310, 632, 2663, 411, 51814], "temperature": 0.0, "avg_logprob": -0.1794612530580501, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.32635045051574707}, {"id": 14, "seek": 5500, "start": 55.0, "end": 60.0, "text": " me and already were thinking about it. So let's use their", "tokens": [50364, 385, 293, 1217, 645, 1953, 466, 309, 13, 407, 718, 311, 764, 641, 50614], "temperature": 0.0, "avg_logprob": -0.123274663599526, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.047070637345314026}, {"id": 15, "seek": 5500, "start": 60.0, "end": 65.0, "text": " ideas as well and don't just talk for half of an hour and let", "tokens": [50614, 3487, 382, 731, 293, 500, 380, 445, 751, 337, 1922, 295, 364, 1773, 293, 718, 50864], "temperature": 0.0, "avg_logprob": -0.123274663599526, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.047070637345314026}, {"id": 16, "seek": 5500, "start": 65.0, "end": 72.0, "text": " them show and share. So I would like you to connect to this", "tokens": [50864, 552, 855, 293, 2073, 13, 407, 286, 576, 411, 291, 281, 1745, 281, 341, 51214], "temperature": 0.0, "avg_logprob": -0.123274663599526, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.047070637345314026}, {"id": 17, "seek": 5500, "start": 72.0, "end": 80.0, "text": " URL or just use menti.com and use this number to just connect", "tokens": [51214, 12905, 420, 445, 764, 3074, 72, 13, 1112, 293, 764, 341, 1230, 281, 445, 1745, 51614], "temperature": 0.0, "avg_logprob": -0.123274663599526, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.047070637345314026}, {"id": 18, "seek": 5500, "start": 80.0, "end": 84.0, "text": " to the slides so you can also provide some feedback for me or", "tokens": [51614, 281, 264, 9788, 370, 291, 393, 611, 2893, 512, 5824, 337, 385, 420, 51814], "temperature": 0.0, "avg_logprob": -0.123274663599526, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.047070637345314026}, {"id": 19, "seek": 8400, "start": 84.0, "end": 89.0, "text": " others. I hope it will not break the Wi-Fi or disappear or", "tokens": [50364, 2357, 13, 286, 1454, 309, 486, 406, 1821, 264, 14035, 12, 13229, 420, 11596, 420, 50614], "temperature": 0.0, "avg_logprob": -0.15555191040039062, "compression_ratio": 1.4901960784313726, "no_speech_prob": 0.036589790135622025}, {"id": 20, "seek": 8400, "start": 89.0, "end": 93.0, "text": " something, so we'll see how it goes. And this is an example", "tokens": [50614, 746, 11, 370, 321, 603, 536, 577, 309, 1709, 13, 400, 341, 307, 364, 1365, 50814], "temperature": 0.0, "avg_logprob": -0.15555191040039062, "compression_ratio": 1.4901960784313726, "no_speech_prob": 0.036589790135622025}, {"id": 21, "seek": 8400, "start": 93.0, "end": 97.0, "text": " question. Thank you for putting the answers there. That's not", "tokens": [50814, 1168, 13, 1044, 291, 337, 3372, 264, 6338, 456, 13, 663, 311, 406, 51014], "temperature": 0.0, "avg_logprob": -0.15555191040039062, "compression_ratio": 1.4901960784313726, "no_speech_prob": 0.036589790135622025}, {"id": 22, "seek": 8400, "start": 97.0, "end": 105.0, "text": " only to test it, but also so we know what are you coming from", "tokens": [51014, 787, 281, 1500, 309, 11, 457, 611, 370, 321, 458, 437, 366, 291, 1348, 490, 51414], "temperature": 0.0, "avg_logprob": -0.15555191040039062, "compression_ratio": 1.4901960784313726, "no_speech_prob": 0.036589790135622025}, {"id": 23, "seek": 8400, "start": 105.0, "end": 112.0, "text": " or what are the, what is your background? So let's give you a", "tokens": [51414, 420, 437, 366, 264, 11, 437, 307, 428, 3678, 30, 407, 718, 311, 976, 291, 257, 51764], "temperature": 0.0, "avg_logprob": -0.15555191040039062, "compression_ratio": 1.4901960784313726, "no_speech_prob": 0.036589790135622025}, {"id": 24, "seek": 11200, "start": 112.0, "end": 123.0, "text": " couple of more seconds. Wow, so many, so many. Okay. Yeah, and", "tokens": [50364, 1916, 295, 544, 3949, 13, 3153, 11, 370, 867, 11, 370, 867, 13, 1033, 13, 865, 11, 293, 50914], "temperature": 0.0, "avg_logprob": -0.12944201301125918, "compression_ratio": 1.5628140703517588, "no_speech_prob": 0.05400291830301285}, {"id": 25, "seek": 11200, "start": 123.0, "end": 127.0, "text": " also a positive thing is that if you don't see the slides", "tokens": [50914, 611, 257, 3353, 551, 307, 300, 498, 291, 500, 380, 536, 264, 9788, 51114], "temperature": 0.0, "avg_logprob": -0.12944201301125918, "compression_ratio": 1.5628140703517588, "no_speech_prob": 0.05400291830301285}, {"id": 26, "seek": 11200, "start": 127.0, "end": 131.0, "text": " correctly, you can watch them on the screen and, yeah, sorry", "tokens": [51114, 8944, 11, 291, 393, 1159, 552, 322, 264, 2568, 293, 11, 1338, 11, 2597, 51314], "temperature": 0.0, "avg_logprob": -0.12944201301125918, "compression_ratio": 1.5628140703517588, "no_speech_prob": 0.05400291830301285}, {"id": 27, "seek": 11200, "start": 131.0, "end": 136.0, "text": " those who wanted to just fix some bugs in the meantime or check", "tokens": [51314, 729, 567, 1415, 281, 445, 3191, 512, 15120, 294, 264, 14991, 420, 1520, 51564], "temperature": 0.0, "avg_logprob": -0.12944201301125918, "compression_ratio": 1.5628140703517588, "no_speech_prob": 0.05400291830301285}, {"id": 28, "seek": 11200, "start": 136.0, "end": 140.0, "text": " the next session so you need to use the phone or laptop for this.", "tokens": [51564, 264, 958, 5481, 370, 291, 643, 281, 764, 264, 2593, 420, 10732, 337, 341, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12944201301125918, "compression_ratio": 1.5628140703517588, "no_speech_prob": 0.05400291830301285}, {"id": 29, "seek": 14000, "start": 140.0, "end": 147.0, "text": " So sorry about that. Okay, so we'll move on. In the title, there", "tokens": [50364, 407, 2597, 466, 300, 13, 1033, 11, 370, 321, 603, 1286, 322, 13, 682, 264, 4876, 11, 456, 50714], "temperature": 0.0, "avg_logprob": -0.14847674435132172, "compression_ratio": 1.5182926829268293, "no_speech_prob": 0.12116283178329468}, {"id": 30, "seek": 14000, "start": 147.0, "end": 153.0, "text": " was two times mentioned stream, but what do I mean by that?", "tokens": [50714, 390, 732, 1413, 2835, 4309, 11, 457, 437, 360, 286, 914, 538, 300, 30, 51014], "temperature": 0.0, "avg_logprob": -0.14847674435132172, "compression_ratio": 1.5182926829268293, "no_speech_prob": 0.12116283178329468}, {"id": 31, "seek": 14000, "start": 153.0, "end": 159.0, "text": " And it's what I mean by that is a stream of code or the program", "tokens": [51014, 400, 309, 311, 437, 286, 914, 538, 300, 307, 257, 4309, 295, 3089, 420, 264, 1461, 51314], "temperature": 0.0, "avg_logprob": -0.14847674435132172, "compression_ratio": 1.5182926829268293, "no_speech_prob": 0.12116283178329468}, {"id": 32, "seek": 14000, "start": 159.0, "end": 163.0, "text": " that comes from up, from the developers, down, down, down to", "tokens": [51314, 300, 1487, 490, 493, 11, 490, 264, 8849, 11, 760, 11, 760, 11, 760, 281, 51514], "temperature": 0.0, "avg_logprob": -0.14847674435132172, "compression_ratio": 1.5182926829268293, "no_speech_prob": 0.12116283178329468}, {"id": 33, "seek": 16300, "start": 163.0, "end": 171.0, "text": " the users. So that's the stream I have in mind. And you can have", "tokens": [50364, 264, 5022, 13, 407, 300, 311, 264, 4309, 286, 362, 294, 1575, 13, 400, 291, 393, 362, 50764], "temperature": 0.0, "avg_logprob": -0.12715525743437978, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.24328051507472992}, {"id": 34, "seek": 16300, "start": 171.0, "end": 176.0, "text": " various pieces on the way and anything what goes up to the", "tokens": [50764, 3683, 3755, 322, 264, 636, 293, 1340, 437, 1709, 493, 281, 264, 51014], "temperature": 0.0, "avg_logprob": -0.12715525743437978, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.24328051507472992}, {"id": 35, "seek": 16300, "start": 176.0, "end": 181.0, "text": " developer is an upstream. What goes down to the user is downstream.", "tokens": [51014, 10754, 307, 364, 33915, 13, 708, 1709, 760, 281, 264, 4195, 307, 30621, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12715525743437978, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.24328051507472992}, {"id": 36, "seek": 16300, "start": 181.0, "end": 186.0, "text": " So for example, Fedora is a downstream when looking from the", "tokens": [51264, 407, 337, 1365, 11, 7772, 3252, 307, 257, 30621, 562, 1237, 490, 264, 51514], "temperature": 0.0, "avg_logprob": -0.12715525743437978, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.24328051507472992}, {"id": 37, "seek": 16300, "start": 186.0, "end": 190.0, "text": " developer point of view or from GitHub, GitHub, but it can be", "tokens": [51514, 10754, 935, 295, 1910, 420, 490, 23331, 11, 23331, 11, 457, 309, 393, 312, 51714], "temperature": 0.0, "avg_logprob": -0.12715525743437978, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.24328051507472992}, {"id": 38, "seek": 19000, "start": 190.0, "end": 196.0, "text": " also an upstream for CentoStream for rel. CentoStream is a downstream", "tokens": [50364, 611, 364, 33915, 337, 3408, 78, 4520, 1572, 337, 1039, 13, 3408, 78, 4520, 1572, 307, 257, 30621, 50664], "temperature": 0.0, "avg_logprob": -0.24994564056396484, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.021846730262041092}, {"id": 39, "seek": 19000, "start": 196.0, "end": 201.0, "text": " of Fedora but upstream of rel. So depends always from what", "tokens": [50664, 295, 7772, 3252, 457, 33915, 295, 1039, 13, 407, 5946, 1009, 490, 437, 50914], "temperature": 0.0, "avg_logprob": -0.24994564056396484, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.021846730262041092}, {"id": 40, "seek": 19000, "start": 201.0, "end": 208.0, "text": " place we are looking from. For this talk, when I mentioned upstream,", "tokens": [50914, 1081, 321, 366, 1237, 490, 13, 1171, 341, 751, 11, 562, 286, 2835, 33915, 11, 51264], "temperature": 0.0, "avg_logprob": -0.24994564056396484, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.021846730262041092}, {"id": 41, "seek": 19000, "start": 208.0, "end": 214.0, "text": " I mean a Gitforge development, the GitHub, GitLab. By downstream,", "tokens": [51264, 286, 914, 257, 16939, 2994, 432, 3250, 11, 264, 23331, 11, 16939, 37880, 13, 3146, 30621, 11, 51564], "temperature": 0.0, "avg_logprob": -0.24994564056396484, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.021846730262041092}, {"id": 42, "seek": 19000, "start": 214.0, "end": 219.0, "text": " I mean some Linux distribution, for example Fedora in my case. I", "tokens": [51564, 286, 914, 512, 18734, 7316, 11, 337, 1365, 7772, 3252, 294, 452, 1389, 13, 286, 51814], "temperature": 0.0, "avg_logprob": -0.24994564056396484, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.021846730262041092}, {"id": 43, "seek": 21900, "start": 219.0, "end": 224.0, "text": " tried to use upstream developers and downstream maintainers to make it", "tokens": [50364, 3031, 281, 764, 33915, 8849, 293, 30621, 6909, 433, 281, 652, 309, 50614], "temperature": 0.0, "avg_logprob": -0.19476883339159418, "compression_ratio": 1.656441717791411, "no_speech_prob": 0.011411807499825954}, {"id": 44, "seek": 21900, "start": 224.0, "end": 232.0, "text": " really clear, but just so you know. So just to check, try to show", "tokens": [50614, 534, 1850, 11, 457, 445, 370, 291, 458, 13, 407, 445, 281, 1520, 11, 853, 281, 855, 51014], "temperature": 0.0, "avg_logprob": -0.19476883339159418, "compression_ratio": 1.656441717791411, "no_speech_prob": 0.011411807499825954}, {"id": 45, "seek": 21900, "start": 232.0, "end": 238.0, "text": " others where do you belong? Are you more a maintainer downstream guy", "tokens": [51014, 2357, 689, 360, 291, 5784, 30, 2014, 291, 544, 257, 6909, 260, 30621, 2146, 51314], "temperature": 0.0, "avg_logprob": -0.19476883339159418, "compression_ratio": 1.656441717791411, "no_speech_prob": 0.011411807499825954}, {"id": 46, "seek": 21900, "start": 238.0, "end": 243.0, "text": " or are you more an upstream developer, maybe curious how you can", "tokens": [51314, 420, 366, 291, 544, 364, 33915, 10754, 11, 1310, 6369, 577, 291, 393, 51564], "temperature": 0.0, "avg_logprob": -0.19476883339159418, "compression_ratio": 1.656441717791411, "no_speech_prob": 0.011411807499825954}, {"id": 47, "seek": 24300, "start": 244.0, "end": 250.0, "text": " get to the distribution. So let's show others how we stand here", "tokens": [50414, 483, 281, 264, 7316, 13, 407, 718, 311, 855, 2357, 577, 321, 1463, 510, 50714], "temperature": 0.0, "avg_logprob": -0.21048532999478853, "compression_ratio": 1.4577464788732395, "no_speech_prob": 0.12217672914266586}, {"id": 48, "seek": 24300, "start": 250.0, "end": 262.0, "text": " and what I'm talking to. So mostly maintainers and if you are both upstream", "tokens": [50714, 293, 437, 286, 478, 1417, 281, 13, 407, 5240, 6909, 433, 293, 498, 291, 366, 1293, 33915, 51314], "temperature": 0.0, "avg_logprob": -0.21048532999478853, "compression_ratio": 1.4577464788732395, "no_speech_prob": 0.12217672914266586}, {"id": 49, "seek": 24300, "start": 262.0, "end": 268.0, "text": " developer and downstream maintainer, you are somehow in the middle.", "tokens": [51314, 10754, 293, 30621, 6909, 260, 11, 291, 366, 6063, 294, 264, 2808, 13, 51614], "temperature": 0.0, "avg_logprob": -0.21048532999478853, "compression_ratio": 1.4577464788732395, "no_speech_prob": 0.12217672914266586}, {"id": 50, "seek": 26800, "start": 269.0, "end": 278.0, "text": " Okay, so it's not moving much, so I'll continue. Okay, so let's", "tokens": [50414, 1033, 11, 370, 309, 311, 406, 2684, 709, 11, 370, 286, 603, 2354, 13, 1033, 11, 370, 718, 311, 50864], "temperature": 0.0, "avg_logprob": -0.2144687940489571, "compression_ratio": 1.3673469387755102, "no_speech_prob": 0.05783220753073692}, {"id": 51, "seek": 26800, "start": 278.0, "end": 283.0, "text": " back to the package project I mentioned in the title slide. Something", "tokens": [50864, 646, 281, 264, 7372, 1716, 286, 2835, 294, 264, 4876, 4137, 13, 6595, 51114], "temperature": 0.0, "avg_logprob": -0.2144687940489571, "compression_ratio": 1.3673469387755102, "no_speech_prob": 0.05783220753073692}, {"id": 52, "seek": 26800, "start": 283.0, "end": 291.0, "text": " around five years ago with few people around. We were thinking that", "tokens": [51114, 926, 1732, 924, 2057, 365, 1326, 561, 926, 13, 492, 645, 1953, 300, 51514], "temperature": 0.0, "avg_logprob": -0.2144687940489571, "compression_ratio": 1.3673469387755102, "no_speech_prob": 0.05783220753073692}, {"id": 53, "seek": 29100, "start": 291.0, "end": 297.0, "text": " we will create a new project and as a goal, we said, hey, let's make", "tokens": [50364, 321, 486, 1884, 257, 777, 1716, 293, 382, 257, 3387, 11, 321, 848, 11, 4177, 11, 718, 311, 652, 50664], "temperature": 0.0, "avg_logprob": -0.1186637764885312, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.09193618595600128}, {"id": 54, "seek": 29100, "start": 297.0, "end": 302.0, "text": " upstream and downstream closer together. So let's provide some downstream", "tokens": [50664, 33915, 293, 30621, 4966, 1214, 13, 407, 718, 311, 2893, 512, 30621, 50914], "temperature": 0.0, "avg_logprob": -0.1186637764885312, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.09193618595600128}, {"id": 55, "seek": 29100, "start": 302.0, "end": 308.0, "text": " feedback to the development and also for downstream maintainers, let's", "tokens": [50914, 5824, 281, 264, 3250, 293, 611, 337, 30621, 6909, 433, 11, 718, 311, 51214], "temperature": 0.0, "avg_logprob": -0.1186637764885312, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.09193618595600128}, {"id": 56, "seek": 29100, "start": 308.0, "end": 313.0, "text": " provide them some connection to the upstream. For example, when they", "tokens": [51214, 2893, 552, 512, 4984, 281, 264, 33915, 13, 1171, 1365, 11, 562, 436, 51464], "temperature": 0.0, "avg_logprob": -0.1186637764885312, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.09193618595600128}, {"id": 57, "seek": 29100, "start": 313.0, "end": 319.0, "text": " release new code in upstream, so let's get it automatically to downstream.", "tokens": [51464, 4374, 777, 3089, 294, 33915, 11, 370, 718, 311, 483, 309, 6772, 281, 30621, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1186637764885312, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.09193618595600128}, {"id": 58, "seek": 31900, "start": 319.0, "end": 325.0, "text": " And it was like, yeah, that will be awesome, everyone will be happy.", "tokens": [50364, 400, 309, 390, 411, 11, 1338, 11, 300, 486, 312, 3476, 11, 1518, 486, 312, 2055, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1524364338364712, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.031199133023619652}, {"id": 59, "seek": 31900, "start": 325.0, "end": 333.0, "text": " So we started work on that and few months ago, yeah, we came to the", "tokens": [50664, 407, 321, 1409, 589, 322, 300, 293, 1326, 2493, 2057, 11, 1338, 11, 321, 1361, 281, 264, 51064], "temperature": 0.0, "avg_logprob": -0.1524364338364712, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.031199133023619652}, {"id": 60, "seek": 31900, "start": 333.0, "end": 336.0, "text": " upstream developers and said, yeah, we have this federal integration for", "tokens": [51064, 33915, 8849, 293, 848, 11, 1338, 11, 321, 362, 341, 6019, 10980, 337, 51214], "temperature": 0.0, "avg_logprob": -0.1524364338364712, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.031199133023619652}, {"id": 61, "seek": 31900, "start": 336.0, "end": 341.0, "text": " your project and it's really easy and you will have like new functionality", "tokens": [51214, 428, 1716, 293, 309, 311, 534, 1858, 293, 291, 486, 362, 411, 777, 14980, 51464], "temperature": 0.0, "avg_logprob": -0.1524364338364712, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.031199133023619652}, {"id": 62, "seek": 31900, "start": 341.0, "end": 347.0, "text": " for your project and can be sure that your code will run.", "tokens": [51464, 337, 428, 1716, 293, 393, 312, 988, 300, 428, 3089, 486, 1190, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1524364338364712, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.031199133023619652}, {"id": 63, "seek": 34700, "start": 347.0, "end": 352.0, "text": " The feedback wasn't so positive and we were really surprised because, yeah,", "tokens": [50364, 440, 5824, 2067, 380, 370, 3353, 293, 321, 645, 534, 6100, 570, 11, 1338, 11, 50614], "temperature": 0.0, "avg_logprob": -0.19429843085152762, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.015432041138410568}, {"id": 64, "seek": 34700, "start": 352.0, "end": 359.0, "text": " we are trying to help you. So what do you think why developers might care", "tokens": [50614, 321, 366, 1382, 281, 854, 291, 13, 407, 437, 360, 291, 519, 983, 8849, 1062, 1127, 50964], "temperature": 0.0, "avg_logprob": -0.19429843085152762, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.015432041138410568}, {"id": 65, "seek": 34700, "start": 359.0, "end": 365.0, "text": " about downstream? Why they might even bother why they shouldn't just live", "tokens": [50964, 466, 30621, 30, 1545, 436, 1062, 754, 8677, 983, 436, 4659, 380, 445, 1621, 51264], "temperature": 0.0, "avg_logprob": -0.19429843085152762, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.015432041138410568}, {"id": 66, "seek": 34700, "start": 365.0, "end": 370.0, "text": " on GitHub, GitHub website and live their awesome life and don't care about", "tokens": [51264, 322, 23331, 11, 23331, 3144, 293, 1621, 641, 3476, 993, 293, 500, 380, 1127, 466, 51514], "temperature": 0.0, "avg_logprob": -0.19429843085152762, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.015432041138410568}, {"id": 67, "seek": 37000, "start": 370.0, "end": 380.0, "text": " any distribution at all. So, yeah, hard question. I hope you are typing.", "tokens": [50364, 604, 7316, 412, 439, 13, 407, 11, 1338, 11, 1152, 1168, 13, 286, 1454, 291, 366, 18444, 13, 50864], "temperature": 0.0, "avg_logprob": -0.37907564640045166, "compression_ratio": 1.474025974025974, "no_speech_prob": 0.12601442635059357}, {"id": 68, "seek": 37000, "start": 380.0, "end": 386.0, "text": " Yeah, availability, software option. Wow, wow, so, for me, articles, okay, yeah,", "tokens": [50864, 865, 11, 17945, 11, 4722, 3614, 13, 3153, 11, 6076, 11, 370, 11, 337, 385, 11, 11290, 11, 1392, 11, 1338, 11, 51164], "temperature": 0.0, "avg_logprob": -0.37907564640045166, "compression_ratio": 1.474025974025974, "no_speech_prob": 0.12601442635059357}, {"id": 69, "seek": 37000, "start": 386.0, "end": 393.0, "text": " many, many reasons. Yeah, without distribution, they might have no users.", "tokens": [51164, 867, 11, 867, 4112, 13, 865, 11, 1553, 7316, 11, 436, 1062, 362, 572, 5022, 13, 51514], "temperature": 0.0, "avg_logprob": -0.37907564640045166, "compression_ratio": 1.474025974025974, "no_speech_prob": 0.12601442635059357}, {"id": 70, "seek": 39300, "start": 393.0, "end": 401.0, "text": " Yeah, a lot of obvious things. Just to note, after the session, I'll share", "tokens": [50364, 865, 11, 257, 688, 295, 6322, 721, 13, 1449, 281, 3637, 11, 934, 264, 5481, 11, 286, 603, 2073, 50764], "temperature": 0.0, "avg_logprob": -0.17738628387451172, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.12457789480686188}, {"id": 71, "seek": 39300, "start": 401.0, "end": 406.0, "text": " the results with you so maybe I'll also set up some blog posts but you will", "tokens": [50764, 264, 3542, 365, 291, 370, 1310, 286, 603, 611, 992, 493, 512, 6968, 12300, 457, 291, 486, 51014], "temperature": 0.0, "avg_logprob": -0.17738628387451172, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.12457789480686188}, {"id": 72, "seek": 39300, "start": 406.0, "end": 412.0, "text": " have it attached and, yeah, wow, so many things. It looks like it makes sense", "tokens": [51014, 362, 309, 8570, 293, 11, 1338, 11, 6076, 11, 370, 867, 721, 13, 467, 1542, 411, 309, 1669, 2020, 51314], "temperature": 0.0, "avg_logprob": -0.17738628387451172, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.12457789480686188}, {"id": 73, "seek": 39300, "start": 412.0, "end": 420.0, "text": " to care about downstream. So just a couple more seconds. Yeah, people,", "tokens": [51314, 281, 1127, 466, 30621, 13, 407, 445, 257, 1916, 544, 3949, 13, 865, 11, 561, 11, 51714], "temperature": 0.0, "avg_logprob": -0.17738628387451172, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.12457789480686188}, {"id": 74, "seek": 42000, "start": 420.0, "end": 433.0, "text": " shitty tools, yeah, revenue maybe also. Yeah, sometimes there is a middleman", "tokens": [50364, 30748, 3873, 11, 1338, 11, 9324, 1310, 611, 13, 865, 11, 2171, 456, 307, 257, 2808, 1601, 51014], "temperature": 0.0, "avg_logprob": -0.18613365173339844, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.11283614486455917}, {"id": 75, "seek": 42000, "start": 433.0, "end": 437.0, "text": " that you don't need to tackle the video users. Sometimes you just don't want", "tokens": [51014, 300, 291, 500, 380, 643, 281, 14896, 264, 960, 5022, 13, 4803, 291, 445, 500, 380, 528, 51214], "temperature": 0.0, "avg_logprob": -0.18613365173339844, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.11283614486455917}, {"id": 76, "seek": 42000, "start": 437.0, "end": 444.0, "text": " users maybe. Okay, so let's move on. So, we asked maintenance.", "tokens": [51214, 5022, 1310, 13, 1033, 11, 370, 718, 311, 1286, 322, 13, 407, 11, 321, 2351, 11258, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18613365173339844, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.11283614486455917}, {"id": 77, "seek": 42000, "start": 444.0, "end": 449.0, "text": " Hey, maintenance, we have this nice service for you that will automatically", "tokens": [51564, 1911, 11, 11258, 11, 321, 362, 341, 1481, 2643, 337, 291, 300, 486, 6772, 51814], "temperature": 0.0, "avg_logprob": -0.18613365173339844, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.11283614486455917}, {"id": 78, "seek": 44900, "start": 449.0, "end": 455.0, "text": " send your upstream releases to your service. Always. We were very positive", "tokens": [50364, 2845, 428, 33915, 16952, 281, 428, 2643, 13, 11270, 13, 492, 645, 588, 3353, 50664], "temperature": 0.0, "avg_logprob": -0.23474519760882268, "compression_ratio": 1.3975155279503106, "no_speech_prob": 0.13351334631443024}, {"id": 79, "seek": 44900, "start": 455.0, "end": 463.0, "text": " that we helped people. I don't care if they produce new code. It's definitely", "tokens": [50664, 300, 321, 4254, 561, 13, 286, 500, 380, 1127, 498, 436, 5258, 777, 3089, 13, 467, 311, 2138, 51064], "temperature": 0.0, "avg_logprob": -0.23474519760882268, "compression_ratio": 1.3975155279503106, "no_speech_prob": 0.13351334631443024}, {"id": 80, "seek": 44900, "start": 463.0, "end": 472.0, "text": " a new box and more work for me. So, I'm not sure if I want this service.", "tokens": [51064, 257, 777, 2424, 293, 544, 589, 337, 385, 13, 407, 11, 286, 478, 406, 988, 498, 286, 528, 341, 2643, 13, 51514], "temperature": 0.0, "avg_logprob": -0.23474519760882268, "compression_ratio": 1.3975155279503106, "no_speech_prob": 0.13351334631443024}, {"id": 81, "seek": 47200, "start": 472.0, "end": 477.0, "text": " So, same question. Why do you think maintenance should care about upstream?", "tokens": [50364, 407, 11, 912, 1168, 13, 1545, 360, 291, 519, 11258, 820, 1127, 466, 33915, 30, 50614], "temperature": 0.0, "avg_logprob": -0.20746062553092226, "compression_ratio": 1.502415458937198, "no_speech_prob": 0.19610881805419922}, {"id": 82, "seek": 47200, "start": 477.0, "end": 484.0, "text": " Why there should not be just the upstream that don't produce anything and I can", "tokens": [50614, 1545, 456, 820, 406, 312, 445, 264, 33915, 300, 500, 380, 5258, 1340, 293, 286, 393, 50964], "temperature": 0.0, "avg_logprob": -0.20746062553092226, "compression_ratio": 1.502415458937198, "no_speech_prob": 0.19610881805419922}, {"id": 83, "seek": 47200, "start": 484.0, "end": 492.0, "text": " happily leave just rebuild my package every half a year or so when there is a", "tokens": [50964, 19909, 1856, 445, 16877, 452, 7372, 633, 1922, 257, 1064, 420, 370, 562, 456, 307, 257, 51364], "temperature": 0.0, "avg_logprob": -0.20746062553092226, "compression_ratio": 1.502415458937198, "no_speech_prob": 0.19610881805419922}, {"id": 84, "seek": 47200, "start": 492.0, "end": 501.0, "text": " new version of the Linux distribution and live in peace. Yeah, users want new", "tokens": [51364, 777, 3037, 295, 264, 18734, 7316, 293, 1621, 294, 4336, 13, 865, 11, 5022, 528, 777, 51814], "temperature": 0.0, "avg_logprob": -0.20746062553092226, "compression_ratio": 1.502415458937198, "no_speech_prob": 0.19610881805419922}, {"id": 85, "seek": 50100, "start": 501.0, "end": 511.0, "text": " releases, yeah. New features that's related. Missing updates. Similar stuff,", "tokens": [50364, 16952, 11, 1338, 13, 1873, 4122, 300, 311, 4077, 13, 5275, 278, 9205, 13, 10905, 1507, 11, 50864], "temperature": 0.0, "avg_logprob": -0.2104234541616132, "compression_ratio": 1.4099378881987579, "no_speech_prob": 0.07583251595497131}, {"id": 86, "seek": 50100, "start": 511.0, "end": 520.0, "text": " similar stuff. Yeah, bug fixes. Yeah. Writing the code is hard and I really", "tokens": [50864, 2531, 1507, 13, 865, 11, 7426, 32539, 13, 865, 13, 32774, 264, 3089, 307, 1152, 293, 286, 534, 51314], "temperature": 0.0, "avg_logprob": -0.2104234541616132, "compression_ratio": 1.4099378881987579, "no_speech_prob": 0.07583251595497131}, {"id": 87, "seek": 50100, "start": 520.0, "end": 526.0, "text": " don't want to do that and all the patching, yeah. Can you be a maintenance", "tokens": [51314, 500, 380, 528, 281, 360, 300, 293, 439, 264, 9972, 278, 11, 1338, 13, 1664, 291, 312, 257, 11258, 51614], "temperature": 0.0, "avg_logprob": -0.2104234541616132, "compression_ratio": 1.4099378881987579, "no_speech_prob": 0.07583251595497131}, {"id": 88, "seek": 52600, "start": 526.0, "end": 531.0, "text": " upstim? Looks like not, but yeah, there are a lot of maintenance with dead", "tokens": [50364, 493, 372, 332, 30, 10027, 411, 406, 11, 457, 1338, 11, 456, 366, 257, 688, 295, 11258, 365, 3116, 50614], "temperature": 0.4, "avg_logprob": -0.30705034732818604, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.15037240087985992}, {"id": 89, "seek": 52600, "start": 531.0, "end": 541.0, "text": " upstream. But availability, security fixes, stability. So, yeah, we have at", "tokens": [50614, 33915, 13, 583, 17945, 11, 3825, 32539, 11, 11826, 13, 407, 11, 1338, 11, 321, 362, 412, 51114], "temperature": 0.4, "avg_logprob": -0.30705034732818604, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.15037240087985992}, {"id": 90, "seek": 52600, "start": 541.0, "end": 546.0, "text": " least 17 reasons why to do that. So, I think it makes sense to care about", "tokens": [51114, 1935, 3282, 4112, 983, 281, 360, 300, 13, 407, 11, 286, 519, 309, 1669, 2020, 281, 1127, 466, 51364], "temperature": 0.4, "avg_logprob": -0.30705034732818604, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.15037240087985992}, {"id": 91, "seek": 52600, "start": 546.0, "end": 552.0, "text": " upstream. Yeah, if there is no upstream project, then there is no downstream", "tokens": [51364, 33915, 13, 865, 11, 498, 456, 307, 572, 33915, 1716, 11, 550, 456, 307, 572, 30621, 51664], "temperature": 0.4, "avg_logprob": -0.30705034732818604, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.15037240087985992}, {"id": 92, "seek": 55200, "start": 552.0, "end": 562.0, "text": " project. So, that makes sense. Okay. So, we really wanted to help people and", "tokens": [50364, 1716, 13, 407, 11, 300, 1669, 2020, 13, 1033, 13, 407, 11, 321, 534, 1415, 281, 854, 561, 293, 50864], "temperature": 0.0, "avg_logprob": -0.17519509164910568, "compression_ratio": 1.4903225806451612, "no_speech_prob": 0.07346934825181961}, {"id": 93, "seek": 55200, "start": 562.0, "end": 569.0, "text": " it was quite a surprise that we were honest on that and that was our goal to", "tokens": [50864, 309, 390, 1596, 257, 6365, 300, 321, 645, 3245, 322, 300, 293, 300, 390, 527, 3387, 281, 51214], "temperature": 0.0, "avg_logprob": -0.17519509164910568, "compression_ratio": 1.4903225806451612, "no_speech_prob": 0.07346934825181961}, {"id": 94, "seek": 55200, "start": 569.0, "end": 575.0, "text": " bring upstream and downstream closer together. There was nothing hidden, just", "tokens": [51214, 1565, 33915, 293, 30621, 4966, 1214, 13, 821, 390, 1825, 7633, 11, 445, 51514], "temperature": 0.0, "avg_logprob": -0.17519509164910568, "compression_ratio": 1.4903225806451612, "no_speech_prob": 0.07346934825181961}, {"id": 95, "seek": 57500, "start": 575.0, "end": 586.0, "text": " a really clear goal to help people. So, on the way, after these feedbacks, we", "tokens": [50364, 257, 534, 1850, 3387, 281, 854, 561, 13, 407, 11, 322, 264, 636, 11, 934, 613, 5824, 82, 11, 321, 50914], "temperature": 0.0, "avg_logprob": -0.13979126277722811, "compression_ratio": 1.6344827586206896, "no_speech_prob": 0.18743447959423065}, {"id": 96, "seek": 57500, "start": 586.0, "end": 590.0, "text": " also get some positive ones and also there were people that were both upstream", "tokens": [50914, 611, 483, 512, 3353, 2306, 293, 611, 456, 645, 561, 300, 645, 1293, 33915, 51114], "temperature": 0.0, "avg_logprob": -0.13979126277722811, "compression_ratio": 1.6344827586206896, "no_speech_prob": 0.18743447959423065}, {"id": 97, "seek": 57500, "start": 590.0, "end": 596.0, "text": " and downstream and after all we get some users and also users that provided some", "tokens": [51114, 293, 30621, 293, 934, 439, 321, 483, 512, 5022, 293, 611, 5022, 300, 5649, 512, 51414], "temperature": 0.0, "avg_logprob": -0.13979126277722811, "compression_ratio": 1.6344827586206896, "no_speech_prob": 0.18743447959423065}, {"id": 98, "seek": 59600, "start": 596.0, "end": 605.0, "text": " feedback and after like these, somehow 40 years, I can say that we are saving", "tokens": [50364, 5824, 293, 934, 411, 613, 11, 6063, 3356, 924, 11, 286, 393, 584, 300, 321, 366, 6816, 50814], "temperature": 0.0, "avg_logprob": -0.18533426920572918, "compression_ratio": 1.5621890547263682, "no_speech_prob": 0.2580055594444275}, {"id": 99, "seek": 59600, "start": 605.0, "end": 614.0, "text": " some time people and helping them and there are also great people that uses", "tokens": [50814, 512, 565, 561, 293, 4315, 552, 293, 456, 366, 611, 869, 561, 300, 4960, 51264], "temperature": 0.0, "avg_logprob": -0.18533426920572918, "compression_ratio": 1.5621890547263682, "no_speech_prob": 0.2580055594444275}, {"id": 100, "seek": 59600, "start": 614.0, "end": 620.0, "text": " our project. So, it looks like it makes some sense. But it wasn't easy and we are", "tokens": [51264, 527, 1716, 13, 407, 11, 309, 1542, 411, 309, 1669, 512, 2020, 13, 583, 309, 2067, 380, 1858, 293, 321, 366, 51564], "temperature": 0.0, "avg_logprob": -0.18533426920572918, "compression_ratio": 1.5621890547263682, "no_speech_prob": 0.2580055594444275}, {"id": 101, "seek": 59600, "start": 620.0, "end": 624.0, "text": " not done definitely and we've collected various feedback and complaints on the", "tokens": [51564, 406, 1096, 2138, 293, 321, 600, 11087, 3683, 5824, 293, 19585, 322, 264, 51764], "temperature": 0.0, "avg_logprob": -0.18533426920572918, "compression_ratio": 1.5621890547263682, "no_speech_prob": 0.2580055594444275}, {"id": 102, "seek": 62400, "start": 624.0, "end": 631.0, "text": " way. So, let's pick few typical sentences that we've heard during those years", "tokens": [50364, 636, 13, 407, 11, 718, 311, 1888, 1326, 7476, 16579, 300, 321, 600, 2198, 1830, 729, 924, 50714], "temperature": 0.0, "avg_logprob": -0.11500690723287649, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.05418732017278671}, {"id": 103, "seek": 62400, "start": 631.0, "end": 640.0, "text": " and let's take a look what we can do to help in those situations. So, the first one,", "tokens": [50714, 293, 718, 311, 747, 257, 574, 437, 321, 393, 360, 281, 854, 294, 729, 6851, 13, 407, 11, 264, 700, 472, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11500690723287649, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.05418732017278671}, {"id": 104, "seek": 62400, "start": 640.0, "end": 645.0, "text": " when things go wrong, I don't want to look into the logs. I don't understand the", "tokens": [51164, 562, 721, 352, 2085, 11, 286, 500, 380, 528, 281, 574, 666, 264, 20820, 13, 286, 500, 380, 1223, 264, 51414], "temperature": 0.0, "avg_logprob": -0.11500690723287649, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.05418732017278671}, {"id": 105, "seek": 62400, "start": 645.0, "end": 653.0, "text": " downstream logs. There is some build failure and I don't understand it. So, what", "tokens": [51414, 30621, 20820, 13, 821, 307, 512, 1322, 7763, 293, 286, 500, 380, 1223, 309, 13, 407, 11, 437, 51814], "temperature": 0.0, "avg_logprob": -0.11500690723287649, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.05418732017278671}, {"id": 106, "seek": 65300, "start": 653.0, "end": 658.0, "text": " would you do in this situation? You are providing a build system integration or", "tokens": [50364, 576, 291, 360, 294, 341, 2590, 30, 509, 366, 6530, 257, 1322, 1185, 10980, 420, 50614], "temperature": 0.0, "avg_logprob": -0.14708090500092844, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.07228037714958191}, {"id": 107, "seek": 65300, "start": 658.0, "end": 665.0, "text": " maybe like you are running RPM builds for the upstream pull requests or testing", "tokens": [50614, 1310, 411, 291, 366, 2614, 37389, 15182, 337, 264, 33915, 2235, 12475, 420, 4997, 50964], "temperature": 0.0, "avg_logprob": -0.14708090500092844, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.07228037714958191}, {"id": 108, "seek": 65300, "start": 665.0, "end": 672.0, "text": " on Ubuntu or anything like that. Just the downstream feedback for any upstream", "tokens": [50964, 322, 30230, 45605, 420, 1340, 411, 300, 13, 1449, 264, 30621, 5824, 337, 604, 33915, 51314], "temperature": 0.0, "avg_logprob": -0.14708090500092844, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.07228037714958191}, {"id": 109, "seek": 65300, "start": 672.0, "end": 680.0, "text": " change and people don't want to tackle with the downstream logs. What, when things", "tokens": [51314, 1319, 293, 561, 500, 380, 528, 281, 14896, 365, 264, 30621, 20820, 13, 708, 11, 562, 721, 51714], "temperature": 0.0, "avg_logprob": -0.14708090500092844, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.07228037714958191}, {"id": 110, "seek": 68000, "start": 680.0, "end": 689.0, "text": " go wrong? What would you do? How to help with that? Yeah, reliable mechanism for", "tokens": [50364, 352, 2085, 30, 708, 576, 291, 360, 30, 1012, 281, 854, 365, 300, 30, 865, 11, 12924, 7513, 337, 50814], "temperature": 0.0, "avg_logprob": -0.21876157820224762, "compression_ratio": 1.4082840236686391, "no_speech_prob": 0.08672699332237244}, {"id": 111, "seek": 68000, "start": 689.0, "end": 699.0, "text": " filling bugs. Yeah, definitely. If the problem is like a packaging problem,", "tokens": [50814, 10623, 15120, 13, 865, 11, 2138, 13, 759, 264, 1154, 307, 411, 257, 16836, 1154, 11, 51314], "temperature": 0.0, "avg_logprob": -0.21876157820224762, "compression_ratio": 1.4082840236686391, "no_speech_prob": 0.08672699332237244}, {"id": 112, "seek": 68000, "start": 699.0, "end": 708.0, "text": " anything else what we can do? Okay, so it would be transparent. So, if we need to", "tokens": [51314, 1340, 1646, 437, 321, 393, 360, 30, 1033, 11, 370, 309, 576, 312, 12737, 13, 407, 11, 498, 321, 643, 281, 51764], "temperature": 0.0, "avg_logprob": -0.21876157820224762, "compression_ratio": 1.4082840236686391, "no_speech_prob": 0.08672699332237244}, {"id": 113, "seek": 70800, "start": 708.0, "end": 718.0, "text": " give the logs, so let's let them suffer as well. Yes, I should have 20 relevant logs", "tokens": [50364, 976, 264, 20820, 11, 370, 718, 311, 718, 552, 9753, 382, 731, 13, 1079, 11, 286, 820, 362, 945, 7340, 20820, 50864], "temperature": 0.0, "avg_logprob": -0.4354515075683594, "compression_ratio": 1.3587786259541985, "no_speech_prob": 0.012448495253920555}, {"id": 114, "seek": 70800, "start": 718.0, "end": 729.0, "text": " to find the one relevant. Yeah, so help with some home combing those. Cool logging libraries.", "tokens": [50864, 281, 915, 264, 472, 7340, 13, 865, 11, 370, 854, 365, 512, 1280, 2512, 278, 729, 13, 8561, 27991, 15148, 13, 51414], "temperature": 0.0, "avg_logprob": -0.4354515075683594, "compression_ratio": 1.3587786259541985, "no_speech_prob": 0.012448495253920555}, {"id": 115, "seek": 72900, "start": 729.0, "end": 740.0, "text": " I'm still missing some crucial. Yeah, you can snap or flat peg, yes. But you can get", "tokens": [50364, 286, 478, 920, 5361, 512, 11462, 13, 865, 11, 291, 393, 13650, 420, 4962, 17199, 11, 2086, 13, 583, 291, 393, 483, 50914], "temperature": 0.0, "avg_logprob": -0.2881512451171875, "compression_ratio": 1.4065040650406504, "no_speech_prob": 0.051625169813632965}, {"id": 116, "seek": 72900, "start": 740.0, "end": 747.0, "text": " like a failure from creating the snap. So, it's, we can treat like snap or flat peg like", "tokens": [50914, 411, 257, 7763, 490, 4084, 264, 13650, 13, 407, 11, 309, 311, 11, 321, 393, 2387, 411, 13650, 420, 4962, 17199, 411, 51264], "temperature": 0.0, "avg_logprob": -0.2881512451171875, "compression_ratio": 1.4065040650406504, "no_speech_prob": 0.051625169813632965}, {"id": 117, "seek": 74700, "start": 747.0, "end": 760.0, "text": " this and another distribution maybe if either of the ends but still. Yeah. Okay, I hope", "tokens": [50364, 341, 293, 1071, 7316, 1310, 498, 2139, 295, 264, 5314, 457, 920, 13, 865, 13, 1033, 11, 286, 1454, 51014], "temperature": 0.0, "avg_logprob": -0.3636795206272856, "compression_ratio": 1.3358778625954197, "no_speech_prob": 0.1304112672805786}, {"id": 118, "seek": 74700, "start": 760.0, "end": 773.0, "text": " that's my job. Okay, I'm still missing one crucial. Like the obvious one. You know that", "tokens": [51014, 300, 311, 452, 1691, 13, 1033, 11, 286, 478, 920, 5361, 472, 11462, 13, 1743, 264, 6322, 472, 13, 509, 458, 300, 51664], "temperature": 0.0, "avg_logprob": -0.3636795206272856, "compression_ratio": 1.3358778625954197, "no_speech_prob": 0.1304112672805786}, {"id": 119, "seek": 77300, "start": 773.0, "end": 785.0, "text": " is not possible. Probably that's why the response is not here. So, better logs. Yeah, it's usually", "tokens": [50364, 307, 406, 1944, 13, 9210, 300, 311, 983, 264, 4134, 307, 406, 510, 13, 407, 11, 1101, 20820, 13, 865, 11, 309, 311, 2673, 50964], "temperature": 0.0, "avg_logprob": -0.19246493594747194, "compression_ratio": 1.516304347826087, "no_speech_prob": 0.2591411769390106}, {"id": 120, "seek": 77300, "start": 785.0, "end": 791.0, "text": " not so easily possible. Sometimes yes, sometimes we can do something about it but with these", "tokens": [50964, 406, 370, 3612, 1944, 13, 4803, 2086, 11, 2171, 321, 393, 360, 746, 466, 309, 457, 365, 613, 51264], "temperature": 0.0, "avg_logprob": -0.19246493594747194, "compression_ratio": 1.516304347826087, "no_speech_prob": 0.2591411769390106}, {"id": 121, "seek": 77300, "start": 791.0, "end": 800.0, "text": " systems, if you've been on the talk like the Dant had an hour ago about all the federal", "tokens": [51264, 3652, 11, 498, 291, 600, 668, 322, 264, 751, 411, 264, 413, 394, 632, 364, 1773, 2057, 466, 439, 264, 6019, 51714], "temperature": 0.0, "avg_logprob": -0.19246493594747194, "compression_ratio": 1.516304347826087, "no_speech_prob": 0.2591411769390106}, {"id": 122, "seek": 80000, "start": 800.0, "end": 805.0, "text": " systems we had in place, yeah, we are trying to integrate with all of them and you, for", "tokens": [50364, 3652, 321, 632, 294, 1081, 11, 1338, 11, 321, 366, 1382, 281, 13365, 365, 439, 295, 552, 293, 291, 11, 337, 50614], "temperature": 0.0, "avg_logprob": -0.1853479448255602, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.24278567731380463}, {"id": 123, "seek": 80000, "start": 805.0, "end": 811.0, "text": " example, copper had multiple logs and all the systems have different logs. So, and they", "tokens": [50614, 1365, 11, 15007, 632, 3866, 20820, 293, 439, 264, 3652, 362, 819, 20820, 13, 407, 11, 293, 436, 50914], "temperature": 0.0, "avg_logprob": -0.1853479448255602, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.24278567731380463}, {"id": 124, "seek": 80000, "start": 811.0, "end": 818.0, "text": " use more or other tools. So, this is layered and we don't have power on all the logs. But", "tokens": [50914, 764, 544, 420, 661, 3873, 13, 407, 11, 341, 307, 34666, 293, 321, 500, 380, 362, 1347, 322, 439, 264, 20820, 13, 583, 51264], "temperature": 0.0, "avg_logprob": -0.1853479448255602, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.24278567731380463}, {"id": 125, "seek": 80000, "start": 818.0, "end": 825.0, "text": " maybe we can, as someone mentioned correctly, we can be good in the aggregation or some", "tokens": [51264, 1310, 321, 393, 11, 382, 1580, 2835, 8944, 11, 321, 393, 312, 665, 294, 264, 16743, 399, 420, 512, 51614], "temperature": 0.0, "avg_logprob": -0.1853479448255602, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.24278567731380463}, {"id": 126, "seek": 82500, "start": 825.0, "end": 834.0, "text": " visualization or we can use AI. Okay, just kidding but a few colleagues and me are actually", "tokens": [50364, 25801, 420, 321, 393, 764, 7318, 13, 1033, 11, 445, 9287, 457, 257, 1326, 7734, 293, 385, 366, 767, 50814], "temperature": 0.0, "avg_logprob": -0.12513501716382575, "compression_ratio": 1.483695652173913, "no_speech_prob": 0.15120287239551544}, {"id": 127, "seek": 82500, "start": 834.0, "end": 841.0, "text": " working on something like that. They are trying to collect various logs with the failures and", "tokens": [50814, 1364, 322, 746, 411, 300, 13, 814, 366, 1382, 281, 2500, 3683, 20820, 365, 264, 20774, 293, 51164], "temperature": 0.0, "avg_logprob": -0.12513501716382575, "compression_ratio": 1.483695652173913, "no_speech_prob": 0.15120287239551544}, {"id": 128, "seek": 82500, "start": 841.0, "end": 848.0, "text": " trying to get like human input what's going on here and how to fix that. So, if you are", "tokens": [51164, 1382, 281, 483, 411, 1952, 4846, 437, 311, 516, 322, 510, 293, 577, 281, 3191, 300, 13, 407, 11, 498, 291, 366, 51514], "temperature": 0.0, "avg_logprob": -0.12513501716382575, "compression_ratio": 1.483695652173913, "no_speech_prob": 0.15120287239551544}, {"id": 129, "seek": 84800, "start": 848.0, "end": 857.0, "text": " interested in that, check this out. I really hope this will happen and will produce some", "tokens": [50364, 3102, 294, 300, 11, 1520, 341, 484, 13, 286, 534, 1454, 341, 486, 1051, 293, 486, 5258, 512, 50814], "temperature": 0.0, "avg_logprob": -0.09455020392118994, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.07260166108608246}, {"id": 130, "seek": 84800, "start": 857.0, "end": 865.0, "text": " really nice data set that can help us to provide some really nice way how to not tackle with", "tokens": [50814, 534, 1481, 1412, 992, 300, 393, 854, 505, 281, 2893, 512, 534, 1481, 636, 577, 281, 406, 14896, 365, 51214], "temperature": 0.0, "avg_logprob": -0.09455020392118994, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.07260166108608246}, {"id": 131, "seek": 84800, "start": 865.0, "end": 872.0, "text": " hundreds of lines of logs that I don't need to tackle. So, that's just in the beginning but", "tokens": [51214, 6779, 295, 3876, 295, 20820, 300, 286, 500, 380, 643, 281, 14896, 13, 407, 11, 300, 311, 445, 294, 264, 2863, 457, 51564], "temperature": 0.0, "avg_logprob": -0.09455020392118994, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.07260166108608246}, {"id": 132, "seek": 87200, "start": 872.0, "end": 881.0, "text": " I'm really looking forward to that. Next thing would help us is provide nice notifications", "tokens": [50364, 286, 478, 534, 1237, 2128, 281, 300, 13, 3087, 551, 576, 854, 505, 307, 2893, 1481, 13426, 50814], "temperature": 0.0, "avg_logprob": -0.15418225176194134, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.08840960264205933}, {"id": 133, "seek": 87200, "start": 881.0, "end": 892.0, "text": " and also connect by that, I mean connect people that can help. And it relates also to the last point", "tokens": [50814, 293, 611, 1745, 538, 300, 11, 286, 914, 1745, 561, 300, 393, 854, 13, 400, 309, 16155, 611, 281, 264, 1036, 935, 51364], "temperature": 0.0, "avg_logprob": -0.15418225176194134, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.08840960264205933}, {"id": 134, "seek": 87200, "start": 892.0, "end": 900.0, "text": " that we need to set really clear expectations. Who is responsible for what? Who should take a look?", "tokens": [51364, 300, 321, 643, 281, 992, 534, 1850, 9843, 13, 2102, 307, 6250, 337, 437, 30, 2102, 820, 747, 257, 574, 30, 51764], "temperature": 0.0, "avg_logprob": -0.15418225176194134, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.08840960264205933}, {"id": 135, "seek": 90000, "start": 900.0, "end": 907.0, "text": " Yes, sometimes it's not clear, sometimes it can be really valid back in the code that is", "tokens": [50364, 1079, 11, 2171, 309, 311, 406, 1850, 11, 2171, 309, 393, 312, 534, 7363, 646, 294, 264, 3089, 300, 307, 50714], "temperature": 0.0, "avg_logprob": -0.16041284448960247, "compression_ratio": 1.7836538461538463, "no_speech_prob": 0.08205185830593109}, {"id": 136, "seek": 90000, "start": 907.0, "end": 914.0, "text": " sketched quite soon and that's really nice but sometimes it can be downstream issue and", "tokens": [50714, 12325, 292, 1596, 2321, 293, 300, 311, 534, 1481, 457, 2171, 309, 393, 312, 30621, 2734, 293, 51064], "temperature": 0.0, "avg_logprob": -0.16041284448960247, "compression_ratio": 1.7836538461538463, "no_speech_prob": 0.08205185830593109}, {"id": 137, "seek": 90000, "start": 914.0, "end": 921.0, "text": " sometimes it's something in the middle and so it's really nice when we are introducing these", "tokens": [51064, 2171, 309, 311, 746, 294, 264, 2808, 293, 370, 309, 311, 534, 1481, 562, 321, 366, 15424, 613, 51414], "temperature": 0.0, "avg_logprob": -0.16041284448960247, "compression_ratio": 1.7836538461538463, "no_speech_prob": 0.08205185830593109}, {"id": 138, "seek": 90000, "start": 921.0, "end": 929.0, "text": " two to make clear expectations. Who will, which like maybe also time-based and with the notifications", "tokens": [51414, 732, 281, 652, 1850, 9843, 13, 2102, 486, 11, 597, 411, 1310, 611, 565, 12, 6032, 293, 365, 264, 13426, 51814], "temperature": 0.0, "avg_logprob": -0.16041284448960247, "compression_ratio": 1.7836538461538463, "no_speech_prob": 0.08205185830593109}, {"id": 139, "seek": 92900, "start": 929.0, "end": 940.0, "text": " we can maybe ping people that can help. Okay, so just a single distribution. Why can't you support", "tokens": [50364, 321, 393, 1310, 26151, 561, 300, 393, 854, 13, 1033, 11, 370, 445, 257, 2167, 7316, 13, 1545, 393, 380, 291, 1406, 50914], "temperature": 0.0, "avg_logprob": -0.16036025791952055, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.02847135439515114}, {"id": 140, "seek": 92900, "start": 940.0, "end": 948.0, "text": " all of them? So, what would you do? Yeah, there are people from various distributions so that's", "tokens": [50914, 439, 295, 552, 30, 407, 11, 437, 576, 291, 360, 30, 865, 11, 456, 366, 561, 490, 3683, 37870, 370, 300, 311, 51314], "temperature": 0.0, "avg_logprob": -0.16036025791952055, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.02847135439515114}, {"id": 141, "seek": 92900, "start": 948.0, "end": 955.0, "text": " maybe common that if you want to introduce some CI or anything that, yeah, but if I enable this", "tokens": [51314, 1310, 2689, 300, 498, 291, 528, 281, 5366, 512, 37777, 420, 1340, 300, 11, 1338, 11, 457, 498, 286, 9528, 341, 51664], "temperature": 0.0, "avg_logprob": -0.16036025791952055, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.02847135439515114}, {"id": 142, "seek": 95500, "start": 955.0, "end": 966.0, "text": " for Fedora, I want also the BN, SUSE and everything. Can we help somehow? What would you do? Yeah, we can", "tokens": [50364, 337, 7772, 3252, 11, 286, 528, 611, 264, 363, 45, 11, 40117, 36, 293, 1203, 13, 1664, 321, 854, 6063, 30, 708, 576, 291, 360, 30, 865, 11, 321, 393, 50914], "temperature": 0.0, "avg_logprob": -0.24531350135803223, "compression_ratio": 1.2787878787878788, "no_speech_prob": 0.10178776830434799}, {"id": 143, "seek": 95500, "start": 966.0, "end": 976.0, "text": " use build systems that support multiple targets like copper or OBS and these. Yeah, snap, flatback, yeah,", "tokens": [50914, 764, 1322, 3652, 300, 1406, 3866, 12911, 411, 15007, 420, 422, 8176, 293, 613, 13, 865, 11, 13650, 11, 4962, 3207, 11, 1338, 11, 51414], "temperature": 0.0, "avg_logprob": -0.24531350135803223, "compression_ratio": 1.2787878787878788, "no_speech_prob": 0.10178776830434799}, {"id": 144, "seek": 97600, "start": 976.0, "end": 985.0, "text": " we can like switch the distribution but when we discuss in the very beginning, we might want to", "tokens": [50364, 321, 393, 411, 3679, 264, 7316, 457, 562, 321, 2248, 294, 264, 588, 2863, 11, 321, 1062, 528, 281, 50814], "temperature": 0.0, "avg_logprob": -0.1513581068619438, "compression_ratio": 1.536, "no_speech_prob": 0.2557881474494934}, {"id": 145, "seek": 97600, "start": 985.0, "end": 996.0, "text": " care still in those distributions that we maybe work on. Yes, some distributions, yeah, a lot of", "tokens": [50814, 1127, 920, 294, 729, 37870, 300, 321, 1310, 589, 322, 13, 1079, 11, 512, 37870, 11, 1338, 11, 257, 688, 295, 51364], "temperature": 0.0, "avg_logprob": -0.1513581068619438, "compression_ratio": 1.536, "no_speech_prob": 0.2557881474494934}, {"id": 146, "seek": 99600, "start": 996.0, "end": 1008.0, "text": " distributions are really, really different and it's hard to somehow compare or maybe some obstruction. Yeah,", "tokens": [50364, 37870, 366, 534, 11, 534, 819, 293, 309, 311, 1152, 281, 6063, 6794, 420, 1310, 512, 49711, 13, 865, 11, 50964], "temperature": 0.0, "avg_logprob": -0.13245482444763185, "compression_ratio": 1.4236111111111112, "no_speech_prob": 0.057248689234256744}, {"id": 147, "seek": 99600, "start": 1008.0, "end": 1017.0, "text": " I'm lost in all the good suggestions. I'll read them probably later. So, yeah, it's probably not", "tokens": [50964, 286, 478, 2731, 294, 439, 264, 665, 13396, 13, 286, 603, 1401, 552, 1391, 1780, 13, 407, 11, 1338, 11, 309, 311, 1391, 406, 51414], "temperature": 0.0, "avg_logprob": -0.13245482444763185, "compression_ratio": 1.4236111111111112, "no_speech_prob": 0.057248689234256744}, {"id": 148, "seek": 101700, "start": 1017.0, "end": 1029.0, "text": " possible, definitely not all. There are a lot of distributions but as someone suggested, we can maybe try, maybe", "tokens": [50364, 1944, 11, 2138, 406, 439, 13, 821, 366, 257, 688, 295, 37870, 457, 382, 1580, 10945, 11, 321, 393, 1310, 853, 11, 1310, 50964], "temperature": 0.0, "avg_logprob": -0.1475672721862793, "compression_ratio": 1.5611510791366907, "no_speech_prob": 0.3278374671936035}, {"id": 149, "seek": 101700, "start": 1029.0, "end": 1040.0, "text": " we can also have open source that someone can contribute, maybe some architecture that can maybe combine", "tokens": [50964, 321, 393, 611, 362, 1269, 4009, 300, 1580, 393, 10586, 11, 1310, 512, 9482, 300, 393, 1310, 10432, 51514], "temperature": 0.0, "avg_logprob": -0.1475672721862793, "compression_ratio": 1.5611510791366907, "no_speech_prob": 0.3278374671936035}, {"id": 150, "seek": 104000, "start": 1040.0, "end": 1049.0, "text": " various backends so we can maybe share. If you have an open source, so for example, then we'll come from SUSE", "tokens": [50364, 3683, 646, 2581, 370, 321, 393, 1310, 2073, 13, 759, 291, 362, 364, 1269, 4009, 11, 370, 337, 1365, 11, 550, 321, 603, 808, 490, 40117, 36, 50814], "temperature": 0.0, "avg_logprob": -0.13342563404756433, "compression_ratio": 1.5044642857142858, "no_speech_prob": 0.18942853808403015}, {"id": 151, "seek": 104000, "start": 1049.0, "end": 1059.0, "text": " and say, hey, let's support impact it also OBS and we can maybe collaborate so that's also possible. The tricky one", "tokens": [50814, 293, 584, 11, 4177, 11, 718, 311, 1406, 2712, 309, 611, 422, 8176, 293, 321, 393, 1310, 18338, 370, 300, 311, 611, 1944, 13, 440, 12414, 472, 51314], "temperature": 0.0, "avg_logprob": -0.13342563404756433, "compression_ratio": 1.5044642857142858, "no_speech_prob": 0.18942853808403015}, {"id": 152, "seek": 104000, "start": 1059.0, "end": 1066.0, "text": " is we are used to do distro specific terms and we need to be really careful about those because when we mention", "tokens": [51314, 307, 321, 366, 1143, 281, 360, 1483, 340, 2685, 2115, 293, 321, 643, 281, 312, 534, 5026, 466, 729, 570, 562, 321, 2152, 51664], "temperature": 0.0, "avg_logprob": -0.13342563404756433, "compression_ratio": 1.5044642857142858, "no_speech_prob": 0.18942853808403015}, {"id": 153, "seek": 106600, "start": 1066.0, "end": 1078.0, "text": " various scratch builds and patches, metadata, bugs, and all the weird terminology, it might be really, it also might be a", "tokens": [50364, 3683, 8459, 15182, 293, 26531, 11, 26603, 11, 15120, 11, 293, 439, 264, 3657, 27575, 11, 309, 1062, 312, 534, 11, 309, 611, 1062, 312, 257, 50964], "temperature": 0.0, "avg_logprob": -0.214253737709739, "compression_ratio": 1.5064102564102564, "no_speech_prob": 0.11799445748329163}, {"id": 154, "seek": 106600, "start": 1078.0, "end": 1087.0, "text": " reason why they are scared and developers don't want to hear about those. So, describe those terms and be careful", "tokens": [50964, 1778, 983, 436, 366, 5338, 293, 8849, 500, 380, 528, 281, 1568, 466, 729, 13, 407, 11, 6786, 729, 2115, 293, 312, 5026, 51414], "temperature": 0.0, "avg_logprob": -0.214253737709739, "compression_ratio": 1.5064102564102564, "no_speech_prob": 0.11799445748329163}, {"id": 155, "seek": 108700, "start": 1087.0, "end": 1099.0, "text": " and also you can hide those somehow so we don't need to speak about co-properties but we can maybe mention RPM builds and these things.", "tokens": [50364, 293, 611, 291, 393, 6479, 729, 6063, 370, 321, 500, 380, 643, 281, 1710, 466, 598, 12, 4318, 610, 6097, 457, 321, 393, 1310, 2152, 37389, 15182, 293, 613, 721, 13, 50964], "temperature": 0.0, "avg_logprob": -0.24422577888734878, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.34084436297416687}, {"id": 156, "seek": 108700, "start": 1099.0, "end": 1113.0, "text": " So, yeah, what helps us is also we are supporting various functionality types and what helps is to provide easy and reliable testing", "tokens": [50964, 407, 11, 1338, 11, 437, 3665, 505, 307, 611, 321, 366, 7231, 3683, 14980, 3467, 293, 437, 3665, 307, 281, 2893, 1858, 293, 12924, 4997, 51664], "temperature": 0.0, "avg_logprob": -0.24422577888734878, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.34084436297416687}, {"id": 157, "seek": 111300, "start": 1113.0, "end": 1126.0, "text": " like infrastructure or so if they can rely on that and they can run their test code or run their tests on this reliable infrastructure", "tokens": [50364, 411, 6896, 420, 370, 498, 436, 393, 10687, 322, 300, 293, 436, 393, 1190, 641, 1500, 3089, 420, 1190, 641, 6921, 322, 341, 12924, 6896, 51014], "temperature": 0.0, "avg_logprob": -0.19606982684526286, "compression_ratio": 1.652694610778443, "no_speech_prob": 0.08153426647186279}, {"id": 158, "seek": 111300, "start": 1126.0, "end": 1135.0, "text": " we are using for example testing bar project for that so we don't need to do that ourselves. And easy on boarding, I'll probably mention that", "tokens": [51014, 321, 366, 1228, 337, 1365, 4997, 2159, 1716, 337, 300, 370, 321, 500, 380, 643, 281, 360, 300, 4175, 13, 400, 1858, 322, 30528, 11, 286, 603, 1391, 2152, 300, 51464], "temperature": 0.0, "avg_logprob": -0.19606982684526286, "compression_ratio": 1.652694610778443, "no_speech_prob": 0.08153426647186279}, {"id": 159, "seek": 113500, "start": 1135.0, "end": 1145.0, "text": " multiple times but it's crucial because if they hid like the very first problem during the way and yeah with those distribution things", "tokens": [50364, 3866, 1413, 457, 309, 311, 11462, 570, 498, 436, 16253, 411, 264, 588, 700, 1154, 1830, 264, 636, 293, 1338, 365, 729, 7316, 721, 50864], "temperature": 0.0, "avg_logprob": -0.14383586760490172, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.43590670824050903}, {"id": 160, "seek": 113500, "start": 1145.0, "end": 1156.0, "text": " it's not easy and we've spoiled it multiple times but it's important. Yes, sorry, I don't want to have more files in the repository.", "tokens": [50864, 309, 311, 406, 1858, 293, 321, 600, 32439, 309, 3866, 1413, 457, 309, 311, 1021, 13, 1079, 11, 2597, 11, 286, 500, 380, 528, 281, 362, 544, 7098, 294, 264, 25841, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14383586760490172, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.43590670824050903}, {"id": 161, "seek": 115600, "start": 1156.0, "end": 1173.0, "text": " So, CI system that's generated maybe for any upstream CI so yet another config file. Don't be lazy, yeah.", "tokens": [50364, 407, 11, 37777, 1185, 300, 311, 10833, 1310, 337, 604, 33915, 37777, 370, 1939, 1071, 6662, 3991, 13, 1468, 380, 312, 14847, 11, 1338, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2804929470193797, "compression_ratio": 1.105263157894737, "no_speech_prob": 0.07788397371768951}, {"id": 162, "seek": 117300, "start": 1173.0, "end": 1185.0, "text": " Yeah, there are thousands of files in the repository and you don't like yet another one or next to. Yeah.", "tokens": [50364, 865, 11, 456, 366, 5383, 295, 7098, 294, 264, 25841, 293, 291, 500, 380, 411, 1939, 1071, 472, 420, 958, 281, 13, 865, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2934533527919224, "compression_ratio": 1.1797752808988764, "no_speech_prob": 0.16793003678321838}, {"id": 163, "seek": 118500, "start": 1185.0, "end": 1205.0, "text": " A few interesting things. Anything interesting? You see, okay, so yeah, we can put more complaints. Yeah, yeah.", "tokens": [50364, 316, 1326, 1880, 721, 13, 11998, 1880, 30, 509, 536, 11, 1392, 11, 370, 1338, 11, 321, 393, 829, 544, 19585, 13, 865, 11, 1338, 13, 51364], "temperature": 0.0, "avg_logprob": -0.40525150299072266, "compression_ratio": 1.2065217391304348, "no_speech_prob": 0.14995165169239044}, {"id": 164, "seek": 120500, "start": 1206.0, "end": 1222.0, "text": " Very interesting things. Yes, I'll probably move on and read those later. It's interesting but so yeah, we might want to stick with the one line if possible, one file if possible", "tokens": [50414, 4372, 1880, 721, 13, 1079, 11, 286, 603, 1391, 1286, 322, 293, 1401, 729, 1780, 13, 467, 311, 1880, 457, 370, 1338, 11, 321, 1062, 528, 281, 2897, 365, 264, 472, 1622, 498, 1944, 11, 472, 3991, 498, 1944, 51214], "temperature": 0.0, "avg_logprob": -0.17245093057321947, "compression_ratio": 1.4015748031496063, "no_speech_prob": 0.11589781194925308}, {"id": 165, "seek": 122200, "start": 1222.0, "end": 1238.0, "text": " because if it is possible but still better one file than multiple files and also not sure why but people rather put a shell script into the JSON or YAML instead of providing a shell script", "tokens": [50364, 570, 498, 309, 307, 1944, 457, 920, 1101, 472, 3991, 813, 3866, 7098, 293, 611, 406, 988, 983, 457, 561, 2831, 829, 257, 8720, 5755, 666, 264, 31828, 420, 398, 2865, 43, 2602, 295, 6530, 257, 8720, 5755, 51164], "temperature": 0.0, "avg_logprob": -0.17445825395129977, "compression_ratio": 1.413533834586466, "no_speech_prob": 0.6827800273895264}, {"id": 166, "seek": 123800, "start": 1238.0, "end": 1250.0, "text": " and specify a name of this shell script in the YAML file so we can help them do that. Yeah, if there is more content we can maybe let them link it.", "tokens": [50364, 293, 16500, 257, 1315, 295, 341, 8720, 5755, 294, 264, 398, 2865, 43, 3991, 370, 321, 393, 854, 552, 360, 300, 13, 865, 11, 498, 456, 307, 544, 2701, 321, 393, 1310, 718, 552, 2113, 309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13385443369547526, "compression_ratio": 1.585, "no_speech_prob": 0.6375206708908081}, {"id": 167, "seek": 123800, "start": 1250.0, "end": 1263.0, "text": " We can also enable some custom locations, custom name of or some sub directory so they can maybe hide it a bit and also for example Zoo project uses global configuration", "tokens": [50964, 492, 393, 611, 9528, 512, 2375, 9253, 11, 2375, 1315, 295, 420, 512, 1422, 21120, 370, 436, 393, 1310, 6479, 309, 257, 857, 293, 611, 337, 1365, 34589, 1716, 4960, 4338, 11694, 51614], "temperature": 0.0, "avg_logprob": -0.13385443369547526, "compression_ratio": 1.585, "no_speech_prob": 0.6375206708908081}, {"id": 168, "seek": 126300, "start": 1263.0, "end": 1274.0, "text": " if people really don't want to put anything to their git repository just provide a separate git repository they can create a pull request and enable this.", "tokens": [50364, 498, 561, 534, 500, 380, 528, 281, 829, 1340, 281, 641, 18331, 25841, 445, 2893, 257, 4994, 18331, 25841, 436, 393, 1884, 257, 2235, 5308, 293, 9528, 341, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18496123604152515, "compression_ratio": 1.527363184079602, "no_speech_prob": 0.09213816374540329}, {"id": 169, "seek": 126300, "start": 1274.0, "end": 1287.0, "text": " It's tricky from the developer point of view like how to do that, how all the messaging should work but yeah. Yeah, I have my own automation works well.", "tokens": [50914, 467, 311, 12414, 490, 264, 10754, 935, 295, 1910, 411, 577, 281, 360, 300, 11, 577, 439, 264, 21812, 820, 589, 457, 1338, 13, 865, 11, 286, 362, 452, 1065, 17769, 1985, 731, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18496123604152515, "compression_ratio": 1.527363184079602, "no_speech_prob": 0.09213816374540329}, {"id": 170, "seek": 128700, "start": 1287.0, "end": 1296.0, "text": " So, I have my script or anything and I'm happy with it.", "tokens": [50364, 407, 11, 286, 362, 452, 5755, 420, 1340, 293, 286, 478, 2055, 365, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3450478744506836, "compression_ratio": 0.9315068493150684, "no_speech_prob": 0.24535450339317322}, {"id": 171, "seek": 128700, "start": 1299.0, "end": 1301.0, "text": " How to live?", "tokens": [50964, 1012, 281, 1621, 30, 51064], "temperature": 0.0, "avg_logprob": -0.3450478744506836, "compression_ratio": 0.9315068493150684, "no_speech_prob": 0.24535450339317322}, {"id": 172, "seek": 130100, "start": 1301.0, "end": 1306.0, "text": " Yeah.", "tokens": [50364, 865, 13, 50614], "temperature": 0.0, "avg_logprob": -0.6102499147740806, "compression_ratio": 1.0975609756097562, "no_speech_prob": 0.2694412171840668}, {"id": 173, "seek": 130100, "start": 1306.0, "end": 1312.0, "text": " Yeah.", "tokens": [50614, 865, 13, 50914], "temperature": 0.0, "avg_logprob": -0.6102499147740806, "compression_ratio": 1.0975609756097562, "no_speech_prob": 0.2694412171840668}, {"id": 174, "seek": 130100, "start": 1312.0, "end": 1316.0, "text": " Use on Civo. Yeah.", "tokens": [50914, 8278, 322, 383, 6340, 13, 865, 13, 51114], "temperature": 0.0, "avg_logprob": -0.6102499147740806, "compression_ratio": 1.0975609756097562, "no_speech_prob": 0.2694412171840668}, {"id": 175, "seek": 130100, "start": 1316.0, "end": 1321.0, "text": " Okay. Good for you.", "tokens": [51114, 1033, 13, 2205, 337, 291, 13, 51364], "temperature": 0.0, "avg_logprob": -0.6102499147740806, "compression_ratio": 1.0975609756097562, "no_speech_prob": 0.2694412171840668}, {"id": 176, "seek": 130100, "start": 1321.0, "end": 1325.0, "text": " Yeah, standard protocols.", "tokens": [51364, 865, 11, 3832, 20618, 13, 51564], "temperature": 0.0, "avg_logprob": -0.6102499147740806, "compression_ratio": 1.0975609756097562, "no_speech_prob": 0.2694412171840668}, {"id": 177, "seek": 130100, "start": 1325.0, "end": 1327.0, "text": " I'll move on.", "tokens": [51564, 286, 603, 1286, 322, 13, 51664], "temperature": 0.0, "avg_logprob": -0.6102499147740806, "compression_ratio": 1.0975609756097562, "no_speech_prob": 0.2694412171840668}, {"id": 178, "seek": 132700, "start": 1327.0, "end": 1338.0, "text": " So, this is a generic when we want someone to start using something else even like with the comparable tools we need some killer feature.", "tokens": [50364, 407, 11, 341, 307, 257, 19577, 562, 321, 528, 1580, 281, 722, 1228, 746, 1646, 754, 411, 365, 264, 25323, 3873, 321, 643, 512, 13364, 4111, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14488925308477682, "compression_ratio": 1.5340909090909092, "no_speech_prob": 0.051692891865968704}, {"id": 179, "seek": 132700, "start": 1338.0, "end": 1349.0, "text": " We don't, having just the same feature set is not enough. They need to have clear motivation to have something extra when they move.", "tokens": [50914, 492, 500, 380, 11, 1419, 445, 264, 912, 4111, 992, 307, 406, 1547, 13, 814, 643, 281, 362, 1850, 12335, 281, 362, 746, 2857, 562, 436, 1286, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14488925308477682, "compression_ratio": 1.5340909090909092, "no_speech_prob": 0.051692891865968704}, {"id": 180, "seek": 134900, "start": 1350.0, "end": 1365.0, "text": " Easy onboarding. I've mentioned that this is crucial. We are for example trying to do some online workshop with the gutter platform and various funny stuff to help them.", "tokens": [50414, 16002, 24033, 278, 13, 286, 600, 2835, 300, 341, 307, 11462, 13, 492, 366, 337, 1365, 1382, 281, 360, 512, 2950, 13541, 365, 264, 5228, 391, 3663, 293, 3683, 4074, 1507, 281, 854, 552, 13, 51164], "temperature": 0.0, "avg_logprob": -0.20977208553216395, "compression_ratio": 1.2803030303030303, "no_speech_prob": 0.054321229457855225}, {"id": 181, "seek": 136500, "start": 1365.0, "end": 1377.0, "text": " Also, and that can be the killer feature if things break. The people or the users does not need to take a look and fix that and then that can save a lot of time.", "tokens": [50364, 2743, 11, 293, 300, 393, 312, 264, 13364, 4111, 498, 721, 1821, 13, 440, 561, 420, 264, 5022, 775, 406, 643, 281, 747, 257, 574, 293, 3191, 300, 293, 550, 300, 393, 3155, 257, 688, 295, 565, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1385878889184249, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.09974560141563416}, {"id": 182, "seek": 136500, "start": 1377.0, "end": 1387.0, "text": " So, with the maintenance of this automation and we can save them a lot of time and we need to clearly communicate this but we need to also do that.", "tokens": [50964, 407, 11, 365, 264, 11258, 295, 341, 17769, 293, 321, 393, 3155, 552, 257, 688, 295, 565, 293, 321, 643, 281, 4448, 7890, 341, 457, 321, 643, 281, 611, 360, 300, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1385878889184249, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.09974560141563416}, {"id": 183, "seek": 138700, "start": 1387.0, "end": 1391.0, "text": " So, if things break we should take a look and just ignore it.", "tokens": [50364, 407, 11, 498, 721, 1821, 321, 820, 747, 257, 574, 293, 445, 11200, 309, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1999648001886183, "compression_ratio": 1.578616352201258, "no_speech_prob": 0.21739299595355988}, {"id": 184, "seek": 138700, "start": 1391.0, "end": 1406.0, "text": " And work on the right things. So, listen to the community, listen to the people and try to like don't just assume that you are doing the right features but just ask and get something there.", "tokens": [50564, 400, 589, 322, 264, 558, 721, 13, 407, 11, 2140, 281, 264, 1768, 11, 2140, 281, 264, 561, 293, 853, 281, 411, 500, 380, 445, 6552, 300, 291, 366, 884, 264, 558, 4122, 457, 445, 1029, 293, 483, 746, 456, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1999648001886183, "compression_ratio": 1.578616352201258, "no_speech_prob": 0.21739299595355988}, {"id": 185, "seek": 140600, "start": 1406.0, "end": 1414.0, "text": " Yeah, your automation can break some rules for example packaging rules. How do you tackle these things?", "tokens": [50364, 865, 11, 428, 17769, 393, 1821, 512, 4474, 337, 1365, 16836, 4474, 13, 1012, 360, 291, 14896, 613, 721, 30, 50764], "temperature": 0.0, "avg_logprob": -0.23274420201778412, "compression_ratio": 1.62, "no_speech_prob": 0.15207992494106293}, {"id": 186, "seek": 140600, "start": 1414.0, "end": 1435.0, "text": " And yes, sometimes when those packaging guidelines were created the automation wasn't such a thing or maybe when they have written that they expected that humans will interact with the packages and maybe standardization,", "tokens": [50764, 400, 2086, 11, 2171, 562, 729, 16836, 12470, 645, 2942, 264, 17769, 2067, 380, 1270, 257, 551, 420, 1310, 562, 436, 362, 3720, 300, 436, 5176, 300, 6255, 486, 4648, 365, 264, 17401, 293, 1310, 3832, 2144, 11, 51814], "temperature": 0.0, "avg_logprob": -0.23274420201778412, "compression_ratio": 1.62, "no_speech_prob": 0.15207992494106293}, {"id": 187, "seek": 143500, "start": 1435.0, "end": 1448.0, "text": " who's rules, yeah. Yeah, we can also tweak the rules. It's not set in stone so we can maybe discuss. Yeah.", "tokens": [50364, 567, 311, 4474, 11, 1338, 13, 865, 11, 321, 393, 611, 29879, 264, 4474, 13, 467, 311, 406, 992, 294, 7581, 370, 321, 393, 1310, 2248, 13, 865, 13, 51014], "temperature": 0.0, "avg_logprob": -0.24475762718602231, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.2861550748348236}, {"id": 188, "seek": 143500, "start": 1448.0, "end": 1464.0, "text": " I trust the both modern and human. Yeah, that's a positive thing on the automation that yeah, it usually does not do like the human like things and be kind and that's I really like.", "tokens": [51014, 286, 3361, 264, 1293, 4363, 293, 1952, 13, 865, 11, 300, 311, 257, 3353, 551, 322, 264, 17769, 300, 1338, 11, 309, 2673, 775, 406, 360, 411, 264, 1952, 411, 721, 293, 312, 733, 293, 300, 311, 286, 534, 411, 13, 51814], "temperature": 0.0, "avg_logprob": -0.24475762718602231, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.2861550748348236}, {"id": 189, "seek": 146400, "start": 1464.0, "end": 1486.0, "text": " So yeah, be open for suggestion, communicate, don't ignore the issues. Just talk and see what others think about that because sometimes it can be like really valuable feedback and maybe people already have a suggestion how to fix that or how you should behave.", "tokens": [50364, 407, 1338, 11, 312, 1269, 337, 16541, 11, 7890, 11, 500, 380, 11200, 264, 2663, 13, 1449, 751, 293, 536, 437, 2357, 519, 466, 300, 570, 2171, 309, 393, 312, 411, 534, 8263, 5824, 293, 1310, 561, 1217, 362, 257, 16541, 577, 281, 3191, 300, 420, 577, 291, 820, 15158, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1286866621537642, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.1682119071483612}, {"id": 190, "seek": 148600, "start": 1486.0, "end": 1507.0, "text": " Yeah, sometimes you can also let the user decide for example we've talked about an issue if we should upload some archives to the cache if we should do that before it's merged into federal this gate and we were not sure we someone wants the automation someone.", "tokens": [50364, 865, 11, 2171, 291, 393, 611, 718, 264, 4195, 4536, 337, 1365, 321, 600, 2825, 466, 364, 2734, 498, 321, 820, 6580, 512, 25607, 281, 264, 19459, 498, 321, 820, 360, 300, 949, 309, 311, 36427, 666, 6019, 341, 8539, 293, 321, 645, 406, 988, 321, 1580, 2738, 264, 17769, 1580, 13, 51414], "temperature": 0.0, "avg_logprob": -0.348247902733939, "compression_ratio": 1.5662650602409638, "no_speech_prob": 0.23227736353874207}, {"id": 191, "seek": 150700, "start": 1508.0, "end": 1534.0, "text": " Yeah, someone wants just the security so let them decide. Yeah, and for us it helped that we are trying as much as possible to have the provisions as a regular user so we are not kind of special in any way with one slide exception to get less permissions that we need but otherwise we are trying.", "tokens": [50414, 865, 11, 1580, 2738, 445, 264, 3825, 370, 718, 552, 4536, 13, 865, 11, 293, 337, 505, 309, 4254, 300, 321, 366, 1382, 382, 709, 382, 1944, 281, 362, 264, 25034, 382, 257, 3890, 4195, 370, 321, 366, 406, 733, 295, 2121, 294, 604, 636, 365, 472, 4137, 11183, 281, 483, 1570, 32723, 300, 321, 643, 457, 5911, 321, 366, 1382, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2297506332397461, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.2719368636608124}, {"id": 192, "seek": 153400, "start": 1535.0, "end": 1537.0, "text": " Yeah, so.", "tokens": [50414, 865, 11, 370, 13, 50514], "temperature": 0.0, "avg_logprob": -0.230802023852313, "compression_ratio": 1.3935483870967742, "no_speech_prob": 0.06270743161439896}, {"id": 193, "seek": 153400, "start": 1539.0, "end": 1546.0, "text": " Similar to the previous one. You can continue with the voting but I'll skip to two points I had.", "tokens": [50614, 10905, 281, 264, 3894, 472, 13, 509, 393, 2354, 365, 264, 10419, 457, 286, 603, 10023, 281, 732, 2793, 286, 632, 13, 50964], "temperature": 0.0, "avg_logprob": -0.230802023852313, "compression_ratio": 1.3935483870967742, "no_speech_prob": 0.06270743161439896}, {"id": 194, "seek": 153400, "start": 1547.0, "end": 1555.0, "text": " Yeah, when people think that they you behave differently we can provide some config options but usually bait.", "tokens": [51014, 865, 11, 562, 561, 519, 300, 436, 291, 15158, 7614, 321, 393, 2893, 512, 6662, 3956, 457, 2673, 16865, 13, 51414], "temperature": 0.0, "avg_logprob": -0.230802023852313, "compression_ratio": 1.3935483870967742, "no_speech_prob": 0.06270743161439896}, {"id": 195, "seek": 155500, "start": 1556.0, "end": 1569.0, "text": " There can maybe be maybe people will realize that they don't need it but maybe they will be a new user that will have similar config option or similar feature request so you can maybe combine it for us.", "tokens": [50414, 821, 393, 1310, 312, 1310, 561, 486, 4325, 300, 436, 500, 380, 643, 309, 457, 1310, 436, 486, 312, 257, 777, 4195, 300, 486, 362, 2531, 6662, 3614, 420, 2531, 4111, 5308, 370, 291, 393, 1310, 10432, 309, 337, 505, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1605438444349501, "compression_ratio": 1.4852941176470589, "no_speech_prob": 0.20033489167690277}, {"id": 196, "seek": 156900, "start": 1569.0, "end": 1585.0, "text": " For us user defined actions helped a lot because we've done everyone has a different workflow and it was really hard to do securely and well but this was for us a huge and other.", "tokens": [50364, 1171, 505, 4195, 7642, 5909, 4254, 257, 688, 570, 321, 600, 1096, 1518, 575, 257, 819, 20993, 293, 309, 390, 534, 1152, 281, 360, 38348, 293, 731, 457, 341, 390, 337, 505, 257, 2603, 293, 661, 13, 51164], "temperature": 0.0, "avg_logprob": -0.28412414178615664, "compression_ratio": 1.37984496124031, "no_speech_prob": 0.10725945234298706}, {"id": 197, "seek": 158500, "start": 1585.0, "end": 1600.0, "text": " Yeah, and respond to the first issue and questions crucial even if it is like a little you weeks thingy that you are showing how you how you treat your users.", "tokens": [50364, 865, 11, 293, 4196, 281, 264, 700, 2734, 293, 1651, 11462, 754, 498, 309, 307, 411, 257, 707, 291, 3259, 551, 88, 300, 291, 366, 4099, 577, 291, 577, 291, 2387, 428, 5022, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3878910917984812, "compression_ratio": 1.4234234234234233, "no_speech_prob": 0.16726379096508026}, {"id": 198, "seek": 160000, "start": 1600.0, "end": 1614.0, "text": " So that's all from me. This is the project page for Stornon account and if we have maybe two minutes for question if you have any but maybe you can ask the audience.", "tokens": [50364, 407, 300, 311, 439, 490, 385, 13, 639, 307, 264, 1716, 3028, 337, 745, 1865, 266, 2696, 293, 498, 321, 362, 1310, 732, 2077, 337, 1168, 498, 291, 362, 604, 457, 1310, 291, 393, 1029, 264, 4034, 13, 51064], "temperature": 0.0, "avg_logprob": -0.291816792017977, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.29005342721939087}, {"id": 199, "seek": 160000, "start": 1615.0, "end": 1621.0, "text": " We don't have. Okay, so sorry about that but I think you've shared your opinion on that. So thanks a lot everyone.", "tokens": [51114, 492, 500, 380, 362, 13, 1033, 11, 370, 2597, 466, 300, 457, 286, 519, 291, 600, 5507, 428, 4800, 322, 300, 13, 407, 3231, 257, 688, 1518, 13, 51414], "temperature": 0.0, "avg_logprob": -0.291816792017977, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.29005342721939087}, {"id": 200, "seek": 163000, "start": 1630.0, "end": 1631.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.43188154697418213, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9866560697555542}], "language": "en"}