{"text": " All right, it's two. We'll get started. Hello, my name is Vojta Uranek. I work as a developer at Red Hat and in this short talk, I would like to discuss one approach, how you can ingest your data from into a machine learning pipeline from your databases in real time. First, this is how the machine learning pipeline can look like. And I will be speaking about where the beginning of this pipeline basically here, how to get the data from your databases into a machine learning pipeline. It can be as complex as this, but I will simplify. Just imagine you have your application running, you insert data in one or several databases, and now you want to get advantage of some machine learning or maybe AI and get your data and do some, for example, prediction. And you want to solve a question how to actually feed the data from your databases into a model, and especially when some new events are coming, how to do, for example, online predictions. It might sound simple, you just do some selects from the database, but when you have quite heavy traffic and you will run the selects in a loop, you will quickly find out that you will overload the databases with the selects. And if you add more conditions like you don't want to miss any data, you won't have consistent view of your data, you don't want to do the right and so on, you will shortly find out that it's a pretty hard problem. There are several ways how to tackle this problem, but I would suggest maybe in my opinion the best one is to change data capture. What does it mean? As the name suggests, change data capture is a process of observing some resource for a change, and if there is any change, you will extract this change and propagate it further, typically into some messaging system. What does it mean in terms of databases? It typically means to observe transaction log of the database, because transaction log of the database is typically the source of true for whole database, and all the changes which are happening to any data in the database are recorded in transaction log. So basically you will observe transaction log of your database, and if there is any change with some data you are interested in, it can be one example, whole database, whatever, you will extract this change from transaction log and send it into, for example, some messaging system. It's pretty, well, probably sounds good, but maybe you have to ask yourself if it's easy to implement. Fortunately, you don't have to solve these questions. You can use Dibizium, it's an open source for change data capture. It's pretty major. It has connectors for most of the popular databases, and it currently comes into two flavors. It originated as a Kafka Connect source connector, when you can deploy the Dibizium connectors into Kafka Connect, and they will connect to one or several databases of you are using, extract the changes, and send it to Kafka, where you can use sync connectors to do whatever you want, like, for example, updating the search index, invalidating the cache. You can, for example, use it also for replicating one database to another database, but in our case, you will probably want to push it into some data lake pool warehouse, feature store, or maybe even directly into some machine learning model. If you don't want to use Kafka for whatever reason, you can use Dibizium standalone, which is standalone process, which basically does the same, but allows you to push the events into whatever system you like, like Apache Pools, Google Pops up, and it can be even, for example, HTTP endpoint. And if you are missing any sync of for Dibizium server, it's pretty easy to implement your own. And Dibizium provides some other features, like, for example, it's capable to do snapshot of your database at any time. It can also transform the records before sending out into the messaging system and so on. So back to our problem. Unfortunately, this talk is too short to go through the whole example. On the page of this talk, for them, there is a link to our blog post when we described in details a use case when you store in the database images of handwritten digits. And whenever the image is stored there, you extract the change using Dibizium, send it to Kafka and Kafka, send it to TensorFlow, which will recognize what number is written in this picture. So it's a well-known example. And everything works in real time. As I said, we also have a full example on GitHub. But what I would like to show you here is that it's really simple. It's basically consists of deploying Dibizium and configuring it. And it's really just one page of JSON config. Here is you just provide credentials. And more interesting part is here. It's some transformation. Here is one predefined transformation where I extract only the content of newly inserted image. And because there is some caveat when you use TensorFlow with Kafka because it can interpret correctly the bytes, I will, I'm transforming the image into string, which is later on parsed in TensorFlow. But I would have to do it in TensorFlow anyway. So it's no overhead. But I can define my own transformation here. So, and it's just a couple of lines of code which just converted into string. And on TensorFlow side, it's similar, easy. It's again one page. Here I define the coding function which decodes the string. And I think it retrieves from Kafka. The most of the code is just defining Kafka endpoint. And it's about three lines to push it into the model which will recognize what is the number on the image and produce the result which you can consume further. So, as I said, if you are interested in it, please go to our website, take it up and take a look. And basically that's it. So to sum up, DBZoom is able to do a snapshot of your database and load existing data from your database into messaging system or directly into TensorFlow. And it can retrieve any change. So it can, once anything is stored into the database, it can immediately extract this change and send it further to your pipeline so you can do real-time analysis of your data. And basically it works for, you can deploy many databases and do more things with that. So that's all. If sounds interesting to you, you can try out DBZoom and please share feedback with us on Zulek, or mailing list. We have pretty large Vibran community and we will appreciate your feedback. Thank you so much for your attention. Thank you very much, Vojta. We have time for one question. Is this working? One question. Someone come up with one question. Come on. So if no one, just if you have any questions, switch pop up later on, just hit me on the corridor or elsewhere in the conference. Thanks for the presentation. My question is, is there any database that's already provided what DBZoom does by default? Any change? Could we repeat? I don't hear. Yeah, sure. Is there any data? Can you please stop moving so we can hear the question? Thanks for the presentation. My question is, is there any database that does this, what DBZoom does natively already change tracing? What do you mean natively? Because without any external tool like DBZoom, is there any database that does this already? Well, it again boils up to what means natively because we leverage typically some native features of database. For example, for Postgres, we use replication slot and we just read replication slot from Postgres. So you always need something which will need something which reads from database or from Mongo, which is from change stream. So always the database provides usually this natively, but you need something which will read it and translate it into something usable, which will parse, for example, the data you get from the replication slot from Postgres and so on. So yeah, that was the question actually. Is there any database that does this anyway without using DBZoom? But you said, I think there is no competitor then. Pardon? Is there any competitors? Yes, like is database is doing this natively already what DBZoom does? I'm not aware if there is any database which uses this. I'm aware that some databases provides this, but several of them use DBZoom under the hood as far as I know. Okay, perfect. Thank you very much, Vojta. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 28.0, "text": " All right, it's two. We'll get started.", "tokens": [50364, 1057, 558, 11, 309, 311, 732, 13, 492, 603, 483, 1409, 13, 51764], "temperature": 0.0, "avg_logprob": -0.6580429077148438, "compression_ratio": 0.8478260869565217, "no_speech_prob": 0.68178391456604}, {"id": 1, "seek": 2800, "start": 28.0, "end": 43.0, "text": " Hello, my name is Vojta Uranek. I work as a developer at Red Hat and in this short talk,", "tokens": [50364, 2425, 11, 452, 1315, 307, 7518, 73, 1328, 624, 424, 23255, 13, 286, 589, 382, 257, 10754, 412, 4477, 15867, 293, 294, 341, 2099, 751, 11, 51114], "temperature": 0.0, "avg_logprob": -0.32899049612192005, "compression_ratio": 1.3237410071942446, "no_speech_prob": 0.6136086583137512}, {"id": 2, "seek": 2800, "start": 43.0, "end": 49.0, "text": " I would like to discuss one approach, how you can ingest your data from into a machine learning", "tokens": [51114, 286, 576, 411, 281, 2248, 472, 3109, 11, 577, 291, 393, 3957, 377, 428, 1412, 490, 666, 257, 3479, 2539, 51414], "temperature": 0.0, "avg_logprob": -0.32899049612192005, "compression_ratio": 1.3237410071942446, "no_speech_prob": 0.6136086583137512}, {"id": 3, "seek": 4900, "start": 49.0, "end": 58.0, "text": " pipeline from your databases in real time. First, this is how the machine learning pipeline can", "tokens": [50364, 15517, 490, 428, 22380, 294, 957, 565, 13, 2386, 11, 341, 307, 577, 264, 3479, 2539, 15517, 393, 50814], "temperature": 0.0, "avg_logprob": -0.17990262806415558, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.952541708946228}, {"id": 4, "seek": 4900, "start": 58.0, "end": 65.0, "text": " look like. And I will be speaking about where the beginning of this pipeline basically here,", "tokens": [50814, 574, 411, 13, 400, 286, 486, 312, 4124, 466, 689, 264, 2863, 295, 341, 15517, 1936, 510, 11, 51164], "temperature": 0.0, "avg_logprob": -0.17990262806415558, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.952541708946228}, {"id": 5, "seek": 4900, "start": 65.0, "end": 72.0, "text": " how to get the data from your databases into a machine learning pipeline. It can be as complex as", "tokens": [51164, 577, 281, 483, 264, 1412, 490, 428, 22380, 666, 257, 3479, 2539, 15517, 13, 467, 393, 312, 382, 3997, 382, 51514], "temperature": 0.0, "avg_logprob": -0.17990262806415558, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.952541708946228}, {"id": 6, "seek": 7200, "start": 72.0, "end": 80.0, "text": " this, but I will simplify. Just imagine you have your application running, you insert data in one or", "tokens": [50364, 341, 11, 457, 286, 486, 20460, 13, 1449, 3811, 291, 362, 428, 3861, 2614, 11, 291, 8969, 1412, 294, 472, 420, 50764], "temperature": 0.0, "avg_logprob": -0.140880114595655, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.5950393080711365}, {"id": 7, "seek": 7200, "start": 80.0, "end": 90.0, "text": " several databases, and now you want to get advantage of some machine learning or maybe AI and get your", "tokens": [50764, 2940, 22380, 11, 293, 586, 291, 528, 281, 483, 5002, 295, 512, 3479, 2539, 420, 1310, 7318, 293, 483, 428, 51264], "temperature": 0.0, "avg_logprob": -0.140880114595655, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.5950393080711365}, {"id": 8, "seek": 7200, "start": 90.0, "end": 98.0, "text": " data and do some, for example, prediction. And you want to solve a question how to actually feed the", "tokens": [51264, 1412, 293, 360, 512, 11, 337, 1365, 11, 17630, 13, 400, 291, 528, 281, 5039, 257, 1168, 577, 281, 767, 3154, 264, 51664], "temperature": 0.0, "avg_logprob": -0.140880114595655, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.5950393080711365}, {"id": 9, "seek": 9800, "start": 98.0, "end": 107.0, "text": " data from your databases into a model, and especially when some new events are coming, how to do, for", "tokens": [50364, 1412, 490, 428, 22380, 666, 257, 2316, 11, 293, 2318, 562, 512, 777, 3931, 366, 1348, 11, 577, 281, 360, 11, 337, 50814], "temperature": 0.0, "avg_logprob": -0.1523819859822591, "compression_ratio": 1.5816326530612246, "no_speech_prob": 0.24533075094223022}, {"id": 10, "seek": 9800, "start": 107.0, "end": 115.0, "text": " example, online predictions. It might sound simple, you just do some selects from the database, but when", "tokens": [50814, 1365, 11, 2950, 21264, 13, 467, 1062, 1626, 2199, 11, 291, 445, 360, 512, 3048, 82, 490, 264, 8149, 11, 457, 562, 51214], "temperature": 0.0, "avg_logprob": -0.1523819859822591, "compression_ratio": 1.5816326530612246, "no_speech_prob": 0.24533075094223022}, {"id": 11, "seek": 9800, "start": 115.0, "end": 124.0, "text": " you have quite heavy traffic and you will run the selects in a loop, you will quickly find out that you", "tokens": [51214, 291, 362, 1596, 4676, 6419, 293, 291, 486, 1190, 264, 3048, 82, 294, 257, 6367, 11, 291, 486, 2661, 915, 484, 300, 291, 51664], "temperature": 0.0, "avg_logprob": -0.1523819859822591, "compression_ratio": 1.5816326530612246, "no_speech_prob": 0.24533075094223022}, {"id": 12, "seek": 12400, "start": 124.0, "end": 130.0, "text": " will overload the databases with the selects. And if you add more conditions like you don't want to miss any", "tokens": [50364, 486, 28777, 264, 22380, 365, 264, 3048, 82, 13, 400, 498, 291, 909, 544, 4487, 411, 291, 500, 380, 528, 281, 1713, 604, 50664], "temperature": 0.0, "avg_logprob": -0.16636439561843872, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.19755718111991882}, {"id": 13, "seek": 12400, "start": 130.0, "end": 139.0, "text": " data, you won't have consistent view of your data, you don't want to do the right and so on, you will", "tokens": [50664, 1412, 11, 291, 1582, 380, 362, 8398, 1910, 295, 428, 1412, 11, 291, 500, 380, 528, 281, 360, 264, 558, 293, 370, 322, 11, 291, 486, 51114], "temperature": 0.0, "avg_logprob": -0.16636439561843872, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.19755718111991882}, {"id": 14, "seek": 12400, "start": 139.0, "end": 147.0, "text": " shortly find out that it's a pretty hard problem. There are several ways how to tackle this problem, but I", "tokens": [51114, 13392, 915, 484, 300, 309, 311, 257, 1238, 1152, 1154, 13, 821, 366, 2940, 2098, 577, 281, 14896, 341, 1154, 11, 457, 286, 51514], "temperature": 0.0, "avg_logprob": -0.16636439561843872, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.19755718111991882}, {"id": 15, "seek": 14700, "start": 147.0, "end": 155.0, "text": " would suggest maybe in my opinion the best one is to change data capture. What does it mean? As the name suggests,", "tokens": [50364, 576, 3402, 1310, 294, 452, 4800, 264, 1151, 472, 307, 281, 1319, 1412, 7983, 13, 708, 775, 309, 914, 30, 1018, 264, 1315, 13409, 11, 50764], "temperature": 0.0, "avg_logprob": -0.08550873962608543, "compression_ratio": 1.712041884816754, "no_speech_prob": 0.3380172848701477}, {"id": 16, "seek": 14700, "start": 155.0, "end": 164.0, "text": " change data capture is a process of observing some resource for a change, and if there is any change, you", "tokens": [50764, 1319, 1412, 7983, 307, 257, 1399, 295, 22107, 512, 7684, 337, 257, 1319, 11, 293, 498, 456, 307, 604, 1319, 11, 291, 51214], "temperature": 0.0, "avg_logprob": -0.08550873962608543, "compression_ratio": 1.712041884816754, "no_speech_prob": 0.3380172848701477}, {"id": 17, "seek": 14700, "start": 164.0, "end": 171.0, "text": " will extract this change and propagate it further, typically into some messaging system. What does it mean", "tokens": [51214, 486, 8947, 341, 1319, 293, 48256, 309, 3052, 11, 5850, 666, 512, 21812, 1185, 13, 708, 775, 309, 914, 51564], "temperature": 0.0, "avg_logprob": -0.08550873962608543, "compression_ratio": 1.712041884816754, "no_speech_prob": 0.3380172848701477}, {"id": 18, "seek": 17100, "start": 171.0, "end": 178.0, "text": " in terms of databases? It typically means to observe transaction log of the database, because transaction", "tokens": [50364, 294, 2115, 295, 22380, 30, 467, 5850, 1355, 281, 11441, 14425, 3565, 295, 264, 8149, 11, 570, 14425, 50714], "temperature": 0.0, "avg_logprob": -0.09756029976738824, "compression_ratio": 2.053658536585366, "no_speech_prob": 0.19234605133533478}, {"id": 19, "seek": 17100, "start": 178.0, "end": 185.0, "text": " log of the database is typically the source of true for whole database, and all the changes which are", "tokens": [50714, 3565, 295, 264, 8149, 307, 5850, 264, 4009, 295, 2074, 337, 1379, 8149, 11, 293, 439, 264, 2962, 597, 366, 51064], "temperature": 0.0, "avg_logprob": -0.09756029976738824, "compression_ratio": 2.053658536585366, "no_speech_prob": 0.19234605133533478}, {"id": 20, "seek": 17100, "start": 185.0, "end": 192.0, "text": " happening to any data in the database are recorded in transaction log. So basically you will observe", "tokens": [51064, 2737, 281, 604, 1412, 294, 264, 8149, 366, 8287, 294, 14425, 3565, 13, 407, 1936, 291, 486, 11441, 51414], "temperature": 0.0, "avg_logprob": -0.09756029976738824, "compression_ratio": 2.053658536585366, "no_speech_prob": 0.19234605133533478}, {"id": 21, "seek": 17100, "start": 192.0, "end": 200.0, "text": " transaction log of your database, and if there is any change with some data you are interested in, it can be one", "tokens": [51414, 14425, 3565, 295, 428, 8149, 11, 293, 498, 456, 307, 604, 1319, 365, 512, 1412, 291, 366, 3102, 294, 11, 309, 393, 312, 472, 51814], "temperature": 0.0, "avg_logprob": -0.09756029976738824, "compression_ratio": 2.053658536585366, "no_speech_prob": 0.19234605133533478}, {"id": 22, "seek": 20000, "start": 200.0, "end": 207.0, "text": " example, whole database, whatever, you will extract this change from transaction log and send it into, for example,", "tokens": [50364, 1365, 11, 1379, 8149, 11, 2035, 11, 291, 486, 8947, 341, 1319, 490, 14425, 3565, 293, 2845, 309, 666, 11, 337, 1365, 11, 50714], "temperature": 0.0, "avg_logprob": -0.20626513929252166, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.10265564918518066}, {"id": 23, "seek": 20000, "start": 207.0, "end": 219.0, "text": " some messaging system. It's pretty, well, probably sounds good, but maybe you have to ask yourself if it's easy", "tokens": [50714, 512, 21812, 1185, 13, 467, 311, 1238, 11, 731, 11, 1391, 3263, 665, 11, 457, 1310, 291, 362, 281, 1029, 1803, 498, 309, 311, 1858, 51314], "temperature": 0.0, "avg_logprob": -0.20626513929252166, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.10265564918518066}, {"id": 24, "seek": 20000, "start": 219.0, "end": 228.0, "text": " to implement. Fortunately, you don't have to solve these questions. You can use Dibizium, it's an open source", "tokens": [51314, 281, 4445, 13, 20652, 11, 291, 500, 380, 362, 281, 5039, 613, 1651, 13, 509, 393, 764, 413, 897, 590, 2197, 11, 309, 311, 364, 1269, 4009, 51764], "temperature": 0.0, "avg_logprob": -0.20626513929252166, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.10265564918518066}, {"id": 25, "seek": 22800, "start": 228.0, "end": 239.0, "text": " for change data capture. It's pretty major. It has connectors for most of the popular databases, and it", "tokens": [50364, 337, 1319, 1412, 7983, 13, 467, 311, 1238, 2563, 13, 467, 575, 31865, 337, 881, 295, 264, 3743, 22380, 11, 293, 309, 50914], "temperature": 0.0, "avg_logprob": -0.16052922835716835, "compression_ratio": 1.3987341772151898, "no_speech_prob": 0.2979072332382202}, {"id": 26, "seek": 22800, "start": 239.0, "end": 249.0, "text": " currently comes into two flavors. It originated as a Kafka Connect source connector, when you can deploy the Dibizium", "tokens": [50914, 4362, 1487, 666, 732, 16303, 13, 467, 31129, 382, 257, 47064, 11653, 4009, 19127, 11, 562, 291, 393, 7274, 264, 413, 897, 590, 2197, 51414], "temperature": 0.0, "avg_logprob": -0.16052922835716835, "compression_ratio": 1.3987341772151898, "no_speech_prob": 0.2979072332382202}, {"id": 27, "seek": 24900, "start": 249.0, "end": 261.0, "text": " connectors into Kafka Connect, and they will connect to one or several databases of you are using, extract the changes,", "tokens": [50364, 31865, 666, 47064, 11653, 11, 293, 436, 486, 1745, 281, 472, 420, 2940, 22380, 295, 291, 366, 1228, 11, 8947, 264, 2962, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1433481090473679, "compression_ratio": 1.5, "no_speech_prob": 0.29937562346458435}, {"id": 28, "seek": 24900, "start": 261.0, "end": 273.0, "text": " and send it to Kafka, where you can use sync connectors to do whatever you want, like, for example, updating", "tokens": [50964, 293, 2845, 309, 281, 47064, 11, 689, 291, 393, 764, 20271, 31865, 281, 360, 2035, 291, 528, 11, 411, 11, 337, 1365, 11, 25113, 51564], "temperature": 0.0, "avg_logprob": -0.1433481090473679, "compression_ratio": 1.5, "no_speech_prob": 0.29937562346458435}, {"id": 29, "seek": 27300, "start": 273.0, "end": 280.0, "text": " the search index, invalidating the cache. You can, for example, use it also for replicating one database to another", "tokens": [50364, 264, 3164, 8186, 11, 34702, 990, 264, 19459, 13, 509, 393, 11, 337, 1365, 11, 764, 309, 611, 337, 3248, 30541, 472, 8149, 281, 1071, 50714], "temperature": 0.0, "avg_logprob": -0.13820244545160337, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.2510318458080292}, {"id": 30, "seek": 27300, "start": 280.0, "end": 289.0, "text": " database, but in our case, you will probably want to push it into some data lake pool warehouse, feature store, or maybe", "tokens": [50714, 8149, 11, 457, 294, 527, 1389, 11, 291, 486, 1391, 528, 281, 2944, 309, 666, 512, 1412, 11001, 7005, 22244, 11, 4111, 3531, 11, 420, 1310, 51164], "temperature": 0.0, "avg_logprob": -0.13820244545160337, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.2510318458080292}, {"id": 31, "seek": 27300, "start": 289.0, "end": 299.0, "text": " even directly into some machine learning model. If you don't want to use Kafka for whatever reason, you can use Dibizium", "tokens": [51164, 754, 3838, 666, 512, 3479, 2539, 2316, 13, 759, 291, 500, 380, 528, 281, 764, 47064, 337, 2035, 1778, 11, 291, 393, 764, 413, 897, 590, 2197, 51664], "temperature": 0.0, "avg_logprob": -0.13820244545160337, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.2510318458080292}, {"id": 32, "seek": 29900, "start": 299.0, "end": 308.0, "text": " standalone, which is standalone process, which basically does the same, but allows you to push the events into whatever", "tokens": [50364, 37454, 11, 597, 307, 37454, 1399, 11, 597, 1936, 775, 264, 912, 11, 457, 4045, 291, 281, 2944, 264, 3931, 666, 2035, 50814], "temperature": 0.0, "avg_logprob": -0.21781626807318794, "compression_ratio": 1.5497835497835497, "no_speech_prob": 0.14839228987693787}, {"id": 33, "seek": 29900, "start": 308.0, "end": 317.0, "text": " system you like, like Apache Pools, Google Pops up, and it can be even, for example, HTTP endpoint. And if you are", "tokens": [50814, 1185, 291, 411, 11, 411, 46597, 430, 29298, 11, 3329, 430, 3370, 493, 11, 293, 309, 393, 312, 754, 11, 337, 1365, 11, 33283, 35795, 13, 400, 498, 291, 366, 51264], "temperature": 0.0, "avg_logprob": -0.21781626807318794, "compression_ratio": 1.5497835497835497, "no_speech_prob": 0.14839228987693787}, {"id": 34, "seek": 29900, "start": 317.0, "end": 326.0, "text": " missing any sync of for Dibizium server, it's pretty easy to implement your own. And Dibizium provides some other features,", "tokens": [51264, 5361, 604, 20271, 295, 337, 413, 897, 590, 2197, 7154, 11, 309, 311, 1238, 1858, 281, 4445, 428, 1065, 13, 400, 413, 897, 590, 2197, 6417, 512, 661, 4122, 11, 51714], "temperature": 0.0, "avg_logprob": -0.21781626807318794, "compression_ratio": 1.5497835497835497, "no_speech_prob": 0.14839228987693787}, {"id": 35, "seek": 32600, "start": 326.0, "end": 334.0, "text": " like, for example, it's capable to do snapshot of your database at any time. It can also transform the records before sending out", "tokens": [50364, 411, 11, 337, 1365, 11, 309, 311, 8189, 281, 360, 30163, 295, 428, 8149, 412, 604, 565, 13, 467, 393, 611, 4088, 264, 7724, 949, 7750, 484, 50764], "temperature": 0.0, "avg_logprob": -0.09556265200598765, "compression_ratio": 1.4482758620689655, "no_speech_prob": 0.17077147960662842}, {"id": 36, "seek": 32600, "start": 334.0, "end": 345.0, "text": " into the messaging system and so on. So back to our problem. Unfortunately, this talk is too short to go through the whole", "tokens": [50764, 666, 264, 21812, 1185, 293, 370, 322, 13, 407, 646, 281, 527, 1154, 13, 8590, 11, 341, 751, 307, 886, 2099, 281, 352, 807, 264, 1379, 51314], "temperature": 0.0, "avg_logprob": -0.09556265200598765, "compression_ratio": 1.4482758620689655, "no_speech_prob": 0.17077147960662842}, {"id": 37, "seek": 34500, "start": 345.0, "end": 357.0, "text": " example. On the page of this talk, for them, there is a link to our blog post when we described in details a use case when you", "tokens": [50364, 1365, 13, 1282, 264, 3028, 295, 341, 751, 11, 337, 552, 11, 456, 307, 257, 2113, 281, 527, 6968, 2183, 562, 321, 7619, 294, 4365, 257, 764, 1389, 562, 291, 50964], "temperature": 0.0, "avg_logprob": -0.16625472903251648, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.4746094346046448}, {"id": 38, "seek": 34500, "start": 357.0, "end": 368.0, "text": " store in the database images of handwritten digits. And whenever the image is stored there, you extract the change using Dibizium,", "tokens": [50964, 3531, 294, 264, 8149, 5267, 295, 1011, 26859, 27011, 13, 400, 5699, 264, 3256, 307, 12187, 456, 11, 291, 8947, 264, 1319, 1228, 413, 897, 590, 2197, 11, 51514], "temperature": 0.0, "avg_logprob": -0.16625472903251648, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.4746094346046448}, {"id": 39, "seek": 36800, "start": 368.0, "end": 379.0, "text": " send it to Kafka and Kafka, send it to TensorFlow, which will recognize what number is written in this picture. So it's a well-known", "tokens": [50364, 2845, 309, 281, 47064, 293, 47064, 11, 2845, 309, 281, 37624, 11, 597, 486, 5521, 437, 1230, 307, 3720, 294, 341, 3036, 13, 407, 309, 311, 257, 731, 12, 6861, 50914], "temperature": 0.0, "avg_logprob": -0.12990198415868423, "compression_ratio": 1.4105263157894736, "no_speech_prob": 0.29323962330818176}, {"id": 40, "seek": 36800, "start": 379.0, "end": 390.0, "text": " example. And everything works in real time. As I said, we also have a full example on GitHub. But what I would like to show you here is", "tokens": [50914, 1365, 13, 400, 1203, 1985, 294, 957, 565, 13, 1018, 286, 848, 11, 321, 611, 362, 257, 1577, 1365, 322, 23331, 13, 583, 437, 286, 576, 411, 281, 855, 291, 510, 307, 51464], "temperature": 0.0, "avg_logprob": -0.12990198415868423, "compression_ratio": 1.4105263157894736, "no_speech_prob": 0.29323962330818176}, {"id": 41, "seek": 39000, "start": 390.0, "end": 403.0, "text": " that it's really simple. It's basically consists of deploying Dibizium and configuring it. And it's really just one page of JSON config. Here is", "tokens": [50364, 300, 309, 311, 534, 2199, 13, 467, 311, 1936, 14689, 295, 34198, 413, 897, 590, 2197, 293, 6662, 1345, 309, 13, 400, 309, 311, 534, 445, 472, 3028, 295, 31828, 6662, 13, 1692, 307, 51014], "temperature": 0.0, "avg_logprob": -0.16344002943772537, "compression_ratio": 1.5786516853932584, "no_speech_prob": 0.47565555572509766}, {"id": 42, "seek": 39000, "start": 403.0, "end": 414.0, "text": " you just provide credentials. And more interesting part is here. It's some transformation. Here is one predefined transformation where I", "tokens": [51014, 291, 445, 2893, 27404, 13, 400, 544, 1880, 644, 307, 510, 13, 467, 311, 512, 9887, 13, 1692, 307, 472, 659, 37716, 9887, 689, 286, 51564], "temperature": 0.0, "avg_logprob": -0.16344002943772537, "compression_ratio": 1.5786516853932584, "no_speech_prob": 0.47565555572509766}, {"id": 43, "seek": 41400, "start": 414.0, "end": 427.0, "text": " extract only the content of newly inserted image. And because there is some caveat when you use TensorFlow with Kafka because it can", "tokens": [50364, 8947, 787, 264, 2701, 295, 15109, 27992, 3256, 13, 400, 570, 456, 307, 512, 43012, 562, 291, 764, 37624, 365, 47064, 570, 309, 393, 51014], "temperature": 0.0, "avg_logprob": -0.12752039201797977, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.2849842607975006}, {"id": 44, "seek": 41400, "start": 427.0, "end": 438.0, "text": " interpret correctly the bytes, I will, I'm transforming the image into string, which is later on parsed in TensorFlow. But I would have to do it", "tokens": [51014, 7302, 8944, 264, 36088, 11, 286, 486, 11, 286, 478, 27210, 264, 3256, 666, 6798, 11, 597, 307, 1780, 322, 21156, 292, 294, 37624, 13, 583, 286, 576, 362, 281, 360, 309, 51564], "temperature": 0.0, "avg_logprob": -0.12752039201797977, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.2849842607975006}, {"id": 45, "seek": 43800, "start": 438.0, "end": 451.0, "text": " in TensorFlow anyway. So it's no overhead. But I can define my own transformation here. So, and it's just a couple of lines of code which", "tokens": [50364, 294, 37624, 4033, 13, 407, 309, 311, 572, 19922, 13, 583, 286, 393, 6964, 452, 1065, 9887, 510, 13, 407, 11, 293, 309, 311, 445, 257, 1916, 295, 3876, 295, 3089, 597, 51014], "temperature": 0.0, "avg_logprob": -0.16584894392225477, "compression_ratio": 1.5343915343915344, "no_speech_prob": 0.2615329623222351}, {"id": 46, "seek": 43800, "start": 451.0, "end": 463.0, "text": " just converted into string. And on TensorFlow side, it's similar, easy. It's again one page. Here I define the coding function which decodes the string.", "tokens": [51014, 445, 16424, 666, 6798, 13, 400, 322, 37624, 1252, 11, 309, 311, 2531, 11, 1858, 13, 467, 311, 797, 472, 3028, 13, 1692, 286, 6964, 264, 17720, 2445, 597, 979, 4789, 264, 6798, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16584894392225477, "compression_ratio": 1.5343915343915344, "no_speech_prob": 0.2615329623222351}, {"id": 47, "seek": 46300, "start": 463.0, "end": 476.0, "text": " And I think it retrieves from Kafka. The most of the code is just defining Kafka endpoint. And it's about three lines to push it into the model which will", "tokens": [50364, 400, 286, 519, 309, 19817, 977, 490, 47064, 13, 440, 881, 295, 264, 3089, 307, 445, 17827, 47064, 35795, 13, 400, 309, 311, 466, 1045, 3876, 281, 2944, 309, 666, 264, 2316, 597, 486, 51014], "temperature": 0.0, "avg_logprob": -0.15819602966308594, "compression_ratio": 1.4951923076923077, "no_speech_prob": 0.49212557077407837}, {"id": 48, "seek": 46300, "start": 476.0, "end": 490.0, "text": " recognize what is the number on the image and produce the result which you can consume further. So, as I said, if you are interested in it, please go to our", "tokens": [51014, 5521, 437, 307, 264, 1230, 322, 264, 3256, 293, 5258, 264, 1874, 597, 291, 393, 14732, 3052, 13, 407, 11, 382, 286, 848, 11, 498, 291, 366, 3102, 294, 309, 11, 1767, 352, 281, 527, 51714], "temperature": 0.0, "avg_logprob": -0.15819602966308594, "compression_ratio": 1.4951923076923077, "no_speech_prob": 0.49212557077407837}, {"id": 49, "seek": 49000, "start": 490.0, "end": 501.0, "text": " website, take it up and take a look. And basically that's it. So to sum up, DBZoom is able to do a snapshot of your database and load existing data from your", "tokens": [50364, 3144, 11, 747, 309, 493, 293, 747, 257, 574, 13, 400, 1936, 300, 311, 309, 13, 407, 281, 2408, 493, 11, 26754, 57, 78, 298, 307, 1075, 281, 360, 257, 30163, 295, 428, 8149, 293, 3677, 6741, 1412, 490, 428, 50914], "temperature": 0.0, "avg_logprob": -0.24768370076229698, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.21899394690990448}, {"id": 50, "seek": 49000, "start": 501.0, "end": 514.0, "text": " database into messaging system or directly into TensorFlow. And it can retrieve any change. So it can, once anything is stored into the database, it can", "tokens": [50914, 8149, 666, 21812, 1185, 420, 3838, 666, 37624, 13, 400, 309, 393, 30254, 604, 1319, 13, 407, 309, 393, 11, 1564, 1340, 307, 12187, 666, 264, 8149, 11, 309, 393, 51564], "temperature": 0.0, "avg_logprob": -0.24768370076229698, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.21899394690990448}, {"id": 51, "seek": 51400, "start": 514.0, "end": 531.0, "text": " immediately extract this change and send it further to your pipeline so you can do real-time analysis of your data. And basically it works for, you can deploy many databases and do more things with that.", "tokens": [50364, 4258, 8947, 341, 1319, 293, 2845, 309, 3052, 281, 428, 15517, 370, 291, 393, 360, 957, 12, 3766, 5215, 295, 428, 1412, 13, 400, 1936, 309, 1985, 337, 11, 291, 393, 7274, 867, 22380, 293, 360, 544, 721, 365, 300, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15530625449286567, "compression_ratio": 1.4195804195804196, "no_speech_prob": 0.21269965171813965}, {"id": 52, "seek": 53100, "start": 531.0, "end": 548.0, "text": " So that's all. If sounds interesting to you, you can try out DBZoom and please share feedback with us on Zulek, or mailing list. We have pretty large Vibran community and we will appreciate your feedback.", "tokens": [50364, 407, 300, 311, 439, 13, 759, 3263, 1880, 281, 291, 11, 291, 393, 853, 484, 26754, 57, 78, 298, 293, 1767, 2073, 5824, 365, 505, 322, 1176, 2271, 74, 11, 420, 41612, 1329, 13, 492, 362, 1238, 2416, 691, 897, 4257, 1768, 293, 321, 486, 4449, 428, 5824, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3145271517195792, "compression_ratio": 1.3161290322580645, "no_speech_prob": 0.6247506141662598}, {"id": 53, "seek": 54800, "start": 548.0, "end": 550.0, "text": " Thank you so much for your attention.", "tokens": [50364, 1044, 291, 370, 709, 337, 428, 3202, 13, 50464], "temperature": 0.0, "avg_logprob": -0.17408521124657164, "compression_ratio": 1.401639344262295, "no_speech_prob": 0.8488855361938477}, {"id": 54, "seek": 54800, "start": 550.0, "end": 569.0, "text": " Thank you very much, Vojta. We have time for one question. Is this working? One question. Someone come up with one question. Come on.", "tokens": [50464, 1044, 291, 588, 709, 11, 7518, 73, 1328, 13, 492, 362, 565, 337, 472, 1168, 13, 1119, 341, 1364, 30, 1485, 1168, 13, 8734, 808, 493, 365, 472, 1168, 13, 2492, 322, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17408521124657164, "compression_ratio": 1.401639344262295, "no_speech_prob": 0.8488855361938477}, {"id": 55, "seek": 56900, "start": 569.0, "end": 591.0, "text": " So if no one, just if you have any questions, switch pop up later on, just hit me on the corridor or elsewhere in the conference.", "tokens": [50364, 407, 498, 572, 472, 11, 445, 498, 291, 362, 604, 1651, 11, 3679, 1665, 493, 1780, 322, 11, 445, 2045, 385, 322, 264, 25602, 420, 14517, 294, 264, 7586, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3096809387207031, "compression_ratio": 1.2772277227722773, "no_speech_prob": 0.4927394390106201}, {"id": 56, "seek": 59100, "start": 591.0, "end": 601.0, "text": " Thanks for the presentation. My question is, is there any database that's already provided what DBZoom does by default? Any change?", "tokens": [50364, 2561, 337, 264, 5860, 13, 1222, 1168, 307, 11, 307, 456, 604, 8149, 300, 311, 1217, 5649, 437, 26754, 57, 78, 298, 775, 538, 7576, 30, 2639, 1319, 30, 50864], "temperature": 0.0, "avg_logprob": -0.153921403746674, "compression_ratio": 1.4310344827586208, "no_speech_prob": 0.8760599493980408}, {"id": 57, "seek": 59100, "start": 601.0, "end": 603.0, "text": " Could we repeat? I don't hear.", "tokens": [50864, 7497, 321, 7149, 30, 286, 500, 380, 1568, 13, 50964], "temperature": 0.0, "avg_logprob": -0.153921403746674, "compression_ratio": 1.4310344827586208, "no_speech_prob": 0.8760599493980408}, {"id": 58, "seek": 59100, "start": 603.0, "end": 606.0, "text": " Yeah, sure. Is there any data?", "tokens": [50964, 865, 11, 988, 13, 1119, 456, 604, 1412, 30, 51114], "temperature": 0.0, "avg_logprob": -0.153921403746674, "compression_ratio": 1.4310344827586208, "no_speech_prob": 0.8760599493980408}, {"id": 59, "seek": 59100, "start": 606.0, "end": 611.0, "text": " Can you please stop moving so we can hear the question?", "tokens": [51114, 1664, 291, 1767, 1590, 2684, 370, 321, 393, 1568, 264, 1168, 30, 51364], "temperature": 0.0, "avg_logprob": -0.153921403746674, "compression_ratio": 1.4310344827586208, "no_speech_prob": 0.8760599493980408}, {"id": 60, "seek": 61100, "start": 611.0, "end": 620.0, "text": " Thanks for the presentation. My question is, is there any database that does this, what DBZoom does natively already change tracing?", "tokens": [50364, 2561, 337, 264, 5860, 13, 1222, 1168, 307, 11, 307, 456, 604, 8149, 300, 775, 341, 11, 437, 26754, 57, 78, 298, 775, 8470, 356, 1217, 1319, 25262, 30, 50814], "temperature": 0.0, "avg_logprob": -0.10819515105216734, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.3789437413215637}, {"id": 61, "seek": 61100, "start": 620.0, "end": 628.0, "text": " What do you mean natively? Because without any external tool like DBZoom, is there any database that does this already?", "tokens": [50814, 708, 360, 291, 914, 8470, 356, 30, 1436, 1553, 604, 8320, 2290, 411, 26754, 57, 78, 298, 11, 307, 456, 604, 8149, 300, 775, 341, 1217, 30, 51214], "temperature": 0.0, "avg_logprob": -0.10819515105216734, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.3789437413215637}, {"id": 62, "seek": 62800, "start": 628.0, "end": 643.0, "text": " Well, it again boils up to what means natively because we leverage typically some native features of database. For example, for Postgres, we use replication slot and we just read replication slot from Postgres.", "tokens": [50364, 1042, 11, 309, 797, 35049, 493, 281, 437, 1355, 8470, 356, 570, 321, 13982, 5850, 512, 8470, 4122, 295, 8149, 13, 1171, 1365, 11, 337, 10223, 45189, 11, 321, 764, 39911, 14747, 293, 321, 445, 1401, 39911, 14747, 490, 10223, 45189, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1796281441398289, "compression_ratio": 1.4685314685314685, "no_speech_prob": 0.634296715259552}, {"id": 63, "seek": 64300, "start": 643.0, "end": 669.0, "text": " So you always need something which will need something which reads from database or from Mongo, which is from change stream. So always the database provides usually this natively, but you need something which will read it and translate it into something usable, which will parse, for example, the data you get from the replication slot from Postgres and so on.", "tokens": [50364, 407, 291, 1009, 643, 746, 597, 486, 643, 746, 597, 15700, 490, 8149, 420, 490, 48380, 11, 597, 307, 490, 1319, 4309, 13, 407, 1009, 264, 8149, 6417, 2673, 341, 8470, 356, 11, 457, 291, 643, 746, 597, 486, 1401, 309, 293, 13799, 309, 666, 746, 29975, 11, 597, 486, 48377, 11, 337, 1365, 11, 264, 1412, 291, 483, 490, 264, 39911, 14747, 490, 10223, 45189, 293, 370, 322, 13, 51664], "temperature": 0.0, "avg_logprob": -0.18173921430433118, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.5022915601730347}, {"id": 64, "seek": 66900, "start": 669.0, "end": 679.0, "text": " So yeah, that was the question actually. Is there any database that does this anyway without using DBZoom? But you said, I think there is no competitor then.", "tokens": [50364, 407, 1338, 11, 300, 390, 264, 1168, 767, 13, 1119, 456, 604, 8149, 300, 775, 341, 4033, 1553, 1228, 26754, 57, 78, 298, 30, 583, 291, 848, 11, 286, 519, 456, 307, 572, 27266, 550, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1744718136994735, "compression_ratio": 1.5411764705882354, "no_speech_prob": 0.3557308316230774}, {"id": 65, "seek": 66900, "start": 679.0, "end": 682.0, "text": " Pardon? Is there any competitors?", "tokens": [50864, 32392, 30, 1119, 456, 604, 18333, 30, 51014], "temperature": 0.0, "avg_logprob": -0.1744718136994735, "compression_ratio": 1.5411764705882354, "no_speech_prob": 0.3557308316230774}, {"id": 66, "seek": 66900, "start": 682.0, "end": 689.0, "text": " Yes, like is database is doing this natively already what DBZoom does?", "tokens": [51014, 1079, 11, 411, 307, 8149, 307, 884, 341, 8470, 356, 1217, 437, 26754, 57, 78, 298, 775, 30, 51364], "temperature": 0.0, "avg_logprob": -0.1744718136994735, "compression_ratio": 1.5411764705882354, "no_speech_prob": 0.3557308316230774}, {"id": 67, "seek": 68900, "start": 689.0, "end": 704.0, "text": " I'm not aware if there is any database which uses this. I'm aware that some databases provides this, but several of them use DBZoom under the hood as far as I know.", "tokens": [50364, 286, 478, 406, 3650, 498, 456, 307, 604, 8149, 597, 4960, 341, 13, 286, 478, 3650, 300, 512, 22380, 6417, 341, 11, 457, 2940, 295, 552, 764, 26754, 57, 78, 298, 833, 264, 13376, 382, 1400, 382, 286, 458, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18092046934982825, "compression_ratio": 1.361842105263158, "no_speech_prob": 0.5657744407653809}, {"id": 68, "seek": 68900, "start": 704.0, "end": 714.0, "text": " Okay, perfect. Thank you very much, Vojta.", "tokens": [51114, 1033, 11, 2176, 13, 1044, 291, 588, 709, 11, 7518, 73, 1328, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18092046934982825, "compression_ratio": 1.361842105263158, "no_speech_prob": 0.5657744407653809}, {"id": 69, "seek": 71900, "start": 719.0, "end": 721.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.663623571395874, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.991004228591919}], "language": "en"}