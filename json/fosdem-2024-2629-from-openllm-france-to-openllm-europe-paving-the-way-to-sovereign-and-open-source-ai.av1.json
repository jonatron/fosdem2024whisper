{"text": " So thank you for your coming. It's quite early, 9.30. It's difficult to start, so I will try to push energy to this session. So just before to get started, I would like to know more about you. So with three very simple questions. First, who has ever locally run LLM on his laptop using Lama CCP, VLLM or LLM Studio? Please raise your hand. Okay, right. Second question, who has ever fine-tuned a model? Fine-tuned. Okay, let's find 10. Okay. And the last one, who has, like me, dreaming to have one-dred open-source model in here? Not only one, not only open-weight model, but really open-source models. Raise your hand. Okay, you are in the right place. So we will do the job. Okay. Yes, my name is Michel Marie Mod\u00e9s. So I have a co-founder company, a software company called Inagora. So we started in 2001. So as the first time, we will be very close to our 25 years. So it will be for next year. And our mission with Inagora is to invent and develop good tech for good. So what I can sum up as ethical open-source. And for AI, we do the same. We do ethical AI. And to come up to achieve this goal, so we started a community, a very large and a brand community called Open LLM France. So we started in June 2020. And we have two main goals. First, as well to build trusted sovereign and real open-source generative AI technologies. And the second goal is to build a strong ecosystem around LLMs and generative AI systems. So for the second objective, so I can say that we have success because the community right now is more over 450 active members with a strong support from the academic and public research in France. So it's very important because, for example, with the GenC, we can use freely supercomputers like GenZ. And it's very useful for us to give freely GPUs to train our models. So it's very important. And at the same time, so we have a lot of corporates, corporate private company, who have are using AI technology or many to build with us AI solutions. So and all this track for today. So I think there is a lot of important things to build ethical AI system. But my talk, I will talk with you about three topics as well, what we could consider as open-source AI. So this is my first part of my talk. The second part of my talk will be related to diversity and the underrepresentation of our culture, our language in these models today. And the third part of my talk this morning will be related to data quality and the evaluation of this model. Okay. Under. Okay. Right. So to be very clear, and to start on the biggest problem, so the most popular open model that you are using today are not open source. They are open wave model. So this afternoon, Stefano Maffuli from the OZ, open source initiative, we have a talk to report to their progress to this definition of what we could consider as open source AI. So I'm very proud because I'm part of this small group and private group inside the experts from external from from the OZ to try to define and to get this definition because it's important to clarify the situation because as you know, and I'm not alone, but Stefano and probably some of you's have used as a published post to raise the problem to the misuse of the open source term today by some players on the far work ecosystem. And so and I put in this slide, you know, the OZ definition of open source. So to be very clear, if you have limitation on the use, the license and the term of use of a lease license, or if you don't have the artifact, the element to train again the model or to make a derefited work on it, you can't say that you are doing open source. This is very clear. And today, the main part of the popular model we have, you don't have a view and access to the data set used to train the model. For us, for this community, what we open source AI means three things. First, as well, that we are able to have the open source of the model. All the tooling system used, for example, to train the models to evaluate the model of the pipeline to do the evaluation of the model. And so for different things, it's not very easy to find this information on an open model today. The second point is related to a license. So if you have for us our license, we don't have this license, we have to have, we thought in the limitation of who and what we are doing with this model. And the most important is the third point is related to that asset, open corpus, open corpora. But you know, it's very interesting because probably if you follow the news related to AI, you saw during these past days some new models with data sets published under open source license. So, and I think it's very important and I think that for 2020, not only the year of open source AI, but also for data set publication, open source license. So I changed my presentation last night, just after the talk of Joss, the co-founder of Next Cloud, because he present an ethical rating system. And I'm very glad to see that we share the same point of view. And it's very simple for also for the Next Cloud community. If all these conditions are met, the three conditions, so you are in the green area. If you have only one, two conditions, so you are in the yellow, only one orange. And if you are using, for example, open AI, in fact, ChabGPT from open AI, zero condition are met. So you are in the red area. So if we have today this morning developers from this beautiful Next Cloud community, thanks for your job. It's amazing and we love it. And so for us, by the way, we are in the first green area and we try to do the job. The second part, the second topic I would like to underline this morning, it's the problem that AI generative models are more and more representation of a picture of what we are in terms of culture, in terms of society, in terms of language. So I think that's figures talked by the by themselves. So in the left, you can see that since 2018, less than 8% of LLM has been created in Europe. And on the right, what you can see that it's the volume of language used to train LAMATU model. So 0.16 for French and 0.17 for German. So percent. So I don't know what do you think about that. So but in my point of view, we can say that we are not really well represented as our culture values in this model today. So we have a problem, I think. And we have a community we try to solve. So first, first try we did, it's to adopt a data first, drive an approach or quite a quality first, drive an approach. And because the small also is beautiful. And we try to get the proof that quality of the data set is more important than the quantity of data you have. And to demonstrate this this point, we release a first model in October called Claire. So Claire like the woman, the show name in France. So I'm not against I have nothing against a podcast, Albert, Alfred, Mr. But you know, we prefer in our community to promote women because by fact, it's our little contribution to have more women in our AI ecosystem and a global unity. So I will, I will not go deeply in Claire because Julie, the real one. Yes. Julie will go deep and tell you all about Claire what we did. Oh, we did this model. But just for very, very, very, we just gave the proof that it's we are able with a lot of amount of French tokens to give a very, very conversational model. Conversational means that Claire is able to understand dialogue between people with their realization. And the second part of Claire, the second features, it's that Claire is able to talk like, like you, to make a dialogue, human like dialogue with defluence, hesitation, because we train Claire with conversational data. So we continue to collect a lot of data. And today, so we are around 140 billion of token in French. So and we I'm very glad and happy to announce that we started to the training phase to train our new model called Lucy. So Lucy, the main goal of Lucy is to fix or to yes, to improve the under representation of the French language in generally in LLMs. But at the same time, we put in our data set some over European language, the German, Spanish, Italian, some code to some some some source code to make our model to have a capacity of reasoning. And we try to build some new features to make this model efficient, not only in French, but for over language. So probably you will be interesting to follow this work and probably our custom tokenizer and so on. But the most important things I would like to share with you this morning is that we are not the only one community involved in this goal to build this sovereign LLM in Europe. So I'm sure that this list is not exhaustive. If anyone knows new or other initiative, please call me just after the presentation. I will be very excited to discuss with you. But the most important is that we are strongly believe that we have all the capacity, all the technology, all the GPUs in Europe to build our models. And it's why I'm very delighted to announce you that today, during the first day, we changed OpenLLM France to become OpenLLM Europe. So you can use this QR code to inboard yourself in this in our Discord server. So we all the content we produce during the six months in French is still available, available. But we have created the channel for each European language. So please welcome. And if someone want to be part of the community management team, please contact us and we will be very pleased to inboard you in our initiative. So that's my tool for today.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.36, "text": " So thank you for your coming. It's quite early, 9.30. It's difficult to start, so I will", "tokens": [50364, 407, 1309, 291, 337, 428, 1348, 13, 467, 311, 1596, 2440, 11, 1722, 13, 3446, 13, 467, 311, 2252, 281, 722, 11, 370, 286, 486, 50982], "temperature": 0.0, "avg_logprob": -0.27477866159358494, "compression_ratio": 1.4108108108108108, "no_speech_prob": 0.4950750470161438}, {"id": 1, "seek": 0, "start": 12.36, "end": 19.080000000000002, "text": " try to push energy to this session. So just before to get started, I would like to know", "tokens": [50982, 853, 281, 2944, 2281, 281, 341, 5481, 13, 407, 445, 949, 281, 483, 1409, 11, 286, 576, 411, 281, 458, 51318], "temperature": 0.0, "avg_logprob": -0.27477866159358494, "compression_ratio": 1.4108108108108108, "no_speech_prob": 0.4950750470161438}, {"id": 2, "seek": 0, "start": 19.080000000000002, "end": 26.560000000000002, "text": " more about you. So with three very simple questions. First, who has ever locally run", "tokens": [51318, 544, 466, 291, 13, 407, 365, 1045, 588, 2199, 1651, 13, 2386, 11, 567, 575, 1562, 16143, 1190, 51692], "temperature": 0.0, "avg_logprob": -0.27477866159358494, "compression_ratio": 1.4108108108108108, "no_speech_prob": 0.4950750470161438}, {"id": 3, "seek": 2656, "start": 26.56, "end": 35.92, "text": " LLM on his laptop using Lama CCP, VLLM or LLM Studio? Please raise your hand. Okay,", "tokens": [50364, 441, 43, 44, 322, 702, 10732, 1228, 441, 2404, 27876, 11, 691, 43, 43, 44, 420, 441, 43, 44, 13500, 30, 2555, 5300, 428, 1011, 13, 1033, 11, 50832], "temperature": 0.0, "avg_logprob": -0.37602040124317954, "compression_ratio": 1.2605633802816902, "no_speech_prob": 0.15101824700832367}, {"id": 4, "seek": 2656, "start": 35.92, "end": 47.12, "text": " right. Second question, who has ever fine-tuned a model? Fine-tuned. Okay, let's find 10. Okay.", "tokens": [50832, 558, 13, 5736, 1168, 11, 567, 575, 1562, 2489, 12, 83, 43703, 257, 2316, 30, 12024, 12, 83, 43703, 13, 1033, 11, 718, 311, 915, 1266, 13, 1033, 13, 51392], "temperature": 0.0, "avg_logprob": -0.37602040124317954, "compression_ratio": 1.2605633802816902, "no_speech_prob": 0.15101824700832367}, {"id": 5, "seek": 4712, "start": 47.64, "end": 58.72, "text": " And the last one, who has, like me, dreaming to have one-dred open-source model in here? Not", "tokens": [50390, 400, 264, 1036, 472, 11, 567, 575, 11, 411, 385, 11, 21475, 281, 362, 472, 12, 67, 986, 1269, 12, 41676, 2316, 294, 510, 30, 1726, 50944], "temperature": 0.0, "avg_logprob": -0.36817673431045705, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.2611008584499359}, {"id": 6, "seek": 4712, "start": 58.72, "end": 67.08, "text": " only one, not only open-weight model, but really open-source models. Raise your hand. Okay, you are", "tokens": [50944, 787, 472, 11, 406, 787, 1269, 12, 12329, 2316, 11, 457, 534, 1269, 12, 41676, 5245, 13, 30062, 428, 1011, 13, 1033, 11, 291, 366, 51362], "temperature": 0.0, "avg_logprob": -0.36817673431045705, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.2611008584499359}, {"id": 7, "seek": 4712, "start": 67.08, "end": 76.68, "text": " in the right place. So we will do the job. Okay. Yes, my name is Michel Marie Mod\u00e9s. So I have a", "tokens": [51362, 294, 264, 558, 1081, 13, 407, 321, 486, 360, 264, 1691, 13, 1033, 13, 1079, 11, 452, 1315, 307, 23709, 15130, 6583, 2191, 13, 407, 286, 362, 257, 51842], "temperature": 0.0, "avg_logprob": -0.36817673431045705, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.2611008584499359}, {"id": 8, "seek": 7668, "start": 76.72000000000001, "end": 83.56, "text": " co-founder company, a software company called Inagora. So we started in 2001. So as the first", "tokens": [50366, 598, 12, 33348, 2237, 11, 257, 4722, 2237, 1219, 682, 559, 3252, 13, 407, 321, 1409, 294, 16382, 13, 407, 382, 264, 700, 50708], "temperature": 0.0, "avg_logprob": -0.2538736581802368, "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.2236822098493576}, {"id": 9, "seek": 7668, "start": 83.56, "end": 90.96000000000001, "text": " time, we will be very close to our 25 years. So it will be for next year. And our mission with", "tokens": [50708, 565, 11, 321, 486, 312, 588, 1998, 281, 527, 3552, 924, 13, 407, 309, 486, 312, 337, 958, 1064, 13, 400, 527, 4447, 365, 51078], "temperature": 0.0, "avg_logprob": -0.2538736581802368, "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.2236822098493576}, {"id": 10, "seek": 7668, "start": 90.96000000000001, "end": 100.56, "text": " Inagora is to invent and develop good tech for good. So what I can sum up as ethical open-source.", "tokens": [51078, 682, 559, 3252, 307, 281, 7962, 293, 1499, 665, 7553, 337, 665, 13, 407, 437, 286, 393, 2408, 493, 382, 18890, 1269, 12, 41676, 13, 51558], "temperature": 0.0, "avg_logprob": -0.2538736581802368, "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.2236822098493576}, {"id": 11, "seek": 10056, "start": 101.44, "end": 111.24000000000001, "text": " And for AI, we do the same. We do ethical AI. And to come up to achieve this goal, so we started", "tokens": [50408, 400, 337, 7318, 11, 321, 360, 264, 912, 13, 492, 360, 18890, 7318, 13, 400, 281, 808, 493, 281, 4584, 341, 3387, 11, 370, 321, 1409, 50898], "temperature": 0.0, "avg_logprob": -0.22115450638991135, "compression_ratio": 1.4568527918781726, "no_speech_prob": 0.322746604681015}, {"id": 12, "seek": 10056, "start": 111.24000000000001, "end": 118.52000000000001, "text": " a community, a very large and a brand community called Open LLM France. So we started in June", "tokens": [50898, 257, 1768, 11, 257, 588, 2416, 293, 257, 3360, 1768, 1219, 7238, 441, 43, 44, 6190, 13, 407, 321, 1409, 294, 6928, 51262], "temperature": 0.0, "avg_logprob": -0.22115450638991135, "compression_ratio": 1.4568527918781726, "no_speech_prob": 0.322746604681015}, {"id": 13, "seek": 10056, "start": 118.52000000000001, "end": 129.0, "text": " 2020. And we have two main goals. First, as well to build trusted sovereign and real open-source", "tokens": [51262, 4808, 13, 400, 321, 362, 732, 2135, 5493, 13, 2386, 11, 382, 731, 281, 1322, 16034, 28756, 293, 957, 1269, 12, 41676, 51786], "temperature": 0.0, "avg_logprob": -0.22115450638991135, "compression_ratio": 1.4568527918781726, "no_speech_prob": 0.322746604681015}, {"id": 14, "seek": 12900, "start": 129.6, "end": 138.28, "text": " generative AI technologies. And the second goal is to build a strong ecosystem around LLMs and", "tokens": [50394, 1337, 1166, 7318, 7943, 13, 400, 264, 1150, 3387, 307, 281, 1322, 257, 2068, 11311, 926, 441, 43, 26386, 293, 50828], "temperature": 0.0, "avg_logprob": -0.18450680646029385, "compression_ratio": 1.5157894736842106, "no_speech_prob": 0.0466509647667408}, {"id": 15, "seek": 12900, "start": 138.28, "end": 147.52, "text": " generative AI systems. So for the second objective, so I can say that we have success because the", "tokens": [50828, 1337, 1166, 7318, 3652, 13, 407, 337, 264, 1150, 10024, 11, 370, 286, 393, 584, 300, 321, 362, 2245, 570, 264, 51290], "temperature": 0.0, "avg_logprob": -0.18450680646029385, "compression_ratio": 1.5157894736842106, "no_speech_prob": 0.0466509647667408}, {"id": 16, "seek": 12900, "start": 147.52, "end": 157.72, "text": " community right now is more over 450 active members with a strong support from the academic and", "tokens": [51290, 1768, 558, 586, 307, 544, 670, 26034, 4967, 2679, 365, 257, 2068, 1406, 490, 264, 7778, 293, 51800], "temperature": 0.0, "avg_logprob": -0.18450680646029385, "compression_ratio": 1.5157894736842106, "no_speech_prob": 0.0466509647667408}, {"id": 17, "seek": 15772, "start": 157.8, "end": 165.52, "text": " public research in France. So it's very important because, for example, with the GenC, we can use", "tokens": [50368, 1908, 2132, 294, 6190, 13, 407, 309, 311, 588, 1021, 570, 11, 337, 1365, 11, 365, 264, 3632, 34, 11, 321, 393, 764, 50754], "temperature": 0.0, "avg_logprob": -0.2369713416466346, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.19525261223316193}, {"id": 18, "seek": 15772, "start": 165.52, "end": 177.24, "text": " freely supercomputers like GenZ. And it's very useful for us to give freely GPUs to train our", "tokens": [50754, 16433, 27839, 2582, 433, 411, 3632, 57, 13, 400, 309, 311, 588, 4420, 337, 505, 281, 976, 16433, 18407, 82, 281, 3847, 527, 51340], "temperature": 0.0, "avg_logprob": -0.2369713416466346, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.19525261223316193}, {"id": 19, "seek": 15772, "start": 177.24, "end": 184.36, "text": " models. So it's very important. And at the same time, so we have a lot of corporates, corporate", "tokens": [51340, 5245, 13, 407, 309, 311, 588, 1021, 13, 400, 412, 264, 912, 565, 11, 370, 321, 362, 257, 688, 295, 6804, 1024, 11, 10896, 51696], "temperature": 0.0, "avg_logprob": -0.2369713416466346, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.19525261223316193}, {"id": 20, "seek": 18436, "start": 184.44000000000003, "end": 196.84, "text": " private company, who have are using AI technology or many to build with us AI solutions. So and all", "tokens": [50368, 4551, 2237, 11, 567, 362, 366, 1228, 7318, 2899, 420, 867, 281, 1322, 365, 505, 7318, 6547, 13, 407, 293, 439, 50988], "temperature": 0.0, "avg_logprob": -0.2372069609792609, "compression_ratio": 1.5024875621890548, "no_speech_prob": 0.044403593987226486}, {"id": 21, "seek": 18436, "start": 196.84, "end": 204.48000000000002, "text": " this track for today. So I think there is a lot of important things to build ethical AI system. But", "tokens": [50988, 341, 2837, 337, 965, 13, 407, 286, 519, 456, 307, 257, 688, 295, 1021, 721, 281, 1322, 18890, 7318, 1185, 13, 583, 51370], "temperature": 0.0, "avg_logprob": -0.2372069609792609, "compression_ratio": 1.5024875621890548, "no_speech_prob": 0.044403593987226486}, {"id": 22, "seek": 18436, "start": 204.48000000000002, "end": 212.92000000000002, "text": " my talk, I will talk with you about three topics as well, what we could consider as open-source AI. So", "tokens": [51370, 452, 751, 11, 286, 486, 751, 365, 291, 466, 1045, 8378, 382, 731, 11, 437, 321, 727, 1949, 382, 1269, 12, 41676, 7318, 13, 407, 51792], "temperature": 0.0, "avg_logprob": -0.2372069609792609, "compression_ratio": 1.5024875621890548, "no_speech_prob": 0.044403593987226486}, {"id": 23, "seek": 21292, "start": 212.95999999999998, "end": 221.83999999999997, "text": " this is my first part of my talk. The second part of my talk will be related to diversity and the", "tokens": [50366, 341, 307, 452, 700, 644, 295, 452, 751, 13, 440, 1150, 644, 295, 452, 751, 486, 312, 4077, 281, 8811, 293, 264, 50810], "temperature": 0.0, "avg_logprob": -0.19146386314840877, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.03001367673277855}, {"id": 24, "seek": 21292, "start": 221.83999999999997, "end": 229.72, "text": " underrepresentation of our culture, our language in these models today. And the third part of my", "tokens": [50810, 833, 19919, 11662, 399, 295, 527, 3713, 11, 527, 2856, 294, 613, 5245, 965, 13, 400, 264, 2636, 644, 295, 452, 51204], "temperature": 0.0, "avg_logprob": -0.19146386314840877, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.03001367673277855}, {"id": 25, "seek": 21292, "start": 230.72, "end": 236.79999999999998, "text": " talk this morning will be related to data quality and the evaluation of this model. Okay.", "tokens": [51254, 751, 341, 2446, 486, 312, 4077, 281, 1412, 3125, 293, 264, 13344, 295, 341, 2316, 13, 1033, 13, 51558], "temperature": 0.0, "avg_logprob": -0.19146386314840877, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.03001367673277855}, {"id": 26, "seek": 24292, "start": 243.83999999999997, "end": 254.83999999999997, "text": " Under. Okay. Right. So to be very clear, and to start on the biggest problem, so the most popular", "tokens": [50410, 6974, 13, 1033, 13, 1779, 13, 407, 281, 312, 588, 1850, 11, 293, 281, 722, 322, 264, 3880, 1154, 11, 370, 264, 881, 3743, 50960], "temperature": 0.0, "avg_logprob": -0.27393430929917556, "compression_ratio": 1.4214285714285715, "no_speech_prob": 0.04597509279847145}, {"id": 27, "seek": 24292, "start": 254.92, "end": 263.2, "text": " open model that you are using today are not open source. They are open wave model. So this afternoon,", "tokens": [50964, 1269, 2316, 300, 291, 366, 1228, 965, 366, 406, 1269, 4009, 13, 814, 366, 1269, 5772, 2316, 13, 407, 341, 6499, 11, 51378], "temperature": 0.0, "avg_logprob": -0.27393430929917556, "compression_ratio": 1.4214285714285715, "no_speech_prob": 0.04597509279847145}, {"id": 28, "seek": 26320, "start": 263.68, "end": 273.2, "text": " Stefano Maffuli from the OZ, open source initiative, we have a talk to report to their progress to", "tokens": [50388, 43421, 3730, 376, 2518, 25484, 490, 264, 422, 57, 11, 1269, 4009, 11552, 11, 321, 362, 257, 751, 281, 2275, 281, 641, 4205, 281, 50864], "temperature": 0.0, "avg_logprob": -0.3151092529296875, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.23192280530929565}, {"id": 29, "seek": 26320, "start": 273.2, "end": 280.36, "text": " this definition of what we could consider as open source AI. So I'm very proud because I'm part of", "tokens": [50864, 341, 7123, 295, 437, 321, 727, 1949, 382, 1269, 4009, 7318, 13, 407, 286, 478, 588, 4570, 570, 286, 478, 644, 295, 51222], "temperature": 0.0, "avg_logprob": -0.3151092529296875, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.23192280530929565}, {"id": 30, "seek": 26320, "start": 280.36, "end": 289.36, "text": " this small group and private group inside the experts from external from from the OZ to try to", "tokens": [51222, 341, 1359, 1594, 293, 4551, 1594, 1854, 264, 8572, 490, 8320, 490, 490, 264, 422, 57, 281, 853, 281, 51672], "temperature": 0.0, "avg_logprob": -0.3151092529296875, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.23192280530929565}, {"id": 31, "seek": 28936, "start": 289.36, "end": 296.04, "text": " define and to get this definition because it's important to clarify the situation because as you", "tokens": [50364, 6964, 293, 281, 483, 341, 7123, 570, 309, 311, 1021, 281, 17594, 264, 2590, 570, 382, 291, 50698], "temperature": 0.0, "avg_logprob": -0.33955643142479053, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.1804022490978241}, {"id": 32, "seek": 28936, "start": 296.04, "end": 307.36, "text": " know, and I'm not alone, but Stefano and probably some of you's have used as a published post to", "tokens": [50698, 458, 11, 293, 286, 478, 406, 3312, 11, 457, 43421, 3730, 293, 1391, 512, 295, 291, 311, 362, 1143, 382, 257, 6572, 2183, 281, 51264], "temperature": 0.0, "avg_logprob": -0.33955643142479053, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.1804022490978241}, {"id": 33, "seek": 28936, "start": 307.8, "end": 315.40000000000003, "text": " raise the problem to the misuse of the open source term today by some players on the far work", "tokens": [51286, 5300, 264, 1154, 281, 264, 3346, 438, 295, 264, 1269, 4009, 1433, 965, 538, 512, 4150, 322, 264, 1400, 589, 51666], "temperature": 0.0, "avg_logprob": -0.33955643142479053, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.1804022490978241}, {"id": 34, "seek": 31540, "start": 315.64, "end": 324.96, "text": " ecosystem. And so and I put in this slide, you know, the OZ definition of open source. So to be very", "tokens": [50376, 11311, 13, 400, 370, 293, 286, 829, 294, 341, 4137, 11, 291, 458, 11, 264, 422, 57, 7123, 295, 1269, 4009, 13, 407, 281, 312, 588, 50842], "temperature": 0.0, "avg_logprob": -0.24810235294294947, "compression_ratio": 1.546875, "no_speech_prob": 0.15153510868549347}, {"id": 35, "seek": 31540, "start": 324.96, "end": 333.32, "text": " clear, if you have limitation on the use, the license and the term of use of a lease license, or", "tokens": [50842, 1850, 11, 498, 291, 362, 27432, 322, 264, 764, 11, 264, 10476, 293, 264, 1433, 295, 764, 295, 257, 24961, 10476, 11, 420, 51260], "temperature": 0.0, "avg_logprob": -0.24810235294294947, "compression_ratio": 1.546875, "no_speech_prob": 0.15153510868549347}, {"id": 36, "seek": 31540, "start": 333.32, "end": 343.64, "text": " if you don't have the artifact, the element to train again the model or to make a derefited work on", "tokens": [51260, 498, 291, 500, 380, 362, 264, 34806, 11, 264, 4478, 281, 3847, 797, 264, 2316, 420, 281, 652, 257, 15969, 69, 1226, 589, 322, 51776], "temperature": 0.0, "avg_logprob": -0.24810235294294947, "compression_ratio": 1.546875, "no_speech_prob": 0.15153510868549347}, {"id": 37, "seek": 34364, "start": 343.68, "end": 353.08, "text": " it, you can't say that you are doing open source. This is very clear. And today, the main part of the", "tokens": [50366, 309, 11, 291, 393, 380, 584, 300, 291, 366, 884, 1269, 4009, 13, 639, 307, 588, 1850, 13, 400, 965, 11, 264, 2135, 644, 295, 264, 50836], "temperature": 0.0, "avg_logprob": -0.14538283406952282, "compression_ratio": 1.5153061224489797, "no_speech_prob": 0.07816983759403229}, {"id": 38, "seek": 34364, "start": 353.08, "end": 359.71999999999997, "text": " popular model we have, you don't have a view and access to the data set used to train the model.", "tokens": [50836, 3743, 2316, 321, 362, 11, 291, 500, 380, 362, 257, 1910, 293, 2105, 281, 264, 1412, 992, 1143, 281, 3847, 264, 2316, 13, 51168], "temperature": 0.0, "avg_logprob": -0.14538283406952282, "compression_ratio": 1.5153061224489797, "no_speech_prob": 0.07816983759403229}, {"id": 39, "seek": 34364, "start": 362.96, "end": 373.59999999999997, "text": " For us, for this community, what we open source AI means three things. First, as well, that we are", "tokens": [51330, 1171, 505, 11, 337, 341, 1768, 11, 437, 321, 1269, 4009, 7318, 1355, 1045, 721, 13, 2386, 11, 382, 731, 11, 300, 321, 366, 51862], "temperature": 0.0, "avg_logprob": -0.14538283406952282, "compression_ratio": 1.5153061224489797, "no_speech_prob": 0.07816983759403229}, {"id": 40, "seek": 37360, "start": 373.6, "end": 382.0, "text": " able to have the open source of the model. All the tooling system used, for example, to train the", "tokens": [50364, 1075, 281, 362, 264, 1269, 4009, 295, 264, 2316, 13, 1057, 264, 46593, 1185, 1143, 11, 337, 1365, 11, 281, 3847, 264, 50784], "temperature": 0.0, "avg_logprob": -0.19480086697472465, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.029815368354320526}, {"id": 41, "seek": 37360, "start": 382.0, "end": 389.44, "text": " models to evaluate the model of the pipeline to do the evaluation of the model. And so for different", "tokens": [50784, 5245, 281, 13059, 264, 2316, 295, 264, 15517, 281, 360, 264, 13344, 295, 264, 2316, 13, 400, 370, 337, 819, 51156], "temperature": 0.0, "avg_logprob": -0.19480086697472465, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.029815368354320526}, {"id": 42, "seek": 37360, "start": 389.44, "end": 397.96000000000004, "text": " things, it's not very easy to find this information on an open model today. The second point is related", "tokens": [51156, 721, 11, 309, 311, 406, 588, 1858, 281, 915, 341, 1589, 322, 364, 1269, 2316, 965, 13, 440, 1150, 935, 307, 4077, 51582], "temperature": 0.0, "avg_logprob": -0.19480086697472465, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.029815368354320526}, {"id": 43, "seek": 39796, "start": 398.03999999999996, "end": 410.96, "text": " to a license. So if you have for us our license, we don't have this license, we have to have, we thought", "tokens": [50368, 281, 257, 10476, 13, 407, 498, 291, 362, 337, 505, 527, 10476, 11, 321, 500, 380, 362, 341, 10476, 11, 321, 362, 281, 362, 11, 321, 1194, 51014], "temperature": 0.0, "avg_logprob": -0.28070592880249023, "compression_ratio": 1.6157894736842104, "no_speech_prob": 0.1428697407245636}, {"id": 44, "seek": 39796, "start": 410.96, "end": 418.52, "text": " in the limitation of who and what we are doing with this model. And the most important is the third", "tokens": [51014, 294, 264, 27432, 295, 567, 293, 437, 321, 366, 884, 365, 341, 2316, 13, 400, 264, 881, 1021, 307, 264, 2636, 51392], "temperature": 0.0, "avg_logprob": -0.28070592880249023, "compression_ratio": 1.6157894736842104, "no_speech_prob": 0.1428697407245636}, {"id": 45, "seek": 39796, "start": 418.52, "end": 425.59999999999997, "text": " point is related to that asset, open corpus, open corpora. But you know, it's very interesting because", "tokens": [51392, 935, 307, 4077, 281, 300, 11999, 11, 1269, 1181, 31624, 11, 1269, 6804, 64, 13, 583, 291, 458, 11, 309, 311, 588, 1880, 570, 51746], "temperature": 0.0, "avg_logprob": -0.28070592880249023, "compression_ratio": 1.6157894736842104, "no_speech_prob": 0.1428697407245636}, {"id": 46, "seek": 42560, "start": 426.0, "end": 436.20000000000005, "text": " probably if you follow the news related to AI, you saw during these past days some new models with", "tokens": [50384, 1391, 498, 291, 1524, 264, 2583, 4077, 281, 7318, 11, 291, 1866, 1830, 613, 1791, 1708, 512, 777, 5245, 365, 50894], "temperature": 0.0, "avg_logprob": -0.2451054981776646, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.05902125686407089}, {"id": 47, "seek": 42560, "start": 436.52000000000004, "end": 444.56, "text": " data sets published under open source license. So, and I think it's very important and I think that", "tokens": [50910, 1412, 6352, 6572, 833, 1269, 4009, 10476, 13, 407, 11, 293, 286, 519, 309, 311, 588, 1021, 293, 286, 519, 300, 51312], "temperature": 0.0, "avg_logprob": -0.2451054981776646, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.05902125686407089}, {"id": 48, "seek": 42560, "start": 444.56, "end": 454.84000000000003, "text": " for 2020, not only the year of open source AI, but also for data set publication, open source", "tokens": [51312, 337, 4808, 11, 406, 787, 264, 1064, 295, 1269, 4009, 7318, 11, 457, 611, 337, 1412, 992, 19953, 11, 1269, 4009, 51826], "temperature": 0.0, "avg_logprob": -0.2451054981776646, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.05902125686407089}, {"id": 49, "seek": 45484, "start": 454.88, "end": 466.91999999999996, "text": " license. So I changed my presentation last night, just after the talk of Joss, the co-founder of Next", "tokens": [50366, 10476, 13, 407, 286, 3105, 452, 5860, 1036, 1818, 11, 445, 934, 264, 751, 295, 508, 772, 11, 264, 598, 12, 33348, 295, 3087, 50968], "temperature": 0.0, "avg_logprob": -0.2396901508547225, "compression_ratio": 1.335483870967742, "no_speech_prob": 0.041736144572496414}, {"id": 50, "seek": 45484, "start": 466.91999999999996, "end": 475.35999999999996, "text": " Cloud, because he present an ethical rating system. And I'm very glad to see that we share the same point", "tokens": [50968, 8061, 11, 570, 415, 1974, 364, 18890, 10990, 1185, 13, 400, 286, 478, 588, 5404, 281, 536, 300, 321, 2073, 264, 912, 935, 51390], "temperature": 0.0, "avg_logprob": -0.2396901508547225, "compression_ratio": 1.335483870967742, "no_speech_prob": 0.041736144572496414}, {"id": 51, "seek": 47536, "start": 475.40000000000003, "end": 485.04, "text": " of view. And it's very simple for also for the Next Cloud community. If all these conditions are met,", "tokens": [50366, 295, 1910, 13, 400, 309, 311, 588, 2199, 337, 611, 337, 264, 3087, 8061, 1768, 13, 759, 439, 613, 4487, 366, 1131, 11, 50848], "temperature": 0.0, "avg_logprob": -0.31362928662981304, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.6257262825965881}, {"id": 52, "seek": 47536, "start": 485.40000000000003, "end": 493.84000000000003, "text": " the three conditions, so you are in the green area. If you have only one, two conditions, so you are", "tokens": [50866, 264, 1045, 4487, 11, 370, 291, 366, 294, 264, 3092, 1859, 13, 759, 291, 362, 787, 472, 11, 732, 4487, 11, 370, 291, 366, 51288], "temperature": 0.0, "avg_logprob": -0.31362928662981304, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.6257262825965881}, {"id": 53, "seek": 47536, "start": 493.84000000000003, "end": 501.88, "text": " in the yellow, only one orange. And if you are using, for example, open AI, in fact, ChabGPT from", "tokens": [51288, 294, 264, 5566, 11, 787, 472, 7671, 13, 400, 498, 291, 366, 1228, 11, 337, 1365, 11, 1269, 7318, 11, 294, 1186, 11, 761, 455, 38, 47, 51, 490, 51690], "temperature": 0.0, "avg_logprob": -0.31362928662981304, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.6257262825965881}, {"id": 54, "seek": 50188, "start": 501.92, "end": 510.4, "text": " open AI, zero condition are met. So you are in the red area. So if we have today this morning", "tokens": [50366, 1269, 7318, 11, 4018, 4188, 366, 1131, 13, 407, 291, 366, 294, 264, 2182, 1859, 13, 407, 498, 321, 362, 965, 341, 2446, 50790], "temperature": 0.0, "avg_logprob": -0.18279126795326792, "compression_ratio": 1.54, "no_speech_prob": 0.12344148755073547}, {"id": 55, "seek": 50188, "start": 510.84, "end": 519.0, "text": " developers from this beautiful Next Cloud community, thanks for your job. It's amazing and we love it. And so", "tokens": [50812, 8849, 490, 341, 2238, 3087, 8061, 1768, 11, 3231, 337, 428, 1691, 13, 467, 311, 2243, 293, 321, 959, 309, 13, 400, 370, 51220], "temperature": 0.0, "avg_logprob": -0.18279126795326792, "compression_ratio": 1.54, "no_speech_prob": 0.12344148755073547}, {"id": 56, "seek": 50188, "start": 519.0, "end": 529.12, "text": " for us, by the way, we are in the first green area and we try to do the job. The second part, the second", "tokens": [51220, 337, 505, 11, 538, 264, 636, 11, 321, 366, 294, 264, 700, 3092, 1859, 293, 321, 853, 281, 360, 264, 1691, 13, 440, 1150, 644, 11, 264, 1150, 51726], "temperature": 0.0, "avg_logprob": -0.18279126795326792, "compression_ratio": 1.54, "no_speech_prob": 0.12344148755073547}, {"id": 57, "seek": 52912, "start": 529.16, "end": 537.04, "text": " topic I would like to underline this morning, it's the problem that AI generative models are more and more", "tokens": [50366, 4829, 286, 576, 411, 281, 833, 1889, 341, 2446, 11, 309, 311, 264, 1154, 300, 7318, 1337, 1166, 5245, 366, 544, 293, 544, 50760], "temperature": 0.0, "avg_logprob": -0.22095079421997071, "compression_ratio": 1.575609756097561, "no_speech_prob": 0.1278076469898224}, {"id": 58, "seek": 52912, "start": 537.64, "end": 546.5600000000001, "text": " representation of a picture of what we are in terms of culture, in terms of society, in terms of language. So I", "tokens": [50790, 10290, 295, 257, 3036, 295, 437, 321, 366, 294, 2115, 295, 3713, 11, 294, 2115, 295, 4086, 11, 294, 2115, 295, 2856, 13, 407, 286, 51236], "temperature": 0.0, "avg_logprob": -0.22095079421997071, "compression_ratio": 1.575609756097561, "no_speech_prob": 0.1278076469898224}, {"id": 59, "seek": 52912, "start": 546.5600000000001, "end": 555.84, "text": " think that's figures talked by the by themselves. So in the left, you can see that since 2018, less than", "tokens": [51236, 519, 300, 311, 9624, 2825, 538, 264, 538, 2969, 13, 407, 294, 264, 1411, 11, 291, 393, 536, 300, 1670, 6096, 11, 1570, 813, 51700], "temperature": 0.0, "avg_logprob": -0.22095079421997071, "compression_ratio": 1.575609756097561, "no_speech_prob": 0.1278076469898224}, {"id": 60, "seek": 55584, "start": 556.12, "end": 567.64, "text": " 8% of LLM has been created in Europe. And on the right, what you can see that it's the volume of", "tokens": [50378, 1649, 4, 295, 441, 43, 44, 575, 668, 2942, 294, 3315, 13, 400, 322, 264, 558, 11, 437, 291, 393, 536, 300, 309, 311, 264, 5523, 295, 50954], "temperature": 0.0, "avg_logprob": -0.17023013886951266, "compression_ratio": 1.264516129032258, "no_speech_prob": 0.1473543792963028}, {"id": 61, "seek": 55584, "start": 567.64, "end": 583.52, "text": " language used to train LAMATU model. So 0.16 for French and 0.17 for German. So percent. So I don't", "tokens": [50954, 2856, 1143, 281, 3847, 441, 2865, 2218, 52, 2316, 13, 407, 1958, 13, 6866, 337, 5522, 293, 1958, 13, 7773, 337, 6521, 13, 407, 3043, 13, 407, 286, 500, 380, 51748], "temperature": 0.0, "avg_logprob": -0.17023013886951266, "compression_ratio": 1.264516129032258, "no_speech_prob": 0.1473543792963028}, {"id": 62, "seek": 58352, "start": 583.56, "end": 589.4399999999999, "text": " know what do you think about that. So but in my point of view, we can say that we are not really well", "tokens": [50366, 458, 437, 360, 291, 519, 466, 300, 13, 407, 457, 294, 452, 935, 295, 1910, 11, 321, 393, 584, 300, 321, 366, 406, 534, 731, 50660], "temperature": 0.0, "avg_logprob": -0.17633484028003835, "compression_ratio": 1.3958333333333333, "no_speech_prob": 0.0859202966094017}, {"id": 63, "seek": 58352, "start": 589.4399999999999, "end": 603.24, "text": " represented as our culture values in this model today. So we have a problem, I think. And we have a", "tokens": [50660, 10379, 382, 527, 3713, 4190, 294, 341, 2316, 965, 13, 407, 321, 362, 257, 1154, 11, 286, 519, 13, 400, 321, 362, 257, 51350], "temperature": 0.0, "avg_logprob": -0.17633484028003835, "compression_ratio": 1.3958333333333333, "no_speech_prob": 0.0859202966094017}, {"id": 64, "seek": 60324, "start": 603.24, "end": 617.92, "text": " community we try to solve. So first, first try we did, it's to adopt a data first, drive an approach or", "tokens": [50364, 1768, 321, 853, 281, 5039, 13, 407, 700, 11, 700, 853, 321, 630, 11, 309, 311, 281, 6878, 257, 1412, 700, 11, 3332, 364, 3109, 420, 51098], "temperature": 0.0, "avg_logprob": -0.33967930186878553, "compression_ratio": 1.5407407407407407, "no_speech_prob": 0.25127866864204407}, {"id": 65, "seek": 60324, "start": 617.92, "end": 626.2, "text": " quite a quality first, drive an approach. And because the small also is beautiful. And we try to get the", "tokens": [51098, 1596, 257, 3125, 700, 11, 3332, 364, 3109, 13, 400, 570, 264, 1359, 611, 307, 2238, 13, 400, 321, 853, 281, 483, 264, 51512], "temperature": 0.0, "avg_logprob": -0.33967930186878553, "compression_ratio": 1.5407407407407407, "no_speech_prob": 0.25127866864204407}, {"id": 66, "seek": 62620, "start": 626.2, "end": 635.9200000000001, "text": " proof that quality of the data set is more important than the quantity of data you have. And to demonstrate", "tokens": [50364, 8177, 300, 3125, 295, 264, 1412, 992, 307, 544, 1021, 813, 264, 11275, 295, 1412, 291, 362, 13, 400, 281, 11698, 50850], "temperature": 0.0, "avg_logprob": -0.4207547903060913, "compression_ratio": 1.4335664335664335, "no_speech_prob": 0.5314913988113403}, {"id": 67, "seek": 62620, "start": 635.96, "end": 648.44, "text": " this this point, we release a first model in October called Claire. So Claire like the woman, the", "tokens": [50852, 341, 341, 935, 11, 321, 4374, 257, 700, 2316, 294, 7617, 1219, 22605, 13, 407, 22605, 411, 264, 3059, 11, 264, 51476], "temperature": 0.0, "avg_logprob": -0.4207547903060913, "compression_ratio": 1.4335664335664335, "no_speech_prob": 0.5314913988113403}, {"id": 68, "seek": 64844, "start": 648.6, "end": 659.2, "text": " show name in France. So I'm not against I have nothing against a podcast, Albert, Alfred, Mr. But you", "tokens": [50372, 855, 1315, 294, 6190, 13, 407, 286, 478, 406, 1970, 286, 362, 1825, 1970, 257, 7367, 11, 20812, 11, 28327, 11, 2221, 13, 583, 291, 50902], "temperature": 0.0, "avg_logprob": -0.3635909682825992, "compression_ratio": 1.5148514851485149, "no_speech_prob": 0.4479566812515259}, {"id": 69, "seek": 64844, "start": 659.2, "end": 668.1600000000001, "text": " know, we prefer in our community to promote women because by fact, it's our little contribution to have", "tokens": [50902, 458, 11, 321, 4382, 294, 527, 1768, 281, 9773, 2266, 570, 538, 1186, 11, 309, 311, 527, 707, 13150, 281, 362, 51350], "temperature": 0.0, "avg_logprob": -0.3635909682825992, "compression_ratio": 1.5148514851485149, "no_speech_prob": 0.4479566812515259}, {"id": 70, "seek": 64844, "start": 668.1600000000001, "end": 678.12, "text": " more women in our AI ecosystem and a global unity. So I will, I will not go deeply in Claire because", "tokens": [51350, 544, 2266, 294, 527, 7318, 11311, 293, 257, 4338, 18205, 13, 407, 286, 486, 11, 286, 486, 406, 352, 8760, 294, 22605, 570, 51848], "temperature": 0.0, "avg_logprob": -0.3635909682825992, "compression_ratio": 1.5148514851485149, "no_speech_prob": 0.4479566812515259}, {"id": 71, "seek": 67844, "start": 679.32, "end": 691.44, "text": " Julie, the real one. Yes. Julie will go deep and tell you all about Claire what we did. Oh, we did this", "tokens": [50408, 18794, 11, 264, 957, 472, 13, 1079, 13, 18794, 486, 352, 2452, 293, 980, 291, 439, 466, 22605, 437, 321, 630, 13, 876, 11, 321, 630, 341, 51014], "temperature": 0.0, "avg_logprob": -0.38175861171034514, "compression_ratio": 1.4189189189189189, "no_speech_prob": 0.08912282437086105}, {"id": 72, "seek": 67844, "start": 691.44, "end": 701.6800000000001, "text": " model. But just for very, very, very, we just gave the proof that it's we are able with a lot of amount of", "tokens": [51014, 2316, 13, 583, 445, 337, 588, 11, 588, 11, 588, 11, 321, 445, 2729, 264, 8177, 300, 309, 311, 321, 366, 1075, 365, 257, 688, 295, 2372, 295, 51526], "temperature": 0.0, "avg_logprob": -0.38175861171034514, "compression_ratio": 1.4189189189189189, "no_speech_prob": 0.08912282437086105}, {"id": 73, "seek": 70168, "start": 701.68, "end": 712.0, "text": " French tokens to give a very, very conversational model. Conversational means that Claire is able to", "tokens": [50364, 5522, 22667, 281, 976, 257, 588, 11, 588, 2615, 1478, 2316, 13, 33247, 1478, 1355, 300, 22605, 307, 1075, 281, 50880], "temperature": 0.0, "avg_logprob": -0.27290208788885584, "compression_ratio": 1.6850828729281768, "no_speech_prob": 0.30400151014328003}, {"id": 74, "seek": 70168, "start": 712.0, "end": 719.88, "text": " understand dialogue between people with their realization. And the second part of Claire, the second", "tokens": [50880, 1223, 10221, 1296, 561, 365, 641, 25138, 13, 400, 264, 1150, 644, 295, 22605, 11, 264, 1150, 51274], "temperature": 0.0, "avg_logprob": -0.27290208788885584, "compression_ratio": 1.6850828729281768, "no_speech_prob": 0.30400151014328003}, {"id": 75, "seek": 70168, "start": 719.88, "end": 729.4, "text": " features, it's that Claire is able to talk like, like you, to make a dialogue, human like dialogue with", "tokens": [51274, 4122, 11, 309, 311, 300, 22605, 307, 1075, 281, 751, 411, 11, 411, 291, 11, 281, 652, 257, 10221, 11, 1952, 411, 10221, 365, 51750], "temperature": 0.0, "avg_logprob": -0.27290208788885584, "compression_ratio": 1.6850828729281768, "no_speech_prob": 0.30400151014328003}, {"id": 76, "seek": 72940, "start": 730.04, "end": 740.68, "text": " defluence, hesitation, because we train Claire with conversational data. So we continue to collect a lot of", "tokens": [50396, 1060, 40432, 11, 36125, 11, 570, 321, 3847, 22605, 365, 2615, 1478, 1412, 13, 407, 321, 2354, 281, 2500, 257, 688, 295, 50928], "temperature": 0.0, "avg_logprob": -0.14009941065752948, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.03861318528652191}, {"id": 77, "seek": 72940, "start": 740.68, "end": 753.56, "text": " data. And today, so we are around 140 billion of token in French. So and we I'm very glad and happy to", "tokens": [50928, 1412, 13, 400, 965, 11, 370, 321, 366, 926, 21548, 5218, 295, 14862, 294, 5522, 13, 407, 293, 321, 286, 478, 588, 5404, 293, 2055, 281, 51572], "temperature": 0.0, "avg_logprob": -0.14009941065752948, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.03861318528652191}, {"id": 78, "seek": 75356, "start": 753.68, "end": 766.8399999999999, "text": " announce that we started to the training phase to train our new model called Lucy. So Lucy, the main goal of", "tokens": [50370, 7478, 300, 321, 1409, 281, 264, 3097, 5574, 281, 3847, 527, 777, 2316, 1219, 22698, 13, 407, 22698, 11, 264, 2135, 3387, 295, 51028], "temperature": 0.0, "avg_logprob": -0.26515043998251153, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.23663385212421417}, {"id": 79, "seek": 75356, "start": 766.88, "end": 777.52, "text": " Lucy is to fix or to yes, to improve the under representation of the French language in generally in", "tokens": [51030, 22698, 307, 281, 3191, 420, 281, 2086, 11, 281, 3470, 264, 833, 10290, 295, 264, 5522, 2856, 294, 5101, 294, 51562], "temperature": 0.0, "avg_logprob": -0.26515043998251153, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.23663385212421417}, {"id": 80, "seek": 77752, "start": 777.52, "end": 788.4399999999999, "text": " LLMs. But at the same time, we put in our data set some over European language, the German, Spanish, Italian,", "tokens": [50364, 441, 43, 26386, 13, 583, 412, 264, 912, 565, 11, 321, 829, 294, 527, 1412, 992, 512, 670, 6473, 2856, 11, 264, 6521, 11, 8058, 11, 10003, 11, 50910], "temperature": 0.0, "avg_logprob": -0.3279533720853036, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.14909951388835907}, {"id": 81, "seek": 77752, "start": 789.24, "end": 801.64, "text": " some code to some some some source code to make our model to have a capacity of reasoning. And we try to", "tokens": [50950, 512, 3089, 281, 512, 512, 512, 4009, 3089, 281, 652, 527, 2316, 281, 362, 257, 6042, 295, 21577, 13, 400, 321, 853, 281, 51570], "temperature": 0.0, "avg_logprob": -0.3279533720853036, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.14909951388835907}, {"id": 82, "seek": 80164, "start": 801.76, "end": 814.96, "text": " build some new features to make this model efficient, not only in French, but for over language. So probably you", "tokens": [50370, 1322, 512, 777, 4122, 281, 652, 341, 2316, 7148, 11, 406, 787, 294, 5522, 11, 457, 337, 670, 2856, 13, 407, 1391, 291, 51030], "temperature": 0.0, "avg_logprob": -0.26798257827758787, "compression_ratio": 1.3924050632911393, "no_speech_prob": 0.1262068748474121}, {"id": 83, "seek": 80164, "start": 814.96, "end": 825.12, "text": " will be interesting to follow this work and probably our custom tokenizer and so on. But the most important", "tokens": [51030, 486, 312, 1880, 281, 1524, 341, 589, 293, 1391, 527, 2375, 14862, 6545, 293, 370, 322, 13, 583, 264, 881, 1021, 51538], "temperature": 0.0, "avg_logprob": -0.26798257827758787, "compression_ratio": 1.3924050632911393, "no_speech_prob": 0.1262068748474121}, {"id": 84, "seek": 82512, "start": 825.2, "end": 831.5600000000001, "text": " things I would like to share with you this morning is that we are not the only one community involved in this", "tokens": [50368, 721, 286, 576, 411, 281, 2073, 365, 291, 341, 2446, 307, 300, 321, 366, 406, 264, 787, 472, 1768, 3288, 294, 341, 50686], "temperature": 0.0, "avg_logprob": -0.20438619896217627, "compression_ratio": 1.396103896103896, "no_speech_prob": 0.5173910856246948}, {"id": 85, "seek": 82512, "start": 832.36, "end": 846.84, "text": " goal to build this sovereign LLM in Europe. So I'm sure that this list is not exhaustive. If anyone knows", "tokens": [50726, 3387, 281, 1322, 341, 28756, 441, 43, 44, 294, 3315, 13, 407, 286, 478, 988, 300, 341, 1329, 307, 406, 14687, 488, 13, 759, 2878, 3255, 51450], "temperature": 0.0, "avg_logprob": -0.20438619896217627, "compression_ratio": 1.396103896103896, "no_speech_prob": 0.5173910856246948}, {"id": 86, "seek": 84684, "start": 847.52, "end": 856.48, "text": " new or other initiative, please call me just after the presentation. I will be very excited to discuss with you. But the", "tokens": [50398, 777, 420, 661, 11552, 11, 1767, 818, 385, 445, 934, 264, 5860, 13, 286, 486, 312, 588, 2919, 281, 2248, 365, 291, 13, 583, 264, 50846], "temperature": 0.0, "avg_logprob": -0.18104204698042436, "compression_ratio": 1.4625, "no_speech_prob": 0.36438703536987305}, {"id": 87, "seek": 84684, "start": 856.48, "end": 867.24, "text": " most important is that we are strongly believe that we have all the capacity, all the technology, all the GPUs in", "tokens": [50846, 881, 1021, 307, 300, 321, 366, 10613, 1697, 300, 321, 362, 439, 264, 6042, 11, 439, 264, 2899, 11, 439, 264, 18407, 82, 294, 51384], "temperature": 0.0, "avg_logprob": -0.18104204698042436, "compression_ratio": 1.4625, "no_speech_prob": 0.36438703536987305}, {"id": 88, "seek": 86724, "start": 867.24, "end": 880.96, "text": " Europe to build our models. And it's why I'm very delighted to announce you that today, during the first day, we", "tokens": [50364, 3315, 281, 1322, 527, 5245, 13, 400, 309, 311, 983, 286, 478, 588, 18783, 281, 7478, 291, 300, 965, 11, 1830, 264, 700, 786, 11, 321, 51050], "temperature": 0.0, "avg_logprob": -0.19787241200931738, "compression_ratio": 1.3742331288343559, "no_speech_prob": 0.3730790615081787}, {"id": 89, "seek": 86724, "start": 880.96, "end": 891.28, "text": " changed OpenLLM France to become OpenLLM Europe. So you can use this QR code to inboard yourself in this in our", "tokens": [51050, 3105, 7238, 43, 43, 44, 6190, 281, 1813, 7238, 43, 43, 44, 3315, 13, 407, 291, 393, 764, 341, 32784, 3089, 281, 294, 3787, 1803, 294, 341, 294, 527, 51566], "temperature": 0.0, "avg_logprob": -0.19787241200931738, "compression_ratio": 1.3742331288343559, "no_speech_prob": 0.3730790615081787}, {"id": 90, "seek": 89128, "start": 891.3199999999999, "end": 901.56, "text": " Discord server. So we all the content we produce during the six months in French is still available, available. But we have", "tokens": [50366, 32623, 7154, 13, 407, 321, 439, 264, 2701, 321, 5258, 1830, 264, 2309, 2493, 294, 5522, 307, 920, 2435, 11, 2435, 13, 583, 321, 362, 50878], "temperature": 0.0, "avg_logprob": -0.2725537857919369, "compression_ratio": 1.4251497005988023, "no_speech_prob": 0.2513735890388489}, {"id": 91, "seek": 89128, "start": 901.56, "end": 914.28, "text": " created the channel for each European language. So please welcome. And if someone want to be part of the community", "tokens": [50878, 2942, 264, 2269, 337, 1184, 6473, 2856, 13, 407, 1767, 2928, 13, 400, 498, 1580, 528, 281, 312, 644, 295, 264, 1768, 51514], "temperature": 0.0, "avg_logprob": -0.2725537857919369, "compression_ratio": 1.4251497005988023, "no_speech_prob": 0.2513735890388489}, {"id": 92, "seek": 91428, "start": 914.28, "end": 927.48, "text": " management team, please contact us and we will be very pleased to inboard you in our initiative. So that's my tool for today.", "tokens": [50364, 4592, 1469, 11, 1767, 3385, 505, 293, 321, 486, 312, 588, 10587, 281, 294, 3787, 291, 294, 527, 11552, 13, 407, 300, 311, 452, 2290, 337, 965, 13, 51024], "temperature": 0.0, "avg_logprob": -0.20825449112922914, "compression_ratio": 1.2135922330097086, "no_speech_prob": 0.358884334564209}], "language": "en"}