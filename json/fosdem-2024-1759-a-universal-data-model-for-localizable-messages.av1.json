{"text": " Hi, I'm Emily. I work for Mozilla. Yeah, so this is a talk I literally don't think I've got. I could give any wealth except an audience like the translations of ROOM at Vosdom. So I thought I did. I would. In my work at Mozilla on localization systems and tools and standards, recently I've ended up spending quite a bit of time participating in the Unicode Consortium's project to define message format 2, an evolution of the ICU message format standard and a bunch of other things. And I'm here not talking about that like specifically, but more like a side product of what we've ended up doing through that work, which is defining a data model for messages. In particular, messages that are not just a single segmented phrase that you've extracted and you might be able to send it to translation, but more dynamic content as well as everything else. And one of the interesting things about what we've ended up effectively not discovering, but kind of stating the obvious, is that there's an upper bound to this sort of what makes up a localizable segmented phrase or message really. That this is limited by the fact that the keyword localizable there because it's dealing with humans. Humans who need to understand it, but also translators who, well still, are mostly humans who need to be able to take in the the source message and be able to produce an output from that that is understandable in their locale. And this ends up depending on a limited number of different dimensions in which messages kind of vary. Variants have kind of hidden it there as the first one and there of course spoiled everything by saying so. It's the way that messages, message content can vary depending on inputs like numbers and their pluralization categories. You have no apples, you have one apple, you have 15 apples and gender-based determinants, grammatical gender, personal gender, all sorts of various things in different locales languages. But this is one dimension. If you can express that, hey, we've got this variance happening, this message depends on these input values. This is a dimension that we can express. Then of course, once we have a single pattern, a single sequence, it might include placeholders. It might include the number n for how many apples you have or it might be something entirely different. But then finally, we've ended up at least through the message format to work, determining that markup should be kind of kept as a separate thing from placeholders. So markup here means something like HTML. It doesn't need to be HTML. It can be any sort of syntax or any sort of indicator that is saying that the content in this part of the message has these attributes or these something about it. Then within a placeholder, we can have values like numbers that we need to deal with. They can be just strings that are coming from an external source. We can also have annotations on them. We can say that this number here, yeah, it's coming in as a number, but I want it to be formatted. So it has at least two fraction digits, for instance. This needs to be accounted for in the whole message, how it ends up being formatted. Then as I mentioned, we need to be able to deal with variables because we are ultimately here talking about the scope of dynamic messages. So we need to be able to say that explicitly that this message might be getting a number from the outside. It might be getting some string content. It might be getting anything as input, and it needs to deal with those. But sometimes we need to, within a message, want to also do a little bit further processing on a variable value. We may want to select a certain part of it, capitalize it if we're talking about a string, do other sorts of transformations, or express the same sort of value in multiple different ways within a message. So we need a little bit of a tooling to deal with variables. And that's it. That's like through the working message format two, for the past like four years, we've not come up with effectively anything else that really is core, driving the qualities of a formatable message. And that's ended up meaning that one of the things we've produced out of this whole project is this data model for what does a message look like. When you don't consider the syntax of it, when you consider it as a data structure, I'm not going to go through like all of this. But roughly speaking, we can say that a message has two different forms that it can take. It could be either just a single pattern, single sequence that we're dealing with, or it can have some variance. That's the select message over there, which then has some selectors from that when formatting guide us towards selecting one of the variants of the message. The declarations help us declare these are some of the input and local to this message sort of variables that exist. And then the variants of the catch-all key end up defining how exactly do the, when we have multiple message variants, how does that work really? And then when you get to within a single pattern, again, as I alluded to, it can really just, obviously, contain literal content, a string, or it can have expressions, placeholders that is, or it can have markup that can be starting or opening. We also included standalone there, so you can have an element, for example, an inline image be expressed within a message. Then we can have literals, variable references, and the annotations that I mentioned. That's it. That's like these two slides are defining the whole data model that we've ended up dealing with. Okay, I left some like tiny little details out, like for example, the annotations, sorry, the expression, it needs to have at least one of an argument or an annotation in order to be valid and stuff like this, and minor details. But that's it. This is, we think, a universal data model for representing messages. And I'm here basically saying, hey, I think this is kind of cool. And this is not necessarily relevant for just the work specifically to do with message format to the syntax. But more that this is effectively a data model that can allow us to separate the concerns around the syntax of whether your messages are stored in get text files, ICU message format, fluent, any, literally any format. You can take that syntax and you can parse it into this data model structure representing a message. And this is, I think, leading us to a world where we can consider more of a UNIX philosophy for, okay, what do we do now? And I've, sort of, separation of concerns here. And I have, yes, cherry picked explicitly the part of the UNIX philosophy where it says to do one thing and do it well. And not included, for instance, the part about, you know, make sure that you're just dealing, you're communicating as strings the values from one process to another because that's kind of not necessarily working so well. Because we need those parsers. And if we need to understand all of the structure in a message every time when we do it, we end up, for the most part, mixing up the syntax concerns with everything else we're doing with messages. So as some of the things you can do with this data model as ideas is that if you can read and write from a syntax to this data model, and you can do this with multiple syntaxes, this is effectively an interface from which you can take messages from one syntax, turn them into this data model representation, and from there to any other syntax with caveats, but roughly. Another thing is we can build tooling on top of this. So you can build a linter or a validator on top of the data model representation of messages, rather than any syntax representation. And this means that you can use the same validation for all messages independently of what syntax they might be coming from. And if you have these capabilities, it means that when you have an established many localization systems right now are very much monolithic. They have expectations about this is the exact syntax in these sorts of files that are used for messages or resources. This is exactly how you deal with them. And this is what you get included in your output or your program, and this is exactly how it works. But as we're defining here a data model that can read any of these syntaxes, it means that you can build a different sort of formatting or a runtime on the same syntax. So you can start from the way you are now and maybe consider if you want to change how you're doing localization. You don't need to change necessarily everything all at once, but you can take just the formatting runtime change that to work with the same messages you've got, and move on from there. Or vice versa, actually. You could change the message structure, how do you store your messages and still use the same runtime because this is bringing in an ability to very effectively transform your messages from one syntax to another. And you can, when you're dealing with localization, you of course need to deal with translation. And this means that you need to somehow present to your translators the messages that they're working with. And if a translation tool or a framework is going through the message format to data model, it means that you can build an interface for localizers. With the localizers, don't need to know what is the syntax underneath for the placeholders, the variables, the markup, anything else, but they can be presented the same thing for all syntaxes, which might make things a little bit easier for everyone. So those are the ideas I came up with here for what could be the next steps from here, but I'm here saying, hey, this is a cool thing. You guys should play around with it. For us, the current and ongoing work is to extend this sort of a definition to also include method resources and also include the sort of comments and metadata that is quite essential for communicating the context of a message to translation, which as I'm kind of hoping some of you noticed was completely left out of the earlier. But that's intentionally so that we can separate these considerations from each other. But that's it for me. Thank you very much for listening. I'd be very happy to have any questions, comments. In another talk, I heard about message format 2 and function invocations. How do function invocations, how does the data model work, or how do they relate? The question is for how do function invocations relate to all of this? And this, yes, they are represented here in the function annotations here. So something like, for example, plural selection could use a function with a name of plural, for instance, for being this element existing in a select message, selector expression, which is there. Question was whether there are a set of built-in functions that are supported. And message format 2 does come with a relatively small set of built-in functions. The data model itself does not presume this set absolutely. But the set of functions can be extended. For message format 2 in particular, we are looking at a very minimum of effectively number, which also does plural and ordinal selection, but also acts as a formatter. And then string, which is a sort of default for string formatting, but also does the same sort of selection as ICU message format select does. And we are still discussing for message format 2 what other things to include here. Now, of course, when representing messages coming from some completely other syntax, it is entirely conceivable that it is not directly possible to express these messages using the functions that message format 2 defines out of the box. But the data model does allow for you to define that a function like this must be used here, and you can otherwise define how that function works, if that makes sense. And it's possible to make translations between these function meanings. Anything more? The reason to separate context from the minimum required effectively, and here I'm jumping into the answer here, the minimum required for formatting a message is that the context is absolutely required for the translation work. But the context is not absolutely required for the message formatting. So we need to be able to represent it, but we do not absolutely need to have it be a part of the message itself when it is being formatted. And this is why we are dealing with it slightly separately. They are very much related concerns, but we've tried to find with the data model the minimum required for representing a message. And when you trim down the minimum, the context kind of ends up as a thing we can define externally, so we've chosen to be doing that. And I mean if you're interested in that, in particular the specifics of what should we include in the sort of base set of metadata and context fields, here's an issue link where we're discussing this right now that I would be very happy to have your input on. Anything more? Regarding the edit the translator tools, so now most translator tools present a string and expect that the translator will write in a string. Do you imagine that this will change and that the translator will see the elements of the data model in a more graphical way and choose translations through Google boxes, or something like that? Or do you think it will stay as a string representation for the translators in the future? I have no idea and anything is possible and that's kind of cool. So predicting the future of what the translator experience is going to be here is shall we say a hard question. One thing I do think is that this sort of a data model makes it easier to build tools that can present to a translator more the options and opportunities that they might have in modifying a message and content like placeholders and markup which might just show up a syntax when presented a string and be a challenge to even realize that I could change how this bit of it is styled. But if we can present interfaces that can read the data model and understand from this that hey hang on this could be tweaked this way, interfaces that are richer could be built. However of course we do need to keep in mind that such a vast majority of cases are just it's best represented as a pure string. So a majority of work is not going to change but the corner case is where it gets interesting and challenging for those there might be opportunities to present messages in a more translator friendly way. And one part of this I kind of skimmed over it was mentioned in the Ujjwelts presentation yesterday on message format too is that here the selection for variants is not an inline component as it is for example in ICU message format or fluent but the variants all of the variants need to be complete messages presented at the sort of top level of the whole message which is entirely intended to guide towards structures that are easier for translators to deal with rather than needing to to figure out you have and then a selector of apples. Instead of that you have a selector which has you have one apple you have three apples and this sort of an interface. But yeah anymore? If not I would like to thank you very much for your time and yeah that's it for me.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 14.5, "text": " Hi, I'm Emily. I work for Mozilla. Yeah, so this is a talk I literally don't think I've", "tokens": [50364, 2421, 11, 286, 478, 15034, 13, 286, 589, 337, 3335, 26403, 13, 865, 11, 370, 341, 307, 257, 751, 286, 3736, 500, 380, 519, 286, 600, 51089], "temperature": 0.0, "avg_logprob": -0.4587218673140914, "compression_ratio": 1.2430555555555556, "no_speech_prob": 0.17647820711135864}, {"id": 1, "seek": 0, "start": 14.5, "end": 21.92, "text": " got. I could give any wealth except an audience like the translations of ROOM at Vosdom. So", "tokens": [51089, 658, 13, 286, 727, 976, 604, 7203, 3993, 364, 4034, 411, 264, 37578, 295, 497, 26345, 412, 691, 329, 4121, 13, 407, 51460], "temperature": 0.0, "avg_logprob": -0.4587218673140914, "compression_ratio": 1.2430555555555556, "no_speech_prob": 0.17647820711135864}, {"id": 2, "seek": 2192, "start": 21.92, "end": 30.800000000000004, "text": " I thought I did. I would. In my work at Mozilla on localization systems and tools and standards,", "tokens": [50364, 286, 1194, 286, 630, 13, 286, 576, 13, 682, 452, 589, 412, 3335, 26403, 322, 2654, 2144, 3652, 293, 3873, 293, 7787, 11, 50808], "temperature": 0.0, "avg_logprob": -0.20148830137391022, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.11970677971839905}, {"id": 3, "seek": 2192, "start": 30.800000000000004, "end": 37.72, "text": " recently I've ended up spending quite a bit of time participating in the Unicode Consortium's", "tokens": [50808, 3938, 286, 600, 4590, 493, 6434, 1596, 257, 857, 295, 565, 13950, 294, 264, 1156, 299, 1429, 31719, 2197, 311, 51154], "temperature": 0.0, "avg_logprob": -0.20148830137391022, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.11970677971839905}, {"id": 4, "seek": 2192, "start": 37.72, "end": 47.08, "text": " project to define message format 2, an evolution of the ICU message format standard and a bunch", "tokens": [51154, 1716, 281, 6964, 3636, 7877, 568, 11, 364, 9303, 295, 264, 38123, 3636, 7877, 3832, 293, 257, 3840, 51622], "temperature": 0.0, "avg_logprob": -0.20148830137391022, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.11970677971839905}, {"id": 5, "seek": 4708, "start": 47.199999999999996, "end": 54.92, "text": " of other things. And I'm here not talking about that like specifically, but more like a side", "tokens": [50370, 295, 661, 721, 13, 400, 286, 478, 510, 406, 1417, 466, 300, 411, 4682, 11, 457, 544, 411, 257, 1252, 50756], "temperature": 0.0, "avg_logprob": -0.17365803879298522, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.059064626693725586}, {"id": 6, "seek": 4708, "start": 54.92, "end": 61.44, "text": " product of what we've ended up doing through that work, which is defining a data model for", "tokens": [50756, 1674, 295, 437, 321, 600, 4590, 493, 884, 807, 300, 589, 11, 597, 307, 17827, 257, 1412, 2316, 337, 51082], "temperature": 0.0, "avg_logprob": -0.17365803879298522, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.059064626693725586}, {"id": 7, "seek": 4708, "start": 61.44, "end": 69.28, "text": " messages. In particular, messages that are not just a single segmented phrase that you've extracted", "tokens": [51082, 7897, 13, 682, 1729, 11, 7897, 300, 366, 406, 445, 257, 2167, 9469, 292, 9535, 300, 291, 600, 34086, 51474], "temperature": 0.0, "avg_logprob": -0.17365803879298522, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.059064626693725586}, {"id": 8, "seek": 4708, "start": 69.28, "end": 76.6, "text": " and you might be able to send it to translation, but more dynamic content as well as everything else.", "tokens": [51474, 293, 291, 1062, 312, 1075, 281, 2845, 309, 281, 12853, 11, 457, 544, 8546, 2701, 382, 731, 382, 1203, 1646, 13, 51840], "temperature": 0.0, "avg_logprob": -0.17365803879298522, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.059064626693725586}, {"id": 9, "seek": 7660, "start": 76.6, "end": 85.11999999999999, "text": " And one of the interesting things about what we've ended up effectively not discovering,", "tokens": [50364, 400, 472, 295, 264, 1880, 721, 466, 437, 321, 600, 4590, 493, 8659, 406, 24773, 11, 50790], "temperature": 0.0, "avg_logprob": -0.20227452686854772, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0008953257347457111}, {"id": 10, "seek": 7660, "start": 85.11999999999999, "end": 92.16, "text": " but kind of stating the obvious, is that there's an upper bound to this sort of what makes up a", "tokens": [50790, 457, 733, 295, 26688, 264, 6322, 11, 307, 300, 456, 311, 364, 6597, 5472, 281, 341, 1333, 295, 437, 1669, 493, 257, 51142], "temperature": 0.0, "avg_logprob": -0.20227452686854772, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0008953257347457111}, {"id": 11, "seek": 7660, "start": 92.16, "end": 101.0, "text": " localizable segmented phrase or message really. That this is limited by the fact that the keyword", "tokens": [51142, 2654, 22395, 9469, 292, 9535, 420, 3636, 534, 13, 663, 341, 307, 5567, 538, 264, 1186, 300, 264, 20428, 51584], "temperature": 0.0, "avg_logprob": -0.20227452686854772, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0008953257347457111}, {"id": 12, "seek": 7660, "start": 101.0, "end": 105.88, "text": " localizable there because it's dealing with humans. Humans who need to understand it,", "tokens": [51584, 2654, 22395, 456, 570, 309, 311, 6260, 365, 6255, 13, 35809, 567, 643, 281, 1223, 309, 11, 51828], "temperature": 0.0, "avg_logprob": -0.20227452686854772, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0008953257347457111}, {"id": 13, "seek": 10588, "start": 105.88, "end": 113.28, "text": " but also translators who, well still, are mostly humans who need to be able to take in the", "tokens": [50364, 457, 611, 5105, 3391, 567, 11, 731, 920, 11, 366, 5240, 6255, 567, 643, 281, 312, 1075, 281, 747, 294, 264, 50734], "temperature": 0.0, "avg_logprob": -0.14747409522533417, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.0007191374897956848}, {"id": 14, "seek": 10588, "start": 113.28, "end": 119.47999999999999, "text": " the source message and be able to produce an output from that that is understandable in their", "tokens": [50734, 264, 4009, 3636, 293, 312, 1075, 281, 5258, 364, 5598, 490, 300, 300, 307, 25648, 294, 641, 51044], "temperature": 0.0, "avg_logprob": -0.14747409522533417, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.0007191374897956848}, {"id": 15, "seek": 10588, "start": 119.47999999999999, "end": 131.76, "text": " locale. And this ends up depending on a limited number of different dimensions in which messages", "tokens": [51044, 1628, 1220, 13, 400, 341, 5314, 493, 5413, 322, 257, 5567, 1230, 295, 819, 12819, 294, 597, 7897, 51658], "temperature": 0.0, "avg_logprob": -0.14747409522533417, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.0007191374897956848}, {"id": 16, "seek": 13176, "start": 132.04, "end": 138.79999999999998, "text": " kind of vary. Variants have kind of hidden it there as the first one and there of course spoiled", "tokens": [50378, 733, 295, 10559, 13, 32511, 1719, 362, 733, 295, 7633, 309, 456, 382, 264, 700, 472, 293, 456, 295, 1164, 32439, 50716], "temperature": 0.0, "avg_logprob": -0.2481714884440104, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.09589819610118866}, {"id": 17, "seek": 13176, "start": 138.79999999999998, "end": 147.67999999999998, "text": " everything by saying so. It's the way that messages, message content can vary depending on", "tokens": [50716, 1203, 538, 1566, 370, 13, 467, 311, 264, 636, 300, 7897, 11, 3636, 2701, 393, 10559, 5413, 322, 51160], "temperature": 0.0, "avg_logprob": -0.2481714884440104, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.09589819610118866}, {"id": 18, "seek": 13176, "start": 147.67999999999998, "end": 154.64, "text": " inputs like numbers and their pluralization categories. You have no apples, you have one apple,", "tokens": [51160, 15743, 411, 3547, 293, 641, 25377, 2144, 10479, 13, 509, 362, 572, 16814, 11, 291, 362, 472, 10606, 11, 51508], "temperature": 0.0, "avg_logprob": -0.2481714884440104, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.09589819610118866}, {"id": 19, "seek": 15464, "start": 155.2, "end": 165.72, "text": " you have 15 apples and gender-based determinants, grammatical gender, personal gender, all sorts of", "tokens": [50392, 291, 362, 2119, 16814, 293, 7898, 12, 6032, 15957, 1719, 11, 17570, 267, 804, 7898, 11, 2973, 7898, 11, 439, 7527, 295, 50918], "temperature": 0.0, "avg_logprob": -0.19294207436697824, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.10169214010238647}, {"id": 20, "seek": 15464, "start": 165.72, "end": 172.32, "text": " various things in different locales languages. But this is one dimension. If you can express that,", "tokens": [50918, 3683, 721, 294, 819, 2654, 279, 8650, 13, 583, 341, 307, 472, 10139, 13, 759, 291, 393, 5109, 300, 11, 51248], "temperature": 0.0, "avg_logprob": -0.19294207436697824, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.10169214010238647}, {"id": 21, "seek": 15464, "start": 172.32, "end": 178.04, "text": " hey, we've got this variance happening, this message depends on these input values. This is a", "tokens": [51248, 4177, 11, 321, 600, 658, 341, 21977, 2737, 11, 341, 3636, 5946, 322, 613, 4846, 4190, 13, 639, 307, 257, 51534], "temperature": 0.0, "avg_logprob": -0.19294207436697824, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.10169214010238647}, {"id": 22, "seek": 17804, "start": 178.07999999999998, "end": 187.48, "text": " dimension that we can express. Then of course, once we have a single pattern, a single sequence,", "tokens": [50366, 10139, 300, 321, 393, 5109, 13, 1396, 295, 1164, 11, 1564, 321, 362, 257, 2167, 5102, 11, 257, 2167, 8310, 11, 50836], "temperature": 0.0, "avg_logprob": -0.18041671023649328, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.047365106642246246}, {"id": 23, "seek": 17804, "start": 187.48, "end": 197.04, "text": " it might include placeholders. It might include the number n for how many apples you have or it", "tokens": [50836, 309, 1062, 4090, 1081, 12916, 13, 467, 1062, 4090, 264, 1230, 297, 337, 577, 867, 16814, 291, 362, 420, 309, 51314], "temperature": 0.0, "avg_logprob": -0.18041671023649328, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.047365106642246246}, {"id": 24, "seek": 17804, "start": 197.04, "end": 202.68, "text": " might be something entirely different. But then finally, we've ended up at least through the message", "tokens": [51314, 1062, 312, 746, 7696, 819, 13, 583, 550, 2721, 11, 321, 600, 4590, 493, 412, 1935, 807, 264, 3636, 51596], "temperature": 0.0, "avg_logprob": -0.18041671023649328, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.047365106642246246}, {"id": 25, "seek": 20268, "start": 202.72, "end": 210.76000000000002, "text": " format to work, determining that markup should be kind of kept as a separate thing from placeholders.", "tokens": [50366, 7877, 281, 589, 11, 23751, 300, 1491, 1010, 820, 312, 733, 295, 4305, 382, 257, 4994, 551, 490, 1081, 12916, 13, 50768], "temperature": 0.0, "avg_logprob": -0.16610234220262984, "compression_ratio": 1.5621621621621622, "no_speech_prob": 0.030557045713067055}, {"id": 26, "seek": 20268, "start": 210.76000000000002, "end": 217.12, "text": " So markup here means something like HTML. It doesn't need to be HTML. It can be any sort of", "tokens": [50768, 407, 1491, 1010, 510, 1355, 746, 411, 17995, 13, 467, 1177, 380, 643, 281, 312, 17995, 13, 467, 393, 312, 604, 1333, 295, 51086], "temperature": 0.0, "avg_logprob": -0.16610234220262984, "compression_ratio": 1.5621621621621622, "no_speech_prob": 0.030557045713067055}, {"id": 27, "seek": 20268, "start": 217.12, "end": 224.44, "text": " syntax or any sort of indicator that is saying that the content in this part of the message has", "tokens": [51086, 28431, 420, 604, 1333, 295, 16961, 300, 307, 1566, 300, 264, 2701, 294, 341, 644, 295, 264, 3636, 575, 51452], "temperature": 0.0, "avg_logprob": -0.16610234220262984, "compression_ratio": 1.5621621621621622, "no_speech_prob": 0.030557045713067055}, {"id": 28, "seek": 22444, "start": 224.52, "end": 237.0, "text": " these attributes or these something about it. Then within a placeholder, we can have values", "tokens": [50368, 613, 17212, 420, 613, 746, 466, 309, 13, 1396, 1951, 257, 1081, 20480, 11, 321, 393, 362, 4190, 50992], "temperature": 0.0, "avg_logprob": -0.1746656256662288, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.009696205146610737}, {"id": 29, "seek": 22444, "start": 237.0, "end": 244.28, "text": " like numbers that we need to deal with. They can be just strings that are coming from an external", "tokens": [50992, 411, 3547, 300, 321, 643, 281, 2028, 365, 13, 814, 393, 312, 445, 13985, 300, 366, 1348, 490, 364, 8320, 51356], "temperature": 0.0, "avg_logprob": -0.1746656256662288, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.009696205146610737}, {"id": 30, "seek": 22444, "start": 244.28, "end": 250.4, "text": " source. We can also have annotations on them. We can say that this number here, yeah, it's coming in", "tokens": [51356, 4009, 13, 492, 393, 611, 362, 25339, 763, 322, 552, 13, 492, 393, 584, 300, 341, 1230, 510, 11, 1338, 11, 309, 311, 1348, 294, 51662], "temperature": 0.0, "avg_logprob": -0.1746656256662288, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.009696205146610737}, {"id": 31, "seek": 25040, "start": 250.44, "end": 256.4, "text": " as a number, but I want it to be formatted. So it has at least two fraction digits, for instance.", "tokens": [50366, 382, 257, 1230, 11, 457, 286, 528, 309, 281, 312, 1254, 32509, 13, 407, 309, 575, 412, 1935, 732, 14135, 27011, 11, 337, 5197, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1852791146056293, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0008673873380757868}, {"id": 32, "seek": 25040, "start": 256.4, "end": 266.84000000000003, "text": " This needs to be accounted for in the whole message, how it ends up being formatted. Then as I", "tokens": [50664, 639, 2203, 281, 312, 43138, 337, 294, 264, 1379, 3636, 11, 577, 309, 5314, 493, 885, 1254, 32509, 13, 1396, 382, 286, 51186], "temperature": 0.0, "avg_logprob": -0.1852791146056293, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0008673873380757868}, {"id": 33, "seek": 25040, "start": 266.84000000000003, "end": 272.24, "text": " mentioned, we need to be able to deal with variables because we are ultimately here talking about", "tokens": [51186, 2835, 11, 321, 643, 281, 312, 1075, 281, 2028, 365, 9102, 570, 321, 366, 6284, 510, 1417, 466, 51456], "temperature": 0.0, "avg_logprob": -0.1852791146056293, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0008673873380757868}, {"id": 34, "seek": 27224, "start": 272.32, "end": 282.48, "text": " the scope of dynamic messages. So we need to be able to say that explicitly that this message might", "tokens": [50368, 264, 11923, 295, 8546, 7897, 13, 407, 321, 643, 281, 312, 1075, 281, 584, 300, 20803, 300, 341, 3636, 1062, 50876], "temperature": 0.0, "avg_logprob": -0.1763139452253069, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.05326360836625099}, {"id": 35, "seek": 27224, "start": 282.48, "end": 287.88, "text": " be getting a number from the outside. It might be getting some string content. It might be getting", "tokens": [50876, 312, 1242, 257, 1230, 490, 264, 2380, 13, 467, 1062, 312, 1242, 512, 6798, 2701, 13, 467, 1062, 312, 1242, 51146], "temperature": 0.0, "avg_logprob": -0.1763139452253069, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.05326360836625099}, {"id": 36, "seek": 27224, "start": 287.88, "end": 296.0, "text": " anything as input, and it needs to deal with those. But sometimes we need to, within a message,", "tokens": [51146, 1340, 382, 4846, 11, 293, 309, 2203, 281, 2028, 365, 729, 13, 583, 2171, 321, 643, 281, 11, 1951, 257, 3636, 11, 51552], "temperature": 0.0, "avg_logprob": -0.1763139452253069, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.05326360836625099}, {"id": 37, "seek": 29600, "start": 296.32, "end": 301.92, "text": " want to also do a little bit further processing on a variable value. We may want to", "tokens": [50380, 528, 281, 611, 360, 257, 707, 857, 3052, 9007, 322, 257, 7006, 2158, 13, 492, 815, 528, 281, 50660], "temperature": 0.0, "avg_logprob": -0.14995471997694534, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.003266444429755211}, {"id": 38, "seek": 29600, "start": 305.92, "end": 311.68, "text": " select a certain part of it, capitalize it if we're talking about a string, do other sorts of", "tokens": [50860, 3048, 257, 1629, 644, 295, 309, 11, 48114, 309, 498, 321, 434, 1417, 466, 257, 6798, 11, 360, 661, 7527, 295, 51148], "temperature": 0.0, "avg_logprob": -0.14995471997694534, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.003266444429755211}, {"id": 39, "seek": 29600, "start": 311.68, "end": 316.64, "text": " transformations, or express the same sort of value in multiple different ways within a message.", "tokens": [51148, 34852, 11, 420, 5109, 264, 912, 1333, 295, 2158, 294, 3866, 819, 2098, 1951, 257, 3636, 13, 51396], "temperature": 0.0, "avg_logprob": -0.14995471997694534, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.003266444429755211}, {"id": 40, "seek": 29600, "start": 316.64, "end": 324.8, "text": " So we need a little bit of a tooling to deal with variables. And that's it. That's like", "tokens": [51396, 407, 321, 643, 257, 707, 857, 295, 257, 46593, 281, 2028, 365, 9102, 13, 400, 300, 311, 309, 13, 663, 311, 411, 51804], "temperature": 0.0, "avg_logprob": -0.14995471997694534, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.003266444429755211}, {"id": 41, "seek": 32480, "start": 325.6, "end": 330.64, "text": " through the working message format two, for the past like four years, we've not come up with", "tokens": [50404, 807, 264, 1364, 3636, 7877, 732, 11, 337, 264, 1791, 411, 1451, 924, 11, 321, 600, 406, 808, 493, 365, 50656], "temperature": 0.0, "avg_logprob": -0.17176777521769207, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.0006548367673531175}, {"id": 42, "seek": 32480, "start": 330.64, "end": 337.6, "text": " effectively anything else that really is core, driving the qualities of a formatable message.", "tokens": [50656, 8659, 1340, 1646, 300, 534, 307, 4965, 11, 4840, 264, 16477, 295, 257, 1254, 31415, 3636, 13, 51004], "temperature": 0.0, "avg_logprob": -0.17176777521769207, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.0006548367673531175}, {"id": 43, "seek": 32480, "start": 338.40000000000003, "end": 344.40000000000003, "text": " And that's ended up meaning that one of the things we've produced out of this whole project is this", "tokens": [51044, 400, 300, 311, 4590, 493, 3620, 300, 472, 295, 264, 721, 321, 600, 7126, 484, 295, 341, 1379, 1716, 307, 341, 51344], "temperature": 0.0, "avg_logprob": -0.17176777521769207, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.0006548367673531175}, {"id": 44, "seek": 32480, "start": 344.40000000000003, "end": 349.52, "text": " data model for what does a message look like. When you don't consider the syntax of it, when you", "tokens": [51344, 1412, 2316, 337, 437, 775, 257, 3636, 574, 411, 13, 1133, 291, 500, 380, 1949, 264, 28431, 295, 309, 11, 562, 291, 51600], "temperature": 0.0, "avg_logprob": -0.17176777521769207, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.0006548367673531175}, {"id": 45, "seek": 34952, "start": 349.52, "end": 359.03999999999996, "text": " consider it as a data structure, I'm not going to go through like all of this. But roughly speaking,", "tokens": [50364, 1949, 309, 382, 257, 1412, 3877, 11, 286, 478, 406, 516, 281, 352, 807, 411, 439, 295, 341, 13, 583, 9810, 4124, 11, 50840], "temperature": 0.0, "avg_logprob": -0.104671262925671, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.003218936501070857}, {"id": 46, "seek": 34952, "start": 359.76, "end": 364.32, "text": " we can say that a message has two different forms that it can take. It could be either just a single", "tokens": [50876, 321, 393, 584, 300, 257, 3636, 575, 732, 819, 6422, 300, 309, 393, 747, 13, 467, 727, 312, 2139, 445, 257, 2167, 51104], "temperature": 0.0, "avg_logprob": -0.104671262925671, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.003218936501070857}, {"id": 47, "seek": 34952, "start": 364.32, "end": 370.71999999999997, "text": " pattern, single sequence that we're dealing with, or it can have some variance. That's the select", "tokens": [51104, 5102, 11, 2167, 8310, 300, 321, 434, 6260, 365, 11, 420, 309, 393, 362, 512, 21977, 13, 663, 311, 264, 3048, 51424], "temperature": 0.0, "avg_logprob": -0.104671262925671, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.003218936501070857}, {"id": 48, "seek": 34952, "start": 370.71999999999997, "end": 377.28, "text": " message over there, which then has some selectors from that when formatting guide us towards", "tokens": [51424, 3636, 670, 456, 11, 597, 550, 575, 512, 3048, 830, 490, 300, 562, 39366, 5934, 505, 3030, 51752], "temperature": 0.0, "avg_logprob": -0.104671262925671, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.003218936501070857}, {"id": 49, "seek": 37728, "start": 377.28, "end": 385.67999999999995, "text": " selecting one of the variants of the message. The declarations help us declare these are some", "tokens": [50364, 18182, 472, 295, 264, 21669, 295, 264, 3636, 13, 440, 16694, 763, 854, 505, 19710, 613, 366, 512, 50784], "temperature": 0.0, "avg_logprob": -0.140886717576247, "compression_ratio": 1.6726190476190477, "no_speech_prob": 0.0011152502847835422}, {"id": 50, "seek": 37728, "start": 385.67999999999995, "end": 393.03999999999996, "text": " of the input and local to this message sort of variables that exist. And then the variants of", "tokens": [50784, 295, 264, 4846, 293, 2654, 281, 341, 3636, 1333, 295, 9102, 300, 2514, 13, 400, 550, 264, 21669, 295, 51152], "temperature": 0.0, "avg_logprob": -0.140886717576247, "compression_ratio": 1.6726190476190477, "no_speech_prob": 0.0011152502847835422}, {"id": 51, "seek": 37728, "start": 393.03999999999996, "end": 399.59999999999997, "text": " the catch-all key end up defining how exactly do the, when we have multiple message variants,", "tokens": [51152, 264, 3745, 12, 336, 2141, 917, 493, 17827, 577, 2293, 360, 264, 11, 562, 321, 362, 3866, 3636, 21669, 11, 51480], "temperature": 0.0, "avg_logprob": -0.140886717576247, "compression_ratio": 1.6726190476190477, "no_speech_prob": 0.0011152502847835422}, {"id": 52, "seek": 39960, "start": 399.6, "end": 404.48, "text": " how does that work really? And then when you get to within a single pattern,", "tokens": [50364, 577, 775, 300, 589, 534, 30, 400, 550, 562, 291, 483, 281, 1951, 257, 2167, 5102, 11, 50608], "temperature": 0.0, "avg_logprob": -0.14649091608384077, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.08873579651117325}, {"id": 53, "seek": 39960, "start": 406.48, "end": 413.52000000000004, "text": " again, as I alluded to, it can really just, obviously, contain literal content,", "tokens": [50708, 797, 11, 382, 286, 33919, 281, 11, 309, 393, 534, 445, 11, 2745, 11, 5304, 20411, 2701, 11, 51060], "temperature": 0.0, "avg_logprob": -0.14649091608384077, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.08873579651117325}, {"id": 54, "seek": 39960, "start": 414.64000000000004, "end": 420.96000000000004, "text": " a string, or it can have expressions, placeholders that is, or it can have markup", "tokens": [51116, 257, 6798, 11, 420, 309, 393, 362, 15277, 11, 1081, 12916, 300, 307, 11, 420, 309, 393, 362, 1491, 1010, 51432], "temperature": 0.0, "avg_logprob": -0.14649091608384077, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.08873579651117325}, {"id": 55, "seek": 39960, "start": 421.52000000000004, "end": 426.96000000000004, "text": " that can be starting or opening. We also included standalone there, so you can have an element,", "tokens": [51460, 300, 393, 312, 2891, 420, 5193, 13, 492, 611, 5556, 37454, 456, 11, 370, 291, 393, 362, 364, 4478, 11, 51732], "temperature": 0.0, "avg_logprob": -0.14649091608384077, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.08873579651117325}, {"id": 56, "seek": 42696, "start": 427.2, "end": 434.0, "text": " for example, an inline image be expressed within a message. Then we can have literals,", "tokens": [50376, 337, 1365, 11, 364, 294, 1889, 3256, 312, 12675, 1951, 257, 3636, 13, 1396, 321, 393, 362, 2733, 1124, 11, 50716], "temperature": 0.0, "avg_logprob": -0.1219281731071053, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.00048779608914628625}, {"id": 57, "seek": 42696, "start": 436.88, "end": 442.79999999999995, "text": " variable references, and the annotations that I mentioned. That's it. That's like these two", "tokens": [50860, 7006, 15400, 11, 293, 264, 25339, 763, 300, 286, 2835, 13, 663, 311, 309, 13, 663, 311, 411, 613, 732, 51156], "temperature": 0.0, "avg_logprob": -0.1219281731071053, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.00048779608914628625}, {"id": 58, "seek": 42696, "start": 442.79999999999995, "end": 448.08, "text": " slides are defining the whole data model that we've ended up dealing with. Okay, I left some", "tokens": [51156, 9788, 366, 17827, 264, 1379, 1412, 2316, 300, 321, 600, 4590, 493, 6260, 365, 13, 1033, 11, 286, 1411, 512, 51420], "temperature": 0.0, "avg_logprob": -0.1219281731071053, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.00048779608914628625}, {"id": 59, "seek": 42696, "start": 448.08, "end": 454.24, "text": " like tiny little details out, like for example, the annotations, sorry, the expression, it needs", "tokens": [51420, 411, 5870, 707, 4365, 484, 11, 411, 337, 1365, 11, 264, 25339, 763, 11, 2597, 11, 264, 6114, 11, 309, 2203, 51728], "temperature": 0.0, "avg_logprob": -0.1219281731071053, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.00048779608914628625}, {"id": 60, "seek": 45424, "start": 454.24, "end": 459.44, "text": " to have at least one of an argument or an annotation in order to be valid and stuff like this, and", "tokens": [50364, 281, 362, 412, 1935, 472, 295, 364, 6770, 420, 364, 48654, 294, 1668, 281, 312, 7363, 293, 1507, 411, 341, 11, 293, 50624], "temperature": 0.0, "avg_logprob": -0.1024639423076923, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00195642514154315}, {"id": 61, "seek": 45424, "start": 461.2, "end": 468.64, "text": " minor details. But that's it. This is, we think, a universal data model for representing messages.", "tokens": [50712, 6696, 4365, 13, 583, 300, 311, 309, 13, 639, 307, 11, 321, 519, 11, 257, 11455, 1412, 2316, 337, 13460, 7897, 13, 51084], "temperature": 0.0, "avg_logprob": -0.1024639423076923, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00195642514154315}, {"id": 62, "seek": 45424, "start": 469.68, "end": 475.76, "text": " And I'm here basically saying, hey, I think this is kind of cool. And this is not necessarily", "tokens": [51136, 400, 286, 478, 510, 1936, 1566, 11, 4177, 11, 286, 519, 341, 307, 733, 295, 1627, 13, 400, 341, 307, 406, 4725, 51440], "temperature": 0.0, "avg_logprob": -0.1024639423076923, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00195642514154315}, {"id": 63, "seek": 45424, "start": 477.04, "end": 483.12, "text": " relevant for just the work specifically to do with message format to the syntax.", "tokens": [51504, 7340, 337, 445, 264, 589, 4682, 281, 360, 365, 3636, 7877, 281, 264, 28431, 13, 51808], "temperature": 0.0, "avg_logprob": -0.1024639423076923, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00195642514154315}, {"id": 64, "seek": 48312, "start": 483.76, "end": 489.04, "text": " But more that this is effectively a data model that", "tokens": [50396, 583, 544, 300, 341, 307, 8659, 257, 1412, 2316, 300, 50660], "temperature": 0.0, "avg_logprob": -0.17154516740278763, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.0016474887961521745}, {"id": 65, "seek": 48312, "start": 491.12, "end": 498.8, "text": " can allow us to separate the concerns around the syntax of whether your messages are stored in", "tokens": [50764, 393, 2089, 505, 281, 4994, 264, 7389, 926, 264, 28431, 295, 1968, 428, 7897, 366, 12187, 294, 51148], "temperature": 0.0, "avg_logprob": -0.17154516740278763, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.0016474887961521745}, {"id": 66, "seek": 48312, "start": 501.04, "end": 512.72, "text": " get text files, ICU message format, fluent, any, literally any format. You can take that syntax", "tokens": [51260, 483, 2487, 7098, 11, 38123, 3636, 7877, 11, 40799, 11, 604, 11, 3736, 604, 7877, 13, 509, 393, 747, 300, 28431, 51844], "temperature": 0.0, "avg_logprob": -0.17154516740278763, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.0016474887961521745}, {"id": 67, "seek": 51312, "start": 513.36, "end": 521.04, "text": " and you can parse it into this data model structure representing a message. And this is, I think,", "tokens": [50376, 293, 291, 393, 48377, 309, 666, 341, 1412, 2316, 3877, 13460, 257, 3636, 13, 400, 341, 307, 11, 286, 519, 11, 50760], "temperature": 0.0, "avg_logprob": -0.15118934631347655, "compression_ratio": 1.4494949494949494, "no_speech_prob": 0.0003855974937323481}, {"id": 68, "seek": 51312, "start": 521.04, "end": 529.76, "text": " leading us to a world where we can consider more of a UNIX philosophy for, okay, what do we do now?", "tokens": [50760, 5775, 505, 281, 257, 1002, 689, 321, 393, 1949, 544, 295, 257, 8229, 21124, 10675, 337, 11, 1392, 11, 437, 360, 321, 360, 586, 30, 51196], "temperature": 0.0, "avg_logprob": -0.15118934631347655, "compression_ratio": 1.4494949494949494, "no_speech_prob": 0.0003855974937323481}, {"id": 69, "seek": 51312, "start": 530.88, "end": 540.64, "text": " And I've, sort of, separation of concerns here. And I have, yes, cherry picked explicitly", "tokens": [51252, 400, 286, 600, 11, 1333, 295, 11, 14634, 295, 7389, 510, 13, 400, 286, 362, 11, 2086, 11, 20164, 6183, 20803, 51740], "temperature": 0.0, "avg_logprob": -0.15118934631347655, "compression_ratio": 1.4494949494949494, "no_speech_prob": 0.0003855974937323481}, {"id": 70, "seek": 54064, "start": 541.6, "end": 545.1999999999999, "text": " the part of the UNIX philosophy where it says to do one thing and do it well.", "tokens": [50412, 264, 644, 295, 264, 8229, 21124, 10675, 689, 309, 1619, 281, 360, 472, 551, 293, 360, 309, 731, 13, 50592], "temperature": 0.0, "avg_logprob": -0.1421081077220828, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.010140255093574524}, {"id": 71, "seek": 54064, "start": 545.76, "end": 549.76, "text": " And not included, for instance, the part about, you know, make sure that you're just", "tokens": [50620, 400, 406, 5556, 11, 337, 5197, 11, 264, 644, 466, 11, 291, 458, 11, 652, 988, 300, 291, 434, 445, 50820], "temperature": 0.0, "avg_logprob": -0.1421081077220828, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.010140255093574524}, {"id": 72, "seek": 54064, "start": 549.76, "end": 555.28, "text": " dealing, you're communicating as strings the values from one process to another because", "tokens": [50820, 6260, 11, 291, 434, 17559, 382, 13985, 264, 4190, 490, 472, 1399, 281, 1071, 570, 51096], "temperature": 0.0, "avg_logprob": -0.1421081077220828, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.010140255093574524}, {"id": 73, "seek": 54064, "start": 556.3199999999999, "end": 562.4, "text": " that's kind of not necessarily working so well. Because we need those parsers. And if we need to", "tokens": [51148, 300, 311, 733, 295, 406, 4725, 1364, 370, 731, 13, 1436, 321, 643, 729, 21156, 433, 13, 400, 498, 321, 643, 281, 51452], "temperature": 0.0, "avg_logprob": -0.1421081077220828, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.010140255093574524}, {"id": 74, "seek": 56240, "start": 562.4, "end": 571.28, "text": " understand all of the structure in a message every time when we do it, we end up, for the most part,", "tokens": [50364, 1223, 439, 295, 264, 3877, 294, 257, 3636, 633, 565, 562, 321, 360, 309, 11, 321, 917, 493, 11, 337, 264, 881, 644, 11, 50808], "temperature": 0.0, "avg_logprob": -0.08824664668032997, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.06555122882127762}, {"id": 75, "seek": 56240, "start": 571.28, "end": 577.6, "text": " mixing up the syntax concerns with everything else we're doing with messages. So as some of the", "tokens": [50808, 11983, 493, 264, 28431, 7389, 365, 1203, 1646, 321, 434, 884, 365, 7897, 13, 407, 382, 512, 295, 264, 51124], "temperature": 0.0, "avg_logprob": -0.08824664668032997, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.06555122882127762}, {"id": 76, "seek": 56240, "start": 578.48, "end": 584.88, "text": " things you can do with this data model as ideas is that if you can read and write from a syntax to", "tokens": [51168, 721, 291, 393, 360, 365, 341, 1412, 2316, 382, 3487, 307, 300, 498, 291, 393, 1401, 293, 2464, 490, 257, 28431, 281, 51488], "temperature": 0.0, "avg_logprob": -0.08824664668032997, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.06555122882127762}, {"id": 77, "seek": 56240, "start": 584.88, "end": 590.96, "text": " this data model, and you can do this with multiple syntaxes, this is effectively an interface from", "tokens": [51488, 341, 1412, 2316, 11, 293, 291, 393, 360, 341, 365, 3866, 28431, 279, 11, 341, 307, 8659, 364, 9226, 490, 51792], "temperature": 0.0, "avg_logprob": -0.08824664668032997, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.06555122882127762}, {"id": 78, "seek": 59096, "start": 590.96, "end": 595.0400000000001, "text": " which you can take messages from one syntax, turn them into this data model representation,", "tokens": [50364, 597, 291, 393, 747, 7897, 490, 472, 28431, 11, 1261, 552, 666, 341, 1412, 2316, 10290, 11, 50568], "temperature": 0.0, "avg_logprob": -0.09637644796660452, "compression_ratio": 1.896103896103896, "no_speech_prob": 0.00048011000035330653}, {"id": 79, "seek": 59096, "start": 595.0400000000001, "end": 599.9200000000001, "text": " and from there to any other syntax with caveats, but roughly.", "tokens": [50568, 293, 490, 456, 281, 604, 661, 28431, 365, 11730, 1720, 11, 457, 9810, 13, 50812], "temperature": 0.0, "avg_logprob": -0.09637644796660452, "compression_ratio": 1.896103896103896, "no_speech_prob": 0.00048011000035330653}, {"id": 80, "seek": 59096, "start": 602.32, "end": 608.1600000000001, "text": " Another thing is we can build tooling on top of this. So you can build a linter or a validator", "tokens": [50932, 3996, 551, 307, 321, 393, 1322, 46593, 322, 1192, 295, 341, 13, 407, 291, 393, 1322, 257, 287, 5106, 420, 257, 7363, 1639, 51224], "temperature": 0.0, "avg_logprob": -0.09637644796660452, "compression_ratio": 1.896103896103896, "no_speech_prob": 0.00048011000035330653}, {"id": 81, "seek": 59096, "start": 609.12, "end": 614.4000000000001, "text": " on top of the data model representation of messages, rather than any syntax representation.", "tokens": [51272, 322, 1192, 295, 264, 1412, 2316, 10290, 295, 7897, 11, 2831, 813, 604, 28431, 10290, 13, 51536], "temperature": 0.0, "avg_logprob": -0.09637644796660452, "compression_ratio": 1.896103896103896, "no_speech_prob": 0.00048011000035330653}, {"id": 82, "seek": 59096, "start": 614.4000000000001, "end": 620.72, "text": " And this means that you can use the same validation for all messages independently of what syntax", "tokens": [51536, 400, 341, 1355, 300, 291, 393, 764, 264, 912, 24071, 337, 439, 7897, 21761, 295, 437, 28431, 51852], "temperature": 0.0, "avg_logprob": -0.09637644796660452, "compression_ratio": 1.896103896103896, "no_speech_prob": 0.00048011000035330653}, {"id": 83, "seek": 62072, "start": 620.72, "end": 630.48, "text": " they might be coming from. And if you have these capabilities, it means that when you have an", "tokens": [50364, 436, 1062, 312, 1348, 490, 13, 400, 498, 291, 362, 613, 10862, 11, 309, 1355, 300, 562, 291, 362, 364, 50852], "temperature": 0.0, "avg_logprob": -0.07931593961493913, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0013028201647102833}, {"id": 84, "seek": 62072, "start": 630.48, "end": 636.96, "text": " established many localization systems right now are very much monolithic. They have expectations", "tokens": [50852, 7545, 867, 2654, 2144, 3652, 558, 586, 366, 588, 709, 1108, 42878, 13, 814, 362, 9843, 51176], "temperature": 0.0, "avg_logprob": -0.07931593961493913, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0013028201647102833}, {"id": 85, "seek": 62072, "start": 636.96, "end": 643.6800000000001, "text": " about this is the exact syntax in these sorts of files that are used for messages or resources.", "tokens": [51176, 466, 341, 307, 264, 1900, 28431, 294, 613, 7527, 295, 7098, 300, 366, 1143, 337, 7897, 420, 3593, 13, 51512], "temperature": 0.0, "avg_logprob": -0.07931593961493913, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0013028201647102833}, {"id": 86, "seek": 62072, "start": 643.6800000000001, "end": 648.5600000000001, "text": " This is exactly how you deal with them. And this is what you get included in your output or your", "tokens": [51512, 639, 307, 2293, 577, 291, 2028, 365, 552, 13, 400, 341, 307, 437, 291, 483, 5556, 294, 428, 5598, 420, 428, 51756], "temperature": 0.0, "avg_logprob": -0.07931593961493913, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0013028201647102833}, {"id": 87, "seek": 64856, "start": 648.56, "end": 654.0799999999999, "text": " program, and this is exactly how it works. But as we're defining here a data model that can read", "tokens": [50364, 1461, 11, 293, 341, 307, 2293, 577, 309, 1985, 13, 583, 382, 321, 434, 17827, 510, 257, 1412, 2316, 300, 393, 1401, 50640], "temperature": 0.0, "avg_logprob": -0.08785593098607557, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.006789726670831442}, {"id": 88, "seek": 64856, "start": 654.0799999999999, "end": 659.52, "text": " any of these syntaxes, it means that you can build a different sort of formatting or a runtime", "tokens": [50640, 604, 295, 613, 28431, 279, 11, 309, 1355, 300, 291, 393, 1322, 257, 819, 1333, 295, 39366, 420, 257, 34474, 50912], "temperature": 0.0, "avg_logprob": -0.08785593098607557, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.006789726670831442}, {"id": 89, "seek": 64856, "start": 659.52, "end": 666.88, "text": " on the same syntax. So you can start from the way you are now and maybe consider if you want to", "tokens": [50912, 322, 264, 912, 28431, 13, 407, 291, 393, 722, 490, 264, 636, 291, 366, 586, 293, 1310, 1949, 498, 291, 528, 281, 51280], "temperature": 0.0, "avg_logprob": -0.08785593098607557, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.006789726670831442}, {"id": 90, "seek": 64856, "start": 666.88, "end": 672.88, "text": " change how you're doing localization. You don't need to change necessarily everything all at once,", "tokens": [51280, 1319, 577, 291, 434, 884, 2654, 2144, 13, 509, 500, 380, 643, 281, 1319, 4725, 1203, 439, 412, 1564, 11, 51580], "temperature": 0.0, "avg_logprob": -0.08785593098607557, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.006789726670831442}, {"id": 91, "seek": 64856, "start": 672.88, "end": 678.0799999999999, "text": " but you can take just the formatting runtime change that to work with the same messages you've got,", "tokens": [51580, 457, 291, 393, 747, 445, 264, 39366, 34474, 1319, 300, 281, 589, 365, 264, 912, 7897, 291, 600, 658, 11, 51840], "temperature": 0.0, "avg_logprob": -0.08785593098607557, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.006789726670831442}, {"id": 92, "seek": 67808, "start": 678.08, "end": 685.36, "text": " and move on from there. Or vice versa, actually. You could change the message", "tokens": [50364, 293, 1286, 322, 490, 456, 13, 1610, 11964, 25650, 11, 767, 13, 509, 727, 1319, 264, 3636, 50728], "temperature": 0.0, "avg_logprob": -0.14079614333164545, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0005522437277249992}, {"id": 93, "seek": 67808, "start": 686.0, "end": 691.76, "text": " structure, how do you store your messages and still use the same runtime because this is bringing", "tokens": [50760, 3877, 11, 577, 360, 291, 3531, 428, 7897, 293, 920, 764, 264, 912, 34474, 570, 341, 307, 5062, 51048], "temperature": 0.0, "avg_logprob": -0.14079614333164545, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0005522437277249992}, {"id": 94, "seek": 67808, "start": 691.76, "end": 699.36, "text": " in an ability to very effectively transform your messages from one syntax to another. And you can,", "tokens": [51048, 294, 364, 3485, 281, 588, 8659, 4088, 428, 7897, 490, 472, 28431, 281, 1071, 13, 400, 291, 393, 11, 51428], "temperature": 0.0, "avg_logprob": -0.14079614333164545, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0005522437277249992}, {"id": 95, "seek": 67808, "start": 700.1600000000001, "end": 704.1600000000001, "text": " when you're dealing with localization, you of course need to deal with translation.", "tokens": [51468, 562, 291, 434, 6260, 365, 2654, 2144, 11, 291, 295, 1164, 643, 281, 2028, 365, 12853, 13, 51668], "temperature": 0.0, "avg_logprob": -0.14079614333164545, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0005522437277249992}, {"id": 96, "seek": 70416, "start": 704.88, "end": 709.76, "text": " And this means that you need to somehow present to your translators the messages that they're", "tokens": [50400, 400, 341, 1355, 300, 291, 643, 281, 6063, 1974, 281, 428, 5105, 3391, 264, 7897, 300, 436, 434, 50644], "temperature": 0.0, "avg_logprob": -0.08315462957728993, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.004526369273662567}, {"id": 97, "seek": 70416, "start": 709.76, "end": 718.64, "text": " working with. And if a translation tool or a framework is going through the message format", "tokens": [50644, 1364, 365, 13, 400, 498, 257, 12853, 2290, 420, 257, 8388, 307, 516, 807, 264, 3636, 7877, 51088], "temperature": 0.0, "avg_logprob": -0.08315462957728993, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.004526369273662567}, {"id": 98, "seek": 70416, "start": 718.64, "end": 724.3199999999999, "text": " to data model, it means that you can build an interface for localizers. With the localizers,", "tokens": [51088, 281, 1412, 2316, 11, 309, 1355, 300, 291, 393, 1322, 364, 9226, 337, 2654, 22525, 13, 2022, 264, 2654, 22525, 11, 51372], "temperature": 0.0, "avg_logprob": -0.08315462957728993, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.004526369273662567}, {"id": 99, "seek": 70416, "start": 724.3199999999999, "end": 731.04, "text": " don't need to know what is the syntax underneath for the placeholders, the variables, the markup,", "tokens": [51372, 500, 380, 643, 281, 458, 437, 307, 264, 28431, 7223, 337, 264, 1081, 12916, 11, 264, 9102, 11, 264, 1491, 1010, 11, 51708], "temperature": 0.0, "avg_logprob": -0.08315462957728993, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.004526369273662567}, {"id": 100, "seek": 73104, "start": 731.04, "end": 738.4, "text": " anything else, but they can be presented the same thing for all syntaxes, which might make things", "tokens": [50364, 1340, 1646, 11, 457, 436, 393, 312, 8212, 264, 912, 551, 337, 439, 28431, 279, 11, 597, 1062, 652, 721, 50732], "temperature": 0.0, "avg_logprob": -0.08924736847748628, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.003374311840161681}, {"id": 101, "seek": 73104, "start": 738.4, "end": 746.88, "text": " a little bit easier for everyone. So those are the ideas I came up with here for what could be", "tokens": [50732, 257, 707, 857, 3571, 337, 1518, 13, 407, 729, 366, 264, 3487, 286, 1361, 493, 365, 510, 337, 437, 727, 312, 51156], "temperature": 0.0, "avg_logprob": -0.08924736847748628, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.003374311840161681}, {"id": 102, "seek": 73104, "start": 746.88, "end": 752.8, "text": " the next steps from here, but I'm here saying, hey, this is a cool thing. You guys should play around", "tokens": [51156, 264, 958, 4439, 490, 510, 11, 457, 286, 478, 510, 1566, 11, 4177, 11, 341, 307, 257, 1627, 551, 13, 509, 1074, 820, 862, 926, 51452], "temperature": 0.0, "avg_logprob": -0.08924736847748628, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.003374311840161681}, {"id": 103, "seek": 75280, "start": 752.8, "end": 763.5999999999999, "text": " with it. For us, the current and ongoing work is to extend this sort of a definition to also", "tokens": [50364, 365, 309, 13, 1171, 505, 11, 264, 2190, 293, 10452, 589, 307, 281, 10101, 341, 1333, 295, 257, 7123, 281, 611, 50904], "temperature": 0.0, "avg_logprob": -0.10478301220629589, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.03547394275665283}, {"id": 104, "seek": 75280, "start": 763.5999999999999, "end": 770.16, "text": " include method resources and also include the sort of comments and metadata that is quite essential", "tokens": [50904, 4090, 3170, 3593, 293, 611, 4090, 264, 1333, 295, 3053, 293, 26603, 300, 307, 1596, 7115, 51232], "temperature": 0.0, "avg_logprob": -0.10478301220629589, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.03547394275665283}, {"id": 105, "seek": 75280, "start": 770.16, "end": 776.0799999999999, "text": " for communicating the context of a message to translation, which as I'm kind of hoping some", "tokens": [51232, 337, 17559, 264, 4319, 295, 257, 3636, 281, 12853, 11, 597, 382, 286, 478, 733, 295, 7159, 512, 51528], "temperature": 0.0, "avg_logprob": -0.10478301220629589, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.03547394275665283}, {"id": 106, "seek": 75280, "start": 776.0799999999999, "end": 782.0799999999999, "text": " of you noticed was completely left out of the earlier. But that's intentionally so that we", "tokens": [51528, 295, 291, 5694, 390, 2584, 1411, 484, 295, 264, 3071, 13, 583, 300, 311, 22062, 370, 300, 321, 51828], "temperature": 0.0, "avg_logprob": -0.10478301220629589, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.03547394275665283}, {"id": 107, "seek": 78208, "start": 782.08, "end": 789.76, "text": " can separate these considerations from each other. But that's it for me. Thank you very much", "tokens": [50364, 393, 4994, 613, 24070, 490, 1184, 661, 13, 583, 300, 311, 309, 337, 385, 13, 1044, 291, 588, 709, 50748], "temperature": 0.0, "avg_logprob": -0.2670297384262085, "compression_ratio": 1.5392156862745099, "no_speech_prob": 0.0009093996486626565}, {"id": 108, "seek": 78208, "start": 789.76, "end": 794.32, "text": " for listening. I'd be very happy to have any questions, comments.", "tokens": [50748, 337, 4764, 13, 286, 1116, 312, 588, 2055, 281, 362, 604, 1651, 11, 3053, 13, 50976], "temperature": 0.0, "avg_logprob": -0.2670297384262085, "compression_ratio": 1.5392156862745099, "no_speech_prob": 0.0009093996486626565}, {"id": 109, "seek": 78208, "start": 797.5200000000001, "end": 802.08, "text": " In another talk, I heard about message format 2 and function invocations.", "tokens": [51136, 682, 1071, 751, 11, 286, 2198, 466, 3636, 7877, 568, 293, 2445, 1048, 905, 763, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2670297384262085, "compression_ratio": 1.5392156862745099, "no_speech_prob": 0.0009093996486626565}, {"id": 110, "seek": 78208, "start": 802.08, "end": 806.32, "text": " How do function invocations, how does the data model work, or how do they relate?", "tokens": [51364, 1012, 360, 2445, 1048, 905, 763, 11, 577, 775, 264, 1412, 2316, 589, 11, 420, 577, 360, 436, 10961, 30, 51576], "temperature": 0.0, "avg_logprob": -0.2670297384262085, "compression_ratio": 1.5392156862745099, "no_speech_prob": 0.0009093996486626565}, {"id": 111, "seek": 80632, "start": 807.0400000000001, "end": 817.0400000000001, "text": " The question is for how do function invocations relate to all of this? And this, yes, they are", "tokens": [50400, 440, 1168, 307, 337, 577, 360, 2445, 1048, 905, 763, 10961, 281, 439, 295, 341, 30, 400, 341, 11, 2086, 11, 436, 366, 50900], "temperature": 0.0, "avg_logprob": -0.18188369020502618, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.0020140325650572777}, {"id": 112, "seek": 80632, "start": 817.0400000000001, "end": 828.96, "text": " represented here in the function annotations here. So something like, for example, plural selection", "tokens": [50900, 10379, 510, 294, 264, 2445, 25339, 763, 510, 13, 407, 746, 411, 11, 337, 1365, 11, 25377, 9450, 51496], "temperature": 0.0, "avg_logprob": -0.18188369020502618, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.0020140325650572777}, {"id": 113, "seek": 82896, "start": 829.6, "end": 838.24, "text": " could use a function with a name of plural, for instance, for being this element existing", "tokens": [50396, 727, 764, 257, 2445, 365, 257, 1315, 295, 25377, 11, 337, 5197, 11, 337, 885, 341, 4478, 6741, 50828], "temperature": 0.0, "avg_logprob": -0.12132316165500218, "compression_ratio": 1.3611111111111112, "no_speech_prob": 0.08006749302148819}, {"id": 114, "seek": 82896, "start": 838.88, "end": 846.8000000000001, "text": " in a select message, selector expression, which is there.", "tokens": [50860, 294, 257, 3048, 3636, 11, 23264, 1672, 6114, 11, 597, 307, 456, 13, 51256], "temperature": 0.0, "avg_logprob": -0.12132316165500218, "compression_ratio": 1.3611111111111112, "no_speech_prob": 0.08006749302148819}, {"id": 115, "seek": 85896, "start": 859.44, "end": 864.0, "text": " Question was whether there are a set of built-in functions that are supported. And message", "tokens": [50388, 14464, 390, 1968, 456, 366, 257, 992, 295, 3094, 12, 259, 6828, 300, 366, 8104, 13, 400, 3636, 50616], "temperature": 0.0, "avg_logprob": -0.11587247183156568, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.050943147391080856}, {"id": 116, "seek": 85896, "start": 864.0, "end": 872.96, "text": " format 2 does come with a relatively small set of built-in functions. The data model itself does", "tokens": [50616, 7877, 568, 775, 808, 365, 257, 7226, 1359, 992, 295, 3094, 12, 259, 6828, 13, 440, 1412, 2316, 2564, 775, 51064], "temperature": 0.0, "avg_logprob": -0.11587247183156568, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.050943147391080856}, {"id": 117, "seek": 85896, "start": 872.96, "end": 881.76, "text": " not presume this set absolutely. But the set of functions can be extended. For message format 2", "tokens": [51064, 406, 43283, 341, 992, 3122, 13, 583, 264, 992, 295, 6828, 393, 312, 10913, 13, 1171, 3636, 7877, 568, 51504], "temperature": 0.0, "avg_logprob": -0.11587247183156568, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.050943147391080856}, {"id": 118, "seek": 85896, "start": 881.76, "end": 888.0, "text": " in particular, we are looking at a very minimum of effectively number, which also does plural and", "tokens": [51504, 294, 1729, 11, 321, 366, 1237, 412, 257, 588, 7285, 295, 8659, 1230, 11, 597, 611, 775, 25377, 293, 51816], "temperature": 0.0, "avg_logprob": -0.11587247183156568, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.050943147391080856}, {"id": 119, "seek": 88800, "start": 888.0, "end": 896.16, "text": " ordinal selection, but also acts as a formatter. And then string, which is a sort of default", "tokens": [50364, 4792, 2071, 9450, 11, 457, 611, 10672, 382, 257, 1254, 1161, 13, 400, 550, 6798, 11, 597, 307, 257, 1333, 295, 7576, 50772], "temperature": 0.0, "avg_logprob": -0.11787358645735116, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.00482269749045372}, {"id": 120, "seek": 88800, "start": 896.16, "end": 901.92, "text": " for string formatting, but also does the same sort of selection as ICU message format select does.", "tokens": [50772, 337, 6798, 39366, 11, 457, 611, 775, 264, 912, 1333, 295, 9450, 382, 38123, 3636, 7877, 3048, 775, 13, 51060], "temperature": 0.0, "avg_logprob": -0.11787358645735116, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.00482269749045372}, {"id": 121, "seek": 88800, "start": 903.76, "end": 910.16, "text": " And we are still discussing for message format 2 what other things to include here. Now, of course,", "tokens": [51152, 400, 321, 366, 920, 10850, 337, 3636, 7877, 568, 437, 661, 721, 281, 4090, 510, 13, 823, 11, 295, 1164, 11, 51472], "temperature": 0.0, "avg_logprob": -0.11787358645735116, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.00482269749045372}, {"id": 122, "seek": 88800, "start": 910.72, "end": 917.44, "text": " when representing messages coming from some completely other syntax, it is entirely conceivable", "tokens": [51500, 562, 13460, 7897, 1348, 490, 512, 2584, 661, 28431, 11, 309, 307, 7696, 10413, 34376, 51836], "temperature": 0.0, "avg_logprob": -0.11787358645735116, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.00482269749045372}, {"id": 123, "seek": 91744, "start": 917.44, "end": 923.6, "text": " that it is not directly possible to express these messages using the functions that message", "tokens": [50364, 300, 309, 307, 406, 3838, 1944, 281, 5109, 613, 7897, 1228, 264, 6828, 300, 3636, 50672], "temperature": 0.0, "avg_logprob": -0.06861576476654449, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009831706993281841}, {"id": 124, "seek": 91744, "start": 923.6, "end": 931.84, "text": " format 2 defines out of the box. But the data model does allow for you to define that a function", "tokens": [50672, 7877, 568, 23122, 484, 295, 264, 2424, 13, 583, 264, 1412, 2316, 775, 2089, 337, 291, 281, 6964, 300, 257, 2445, 51084], "temperature": 0.0, "avg_logprob": -0.06861576476654449, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009831706993281841}, {"id": 125, "seek": 91744, "start": 931.84, "end": 938.08, "text": " like this must be used here, and you can otherwise define how that function works,", "tokens": [51084, 411, 341, 1633, 312, 1143, 510, 11, 293, 291, 393, 5911, 6964, 577, 300, 2445, 1985, 11, 51396], "temperature": 0.0, "avg_logprob": -0.06861576476654449, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009831706993281841}, {"id": 126, "seek": 91744, "start": 938.8800000000001, "end": 942.96, "text": " if that makes sense. And it's possible to make translations between these", "tokens": [51436, 498, 300, 1669, 2020, 13, 400, 309, 311, 1944, 281, 652, 37578, 1296, 613, 51640], "temperature": 0.0, "avg_logprob": -0.06861576476654449, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009831706993281841}, {"id": 127, "seek": 94296, "start": 943.0400000000001, "end": 949.2, "text": " function meanings. Anything more?", "tokens": [50368, 2445, 28138, 13, 11998, 544, 30, 50676], "temperature": 0.0, "avg_logprob": -0.3579319885798863, "compression_ratio": 1.1376146788990826, "no_speech_prob": 0.0061651268042624}, {"id": 128, "seek": 94296, "start": 962.0, "end": 972.4000000000001, "text": " The reason to separate context from the minimum required effectively, and here I'm jumping", "tokens": [51316, 440, 1778, 281, 4994, 4319, 490, 264, 7285, 4739, 8659, 11, 293, 510, 286, 478, 11233, 51836], "temperature": 0.0, "avg_logprob": -0.3579319885798863, "compression_ratio": 1.1376146788990826, "no_speech_prob": 0.0061651268042624}, {"id": 129, "seek": 97240, "start": 972.4, "end": 977.76, "text": " into the answer here, the minimum required for formatting a message is that the context is", "tokens": [50364, 666, 264, 1867, 510, 11, 264, 7285, 4739, 337, 39366, 257, 3636, 307, 300, 264, 4319, 307, 50632], "temperature": 0.0, "avg_logprob": -0.08502073066179143, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.025469541549682617}, {"id": 130, "seek": 97240, "start": 977.76, "end": 984.8, "text": " absolutely required for the translation work. But the context is not absolutely required for", "tokens": [50632, 3122, 4739, 337, 264, 12853, 589, 13, 583, 264, 4319, 307, 406, 3122, 4739, 337, 50984], "temperature": 0.0, "avg_logprob": -0.08502073066179143, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.025469541549682617}, {"id": 131, "seek": 97240, "start": 984.8, "end": 994.0799999999999, "text": " the message formatting. So we need to be able to represent it, but we do not absolutely need to have", "tokens": [50984, 264, 3636, 39366, 13, 407, 321, 643, 281, 312, 1075, 281, 2906, 309, 11, 457, 321, 360, 406, 3122, 643, 281, 362, 51448], "temperature": 0.0, "avg_logprob": -0.08502073066179143, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.025469541549682617}, {"id": 132, "seek": 97240, "start": 994.0799999999999, "end": 1000.8, "text": " it be a part of the message itself when it is being formatted. And this is why we are dealing", "tokens": [51448, 309, 312, 257, 644, 295, 264, 3636, 2564, 562, 309, 307, 885, 1254, 32509, 13, 400, 341, 307, 983, 321, 366, 6260, 51784], "temperature": 0.0, "avg_logprob": -0.08502073066179143, "compression_ratio": 1.8805970149253732, "no_speech_prob": 0.025469541549682617}, {"id": 133, "seek": 100080, "start": 1000.88, "end": 1008.0, "text": " with it slightly separately. They are very much related concerns, but we've tried to find with", "tokens": [50368, 365, 309, 4748, 14759, 13, 814, 366, 588, 709, 4077, 7389, 11, 457, 321, 600, 3031, 281, 915, 365, 50724], "temperature": 0.0, "avg_logprob": -0.13620163737863733, "compression_ratio": 1.5103092783505154, "no_speech_prob": 0.0027838246896862984}, {"id": 134, "seek": 100080, "start": 1009.12, "end": 1016.88, "text": " the data model the minimum required for representing a message. And when you trim down the minimum,", "tokens": [50780, 264, 1412, 2316, 264, 7285, 4739, 337, 13460, 257, 3636, 13, 400, 562, 291, 10445, 760, 264, 7285, 11, 51168], "temperature": 0.0, "avg_logprob": -0.13620163737863733, "compression_ratio": 1.5103092783505154, "no_speech_prob": 0.0027838246896862984}, {"id": 135, "seek": 100080, "start": 1016.88, "end": 1022.56, "text": " the context kind of ends up as a thing we can define externally, so we've chosen to be doing that.", "tokens": [51168, 264, 4319, 733, 295, 5314, 493, 382, 257, 551, 321, 393, 6964, 40899, 11, 370, 321, 600, 8614, 281, 312, 884, 300, 13, 51452], "temperature": 0.0, "avg_logprob": -0.13620163737863733, "compression_ratio": 1.5103092783505154, "no_speech_prob": 0.0027838246896862984}, {"id": 136, "seek": 102256, "start": 1023.1199999999999, "end": 1026.08, "text": " And I mean if you're interested in that, in particular", "tokens": [50392, 400, 286, 914, 498, 291, 434, 3102, 294, 300, 11, 294, 1729, 50540], "temperature": 0.0, "avg_logprob": -0.1201217770576477, "compression_ratio": 1.4715909090909092, "no_speech_prob": 0.014791762456297874}, {"id": 137, "seek": 102256, "start": 1028.48, "end": 1034.72, "text": " the specifics of what should we include in the sort of base set of", "tokens": [50660, 264, 28454, 295, 437, 820, 321, 4090, 294, 264, 1333, 295, 3096, 992, 295, 50972], "temperature": 0.0, "avg_logprob": -0.1201217770576477, "compression_ratio": 1.4715909090909092, "no_speech_prob": 0.014791762456297874}, {"id": 138, "seek": 102256, "start": 1036.1599999999999, "end": 1042.96, "text": " metadata and context fields, here's an issue link where we're discussing this right now", "tokens": [51044, 26603, 293, 4319, 7909, 11, 510, 311, 364, 2734, 2113, 689, 321, 434, 10850, 341, 558, 586, 51384], "temperature": 0.0, "avg_logprob": -0.1201217770576477, "compression_ratio": 1.4715909090909092, "no_speech_prob": 0.014791762456297874}, {"id": 139, "seek": 102256, "start": 1043.76, "end": 1046.3999999999999, "text": " that I would be very happy to have your input on.", "tokens": [51424, 300, 286, 576, 312, 588, 2055, 281, 362, 428, 4846, 322, 13, 51556], "temperature": 0.0, "avg_logprob": -0.1201217770576477, "compression_ratio": 1.4715909090909092, "no_speech_prob": 0.014791762456297874}, {"id": 140, "seek": 105256, "start": 1052.6399999999999, "end": 1053.2, "text": " Anything more?", "tokens": [50368, 11998, 544, 30, 50396], "temperature": 0.0, "avg_logprob": -0.26097579229445683, "compression_ratio": 1.7784810126582278, "no_speech_prob": 0.018696650862693787}, {"id": 141, "seek": 105256, "start": 1055.44, "end": 1063.6, "text": " Regarding the edit the translator tools, so now most translator tools", "tokens": [50508, 35523, 264, 8129, 264, 35223, 3873, 11, 370, 586, 881, 35223, 3873, 50916], "temperature": 0.0, "avg_logprob": -0.26097579229445683, "compression_ratio": 1.7784810126582278, "no_speech_prob": 0.018696650862693787}, {"id": 142, "seek": 105256, "start": 1064.6399999999999, "end": 1071.2, "text": " present a string and expect that the translator will write in a string. Do you imagine that this", "tokens": [50968, 1974, 257, 6798, 293, 2066, 300, 264, 35223, 486, 2464, 294, 257, 6798, 13, 1144, 291, 3811, 300, 341, 51296], "temperature": 0.0, "avg_logprob": -0.26097579229445683, "compression_ratio": 1.7784810126582278, "no_speech_prob": 0.018696650862693787}, {"id": 143, "seek": 105256, "start": 1071.2, "end": 1079.52, "text": " will change and that the translator will see the elements of the data model in a more graphical way", "tokens": [51296, 486, 1319, 293, 300, 264, 35223, 486, 536, 264, 4959, 295, 264, 1412, 2316, 294, 257, 544, 35942, 636, 51712], "temperature": 0.0, "avg_logprob": -0.26097579229445683, "compression_ratio": 1.7784810126582278, "no_speech_prob": 0.018696650862693787}, {"id": 144, "seek": 107952, "start": 1080.48, "end": 1088.8, "text": " and choose translations through Google boxes, or something like that? Or do you think it will", "tokens": [50412, 293, 2826, 37578, 807, 3329, 9002, 11, 420, 746, 411, 300, 30, 1610, 360, 291, 519, 309, 486, 50828], "temperature": 0.0, "avg_logprob": -0.36781994501749676, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.002522160764783621}, {"id": 145, "seek": 107952, "start": 1089.44, "end": 1093.36, "text": " stay as a string representation for the translators in the future?", "tokens": [50860, 1754, 382, 257, 6798, 10290, 337, 264, 5105, 3391, 294, 264, 2027, 30, 51056], "temperature": 0.0, "avg_logprob": -0.36781994501749676, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.002522160764783621}, {"id": 146, "seek": 107952, "start": 1093.92, "end": 1101.36, "text": " I have no idea and anything is possible and that's kind of cool. So predicting the future of what", "tokens": [51084, 286, 362, 572, 1558, 293, 1340, 307, 1944, 293, 300, 311, 733, 295, 1627, 13, 407, 32884, 264, 2027, 295, 437, 51456], "temperature": 0.0, "avg_logprob": -0.36781994501749676, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.002522160764783621}, {"id": 147, "seek": 110136, "start": 1101.36, "end": 1107.36, "text": " the translator experience is going to be here is shall we say a hard question.", "tokens": [50364, 264, 35223, 1752, 307, 516, 281, 312, 510, 307, 4393, 321, 584, 257, 1152, 1168, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12327569325764974, "compression_ratio": 1.5647058823529412, "no_speech_prob": 0.0022790601942688227}, {"id": 148, "seek": 110136, "start": 1109.28, "end": 1116.08, "text": " One thing I do think is that this sort of a data model makes it easier to build tools that", "tokens": [50760, 1485, 551, 286, 360, 519, 307, 300, 341, 1333, 295, 257, 1412, 2316, 1669, 309, 3571, 281, 1322, 3873, 300, 51100], "temperature": 0.0, "avg_logprob": -0.12327569325764974, "compression_ratio": 1.5647058823529412, "no_speech_prob": 0.0022790601942688227}, {"id": 149, "seek": 110136, "start": 1119.12, "end": 1125.76, "text": " can present to a translator more the options and opportunities that they might have in modifying", "tokens": [51252, 393, 1974, 281, 257, 35223, 544, 264, 3956, 293, 4786, 300, 436, 1062, 362, 294, 42626, 51584], "temperature": 0.0, "avg_logprob": -0.12327569325764974, "compression_ratio": 1.5647058823529412, "no_speech_prob": 0.0022790601942688227}, {"id": 150, "seek": 112576, "start": 1125.76, "end": 1132.96, "text": " a message and content like placeholders and markup which might just show up a syntax when", "tokens": [50364, 257, 3636, 293, 2701, 411, 1081, 12916, 293, 1491, 1010, 597, 1062, 445, 855, 493, 257, 28431, 562, 50724], "temperature": 0.0, "avg_logprob": -0.10943642529574307, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.16553643345832825}, {"id": 151, "seek": 112576, "start": 1132.96, "end": 1139.68, "text": " presented a string and be a challenge to even realize that I could change how this bit of it is", "tokens": [50724, 8212, 257, 6798, 293, 312, 257, 3430, 281, 754, 4325, 300, 286, 727, 1319, 577, 341, 857, 295, 309, 307, 51060], "temperature": 0.0, "avg_logprob": -0.10943642529574307, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.16553643345832825}, {"id": 152, "seek": 112576, "start": 1140.24, "end": 1146.8, "text": " styled. But if we can present interfaces that can read the data model and understand from this", "tokens": [51088, 7952, 1493, 13, 583, 498, 321, 393, 1974, 28416, 300, 393, 1401, 264, 1412, 2316, 293, 1223, 490, 341, 51416], "temperature": 0.0, "avg_logprob": -0.10943642529574307, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.16553643345832825}, {"id": 153, "seek": 112576, "start": 1146.8, "end": 1153.76, "text": " that hey hang on this could be tweaked this way, interfaces that are richer could be built. However", "tokens": [51416, 300, 4177, 3967, 322, 341, 727, 312, 6986, 7301, 341, 636, 11, 28416, 300, 366, 29021, 727, 312, 3094, 13, 2908, 51764], "temperature": 0.0, "avg_logprob": -0.10943642529574307, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.16553643345832825}, {"id": 154, "seek": 115376, "start": 1153.76, "end": 1161.6, "text": " of course we do need to keep in mind that such a vast majority of cases are just it's best represented", "tokens": [50364, 295, 1164, 321, 360, 643, 281, 1066, 294, 1575, 300, 1270, 257, 8369, 6286, 295, 3331, 366, 445, 309, 311, 1151, 10379, 50756], "temperature": 0.0, "avg_logprob": -0.10754114834230337, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0010627718875184655}, {"id": 155, "seek": 115376, "start": 1161.6, "end": 1168.32, "text": " as a pure string. So a majority of work is not going to change but the corner case is where it", "tokens": [50756, 382, 257, 6075, 6798, 13, 407, 257, 6286, 295, 589, 307, 406, 516, 281, 1319, 457, 264, 4538, 1389, 307, 689, 309, 51092], "temperature": 0.0, "avg_logprob": -0.10754114834230337, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0010627718875184655}, {"id": 156, "seek": 115376, "start": 1168.32, "end": 1174.32, "text": " gets interesting and challenging for those there might be opportunities to present messages in a", "tokens": [51092, 2170, 1880, 293, 7595, 337, 729, 456, 1062, 312, 4786, 281, 1974, 7897, 294, 257, 51392], "temperature": 0.0, "avg_logprob": -0.10754114834230337, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0010627718875184655}, {"id": 157, "seek": 117432, "start": 1174.32, "end": 1182.96, "text": " more translator friendly way. And one part of this I kind of skimmed over it was mentioned in the", "tokens": [50364, 544, 35223, 9208, 636, 13, 400, 472, 644, 295, 341, 286, 733, 295, 1110, 332, 1912, 670, 309, 390, 2835, 294, 264, 50796], "temperature": 0.0, "avg_logprob": -0.21673891123603373, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.022899163886904716}, {"id": 158, "seek": 117432, "start": 1184.48, "end": 1190.8799999999999, "text": " Ujjwelts presentation yesterday on message format too is that here the selection for variants is", "tokens": [50872, 624, 73, 73, 45512, 1373, 5860, 5186, 322, 3636, 7877, 886, 307, 300, 510, 264, 9450, 337, 21669, 307, 51192], "temperature": 0.0, "avg_logprob": -0.21673891123603373, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.022899163886904716}, {"id": 159, "seek": 117432, "start": 1190.8799999999999, "end": 1198.3999999999999, "text": " not an inline component as it is for example in ICU message format or fluent but the variants", "tokens": [51192, 406, 364, 294, 1889, 6542, 382, 309, 307, 337, 1365, 294, 38123, 3636, 7877, 420, 40799, 457, 264, 21669, 51568], "temperature": 0.0, "avg_logprob": -0.21673891123603373, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.022899163886904716}, {"id": 160, "seek": 119840, "start": 1198.4, "end": 1205.68, "text": " all of the variants need to be complete messages presented at the sort of top level of the whole", "tokens": [50364, 439, 295, 264, 21669, 643, 281, 312, 3566, 7897, 8212, 412, 264, 1333, 295, 1192, 1496, 295, 264, 1379, 50728], "temperature": 0.0, "avg_logprob": -0.07720743317201913, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.005883588921278715}, {"id": 161, "seek": 119840, "start": 1205.68, "end": 1214.4, "text": " message which is entirely intended to guide towards structures that are easier for translators to", "tokens": [50728, 3636, 597, 307, 7696, 10226, 281, 5934, 3030, 9227, 300, 366, 3571, 337, 5105, 3391, 281, 51164], "temperature": 0.0, "avg_logprob": -0.07720743317201913, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.005883588921278715}, {"id": 162, "seek": 119840, "start": 1214.4, "end": 1219.3600000000001, "text": " deal with rather than needing to to figure out you have and then a selector of apples.", "tokens": [51164, 2028, 365, 2831, 813, 18006, 281, 281, 2573, 484, 291, 362, 293, 550, 257, 23264, 1672, 295, 16814, 13, 51412], "temperature": 0.0, "avg_logprob": -0.07720743317201913, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.005883588921278715}, {"id": 163, "seek": 119840, "start": 1221.68, "end": 1227.8400000000001, "text": " Instead of that you have a selector which has you have one apple you have three apples and", "tokens": [51528, 7156, 295, 300, 291, 362, 257, 23264, 1672, 597, 575, 291, 362, 472, 10606, 291, 362, 1045, 16814, 293, 51836], "temperature": 0.0, "avg_logprob": -0.07720743317201913, "compression_ratio": 1.8058252427184467, "no_speech_prob": 0.005883588921278715}, {"id": 164, "seek": 122784, "start": 1227.84, "end": 1238.3999999999999, "text": " this sort of an interface. But yeah anymore? If not I would like to thank you very much for your time", "tokens": [50364, 341, 1333, 295, 364, 9226, 13, 583, 1338, 3602, 30, 759, 406, 286, 576, 411, 281, 1309, 291, 588, 709, 337, 428, 565, 50892], "temperature": 0.0, "avg_logprob": -0.22770577006869847, "compression_ratio": 1.2075471698113207, "no_speech_prob": 0.002668481320142746}, {"id": 165, "seek": 122784, "start": 1239.36, "end": 1248.08, "text": " and yeah that's it for me.", "tokens": [50940, 293, 1338, 300, 311, 309, 337, 385, 13, 51376], "temperature": 0.0, "avg_logprob": -0.22770577006869847, "compression_ratio": 1.2075471698113207, "no_speech_prob": 0.002668481320142746}], "language": "en"}