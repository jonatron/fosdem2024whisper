{"text": " We'll be starting our next talk here from Tawana Czelec. She lives in Amsterdam, but she's from Istanbul. She loves historical fiction and free diving. Surprisingly, she's dived 25 meters down to save her GoPro before, which we got a bunch of asses this year. This is pretty crazy. So thank you so much and take it away. Alright, thank you. I hope, let me know if I need to eat the mic. Alright, so this particular talk is a bit of an outlier to the talks I usually give because it's nearly fully a showcase of a very simple actually project that I built with some community members of Haystack and some functionalities of Haystack that made this project possible. And then I'll end up by showing a few other projects that we built together. And the way it usually goes with the Haystack community, so a quick side note, I work for Deepset, which is the company behind the open source project Haystack. And we have a Discord server, and from time to time, this is basically what happens. So this evening I say, I'm a bit bored, I want to do something, and I go and join a voice channel on our Discord, and there's one particular community member that I have to give a shout out to for this particular talk, Rec, because oftentimes Rec will then come up with a random idea, and either myself or the two of us will just share screens and do some pair coding, hack together something. And this particular project is exactly something that happened like this. I'm pretty sure a lot of you know this page, it's Hack and Use, there's a lot, it changes a lot. So the idea that Rec came up with was, well, why don't we build something very simple that gives you like a TLDR of the top K Hack and Use articles. So we built that, and then just recently, and when I say recently like two days ago, that became a hugging face space that you can actually get to at this QR code. And we've kind of vamped it up a bit, and we can now pick between two models. You can use MixTrail or an OpenAI GPT-4 model, and then provide a number of the top something, and I make it go up to five because no one's made of money, and you're going to be making APR calls to OpenAI possibly, so it goes up to five, and this is literally when I ran it yesterday, and the funny thing about this one was that at the time, the second top article was actually the FOSTA and Livestreams, but you get like a short summary of what the top three articles are at this point with a URL to get to the full article itself. So my whole talk is based on how this was made possible, how we actually built this project, and we built it with Haystack. Haystack is a fully open source, large language model framework. It's all written in Python. The main ideas behind Haystack is providing tooling for developers, so nothing is a plug-and-play really, you're building it all yourself, and the two main structures in Haystack that make it possible are called pipelines and components, and a pipeline is made up of a few components attached to each other where every component is forwarding some data to the next one. And I'm not going to get into what RAG is, I'm pretty sure a lot of you know what RAG is at this point, but a retrieval augmented generative pipeline might look a bit like this. You have a query, and then an embedder component is creating an embedding for that query, then a retriever component is retrieving the most relevant context for your LLM to actually use, and then it's forwarding that to what in Haystack world we call a prompt builder, so that context gets embedded into your prompt itself, and then you use the generator, and that can be any model, open source model of a hugging phase, or open AI, etc. And then you get an answer. This is a pipeline, but what a pipeline does is really dictated by what components it's comprised of. You might also create a pipeline that indexes documents. I'm not really going to get into this here, you're just basically fetching the contents of a URL, and then writing that into one of the available document stores that we have. But this is all made possible because of the structure called a component, and a component in Haystack is something that could have, for example, in this case it has two inputs and an output, but you don't necessarily have to have a defined output. Haystack doesn't really make assumptions as to what a component has to be. It can also be something that has two inputs, two outputs, and then the idea is you attach those components to each other, and you can be very, very specific here. You can be specific into saying, like, I want output one to be forwarded to input two of the next component. You can be very, very precise here. And maybe you can already start to see this starts to look quite like a graph. So how do we build these components? There's only a few requirements for something to be a component in Haystack world. We provide a bunch of ready-made components that, if you go to the Haystack documentation, you'll see a bunch of sections there, generators, converters, embedders, etc. These are all basically components that have been built exactly like this that we just provide in the package itself. But what you can do is build your own. And what you need to have is a class. So here I've just got a very, very well-named MyCustomComponent class. I need a run function. And the other thing you need are these decorators. So basically this is telling Haystack that this class is a component. And then the second one is around the run function. And this is actually used for pipeline validation down the line. But it's basically telling the Haystack pipeline what outputs it should expect from this component. In this scenario, I've got a MyCustomComponent that's expecting a query, which is supposed to be a string, and then it's returning documents. In this case, it's just hard-coded. It returns high and by. So we know that this, whatever query this gets, is going to be returning two documents, high and by. And this has then led to quite a bunch of components that don't actually, they're not served through the Haystack framework itself, not all of them are, but you can just install them as separate packages. And it's meant community members have just gone ahead and built components that they need for their very specific custom needs and made them available to the rest of the Haystack community. So let's come to our Hack and Use TLDR, if you will, project. The idea was that we wanted a component that would take top K, that could be a number, and it would return articles. And again, this is a colab that you can use, it should be running. And the way we did that was, this is very much pseudocode. Later, if we have time, I'm going to show you the actual code. But we built this component called Hack and Use Fetcher. It takes top K, it queries the Hack and Use API and gets the top whatever number we've decided. And the other thing I wanted to show here is, at the end, I don't know how well you can see it, but we've also added some meta information, because down the line, we can use meta information in our prompt, because you also get titles of the Hack and Use articles, you also get URLs, which is great for referencing down the line too. So we return full documents that have the content, the title, and the URLs of each Hack and Use articles that we fetched. And at the end of the day, we're going to be building a pipeline that looks like this, and everything you see in green is already provided with Haystack. So that came with pip install Haystack AI anyway. And the orange is what we've just built for ourselves, and it just slots into the rest of the Haystack pipeline ecosystem. And for this co-lab that I've shared with everyone here is, I've decided I'd just go ahead and use Mixstrahl, I've tried this with OpenAI models a lot, so why not try something new? And then the last thing I want to highlight about this particular pipeline is how the prompt is being built. So prompt templating happens in Haystack world by a component called the prompt builder. And templates use Ginger templating. And what's really important here is, okay, we have an instruction, you'll be provided with one or more hack and use articles, please provide summaries. But if you look at this close theme, we have actually a for loop. So this prompt builder automatically knows that it should be expecting an input called articles, and it can loop through those articles, and then it can access the contents of that article object individually in every step of that for loop. And that's how we're embedding URL here as well. And this is the final product. At the end of the day, we were able to build a pipeline where, given top three, we were able to run it, and we've got the TLDR summary and the URLs that you can find, the full articles of hack and use, current hack and use top articles. So with that, I want to show a few other projects that this custom component building functionality has enabled. The next one is slightly questionable. Please take it with a pinch of salt. I put it everywhere on that tugging face space too. And the idea came from, at the time, Twitter was very different. So the idea was, could we like build a Twitter fetcher that, given a username, could give you, this is really bad, could give you like a vibe check of the account, and we called it like, should I follow? And it gets like the last, I think, 40 posts of that user. Obviously after that, Twitter changed, so I went ahead and built a master on fetcher. You can also find that on the Haystack integrations page. And the best way I like showcasing this is actually using my boyfriend's master on account, because every time this tells me something a bit funny about his account, once it called him pessimistic, this time it called him sarcastic when discussing personal opinions. So that's also open. I think I linked to it in the notes as well, so you can go ahead and try that out. You just need to provide the full master on username. Without the at at the front, that's a bug I haven't fixed yet. Another thing that this enabled, actually not only used the Haystack custom component functionality, but also, I don't know if you remember when I showed the components earlier with the two outputs and the two inputs, et cetera, you can already start to imagine that you can actually have these pipelines loop too. So the idea was, what if we have some meeting notes, and we have our own GitHub repository, and anyone who's used GitHub repositories, you know that you can create those issue labels that are very specific to that GitHub repository. Could we build a system that, given meeting notes, generates a list of GitHub issues specifically for that repository that you're discussing in that meeting? And could we actually then use those generated structured outputs to query GitHub to actually create those issues? Now, this is great, and our experience has been that actually a lot of large language models are great at generating structured output, but not necessarily in the structure that you need. So it's going to be JSON, but is it going to abide by what you need that JSON object to look like? So the idea here was, okay, well, why don't we create an output validator component, and we use Pydantic for that. This is all based on a tutorial that's up on the Haystack website right now, and basically what we did for the GitHub demo was modify this tutorial just a bit. In the tutorial, we said that we provide a Pydantic model, and we said we need the output to be cities data, where in cities data you've got cities, and each city has a name, a country, and a population. And then we used a GPT model, and we saw that initially, for the first round, we did get structured output, but it's not valid JSON, or it doesn't abide by what we need that object to look like. So the idea is, what if we provide back to the LLM, the output that it just gave us, with an error message from Pydantic as to why it's wrong, why it doesn't abide by the Pydantic model we just provided. So the resulting pipeline looks a bit like this for our GitHub issues demo. We want to provide meeting notes, and we want to provide a schema. We give that to the prompt builder, the prompt builder exists in Haystack World. Then that whole prompt is given to a generator that generates... There's a one pass, like a first attempt at generating some structured output, which is then validated by our output validator, which doesn't exist in Haystack World, so this is a custom component. And either you're all good, done. But if it's not good, then we go back to the prompt builder with the invalid reply that was produced, plus the error messages. So for our use case, where you were trying to build this for Haystack, this is not accurate, by the way, our labels are not exactly that, but just for demonstration purposes, we went ahead and built a Pydantic model called issues, and we had to be very specific as to what our labels were, because you can't make a query with a new label that doesn't belong to that repository. And then we used our output validator. And then this is where things start to look a bit complicated, but the ginger templating is very useful here. Earlier, for the Hacker News articles, you saw a for loop. Instead here, we have an if statement. So if we have an error message and invalid replies coming in from any component in our pipeline, then this little section here, you already created the following output, yada yada, is appended to the full prompt. Again, at the end of the day, we ended up with a pipeline that looks a bit like this. So do I have time? I do have time, right? Four minutes. Okay, so the last thing I wanted to show is how these pipeline connections are actually defined in Haystack. Oh, great. Okay, I have plenty of time. All right, so ignore the corgis running around. Can everyone see this, or should I make this bigger? Okay. All right, so I told you that before the Hacker News Fetcher component was very much pseudocode, this is kind of boring. We're basically making requests to the API and getting the articles. Here, we're going to be using mixed trial through hugging face TGI. Hugging face TGI is free, but it is rate limited, and you need to provide an API key to use it. So you can go ahead and use this collab, but you do have to provide an API key. And then you see the prompt template you saw, and here's what's going on in the Haystack pipeline itself. We've got our prompt builder. We've got our LLM, so mixed trial via hugging face TGI. We just created the Hacker News Fetcher. What we do is simply add those to the Haystack pipeline. And then this is where Haystack can be quite verbose, but it can also mean that you can be very, you can create very custom pipelines, and it can get a bit crazy. You can have pipelines that branch out, loop back in, et cetera. We're being very specific that Hacker News Fetcher's articles output is being provided to prompt builder articles, which is going in here. And then finally, the only thing missing to actually run this is Hacker News Fetcher is the only component here that is missing an input. All of the rest have been provided inputs through the pipeline itself. So I can then define what the input of Hacker News Fetcher is when I do pipeline.run or pipe.run. And then, optionally, you can also give more inputs that are not necessary for the others, but for example, here, I'm using mixed-rull, and I wanted to up the created max tokens at the end, so I can also provide that at runtime. And that's it. Thank you very much. And you can also access the GitHub issues pipelines here, but I'm happy to take questions if there are any. Thank you very much. Thank you. Thank you. Hi. So in the Hacker News article summarizer, to the LLM, the URL, and asking it to both summarize the article and also just print back the URL. That appears to me a bit risky because it might change the URL. Do you consider it best practice to pass the URL through some other way, or do you find it fine to always ask the LLM to do that? I love this question because try that hugging face space, especially with mixed trial a few times and sometimes you just won't get the URL. Yes, and there are a few ways to make this a lot better because actually the Hack and Use Fetcher component itself earlier is just an API called Hack and Use and you have the URL there. So probably the best practice here would be to have the LLM only produce summaries and the other component provide an output of the URLs that was used to produce those summaries because yes, my experiences a lot of the OpenAI GPT models do a great job of following that instruction, reference this specific URL, but this is very much LLM based and how that large language model expects to be prompted. Not every instruction works the same way with every model. Any other questions? Oh, this one. Thanks for the presentation. I have a question on the prompt. I saw for an if, is it specific to Haystack or? Not at all. We use Ginger for the templating language. Actually, I will add a link to the Ginger documentation in the speaker notes of this slide deck that you'll find on FOSM2, but that's all Ginger syntax, which comes very handy because you get for loops, if loops, and you can actually start defining your own custom functions for that Ginger templating as well. All right. Give her a round of applause, please.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.0, "text": " We'll be starting our next talk here from Tawana Czelec.", "tokens": [50364, 492, 603, 312, 2891, 527, 958, 751, 510, 490, 314, 1607, 2095, 383, 1381, 306, 66, 13, 50814], "temperature": 0.0, "avg_logprob": -0.4034375371159734, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.37815845012664795}, {"id": 1, "seek": 0, "start": 9.0, "end": 16.0, "text": " She lives in Amsterdam, but she's from Istanbul.", "tokens": [50814, 1240, 2909, 294, 28291, 11, 457, 750, 311, 490, 36340, 13, 51164], "temperature": 0.0, "avg_logprob": -0.4034375371159734, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.37815845012664795}, {"id": 2, "seek": 0, "start": 16.0, "end": 20.0, "text": " She loves historical fiction and free diving.", "tokens": [51164, 1240, 6752, 8584, 13266, 293, 1737, 20241, 13, 51364], "temperature": 0.0, "avg_logprob": -0.4034375371159734, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.37815845012664795}, {"id": 3, "seek": 0, "start": 20.0, "end": 26.0, "text": " Surprisingly, she's dived 25 meters down to save her GoPro before,", "tokens": [51364, 49908, 11, 750, 311, 274, 3194, 3552, 8146, 760, 281, 3155, 720, 30259, 949, 11, 51664], "temperature": 0.0, "avg_logprob": -0.4034375371159734, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.37815845012664795}, {"id": 4, "seek": 0, "start": 26.0, "end": 28.0, "text": " which we got a bunch of asses this year.", "tokens": [51664, 597, 321, 658, 257, 3840, 295, 1256, 279, 341, 1064, 13, 51764], "temperature": 0.0, "avg_logprob": -0.4034375371159734, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.37815845012664795}, {"id": 5, "seek": 2800, "start": 28.0, "end": 30.0, "text": " This is pretty crazy.", "tokens": [50364, 639, 307, 1238, 3219, 13, 50464], "temperature": 0.0, "avg_logprob": -0.14631471326274256, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.14366793632507324}, {"id": 6, "seek": 2800, "start": 30.0, "end": 32.0, "text": " So thank you so much and take it away.", "tokens": [50464, 407, 1309, 291, 370, 709, 293, 747, 309, 1314, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14631471326274256, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.14366793632507324}, {"id": 7, "seek": 2800, "start": 32.0, "end": 38.0, "text": " Alright, thank you.", "tokens": [50564, 2798, 11, 1309, 291, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14631471326274256, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.14366793632507324}, {"id": 8, "seek": 2800, "start": 38.0, "end": 41.0, "text": " I hope, let me know if I need to eat the mic.", "tokens": [50864, 286, 1454, 11, 718, 385, 458, 498, 286, 643, 281, 1862, 264, 3123, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14631471326274256, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.14366793632507324}, {"id": 9, "seek": 2800, "start": 41.0, "end": 47.0, "text": " Alright, so this particular talk is a bit of an outlier to the talks I usually give", "tokens": [51014, 2798, 11, 370, 341, 1729, 751, 307, 257, 857, 295, 364, 484, 2753, 281, 264, 6686, 286, 2673, 976, 51314], "temperature": 0.0, "avg_logprob": -0.14631471326274256, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.14366793632507324}, {"id": 10, "seek": 2800, "start": 47.0, "end": 52.0, "text": " because it's nearly fully a showcase of a very simple actually project", "tokens": [51314, 570, 309, 311, 6217, 4498, 257, 20388, 295, 257, 588, 2199, 767, 1716, 51564], "temperature": 0.0, "avg_logprob": -0.14631471326274256, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.14366793632507324}, {"id": 11, "seek": 2800, "start": 52.0, "end": 56.0, "text": " that I built with some community members of Haystack", "tokens": [51564, 300, 286, 3094, 365, 512, 1768, 2679, 295, 8721, 372, 501, 51764], "temperature": 0.0, "avg_logprob": -0.14631471326274256, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.14366793632507324}, {"id": 12, "seek": 5600, "start": 56.0, "end": 61.0, "text": " and some functionalities of Haystack that made this project possible.", "tokens": [50364, 293, 512, 11745, 1088, 295, 8721, 372, 501, 300, 1027, 341, 1716, 1944, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11641778197942995, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.05702337995171547}, {"id": 13, "seek": 5600, "start": 61.0, "end": 66.0, "text": " And then I'll end up by showing a few other projects that we built together.", "tokens": [50614, 400, 550, 286, 603, 917, 493, 538, 4099, 257, 1326, 661, 4455, 300, 321, 3094, 1214, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11641778197942995, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.05702337995171547}, {"id": 14, "seek": 5600, "start": 66.0, "end": 69.0, "text": " And the way it usually goes with the Haystack community,", "tokens": [50864, 400, 264, 636, 309, 2673, 1709, 365, 264, 8721, 372, 501, 1768, 11, 51014], "temperature": 0.0, "avg_logprob": -0.11641778197942995, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.05702337995171547}, {"id": 15, "seek": 5600, "start": 69.0, "end": 73.0, "text": " so a quick side note, I work for Deepset,", "tokens": [51014, 370, 257, 1702, 1252, 3637, 11, 286, 589, 337, 14895, 3854, 11, 51214], "temperature": 0.0, "avg_logprob": -0.11641778197942995, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.05702337995171547}, {"id": 16, "seek": 5600, "start": 73.0, "end": 76.0, "text": " which is the company behind the open source project Haystack.", "tokens": [51214, 597, 307, 264, 2237, 2261, 264, 1269, 4009, 1716, 8721, 372, 501, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11641778197942995, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.05702337995171547}, {"id": 17, "seek": 5600, "start": 76.0, "end": 82.0, "text": " And we have a Discord server, and from time to time, this is basically what happens.", "tokens": [51364, 400, 321, 362, 257, 32623, 7154, 11, 293, 490, 565, 281, 565, 11, 341, 307, 1936, 437, 2314, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11641778197942995, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.05702337995171547}, {"id": 18, "seek": 8200, "start": 82.0, "end": 86.0, "text": " So this evening I say, I'm a bit bored, I want to do something,", "tokens": [50364, 407, 341, 5634, 286, 584, 11, 286, 478, 257, 857, 13521, 11, 286, 528, 281, 360, 746, 11, 50564], "temperature": 0.0, "avg_logprob": -0.1163269709614874, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.2805367708206177}, {"id": 19, "seek": 8200, "start": 86.0, "end": 90.0, "text": " and I go and join a voice channel on our Discord,", "tokens": [50564, 293, 286, 352, 293, 3917, 257, 3177, 2269, 322, 527, 32623, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1163269709614874, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.2805367708206177}, {"id": 20, "seek": 8200, "start": 90.0, "end": 94.0, "text": " and there's one particular community member that I have to give a shout out to", "tokens": [50764, 293, 456, 311, 472, 1729, 1768, 4006, 300, 286, 362, 281, 976, 257, 8043, 484, 281, 50964], "temperature": 0.0, "avg_logprob": -0.1163269709614874, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.2805367708206177}, {"id": 21, "seek": 8200, "start": 94.0, "end": 97.0, "text": " for this particular talk, Rec,", "tokens": [50964, 337, 341, 1729, 751, 11, 9647, 11, 51114], "temperature": 0.0, "avg_logprob": -0.1163269709614874, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.2805367708206177}, {"id": 22, "seek": 8200, "start": 97.0, "end": 101.0, "text": " because oftentimes Rec will then come up with a random idea,", "tokens": [51114, 570, 18349, 9647, 486, 550, 808, 493, 365, 257, 4974, 1558, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1163269709614874, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.2805367708206177}, {"id": 23, "seek": 8200, "start": 101.0, "end": 105.0, "text": " and either myself or the two of us will just share screens", "tokens": [51314, 293, 2139, 2059, 420, 264, 732, 295, 505, 486, 445, 2073, 11171, 51514], "temperature": 0.0, "avg_logprob": -0.1163269709614874, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.2805367708206177}, {"id": 24, "seek": 8200, "start": 105.0, "end": 108.0, "text": " and do some pair coding, hack together something.", "tokens": [51514, 293, 360, 512, 6119, 17720, 11, 10339, 1214, 746, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1163269709614874, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.2805367708206177}, {"id": 25, "seek": 10800, "start": 108.0, "end": 114.0, "text": " And this particular project is exactly something that happened like this.", "tokens": [50364, 400, 341, 1729, 1716, 307, 2293, 746, 300, 2011, 411, 341, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13990729331970214, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.017232036218047142}, {"id": 26, "seek": 10800, "start": 114.0, "end": 119.0, "text": " I'm pretty sure a lot of you know this page, it's Hack and Use, there's a lot,", "tokens": [50664, 286, 478, 1238, 988, 257, 688, 295, 291, 458, 341, 3028, 11, 309, 311, 35170, 293, 8278, 11, 456, 311, 257, 688, 11, 50914], "temperature": 0.0, "avg_logprob": -0.13990729331970214, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.017232036218047142}, {"id": 27, "seek": 10800, "start": 119.0, "end": 121.0, "text": " it changes a lot.", "tokens": [50914, 309, 2962, 257, 688, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13990729331970214, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.017232036218047142}, {"id": 28, "seek": 10800, "start": 121.0, "end": 126.0, "text": " So the idea that Rec came up with was, well, why don't we build something", "tokens": [51014, 407, 264, 1558, 300, 9647, 1361, 493, 365, 390, 11, 731, 11, 983, 500, 380, 321, 1322, 746, 51264], "temperature": 0.0, "avg_logprob": -0.13990729331970214, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.017232036218047142}, {"id": 29, "seek": 10800, "start": 126.0, "end": 132.0, "text": " very simple that gives you like a TLDR of the top K Hack and Use articles.", "tokens": [51264, 588, 2199, 300, 2709, 291, 411, 257, 40277, 9301, 295, 264, 1192, 591, 35170, 293, 8278, 11290, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13990729331970214, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.017232036218047142}, {"id": 30, "seek": 10800, "start": 132.0, "end": 135.0, "text": " So we built that, and then just recently,", "tokens": [51564, 407, 321, 3094, 300, 11, 293, 550, 445, 3938, 11, 51714], "temperature": 0.0, "avg_logprob": -0.13990729331970214, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.017232036218047142}, {"id": 31, "seek": 13500, "start": 135.0, "end": 138.0, "text": " and when I say recently like two days ago,", "tokens": [50364, 293, 562, 286, 584, 3938, 411, 732, 1708, 2057, 11, 50514], "temperature": 0.0, "avg_logprob": -0.1335581513338311, "compression_ratio": 1.4, "no_speech_prob": 0.011857257224619389}, {"id": 32, "seek": 13500, "start": 138.0, "end": 144.0, "text": " that became a hugging face space that you can actually get to at this QR code.", "tokens": [50514, 300, 3062, 257, 41706, 1851, 1901, 300, 291, 393, 767, 483, 281, 412, 341, 32784, 3089, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1335581513338311, "compression_ratio": 1.4, "no_speech_prob": 0.011857257224619389}, {"id": 33, "seek": 13500, "start": 144.0, "end": 151.0, "text": " And we've kind of vamped it up a bit, and we can now pick between two models.", "tokens": [50814, 400, 321, 600, 733, 295, 20017, 292, 309, 493, 257, 857, 11, 293, 321, 393, 586, 1888, 1296, 732, 5245, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1335581513338311, "compression_ratio": 1.4, "no_speech_prob": 0.011857257224619389}, {"id": 34, "seek": 13500, "start": 151.0, "end": 155.0, "text": " You can use MixTrail or an OpenAI GPT-4 model,", "tokens": [51164, 509, 393, 764, 12769, 38971, 388, 420, 364, 7238, 48698, 26039, 51, 12, 19, 2316, 11, 51364], "temperature": 0.0, "avg_logprob": -0.1335581513338311, "compression_ratio": 1.4, "no_speech_prob": 0.011857257224619389}, {"id": 35, "seek": 13500, "start": 155.0, "end": 160.0, "text": " and then provide a number of the top something,", "tokens": [51364, 293, 550, 2893, 257, 1230, 295, 264, 1192, 746, 11, 51614], "temperature": 0.0, "avg_logprob": -0.1335581513338311, "compression_ratio": 1.4, "no_speech_prob": 0.011857257224619389}, {"id": 36, "seek": 16000, "start": 160.0, "end": 164.0, "text": " and I make it go up to five because no one's made of money,", "tokens": [50364, 293, 286, 652, 309, 352, 493, 281, 1732, 570, 572, 472, 311, 1027, 295, 1460, 11, 50564], "temperature": 0.0, "avg_logprob": -0.12342464825338569, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.026025382801890373}, {"id": 37, "seek": 16000, "start": 164.0, "end": 169.0, "text": " and you're going to be making APR calls to OpenAI possibly,", "tokens": [50564, 293, 291, 434, 516, 281, 312, 1455, 5372, 49, 5498, 281, 7238, 48698, 6264, 11, 50814], "temperature": 0.0, "avg_logprob": -0.12342464825338569, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.026025382801890373}, {"id": 38, "seek": 16000, "start": 169.0, "end": 173.0, "text": " so it goes up to five, and this is literally when I ran it yesterday,", "tokens": [50814, 370, 309, 1709, 493, 281, 1732, 11, 293, 341, 307, 3736, 562, 286, 5872, 309, 5186, 11, 51014], "temperature": 0.0, "avg_logprob": -0.12342464825338569, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.026025382801890373}, {"id": 39, "seek": 16000, "start": 173.0, "end": 176.0, "text": " and the funny thing about this one was that at the time,", "tokens": [51014, 293, 264, 4074, 551, 466, 341, 472, 390, 300, 412, 264, 565, 11, 51164], "temperature": 0.0, "avg_logprob": -0.12342464825338569, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.026025382801890373}, {"id": 40, "seek": 16000, "start": 176.0, "end": 179.0, "text": " the second top article was actually the FOSTA and Livestreams,", "tokens": [51164, 264, 1150, 1192, 7222, 390, 767, 264, 479, 4367, 8241, 293, 31738, 377, 1572, 82, 11, 51314], "temperature": 0.0, "avg_logprob": -0.12342464825338569, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.026025382801890373}, {"id": 41, "seek": 16000, "start": 179.0, "end": 184.0, "text": " but you get like a short summary of what the top three articles are at this point", "tokens": [51314, 457, 291, 483, 411, 257, 2099, 12691, 295, 437, 264, 1192, 1045, 11290, 366, 412, 341, 935, 51564], "temperature": 0.0, "avg_logprob": -0.12342464825338569, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.026025382801890373}, {"id": 42, "seek": 16000, "start": 184.0, "end": 189.0, "text": " with a URL to get to the full article itself.", "tokens": [51564, 365, 257, 12905, 281, 483, 281, 264, 1577, 7222, 2564, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12342464825338569, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.026025382801890373}, {"id": 43, "seek": 18900, "start": 189.0, "end": 193.0, "text": " So my whole talk is based on how this was made possible,", "tokens": [50364, 407, 452, 1379, 751, 307, 2361, 322, 577, 341, 390, 1027, 1944, 11, 50564], "temperature": 0.0, "avg_logprob": -0.07553046833385121, "compression_ratio": 1.62890625, "no_speech_prob": 0.001612310647033155}, {"id": 44, "seek": 18900, "start": 193.0, "end": 197.0, "text": " how we actually built this project, and we built it with Haystack.", "tokens": [50564, 577, 321, 767, 3094, 341, 1716, 11, 293, 321, 3094, 309, 365, 8721, 372, 501, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07553046833385121, "compression_ratio": 1.62890625, "no_speech_prob": 0.001612310647033155}, {"id": 45, "seek": 18900, "start": 197.0, "end": 201.0, "text": " Haystack is a fully open source, large language model framework.", "tokens": [50764, 8721, 372, 501, 307, 257, 4498, 1269, 4009, 11, 2416, 2856, 2316, 8388, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07553046833385121, "compression_ratio": 1.62890625, "no_speech_prob": 0.001612310647033155}, {"id": 46, "seek": 18900, "start": 201.0, "end": 203.0, "text": " It's all written in Python.", "tokens": [50964, 467, 311, 439, 3720, 294, 15329, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07553046833385121, "compression_ratio": 1.62890625, "no_speech_prob": 0.001612310647033155}, {"id": 47, "seek": 18900, "start": 203.0, "end": 207.0, "text": " The main ideas behind Haystack is providing tooling for developers,", "tokens": [51064, 440, 2135, 3487, 2261, 8721, 372, 501, 307, 6530, 46593, 337, 8849, 11, 51264], "temperature": 0.0, "avg_logprob": -0.07553046833385121, "compression_ratio": 1.62890625, "no_speech_prob": 0.001612310647033155}, {"id": 48, "seek": 18900, "start": 207.0, "end": 211.0, "text": " so nothing is a plug-and-play really, you're building it all yourself,", "tokens": [51264, 370, 1825, 307, 257, 5452, 12, 474, 12, 2858, 534, 11, 291, 434, 2390, 309, 439, 1803, 11, 51464], "temperature": 0.0, "avg_logprob": -0.07553046833385121, "compression_ratio": 1.62890625, "no_speech_prob": 0.001612310647033155}, {"id": 49, "seek": 18900, "start": 211.0, "end": 216.0, "text": " and the two main structures in Haystack that make it possible", "tokens": [51464, 293, 264, 732, 2135, 9227, 294, 8721, 372, 501, 300, 652, 309, 1944, 51714], "temperature": 0.0, "avg_logprob": -0.07553046833385121, "compression_ratio": 1.62890625, "no_speech_prob": 0.001612310647033155}, {"id": 50, "seek": 21600, "start": 216.0, "end": 219.0, "text": " are called pipelines and components,", "tokens": [50364, 366, 1219, 40168, 293, 6677, 11, 50514], "temperature": 0.0, "avg_logprob": -0.07752435249194764, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.025832179933786392}, {"id": 51, "seek": 21600, "start": 219.0, "end": 224.0, "text": " and a pipeline is made up of a few components attached to each other", "tokens": [50514, 293, 257, 15517, 307, 1027, 493, 295, 257, 1326, 6677, 8570, 281, 1184, 661, 50764], "temperature": 0.0, "avg_logprob": -0.07752435249194764, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.025832179933786392}, {"id": 52, "seek": 21600, "start": 224.0, "end": 228.0, "text": " where every component is forwarding some data to the next one.", "tokens": [50764, 689, 633, 6542, 307, 2128, 278, 512, 1412, 281, 264, 958, 472, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07752435249194764, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.025832179933786392}, {"id": 53, "seek": 21600, "start": 228.0, "end": 231.0, "text": " And I'm not going to get into what RAG is,", "tokens": [50964, 400, 286, 478, 406, 516, 281, 483, 666, 437, 14626, 38, 307, 11, 51114], "temperature": 0.0, "avg_logprob": -0.07752435249194764, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.025832179933786392}, {"id": 54, "seek": 21600, "start": 231.0, "end": 233.0, "text": " I'm pretty sure a lot of you know what RAG is at this point,", "tokens": [51114, 286, 478, 1238, 988, 257, 688, 295, 291, 458, 437, 14626, 38, 307, 412, 341, 935, 11, 51214], "temperature": 0.0, "avg_logprob": -0.07752435249194764, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.025832179933786392}, {"id": 55, "seek": 21600, "start": 233.0, "end": 238.0, "text": " but a retrieval augmented generative pipeline might look a bit like this.", "tokens": [51214, 457, 257, 19817, 3337, 36155, 1337, 1166, 15517, 1062, 574, 257, 857, 411, 341, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07752435249194764, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.025832179933786392}, {"id": 56, "seek": 21600, "start": 238.0, "end": 244.0, "text": " You have a query, and then an embedder component is creating an embedding for that query,", "tokens": [51464, 509, 362, 257, 14581, 11, 293, 550, 364, 12240, 1068, 6542, 307, 4084, 364, 12240, 3584, 337, 300, 14581, 11, 51764], "temperature": 0.0, "avg_logprob": -0.07752435249194764, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.025832179933786392}, {"id": 57, "seek": 24400, "start": 244.0, "end": 249.0, "text": " then a retriever component is retrieving the most relevant context", "tokens": [50364, 550, 257, 19817, 331, 6542, 307, 19817, 798, 264, 881, 7340, 4319, 50614], "temperature": 0.0, "avg_logprob": -0.11932837729360543, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.001113543868996203}, {"id": 58, "seek": 24400, "start": 249.0, "end": 251.0, "text": " for your LLM to actually use,", "tokens": [50614, 337, 428, 441, 43, 44, 281, 767, 764, 11, 50714], "temperature": 0.0, "avg_logprob": -0.11932837729360543, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.001113543868996203}, {"id": 59, "seek": 24400, "start": 251.0, "end": 256.0, "text": " and then it's forwarding that to what in Haystack world we call a prompt builder,", "tokens": [50714, 293, 550, 309, 311, 2128, 278, 300, 281, 437, 294, 8721, 372, 501, 1002, 321, 818, 257, 12391, 27377, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11932837729360543, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.001113543868996203}, {"id": 60, "seek": 24400, "start": 256.0, "end": 260.0, "text": " so that context gets embedded into your prompt itself,", "tokens": [50964, 370, 300, 4319, 2170, 16741, 666, 428, 12391, 2564, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11932837729360543, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.001113543868996203}, {"id": 61, "seek": 24400, "start": 260.0, "end": 263.0, "text": " and then you use the generator,", "tokens": [51164, 293, 550, 291, 764, 264, 19265, 11, 51314], "temperature": 0.0, "avg_logprob": -0.11932837729360543, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.001113543868996203}, {"id": 62, "seek": 24400, "start": 263.0, "end": 268.0, "text": " and that can be any model, open source model of a hugging phase, or open AI, etc.", "tokens": [51314, 293, 300, 393, 312, 604, 2316, 11, 1269, 4009, 2316, 295, 257, 41706, 5574, 11, 420, 1269, 7318, 11, 5183, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11932837729360543, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.001113543868996203}, {"id": 63, "seek": 24400, "start": 268.0, "end": 270.0, "text": " And then you get an answer.", "tokens": [51564, 400, 550, 291, 483, 364, 1867, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11932837729360543, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.001113543868996203}, {"id": 64, "seek": 27000, "start": 270.0, "end": 277.0, "text": " This is a pipeline, but what a pipeline does is really dictated by what components it's comprised of.", "tokens": [50364, 639, 307, 257, 15517, 11, 457, 437, 257, 15517, 775, 307, 534, 12569, 770, 538, 437, 6677, 309, 311, 38062, 295, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06804279365924874, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.0025365683250129223}, {"id": 65, "seek": 27000, "start": 277.0, "end": 282.0, "text": " You might also create a pipeline that indexes documents.", "tokens": [50714, 509, 1062, 611, 1884, 257, 15517, 300, 8186, 279, 8512, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06804279365924874, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.0025365683250129223}, {"id": 66, "seek": 27000, "start": 282.0, "end": 284.0, "text": " I'm not really going to get into this here,", "tokens": [50964, 286, 478, 406, 534, 516, 281, 483, 666, 341, 510, 11, 51064], "temperature": 0.0, "avg_logprob": -0.06804279365924874, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.0025365683250129223}, {"id": 67, "seek": 27000, "start": 284.0, "end": 287.0, "text": " you're just basically fetching the contents of a URL,", "tokens": [51064, 291, 434, 445, 1936, 23673, 278, 264, 15768, 295, 257, 12905, 11, 51214], "temperature": 0.0, "avg_logprob": -0.06804279365924874, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.0025365683250129223}, {"id": 68, "seek": 27000, "start": 287.0, "end": 293.0, "text": " and then writing that into one of the available document stores that we have.", "tokens": [51214, 293, 550, 3579, 300, 666, 472, 295, 264, 2435, 4166, 9512, 300, 321, 362, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06804279365924874, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.0025365683250129223}, {"id": 69, "seek": 27000, "start": 293.0, "end": 299.0, "text": " But this is all made possible because of the structure called a component,", "tokens": [51514, 583, 341, 307, 439, 1027, 1944, 570, 295, 264, 3877, 1219, 257, 6542, 11, 51814], "temperature": 0.0, "avg_logprob": -0.06804279365924874, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.0025365683250129223}, {"id": 70, "seek": 29900, "start": 299.0, "end": 303.0, "text": " and a component in Haystack is something that could have, for example,", "tokens": [50364, 293, 257, 6542, 294, 8721, 372, 501, 307, 746, 300, 727, 362, 11, 337, 1365, 11, 50564], "temperature": 0.0, "avg_logprob": -0.052626455271685565, "compression_ratio": 1.7991266375545851, "no_speech_prob": 0.012068175710737705}, {"id": 71, "seek": 29900, "start": 303.0, "end": 306.0, "text": " in this case it has two inputs and an output,", "tokens": [50564, 294, 341, 1389, 309, 575, 732, 15743, 293, 364, 5598, 11, 50714], "temperature": 0.0, "avg_logprob": -0.052626455271685565, "compression_ratio": 1.7991266375545851, "no_speech_prob": 0.012068175710737705}, {"id": 72, "seek": 29900, "start": 306.0, "end": 310.0, "text": " but you don't necessarily have to have a defined output.", "tokens": [50714, 457, 291, 500, 380, 4725, 362, 281, 362, 257, 7642, 5598, 13, 50914], "temperature": 0.0, "avg_logprob": -0.052626455271685565, "compression_ratio": 1.7991266375545851, "no_speech_prob": 0.012068175710737705}, {"id": 73, "seek": 29900, "start": 310.0, "end": 316.0, "text": " Haystack doesn't really make assumptions as to what a component has to be.", "tokens": [50914, 8721, 372, 501, 1177, 380, 534, 652, 17695, 382, 281, 437, 257, 6542, 575, 281, 312, 13, 51214], "temperature": 0.0, "avg_logprob": -0.052626455271685565, "compression_ratio": 1.7991266375545851, "no_speech_prob": 0.012068175710737705}, {"id": 74, "seek": 29900, "start": 316.0, "end": 319.0, "text": " It can also be something that has two inputs, two outputs,", "tokens": [51214, 467, 393, 611, 312, 746, 300, 575, 732, 15743, 11, 732, 23930, 11, 51364], "temperature": 0.0, "avg_logprob": -0.052626455271685565, "compression_ratio": 1.7991266375545851, "no_speech_prob": 0.012068175710737705}, {"id": 75, "seek": 29900, "start": 319.0, "end": 323.0, "text": " and then the idea is you attach those components to each other,", "tokens": [51364, 293, 550, 264, 1558, 307, 291, 5085, 729, 6677, 281, 1184, 661, 11, 51564], "temperature": 0.0, "avg_logprob": -0.052626455271685565, "compression_ratio": 1.7991266375545851, "no_speech_prob": 0.012068175710737705}, {"id": 76, "seek": 29900, "start": 323.0, "end": 325.0, "text": " and you can be very, very specific here.", "tokens": [51564, 293, 291, 393, 312, 588, 11, 588, 2685, 510, 13, 51664], "temperature": 0.0, "avg_logprob": -0.052626455271685565, "compression_ratio": 1.7991266375545851, "no_speech_prob": 0.012068175710737705}, {"id": 77, "seek": 32500, "start": 325.0, "end": 333.0, "text": " You can be specific into saying, like, I want output one to be forwarded to input two of the next component.", "tokens": [50364, 509, 393, 312, 2685, 666, 1566, 11, 411, 11, 286, 528, 5598, 472, 281, 312, 2128, 292, 281, 4846, 732, 295, 264, 958, 6542, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0726743708835559, "compression_ratio": 1.559090909090909, "no_speech_prob": 0.009693076834082603}, {"id": 78, "seek": 32500, "start": 333.0, "end": 335.0, "text": " You can be very, very precise here.", "tokens": [50764, 509, 393, 312, 588, 11, 588, 13600, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0726743708835559, "compression_ratio": 1.559090909090909, "no_speech_prob": 0.009693076834082603}, {"id": 79, "seek": 32500, "start": 335.0, "end": 341.0, "text": " And maybe you can already start to see this starts to look quite like a graph.", "tokens": [50864, 400, 1310, 291, 393, 1217, 722, 281, 536, 341, 3719, 281, 574, 1596, 411, 257, 4295, 13, 51164], "temperature": 0.0, "avg_logprob": -0.0726743708835559, "compression_ratio": 1.559090909090909, "no_speech_prob": 0.009693076834082603}, {"id": 80, "seek": 32500, "start": 341.0, "end": 344.0, "text": " So how do we build these components?", "tokens": [51164, 407, 577, 360, 321, 1322, 613, 6677, 30, 51314], "temperature": 0.0, "avg_logprob": -0.0726743708835559, "compression_ratio": 1.559090909090909, "no_speech_prob": 0.009693076834082603}, {"id": 81, "seek": 32500, "start": 344.0, "end": 350.0, "text": " There's only a few requirements for something to be a component in Haystack world.", "tokens": [51314, 821, 311, 787, 257, 1326, 7728, 337, 746, 281, 312, 257, 6542, 294, 8721, 372, 501, 1002, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0726743708835559, "compression_ratio": 1.559090909090909, "no_speech_prob": 0.009693076834082603}, {"id": 82, "seek": 35000, "start": 350.0, "end": 356.0, "text": " We provide a bunch of ready-made components that, if you go to the Haystack documentation,", "tokens": [50364, 492, 2893, 257, 3840, 295, 1919, 12, 10341, 6677, 300, 11, 498, 291, 352, 281, 264, 8721, 372, 501, 14333, 11, 50664], "temperature": 0.0, "avg_logprob": -0.08127992529618112, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.005337626673281193}, {"id": 83, "seek": 35000, "start": 356.0, "end": 363.0, "text": " you'll see a bunch of sections there, generators, converters, embedders, etc.", "tokens": [50664, 291, 603, 536, 257, 3840, 295, 10863, 456, 11, 38662, 11, 9652, 1559, 11, 12240, 15633, 11, 5183, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08127992529618112, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.005337626673281193}, {"id": 84, "seek": 35000, "start": 363.0, "end": 368.0, "text": " These are all basically components that have been built exactly like this", "tokens": [51014, 1981, 366, 439, 1936, 6677, 300, 362, 668, 3094, 2293, 411, 341, 51264], "temperature": 0.0, "avg_logprob": -0.08127992529618112, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.005337626673281193}, {"id": 85, "seek": 35000, "start": 368.0, "end": 371.0, "text": " that we just provide in the package itself.", "tokens": [51264, 300, 321, 445, 2893, 294, 264, 7372, 2564, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08127992529618112, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.005337626673281193}, {"id": 86, "seek": 35000, "start": 371.0, "end": 373.0, "text": " But what you can do is build your own.", "tokens": [51414, 583, 437, 291, 393, 360, 307, 1322, 428, 1065, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08127992529618112, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.005337626673281193}, {"id": 87, "seek": 35000, "start": 373.0, "end": 376.0, "text": " And what you need to have is a class.", "tokens": [51514, 400, 437, 291, 643, 281, 362, 307, 257, 1508, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08127992529618112, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.005337626673281193}, {"id": 88, "seek": 37600, "start": 376.0, "end": 382.0, "text": " So here I've just got a very, very well-named MyCustomComponent class.", "tokens": [50364, 407, 510, 286, 600, 445, 658, 257, 588, 11, 588, 731, 12, 33465, 1222, 34, 2239, 34, 8586, 30365, 1508, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06847437437590179, "compression_ratio": 1.7736625514403292, "no_speech_prob": 0.025208210572600365}, {"id": 89, "seek": 37600, "start": 382.0, "end": 384.0, "text": " I need a run function.", "tokens": [50664, 286, 643, 257, 1190, 2445, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06847437437590179, "compression_ratio": 1.7736625514403292, "no_speech_prob": 0.025208210572600365}, {"id": 90, "seek": 37600, "start": 384.0, "end": 387.0, "text": " And the other thing you need are these decorators.", "tokens": [50764, 400, 264, 661, 551, 291, 643, 366, 613, 7919, 3391, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06847437437590179, "compression_ratio": 1.7736625514403292, "no_speech_prob": 0.025208210572600365}, {"id": 91, "seek": 37600, "start": 387.0, "end": 391.0, "text": " So basically this is telling Haystack that this class is a component.", "tokens": [50914, 407, 1936, 341, 307, 3585, 8721, 372, 501, 300, 341, 1508, 307, 257, 6542, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06847437437590179, "compression_ratio": 1.7736625514403292, "no_speech_prob": 0.025208210572600365}, {"id": 92, "seek": 37600, "start": 391.0, "end": 394.0, "text": " And then the second one is around the run function.", "tokens": [51114, 400, 550, 264, 1150, 472, 307, 926, 264, 1190, 2445, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06847437437590179, "compression_ratio": 1.7736625514403292, "no_speech_prob": 0.025208210572600365}, {"id": 93, "seek": 37600, "start": 394.0, "end": 397.0, "text": " And this is actually used for pipeline validation down the line.", "tokens": [51264, 400, 341, 307, 767, 1143, 337, 15517, 24071, 760, 264, 1622, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06847437437590179, "compression_ratio": 1.7736625514403292, "no_speech_prob": 0.025208210572600365}, {"id": 94, "seek": 37600, "start": 397.0, "end": 404.0, "text": " But it's basically telling the Haystack pipeline what outputs it should expect from this component.", "tokens": [51414, 583, 309, 311, 1936, 3585, 264, 8721, 372, 501, 15517, 437, 23930, 309, 820, 2066, 490, 341, 6542, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06847437437590179, "compression_ratio": 1.7736625514403292, "no_speech_prob": 0.025208210572600365}, {"id": 95, "seek": 40400, "start": 404.0, "end": 411.0, "text": " In this scenario, I've got a MyCustomComponent that's expecting a query,", "tokens": [50364, 682, 341, 9005, 11, 286, 600, 658, 257, 1222, 34, 2239, 34, 8586, 30365, 300, 311, 9650, 257, 14581, 11, 50714], "temperature": 0.0, "avg_logprob": -0.10409175025092231, "compression_ratio": 1.6063829787234043, "no_speech_prob": 0.001264645834453404}, {"id": 96, "seek": 40400, "start": 411.0, "end": 416.0, "text": " which is supposed to be a string, and then it's returning documents.", "tokens": [50714, 597, 307, 3442, 281, 312, 257, 6798, 11, 293, 550, 309, 311, 12678, 8512, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10409175025092231, "compression_ratio": 1.6063829787234043, "no_speech_prob": 0.001264645834453404}, {"id": 97, "seek": 40400, "start": 416.0, "end": 418.0, "text": " In this case, it's just hard-coded.", "tokens": [50964, 682, 341, 1389, 11, 309, 311, 445, 1152, 12, 66, 12340, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10409175025092231, "compression_ratio": 1.6063829787234043, "no_speech_prob": 0.001264645834453404}, {"id": 98, "seek": 40400, "start": 418.0, "end": 420.0, "text": " It returns high and by.", "tokens": [51064, 467, 11247, 1090, 293, 538, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10409175025092231, "compression_ratio": 1.6063829787234043, "no_speech_prob": 0.001264645834453404}, {"id": 99, "seek": 40400, "start": 420.0, "end": 423.0, "text": " So we know that this, whatever query this gets,", "tokens": [51164, 407, 321, 458, 300, 341, 11, 2035, 14581, 341, 2170, 11, 51314], "temperature": 0.0, "avg_logprob": -0.10409175025092231, "compression_ratio": 1.6063829787234043, "no_speech_prob": 0.001264645834453404}, {"id": 100, "seek": 40400, "start": 423.0, "end": 427.0, "text": " is going to be returning two documents, high and by.", "tokens": [51314, 307, 516, 281, 312, 12678, 732, 8512, 11, 1090, 293, 538, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10409175025092231, "compression_ratio": 1.6063829787234043, "no_speech_prob": 0.001264645834453404}, {"id": 101, "seek": 42700, "start": 427.0, "end": 433.0, "text": " And this has then led to quite a bunch of components that don't actually,", "tokens": [50364, 400, 341, 575, 550, 4684, 281, 1596, 257, 3840, 295, 6677, 300, 500, 380, 767, 11, 50664], "temperature": 0.0, "avg_logprob": -0.08835441508191696, "compression_ratio": 1.6694915254237288, "no_speech_prob": 0.05434666574001312}, {"id": 102, "seek": 42700, "start": 433.0, "end": 437.0, "text": " they're not served through the Haystack framework itself, not all of them are,", "tokens": [50664, 436, 434, 406, 7584, 807, 264, 8721, 372, 501, 8388, 2564, 11, 406, 439, 295, 552, 366, 11, 50864], "temperature": 0.0, "avg_logprob": -0.08835441508191696, "compression_ratio": 1.6694915254237288, "no_speech_prob": 0.05434666574001312}, {"id": 103, "seek": 42700, "start": 437.0, "end": 440.0, "text": " but you can just install them as separate packages.", "tokens": [50864, 457, 291, 393, 445, 3625, 552, 382, 4994, 17401, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08835441508191696, "compression_ratio": 1.6694915254237288, "no_speech_prob": 0.05434666574001312}, {"id": 104, "seek": 42700, "start": 440.0, "end": 444.0, "text": " And it's meant community members have just gone ahead and built components", "tokens": [51014, 400, 309, 311, 4140, 1768, 2679, 362, 445, 2780, 2286, 293, 3094, 6677, 51214], "temperature": 0.0, "avg_logprob": -0.08835441508191696, "compression_ratio": 1.6694915254237288, "no_speech_prob": 0.05434666574001312}, {"id": 105, "seek": 42700, "start": 444.0, "end": 448.0, "text": " that they need for their very specific custom needs", "tokens": [51214, 300, 436, 643, 337, 641, 588, 2685, 2375, 2203, 51414], "temperature": 0.0, "avg_logprob": -0.08835441508191696, "compression_ratio": 1.6694915254237288, "no_speech_prob": 0.05434666574001312}, {"id": 106, "seek": 42700, "start": 448.0, "end": 453.0, "text": " and made them available to the rest of the Haystack community.", "tokens": [51414, 293, 1027, 552, 2435, 281, 264, 1472, 295, 264, 8721, 372, 501, 1768, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08835441508191696, "compression_ratio": 1.6694915254237288, "no_speech_prob": 0.05434666574001312}, {"id": 107, "seek": 45300, "start": 453.0, "end": 458.0, "text": " So let's come to our Hack and Use TLDR, if you will, project.", "tokens": [50364, 407, 718, 311, 808, 281, 527, 35170, 293, 8278, 40277, 9301, 11, 498, 291, 486, 11, 1716, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11632719906893643, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.00821963045746088}, {"id": 108, "seek": 45300, "start": 458.0, "end": 464.0, "text": " The idea was that we wanted a component that would take top K,", "tokens": [50614, 440, 1558, 390, 300, 321, 1415, 257, 6542, 300, 576, 747, 1192, 591, 11, 50914], "temperature": 0.0, "avg_logprob": -0.11632719906893643, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.00821963045746088}, {"id": 109, "seek": 45300, "start": 464.0, "end": 469.0, "text": " that could be a number, and it would return articles.", "tokens": [50914, 300, 727, 312, 257, 1230, 11, 293, 309, 576, 2736, 11290, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11632719906893643, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.00821963045746088}, {"id": 110, "seek": 45300, "start": 469.0, "end": 474.0, "text": " And again, this is a colab that you can use, it should be running.", "tokens": [51164, 400, 797, 11, 341, 307, 257, 1173, 455, 300, 291, 393, 764, 11, 309, 820, 312, 2614, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11632719906893643, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.00821963045746088}, {"id": 111, "seek": 45300, "start": 474.0, "end": 478.0, "text": " And the way we did that was, this is very much pseudocode.", "tokens": [51414, 400, 264, 636, 321, 630, 300, 390, 11, 341, 307, 588, 709, 25505, 532, 905, 1429, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11632719906893643, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.00821963045746088}, {"id": 112, "seek": 45300, "start": 478.0, "end": 481.0, "text": " Later, if we have time, I'm going to show you the actual code.", "tokens": [51614, 11965, 11, 498, 321, 362, 565, 11, 286, 478, 516, 281, 855, 291, 264, 3539, 3089, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11632719906893643, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.00821963045746088}, {"id": 113, "seek": 48100, "start": 481.0, "end": 486.0, "text": " But we built this component called Hack and Use Fetcher.", "tokens": [50364, 583, 321, 3094, 341, 6542, 1219, 35170, 293, 8278, 479, 302, 6759, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10658353688765546, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.02027597650885582}, {"id": 114, "seek": 48100, "start": 486.0, "end": 491.0, "text": " It takes top K, it queries the Hack and Use API", "tokens": [50614, 467, 2516, 1192, 591, 11, 309, 24109, 264, 35170, 293, 8278, 9362, 50864], "temperature": 0.0, "avg_logprob": -0.10658353688765546, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.02027597650885582}, {"id": 115, "seek": 48100, "start": 491.0, "end": 494.0, "text": " and gets the top whatever number we've decided.", "tokens": [50864, 293, 2170, 264, 1192, 2035, 1230, 321, 600, 3047, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10658353688765546, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.02027597650885582}, {"id": 116, "seek": 48100, "start": 494.0, "end": 498.0, "text": " And the other thing I wanted to show here is, at the end,", "tokens": [51014, 400, 264, 661, 551, 286, 1415, 281, 855, 510, 307, 11, 412, 264, 917, 11, 51214], "temperature": 0.0, "avg_logprob": -0.10658353688765546, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.02027597650885582}, {"id": 117, "seek": 48100, "start": 498.0, "end": 503.0, "text": " I don't know how well you can see it, but we've also added some meta information,", "tokens": [51214, 286, 500, 380, 458, 577, 731, 291, 393, 536, 309, 11, 457, 321, 600, 611, 3869, 512, 19616, 1589, 11, 51464], "temperature": 0.0, "avg_logprob": -0.10658353688765546, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.02027597650885582}, {"id": 118, "seek": 48100, "start": 503.0, "end": 507.0, "text": " because down the line, we can use meta information in our prompt,", "tokens": [51464, 570, 760, 264, 1622, 11, 321, 393, 764, 19616, 1589, 294, 527, 12391, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10658353688765546, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.02027597650885582}, {"id": 119, "seek": 50700, "start": 507.0, "end": 511.0, "text": " because you also get titles of the Hack and Use articles,", "tokens": [50364, 570, 291, 611, 483, 12992, 295, 264, 35170, 293, 8278, 11290, 11, 50564], "temperature": 0.0, "avg_logprob": -0.07212839580717541, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.007847561500966549}, {"id": 120, "seek": 50700, "start": 511.0, "end": 515.0, "text": " you also get URLs, which is great for referencing down the line too.", "tokens": [50564, 291, 611, 483, 43267, 11, 597, 307, 869, 337, 40582, 760, 264, 1622, 886, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07212839580717541, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.007847561500966549}, {"id": 121, "seek": 50700, "start": 515.0, "end": 519.0, "text": " So we return full documents that have the content,", "tokens": [50764, 407, 321, 2736, 1577, 8512, 300, 362, 264, 2701, 11, 50964], "temperature": 0.0, "avg_logprob": -0.07212839580717541, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.007847561500966549}, {"id": 122, "seek": 50700, "start": 519.0, "end": 524.0, "text": " the title, and the URLs of each Hack and Use articles that we fetched.", "tokens": [50964, 264, 4876, 11, 293, 264, 43267, 295, 1184, 35170, 293, 8278, 11290, 300, 321, 23673, 292, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07212839580717541, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.007847561500966549}, {"id": 123, "seek": 50700, "start": 524.0, "end": 528.0, "text": " And at the end of the day, we're going to be building a pipeline", "tokens": [51214, 400, 412, 264, 917, 295, 264, 786, 11, 321, 434, 516, 281, 312, 2390, 257, 15517, 51414], "temperature": 0.0, "avg_logprob": -0.07212839580717541, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.007847561500966549}, {"id": 124, "seek": 50700, "start": 528.0, "end": 531.0, "text": " that looks like this, and everything you see in green", "tokens": [51414, 300, 1542, 411, 341, 11, 293, 1203, 291, 536, 294, 3092, 51564], "temperature": 0.0, "avg_logprob": -0.07212839580717541, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.007847561500966549}, {"id": 125, "seek": 50700, "start": 531.0, "end": 533.0, "text": " is already provided with Haystack.", "tokens": [51564, 307, 1217, 5649, 365, 8721, 372, 501, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07212839580717541, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.007847561500966549}, {"id": 126, "seek": 53300, "start": 533.0, "end": 537.0, "text": " So that came with pip install Haystack AI anyway.", "tokens": [50364, 407, 300, 1361, 365, 8489, 3625, 8721, 372, 501, 7318, 4033, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15596631493898902, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.014431667514145374}, {"id": 127, "seek": 53300, "start": 537.0, "end": 540.0, "text": " And the orange is what we've just built for ourselves,", "tokens": [50564, 400, 264, 7671, 307, 437, 321, 600, 445, 3094, 337, 4175, 11, 50714], "temperature": 0.0, "avg_logprob": -0.15596631493898902, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.014431667514145374}, {"id": 128, "seek": 53300, "start": 540.0, "end": 544.0, "text": " and it just slots into the rest of the Haystack pipeline ecosystem.", "tokens": [50714, 293, 309, 445, 24266, 666, 264, 1472, 295, 264, 8721, 372, 501, 15517, 11311, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15596631493898902, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.014431667514145374}, {"id": 129, "seek": 53300, "start": 544.0, "end": 551.0, "text": " And for this co-lab that I've shared with everyone here is,", "tokens": [50914, 400, 337, 341, 598, 12, 44990, 300, 286, 600, 5507, 365, 1518, 510, 307, 11, 51264], "temperature": 0.0, "avg_logprob": -0.15596631493898902, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.014431667514145374}, {"id": 130, "seek": 53300, "start": 551.0, "end": 553.0, "text": " I've decided I'd just go ahead and use Mixstrahl,", "tokens": [51264, 286, 600, 3047, 286, 1116, 445, 352, 2286, 293, 764, 12769, 372, 15688, 75, 11, 51364], "temperature": 0.0, "avg_logprob": -0.15596631493898902, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.014431667514145374}, {"id": 131, "seek": 53300, "start": 553.0, "end": 558.0, "text": " I've tried this with OpenAI models a lot, so why not try something new?", "tokens": [51364, 286, 600, 3031, 341, 365, 7238, 48698, 5245, 257, 688, 11, 370, 983, 406, 853, 746, 777, 30, 51614], "temperature": 0.0, "avg_logprob": -0.15596631493898902, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.014431667514145374}, {"id": 132, "seek": 55800, "start": 559.0, "end": 565.0, "text": " And then the last thing I want to highlight about this particular pipeline", "tokens": [50414, 400, 550, 264, 1036, 551, 286, 528, 281, 5078, 466, 341, 1729, 15517, 50714], "temperature": 0.0, "avg_logprob": -0.08470034879796645, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.00714066531509161}, {"id": 133, "seek": 55800, "start": 565.0, "end": 567.0, "text": " is how the prompt is being built.", "tokens": [50714, 307, 577, 264, 12391, 307, 885, 3094, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08470034879796645, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.00714066531509161}, {"id": 134, "seek": 55800, "start": 567.0, "end": 571.0, "text": " So prompt templating happens in Haystack world", "tokens": [50814, 407, 12391, 9100, 990, 2314, 294, 8721, 372, 501, 1002, 51014], "temperature": 0.0, "avg_logprob": -0.08470034879796645, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.00714066531509161}, {"id": 135, "seek": 55800, "start": 571.0, "end": 575.0, "text": " by a component called the prompt builder.", "tokens": [51014, 538, 257, 6542, 1219, 264, 12391, 27377, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08470034879796645, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.00714066531509161}, {"id": 136, "seek": 55800, "start": 575.0, "end": 579.0, "text": " And templates use Ginger templating.", "tokens": [51214, 400, 21165, 764, 34637, 9100, 990, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08470034879796645, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.00714066531509161}, {"id": 137, "seek": 55800, "start": 579.0, "end": 582.0, "text": " And what's really important here is,", "tokens": [51414, 400, 437, 311, 534, 1021, 510, 307, 11, 51564], "temperature": 0.0, "avg_logprob": -0.08470034879796645, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.00714066531509161}, {"id": 138, "seek": 55800, "start": 582.0, "end": 586.0, "text": " okay, we have an instruction, you'll be provided with one or more", "tokens": [51564, 1392, 11, 321, 362, 364, 10951, 11, 291, 603, 312, 5649, 365, 472, 420, 544, 51764], "temperature": 0.0, "avg_logprob": -0.08470034879796645, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.00714066531509161}, {"id": 139, "seek": 58600, "start": 586.0, "end": 589.0, "text": " hack and use articles, please provide summaries.", "tokens": [50364, 10339, 293, 764, 11290, 11, 1767, 2893, 8367, 4889, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10332168851579938, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008765090256929398}, {"id": 140, "seek": 58600, "start": 589.0, "end": 595.0, "text": " But if you look at this close theme, we have actually a for loop.", "tokens": [50514, 583, 498, 291, 574, 412, 341, 1998, 6314, 11, 321, 362, 767, 257, 337, 6367, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10332168851579938, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008765090256929398}, {"id": 141, "seek": 58600, "start": 595.0, "end": 599.0, "text": " So this prompt builder automatically knows that it should be expecting", "tokens": [50814, 407, 341, 12391, 27377, 6772, 3255, 300, 309, 820, 312, 9650, 51014], "temperature": 0.0, "avg_logprob": -0.10332168851579938, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008765090256929398}, {"id": 142, "seek": 58600, "start": 599.0, "end": 603.0, "text": " an input called articles, and it can loop through those articles,", "tokens": [51014, 364, 4846, 1219, 11290, 11, 293, 309, 393, 6367, 807, 729, 11290, 11, 51214], "temperature": 0.0, "avg_logprob": -0.10332168851579938, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008765090256929398}, {"id": 143, "seek": 58600, "start": 603.0, "end": 608.0, "text": " and then it can access the contents of that article object", "tokens": [51214, 293, 550, 309, 393, 2105, 264, 15768, 295, 300, 7222, 2657, 51464], "temperature": 0.0, "avg_logprob": -0.10332168851579938, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008765090256929398}, {"id": 144, "seek": 58600, "start": 608.0, "end": 611.0, "text": " individually in every step of that for loop.", "tokens": [51464, 16652, 294, 633, 1823, 295, 300, 337, 6367, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10332168851579938, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008765090256929398}, {"id": 145, "seek": 61100, "start": 611.0, "end": 616.0, "text": " And that's how we're embedding URL here as well.", "tokens": [50364, 400, 300, 311, 577, 321, 434, 12240, 3584, 12905, 510, 382, 731, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09153398477806235, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.006145923398435116}, {"id": 146, "seek": 61100, "start": 616.0, "end": 618.0, "text": " And this is the final product.", "tokens": [50614, 400, 341, 307, 264, 2572, 1674, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09153398477806235, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.006145923398435116}, {"id": 147, "seek": 61100, "start": 618.0, "end": 621.0, "text": " At the end of the day, we were able to build a pipeline where,", "tokens": [50714, 1711, 264, 917, 295, 264, 786, 11, 321, 645, 1075, 281, 1322, 257, 15517, 689, 11, 50864], "temperature": 0.0, "avg_logprob": -0.09153398477806235, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.006145923398435116}, {"id": 148, "seek": 61100, "start": 621.0, "end": 624.0, "text": " given top three, we were able to run it,", "tokens": [50864, 2212, 1192, 1045, 11, 321, 645, 1075, 281, 1190, 309, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09153398477806235, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.006145923398435116}, {"id": 149, "seek": 61100, "start": 624.0, "end": 628.0, "text": " and we've got the TLDR summary and the URLs that you can find,", "tokens": [51014, 293, 321, 600, 658, 264, 40277, 9301, 12691, 293, 264, 43267, 300, 291, 393, 915, 11, 51214], "temperature": 0.0, "avg_logprob": -0.09153398477806235, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.006145923398435116}, {"id": 150, "seek": 61100, "start": 628.0, "end": 634.0, "text": " the full articles of hack and use, current hack and use top articles.", "tokens": [51214, 264, 1577, 11290, 295, 10339, 293, 764, 11, 2190, 10339, 293, 764, 1192, 11290, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09153398477806235, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.006145923398435116}, {"id": 151, "seek": 61100, "start": 634.0, "end": 637.0, "text": " So with that, I want to show a few other projects", "tokens": [51514, 407, 365, 300, 11, 286, 528, 281, 855, 257, 1326, 661, 4455, 51664], "temperature": 0.0, "avg_logprob": -0.09153398477806235, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.006145923398435116}, {"id": 152, "seek": 63700, "start": 637.0, "end": 641.0, "text": " that this custom component building functionality has enabled.", "tokens": [50364, 300, 341, 2375, 6542, 2390, 14980, 575, 15172, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11465380607394997, "compression_ratio": 1.6303501945525292, "no_speech_prob": 0.032476428896188736}, {"id": 153, "seek": 63700, "start": 641.0, "end": 644.0, "text": " The next one is slightly questionable.", "tokens": [50564, 440, 958, 472, 307, 4748, 37158, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11465380607394997, "compression_ratio": 1.6303501945525292, "no_speech_prob": 0.032476428896188736}, {"id": 154, "seek": 63700, "start": 644.0, "end": 646.0, "text": " Please take it with a pinch of salt.", "tokens": [50714, 2555, 747, 309, 365, 257, 14614, 295, 5139, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11465380607394997, "compression_ratio": 1.6303501945525292, "no_speech_prob": 0.032476428896188736}, {"id": 155, "seek": 63700, "start": 646.0, "end": 650.0, "text": " I put it everywhere on that tugging face space too.", "tokens": [50814, 286, 829, 309, 5315, 322, 300, 33543, 3249, 1851, 1901, 886, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11465380607394997, "compression_ratio": 1.6303501945525292, "no_speech_prob": 0.032476428896188736}, {"id": 156, "seek": 63700, "start": 650.0, "end": 655.0, "text": " And the idea came from, at the time, Twitter was very different.", "tokens": [51014, 400, 264, 1558, 1361, 490, 11, 412, 264, 565, 11, 5794, 390, 588, 819, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11465380607394997, "compression_ratio": 1.6303501945525292, "no_speech_prob": 0.032476428896188736}, {"id": 157, "seek": 63700, "start": 655.0, "end": 660.0, "text": " So the idea was, could we like build a Twitter fetcher that,", "tokens": [51264, 407, 264, 1558, 390, 11, 727, 321, 411, 1322, 257, 5794, 15136, 6759, 300, 11, 51514], "temperature": 0.0, "avg_logprob": -0.11465380607394997, "compression_ratio": 1.6303501945525292, "no_speech_prob": 0.032476428896188736}, {"id": 158, "seek": 63700, "start": 660.0, "end": 663.0, "text": " given a username, could give you, this is really bad,", "tokens": [51514, 2212, 257, 30351, 11, 727, 976, 291, 11, 341, 307, 534, 1578, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11465380607394997, "compression_ratio": 1.6303501945525292, "no_speech_prob": 0.032476428896188736}, {"id": 159, "seek": 63700, "start": 663.0, "end": 666.0, "text": " could give you like a vibe check of the account,", "tokens": [51664, 727, 976, 291, 411, 257, 14606, 1520, 295, 264, 2696, 11, 51814], "temperature": 0.0, "avg_logprob": -0.11465380607394997, "compression_ratio": 1.6303501945525292, "no_speech_prob": 0.032476428896188736}, {"id": 160, "seek": 66600, "start": 666.0, "end": 669.0, "text": " and we called it like, should I follow?", "tokens": [50364, 293, 321, 1219, 309, 411, 11, 820, 286, 1524, 30, 50514], "temperature": 0.0, "avg_logprob": -0.12001379942282653, "compression_ratio": 1.5658362989323844, "no_speech_prob": 0.003626760793849826}, {"id": 161, "seek": 66600, "start": 669.0, "end": 673.0, "text": " And it gets like the last, I think, 40 posts of that user.", "tokens": [50514, 400, 309, 2170, 411, 264, 1036, 11, 286, 519, 11, 3356, 12300, 295, 300, 4195, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12001379942282653, "compression_ratio": 1.5658362989323844, "no_speech_prob": 0.003626760793849826}, {"id": 162, "seek": 66600, "start": 673.0, "end": 675.0, "text": " Obviously after that, Twitter changed,", "tokens": [50714, 7580, 934, 300, 11, 5794, 3105, 11, 50814], "temperature": 0.0, "avg_logprob": -0.12001379942282653, "compression_ratio": 1.5658362989323844, "no_speech_prob": 0.003626760793849826}, {"id": 163, "seek": 66600, "start": 675.0, "end": 678.0, "text": " so I went ahead and built a master on fetcher.", "tokens": [50814, 370, 286, 1437, 2286, 293, 3094, 257, 4505, 322, 15136, 6759, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12001379942282653, "compression_ratio": 1.5658362989323844, "no_speech_prob": 0.003626760793849826}, {"id": 164, "seek": 66600, "start": 678.0, "end": 682.0, "text": " You can also find that on the Haystack integrations page.", "tokens": [50964, 509, 393, 611, 915, 300, 322, 264, 8721, 372, 501, 3572, 763, 3028, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12001379942282653, "compression_ratio": 1.5658362989323844, "no_speech_prob": 0.003626760793849826}, {"id": 165, "seek": 66600, "start": 682.0, "end": 687.0, "text": " And the best way I like showcasing this is actually using my boyfriend's", "tokens": [51164, 400, 264, 1151, 636, 286, 411, 29794, 3349, 341, 307, 767, 1228, 452, 11457, 311, 51414], "temperature": 0.0, "avg_logprob": -0.12001379942282653, "compression_ratio": 1.5658362989323844, "no_speech_prob": 0.003626760793849826}, {"id": 166, "seek": 66600, "start": 687.0, "end": 691.0, "text": " master on account, because every time this tells me something a bit funny", "tokens": [51414, 4505, 322, 2696, 11, 570, 633, 565, 341, 5112, 385, 746, 257, 857, 4074, 51614], "temperature": 0.0, "avg_logprob": -0.12001379942282653, "compression_ratio": 1.5658362989323844, "no_speech_prob": 0.003626760793849826}, {"id": 167, "seek": 66600, "start": 691.0, "end": 694.0, "text": " about his account, once it called him pessimistic,", "tokens": [51614, 466, 702, 2696, 11, 1564, 309, 1219, 796, 37399, 3142, 11, 51764], "temperature": 0.0, "avg_logprob": -0.12001379942282653, "compression_ratio": 1.5658362989323844, "no_speech_prob": 0.003626760793849826}, {"id": 168, "seek": 69400, "start": 694.0, "end": 699.0, "text": " this time it called him sarcastic when discussing personal opinions.", "tokens": [50364, 341, 565, 309, 1219, 796, 36836, 2750, 562, 10850, 2973, 11819, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08079445362091064, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.009063580073416233}, {"id": 169, "seek": 69400, "start": 699.0, "end": 701.0, "text": " So that's also open.", "tokens": [50614, 407, 300, 311, 611, 1269, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08079445362091064, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.009063580073416233}, {"id": 170, "seek": 69400, "start": 701.0, "end": 704.0, "text": " I think I linked to it in the notes as well,", "tokens": [50714, 286, 519, 286, 9408, 281, 309, 294, 264, 5570, 382, 731, 11, 50864], "temperature": 0.0, "avg_logprob": -0.08079445362091064, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.009063580073416233}, {"id": 171, "seek": 69400, "start": 704.0, "end": 706.0, "text": " so you can go ahead and try that out.", "tokens": [50864, 370, 291, 393, 352, 2286, 293, 853, 300, 484, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08079445362091064, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.009063580073416233}, {"id": 172, "seek": 69400, "start": 706.0, "end": 709.0, "text": " You just need to provide the full master on username.", "tokens": [50964, 509, 445, 643, 281, 2893, 264, 1577, 4505, 322, 30351, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08079445362091064, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.009063580073416233}, {"id": 173, "seek": 69400, "start": 709.0, "end": 713.0, "text": " Without the at at the front, that's a bug I haven't fixed yet.", "tokens": [51114, 9129, 264, 412, 412, 264, 1868, 11, 300, 311, 257, 7426, 286, 2378, 380, 6806, 1939, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08079445362091064, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.009063580073416233}, {"id": 174, "seek": 69400, "start": 713.0, "end": 715.0, "text": " Another thing that this enabled,", "tokens": [51314, 3996, 551, 300, 341, 15172, 11, 51414], "temperature": 0.0, "avg_logprob": -0.08079445362091064, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.009063580073416233}, {"id": 175, "seek": 69400, "start": 715.0, "end": 720.0, "text": " actually not only used the Haystack custom component functionality,", "tokens": [51414, 767, 406, 787, 1143, 264, 8721, 372, 501, 2375, 6542, 14980, 11, 51664], "temperature": 0.0, "avg_logprob": -0.08079445362091064, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.009063580073416233}, {"id": 176, "seek": 72000, "start": 720.0, "end": 724.0, "text": " but also, I don't know if you remember when I showed the components earlier", "tokens": [50364, 457, 611, 11, 286, 500, 380, 458, 498, 291, 1604, 562, 286, 4712, 264, 6677, 3071, 50564], "temperature": 0.0, "avg_logprob": -0.08136909658258612, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.020322272554039955}, {"id": 177, "seek": 72000, "start": 724.0, "end": 727.0, "text": " with the two outputs and the two inputs, et cetera,", "tokens": [50564, 365, 264, 732, 23930, 293, 264, 732, 15743, 11, 1030, 11458, 11, 50714], "temperature": 0.0, "avg_logprob": -0.08136909658258612, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.020322272554039955}, {"id": 178, "seek": 72000, "start": 727.0, "end": 733.0, "text": " you can already start to imagine that you can actually have these pipelines loop too.", "tokens": [50714, 291, 393, 1217, 722, 281, 3811, 300, 291, 393, 767, 362, 613, 40168, 6367, 886, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08136909658258612, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.020322272554039955}, {"id": 179, "seek": 72000, "start": 733.0, "end": 738.0, "text": " So the idea was, what if we have some meeting notes,", "tokens": [51014, 407, 264, 1558, 390, 11, 437, 498, 321, 362, 512, 3440, 5570, 11, 51264], "temperature": 0.0, "avg_logprob": -0.08136909658258612, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.020322272554039955}, {"id": 180, "seek": 72000, "start": 738.0, "end": 741.0, "text": " and we have our own GitHub repository,", "tokens": [51264, 293, 321, 362, 527, 1065, 23331, 25841, 11, 51414], "temperature": 0.0, "avg_logprob": -0.08136909658258612, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.020322272554039955}, {"id": 181, "seek": 72000, "start": 741.0, "end": 743.0, "text": " and anyone who's used GitHub repositories,", "tokens": [51414, 293, 2878, 567, 311, 1143, 23331, 22283, 2083, 11, 51514], "temperature": 0.0, "avg_logprob": -0.08136909658258612, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.020322272554039955}, {"id": 182, "seek": 72000, "start": 743.0, "end": 746.0, "text": " you know that you can create those issue labels", "tokens": [51514, 291, 458, 300, 291, 393, 1884, 729, 2734, 16949, 51664], "temperature": 0.0, "avg_logprob": -0.08136909658258612, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.020322272554039955}, {"id": 183, "seek": 72000, "start": 746.0, "end": 749.0, "text": " that are very specific to that GitHub repository.", "tokens": [51664, 300, 366, 588, 2685, 281, 300, 23331, 25841, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08136909658258612, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.020322272554039955}, {"id": 184, "seek": 74900, "start": 749.0, "end": 753.0, "text": " Could we build a system that, given meeting notes,", "tokens": [50364, 7497, 321, 1322, 257, 1185, 300, 11, 2212, 3440, 5570, 11, 50564], "temperature": 0.0, "avg_logprob": -0.09113053215874566, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.003577014897018671}, {"id": 185, "seek": 74900, "start": 753.0, "end": 759.0, "text": " generates a list of GitHub issues specifically for that repository", "tokens": [50564, 23815, 257, 1329, 295, 23331, 2663, 4682, 337, 300, 25841, 50864], "temperature": 0.0, "avg_logprob": -0.09113053215874566, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.003577014897018671}, {"id": 186, "seek": 74900, "start": 759.0, "end": 761.0, "text": " that you're discussing in that meeting?", "tokens": [50864, 300, 291, 434, 10850, 294, 300, 3440, 30, 50964], "temperature": 0.0, "avg_logprob": -0.09113053215874566, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.003577014897018671}, {"id": 187, "seek": 74900, "start": 761.0, "end": 766.0, "text": " And could we actually then use those generated structured outputs", "tokens": [50964, 400, 727, 321, 767, 550, 764, 729, 10833, 18519, 23930, 51214], "temperature": 0.0, "avg_logprob": -0.09113053215874566, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.003577014897018671}, {"id": 188, "seek": 74900, "start": 766.0, "end": 769.0, "text": " to query GitHub to actually create those issues?", "tokens": [51214, 281, 14581, 23331, 281, 767, 1884, 729, 2663, 30, 51364], "temperature": 0.0, "avg_logprob": -0.09113053215874566, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.003577014897018671}, {"id": 189, "seek": 74900, "start": 769.0, "end": 774.0, "text": " Now, this is great, and our experience has been that actually a lot of large language models", "tokens": [51364, 823, 11, 341, 307, 869, 11, 293, 527, 1752, 575, 668, 300, 767, 257, 688, 295, 2416, 2856, 5245, 51614], "temperature": 0.0, "avg_logprob": -0.09113053215874566, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.003577014897018671}, {"id": 190, "seek": 74900, "start": 774.0, "end": 777.0, "text": " are great at generating structured output,", "tokens": [51614, 366, 869, 412, 17746, 18519, 5598, 11, 51764], "temperature": 0.0, "avg_logprob": -0.09113053215874566, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.003577014897018671}, {"id": 191, "seek": 77700, "start": 778.0, "end": 782.0, "text": " but not necessarily in the structure that you need.", "tokens": [50414, 457, 406, 4725, 294, 264, 3877, 300, 291, 643, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09296710968017578, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.002147675259038806}, {"id": 192, "seek": 77700, "start": 782.0, "end": 785.0, "text": " So it's going to be JSON, but is it going to abide by", "tokens": [50614, 407, 309, 311, 516, 281, 312, 31828, 11, 457, 307, 309, 516, 281, 39663, 538, 50764], "temperature": 0.0, "avg_logprob": -0.09296710968017578, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.002147675259038806}, {"id": 193, "seek": 77700, "start": 785.0, "end": 788.0, "text": " what you need that JSON object to look like?", "tokens": [50764, 437, 291, 643, 300, 31828, 2657, 281, 574, 411, 30, 50914], "temperature": 0.0, "avg_logprob": -0.09296710968017578, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.002147675259038806}, {"id": 194, "seek": 77700, "start": 788.0, "end": 791.0, "text": " So the idea here was, okay,", "tokens": [50914, 407, 264, 1558, 510, 390, 11, 1392, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09296710968017578, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.002147675259038806}, {"id": 195, "seek": 77700, "start": 791.0, "end": 795.0, "text": " well, why don't we create an output validator component,", "tokens": [51064, 731, 11, 983, 500, 380, 321, 1884, 364, 5598, 7363, 1639, 6542, 11, 51264], "temperature": 0.0, "avg_logprob": -0.09296710968017578, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.002147675259038806}, {"id": 196, "seek": 77700, "start": 795.0, "end": 798.0, "text": " and we use Pydantic for that.", "tokens": [51264, 293, 321, 764, 430, 6655, 7128, 337, 300, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09296710968017578, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.002147675259038806}, {"id": 197, "seek": 77700, "start": 798.0, "end": 804.0, "text": " This is all based on a tutorial that's up on the Haystack website right now,", "tokens": [51414, 639, 307, 439, 2361, 322, 257, 7073, 300, 311, 493, 322, 264, 8721, 372, 501, 3144, 558, 586, 11, 51714], "temperature": 0.0, "avg_logprob": -0.09296710968017578, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.002147675259038806}, {"id": 198, "seek": 80400, "start": 804.0, "end": 809.0, "text": " and basically what we did for the GitHub demo was modify this tutorial just a bit.", "tokens": [50364, 293, 1936, 437, 321, 630, 337, 264, 23331, 10723, 390, 16927, 341, 7073, 445, 257, 857, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08068819879328162, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.004488452337682247}, {"id": 199, "seek": 80400, "start": 809.0, "end": 814.0, "text": " In the tutorial, we said that we provide a Pydantic model,", "tokens": [50614, 682, 264, 7073, 11, 321, 848, 300, 321, 2893, 257, 430, 6655, 7128, 2316, 11, 50864], "temperature": 0.0, "avg_logprob": -0.08068819879328162, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.004488452337682247}, {"id": 200, "seek": 80400, "start": 814.0, "end": 818.0, "text": " and we said we need the output to be cities data,", "tokens": [50864, 293, 321, 848, 321, 643, 264, 5598, 281, 312, 6486, 1412, 11, 51064], "temperature": 0.0, "avg_logprob": -0.08068819879328162, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.004488452337682247}, {"id": 201, "seek": 80400, "start": 818.0, "end": 821.0, "text": " where in cities data you've got cities,", "tokens": [51064, 689, 294, 6486, 1412, 291, 600, 658, 6486, 11, 51214], "temperature": 0.0, "avg_logprob": -0.08068819879328162, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.004488452337682247}, {"id": 202, "seek": 80400, "start": 821.0, "end": 825.0, "text": " and each city has a name, a country, and a population.", "tokens": [51214, 293, 1184, 2307, 575, 257, 1315, 11, 257, 1941, 11, 293, 257, 4415, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08068819879328162, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.004488452337682247}, {"id": 203, "seek": 80400, "start": 825.0, "end": 828.0, "text": " And then we used a GPT model,", "tokens": [51414, 400, 550, 321, 1143, 257, 26039, 51, 2316, 11, 51564], "temperature": 0.0, "avg_logprob": -0.08068819879328162, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.004488452337682247}, {"id": 204, "seek": 80400, "start": 828.0, "end": 832.0, "text": " and we saw that initially, for the first round,", "tokens": [51564, 293, 321, 1866, 300, 9105, 11, 337, 264, 700, 3098, 11, 51764], "temperature": 0.0, "avg_logprob": -0.08068819879328162, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.004488452337682247}, {"id": 205, "seek": 83200, "start": 832.0, "end": 836.0, "text": " we did get structured output, but it's not valid JSON,", "tokens": [50364, 321, 630, 483, 18519, 5598, 11, 457, 309, 311, 406, 7363, 31828, 11, 50564], "temperature": 0.0, "avg_logprob": -0.06469756866169867, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.00046618832857348025}, {"id": 206, "seek": 83200, "start": 836.0, "end": 840.0, "text": " or it doesn't abide by what we need that object to look like.", "tokens": [50564, 420, 309, 1177, 380, 39663, 538, 437, 321, 643, 300, 2657, 281, 574, 411, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06469756866169867, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.00046618832857348025}, {"id": 207, "seek": 83200, "start": 840.0, "end": 844.0, "text": " So the idea is, what if we provide back to the LLM,", "tokens": [50764, 407, 264, 1558, 307, 11, 437, 498, 321, 2893, 646, 281, 264, 441, 43, 44, 11, 50964], "temperature": 0.0, "avg_logprob": -0.06469756866169867, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.00046618832857348025}, {"id": 208, "seek": 83200, "start": 844.0, "end": 847.0, "text": " the output that it just gave us,", "tokens": [50964, 264, 5598, 300, 309, 445, 2729, 505, 11, 51114], "temperature": 0.0, "avg_logprob": -0.06469756866169867, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.00046618832857348025}, {"id": 209, "seek": 83200, "start": 847.0, "end": 850.0, "text": " with an error message from Pydantic as to why it's wrong,", "tokens": [51114, 365, 364, 6713, 3636, 490, 430, 6655, 7128, 382, 281, 983, 309, 311, 2085, 11, 51264], "temperature": 0.0, "avg_logprob": -0.06469756866169867, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.00046618832857348025}, {"id": 210, "seek": 83200, "start": 850.0, "end": 854.0, "text": " why it doesn't abide by the Pydantic model we just provided.", "tokens": [51264, 983, 309, 1177, 380, 39663, 538, 264, 430, 6655, 7128, 2316, 321, 445, 5649, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06469756866169867, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.00046618832857348025}, {"id": 211, "seek": 83200, "start": 855.0, "end": 859.0, "text": " So the resulting pipeline looks a bit like this", "tokens": [51514, 407, 264, 16505, 15517, 1542, 257, 857, 411, 341, 51714], "temperature": 0.0, "avg_logprob": -0.06469756866169867, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.00046618832857348025}, {"id": 212, "seek": 85900, "start": 859.0, "end": 862.0, "text": " for our GitHub issues demo.", "tokens": [50364, 337, 527, 23331, 2663, 10723, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11448274339948382, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.01462737936526537}, {"id": 213, "seek": 85900, "start": 862.0, "end": 867.0, "text": " We want to provide meeting notes, and we want to provide a schema.", "tokens": [50514, 492, 528, 281, 2893, 3440, 5570, 11, 293, 321, 528, 281, 2893, 257, 34078, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11448274339948382, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.01462737936526537}, {"id": 214, "seek": 85900, "start": 867.0, "end": 872.0, "text": " We give that to the prompt builder, the prompt builder exists in Haystack World.", "tokens": [50764, 492, 976, 300, 281, 264, 12391, 27377, 11, 264, 12391, 27377, 8198, 294, 8721, 372, 501, 3937, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11448274339948382, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.01462737936526537}, {"id": 215, "seek": 85900, "start": 872.0, "end": 876.0, "text": " Then that whole prompt is given to a generator that generates...", "tokens": [51014, 1396, 300, 1379, 12391, 307, 2212, 281, 257, 19265, 300, 23815, 485, 51214], "temperature": 0.0, "avg_logprob": -0.11448274339948382, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.01462737936526537}, {"id": 216, "seek": 85900, "start": 876.0, "end": 882.0, "text": " There's a one pass, like a first attempt at generating some structured output,", "tokens": [51214, 821, 311, 257, 472, 1320, 11, 411, 257, 700, 5217, 412, 17746, 512, 18519, 5598, 11, 51514], "temperature": 0.0, "avg_logprob": -0.11448274339948382, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.01462737936526537}, {"id": 217, "seek": 85900, "start": 882.0, "end": 886.0, "text": " which is then validated by our output validator,", "tokens": [51514, 597, 307, 550, 40693, 538, 527, 5598, 7363, 1639, 11, 51714], "temperature": 0.0, "avg_logprob": -0.11448274339948382, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.01462737936526537}, {"id": 218, "seek": 88600, "start": 886.0, "end": 890.0, "text": " which doesn't exist in Haystack World, so this is a custom component.", "tokens": [50364, 597, 1177, 380, 2514, 294, 8721, 372, 501, 3937, 11, 370, 341, 307, 257, 2375, 6542, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07487187819047407, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.0031729466281831264}, {"id": 219, "seek": 88600, "start": 890.0, "end": 893.0, "text": " And either you're all good, done.", "tokens": [50564, 400, 2139, 291, 434, 439, 665, 11, 1096, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07487187819047407, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.0031729466281831264}, {"id": 220, "seek": 88600, "start": 893.0, "end": 897.0, "text": " But if it's not good, then we go back to the prompt builder", "tokens": [50714, 583, 498, 309, 311, 406, 665, 11, 550, 321, 352, 646, 281, 264, 12391, 27377, 50914], "temperature": 0.0, "avg_logprob": -0.07487187819047407, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.0031729466281831264}, {"id": 221, "seek": 88600, "start": 897.0, "end": 902.0, "text": " with the invalid reply that was produced, plus the error messages.", "tokens": [50914, 365, 264, 34702, 16972, 300, 390, 7126, 11, 1804, 264, 6713, 7897, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07487187819047407, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.0031729466281831264}, {"id": 222, "seek": 88600, "start": 902.0, "end": 908.0, "text": " So for our use case, where you were trying to build this for Haystack,", "tokens": [51164, 407, 337, 527, 764, 1389, 11, 689, 291, 645, 1382, 281, 1322, 341, 337, 8721, 372, 501, 11, 51464], "temperature": 0.0, "avg_logprob": -0.07487187819047407, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.0031729466281831264}, {"id": 223, "seek": 88600, "start": 908.0, "end": 911.0, "text": " this is not accurate, by the way, our labels are not exactly that,", "tokens": [51464, 341, 307, 406, 8559, 11, 538, 264, 636, 11, 527, 16949, 366, 406, 2293, 300, 11, 51614], "temperature": 0.0, "avg_logprob": -0.07487187819047407, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.0031729466281831264}, {"id": 224, "seek": 88600, "start": 911.0, "end": 913.0, "text": " but just for demonstration purposes,", "tokens": [51614, 457, 445, 337, 16520, 9932, 11, 51714], "temperature": 0.0, "avg_logprob": -0.07487187819047407, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.0031729466281831264}, {"id": 225, "seek": 91300, "start": 913.0, "end": 917.0, "text": " we went ahead and built a Pydantic model called issues,", "tokens": [50364, 321, 1437, 2286, 293, 3094, 257, 430, 6655, 7128, 2316, 1219, 2663, 11, 50564], "temperature": 0.0, "avg_logprob": -0.07854784519300548, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.0038799901958554983}, {"id": 226, "seek": 91300, "start": 917.0, "end": 921.0, "text": " and we had to be very specific as to what our labels were,", "tokens": [50564, 293, 321, 632, 281, 312, 588, 2685, 382, 281, 437, 527, 16949, 645, 11, 50764], "temperature": 0.0, "avg_logprob": -0.07854784519300548, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.0038799901958554983}, {"id": 227, "seek": 91300, "start": 921.0, "end": 926.0, "text": " because you can't make a query with a new label that doesn't belong to that repository.", "tokens": [50764, 570, 291, 393, 380, 652, 257, 14581, 365, 257, 777, 7645, 300, 1177, 380, 5784, 281, 300, 25841, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07854784519300548, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.0038799901958554983}, {"id": 228, "seek": 91300, "start": 926.0, "end": 930.0, "text": " And then we used our output validator.", "tokens": [51014, 400, 550, 321, 1143, 527, 5598, 7363, 1639, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07854784519300548, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.0038799901958554983}, {"id": 229, "seek": 91300, "start": 930.0, "end": 934.0, "text": " And then this is where things start to look a bit complicated,", "tokens": [51214, 400, 550, 341, 307, 689, 721, 722, 281, 574, 257, 857, 6179, 11, 51414], "temperature": 0.0, "avg_logprob": -0.07854784519300548, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.0038799901958554983}, {"id": 230, "seek": 91300, "start": 934.0, "end": 937.0, "text": " but the ginger templating is very useful here.", "tokens": [51414, 457, 264, 14966, 9100, 990, 307, 588, 4420, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07854784519300548, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.0038799901958554983}, {"id": 231, "seek": 91300, "start": 937.0, "end": 941.0, "text": " Earlier, for the Hacker News articles, you saw a for loop.", "tokens": [51564, 24552, 11, 337, 264, 389, 23599, 7987, 11290, 11, 291, 1866, 257, 337, 6367, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07854784519300548, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.0038799901958554983}, {"id": 232, "seek": 94100, "start": 941.0, "end": 944.0, "text": " Instead here, we have an if statement.", "tokens": [50364, 7156, 510, 11, 321, 362, 364, 498, 5629, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08531614325263283, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0020866713020950556}, {"id": 233, "seek": 94100, "start": 944.0, "end": 953.0, "text": " So if we have an error message and invalid replies coming in from any component in our pipeline,", "tokens": [50514, 407, 498, 321, 362, 364, 6713, 3636, 293, 34702, 42289, 1348, 294, 490, 604, 6542, 294, 527, 15517, 11, 50964], "temperature": 0.0, "avg_logprob": -0.08531614325263283, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0020866713020950556}, {"id": 234, "seek": 94100, "start": 953.0, "end": 957.0, "text": " then this little section here, you already created the following output,", "tokens": [50964, 550, 341, 707, 3541, 510, 11, 291, 1217, 2942, 264, 3480, 5598, 11, 51164], "temperature": 0.0, "avg_logprob": -0.08531614325263283, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0020866713020950556}, {"id": 235, "seek": 94100, "start": 957.0, "end": 962.0, "text": " yada yada, is appended to the full prompt.", "tokens": [51164, 288, 1538, 288, 1538, 11, 307, 724, 3502, 281, 264, 1577, 12391, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08531614325263283, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0020866713020950556}, {"id": 236, "seek": 94100, "start": 962.0, "end": 967.0, "text": " Again, at the end of the day, we ended up with a pipeline that looks a bit like this.", "tokens": [51414, 3764, 11, 412, 264, 917, 295, 264, 786, 11, 321, 4590, 493, 365, 257, 15517, 300, 1542, 257, 857, 411, 341, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08531614325263283, "compression_ratio": 1.6201923076923077, "no_speech_prob": 0.0020866713020950556}, {"id": 237, "seek": 96700, "start": 967.0, "end": 971.0, "text": " So do I have time? I do have time, right?", "tokens": [50364, 407, 360, 286, 362, 565, 30, 286, 360, 362, 565, 11, 558, 30, 50564], "temperature": 0.0, "avg_logprob": -0.1509344736735026, "compression_ratio": 1.502415458937198, "no_speech_prob": 0.03622518852353096}, {"id": 238, "seek": 96700, "start": 971.0, "end": 979.0, "text": " Four minutes. Okay, so the last thing I wanted to show is how these pipeline connections are actually defined in Haystack.", "tokens": [50564, 7451, 2077, 13, 1033, 11, 370, 264, 1036, 551, 286, 1415, 281, 855, 307, 577, 613, 15517, 9271, 366, 767, 7642, 294, 8721, 372, 501, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1509344736735026, "compression_ratio": 1.502415458937198, "no_speech_prob": 0.03622518852353096}, {"id": 239, "seek": 96700, "start": 979.0, "end": 982.0, "text": " Oh, great. Okay, I have plenty of time.", "tokens": [50964, 876, 11, 869, 13, 1033, 11, 286, 362, 7140, 295, 565, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1509344736735026, "compression_ratio": 1.502415458937198, "no_speech_prob": 0.03622518852353096}, {"id": 240, "seek": 96700, "start": 982.0, "end": 985.0, "text": " All right, so ignore the corgis running around.", "tokens": [51114, 1057, 558, 11, 370, 11200, 264, 1181, 70, 271, 2614, 926, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1509344736735026, "compression_ratio": 1.502415458937198, "no_speech_prob": 0.03622518852353096}, {"id": 241, "seek": 96700, "start": 985.0, "end": 989.0, "text": " Can everyone see this, or should I make this bigger?", "tokens": [51264, 1664, 1518, 536, 341, 11, 420, 820, 286, 652, 341, 3801, 30, 51464], "temperature": 0.0, "avg_logprob": -0.1509344736735026, "compression_ratio": 1.502415458937198, "no_speech_prob": 0.03622518852353096}, {"id": 242, "seek": 96700, "start": 989.0, "end": 991.0, "text": " Okay.", "tokens": [51464, 1033, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1509344736735026, "compression_ratio": 1.502415458937198, "no_speech_prob": 0.03622518852353096}, {"id": 243, "seek": 99700, "start": 998.0, "end": 1004.0, "text": " All right, so I told you that before the Hacker News Fetcher component was very much pseudocode,", "tokens": [50414, 1057, 558, 11, 370, 286, 1907, 291, 300, 949, 264, 389, 23599, 7987, 479, 302, 6759, 6542, 390, 588, 709, 25505, 532, 905, 1429, 11, 50714], "temperature": 0.0, "avg_logprob": -0.1484600086601413, "compression_ratio": 1.5021276595744681, "no_speech_prob": 0.007820687256753445}, {"id": 244, "seek": 99700, "start": 1004.0, "end": 1006.0, "text": " this is kind of boring.", "tokens": [50714, 341, 307, 733, 295, 9989, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1484600086601413, "compression_ratio": 1.5021276595744681, "no_speech_prob": 0.007820687256753445}, {"id": 245, "seek": 99700, "start": 1006.0, "end": 1011.0, "text": " We're basically making requests to the API and getting the articles.", "tokens": [50814, 492, 434, 1936, 1455, 12475, 281, 264, 9362, 293, 1242, 264, 11290, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1484600086601413, "compression_ratio": 1.5021276595744681, "no_speech_prob": 0.007820687256753445}, {"id": 246, "seek": 99700, "start": 1011.0, "end": 1016.0, "text": " Here, we're going to be using mixed trial through hugging face TGI.", "tokens": [51064, 1692, 11, 321, 434, 516, 281, 312, 1228, 7467, 7308, 807, 41706, 1851, 314, 26252, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1484600086601413, "compression_ratio": 1.5021276595744681, "no_speech_prob": 0.007820687256753445}, {"id": 247, "seek": 99700, "start": 1016.0, "end": 1022.0, "text": " Hugging face TGI is free, but it is rate limited, and you need to provide an API key to use it.", "tokens": [51314, 46892, 3249, 1851, 314, 26252, 307, 1737, 11, 457, 309, 307, 3314, 5567, 11, 293, 291, 643, 281, 2893, 364, 9362, 2141, 281, 764, 309, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1484600086601413, "compression_ratio": 1.5021276595744681, "no_speech_prob": 0.007820687256753445}, {"id": 248, "seek": 102200, "start": 1022.0, "end": 1027.0, "text": " So you can go ahead and use this collab, but you do have to provide an API key.", "tokens": [50364, 407, 291, 393, 352, 2286, 293, 764, 341, 44228, 11, 457, 291, 360, 362, 281, 2893, 364, 9362, 2141, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06963318365591543, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.0108311977237463}, {"id": 249, "seek": 102200, "start": 1027.0, "end": 1036.0, "text": " And then you see the prompt template you saw, and here's what's going on in the Haystack pipeline itself.", "tokens": [50614, 400, 550, 291, 536, 264, 12391, 12379, 291, 1866, 11, 293, 510, 311, 437, 311, 516, 322, 294, 264, 8721, 372, 501, 15517, 2564, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06963318365591543, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.0108311977237463}, {"id": 250, "seek": 102200, "start": 1036.0, "end": 1038.0, "text": " We've got our prompt builder.", "tokens": [51064, 492, 600, 658, 527, 12391, 27377, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06963318365591543, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.0108311977237463}, {"id": 251, "seek": 102200, "start": 1038.0, "end": 1043.0, "text": " We've got our LLM, so mixed trial via hugging face TGI.", "tokens": [51164, 492, 600, 658, 527, 441, 43, 44, 11, 370, 7467, 7308, 5766, 41706, 1851, 314, 26252, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06963318365591543, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.0108311977237463}, {"id": 252, "seek": 102200, "start": 1043.0, "end": 1046.0, "text": " We just created the Hacker News Fetcher.", "tokens": [51414, 492, 445, 2942, 264, 389, 23599, 7987, 479, 302, 6759, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06963318365591543, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.0108311977237463}, {"id": 253, "seek": 102200, "start": 1046.0, "end": 1050.0, "text": " What we do is simply add those to the Haystack pipeline.", "tokens": [51564, 708, 321, 360, 307, 2935, 909, 729, 281, 264, 8721, 372, 501, 15517, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06963318365591543, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.0108311977237463}, {"id": 254, "seek": 105000, "start": 1051.0, "end": 1058.0, "text": " And then this is where Haystack can be quite verbose, but it can also mean that you can be very,", "tokens": [50414, 400, 550, 341, 307, 689, 8721, 372, 501, 393, 312, 1596, 9595, 541, 11, 457, 309, 393, 611, 914, 300, 291, 393, 312, 588, 11, 50764], "temperature": 0.0, "avg_logprob": -0.07663414875666301, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.003239783924072981}, {"id": 255, "seek": 105000, "start": 1058.0, "end": 1062.0, "text": " you can create very custom pipelines, and it can get a bit crazy.", "tokens": [50764, 291, 393, 1884, 588, 2375, 40168, 11, 293, 309, 393, 483, 257, 857, 3219, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07663414875666301, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.003239783924072981}, {"id": 256, "seek": 105000, "start": 1062.0, "end": 1066.0, "text": " You can have pipelines that branch out, loop back in, et cetera.", "tokens": [50964, 509, 393, 362, 40168, 300, 9819, 484, 11, 6367, 646, 294, 11, 1030, 11458, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07663414875666301, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.003239783924072981}, {"id": 257, "seek": 105000, "start": 1066.0, "end": 1076.0, "text": " We're being very specific that Hacker News Fetcher's articles output is being provided to prompt builder articles,", "tokens": [51164, 492, 434, 885, 588, 2685, 300, 389, 23599, 7987, 479, 302, 6759, 311, 11290, 5598, 307, 885, 5649, 281, 12391, 27377, 11290, 11, 51664], "temperature": 0.0, "avg_logprob": -0.07663414875666301, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.003239783924072981}, {"id": 258, "seek": 105000, "start": 1076.0, "end": 1079.0, "text": " which is going in here.", "tokens": [51664, 597, 307, 516, 294, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07663414875666301, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.003239783924072981}, {"id": 259, "seek": 107900, "start": 1079.0, "end": 1088.0, "text": " And then finally, the only thing missing to actually run this is Hacker News Fetcher is the only component here", "tokens": [50364, 400, 550, 2721, 11, 264, 787, 551, 5361, 281, 767, 1190, 341, 307, 389, 23599, 7987, 479, 302, 6759, 307, 264, 787, 6542, 510, 50814], "temperature": 0.0, "avg_logprob": -0.058961709340413414, "compression_ratio": 1.7412280701754386, "no_speech_prob": 0.0031557471957057714}, {"id": 260, "seek": 107900, "start": 1088.0, "end": 1090.0, "text": " that is missing an input.", "tokens": [50814, 300, 307, 5361, 364, 4846, 13, 50914], "temperature": 0.0, "avg_logprob": -0.058961709340413414, "compression_ratio": 1.7412280701754386, "no_speech_prob": 0.0031557471957057714}, {"id": 261, "seek": 107900, "start": 1090.0, "end": 1095.0, "text": " All of the rest have been provided inputs through the pipeline itself.", "tokens": [50914, 1057, 295, 264, 1472, 362, 668, 5649, 15743, 807, 264, 15517, 2564, 13, 51164], "temperature": 0.0, "avg_logprob": -0.058961709340413414, "compression_ratio": 1.7412280701754386, "no_speech_prob": 0.0031557471957057714}, {"id": 262, "seek": 107900, "start": 1095.0, "end": 1102.0, "text": " So I can then define what the input of Hacker News Fetcher is when I do pipeline.run or pipe.run.", "tokens": [51164, 407, 286, 393, 550, 6964, 437, 264, 4846, 295, 389, 23599, 7987, 479, 302, 6759, 307, 562, 286, 360, 15517, 13, 12997, 420, 11240, 13, 12997, 13, 51514], "temperature": 0.0, "avg_logprob": -0.058961709340413414, "compression_ratio": 1.7412280701754386, "no_speech_prob": 0.0031557471957057714}, {"id": 263, "seek": 107900, "start": 1102.0, "end": 1107.0, "text": " And then, optionally, you can also give more inputs that are not necessary for the others,", "tokens": [51514, 400, 550, 11, 3614, 379, 11, 291, 393, 611, 976, 544, 15743, 300, 366, 406, 4818, 337, 264, 2357, 11, 51764], "temperature": 0.0, "avg_logprob": -0.058961709340413414, "compression_ratio": 1.7412280701754386, "no_speech_prob": 0.0031557471957057714}, {"id": 264, "seek": 110700, "start": 1107.0, "end": 1113.0, "text": " but for example, here, I'm using mixed-rull, and I wanted to up the created max tokens at the end,", "tokens": [50364, 457, 337, 1365, 11, 510, 11, 286, 478, 1228, 7467, 12, 81, 858, 11, 293, 286, 1415, 281, 493, 264, 2942, 11469, 22667, 412, 264, 917, 11, 50664], "temperature": 0.0, "avg_logprob": -0.15123936067144556, "compression_ratio": 1.455958549222798, "no_speech_prob": 0.009758362546563148}, {"id": 265, "seek": 110700, "start": 1113.0, "end": 1116.0, "text": " so I can also provide that at runtime.", "tokens": [50664, 370, 286, 393, 611, 2893, 300, 412, 34474, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15123936067144556, "compression_ratio": 1.455958549222798, "no_speech_prob": 0.009758362546563148}, {"id": 266, "seek": 110700, "start": 1116.0, "end": 1118.0, "text": " And that's it.", "tokens": [50814, 400, 300, 311, 309, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15123936067144556, "compression_ratio": 1.455958549222798, "no_speech_prob": 0.009758362546563148}, {"id": 267, "seek": 110700, "start": 1118.0, "end": 1120.0, "text": " Thank you very much.", "tokens": [50914, 1044, 291, 588, 709, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15123936067144556, "compression_ratio": 1.455958549222798, "no_speech_prob": 0.009758362546563148}, {"id": 268, "seek": 110700, "start": 1120.0, "end": 1124.0, "text": " And you can also access the GitHub issues pipelines here,", "tokens": [51014, 400, 291, 393, 611, 2105, 264, 23331, 2663, 40168, 510, 11, 51214], "temperature": 0.0, "avg_logprob": -0.15123936067144556, "compression_ratio": 1.455958549222798, "no_speech_prob": 0.009758362546563148}, {"id": 269, "seek": 110700, "start": 1124.0, "end": 1127.0, "text": " but I'm happy to take questions if there are any.", "tokens": [51214, 457, 286, 478, 2055, 281, 747, 1651, 498, 456, 366, 604, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15123936067144556, "compression_ratio": 1.455958549222798, "no_speech_prob": 0.009758362546563148}, {"id": 270, "seek": 112700, "start": 1127.0, "end": 1132.0, "text": " Thank you very much.", "tokens": [50364, 1044, 291, 588, 709, 13, 50614], "temperature": 0.6, "avg_logprob": -0.5130336216517857, "compression_ratio": 1.1282051282051282, "no_speech_prob": 0.0687405988574028}, {"id": 271, "seek": 112700, "start": 1132.0, "end": 1134.0, "text": " Thank you.", "tokens": [50614, 1044, 291, 13, 50714], "temperature": 0.6, "avg_logprob": -0.5130336216517857, "compression_ratio": 1.1282051282051282, "no_speech_prob": 0.0687405988574028}, {"id": 272, "seek": 112700, "start": 1134.0, "end": 1142.0, "text": " Thank you.", "tokens": [50714, 1044, 291, 13, 51114], "temperature": 0.6, "avg_logprob": -0.5130336216517857, "compression_ratio": 1.1282051282051282, "no_speech_prob": 0.0687405988574028}, {"id": 273, "seek": 112700, "start": 1142.0, "end": 1144.0, "text": " Hi.", "tokens": [51114, 2421, 13, 51214], "temperature": 0.6, "avg_logprob": -0.5130336216517857, "compression_ratio": 1.1282051282051282, "no_speech_prob": 0.0687405988574028}, {"id": 274, "seek": 112700, "start": 1144.0, "end": 1149.0, "text": " So in the Hacker News article summarizer,", "tokens": [51214, 407, 294, 264, 389, 23599, 7987, 7222, 14611, 6545, 11, 51464], "temperature": 0.6, "avg_logprob": -0.5130336216517857, "compression_ratio": 1.1282051282051282, "no_speech_prob": 0.0687405988574028}, {"id": 275, "seek": 114900, "start": 1149.0, "end": 1155.84, "text": " to the LLM, the URL, and asking it to both summarize the article and also just print", "tokens": [50364, 281, 264, 441, 43, 44, 11, 264, 12905, 11, 293, 3365, 309, 281, 1293, 20858, 264, 7222, 293, 611, 445, 4482, 50706], "temperature": 0.0, "avg_logprob": -0.15448473706657503, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.5925475358963013}, {"id": 276, "seek": 114900, "start": 1155.84, "end": 1161.64, "text": " back the URL. That appears to me a bit risky because it might change the URL. Do you consider", "tokens": [50706, 646, 264, 12905, 13, 663, 7038, 281, 385, 257, 857, 21137, 570, 309, 1062, 1319, 264, 12905, 13, 1144, 291, 1949, 50996], "temperature": 0.0, "avg_logprob": -0.15448473706657503, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.5925475358963013}, {"id": 277, "seek": 114900, "start": 1161.64, "end": 1166.12, "text": " it best practice to pass the URL through some other way, or do you find it fine to always", "tokens": [50996, 309, 1151, 3124, 281, 1320, 264, 12905, 807, 512, 661, 636, 11, 420, 360, 291, 915, 309, 2489, 281, 1009, 51220], "temperature": 0.0, "avg_logprob": -0.15448473706657503, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.5925475358963013}, {"id": 278, "seek": 114900, "start": 1166.12, "end": 1174.6, "text": " ask the LLM to do that?", "tokens": [51220, 1029, 264, 441, 43, 44, 281, 360, 300, 30, 51644], "temperature": 0.0, "avg_logprob": -0.15448473706657503, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.5925475358963013}, {"id": 279, "seek": 117460, "start": 1174.6, "end": 1179.48, "text": " I love this question because try that hugging face space, especially with mixed trial a", "tokens": [50364, 286, 959, 341, 1168, 570, 853, 300, 41706, 1851, 1901, 11, 2318, 365, 7467, 7308, 257, 50608], "temperature": 0.0, "avg_logprob": -0.23728127706618535, "compression_ratio": 1.5570175438596492, "no_speech_prob": 0.296076238155365}, {"id": 280, "seek": 117460, "start": 1179.48, "end": 1187.52, "text": " few times and sometimes you just won't get the URL. Yes, and there are a few ways to make", "tokens": [50608, 1326, 1413, 293, 2171, 291, 445, 1582, 380, 483, 264, 12905, 13, 1079, 11, 293, 456, 366, 257, 1326, 2098, 281, 652, 51010], "temperature": 0.0, "avg_logprob": -0.23728127706618535, "compression_ratio": 1.5570175438596492, "no_speech_prob": 0.296076238155365}, {"id": 281, "seek": 117460, "start": 1187.52, "end": 1193.76, "text": " this a lot better because actually the Hack and Use Fetcher component itself earlier is", "tokens": [51010, 341, 257, 688, 1101, 570, 767, 264, 35170, 293, 8278, 479, 302, 6759, 6542, 2564, 3071, 307, 51322], "temperature": 0.0, "avg_logprob": -0.23728127706618535, "compression_ratio": 1.5570175438596492, "no_speech_prob": 0.296076238155365}, {"id": 282, "seek": 117460, "start": 1193.76, "end": 1199.84, "text": " just an API called Hack and Use and you have the URL there. So probably the best practice", "tokens": [51322, 445, 364, 9362, 1219, 35170, 293, 8278, 293, 291, 362, 264, 12905, 456, 13, 407, 1391, 264, 1151, 3124, 51626], "temperature": 0.0, "avg_logprob": -0.23728127706618535, "compression_ratio": 1.5570175438596492, "no_speech_prob": 0.296076238155365}, {"id": 283, "seek": 119984, "start": 1199.84, "end": 1205.6799999999998, "text": " here would be to have the LLM only produce summaries and the other component provide", "tokens": [50364, 510, 576, 312, 281, 362, 264, 441, 43, 44, 787, 5258, 8367, 4889, 293, 264, 661, 6542, 2893, 50656], "temperature": 0.0, "avg_logprob": -0.16726551055908204, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.20223036408424377}, {"id": 284, "seek": 119984, "start": 1205.6799999999998, "end": 1213.6, "text": " an output of the URLs that was used to produce those summaries because yes, my experiences", "tokens": [50656, 364, 5598, 295, 264, 43267, 300, 390, 1143, 281, 5258, 729, 8367, 4889, 570, 2086, 11, 452, 5235, 51052], "temperature": 0.0, "avg_logprob": -0.16726551055908204, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.20223036408424377}, {"id": 285, "seek": 119984, "start": 1213.6, "end": 1219.04, "text": " a lot of the OpenAI GPT models do a great job of following that instruction, reference", "tokens": [51052, 257, 688, 295, 264, 7238, 48698, 26039, 51, 5245, 360, 257, 869, 1691, 295, 3480, 300, 10951, 11, 6408, 51324], "temperature": 0.0, "avg_logprob": -0.16726551055908204, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.20223036408424377}, {"id": 286, "seek": 119984, "start": 1219.04, "end": 1226.08, "text": " this specific URL, but this is very much LLM based and how that large language model expects", "tokens": [51324, 341, 2685, 12905, 11, 457, 341, 307, 588, 709, 441, 43, 44, 2361, 293, 577, 300, 2416, 2856, 2316, 33280, 51676], "temperature": 0.0, "avg_logprob": -0.16726551055908204, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.20223036408424377}, {"id": 287, "seek": 122608, "start": 1226.08, "end": 1235.36, "text": " to be prompted. Not every instruction works the same way with every model. Any other questions?", "tokens": [50364, 281, 312, 31042, 13, 1726, 633, 10951, 1985, 264, 912, 636, 365, 633, 2316, 13, 2639, 661, 1651, 30, 50828], "temperature": 0.0, "avg_logprob": -0.286735431007717, "compression_ratio": 1.381679389312977, "no_speech_prob": 0.21787960827350616}, {"id": 288, "seek": 122608, "start": 1235.36, "end": 1247.6, "text": " Oh, this one. Thanks for the presentation. I have a question on the prompt. I saw for", "tokens": [50828, 876, 11, 341, 472, 13, 2561, 337, 264, 5860, 13, 286, 362, 257, 1168, 322, 264, 12391, 13, 286, 1866, 337, 51440], "temperature": 0.0, "avg_logprob": -0.286735431007717, "compression_ratio": 1.381679389312977, "no_speech_prob": 0.21787960827350616}, {"id": 289, "seek": 124760, "start": 1247.6, "end": 1263.9199999999998, "text": " an if, is it specific to Haystack or? Not at all. We use Ginger for the templating", "tokens": [50364, 364, 498, 11, 307, 309, 2685, 281, 8721, 372, 501, 420, 30, 1726, 412, 439, 13, 492, 764, 34637, 337, 264, 9100, 990, 51180], "temperature": 0.0, "avg_logprob": -0.2886255291146292, "compression_ratio": 1.3670212765957446, "no_speech_prob": 0.178972065448761}, {"id": 290, "seek": 124760, "start": 1263.9199999999998, "end": 1269.9199999999998, "text": " language. Actually, I will add a link to the Ginger documentation in the speaker notes", "tokens": [51180, 2856, 13, 5135, 11, 286, 486, 909, 257, 2113, 281, 264, 34637, 14333, 294, 264, 8145, 5570, 51480], "temperature": 0.0, "avg_logprob": -0.2886255291146292, "compression_ratio": 1.3670212765957446, "no_speech_prob": 0.178972065448761}, {"id": 291, "seek": 124760, "start": 1269.9199999999998, "end": 1276.8799999999999, "text": " of this slide deck that you'll find on FOSM2, but that's all Ginger syntax, which comes", "tokens": [51480, 295, 341, 4137, 9341, 300, 291, 603, 915, 322, 479, 4367, 44, 17, 11, 457, 300, 311, 439, 34637, 28431, 11, 597, 1487, 51828], "temperature": 0.0, "avg_logprob": -0.2886255291146292, "compression_ratio": 1.3670212765957446, "no_speech_prob": 0.178972065448761}, {"id": 292, "seek": 127688, "start": 1276.88, "end": 1280.8000000000002, "text": " very handy because you get for loops, if loops, and you can actually start defining", "tokens": [50364, 588, 13239, 570, 291, 483, 337, 16121, 11, 498, 16121, 11, 293, 291, 393, 767, 722, 17827, 50560], "temperature": 0.0, "avg_logprob": -0.30214009682337445, "compression_ratio": 1.375886524822695, "no_speech_prob": 0.0833234190940857}, {"id": 293, "seek": 127688, "start": 1280.8000000000002, "end": 1288.24, "text": " your own custom functions for that Ginger templating as well.", "tokens": [50560, 428, 1065, 2375, 6828, 337, 300, 34637, 9100, 990, 382, 731, 13, 50932], "temperature": 0.0, "avg_logprob": -0.30214009682337445, "compression_ratio": 1.375886524822695, "no_speech_prob": 0.0833234190940857}, {"id": 294, "seek": 127688, "start": 1288.24, "end": 1299.2800000000002, "text": " All right. Give her a round of applause, please.", "tokens": [50932, 1057, 558, 13, 5303, 720, 257, 3098, 295, 9969, 11, 1767, 13, 51484], "temperature": 0.0, "avg_logprob": -0.30214009682337445, "compression_ratio": 1.375886524822695, "no_speech_prob": 0.0833234190940857}], "language": "en"}