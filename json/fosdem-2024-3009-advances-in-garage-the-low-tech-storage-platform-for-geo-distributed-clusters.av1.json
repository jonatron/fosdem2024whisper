{"text": " You can't hear me at the back. Do not hesitate to ask for me to speak louder. So I'm Alex and I'm a co-founder of the DeFloor Association, which is a non-profit self-hosting collective. We're a member of the Chateau Network in France. And so what that means is we're doing self-hosting and we're trying to promote self-hosting as an alternative to putting everything in large data centers and relying on cloud providers. The thing is actually doing this is relatively hard and mostly it's hard because we want systems that are reliable, which are available most of the time. And if you have computers at home, you have a lot of issues. In particular, the computers we're using at DeFloor are these kind of computers, very cheap old desktop computers. They're not meant to be servers and we expect that they could crash at any time. These are some other examples that we had and those are still used actually. So these are also old desktop computers and we have some system which is based on only these kinds of machines. So they can die. We also have issues possibly with the internet where the electricity connection because we're at home so we don't have redundancy. It can go at any time. And to alleviate these issues, what we do is that we do distributed systems and we have a multi-site geo-replicated cluster. And so in our case, the DeFloor cluster is in three places. There's some nodes in Brussels here, some nodes in Lille and some nodes in Paris. And basically the aim is to build a system that makes use of some cheap hardware which is disseminated in all of these locations and they can basically relay one another when there's an issue somewhere and the whole thing stays up even if there are issues in individual locations. And so this is one of the reasons why I call this a low-tech platform because we're using what we have at hand, cheap machines and regular internet connections. One of the main components in this platform is object storage. And so I will not enter too much into why object storage except that it's very adapted to flexible deployments which are kind of inspired by what is done in the cloud. And indeed, Amazon S3 was created as a cloud product and in 2006 was introduced. And it became since then a de facto standard and many applications are compatible with this object storage. And so it makes sense to base our infrastructure on this kind of software because we can just like plug and play all kinds of various things which are already able to use this kind of storage layer as a backend. There were many actually alternative implementations of S3. MENU is one of the most common ones. I think CEPH is also an implementation. What we discovered is actually that these implementations are not very well suited to geo-distributed deployments. So deployments where nodes are in remote locations because in such case you will have higher latency between the nodes and it can cause issues and the system is basically a bit slower. And sometimes it's even really unusable. So Garage was made specifically for this use case. We make use of distributed systems theory, CRDT in particular, which I will talk about later. And this is basically the aim is to provide a drop-in replacement for Amazon S3 or S3-compatible storage systems which is available, possible to run directly on this kind of geo-distributed cluster and the data will be replicated at several locations and it's kind of transparent and it's supposed to be reasonably fast, not completely slow down all the replications which are running on it. One of the main ways we were able to achieve this outcome was to use CRDT and weak consistency. So this is a bit theoretical explanation of what is going on in Garage and I will have another slide talking about this later. But basically we're trying to avoid so-called consensus algorithms like RAF or PAXOS because these algorithms have issues and are actually very sensitive to latency. But just to list the issues in a clear way, the first of them is software complexity. I think RAF is actually a complex piece of software and it can be implemented badly and if you do it wrong it can lead to various unacceptable outcomes. And of course the issue of performance which I've talked about already. Those algorithms like RAF are using a leader so the leader is becoming a bottleneck for most requests in the system. So if you cannot really scale if you have a naive strategy with just one leader in the system it's also sensitive to higher latency because if the leader happens to be a node in a very far away location, well everything has to transit from there and then come back. And so if the leader happens to be the wrong node everything is going to be much slower in the system. And also if the system is disrupted and the leader goes down the system will have to take some time to reconverge and it's actually something that can take a long time especially if the latency between nodes is high and those are not able to communicate very efficiently. And so for this reason we made Garage a completely different design which is based entirely on CRDT internally which kind of solves most of these issues. Object storage is very likely very similar to basic key value store except that the values are objects like big blobs of data. And so here we have an example where we have the key which is there's no notion of a file system hierarchy so we will just have the entire path in the key with the slash it doesn't have any specific meaning. And the value is like some metadata here it's inspired from the HTTP headers because it's very strongly based on the HTTP semantics and then you have the binary data for each of your files. It happens that this semantics key value actually maps very well to CRDT and this is why we were able to make this work. So just to convince you in one slide that this is actually a worthwhile trade off this is one of the best results we have for Garage and it's a performance comparison for Garage versus Migno. So it's a simulated deployment where we have nodes which are simulated on a single machine and we add some artificial latency between the nodes. And so here we have nodes with 50 milliseconds so pretty long delay between them and so basically we can see that they take some duration which is a multiple of the round trip time latency but for Migno it's a very high multiple so some very common requests like remove object or put object will take like more than one second and for Garage we were able to bring this down to somewhere between 300, 500 milliseconds. So quite an improvement. So the main focus of this talk is to basically discuss recent developments in Garage because so we were here at Fosdame two years ago and I think maybe lots of people in the room are already aware of Garage. So yeah two years ago we were at the beginning of a grant by NGI Pointer and which was the first grant and it allowed us to bring this version 0.6.0 which was the first like public beta version that we launched. So it was like a point where we considered that we had some basic feature which was pretty good actually and we could ask people to come and actually many people were interested and this is the point where we started also to have some external contributions to the project. So we did Fosdame about at the time. In April we did version 0.7 and so version 0.7 was so focused mostly on observability and integration with the ecosystem. So we added support for metrics and traces using OpenTelemetry which is a standard for exporting observability data. We also added some flexibility because while we had originally built the system like supposed to have three copies of everything so we would expect to have nodes in three different data centers actually people were also willing to use the system with less copies so we added one or two copies and we also had some weaker consistency which was useful to like make the system faster or help recover data in some scenarios. We also added integration with Kubernetes for discovery of nodes so that the cluster is able to set up automatically the links between each nodes and we also added an administration API which is useful to like set up the cluster. It's basically a very simple REST API where you can create buckets which are stored spaces, create access keys, give rights to access keys, etc. I will just show a little bit about the monitoring part. So this is a graphnet dashboard that we made for Garage and as you can see it's actually pretty complete. We can monitor so here is the request going on through the S3 API endpoints and here's the request going through the web endpoints because Garage supports serving buckets directly as websites which is something we make heavy use of at Le Fleur. Here we have the error rate and more interestingly here we have some internal metrics so like this is the data which is being read and written to the disk on the nodes. This is some internal metrics for the communications between nodes RPC and these are some queues so how much data is remaining to be processed and so yeah just quick note here the GCQ is common points where people are like why is this queue not going to zero? It's normal that it's not going to zero because items are staying in the queue for 24 hours before they're processed just for information. So basically this queue should almost be to zero and this one too and if it's not then probably your system is under too much load. And we also have tracing so if you want to go further into like how Garage is handling a request you can use this feature. So here we're exporting traces to Yeager and this is a trace of a pretty standard list objects API call and so we can see that the list objects is first reading some data to get some access information on the access key and the buckets. So this is some very fast call because all this information is copied on the node and it can just read it locally and then it's going to do some actual requesting on remote nodes for the list of objects that should return and we see here that it's sending a request to two nodes and the request is taking a bit of time before it completes and then so yeah I think this is a pretty slow cluster and it's taking 100 milliseconds but on faster hardware it can be of course much faster. So this was 0.7 and then we did 0.8 so that was at the end of the NGI pointer grant and for 0.8 we had a pretty high focus on making the performance better. So first thing we did was like change the metadata engine because we were using sled and it had a lot of issues I'll talk about that and we did some various performance improvements across the board making basically some pretty good improvements in this kind of area and in terms of features we added CODAS so this is not a feature from Amazon but it's a feature which you can add on Garage is like limit the size of a bucket to a maximum size of objects or maximum number of objects and it's pretty useful in a multi-tonnets setup where we'd like to lend some storage space to someone but have them restrain to some fixed capacity and of course some regular developments on quality of life improvements etc. So yeah just to talk a little bit about the metadata engine so we were using sled which is a metadata key value store embedded key value store which is written in Rust so we thought yeah it's written in Rust it's pretty good Garage is also written in Rust so let's just integrate them and at the point when we started Garage sled was like one of the most popular key value stores for Rust but actually it's not very well maintained anymore and it had many issues so it was making very large files on disk because it was like just writing and writing and writing and probably it was some internal way to optimize performance but it was not very satisfactory for us to have like data files that were 10 times too big. The performance was also pretty unpredictable on spinning hard drives it was actually very bad and also from a developer perspective it has some API limitations and this has prevented us from implementing some specific features in Garage and hopefully when we get rid of sled we can actually do that. So as an alternative we added LMDB so LMDB is a key value storage which is used I think in OpenLDAP and some other software and it's a pretty established piece of software at this point so we consider it pretty stable it has good performance and it maintains a reasonable size of files on disk so this is the default now and we also have SQLite as a second choice originally we had not optimized SQLite that much so it was not recommended we had not made another bunch of tests but probably now it's okay to use as well and just to show some comparison we did some benchmarks and basically LMDB is much faster pretty much twice as fast as sled not really twice but actually significantly faster and for all these common API endpoints and SQLite was not optimized at that time I cannot I do not have the data updated for now. Another optimization we made is block streaming so the idea here is that Garage will store your data so when it receives an object it will split the object into pieces of by default 1 megabyte and then store these pieces on data servers all around the cluster and then when you want to read the data well your API request is going to go through some Garage node which is going to receive the request is going to look at the objects the metadata and determine okay we have to get this part this part this part from these different nodes in the cluster so it's going to do an internal RPC request to the storage node which has the actual 1 megabyte data block and so this is how it was working before basically this first node that was receiving the API request it would like just read the 1 megabyte into RAM and not send anything to the client before so basically here the client is just waiting for the data to arrive and the data is being transferred here between these two nodes between inside the cluster and so basically the client is just waiting for some stuff to happen inside the cluster where it could just have received some data earlier and so the optimization we made was actually pretty simple but it's pretty big change in the code it was to start sending the data as soon as it arrives to this intermediate node and so here we just have a small buffer of data which is received and waiting to be sent back to the client and so by doing this pretty small change we actually managed to reduce the time to first byte measurement so this measurement is when you do a request to Garage to receive to get an object you will specify the path of the object send your HTTP request all the headers etc and then you will wait for the server to reply the server will give you some headers saying okay the object is coming and then he will start streaming some data and so this measures the time between you the point where you start sending a request and the moment where the first actual bytes of the data file are coming back and here we are in a actually again it's a simulated deployment but we have pretty slow networking so 5 megabits per second so it's actually very slow and so before the optimization garage was here so we would have to wait pretty much two seconds before some data was coming because the like a one megabyte file was being transferred out this very slow connection before it could be returned. Minio has some average performance here and with the optimization garage is very fast and we're able to return the first bytes of the data and so this is important because for instance for websites you want to display the content as fast as possible and even if it's a big file then maybe the first bytes are very relevant so for an image you can have a preview in the first bytes for an HTML file we can have pretty much everything and so minimizing this time is very critical to user experience. So I think we pretty much managed to do this and we also did some other various improvements on the code pass and garage so on the bottom we have 0.7 then we have 0.8 beta 1 beta 2 here we removed some F-sync and it's completely optional to have F-sync and we're almost matching so here is like raw throughput when you're reading and writing big objects continuously to garage the throughput is still a bit worse than Minio but it's actually getting pretty close so there's still room for improvement in this domain and it's yeah we haven't done much more work on this but it's definitely something that could still be optimized I believe. So then it was the end of the NGI pointer grants so we did a bunch of conferences in France this was not me this was other people from Duffleur and then we started another grant by NGI 0 through NLNet and this led to the release of 0.9 and so 0.9 was actually a pretty big release so yeah we had a support for multiple AGDs per node and this is actually a pretty big feature because now you can have one garage node which is directly talking to the hard drive and you don't have to do some pooling at the file system level or some RAID system basically you will just format each of your drives independently as a file system and each of them has a directory, a mount point and garage will just use all of these mount points and like share the data between the drives. This is probably the model which allows for the best performance on the server with multiple drives. We also added some features for S3 compatibility so we added support for basic lifecycle and lifecycle is a feature where it allows you to clean basically some stuff which is going on in the bucket and so for instance in S3 you can start uploading an object using a multi-part upload so multi-part upload means you're initiating the upload at one point and then you're going to do individual requests to add pieces of the file and then once you're finished you do a complete request and then the files get uploaded that gets stored completely in the system and so it could happen that these multi-parts upload they get aborted in the middle you never get to finish the the the requests and in this case there's some data that's lying around in the cluster and so if you configure a lifecycle using this is a very standard S3 API if you support if you configure a lifecycle in your brackets you can basically get rid of all this tail data after say a delay of one day or something like that. And another thing we added for S3 compatibility is retries of multi-parts upload and this was actually because in S3 if you fail a part you can because maybe your network was broken you can try again this part and you can still complete your multi-part upload and in the first versions of garage we did not have that and you would have to restart the upload from the beginning now you can resume only a single part. LMDB is now by default we're deprecating SLED and we have this new layout computation algorithm which I will talk a little bit about. So as I said garage is meant to work on geo-distributed clusters so you have nodes which are in different geographical locations we call them zones in garage so here we have three different zones and the data is going to be replicated and each file has to be on different zones for optimal redundancy. So here is an illustration if we have five zones for example the blue file will be in Belgium France and Switzerland so in three different places and the red file will also be in three different places not necessarily the same here it's UK France and Germany. And the idea is that we do this using this kind of pre-computed layout which is a table which will say okay the cluster the data in the cluster is divided in 256 parts and each of these parts is assigned to a fixed set of three servers and for each part we have to decide so three servers which are in different places in the cluster and we have to also balance the quantity of data that is going to go on each server. So basically for 0.9 we added an algorithm which is able to do this in an optimal fashion so basically this table is computed once when you set up the cluster or when you add some new nodes and then it's propagated to everybody and everybody then knows this table and knows where to look for the data. We actually published a paper if you're interested in the details of the algorithms that we use. Okay so that was 0.9 and then we went on and worked on 0.10 and 0.10 is actually a beta version and I think we will not have a stable 0.10 because it's not worth it to like update to 0.10 and then update again to 1.0 when it's going to be out so I think we will just leave the 0.10 at beta and do the 0.1.0 in May but so I'll just talk a little bit about the 0.10 beta. It's mostly focused on fixing some consistency issues that would happen like when you were adding some servers in the system or removing some servers and so I will enter into a bit of distributed system theory to try to explain why exactly it's an issue and what is the solution that we made. So since I've said that garage is not based on consensus it means that we have to work with inconsistent primitives so this means we have to work with conflict-free, replicated data types, CRDTs and so these are not transactional, they are pretty much very very weakly consistent, very freeform to use and there's this last-writer wind register which is pretty much the fundamental building block of garage and so CRDTs alone are not enough to insert consistency so what we add is some read after write guarantee which is implemented using quorums and I will try to explain, I hope you will understand how it works, I think it's not so complicated but it's a bit theoretical so yeah, hold on. So read after write means if a client one is doing an operation right and the system returns to the client okay your write is saved in the system and then another client is sending a read for this data after the write is returned okay then the client two will read a value which is at least the value x that was written or a newer value this is what this means and so in practice it means that the system is basically evolving between these states so for instance we have the state here where the system is not storing anything and then we can store some value a or we can store some value b and if this is like a basic set if you have stored a on one node and b on another node then when the two nodes like merge together they will have stored a and b okay but let's do an example here for the writes so these are the three storage nodes and we're supposing that a node, a client is sending a write operation for value a so the value a is going to be sent to the network to these three nodes and at some point like maybe the purple node is going to receive the value a so it's going to move from not knowing anything to knowing the value a then the green node is also going to move from not knowing anything to knowing a when it receives the messages and so those two nodes are going to return to the client who did the operation okay I've stored the value a so at this point the client says so I've received two responses this is two over three so it's what we call a quorum and at that point the client says okay the data is stored in the system even if the third node has not received it yet and so this is the point where we can start a read request and so the read will basically is the client will ask all of the three nodes to return the value that they have stored and maybe the first node that will return its value is the red node and the red node has stored nothing so the read will first receive a value of nothing but then it will wait for another response and the other response will necessarily come from one of these two nodes and so it will necessarily read the value that was written and so it will just merge these two so this is why we use CRDTs to do this merge operation and consistency is guaranteed and maybe at some later point through some synchronization mechanism the red node will catch up and also receive the value so we have this in algorithmic form but okay and so the issue we have with this is that we're relying very strongly on these quorum properties so if we have three copies of data a quorum is at least two nodes of the three but what happens when you remove some nodes and add some other nodes in the intersystem so we will have some some data which was stored maybe on the nodes in red here and in the new system the data is being moved and it should be stored on the green nodes and so now if you do some quorum some right quorum on the red nodes and some some read quorum on the green nodes there is not necessarily an intersection of one node that has seen the read and the right and basically the consistency is broken so the question is how do we coordinate in this situation and how do we ensure that even when the cluster is rebalancing data we insert consistency and so the solution is a bit complex but basically we need to keep track of what data is being transferred between the nodes we use multiple right quorum so we're going to use quorums to write on the old set of nodes and the new set of nodes and switching reads to the new nodes only once the copy is finished so this is something we implemented for the in the context of the ngi grants we did some testing using a tool which is called jepsen which is very good for validating these kind of things and so as you can see in garage 0.9 we had consistency issues in most of our runs and in point 10 we have all runs are green except one which failed but at least there was no run where the data was plain wrong and it's actually this is very good result for us okay so this was point 10 now we're at fosdem and we're going looking forward to making a version one in april or may basically we're going to focus on security and stability there's a security audit that is going to be done by radically open security miscellaneous features should be improved this would be added and improvements may be in the user experience refactoring stuff and that's it for 1.0 hopefully we'll have that out in april this year and beyond so we have this survey which is going on in the community right now and so this is a list of the most requested features by the users of garage and actually there's a lot of work to do so the first thing is a web interface for cluster management so I guess for like visualizing the state of the cluster and setting up a new bucket as new access then it's s3 versioning which is so it's a feature of amazon s3 where you can have a you can save the historical data in the bucket and it's pretty good for like a backup system where you don't want to override data accidentally and this is a pretty crucial feature that we would need to have ACLs are here monitoring and various other things and so this is the point where I'm calling for help actually because there's a lot of work and I cannot do it myself so if anyone wants to step in and help us with this please do so we can probably find some some more funding actually we do have some funding in progress for someone who would like to do a phd on this system in in relationship with the garage so if anyone wants to do a phd in France working on some stuff come to us we have this application going on and we also can probably ask some money to nlnet which have funded us once and nji also once so we can probably get some more money if there's some specific task that that is planned and we have somebody who is willing to do it okay and so I will just spend the last few minutes of this talk to explain a little bit about how you can operate garage for people who have not run it or who are willing to scale their clusters to bigger systems so this is the basically what I would call the main screen of garage so when you interact with the cluster just start always by doing garage status and it will tell you if everything is fine so this is a five node cluster and everything seems to be fine but maybe you will have like failed nodes so this means that the connection could not be established and something is wrong and you should fix it garage is made like a some cake of different pieces like this on top we have the s3 api we also have some custom api which I'm not talking about in this talk and this is three api is actually implementing using some internal key value store for metadata and some block manager for the actual data of the big objects and then we have some systems here which maintain consistency in the system and so maybe to be a bit more specific about what's going on we have these three metadata data tables here so the first one is like the list of objects in the system the second is the list of versions of objects and so it's a bit different because an object can have a version which is currently in the cluster and a version which is currently being uploaded so for the same objects multiple versions can exist and then this version will also reference a bunch of data blocks so this is the table which has the reference to actual data blocks and so all of these tables are sharded across the nodes and in particular for the block reference table if a node has the has the shard for some references it means it's also responsible for storing the blocks associated with these references so basically from this metadata table we have a local counter for how many references for each block and then we have this rescind queue and scheduler which is responsible for ensuring that the locally stored data blocks are actually matching the number of blocks which have a reference in the in the store so yeah we have this block rescind for data blocks and this merkle merkle tree based system for the metadata and so if you do this garage stats command so there's not status it stats never command you will get some information about the internals of what's going on so these are the metadata tables and you can see here objects version and block reference so these are the number of items in the table and there are also the number of items in the merkle tree which is always a bit bigger and then you have here the number of rc entries for the block table so the number of blocks which actually have a reference in the system so here we have 42,000 data blocks but we have actually 334,000 block references so this means that blocks are almost referenced by 10 different objects each on average and then we have some information on the actual nodes so the partitions here means basically is how many of the lines in the tables are affected to each of these nodes so if you have more more partitions you're going to use more storage space basically on that node it's proportional and this is a metric which is given by the node actually it's it's measuring on disk how much space is available it's not the use space it's the available space for the data partition and the metadata which is not necessarily on the same drive and so from all this information garage is able to basically tell you how much data you can still store on the cluster so here for 600 gigabytes and if you go even further you can get this list of workers so workers are basically background tasks which are running in garage all the time and so you have these tasks which are block readings so these are copying data blocks between nodes when they're missing and these are synchronization tasks for each of the metadata tables and you can change a bit the parameters of these tasks for so for instance for the the block re-synchronization you have re-sync tranquility and re-sync worker count and tranquility is a metric which can be increased to make the system go slower and use less i o if if it's serring you're saturating your i o you can increase the tranquility and if you want it to go faster you can just put it to zero and then there's also the worker count so you can set it up to eight and then you have eight parallel threads which are sending and receiving data blocks in the network there are some potential limitations if you're running extremely extremely big clusters probably you cannot run with more than about 100 100 nodes i mean you can but then the the data will not be very well balanced between the nodes and this is because we're using only 256 partitions we could probably compile a bigger version in garage but it's currently not the case and on the metadata side if you have one big bucket which is containing all your objects well you will have a bottleneck also because the first table the object table is going to store the list of objects on only three of all of your cluster nodes so if you have lots of data split your data over different buckets and also on the side on the side of the data blocks so the data is split into so if you have a hundred megabytes file in your block size is one megabytes your your file is going to be split into a hundred different files so we will have a lot of small files on disk you can increase the block size to reduce the number of files and if you have more files the processing of the queue can also be kind of slow and this is of course also dependent on your networking conditions and so just some advice for actual deployments for the metadata if you're going to do a very large cluster we recommend doing some mirroring on two fast NVMe drives possibly ZFS is a good choice garage itself does not do check summing on the metadata so it's good to have a file system that does it for you lmdb is the recommended storage engine and for data block it's a bit different and we have other recommendations we recommend using an XFS file system because we actually do some check summing for each blocks because we always compute hashes of the blocks in garage so you do not need to have a file system which is doing this this check summing again it would be wasteful so just format your partitions as XFS which is one of the fastest file systems and store your data directly on this if you have a good network and some nodes with a lot of RAM you can increase the block size to 10 megabytes at minimum and you can tune these two parameters according to your needs and of course you can do some more like global tuning split your data over several buckets use less than a hundred nodes if possible or come to us and we can work out a solution and you can use also gateway nodes which are good way to like have have nodes which are so have the request go faster because if you if you have a local gateway on the same server as the client it can basically route the request directly to the data server and you can possibly avoid run for time we have not made any deployment bigger than 10 terabytes on the side of the floor but actually some people have as we learned from the survey and so if some people are in the room it would be great to share your experience and with this I think I've talked enough garage is available as a open source software on the website of the floor at switch and in Rust and we have a matrix channel and email you can contact us and I'm taking some questions um so the question was if you store websites on garage can you integrate with dns and basically we copied the semantics of amazon where you can have a bucket whose name is the the domain of a website and so garage will route requests to the data according to the host header of the htp request and basically you just have to to configure your dns server so this is something you have to do as of at sort of garage but you configure your dns server to write the request to your garage server and then garage will just select the good bucket with the good content based on the name of the bucket and you should add some reverse proxy probably in the middle if you want to tell us because garage does not do tls yeah it's because when one of those website servers goes down then you need to reroute to some yeah so at the floor we have a solution but it's external to garage so it's more tooling yeah so in all the examples you mentioned you have effectively one node for one zone what if is that by design or can you have multiple nodes per zone or how does that I think it's uh it's uh so the question was in the examples we have uh one node on each zone and can we have more than one node and so I think it's yeah it's just the examples were not very good but yeah of course we can have multiple nodes in a single zone I think maybe in this in this graph no this is not the good one but there is a there is an example where we have several nodes in the same zone it's not a problem yeah and if you have let's say everything else calls and you only have the one zone that's remaining will the node still try to balance the data between themselves or is that effectively a you're in trouble so the question is how is data get being balanced between the nodes if you have like one zone where it's have only one node and maybe the node is smaller and so garage is trying to preserve this property of having three copies in different places you can you can ask it to have only in two places but by default it's three places and this means that if you have only three zones and one is a smaller server then you have smaller capacity of the cluster yes yes so the question yeah so the question is why did we integrate multiple disk support instead of having multiple nodes in the same zone and I think one of the most important reasons is that this way you can reduce the total number of garage processes and entries in this in this table basically because this table has only so many rows and if you have start having many different nodes it's not going to be well balanced so reducing the number of nodes helps us be better balanced basically yes I saw many of your design matching the one of open stack swift and I was wondering if you investigated using it okay so the question is there's many design points which are matching open stack swift and have we investigated using it I personally have not used open stack swift and I have not looked so much into it yes so the question is despite putting this much effort in multi multi node deployments is it still worth running the system on a single node I think it's it's so many people are doing it and I think one of the reason people are doing it is because garage is pretty simple to set up and to use so I think it's definitely possible I think there are also other solutions which are good for single node setups so yeah try it out and figure what's works best for you and okay so I think we're done for this talk thank you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.24, "text": " You can't hear me at the back.", "tokens": [50364, 509, 393, 380, 1568, 385, 412, 264, 646, 13, 50676], "temperature": 0.0, "avg_logprob": -0.22916668536616305, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.07149765640497208}, {"id": 1, "seek": 0, "start": 6.24, "end": 10.08, "text": " Do not hesitate to ask for me to speak louder.", "tokens": [50676, 1144, 406, 20842, 281, 1029, 337, 385, 281, 1710, 22717, 13, 50868], "temperature": 0.0, "avg_logprob": -0.22916668536616305, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.07149765640497208}, {"id": 2, "seek": 0, "start": 10.08, "end": 17.240000000000002, "text": " So I'm Alex and I'm a co-founder of the DeFloor Association, which is a non-profit self-hosting", "tokens": [50868, 407, 286, 478, 5202, 293, 286, 478, 257, 598, 12, 33348, 295, 264, 1346, 37, 752, 284, 10734, 11, 597, 307, 257, 2107, 12, 14583, 2698, 12, 6037, 278, 51226], "temperature": 0.0, "avg_logprob": -0.22916668536616305, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.07149765640497208}, {"id": 3, "seek": 0, "start": 17.240000000000002, "end": 18.240000000000002, "text": " collective.", "tokens": [51226, 12590, 13, 51276], "temperature": 0.0, "avg_logprob": -0.22916668536616305, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.07149765640497208}, {"id": 4, "seek": 0, "start": 18.240000000000002, "end": 21.16, "text": " We're a member of the Chateau Network in France.", "tokens": [51276, 492, 434, 257, 4006, 295, 264, 761, 473, 1459, 12640, 294, 6190, 13, 51422], "temperature": 0.0, "avg_logprob": -0.22916668536616305, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.07149765640497208}, {"id": 5, "seek": 0, "start": 21.16, "end": 26.0, "text": " And so what that means is we're doing self-hosting and we're trying to promote self-hosting as", "tokens": [51422, 400, 370, 437, 300, 1355, 307, 321, 434, 884, 2698, 12, 6037, 278, 293, 321, 434, 1382, 281, 9773, 2698, 12, 6037, 278, 382, 51664], "temperature": 0.0, "avg_logprob": -0.22916668536616305, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.07149765640497208}, {"id": 6, "seek": 2600, "start": 26.0, "end": 34.96, "text": " an alternative to putting everything in large data centers and relying on cloud providers.", "tokens": [50364, 364, 8535, 281, 3372, 1203, 294, 2416, 1412, 10898, 293, 24140, 322, 4588, 11330, 13, 50812], "temperature": 0.0, "avg_logprob": -0.13345041178693676, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.03923771157860756}, {"id": 7, "seek": 2600, "start": 34.96, "end": 40.32, "text": " The thing is actually doing this is relatively hard and mostly it's hard because we want", "tokens": [50812, 440, 551, 307, 767, 884, 341, 307, 7226, 1152, 293, 5240, 309, 311, 1152, 570, 321, 528, 51080], "temperature": 0.0, "avg_logprob": -0.13345041178693676, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.03923771157860756}, {"id": 8, "seek": 2600, "start": 40.32, "end": 44.6, "text": " systems that are reliable, which are available most of the time.", "tokens": [51080, 3652, 300, 366, 12924, 11, 597, 366, 2435, 881, 295, 264, 565, 13, 51294], "temperature": 0.0, "avg_logprob": -0.13345041178693676, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.03923771157860756}, {"id": 9, "seek": 2600, "start": 44.6, "end": 48.72, "text": " And if you have computers at home, you have a lot of issues.", "tokens": [51294, 400, 498, 291, 362, 10807, 412, 1280, 11, 291, 362, 257, 688, 295, 2663, 13, 51500], "temperature": 0.0, "avg_logprob": -0.13345041178693676, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.03923771157860756}, {"id": 10, "seek": 2600, "start": 48.72, "end": 53.72, "text": " In particular, the computers we're using at DeFloor are these kind of computers, very", "tokens": [51500, 682, 1729, 11, 264, 10807, 321, 434, 1228, 412, 1346, 37, 752, 284, 366, 613, 733, 295, 10807, 11, 588, 51750], "temperature": 0.0, "avg_logprob": -0.13345041178693676, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.03923771157860756}, {"id": 11, "seek": 2600, "start": 53.72, "end": 55.72, "text": " cheap old desktop computers.", "tokens": [51750, 7084, 1331, 14502, 10807, 13, 51850], "temperature": 0.0, "avg_logprob": -0.13345041178693676, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.03923771157860756}, {"id": 12, "seek": 5572, "start": 55.72, "end": 60.879999999999995, "text": " They're not meant to be servers and we expect that they could crash at any time.", "tokens": [50364, 814, 434, 406, 4140, 281, 312, 15909, 293, 321, 2066, 300, 436, 727, 8252, 412, 604, 565, 13, 50622], "temperature": 0.0, "avg_logprob": -0.15735726789994672, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.0025872818659991026}, {"id": 13, "seek": 5572, "start": 60.879999999999995, "end": 64.2, "text": " These are some other examples that we had and those are still used actually.", "tokens": [50622, 1981, 366, 512, 661, 5110, 300, 321, 632, 293, 729, 366, 920, 1143, 767, 13, 50788], "temperature": 0.0, "avg_logprob": -0.15735726789994672, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.0025872818659991026}, {"id": 14, "seek": 5572, "start": 64.2, "end": 68.16, "text": " So these are also old desktop computers and we have some system which is based on only", "tokens": [50788, 407, 613, 366, 611, 1331, 14502, 10807, 293, 321, 362, 512, 1185, 597, 307, 2361, 322, 787, 50986], "temperature": 0.0, "avg_logprob": -0.15735726789994672, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.0025872818659991026}, {"id": 15, "seek": 5572, "start": 68.16, "end": 70.52, "text": " these kinds of machines.", "tokens": [50986, 613, 3685, 295, 8379, 13, 51104], "temperature": 0.0, "avg_logprob": -0.15735726789994672, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.0025872818659991026}, {"id": 16, "seek": 5572, "start": 70.52, "end": 72.4, "text": " So they can die.", "tokens": [51104, 407, 436, 393, 978, 13, 51198], "temperature": 0.0, "avg_logprob": -0.15735726789994672, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.0025872818659991026}, {"id": 17, "seek": 5572, "start": 72.4, "end": 77.03999999999999, "text": " We also have issues possibly with the internet where the electricity connection because we're", "tokens": [51198, 492, 611, 362, 2663, 6264, 365, 264, 4705, 689, 264, 10356, 4984, 570, 321, 434, 51430], "temperature": 0.0, "avg_logprob": -0.15735726789994672, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.0025872818659991026}, {"id": 18, "seek": 5572, "start": 77.03999999999999, "end": 79.52, "text": " at home so we don't have redundancy.", "tokens": [51430, 412, 1280, 370, 321, 500, 380, 362, 27830, 6717, 13, 51554], "temperature": 0.0, "avg_logprob": -0.15735726789994672, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.0025872818659991026}, {"id": 19, "seek": 5572, "start": 79.52, "end": 81.88, "text": " It can go at any time.", "tokens": [51554, 467, 393, 352, 412, 604, 565, 13, 51672], "temperature": 0.0, "avg_logprob": -0.15735726789994672, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.0025872818659991026}, {"id": 20, "seek": 8188, "start": 81.88, "end": 88.19999999999999, "text": " And to alleviate these issues, what we do is that we do distributed systems and we have", "tokens": [50364, 400, 281, 42701, 613, 2663, 11, 437, 321, 360, 307, 300, 321, 360, 12631, 3652, 293, 321, 362, 50680], "temperature": 0.0, "avg_logprob": -0.17333200999668666, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.23891031742095947}, {"id": 21, "seek": 8188, "start": 88.19999999999999, "end": 91.39999999999999, "text": " a multi-site geo-replicated cluster.", "tokens": [50680, 257, 4825, 12, 30417, 43198, 12, 265, 564, 3587, 13630, 13, 50840], "temperature": 0.0, "avg_logprob": -0.17333200999668666, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.23891031742095947}, {"id": 22, "seek": 8188, "start": 91.39999999999999, "end": 95.56, "text": " And so in our case, the DeFloor cluster is in three places.", "tokens": [50840, 400, 370, 294, 527, 1389, 11, 264, 1346, 37, 752, 284, 13630, 307, 294, 1045, 3190, 13, 51048], "temperature": 0.0, "avg_logprob": -0.17333200999668666, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.23891031742095947}, {"id": 23, "seek": 8188, "start": 95.56, "end": 99.75999999999999, "text": " There's some nodes in Brussels here, some nodes in Lille and some nodes in Paris.", "tokens": [51048, 821, 311, 512, 13891, 294, 38717, 510, 11, 512, 13891, 294, 441, 3409, 293, 512, 13891, 294, 8380, 13, 51258], "temperature": 0.0, "avg_logprob": -0.17333200999668666, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.23891031742095947}, {"id": 24, "seek": 8188, "start": 99.75999999999999, "end": 104.44, "text": " And basically the aim is to build a system that makes use of some cheap hardware which", "tokens": [51258, 400, 1936, 264, 5939, 307, 281, 1322, 257, 1185, 300, 1669, 764, 295, 512, 7084, 8837, 597, 51492], "temperature": 0.0, "avg_logprob": -0.17333200999668666, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.23891031742095947}, {"id": 25, "seek": 8188, "start": 104.44, "end": 109.44, "text": " is disseminated in all of these locations and they can basically relay one another when", "tokens": [51492, 307, 34585, 770, 294, 439, 295, 613, 9253, 293, 436, 393, 1936, 24214, 472, 1071, 562, 51742], "temperature": 0.0, "avg_logprob": -0.17333200999668666, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.23891031742095947}, {"id": 26, "seek": 10944, "start": 109.44, "end": 113.72, "text": " there's an issue somewhere and the whole thing stays up even if there are issues in", "tokens": [50364, 456, 311, 364, 2734, 4079, 293, 264, 1379, 551, 10834, 493, 754, 498, 456, 366, 2663, 294, 50578], "temperature": 0.0, "avg_logprob": -0.12720105112815389, "compression_ratio": 1.664, "no_speech_prob": 0.003122254041954875}, {"id": 27, "seek": 10944, "start": 113.72, "end": 115.92, "text": " individual locations.", "tokens": [50578, 2609, 9253, 13, 50688], "temperature": 0.0, "avg_logprob": -0.12720105112815389, "compression_ratio": 1.664, "no_speech_prob": 0.003122254041954875}, {"id": 28, "seek": 10944, "start": 115.92, "end": 119.84, "text": " And so this is one of the reasons why I call this a low-tech platform because we're using", "tokens": [50688, 400, 370, 341, 307, 472, 295, 264, 4112, 983, 286, 818, 341, 257, 2295, 12, 25970, 3663, 570, 321, 434, 1228, 50884], "temperature": 0.0, "avg_logprob": -0.12720105112815389, "compression_ratio": 1.664, "no_speech_prob": 0.003122254041954875}, {"id": 29, "seek": 10944, "start": 119.84, "end": 126.88, "text": " what we have at hand, cheap machines and regular internet connections.", "tokens": [50884, 437, 321, 362, 412, 1011, 11, 7084, 8379, 293, 3890, 4705, 9271, 13, 51236], "temperature": 0.0, "avg_logprob": -0.12720105112815389, "compression_ratio": 1.664, "no_speech_prob": 0.003122254041954875}, {"id": 30, "seek": 10944, "start": 126.88, "end": 131.68, "text": " One of the main components in this platform is object storage.", "tokens": [51236, 1485, 295, 264, 2135, 6677, 294, 341, 3663, 307, 2657, 6725, 13, 51476], "temperature": 0.0, "avg_logprob": -0.12720105112815389, "compression_ratio": 1.664, "no_speech_prob": 0.003122254041954875}, {"id": 31, "seek": 10944, "start": 131.68, "end": 136.84, "text": " And so I will not enter too much into why object storage except that it's very adapted", "tokens": [51476, 400, 370, 286, 486, 406, 3242, 886, 709, 666, 983, 2657, 6725, 3993, 300, 309, 311, 588, 20871, 51734], "temperature": 0.0, "avg_logprob": -0.12720105112815389, "compression_ratio": 1.664, "no_speech_prob": 0.003122254041954875}, {"id": 32, "seek": 13684, "start": 136.84, "end": 142.52, "text": " to flexible deployments which are kind of inspired by what is done in the cloud.", "tokens": [50364, 281, 11358, 7274, 1117, 597, 366, 733, 295, 7547, 538, 437, 307, 1096, 294, 264, 4588, 13, 50648], "temperature": 0.0, "avg_logprob": -0.13554678513453558, "compression_ratio": 1.6654411764705883, "no_speech_prob": 0.011152205988764763}, {"id": 33, "seek": 13684, "start": 142.52, "end": 149.8, "text": " And indeed, Amazon S3 was created as a cloud product and in 2006 was introduced.", "tokens": [50648, 400, 6451, 11, 6795, 318, 18, 390, 2942, 382, 257, 4588, 1674, 293, 294, 14062, 390, 7268, 13, 51012], "temperature": 0.0, "avg_logprob": -0.13554678513453558, "compression_ratio": 1.6654411764705883, "no_speech_prob": 0.011152205988764763}, {"id": 34, "seek": 13684, "start": 149.8, "end": 154.32, "text": " And it became since then a de facto standard and many applications are compatible with", "tokens": [51012, 400, 309, 3062, 1670, 550, 257, 368, 42225, 3832, 293, 867, 5821, 366, 18218, 365, 51238], "temperature": 0.0, "avg_logprob": -0.13554678513453558, "compression_ratio": 1.6654411764705883, "no_speech_prob": 0.011152205988764763}, {"id": 35, "seek": 13684, "start": 154.32, "end": 155.84, "text": " this object storage.", "tokens": [51238, 341, 2657, 6725, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13554678513453558, "compression_ratio": 1.6654411764705883, "no_speech_prob": 0.011152205988764763}, {"id": 36, "seek": 13684, "start": 155.84, "end": 159.96, "text": " And so it makes sense to base our infrastructure on this kind of software because we can just", "tokens": [51314, 400, 370, 309, 1669, 2020, 281, 3096, 527, 6896, 322, 341, 733, 295, 4722, 570, 321, 393, 445, 51520], "temperature": 0.0, "avg_logprob": -0.13554678513453558, "compression_ratio": 1.6654411764705883, "no_speech_prob": 0.011152205988764763}, {"id": 37, "seek": 13684, "start": 159.96, "end": 164.44, "text": " like plug and play all kinds of various things which are already able to use this kind of", "tokens": [51520, 411, 5452, 293, 862, 439, 3685, 295, 3683, 721, 597, 366, 1217, 1075, 281, 764, 341, 733, 295, 51744], "temperature": 0.0, "avg_logprob": -0.13554678513453558, "compression_ratio": 1.6654411764705883, "no_speech_prob": 0.011152205988764763}, {"id": 38, "seek": 16444, "start": 164.44, "end": 168.88, "text": " storage layer as a backend.", "tokens": [50364, 6725, 4583, 382, 257, 38087, 13, 50586], "temperature": 0.0, "avg_logprob": -0.20891613207365337, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.00172607961576432}, {"id": 39, "seek": 16444, "start": 168.88, "end": 172.6, "text": " There were many actually alternative implementations of S3.", "tokens": [50586, 821, 645, 867, 767, 8535, 4445, 763, 295, 318, 18, 13, 50772], "temperature": 0.0, "avg_logprob": -0.20891613207365337, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.00172607961576432}, {"id": 40, "seek": 16444, "start": 172.6, "end": 173.96, "text": " MENU is one of the most common ones.", "tokens": [50772, 376, 2195, 52, 307, 472, 295, 264, 881, 2689, 2306, 13, 50840], "temperature": 0.0, "avg_logprob": -0.20891613207365337, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.00172607961576432}, {"id": 41, "seek": 16444, "start": 173.96, "end": 177.68, "text": " I think CEPH is also an implementation.", "tokens": [50840, 286, 519, 383, 8929, 39, 307, 611, 364, 11420, 13, 51026], "temperature": 0.0, "avg_logprob": -0.20891613207365337, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.00172607961576432}, {"id": 42, "seek": 16444, "start": 177.68, "end": 182.8, "text": " What we discovered is actually that these implementations are not very well suited to", "tokens": [51026, 708, 321, 6941, 307, 767, 300, 613, 4445, 763, 366, 406, 588, 731, 24736, 281, 51282], "temperature": 0.0, "avg_logprob": -0.20891613207365337, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.00172607961576432}, {"id": 43, "seek": 16444, "start": 182.8, "end": 184.07999999999998, "text": " geo-distributed deployments.", "tokens": [51282, 43198, 12, 42649, 2024, 4866, 7274, 1117, 13, 51346], "temperature": 0.0, "avg_logprob": -0.20891613207365337, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.00172607961576432}, {"id": 44, "seek": 16444, "start": 184.07999999999998, "end": 190.24, "text": " So deployments where nodes are in remote locations because in such case you will have higher latency", "tokens": [51346, 407, 7274, 1117, 689, 13891, 366, 294, 8607, 9253, 570, 294, 1270, 1389, 291, 486, 362, 2946, 27043, 51654], "temperature": 0.0, "avg_logprob": -0.20891613207365337, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.00172607961576432}, {"id": 45, "seek": 19024, "start": 190.24, "end": 196.72, "text": " between the nodes and it can cause issues and the system is basically a bit slower.", "tokens": [50364, 1296, 264, 13891, 293, 309, 393, 3082, 2663, 293, 264, 1185, 307, 1936, 257, 857, 14009, 13, 50688], "temperature": 0.0, "avg_logprob": -0.1809004545211792, "compression_ratio": 1.525, "no_speech_prob": 0.0023593557998538017}, {"id": 46, "seek": 19024, "start": 196.72, "end": 199.48000000000002, "text": " And sometimes it's even really unusable.", "tokens": [50688, 400, 2171, 309, 311, 754, 534, 10054, 712, 13, 50826], "temperature": 0.0, "avg_logprob": -0.1809004545211792, "compression_ratio": 1.525, "no_speech_prob": 0.0023593557998538017}, {"id": 47, "seek": 19024, "start": 199.48000000000002, "end": 203.52, "text": " So Garage was made specifically for this use case.", "tokens": [50826, 407, 47918, 390, 1027, 4682, 337, 341, 764, 1389, 13, 51028], "temperature": 0.0, "avg_logprob": -0.1809004545211792, "compression_ratio": 1.525, "no_speech_prob": 0.0023593557998538017}, {"id": 48, "seek": 19024, "start": 203.52, "end": 208.52, "text": " We make use of distributed systems theory, CRDT in particular, which I will talk about", "tokens": [51028, 492, 652, 764, 295, 12631, 3652, 5261, 11, 14123, 35, 51, 294, 1729, 11, 597, 286, 486, 751, 466, 51278], "temperature": 0.0, "avg_logprob": -0.1809004545211792, "compression_ratio": 1.525, "no_speech_prob": 0.0023593557998538017}, {"id": 49, "seek": 19024, "start": 208.52, "end": 209.60000000000002, "text": " later.", "tokens": [51278, 1780, 13, 51332], "temperature": 0.0, "avg_logprob": -0.1809004545211792, "compression_ratio": 1.525, "no_speech_prob": 0.0023593557998538017}, {"id": 50, "seek": 19024, "start": 209.60000000000002, "end": 215.4, "text": " And this is basically the aim is to provide a drop-in replacement for Amazon S3 or S3-compatible", "tokens": [51332, 400, 341, 307, 1936, 264, 5939, 307, 281, 2893, 257, 3270, 12, 259, 14419, 337, 6795, 318, 18, 420, 318, 18, 12, 1112, 11584, 964, 51622], "temperature": 0.0, "avg_logprob": -0.1809004545211792, "compression_ratio": 1.525, "no_speech_prob": 0.0023593557998538017}, {"id": 51, "seek": 21540, "start": 215.4, "end": 221.56, "text": " storage systems which is available, possible to run directly on this kind of geo-distributed", "tokens": [50364, 6725, 3652, 597, 307, 2435, 11, 1944, 281, 1190, 3838, 322, 341, 733, 295, 43198, 12, 42649, 2024, 4866, 50672], "temperature": 0.0, "avg_logprob": -0.1779408662215523, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.003764652879908681}, {"id": 52, "seek": 21540, "start": 221.56, "end": 226.96, "text": " cluster and the data will be replicated at several locations and it's kind of transparent", "tokens": [50672, 13630, 293, 264, 1412, 486, 312, 46365, 412, 2940, 9253, 293, 309, 311, 733, 295, 12737, 50942], "temperature": 0.0, "avg_logprob": -0.1779408662215523, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.003764652879908681}, {"id": 53, "seek": 21540, "start": 226.96, "end": 231.68, "text": " and it's supposed to be reasonably fast, not completely slow down all the replications", "tokens": [50942, 293, 309, 311, 3442, 281, 312, 23551, 2370, 11, 406, 2584, 2964, 760, 439, 264, 3248, 24847, 51178], "temperature": 0.0, "avg_logprob": -0.1779408662215523, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.003764652879908681}, {"id": 54, "seek": 21540, "start": 231.68, "end": 235.20000000000002, "text": " which are running on it.", "tokens": [51178, 597, 366, 2614, 322, 309, 13, 51354], "temperature": 0.0, "avg_logprob": -0.1779408662215523, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.003764652879908681}, {"id": 55, "seek": 21540, "start": 235.20000000000002, "end": 241.16, "text": " One of the main ways we were able to achieve this outcome was to use CRDT and weak consistency.", "tokens": [51354, 1485, 295, 264, 2135, 2098, 321, 645, 1075, 281, 4584, 341, 9700, 390, 281, 764, 14123, 35, 51, 293, 5336, 14416, 13, 51652], "temperature": 0.0, "avg_logprob": -0.1779408662215523, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.003764652879908681}, {"id": 56, "seek": 24116, "start": 241.16, "end": 247.07999999999998, "text": " So this is a bit theoretical explanation of what is going on in Garage and I will have", "tokens": [50364, 407, 341, 307, 257, 857, 20864, 10835, 295, 437, 307, 516, 322, 294, 47918, 293, 286, 486, 362, 50660], "temperature": 0.0, "avg_logprob": -0.17248953713311088, "compression_ratio": 1.646643109540636, "no_speech_prob": 0.13103726506233215}, {"id": 57, "seek": 24116, "start": 247.07999999999998, "end": 249.07999999999998, "text": " another slide talking about this later.", "tokens": [50660, 1071, 4137, 1417, 466, 341, 1780, 13, 50760], "temperature": 0.0, "avg_logprob": -0.17248953713311088, "compression_ratio": 1.646643109540636, "no_speech_prob": 0.13103726506233215}, {"id": 58, "seek": 24116, "start": 249.07999999999998, "end": 254.64, "text": " But basically we're trying to avoid so-called consensus algorithms like RAF or PAXOS because", "tokens": [50760, 583, 1936, 321, 434, 1382, 281, 5042, 370, 12, 11880, 19115, 14642, 411, 14626, 37, 420, 17718, 55, 4367, 570, 51038], "temperature": 0.0, "avg_logprob": -0.17248953713311088, "compression_ratio": 1.646643109540636, "no_speech_prob": 0.13103726506233215}, {"id": 59, "seek": 24116, "start": 254.64, "end": 259.0, "text": " these algorithms have issues and are actually very sensitive to latency.", "tokens": [51038, 613, 14642, 362, 2663, 293, 366, 767, 588, 9477, 281, 27043, 13, 51256], "temperature": 0.0, "avg_logprob": -0.17248953713311088, "compression_ratio": 1.646643109540636, "no_speech_prob": 0.13103726506233215}, {"id": 60, "seek": 24116, "start": 259.0, "end": 264.36, "text": " But just to list the issues in a clear way, the first of them is software complexity.", "tokens": [51256, 583, 445, 281, 1329, 264, 2663, 294, 257, 1850, 636, 11, 264, 700, 295, 552, 307, 4722, 14024, 13, 51524], "temperature": 0.0, "avg_logprob": -0.17248953713311088, "compression_ratio": 1.646643109540636, "no_speech_prob": 0.13103726506233215}, {"id": 61, "seek": 24116, "start": 264.36, "end": 269.28, "text": " I think RAF is actually a complex piece of software and it can be implemented badly and", "tokens": [51524, 286, 519, 14626, 37, 307, 767, 257, 3997, 2522, 295, 4722, 293, 309, 393, 312, 12270, 13425, 293, 51770], "temperature": 0.0, "avg_logprob": -0.17248953713311088, "compression_ratio": 1.646643109540636, "no_speech_prob": 0.13103726506233215}, {"id": 62, "seek": 26928, "start": 269.32, "end": 274.35999999999996, "text": " if you do it wrong it can lead to various unacceptable outcomes.", "tokens": [50366, 498, 291, 360, 309, 2085, 309, 393, 1477, 281, 3683, 31812, 10070, 13, 50618], "temperature": 0.0, "avg_logprob": -0.16791461944580077, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.001867101644165814}, {"id": 63, "seek": 26928, "start": 274.35999999999996, "end": 280.23999999999995, "text": " And of course the issue of performance which I've talked about already.", "tokens": [50618, 400, 295, 1164, 264, 2734, 295, 3389, 597, 286, 600, 2825, 466, 1217, 13, 50912], "temperature": 0.0, "avg_logprob": -0.16791461944580077, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.001867101644165814}, {"id": 64, "seek": 26928, "start": 280.23999999999995, "end": 284.15999999999997, "text": " Those algorithms like RAF are using a leader so the leader is becoming a bottleneck for", "tokens": [50912, 3950, 14642, 411, 14626, 37, 366, 1228, 257, 5263, 370, 264, 5263, 307, 5617, 257, 44641, 547, 337, 51108], "temperature": 0.0, "avg_logprob": -0.16791461944580077, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.001867101644165814}, {"id": 65, "seek": 26928, "start": 284.15999999999997, "end": 287.08, "text": " most requests in the system.", "tokens": [51108, 881, 12475, 294, 264, 1185, 13, 51254], "temperature": 0.0, "avg_logprob": -0.16791461944580077, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.001867101644165814}, {"id": 66, "seek": 26928, "start": 287.08, "end": 291.32, "text": " So if you cannot really scale if you have a naive strategy with just one leader in the", "tokens": [51254, 407, 498, 291, 2644, 534, 4373, 498, 291, 362, 257, 29052, 5206, 365, 445, 472, 5263, 294, 264, 51466], "temperature": 0.0, "avg_logprob": -0.16791461944580077, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.001867101644165814}, {"id": 67, "seek": 26928, "start": 291.32, "end": 297.91999999999996, "text": " system it's also sensitive to higher latency because if the leader happens to be a node", "tokens": [51466, 1185, 309, 311, 611, 9477, 281, 2946, 27043, 570, 498, 264, 5263, 2314, 281, 312, 257, 9984, 51796], "temperature": 0.0, "avg_logprob": -0.16791461944580077, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.001867101644165814}, {"id": 68, "seek": 29792, "start": 297.92, "end": 303.36, "text": " in a very far away location, well everything has to transit from there and then come back.", "tokens": [50364, 294, 257, 588, 1400, 1314, 4914, 11, 731, 1203, 575, 281, 17976, 490, 456, 293, 550, 808, 646, 13, 50636], "temperature": 0.0, "avg_logprob": -0.1344225425419845, "compression_ratio": 1.8300653594771241, "no_speech_prob": 0.0029795796144753695}, {"id": 69, "seek": 29792, "start": 303.36, "end": 307.04, "text": " And so if the leader happens to be the wrong node everything is going to be much slower", "tokens": [50636, 400, 370, 498, 264, 5263, 2314, 281, 312, 264, 2085, 9984, 1203, 307, 516, 281, 312, 709, 14009, 50820], "temperature": 0.0, "avg_logprob": -0.1344225425419845, "compression_ratio": 1.8300653594771241, "no_speech_prob": 0.0029795796144753695}, {"id": 70, "seek": 29792, "start": 307.04, "end": 308.76, "text": " in the system.", "tokens": [50820, 294, 264, 1185, 13, 50906], "temperature": 0.0, "avg_logprob": -0.1344225425419845, "compression_ratio": 1.8300653594771241, "no_speech_prob": 0.0029795796144753695}, {"id": 71, "seek": 29792, "start": 308.76, "end": 312.76, "text": " And also if the system is disrupted and the leader goes down the system will have to take", "tokens": [50906, 400, 611, 498, 264, 1185, 307, 42271, 293, 264, 5263, 1709, 760, 264, 1185, 486, 362, 281, 747, 51106], "temperature": 0.0, "avg_logprob": -0.1344225425419845, "compression_ratio": 1.8300653594771241, "no_speech_prob": 0.0029795796144753695}, {"id": 72, "seek": 29792, "start": 312.76, "end": 317.24, "text": " some time to reconverge and it's actually something that can take a long time especially", "tokens": [51106, 512, 565, 281, 9993, 331, 432, 293, 309, 311, 767, 746, 300, 393, 747, 257, 938, 565, 2318, 51330], "temperature": 0.0, "avg_logprob": -0.1344225425419845, "compression_ratio": 1.8300653594771241, "no_speech_prob": 0.0029795796144753695}, {"id": 73, "seek": 29792, "start": 317.24, "end": 322.96000000000004, "text": " if the latency between nodes is high and those are not able to communicate very efficiently.", "tokens": [51330, 498, 264, 27043, 1296, 13891, 307, 1090, 293, 729, 366, 406, 1075, 281, 7890, 588, 19621, 13, 51616], "temperature": 0.0, "avg_logprob": -0.1344225425419845, "compression_ratio": 1.8300653594771241, "no_speech_prob": 0.0029795796144753695}, {"id": 74, "seek": 29792, "start": 322.96000000000004, "end": 327.40000000000003, "text": " And so for this reason we made Garage a completely different design which is based entirely on", "tokens": [51616, 400, 370, 337, 341, 1778, 321, 1027, 47918, 257, 2584, 819, 1715, 597, 307, 2361, 7696, 322, 51838], "temperature": 0.0, "avg_logprob": -0.1344225425419845, "compression_ratio": 1.8300653594771241, "no_speech_prob": 0.0029795796144753695}, {"id": 75, "seek": 32740, "start": 327.4, "end": 334.52, "text": " CRDT internally which kind of solves most of these issues.", "tokens": [50364, 14123, 35, 51, 19501, 597, 733, 295, 39890, 881, 295, 613, 2663, 13, 50720], "temperature": 0.0, "avg_logprob": -0.13405561447143555, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0014100698754191399}, {"id": 76, "seek": 32740, "start": 334.52, "end": 340.03999999999996, "text": " Object storage is very likely very similar to basic key value store except that the values", "tokens": [50720, 24753, 6725, 307, 588, 3700, 588, 2531, 281, 3875, 2141, 2158, 3531, 3993, 300, 264, 4190, 50996], "temperature": 0.0, "avg_logprob": -0.13405561447143555, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0014100698754191399}, {"id": 77, "seek": 32740, "start": 340.03999999999996, "end": 344.47999999999996, "text": " are objects like big blobs of data.", "tokens": [50996, 366, 6565, 411, 955, 1749, 929, 295, 1412, 13, 51218], "temperature": 0.0, "avg_logprob": -0.13405561447143555, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0014100698754191399}, {"id": 78, "seek": 32740, "start": 344.47999999999996, "end": 349.08, "text": " And so here we have an example where we have the key which is there's no notion of a file", "tokens": [51218, 400, 370, 510, 321, 362, 364, 1365, 689, 321, 362, 264, 2141, 597, 307, 456, 311, 572, 10710, 295, 257, 3991, 51448], "temperature": 0.0, "avg_logprob": -0.13405561447143555, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0014100698754191399}, {"id": 79, "seek": 32740, "start": 349.08, "end": 354.2, "text": " system hierarchy so we will just have the entire path in the key with the slash it doesn't", "tokens": [51448, 1185, 22333, 370, 321, 486, 445, 362, 264, 2302, 3100, 294, 264, 2141, 365, 264, 17330, 309, 1177, 380, 51704], "temperature": 0.0, "avg_logprob": -0.13405561447143555, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0014100698754191399}, {"id": 80, "seek": 35420, "start": 354.2, "end": 356.2, "text": " have any specific meaning.", "tokens": [50364, 362, 604, 2685, 3620, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1411142803373791, "compression_ratio": 1.6627450980392158, "no_speech_prob": 0.007114506792277098}, {"id": 81, "seek": 35420, "start": 356.2, "end": 360.68, "text": " And the value is like some metadata here it's inspired from the HTTP headers because it's", "tokens": [50464, 400, 264, 2158, 307, 411, 512, 26603, 510, 309, 311, 7547, 490, 264, 33283, 45101, 570, 309, 311, 50688], "temperature": 0.0, "avg_logprob": -0.1411142803373791, "compression_ratio": 1.6627450980392158, "no_speech_prob": 0.007114506792277098}, {"id": 82, "seek": 35420, "start": 360.68, "end": 365.88, "text": " very strongly based on the HTTP semantics and then you have the binary data for each", "tokens": [50688, 588, 10613, 2361, 322, 264, 33283, 4361, 45298, 293, 550, 291, 362, 264, 17434, 1412, 337, 1184, 50948], "temperature": 0.0, "avg_logprob": -0.1411142803373791, "compression_ratio": 1.6627450980392158, "no_speech_prob": 0.007114506792277098}, {"id": 83, "seek": 35420, "start": 365.88, "end": 367.96, "text": " of your files.", "tokens": [50948, 295, 428, 7098, 13, 51052], "temperature": 0.0, "avg_logprob": -0.1411142803373791, "compression_ratio": 1.6627450980392158, "no_speech_prob": 0.007114506792277098}, {"id": 84, "seek": 35420, "start": 367.96, "end": 372.48, "text": " It happens that this semantics key value actually maps very well to CRDT and this is why we", "tokens": [51052, 467, 2314, 300, 341, 4361, 45298, 2141, 2158, 767, 11317, 588, 731, 281, 14123, 35, 51, 293, 341, 307, 983, 321, 51278], "temperature": 0.0, "avg_logprob": -0.1411142803373791, "compression_ratio": 1.6627450980392158, "no_speech_prob": 0.007114506792277098}, {"id": 85, "seek": 35420, "start": 372.48, "end": 375.91999999999996, "text": " were able to make this work.", "tokens": [51278, 645, 1075, 281, 652, 341, 589, 13, 51450], "temperature": 0.0, "avg_logprob": -0.1411142803373791, "compression_ratio": 1.6627450980392158, "no_speech_prob": 0.007114506792277098}, {"id": 86, "seek": 35420, "start": 375.91999999999996, "end": 379.76, "text": " So just to convince you in one slide that this is actually a worthwhile trade off this", "tokens": [51450, 407, 445, 281, 13447, 291, 294, 472, 4137, 300, 341, 307, 767, 257, 28159, 4923, 766, 341, 51642], "temperature": 0.0, "avg_logprob": -0.1411142803373791, "compression_ratio": 1.6627450980392158, "no_speech_prob": 0.007114506792277098}, {"id": 87, "seek": 37976, "start": 379.76, "end": 385.15999999999997, "text": " is one of the best results we have for Garage and it's a performance comparison for Garage", "tokens": [50364, 307, 472, 295, 264, 1151, 3542, 321, 362, 337, 47918, 293, 309, 311, 257, 3389, 9660, 337, 47918, 50634], "temperature": 0.0, "avg_logprob": -0.15908477783203126, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.06270331144332886}, {"id": 88, "seek": 37976, "start": 385.15999999999997, "end": 387.03999999999996, "text": " versus Migno.", "tokens": [50634, 5717, 376, 788, 78, 13, 50728], "temperature": 0.0, "avg_logprob": -0.15908477783203126, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.06270331144332886}, {"id": 89, "seek": 37976, "start": 387.03999999999996, "end": 391.2, "text": " So it's a simulated deployment where we have nodes which are simulated on a single machine", "tokens": [50728, 407, 309, 311, 257, 41713, 19317, 689, 321, 362, 13891, 597, 366, 41713, 322, 257, 2167, 3479, 50936], "temperature": 0.0, "avg_logprob": -0.15908477783203126, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.06270331144332886}, {"id": 90, "seek": 37976, "start": 391.2, "end": 394.71999999999997, "text": " and we add some artificial latency between the nodes.", "tokens": [50936, 293, 321, 909, 512, 11677, 27043, 1296, 264, 13891, 13, 51112], "temperature": 0.0, "avg_logprob": -0.15908477783203126, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.06270331144332886}, {"id": 91, "seek": 37976, "start": 394.71999999999997, "end": 401.76, "text": " And so here we have nodes with 50 milliseconds so pretty long delay between them and so basically", "tokens": [51112, 400, 370, 510, 321, 362, 13891, 365, 2625, 34184, 370, 1238, 938, 8577, 1296, 552, 293, 370, 1936, 51464], "temperature": 0.0, "avg_logprob": -0.15908477783203126, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.06270331144332886}, {"id": 92, "seek": 37976, "start": 401.76, "end": 407.64, "text": " we can see that they take some duration which is a multiple of the round trip time latency", "tokens": [51464, 321, 393, 536, 300, 436, 747, 512, 16365, 597, 307, 257, 3866, 295, 264, 3098, 4931, 565, 27043, 51758], "temperature": 0.0, "avg_logprob": -0.15908477783203126, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.06270331144332886}, {"id": 93, "seek": 40764, "start": 407.84, "end": 412.03999999999996, "text": " but for Migno it's a very high multiple so some very common requests like remove object", "tokens": [50374, 457, 337, 376, 788, 78, 309, 311, 257, 588, 1090, 3866, 370, 512, 588, 2689, 12475, 411, 4159, 2657, 50584], "temperature": 0.0, "avg_logprob": -0.17615281618558443, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.0035914035979658365}, {"id": 94, "seek": 40764, "start": 412.03999999999996, "end": 416.64, "text": " or put object will take like more than one second and for Garage we were able to bring", "tokens": [50584, 420, 829, 2657, 486, 747, 411, 544, 813, 472, 1150, 293, 337, 47918, 321, 645, 1075, 281, 1565, 50814], "temperature": 0.0, "avg_logprob": -0.17615281618558443, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.0035914035979658365}, {"id": 95, "seek": 40764, "start": 416.64, "end": 421.4, "text": " this down to somewhere between 300, 500 milliseconds.", "tokens": [50814, 341, 760, 281, 4079, 1296, 6641, 11, 5923, 34184, 13, 51052], "temperature": 0.0, "avg_logprob": -0.17615281618558443, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.0035914035979658365}, {"id": 96, "seek": 40764, "start": 421.4, "end": 425.47999999999996, "text": " So quite an improvement.", "tokens": [51052, 407, 1596, 364, 10444, 13, 51256], "temperature": 0.0, "avg_logprob": -0.17615281618558443, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.0035914035979658365}, {"id": 97, "seek": 40764, "start": 425.47999999999996, "end": 432.71999999999997, "text": " So the main focus of this talk is to basically discuss recent developments in Garage because", "tokens": [51256, 407, 264, 2135, 1879, 295, 341, 751, 307, 281, 1936, 2248, 5162, 20862, 294, 47918, 570, 51618], "temperature": 0.0, "avg_logprob": -0.17615281618558443, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.0035914035979658365}, {"id": 98, "seek": 40764, "start": 432.71999999999997, "end": 436.91999999999996, "text": " so we were here at Fosdame two years ago and I think maybe lots of people in the room", "tokens": [51618, 370, 321, 645, 510, 412, 479, 329, 67, 529, 732, 924, 2057, 293, 286, 519, 1310, 3195, 295, 561, 294, 264, 1808, 51828], "temperature": 0.0, "avg_logprob": -0.17615281618558443, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.0035914035979658365}, {"id": 99, "seek": 43692, "start": 436.92, "end": 439.76, "text": " are already aware of Garage.", "tokens": [50364, 366, 1217, 3650, 295, 47918, 13, 50506], "temperature": 0.0, "avg_logprob": -0.15078127617929496, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0019872356206178665}, {"id": 100, "seek": 43692, "start": 439.76, "end": 446.64000000000004, "text": " So yeah two years ago we were at the beginning of a grant by NGI Pointer and which was the", "tokens": [50506, 407, 1338, 732, 924, 2057, 321, 645, 412, 264, 2863, 295, 257, 6386, 538, 426, 26252, 6165, 5106, 293, 597, 390, 264, 50850], "temperature": 0.0, "avg_logprob": -0.15078127617929496, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0019872356206178665}, {"id": 101, "seek": 43692, "start": 446.64000000000004, "end": 453.36, "text": " first grant and it allowed us to bring this version 0.6.0 which was the first like public", "tokens": [50850, 700, 6386, 293, 309, 4350, 505, 281, 1565, 341, 3037, 1958, 13, 21, 13, 15, 597, 390, 264, 700, 411, 1908, 51186], "temperature": 0.0, "avg_logprob": -0.15078127617929496, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0019872356206178665}, {"id": 102, "seek": 43692, "start": 453.36, "end": 456.64, "text": " beta version that we launched.", "tokens": [51186, 9861, 3037, 300, 321, 8730, 13, 51350], "temperature": 0.0, "avg_logprob": -0.15078127617929496, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0019872356206178665}, {"id": 103, "seek": 43692, "start": 456.64, "end": 461.04, "text": " So it was like a point where we considered that we had some basic feature which was pretty", "tokens": [51350, 407, 309, 390, 411, 257, 935, 689, 321, 4888, 300, 321, 632, 512, 3875, 4111, 597, 390, 1238, 51570], "temperature": 0.0, "avg_logprob": -0.15078127617929496, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0019872356206178665}, {"id": 104, "seek": 43692, "start": 461.04, "end": 465.32, "text": " good actually and we could ask people to come and actually many people were interested", "tokens": [51570, 665, 767, 293, 321, 727, 1029, 561, 281, 808, 293, 767, 867, 561, 645, 3102, 51784], "temperature": 0.0, "avg_logprob": -0.15078127617929496, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0019872356206178665}, {"id": 105, "seek": 46532, "start": 465.56, "end": 469.0, "text": " and this is the point where we started also to have some external contributions to the", "tokens": [50376, 293, 341, 307, 264, 935, 689, 321, 1409, 611, 281, 362, 512, 8320, 15725, 281, 264, 50548], "temperature": 0.0, "avg_logprob": -0.1650236129760742, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0016224735882133245}, {"id": 106, "seek": 46532, "start": 469.0, "end": 471.44, "text": " project.", "tokens": [50548, 1716, 13, 50670], "temperature": 0.0, "avg_logprob": -0.1650236129760742, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0016224735882133245}, {"id": 107, "seek": 46532, "start": 471.44, "end": 474.71999999999997, "text": " So we did Fosdame about at the time.", "tokens": [50670, 407, 321, 630, 479, 329, 67, 529, 466, 412, 264, 565, 13, 50834], "temperature": 0.0, "avg_logprob": -0.1650236129760742, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0016224735882133245}, {"id": 108, "seek": 46532, "start": 474.71999999999997, "end": 485.08, "text": " In April we did version 0.7 and so version 0.7 was so focused mostly on observability", "tokens": [50834, 682, 6929, 321, 630, 3037, 1958, 13, 22, 293, 370, 3037, 1958, 13, 22, 390, 370, 5178, 5240, 322, 9951, 2310, 51352], "temperature": 0.0, "avg_logprob": -0.1650236129760742, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0016224735882133245}, {"id": 109, "seek": 46532, "start": 485.08, "end": 488.2, "text": " and integration with the ecosystem.", "tokens": [51352, 293, 10980, 365, 264, 11311, 13, 51508], "temperature": 0.0, "avg_logprob": -0.1650236129760742, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0016224735882133245}, {"id": 110, "seek": 46532, "start": 488.2, "end": 493.24, "text": " So we added support for metrics and traces using OpenTelemetry which is a standard for", "tokens": [51508, 407, 321, 3869, 1406, 337, 16367, 293, 26076, 1228, 7238, 14233, 306, 5537, 627, 597, 307, 257, 3832, 337, 51760], "temperature": 0.0, "avg_logprob": -0.1650236129760742, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0016224735882133245}, {"id": 111, "seek": 49324, "start": 493.28000000000003, "end": 496.24, "text": " exporting observability data.", "tokens": [50366, 44686, 9951, 2310, 1412, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15318049380653784, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.004979399498552084}, {"id": 112, "seek": 49324, "start": 496.24, "end": 501.68, "text": " We also added some flexibility because while we had originally built the system like supposed", "tokens": [50514, 492, 611, 3869, 512, 12635, 570, 1339, 321, 632, 7993, 3094, 264, 1185, 411, 3442, 50786], "temperature": 0.0, "avg_logprob": -0.15318049380653784, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.004979399498552084}, {"id": 113, "seek": 49324, "start": 501.68, "end": 505.88, "text": " to have three copies of everything so we would expect to have nodes in three different data", "tokens": [50786, 281, 362, 1045, 14341, 295, 1203, 370, 321, 576, 2066, 281, 362, 13891, 294, 1045, 819, 1412, 50996], "temperature": 0.0, "avg_logprob": -0.15318049380653784, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.004979399498552084}, {"id": 114, "seek": 49324, "start": 505.88, "end": 510.72, "text": " centers actually people were also willing to use the system with less copies so we added", "tokens": [50996, 10898, 767, 561, 645, 611, 4950, 281, 764, 264, 1185, 365, 1570, 14341, 370, 321, 3869, 51238], "temperature": 0.0, "avg_logprob": -0.15318049380653784, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.004979399498552084}, {"id": 115, "seek": 49324, "start": 510.72, "end": 517.04, "text": " one or two copies and we also had some weaker consistency which was useful to like make the", "tokens": [51238, 472, 420, 732, 14341, 293, 321, 611, 632, 512, 24286, 14416, 597, 390, 4420, 281, 411, 652, 264, 51554], "temperature": 0.0, "avg_logprob": -0.15318049380653784, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.004979399498552084}, {"id": 116, "seek": 49324, "start": 517.04, "end": 521.04, "text": " system faster or help recover data in some scenarios.", "tokens": [51554, 1185, 4663, 420, 854, 8114, 1412, 294, 512, 15077, 13, 51754], "temperature": 0.0, "avg_logprob": -0.15318049380653784, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.004979399498552084}, {"id": 117, "seek": 52104, "start": 521.04, "end": 526.0, "text": " We also added integration with Kubernetes for discovery of nodes so that the cluster", "tokens": [50364, 492, 611, 3869, 10980, 365, 23145, 337, 12114, 295, 13891, 370, 300, 264, 13630, 50612], "temperature": 0.0, "avg_logprob": -0.12945519071636777, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.00023779085313435644}, {"id": 118, "seek": 52104, "start": 526.0, "end": 532.1999999999999, "text": " is able to set up automatically the links between each nodes and we also added an administration", "tokens": [50612, 307, 1075, 281, 992, 493, 6772, 264, 6123, 1296, 1184, 13891, 293, 321, 611, 3869, 364, 7236, 50922], "temperature": 0.0, "avg_logprob": -0.12945519071636777, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.00023779085313435644}, {"id": 119, "seek": 52104, "start": 532.1999999999999, "end": 535.5999999999999, "text": " API which is useful to like set up the cluster.", "tokens": [50922, 9362, 597, 307, 4420, 281, 411, 992, 493, 264, 13630, 13, 51092], "temperature": 0.0, "avg_logprob": -0.12945519071636777, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.00023779085313435644}, {"id": 120, "seek": 52104, "start": 535.5999999999999, "end": 541.36, "text": " It's basically a very simple REST API where you can create buckets which are stored spaces,", "tokens": [51092, 467, 311, 1936, 257, 588, 2199, 497, 14497, 9362, 689, 291, 393, 1884, 32191, 597, 366, 12187, 7673, 11, 51380], "temperature": 0.0, "avg_logprob": -0.12945519071636777, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.00023779085313435644}, {"id": 121, "seek": 52104, "start": 541.36, "end": 546.1999999999999, "text": " create access keys, give rights to access keys, etc.", "tokens": [51380, 1884, 2105, 9317, 11, 976, 4601, 281, 2105, 9317, 11, 5183, 13, 51622], "temperature": 0.0, "avg_logprob": -0.12945519071636777, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.00023779085313435644}, {"id": 122, "seek": 52104, "start": 546.1999999999999, "end": 549.1999999999999, "text": " I will just show a little bit about the monitoring part.", "tokens": [51622, 286, 486, 445, 855, 257, 707, 857, 466, 264, 11028, 644, 13, 51772], "temperature": 0.0, "avg_logprob": -0.12945519071636777, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.00023779085313435644}, {"id": 123, "seek": 54920, "start": 549.2, "end": 554.72, "text": " So this is a graphnet dashboard that we made for Garage and as you can see it's actually", "tokens": [50364, 407, 341, 307, 257, 4295, 7129, 18342, 300, 321, 1027, 337, 47918, 293, 382, 291, 393, 536, 309, 311, 767, 50640], "temperature": 0.0, "avg_logprob": -0.1937275303220286, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.0012208858970552683}, {"id": 124, "seek": 54920, "start": 554.72, "end": 556.44, "text": " pretty complete.", "tokens": [50640, 1238, 3566, 13, 50726], "temperature": 0.0, "avg_logprob": -0.1937275303220286, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.0012208858970552683}, {"id": 125, "seek": 54920, "start": 556.44, "end": 561.48, "text": " We can monitor so here is the request going on through the S3 API endpoints and here's", "tokens": [50726, 492, 393, 6002, 370, 510, 307, 264, 5308, 516, 322, 807, 264, 318, 18, 9362, 917, 20552, 293, 510, 311, 50978], "temperature": 0.0, "avg_logprob": -0.1937275303220286, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.0012208858970552683}, {"id": 126, "seek": 54920, "start": 561.48, "end": 566.1600000000001, "text": " the request going through the web endpoints because Garage supports serving buckets directly", "tokens": [50978, 264, 5308, 516, 807, 264, 3670, 917, 20552, 570, 47918, 9346, 8148, 32191, 3838, 51212], "temperature": 0.0, "avg_logprob": -0.1937275303220286, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.0012208858970552683}, {"id": 127, "seek": 54920, "start": 566.1600000000001, "end": 572.08, "text": " as websites which is something we make heavy use of at Le Fleur.", "tokens": [51212, 382, 12891, 597, 307, 746, 321, 652, 4676, 764, 295, 412, 1456, 18612, 374, 13, 51508], "temperature": 0.0, "avg_logprob": -0.1937275303220286, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.0012208858970552683}, {"id": 128, "seek": 54920, "start": 572.08, "end": 576.72, "text": " Here we have the error rate and more interestingly here we have some internal metrics so like", "tokens": [51508, 1692, 321, 362, 264, 6713, 3314, 293, 544, 25873, 510, 321, 362, 512, 6920, 16367, 370, 411, 51740], "temperature": 0.0, "avg_logprob": -0.1937275303220286, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.0012208858970552683}, {"id": 129, "seek": 57672, "start": 576.76, "end": 582.64, "text": " this is the data which is being read and written to the disk on the nodes.", "tokens": [50366, 341, 307, 264, 1412, 597, 307, 885, 1401, 293, 3720, 281, 264, 12355, 322, 264, 13891, 13, 50660], "temperature": 0.0, "avg_logprob": -0.21451493671962193, "compression_ratio": 1.72, "no_speech_prob": 0.004069354385137558}, {"id": 130, "seek": 57672, "start": 582.64, "end": 587.8000000000001, "text": " This is some internal metrics for the communications between nodes RPC and these are some queues", "tokens": [50660, 639, 307, 512, 6920, 16367, 337, 264, 15163, 1296, 13891, 497, 12986, 293, 613, 366, 512, 631, 1247, 50918], "temperature": 0.0, "avg_logprob": -0.21451493671962193, "compression_ratio": 1.72, "no_speech_prob": 0.004069354385137558}, {"id": 131, "seek": 57672, "start": 587.8000000000001, "end": 595.0400000000001, "text": " so how much data is remaining to be processed and so yeah just quick note here the GCQ is", "tokens": [50918, 370, 577, 709, 1412, 307, 8877, 281, 312, 18846, 293, 370, 1338, 445, 1702, 3637, 510, 264, 29435, 48, 307, 51280], "temperature": 0.0, "avg_logprob": -0.21451493671962193, "compression_ratio": 1.72, "no_speech_prob": 0.004069354385137558}, {"id": 132, "seek": 57672, "start": 595.0400000000001, "end": 598.76, "text": " common points where people are like why is this queue not going to zero?", "tokens": [51280, 2689, 2793, 689, 561, 366, 411, 983, 307, 341, 18639, 406, 516, 281, 4018, 30, 51466], "temperature": 0.0, "avg_logprob": -0.21451493671962193, "compression_ratio": 1.72, "no_speech_prob": 0.004069354385137558}, {"id": 133, "seek": 57672, "start": 598.76, "end": 602.12, "text": " It's normal that it's not going to zero because items are staying in the queue for 24 hours", "tokens": [51466, 467, 311, 2710, 300, 309, 311, 406, 516, 281, 4018, 570, 4754, 366, 7939, 294, 264, 18639, 337, 4022, 2496, 51634], "temperature": 0.0, "avg_logprob": -0.21451493671962193, "compression_ratio": 1.72, "no_speech_prob": 0.004069354385137558}, {"id": 134, "seek": 57672, "start": 602.12, "end": 606.0400000000001, "text": " before they're processed just for information.", "tokens": [51634, 949, 436, 434, 18846, 445, 337, 1589, 13, 51830], "temperature": 0.0, "avg_logprob": -0.21451493671962193, "compression_ratio": 1.72, "no_speech_prob": 0.004069354385137558}, {"id": 135, "seek": 60604, "start": 606.04, "end": 610.8, "text": " So basically this queue should almost be to zero and this one too and if it's not then", "tokens": [50364, 407, 1936, 341, 18639, 820, 1920, 312, 281, 4018, 293, 341, 472, 886, 293, 498, 309, 311, 406, 550, 50602], "temperature": 0.0, "avg_logprob": -0.19277885073707218, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.000552693207282573}, {"id": 136, "seek": 60604, "start": 610.8, "end": 614.16, "text": " probably your system is under too much load.", "tokens": [50602, 1391, 428, 1185, 307, 833, 886, 709, 3677, 13, 50770], "temperature": 0.0, "avg_logprob": -0.19277885073707218, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.000552693207282573}, {"id": 137, "seek": 60604, "start": 614.16, "end": 618.8, "text": " And we also have tracing so if you want to go further into like how Garage is handling", "tokens": [50770, 400, 321, 611, 362, 25262, 370, 498, 291, 528, 281, 352, 3052, 666, 411, 577, 47918, 307, 13175, 51002], "temperature": 0.0, "avg_logprob": -0.19277885073707218, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.000552693207282573}, {"id": 138, "seek": 60604, "start": 618.8, "end": 621.9599999999999, "text": " a request you can use this feature.", "tokens": [51002, 257, 5308, 291, 393, 764, 341, 4111, 13, 51160], "temperature": 0.0, "avg_logprob": -0.19277885073707218, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.000552693207282573}, {"id": 139, "seek": 60604, "start": 621.9599999999999, "end": 626.92, "text": " So here we're exporting traces to Yeager and this is a trace of a pretty standard list", "tokens": [51160, 407, 510, 321, 434, 44686, 26076, 281, 835, 3557, 293, 341, 307, 257, 13508, 295, 257, 1238, 3832, 1329, 51408], "temperature": 0.0, "avg_logprob": -0.19277885073707218, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.000552693207282573}, {"id": 140, "seek": 60604, "start": 626.92, "end": 632.4, "text": " objects API call and so we can see that the list objects is first reading some data to", "tokens": [51408, 6565, 9362, 818, 293, 370, 321, 393, 536, 300, 264, 1329, 6565, 307, 700, 3760, 512, 1412, 281, 51682], "temperature": 0.0, "avg_logprob": -0.19277885073707218, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.000552693207282573}, {"id": 141, "seek": 63240, "start": 632.4, "end": 636.0799999999999, "text": " get some access information on the access key and the buckets.", "tokens": [50364, 483, 512, 2105, 1589, 322, 264, 2105, 2141, 293, 264, 32191, 13, 50548], "temperature": 0.0, "avg_logprob": -0.11417360963492558, "compression_ratio": 1.862962962962963, "no_speech_prob": 0.003376295091584325}, {"id": 142, "seek": 63240, "start": 636.0799999999999, "end": 640.16, "text": " So this is some very fast call because all this information is copied on the node and", "tokens": [50548, 407, 341, 307, 512, 588, 2370, 818, 570, 439, 341, 1589, 307, 25365, 322, 264, 9984, 293, 50752], "temperature": 0.0, "avg_logprob": -0.11417360963492558, "compression_ratio": 1.862962962962963, "no_speech_prob": 0.003376295091584325}, {"id": 143, "seek": 63240, "start": 640.16, "end": 645.12, "text": " it can just read it locally and then it's going to do some actual requesting on remote", "tokens": [50752, 309, 393, 445, 1401, 309, 16143, 293, 550, 309, 311, 516, 281, 360, 512, 3539, 31937, 322, 8607, 51000], "temperature": 0.0, "avg_logprob": -0.11417360963492558, "compression_ratio": 1.862962962962963, "no_speech_prob": 0.003376295091584325}, {"id": 144, "seek": 63240, "start": 645.12, "end": 649.64, "text": " nodes for the list of objects that should return and we see here that it's sending a", "tokens": [51000, 13891, 337, 264, 1329, 295, 6565, 300, 820, 2736, 293, 321, 536, 510, 300, 309, 311, 7750, 257, 51226], "temperature": 0.0, "avg_logprob": -0.11417360963492558, "compression_ratio": 1.862962962962963, "no_speech_prob": 0.003376295091584325}, {"id": 145, "seek": 63240, "start": 649.64, "end": 655.1999999999999, "text": " request to two nodes and the request is taking a bit of time before it completes and then", "tokens": [51226, 5308, 281, 732, 13891, 293, 264, 5308, 307, 1940, 257, 857, 295, 565, 949, 309, 36362, 293, 550, 51504], "temperature": 0.0, "avg_logprob": -0.11417360963492558, "compression_ratio": 1.862962962962963, "no_speech_prob": 0.003376295091584325}, {"id": 146, "seek": 63240, "start": 655.1999999999999, "end": 660.72, "text": " so yeah I think this is a pretty slow cluster and it's taking 100 milliseconds but on faster", "tokens": [51504, 370, 1338, 286, 519, 341, 307, 257, 1238, 2964, 13630, 293, 309, 311, 1940, 2319, 34184, 457, 322, 4663, 51780], "temperature": 0.0, "avg_logprob": -0.11417360963492558, "compression_ratio": 1.862962962962963, "no_speech_prob": 0.003376295091584325}, {"id": 147, "seek": 66072, "start": 660.72, "end": 665.12, "text": " hardware it can be of course much faster.", "tokens": [50364, 8837, 309, 393, 312, 295, 1164, 709, 4663, 13, 50584], "temperature": 0.0, "avg_logprob": -0.15628500108595017, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.015900403261184692}, {"id": 148, "seek": 66072, "start": 665.12, "end": 673.96, "text": " So this was 0.7 and then we did 0.8 so that was at the end of the NGI pointer grant and", "tokens": [50584, 407, 341, 390, 1958, 13, 22, 293, 550, 321, 630, 1958, 13, 23, 370, 300, 390, 412, 264, 917, 295, 264, 426, 26252, 23918, 6386, 293, 51026], "temperature": 0.0, "avg_logprob": -0.15628500108595017, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.015900403261184692}, {"id": 149, "seek": 66072, "start": 673.96, "end": 682.5600000000001, "text": " for 0.8 we had a pretty high focus on making the performance better.", "tokens": [51026, 337, 1958, 13, 23, 321, 632, 257, 1238, 1090, 1879, 322, 1455, 264, 3389, 1101, 13, 51456], "temperature": 0.0, "avg_logprob": -0.15628500108595017, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.015900403261184692}, {"id": 150, "seek": 66072, "start": 682.5600000000001, "end": 687.12, "text": " So first thing we did was like change the metadata engine because we were using sled", "tokens": [51456, 407, 700, 551, 321, 630, 390, 411, 1319, 264, 26603, 2848, 570, 321, 645, 1228, 46242, 51684], "temperature": 0.0, "avg_logprob": -0.15628500108595017, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.015900403261184692}, {"id": 151, "seek": 68712, "start": 687.12, "end": 691.72, "text": " and it had a lot of issues I'll talk about that and we did some various performance improvements", "tokens": [50364, 293, 309, 632, 257, 688, 295, 2663, 286, 603, 751, 466, 300, 293, 321, 630, 512, 3683, 3389, 13797, 50594], "temperature": 0.0, "avg_logprob": -0.18848880874776394, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.044599615037441254}, {"id": 152, "seek": 68712, "start": 691.72, "end": 700.36, "text": " across the board making basically some pretty good improvements in this kind of area and", "tokens": [50594, 2108, 264, 3150, 1455, 1936, 512, 1238, 665, 13797, 294, 341, 733, 295, 1859, 293, 51026], "temperature": 0.0, "avg_logprob": -0.18848880874776394, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.044599615037441254}, {"id": 153, "seek": 68712, "start": 700.36, "end": 704.24, "text": " in terms of features we added CODAS so this is not a feature from Amazon but it's a feature", "tokens": [51026, 294, 2115, 295, 4122, 321, 3869, 3002, 35, 3160, 370, 341, 307, 406, 257, 4111, 490, 6795, 457, 309, 311, 257, 4111, 51220], "temperature": 0.0, "avg_logprob": -0.18848880874776394, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.044599615037441254}, {"id": 154, "seek": 68712, "start": 704.24, "end": 708.88, "text": " which you can add on Garage is like limit the size of a bucket to a maximum size of", "tokens": [51220, 597, 291, 393, 909, 322, 47918, 307, 411, 4948, 264, 2744, 295, 257, 13058, 281, 257, 6674, 2744, 295, 51452], "temperature": 0.0, "avg_logprob": -0.18848880874776394, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.044599615037441254}, {"id": 155, "seek": 68712, "start": 708.88, "end": 713.24, "text": " objects or maximum number of objects and it's pretty useful in a multi-tonnets setup where", "tokens": [51452, 6565, 420, 6674, 1230, 295, 6565, 293, 309, 311, 1238, 4420, 294, 257, 4825, 12, 1756, 77, 1385, 8657, 689, 51670], "temperature": 0.0, "avg_logprob": -0.18848880874776394, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.044599615037441254}, {"id": 156, "seek": 71324, "start": 713.36, "end": 720.28, "text": " we'd like to lend some storage space to someone but have them restrain to some fixed capacity", "tokens": [50370, 321, 1116, 411, 281, 21774, 512, 6725, 1901, 281, 1580, 457, 362, 552, 1472, 7146, 281, 512, 6806, 6042, 50716], "temperature": 0.0, "avg_logprob": -0.153832551204797, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.08381757885217667}, {"id": 157, "seek": 71324, "start": 720.28, "end": 726.28, "text": " and of course some regular developments on quality of life improvements etc.", "tokens": [50716, 293, 295, 1164, 512, 3890, 20862, 322, 3125, 295, 993, 13797, 5183, 13, 51016], "temperature": 0.0, "avg_logprob": -0.153832551204797, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.08381757885217667}, {"id": 158, "seek": 71324, "start": 726.28, "end": 730.48, "text": " So yeah just to talk a little bit about the metadata engine so we were using sled which", "tokens": [51016, 407, 1338, 445, 281, 751, 257, 707, 857, 466, 264, 26603, 2848, 370, 321, 645, 1228, 46242, 597, 51226], "temperature": 0.0, "avg_logprob": -0.153832551204797, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.08381757885217667}, {"id": 159, "seek": 71324, "start": 730.48, "end": 736.48, "text": " is a metadata key value store embedded key value store which is written in Rust so we", "tokens": [51226, 307, 257, 26603, 2141, 2158, 3531, 16741, 2141, 2158, 3531, 597, 307, 3720, 294, 34952, 370, 321, 51526], "temperature": 0.0, "avg_logprob": -0.153832551204797, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.08381757885217667}, {"id": 160, "seek": 71324, "start": 736.48, "end": 740.0, "text": " thought yeah it's written in Rust it's pretty good Garage is also written in Rust so let's", "tokens": [51526, 1194, 1338, 309, 311, 3720, 294, 34952, 309, 311, 1238, 665, 47918, 307, 611, 3720, 294, 34952, 370, 718, 311, 51702], "temperature": 0.0, "avg_logprob": -0.153832551204797, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.08381757885217667}, {"id": 161, "seek": 74000, "start": 740.04, "end": 746.28, "text": " just integrate them and at the point when we started Garage sled was like one of the most", "tokens": [50366, 445, 13365, 552, 293, 412, 264, 935, 562, 321, 1409, 47918, 46242, 390, 411, 472, 295, 264, 881, 50678], "temperature": 0.0, "avg_logprob": -0.14936773059437575, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.005551887676119804}, {"id": 162, "seek": 74000, "start": 746.28, "end": 752.96, "text": " popular key value stores for Rust but actually it's not very well maintained anymore and it had", "tokens": [50678, 3743, 2141, 2158, 9512, 337, 34952, 457, 767, 309, 311, 406, 588, 731, 17578, 3602, 293, 309, 632, 51012], "temperature": 0.0, "avg_logprob": -0.14936773059437575, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.005551887676119804}, {"id": 163, "seek": 74000, "start": 752.96, "end": 758.96, "text": " many issues so it was making very large files on disk because it was like just writing and writing", "tokens": [51012, 867, 2663, 370, 309, 390, 1455, 588, 2416, 7098, 322, 12355, 570, 309, 390, 411, 445, 3579, 293, 3579, 51312], "temperature": 0.0, "avg_logprob": -0.14936773059437575, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.005551887676119804}, {"id": 164, "seek": 74000, "start": 758.96, "end": 763.12, "text": " and writing and probably it was some internal way to optimize performance but it was not very", "tokens": [51312, 293, 3579, 293, 1391, 309, 390, 512, 6920, 636, 281, 19719, 3389, 457, 309, 390, 406, 588, 51520], "temperature": 0.0, "avg_logprob": -0.14936773059437575, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.005551887676119804}, {"id": 165, "seek": 74000, "start": 763.12, "end": 769.44, "text": " satisfactory for us to have like data files that were 10 times too big. The performance was also", "tokens": [51520, 48614, 337, 505, 281, 362, 411, 1412, 7098, 300, 645, 1266, 1413, 886, 955, 13, 440, 3389, 390, 611, 51836], "temperature": 0.0, "avg_logprob": -0.14936773059437575, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.005551887676119804}, {"id": 166, "seek": 76944, "start": 769.8800000000001, "end": 776.1600000000001, "text": " pretty unpredictable on spinning hard drives it was actually very bad and also from a developer", "tokens": [50386, 1238, 31160, 322, 15640, 1152, 11754, 309, 390, 767, 588, 1578, 293, 611, 490, 257, 10754, 50700], "temperature": 0.0, "avg_logprob": -0.1761311235881987, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.0004369694506749511}, {"id": 167, "seek": 76944, "start": 776.1600000000001, "end": 781.96, "text": " perspective it has some API limitations and this has prevented us from implementing some specific", "tokens": [50700, 4585, 309, 575, 512, 9362, 15705, 293, 341, 575, 27314, 505, 490, 18114, 512, 2685, 50990], "temperature": 0.0, "avg_logprob": -0.1761311235881987, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.0004369694506749511}, {"id": 168, "seek": 76944, "start": 781.96, "end": 788.72, "text": " features in Garage and hopefully when we get rid of sled we can actually do that. So as an", "tokens": [50990, 4122, 294, 47918, 293, 4696, 562, 321, 483, 3973, 295, 46242, 321, 393, 767, 360, 300, 13, 407, 382, 364, 51328], "temperature": 0.0, "avg_logprob": -0.1761311235881987, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.0004369694506749511}, {"id": 169, "seek": 76944, "start": 788.72, "end": 795.4000000000001, "text": " alternative we added LMDB so LMDB is a key value storage which is used I think in OpenLDAP", "tokens": [51328, 8535, 321, 3869, 46529, 27735, 370, 46529, 27735, 307, 257, 2141, 2158, 6725, 597, 307, 1143, 286, 519, 294, 7238, 43, 35, 4715, 51662], "temperature": 0.0, "avg_logprob": -0.1761311235881987, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.0004369694506749511}, {"id": 170, "seek": 79540, "start": 795.4399999999999, "end": 800.6, "text": " and some other software and it's a pretty established piece of software at this point so we", "tokens": [50366, 293, 512, 661, 4722, 293, 309, 311, 257, 1238, 7545, 2522, 295, 4722, 412, 341, 935, 370, 321, 50624], "temperature": 0.0, "avg_logprob": -0.13403010648839614, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.008569393306970596}, {"id": 171, "seek": 79540, "start": 800.6, "end": 805.72, "text": " consider it pretty stable it has good performance and it maintains a reasonable size of files on", "tokens": [50624, 1949, 309, 1238, 8351, 309, 575, 665, 3389, 293, 309, 33385, 257, 10585, 2744, 295, 7098, 322, 50880], "temperature": 0.0, "avg_logprob": -0.13403010648839614, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.008569393306970596}, {"id": 172, "seek": 79540, "start": 805.72, "end": 813.3199999999999, "text": " disk so this is the default now and we also have SQLite as a second choice originally we had not", "tokens": [50880, 12355, 370, 341, 307, 264, 7576, 586, 293, 321, 611, 362, 19200, 642, 382, 257, 1150, 3922, 7993, 321, 632, 406, 51260], "temperature": 0.0, "avg_logprob": -0.13403010648839614, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.008569393306970596}, {"id": 173, "seek": 79540, "start": 813.3199999999999, "end": 818.84, "text": " optimized SQLite that much so it was not recommended we had not made another bunch of tests but", "tokens": [51260, 26941, 19200, 642, 300, 709, 370, 309, 390, 406, 9628, 321, 632, 406, 1027, 1071, 3840, 295, 6921, 457, 51536], "temperature": 0.0, "avg_logprob": -0.13403010648839614, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.008569393306970596}, {"id": 174, "seek": 81884, "start": 818.88, "end": 826.0400000000001, "text": " probably now it's okay to use as well and just to show some comparison we did some benchmarks", "tokens": [50366, 1391, 586, 309, 311, 1392, 281, 764, 382, 731, 293, 445, 281, 855, 512, 9660, 321, 630, 512, 43751, 50724], "temperature": 0.0, "avg_logprob": -0.16814513433547246, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.002433838089928031}, {"id": 175, "seek": 81884, "start": 826.0400000000001, "end": 832.32, "text": " and basically LMDB is much faster pretty much twice as fast as sled not really twice but", "tokens": [50724, 293, 1936, 46529, 27735, 307, 709, 4663, 1238, 709, 6091, 382, 2370, 382, 46242, 406, 534, 6091, 457, 51038], "temperature": 0.0, "avg_logprob": -0.16814513433547246, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.002433838089928031}, {"id": 176, "seek": 81884, "start": 832.32, "end": 838.24, "text": " actually significantly faster and for all these common API endpoints and SQLite was not optimized", "tokens": [51038, 767, 10591, 4663, 293, 337, 439, 613, 2689, 9362, 917, 20552, 293, 19200, 642, 390, 406, 26941, 51334], "temperature": 0.0, "avg_logprob": -0.16814513433547246, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.002433838089928031}, {"id": 177, "seek": 81884, "start": 838.24, "end": 847.1600000000001, "text": " at that time I cannot I do not have the data updated for now. Another optimization we made is", "tokens": [51334, 412, 300, 565, 286, 2644, 286, 360, 406, 362, 264, 1412, 10588, 337, 586, 13, 3996, 19618, 321, 1027, 307, 51780], "temperature": 0.0, "avg_logprob": -0.16814513433547246, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.002433838089928031}, {"id": 178, "seek": 84716, "start": 847.52, "end": 854.12, "text": " block streaming so the idea here is that Garage will store your data so when it receives an object", "tokens": [50382, 3461, 11791, 370, 264, 1558, 510, 307, 300, 47918, 486, 3531, 428, 1412, 370, 562, 309, 20717, 364, 2657, 50712], "temperature": 0.0, "avg_logprob": -0.17029464107820358, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.0027137529104948044}, {"id": 179, "seek": 84716, "start": 854.12, "end": 860.0799999999999, "text": " it will split the object into pieces of by default 1 megabyte and then store these pieces on data", "tokens": [50712, 309, 486, 7472, 264, 2657, 666, 3755, 295, 538, 7576, 502, 10816, 34529, 293, 550, 3531, 613, 3755, 322, 1412, 51010], "temperature": 0.0, "avg_logprob": -0.17029464107820358, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.0027137529104948044}, {"id": 180, "seek": 84716, "start": 860.0799999999999, "end": 867.4, "text": " servers all around the cluster and then when you want to read the data well your API request is", "tokens": [51010, 15909, 439, 926, 264, 13630, 293, 550, 562, 291, 528, 281, 1401, 264, 1412, 731, 428, 9362, 5308, 307, 51376], "temperature": 0.0, "avg_logprob": -0.17029464107820358, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.0027137529104948044}, {"id": 181, "seek": 84716, "start": 867.4, "end": 872.8, "text": " going to go through some Garage node which is going to receive the request is going to look at", "tokens": [51376, 516, 281, 352, 807, 512, 47918, 9984, 597, 307, 516, 281, 4774, 264, 5308, 307, 516, 281, 574, 412, 51646], "temperature": 0.0, "avg_logprob": -0.17029464107820358, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.0027137529104948044}, {"id": 182, "seek": 87280, "start": 873.12, "end": 877.4399999999999, "text": " the objects the metadata and determine okay we have to get this part this part this part from", "tokens": [50380, 264, 6565, 264, 26603, 293, 6997, 1392, 321, 362, 281, 483, 341, 644, 341, 644, 341, 644, 490, 50596], "temperature": 0.0, "avg_logprob": -0.12606569094078562, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.0041976142674684525}, {"id": 183, "seek": 87280, "start": 877.4399999999999, "end": 883.12, "text": " these different nodes in the cluster so it's going to do an internal RPC request to the storage", "tokens": [50596, 613, 819, 13891, 294, 264, 13630, 370, 309, 311, 516, 281, 360, 364, 6920, 497, 12986, 5308, 281, 264, 6725, 50880], "temperature": 0.0, "avg_logprob": -0.12606569094078562, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.0041976142674684525}, {"id": 184, "seek": 87280, "start": 883.12, "end": 890.56, "text": " node which has the actual 1 megabyte data block and so this is how it was working before basically", "tokens": [50880, 9984, 597, 575, 264, 3539, 502, 10816, 34529, 1412, 3461, 293, 370, 341, 307, 577, 309, 390, 1364, 949, 1936, 51252], "temperature": 0.0, "avg_logprob": -0.12606569094078562, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.0041976142674684525}, {"id": 185, "seek": 87280, "start": 890.56, "end": 895.8, "text": " this first node that was receiving the API request it would like just read the 1 megabyte into RAM", "tokens": [51252, 341, 700, 9984, 300, 390, 10040, 264, 9362, 5308, 309, 576, 411, 445, 1401, 264, 502, 10816, 34529, 666, 14561, 51514], "temperature": 0.0, "avg_logprob": -0.12606569094078562, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.0041976142674684525}, {"id": 186, "seek": 87280, "start": 895.8, "end": 901.04, "text": " and not send anything to the client before so basically here the client is just waiting for", "tokens": [51514, 293, 406, 2845, 1340, 281, 264, 6423, 949, 370, 1936, 510, 264, 6423, 307, 445, 3806, 337, 51776], "temperature": 0.0, "avg_logprob": -0.12606569094078562, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.0041976142674684525}, {"id": 187, "seek": 90104, "start": 901.28, "end": 905.52, "text": " the data to arrive and the data is being transferred here between these two nodes between inside the", "tokens": [50376, 264, 1412, 281, 8881, 293, 264, 1412, 307, 885, 15809, 510, 1296, 613, 732, 13891, 1296, 1854, 264, 50588], "temperature": 0.0, "avg_logprob": -0.1457808503826845, "compression_ratio": 1.8937007874015748, "no_speech_prob": 0.0009696478373371065}, {"id": 188, "seek": 90104, "start": 905.52, "end": 909.68, "text": " cluster and so basically the client is just waiting for some stuff to happen inside the cluster", "tokens": [50588, 13630, 293, 370, 1936, 264, 6423, 307, 445, 3806, 337, 512, 1507, 281, 1051, 1854, 264, 13630, 50796], "temperature": 0.0, "avg_logprob": -0.1457808503826845, "compression_ratio": 1.8937007874015748, "no_speech_prob": 0.0009696478373371065}, {"id": 189, "seek": 90104, "start": 909.68, "end": 915.28, "text": " where it could just have received some data earlier and so the optimization we made was", "tokens": [50796, 689, 309, 727, 445, 362, 4613, 512, 1412, 3071, 293, 370, 264, 19618, 321, 1027, 390, 51076], "temperature": 0.0, "avg_logprob": -0.1457808503826845, "compression_ratio": 1.8937007874015748, "no_speech_prob": 0.0009696478373371065}, {"id": 190, "seek": 90104, "start": 915.28, "end": 920.28, "text": " actually pretty simple but it's pretty big change in the code it was to start sending the data as", "tokens": [51076, 767, 1238, 2199, 457, 309, 311, 1238, 955, 1319, 294, 264, 3089, 309, 390, 281, 722, 7750, 264, 1412, 382, 51326], "temperature": 0.0, "avg_logprob": -0.1457808503826845, "compression_ratio": 1.8937007874015748, "no_speech_prob": 0.0009696478373371065}, {"id": 191, "seek": 90104, "start": 920.28, "end": 925.16, "text": " soon as it arrives to this intermediate node and so here we just have a small buffer of data which", "tokens": [51326, 2321, 382, 309, 20116, 281, 341, 19376, 9984, 293, 370, 510, 321, 445, 362, 257, 1359, 21762, 295, 1412, 597, 51570], "temperature": 0.0, "avg_logprob": -0.1457808503826845, "compression_ratio": 1.8937007874015748, "no_speech_prob": 0.0009696478373371065}, {"id": 192, "seek": 92516, "start": 925.1999999999999, "end": 933.8399999999999, "text": " is received and waiting to be sent back to the client and so by doing this pretty small change", "tokens": [50366, 307, 4613, 293, 3806, 281, 312, 2279, 646, 281, 264, 6423, 293, 370, 538, 884, 341, 1238, 1359, 1319, 50798], "temperature": 0.0, "avg_logprob": -0.1458678077248966, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.004752676002681255}, {"id": 193, "seek": 92516, "start": 933.8399999999999, "end": 940.04, "text": " we actually managed to reduce the time to first byte measurement so this measurement is when you", "tokens": [50798, 321, 767, 6453, 281, 5407, 264, 565, 281, 700, 40846, 13160, 370, 341, 13160, 307, 562, 291, 51108], "temperature": 0.0, "avg_logprob": -0.1458678077248966, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.004752676002681255}, {"id": 194, "seek": 92516, "start": 940.04, "end": 946.56, "text": " do a request to Garage to receive to get an object you will specify the path of the object send", "tokens": [51108, 360, 257, 5308, 281, 47918, 281, 4774, 281, 483, 364, 2657, 291, 486, 16500, 264, 3100, 295, 264, 2657, 2845, 51434], "temperature": 0.0, "avg_logprob": -0.1458678077248966, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.004752676002681255}, {"id": 195, "seek": 92516, "start": 946.56, "end": 951.56, "text": " your HTTP request all the headers etc and then you will wait for the server to reply the server", "tokens": [51434, 428, 33283, 5308, 439, 264, 45101, 5183, 293, 550, 291, 486, 1699, 337, 264, 7154, 281, 16972, 264, 7154, 51684], "temperature": 0.0, "avg_logprob": -0.1458678077248966, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.004752676002681255}, {"id": 196, "seek": 95156, "start": 951.5999999999999, "end": 956.3199999999999, "text": " will give you some headers saying okay the object is coming and then he will start streaming some", "tokens": [50366, 486, 976, 291, 512, 45101, 1566, 1392, 264, 2657, 307, 1348, 293, 550, 415, 486, 722, 11791, 512, 50602], "temperature": 0.0, "avg_logprob": -0.14849272398191077, "compression_ratio": 1.8505747126436782, "no_speech_prob": 0.0028442940674722195}, {"id": 197, "seek": 95156, "start": 956.3199999999999, "end": 961.1199999999999, "text": " data and so this measures the time between you the point where you start sending a request and the", "tokens": [50602, 1412, 293, 370, 341, 8000, 264, 565, 1296, 291, 264, 935, 689, 291, 722, 7750, 257, 5308, 293, 264, 50842], "temperature": 0.0, "avg_logprob": -0.14849272398191077, "compression_ratio": 1.8505747126436782, "no_speech_prob": 0.0028442940674722195}, {"id": 198, "seek": 95156, "start": 961.1199999999999, "end": 968.28, "text": " moment where the first actual bytes of the data file are coming back and here we are in a actually", "tokens": [50842, 1623, 689, 264, 700, 3539, 36088, 295, 264, 1412, 3991, 366, 1348, 646, 293, 510, 321, 366, 294, 257, 767, 51200], "temperature": 0.0, "avg_logprob": -0.14849272398191077, "compression_ratio": 1.8505747126436782, "no_speech_prob": 0.0028442940674722195}, {"id": 199, "seek": 95156, "start": 968.28, "end": 973.56, "text": " again it's a simulated deployment but we have pretty slow networking so 5 megabits per second so", "tokens": [51200, 797, 309, 311, 257, 41713, 19317, 457, 321, 362, 1238, 2964, 17985, 370, 1025, 10816, 455, 1208, 680, 1150, 370, 51464], "temperature": 0.0, "avg_logprob": -0.14849272398191077, "compression_ratio": 1.8505747126436782, "no_speech_prob": 0.0028442940674722195}, {"id": 200, "seek": 95156, "start": 973.56, "end": 979.1199999999999, "text": " it's actually very slow and so before the optimization garage was here so we would have to", "tokens": [51464, 309, 311, 767, 588, 2964, 293, 370, 949, 264, 19618, 14400, 390, 510, 370, 321, 576, 362, 281, 51742], "temperature": 0.0, "avg_logprob": -0.14849272398191077, "compression_ratio": 1.8505747126436782, "no_speech_prob": 0.0028442940674722195}, {"id": 201, "seek": 97912, "start": 979.32, "end": 984.24, "text": " wait pretty much two seconds before some data was coming because the like a one megabyte file was", "tokens": [50374, 1699, 1238, 709, 732, 3949, 949, 512, 1412, 390, 1348, 570, 264, 411, 257, 472, 10816, 34529, 3991, 390, 50620], "temperature": 0.0, "avg_logprob": -0.1731848493914738, "compression_ratio": 1.7942238267148014, "no_speech_prob": 0.016638724133372307}, {"id": 202, "seek": 97912, "start": 984.24, "end": 989.4, "text": " being transferred out this very slow connection before it could be returned. Minio has some average", "tokens": [50620, 885, 15809, 484, 341, 588, 2964, 4984, 949, 309, 727, 312, 8752, 13, 2829, 1004, 575, 512, 4274, 50878], "temperature": 0.0, "avg_logprob": -0.1731848493914738, "compression_ratio": 1.7942238267148014, "no_speech_prob": 0.016638724133372307}, {"id": 203, "seek": 97912, "start": 989.4, "end": 994.28, "text": " performance here and with the optimization garage is very fast and we're able to return the first", "tokens": [50878, 3389, 510, 293, 365, 264, 19618, 14400, 307, 588, 2370, 293, 321, 434, 1075, 281, 2736, 264, 700, 51122], "temperature": 0.0, "avg_logprob": -0.1731848493914738, "compression_ratio": 1.7942238267148014, "no_speech_prob": 0.016638724133372307}, {"id": 204, "seek": 97912, "start": 994.28, "end": 999.52, "text": " bytes of the data and so this is important because for instance for websites you want to display the", "tokens": [51122, 36088, 295, 264, 1412, 293, 370, 341, 307, 1021, 570, 337, 5197, 337, 12891, 291, 528, 281, 4674, 264, 51384], "temperature": 0.0, "avg_logprob": -0.1731848493914738, "compression_ratio": 1.7942238267148014, "no_speech_prob": 0.016638724133372307}, {"id": 205, "seek": 97912, "start": 999.52, "end": 1004.32, "text": " content as fast as possible and even if it's a big file then maybe the first bytes are very relevant", "tokens": [51384, 2701, 382, 2370, 382, 1944, 293, 754, 498, 309, 311, 257, 955, 3991, 550, 1310, 264, 700, 36088, 366, 588, 7340, 51624], "temperature": 0.0, "avg_logprob": -0.1731848493914738, "compression_ratio": 1.7942238267148014, "no_speech_prob": 0.016638724133372307}, {"id": 206, "seek": 100432, "start": 1004.32, "end": 1009.88, "text": " so for an image you can have a preview in the first bytes for an HTML file we can have pretty much", "tokens": [50364, 370, 337, 364, 3256, 291, 393, 362, 257, 14281, 294, 264, 700, 36088, 337, 364, 17995, 3991, 321, 393, 362, 1238, 709, 50642], "temperature": 0.0, "avg_logprob": -0.1845556436125765, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0003569464315660298}, {"id": 207, "seek": 100432, "start": 1009.88, "end": 1016.48, "text": " everything and so minimizing this time is very critical to user experience. So I think we pretty", "tokens": [50642, 1203, 293, 370, 46608, 341, 565, 307, 588, 4924, 281, 4195, 1752, 13, 407, 286, 519, 321, 1238, 50972], "temperature": 0.0, "avg_logprob": -0.1845556436125765, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0003569464315660298}, {"id": 208, "seek": 100432, "start": 1016.48, "end": 1021.5600000000001, "text": " much managed to do this and we also did some other various improvements on the code pass and", "tokens": [50972, 709, 6453, 281, 360, 341, 293, 321, 611, 630, 512, 661, 3683, 13797, 322, 264, 3089, 1320, 293, 51226], "temperature": 0.0, "avg_logprob": -0.1845556436125765, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0003569464315660298}, {"id": 209, "seek": 100432, "start": 1021.5600000000001, "end": 1031.28, "text": " garage so on the bottom we have 0.7 then we have 0.8 beta 1 beta 2 here we removed some F-sync and", "tokens": [51226, 14400, 370, 322, 264, 2767, 321, 362, 1958, 13, 22, 550, 321, 362, 1958, 13, 23, 9861, 502, 9861, 568, 510, 321, 7261, 512, 479, 12, 82, 34015, 293, 51712], "temperature": 0.0, "avg_logprob": -0.1845556436125765, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0003569464315660298}, {"id": 210, "seek": 103128, "start": 1031.32, "end": 1037.48, "text": " it's completely optional to have F-sync and we're almost matching so here is like raw throughput", "tokens": [50366, 309, 311, 2584, 17312, 281, 362, 479, 12, 82, 34015, 293, 321, 434, 1920, 14324, 370, 510, 307, 411, 8936, 44629, 50674], "temperature": 0.0, "avg_logprob": -0.13908802777871318, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.002714778995141387}, {"id": 211, "seek": 103128, "start": 1037.48, "end": 1042.12, "text": " when you're reading and writing big objects continuously to garage the throughput is still", "tokens": [50674, 562, 291, 434, 3760, 293, 3579, 955, 6565, 15684, 281, 14400, 264, 44629, 307, 920, 50906], "temperature": 0.0, "avg_logprob": -0.13908802777871318, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.002714778995141387}, {"id": 212, "seek": 103128, "start": 1042.12, "end": 1048.24, "text": " a bit worse than Minio but it's actually getting pretty close so there's still room for improvement", "tokens": [50906, 257, 857, 5324, 813, 2829, 1004, 457, 309, 311, 767, 1242, 1238, 1998, 370, 456, 311, 920, 1808, 337, 10444, 51212], "temperature": 0.0, "avg_logprob": -0.13908802777871318, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.002714778995141387}, {"id": 213, "seek": 103128, "start": 1048.24, "end": 1053.92, "text": " in this domain and it's yeah we haven't done much more work on this but it's definitely something", "tokens": [51212, 294, 341, 9274, 293, 309, 311, 1338, 321, 2378, 380, 1096, 709, 544, 589, 322, 341, 457, 309, 311, 2138, 746, 51496], "temperature": 0.0, "avg_logprob": -0.13908802777871318, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.002714778995141387}, {"id": 214, "seek": 105392, "start": 1053.92, "end": 1062.64, "text": " that could still be optimized I believe. So then it was the end of the NGI pointer grants so we did", "tokens": [50364, 300, 727, 920, 312, 26941, 286, 1697, 13, 407, 550, 309, 390, 264, 917, 295, 264, 426, 26252, 23918, 16101, 370, 321, 630, 50800], "temperature": 0.0, "avg_logprob": -0.1806003212928772, "compression_ratio": 1.5077720207253886, "no_speech_prob": 0.00711096590384841}, {"id": 215, "seek": 105392, "start": 1062.64, "end": 1067.88, "text": " a bunch of conferences in France this was not me this was other people from Duffleur and then we", "tokens": [50800, 257, 3840, 295, 22032, 294, 6190, 341, 390, 406, 385, 341, 390, 661, 561, 490, 413, 1245, 306, 374, 293, 550, 321, 51062], "temperature": 0.0, "avg_logprob": -0.1806003212928772, "compression_ratio": 1.5077720207253886, "no_speech_prob": 0.00711096590384841}, {"id": 216, "seek": 105392, "start": 1067.88, "end": 1079.48, "text": " started another grant by NGI 0 through NLNet and this led to the release of 0.9 and so 0.9 was", "tokens": [51062, 1409, 1071, 6386, 538, 426, 26252, 1958, 807, 426, 43, 31890, 293, 341, 4684, 281, 264, 4374, 295, 1958, 13, 24, 293, 370, 1958, 13, 24, 390, 51642], "temperature": 0.0, "avg_logprob": -0.1806003212928772, "compression_ratio": 1.5077720207253886, "no_speech_prob": 0.00711096590384841}, {"id": 217, "seek": 107948, "start": 1079.48, "end": 1087.8, "text": " actually a pretty big release so yeah we had a support for multiple AGDs per node and this is", "tokens": [50364, 767, 257, 1238, 955, 4374, 370, 1338, 321, 632, 257, 1406, 337, 3866, 28406, 35, 82, 680, 9984, 293, 341, 307, 50780], "temperature": 0.0, "avg_logprob": -0.1499680201212565, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.09245523065328598}, {"id": 218, "seek": 107948, "start": 1087.8, "end": 1094.16, "text": " actually a pretty big feature because now you can have one garage node which is directly talking to", "tokens": [50780, 767, 257, 1238, 955, 4111, 570, 586, 291, 393, 362, 472, 14400, 9984, 597, 307, 3838, 1417, 281, 51098], "temperature": 0.0, "avg_logprob": -0.1499680201212565, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.09245523065328598}, {"id": 219, "seek": 107948, "start": 1094.16, "end": 1098.84, "text": " the hard drive and you don't have to do some pooling at the file system level or some RAID system", "tokens": [51098, 264, 1152, 3332, 293, 291, 500, 380, 362, 281, 360, 512, 7005, 278, 412, 264, 3991, 1185, 1496, 420, 512, 14626, 2777, 1185, 51332], "temperature": 0.0, "avg_logprob": -0.1499680201212565, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.09245523065328598}, {"id": 220, "seek": 107948, "start": 1098.84, "end": 1105.52, "text": " basically you will just format each of your drives independently as a file system and each of them", "tokens": [51332, 1936, 291, 486, 445, 7877, 1184, 295, 428, 11754, 21761, 382, 257, 3991, 1185, 293, 1184, 295, 552, 51666], "temperature": 0.0, "avg_logprob": -0.1499680201212565, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.09245523065328598}, {"id": 221, "seek": 110552, "start": 1105.56, "end": 1110.08, "text": " has a directory, a mount point and garage will just use all of these mount points and like share", "tokens": [50366, 575, 257, 21120, 11, 257, 3746, 935, 293, 14400, 486, 445, 764, 439, 295, 613, 3746, 2793, 293, 411, 2073, 50592], "temperature": 0.0, "avg_logprob": -0.12852592651660627, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.002549575874581933}, {"id": 222, "seek": 110552, "start": 1110.08, "end": 1115.8799999999999, "text": " the data between the drives. This is probably the model which allows for the best performance on", "tokens": [50592, 264, 1412, 1296, 264, 11754, 13, 639, 307, 1391, 264, 2316, 597, 4045, 337, 264, 1151, 3389, 322, 50882], "temperature": 0.0, "avg_logprob": -0.12852592651660627, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.002549575874581933}, {"id": 223, "seek": 110552, "start": 1115.8799999999999, "end": 1122.6399999999999, "text": " the server with multiple drives. We also added some features for S3 compatibility so we added", "tokens": [50882, 264, 7154, 365, 3866, 11754, 13, 492, 611, 3869, 512, 4122, 337, 318, 18, 34237, 370, 321, 3869, 51220], "temperature": 0.0, "avg_logprob": -0.12852592651660627, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.002549575874581933}, {"id": 224, "seek": 110552, "start": 1122.6399999999999, "end": 1129.36, "text": " support for basic lifecycle and lifecycle is a feature where it allows you to clean basically", "tokens": [51220, 1406, 337, 3875, 45722, 293, 45722, 307, 257, 4111, 689, 309, 4045, 291, 281, 2541, 1936, 51556], "temperature": 0.0, "avg_logprob": -0.12852592651660627, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.002549575874581933}, {"id": 225, "seek": 110552, "start": 1129.36, "end": 1134.16, "text": " some stuff which is going on in the bucket and so for instance in S3 you can start uploading", "tokens": [51556, 512, 1507, 597, 307, 516, 322, 294, 264, 13058, 293, 370, 337, 5197, 294, 318, 18, 291, 393, 722, 27301, 51796], "temperature": 0.0, "avg_logprob": -0.12852592651660627, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.002549575874581933}, {"id": 226, "seek": 113416, "start": 1134.2, "end": 1139.3200000000002, "text": " an object using a multi-part upload so multi-part upload means you're initiating the upload at one", "tokens": [50366, 364, 2657, 1228, 257, 4825, 12, 6971, 6580, 370, 4825, 12, 6971, 6580, 1355, 291, 434, 6265, 990, 264, 6580, 412, 472, 50622], "temperature": 0.0, "avg_logprob": -0.15934649220219366, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0017812812002375722}, {"id": 227, "seek": 113416, "start": 1139.3200000000002, "end": 1144.6000000000001, "text": " point and then you're going to do individual requests to add pieces of the file and then", "tokens": [50622, 935, 293, 550, 291, 434, 516, 281, 360, 2609, 12475, 281, 909, 3755, 295, 264, 3991, 293, 550, 50886], "temperature": 0.0, "avg_logprob": -0.15934649220219366, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0017812812002375722}, {"id": 228, "seek": 113416, "start": 1144.6000000000001, "end": 1149.5600000000002, "text": " once you're finished you do a complete request and then the files get uploaded that gets stored", "tokens": [50886, 1564, 291, 434, 4335, 291, 360, 257, 3566, 5308, 293, 550, 264, 7098, 483, 17135, 300, 2170, 12187, 51134], "temperature": 0.0, "avg_logprob": -0.15934649220219366, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0017812812002375722}, {"id": 229, "seek": 113416, "start": 1149.5600000000002, "end": 1154.76, "text": " completely in the system and so it could happen that these multi-parts upload they get", "tokens": [51134, 2584, 294, 264, 1185, 293, 370, 309, 727, 1051, 300, 613, 4825, 12, 6971, 82, 6580, 436, 483, 51394], "temperature": 0.0, "avg_logprob": -0.15934649220219366, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0017812812002375722}, {"id": 230, "seek": 113416, "start": 1154.76, "end": 1159.92, "text": " aborted in the middle you never get to finish the the the requests and in this case there's", "tokens": [51394, 410, 14813, 294, 264, 2808, 291, 1128, 483, 281, 2413, 264, 264, 264, 12475, 293, 294, 341, 1389, 456, 311, 51652], "temperature": 0.0, "avg_logprob": -0.15934649220219366, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0017812812002375722}, {"id": 231, "seek": 115992, "start": 1159.96, "end": 1164.96, "text": " some data that's lying around in the cluster and so if you configure a lifecycle using this is a", "tokens": [50366, 512, 1412, 300, 311, 8493, 926, 294, 264, 13630, 293, 370, 498, 291, 22162, 257, 45722, 1228, 341, 307, 257, 50616], "temperature": 0.0, "avg_logprob": -0.1733086732056764, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.001896547619253397}, {"id": 232, "seek": 115992, "start": 1164.96, "end": 1169.88, "text": " very standard S3 API if you support if you configure a lifecycle in your brackets you can", "tokens": [50616, 588, 3832, 318, 18, 9362, 498, 291, 1406, 498, 291, 22162, 257, 45722, 294, 428, 26179, 291, 393, 50862], "temperature": 0.0, "avg_logprob": -0.1733086732056764, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.001896547619253397}, {"id": 233, "seek": 115992, "start": 1169.88, "end": 1174.04, "text": " basically get rid of all this tail data after say a delay of one day or something like that.", "tokens": [50862, 1936, 483, 3973, 295, 439, 341, 6838, 1412, 934, 584, 257, 8577, 295, 472, 786, 420, 746, 411, 300, 13, 51070], "temperature": 0.0, "avg_logprob": -0.1733086732056764, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.001896547619253397}, {"id": 234, "seek": 115992, "start": 1174.04, "end": 1182.1200000000001, "text": " And another thing we added for S3 compatibility is retries of multi-parts upload and this was", "tokens": [51070, 400, 1071, 551, 321, 3869, 337, 318, 18, 34237, 307, 1533, 2244, 295, 4825, 12, 6971, 82, 6580, 293, 341, 390, 51474], "temperature": 0.0, "avg_logprob": -0.1733086732056764, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.001896547619253397}, {"id": 235, "seek": 115992, "start": 1182.1200000000001, "end": 1187.64, "text": " actually because in S3 if you fail a part you can because maybe your network was broken you can", "tokens": [51474, 767, 570, 294, 318, 18, 498, 291, 3061, 257, 644, 291, 393, 570, 1310, 428, 3209, 390, 5463, 291, 393, 51750], "temperature": 0.0, "avg_logprob": -0.1733086732056764, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.001896547619253397}, {"id": 236, "seek": 118764, "start": 1187.68, "end": 1192.5200000000002, "text": " try again this part and you can still complete your multi-part upload and in the first versions of", "tokens": [50366, 853, 797, 341, 644, 293, 291, 393, 920, 3566, 428, 4825, 12, 6971, 6580, 293, 294, 264, 700, 9606, 295, 50608], "temperature": 0.0, "avg_logprob": -0.15946080391867118, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.00342840445227921}, {"id": 237, "seek": 118764, "start": 1192.5200000000002, "end": 1196.3200000000002, "text": " garage we did not have that and you would have to restart the upload from the beginning now you", "tokens": [50608, 14400, 321, 630, 406, 362, 300, 293, 291, 576, 362, 281, 21022, 264, 6580, 490, 264, 2863, 586, 291, 50798], "temperature": 0.0, "avg_logprob": -0.15946080391867118, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.00342840445227921}, {"id": 238, "seek": 118764, "start": 1196.3200000000002, "end": 1203.3200000000002, "text": " can resume only a single part. LMDB is now by default we're deprecating SLED and we have this", "tokens": [50798, 393, 15358, 787, 257, 2167, 644, 13, 46529, 27735, 307, 586, 538, 7576, 321, 434, 1367, 13867, 990, 318, 23459, 293, 321, 362, 341, 51148], "temperature": 0.0, "avg_logprob": -0.15946080391867118, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.00342840445227921}, {"id": 239, "seek": 118764, "start": 1203.3200000000002, "end": 1209.64, "text": " new layout computation algorithm which I will talk a little bit about. So as I said garage is meant", "tokens": [51148, 777, 13333, 24903, 9284, 597, 286, 486, 751, 257, 707, 857, 466, 13, 407, 382, 286, 848, 14400, 307, 4140, 51464], "temperature": 0.0, "avg_logprob": -0.15946080391867118, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.00342840445227921}, {"id": 240, "seek": 118764, "start": 1209.64, "end": 1214.92, "text": " to work on geo-distributed clusters so you have nodes which are in different geographical locations", "tokens": [51464, 281, 589, 322, 43198, 12, 42649, 2024, 4866, 23313, 370, 291, 362, 13891, 597, 366, 294, 819, 39872, 9253, 51728], "temperature": 0.0, "avg_logprob": -0.15946080391867118, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.00342840445227921}, {"id": 241, "seek": 121492, "start": 1214.92, "end": 1220.88, "text": " we call them zones in garage so here we have three different zones and the data is going to be", "tokens": [50364, 321, 818, 552, 16025, 294, 14400, 370, 510, 321, 362, 1045, 819, 16025, 293, 264, 1412, 307, 516, 281, 312, 50662], "temperature": 0.0, "avg_logprob": -0.11959895018105195, "compression_ratio": 1.9076305220883534, "no_speech_prob": 0.003942152485251427}, {"id": 242, "seek": 121492, "start": 1220.88, "end": 1226.1200000000001, "text": " replicated and each file has to be on different zones for optimal redundancy. So here is an", "tokens": [50662, 46365, 293, 1184, 3991, 575, 281, 312, 322, 819, 16025, 337, 16252, 27830, 6717, 13, 407, 510, 307, 364, 50924], "temperature": 0.0, "avg_logprob": -0.11959895018105195, "compression_ratio": 1.9076305220883534, "no_speech_prob": 0.003942152485251427}, {"id": 243, "seek": 121492, "start": 1226.1200000000001, "end": 1230.88, "text": " illustration if we have five zones for example the blue file will be in Belgium France and", "tokens": [50924, 22645, 498, 321, 362, 1732, 16025, 337, 1365, 264, 3344, 3991, 486, 312, 294, 28094, 6190, 293, 51162], "temperature": 0.0, "avg_logprob": -0.11959895018105195, "compression_ratio": 1.9076305220883534, "no_speech_prob": 0.003942152485251427}, {"id": 244, "seek": 121492, "start": 1230.88, "end": 1235.16, "text": " Switzerland so in three different places and the red file will also be in three different places", "tokens": [51162, 23312, 370, 294, 1045, 819, 3190, 293, 264, 2182, 3991, 486, 611, 312, 294, 1045, 819, 3190, 51376], "temperature": 0.0, "avg_logprob": -0.11959895018105195, "compression_ratio": 1.9076305220883534, "no_speech_prob": 0.003942152485251427}, {"id": 245, "seek": 121492, "start": 1235.16, "end": 1242.1200000000001, "text": " not necessarily the same here it's UK France and Germany. And the idea is that we do this using this", "tokens": [51376, 406, 4725, 264, 912, 510, 309, 311, 7051, 6190, 293, 7244, 13, 400, 264, 1558, 307, 300, 321, 360, 341, 1228, 341, 51724], "temperature": 0.0, "avg_logprob": -0.11959895018105195, "compression_ratio": 1.9076305220883534, "no_speech_prob": 0.003942152485251427}, {"id": 246, "seek": 124212, "start": 1242.1599999999999, "end": 1247.0, "text": " kind of pre-computed layout which is a table which will say okay the cluster the data in the", "tokens": [50366, 733, 295, 659, 12, 1112, 2582, 292, 13333, 597, 307, 257, 3199, 597, 486, 584, 1392, 264, 13630, 264, 1412, 294, 264, 50608], "temperature": 0.0, "avg_logprob": -0.11787265280018681, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.0018380822148174047}, {"id": 247, "seek": 124212, "start": 1247.0, "end": 1252.6, "text": " cluster is divided in 256 parts and each of these parts is assigned to a fixed set of three servers", "tokens": [50608, 13630, 307, 6666, 294, 38882, 3166, 293, 1184, 295, 613, 3166, 307, 13279, 281, 257, 6806, 992, 295, 1045, 15909, 50888], "temperature": 0.0, "avg_logprob": -0.11787265280018681, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.0018380822148174047}, {"id": 248, "seek": 124212, "start": 1252.6, "end": 1258.4799999999998, "text": " and for each part we have to decide so three servers which are in different places in the cluster", "tokens": [50888, 293, 337, 1184, 644, 321, 362, 281, 4536, 370, 1045, 15909, 597, 366, 294, 819, 3190, 294, 264, 13630, 51182], "temperature": 0.0, "avg_logprob": -0.11787265280018681, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.0018380822148174047}, {"id": 249, "seek": 124212, "start": 1258.4799999999998, "end": 1265.28, "text": " and we have to also balance the quantity of data that is going to go on each server. So basically", "tokens": [51182, 293, 321, 362, 281, 611, 4772, 264, 11275, 295, 1412, 300, 307, 516, 281, 352, 322, 1184, 7154, 13, 407, 1936, 51522], "temperature": 0.0, "avg_logprob": -0.11787265280018681, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.0018380822148174047}, {"id": 250, "seek": 126528, "start": 1266.16, "end": 1272.8, "text": " for 0.9 we added an algorithm which is able to do this in an optimal fashion so basically this", "tokens": [50408, 337, 1958, 13, 24, 321, 3869, 364, 9284, 597, 307, 1075, 281, 360, 341, 294, 364, 16252, 6700, 370, 1936, 341, 50740], "temperature": 0.0, "avg_logprob": -0.12617310418023003, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0042625064961612225}, {"id": 251, "seek": 126528, "start": 1272.8, "end": 1276.48, "text": " table is computed once when you set up the cluster or when you add some new nodes and then it's", "tokens": [50740, 3199, 307, 40610, 1564, 562, 291, 992, 493, 264, 13630, 420, 562, 291, 909, 512, 777, 13891, 293, 550, 309, 311, 50924], "temperature": 0.0, "avg_logprob": -0.12617310418023003, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0042625064961612225}, {"id": 252, "seek": 126528, "start": 1276.48, "end": 1280.56, "text": " propagated to everybody and everybody then knows this table and knows where to look for the data.", "tokens": [50924, 12425, 770, 281, 2201, 293, 2201, 550, 3255, 341, 3199, 293, 3255, 689, 281, 574, 337, 264, 1412, 13, 51128], "temperature": 0.0, "avg_logprob": -0.12617310418023003, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0042625064961612225}, {"id": 253, "seek": 126528, "start": 1282.32, "end": 1286.48, "text": " We actually published a paper if you're interested in the details of the algorithms that we use.", "tokens": [51216, 492, 767, 6572, 257, 3035, 498, 291, 434, 3102, 294, 264, 4365, 295, 264, 14642, 300, 321, 764, 13, 51424], "temperature": 0.0, "avg_logprob": -0.12617310418023003, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0042625064961612225}, {"id": 254, "seek": 128648, "start": 1286.48, "end": 1297.04, "text": " Okay so that was 0.9 and then we went on and worked on 0.10 and 0.10 is actually a beta version and I", "tokens": [50364, 1033, 370, 300, 390, 1958, 13, 24, 293, 550, 321, 1437, 322, 293, 2732, 322, 1958, 13, 3279, 293, 1958, 13, 3279, 307, 767, 257, 9861, 3037, 293, 286, 50892], "temperature": 0.0, "avg_logprob": -0.12623121450235555, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008297499269247055}, {"id": 255, "seek": 128648, "start": 1297.04, "end": 1302.72, "text": " think we will not have a stable 0.10 because it's not worth it to like update to 0.10 and then update", "tokens": [50892, 519, 321, 486, 406, 362, 257, 8351, 1958, 13, 3279, 570, 309, 311, 406, 3163, 309, 281, 411, 5623, 281, 1958, 13, 3279, 293, 550, 5623, 51176], "temperature": 0.0, "avg_logprob": -0.12623121450235555, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008297499269247055}, {"id": 256, "seek": 128648, "start": 1302.72, "end": 1308.24, "text": " again to 1.0 when it's going to be out so I think we will just leave the 0.10 at beta and do the", "tokens": [51176, 797, 281, 502, 13, 15, 562, 309, 311, 516, 281, 312, 484, 370, 286, 519, 321, 486, 445, 1856, 264, 1958, 13, 3279, 412, 9861, 293, 360, 264, 51452], "temperature": 0.0, "avg_logprob": -0.12623121450235555, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008297499269247055}, {"id": 257, "seek": 130824, "start": 1308.8, "end": 1318.0, "text": " 0.1.0 in May but so I'll just talk a little bit about the 0.10 beta. It's mostly focused on fixing", "tokens": [50392, 1958, 13, 16, 13, 15, 294, 1891, 457, 370, 286, 603, 445, 751, 257, 707, 857, 466, 264, 1958, 13, 3279, 9861, 13, 467, 311, 5240, 5178, 322, 19442, 50852], "temperature": 0.0, "avg_logprob": -0.11726543307304382, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.011864587664604187}, {"id": 258, "seek": 130824, "start": 1318.0, "end": 1322.72, "text": " some consistency issues that would happen like when you were adding some servers in the system or", "tokens": [50852, 512, 14416, 2663, 300, 576, 1051, 411, 562, 291, 645, 5127, 512, 15909, 294, 264, 1185, 420, 51088], "temperature": 0.0, "avg_logprob": -0.11726543307304382, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.011864587664604187}, {"id": 259, "seek": 130824, "start": 1322.72, "end": 1330.24, "text": " removing some servers and so I will enter into a bit of distributed system theory to try to explain", "tokens": [51088, 12720, 512, 15909, 293, 370, 286, 486, 3242, 666, 257, 857, 295, 12631, 1185, 5261, 281, 853, 281, 2903, 51464], "temperature": 0.0, "avg_logprob": -0.11726543307304382, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.011864587664604187}, {"id": 260, "seek": 130824, "start": 1330.96, "end": 1336.8, "text": " why exactly it's an issue and what is the solution that we made. So since I've said that", "tokens": [51500, 983, 2293, 309, 311, 364, 2734, 293, 437, 307, 264, 3827, 300, 321, 1027, 13, 407, 1670, 286, 600, 848, 300, 51792], "temperature": 0.0, "avg_logprob": -0.11726543307304382, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.011864587664604187}, {"id": 261, "seek": 133680, "start": 1336.8, "end": 1343.76, "text": " garage is not based on consensus it means that we have to work with inconsistent primitives", "tokens": [50364, 14400, 307, 406, 2361, 322, 19115, 309, 1355, 300, 321, 362, 281, 589, 365, 36891, 2886, 38970, 50712], "temperature": 0.0, "avg_logprob": -0.2436701237470254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013879927573725581}, {"id": 262, "seek": 133680, "start": 1343.76, "end": 1350.3999999999999, "text": " so this means we have to work with conflict-free, replicated data types, CRDTs and so these are", "tokens": [50712, 370, 341, 1355, 321, 362, 281, 589, 365, 6596, 12, 10792, 11, 46365, 1412, 3467, 11, 14123, 35, 33424, 293, 370, 613, 366, 51044], "temperature": 0.0, "avg_logprob": -0.2436701237470254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013879927573725581}, {"id": 263, "seek": 133680, "start": 1350.3999999999999, "end": 1356.72, "text": " not transactional, they are pretty much very very weakly consistent, very freeform to use", "tokens": [51044, 406, 46688, 1966, 11, 436, 366, 1238, 709, 588, 588, 5336, 356, 8398, 11, 588, 1737, 837, 281, 764, 51360], "temperature": 0.0, "avg_logprob": -0.2436701237470254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013879927573725581}, {"id": 264, "seek": 133680, "start": 1356.72, "end": 1362.24, "text": " and there's this last-writer wind register which is pretty much the fundamental building block of", "tokens": [51360, 293, 456, 311, 341, 1036, 12, 23681, 2468, 7280, 597, 307, 1238, 709, 264, 8088, 2390, 3461, 295, 51636], "temperature": 0.0, "avg_logprob": -0.2436701237470254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013879927573725581}, {"id": 265, "seek": 136224, "start": 1362.24, "end": 1369.68, "text": " garage and so CRDTs alone are not enough to insert consistency so what we add is some read after", "tokens": [50364, 14400, 293, 370, 14123, 35, 33424, 3312, 366, 406, 1547, 281, 8969, 14416, 370, 437, 321, 909, 307, 512, 1401, 934, 50736], "temperature": 0.0, "avg_logprob": -0.12787716285042142, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.001524290768429637}, {"id": 266, "seek": 136224, "start": 1369.68, "end": 1375.04, "text": " write guarantee which is implemented using quorums and I will try to explain, I hope you will", "tokens": [50736, 2464, 10815, 597, 307, 12270, 1228, 421, 284, 8099, 293, 286, 486, 853, 281, 2903, 11, 286, 1454, 291, 486, 51004], "temperature": 0.0, "avg_logprob": -0.12787716285042142, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.001524290768429637}, {"id": 267, "seek": 136224, "start": 1375.04, "end": 1380.96, "text": " understand how it works, I think it's not so complicated but it's a bit theoretical so yeah,", "tokens": [51004, 1223, 577, 309, 1985, 11, 286, 519, 309, 311, 406, 370, 6179, 457, 309, 311, 257, 857, 20864, 370, 1338, 11, 51300], "temperature": 0.0, "avg_logprob": -0.12787716285042142, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.001524290768429637}, {"id": 268, "seek": 136224, "start": 1380.96, "end": 1388.96, "text": " hold on. So read after write means if a client one is doing an operation right and the system", "tokens": [51300, 1797, 322, 13, 407, 1401, 934, 2464, 1355, 498, 257, 6423, 472, 307, 884, 364, 6916, 558, 293, 264, 1185, 51700], "temperature": 0.0, "avg_logprob": -0.12787716285042142, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.001524290768429637}, {"id": 269, "seek": 138896, "start": 1388.96, "end": 1394.32, "text": " returns to the client okay your write is saved in the system and then another client is sending a", "tokens": [50364, 11247, 281, 264, 6423, 1392, 428, 2464, 307, 6624, 294, 264, 1185, 293, 550, 1071, 6423, 307, 7750, 257, 50632], "temperature": 0.0, "avg_logprob": -0.08994836896379417, "compression_ratio": 1.9591836734693877, "no_speech_prob": 0.008056995458900928}, {"id": 270, "seek": 138896, "start": 1394.32, "end": 1400.08, "text": " read for this data after the write is returned okay then the client two will read a value which is", "tokens": [50632, 1401, 337, 341, 1412, 934, 264, 2464, 307, 8752, 1392, 550, 264, 6423, 732, 486, 1401, 257, 2158, 597, 307, 50920], "temperature": 0.0, "avg_logprob": -0.08994836896379417, "compression_ratio": 1.9591836734693877, "no_speech_prob": 0.008056995458900928}, {"id": 271, "seek": 138896, "start": 1400.08, "end": 1407.04, "text": " at least the value x that was written or a newer value this is what this means and so in practice", "tokens": [50920, 412, 1935, 264, 2158, 2031, 300, 390, 3720, 420, 257, 17628, 2158, 341, 307, 437, 341, 1355, 293, 370, 294, 3124, 51268], "temperature": 0.0, "avg_logprob": -0.08994836896379417, "compression_ratio": 1.9591836734693877, "no_speech_prob": 0.008056995458900928}, {"id": 272, "seek": 138896, "start": 1407.04, "end": 1411.04, "text": " it means that the system is basically evolving between these states so for instance we have", "tokens": [51268, 309, 1355, 300, 264, 1185, 307, 1936, 21085, 1296, 613, 4368, 370, 337, 5197, 321, 362, 51468], "temperature": 0.0, "avg_logprob": -0.08994836896379417, "compression_ratio": 1.9591836734693877, "no_speech_prob": 0.008056995458900928}, {"id": 273, "seek": 138896, "start": 1411.04, "end": 1415.68, "text": " the state here where the system is not storing anything and then we can store some value a or", "tokens": [51468, 264, 1785, 510, 689, 264, 1185, 307, 406, 26085, 1340, 293, 550, 321, 393, 3531, 512, 2158, 257, 420, 51700], "temperature": 0.0, "avg_logprob": -0.08994836896379417, "compression_ratio": 1.9591836734693877, "no_speech_prob": 0.008056995458900928}, {"id": 274, "seek": 141568, "start": 1415.68, "end": 1421.04, "text": " we can store some value b and if this is like a basic set if you have stored a on one node and b", "tokens": [50364, 321, 393, 3531, 512, 2158, 272, 293, 498, 341, 307, 411, 257, 3875, 992, 498, 291, 362, 12187, 257, 322, 472, 9984, 293, 272, 50632], "temperature": 0.0, "avg_logprob": -0.10908434867858886, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0019256401574239135}, {"id": 275, "seek": 141568, "start": 1421.04, "end": 1427.3600000000001, "text": " on another node then when the two nodes like merge together they will have stored a and b okay but", "tokens": [50632, 322, 1071, 9984, 550, 562, 264, 732, 13891, 411, 22183, 1214, 436, 486, 362, 12187, 257, 293, 272, 1392, 457, 50948], "temperature": 0.0, "avg_logprob": -0.10908434867858886, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0019256401574239135}, {"id": 276, "seek": 141568, "start": 1427.3600000000001, "end": 1433.1200000000001, "text": " let's do an example here for the writes so these are the three storage nodes and we're supposing that", "tokens": [50948, 718, 311, 360, 364, 1365, 510, 337, 264, 13657, 370, 613, 366, 264, 1045, 6725, 13891, 293, 321, 434, 1003, 6110, 300, 51236], "temperature": 0.0, "avg_logprob": -0.10908434867858886, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0019256401574239135}, {"id": 277, "seek": 141568, "start": 1433.1200000000001, "end": 1438.88, "text": " a node, a client is sending a write operation for value a so the value a is going to be sent to the", "tokens": [51236, 257, 9984, 11, 257, 6423, 307, 7750, 257, 2464, 6916, 337, 2158, 257, 370, 264, 2158, 257, 307, 516, 281, 312, 2279, 281, 264, 51524], "temperature": 0.0, "avg_logprob": -0.10908434867858886, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.0019256401574239135}, {"id": 278, "seek": 143888, "start": 1438.88, "end": 1445.68, "text": " network to these three nodes and at some point like maybe the purple node is going to receive the", "tokens": [50364, 3209, 281, 613, 1045, 13891, 293, 412, 512, 935, 411, 1310, 264, 9656, 9984, 307, 516, 281, 4774, 264, 50704], "temperature": 0.0, "avg_logprob": -0.04904827065424088, "compression_ratio": 2.1538461538461537, "no_speech_prob": 0.04465353488922119}, {"id": 279, "seek": 143888, "start": 1445.68, "end": 1450.72, "text": " value a so it's going to move from not knowing anything to knowing the value a then the green", "tokens": [50704, 2158, 257, 370, 309, 311, 516, 281, 1286, 490, 406, 5276, 1340, 281, 5276, 264, 2158, 257, 550, 264, 3092, 50956], "temperature": 0.0, "avg_logprob": -0.04904827065424088, "compression_ratio": 2.1538461538461537, "no_speech_prob": 0.04465353488922119}, {"id": 280, "seek": 143888, "start": 1450.72, "end": 1455.5200000000002, "text": " node is also going to move from not knowing anything to knowing a when it receives the messages", "tokens": [50956, 9984, 307, 611, 516, 281, 1286, 490, 406, 5276, 1340, 281, 5276, 257, 562, 309, 20717, 264, 7897, 51196], "temperature": 0.0, "avg_logprob": -0.04904827065424088, "compression_ratio": 2.1538461538461537, "no_speech_prob": 0.04465353488922119}, {"id": 281, "seek": 143888, "start": 1455.5200000000002, "end": 1460.0, "text": " and so those two nodes are going to return to the client who did the operation okay I've stored", "tokens": [51196, 293, 370, 729, 732, 13891, 366, 516, 281, 2736, 281, 264, 6423, 567, 630, 264, 6916, 1392, 286, 600, 12187, 51420], "temperature": 0.0, "avg_logprob": -0.04904827065424088, "compression_ratio": 2.1538461538461537, "no_speech_prob": 0.04465353488922119}, {"id": 282, "seek": 143888, "start": 1460.0, "end": 1465.5200000000002, "text": " the value a so at this point the client says so I've received two responses this is two over", "tokens": [51420, 264, 2158, 257, 370, 412, 341, 935, 264, 6423, 1619, 370, 286, 600, 4613, 732, 13019, 341, 307, 732, 670, 51696], "temperature": 0.0, "avg_logprob": -0.04904827065424088, "compression_ratio": 2.1538461538461537, "no_speech_prob": 0.04465353488922119}, {"id": 283, "seek": 146552, "start": 1465.52, "end": 1469.84, "text": " three so it's what we call a quorum and at that point the client says okay the data is stored in", "tokens": [50364, 1045, 370, 309, 311, 437, 321, 818, 257, 421, 36543, 293, 412, 300, 935, 264, 6423, 1619, 1392, 264, 1412, 307, 12187, 294, 50580], "temperature": 0.0, "avg_logprob": -0.06356192847429695, "compression_ratio": 2.049382716049383, "no_speech_prob": 0.0026301059406250715}, {"id": 284, "seek": 146552, "start": 1469.84, "end": 1475.76, "text": " the system even if the third node has not received it yet and so this is the point where we can start", "tokens": [50580, 264, 1185, 754, 498, 264, 2636, 9984, 575, 406, 4613, 309, 1939, 293, 370, 341, 307, 264, 935, 689, 321, 393, 722, 50876], "temperature": 0.0, "avg_logprob": -0.06356192847429695, "compression_ratio": 2.049382716049383, "no_speech_prob": 0.0026301059406250715}, {"id": 285, "seek": 146552, "start": 1475.76, "end": 1483.52, "text": " a read request and so the read will basically is the client will ask all of the three nodes to return", "tokens": [50876, 257, 1401, 5308, 293, 370, 264, 1401, 486, 1936, 307, 264, 6423, 486, 1029, 439, 295, 264, 1045, 13891, 281, 2736, 51264], "temperature": 0.0, "avg_logprob": -0.06356192847429695, "compression_ratio": 2.049382716049383, "no_speech_prob": 0.0026301059406250715}, {"id": 286, "seek": 146552, "start": 1483.52, "end": 1488.4, "text": " the value that they have stored and maybe the first node that will return its value is the red node", "tokens": [51264, 264, 2158, 300, 436, 362, 12187, 293, 1310, 264, 700, 9984, 300, 486, 2736, 1080, 2158, 307, 264, 2182, 9984, 51508], "temperature": 0.0, "avg_logprob": -0.06356192847429695, "compression_ratio": 2.049382716049383, "no_speech_prob": 0.0026301059406250715}, {"id": 287, "seek": 146552, "start": 1488.4, "end": 1493.6, "text": " and the red node has stored nothing so the read will first receive a value of nothing but then it", "tokens": [51508, 293, 264, 2182, 9984, 575, 12187, 1825, 370, 264, 1401, 486, 700, 4774, 257, 2158, 295, 1825, 457, 550, 309, 51768], "temperature": 0.0, "avg_logprob": -0.06356192847429695, "compression_ratio": 2.049382716049383, "no_speech_prob": 0.0026301059406250715}, {"id": 288, "seek": 149360, "start": 1493.6, "end": 1498.32, "text": " will wait for another response and the other response will necessarily come from one of these", "tokens": [50364, 486, 1699, 337, 1071, 4134, 293, 264, 661, 4134, 486, 4725, 808, 490, 472, 295, 613, 50600], "temperature": 0.0, "avg_logprob": -0.06193964367821103, "compression_ratio": 1.859375, "no_speech_prob": 0.00017951903282664716}, {"id": 289, "seek": 149360, "start": 1498.32, "end": 1503.84, "text": " two nodes and so it will necessarily read the value that was written and so it will just merge these", "tokens": [50600, 732, 13891, 293, 370, 309, 486, 4725, 1401, 264, 2158, 300, 390, 3720, 293, 370, 309, 486, 445, 22183, 613, 50876], "temperature": 0.0, "avg_logprob": -0.06193964367821103, "compression_ratio": 1.859375, "no_speech_prob": 0.00017951903282664716}, {"id": 290, "seek": 149360, "start": 1503.84, "end": 1508.8799999999999, "text": " two so this is why we use CRDTs to do this merge operation and consistency is guaranteed", "tokens": [50876, 732, 370, 341, 307, 983, 321, 764, 14123, 35, 33424, 281, 360, 341, 22183, 6916, 293, 14416, 307, 18031, 51128], "temperature": 0.0, "avg_logprob": -0.06193964367821103, "compression_ratio": 1.859375, "no_speech_prob": 0.00017951903282664716}, {"id": 291, "seek": 149360, "start": 1509.84, "end": 1513.84, "text": " and maybe at some later point through some synchronization mechanism the red node will catch", "tokens": [51176, 293, 1310, 412, 512, 1780, 935, 807, 512, 19331, 2144, 7513, 264, 2182, 9984, 486, 3745, 51376], "temperature": 0.0, "avg_logprob": -0.06193964367821103, "compression_ratio": 1.859375, "no_speech_prob": 0.00017951903282664716}, {"id": 292, "seek": 149360, "start": 1513.84, "end": 1522.32, "text": " up and also receive the value so we have this in algorithmic form but okay and so the issue we have", "tokens": [51376, 493, 293, 611, 4774, 264, 2158, 370, 321, 362, 341, 294, 9284, 299, 1254, 457, 1392, 293, 370, 264, 2734, 321, 362, 51800], "temperature": 0.0, "avg_logprob": -0.06193964367821103, "compression_ratio": 1.859375, "no_speech_prob": 0.00017951903282664716}, {"id": 293, "seek": 152232, "start": 1522.32, "end": 1526.8799999999999, "text": " with this is that we're relying very strongly on these quorum properties so if we have three copies", "tokens": [50364, 365, 341, 307, 300, 321, 434, 24140, 588, 10613, 322, 613, 421, 36543, 7221, 370, 498, 321, 362, 1045, 14341, 50592], "temperature": 0.0, "avg_logprob": -0.053127509503325156, "compression_ratio": 2.0120481927710845, "no_speech_prob": 0.005908213090151548}, {"id": 294, "seek": 152232, "start": 1526.8799999999999, "end": 1532.8, "text": " of data a quorum is at least two nodes of the three but what happens when you remove some nodes and", "tokens": [50592, 295, 1412, 257, 421, 36543, 307, 412, 1935, 732, 13891, 295, 264, 1045, 457, 437, 2314, 562, 291, 4159, 512, 13891, 293, 50888], "temperature": 0.0, "avg_logprob": -0.053127509503325156, "compression_ratio": 2.0120481927710845, "no_speech_prob": 0.005908213090151548}, {"id": 295, "seek": 152232, "start": 1532.8, "end": 1538.08, "text": " add some other nodes in the intersystem so we will have some some data which was stored maybe on the", "tokens": [50888, 909, 512, 661, 13891, 294, 264, 728, 28215, 370, 321, 486, 362, 512, 512, 1412, 597, 390, 12187, 1310, 322, 264, 51152], "temperature": 0.0, "avg_logprob": -0.053127509503325156, "compression_ratio": 2.0120481927710845, "no_speech_prob": 0.005908213090151548}, {"id": 296, "seek": 152232, "start": 1538.08, "end": 1543.28, "text": " nodes in red here and in the new system the data is being moved and it should be stored on the green", "tokens": [51152, 13891, 294, 2182, 510, 293, 294, 264, 777, 1185, 264, 1412, 307, 885, 4259, 293, 309, 820, 312, 12187, 322, 264, 3092, 51412], "temperature": 0.0, "avg_logprob": -0.053127509503325156, "compression_ratio": 2.0120481927710845, "no_speech_prob": 0.005908213090151548}, {"id": 297, "seek": 152232, "start": 1543.28, "end": 1548.24, "text": " nodes and so now if you do some quorum some right quorum on the red nodes and some some read quorum", "tokens": [51412, 13891, 293, 370, 586, 498, 291, 360, 512, 421, 36543, 512, 558, 421, 36543, 322, 264, 2182, 13891, 293, 512, 512, 1401, 421, 36543, 51660], "temperature": 0.0, "avg_logprob": -0.053127509503325156, "compression_ratio": 2.0120481927710845, "no_speech_prob": 0.005908213090151548}, {"id": 298, "seek": 154824, "start": 1548.24, "end": 1552.4, "text": " on the green nodes there is not necessarily an intersection of one node that has seen the read", "tokens": [50364, 322, 264, 3092, 13891, 456, 307, 406, 4725, 364, 15236, 295, 472, 9984, 300, 575, 1612, 264, 1401, 50572], "temperature": 0.0, "avg_logprob": -0.05770515238197105, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.0004044435336254537}, {"id": 299, "seek": 154824, "start": 1552.4, "end": 1558.72, "text": " and the right and basically the consistency is broken so the question is how do we coordinate", "tokens": [50572, 293, 264, 558, 293, 1936, 264, 14416, 307, 5463, 370, 264, 1168, 307, 577, 360, 321, 15670, 50888], "temperature": 0.0, "avg_logprob": -0.05770515238197105, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.0004044435336254537}, {"id": 300, "seek": 154824, "start": 1558.72, "end": 1562.4, "text": " in this situation and how do we ensure that even when the cluster is rebalancing data we", "tokens": [50888, 294, 341, 2590, 293, 577, 360, 321, 5586, 300, 754, 562, 264, 13630, 307, 319, 2645, 8779, 1412, 321, 51072], "temperature": 0.0, "avg_logprob": -0.05770515238197105, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.0004044435336254537}, {"id": 301, "seek": 154824, "start": 1562.4, "end": 1568.4, "text": " insert consistency and so the solution is a bit complex but basically we need to keep track of", "tokens": [51072, 8969, 14416, 293, 370, 264, 3827, 307, 257, 857, 3997, 457, 1936, 321, 643, 281, 1066, 2837, 295, 51372], "temperature": 0.0, "avg_logprob": -0.05770515238197105, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.0004044435336254537}, {"id": 302, "seek": 154824, "start": 1568.4, "end": 1573.1200000000001, "text": " what data is being transferred between the nodes we use multiple right quorum so we're going to", "tokens": [51372, 437, 1412, 307, 885, 15809, 1296, 264, 13891, 321, 764, 3866, 558, 421, 36543, 370, 321, 434, 516, 281, 51608], "temperature": 0.0, "avg_logprob": -0.05770515238197105, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.0004044435336254537}, {"id": 303, "seek": 157312, "start": 1573.12, "end": 1578.6399999999999, "text": " use quorums to write on the old set of nodes and the new set of nodes and switching reads to the", "tokens": [50364, 764, 421, 284, 8099, 281, 2464, 322, 264, 1331, 992, 295, 13891, 293, 264, 777, 992, 295, 13891, 293, 16493, 15700, 281, 264, 50640], "temperature": 0.0, "avg_logprob": -0.0911798086322722, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.0076849088072776794}, {"id": 304, "seek": 157312, "start": 1578.6399999999999, "end": 1583.9199999999998, "text": " new nodes only once the copy is finished so this is something we implemented for the in the context", "tokens": [50640, 777, 13891, 787, 1564, 264, 5055, 307, 4335, 370, 341, 307, 746, 321, 12270, 337, 264, 294, 264, 4319, 50904], "temperature": 0.0, "avg_logprob": -0.0911798086322722, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.0076849088072776794}, {"id": 305, "seek": 157312, "start": 1583.9199999999998, "end": 1588.8, "text": " of the ngi grants we did some testing using a tool which is called jepsen which is very good for", "tokens": [50904, 295, 264, 297, 7834, 16101, 321, 630, 512, 4997, 1228, 257, 2290, 597, 307, 1219, 361, 595, 6748, 597, 307, 588, 665, 337, 51148], "temperature": 0.0, "avg_logprob": -0.0911798086322722, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.0076849088072776794}, {"id": 306, "seek": 157312, "start": 1588.8, "end": 1594.7199999999998, "text": " validating these kind of things and so as you can see in garage 0.9 we had consistency issues in most", "tokens": [51148, 7363, 990, 613, 733, 295, 721, 293, 370, 382, 291, 393, 536, 294, 14400, 1958, 13, 24, 321, 632, 14416, 2663, 294, 881, 51444], "temperature": 0.0, "avg_logprob": -0.0911798086322722, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.0076849088072776794}, {"id": 307, "seek": 157312, "start": 1594.7199999999998, "end": 1600.3999999999999, "text": " of our runs and in point 10 we have all runs are green except one which failed but at least there was", "tokens": [51444, 295, 527, 6676, 293, 294, 935, 1266, 321, 362, 439, 6676, 366, 3092, 3993, 472, 597, 7612, 457, 412, 1935, 456, 390, 51728], "temperature": 0.0, "avg_logprob": -0.0911798086322722, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.0076849088072776794}, {"id": 308, "seek": 160040, "start": 1600.4, "end": 1605.2800000000002, "text": " no run where the data was plain wrong and it's actually this is very good result for us", "tokens": [50364, 572, 1190, 689, 264, 1412, 390, 11121, 2085, 293, 309, 311, 767, 341, 307, 588, 665, 1874, 337, 505, 50608], "temperature": 0.0, "avg_logprob": -0.11066591474745009, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0008166151237674057}, {"id": 309, "seek": 160040, "start": 1607.3600000000001, "end": 1613.44, "text": " okay so this was point 10 now we're at fosdem and we're going looking forward to making a version", "tokens": [50712, 1392, 370, 341, 390, 935, 1266, 586, 321, 434, 412, 283, 329, 10730, 293, 321, 434, 516, 1237, 2128, 281, 1455, 257, 3037, 51016], "temperature": 0.0, "avg_logprob": -0.11066591474745009, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0008166151237674057}, {"id": 310, "seek": 160040, "start": 1613.44, "end": 1620.0, "text": " one in april or may basically we're going to focus on security and stability there's a security", "tokens": [51016, 472, 294, 10992, 388, 420, 815, 1936, 321, 434, 516, 281, 1879, 322, 3825, 293, 11826, 456, 311, 257, 3825, 51344], "temperature": 0.0, "avg_logprob": -0.11066591474745009, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0008166151237674057}, {"id": 311, "seek": 160040, "start": 1620.0, "end": 1625.68, "text": " audit that is going to be done by radically open security miscellaneous features should be improved", "tokens": [51344, 17748, 300, 307, 516, 281, 312, 1096, 538, 35508, 1269, 3825, 3346, 4164, 15447, 4122, 820, 312, 9689, 51628], "temperature": 0.0, "avg_logprob": -0.11066591474745009, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0008166151237674057}, {"id": 312, "seek": 162568, "start": 1625.76, "end": 1630.24, "text": " this would be added and improvements may be in the user experience refactoring stuff", "tokens": [50368, 341, 576, 312, 3869, 293, 13797, 815, 312, 294, 264, 4195, 1752, 1895, 578, 3662, 1507, 50592], "temperature": 0.0, "avg_logprob": -0.08801005436823918, "compression_ratio": 1.66147859922179, "no_speech_prob": 0.002148427302017808}, {"id": 313, "seek": 162568, "start": 1631.28, "end": 1635.3600000000001, "text": " and that's it for 1.0 hopefully we'll have that out in april this year", "tokens": [50644, 293, 300, 311, 309, 337, 502, 13, 15, 4696, 321, 603, 362, 300, 484, 294, 10992, 388, 341, 1064, 50848], "temperature": 0.0, "avg_logprob": -0.08801005436823918, "compression_ratio": 1.66147859922179, "no_speech_prob": 0.002148427302017808}, {"id": 314, "seek": 162568, "start": 1636.96, "end": 1641.68, "text": " and beyond so we have this survey which is going on in the community right now", "tokens": [50928, 293, 4399, 370, 321, 362, 341, 8984, 597, 307, 516, 322, 294, 264, 1768, 558, 586, 51164], "temperature": 0.0, "avg_logprob": -0.08801005436823918, "compression_ratio": 1.66147859922179, "no_speech_prob": 0.002148427302017808}, {"id": 315, "seek": 162568, "start": 1641.68, "end": 1647.2, "text": " and so this is a list of the most requested features by the users of garage and actually", "tokens": [51164, 293, 370, 341, 307, 257, 1329, 295, 264, 881, 16436, 4122, 538, 264, 5022, 295, 14400, 293, 767, 51440], "temperature": 0.0, "avg_logprob": -0.08801005436823918, "compression_ratio": 1.66147859922179, "no_speech_prob": 0.002148427302017808}, {"id": 316, "seek": 162568, "start": 1647.2, "end": 1651.92, "text": " there's a lot of work to do so the first thing is a web interface for cluster management so I guess for", "tokens": [51440, 456, 311, 257, 688, 295, 589, 281, 360, 370, 264, 700, 551, 307, 257, 3670, 9226, 337, 13630, 4592, 370, 286, 2041, 337, 51676], "temperature": 0.0, "avg_logprob": -0.08801005436823918, "compression_ratio": 1.66147859922179, "no_speech_prob": 0.002148427302017808}, {"id": 317, "seek": 165192, "start": 1652.0, "end": 1656.0, "text": " like visualizing the state of the cluster and setting up a new bucket as new access", "tokens": [50368, 411, 5056, 3319, 264, 1785, 295, 264, 13630, 293, 3287, 493, 257, 777, 13058, 382, 777, 2105, 50568], "temperature": 0.0, "avg_logprob": -0.10441088043482957, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.003428627271205187}, {"id": 318, "seek": 165192, "start": 1656.88, "end": 1662.72, "text": " then it's s3 versioning which is so it's a feature of amazon s3 where you can have a you can save the", "tokens": [50612, 550, 309, 311, 262, 18, 3037, 278, 597, 307, 370, 309, 311, 257, 4111, 295, 47010, 262, 18, 689, 291, 393, 362, 257, 291, 393, 3155, 264, 50904], "temperature": 0.0, "avg_logprob": -0.10441088043482957, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.003428627271205187}, {"id": 319, "seek": 165192, "start": 1662.72, "end": 1668.0, "text": " historical data in the bucket and it's pretty good for like a backup system where you don't want to", "tokens": [50904, 8584, 1412, 294, 264, 13058, 293, 309, 311, 1238, 665, 337, 411, 257, 14807, 1185, 689, 291, 500, 380, 528, 281, 51168], "temperature": 0.0, "avg_logprob": -0.10441088043482957, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.003428627271205187}, {"id": 320, "seek": 165192, "start": 1668.0, "end": 1672.24, "text": " override data accidentally and this is a pretty crucial feature that we would need to have", "tokens": [51168, 42321, 1412, 15715, 293, 341, 307, 257, 1238, 11462, 4111, 300, 321, 576, 643, 281, 362, 51380], "temperature": 0.0, "avg_logprob": -0.10441088043482957, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.003428627271205187}, {"id": 321, "seek": 165192, "start": 1673.44, "end": 1680.72, "text": " ACLs are here monitoring and various other things and so this is the point where I'm calling for", "tokens": [51440, 43873, 82, 366, 510, 11028, 293, 3683, 661, 721, 293, 370, 341, 307, 264, 935, 689, 286, 478, 5141, 337, 51804], "temperature": 0.0, "avg_logprob": -0.10441088043482957, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.003428627271205187}, {"id": 322, "seek": 168072, "start": 1680.72, "end": 1686.0, "text": " help actually because there's a lot of work and I cannot do it myself so if anyone wants to step", "tokens": [50364, 854, 767, 570, 456, 311, 257, 688, 295, 589, 293, 286, 2644, 360, 309, 2059, 370, 498, 2878, 2738, 281, 1823, 50628], "temperature": 0.0, "avg_logprob": -0.10239485512792537, "compression_ratio": 1.8537549407114624, "no_speech_prob": 0.016903597861528397}, {"id": 323, "seek": 168072, "start": 1686.0, "end": 1692.8, "text": " in and help us with this please do so we can probably find some some more funding actually we", "tokens": [50628, 294, 293, 854, 505, 365, 341, 1767, 360, 370, 321, 393, 1391, 915, 512, 512, 544, 6137, 767, 321, 50968], "temperature": 0.0, "avg_logprob": -0.10239485512792537, "compression_ratio": 1.8537549407114624, "no_speech_prob": 0.016903597861528397}, {"id": 324, "seek": 168072, "start": 1692.8, "end": 1698.0, "text": " do have some funding in progress for someone who would like to do a phd on this system in", "tokens": [50968, 360, 362, 512, 6137, 294, 4205, 337, 1580, 567, 576, 411, 281, 360, 257, 903, 67, 322, 341, 1185, 294, 51228], "temperature": 0.0, "avg_logprob": -0.10239485512792537, "compression_ratio": 1.8537549407114624, "no_speech_prob": 0.016903597861528397}, {"id": 325, "seek": 168072, "start": 1698.56, "end": 1703.28, "text": " in relationship with the garage so if anyone wants to do a phd in France working on some stuff", "tokens": [51256, 294, 2480, 365, 264, 14400, 370, 498, 2878, 2738, 281, 360, 257, 903, 67, 294, 6190, 1364, 322, 512, 1507, 51492], "temperature": 0.0, "avg_logprob": -0.10239485512792537, "compression_ratio": 1.8537549407114624, "no_speech_prob": 0.016903597861528397}, {"id": 326, "seek": 168072, "start": 1703.28, "end": 1708.88, "text": " come to us we have this application going on and we also can probably ask some money to nlnet", "tokens": [51492, 808, 281, 505, 321, 362, 341, 3861, 516, 322, 293, 321, 611, 393, 1391, 1029, 512, 1460, 281, 297, 75, 7129, 51772], "temperature": 0.0, "avg_logprob": -0.10239485512792537, "compression_ratio": 1.8537549407114624, "no_speech_prob": 0.016903597861528397}, {"id": 327, "seek": 170888, "start": 1708.88, "end": 1714.3200000000002, "text": " which have funded us once and nji also once so we can probably get some more money if there's some", "tokens": [50364, 597, 362, 14385, 505, 1564, 293, 297, 4013, 611, 1564, 370, 321, 393, 1391, 483, 512, 544, 1460, 498, 456, 311, 512, 50636], "temperature": 0.0, "avg_logprob": -0.06626969973246256, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.003478243015706539}, {"id": 328, "seek": 170888, "start": 1714.3200000000002, "end": 1717.92, "text": " specific task that that is planned and we have somebody who is willing to do it", "tokens": [50636, 2685, 5633, 300, 300, 307, 8589, 293, 321, 362, 2618, 567, 307, 4950, 281, 360, 309, 50816], "temperature": 0.0, "avg_logprob": -0.06626969973246256, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.003478243015706539}, {"id": 329, "seek": 170888, "start": 1720.16, "end": 1726.8000000000002, "text": " okay and so I will just spend the last few minutes of this talk to explain a little bit about how you", "tokens": [50928, 1392, 293, 370, 286, 486, 445, 3496, 264, 1036, 1326, 2077, 295, 341, 751, 281, 2903, 257, 707, 857, 466, 577, 291, 51260], "temperature": 0.0, "avg_logprob": -0.06626969973246256, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.003478243015706539}, {"id": 330, "seek": 170888, "start": 1726.8000000000002, "end": 1732.48, "text": " can operate garage for people who have not run it or who are willing to scale their clusters to bigger", "tokens": [51260, 393, 9651, 14400, 337, 561, 567, 362, 406, 1190, 309, 420, 567, 366, 4950, 281, 4373, 641, 23313, 281, 3801, 51544], "temperature": 0.0, "avg_logprob": -0.06626969973246256, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.003478243015706539}, {"id": 331, "seek": 173248, "start": 1732.48, "end": 1739.28, "text": " systems so this is the basically what I would call the main screen of garage so when you interact", "tokens": [50364, 3652, 370, 341, 307, 264, 1936, 437, 286, 576, 818, 264, 2135, 2568, 295, 14400, 370, 562, 291, 4648, 50704], "temperature": 0.0, "avg_logprob": -0.09583602632795062, "compression_ratio": 1.8582089552238805, "no_speech_prob": 0.0035920317750424147}, {"id": 332, "seek": 173248, "start": 1739.28, "end": 1744.32, "text": " with the cluster just start always by doing garage status and it will tell you if everything is fine", "tokens": [50704, 365, 264, 13630, 445, 722, 1009, 538, 884, 14400, 6558, 293, 309, 486, 980, 291, 498, 1203, 307, 2489, 50956], "temperature": 0.0, "avg_logprob": -0.09583602632795062, "compression_ratio": 1.8582089552238805, "no_speech_prob": 0.0035920317750424147}, {"id": 333, "seek": 173248, "start": 1744.32, "end": 1748.56, "text": " so this is a five node cluster and everything seems to be fine but maybe you will have like failed", "tokens": [50956, 370, 341, 307, 257, 1732, 9984, 13630, 293, 1203, 2544, 281, 312, 2489, 457, 1310, 291, 486, 362, 411, 7612, 51168], "temperature": 0.0, "avg_logprob": -0.09583602632795062, "compression_ratio": 1.8582089552238805, "no_speech_prob": 0.0035920317750424147}, {"id": 334, "seek": 173248, "start": 1748.56, "end": 1752.32, "text": " nodes so this means that the connection could not be established and something is wrong and you should", "tokens": [51168, 13891, 370, 341, 1355, 300, 264, 4984, 727, 406, 312, 7545, 293, 746, 307, 2085, 293, 291, 820, 51356], "temperature": 0.0, "avg_logprob": -0.09583602632795062, "compression_ratio": 1.8582089552238805, "no_speech_prob": 0.0035920317750424147}, {"id": 335, "seek": 173248, "start": 1752.32, "end": 1762.16, "text": " fix it garage is made like a some cake of different pieces like this on top we have the s3 api we", "tokens": [51356, 3191, 309, 14400, 307, 1027, 411, 257, 512, 5908, 295, 819, 3755, 411, 341, 322, 1192, 321, 362, 264, 262, 18, 1882, 72, 321, 51848], "temperature": 0.0, "avg_logprob": -0.09583602632795062, "compression_ratio": 1.8582089552238805, "no_speech_prob": 0.0035920317750424147}, {"id": 336, "seek": 176216, "start": 1762.16, "end": 1767.28, "text": " also have some custom api which I'm not talking about in this talk and this is three api is actually", "tokens": [50364, 611, 362, 512, 2375, 1882, 72, 597, 286, 478, 406, 1417, 466, 294, 341, 751, 293, 341, 307, 1045, 1882, 72, 307, 767, 50620], "temperature": 0.0, "avg_logprob": -0.07125759558244185, "compression_ratio": 1.921875, "no_speech_prob": 0.0005883054691366851}, {"id": 337, "seek": 176216, "start": 1767.28, "end": 1772.72, "text": " implementing using some internal key value store for metadata and some block manager for the actual", "tokens": [50620, 18114, 1228, 512, 6920, 2141, 2158, 3531, 337, 26603, 293, 512, 3461, 6598, 337, 264, 3539, 50892], "temperature": 0.0, "avg_logprob": -0.07125759558244185, "compression_ratio": 1.921875, "no_speech_prob": 0.0005883054691366851}, {"id": 338, "seek": 176216, "start": 1772.72, "end": 1778.3200000000002, "text": " data of the big objects and then we have some systems here which maintain consistency in the system", "tokens": [50892, 1412, 295, 264, 955, 6565, 293, 550, 321, 362, 512, 3652, 510, 597, 6909, 14416, 294, 264, 1185, 51172], "temperature": 0.0, "avg_logprob": -0.07125759558244185, "compression_ratio": 1.921875, "no_speech_prob": 0.0005883054691366851}, {"id": 339, "seek": 176216, "start": 1779.28, "end": 1784.88, "text": " and so maybe to be a bit more specific about what's going on we have these three metadata", "tokens": [51220, 293, 370, 1310, 281, 312, 257, 857, 544, 2685, 466, 437, 311, 516, 322, 321, 362, 613, 1045, 26603, 51500], "temperature": 0.0, "avg_logprob": -0.07125759558244185, "compression_ratio": 1.921875, "no_speech_prob": 0.0005883054691366851}, {"id": 340, "seek": 176216, "start": 1784.88, "end": 1791.2, "text": " data tables here so the first one is like the list of objects in the system the second is the list of", "tokens": [51500, 1412, 8020, 510, 370, 264, 700, 472, 307, 411, 264, 1329, 295, 6565, 294, 264, 1185, 264, 1150, 307, 264, 1329, 295, 51816], "temperature": 0.0, "avg_logprob": -0.07125759558244185, "compression_ratio": 1.921875, "no_speech_prob": 0.0005883054691366851}, {"id": 341, "seek": 179120, "start": 1791.2, "end": 1795.68, "text": " versions of objects and so it's a bit different because an object can have a version which is", "tokens": [50364, 9606, 295, 6565, 293, 370, 309, 311, 257, 857, 819, 570, 364, 2657, 393, 362, 257, 3037, 597, 307, 50588], "temperature": 0.0, "avg_logprob": -0.05180912248549923, "compression_ratio": 2.101083032490975, "no_speech_prob": 0.0006877952255308628}, {"id": 342, "seek": 179120, "start": 1795.68, "end": 1799.8400000000001, "text": " currently in the cluster and a version which is currently being uploaded so for the same objects", "tokens": [50588, 4362, 294, 264, 13630, 293, 257, 3037, 597, 307, 4362, 885, 17135, 370, 337, 264, 912, 6565, 50796], "temperature": 0.0, "avg_logprob": -0.05180912248549923, "compression_ratio": 2.101083032490975, "no_speech_prob": 0.0006877952255308628}, {"id": 343, "seek": 179120, "start": 1799.8400000000001, "end": 1805.52, "text": " multiple versions can exist and then this version will also reference a bunch of data blocks so this", "tokens": [50796, 3866, 9606, 393, 2514, 293, 550, 341, 3037, 486, 611, 6408, 257, 3840, 295, 1412, 8474, 370, 341, 51080], "temperature": 0.0, "avg_logprob": -0.05180912248549923, "compression_ratio": 2.101083032490975, "no_speech_prob": 0.0006877952255308628}, {"id": 344, "seek": 179120, "start": 1805.52, "end": 1810.64, "text": " is the table which has the reference to actual data blocks and so all of these tables are sharded", "tokens": [51080, 307, 264, 3199, 597, 575, 264, 6408, 281, 3539, 1412, 8474, 293, 370, 439, 295, 613, 8020, 366, 402, 22803, 51336], "temperature": 0.0, "avg_logprob": -0.05180912248549923, "compression_ratio": 2.101083032490975, "no_speech_prob": 0.0006877952255308628}, {"id": 345, "seek": 179120, "start": 1810.64, "end": 1816.0, "text": " across the nodes and in particular for the block reference table if a node has the has the shard", "tokens": [51336, 2108, 264, 13891, 293, 294, 1729, 337, 264, 3461, 6408, 3199, 498, 257, 9984, 575, 264, 575, 264, 402, 515, 51604], "temperature": 0.0, "avg_logprob": -0.05180912248549923, "compression_ratio": 2.101083032490975, "no_speech_prob": 0.0006877952255308628}, {"id": 346, "seek": 179120, "start": 1816.0, "end": 1820.64, "text": " for some references it means it's also responsible for storing the blocks associated with these", "tokens": [51604, 337, 512, 15400, 309, 1355, 309, 311, 611, 6250, 337, 26085, 264, 8474, 6615, 365, 613, 51836], "temperature": 0.0, "avg_logprob": -0.05180912248549923, "compression_ratio": 2.101083032490975, "no_speech_prob": 0.0006877952255308628}, {"id": 347, "seek": 182064, "start": 1820.64, "end": 1826.16, "text": " references so basically from this metadata table we have a local counter for how many references", "tokens": [50364, 15400, 370, 1936, 490, 341, 26603, 3199, 321, 362, 257, 2654, 5682, 337, 577, 867, 15400, 50640], "temperature": 0.0, "avg_logprob": -0.14363018671671549, "compression_ratio": 1.9356435643564356, "no_speech_prob": 0.00042382354149594903}, {"id": 348, "seek": 182064, "start": 1826.16, "end": 1831.92, "text": " for each block and then we have this rescind queue and scheduler which is responsible for ensuring", "tokens": [50640, 337, 1184, 3461, 293, 550, 321, 362, 341, 9610, 471, 18639, 293, 12000, 260, 597, 307, 6250, 337, 16882, 50928], "temperature": 0.0, "avg_logprob": -0.14363018671671549, "compression_ratio": 1.9356435643564356, "no_speech_prob": 0.00042382354149594903}, {"id": 349, "seek": 182064, "start": 1831.92, "end": 1836.8000000000002, "text": " that the locally stored data blocks are actually matching the number of blocks which have a reference", "tokens": [50928, 300, 264, 16143, 12187, 1412, 8474, 366, 767, 14324, 264, 1230, 295, 8474, 597, 362, 257, 6408, 51172], "temperature": 0.0, "avg_logprob": -0.14363018671671549, "compression_ratio": 1.9356435643564356, "no_speech_prob": 0.00042382354149594903}, {"id": 350, "seek": 182064, "start": 1836.8000000000002, "end": 1843.6000000000001, "text": " in the in the store so yeah we have this block rescind for data blocks and this merkle merkle", "tokens": [51172, 294, 264, 294, 264, 3531, 370, 1338, 321, 362, 341, 3461, 9610, 471, 337, 1412, 8474, 293, 341, 3551, 14677, 3551, 14677, 51512], "temperature": 0.0, "avg_logprob": -0.14363018671671549, "compression_ratio": 1.9356435643564356, "no_speech_prob": 0.00042382354149594903}, {"id": 351, "seek": 184360, "start": 1843.6799999999998, "end": 1851.52, "text": " tree based system for the metadata and so if you do this garage stats command so there's not", "tokens": [50368, 4230, 2361, 1185, 337, 264, 26603, 293, 370, 498, 291, 360, 341, 14400, 18152, 5622, 370, 456, 311, 406, 50760], "temperature": 0.0, "avg_logprob": -0.10207628320764613, "compression_ratio": 1.9665271966527196, "no_speech_prob": 0.03731807321310043}, {"id": 352, "seek": 184360, "start": 1851.52, "end": 1855.6799999999998, "text": " status it stats never command you will get some information about the internals of what's going", "tokens": [50760, 6558, 309, 18152, 1128, 5622, 291, 486, 483, 512, 1589, 466, 264, 2154, 1124, 295, 437, 311, 516, 50968], "temperature": 0.0, "avg_logprob": -0.10207628320764613, "compression_ratio": 1.9665271966527196, "no_speech_prob": 0.03731807321310043}, {"id": 353, "seek": 184360, "start": 1855.6799999999998, "end": 1860.56, "text": " on so these are the metadata tables and you can see here objects version and block reference", "tokens": [50968, 322, 370, 613, 366, 264, 26603, 8020, 293, 291, 393, 536, 510, 6565, 3037, 293, 3461, 6408, 51212], "temperature": 0.0, "avg_logprob": -0.10207628320764613, "compression_ratio": 1.9665271966527196, "no_speech_prob": 0.03731807321310043}, {"id": 354, "seek": 184360, "start": 1861.1999999999998, "end": 1865.28, "text": " so these are the number of items in the table and there are also the number of items in the", "tokens": [51244, 370, 613, 366, 264, 1230, 295, 4754, 294, 264, 3199, 293, 456, 366, 611, 264, 1230, 295, 4754, 294, 264, 51448], "temperature": 0.0, "avg_logprob": -0.10207628320764613, "compression_ratio": 1.9665271966527196, "no_speech_prob": 0.03731807321310043}, {"id": 355, "seek": 184360, "start": 1865.28, "end": 1870.7199999999998, "text": " merkle tree which is always a bit bigger and then you have here the number of rc entries for the", "tokens": [51448, 3551, 14677, 4230, 597, 307, 1009, 257, 857, 3801, 293, 550, 291, 362, 510, 264, 1230, 295, 367, 66, 23041, 337, 264, 51720], "temperature": 0.0, "avg_logprob": -0.10207628320764613, "compression_ratio": 1.9665271966527196, "no_speech_prob": 0.03731807321310043}, {"id": 356, "seek": 187072, "start": 1870.72, "end": 1875.04, "text": " block table so the number of blocks which actually have a reference in the system so here we have", "tokens": [50364, 3461, 3199, 370, 264, 1230, 295, 8474, 597, 767, 362, 257, 6408, 294, 264, 1185, 370, 510, 321, 362, 50580], "temperature": 0.0, "avg_logprob": -0.07212873741432473, "compression_ratio": 1.9236947791164658, "no_speech_prob": 0.001032172585837543}, {"id": 357, "seek": 187072, "start": 1875.04, "end": 1882.24, "text": " 42,000 data blocks but we have actually 334,000 block references so this means that blocks are", "tokens": [50580, 14034, 11, 1360, 1412, 8474, 457, 321, 362, 767, 11816, 19, 11, 1360, 3461, 15400, 370, 341, 1355, 300, 8474, 366, 50940], "temperature": 0.0, "avg_logprob": -0.07212873741432473, "compression_ratio": 1.9236947791164658, "no_speech_prob": 0.001032172585837543}, {"id": 358, "seek": 187072, "start": 1882.24, "end": 1888.88, "text": " almost referenced by 10 different objects each on average and then we have some information on the", "tokens": [50940, 1920, 32734, 538, 1266, 819, 6565, 1184, 322, 4274, 293, 550, 321, 362, 512, 1589, 322, 264, 51272], "temperature": 0.0, "avg_logprob": -0.07212873741432473, "compression_ratio": 1.9236947791164658, "no_speech_prob": 0.001032172585837543}, {"id": 359, "seek": 187072, "start": 1889.52, "end": 1894.32, "text": " actual nodes so the partitions here means basically is how many of the lines in the tables are", "tokens": [51304, 3539, 13891, 370, 264, 644, 2451, 510, 1355, 1936, 307, 577, 867, 295, 264, 3876, 294, 264, 8020, 366, 51544], "temperature": 0.0, "avg_logprob": -0.07212873741432473, "compression_ratio": 1.9236947791164658, "no_speech_prob": 0.001032172585837543}, {"id": 360, "seek": 187072, "start": 1894.32, "end": 1898.64, "text": " affected to each of these nodes so if you have more more partitions you're going to use more", "tokens": [51544, 8028, 281, 1184, 295, 613, 13891, 370, 498, 291, 362, 544, 544, 644, 2451, 291, 434, 516, 281, 764, 544, 51760], "temperature": 0.0, "avg_logprob": -0.07212873741432473, "compression_ratio": 1.9236947791164658, "no_speech_prob": 0.001032172585837543}, {"id": 361, "seek": 189864, "start": 1898.64, "end": 1903.6000000000001, "text": " storage space basically on that node it's proportional and this is a metric which is", "tokens": [50364, 6725, 1901, 1936, 322, 300, 9984, 309, 311, 24969, 293, 341, 307, 257, 20678, 597, 307, 50612], "temperature": 0.0, "avg_logprob": -0.07231796660074373, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0017818943597376347}, {"id": 362, "seek": 189864, "start": 1903.6000000000001, "end": 1908.8000000000002, "text": " given by the node actually it's it's measuring on disk how much space is available it's not", "tokens": [50612, 2212, 538, 264, 9984, 767, 309, 311, 309, 311, 13389, 322, 12355, 577, 709, 1901, 307, 2435, 309, 311, 406, 50872], "temperature": 0.0, "avg_logprob": -0.07231796660074373, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0017818943597376347}, {"id": 363, "seek": 189864, "start": 1908.8000000000002, "end": 1913.3600000000001, "text": " the use space it's the available space for the data partition and the metadata which is not", "tokens": [50872, 264, 764, 1901, 309, 311, 264, 2435, 1901, 337, 264, 1412, 24808, 293, 264, 26603, 597, 307, 406, 51100], "temperature": 0.0, "avg_logprob": -0.07231796660074373, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0017818943597376347}, {"id": 364, "seek": 189864, "start": 1913.3600000000001, "end": 1919.76, "text": " necessarily on the same drive and so from all this information garage is able to basically tell you", "tokens": [51100, 4725, 322, 264, 912, 3332, 293, 370, 490, 439, 341, 1589, 14400, 307, 1075, 281, 1936, 980, 291, 51420], "temperature": 0.0, "avg_logprob": -0.07231796660074373, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0017818943597376347}, {"id": 365, "seek": 191976, "start": 1919.76, "end": 1929.2, "text": " how much data you can still store on the cluster so here for 600 gigabytes and if you go even further", "tokens": [50364, 577, 709, 1412, 291, 393, 920, 3531, 322, 264, 13630, 370, 510, 337, 11849, 42741, 293, 498, 291, 352, 754, 3052, 50836], "temperature": 0.0, "avg_logprob": -0.10193605650038946, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0049050175584852695}, {"id": 366, "seek": 191976, "start": 1929.2, "end": 1933.76, "text": " you can get this list of workers so workers are basically background tasks which are running in", "tokens": [50836, 291, 393, 483, 341, 1329, 295, 5600, 370, 5600, 366, 1936, 3678, 9608, 597, 366, 2614, 294, 51064], "temperature": 0.0, "avg_logprob": -0.10193605650038946, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0049050175584852695}, {"id": 367, "seek": 191976, "start": 1933.76, "end": 1938.72, "text": " garage all the time and so you have these tasks which are block readings so these are copying", "tokens": [51064, 14400, 439, 264, 565, 293, 370, 291, 362, 613, 9608, 597, 366, 3461, 27319, 370, 613, 366, 27976, 51312], "temperature": 0.0, "avg_logprob": -0.10193605650038946, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0049050175584852695}, {"id": 368, "seek": 191976, "start": 1938.72, "end": 1945.2, "text": " data blocks between nodes when they're missing and these are synchronization tasks for each of the", "tokens": [51312, 1412, 8474, 1296, 13891, 562, 436, 434, 5361, 293, 613, 366, 19331, 2144, 9608, 337, 1184, 295, 264, 51636], "temperature": 0.0, "avg_logprob": -0.10193605650038946, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0049050175584852695}, {"id": 369, "seek": 194520, "start": 1945.2, "end": 1953.52, "text": " metadata tables and you can change a bit the parameters of these tasks for so for instance for", "tokens": [50364, 26603, 8020, 293, 291, 393, 1319, 257, 857, 264, 9834, 295, 613, 9608, 337, 370, 337, 5197, 337, 50780], "temperature": 0.0, "avg_logprob": -0.1324398536992267, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.000646069529466331}, {"id": 370, "seek": 194520, "start": 1953.52, "end": 1958.48, "text": " the the block re-synchronization you have re-sync tranquility and re-sync worker count", "tokens": [50780, 264, 264, 3461, 319, 12, 82, 36420, 2144, 291, 362, 319, 12, 82, 34015, 17640, 1140, 293, 319, 12, 82, 34015, 11346, 1207, 51028], "temperature": 0.0, "avg_logprob": -0.1324398536992267, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.000646069529466331}, {"id": 371, "seek": 194520, "start": 1959.1200000000001, "end": 1964.56, "text": " and tranquility is a metric which can be increased to make the system go slower and use less i o if", "tokens": [51060, 293, 17640, 1140, 307, 257, 20678, 597, 393, 312, 6505, 281, 652, 264, 1185, 352, 14009, 293, 764, 1570, 741, 277, 498, 51332], "temperature": 0.0, "avg_logprob": -0.1324398536992267, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.000646069529466331}, {"id": 372, "seek": 194520, "start": 1964.56, "end": 1968.88, "text": " if it's serring you're saturating your i o you can increase the tranquility and if you want it to go", "tokens": [51332, 498, 309, 311, 816, 2937, 291, 434, 21160, 990, 428, 741, 277, 291, 393, 3488, 264, 17640, 1140, 293, 498, 291, 528, 309, 281, 352, 51548], "temperature": 0.0, "avg_logprob": -0.1324398536992267, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.000646069529466331}, {"id": 373, "seek": 194520, "start": 1968.88, "end": 1974.0, "text": " faster you can just put it to zero and then there's also the worker count so you can set it up to", "tokens": [51548, 4663, 291, 393, 445, 829, 309, 281, 4018, 293, 550, 456, 311, 611, 264, 11346, 1207, 370, 291, 393, 992, 309, 493, 281, 51804], "temperature": 0.0, "avg_logprob": -0.1324398536992267, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.000646069529466331}, {"id": 374, "seek": 197400, "start": 1974.0, "end": 1978.0, "text": " eight and then you have eight parallel threads which are sending and receiving data blocks in the", "tokens": [50364, 3180, 293, 550, 291, 362, 3180, 8952, 19314, 597, 366, 7750, 293, 10040, 1412, 8474, 294, 264, 50564], "temperature": 0.0, "avg_logprob": -0.07187972405944208, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.0017820369685068727}, {"id": 375, "seek": 197400, "start": 1978.0, "end": 1985.68, "text": " network there are some potential limitations if you're running extremely extremely big clusters", "tokens": [50564, 3209, 456, 366, 512, 3995, 15705, 498, 291, 434, 2614, 4664, 4664, 955, 23313, 50948], "temperature": 0.0, "avg_logprob": -0.07187972405944208, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.0017820369685068727}, {"id": 376, "seek": 197400, "start": 1986.88, "end": 1992.56, "text": " probably you cannot run with more than about 100 100 nodes i mean you can but then the the data will", "tokens": [51008, 1391, 291, 2644, 1190, 365, 544, 813, 466, 2319, 2319, 13891, 741, 914, 291, 393, 457, 550, 264, 264, 1412, 486, 51292], "temperature": 0.0, "avg_logprob": -0.07187972405944208, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.0017820369685068727}, {"id": 377, "seek": 197400, "start": 1992.56, "end": 1997.36, "text": " not be very well balanced between the nodes and this is because we're using only 256 partitions", "tokens": [51292, 406, 312, 588, 731, 13902, 1296, 264, 13891, 293, 341, 307, 570, 321, 434, 1228, 787, 38882, 644, 2451, 51532], "temperature": 0.0, "avg_logprob": -0.07187972405944208, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.0017820369685068727}, {"id": 378, "seek": 197400, "start": 1998.0, "end": 2001.12, "text": " we could probably compile a bigger version in garage but it's currently not the case", "tokens": [51564, 321, 727, 1391, 31413, 257, 3801, 3037, 294, 14400, 457, 309, 311, 4362, 406, 264, 1389, 51720], "temperature": 0.0, "avg_logprob": -0.07187972405944208, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.0017820369685068727}, {"id": 379, "seek": 200112, "start": 2001.1999999999998, "end": 2008.2399999999998, "text": " and on the metadata side if you have one big bucket which is containing all your objects well you", "tokens": [50368, 293, 322, 264, 26603, 1252, 498, 291, 362, 472, 955, 13058, 597, 307, 19273, 439, 428, 6565, 731, 291, 50720], "temperature": 0.0, "avg_logprob": -0.12782676003196022, "compression_ratio": 1.9708333333333334, "no_speech_prob": 0.0006878244457766414}, {"id": 380, "seek": 200112, "start": 2008.2399999999998, "end": 2013.28, "text": " will have a bottleneck also because the first table the object table is going to store the list of", "tokens": [50720, 486, 362, 257, 44641, 547, 611, 570, 264, 700, 3199, 264, 2657, 3199, 307, 516, 281, 3531, 264, 1329, 295, 50972], "temperature": 0.0, "avg_logprob": -0.12782676003196022, "compression_ratio": 1.9708333333333334, "no_speech_prob": 0.0006878244457766414}, {"id": 381, "seek": 200112, "start": 2013.28, "end": 2018.3999999999999, "text": " objects on only three of all of your cluster nodes so if you have lots of data split your data over", "tokens": [50972, 6565, 322, 787, 1045, 295, 439, 295, 428, 13630, 13891, 370, 498, 291, 362, 3195, 295, 1412, 7472, 428, 1412, 670, 51228], "temperature": 0.0, "avg_logprob": -0.12782676003196022, "compression_ratio": 1.9708333333333334, "no_speech_prob": 0.0006878244457766414}, {"id": 382, "seek": 200112, "start": 2018.3999999999999, "end": 2025.76, "text": " different buckets and also on the side on the side of the data blocks so the data is split into", "tokens": [51228, 819, 32191, 293, 611, 322, 264, 1252, 322, 264, 1252, 295, 264, 1412, 8474, 370, 264, 1412, 307, 7472, 666, 51596], "temperature": 0.0, "avg_logprob": -0.12782676003196022, "compression_ratio": 1.9708333333333334, "no_speech_prob": 0.0006878244457766414}, {"id": 383, "seek": 200112, "start": 2026.4799999999998, "end": 2029.76, "text": " so if you have a hundred megabytes file in your block size is one megabytes your", "tokens": [51632, 370, 498, 291, 362, 257, 3262, 10816, 24538, 3991, 294, 428, 3461, 2744, 307, 472, 10816, 24538, 428, 51796], "temperature": 0.0, "avg_logprob": -0.12782676003196022, "compression_ratio": 1.9708333333333334, "no_speech_prob": 0.0006878244457766414}, {"id": 384, "seek": 202976, "start": 2030.24, "end": 2034.72, "text": " your file is going to be split into a hundred different files so we will have a lot of small", "tokens": [50388, 428, 3991, 307, 516, 281, 312, 7472, 666, 257, 3262, 819, 7098, 370, 321, 486, 362, 257, 688, 295, 1359, 50612], "temperature": 0.0, "avg_logprob": -0.07304282327300136, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0002571019867900759}, {"id": 385, "seek": 202976, "start": 2034.72, "end": 2040.16, "text": " files on disk you can increase the block size to reduce the number of files and if you have more", "tokens": [50612, 7098, 322, 12355, 291, 393, 3488, 264, 3461, 2744, 281, 5407, 264, 1230, 295, 7098, 293, 498, 291, 362, 544, 50884], "temperature": 0.0, "avg_logprob": -0.07304282327300136, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0002571019867900759}, {"id": 386, "seek": 202976, "start": 2040.16, "end": 2044.8, "text": " files the processing of the queue can also be kind of slow and this is of course also", "tokens": [50884, 7098, 264, 9007, 295, 264, 18639, 393, 611, 312, 733, 295, 2964, 293, 341, 307, 295, 1164, 611, 51116], "temperature": 0.0, "avg_logprob": -0.07304282327300136, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0002571019867900759}, {"id": 387, "seek": 202976, "start": 2045.36, "end": 2050.96, "text": " dependent on your networking conditions and so just some advice for actual deployments", "tokens": [51144, 12334, 322, 428, 17985, 4487, 293, 370, 445, 512, 5192, 337, 3539, 7274, 1117, 51424], "temperature": 0.0, "avg_logprob": -0.07304282327300136, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0002571019867900759}, {"id": 388, "seek": 202976, "start": 2051.6, "end": 2056.32, "text": " for the metadata if you're going to do a very large cluster we recommend doing some mirroring", "tokens": [51456, 337, 264, 26603, 498, 291, 434, 516, 281, 360, 257, 588, 2416, 13630, 321, 2748, 884, 512, 8013, 278, 51692], "temperature": 0.0, "avg_logprob": -0.07304282327300136, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0002571019867900759}, {"id": 389, "seek": 205632, "start": 2056.32, "end": 2062.8, "text": " on two fast NVMe drives possibly ZFS is a good choice garage itself does not do check summing", "tokens": [50364, 322, 732, 2370, 46512, 12671, 11754, 6264, 1176, 29318, 307, 257, 665, 3922, 14400, 2564, 775, 406, 360, 1520, 2408, 2810, 50688], "temperature": 0.0, "avg_logprob": -0.09508961702870057, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0016472346615046263}, {"id": 390, "seek": 205632, "start": 2062.8, "end": 2067.36, "text": " on the metadata so it's good to have a file system that does it for you lmdb is the recommended", "tokens": [50688, 322, 264, 26603, 370, 309, 311, 665, 281, 362, 257, 3991, 1185, 300, 775, 309, 337, 291, 287, 76, 67, 65, 307, 264, 9628, 50916], "temperature": 0.0, "avg_logprob": -0.09508961702870057, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0016472346615046263}, {"id": 391, "seek": 205632, "start": 2067.36, "end": 2072.4, "text": " storage engine and for data block it's a bit different and we have other recommendations we", "tokens": [50916, 6725, 2848, 293, 337, 1412, 3461, 309, 311, 257, 857, 819, 293, 321, 362, 661, 10434, 321, 51168], "temperature": 0.0, "avg_logprob": -0.09508961702870057, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0016472346615046263}, {"id": 392, "seek": 205632, "start": 2072.4, "end": 2076.88, "text": " recommend using an XFS file system because we actually do some check summing for each blocks", "tokens": [51168, 2748, 1228, 364, 1783, 29318, 3991, 1185, 570, 321, 767, 360, 512, 1520, 2408, 2810, 337, 1184, 8474, 51392], "temperature": 0.0, "avg_logprob": -0.09508961702870057, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0016472346615046263}, {"id": 393, "seek": 205632, "start": 2076.88, "end": 2081.44, "text": " because we always compute hashes of the blocks in garage so you do not need to have a file system", "tokens": [51392, 570, 321, 1009, 14722, 575, 8076, 295, 264, 8474, 294, 14400, 370, 291, 360, 406, 643, 281, 362, 257, 3991, 1185, 51620], "temperature": 0.0, "avg_logprob": -0.09508961702870057, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.0016472346615046263}, {"id": 394, "seek": 208144, "start": 2081.44, "end": 2086.64, "text": " which is doing this this check summing again it would be wasteful so just format your partitions", "tokens": [50364, 597, 307, 884, 341, 341, 1520, 2408, 2810, 797, 309, 576, 312, 5964, 906, 370, 445, 7877, 428, 644, 2451, 50624], "temperature": 0.0, "avg_logprob": -0.04668061667626057, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.0008557698456570506}, {"id": 395, "seek": 208144, "start": 2086.64, "end": 2092.32, "text": " as XFS which is one of the fastest file systems and store your data directly on this if you have", "tokens": [50624, 382, 1783, 29318, 597, 307, 472, 295, 264, 14573, 3991, 3652, 293, 3531, 428, 1412, 3838, 322, 341, 498, 291, 362, 50908], "temperature": 0.0, "avg_logprob": -0.04668061667626057, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.0008557698456570506}, {"id": 396, "seek": 208144, "start": 2092.32, "end": 2096.56, "text": " a good network and some nodes with a lot of RAM you can increase the block size to 10 megabytes", "tokens": [50908, 257, 665, 3209, 293, 512, 13891, 365, 257, 688, 295, 14561, 291, 393, 3488, 264, 3461, 2744, 281, 1266, 10816, 24538, 51120], "temperature": 0.0, "avg_logprob": -0.04668061667626057, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.0008557698456570506}, {"id": 397, "seek": 208144, "start": 2096.56, "end": 2103.6, "text": " at minimum and you can tune these two parameters according to your needs and of course you can do", "tokens": [51120, 412, 7285, 293, 291, 393, 10864, 613, 732, 9834, 4650, 281, 428, 2203, 293, 295, 1164, 291, 393, 360, 51472], "temperature": 0.0, "avg_logprob": -0.04668061667626057, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.0008557698456570506}, {"id": 398, "seek": 208144, "start": 2103.6, "end": 2108.56, "text": " some more like global tuning split your data over several buckets use less than a hundred", "tokens": [51472, 512, 544, 411, 4338, 15164, 7472, 428, 1412, 670, 2940, 32191, 764, 1570, 813, 257, 3262, 51720], "temperature": 0.0, "avg_logprob": -0.04668061667626057, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.0008557698456570506}, {"id": 399, "seek": 210856, "start": 2108.56, "end": 2114.32, "text": " nodes if possible or come to us and we can work out a solution and you can use also gateway nodes", "tokens": [50364, 13891, 498, 1944, 420, 808, 281, 505, 293, 321, 393, 589, 484, 257, 3827, 293, 291, 393, 764, 611, 28532, 13891, 50652], "temperature": 0.0, "avg_logprob": -0.08172991469099715, "compression_ratio": 1.8773946360153257, "no_speech_prob": 0.005551648326218128}, {"id": 400, "seek": 210856, "start": 2114.32, "end": 2121.2, "text": " which are good way to like have have nodes which are so have the request go faster because if you", "tokens": [50652, 597, 366, 665, 636, 281, 411, 362, 362, 13891, 597, 366, 370, 362, 264, 5308, 352, 4663, 570, 498, 291, 50996], "temperature": 0.0, "avg_logprob": -0.08172991469099715, "compression_ratio": 1.8773946360153257, "no_speech_prob": 0.005551648326218128}, {"id": 401, "seek": 210856, "start": 2121.2, "end": 2125.44, "text": " if you have a local gateway on the same server as the client it can basically route the request", "tokens": [50996, 498, 291, 362, 257, 2654, 28532, 322, 264, 912, 7154, 382, 264, 6423, 309, 393, 1936, 7955, 264, 5308, 51208], "temperature": 0.0, "avg_logprob": -0.08172991469099715, "compression_ratio": 1.8773946360153257, "no_speech_prob": 0.005551648326218128}, {"id": 402, "seek": 210856, "start": 2125.44, "end": 2132.7999999999997, "text": " directly to the data server and you can possibly avoid run for time we have not made any deployment", "tokens": [51208, 3838, 281, 264, 1412, 7154, 293, 291, 393, 6264, 5042, 1190, 337, 565, 321, 362, 406, 1027, 604, 19317, 51576], "temperature": 0.0, "avg_logprob": -0.08172991469099715, "compression_ratio": 1.8773946360153257, "no_speech_prob": 0.005551648326218128}, {"id": 403, "seek": 210856, "start": 2132.7999999999997, "end": 2136.7999999999997, "text": " bigger than 10 terabytes on the side of the floor but actually some people have as we learned from", "tokens": [51576, 3801, 813, 1266, 1796, 24538, 322, 264, 1252, 295, 264, 4123, 457, 767, 512, 561, 362, 382, 321, 3264, 490, 51776], "temperature": 0.0, "avg_logprob": -0.08172991469099715, "compression_ratio": 1.8773946360153257, "no_speech_prob": 0.005551648326218128}, {"id": 404, "seek": 213680, "start": 2136.8, "end": 2141.28, "text": " the survey and so if some people are in the room it would be great to share your experience", "tokens": [50364, 264, 8984, 293, 370, 498, 512, 561, 366, 294, 264, 1808, 309, 576, 312, 869, 281, 2073, 428, 1752, 50588], "temperature": 0.0, "avg_logprob": -0.2089265441894531, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.008940964005887508}, {"id": 405, "seek": 213680, "start": 2142.0800000000004, "end": 2148.0800000000004, "text": " and with this I think I've talked enough garage is available as a open source software on the", "tokens": [50628, 293, 365, 341, 286, 519, 286, 600, 2825, 1547, 14400, 307, 2435, 382, 257, 1269, 4009, 4722, 322, 264, 50928], "temperature": 0.0, "avg_logprob": -0.2089265441894531, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.008940964005887508}, {"id": 406, "seek": 213680, "start": 2148.0800000000004, "end": 2153.6000000000004, "text": " website of the floor at switch and in Rust and we have a matrix channel and email you can contact us", "tokens": [50928, 3144, 295, 264, 4123, 412, 3679, 293, 294, 34952, 293, 321, 362, 257, 8141, 2269, 293, 3796, 291, 393, 3385, 505, 51204], "temperature": 0.0, "avg_logprob": -0.2089265441894531, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.008940964005887508}, {"id": 407, "seek": 213680, "start": 2153.6000000000004, "end": 2162.6400000000003, "text": " and I'm taking some questions", "tokens": [51204, 293, 286, 478, 1940, 512, 1651, 51656], "temperature": 0.0, "avg_logprob": -0.2089265441894531, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.008940964005887508}, {"id": 408, "seek": 216264, "start": 2162.64, "end": 2166.48, "text": " um", "tokens": [50364, 1105, 50556], "temperature": 0.0, "avg_logprob": -0.18586649021632234, "compression_ratio": 1.6742857142857144, "no_speech_prob": 0.0025440678000450134}, {"id": 409, "seek": 216264, "start": 2174.48, "end": 2180.16, "text": " so the question was if you store websites on garage can you integrate with dns and basically we copied", "tokens": [50956, 370, 264, 1168, 390, 498, 291, 3531, 12891, 322, 14400, 393, 291, 13365, 365, 274, 3695, 293, 1936, 321, 25365, 51240], "temperature": 0.0, "avg_logprob": -0.18586649021632234, "compression_ratio": 1.6742857142857144, "no_speech_prob": 0.0025440678000450134}, {"id": 410, "seek": 216264, "start": 2180.16, "end": 2186.3199999999997, "text": " the semantics of amazon where you can have a bucket whose name is the the domain of a website", "tokens": [51240, 264, 4361, 45298, 295, 47010, 689, 291, 393, 362, 257, 13058, 6104, 1315, 307, 264, 264, 9274, 295, 257, 3144, 51548], "temperature": 0.0, "avg_logprob": -0.18586649021632234, "compression_ratio": 1.6742857142857144, "no_speech_prob": 0.0025440678000450134}, {"id": 411, "seek": 216264, "start": 2186.3199999999997, "end": 2191.3599999999997, "text": " and so garage will route requests to the data according to the host header of the htp request", "tokens": [51548, 293, 370, 14400, 486, 7955, 12475, 281, 264, 1412, 4650, 281, 264, 3975, 23117, 295, 264, 276, 83, 79, 5308, 51800], "temperature": 0.0, "avg_logprob": -0.18586649021632234, "compression_ratio": 1.6742857142857144, "no_speech_prob": 0.0025440678000450134}, {"id": 412, "seek": 219136, "start": 2191.44, "end": 2196.7200000000003, "text": " and basically you just have to to configure your dns server so this is something you have to do as", "tokens": [50368, 293, 1936, 291, 445, 362, 281, 281, 22162, 428, 274, 3695, 7154, 370, 341, 307, 746, 291, 362, 281, 360, 382, 50632], "temperature": 0.0, "avg_logprob": -0.1419469782736449, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.0041321394965052605}, {"id": 413, "seek": 219136, "start": 2196.7200000000003, "end": 2201.44, "text": " of at sort of garage but you configure your dns server to write the request to your garage server", "tokens": [50632, 295, 412, 1333, 295, 14400, 457, 291, 22162, 428, 274, 3695, 7154, 281, 2464, 264, 5308, 281, 428, 14400, 7154, 50868], "temperature": 0.0, "avg_logprob": -0.1419469782736449, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.0041321394965052605}, {"id": 414, "seek": 219136, "start": 2201.44, "end": 2206.2400000000002, "text": " and then garage will just select the good bucket with the good content based on the name of the", "tokens": [50868, 293, 550, 14400, 486, 445, 3048, 264, 665, 13058, 365, 264, 665, 2701, 2361, 322, 264, 1315, 295, 264, 51108], "temperature": 0.0, "avg_logprob": -0.1419469782736449, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.0041321394965052605}, {"id": 415, "seek": 219136, "start": 2206.2400000000002, "end": 2210.32, "text": " bucket and you should add some reverse proxy probably in the middle if you want to tell us because", "tokens": [51108, 13058, 293, 291, 820, 909, 512, 9943, 29690, 1391, 294, 264, 2808, 498, 291, 528, 281, 980, 505, 570, 51312], "temperature": 0.0, "avg_logprob": -0.1419469782736449, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.0041321394965052605}, {"id": 416, "seek": 219136, "start": 2210.32, "end": 2216.0, "text": " garage does not do tls yeah it's because when one of those website servers goes down then you need", "tokens": [51312, 14400, 775, 406, 360, 256, 75, 82, 1338, 309, 311, 570, 562, 472, 295, 729, 3144, 15909, 1709, 760, 550, 291, 643, 51596], "temperature": 0.0, "avg_logprob": -0.1419469782736449, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.0041321394965052605}, {"id": 417, "seek": 221600, "start": 2216.0, "end": 2221.6, "text": " to reroute to some yeah so at the floor we have a solution but it's external to garage so it's more", "tokens": [50364, 281, 43819, 14040, 281, 512, 1338, 370, 412, 264, 4123, 321, 362, 257, 3827, 457, 309, 311, 8320, 281, 14400, 370, 309, 311, 544, 50644], "temperature": 0.0, "avg_logprob": -0.24764046139187282, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.004003691952675581}, {"id": 418, "seek": 221600, "start": 2221.6, "end": 2224.16, "text": " tooling", "tokens": [50644, 46593, 50772], "temperature": 0.0, "avg_logprob": -0.24764046139187282, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.004003691952675581}, {"id": 419, "seek": 221600, "start": 2224.72, "end": 2231.68, "text": " yeah so in all the examples you mentioned you have effectively one node for one zone", "tokens": [50800, 1338, 370, 294, 439, 264, 5110, 291, 2835, 291, 362, 8659, 472, 9984, 337, 472, 6668, 51148], "temperature": 0.0, "avg_logprob": -0.24764046139187282, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.004003691952675581}, {"id": 420, "seek": 221600, "start": 2232.72, "end": 2237.36, "text": " what if is that by design or can you have multiple nodes per zone or how does that", "tokens": [51200, 437, 498, 307, 300, 538, 1715, 420, 393, 291, 362, 3866, 13891, 680, 6668, 420, 577, 775, 300, 51432], "temperature": 0.0, "avg_logprob": -0.24764046139187282, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.004003691952675581}, {"id": 421, "seek": 221600, "start": 2238.16, "end": 2242.24, "text": " I think it's uh it's uh so the question was in the examples we have uh one", "tokens": [51472, 286, 519, 309, 311, 2232, 309, 311, 2232, 370, 264, 1168, 390, 294, 264, 5110, 321, 362, 2232, 472, 51676], "temperature": 0.0, "avg_logprob": -0.24764046139187282, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.004003691952675581}, {"id": 422, "seek": 224224, "start": 2242.9599999999996, "end": 2247.04, "text": " node on each zone and can we have more than one node and so I think it's yeah it's just the", "tokens": [50400, 9984, 322, 1184, 6668, 293, 393, 321, 362, 544, 813, 472, 9984, 293, 370, 286, 519, 309, 311, 1338, 309, 311, 445, 264, 50604], "temperature": 0.0, "avg_logprob": -0.09893568824319278, "compression_ratio": 1.94, "no_speech_prob": 0.004129514098167419}, {"id": 423, "seek": 224224, "start": 2247.04, "end": 2251.3599999999997, "text": " examples were not very good but yeah of course we can have multiple nodes in a single zone I think", "tokens": [50604, 5110, 645, 406, 588, 665, 457, 1338, 295, 1164, 321, 393, 362, 3866, 13891, 294, 257, 2167, 6668, 286, 519, 50820], "temperature": 0.0, "avg_logprob": -0.09893568824319278, "compression_ratio": 1.94, "no_speech_prob": 0.004129514098167419}, {"id": 424, "seek": 224224, "start": 2251.3599999999997, "end": 2257.04, "text": " maybe in this in this graph no this is not the good one but there is a there is an example where", "tokens": [50820, 1310, 294, 341, 294, 341, 4295, 572, 341, 307, 406, 264, 665, 472, 457, 456, 307, 257, 456, 307, 364, 1365, 689, 51104], "temperature": 0.0, "avg_logprob": -0.09893568824319278, "compression_ratio": 1.94, "no_speech_prob": 0.004129514098167419}, {"id": 425, "seek": 224224, "start": 2257.04, "end": 2263.2799999999997, "text": " we have several nodes in the same zone it's not a problem yeah and if you have let's say everything", "tokens": [51104, 321, 362, 2940, 13891, 294, 264, 912, 6668, 309, 311, 406, 257, 1154, 1338, 293, 498, 291, 362, 718, 311, 584, 1203, 51416], "temperature": 0.0, "avg_logprob": -0.09893568824319278, "compression_ratio": 1.94, "no_speech_prob": 0.004129514098167419}, {"id": 426, "seek": 224224, "start": 2263.2799999999997, "end": 2267.9199999999996, "text": " else calls and you only have the one zone that's remaining will the node still try to balance the", "tokens": [51416, 1646, 5498, 293, 291, 787, 362, 264, 472, 6668, 300, 311, 8877, 486, 264, 9984, 920, 853, 281, 4772, 264, 51648], "temperature": 0.0, "avg_logprob": -0.09893568824319278, "compression_ratio": 1.94, "no_speech_prob": 0.004129514098167419}, {"id": 427, "seek": 226792, "start": 2268.2400000000002, "end": 2273.2000000000003, "text": " data between themselves or is that effectively a you're in trouble so the question is how is", "tokens": [50380, 1412, 1296, 2969, 420, 307, 300, 8659, 257, 291, 434, 294, 5253, 370, 264, 1168, 307, 577, 307, 50628], "temperature": 0.0, "avg_logprob": -0.08246617798411518, "compression_ratio": 1.9126984126984128, "no_speech_prob": 0.00037394516402855515}, {"id": 428, "seek": 226792, "start": 2273.2000000000003, "end": 2277.52, "text": " data get being balanced between the nodes if you have like one zone where it's have only one node", "tokens": [50628, 1412, 483, 885, 13902, 1296, 264, 13891, 498, 291, 362, 411, 472, 6668, 689, 309, 311, 362, 787, 472, 9984, 50844], "temperature": 0.0, "avg_logprob": -0.08246617798411518, "compression_ratio": 1.9126984126984128, "no_speech_prob": 0.00037394516402855515}, {"id": 429, "seek": 226792, "start": 2277.52, "end": 2283.92, "text": " and maybe the node is smaller and so garage is trying to preserve this property of having three", "tokens": [50844, 293, 1310, 264, 9984, 307, 4356, 293, 370, 14400, 307, 1382, 281, 15665, 341, 4707, 295, 1419, 1045, 51164], "temperature": 0.0, "avg_logprob": -0.08246617798411518, "compression_ratio": 1.9126984126984128, "no_speech_prob": 0.00037394516402855515}, {"id": 430, "seek": 226792, "start": 2283.92, "end": 2288.32, "text": " copies in different places you can you can ask it to have only in two places but by default it's", "tokens": [51164, 14341, 294, 819, 3190, 291, 393, 291, 393, 1029, 309, 281, 362, 787, 294, 732, 3190, 457, 538, 7576, 309, 311, 51384], "temperature": 0.0, "avg_logprob": -0.08246617798411518, "compression_ratio": 1.9126984126984128, "no_speech_prob": 0.00037394516402855515}, {"id": 431, "seek": 226792, "start": 2288.32, "end": 2292.88, "text": " three places and this means that if you have only three zones and one is a smaller server then you", "tokens": [51384, 1045, 3190, 293, 341, 1355, 300, 498, 291, 362, 787, 1045, 16025, 293, 472, 307, 257, 4356, 7154, 550, 291, 51612], "temperature": 0.0, "avg_logprob": -0.08246617798411518, "compression_ratio": 1.9126984126984128, "no_speech_prob": 0.00037394516402855515}, {"id": 432, "seek": 229288, "start": 2292.88, "end": 2310.32, "text": " have smaller capacity of the cluster yes yes so the question yeah so the question is why did we", "tokens": [50364, 362, 4356, 6042, 295, 264, 13630, 2086, 2086, 370, 264, 1168, 1338, 370, 264, 1168, 307, 983, 630, 321, 51236], "temperature": 0.0, "avg_logprob": -0.08097866273695423, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00406739441677928}, {"id": 433, "seek": 229288, "start": 2310.32, "end": 2316.08, "text": " integrate multiple disk support instead of having multiple nodes in the same zone and I think one", "tokens": [51236, 13365, 3866, 12355, 1406, 2602, 295, 1419, 3866, 13891, 294, 264, 912, 6668, 293, 286, 519, 472, 51524], "temperature": 0.0, "avg_logprob": -0.08097866273695423, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00406739441677928}, {"id": 434, "seek": 229288, "start": 2316.08, "end": 2320.56, "text": " of the most important reasons is that this way you can reduce the total number of garage processes", "tokens": [51524, 295, 264, 881, 1021, 4112, 307, 300, 341, 636, 291, 393, 5407, 264, 3217, 1230, 295, 14400, 7555, 51748], "temperature": 0.0, "avg_logprob": -0.08097866273695423, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00406739441677928}, {"id": 435, "seek": 232056, "start": 2320.56, "end": 2326.08, "text": " and entries in this in this table basically because this table has only so many rows and if you", "tokens": [50364, 293, 23041, 294, 341, 294, 341, 3199, 1936, 570, 341, 3199, 575, 787, 370, 867, 13241, 293, 498, 291, 50640], "temperature": 0.0, "avg_logprob": -0.17643074816968068, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.004899147432297468}, {"id": 436, "seek": 232056, "start": 2326.08, "end": 2329.6, "text": " have start having many different nodes it's not going to be well balanced so reducing the number", "tokens": [50640, 362, 722, 1419, 867, 819, 13891, 309, 311, 406, 516, 281, 312, 731, 13902, 370, 12245, 264, 1230, 50816], "temperature": 0.0, "avg_logprob": -0.17643074816968068, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.004899147432297468}, {"id": 437, "seek": 232056, "start": 2329.6, "end": 2340.88, "text": " of nodes helps us be better balanced basically yes I saw many of your design matching the one of", "tokens": [50816, 295, 13891, 3665, 505, 312, 1101, 13902, 1936, 2086, 286, 1866, 867, 295, 428, 1715, 14324, 264, 472, 295, 51380], "temperature": 0.0, "avg_logprob": -0.17643074816968068, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.004899147432297468}, {"id": 438, "seek": 232056, "start": 2340.88, "end": 2348.7999999999997, "text": " open stack swift and I was wondering if you investigated using it okay so the question is", "tokens": [51380, 1269, 8630, 29184, 293, 286, 390, 6359, 498, 291, 30070, 1228, 309, 1392, 370, 264, 1168, 307, 51776], "temperature": 0.0, "avg_logprob": -0.17643074816968068, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.004899147432297468}, {"id": 439, "seek": 234880, "start": 2348.8, "end": 2353.2000000000003, "text": " there's many design points which are matching open stack swift and have we investigated using it", "tokens": [50364, 456, 311, 867, 1715, 2793, 597, 366, 14324, 1269, 8630, 29184, 293, 362, 321, 30070, 1228, 309, 50584], "temperature": 0.0, "avg_logprob": -0.11605572700500488, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.003941121511161327}, {"id": 440, "seek": 234880, "start": 2354.8, "end": 2360.88, "text": " I personally have not used open stack swift and I have not looked so much into it yes", "tokens": [50664, 286, 5665, 362, 406, 1143, 1269, 8630, 29184, 293, 286, 362, 406, 2956, 370, 709, 666, 309, 2086, 50968], "temperature": 0.0, "avg_logprob": -0.11605572700500488, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.003941121511161327}, {"id": 441, "seek": 234880, "start": 2370.88, "end": 2375.76, "text": " so the question is despite putting this much effort in multi multi node deployments is it", "tokens": [51468, 370, 264, 1168, 307, 7228, 3372, 341, 709, 4630, 294, 4825, 4825, 9984, 7274, 1117, 307, 309, 51712], "temperature": 0.0, "avg_logprob": -0.11605572700500488, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.003941121511161327}, {"id": 442, "seek": 237576, "start": 2375.84, "end": 2384.4, "text": " still worth running the system on a single node I think it's it's so many people are doing it and", "tokens": [50368, 920, 3163, 2614, 264, 1185, 322, 257, 2167, 9984, 286, 519, 309, 311, 309, 311, 370, 867, 561, 366, 884, 309, 293, 50796], "temperature": 0.0, "avg_logprob": -0.07524115552184402, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0020824228413403034}, {"id": 443, "seek": 237576, "start": 2384.4, "end": 2388.32, "text": " I think one of the reason people are doing it is because garage is pretty simple to set up and to", "tokens": [50796, 286, 519, 472, 295, 264, 1778, 561, 366, 884, 309, 307, 570, 14400, 307, 1238, 2199, 281, 992, 493, 293, 281, 50992], "temperature": 0.0, "avg_logprob": -0.07524115552184402, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0020824228413403034}, {"id": 444, "seek": 237576, "start": 2388.32, "end": 2394.32, "text": " use so I think it's definitely possible I think there are also other solutions which are good", "tokens": [50992, 764, 370, 286, 519, 309, 311, 2138, 1944, 286, 519, 456, 366, 611, 661, 6547, 597, 366, 665, 51292], "temperature": 0.0, "avg_logprob": -0.07524115552184402, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0020824228413403034}, {"id": 445, "seek": 237576, "start": 2394.32, "end": 2401.0400000000004, "text": " for single node setups so yeah try it out and figure what's works best for you and okay so I think", "tokens": [51292, 337, 2167, 9984, 46832, 370, 1338, 853, 309, 484, 293, 2573, 437, 311, 1985, 1151, 337, 291, 293, 1392, 370, 286, 519, 51628], "temperature": 0.0, "avg_logprob": -0.07524115552184402, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0020824228413403034}, {"id": 446, "seek": 240104, "start": 2401.04, "end": 2404.88, "text": " we're done for this talk thank you", "tokens": [50364, 321, 434, 1096, 337, 341, 751, 1309, 291, 50556], "temperature": 0.0, "avg_logprob": -0.2699521671641957, "compression_ratio": 0.85, "no_speech_prob": 0.006539507769048214}], "language": "en"}