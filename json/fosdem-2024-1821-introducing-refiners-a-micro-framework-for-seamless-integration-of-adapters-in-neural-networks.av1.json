{"text": " Hi everyone. Thanks for that. Sorry about that delay. I have a great privilege to introduce Benjamin to us. Everyone give Benjamin a round of applause. Thanks. Okay. So today I'm going to introduce to you Re-finals which is a micro framework we developed at Fine Grand. And I would like also in this talk to give you, to inspire you to, if you're deaf or you're not into ML or you haven't trained a model before, I think what I'm going to show you today is a great way to start and get started with ML training. So why is it so daunting right now to train ML? I think that's because we're in a phase called, we could call like the foundational models phase where those huge companies are training big, big models and it's really hard to do. And the goal of course when training those foundational models will be to reach the AGI. So I have a very weak and restrictive definition of AGI but I have a 100% scientifically accurate definition which is we're going to reach AGI when everyone in this room is going to lose its job and become unemployed engineers. In the meantime, sorry, what can we do? So generally we do two kinds of things, right? So you say you can either become a prompt engineer, you don't touch ML, you're just relying on an external API or something like that to try to make the foundational model do what you want it actually to do or you can like do the training and it's very good for bragging but you cannot build it in open source because it's really costly, you need GPUs, you need data and it's risky. Like if you want to actually solve something, you train your MLM, you don't know if it's going to solve it. So there is a third way in between prompt engineering and training for a national model and the idea is the idea of adapters. So an adapter is just a way to patch a foundational model. So generally a foundational model or like chains of transformer layers and then for instance you could just inject some new weights into it and freeze the rest and you train those. And the advantage is that you require a lot less VRAM and you use the foundation on which the foundation model is built to train something very powerful and the idea is that you get a lot more flexibility than using prompt engineering. So it's something that's really exploding right now. So for instance that's a list of all adapters that exist for large-rengorge models. There's a lot. Also for generative image, there's a lot of adapters that allows you to generate images exactly like you want and unlock new possibilities that for instance table diffusion, the foundational model cannot do. And like every week you get two new papers on that domain. So it's really, really exploding right now. Okay, so let's talk a bit about why did we do that? So most of AI codes today is written in PyTorch, which is an imperative language to write deep learning model. It's very convenient because the only thing you have to do is to write your operation in a procedural way like with the NumPy API and everything works for you. And that's really great. But the issue is that when you want to patch or modify your model, you cannot do it because the code is already written. So you could monkey patch it, but you get into huge complications when you have multiple adapters or you want to compose them, et cetera. So we wrote yet another machine learning framework to solve that. So maybe that's not a good idea. So we wrote a micro framework. And what we mean by that is that Refiners is built on top of PyTorch. So everything is intercompatible. So if you train a model with Refiners, it's going to work in PyTorch. And if you have some elements you have already in PyTorch, it's going to work in Refiners. So Refiners is based on three key concepts, the chain, the context, and the adapter. And I'm going to go through each one to show you a bit how it works. So the idea of a chain is instead of writing just operation by operation in your code, you write each model as three of different layers. And what you get is that it's easy to edit dynamically. And it's completely explicit because when you see the graph of computation, you know what your model do. You don't have to look at the actual code. You just have to look at the model. So here's a comparison of PyTorch code. So you define your layers and you write all the operations. And on the right, what you will do with Refiners, where you just put each layer one after another. So that seems very basic. But so what you get is we have a very good representation of everything. And now you have a lot of helpers to help do some operation on it. So for instance, you can wrap model, pop them up, add some others. And then when you look at the rep of the model, everything is explicit. But you know what everything does. So even if you change the name of a layer, you still see that it's a chain. So now the chain is powerful. But you say, what if I want to do really complex models? Some models have some data that can pass through different layers. So we need something to simplify the flow of the chains. And so we introduce the context API that works a bit like in UI framework where you have a store. And everything nested down there can have access to that store. And so the idea is that even if you have a very nested chain, you can set the context. And then every sub-layer is going to inherit from it. And even in very complex models, you just add something like deep nested in the model. You can have access from the outside to any tensor you add to it. So for adaptation, it's very convenient. And the third and last concept is the idea of adapter. And the idea is to have an abstraction that make it easy to perform model surgery. Because when you're patching, you want maybe to add some parts, remove some parts, and let everything connected together again. So obviously, you're not going to do all the operation by hand every time. And so we have the adapter class. And the idea is, let's say, for instance, we want to target this linear, add some more logic to it. Then we can write an adapter that's going to plug itself into it. In terms of code, it just looks like that. We have a mixing called the adapter. And you can just rub it into it. And you get for free an inject method. And the inject method is going to do exactly that. We place the linear by the adapter. So for instance, this schema, it's look like the adapter that's called the Lora adapter, which is really common. OK. And so now we're using this to train new adapters. And we're doing it in the open. So you can have a look. We have a page called Boonties, where you can come and train adapters. For instance, the color palette adapter is currently being trained by someone who hasn't had ML training for monitoring before. And so if you could come and do some stuff with us, that would be really nice. Thank you. Thank you for listening. Any questions? No? Cool. All right, let's give them another round of applause.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.9, "text": " Hi everyone.", "tokens": [50364, 2421, 1518, 13, 50609], "temperature": 0.8, "avg_logprob": -0.9638405329064478, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.5258693099021912}, {"id": 1, "seek": 0, "start": 4.9, "end": 7.32, "text": " Thanks for that.", "tokens": [50609, 2561, 337, 300, 13, 50730], "temperature": 0.8, "avg_logprob": -0.9638405329064478, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.5258693099021912}, {"id": 2, "seek": 0, "start": 7.32, "end": 10.200000000000001, "text": " Sorry about that delay.", "tokens": [50730, 4919, 466, 300, 8577, 13, 50874], "temperature": 0.8, "avg_logprob": -0.9638405329064478, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.5258693099021912}, {"id": 3, "seek": 0, "start": 10.200000000000001, "end": 14.44, "text": " I have a great privilege to introduce Benjamin to us.", "tokens": [50874, 286, 362, 257, 869, 12122, 281, 5366, 22231, 281, 505, 13, 51086], "temperature": 0.8, "avg_logprob": -0.9638405329064478, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.5258693099021912}, {"id": 4, "seek": 0, "start": 14.44, "end": 16.28, "text": " Everyone give Benjamin a round of applause.", "tokens": [51086, 5198, 976, 22231, 257, 3098, 295, 9969, 13, 51178], "temperature": 0.8, "avg_logprob": -0.9638405329064478, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.5258693099021912}, {"id": 5, "seek": 0, "start": 16.28, "end": 17.52, "text": " Thanks.", "tokens": [51178, 2561, 13, 51240], "temperature": 0.8, "avg_logprob": -0.9638405329064478, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.5258693099021912}, {"id": 6, "seek": 0, "start": 17.52, "end": 18.8, "text": " Okay.", "tokens": [51240, 1033, 13, 51304], "temperature": 0.8, "avg_logprob": -0.9638405329064478, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.5258693099021912}, {"id": 7, "seek": 0, "start": 18.8, "end": 26.48, "text": " So today I'm going to introduce to you Re-finals which is a micro framework we developed at", "tokens": [51304, 407, 965, 286, 478, 516, 281, 5366, 281, 291, 1300, 12, 5194, 1124, 597, 307, 257, 4532, 8388, 321, 4743, 412, 51688], "temperature": 0.8, "avg_logprob": -0.9638405329064478, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.5258693099021912}, {"id": 8, "seek": 0, "start": 26.48, "end": 27.34, "text": " Fine Grand.", "tokens": [51688, 12024, 6757, 13, 51731], "temperature": 0.8, "avg_logprob": -0.9638405329064478, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.5258693099021912}, {"id": 9, "seek": 2734, "start": 27.34, "end": 34.34, "text": " And I would like also in this talk to give you, to inspire you to, if you're deaf or", "tokens": [50364, 400, 286, 576, 411, 611, 294, 341, 751, 281, 976, 291, 11, 281, 15638, 291, 281, 11, 498, 291, 434, 15559, 420, 50714], "temperature": 0.0, "avg_logprob": -0.21362817053701363, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.44935342669487}, {"id": 10, "seek": 2734, "start": 34.34, "end": 39.74, "text": " you're not into ML or you haven't trained a model before, I think what I'm going to", "tokens": [50714, 291, 434, 406, 666, 21601, 420, 291, 2378, 380, 8895, 257, 2316, 949, 11, 286, 519, 437, 286, 478, 516, 281, 50984], "temperature": 0.0, "avg_logprob": -0.21362817053701363, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.44935342669487}, {"id": 11, "seek": 2734, "start": 39.74, "end": 46.22, "text": " show you today is a great way to start and get started with ML training.", "tokens": [50984, 855, 291, 965, 307, 257, 869, 636, 281, 722, 293, 483, 1409, 365, 21601, 3097, 13, 51308], "temperature": 0.0, "avg_logprob": -0.21362817053701363, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.44935342669487}, {"id": 12, "seek": 2734, "start": 46.22, "end": 49.620000000000005, "text": " So why is it so daunting right now to train ML?", "tokens": [51308, 407, 983, 307, 309, 370, 37657, 558, 586, 281, 3847, 21601, 30, 51478], "temperature": 0.0, "avg_logprob": -0.21362817053701363, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.44935342669487}, {"id": 13, "seek": 2734, "start": 49.620000000000005, "end": 55.66, "text": " I think that's because we're in a phase called, we could call like the foundational models", "tokens": [51478, 286, 519, 300, 311, 570, 321, 434, 294, 257, 5574, 1219, 11, 321, 727, 818, 411, 264, 32195, 5245, 51780], "temperature": 0.0, "avg_logprob": -0.21362817053701363, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.44935342669487}, {"id": 14, "seek": 5566, "start": 55.66, "end": 63.18, "text": " phase where those huge companies are training big, big models and it's really hard to do.", "tokens": [50364, 5574, 689, 729, 2603, 3431, 366, 3097, 955, 11, 955, 5245, 293, 309, 311, 534, 1152, 281, 360, 13, 50740], "temperature": 0.0, "avg_logprob": -0.2125947641771893, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.03125835210084915}, {"id": 15, "seek": 5566, "start": 63.18, "end": 69.02, "text": " And the goal of course when training those foundational models will be to reach the AGI.", "tokens": [50740, 400, 264, 3387, 295, 1164, 562, 3097, 729, 32195, 5245, 486, 312, 281, 2524, 264, 316, 26252, 13, 51032], "temperature": 0.0, "avg_logprob": -0.2125947641771893, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.03125835210084915}, {"id": 16, "seek": 5566, "start": 69.02, "end": 75.97999999999999, "text": " So I have a very weak and restrictive definition of AGI but I have a 100% scientifically accurate", "tokens": [51032, 407, 286, 362, 257, 588, 5336, 293, 43220, 7123, 295, 316, 26252, 457, 286, 362, 257, 2319, 4, 39719, 8559, 51380], "temperature": 0.0, "avg_logprob": -0.2125947641771893, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.03125835210084915}, {"id": 17, "seek": 5566, "start": 75.97999999999999, "end": 80.78, "text": " definition which is we're going to reach AGI when everyone in this room is going to lose", "tokens": [51380, 7123, 597, 307, 321, 434, 516, 281, 2524, 316, 26252, 562, 1518, 294, 341, 1808, 307, 516, 281, 3624, 51620], "temperature": 0.0, "avg_logprob": -0.2125947641771893, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.03125835210084915}, {"id": 18, "seek": 8078, "start": 80.78, "end": 85.34, "text": " its job and become unemployed engineers.", "tokens": [50364, 1080, 1691, 293, 1813, 34411, 11955, 13, 50592], "temperature": 0.0, "avg_logprob": -0.18906143733433314, "compression_ratio": 1.495, "no_speech_prob": 0.06311989575624466}, {"id": 19, "seek": 8078, "start": 85.34, "end": 89.9, "text": " In the meantime, sorry, what can we do?", "tokens": [50592, 682, 264, 14991, 11, 2597, 11, 437, 393, 321, 360, 30, 50820], "temperature": 0.0, "avg_logprob": -0.18906143733433314, "compression_ratio": 1.495, "no_speech_prob": 0.06311989575624466}, {"id": 20, "seek": 8078, "start": 89.9, "end": 93.54, "text": " So generally we do two kinds of things, right?", "tokens": [50820, 407, 5101, 321, 360, 732, 3685, 295, 721, 11, 558, 30, 51002], "temperature": 0.0, "avg_logprob": -0.18906143733433314, "compression_ratio": 1.495, "no_speech_prob": 0.06311989575624466}, {"id": 21, "seek": 8078, "start": 93.54, "end": 99.1, "text": " So you say you can either become a prompt engineer, you don't touch ML, you're just", "tokens": [51002, 407, 291, 584, 291, 393, 2139, 1813, 257, 12391, 11403, 11, 291, 500, 380, 2557, 21601, 11, 291, 434, 445, 51280], "temperature": 0.0, "avg_logprob": -0.18906143733433314, "compression_ratio": 1.495, "no_speech_prob": 0.06311989575624466}, {"id": 22, "seek": 8078, "start": 99.1, "end": 104.94, "text": " relying on an external API or something like that to try to make the foundational model", "tokens": [51280, 24140, 322, 364, 8320, 9362, 420, 746, 411, 300, 281, 853, 281, 652, 264, 32195, 2316, 51572], "temperature": 0.0, "avg_logprob": -0.18906143733433314, "compression_ratio": 1.495, "no_speech_prob": 0.06311989575624466}, {"id": 23, "seek": 10494, "start": 104.94, "end": 111.98, "text": " do what you want it actually to do or you can like do the training and it's very good", "tokens": [50364, 360, 437, 291, 528, 309, 767, 281, 360, 420, 291, 393, 411, 360, 264, 3097, 293, 309, 311, 588, 665, 50716], "temperature": 0.0, "avg_logprob": -0.18000873672627957, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.23898349702358246}, {"id": 24, "seek": 10494, "start": 111.98, "end": 116.53999999999999, "text": " for bragging but you cannot build it in open source because it's really costly, you need", "tokens": [50716, 337, 1548, 10877, 457, 291, 2644, 1322, 309, 294, 1269, 4009, 570, 309, 311, 534, 28328, 11, 291, 643, 50944], "temperature": 0.0, "avg_logprob": -0.18000873672627957, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.23898349702358246}, {"id": 25, "seek": 10494, "start": 116.53999999999999, "end": 119.46, "text": " GPUs, you need data and it's risky.", "tokens": [50944, 18407, 82, 11, 291, 643, 1412, 293, 309, 311, 21137, 13, 51090], "temperature": 0.0, "avg_logprob": -0.18000873672627957, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.23898349702358246}, {"id": 26, "seek": 10494, "start": 119.46, "end": 123.46, "text": " Like if you want to actually solve something, you train your MLM, you don't know if it's", "tokens": [51090, 1743, 498, 291, 528, 281, 767, 5039, 746, 11, 291, 3847, 428, 21601, 44, 11, 291, 500, 380, 458, 498, 309, 311, 51290], "temperature": 0.0, "avg_logprob": -0.18000873672627957, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.23898349702358246}, {"id": 27, "seek": 10494, "start": 123.46, "end": 126.34, "text": " going to solve it.", "tokens": [51290, 516, 281, 5039, 309, 13, 51434], "temperature": 0.0, "avg_logprob": -0.18000873672627957, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.23898349702358246}, {"id": 28, "seek": 10494, "start": 126.34, "end": 132.22, "text": " So there is a third way in between prompt engineering and training for a national model", "tokens": [51434, 407, 456, 307, 257, 2636, 636, 294, 1296, 12391, 7043, 293, 3097, 337, 257, 4048, 2316, 51728], "temperature": 0.0, "avg_logprob": -0.18000873672627957, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.23898349702358246}, {"id": 29, "seek": 13222, "start": 132.34, "end": 135.18, "text": " and the idea is the idea of adapters.", "tokens": [50370, 293, 264, 1558, 307, 264, 1558, 295, 23169, 1559, 13, 50512], "temperature": 0.0, "avg_logprob": -0.20512955764244342, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.08679909259080887}, {"id": 30, "seek": 13222, "start": 135.18, "end": 139.78, "text": " So an adapter is just a way to patch a foundational model.", "tokens": [50512, 407, 364, 22860, 307, 445, 257, 636, 281, 9972, 257, 32195, 2316, 13, 50742], "temperature": 0.0, "avg_logprob": -0.20512955764244342, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.08679909259080887}, {"id": 31, "seek": 13222, "start": 139.78, "end": 146.1, "text": " So generally a foundational model or like chains of transformer layers and then for instance", "tokens": [50742, 407, 5101, 257, 32195, 2316, 420, 411, 12626, 295, 31782, 7914, 293, 550, 337, 5197, 51058], "temperature": 0.0, "avg_logprob": -0.20512955764244342, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.08679909259080887}, {"id": 32, "seek": 13222, "start": 146.1, "end": 154.38, "text": " you could just inject some new weights into it and freeze the rest and you train those.", "tokens": [51058, 291, 727, 445, 10711, 512, 777, 17443, 666, 309, 293, 15959, 264, 1472, 293, 291, 3847, 729, 13, 51472], "temperature": 0.0, "avg_logprob": -0.20512955764244342, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.08679909259080887}, {"id": 33, "seek": 13222, "start": 154.38, "end": 162.1, "text": " And the advantage is that you require a lot less VRAM and you use the foundation on which", "tokens": [51472, 400, 264, 5002, 307, 300, 291, 3651, 257, 688, 1570, 13722, 2865, 293, 291, 764, 264, 7030, 322, 597, 51858], "temperature": 0.0, "avg_logprob": -0.20512955764244342, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.08679909259080887}, {"id": 34, "seek": 16210, "start": 162.14, "end": 168.9, "text": " the foundation model is built to train something very powerful and the idea is that you get", "tokens": [50366, 264, 7030, 2316, 307, 3094, 281, 3847, 746, 588, 4005, 293, 264, 1558, 307, 300, 291, 483, 50704], "temperature": 0.0, "avg_logprob": -0.26229681571324664, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.007261245511472225}, {"id": 35, "seek": 16210, "start": 168.9, "end": 174.54, "text": " a lot more flexibility than using prompt engineering.", "tokens": [50704, 257, 688, 544, 12635, 813, 1228, 12391, 7043, 13, 50986], "temperature": 0.0, "avg_logprob": -0.26229681571324664, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.007261245511472225}, {"id": 36, "seek": 16210, "start": 174.54, "end": 178.06, "text": " So it's something that's really exploding right now.", "tokens": [50986, 407, 309, 311, 746, 300, 311, 534, 35175, 558, 586, 13, 51162], "temperature": 0.0, "avg_logprob": -0.26229681571324664, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.007261245511472225}, {"id": 37, "seek": 16210, "start": 178.06, "end": 183.06, "text": " So for instance that's a list of all adapters that exist for large-rengorge models.", "tokens": [51162, 407, 337, 5197, 300, 311, 257, 1329, 295, 439, 23169, 1559, 300, 2514, 337, 2416, 12, 265, 872, 4685, 5245, 13, 51412], "temperature": 0.0, "avg_logprob": -0.26229681571324664, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.007261245511472225}, {"id": 38, "seek": 16210, "start": 183.06, "end": 185.06, "text": " There's a lot.", "tokens": [51412, 821, 311, 257, 688, 13, 51512], "temperature": 0.0, "avg_logprob": -0.26229681571324664, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.007261245511472225}, {"id": 39, "seek": 16210, "start": 185.06, "end": 190.98, "text": " Also for generative image, there's a lot of adapters that allows you to generate images", "tokens": [51512, 2743, 337, 1337, 1166, 3256, 11, 456, 311, 257, 688, 295, 23169, 1559, 300, 4045, 291, 281, 8460, 5267, 51808], "temperature": 0.0, "avg_logprob": -0.26229681571324664, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.007261245511472225}, {"id": 40, "seek": 19098, "start": 191.01999999999998, "end": 196.66, "text": " exactly like you want and unlock new possibilities that for instance table diffusion, the foundational", "tokens": [50366, 2293, 411, 291, 528, 293, 11634, 777, 12178, 300, 337, 5197, 3199, 25242, 11, 264, 32195, 50648], "temperature": 0.0, "avg_logprob": -0.2640895234777572, "compression_ratio": 1.5, "no_speech_prob": 0.014578492380678654}, {"id": 41, "seek": 19098, "start": 196.66, "end": 198.5, "text": " model cannot do.", "tokens": [50648, 2316, 2644, 360, 13, 50740], "temperature": 0.0, "avg_logprob": -0.2640895234777572, "compression_ratio": 1.5, "no_speech_prob": 0.014578492380678654}, {"id": 42, "seek": 19098, "start": 198.5, "end": 202.82, "text": " And like every week you get two new papers on that domain.", "tokens": [50740, 400, 411, 633, 1243, 291, 483, 732, 777, 10577, 322, 300, 9274, 13, 50956], "temperature": 0.0, "avg_logprob": -0.2640895234777572, "compression_ratio": 1.5, "no_speech_prob": 0.014578492380678654}, {"id": 43, "seek": 19098, "start": 202.82, "end": 207.01999999999998, "text": " So it's really, really exploding right now.", "tokens": [50956, 407, 309, 311, 534, 11, 534, 35175, 558, 586, 13, 51166], "temperature": 0.0, "avg_logprob": -0.2640895234777572, "compression_ratio": 1.5, "no_speech_prob": 0.014578492380678654}, {"id": 44, "seek": 19098, "start": 207.01999999999998, "end": 213.1, "text": " Okay, so let's talk a bit about why did we do that?", "tokens": [51166, 1033, 11, 370, 718, 311, 751, 257, 857, 466, 983, 630, 321, 360, 300, 30, 51470], "temperature": 0.0, "avg_logprob": -0.2640895234777572, "compression_ratio": 1.5, "no_speech_prob": 0.014578492380678654}, {"id": 45, "seek": 19098, "start": 213.1, "end": 220.38, "text": " So most of AI codes today is written in PyTorch, which is an imperative language to write deep", "tokens": [51470, 407, 881, 295, 7318, 14211, 965, 307, 3720, 294, 9953, 51, 284, 339, 11, 597, 307, 364, 32490, 2856, 281, 2464, 2452, 51834], "temperature": 0.0, "avg_logprob": -0.2640895234777572, "compression_ratio": 1.5, "no_speech_prob": 0.014578492380678654}, {"id": 46, "seek": 22038, "start": 220.42, "end": 221.7, "text": " learning model.", "tokens": [50366, 2539, 2316, 13, 50430], "temperature": 0.0, "avg_logprob": -0.14772938747032016, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.033010806888341904}, {"id": 47, "seek": 22038, "start": 221.7, "end": 227.01999999999998, "text": " It's very convenient because the only thing you have to do is to write your operation", "tokens": [50430, 467, 311, 588, 10851, 570, 264, 787, 551, 291, 362, 281, 360, 307, 281, 2464, 428, 6916, 50696], "temperature": 0.0, "avg_logprob": -0.14772938747032016, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.033010806888341904}, {"id": 48, "seek": 22038, "start": 227.01999999999998, "end": 232.78, "text": " in a procedural way like with the NumPy API and everything works for you.", "tokens": [50696, 294, 257, 43951, 636, 411, 365, 264, 22592, 47, 88, 9362, 293, 1203, 1985, 337, 291, 13, 50984], "temperature": 0.0, "avg_logprob": -0.14772938747032016, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.033010806888341904}, {"id": 49, "seek": 22038, "start": 232.78, "end": 234.14, "text": " And that's really great.", "tokens": [50984, 400, 300, 311, 534, 869, 13, 51052], "temperature": 0.0, "avg_logprob": -0.14772938747032016, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.033010806888341904}, {"id": 50, "seek": 22038, "start": 234.14, "end": 241.34, "text": " But the issue is that when you want to patch or modify your model, you cannot do it because", "tokens": [51052, 583, 264, 2734, 307, 300, 562, 291, 528, 281, 9972, 420, 16927, 428, 2316, 11, 291, 2644, 360, 309, 570, 51412], "temperature": 0.0, "avg_logprob": -0.14772938747032016, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.033010806888341904}, {"id": 51, "seek": 22038, "start": 241.34, "end": 244.14, "text": " the code is already written.", "tokens": [51412, 264, 3089, 307, 1217, 3720, 13, 51552], "temperature": 0.0, "avg_logprob": -0.14772938747032016, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.033010806888341904}, {"id": 52, "seek": 22038, "start": 244.14, "end": 249.5, "text": " So you could monkey patch it, but you get into huge complications when you have multiple", "tokens": [51552, 407, 291, 727, 17847, 9972, 309, 11, 457, 291, 483, 666, 2603, 26566, 562, 291, 362, 3866, 51820], "temperature": 0.0, "avg_logprob": -0.14772938747032016, "compression_ratio": 1.626984126984127, "no_speech_prob": 0.033010806888341904}, {"id": 53, "seek": 24950, "start": 249.54, "end": 254.42, "text": " adapters or you want to compose them, et cetera.", "tokens": [50366, 23169, 1559, 420, 291, 528, 281, 35925, 552, 11, 1030, 11458, 13, 50610], "temperature": 0.0, "avg_logprob": -0.16632867973541546, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00589008117094636}, {"id": 54, "seek": 24950, "start": 254.42, "end": 259.18, "text": " So we wrote yet another machine learning framework to solve that.", "tokens": [50610, 407, 321, 4114, 1939, 1071, 3479, 2539, 8388, 281, 5039, 300, 13, 50848], "temperature": 0.0, "avg_logprob": -0.16632867973541546, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00589008117094636}, {"id": 55, "seek": 24950, "start": 259.18, "end": 260.9, "text": " So maybe that's not a good idea.", "tokens": [50848, 407, 1310, 300, 311, 406, 257, 665, 1558, 13, 50934], "temperature": 0.0, "avg_logprob": -0.16632867973541546, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00589008117094636}, {"id": 56, "seek": 24950, "start": 260.9, "end": 264.46, "text": " So we wrote a micro framework.", "tokens": [50934, 407, 321, 4114, 257, 4532, 8388, 13, 51112], "temperature": 0.0, "avg_logprob": -0.16632867973541546, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00589008117094636}, {"id": 57, "seek": 24950, "start": 264.46, "end": 269.9, "text": " And what we mean by that is that Refiners is built on top of PyTorch.", "tokens": [51112, 400, 437, 321, 914, 538, 300, 307, 300, 1300, 5194, 433, 307, 3094, 322, 1192, 295, 9953, 51, 284, 339, 13, 51384], "temperature": 0.0, "avg_logprob": -0.16632867973541546, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00589008117094636}, {"id": 58, "seek": 24950, "start": 269.9, "end": 272.74, "text": " So everything is intercompatible.", "tokens": [51384, 407, 1203, 307, 728, 1112, 11584, 964, 13, 51526], "temperature": 0.0, "avg_logprob": -0.16632867973541546, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00589008117094636}, {"id": 59, "seek": 24950, "start": 272.74, "end": 276.58, "text": " So if you train a model with Refiners, it's going to work in PyTorch.", "tokens": [51526, 407, 498, 291, 3847, 257, 2316, 365, 1300, 5194, 433, 11, 309, 311, 516, 281, 589, 294, 9953, 51, 284, 339, 13, 51718], "temperature": 0.0, "avg_logprob": -0.16632867973541546, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00589008117094636}, {"id": 60, "seek": 27658, "start": 276.62, "end": 282.9, "text": " And if you have some elements you have already in PyTorch, it's going to work in Refiners.", "tokens": [50366, 400, 498, 291, 362, 512, 4959, 291, 362, 1217, 294, 9953, 51, 284, 339, 11, 309, 311, 516, 281, 589, 294, 1300, 5194, 433, 13, 50680], "temperature": 0.0, "avg_logprob": -0.12988025209178095, "compression_ratio": 1.5990338164251208, "no_speech_prob": 0.008792375214397907}, {"id": 61, "seek": 27658, "start": 282.9, "end": 289.9, "text": " So Refiners is based on three key concepts, the chain, the context, and the adapter.", "tokens": [50680, 407, 1300, 5194, 433, 307, 2361, 322, 1045, 2141, 10392, 11, 264, 5021, 11, 264, 4319, 11, 293, 264, 22860, 13, 51030], "temperature": 0.0, "avg_logprob": -0.12988025209178095, "compression_ratio": 1.5990338164251208, "no_speech_prob": 0.008792375214397907}, {"id": 62, "seek": 27658, "start": 289.9, "end": 295.62, "text": " And I'm going to go through each one to show you a bit how it works.", "tokens": [51030, 400, 286, 478, 516, 281, 352, 807, 1184, 472, 281, 855, 291, 257, 857, 577, 309, 1985, 13, 51316], "temperature": 0.0, "avg_logprob": -0.12988025209178095, "compression_ratio": 1.5990338164251208, "no_speech_prob": 0.008792375214397907}, {"id": 63, "seek": 27658, "start": 295.62, "end": 301.46, "text": " So the idea of a chain is instead of writing just operation by operation in your code,", "tokens": [51316, 407, 264, 1558, 295, 257, 5021, 307, 2602, 295, 3579, 445, 6916, 538, 6916, 294, 428, 3089, 11, 51608], "temperature": 0.0, "avg_logprob": -0.12988025209178095, "compression_ratio": 1.5990338164251208, "no_speech_prob": 0.008792375214397907}, {"id": 64, "seek": 30146, "start": 301.46, "end": 308.06, "text": " you write each model as three of different layers.", "tokens": [50364, 291, 2464, 1184, 2316, 382, 1045, 295, 819, 7914, 13, 50694], "temperature": 0.0, "avg_logprob": -0.1726306699356943, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.01834814064204693}, {"id": 65, "seek": 30146, "start": 308.06, "end": 313.26, "text": " And what you get is that it's easy to edit dynamically.", "tokens": [50694, 400, 437, 291, 483, 307, 300, 309, 311, 1858, 281, 8129, 43492, 13, 50954], "temperature": 0.0, "avg_logprob": -0.1726306699356943, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.01834814064204693}, {"id": 66, "seek": 30146, "start": 313.26, "end": 318.09999999999997, "text": " And it's completely explicit because when you see the graph of computation, you know", "tokens": [50954, 400, 309, 311, 2584, 13691, 570, 562, 291, 536, 264, 4295, 295, 24903, 11, 291, 458, 51196], "temperature": 0.0, "avg_logprob": -0.1726306699356943, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.01834814064204693}, {"id": 67, "seek": 30146, "start": 318.09999999999997, "end": 319.06, "text": " what your model do.", "tokens": [51196, 437, 428, 2316, 360, 13, 51244], "temperature": 0.0, "avg_logprob": -0.1726306699356943, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.01834814064204693}, {"id": 68, "seek": 30146, "start": 319.06, "end": 320.97999999999996, "text": " You don't have to look at the actual code.", "tokens": [51244, 509, 500, 380, 362, 281, 574, 412, 264, 3539, 3089, 13, 51340], "temperature": 0.0, "avg_logprob": -0.1726306699356943, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.01834814064204693}, {"id": 69, "seek": 30146, "start": 320.97999999999996, "end": 323.38, "text": " You just have to look at the model.", "tokens": [51340, 509, 445, 362, 281, 574, 412, 264, 2316, 13, 51460], "temperature": 0.0, "avg_logprob": -0.1726306699356943, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.01834814064204693}, {"id": 70, "seek": 30146, "start": 323.38, "end": 326.9, "text": " So here's a comparison of PyTorch code.", "tokens": [51460, 407, 510, 311, 257, 9660, 295, 9953, 51, 284, 339, 3089, 13, 51636], "temperature": 0.0, "avg_logprob": -0.1726306699356943, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.01834814064204693}, {"id": 71, "seek": 30146, "start": 326.9, "end": 330.7, "text": " So you define your layers and you write all the operations.", "tokens": [51636, 407, 291, 6964, 428, 7914, 293, 291, 2464, 439, 264, 7705, 13, 51826], "temperature": 0.0, "avg_logprob": -0.1726306699356943, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.01834814064204693}, {"id": 72, "seek": 33070, "start": 330.7, "end": 338.21999999999997, "text": " And on the right, what you will do with Refiners, where you just put each layer one after another.", "tokens": [50364, 400, 322, 264, 558, 11, 437, 291, 486, 360, 365, 1300, 5194, 433, 11, 689, 291, 445, 829, 1184, 4583, 472, 934, 1071, 13, 50740], "temperature": 0.0, "avg_logprob": -0.16182596749121989, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.013155603781342506}, {"id": 73, "seek": 33070, "start": 338.21999999999997, "end": 341.09999999999997, "text": " So that seems very basic.", "tokens": [50740, 407, 300, 2544, 588, 3875, 13, 50884], "temperature": 0.0, "avg_logprob": -0.16182596749121989, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.013155603781342506}, {"id": 74, "seek": 33070, "start": 341.09999999999997, "end": 347.82, "text": " But so what you get is we have a very good representation of everything.", "tokens": [50884, 583, 370, 437, 291, 483, 307, 321, 362, 257, 588, 665, 10290, 295, 1203, 13, 51220], "temperature": 0.0, "avg_logprob": -0.16182596749121989, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.013155603781342506}, {"id": 75, "seek": 33070, "start": 347.82, "end": 352.41999999999996, "text": " And now you have a lot of helpers to help do some operation on it.", "tokens": [51220, 400, 586, 291, 362, 257, 688, 295, 854, 433, 281, 854, 360, 512, 6916, 322, 309, 13, 51450], "temperature": 0.0, "avg_logprob": -0.16182596749121989, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.013155603781342506}, {"id": 76, "seek": 33070, "start": 352.41999999999996, "end": 356.38, "text": " So for instance, you can wrap model, pop them up, add some others.", "tokens": [51450, 407, 337, 5197, 11, 291, 393, 7019, 2316, 11, 1665, 552, 493, 11, 909, 512, 2357, 13, 51648], "temperature": 0.0, "avg_logprob": -0.16182596749121989, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.013155603781342506}, {"id": 77, "seek": 33070, "start": 356.38, "end": 359.53999999999996, "text": " And then when you look at the rep of the model, everything is explicit.", "tokens": [51648, 400, 550, 562, 291, 574, 412, 264, 1085, 295, 264, 2316, 11, 1203, 307, 13691, 13, 51806], "temperature": 0.0, "avg_logprob": -0.16182596749121989, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.013155603781342506}, {"id": 78, "seek": 35954, "start": 359.54, "end": 362.3, "text": " But you know what everything does.", "tokens": [50364, 583, 291, 458, 437, 1203, 775, 13, 50502], "temperature": 0.0, "avg_logprob": -0.1602510407913563, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.21691463887691498}, {"id": 79, "seek": 35954, "start": 362.3, "end": 367.90000000000003, "text": " So even if you change the name of a layer, you still see that it's a chain.", "tokens": [50502, 407, 754, 498, 291, 1319, 264, 1315, 295, 257, 4583, 11, 291, 920, 536, 300, 309, 311, 257, 5021, 13, 50782], "temperature": 0.0, "avg_logprob": -0.1602510407913563, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.21691463887691498}, {"id": 80, "seek": 35954, "start": 367.90000000000003, "end": 372.06, "text": " So now the chain is powerful.", "tokens": [50782, 407, 586, 264, 5021, 307, 4005, 13, 50990], "temperature": 0.0, "avg_logprob": -0.1602510407913563, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.21691463887691498}, {"id": 81, "seek": 35954, "start": 372.06, "end": 375.46000000000004, "text": " But you say, what if I want to do really complex models?", "tokens": [50990, 583, 291, 584, 11, 437, 498, 286, 528, 281, 360, 534, 3997, 5245, 30, 51160], "temperature": 0.0, "avg_logprob": -0.1602510407913563, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.21691463887691498}, {"id": 82, "seek": 35954, "start": 375.46000000000004, "end": 379.54, "text": " Some models have some data that can pass through different layers.", "tokens": [51160, 2188, 5245, 362, 512, 1412, 300, 393, 1320, 807, 819, 7914, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1602510407913563, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.21691463887691498}, {"id": 83, "seek": 35954, "start": 379.54, "end": 384.14000000000004, "text": " So we need something to simplify the flow of the chains.", "tokens": [51364, 407, 321, 643, 746, 281, 20460, 264, 3095, 295, 264, 12626, 13, 51594], "temperature": 0.0, "avg_logprob": -0.1602510407913563, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.21691463887691498}, {"id": 84, "seek": 38414, "start": 384.14, "end": 391.62, "text": " And so we introduce the context API that works a bit like in UI framework where you have a store.", "tokens": [50364, 400, 370, 321, 5366, 264, 4319, 9362, 300, 1985, 257, 857, 411, 294, 15682, 8388, 689, 291, 362, 257, 3531, 13, 50738], "temperature": 0.0, "avg_logprob": -0.17203176139604928, "compression_ratio": 1.768181818181818, "no_speech_prob": 0.03424106538295746}, {"id": 85, "seek": 38414, "start": 391.62, "end": 396.94, "text": " And everything nested down there can have access to that store.", "tokens": [50738, 400, 1203, 15646, 292, 760, 456, 393, 362, 2105, 281, 300, 3531, 13, 51004], "temperature": 0.0, "avg_logprob": -0.17203176139604928, "compression_ratio": 1.768181818181818, "no_speech_prob": 0.03424106538295746}, {"id": 86, "seek": 38414, "start": 396.94, "end": 402.7, "text": " And so the idea is that even if you have a very nested chain, you can set the context.", "tokens": [51004, 400, 370, 264, 1558, 307, 300, 754, 498, 291, 362, 257, 588, 15646, 292, 5021, 11, 291, 393, 992, 264, 4319, 13, 51292], "temperature": 0.0, "avg_logprob": -0.17203176139604928, "compression_ratio": 1.768181818181818, "no_speech_prob": 0.03424106538295746}, {"id": 87, "seek": 38414, "start": 402.7, "end": 405.97999999999996, "text": " And then every sub-layer is going to inherit from it.", "tokens": [51292, 400, 550, 633, 1422, 12, 8376, 260, 307, 516, 281, 21389, 490, 309, 13, 51456], "temperature": 0.0, "avg_logprob": -0.17203176139604928, "compression_ratio": 1.768181818181818, "no_speech_prob": 0.03424106538295746}, {"id": 88, "seek": 38414, "start": 405.97999999999996, "end": 411.53999999999996, "text": " And even in very complex models, you just add something like deep nested in the model.", "tokens": [51456, 400, 754, 294, 588, 3997, 5245, 11, 291, 445, 909, 746, 411, 2452, 15646, 292, 294, 264, 2316, 13, 51734], "temperature": 0.0, "avg_logprob": -0.17203176139604928, "compression_ratio": 1.768181818181818, "no_speech_prob": 0.03424106538295746}, {"id": 89, "seek": 41154, "start": 411.54, "end": 415.5, "text": " You can have access from the outside to any tensor you add to it.", "tokens": [50364, 509, 393, 362, 2105, 490, 264, 2380, 281, 604, 40863, 291, 909, 281, 309, 13, 50562], "temperature": 0.0, "avg_logprob": -0.14686431203569686, "compression_ratio": 1.6867924528301887, "no_speech_prob": 0.03132755309343338}, {"id": 90, "seek": 41154, "start": 415.5, "end": 419.38, "text": " So for adaptation, it's very convenient.", "tokens": [50562, 407, 337, 21549, 11, 309, 311, 588, 10851, 13, 50756], "temperature": 0.0, "avg_logprob": -0.14686431203569686, "compression_ratio": 1.6867924528301887, "no_speech_prob": 0.03132755309343338}, {"id": 91, "seek": 41154, "start": 419.38, "end": 422.74, "text": " And the third and last concept is the idea of adapter.", "tokens": [50756, 400, 264, 2636, 293, 1036, 3410, 307, 264, 1558, 295, 22860, 13, 50924], "temperature": 0.0, "avg_logprob": -0.14686431203569686, "compression_ratio": 1.6867924528301887, "no_speech_prob": 0.03132755309343338}, {"id": 92, "seek": 41154, "start": 422.74, "end": 427.90000000000003, "text": " And the idea is to have an abstraction that make it easy to perform model surgery.", "tokens": [50924, 400, 264, 1558, 307, 281, 362, 364, 37765, 300, 652, 309, 1858, 281, 2042, 2316, 7930, 13, 51182], "temperature": 0.0, "avg_logprob": -0.14686431203569686, "compression_ratio": 1.6867924528301887, "no_speech_prob": 0.03132755309343338}, {"id": 93, "seek": 41154, "start": 427.90000000000003, "end": 432.58000000000004, "text": " Because when you're patching, you want maybe to add some parts, remove some parts, and", "tokens": [51182, 1436, 562, 291, 434, 9972, 278, 11, 291, 528, 1310, 281, 909, 512, 3166, 11, 4159, 512, 3166, 11, 293, 51416], "temperature": 0.0, "avg_logprob": -0.14686431203569686, "compression_ratio": 1.6867924528301887, "no_speech_prob": 0.03132755309343338}, {"id": 94, "seek": 41154, "start": 432.58000000000004, "end": 435.02000000000004, "text": " let everything connected together again.", "tokens": [51416, 718, 1203, 4582, 1214, 797, 13, 51538], "temperature": 0.0, "avg_logprob": -0.14686431203569686, "compression_ratio": 1.6867924528301887, "no_speech_prob": 0.03132755309343338}, {"id": 95, "seek": 41154, "start": 435.02000000000004, "end": 439.34000000000003, "text": " So obviously, you're not going to do all the operation by hand every time.", "tokens": [51538, 407, 2745, 11, 291, 434, 406, 516, 281, 360, 439, 264, 6916, 538, 1011, 633, 565, 13, 51754], "temperature": 0.0, "avg_logprob": -0.14686431203569686, "compression_ratio": 1.6867924528301887, "no_speech_prob": 0.03132755309343338}, {"id": 96, "seek": 43934, "start": 439.34, "end": 441.29999999999995, "text": " And so we have the adapter class.", "tokens": [50364, 400, 370, 321, 362, 264, 22860, 1508, 13, 50462], "temperature": 0.0, "avg_logprob": -0.14730695641559102, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.021907631307840347}, {"id": 97, "seek": 43934, "start": 441.29999999999995, "end": 447.34, "text": " And the idea is, let's say, for instance, we want to target this linear, add some more logic to it.", "tokens": [50462, 400, 264, 1558, 307, 11, 718, 311, 584, 11, 337, 5197, 11, 321, 528, 281, 3779, 341, 8213, 11, 909, 512, 544, 9952, 281, 309, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14730695641559102, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.021907631307840347}, {"id": 98, "seek": 43934, "start": 447.34, "end": 454.29999999999995, "text": " Then we can write an adapter that's going to plug itself into it.", "tokens": [50764, 1396, 321, 393, 2464, 364, 22860, 300, 311, 516, 281, 5452, 2564, 666, 309, 13, 51112], "temperature": 0.0, "avg_logprob": -0.14730695641559102, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.021907631307840347}, {"id": 99, "seek": 43934, "start": 454.29999999999995, "end": 457.34, "text": " In terms of code, it just looks like that.", "tokens": [51112, 682, 2115, 295, 3089, 11, 309, 445, 1542, 411, 300, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14730695641559102, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.021907631307840347}, {"id": 100, "seek": 43934, "start": 457.34, "end": 459.46, "text": " We have a mixing called the adapter.", "tokens": [51264, 492, 362, 257, 11983, 1219, 264, 22860, 13, 51370], "temperature": 0.0, "avg_logprob": -0.14730695641559102, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.021907631307840347}, {"id": 101, "seek": 43934, "start": 459.46, "end": 462.21999999999997, "text": " And you can just rub it into it.", "tokens": [51370, 400, 291, 393, 445, 5915, 309, 666, 309, 13, 51508], "temperature": 0.0, "avg_logprob": -0.14730695641559102, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.021907631307840347}, {"id": 102, "seek": 43934, "start": 462.21999999999997, "end": 466.29999999999995, "text": " And you get for free an inject method.", "tokens": [51508, 400, 291, 483, 337, 1737, 364, 10711, 3170, 13, 51712], "temperature": 0.0, "avg_logprob": -0.14730695641559102, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.021907631307840347}, {"id": 103, "seek": 43934, "start": 466.29999999999995, "end": 468.5, "text": " And the inject method is going to do exactly that.", "tokens": [51712, 400, 264, 10711, 3170, 307, 516, 281, 360, 2293, 300, 13, 51822], "temperature": 0.0, "avg_logprob": -0.14730695641559102, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.021907631307840347}, {"id": 104, "seek": 46850, "start": 468.5, "end": 470.54, "text": " We place the linear by the adapter.", "tokens": [50364, 492, 1081, 264, 8213, 538, 264, 22860, 13, 50466], "temperature": 0.0, "avg_logprob": -0.23536421294905183, "compression_ratio": 1.7531914893617022, "no_speech_prob": 0.027704918757081032}, {"id": 105, "seek": 46850, "start": 470.54, "end": 472.7, "text": " So for instance, this schema, it's", "tokens": [50466, 407, 337, 5197, 11, 341, 34078, 11, 309, 311, 50574], "temperature": 0.0, "avg_logprob": -0.23536421294905183, "compression_ratio": 1.7531914893617022, "no_speech_prob": 0.027704918757081032}, {"id": 106, "seek": 46850, "start": 472.7, "end": 476.26, "text": " look like the adapter that's called the Lora adapter, which is really common.", "tokens": [50574, 574, 411, 264, 22860, 300, 311, 1219, 264, 441, 3252, 22860, 11, 597, 307, 534, 2689, 13, 50752], "temperature": 0.0, "avg_logprob": -0.23536421294905183, "compression_ratio": 1.7531914893617022, "no_speech_prob": 0.027704918757081032}, {"id": 107, "seek": 46850, "start": 478.94, "end": 479.38, "text": " OK.", "tokens": [50886, 2264, 13, 50908], "temperature": 0.0, "avg_logprob": -0.23536421294905183, "compression_ratio": 1.7531914893617022, "no_speech_prob": 0.027704918757081032}, {"id": 108, "seek": 46850, "start": 479.38, "end": 483.18, "text": " And so now we're using this to train new adapters.", "tokens": [50908, 400, 370, 586, 321, 434, 1228, 341, 281, 3847, 777, 23169, 1559, 13, 51098], "temperature": 0.0, "avg_logprob": -0.23536421294905183, "compression_ratio": 1.7531914893617022, "no_speech_prob": 0.027704918757081032}, {"id": 109, "seek": 46850, "start": 483.18, "end": 485.14, "text": " And we're doing it in the open.", "tokens": [51098, 400, 321, 434, 884, 309, 294, 264, 1269, 13, 51196], "temperature": 0.0, "avg_logprob": -0.23536421294905183, "compression_ratio": 1.7531914893617022, "no_speech_prob": 0.027704918757081032}, {"id": 110, "seek": 46850, "start": 485.14, "end": 486.26, "text": " So you can have a look.", "tokens": [51196, 407, 291, 393, 362, 257, 574, 13, 51252], "temperature": 0.0, "avg_logprob": -0.23536421294905183, "compression_ratio": 1.7531914893617022, "no_speech_prob": 0.027704918757081032}, {"id": 111, "seek": 46850, "start": 486.26, "end": 491.74, "text": " We have a page called Boonties, where you can come and train adapters.", "tokens": [51252, 492, 362, 257, 3028, 1219, 3286, 896, 530, 11, 689, 291, 393, 808, 293, 3847, 23169, 1559, 13, 51526], "temperature": 0.0, "avg_logprob": -0.23536421294905183, "compression_ratio": 1.7531914893617022, "no_speech_prob": 0.027704918757081032}, {"id": 112, "seek": 46850, "start": 491.74, "end": 495.94, "text": " For instance, the color palette adapter is currently being trained by someone who", "tokens": [51526, 1171, 5197, 11, 264, 2017, 15851, 22860, 307, 4362, 885, 8895, 538, 1580, 567, 51736], "temperature": 0.0, "avg_logprob": -0.23536421294905183, "compression_ratio": 1.7531914893617022, "no_speech_prob": 0.027704918757081032}, {"id": 113, "seek": 49594, "start": 495.94, "end": 499.9, "text": " hasn't had ML training for monitoring before.", "tokens": [50364, 6132, 380, 632, 21601, 3097, 337, 11028, 949, 13, 50562], "temperature": 0.0, "avg_logprob": -0.36579418182373047, "compression_ratio": 1.3976608187134503, "no_speech_prob": 0.008443639613687992}, {"id": 114, "seek": 49594, "start": 499.9, "end": 505.7, "text": " And so if you could come and do some stuff with us, that would be really nice.", "tokens": [50562, 400, 370, 498, 291, 727, 808, 293, 360, 512, 1507, 365, 505, 11, 300, 576, 312, 534, 1481, 13, 50852], "temperature": 0.0, "avg_logprob": -0.36579418182373047, "compression_ratio": 1.3976608187134503, "no_speech_prob": 0.008443639613687992}, {"id": 115, "seek": 49594, "start": 505.7, "end": 506.26, "text": " Thank you.", "tokens": [50852, 1044, 291, 13, 50880], "temperature": 0.0, "avg_logprob": -0.36579418182373047, "compression_ratio": 1.3976608187134503, "no_speech_prob": 0.008443639613687992}, {"id": 116, "seek": 49594, "start": 506.26, "end": 507.54, "text": " Thank you for listening.", "tokens": [50880, 1044, 291, 337, 4764, 13, 50944], "temperature": 0.0, "avg_logprob": -0.36579418182373047, "compression_ratio": 1.3976608187134503, "no_speech_prob": 0.008443639613687992}, {"id": 117, "seek": 49594, "start": 507.54, "end": 518.54, "text": " Any questions?", "tokens": [50944, 2639, 1651, 30, 51494], "temperature": 0.0, "avg_logprob": -0.36579418182373047, "compression_ratio": 1.3976608187134503, "no_speech_prob": 0.008443639613687992}, {"id": 118, "seek": 49594, "start": 518.54, "end": 520.26, "text": " No?", "tokens": [51494, 883, 30, 51580], "temperature": 0.0, "avg_logprob": -0.36579418182373047, "compression_ratio": 1.3976608187134503, "no_speech_prob": 0.008443639613687992}, {"id": 119, "seek": 49594, "start": 520.26, "end": 520.98, "text": " Cool.", "tokens": [51580, 8561, 13, 51616], "temperature": 0.0, "avg_logprob": -0.36579418182373047, "compression_ratio": 1.3976608187134503, "no_speech_prob": 0.008443639613687992}, {"id": 120, "seek": 49594, "start": 520.98, "end": 522.62, "text": " All right, let's give them another round of applause.", "tokens": [51616, 1057, 558, 11, 718, 311, 976, 552, 1071, 3098, 295, 9969, 13, 51698], "temperature": 0.0, "avg_logprob": -0.36579418182373047, "compression_ratio": 1.3976608187134503, "no_speech_prob": 0.008443639613687992}], "language": "en"}