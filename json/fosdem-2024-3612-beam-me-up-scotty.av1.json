{"text": " Very short presentation about me, what is BIM for everyone that doesn't know it, of course. Thank you, David. Thank you. So thank you all for coming. As David said, this is just a brief introduction on what most of us, I think, already know the BIM and what's around there. I'll try to speak louder because I don't think that those things are working, but the camera is okay. So obviously, this one doesn't work no more. Come on. Oh, spoiler. Okay. So what is BIM? BIM, as we know, is the virtual machine that powers the old Erlang, Elixir, Glim, etc., etc. ecosystem. It was originally built in the late 70s to early 80s, and it runs in production ever since. Very nice. It's almost 40 years of running in production. Okay. The focus of the virtual machine, since it's very conception, was on concurrency and distribution in a moment where nobody was considering it. Actually, people are still inventing the wheel of concurrency to these days. Let's think of ACCA, for example, or things like that. An interesting statistics about the BIM is that according to Cisco, about 90 percent of internet traffic goes at least one time through BIM and Erlang node. BIM is being used in production successfully by WhatsApp, Discord, and such, to handle a very large amount of messages per second, like more than one million. Obviously, since we're all here and we are cool kids, we do love doing things cool, like making programming languages. So the first of all is obviously Erlang, the one that started it all, the old-wise guy, and that's an example of Erlang. It's very inspired in its syntax by Prologue. It is a functional language, dynamically typed. There are some very nice things like pattern matching, binary pattern matching. Actually, this is an example of the Ranch library, which is a framework for handling TCP connections. As you can see, there are things like tuples, lists, etc., etc., supervisors. I'll talk about this later on. And it's an example of a behavior, of an application. I'll talk about this later on, too. There are macros in Erlang. There are preposers based. And this leads to Elixir, another language in which macros are first class, first of all. And Elixir is much more recent. Some say it triggered a renaissance of the beam, of interest in the beam, because of its ruby-like syntax, which is, in my opinion, nicer than Prologue, based on one. But I don't say so for everyone. This is an example of another library, and it's under for connections to a kind of database. Also, Elixir is functional, just like Erlang, and it's dynamically typed for now. Because there's some work on bringing types to the Elixir compiler. Being a beam language, it also has pattern matching, also binary pattern matching, and macros, as I said, are first class. And then there's the cool new kid, Glim. I see many people rejoicing. The syntax, it's a strange mix between an ML language and a beam language. Its syntax is a bit inspired by Rust, but the most important thing is that it's statically typed, rather than dynamically. And it also has a philosophy, being a statically type of handling some errors before they happen, rather than just doing it later on. And it can also compile to JavaScript. As I say, it is the cool new kid, so it has to compile to JavaScript. But then, as I said, there are a number of other friends of the beam. Least flavored Erlang, if you really like parenthesis. Pure Erl, if you're an Askel guy who wants to run on Erlang for some reason. Then if you're more Askel-y than Askel's guy, and you want to try dependent types, you can also compile Idris2 to Erlang, if you want. And then just a bunch of languages that the great wise man Peter Weirding did. So, Braggful, which is a PHP compiling to Erlang, Lua compiling to Erlang, and all things like that. He just loves making beam languages. And he's a very nice guy. So, why is people still continuing to build languages on the beam? Because the beam has some kind of superpowers built in there. Actually, let me interject for a moment. What I'm referring to as beam is actually beam OTP, or as I have written, he started calling beam plus OTP. What's the difference? So, beam is just the bytecode VM that runs the core bytecode code. And it's register-based. Then there's a runtime system called ERTS, the Erlang runtime system. That handles how to make this binary code run on the beam. Yeah, that's true. So, we have concepts, processes, synchronization, because everything in the beam is a synchronous. And ports, ETS tables, which are used for storage of persistent data and such. It also takes care of scheduling the processes on the beam in a preemptive fashion. And it's usually mixed, confused with the beam. So, I keep referring to beam plus ERTS as beam, as everyone is doing. Then there's another part, very important, in the Erlang ecosystem, that's OTP. It's a framework that provides you with some battle-tested abstractions. I think many of us use supervisors, gen servers, state machines, and things like that. Supervisors handle what happens if a process fails. Gen servers are an abstraction on the concept of a server. State machines are abstractions on the concept of state machines, obviously. The great part of using the Erlang ecosystem is that all of these three components provide one alpha of greatness. So, we have three albs of greatness in total. So, what are the superpowers, or beam plus OTP? As I've recently started talking, calling it. First of all, concurrency. As I said before, the main unit of concurrency is the process. The process is just a piece of sequential code that's running, having his own memory, his own it, and sharing nothing with all other processes. What's the point of it? Handling failure in just one simple and easy way, located just in the process. Meaning that when you have a crash in your process, only that process will crash and not the whole application. Then, a consequence of that is that garbage collection runs on a per process basis. So, you don't need like in Java to stop everything just to run the garbage collector, you can run it synchronously. And finally, since we do share nothing, the only way process is ever to communicate with each other is by sending messages or signals actually. This allows the beam to scale seamlessly from a single node setup to a multi-node setup. You just rather than sending it to a local process, you send a message to a process in a different beam node. Then, this leads to the let it crash philosophy of beam and OTP. Since every part in the end is just a tree of supervisors, a supervision tree. You can also obviously propagate failure between processes so that you can handle doing the right thing depending on what process is crashed. But why in this world would you need supervisors? Since we have Kubernetes that almost seem to do the right and the same thing. Restarting our pod if something is not working. Well, the main idea is that when if a network connection goes down, you don't want to restart your application. So, those are two different layers. OTP focuses on the application layer, handling failures in your domain. And then Kubernetes focuses on the larger aspect of orchestration. So, you don't want to crash, as I said before, if a network question is going down. You just need to handle that in your application and you restart the pod or your deployment just in very specific cases or very tragic cases. That's where Kubernetes comes into the rescue. Being application-based, the supervision trees are obviously also more granular. So, that you can define a different strategy rather than just turning it on when the process crashed. So, it provides for your application a more flexible way to handle crashes. Kubernetes does not. It just handles networking, bring up and down pods, and that's it. Those are different level orchestrating containers is a different level than handling failures in your application. Next superpower of being a plus OTP is immutability. It didn't seem so when it was built, but now we see its value together with a share nothing concurrency. Because having no shared memory means that the processes cannot change the state of under process unexpectedly. And that's the reason why we also have immutability of data structures. This also leads to a referential transparency. Even if the beam allows you to have some kind of side effects, for example, logging or networking, it's just a pragmatic way of handling it rather than setting up a whole monad stack, for example. Then, final superpower distribution. As I said before, from the point of view of an application, it's the same to pass messages to a local node, a node in the same virtual machine or in another node that is running maybe in another part of the world. This allows you to distribute the work efficiently, seamlessly as a programmer. And this is made by a built-in protocol to discover nodes. And you can scale horizontally however you want. The code that you wrote will just work most of the time. Finally, since those things run in production ever since, there's a huge interest in observability and debuggability. The most easy thing to do is connect to your live application and start an interactive shell just to see what's going wrong. You can trace. There's an interesting tool called tracing later on, and access all the information of a process and so on and so forth. Tracing and profiling, as I said, are built-in into the machine. They are battle tested by running in production for years. If anybody uses ReContrace, it's very nice, but can anyone give me a match spec that works? I never found a way to make it work. And let's not forget hot-code reloading. It's really interesting. You can change your code while it is running so that, for example, if you have detected a bug in your live application, you can just fix it on the fly and your application will just work with the new fix. Then there are other interesting aspects. The first one, pattern matching, is more interesting from one who is writing programming languages. The BIM makes it really easy to write functional languages with pattern matching, also at binary level. Then BIM is already container aware since a number of years, I think, allowing you to use C-groups and Kubernetes, obviously, seamlessly, not what Java does, for example. It makes it easy to have foreign function codes. You just write a C-node that seems to be an orang node and just communicate with it. Or you can use the FFI built-in. What's the future for the BIM? Obviously, the first part is being more and more largely adopted. And then there are some interesting research level developments on bringing gradual typing to some languages like Elixir and Erlang. Also, Elixir, but a number of languages, are doing a pretty huge job of developing numerical computing and AI on BIM nodes in order to distribute those calculations in an easier way. Obviously, there's also work on embedded systems, like bringing, for example, a small instance of the BIM vehicle machine on microcontrollers. There's a work on that, especially at UMBM. Then, the main challenge is obviously becoming a wider and wider adopted choice for the backend. And in that sense, having all these people here is a very good sign. Thank you for being there. And if you have any questions, feel free to answer. I mean, feel free to ask. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.92, "text": " Very short presentation about me,", "tokens": [50364, 4372, 2099, 5860, 466, 385, 11, 50760], "temperature": 0.0, "avg_logprob": -0.5284693746855764, "compression_ratio": 1.379746835443038, "no_speech_prob": 0.26102015376091003}, {"id": 1, "seek": 0, "start": 7.92, "end": 12.52, "text": " what is BIM for everyone that doesn't know it, of course.", "tokens": [50760, 437, 307, 363, 6324, 337, 1518, 300, 1177, 380, 458, 309, 11, 295, 1164, 13, 50990], "temperature": 0.0, "avg_logprob": -0.5284693746855764, "compression_ratio": 1.379746835443038, "no_speech_prob": 0.26102015376091003}, {"id": 2, "seek": 0, "start": 12.52, "end": 13.92, "text": " Thank you, David.", "tokens": [50990, 1044, 291, 11, 4389, 13, 51060], "temperature": 0.0, "avg_logprob": -0.5284693746855764, "compression_ratio": 1.379746835443038, "no_speech_prob": 0.26102015376091003}, {"id": 3, "seek": 0, "start": 13.92, "end": 17.6, "text": " Thank you.", "tokens": [51060, 1044, 291, 13, 51244], "temperature": 0.0, "avg_logprob": -0.5284693746855764, "compression_ratio": 1.379746835443038, "no_speech_prob": 0.26102015376091003}, {"id": 4, "seek": 0, "start": 18.8, "end": 22.72, "text": " So thank you all for coming.", "tokens": [51304, 407, 1309, 291, 439, 337, 1348, 13, 51500], "temperature": 0.0, "avg_logprob": -0.5284693746855764, "compression_ratio": 1.379746835443038, "no_speech_prob": 0.26102015376091003}, {"id": 5, "seek": 0, "start": 22.72, "end": 27.400000000000002, "text": " As David said, this is just a brief introduction on what most of us,", "tokens": [51500, 1018, 4389, 848, 11, 341, 307, 445, 257, 5353, 9339, 322, 437, 881, 295, 505, 11, 51734], "temperature": 0.0, "avg_logprob": -0.5284693746855764, "compression_ratio": 1.379746835443038, "no_speech_prob": 0.26102015376091003}, {"id": 6, "seek": 2740, "start": 27.4, "end": 31.279999999999998, "text": " I think, already know the BIM and what's around there.", "tokens": [50364, 286, 519, 11, 1217, 458, 264, 363, 6324, 293, 437, 311, 926, 456, 13, 50558], "temperature": 0.0, "avg_logprob": -0.3079059354720577, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.0475623793900013}, {"id": 7, "seek": 2740, "start": 31.279999999999998, "end": 37.6, "text": " I'll try to speak louder because I don't think that those things are working,", "tokens": [50558, 286, 603, 853, 281, 1710, 22717, 570, 286, 500, 380, 519, 300, 729, 721, 366, 1364, 11, 50874], "temperature": 0.0, "avg_logprob": -0.3079059354720577, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.0475623793900013}, {"id": 8, "seek": 2740, "start": 37.6, "end": 39.76, "text": " but the camera is okay.", "tokens": [50874, 457, 264, 2799, 307, 1392, 13, 50982], "temperature": 0.0, "avg_logprob": -0.3079059354720577, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.0475623793900013}, {"id": 9, "seek": 2740, "start": 39.76, "end": 45.4, "text": " So obviously, this one doesn't work no more.", "tokens": [50982, 407, 2745, 11, 341, 472, 1177, 380, 589, 572, 544, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3079059354720577, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.0475623793900013}, {"id": 10, "seek": 2740, "start": 50.84, "end": 53.36, "text": " Come on.", "tokens": [51536, 2492, 322, 13, 51662], "temperature": 0.0, "avg_logprob": -0.3079059354720577, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.0475623793900013}, {"id": 11, "seek": 5740, "start": 57.4, "end": 62.4, "text": " Oh, spoiler.", "tokens": [50364, 876, 11, 26927, 13, 50614], "temperature": 0.0, "avg_logprob": -0.28245641830119683, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.008052768185734749}, {"id": 12, "seek": 5740, "start": 62.4, "end": 66.84, "text": " Okay. So what is BIM?", "tokens": [50614, 1033, 13, 407, 437, 307, 363, 6324, 30, 50836], "temperature": 0.0, "avg_logprob": -0.28245641830119683, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.008052768185734749}, {"id": 13, "seek": 5740, "start": 66.84, "end": 71.72, "text": " BIM, as we know, is the virtual machine that powers the old Erlang,", "tokens": [50836, 363, 6324, 11, 382, 321, 458, 11, 307, 264, 6374, 3479, 300, 8674, 264, 1331, 3300, 25241, 11, 51080], "temperature": 0.0, "avg_logprob": -0.28245641830119683, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.008052768185734749}, {"id": 14, "seek": 5740, "start": 71.72, "end": 74.56, "text": " Elixir, Glim, etc., etc. ecosystem.", "tokens": [51080, 2699, 970, 347, 11, 460, 4197, 11, 5183, 7933, 5183, 13, 11311, 13, 51222], "temperature": 0.0, "avg_logprob": -0.28245641830119683, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.008052768185734749}, {"id": 15, "seek": 5740, "start": 74.56, "end": 80.44, "text": " It was originally built in the late 70s to early 80s,", "tokens": [51222, 467, 390, 7993, 3094, 294, 264, 3469, 5285, 82, 281, 2440, 4688, 82, 11, 51516], "temperature": 0.0, "avg_logprob": -0.28245641830119683, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.008052768185734749}, {"id": 16, "seek": 5740, "start": 80.44, "end": 82.68, "text": " and it runs in production ever since.", "tokens": [51516, 293, 309, 6676, 294, 4265, 1562, 1670, 13, 51628], "temperature": 0.0, "avg_logprob": -0.28245641830119683, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.008052768185734749}, {"id": 17, "seek": 5740, "start": 82.68, "end": 86.6, "text": " Very nice. It's almost 40 years of running in production.", "tokens": [51628, 4372, 1481, 13, 467, 311, 1920, 3356, 924, 295, 2614, 294, 4265, 13, 51824], "temperature": 0.0, "avg_logprob": -0.28245641830119683, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.008052768185734749}, {"id": 18, "seek": 8660, "start": 87.6, "end": 91.96, "text": " Okay. The focus of the virtual machine,", "tokens": [50414, 1033, 13, 440, 1879, 295, 264, 6374, 3479, 11, 50632], "temperature": 0.0, "avg_logprob": -0.2175317027352073, "compression_ratio": 1.5330396475770924, "no_speech_prob": 0.002441976685076952}, {"id": 19, "seek": 8660, "start": 91.96, "end": 94.24, "text": " since it's very conception,", "tokens": [50632, 1670, 309, 311, 588, 30698, 11, 50746], "temperature": 0.0, "avg_logprob": -0.2175317027352073, "compression_ratio": 1.5330396475770924, "no_speech_prob": 0.002441976685076952}, {"id": 20, "seek": 8660, "start": 94.24, "end": 99.52, "text": " was on concurrency and distribution in a moment where nobody was considering it.", "tokens": [50746, 390, 322, 23702, 10457, 293, 7316, 294, 257, 1623, 689, 5079, 390, 8079, 309, 13, 51010], "temperature": 0.0, "avg_logprob": -0.2175317027352073, "compression_ratio": 1.5330396475770924, "no_speech_prob": 0.002441976685076952}, {"id": 21, "seek": 8660, "start": 99.52, "end": 104.88, "text": " Actually, people are still inventing the wheel of concurrency to these days.", "tokens": [51010, 5135, 11, 561, 366, 920, 7962, 278, 264, 5589, 295, 23702, 10457, 281, 613, 1708, 13, 51278], "temperature": 0.0, "avg_logprob": -0.2175317027352073, "compression_ratio": 1.5330396475770924, "no_speech_prob": 0.002441976685076952}, {"id": 22, "seek": 8660, "start": 104.88, "end": 108.36, "text": " Let's think of ACCA, for example, or things like that.", "tokens": [51278, 961, 311, 519, 295, 8157, 15515, 11, 337, 1365, 11, 420, 721, 411, 300, 13, 51452], "temperature": 0.0, "avg_logprob": -0.2175317027352073, "compression_ratio": 1.5330396475770924, "no_speech_prob": 0.002441976685076952}, {"id": 23, "seek": 8660, "start": 108.36, "end": 114.63999999999999, "text": " An interesting statistics about the BIM is that according to Cisco,", "tokens": [51452, 1107, 1880, 12523, 466, 264, 363, 6324, 307, 300, 4650, 281, 38528, 11, 51766], "temperature": 0.0, "avg_logprob": -0.2175317027352073, "compression_ratio": 1.5330396475770924, "no_speech_prob": 0.002441976685076952}, {"id": 24, "seek": 11464, "start": 115.0, "end": 123.24, "text": " about 90 percent of internet traffic goes at least one time through BIM and Erlang node.", "tokens": [50382, 466, 4289, 3043, 295, 4705, 6419, 1709, 412, 1935, 472, 565, 807, 363, 6324, 293, 3300, 25241, 9984, 13, 50794], "temperature": 0.0, "avg_logprob": -0.3162129142067649, "compression_ratio": 1.419811320754717, "no_speech_prob": 0.0025397015269845724}, {"id": 25, "seek": 11464, "start": 123.24, "end": 129.52, "text": " BIM is being used in production successfully by WhatsApp, Discord, and such,", "tokens": [50794, 363, 6324, 307, 885, 1143, 294, 4265, 10727, 538, 30513, 11, 32623, 11, 293, 1270, 11, 51108], "temperature": 0.0, "avg_logprob": -0.3162129142067649, "compression_ratio": 1.419811320754717, "no_speech_prob": 0.0025397015269845724}, {"id": 26, "seek": 11464, "start": 129.52, "end": 134.88, "text": " to handle a very large amount of messages per second,", "tokens": [51108, 281, 4813, 257, 588, 2416, 2372, 295, 7897, 680, 1150, 11, 51376], "temperature": 0.0, "avg_logprob": -0.3162129142067649, "compression_ratio": 1.419811320754717, "no_speech_prob": 0.0025397015269845724}, {"id": 27, "seek": 11464, "start": 134.88, "end": 138.8, "text": " like more than one million.", "tokens": [51376, 411, 544, 813, 472, 2459, 13, 51572], "temperature": 0.0, "avg_logprob": -0.3162129142067649, "compression_ratio": 1.419811320754717, "no_speech_prob": 0.0025397015269845724}, {"id": 28, "seek": 11464, "start": 138.8, "end": 141.84, "text": " Obviously, since we're all here and we are cool kids,", "tokens": [51572, 7580, 11, 1670, 321, 434, 439, 510, 293, 321, 366, 1627, 2301, 11, 51724], "temperature": 0.0, "avg_logprob": -0.3162129142067649, "compression_ratio": 1.419811320754717, "no_speech_prob": 0.0025397015269845724}, {"id": 29, "seek": 14184, "start": 142.04, "end": 146.08, "text": " we do love doing things cool, like making programming languages.", "tokens": [50374, 321, 360, 959, 884, 721, 1627, 11, 411, 1455, 9410, 8650, 13, 50576], "temperature": 0.0, "avg_logprob": -0.26343315526058797, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.002351969014853239}, {"id": 30, "seek": 14184, "start": 146.08, "end": 149.64000000000001, "text": " So the first of all is obviously Erlang,", "tokens": [50576, 407, 264, 700, 295, 439, 307, 2745, 3300, 25241, 11, 50754], "temperature": 0.0, "avg_logprob": -0.26343315526058797, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.002351969014853239}, {"id": 31, "seek": 14184, "start": 149.64000000000001, "end": 151.4, "text": " the one that started it all,", "tokens": [50754, 264, 472, 300, 1409, 309, 439, 11, 50842], "temperature": 0.0, "avg_logprob": -0.26343315526058797, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.002351969014853239}, {"id": 32, "seek": 14184, "start": 151.4, "end": 155.56, "text": " the old-wise guy, and that's an example of Erlang.", "tokens": [50842, 264, 1331, 12, 3711, 2146, 11, 293, 300, 311, 364, 1365, 295, 3300, 25241, 13, 51050], "temperature": 0.0, "avg_logprob": -0.26343315526058797, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.002351969014853239}, {"id": 33, "seek": 14184, "start": 155.56, "end": 158.6, "text": " It's very inspired in its syntax by Prologue.", "tokens": [51050, 467, 311, 588, 7547, 294, 1080, 28431, 538, 1705, 4987, 622, 13, 51202], "temperature": 0.0, "avg_logprob": -0.26343315526058797, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.002351969014853239}, {"id": 34, "seek": 14184, "start": 158.6, "end": 162.76, "text": " It is a functional language, dynamically typed.", "tokens": [51202, 467, 307, 257, 11745, 2856, 11, 43492, 33941, 13, 51410], "temperature": 0.0, "avg_logprob": -0.26343315526058797, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.002351969014853239}, {"id": 35, "seek": 14184, "start": 162.76, "end": 169.6, "text": " There are some very nice things like pattern matching, binary pattern matching.", "tokens": [51410, 821, 366, 512, 588, 1481, 721, 411, 5102, 14324, 11, 17434, 5102, 14324, 13, 51752], "temperature": 0.0, "avg_logprob": -0.26343315526058797, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.002351969014853239}, {"id": 36, "seek": 16960, "start": 169.72, "end": 173.96, "text": " Actually, this is an example of the Ranch library,", "tokens": [50370, 5135, 11, 341, 307, 364, 1365, 295, 264, 37740, 6405, 11, 50582], "temperature": 0.0, "avg_logprob": -0.24387299097501314, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.00388185097835958}, {"id": 37, "seek": 16960, "start": 173.96, "end": 177.88, "text": " which is a framework for handling TCP connections.", "tokens": [50582, 597, 307, 257, 8388, 337, 13175, 48965, 9271, 13, 50778], "temperature": 0.0, "avg_logprob": -0.24387299097501314, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.00388185097835958}, {"id": 38, "seek": 16960, "start": 177.88, "end": 184.16, "text": " As you can see, there are things like tuples, lists, etc., etc., supervisors.", "tokens": [50778, 1018, 291, 393, 536, 11, 456, 366, 721, 411, 2604, 2622, 11, 14511, 11, 5183, 7933, 5183, 7933, 42218, 13, 51092], "temperature": 0.0, "avg_logprob": -0.24387299097501314, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.00388185097835958}, {"id": 39, "seek": 16960, "start": 184.16, "end": 186.51999999999998, "text": " I'll talk about this later on.", "tokens": [51092, 286, 603, 751, 466, 341, 1780, 322, 13, 51210], "temperature": 0.0, "avg_logprob": -0.24387299097501314, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.00388185097835958}, {"id": 40, "seek": 16960, "start": 186.51999999999998, "end": 191.04, "text": " And it's an example of a behavior, of an application.", "tokens": [51210, 400, 309, 311, 364, 1365, 295, 257, 5223, 11, 295, 364, 3861, 13, 51436], "temperature": 0.0, "avg_logprob": -0.24387299097501314, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.00388185097835958}, {"id": 41, "seek": 16960, "start": 191.04, "end": 193.32, "text": " I'll talk about this later on, too.", "tokens": [51436, 286, 603, 751, 466, 341, 1780, 322, 11, 886, 13, 51550], "temperature": 0.0, "avg_logprob": -0.24387299097501314, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.00388185097835958}, {"id": 42, "seek": 16960, "start": 193.32, "end": 194.92, "text": " There are macros in Erlang.", "tokens": [51550, 821, 366, 7912, 2635, 294, 3300, 25241, 13, 51630], "temperature": 0.0, "avg_logprob": -0.24387299097501314, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.00388185097835958}, {"id": 43, "seek": 16960, "start": 194.92, "end": 197.4, "text": " There are preposers based.", "tokens": [51630, 821, 366, 2666, 329, 433, 2361, 13, 51754], "temperature": 0.0, "avg_logprob": -0.24387299097501314, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.00388185097835958}, {"id": 44, "seek": 19740, "start": 197.52, "end": 200.32, "text": " And this leads to Elixir,", "tokens": [50370, 400, 341, 6689, 281, 2699, 970, 347, 11, 50510], "temperature": 0.0, "avg_logprob": -0.2484695270497312, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.0025534769520163536}, {"id": 45, "seek": 19740, "start": 200.32, "end": 205.44, "text": " another language in which macros are first class, first of all.", "tokens": [50510, 1071, 2856, 294, 597, 7912, 2635, 366, 700, 1508, 11, 700, 295, 439, 13, 50766], "temperature": 0.0, "avg_logprob": -0.2484695270497312, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.0025534769520163536}, {"id": 46, "seek": 19740, "start": 205.44, "end": 208.08, "text": " And Elixir is much more recent.", "tokens": [50766, 400, 2699, 970, 347, 307, 709, 544, 5162, 13, 50898], "temperature": 0.0, "avg_logprob": -0.2484695270497312, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.0025534769520163536}, {"id": 47, "seek": 19740, "start": 208.08, "end": 212.12, "text": " Some say it triggered a renaissance of the beam,", "tokens": [50898, 2188, 584, 309, 21710, 257, 319, 629, 14431, 295, 264, 14269, 11, 51100], "temperature": 0.0, "avg_logprob": -0.2484695270497312, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.0025534769520163536}, {"id": 48, "seek": 19740, "start": 212.12, "end": 213.56, "text": " of interest in the beam,", "tokens": [51100, 295, 1179, 294, 264, 14269, 11, 51172], "temperature": 0.0, "avg_logprob": -0.2484695270497312, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.0025534769520163536}, {"id": 49, "seek": 19740, "start": 213.56, "end": 217.08, "text": " because of its ruby-like syntax,", "tokens": [51172, 570, 295, 1080, 5915, 88, 12, 4092, 28431, 11, 51348], "temperature": 0.0, "avg_logprob": -0.2484695270497312, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.0025534769520163536}, {"id": 50, "seek": 19740, "start": 217.08, "end": 221.20000000000002, "text": " which is, in my opinion, nicer than Prologue,", "tokens": [51348, 597, 307, 11, 294, 452, 4800, 11, 22842, 813, 1705, 4987, 622, 11, 51554], "temperature": 0.0, "avg_logprob": -0.2484695270497312, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.0025534769520163536}, {"id": 51, "seek": 19740, "start": 221.20000000000002, "end": 222.64000000000001, "text": " based on one.", "tokens": [51554, 2361, 322, 472, 13, 51626], "temperature": 0.0, "avg_logprob": -0.2484695270497312, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.0025534769520163536}, {"id": 52, "seek": 22264, "start": 222.64, "end": 227.51999999999998, "text": " But I don't say so for everyone.", "tokens": [50364, 583, 286, 500, 380, 584, 370, 337, 1518, 13, 50608], "temperature": 0.0, "avg_logprob": -0.2823421330127901, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0024324278347194195}, {"id": 53, "seek": 22264, "start": 227.51999999999998, "end": 230.72, "text": " This is an example of another library,", "tokens": [50608, 639, 307, 364, 1365, 295, 1071, 6405, 11, 50768], "temperature": 0.0, "avg_logprob": -0.2823421330127901, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0024324278347194195}, {"id": 54, "seek": 22264, "start": 230.72, "end": 234.04, "text": " and it's under for connections to a kind of database.", "tokens": [50768, 293, 309, 311, 833, 337, 9271, 281, 257, 733, 295, 8149, 13, 50934], "temperature": 0.0, "avg_logprob": -0.2823421330127901, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0024324278347194195}, {"id": 55, "seek": 22264, "start": 234.04, "end": 236.2, "text": " Also, Elixir is functional,", "tokens": [50934, 2743, 11, 2699, 970, 347, 307, 11745, 11, 51042], "temperature": 0.0, "avg_logprob": -0.2823421330127901, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0024324278347194195}, {"id": 56, "seek": 22264, "start": 236.2, "end": 237.35999999999999, "text": " just like Erlang,", "tokens": [51042, 445, 411, 3300, 25241, 11, 51100], "temperature": 0.0, "avg_logprob": -0.2823421330127901, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0024324278347194195}, {"id": 57, "seek": 22264, "start": 237.35999999999999, "end": 239.95999999999998, "text": " and it's dynamically typed for now.", "tokens": [51100, 293, 309, 311, 43492, 33941, 337, 586, 13, 51230], "temperature": 0.0, "avg_logprob": -0.2823421330127901, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0024324278347194195}, {"id": 58, "seek": 22264, "start": 239.95999999999998, "end": 245.76, "text": " Because there's some work on bringing types to the Elixir compiler.", "tokens": [51230, 1436, 456, 311, 512, 589, 322, 5062, 3467, 281, 264, 2699, 970, 347, 31958, 13, 51520], "temperature": 0.0, "avg_logprob": -0.2823421330127901, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0024324278347194195}, {"id": 59, "seek": 22264, "start": 245.76, "end": 248.48, "text": " Being a beam language,", "tokens": [51520, 8891, 257, 14269, 2856, 11, 51656], "temperature": 0.0, "avg_logprob": -0.2823421330127901, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0024324278347194195}, {"id": 60, "seek": 22264, "start": 248.48, "end": 250.04, "text": " it also has pattern matching,", "tokens": [51656, 309, 611, 575, 5102, 14324, 11, 51734], "temperature": 0.0, "avg_logprob": -0.2823421330127901, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0024324278347194195}, {"id": 61, "seek": 22264, "start": 250.04, "end": 251.64, "text": " also binary pattern matching,", "tokens": [51734, 611, 17434, 5102, 14324, 11, 51814], "temperature": 0.0, "avg_logprob": -0.2823421330127901, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0024324278347194195}, {"id": 62, "seek": 25164, "start": 251.64, "end": 254.35999999999999, "text": " and macros, as I said, are first class.", "tokens": [50364, 293, 7912, 2635, 11, 382, 286, 848, 11, 366, 700, 1508, 13, 50500], "temperature": 0.0, "avg_logprob": -0.32609918382432723, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.004273153841495514}, {"id": 63, "seek": 25164, "start": 254.35999999999999, "end": 257.64, "text": " And then there's the cool new kid, Glim.", "tokens": [50500, 400, 550, 456, 311, 264, 1627, 777, 1636, 11, 460, 4197, 13, 50664], "temperature": 0.0, "avg_logprob": -0.32609918382432723, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.004273153841495514}, {"id": 64, "seek": 25164, "start": 257.64, "end": 261.24, "text": " I see many people rejoicing.", "tokens": [50664, 286, 536, 867, 561, 22087, 5776, 13, 50844], "temperature": 0.0, "avg_logprob": -0.32609918382432723, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.004273153841495514}, {"id": 65, "seek": 25164, "start": 261.24, "end": 269.08, "text": " The syntax, it's a strange mix between an ML language and a beam language.", "tokens": [50844, 440, 28431, 11, 309, 311, 257, 5861, 2890, 1296, 364, 21601, 2856, 293, 257, 14269, 2856, 13, 51236], "temperature": 0.0, "avg_logprob": -0.32609918382432723, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.004273153841495514}, {"id": 66, "seek": 25164, "start": 269.08, "end": 271.84, "text": " Its syntax is a bit inspired by Rust,", "tokens": [51236, 6953, 28431, 307, 257, 857, 7547, 538, 34952, 11, 51374], "temperature": 0.0, "avg_logprob": -0.32609918382432723, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.004273153841495514}, {"id": 67, "seek": 25164, "start": 271.84, "end": 275.36, "text": " but the most important thing is that it's statically typed,", "tokens": [51374, 457, 264, 881, 1021, 551, 307, 300, 309, 311, 2219, 984, 33941, 11, 51550], "temperature": 0.0, "avg_logprob": -0.32609918382432723, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.004273153841495514}, {"id": 68, "seek": 25164, "start": 275.36, "end": 276.91999999999996, "text": " rather than dynamically.", "tokens": [51550, 2831, 813, 43492, 13, 51628], "temperature": 0.0, "avg_logprob": -0.32609918382432723, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.004273153841495514}, {"id": 69, "seek": 27692, "start": 277.04, "end": 280.16, "text": " And it also has a philosophy,", "tokens": [50370, 400, 309, 611, 575, 257, 10675, 11, 50526], "temperature": 0.0, "avg_logprob": -0.228118896484375, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.007486991584300995}, {"id": 70, "seek": 27692, "start": 280.16, "end": 285.68, "text": " being a statically type of handling some errors before they happen,", "tokens": [50526, 885, 257, 2219, 984, 2010, 295, 13175, 512, 13603, 949, 436, 1051, 11, 50802], "temperature": 0.0, "avg_logprob": -0.228118896484375, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.007486991584300995}, {"id": 71, "seek": 27692, "start": 285.68, "end": 288.68, "text": " rather than just doing it later on.", "tokens": [50802, 2831, 813, 445, 884, 309, 1780, 322, 13, 50952], "temperature": 0.0, "avg_logprob": -0.228118896484375, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.007486991584300995}, {"id": 72, "seek": 27692, "start": 288.68, "end": 290.68, "text": " And it can also compile to JavaScript.", "tokens": [50952, 400, 309, 393, 611, 31413, 281, 15778, 13, 51052], "temperature": 0.0, "avg_logprob": -0.228118896484375, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.007486991584300995}, {"id": 73, "seek": 27692, "start": 290.68, "end": 292.96000000000004, "text": " As I say, it is the cool new kid,", "tokens": [51052, 1018, 286, 584, 11, 309, 307, 264, 1627, 777, 1636, 11, 51166], "temperature": 0.0, "avg_logprob": -0.228118896484375, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.007486991584300995}, {"id": 74, "seek": 27692, "start": 292.96000000000004, "end": 294.84000000000003, "text": " so it has to compile to JavaScript.", "tokens": [51166, 370, 309, 575, 281, 31413, 281, 15778, 13, 51260], "temperature": 0.0, "avg_logprob": -0.228118896484375, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.007486991584300995}, {"id": 75, "seek": 27692, "start": 294.84000000000003, "end": 301.84000000000003, "text": " But then, as I said,", "tokens": [51260, 583, 550, 11, 382, 286, 848, 11, 51610], "temperature": 0.0, "avg_logprob": -0.228118896484375, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.007486991584300995}, {"id": 76, "seek": 27692, "start": 301.84000000000003, "end": 306.24, "text": " there are a number of other friends of the beam.", "tokens": [51610, 456, 366, 257, 1230, 295, 661, 1855, 295, 264, 14269, 13, 51830], "temperature": 0.0, "avg_logprob": -0.228118896484375, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.007486991584300995}, {"id": 77, "seek": 30624, "start": 306.24, "end": 310.76, "text": " Least flavored Erlang, if you really like parenthesis.", "tokens": [50364, 1456, 525, 37261, 3300, 25241, 11, 498, 291, 534, 411, 23350, 9374, 13, 50590], "temperature": 0.0, "avg_logprob": -0.28378056117466516, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.005841780453920364}, {"id": 78, "seek": 30624, "start": 310.76, "end": 317.16, "text": " Pure Erl, if you're an Askel guy who wants to run on Erlang for some reason.", "tokens": [50590, 29474, 3300, 75, 11, 498, 291, 434, 364, 1018, 7124, 2146, 567, 2738, 281, 1190, 322, 3300, 25241, 337, 512, 1778, 13, 50910], "temperature": 0.0, "avg_logprob": -0.28378056117466516, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.005841780453920364}, {"id": 79, "seek": 30624, "start": 317.16, "end": 321.16, "text": " Then if you're more Askel-y than Askel's guy,", "tokens": [50910, 1396, 498, 291, 434, 544, 1018, 7124, 12, 88, 813, 1018, 7124, 311, 2146, 11, 51110], "temperature": 0.0, "avg_logprob": -0.28378056117466516, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.005841780453920364}, {"id": 80, "seek": 30624, "start": 321.16, "end": 323.28000000000003, "text": " and you want to try dependent types,", "tokens": [51110, 293, 291, 528, 281, 853, 12334, 3467, 11, 51216], "temperature": 0.0, "avg_logprob": -0.28378056117466516, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.005841780453920364}, {"id": 81, "seek": 30624, "start": 323.28000000000003, "end": 327.16, "text": " you can also compile Idris2 to Erlang, if you want.", "tokens": [51216, 291, 393, 611, 31413, 11506, 5714, 17, 281, 3300, 25241, 11, 498, 291, 528, 13, 51410], "temperature": 0.0, "avg_logprob": -0.28378056117466516, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.005841780453920364}, {"id": 82, "seek": 30624, "start": 327.16, "end": 333.44, "text": " And then just a bunch of languages that the great wise man Peter Weirding did.", "tokens": [51410, 400, 550, 445, 257, 3840, 295, 8650, 300, 264, 869, 10829, 587, 6508, 492, 1271, 278, 630, 13, 51724], "temperature": 0.0, "avg_logprob": -0.28378056117466516, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.005841780453920364}, {"id": 83, "seek": 33344, "start": 333.44, "end": 336.76, "text": " So, Braggful, which is a PHP compiling to Erlang,", "tokens": [50364, 407, 11, 4991, 1615, 906, 11, 597, 307, 257, 47298, 715, 4883, 281, 3300, 25241, 11, 50530], "temperature": 0.0, "avg_logprob": -0.23461980918019087, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.002139369258657098}, {"id": 84, "seek": 33344, "start": 336.76, "end": 340.84, "text": " Lua compiling to Erlang, and all things like that.", "tokens": [50530, 441, 4398, 715, 4883, 281, 3300, 25241, 11, 293, 439, 721, 411, 300, 13, 50734], "temperature": 0.0, "avg_logprob": -0.23461980918019087, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.002139369258657098}, {"id": 85, "seek": 33344, "start": 340.84, "end": 344.48, "text": " He just loves making beam languages.", "tokens": [50734, 634, 445, 6752, 1455, 14269, 8650, 13, 50916], "temperature": 0.0, "avg_logprob": -0.23461980918019087, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.002139369258657098}, {"id": 86, "seek": 33344, "start": 344.48, "end": 347.0, "text": " And he's a very nice guy.", "tokens": [50916, 400, 415, 311, 257, 588, 1481, 2146, 13, 51042], "temperature": 0.0, "avg_logprob": -0.23461980918019087, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.002139369258657098}, {"id": 87, "seek": 33344, "start": 347.0, "end": 352.4, "text": " So, why is people still continuing to build languages on the beam?", "tokens": [51042, 407, 11, 983, 307, 561, 920, 9289, 281, 1322, 8650, 322, 264, 14269, 30, 51312], "temperature": 0.0, "avg_logprob": -0.23461980918019087, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.002139369258657098}, {"id": 88, "seek": 33344, "start": 352.4, "end": 358.88, "text": " Because the beam has some kind of superpowers built in there.", "tokens": [51312, 1436, 264, 14269, 575, 512, 733, 295, 1687, 47953, 3094, 294, 456, 13, 51636], "temperature": 0.0, "avg_logprob": -0.23461980918019087, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.002139369258657098}, {"id": 89, "seek": 33344, "start": 358.88, "end": 361.52, "text": " Actually, let me interject for a moment.", "tokens": [51636, 5135, 11, 718, 385, 46787, 337, 257, 1623, 13, 51768], "temperature": 0.0, "avg_logprob": -0.23461980918019087, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.002139369258657098}, {"id": 90, "seek": 36152, "start": 361.59999999999997, "end": 366.15999999999997, "text": " What I'm referring to as beam is actually beam OTP,", "tokens": [50368, 708, 286, 478, 13761, 281, 382, 14269, 307, 767, 14269, 422, 16804, 11, 50596], "temperature": 0.0, "avg_logprob": -0.2939156185496937, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.005288659129291773}, {"id": 91, "seek": 36152, "start": 366.15999999999997, "end": 371.47999999999996, "text": " or as I have written, he started calling beam plus OTP.", "tokens": [50596, 420, 382, 286, 362, 3720, 11, 415, 1409, 5141, 14269, 1804, 422, 16804, 13, 50862], "temperature": 0.0, "avg_logprob": -0.2939156185496937, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.005288659129291773}, {"id": 92, "seek": 36152, "start": 371.47999999999996, "end": 372.24, "text": " What's the difference?", "tokens": [50862, 708, 311, 264, 2649, 30, 50900], "temperature": 0.0, "avg_logprob": -0.2939156185496937, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.005288659129291773}, {"id": 93, "seek": 36152, "start": 373.32, "end": 380.12, "text": " So, beam is just the bytecode VM that runs the core bytecode code.", "tokens": [50954, 407, 11, 14269, 307, 445, 264, 40846, 22332, 18038, 300, 6676, 264, 4965, 40846, 22332, 3089, 13, 51294], "temperature": 0.0, "avg_logprob": -0.2939156185496937, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.005288659129291773}, {"id": 94, "seek": 36152, "start": 380.12, "end": 382.52, "text": " And it's register-based.", "tokens": [51294, 400, 309, 311, 7280, 12, 6032, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2939156185496937, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.005288659129291773}, {"id": 95, "seek": 36152, "start": 382.52, "end": 387.84, "text": " Then there's a runtime system called ERTS,", "tokens": [51414, 1396, 456, 311, 257, 34474, 1185, 1219, 14929, 7327, 11, 51680], "temperature": 0.0, "avg_logprob": -0.2939156185496937, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.005288659129291773}, {"id": 96, "seek": 36152, "start": 387.84, "end": 389.59999999999997, "text": " the Erlang runtime system.", "tokens": [51680, 264, 3300, 25241, 34474, 1185, 13, 51768], "temperature": 0.0, "avg_logprob": -0.2939156185496937, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.005288659129291773}, {"id": 97, "seek": 38960, "start": 389.6, "end": 395.40000000000003, "text": " That handles how to make this binary code run on the beam.", "tokens": [50364, 663, 18722, 577, 281, 652, 341, 17434, 3089, 1190, 322, 264, 14269, 13, 50654], "temperature": 0.0, "avg_logprob": -0.36061458311219147, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.004461898002773523}, {"id": 98, "seek": 38960, "start": 395.40000000000003, "end": 398.0, "text": " Yeah, that's true.", "tokens": [50654, 865, 11, 300, 311, 2074, 13, 50784], "temperature": 0.0, "avg_logprob": -0.36061458311219147, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.004461898002773523}, {"id": 99, "seek": 38960, "start": 398.0, "end": 402.92, "text": " So, we have concepts, processes, synchronization,", "tokens": [50784, 407, 11, 321, 362, 10392, 11, 7555, 11, 19331, 2144, 11, 51030], "temperature": 0.0, "avg_logprob": -0.36061458311219147, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.004461898002773523}, {"id": 100, "seek": 38960, "start": 402.92, "end": 408.0, "text": " because everything in the beam is a synchronous.", "tokens": [51030, 570, 1203, 294, 264, 14269, 307, 257, 44743, 13, 51284], "temperature": 0.0, "avg_logprob": -0.36061458311219147, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.004461898002773523}, {"id": 101, "seek": 38960, "start": 408.0, "end": 414.72, "text": " And ports, ETS tables, which are used for storage of persistent data and such.", "tokens": [51284, 400, 18160, 11, 462, 7327, 8020, 11, 597, 366, 1143, 337, 6725, 295, 24315, 1412, 293, 1270, 13, 51620], "temperature": 0.0, "avg_logprob": -0.36061458311219147, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.004461898002773523}, {"id": 102, "seek": 41472, "start": 414.72, "end": 422.08000000000004, "text": " It also takes care of scheduling the processes on the beam in a preemptive fashion.", "tokens": [50364, 467, 611, 2516, 1127, 295, 29055, 264, 7555, 322, 264, 14269, 294, 257, 659, 4543, 488, 6700, 13, 50732], "temperature": 0.0, "avg_logprob": -0.19057811390269885, "compression_ratio": 1.433673469387755, "no_speech_prob": 0.0035645358730107546}, {"id": 103, "seek": 41472, "start": 422.08000000000004, "end": 425.68, "text": " And it's usually mixed, confused with the beam.", "tokens": [50732, 400, 309, 311, 2673, 7467, 11, 9019, 365, 264, 14269, 13, 50912], "temperature": 0.0, "avg_logprob": -0.19057811390269885, "compression_ratio": 1.433673469387755, "no_speech_prob": 0.0035645358730107546}, {"id": 104, "seek": 41472, "start": 425.68, "end": 432.72, "text": " So, I keep referring to beam plus ERTS as beam, as everyone is doing.", "tokens": [50912, 407, 11, 286, 1066, 13761, 281, 14269, 1804, 14929, 7327, 382, 14269, 11, 382, 1518, 307, 884, 13, 51264], "temperature": 0.0, "avg_logprob": -0.19057811390269885, "compression_ratio": 1.433673469387755, "no_speech_prob": 0.0035645358730107546}, {"id": 105, "seek": 41472, "start": 432.72, "end": 440.28000000000003, "text": " Then there's another part, very important, in the Erlang ecosystem, that's OTP.", "tokens": [51264, 1396, 456, 311, 1071, 644, 11, 588, 1021, 11, 294, 264, 3300, 25241, 11311, 11, 300, 311, 422, 16804, 13, 51642], "temperature": 0.0, "avg_logprob": -0.19057811390269885, "compression_ratio": 1.433673469387755, "no_speech_prob": 0.0035645358730107546}, {"id": 106, "seek": 44028, "start": 440.28, "end": 446.23999999999995, "text": " It's a framework that provides you with some battle-tested abstractions.", "tokens": [50364, 467, 311, 257, 8388, 300, 6417, 291, 365, 512, 4635, 12, 83, 21885, 12649, 626, 13, 50662], "temperature": 0.0, "avg_logprob": -0.21170192541078078, "compression_ratio": 1.7704081632653061, "no_speech_prob": 0.02084965445101261}, {"id": 107, "seek": 44028, "start": 446.23999999999995, "end": 450.88, "text": " I think many of us use supervisors, gen servers,", "tokens": [50662, 286, 519, 867, 295, 505, 764, 42218, 11, 1049, 15909, 11, 50894], "temperature": 0.0, "avg_logprob": -0.21170192541078078, "compression_ratio": 1.7704081632653061, "no_speech_prob": 0.02084965445101261}, {"id": 108, "seek": 44028, "start": 450.88, "end": 453.0, "text": " state machines, and things like that.", "tokens": [50894, 1785, 8379, 11, 293, 721, 411, 300, 13, 51000], "temperature": 0.0, "avg_logprob": -0.21170192541078078, "compression_ratio": 1.7704081632653061, "no_speech_prob": 0.02084965445101261}, {"id": 109, "seek": 44028, "start": 453.0, "end": 457.32, "text": " Supervisors handle what happens if a process fails.", "tokens": [51000, 4548, 4938, 830, 4813, 437, 2314, 498, 257, 1399, 18199, 13, 51216], "temperature": 0.0, "avg_logprob": -0.21170192541078078, "compression_ratio": 1.7704081632653061, "no_speech_prob": 0.02084965445101261}, {"id": 110, "seek": 44028, "start": 457.32, "end": 461.28, "text": " Gen servers are an abstraction on the concept of a server.", "tokens": [51216, 3632, 15909, 366, 364, 37765, 322, 264, 3410, 295, 257, 7154, 13, 51414], "temperature": 0.0, "avg_logprob": -0.21170192541078078, "compression_ratio": 1.7704081632653061, "no_speech_prob": 0.02084965445101261}, {"id": 111, "seek": 44028, "start": 461.28, "end": 466.4, "text": " State machines are abstractions on the concept of state machines, obviously.", "tokens": [51414, 4533, 8379, 366, 12649, 626, 322, 264, 3410, 295, 1785, 8379, 11, 2745, 13, 51670], "temperature": 0.0, "avg_logprob": -0.21170192541078078, "compression_ratio": 1.7704081632653061, "no_speech_prob": 0.02084965445101261}, {"id": 112, "seek": 46640, "start": 466.4, "end": 471.91999999999996, "text": " The great part of using the Erlang ecosystem is that all of these three", "tokens": [50364, 440, 869, 644, 295, 1228, 264, 3300, 25241, 11311, 307, 300, 439, 295, 613, 1045, 50640], "temperature": 0.0, "avg_logprob": -0.26364409923553467, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.0025589652359485626}, {"id": 113, "seek": 46640, "start": 471.91999999999996, "end": 474.84, "text": " components provide one alpha of greatness.", "tokens": [50640, 6677, 2893, 472, 8961, 295, 31196, 13, 50786], "temperature": 0.0, "avg_logprob": -0.26364409923553467, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.0025589652359485626}, {"id": 114, "seek": 46640, "start": 474.84, "end": 476.67999999999995, "text": " So, we have three albs of greatness in total.", "tokens": [50786, 407, 11, 321, 362, 1045, 419, 929, 295, 31196, 294, 3217, 13, 50878], "temperature": 0.0, "avg_logprob": -0.26364409923553467, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.0025589652359485626}, {"id": 115, "seek": 46640, "start": 478.88, "end": 482.91999999999996, "text": " So, what are the superpowers, or beam plus OTP?", "tokens": [50988, 407, 11, 437, 366, 264, 1687, 47953, 11, 420, 14269, 1804, 422, 16804, 30, 51190], "temperature": 0.0, "avg_logprob": -0.26364409923553467, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.0025589652359485626}, {"id": 116, "seek": 46640, "start": 482.91999999999996, "end": 486.08, "text": " As I've recently started talking, calling it.", "tokens": [51190, 1018, 286, 600, 3938, 1409, 1417, 11, 5141, 309, 13, 51348], "temperature": 0.0, "avg_logprob": -0.26364409923553467, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.0025589652359485626}, {"id": 117, "seek": 46640, "start": 486.08, "end": 488.32, "text": " First of all, concurrency.", "tokens": [51348, 2386, 295, 439, 11, 23702, 10457, 13, 51460], "temperature": 0.0, "avg_logprob": -0.26364409923553467, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.0025589652359485626}, {"id": 118, "seek": 46640, "start": 488.32, "end": 494.59999999999997, "text": " As I said before, the main unit of concurrency is the process.", "tokens": [51460, 1018, 286, 848, 949, 11, 264, 2135, 4985, 295, 23702, 10457, 307, 264, 1399, 13, 51774], "temperature": 0.0, "avg_logprob": -0.26364409923553467, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.0025589652359485626}, {"id": 119, "seek": 49460, "start": 495.56, "end": 500.40000000000003, "text": " The process is just a piece of sequential code that's running,", "tokens": [50412, 440, 1399, 307, 445, 257, 2522, 295, 42881, 3089, 300, 311, 2614, 11, 50654], "temperature": 0.0, "avg_logprob": -0.22365292258884595, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.003304980229586363}, {"id": 120, "seek": 49460, "start": 500.40000000000003, "end": 506.72, "text": " having his own memory, his own it, and sharing nothing with all other processes.", "tokens": [50654, 1419, 702, 1065, 4675, 11, 702, 1065, 309, 11, 293, 5414, 1825, 365, 439, 661, 7555, 13, 50970], "temperature": 0.0, "avg_logprob": -0.22365292258884595, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.003304980229586363}, {"id": 121, "seek": 49460, "start": 506.72, "end": 507.92, "text": " What's the point of it?", "tokens": [50970, 708, 311, 264, 935, 295, 309, 30, 51030], "temperature": 0.0, "avg_logprob": -0.22365292258884595, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.003304980229586363}, {"id": 122, "seek": 49460, "start": 507.92, "end": 513.6, "text": " Handling failure in just one simple and easy way,", "tokens": [51030, 8854, 1688, 7763, 294, 445, 472, 2199, 293, 1858, 636, 11, 51314], "temperature": 0.0, "avg_logprob": -0.22365292258884595, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.003304980229586363}, {"id": 123, "seek": 49460, "start": 513.6, "end": 516.12, "text": " located just in the process.", "tokens": [51314, 6870, 445, 294, 264, 1399, 13, 51440], "temperature": 0.0, "avg_logprob": -0.22365292258884595, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.003304980229586363}, {"id": 124, "seek": 49460, "start": 516.12, "end": 519.6, "text": " Meaning that when you have a crash in your process,", "tokens": [51440, 19948, 300, 562, 291, 362, 257, 8252, 294, 428, 1399, 11, 51614], "temperature": 0.0, "avg_logprob": -0.22365292258884595, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.003304980229586363}, {"id": 125, "seek": 49460, "start": 519.6, "end": 522.76, "text": " only that process will crash and not the whole application.", "tokens": [51614, 787, 300, 1399, 486, 8252, 293, 406, 264, 1379, 3861, 13, 51772], "temperature": 0.0, "avg_logprob": -0.22365292258884595, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.003304980229586363}, {"id": 126, "seek": 52276, "start": 523.76, "end": 530.24, "text": " Then, a consequence of that is that garbage collection runs on a per process basis.", "tokens": [50414, 1396, 11, 257, 18326, 295, 300, 307, 300, 14150, 5765, 6676, 322, 257, 680, 1399, 5143, 13, 50738], "temperature": 0.0, "avg_logprob": -0.3216029017804617, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.0010505623649805784}, {"id": 127, "seek": 52276, "start": 530.24, "end": 534.52, "text": " So, you don't need like in Java to stop everything just to run the garbage", "tokens": [50738, 407, 11, 291, 500, 380, 643, 411, 294, 10745, 281, 1590, 1203, 445, 281, 1190, 264, 14150, 50952], "temperature": 0.0, "avg_logprob": -0.3216029017804617, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.0010505623649805784}, {"id": 128, "seek": 52276, "start": 534.52, "end": 537.4, "text": " collector, you can run it synchronously.", "tokens": [50952, 23960, 11, 291, 393, 1190, 309, 19331, 5098, 13, 51096], "temperature": 0.0, "avg_logprob": -0.3216029017804617, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.0010505623649805784}, {"id": 129, "seek": 52276, "start": 537.4, "end": 542.96, "text": " And finally, since we do share nothing, the only way process is ever to", "tokens": [51096, 400, 2721, 11, 1670, 321, 360, 2073, 1825, 11, 264, 787, 636, 1399, 307, 1562, 281, 51374], "temperature": 0.0, "avg_logprob": -0.3216029017804617, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.0010505623649805784}, {"id": 130, "seek": 52276, "start": 542.96, "end": 548.04, "text": " communicate with each other is by sending messages or signals actually.", "tokens": [51374, 7890, 365, 1184, 661, 307, 538, 7750, 7897, 420, 12354, 767, 13, 51628], "temperature": 0.0, "avg_logprob": -0.3216029017804617, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.0010505623649805784}, {"id": 131, "seek": 54804, "start": 549.04, "end": 556.1999999999999, "text": " This allows the beam to scale seamlessly from a single node setup to a multi-node setup.", "tokens": [50414, 639, 4045, 264, 14269, 281, 4373, 38083, 490, 257, 2167, 9984, 8657, 281, 257, 4825, 12, 77, 1429, 8657, 13, 50772], "temperature": 0.0, "avg_logprob": -0.3217022927959314, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004468467552214861}, {"id": 132, "seek": 54804, "start": 556.1999999999999, "end": 559.92, "text": " You just rather than sending it to a local process,", "tokens": [50772, 509, 445, 2831, 813, 7750, 309, 281, 257, 2654, 1399, 11, 50958], "temperature": 0.0, "avg_logprob": -0.3217022927959314, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004468467552214861}, {"id": 133, "seek": 54804, "start": 559.92, "end": 563.92, "text": " you send a message to a process in a different beam node.", "tokens": [50958, 291, 2845, 257, 3636, 281, 257, 1399, 294, 257, 819, 14269, 9984, 13, 51158], "temperature": 0.0, "avg_logprob": -0.3217022927959314, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004468467552214861}, {"id": 134, "seek": 54804, "start": 563.92, "end": 570.3199999999999, "text": " Then, this leads to the let it crash philosophy of beam and OTP.", "tokens": [51158, 1396, 11, 341, 6689, 281, 264, 718, 309, 8252, 10675, 295, 14269, 293, 422, 16804, 13, 51478], "temperature": 0.0, "avg_logprob": -0.3217022927959314, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004468467552214861}, {"id": 135, "seek": 54804, "start": 570.3199999999999, "end": 577.04, "text": " Since every part in the end is just a tree of supervisors, a supervision tree.", "tokens": [51478, 4162, 633, 644, 294, 264, 917, 307, 445, 257, 4230, 295, 42218, 11, 257, 32675, 4230, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3217022927959314, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004468467552214861}, {"id": 136, "seek": 57704, "start": 578.04, "end": 583.04, "text": " You can also obviously propagate failure between processes so", "tokens": [50414, 509, 393, 611, 2745, 48256, 7763, 1296, 7555, 370, 50664], "temperature": 0.0, "avg_logprob": -0.312167485555013, "compression_ratio": 1.5197740112994351, "no_speech_prob": 0.00966621097177267}, {"id": 137, "seek": 57704, "start": 583.04, "end": 591.04, "text": " that you can handle doing the right thing depending on what process is crashed.", "tokens": [50664, 300, 291, 393, 4813, 884, 264, 558, 551, 5413, 322, 437, 1399, 307, 24190, 13, 51064], "temperature": 0.0, "avg_logprob": -0.312167485555013, "compression_ratio": 1.5197740112994351, "no_speech_prob": 0.00966621097177267}, {"id": 138, "seek": 57704, "start": 591.04, "end": 599.04, "text": " But why in this world would you need supervisors?", "tokens": [51064, 583, 983, 294, 341, 1002, 576, 291, 643, 42218, 30, 51464], "temperature": 0.0, "avg_logprob": -0.312167485555013, "compression_ratio": 1.5197740112994351, "no_speech_prob": 0.00966621097177267}, {"id": 139, "seek": 57704, "start": 599.04, "end": 604.04, "text": " Since we have Kubernetes that almost seem to do the right and the same thing.", "tokens": [51464, 4162, 321, 362, 23145, 300, 1920, 1643, 281, 360, 264, 558, 293, 264, 912, 551, 13, 51714], "temperature": 0.0, "avg_logprob": -0.312167485555013, "compression_ratio": 1.5197740112994351, "no_speech_prob": 0.00966621097177267}, {"id": 140, "seek": 60404, "start": 604.04, "end": 607.04, "text": " Restarting our pod if something is not working.", "tokens": [50364, 13094, 446, 278, 527, 2497, 498, 746, 307, 406, 1364, 13, 50514], "temperature": 0.0, "avg_logprob": -0.12656500253332667, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.02806497924029827}, {"id": 141, "seek": 60404, "start": 607.04, "end": 612.04, "text": " Well, the main idea is that when if a network connection goes down,", "tokens": [50514, 1042, 11, 264, 2135, 1558, 307, 300, 562, 498, 257, 3209, 4984, 1709, 760, 11, 50764], "temperature": 0.0, "avg_logprob": -0.12656500253332667, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.02806497924029827}, {"id": 142, "seek": 60404, "start": 612.04, "end": 615.04, "text": " you don't want to restart your application.", "tokens": [50764, 291, 500, 380, 528, 281, 21022, 428, 3861, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12656500253332667, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.02806497924029827}, {"id": 143, "seek": 60404, "start": 615.04, "end": 617.04, "text": " So, those are two different layers.", "tokens": [50914, 407, 11, 729, 366, 732, 819, 7914, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12656500253332667, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.02806497924029827}, {"id": 144, "seek": 60404, "start": 617.04, "end": 625.04, "text": " OTP focuses on the application layer, handling failures in your domain.", "tokens": [51014, 422, 16804, 16109, 322, 264, 3861, 4583, 11, 13175, 20774, 294, 428, 9274, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12656500253332667, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.02806497924029827}, {"id": 145, "seek": 60404, "start": 625.04, "end": 630.04, "text": " And then Kubernetes focuses on the larger aspect of orchestration.", "tokens": [51414, 400, 550, 23145, 16109, 322, 264, 4833, 4171, 295, 14161, 2405, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12656500253332667, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.02806497924029827}, {"id": 146, "seek": 63004, "start": 630.04, "end": 638.04, "text": " So, you don't want to crash, as I said before, if a network question is going down.", "tokens": [50364, 407, 11, 291, 500, 380, 528, 281, 8252, 11, 382, 286, 848, 949, 11, 498, 257, 3209, 1168, 307, 516, 760, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16195232727948358, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.004834363237023354}, {"id": 147, "seek": 63004, "start": 638.04, "end": 645.04, "text": " You just need to handle that in your application and you restart the pod or your deployment", "tokens": [50764, 509, 445, 643, 281, 4813, 300, 294, 428, 3861, 293, 291, 21022, 264, 2497, 420, 428, 19317, 51114], "temperature": 0.0, "avg_logprob": -0.16195232727948358, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.004834363237023354}, {"id": 148, "seek": 63004, "start": 645.04, "end": 651.04, "text": " just in very specific cases or very tragic cases.", "tokens": [51114, 445, 294, 588, 2685, 3331, 420, 588, 20385, 3331, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16195232727948358, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.004834363237023354}, {"id": 149, "seek": 63004, "start": 651.04, "end": 653.04, "text": " That's where Kubernetes comes into the rescue.", "tokens": [51414, 663, 311, 689, 23145, 1487, 666, 264, 13283, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16195232727948358, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.004834363237023354}, {"id": 150, "seek": 65304, "start": 654.04, "end": 661.04, "text": " Being application-based, the supervision trees are obviously also more granular.", "tokens": [50414, 8891, 3861, 12, 6032, 11, 264, 32675, 5852, 366, 2745, 611, 544, 39962, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1215243841472425, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.00808396004140377}, {"id": 151, "seek": 65304, "start": 661.04, "end": 670.04, "text": " So, that you can define a different strategy rather than just turning it on when the process crashed.", "tokens": [50764, 407, 11, 300, 291, 393, 6964, 257, 819, 5206, 2831, 813, 445, 6246, 309, 322, 562, 264, 1399, 24190, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1215243841472425, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.00808396004140377}, {"id": 152, "seek": 65304, "start": 670.04, "end": 676.04, "text": " So, it provides for your application a more flexible way to handle crashes.", "tokens": [51214, 407, 11, 309, 6417, 337, 428, 3861, 257, 544, 11358, 636, 281, 4813, 28642, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1215243841472425, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.00808396004140377}, {"id": 153, "seek": 67604, "start": 677.04, "end": 679.04, "text": " Kubernetes does not.", "tokens": [50414, 23145, 775, 406, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2113989468278556, "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.013094881549477577}, {"id": 154, "seek": 67604, "start": 679.04, "end": 685.04, "text": " It just handles networking, bring up and down pods, and that's it.", "tokens": [50514, 467, 445, 18722, 17985, 11, 1565, 493, 293, 760, 31925, 11, 293, 300, 311, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2113989468278556, "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.013094881549477577}, {"id": 155, "seek": 67604, "start": 685.04, "end": 692.04, "text": " Those are different level orchestrating containers is a different level than handling failures in your application.", "tokens": [50814, 3950, 366, 819, 1496, 14161, 8754, 17089, 307, 257, 819, 1496, 813, 13175, 20774, 294, 428, 3861, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2113989468278556, "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.013094881549477577}, {"id": 156, "seek": 67604, "start": 692.04, "end": 697.04, "text": " Next superpower of being a plus OTP is immutability.", "tokens": [51164, 3087, 45765, 295, 885, 257, 1804, 422, 16804, 307, 3397, 325, 2310, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2113989468278556, "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.013094881549477577}, {"id": 157, "seek": 67604, "start": 697.04, "end": 704.04, "text": " It didn't seem so when it was built, but now we see its value together with a share nothing concurrency.", "tokens": [51414, 467, 994, 380, 1643, 370, 562, 309, 390, 3094, 11, 457, 586, 321, 536, 1080, 2158, 1214, 365, 257, 2073, 1825, 23702, 10457, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2113989468278556, "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.013094881549477577}, {"id": 158, "seek": 70404, "start": 705.04, "end": 714.04, "text": " Because having no shared memory means that the processes cannot change the state of under process unexpectedly.", "tokens": [50414, 1436, 1419, 572, 5507, 4675, 1355, 300, 264, 7555, 2644, 1319, 264, 1785, 295, 833, 1399, 40452, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15048284788389463, "compression_ratio": 1.4954545454545454, "no_speech_prob": 0.008958190679550171}, {"id": 159, "seek": 70404, "start": 714.04, "end": 719.04, "text": " And that's the reason why we also have immutability of data structures.", "tokens": [50864, 400, 300, 311, 264, 1778, 983, 321, 611, 362, 3397, 325, 2310, 295, 1412, 9227, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15048284788389463, "compression_ratio": 1.4954545454545454, "no_speech_prob": 0.008958190679550171}, {"id": 160, "seek": 70404, "start": 719.04, "end": 723.04, "text": " This also leads to a referential transparency.", "tokens": [51114, 639, 611, 6689, 281, 257, 2864, 2549, 17131, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15048284788389463, "compression_ratio": 1.4954545454545454, "no_speech_prob": 0.008958190679550171}, {"id": 161, "seek": 70404, "start": 723.04, "end": 733.04, "text": " Even if the beam allows you to have some kind of side effects, for example, logging or networking,", "tokens": [51314, 2754, 498, 264, 14269, 4045, 291, 281, 362, 512, 733, 295, 1252, 5065, 11, 337, 1365, 11, 27991, 420, 17985, 11, 51814], "temperature": 0.0, "avg_logprob": -0.15048284788389463, "compression_ratio": 1.4954545454545454, "no_speech_prob": 0.008958190679550171}, {"id": 162, "seek": 73304, "start": 733.04, "end": 740.04, "text": " it's just a pragmatic way of handling it rather than setting up a whole monad stack, for example.", "tokens": [50364, 309, 311, 445, 257, 46904, 636, 295, 13175, 309, 2831, 813, 3287, 493, 257, 1379, 1108, 345, 8630, 11, 337, 1365, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1467652936135569, "compression_ratio": 1.4069767441860466, "no_speech_prob": 0.005804205313324928}, {"id": 163, "seek": 73304, "start": 743.04, "end": 746.04, "text": " Then, final superpower distribution.", "tokens": [50864, 1396, 11, 2572, 45765, 7316, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1467652936135569, "compression_ratio": 1.4069767441860466, "no_speech_prob": 0.005804205313324928}, {"id": 164, "seek": 73304, "start": 746.04, "end": 756.04, "text": " As I said before, from the point of view of an application, it's the same to pass messages to a local node,", "tokens": [51014, 1018, 286, 848, 949, 11, 490, 264, 935, 295, 1910, 295, 364, 3861, 11, 309, 311, 264, 912, 281, 1320, 7897, 281, 257, 2654, 9984, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1467652936135569, "compression_ratio": 1.4069767441860466, "no_speech_prob": 0.005804205313324928}, {"id": 165, "seek": 75604, "start": 757.04, "end": 764.04, "text": " a node in the same virtual machine or in another node that is running maybe in another part of the world.", "tokens": [50414, 257, 9984, 294, 264, 912, 6374, 3479, 420, 294, 1071, 9984, 300, 307, 2614, 1310, 294, 1071, 644, 295, 264, 1002, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14990427407873683, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.04274536296725273}, {"id": 166, "seek": 75604, "start": 764.04, "end": 770.04, "text": " This allows you to distribute the work efficiently, seamlessly as a programmer.", "tokens": [50764, 639, 4045, 291, 281, 20594, 264, 589, 19621, 11, 38083, 382, 257, 32116, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14990427407873683, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.04274536296725273}, {"id": 167, "seek": 75604, "start": 770.04, "end": 775.04, "text": " And this is made by a built-in protocol to discover nodes.", "tokens": [51064, 400, 341, 307, 1027, 538, 257, 3094, 12, 259, 10336, 281, 4411, 13891, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14990427407873683, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.04274536296725273}, {"id": 168, "seek": 75604, "start": 775.04, "end": 780.04, "text": " And you can scale horizontally however you want.", "tokens": [51314, 400, 291, 393, 4373, 33796, 4461, 291, 528, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14990427407873683, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.04274536296725273}, {"id": 169, "seek": 75604, "start": 780.04, "end": 784.04, "text": " The code that you wrote will just work most of the time.", "tokens": [51564, 440, 3089, 300, 291, 4114, 486, 445, 589, 881, 295, 264, 565, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14990427407873683, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.04274536296725273}, {"id": 170, "seek": 78604, "start": 787.04, "end": 798.04, "text": " Finally, since those things run in production ever since, there's a huge interest in observability and debuggability.", "tokens": [50414, 6288, 11, 1670, 729, 721, 1190, 294, 4265, 1562, 1670, 11, 456, 311, 257, 2603, 1179, 294, 9951, 2310, 293, 3001, 3562, 2310, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1296027877114036, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.0023373819421976805}, {"id": 171, "seek": 78604, "start": 798.04, "end": 808.04, "text": " The most easy thing to do is connect to your live application and start an interactive shell just to see what's going wrong.", "tokens": [50964, 440, 881, 1858, 551, 281, 360, 307, 1745, 281, 428, 1621, 3861, 293, 722, 364, 15141, 8720, 445, 281, 536, 437, 311, 516, 2085, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1296027877114036, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.0023373819421976805}, {"id": 172, "seek": 80804, "start": 809.04, "end": 810.04, "text": " You can trace.", "tokens": [50414, 509, 393, 13508, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1724792845705722, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.025169875472784042}, {"id": 173, "seek": 80804, "start": 810.04, "end": 817.04, "text": " There's an interesting tool called tracing later on, and access all the information of a process and so on and so forth.", "tokens": [50464, 821, 311, 364, 1880, 2290, 1219, 25262, 1780, 322, 11, 293, 2105, 439, 264, 1589, 295, 257, 1399, 293, 370, 322, 293, 370, 5220, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1724792845705722, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.025169875472784042}, {"id": 174, "seek": 80804, "start": 817.04, "end": 822.04, "text": " Tracing and profiling, as I said, are built-in into the machine.", "tokens": [50814, 1765, 5615, 293, 1740, 4883, 11, 382, 286, 848, 11, 366, 3094, 12, 259, 666, 264, 3479, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1724792845705722, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.025169875472784042}, {"id": 175, "seek": 80804, "start": 822.04, "end": 828.04, "text": " They are battle tested by running in production for years.", "tokens": [51064, 814, 366, 4635, 8246, 538, 2614, 294, 4265, 337, 924, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1724792845705722, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.025169875472784042}, {"id": 176, "seek": 80804, "start": 828.04, "end": 837.04, "text": " If anybody uses ReContrace, it's very nice, but can anyone give me a match spec that works?", "tokens": [51364, 759, 4472, 4960, 1300, 29821, 81, 617, 11, 309, 311, 588, 1481, 11, 457, 393, 2878, 976, 385, 257, 2995, 1608, 300, 1985, 30, 51814], "temperature": 0.0, "avg_logprob": -0.1724792845705722, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.025169875472784042}, {"id": 177, "seek": 83704, "start": 838.04, "end": 840.04, "text": " I never found a way to make it work.", "tokens": [50414, 286, 1128, 1352, 257, 636, 281, 652, 309, 589, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11760465995125148, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.003235848853364587}, {"id": 178, "seek": 83704, "start": 840.04, "end": 844.04, "text": " And let's not forget hot-code reloading.", "tokens": [50514, 400, 718, 311, 406, 2870, 2368, 12, 22332, 25628, 278, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11760465995125148, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.003235848853364587}, {"id": 179, "seek": 83704, "start": 844.04, "end": 846.04, "text": " It's really interesting.", "tokens": [50714, 467, 311, 534, 1880, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11760465995125148, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.003235848853364587}, {"id": 180, "seek": 83704, "start": 846.04, "end": 853.04, "text": " You can change your code while it is running so that, for example, if you have detected a bug in your live application,", "tokens": [50814, 509, 393, 1319, 428, 3089, 1339, 309, 307, 2614, 370, 300, 11, 337, 1365, 11, 498, 291, 362, 21896, 257, 7426, 294, 428, 1621, 3861, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11760465995125148, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.003235848853364587}, {"id": 181, "seek": 83704, "start": 853.04, "end": 859.04, "text": " you can just fix it on the fly and your application will just work with the new fix.", "tokens": [51164, 291, 393, 445, 3191, 309, 322, 264, 3603, 293, 428, 3861, 486, 445, 589, 365, 264, 777, 3191, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11760465995125148, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.003235848853364587}, {"id": 182, "seek": 83704, "start": 859.04, "end": 863.04, "text": " Then there are other interesting aspects.", "tokens": [51464, 1396, 456, 366, 661, 1880, 7270, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11760465995125148, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.003235848853364587}, {"id": 183, "seek": 86304, "start": 863.04, "end": 871.04, "text": " The first one, pattern matching, is more interesting from one who is writing programming languages.", "tokens": [50364, 440, 700, 472, 11, 5102, 14324, 11, 307, 544, 1880, 490, 472, 567, 307, 3579, 9410, 8650, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1560586190992786, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.0057142749428749084}, {"id": 184, "seek": 86304, "start": 871.04, "end": 882.04, "text": " The BIM makes it really easy to write functional languages with pattern matching, also at binary level.", "tokens": [50764, 440, 363, 6324, 1669, 309, 534, 1858, 281, 2464, 11745, 8650, 365, 5102, 14324, 11, 611, 412, 17434, 1496, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1560586190992786, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.0057142749428749084}, {"id": 185, "seek": 86304, "start": 882.04, "end": 889.04, "text": " Then BIM is already container aware since a number of years, I think,", "tokens": [51314, 1396, 363, 6324, 307, 1217, 10129, 3650, 1670, 257, 1230, 295, 924, 11, 286, 519, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1560586190992786, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.0057142749428749084}, {"id": 186, "seek": 88904, "start": 889.04, "end": 898.04, "text": " allowing you to use C-groups and Kubernetes, obviously, seamlessly, not what Java does, for example.", "tokens": [50364, 8293, 291, 281, 764, 383, 12, 17377, 82, 293, 23145, 11, 2745, 11, 38083, 11, 406, 437, 10745, 775, 11, 337, 1365, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2436415918411747, "compression_ratio": 1.4096385542168675, "no_speech_prob": 0.012497493997216225}, {"id": 187, "seek": 88904, "start": 898.04, "end": 904.04, "text": " It makes it easy to have foreign function codes.", "tokens": [50814, 467, 1669, 309, 1858, 281, 362, 5329, 2445, 14211, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2436415918411747, "compression_ratio": 1.4096385542168675, "no_speech_prob": 0.012497493997216225}, {"id": 188, "seek": 88904, "start": 904.04, "end": 916.04, "text": " You just write a C-node that seems to be an orang node and just communicate with it.", "tokens": [51114, 509, 445, 2464, 257, 383, 12, 77, 1429, 300, 2544, 281, 312, 364, 17481, 9984, 293, 445, 7890, 365, 309, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2436415918411747, "compression_ratio": 1.4096385542168675, "no_speech_prob": 0.012497493997216225}, {"id": 189, "seek": 91604, "start": 916.04, "end": 920.04, "text": " Or you can use the FFI built-in.", "tokens": [50364, 1610, 291, 393, 764, 264, 479, 38568, 3094, 12, 259, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13345923352597364, "compression_ratio": 1.4316939890710383, "no_speech_prob": 0.01322874240577221}, {"id": 190, "seek": 91604, "start": 920.04, "end": 922.04, "text": " What's the future for the BIM?", "tokens": [50564, 708, 311, 264, 2027, 337, 264, 363, 6324, 30, 50664], "temperature": 0.0, "avg_logprob": -0.13345923352597364, "compression_ratio": 1.4316939890710383, "no_speech_prob": 0.01322874240577221}, {"id": 191, "seek": 91604, "start": 922.04, "end": 928.04, "text": " Obviously, the first part is being more and more largely adopted.", "tokens": [50664, 7580, 11, 264, 700, 644, 307, 885, 544, 293, 544, 11611, 12175, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13345923352597364, "compression_ratio": 1.4316939890710383, "no_speech_prob": 0.01322874240577221}, {"id": 192, "seek": 91604, "start": 928.04, "end": 940.04, "text": " And then there are some interesting research level developments on bringing gradual typing to some languages like Elixir and Erlang.", "tokens": [50964, 400, 550, 456, 366, 512, 1880, 2132, 1496, 20862, 322, 5062, 32890, 18444, 281, 512, 8650, 411, 2699, 970, 347, 293, 3300, 25241, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13345923352597364, "compression_ratio": 1.4316939890710383, "no_speech_prob": 0.01322874240577221}, {"id": 193, "seek": 94004, "start": 940.04, "end": 952.04, "text": " Also, Elixir, but a number of languages, are doing a pretty huge job of developing numerical computing and AI on BIM nodes", "tokens": [50364, 2743, 11, 2699, 970, 347, 11, 457, 257, 1230, 295, 8650, 11, 366, 884, 257, 1238, 2603, 1691, 295, 6416, 29054, 15866, 293, 7318, 322, 363, 6324, 13891, 50964], "temperature": 0.0, "avg_logprob": -0.08831613491743039, "compression_ratio": 1.4573991031390134, "no_speech_prob": 0.015506954863667488}, {"id": 194, "seek": 94004, "start": 952.04, "end": 958.04, "text": " in order to distribute those calculations in an easier way.", "tokens": [50964, 294, 1668, 281, 20594, 729, 20448, 294, 364, 3571, 636, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08831613491743039, "compression_ratio": 1.4573991031390134, "no_speech_prob": 0.015506954863667488}, {"id": 195, "seek": 94004, "start": 958.04, "end": 968.04, "text": " Obviously, there's also work on embedded systems, like bringing, for example, a small instance of the BIM vehicle machine on microcontrollers.", "tokens": [51264, 7580, 11, 456, 311, 611, 589, 322, 16741, 3652, 11, 411, 5062, 11, 337, 1365, 11, 257, 1359, 5197, 295, 264, 363, 6324, 5864, 3479, 322, 4532, 9000, 3970, 433, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08831613491743039, "compression_ratio": 1.4573991031390134, "no_speech_prob": 0.015506954863667488}, {"id": 196, "seek": 96804, "start": 968.04, "end": 971.04, "text": " There's a work on that, especially at UMBM.", "tokens": [50364, 821, 311, 257, 589, 322, 300, 11, 2318, 412, 31335, 18345, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1765241398530848, "compression_ratio": 1.5544554455445545, "no_speech_prob": 0.020945383235812187}, {"id": 197, "seek": 96804, "start": 971.04, "end": 980.04, "text": " Then, the main challenge is obviously becoming a wider and wider adopted choice for the backend.", "tokens": [50514, 1396, 11, 264, 2135, 3430, 307, 2745, 5617, 257, 11842, 293, 11842, 12175, 3922, 337, 264, 38087, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1765241398530848, "compression_ratio": 1.5544554455445545, "no_speech_prob": 0.020945383235812187}, {"id": 198, "seek": 96804, "start": 980.04, "end": 986.04, "text": " And in that sense, having all these people here is a very good sign.", "tokens": [50964, 400, 294, 300, 2020, 11, 1419, 439, 613, 561, 510, 307, 257, 588, 665, 1465, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1765241398530848, "compression_ratio": 1.5544554455445545, "no_speech_prob": 0.020945383235812187}, {"id": 199, "seek": 96804, "start": 986.04, "end": 988.04, "text": " Thank you for being there.", "tokens": [51264, 1044, 291, 337, 885, 456, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1765241398530848, "compression_ratio": 1.5544554455445545, "no_speech_prob": 0.020945383235812187}, {"id": 200, "seek": 96804, "start": 988.04, "end": 991.04, "text": " And if you have any questions, feel free to answer.", "tokens": [51364, 400, 498, 291, 362, 604, 1651, 11, 841, 1737, 281, 1867, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1765241398530848, "compression_ratio": 1.5544554455445545, "no_speech_prob": 0.020945383235812187}, {"id": 201, "seek": 96804, "start": 991.04, "end": 993.04, "text": " I mean, feel free to ask.", "tokens": [51514, 286, 914, 11, 841, 1737, 281, 1029, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1765241398530848, "compression_ratio": 1.5544554455445545, "no_speech_prob": 0.020945383235812187}, {"id": 202, "seek": 99804, "start": 998.04, "end": 1000.04, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.39433133602142334, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.986035943031311}], "language": "en"}