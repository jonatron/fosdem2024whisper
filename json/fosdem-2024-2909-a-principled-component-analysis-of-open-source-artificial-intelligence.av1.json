{"text": " As Julia is getting herself up and running, I'd like to introduce her to you. She is from Seattle, so she's another American here. She is a social technical, I can't say that, systems nerd and a huge fan of Lego, so that's important to know. You used to have a Scottish accent despite being an American and never being to Scotland. Now that is fascinating, that truly is. And her favorite joke is just her own humor. Julia, take it away. I think that one landed is exactly how I intended to, so thank you. Hi everyone, it's great to be here today to talk about what I am calling a principled component analysis of open source AI. So who am I? I'm Julia. I've been focused primarily on open source resilience and the software supply chain for the past five, ten years. I feel like a little bit of an open source hipster because I've been talking about some of these things before they were cool. But I've been working in and around open source AI since undergrad, so I'm not going to leave that to your imagination. Some things will probably give it away though. So in case you were wondering, that isn't a typo. It is a pun. It is a good slash bad pun, and I hope you are ready for more. I couldn't make a pun out of support vector machines though, so if you have one, please come talk to me later. That would be good. It would be much appreciated. So open source AI is not new. I've been doing open source AI for a while now, and I threw a bunch of stuff up on this slide, mostly copying and pasting from former poster presentations, but I have a chapter in this lovely book. I believe there is only one left in stock. They haven't had much of a call to reprint it. So you too could own constrained clustering, not about neural networks. And in that chapter, we had a very interesting approach to exploring information with user feedback using a variant of the K-means algorithm. So that was the basis of our chapter. We used this fantastic open source machine learning library called Weka. Is anyone familiar with Weka here? My people, hello. Excellent. Weka is wonderful. It has so many great machine learning algorithms in it. When I first started using Weka, I went to this website called SourceForge and downloaded it. And I was just entranced. It was my first experience with open source. I was entranced by this idea that I could go and see the code. I could go and modify it. And in fact, some researchers at the University of Texas had done this and modified it and redistributed it. Just this magical thing that I could then go and use in my own research, knowing that somebody who is much better at math than I am had validated all the algorithms. I also built this lovely autonomous robot, aerial autonomous robot, that won the dubious award of innovative hardware design from AAAI, which I think means we don't know how you got this to lift off the ground. But lift it did, mostly because we made it a giant dodecahedron to lift all of the camera components. And a few years ago, I used machine learning to tackle one of the world's hardest problems. Determining whether or not you should hug something. So I trained a little model when Fed and Image would tell you if it was a good idea to hug it or not. I am sad to say I am not huggable, apparently. I am not as mathematically proven. So, you know, maybe I should take my picture again, try it out. So that's me. So the bad news here is that I don't have any answers for any of you in this presentation. I only have open questions. We are not going into the deep specifics of models, algorithms, or approaches. This is going to be probably, for some of you, a little bit too high level. And for some, probably a little too low level. We are exploring this new area of technology that has ballooned kind of seemingly overnight. It feels like that to me. I don't know if that feels like that to you. And we are facing some really interesting challenges when talking about the advances that we have seen in artificial intelligence and how it intersects with open source if you are not using Weka. If you are using Weka, you are set. It's great. They are not paying me for this. I promise. So level set, like AI draws from a lot of different fields. If you go back to your AI 101 course, you are going to probably get a little bit of a survey overview of all of these different fields, from ethics to philosophy. Philosophy plays a big part in AI. Economics, my favorite part of AI is the formal logic side. But that's because statistics was never really my strong suit, which is why I love computers. So there are a lot of different considerations when it comes into building AI systems, AI technologies, and looking at new approaches for things both as practitioners and as researchers. I'm hearing a lot of echo. Is that like everyone? Okay. It will say everything twice. It's fine. So at a very high level, this is one of the slides I show people when they ask me why I don't use the phrase AI. It's because generally speaking, when people are talking about artificial intelligence, they are not talking about the entire field of artificial intelligence. They are talking about machine learning. So we can break it down into roughly two camps. And I call them camps because people are really settled in one or settled in the other, and they usually don't switch back and forth. That's been my experience anyway. So we've got symbolic artificial intelligence, the logic, logical AI. It's also referred to as logical AI. How do people think? How do we teach machines to think in ways that are similar to how people actually think? So this is where cognitive science really comes into play. And then the much bigger circle up there is what we're mostly concerned with these days is machine learning, the math. And this is what I tend to characterize as thinking is hard. We can probably build a model that comes close and then we'll do some math and we want to get stuff done. So we're just going to use the data that we've got and cross our fingers. You can argue with me about that opinion, all you like. I'm cool with that. So while I do have this, as I mentioned, the deep abiding love for symbolic AI, we're focusing primarily on machine learning here. Unless anybody wants to talk about slime. Not that slime. So some elements of AI, or machine learning, see? I'm also getting hit by the AI means machine learning bug. So some possible elements do include things like what data do we have that go into training the system? How do we actually train the system? How do we evaluate the system? All of the different elements, is there a model as an output? Is there a user interface as a way of interacting with machine learning? Now, not all of these are going to be present in every machine learning system. It kind of blows some people's minds to realize that you can have machine learning without a model. Or you can have machine learning without a task or prompt. But it's true. And we have to account for that when thinking about open source machine learning. And when we're looking at all of these different components, it gets a little bit hard to reason about. But if we reduce the dimensionality, PCA pun, we see roughly four buckets emerge. We've got the data, which is pretty familiar to us. We know what data is. We've got a good understanding of what might be training data, what's validation tasks, et cetera. We've got code, also a well-worn path for open source. And then we have what I call the other stuff. Because one of my skills is not naming things. And then finally, we've got output. So by doing a rough grouping with K equals four, we wind up with these four buckets. And I think that by thinking of them in this schematic, it makes it much easier to tackle the challenges that we face one by one. Now, some elements might appear in multiple buckets. Not on this slide, because simplicity. And some might not appear at all. But it's a starting point. So let's first talk about data. So when it comes to machine learning and data, we have some interesting problems. Some of them are known. Some of them are unknown. We have a lot of data out there. Machine learning research has been going on for what, since the 50s? Right? Ish? And that means that a lot of the data that has been used in this research doesn't have known provenance. So we don't actually know where some of the data came from. And if we're talking about things, I'm not going to talk about licenses, by the way. I forgot to mention that. I'm not talking about licenses. I'm just going to talk about the challenges in building open machine learning systems. But when there's data without known provenance, we don't know if it is truly game for us to use. If we have data that we don't know how it was collected, we don't know if it's truly game for us to use. Or if it suits our purpose. It could be an incredibly flawed data set, and we have no way of knowing. In terms of privacy, we've got a few big challenges with de-identification and anonymization. There's a lot of really interesting work that can be done in machine learning, but can't necessarily be done in an open way. And I don't actually know how to solve that particular beast, because I feel like I'm kind of splitten, too, on that. On the one hand, I do think that having full access to the training data, validation data, test, et cetera, is really important for building open systems. I don't think it can come at the expense of people's safety. And so if you are training something based on data that includes personally identifiable information, we kind of have to weigh that. There is that question of, should that be an open source system in the first place? I know that's a very controversial thing to say at FOSDEM. So I can leave now, if you like. For systems that also incorporate user feedback as part of the training data, because that's a fun place to get into, how do you build in, again, that de-identification and anonymization? And there are things like, okay, well, how are we actually going about splitting the corpus? If we are splitting the corpus into training and validation data, it's actually important to know what proportions we're using and how we're sampling in order to do that splitting. But again, some of these may not be applicable for all machine learning systems. So one of the things that I kept asking myself is, are these things required to recompile, for some definition of recompile, a model? So if I wanted to create the model from scratch and build it myself, what do I need? The entire dataset? If you want to show hands and tell me if you think it's required, I'm fascinated to know, but there's no obligation to. So is the entire dataset required to recompile a model? Would a description of the data suffice? How about a datasheet? Who thinks that you need to know about how the data was collected? Well, thank you. I appreciate you. I appreciate all of you. So I kept coming back to this question, is it required to recompile a model? And so you'll see that question as we go through some of these other sections. I do take the stance that you need the entire dataset and the methodology, but that introduces some big problems and big as in dollar signs, because these corpus are not small. Hosting them is expensive. So if we want to make open source machine learning open to all of the people who are interested in participating in it, how do we break down the cost of doing so? How do we make it available? The methodology, publishing the methodology for how the data was collected, helps with transparency if you trust it. But we're open source. We try to trust each other mostly. Except for a few of you. You know who you are. And there's some open questions about attribution. If the data also needs attribution. I'm not talking about from a legal sense or a licensed sense, just for transparency and for credit, because we appreciate giving credit where credit is due. And if somebody wants to opt out of having their data in a corpus, how do we handle that as well? Lots of unknown problems with data. But code gets a little bit easier. This is going to be the second time I make a Jurassic Park joke this week. But we know how to do open source software. This is code. We know how to do this. And despite what we've been hearing, most of machine learning fits solely in this camp. It fits solely in the camp of open source software. Job well done. Great. We've got Weka. What else do we need? So it does, they are governed by the same requirements as normal open source software. No special casing needed. Cool. One of the unique things, though, about this type of software is that it may actually produce one of those things we don't yet know how to deal with. The model. It also might involve an interface, how to interact with whatever system it produces, which may or may not be a model. And it does intersect with the data and some interesting problems that go along with that. So one of the things that we do when we process data is we clean it. So does that code need to be open source? Alongside with cleaning is some interesting value judgments that we have. We may say, okay, deprioritize this feature a little bit. Let's increase the priority on this one. And in that way, we are actually making moral and ethical judgments, and we're encoding them. Now, the great thing about code is that it's very easily inspected. For some definition of very and some definition of easily. That's an exercise for the reader. But if we dig into it, we can see where those value judgments are made. The other stuff, this is my favorite part. If somebody has a better name for it, let me know. So our hardware specifications required to recompile a model. How about disclosure of training time? How long it was trained for? A definition of correctness. So all of these do impact what comes out of the data. All of these do impact what comes out of your machine learning algorithm. I was doing like a one-day course, brushing up some knowledge, and there was a bit of a competition, ordinarily I hate competitions in classwork. But the idea was, okay, let's... Here are all the concepts. Do some fine-tuning, play around, and see who can have the highest accuracy for some random task. And I thought I did a pretty good job. The story of my life. I thought I did a pretty good job. But there was one person who achieved nearly a perfect score. So of course, the question is, would you do? And they said, oh, well, I just ran the training for, you know, two days. I'm like, oh, okay. Yeah, this was a one-day course. I didn't think it was a two-day investment. So the training time does absolutely affect the quality and the output of the resulting system, as does the hardware. So they are required to recompile a model. But similar to data, we also have the question of access. Access to equitable compute. Access to the hardware itself. And again, we have a problem of attribution. So finally, output. And since we are focusing on models and machine learning models, we've got matrices. I don't really know how to make that work in an open way. Yes, I can inspect a matrix. Can I make sense of it in isolation? Not so much. So if we do a litmus test, if this is all we have, can we do arbitrary machine learning tasks with just this? Probably not. How about just the code? Probably not. You still need some data. Just the hardware? Maybe. I'd like to see that. Just the model? I'm going to say no. That's my... I'm putting my foot down there. So what do we really need to make a transparent machine learning system? We need all of it. It all needs to be there. It all needs to be open and available. And that might mean that some things are not suitable for being open. So some other questions that I'd love for you to think about as you're thinking about open source and machine learning is what does contribution to a model look like? What does correctness of a contribution look like? How do we actually verify the openness of these systems in a way that doesn't require a huge amount of investment that only a select few have?", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.74, "text": " As Julia is getting herself up and running, I'd like to introduce her to you.", "tokens": [50364, 1018, 18551, 307, 1242, 7530, 493, 293, 2614, 11, 286, 1116, 411, 281, 5366, 720, 281, 291, 13, 50851], "temperature": 0.0, "avg_logprob": -0.21947862120235667, "compression_ratio": 1.4345238095238095, "no_speech_prob": 0.13750046491622925}, {"id": 1, "seek": 0, "start": 9.74, "end": 14.8, "text": " She is from Seattle, so she's another American here.", "tokens": [50851, 1240, 307, 490, 15721, 11, 370, 750, 311, 1071, 2665, 510, 13, 51104], "temperature": 0.0, "avg_logprob": -0.21947862120235667, "compression_ratio": 1.4345238095238095, "no_speech_prob": 0.13750046491622925}, {"id": 2, "seek": 0, "start": 14.8, "end": 23.64, "text": " She is a social technical, I can't say that, systems nerd and a huge fan of Lego, so that's", "tokens": [51104, 1240, 307, 257, 2093, 6191, 11, 286, 393, 380, 584, 300, 11, 3652, 23229, 293, 257, 2603, 3429, 295, 28761, 11, 370, 300, 311, 51546], "temperature": 0.0, "avg_logprob": -0.21947862120235667, "compression_ratio": 1.4345238095238095, "no_speech_prob": 0.13750046491622925}, {"id": 3, "seek": 0, "start": 23.64, "end": 27.72, "text": " important to know.", "tokens": [51546, 1021, 281, 458, 13, 51750], "temperature": 0.0, "avg_logprob": -0.21947862120235667, "compression_ratio": 1.4345238095238095, "no_speech_prob": 0.13750046491622925}, {"id": 4, "seek": 2772, "start": 27.72, "end": 32.48, "text": " You used to have a Scottish accent despite being an American and never being to Scotland.", "tokens": [50364, 509, 1143, 281, 362, 257, 13777, 11982, 7228, 885, 364, 2665, 293, 1128, 885, 281, 11180, 13, 50602], "temperature": 0.0, "avg_logprob": -0.24711577788643216, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.009691995568573475}, {"id": 5, "seek": 2772, "start": 32.48, "end": 35.44, "text": " Now that is fascinating, that truly is.", "tokens": [50602, 823, 300, 307, 10343, 11, 300, 4908, 307, 13, 50750], "temperature": 0.0, "avg_logprob": -0.24711577788643216, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.009691995568573475}, {"id": 6, "seek": 2772, "start": 35.44, "end": 37.84, "text": " And her favorite joke is just her own humor.", "tokens": [50750, 400, 720, 2954, 7647, 307, 445, 720, 1065, 14318, 13, 50870], "temperature": 0.0, "avg_logprob": -0.24711577788643216, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.009691995568573475}, {"id": 7, "seek": 2772, "start": 37.84, "end": 40.68, "text": " Julia, take it away.", "tokens": [50870, 18551, 11, 747, 309, 1314, 13, 51012], "temperature": 0.0, "avg_logprob": -0.24711577788643216, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.009691995568573475}, {"id": 8, "seek": 2772, "start": 40.68, "end": 46.76, "text": " I think that one landed is exactly how I intended to, so thank you.", "tokens": [51012, 286, 519, 300, 472, 15336, 307, 2293, 577, 286, 10226, 281, 11, 370, 1309, 291, 13, 51316], "temperature": 0.0, "avg_logprob": -0.24711577788643216, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.009691995568573475}, {"id": 9, "seek": 2772, "start": 46.76, "end": 55.120000000000005, "text": " Hi everyone, it's great to be here today to talk about what I am calling a principled", "tokens": [51316, 2421, 1518, 11, 309, 311, 869, 281, 312, 510, 965, 281, 751, 466, 437, 286, 669, 5141, 257, 3681, 15551, 51734], "temperature": 0.0, "avg_logprob": -0.24711577788643216, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.009691995568573475}, {"id": 10, "seek": 5512, "start": 55.12, "end": 60.0, "text": " component analysis of open source AI.", "tokens": [50364, 6542, 5215, 295, 1269, 4009, 7318, 13, 50608], "temperature": 0.0, "avg_logprob": -0.3091291498254847, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.06109016016125679}, {"id": 11, "seek": 5512, "start": 60.0, "end": 62.879999999999995, "text": " So who am I?", "tokens": [50608, 407, 567, 669, 286, 30, 50752], "temperature": 0.0, "avg_logprob": -0.3091291498254847, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.06109016016125679}, {"id": 12, "seek": 5512, "start": 62.879999999999995, "end": 65.12, "text": " I'm Julia.", "tokens": [50752, 286, 478, 18551, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3091291498254847, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.06109016016125679}, {"id": 13, "seek": 5512, "start": 65.12, "end": 73.47999999999999, "text": " I've been focused primarily on open source resilience and the software supply chain for", "tokens": [50864, 286, 600, 668, 5178, 10029, 322, 1269, 4009, 19980, 293, 264, 4722, 5847, 5021, 337, 51282], "temperature": 0.0, "avg_logprob": -0.3091291498254847, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.06109016016125679}, {"id": 14, "seek": 5512, "start": 73.47999999999999, "end": 76.47999999999999, "text": " the past five, ten years.", "tokens": [51282, 264, 1791, 1732, 11, 2064, 924, 13, 51432], "temperature": 0.0, "avg_logprob": -0.3091291498254847, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.06109016016125679}, {"id": 15, "seek": 5512, "start": 76.47999999999999, "end": 81.24, "text": " I feel like a little bit of an open source hipster because I've been talking about some", "tokens": [51432, 286, 841, 411, 257, 707, 857, 295, 364, 1269, 4009, 8103, 3120, 570, 286, 600, 668, 1417, 466, 512, 51670], "temperature": 0.0, "avg_logprob": -0.3091291498254847, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.06109016016125679}, {"id": 16, "seek": 5512, "start": 81.24, "end": 84.52, "text": " of these things before they were cool.", "tokens": [51670, 295, 613, 721, 949, 436, 645, 1627, 13, 51834], "temperature": 0.0, "avg_logprob": -0.3091291498254847, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.06109016016125679}, {"id": 17, "seek": 8452, "start": 84.52, "end": 94.56, "text": " But I've been working in and around open source AI since undergrad, so I'm not going", "tokens": [50364, 583, 286, 600, 668, 1364, 294, 293, 926, 1269, 4009, 7318, 1670, 14295, 11, 370, 286, 478, 406, 516, 50866], "temperature": 0.0, "avg_logprob": -0.21678249763719964, "compression_ratio": 1.43125, "no_speech_prob": 0.004594200290739536}, {"id": 18, "seek": 8452, "start": 94.56, "end": 98.32, "text": " to leave that to your imagination.", "tokens": [50866, 281, 1856, 300, 281, 428, 12938, 13, 51054], "temperature": 0.0, "avg_logprob": -0.21678249763719964, "compression_ratio": 1.43125, "no_speech_prob": 0.004594200290739536}, {"id": 19, "seek": 8452, "start": 98.32, "end": 103.88, "text": " Some things will probably give it away though.", "tokens": [51054, 2188, 721, 486, 1391, 976, 309, 1314, 1673, 13, 51332], "temperature": 0.0, "avg_logprob": -0.21678249763719964, "compression_ratio": 1.43125, "no_speech_prob": 0.004594200290739536}, {"id": 20, "seek": 8452, "start": 103.88, "end": 107.36, "text": " So in case you were wondering, that isn't a typo.", "tokens": [51332, 407, 294, 1389, 291, 645, 6359, 11, 300, 1943, 380, 257, 2125, 78, 13, 51506], "temperature": 0.0, "avg_logprob": -0.21678249763719964, "compression_ratio": 1.43125, "no_speech_prob": 0.004594200290739536}, {"id": 21, "seek": 8452, "start": 107.36, "end": 109.67999999999999, "text": " It is a pun.", "tokens": [51506, 467, 307, 257, 4468, 13, 51622], "temperature": 0.0, "avg_logprob": -0.21678249763719964, "compression_ratio": 1.43125, "no_speech_prob": 0.004594200290739536}, {"id": 22, "seek": 10968, "start": 109.68, "end": 116.2, "text": " It is a good slash bad pun, and I hope you are ready for more.", "tokens": [50364, 467, 307, 257, 665, 17330, 1578, 4468, 11, 293, 286, 1454, 291, 366, 1919, 337, 544, 13, 50690], "temperature": 0.0, "avg_logprob": -0.17230836968672902, "compression_ratio": 1.4375, "no_speech_prob": 0.023625602945685387}, {"id": 23, "seek": 10968, "start": 116.2, "end": 122.4, "text": " I couldn't make a pun out of support vector machines though, so if you have one, please", "tokens": [50690, 286, 2809, 380, 652, 257, 4468, 484, 295, 1406, 8062, 8379, 1673, 11, 370, 498, 291, 362, 472, 11, 1767, 51000], "temperature": 0.0, "avg_logprob": -0.17230836968672902, "compression_ratio": 1.4375, "no_speech_prob": 0.023625602945685387}, {"id": 24, "seek": 10968, "start": 122.4, "end": 125.28, "text": " come talk to me later.", "tokens": [51000, 808, 751, 281, 385, 1780, 13, 51144], "temperature": 0.0, "avg_logprob": -0.17230836968672902, "compression_ratio": 1.4375, "no_speech_prob": 0.023625602945685387}, {"id": 25, "seek": 10968, "start": 125.28, "end": 126.28, "text": " That would be good.", "tokens": [51144, 663, 576, 312, 665, 13, 51194], "temperature": 0.0, "avg_logprob": -0.17230836968672902, "compression_ratio": 1.4375, "no_speech_prob": 0.023625602945685387}, {"id": 26, "seek": 10968, "start": 126.28, "end": 131.92000000000002, "text": " It would be much appreciated.", "tokens": [51194, 467, 576, 312, 709, 17169, 13, 51476], "temperature": 0.0, "avg_logprob": -0.17230836968672902, "compression_ratio": 1.4375, "no_speech_prob": 0.023625602945685387}, {"id": 27, "seek": 10968, "start": 131.92000000000002, "end": 136.52, "text": " So open source AI is not new.", "tokens": [51476, 407, 1269, 4009, 7318, 307, 406, 777, 13, 51706], "temperature": 0.0, "avg_logprob": -0.17230836968672902, "compression_ratio": 1.4375, "no_speech_prob": 0.023625602945685387}, {"id": 28, "seek": 13652, "start": 136.52, "end": 144.8, "text": " I've been doing open source AI for a while now, and I threw a bunch of stuff up on this", "tokens": [50364, 286, 600, 668, 884, 1269, 4009, 7318, 337, 257, 1339, 586, 11, 293, 286, 11918, 257, 3840, 295, 1507, 493, 322, 341, 50778], "temperature": 0.0, "avg_logprob": -0.20339229438878312, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.01875423826277256}, {"id": 29, "seek": 13652, "start": 144.8, "end": 152.76000000000002, "text": " slide, mostly copying and pasting from former poster presentations, but I have a chapter", "tokens": [50778, 4137, 11, 5240, 27976, 293, 1791, 278, 490, 5819, 17171, 18964, 11, 457, 286, 362, 257, 7187, 51176], "temperature": 0.0, "avg_logprob": -0.20339229438878312, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.01875423826277256}, {"id": 30, "seek": 13652, "start": 152.76000000000002, "end": 154.48000000000002, "text": " in this lovely book.", "tokens": [51176, 294, 341, 7496, 1446, 13, 51262], "temperature": 0.0, "avg_logprob": -0.20339229438878312, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.01875423826277256}, {"id": 31, "seek": 13652, "start": 154.48000000000002, "end": 159.48000000000002, "text": " I believe there is only one left in stock.", "tokens": [51262, 286, 1697, 456, 307, 787, 472, 1411, 294, 4127, 13, 51512], "temperature": 0.0, "avg_logprob": -0.20339229438878312, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.01875423826277256}, {"id": 32, "seek": 13652, "start": 159.48000000000002, "end": 162.4, "text": " They haven't had much of a call to reprint it.", "tokens": [51512, 814, 2378, 380, 632, 709, 295, 257, 818, 281, 1085, 19014, 309, 13, 51658], "temperature": 0.0, "avg_logprob": -0.20339229438878312, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.01875423826277256}, {"id": 33, "seek": 16240, "start": 162.4, "end": 171.48000000000002, "text": " So you too could own constrained clustering, not about neural networks.", "tokens": [50364, 407, 291, 886, 727, 1065, 38901, 596, 48673, 11, 406, 466, 18161, 9590, 13, 50818], "temperature": 0.0, "avg_logprob": -0.21255646377313334, "compression_ratio": 1.4425287356321839, "no_speech_prob": 0.0027499422430992126}, {"id": 34, "seek": 16240, "start": 171.48000000000002, "end": 182.16, "text": " And in that chapter, we had a very interesting approach to exploring information with user", "tokens": [50818, 400, 294, 300, 7187, 11, 321, 632, 257, 588, 1880, 3109, 281, 12736, 1589, 365, 4195, 51352], "temperature": 0.0, "avg_logprob": -0.21255646377313334, "compression_ratio": 1.4425287356321839, "no_speech_prob": 0.0027499422430992126}, {"id": 35, "seek": 16240, "start": 182.16, "end": 187.08, "text": " feedback using a variant of the K-means algorithm.", "tokens": [51352, 5824, 1228, 257, 17501, 295, 264, 591, 12, 1398, 599, 9284, 13, 51598], "temperature": 0.0, "avg_logprob": -0.21255646377313334, "compression_ratio": 1.4425287356321839, "no_speech_prob": 0.0027499422430992126}, {"id": 36, "seek": 16240, "start": 187.08, "end": 190.24, "text": " So that was the basis of our chapter.", "tokens": [51598, 407, 300, 390, 264, 5143, 295, 527, 7187, 13, 51756], "temperature": 0.0, "avg_logprob": -0.21255646377313334, "compression_ratio": 1.4425287356321839, "no_speech_prob": 0.0027499422430992126}, {"id": 37, "seek": 19024, "start": 190.24, "end": 197.36, "text": " We used this fantastic open source machine learning library called Weka.", "tokens": [50364, 492, 1143, 341, 5456, 1269, 4009, 3479, 2539, 6405, 1219, 492, 2330, 13, 50720], "temperature": 0.0, "avg_logprob": -0.26710621003181706, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.0493093766272068}, {"id": 38, "seek": 19024, "start": 197.36, "end": 200.04000000000002, "text": " Is anyone familiar with Weka here?", "tokens": [50720, 1119, 2878, 4963, 365, 492, 2330, 510, 30, 50854], "temperature": 0.0, "avg_logprob": -0.26710621003181706, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.0493093766272068}, {"id": 39, "seek": 19024, "start": 200.04000000000002, "end": 202.12, "text": " My people, hello.", "tokens": [50854, 1222, 561, 11, 7751, 13, 50958], "temperature": 0.0, "avg_logprob": -0.26710621003181706, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.0493093766272068}, {"id": 40, "seek": 19024, "start": 202.12, "end": 203.84, "text": " Excellent.", "tokens": [50958, 16723, 13, 51044], "temperature": 0.0, "avg_logprob": -0.26710621003181706, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.0493093766272068}, {"id": 41, "seek": 19024, "start": 203.84, "end": 205.44, "text": " Weka is wonderful.", "tokens": [51044, 492, 2330, 307, 3715, 13, 51124], "temperature": 0.0, "avg_logprob": -0.26710621003181706, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.0493093766272068}, {"id": 42, "seek": 19024, "start": 205.44, "end": 210.72, "text": " It has so many great machine learning algorithms in it.", "tokens": [51124, 467, 575, 370, 867, 869, 3479, 2539, 14642, 294, 309, 13, 51388], "temperature": 0.0, "avg_logprob": -0.26710621003181706, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.0493093766272068}, {"id": 43, "seek": 19024, "start": 210.72, "end": 216.48000000000002, "text": " When I first started using Weka, I went to this website called SourceForge and downloaded", "tokens": [51388, 1133, 286, 700, 1409, 1228, 492, 2330, 11, 286, 1437, 281, 341, 3144, 1219, 29629, 37, 4685, 293, 21748, 51676], "temperature": 0.0, "avg_logprob": -0.26710621003181706, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.0493093766272068}, {"id": 44, "seek": 19024, "start": 216.48000000000002, "end": 217.64000000000001, "text": " it.", "tokens": [51676, 309, 13, 51734], "temperature": 0.0, "avg_logprob": -0.26710621003181706, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.0493093766272068}, {"id": 45, "seek": 19024, "start": 217.64000000000001, "end": 218.64000000000001, "text": " And I was just entranced.", "tokens": [51734, 400, 286, 390, 445, 948, 4257, 1232, 13, 51784], "temperature": 0.0, "avg_logprob": -0.26710621003181706, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.0493093766272068}, {"id": 46, "seek": 21864, "start": 219.04, "end": 221.83999999999997, "text": " It was my first experience with open source.", "tokens": [50384, 467, 390, 452, 700, 1752, 365, 1269, 4009, 13, 50524], "temperature": 0.0, "avg_logprob": -0.1682098453695124, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.06922391057014465}, {"id": 47, "seek": 21864, "start": 221.83999999999997, "end": 225.88, "text": " I was entranced by this idea that I could go and see the code.", "tokens": [50524, 286, 390, 948, 4257, 1232, 538, 341, 1558, 300, 286, 727, 352, 293, 536, 264, 3089, 13, 50726], "temperature": 0.0, "avg_logprob": -0.1682098453695124, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.06922391057014465}, {"id": 48, "seek": 21864, "start": 225.88, "end": 228.79999999999998, "text": " I could go and modify it.", "tokens": [50726, 286, 727, 352, 293, 16927, 309, 13, 50872], "temperature": 0.0, "avg_logprob": -0.1682098453695124, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.06922391057014465}, {"id": 49, "seek": 21864, "start": 228.79999999999998, "end": 238.2, "text": " And in fact, some researchers at the University of Texas had done this and modified it and", "tokens": [50872, 400, 294, 1186, 11, 512, 10309, 412, 264, 3535, 295, 7885, 632, 1096, 341, 293, 15873, 309, 293, 51342], "temperature": 0.0, "avg_logprob": -0.1682098453695124, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.06922391057014465}, {"id": 50, "seek": 21864, "start": 238.2, "end": 240.2, "text": " redistributed it.", "tokens": [51342, 36198, 2024, 4866, 309, 13, 51442], "temperature": 0.0, "avg_logprob": -0.1682098453695124, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.06922391057014465}, {"id": 51, "seek": 21864, "start": 240.2, "end": 246.88, "text": " Just this magical thing that I could then go and use in my own research, knowing that", "tokens": [51442, 1449, 341, 12066, 551, 300, 286, 727, 550, 352, 293, 764, 294, 452, 1065, 2132, 11, 5276, 300, 51776], "temperature": 0.0, "avg_logprob": -0.1682098453695124, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.06922391057014465}, {"id": 52, "seek": 24688, "start": 246.92, "end": 252.51999999999998, "text": " somebody who is much better at math than I am had validated all the algorithms.", "tokens": [50366, 2618, 567, 307, 709, 1101, 412, 5221, 813, 286, 669, 632, 40693, 439, 264, 14642, 13, 50646], "temperature": 0.0, "avg_logprob": -0.22972207069396972, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.005547504872083664}, {"id": 53, "seek": 24688, "start": 252.51999999999998, "end": 264.04, "text": " I also built this lovely autonomous robot, aerial autonomous robot, that won the dubious", "tokens": [50646, 286, 611, 3094, 341, 7496, 23797, 7881, 11, 31026, 23797, 7881, 11, 300, 1582, 264, 18540, 851, 51222], "temperature": 0.0, "avg_logprob": -0.22972207069396972, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.005547504872083664}, {"id": 54, "seek": 24688, "start": 264.04, "end": 271.12, "text": " award of innovative hardware design from AAAI, which I think means we don't know how you", "tokens": [51222, 7130, 295, 12999, 8837, 1715, 490, 34347, 40, 11, 597, 286, 519, 1355, 321, 500, 380, 458, 577, 291, 51576], "temperature": 0.0, "avg_logprob": -0.22972207069396972, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.005547504872083664}, {"id": 55, "seek": 27112, "start": 271.16, "end": 274.44, "text": " got this to lift off the ground.", "tokens": [50366, 658, 341, 281, 5533, 766, 264, 2727, 13, 50530], "temperature": 0.0, "avg_logprob": -0.2243310988895477, "compression_ratio": 1.4487179487179487, "no_speech_prob": 0.00857161357998848}, {"id": 56, "seek": 27112, "start": 274.44, "end": 283.68, "text": " But lift it did, mostly because we made it a giant dodecahedron to lift all of the camera", "tokens": [50530, 583, 5533, 309, 630, 11, 5240, 570, 321, 1027, 309, 257, 7410, 360, 1479, 496, 27096, 2044, 281, 5533, 439, 295, 264, 2799, 50992], "temperature": 0.0, "avg_logprob": -0.2243310988895477, "compression_ratio": 1.4487179487179487, "no_speech_prob": 0.00857161357998848}, {"id": 57, "seek": 27112, "start": 283.68, "end": 286.56, "text": " components.", "tokens": [50992, 6677, 13, 51136], "temperature": 0.0, "avg_logprob": -0.2243310988895477, "compression_ratio": 1.4487179487179487, "no_speech_prob": 0.00857161357998848}, {"id": 58, "seek": 27112, "start": 286.56, "end": 294.24, "text": " And a few years ago, I used machine learning to tackle one of the world's hardest problems.", "tokens": [51136, 400, 257, 1326, 924, 2057, 11, 286, 1143, 3479, 2539, 281, 14896, 472, 295, 264, 1002, 311, 13158, 2740, 13, 51520], "temperature": 0.0, "avg_logprob": -0.2243310988895477, "compression_ratio": 1.4487179487179487, "no_speech_prob": 0.00857161357998848}, {"id": 59, "seek": 29424, "start": 295.24, "end": 300.24, "text": " Determining whether or not you should hug something.", "tokens": [50414, 4237, 966, 1760, 1968, 420, 406, 291, 820, 8777, 746, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3588383356730143, "compression_ratio": 1.4154929577464788, "no_speech_prob": 0.0005192400421947241}, {"id": 60, "seek": 29424, "start": 300.24, "end": 310.92, "text": " So I trained a little model when Fed and Image would tell you if it was a good idea to hug", "tokens": [50664, 407, 286, 8895, 257, 707, 2316, 562, 7772, 293, 29903, 576, 980, 291, 498, 309, 390, 257, 665, 1558, 281, 8777, 51198], "temperature": 0.0, "avg_logprob": -0.3588383356730143, "compression_ratio": 1.4154929577464788, "no_speech_prob": 0.0005192400421947241}, {"id": 61, "seek": 29424, "start": 310.92, "end": 312.92, "text": " it or not.", "tokens": [51198, 309, 420, 406, 13, 51298], "temperature": 0.0, "avg_logprob": -0.3588383356730143, "compression_ratio": 1.4154929577464788, "no_speech_prob": 0.0005192400421947241}, {"id": 62, "seek": 29424, "start": 312.92, "end": 321.32, "text": " I am sad to say I am not huggable, apparently.", "tokens": [51298, 286, 669, 4227, 281, 584, 286, 669, 406, 8777, 70, 712, 11, 7970, 13, 51718], "temperature": 0.0, "avg_logprob": -0.3588383356730143, "compression_ratio": 1.4154929577464788, "no_speech_prob": 0.0005192400421947241}, {"id": 63, "seek": 32132, "start": 321.4, "end": 323.4, "text": " I am not as mathematically proven.", "tokens": [50368, 286, 669, 406, 382, 44003, 12785, 13, 50468], "temperature": 0.0, "avg_logprob": -0.31257618305295015, "compression_ratio": 1.497584541062802, "no_speech_prob": 0.039513904601335526}, {"id": 64, "seek": 32132, "start": 323.4, "end": 329.4, "text": " So, you know, maybe I should take my picture again, try it out.", "tokens": [50468, 407, 11, 291, 458, 11, 1310, 286, 820, 747, 452, 3036, 797, 11, 853, 309, 484, 13, 50768], "temperature": 0.0, "avg_logprob": -0.31257618305295015, "compression_ratio": 1.497584541062802, "no_speech_prob": 0.039513904601335526}, {"id": 65, "seek": 32132, "start": 329.4, "end": 331.4, "text": " So that's me.", "tokens": [50768, 407, 300, 311, 385, 13, 50868], "temperature": 0.0, "avg_logprob": -0.31257618305295015, "compression_ratio": 1.497584541062802, "no_speech_prob": 0.039513904601335526}, {"id": 66, "seek": 32132, "start": 331.4, "end": 338.4, "text": " So the bad news here is that I don't have any answers for any of you in this presentation.", "tokens": [50868, 407, 264, 1578, 2583, 510, 307, 300, 286, 500, 380, 362, 604, 6338, 337, 604, 295, 291, 294, 341, 5860, 13, 51218], "temperature": 0.0, "avg_logprob": -0.31257618305295015, "compression_ratio": 1.497584541062802, "no_speech_prob": 0.039513904601335526}, {"id": 67, "seek": 32132, "start": 338.4, "end": 341.4, "text": " I only have open questions.", "tokens": [51218, 286, 787, 362, 1269, 1651, 13, 51368], "temperature": 0.0, "avg_logprob": -0.31257618305295015, "compression_ratio": 1.497584541062802, "no_speech_prob": 0.039513904601335526}, {"id": 68, "seek": 32132, "start": 341.4, "end": 350.4, "text": " We are not going into the deep specifics of models, algorithms, or approaches.", "tokens": [51368, 492, 366, 406, 516, 666, 264, 2452, 28454, 295, 5245, 11, 14642, 11, 420, 11587, 13, 51818], "temperature": 0.0, "avg_logprob": -0.31257618305295015, "compression_ratio": 1.497584541062802, "no_speech_prob": 0.039513904601335526}, {"id": 69, "seek": 35040, "start": 350.47999999999996, "end": 356.47999999999996, "text": " This is going to be probably, for some of you, a little bit too high level.", "tokens": [50368, 639, 307, 516, 281, 312, 1391, 11, 337, 512, 295, 291, 11, 257, 707, 857, 886, 1090, 1496, 13, 50668], "temperature": 0.0, "avg_logprob": -0.16929348566198862, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.010762364603579044}, {"id": 70, "seek": 35040, "start": 356.47999999999996, "end": 360.28, "text": " And for some, probably a little too low level.", "tokens": [50668, 400, 337, 512, 11, 1391, 257, 707, 886, 2295, 1496, 13, 50858], "temperature": 0.0, "avg_logprob": -0.16929348566198862, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.010762364603579044}, {"id": 71, "seek": 35040, "start": 360.28, "end": 367.84, "text": " We are exploring this new area of technology that has ballooned kind of seemingly overnight.", "tokens": [50858, 492, 366, 12736, 341, 777, 1859, 295, 2899, 300, 575, 16994, 292, 733, 295, 18709, 13935, 13, 51236], "temperature": 0.0, "avg_logprob": -0.16929348566198862, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.010762364603579044}, {"id": 72, "seek": 35040, "start": 367.84, "end": 369.84, "text": " It feels like that to me.", "tokens": [51236, 467, 3417, 411, 300, 281, 385, 13, 51336], "temperature": 0.0, "avg_logprob": -0.16929348566198862, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.010762364603579044}, {"id": 73, "seek": 35040, "start": 369.84, "end": 372.47999999999996, "text": " I don't know if that feels like that to you.", "tokens": [51336, 286, 500, 380, 458, 498, 300, 3417, 411, 300, 281, 291, 13, 51468], "temperature": 0.0, "avg_logprob": -0.16929348566198862, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.010762364603579044}, {"id": 74, "seek": 35040, "start": 372.47999999999996, "end": 378.23999999999995, "text": " And we are facing some really interesting challenges when talking about the advances", "tokens": [51468, 400, 321, 366, 7170, 512, 534, 1880, 4759, 562, 1417, 466, 264, 25297, 51756], "temperature": 0.0, "avg_logprob": -0.16929348566198862, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.010762364603579044}, {"id": 75, "seek": 37824, "start": 378.28000000000003, "end": 385.68, "text": " that we have seen in artificial intelligence and how it intersects with open source if", "tokens": [50366, 300, 321, 362, 1612, 294, 11677, 7599, 293, 577, 309, 27815, 82, 365, 1269, 4009, 498, 50736], "temperature": 0.0, "avg_logprob": -0.21916337615077935, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.0021798231173306704}, {"id": 76, "seek": 37824, "start": 385.68, "end": 387.68, "text": " you are not using Weka.", "tokens": [50736, 291, 366, 406, 1228, 492, 2330, 13, 50836], "temperature": 0.0, "avg_logprob": -0.21916337615077935, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.0021798231173306704}, {"id": 77, "seek": 37824, "start": 387.68, "end": 389.68, "text": " If you are using Weka, you are set.", "tokens": [50836, 759, 291, 366, 1228, 492, 2330, 11, 291, 366, 992, 13, 50936], "temperature": 0.0, "avg_logprob": -0.21916337615077935, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.0021798231173306704}, {"id": 78, "seek": 37824, "start": 389.68, "end": 391.68, "text": " It's great.", "tokens": [50936, 467, 311, 869, 13, 51036], "temperature": 0.0, "avg_logprob": -0.21916337615077935, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.0021798231173306704}, {"id": 79, "seek": 37824, "start": 391.68, "end": 393.68, "text": " They are not paying me for this.", "tokens": [51036, 814, 366, 406, 6229, 385, 337, 341, 13, 51136], "temperature": 0.0, "avg_logprob": -0.21916337615077935, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.0021798231173306704}, {"id": 80, "seek": 37824, "start": 393.68, "end": 395.68, "text": " I promise.", "tokens": [51136, 286, 6228, 13, 51236], "temperature": 0.0, "avg_logprob": -0.21916337615077935, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.0021798231173306704}, {"id": 81, "seek": 37824, "start": 395.68, "end": 401.92, "text": " So level set, like AI draws from a lot of different fields.", "tokens": [51236, 407, 1496, 992, 11, 411, 7318, 20045, 490, 257, 688, 295, 819, 7909, 13, 51548], "temperature": 0.0, "avg_logprob": -0.21916337615077935, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.0021798231173306704}, {"id": 82, "seek": 37824, "start": 401.92, "end": 407.08, "text": " If you go back to your AI 101 course, you are going to probably get a little bit of a survey", "tokens": [51548, 759, 291, 352, 646, 281, 428, 7318, 21055, 1164, 11, 291, 366, 516, 281, 1391, 483, 257, 707, 857, 295, 257, 8984, 51806], "temperature": 0.0, "avg_logprob": -0.21916337615077935, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.0021798231173306704}, {"id": 83, "seek": 40708, "start": 407.12, "end": 413.12, "text": " overview of all of these different fields, from ethics to philosophy.", "tokens": [50366, 12492, 295, 439, 295, 613, 819, 7909, 11, 490, 19769, 281, 10675, 13, 50666], "temperature": 0.0, "avg_logprob": -0.17386454627627418, "compression_ratio": 1.4021739130434783, "no_speech_prob": 0.0076615093275904655}, {"id": 84, "seek": 40708, "start": 413.12, "end": 417.12, "text": " Philosophy plays a big part in AI.", "tokens": [50666, 43655, 5749, 257, 955, 644, 294, 7318, 13, 50866], "temperature": 0.0, "avg_logprob": -0.17386454627627418, "compression_ratio": 1.4021739130434783, "no_speech_prob": 0.0076615093275904655}, {"id": 85, "seek": 40708, "start": 417.12, "end": 423.12, "text": " Economics, my favorite part of AI is the formal logic side.", "tokens": [50866, 39024, 11, 452, 2954, 644, 295, 7318, 307, 264, 9860, 9952, 1252, 13, 51166], "temperature": 0.0, "avg_logprob": -0.17386454627627418, "compression_ratio": 1.4021739130434783, "no_speech_prob": 0.0076615093275904655}, {"id": 86, "seek": 40708, "start": 423.12, "end": 430.12, "text": " But that's because statistics was never really my strong suit, which is why I love computers.", "tokens": [51166, 583, 300, 311, 570, 12523, 390, 1128, 534, 452, 2068, 5722, 11, 597, 307, 983, 286, 959, 10807, 13, 51516], "temperature": 0.0, "avg_logprob": -0.17386454627627418, "compression_ratio": 1.4021739130434783, "no_speech_prob": 0.0076615093275904655}, {"id": 87, "seek": 43012, "start": 430.16, "end": 439.16, "text": " So there are a lot of different considerations when it comes into building AI systems, AI", "tokens": [50366, 407, 456, 366, 257, 688, 295, 819, 24070, 562, 309, 1487, 666, 2390, 7318, 3652, 11, 7318, 50816], "temperature": 0.0, "avg_logprob": -0.19747231457684492, "compression_ratio": 1.4816753926701571, "no_speech_prob": 0.0072249057702720165}, {"id": 88, "seek": 43012, "start": 439.16, "end": 446.16, "text": " technologies, and looking at new approaches for things both as practitioners and as researchers.", "tokens": [50816, 7943, 11, 293, 1237, 412, 777, 11587, 337, 721, 1293, 382, 25742, 293, 382, 10309, 13, 51166], "temperature": 0.0, "avg_logprob": -0.19747231457684492, "compression_ratio": 1.4816753926701571, "no_speech_prob": 0.0072249057702720165}, {"id": 89, "seek": 43012, "start": 446.16, "end": 448.16, "text": " I'm hearing a lot of echo.", "tokens": [51166, 286, 478, 4763, 257, 688, 295, 14300, 13, 51266], "temperature": 0.0, "avg_logprob": -0.19747231457684492, "compression_ratio": 1.4816753926701571, "no_speech_prob": 0.0072249057702720165}, {"id": 90, "seek": 43012, "start": 448.16, "end": 450.16, "text": " Is that like everyone?", "tokens": [51266, 1119, 300, 411, 1518, 30, 51366], "temperature": 0.0, "avg_logprob": -0.19747231457684492, "compression_ratio": 1.4816753926701571, "no_speech_prob": 0.0072249057702720165}, {"id": 91, "seek": 43012, "start": 450.16, "end": 452.16, "text": " Okay.", "tokens": [51366, 1033, 13, 51466], "temperature": 0.0, "avg_logprob": -0.19747231457684492, "compression_ratio": 1.4816753926701571, "no_speech_prob": 0.0072249057702720165}, {"id": 92, "seek": 43012, "start": 452.16, "end": 455.16, "text": " It will say everything twice.", "tokens": [51466, 467, 486, 584, 1203, 6091, 13, 51616], "temperature": 0.0, "avg_logprob": -0.19747231457684492, "compression_ratio": 1.4816753926701571, "no_speech_prob": 0.0072249057702720165}, {"id": 93, "seek": 43012, "start": 455.16, "end": 457.16, "text": " It's fine.", "tokens": [51616, 467, 311, 2489, 13, 51716], "temperature": 0.0, "avg_logprob": -0.19747231457684492, "compression_ratio": 1.4816753926701571, "no_speech_prob": 0.0072249057702720165}, {"id": 94, "seek": 45716, "start": 457.20000000000005, "end": 469.20000000000005, "text": " So at a very high level, this is one of the slides I show people when they ask me why I don't use the phrase AI.", "tokens": [50366, 407, 412, 257, 588, 1090, 1496, 11, 341, 307, 472, 295, 264, 9788, 286, 855, 561, 562, 436, 1029, 385, 983, 286, 500, 380, 764, 264, 9535, 7318, 13, 50966], "temperature": 0.0, "avg_logprob": -0.16013852528163364, "compression_ratio": 1.656084656084656, "no_speech_prob": 0.0010968191782012582}, {"id": 95, "seek": 45716, "start": 469.20000000000005, "end": 478.20000000000005, "text": " It's because generally speaking, when people are talking about artificial intelligence, they are not talking about the entire field of artificial intelligence.", "tokens": [50966, 467, 311, 570, 5101, 4124, 11, 562, 561, 366, 1417, 466, 11677, 7599, 11, 436, 366, 406, 1417, 466, 264, 2302, 2519, 295, 11677, 7599, 13, 51416], "temperature": 0.0, "avg_logprob": -0.16013852528163364, "compression_ratio": 1.656084656084656, "no_speech_prob": 0.0010968191782012582}, {"id": 96, "seek": 45716, "start": 478.20000000000005, "end": 481.20000000000005, "text": " They are talking about machine learning.", "tokens": [51416, 814, 366, 1417, 466, 3479, 2539, 13, 51566], "temperature": 0.0, "avg_logprob": -0.16013852528163364, "compression_ratio": 1.656084656084656, "no_speech_prob": 0.0010968191782012582}, {"id": 97, "seek": 48120, "start": 481.24, "end": 487.24, "text": " So we can break it down into roughly two camps.", "tokens": [50366, 407, 321, 393, 1821, 309, 760, 666, 9810, 732, 16573, 13, 50666], "temperature": 0.0, "avg_logprob": -0.10752842750078366, "compression_ratio": 1.5480769230769231, "no_speech_prob": 0.0041910819709300995}, {"id": 98, "seek": 48120, "start": 487.24, "end": 497.24, "text": " And I call them camps because people are really settled in one or settled in the other, and they usually don't switch back and forth.", "tokens": [50666, 400, 286, 818, 552, 16573, 570, 561, 366, 534, 14819, 294, 472, 420, 14819, 294, 264, 661, 11, 293, 436, 2673, 500, 380, 3679, 646, 293, 5220, 13, 51166], "temperature": 0.0, "avg_logprob": -0.10752842750078366, "compression_ratio": 1.5480769230769231, "no_speech_prob": 0.0041910819709300995}, {"id": 99, "seek": 48120, "start": 497.24, "end": 499.24, "text": " That's been my experience anyway.", "tokens": [51166, 663, 311, 668, 452, 1752, 4033, 13, 51266], "temperature": 0.0, "avg_logprob": -0.10752842750078366, "compression_ratio": 1.5480769230769231, "no_speech_prob": 0.0041910819709300995}, {"id": 100, "seek": 48120, "start": 499.24, "end": 504.24, "text": " So we've got symbolic artificial intelligence, the logic, logical AI.", "tokens": [51266, 407, 321, 600, 658, 25755, 11677, 7599, 11, 264, 9952, 11, 14978, 7318, 13, 51516], "temperature": 0.0, "avg_logprob": -0.10752842750078366, "compression_ratio": 1.5480769230769231, "no_speech_prob": 0.0041910819709300995}, {"id": 101, "seek": 48120, "start": 504.24, "end": 507.24, "text": " It's also referred to as logical AI.", "tokens": [51516, 467, 311, 611, 10839, 281, 382, 14978, 7318, 13, 51666], "temperature": 0.0, "avg_logprob": -0.10752842750078366, "compression_ratio": 1.5480769230769231, "no_speech_prob": 0.0041910819709300995}, {"id": 102, "seek": 50724, "start": 507.28000000000003, "end": 509.28000000000003, "text": " How do people think?", "tokens": [50366, 1012, 360, 561, 519, 30, 50466], "temperature": 0.0, "avg_logprob": -0.09485227220198687, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.004813824314624071}, {"id": 103, "seek": 50724, "start": 509.28000000000003, "end": 516.28, "text": " How do we teach machines to think in ways that are similar to how people actually think?", "tokens": [50466, 1012, 360, 321, 2924, 8379, 281, 519, 294, 2098, 300, 366, 2531, 281, 577, 561, 767, 519, 30, 50816], "temperature": 0.0, "avg_logprob": -0.09485227220198687, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.004813824314624071}, {"id": 104, "seek": 50724, "start": 516.28, "end": 520.28, "text": " So this is where cognitive science really comes into play.", "tokens": [50816, 407, 341, 307, 689, 15605, 3497, 534, 1487, 666, 862, 13, 51016], "temperature": 0.0, "avg_logprob": -0.09485227220198687, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.004813824314624071}, {"id": 105, "seek": 50724, "start": 520.28, "end": 533.28, "text": " And then the much bigger circle up there is what we're mostly concerned with these days is machine learning, the math.", "tokens": [51016, 400, 550, 264, 709, 3801, 6329, 493, 456, 307, 437, 321, 434, 5240, 5922, 365, 613, 1708, 307, 3479, 2539, 11, 264, 5221, 13, 51666], "temperature": 0.0, "avg_logprob": -0.09485227220198687, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.004813824314624071}, {"id": 106, "seek": 53328, "start": 533.3199999999999, "end": 541.3199999999999, "text": " And this is what I tend to characterize as thinking is hard.", "tokens": [50366, 400, 341, 307, 437, 286, 3928, 281, 38463, 382, 1953, 307, 1152, 13, 50766], "temperature": 0.0, "avg_logprob": -0.13490021418011378, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.018193302676081657}, {"id": 107, "seek": 53328, "start": 541.3199999999999, "end": 550.3199999999999, "text": " We can probably build a model that comes close and then we'll do some math and we want to get stuff done.", "tokens": [50766, 492, 393, 1391, 1322, 257, 2316, 300, 1487, 1998, 293, 550, 321, 603, 360, 512, 5221, 293, 321, 528, 281, 483, 1507, 1096, 13, 51216], "temperature": 0.0, "avg_logprob": -0.13490021418011378, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.018193302676081657}, {"id": 108, "seek": 53328, "start": 550.3199999999999, "end": 557.3199999999999, "text": " So we're just going to use the data that we've got and cross our fingers.", "tokens": [51216, 407, 321, 434, 445, 516, 281, 764, 264, 1412, 300, 321, 600, 658, 293, 3278, 527, 7350, 13, 51566], "temperature": 0.0, "avg_logprob": -0.13490021418011378, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.018193302676081657}, {"id": 109, "seek": 55732, "start": 557.36, "end": 565.36, "text": " You can argue with me about that opinion, all you like.", "tokens": [50366, 509, 393, 9695, 365, 385, 466, 300, 4800, 11, 439, 291, 411, 13, 50766], "temperature": 0.0, "avg_logprob": -0.1488833138436982, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.000909502967260778}, {"id": 110, "seek": 55732, "start": 565.36, "end": 567.36, "text": " I'm cool with that.", "tokens": [50766, 286, 478, 1627, 365, 300, 13, 50866], "temperature": 0.0, "avg_logprob": -0.1488833138436982, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.000909502967260778}, {"id": 111, "seek": 55732, "start": 567.36, "end": 579.36, "text": " So while I do have this, as I mentioned, the deep abiding love for symbolic AI, we're focusing primarily on machine learning here.", "tokens": [50866, 407, 1339, 286, 360, 362, 341, 11, 382, 286, 2835, 11, 264, 2452, 410, 2819, 959, 337, 25755, 7318, 11, 321, 434, 8416, 10029, 322, 3479, 2539, 510, 13, 51466], "temperature": 0.0, "avg_logprob": -0.1488833138436982, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.000909502967260778}, {"id": 112, "seek": 55732, "start": 579.36, "end": 584.36, "text": " Unless anybody wants to talk about slime.", "tokens": [51466, 16581, 4472, 2738, 281, 751, 466, 20650, 13, 51716], "temperature": 0.0, "avg_logprob": -0.1488833138436982, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.000909502967260778}, {"id": 113, "seek": 58436, "start": 584.4, "end": 588.4, "text": " Not that slime.", "tokens": [50366, 1726, 300, 20650, 13, 50566], "temperature": 0.0, "avg_logprob": -0.20820279848777642, "compression_ratio": 1.5, "no_speech_prob": 0.0017509795725345612}, {"id": 114, "seek": 58436, "start": 588.4, "end": 595.4, "text": " So some elements of AI, or machine learning, see?", "tokens": [50566, 407, 512, 4959, 295, 7318, 11, 420, 3479, 2539, 11, 536, 30, 50916], "temperature": 0.0, "avg_logprob": -0.20820279848777642, "compression_ratio": 1.5, "no_speech_prob": 0.0017509795725345612}, {"id": 115, "seek": 58436, "start": 595.4, "end": 600.4, "text": " I'm also getting hit by the AI means machine learning bug.", "tokens": [50916, 286, 478, 611, 1242, 2045, 538, 264, 7318, 1355, 3479, 2539, 7426, 13, 51166], "temperature": 0.0, "avg_logprob": -0.20820279848777642, "compression_ratio": 1.5, "no_speech_prob": 0.0017509795725345612}, {"id": 116, "seek": 58436, "start": 600.4, "end": 611.4, "text": " So some possible elements do include things like what data do we have that go into training the system?", "tokens": [51166, 407, 512, 1944, 4959, 360, 4090, 721, 411, 437, 1412, 360, 321, 362, 300, 352, 666, 3097, 264, 1185, 30, 51716], "temperature": 0.0, "avg_logprob": -0.20820279848777642, "compression_ratio": 1.5, "no_speech_prob": 0.0017509795725345612}, {"id": 117, "seek": 61140, "start": 611.4399999999999, "end": 613.4399999999999, "text": " How do we actually train the system?", "tokens": [50366, 1012, 360, 321, 767, 3847, 264, 1185, 30, 50466], "temperature": 0.0, "avg_logprob": -0.07973966995875041, "compression_ratio": 1.630057803468208, "no_speech_prob": 0.001497812569141388}, {"id": 118, "seek": 61140, "start": 613.4399999999999, "end": 617.4399999999999, "text": " How do we evaluate the system?", "tokens": [50466, 1012, 360, 321, 13059, 264, 1185, 30, 50666], "temperature": 0.0, "avg_logprob": -0.07973966995875041, "compression_ratio": 1.630057803468208, "no_speech_prob": 0.001497812569141388}, {"id": 119, "seek": 61140, "start": 617.4399999999999, "end": 622.4399999999999, "text": " All of the different elements, is there a model as an output?", "tokens": [50666, 1057, 295, 264, 819, 4959, 11, 307, 456, 257, 2316, 382, 364, 5598, 30, 50916], "temperature": 0.0, "avg_logprob": -0.07973966995875041, "compression_ratio": 1.630057803468208, "no_speech_prob": 0.001497812569141388}, {"id": 120, "seek": 61140, "start": 622.4399999999999, "end": 628.4399999999999, "text": " Is there a user interface as a way of interacting with machine learning?", "tokens": [50916, 1119, 456, 257, 4195, 9226, 382, 257, 636, 295, 18017, 365, 3479, 2539, 30, 51216], "temperature": 0.0, "avg_logprob": -0.07973966995875041, "compression_ratio": 1.630057803468208, "no_speech_prob": 0.001497812569141388}, {"id": 121, "seek": 61140, "start": 628.4399999999999, "end": 636.4399999999999, "text": " Now, not all of these are going to be present in every machine learning system.", "tokens": [51216, 823, 11, 406, 439, 295, 613, 366, 516, 281, 312, 1974, 294, 633, 3479, 2539, 1185, 13, 51616], "temperature": 0.0, "avg_logprob": -0.07973966995875041, "compression_ratio": 1.630057803468208, "no_speech_prob": 0.001497812569141388}, {"id": 122, "seek": 63644, "start": 636.48, "end": 645.48, "text": " It kind of blows some people's minds to realize that you can have machine learning without a model.", "tokens": [50366, 467, 733, 295, 18458, 512, 561, 311, 9634, 281, 4325, 300, 291, 393, 362, 3479, 2539, 1553, 257, 2316, 13, 50816], "temperature": 0.0, "avg_logprob": -0.1289963645319785, "compression_ratio": 1.655844155844156, "no_speech_prob": 0.0014536480884999037}, {"id": 123, "seek": 63644, "start": 645.48, "end": 653.48, "text": " Or you can have machine learning without a task or prompt.", "tokens": [50816, 1610, 291, 393, 362, 3479, 2539, 1553, 257, 5633, 420, 12391, 13, 51216], "temperature": 0.0, "avg_logprob": -0.1289963645319785, "compression_ratio": 1.655844155844156, "no_speech_prob": 0.0014536480884999037}, {"id": 124, "seek": 63644, "start": 653.48, "end": 655.48, "text": " But it's true.", "tokens": [51216, 583, 309, 311, 2074, 13, 51316], "temperature": 0.0, "avg_logprob": -0.1289963645319785, "compression_ratio": 1.655844155844156, "no_speech_prob": 0.0014536480884999037}, {"id": 125, "seek": 63644, "start": 655.48, "end": 660.48, "text": " And we have to account for that when thinking about open source machine learning.", "tokens": [51316, 400, 321, 362, 281, 2696, 337, 300, 562, 1953, 466, 1269, 4009, 3479, 2539, 13, 51566], "temperature": 0.0, "avg_logprob": -0.1289963645319785, "compression_ratio": 1.655844155844156, "no_speech_prob": 0.0014536480884999037}, {"id": 126, "seek": 66048, "start": 660.52, "end": 668.52, "text": " And when we're looking at all of these different components, it gets a little bit hard to reason about.", "tokens": [50366, 400, 562, 321, 434, 1237, 412, 439, 295, 613, 819, 6677, 11, 309, 2170, 257, 707, 857, 1152, 281, 1778, 466, 13, 50766], "temperature": 0.0, "avg_logprob": -0.10380138669695173, "compression_ratio": 1.4153005464480874, "no_speech_prob": 0.0016984109533950686}, {"id": 127, "seek": 66048, "start": 668.52, "end": 679.52, "text": " But if we reduce the dimensionality, PCA pun, we see roughly four buckets emerge.", "tokens": [50766, 583, 498, 321, 5407, 264, 10139, 1860, 11, 6465, 32, 4468, 11, 321, 536, 9810, 1451, 32191, 21511, 13, 51316], "temperature": 0.0, "avg_logprob": -0.10380138669695173, "compression_ratio": 1.4153005464480874, "no_speech_prob": 0.0016984109533950686}, {"id": 128, "seek": 66048, "start": 679.52, "end": 683.52, "text": " We've got the data, which is pretty familiar to us.", "tokens": [51316, 492, 600, 658, 264, 1412, 11, 597, 307, 1238, 4963, 281, 505, 13, 51516], "temperature": 0.0, "avg_logprob": -0.10380138669695173, "compression_ratio": 1.4153005464480874, "no_speech_prob": 0.0016984109533950686}, {"id": 129, "seek": 66048, "start": 683.52, "end": 685.52, "text": " We know what data is.", "tokens": [51516, 492, 458, 437, 1412, 307, 13, 51616], "temperature": 0.0, "avg_logprob": -0.10380138669695173, "compression_ratio": 1.4153005464480874, "no_speech_prob": 0.0016984109533950686}, {"id": 130, "seek": 68552, "start": 685.56, "end": 692.56, "text": " We've got a good understanding of what might be training data, what's validation tasks, et cetera.", "tokens": [50366, 492, 600, 658, 257, 665, 3701, 295, 437, 1062, 312, 3097, 1412, 11, 437, 311, 24071, 9608, 11, 1030, 11458, 13, 50716], "temperature": 0.0, "avg_logprob": -0.11908756932125816, "compression_ratio": 1.5, "no_speech_prob": 0.0022830977104604244}, {"id": 131, "seek": 68552, "start": 692.56, "end": 696.56, "text": " We've got code, also a well-worn path for open source.", "tokens": [50716, 492, 600, 658, 3089, 11, 611, 257, 731, 12, 86, 1865, 3100, 337, 1269, 4009, 13, 50916], "temperature": 0.0, "avg_logprob": -0.11908756932125816, "compression_ratio": 1.5, "no_speech_prob": 0.0022830977104604244}, {"id": 132, "seek": 68552, "start": 696.56, "end": 700.56, "text": " And then we have what I call the other stuff.", "tokens": [50916, 400, 550, 321, 362, 437, 286, 818, 264, 661, 1507, 13, 51116], "temperature": 0.0, "avg_logprob": -0.11908756932125816, "compression_ratio": 1.5, "no_speech_prob": 0.0022830977104604244}, {"id": 133, "seek": 68552, "start": 700.56, "end": 707.56, "text": " Because one of my skills is not naming things.", "tokens": [51116, 1436, 472, 295, 452, 3942, 307, 406, 25290, 721, 13, 51466], "temperature": 0.0, "avg_logprob": -0.11908756932125816, "compression_ratio": 1.5, "no_speech_prob": 0.0022830977104604244}, {"id": 134, "seek": 68552, "start": 707.56, "end": 711.56, "text": " And then finally, we've got output.", "tokens": [51466, 400, 550, 2721, 11, 321, 600, 658, 5598, 13, 51666], "temperature": 0.0, "avg_logprob": -0.11908756932125816, "compression_ratio": 1.5, "no_speech_prob": 0.0022830977104604244}, {"id": 135, "seek": 71156, "start": 711.5999999999999, "end": 718.5999999999999, "text": " So by doing a rough grouping with K equals four, we wind up with these four buckets.", "tokens": [50366, 407, 538, 884, 257, 5903, 40149, 365, 591, 6915, 1451, 11, 321, 2468, 493, 365, 613, 1451, 32191, 13, 50716], "temperature": 0.0, "avg_logprob": -0.11060382843017579, "compression_ratio": 1.53, "no_speech_prob": 0.0009235632605850697}, {"id": 136, "seek": 71156, "start": 718.5999999999999, "end": 732.5999999999999, "text": " And I think that by thinking of them in this schematic, it makes it much easier to tackle the challenges that we face one by one.", "tokens": [50716, 400, 286, 519, 300, 538, 1953, 295, 552, 294, 341, 44739, 11, 309, 1669, 309, 709, 3571, 281, 14896, 264, 4759, 300, 321, 1851, 472, 538, 472, 13, 51416], "temperature": 0.0, "avg_logprob": -0.11060382843017579, "compression_ratio": 1.53, "no_speech_prob": 0.0009235632605850697}, {"id": 137, "seek": 71156, "start": 732.5999999999999, "end": 736.5999999999999, "text": " Now, some elements might appear in multiple buckets.", "tokens": [51416, 823, 11, 512, 4959, 1062, 4204, 294, 3866, 32191, 13, 51616], "temperature": 0.0, "avg_logprob": -0.11060382843017579, "compression_ratio": 1.53, "no_speech_prob": 0.0009235632605850697}, {"id": 138, "seek": 71156, "start": 736.5999999999999, "end": 740.5999999999999, "text": " Not on this slide, because simplicity.", "tokens": [51616, 1726, 322, 341, 4137, 11, 570, 25632, 13, 51816], "temperature": 0.0, "avg_logprob": -0.11060382843017579, "compression_ratio": 1.53, "no_speech_prob": 0.0009235632605850697}, {"id": 139, "seek": 74060, "start": 740.64, "end": 743.64, "text": " And some might not appear at all.", "tokens": [50366, 400, 512, 1062, 406, 4204, 412, 439, 13, 50516], "temperature": 0.0, "avg_logprob": -0.16502411511479592, "compression_ratio": 1.3488372093023255, "no_speech_prob": 0.00022677901142742485}, {"id": 140, "seek": 74060, "start": 743.64, "end": 746.64, "text": " But it's a starting point.", "tokens": [50516, 583, 309, 311, 257, 2891, 935, 13, 50666], "temperature": 0.0, "avg_logprob": -0.16502411511479592, "compression_ratio": 1.3488372093023255, "no_speech_prob": 0.00022677901142742485}, {"id": 141, "seek": 74060, "start": 750.64, "end": 756.64, "text": " So let's first talk about data.", "tokens": [50866, 407, 718, 311, 700, 751, 466, 1412, 13, 51166], "temperature": 0.0, "avg_logprob": -0.16502411511479592, "compression_ratio": 1.3488372093023255, "no_speech_prob": 0.00022677901142742485}, {"id": 142, "seek": 74060, "start": 761.64, "end": 769.64, "text": " So when it comes to machine learning and data, we have some interesting problems.", "tokens": [51416, 407, 562, 309, 1487, 281, 3479, 2539, 293, 1412, 11, 321, 362, 512, 1880, 2740, 13, 51816], "temperature": 0.0, "avg_logprob": -0.16502411511479592, "compression_ratio": 1.3488372093023255, "no_speech_prob": 0.00022677901142742485}, {"id": 143, "seek": 76964, "start": 769.68, "end": 771.68, "text": " Some of them are known.", "tokens": [50366, 2188, 295, 552, 366, 2570, 13, 50466], "temperature": 0.0, "avg_logprob": -0.14252268803584112, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.0005698119639419019}, {"id": 144, "seek": 76964, "start": 771.68, "end": 775.68, "text": " Some of them are unknown.", "tokens": [50466, 2188, 295, 552, 366, 9841, 13, 50666], "temperature": 0.0, "avg_logprob": -0.14252268803584112, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.0005698119639419019}, {"id": 145, "seek": 76964, "start": 775.68, "end": 778.68, "text": " We have a lot of data out there.", "tokens": [50666, 492, 362, 257, 688, 295, 1412, 484, 456, 13, 50816], "temperature": 0.0, "avg_logprob": -0.14252268803584112, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.0005698119639419019}, {"id": 146, "seek": 76964, "start": 778.68, "end": 783.68, "text": " Machine learning research has been going on for what, since the 50s?", "tokens": [50816, 22155, 2539, 2132, 575, 668, 516, 322, 337, 437, 11, 1670, 264, 2625, 82, 30, 51066], "temperature": 0.0, "avg_logprob": -0.14252268803584112, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.0005698119639419019}, {"id": 147, "seek": 76964, "start": 783.68, "end": 786.68, "text": " Right? Ish?", "tokens": [51066, 1779, 30, 42854, 30, 51216], "temperature": 0.0, "avg_logprob": -0.14252268803584112, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.0005698119639419019}, {"id": 148, "seek": 76964, "start": 786.68, "end": 796.68, "text": " And that means that a lot of the data that has been used in this research doesn't have known provenance.", "tokens": [51216, 400, 300, 1355, 300, 257, 688, 295, 264, 1412, 300, 575, 668, 1143, 294, 341, 2132, 1177, 380, 362, 2570, 12785, 719, 13, 51716], "temperature": 0.0, "avg_logprob": -0.14252268803584112, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.0005698119639419019}, {"id": 149, "seek": 79668, "start": 796.7199999999999, "end": 802.7199999999999, "text": " So we don't actually know where some of the data came from.", "tokens": [50366, 407, 321, 500, 380, 767, 458, 689, 512, 295, 264, 1412, 1361, 490, 13, 50666], "temperature": 0.0, "avg_logprob": -0.10551711132651881, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.0043917750008404255}, {"id": 150, "seek": 79668, "start": 802.7199999999999, "end": 807.7199999999999, "text": " And if we're talking about things, I'm not going to talk about licenses, by the way.", "tokens": [50666, 400, 498, 321, 434, 1417, 466, 721, 11, 286, 478, 406, 516, 281, 751, 466, 32821, 11, 538, 264, 636, 13, 50916], "temperature": 0.0, "avg_logprob": -0.10551711132651881, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.0043917750008404255}, {"id": 151, "seek": 79668, "start": 807.7199999999999, "end": 809.7199999999999, "text": " I forgot to mention that.", "tokens": [50916, 286, 5298, 281, 2152, 300, 13, 51016], "temperature": 0.0, "avg_logprob": -0.10551711132651881, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.0043917750008404255}, {"id": 152, "seek": 79668, "start": 809.7199999999999, "end": 811.7199999999999, "text": " I'm not talking about licenses.", "tokens": [51016, 286, 478, 406, 1417, 466, 32821, 13, 51116], "temperature": 0.0, "avg_logprob": -0.10551711132651881, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.0043917750008404255}, {"id": 153, "seek": 79668, "start": 811.7199999999999, "end": 818.7199999999999, "text": " I'm just going to talk about the challenges in building open machine learning systems.", "tokens": [51116, 286, 478, 445, 516, 281, 751, 466, 264, 4759, 294, 2390, 1269, 3479, 2539, 3652, 13, 51466], "temperature": 0.0, "avg_logprob": -0.10551711132651881, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.0043917750008404255}, {"id": 154, "seek": 81872, "start": 818.76, "end": 828.76, "text": " But when there's data without known provenance, we don't know if it is truly game for us to use.", "tokens": [50366, 583, 562, 456, 311, 1412, 1553, 2570, 12785, 719, 11, 321, 500, 380, 458, 498, 309, 307, 4908, 1216, 337, 505, 281, 764, 13, 50866], "temperature": 0.0, "avg_logprob": -0.10848217579855848, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.08989866077899933}, {"id": 155, "seek": 81872, "start": 828.76, "end": 838.76, "text": " If we have data that we don't know how it was collected, we don't know if it's truly game for us to use.", "tokens": [50866, 759, 321, 362, 1412, 300, 321, 500, 380, 458, 577, 309, 390, 11087, 11, 321, 500, 380, 458, 498, 309, 311, 4908, 1216, 337, 505, 281, 764, 13, 51366], "temperature": 0.0, "avg_logprob": -0.10848217579855848, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.08989866077899933}, {"id": 156, "seek": 81872, "start": 838.76, "end": 842.76, "text": " Or if it suits our purpose.", "tokens": [51366, 1610, 498, 309, 15278, 527, 4334, 13, 51566], "temperature": 0.0, "avg_logprob": -0.10848217579855848, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.08989866077899933}, {"id": 157, "seek": 84276, "start": 842.8, "end": 850.8, "text": " It could be an incredibly flawed data set, and we have no way of knowing.", "tokens": [50366, 467, 727, 312, 364, 6252, 38823, 1412, 992, 11, 293, 321, 362, 572, 636, 295, 5276, 13, 50766], "temperature": 0.0, "avg_logprob": -0.11172073118148311, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.0014994980301707983}, {"id": 158, "seek": 84276, "start": 852.8, "end": 861.8, "text": " In terms of privacy, we've got a few big challenges with de-identification and anonymization.", "tokens": [50866, 682, 2115, 295, 11427, 11, 321, 600, 658, 257, 1326, 955, 4759, 365, 368, 12, 1078, 3774, 293, 37293, 2144, 13, 51316], "temperature": 0.0, "avg_logprob": -0.11172073118148311, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.0014994980301707983}, {"id": 159, "seek": 84276, "start": 861.8, "end": 867.8, "text": " There's a lot of really interesting work that can be done in machine learning,", "tokens": [51316, 821, 311, 257, 688, 295, 534, 1880, 589, 300, 393, 312, 1096, 294, 3479, 2539, 11, 51616], "temperature": 0.0, "avg_logprob": -0.11172073118148311, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.0014994980301707983}, {"id": 160, "seek": 86780, "start": 867.8399999999999, "end": 872.8399999999999, "text": " but can't necessarily be done in an open way.", "tokens": [50366, 457, 393, 380, 4725, 312, 1096, 294, 364, 1269, 636, 13, 50616], "temperature": 0.0, "avg_logprob": -0.11987343119151557, "compression_ratio": 1.427710843373494, "no_speech_prob": 0.0026675225235521793}, {"id": 161, "seek": 86780, "start": 872.8399999999999, "end": 878.8399999999999, "text": " And I don't actually know how to solve that particular beast,", "tokens": [50616, 400, 286, 500, 380, 767, 458, 577, 281, 5039, 300, 1729, 13464, 11, 50916], "temperature": 0.0, "avg_logprob": -0.11987343119151557, "compression_ratio": 1.427710843373494, "no_speech_prob": 0.0026675225235521793}, {"id": 162, "seek": 86780, "start": 878.8399999999999, "end": 885.8399999999999, "text": " because I feel like I'm kind of splitten, too, on that.", "tokens": [50916, 570, 286, 841, 411, 286, 478, 733, 295, 4732, 2987, 11, 886, 11, 322, 300, 13, 51266], "temperature": 0.0, "avg_logprob": -0.11987343119151557, "compression_ratio": 1.427710843373494, "no_speech_prob": 0.0026675225235521793}, {"id": 163, "seek": 86780, "start": 885.8399999999999, "end": 893.8399999999999, "text": " On the one hand, I do think that having full access to the training data,", "tokens": [51266, 1282, 264, 472, 1011, 11, 286, 360, 519, 300, 1419, 1577, 2105, 281, 264, 3097, 1412, 11, 51666], "temperature": 0.0, "avg_logprob": -0.11987343119151557, "compression_ratio": 1.427710843373494, "no_speech_prob": 0.0026675225235521793}, {"id": 164, "seek": 89384, "start": 893.88, "end": 901.88, "text": " validation data, test, et cetera, is really important for building open systems.", "tokens": [50366, 24071, 1412, 11, 1500, 11, 1030, 11458, 11, 307, 534, 1021, 337, 2390, 1269, 3652, 13, 50766], "temperature": 0.0, "avg_logprob": -0.14953327178955078, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.0023509294260293245}, {"id": 165, "seek": 89384, "start": 901.88, "end": 907.88, "text": " I don't think it can come at the expense of people's safety.", "tokens": [50766, 286, 500, 380, 519, 309, 393, 808, 412, 264, 18406, 295, 561, 311, 4514, 13, 51066], "temperature": 0.0, "avg_logprob": -0.14953327178955078, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.0023509294260293245}, {"id": 166, "seek": 89384, "start": 907.88, "end": 919.88, "text": " And so if you are training something based on data that includes personally identifiable information,", "tokens": [51066, 400, 370, 498, 291, 366, 3097, 746, 2361, 322, 1412, 300, 5974, 5665, 2473, 30876, 1589, 11, 51666], "temperature": 0.0, "avg_logprob": -0.14953327178955078, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.0023509294260293245}, {"id": 167, "seek": 91988, "start": 919.92, "end": 922.92, "text": " we kind of have to weigh that.", "tokens": [50366, 321, 733, 295, 362, 281, 13843, 300, 13, 50516], "temperature": 0.0, "avg_logprob": -0.19444739434026903, "compression_ratio": 1.3205128205128205, "no_speech_prob": 0.001521466881968081}, {"id": 168, "seek": 91988, "start": 922.92, "end": 930.92, "text": " There is that question of, should that be an open source system in the first place?", "tokens": [50516, 821, 307, 300, 1168, 295, 11, 820, 300, 312, 364, 1269, 4009, 1185, 294, 264, 700, 1081, 30, 50916], "temperature": 0.0, "avg_logprob": -0.19444739434026903, "compression_ratio": 1.3205128205128205, "no_speech_prob": 0.001521466881968081}, {"id": 169, "seek": 91988, "start": 930.92, "end": 934.92, "text": " I know that's a very controversial thing to say at FOSDEM.", "tokens": [50916, 286, 458, 300, 311, 257, 588, 17323, 551, 281, 584, 412, 479, 4367, 35, 6683, 13, 51116], "temperature": 0.0, "avg_logprob": -0.19444739434026903, "compression_ratio": 1.3205128205128205, "no_speech_prob": 0.001521466881968081}, {"id": 170, "seek": 91988, "start": 934.92, "end": 939.92, "text": " So I can leave now, if you like.", "tokens": [51116, 407, 286, 393, 1856, 586, 11, 498, 291, 411, 13, 51366], "temperature": 0.0, "avg_logprob": -0.19444739434026903, "compression_ratio": 1.3205128205128205, "no_speech_prob": 0.001521466881968081}, {"id": 171, "seek": 93992, "start": 939.9599999999999, "end": 947.9599999999999, "text": " For systems that also incorporate user feedback as part of the training data,", "tokens": [50366, 1171, 3652, 300, 611, 16091, 4195, 5824, 382, 644, 295, 264, 3097, 1412, 11, 50766], "temperature": 0.0, "avg_logprob": -0.13158301512400308, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.002629736438393593}, {"id": 172, "seek": 93992, "start": 947.9599999999999, "end": 952.9599999999999, "text": " because that's a fun place to get into,", "tokens": [50766, 570, 300, 311, 257, 1019, 1081, 281, 483, 666, 11, 51016], "temperature": 0.0, "avg_logprob": -0.13158301512400308, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.002629736438393593}, {"id": 173, "seek": 93992, "start": 952.9599999999999, "end": 961.9599999999999, "text": " how do you build in, again, that de-identification and anonymization?", "tokens": [51016, 577, 360, 291, 1322, 294, 11, 797, 11, 300, 368, 12, 1078, 3774, 293, 37293, 2144, 30, 51466], "temperature": 0.0, "avg_logprob": -0.13158301512400308, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.002629736438393593}, {"id": 174, "seek": 93992, "start": 961.9599999999999, "end": 966.9599999999999, "text": " And there are things like, okay, well, how are we actually going about splitting the corpus?", "tokens": [51466, 400, 456, 366, 721, 411, 11, 1392, 11, 731, 11, 577, 366, 321, 767, 516, 466, 30348, 264, 1181, 31624, 30, 51716], "temperature": 0.0, "avg_logprob": -0.13158301512400308, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.002629736438393593}, {"id": 175, "seek": 96696, "start": 967.0, "end": 971.0, "text": " If we are splitting the corpus into training and validation data,", "tokens": [50366, 759, 321, 366, 30348, 264, 1181, 31624, 666, 3097, 293, 24071, 1412, 11, 50566], "temperature": 0.0, "avg_logprob": -0.08739077660345262, "compression_ratio": 1.4829545454545454, "no_speech_prob": 0.0011688319500535727}, {"id": 176, "seek": 96696, "start": 971.0, "end": 975.0, "text": " it's actually important to know what proportions we're using", "tokens": [50566, 309, 311, 767, 1021, 281, 458, 437, 32482, 321, 434, 1228, 50766], "temperature": 0.0, "avg_logprob": -0.08739077660345262, "compression_ratio": 1.4829545454545454, "no_speech_prob": 0.0011688319500535727}, {"id": 177, "seek": 96696, "start": 975.0, "end": 981.0, "text": " and how we're sampling in order to do that splitting.", "tokens": [50766, 293, 577, 321, 434, 21179, 294, 1668, 281, 360, 300, 30348, 13, 51066], "temperature": 0.0, "avg_logprob": -0.08739077660345262, "compression_ratio": 1.4829545454545454, "no_speech_prob": 0.0011688319500535727}, {"id": 178, "seek": 96696, "start": 981.0, "end": 989.0, "text": " But again, some of these may not be applicable for all machine learning systems.", "tokens": [51066, 583, 797, 11, 512, 295, 613, 815, 406, 312, 21142, 337, 439, 3479, 2539, 3652, 13, 51466], "temperature": 0.0, "avg_logprob": -0.08739077660345262, "compression_ratio": 1.4829545454545454, "no_speech_prob": 0.0011688319500535727}, {"id": 179, "seek": 98900, "start": 989.04, "end": 998.04, "text": " So one of the things that I kept asking myself is,", "tokens": [50366, 407, 472, 295, 264, 721, 300, 286, 4305, 3365, 2059, 307, 11, 50816], "temperature": 0.0, "avg_logprob": -0.15435571490593678, "compression_ratio": 1.463768115942029, "no_speech_prob": 0.0015240180073305964}, {"id": 180, "seek": 98900, "start": 998.04, "end": 1007.04, "text": " are these things required to recompile, for some definition of recompile, a model?", "tokens": [50816, 366, 613, 721, 4739, 281, 48000, 794, 11, 337, 512, 7123, 295, 48000, 794, 11, 257, 2316, 30, 51266], "temperature": 0.0, "avg_logprob": -0.15435571490593678, "compression_ratio": 1.463768115942029, "no_speech_prob": 0.0015240180073305964}, {"id": 181, "seek": 98900, "start": 1007.04, "end": 1016.04, "text": " So if I wanted to create the model from scratch and build it myself,", "tokens": [51266, 407, 498, 286, 1415, 281, 1884, 264, 2316, 490, 8459, 293, 1322, 309, 2059, 11, 51716], "temperature": 0.0, "avg_logprob": -0.15435571490593678, "compression_ratio": 1.463768115942029, "no_speech_prob": 0.0015240180073305964}, {"id": 182, "seek": 101604, "start": 1016.0799999999999, "end": 1022.0799999999999, "text": " what do I need?", "tokens": [50366, 437, 360, 286, 643, 30, 50666], "temperature": 0.0, "avg_logprob": -0.1320686032695155, "compression_ratio": 1.4256756756756757, "no_speech_prob": 0.00042980071157217026}, {"id": 183, "seek": 101604, "start": 1022.0799999999999, "end": 1024.08, "text": " The entire dataset?", "tokens": [50666, 440, 2302, 28872, 30, 50766], "temperature": 0.0, "avg_logprob": -0.1320686032695155, "compression_ratio": 1.4256756756756757, "no_speech_prob": 0.00042980071157217026}, {"id": 184, "seek": 101604, "start": 1024.08, "end": 1030.08, "text": " If you want to show hands and tell me if you think it's required,", "tokens": [50766, 759, 291, 528, 281, 855, 2377, 293, 980, 385, 498, 291, 519, 309, 311, 4739, 11, 51066], "temperature": 0.0, "avg_logprob": -0.1320686032695155, "compression_ratio": 1.4256756756756757, "no_speech_prob": 0.00042980071157217026}, {"id": 185, "seek": 101604, "start": 1030.08, "end": 1034.08, "text": " I'm fascinated to know, but there's no obligation to.", "tokens": [51066, 286, 478, 24597, 281, 458, 11, 457, 456, 311, 572, 20326, 281, 13, 51266], "temperature": 0.0, "avg_logprob": -0.1320686032695155, "compression_ratio": 1.4256756756756757, "no_speech_prob": 0.00042980071157217026}, {"id": 186, "seek": 101604, "start": 1034.08, "end": 1041.08, "text": " So is the entire dataset required to recompile a model?", "tokens": [51266, 407, 307, 264, 2302, 28872, 4739, 281, 48000, 794, 257, 2316, 30, 51616], "temperature": 0.0, "avg_logprob": -0.1320686032695155, "compression_ratio": 1.4256756756756757, "no_speech_prob": 0.00042980071157217026}, {"id": 187, "seek": 104108, "start": 1041.12, "end": 1047.12, "text": " Would a description of the data suffice?", "tokens": [50366, 6068, 257, 3855, 295, 264, 1412, 3889, 573, 30, 50666], "temperature": 0.0, "avg_logprob": -0.14891101574075633, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.001546089886687696}, {"id": 188, "seek": 104108, "start": 1047.12, "end": 1053.12, "text": " How about a datasheet?", "tokens": [50666, 1012, 466, 257, 20377, 38164, 30, 50966], "temperature": 0.0, "avg_logprob": -0.14891101574075633, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.001546089886687696}, {"id": 189, "seek": 104108, "start": 1053.12, "end": 1063.12, "text": " Who thinks that you need to know about how the data was collected?", "tokens": [50966, 2102, 7309, 300, 291, 643, 281, 458, 466, 577, 264, 1412, 390, 11087, 30, 51466], "temperature": 0.0, "avg_logprob": -0.14891101574075633, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.001546089886687696}, {"id": 190, "seek": 104108, "start": 1063.12, "end": 1065.12, "text": " Well, thank you.", "tokens": [51466, 1042, 11, 1309, 291, 13, 51566], "temperature": 0.0, "avg_logprob": -0.14891101574075633, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.001546089886687696}, {"id": 191, "seek": 104108, "start": 1065.12, "end": 1067.12, "text": " I appreciate you.", "tokens": [51566, 286, 4449, 291, 13, 51666], "temperature": 0.0, "avg_logprob": -0.14891101574075633, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.001546089886687696}, {"id": 192, "seek": 104108, "start": 1067.12, "end": 1069.12, "text": " I appreciate all of you.", "tokens": [51666, 286, 4449, 439, 295, 291, 13, 51766], "temperature": 0.0, "avg_logprob": -0.14891101574075633, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.001546089886687696}, {"id": 193, "seek": 106912, "start": 1069.1599999999999, "end": 1073.1599999999999, "text": " So I kept coming back to this question,", "tokens": [50366, 407, 286, 4305, 1348, 646, 281, 341, 1168, 11, 50566], "temperature": 0.0, "avg_logprob": -0.09723563989003499, "compression_ratio": 1.4465408805031446, "no_speech_prob": 0.0002000984677579254}, {"id": 194, "seek": 106912, "start": 1073.1599999999999, "end": 1075.1599999999999, "text": " is it required to recompile a model?", "tokens": [50566, 307, 309, 4739, 281, 48000, 794, 257, 2316, 30, 50666], "temperature": 0.0, "avg_logprob": -0.09723563989003499, "compression_ratio": 1.4465408805031446, "no_speech_prob": 0.0002000984677579254}, {"id": 195, "seek": 106912, "start": 1075.1599999999999, "end": 1085.1599999999999, "text": " And so you'll see that question as we go through some of these other sections.", "tokens": [50666, 400, 370, 291, 603, 536, 300, 1168, 382, 321, 352, 807, 512, 295, 613, 661, 10863, 13, 51166], "temperature": 0.0, "avg_logprob": -0.09723563989003499, "compression_ratio": 1.4465408805031446, "no_speech_prob": 0.0002000984677579254}, {"id": 196, "seek": 106912, "start": 1085.1599999999999, "end": 1091.1599999999999, "text": " I do take the stance that you need the entire dataset and the methodology,", "tokens": [51166, 286, 360, 747, 264, 21033, 300, 291, 643, 264, 2302, 28872, 293, 264, 24850, 11, 51466], "temperature": 0.0, "avg_logprob": -0.09723563989003499, "compression_ratio": 1.4465408805031446, "no_speech_prob": 0.0002000984677579254}, {"id": 197, "seek": 109116, "start": 1091.2, "end": 1099.2, "text": " but that introduces some big problems and big as in dollar signs,", "tokens": [50366, 457, 300, 31472, 512, 955, 2740, 293, 955, 382, 294, 7241, 7880, 11, 50766], "temperature": 0.0, "avg_logprob": -0.17848304060638928, "compression_ratio": 1.467455621301775, "no_speech_prob": 0.013790328055620193}, {"id": 198, "seek": 109116, "start": 1099.2, "end": 1102.2, "text": " because these corpus are not small.", "tokens": [50766, 570, 613, 1181, 31624, 366, 406, 1359, 13, 50916], "temperature": 0.0, "avg_logprob": -0.17848304060638928, "compression_ratio": 1.467455621301775, "no_speech_prob": 0.013790328055620193}, {"id": 199, "seek": 109116, "start": 1102.2, "end": 1106.2, "text": " Hosting them is expensive.", "tokens": [50916, 22047, 278, 552, 307, 5124, 13, 51116], "temperature": 0.0, "avg_logprob": -0.17848304060638928, "compression_ratio": 1.467455621301775, "no_speech_prob": 0.013790328055620193}, {"id": 200, "seek": 109116, "start": 1106.2, "end": 1119.2, "text": " So if we want to make open source machine learning open to all of the people who are interested in participating in it,", "tokens": [51116, 407, 498, 321, 528, 281, 652, 1269, 4009, 3479, 2539, 1269, 281, 439, 295, 264, 561, 567, 366, 3102, 294, 13950, 294, 309, 11, 51766], "temperature": 0.0, "avg_logprob": -0.17848304060638928, "compression_ratio": 1.467455621301775, "no_speech_prob": 0.013790328055620193}, {"id": 201, "seek": 111920, "start": 1119.24, "end": 1124.24, "text": " how do we break down the cost of doing so?", "tokens": [50366, 577, 360, 321, 1821, 760, 264, 2063, 295, 884, 370, 30, 50616], "temperature": 0.0, "avg_logprob": -0.15801573511379868, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.0010641226544976234}, {"id": 202, "seek": 111920, "start": 1124.24, "end": 1131.24, "text": " How do we make it available?", "tokens": [50616, 1012, 360, 321, 652, 309, 2435, 30, 50966], "temperature": 0.0, "avg_logprob": -0.15801573511379868, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.0010641226544976234}, {"id": 203, "seek": 111920, "start": 1131.24, "end": 1136.24, "text": " The methodology, publishing the methodology for how the data was collected,", "tokens": [50966, 440, 24850, 11, 17832, 264, 24850, 337, 577, 264, 1412, 390, 11087, 11, 51216], "temperature": 0.0, "avg_logprob": -0.15801573511379868, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.0010641226544976234}, {"id": 204, "seek": 111920, "start": 1136.24, "end": 1142.24, "text": " helps with transparency if you trust it.", "tokens": [51216, 3665, 365, 17131, 498, 291, 3361, 309, 13, 51516], "temperature": 0.0, "avg_logprob": -0.15801573511379868, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.0010641226544976234}, {"id": 205, "seek": 111920, "start": 1142.24, "end": 1144.24, "text": " But we're open source.", "tokens": [51516, 583, 321, 434, 1269, 4009, 13, 51616], "temperature": 0.0, "avg_logprob": -0.15801573511379868, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.0010641226544976234}, {"id": 206, "seek": 111920, "start": 1144.24, "end": 1148.24, "text": " We try to trust each other mostly.", "tokens": [51616, 492, 853, 281, 3361, 1184, 661, 5240, 13, 51816], "temperature": 0.0, "avg_logprob": -0.15801573511379868, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.0010641226544976234}, {"id": 207, "seek": 114824, "start": 1148.28, "end": 1150.28, "text": " Except for a few of you.", "tokens": [50366, 16192, 337, 257, 1326, 295, 291, 13, 50466], "temperature": 0.0, "avg_logprob": -0.1493990360162197, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0015464983880519867}, {"id": 208, "seek": 114824, "start": 1150.28, "end": 1152.28, "text": " You know who you are.", "tokens": [50466, 509, 458, 567, 291, 366, 13, 50566], "temperature": 0.0, "avg_logprob": -0.1493990360162197, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0015464983880519867}, {"id": 209, "seek": 114824, "start": 1152.28, "end": 1160.28, "text": " And there's some open questions about attribution.", "tokens": [50566, 400, 456, 311, 512, 1269, 1651, 466, 9080, 1448, 13, 50966], "temperature": 0.0, "avg_logprob": -0.1493990360162197, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0015464983880519867}, {"id": 210, "seek": 114824, "start": 1160.28, "end": 1165.28, "text": " If the data also needs attribution.", "tokens": [50966, 759, 264, 1412, 611, 2203, 9080, 1448, 13, 51216], "temperature": 0.0, "avg_logprob": -0.1493990360162197, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0015464983880519867}, {"id": 211, "seek": 114824, "start": 1165.28, "end": 1169.28, "text": " I'm not talking about from a legal sense or a licensed sense,", "tokens": [51216, 286, 478, 406, 1417, 466, 490, 257, 5089, 2020, 420, 257, 25225, 2020, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1493990360162197, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0015464983880519867}, {"id": 212, "seek": 114824, "start": 1169.28, "end": 1173.28, "text": " just for transparency and for credit,", "tokens": [51416, 445, 337, 17131, 293, 337, 5397, 11, 51616], "temperature": 0.0, "avg_logprob": -0.1493990360162197, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0015464983880519867}, {"id": 213, "seek": 114824, "start": 1173.28, "end": 1177.28, "text": " because we appreciate giving credit where credit is due.", "tokens": [51616, 570, 321, 4449, 2902, 5397, 689, 5397, 307, 3462, 13, 51816], "temperature": 0.0, "avg_logprob": -0.1493990360162197, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0015464983880519867}, {"id": 214, "seek": 117728, "start": 1177.32, "end": 1182.32, "text": " And if somebody wants to opt out of having their data in a corpus,", "tokens": [50366, 400, 498, 2618, 2738, 281, 2427, 484, 295, 1419, 641, 1412, 294, 257, 1181, 31624, 11, 50616], "temperature": 0.0, "avg_logprob": -0.11258915592642392, "compression_ratio": 1.3651685393258426, "no_speech_prob": 0.00021645407832693309}, {"id": 215, "seek": 117728, "start": 1182.32, "end": 1190.32, "text": " how do we handle that as well?", "tokens": [50616, 577, 360, 321, 4813, 300, 382, 731, 30, 51016], "temperature": 0.0, "avg_logprob": -0.11258915592642392, "compression_ratio": 1.3651685393258426, "no_speech_prob": 0.00021645407832693309}, {"id": 216, "seek": 117728, "start": 1190.32, "end": 1194.32, "text": " Lots of unknown problems with data.", "tokens": [51016, 15908, 295, 9841, 2740, 365, 1412, 13, 51216], "temperature": 0.0, "avg_logprob": -0.11258915592642392, "compression_ratio": 1.3651685393258426, "no_speech_prob": 0.00021645407832693309}, {"id": 217, "seek": 117728, "start": 1194.32, "end": 1198.32, "text": " But code gets a little bit easier.", "tokens": [51216, 583, 3089, 2170, 257, 707, 857, 3571, 13, 51416], "temperature": 0.0, "avg_logprob": -0.11258915592642392, "compression_ratio": 1.3651685393258426, "no_speech_prob": 0.00021645407832693309}, {"id": 218, "seek": 117728, "start": 1198.32, "end": 1204.32, "text": " This is going to be the second time I make a Jurassic Park joke this week.", "tokens": [51416, 639, 307, 516, 281, 312, 264, 1150, 565, 286, 652, 257, 44730, 4964, 7647, 341, 1243, 13, 51716], "temperature": 0.0, "avg_logprob": -0.11258915592642392, "compression_ratio": 1.3651685393258426, "no_speech_prob": 0.00021645407832693309}, {"id": 219, "seek": 120432, "start": 1204.36, "end": 1208.36, "text": " But we know how to do open source software.", "tokens": [50366, 583, 321, 458, 577, 281, 360, 1269, 4009, 4722, 13, 50566], "temperature": 0.0, "avg_logprob": -0.13110877407921684, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.005898452829569578}, {"id": 220, "seek": 120432, "start": 1208.36, "end": 1212.36, "text": " This is code. We know how to do this.", "tokens": [50566, 639, 307, 3089, 13, 492, 458, 577, 281, 360, 341, 13, 50766], "temperature": 0.0, "avg_logprob": -0.13110877407921684, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.005898452829569578}, {"id": 221, "seek": 120432, "start": 1212.36, "end": 1217.36, "text": " And despite what we've been hearing,", "tokens": [50766, 400, 7228, 437, 321, 600, 668, 4763, 11, 51016], "temperature": 0.0, "avg_logprob": -0.13110877407921684, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.005898452829569578}, {"id": 222, "seek": 120432, "start": 1217.36, "end": 1222.36, "text": " most of machine learning fits solely in this camp.", "tokens": [51016, 881, 295, 3479, 2539, 9001, 23309, 294, 341, 2255, 13, 51266], "temperature": 0.0, "avg_logprob": -0.13110877407921684, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.005898452829569578}, {"id": 223, "seek": 120432, "start": 1222.36, "end": 1228.36, "text": " It fits solely in the camp of open source software.", "tokens": [51266, 467, 9001, 23309, 294, 264, 2255, 295, 1269, 4009, 4722, 13, 51566], "temperature": 0.0, "avg_logprob": -0.13110877407921684, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.005898452829569578}, {"id": 224, "seek": 120432, "start": 1228.36, "end": 1230.36, "text": " Job well done.", "tokens": [51566, 18602, 731, 1096, 13, 51666], "temperature": 0.0, "avg_logprob": -0.13110877407921684, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.005898452829569578}, {"id": 225, "seek": 120432, "start": 1230.36, "end": 1231.36, "text": " Great.", "tokens": [51666, 3769, 13, 51716], "temperature": 0.0, "avg_logprob": -0.13110877407921684, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.005898452829569578}, {"id": 226, "seek": 123136, "start": 1231.3999999999999, "end": 1235.3999999999999, "text": " We've got Weka. What else do we need?", "tokens": [50366, 492, 600, 658, 492, 2330, 13, 708, 1646, 360, 321, 643, 30, 50566], "temperature": 0.0, "avg_logprob": -0.14882157796836762, "compression_ratio": 1.5, "no_speech_prob": 0.0013003658968955278}, {"id": 227, "seek": 123136, "start": 1235.3999999999999, "end": 1243.3999999999999, "text": " So it does, they are governed by the same requirements as normal open source software.", "tokens": [50566, 407, 309, 775, 11, 436, 366, 35529, 538, 264, 912, 7728, 382, 2710, 1269, 4009, 4722, 13, 50966], "temperature": 0.0, "avg_logprob": -0.14882157796836762, "compression_ratio": 1.5, "no_speech_prob": 0.0013003658968955278}, {"id": 228, "seek": 123136, "start": 1243.3999999999999, "end": 1245.3999999999999, "text": " No special casing needed.", "tokens": [50966, 883, 2121, 45109, 2978, 13, 51066], "temperature": 0.0, "avg_logprob": -0.14882157796836762, "compression_ratio": 1.5, "no_speech_prob": 0.0013003658968955278}, {"id": 229, "seek": 123136, "start": 1245.3999999999999, "end": 1248.3999999999999, "text": " Cool.", "tokens": [51066, 8561, 13, 51216], "temperature": 0.0, "avg_logprob": -0.14882157796836762, "compression_ratio": 1.5, "no_speech_prob": 0.0013003658968955278}, {"id": 230, "seek": 123136, "start": 1248.3999999999999, "end": 1255.3999999999999, "text": " One of the unique things, though, about this type of software", "tokens": [51216, 1485, 295, 264, 3845, 721, 11, 1673, 11, 466, 341, 2010, 295, 4722, 51566], "temperature": 0.0, "avg_logprob": -0.14882157796836762, "compression_ratio": 1.5, "no_speech_prob": 0.0013003658968955278}, {"id": 231, "seek": 123136, "start": 1255.3999999999999, "end": 1259.3999999999999, "text": " is that it may actually produce one of those things we don't yet know how to deal with.", "tokens": [51566, 307, 300, 309, 815, 767, 5258, 472, 295, 729, 721, 321, 500, 380, 1939, 458, 577, 281, 2028, 365, 13, 51766], "temperature": 0.0, "avg_logprob": -0.14882157796836762, "compression_ratio": 1.5, "no_speech_prob": 0.0013003658968955278}, {"id": 232, "seek": 125940, "start": 1259.44, "end": 1261.44, "text": " The model.", "tokens": [50366, 440, 2316, 13, 50466], "temperature": 0.0, "avg_logprob": -0.13199118021372203, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.001386392628774047}, {"id": 233, "seek": 125940, "start": 1261.44, "end": 1267.44, "text": " It also might involve an interface, how to interact with whatever system it produces,", "tokens": [50466, 467, 611, 1062, 9494, 364, 9226, 11, 577, 281, 4648, 365, 2035, 1185, 309, 14725, 11, 50766], "temperature": 0.0, "avg_logprob": -0.13199118021372203, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.001386392628774047}, {"id": 234, "seek": 125940, "start": 1267.44, "end": 1271.44, "text": " which may or may not be a model.", "tokens": [50766, 597, 815, 420, 815, 406, 312, 257, 2316, 13, 50966], "temperature": 0.0, "avg_logprob": -0.13199118021372203, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.001386392628774047}, {"id": 235, "seek": 125940, "start": 1271.44, "end": 1278.44, "text": " And it does intersect with the data and some interesting problems that go along with that.", "tokens": [50966, 400, 309, 775, 27815, 365, 264, 1412, 293, 512, 1880, 2740, 300, 352, 2051, 365, 300, 13, 51316], "temperature": 0.0, "avg_logprob": -0.13199118021372203, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.001386392628774047}, {"id": 236, "seek": 125940, "start": 1278.44, "end": 1286.44, "text": " So one of the things that we do when we process data is we clean it.", "tokens": [51316, 407, 472, 295, 264, 721, 300, 321, 360, 562, 321, 1399, 1412, 307, 321, 2541, 309, 13, 51716], "temperature": 0.0, "avg_logprob": -0.13199118021372203, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.001386392628774047}, {"id": 237, "seek": 128644, "start": 1286.48, "end": 1290.48, "text": " So does that code need to be open source?", "tokens": [50366, 407, 775, 300, 3089, 643, 281, 312, 1269, 4009, 30, 50566], "temperature": 0.0, "avg_logprob": -0.1432009645410486, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.0004103664541617036}, {"id": 238, "seek": 128644, "start": 1295.48, "end": 1301.48, "text": " Alongside with cleaning is some interesting value judgments that we have.", "tokens": [50816, 17457, 1812, 365, 8924, 307, 512, 1880, 2158, 40337, 300, 321, 362, 13, 51116], "temperature": 0.0, "avg_logprob": -0.1432009645410486, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.0004103664541617036}, {"id": 239, "seek": 128644, "start": 1301.48, "end": 1306.48, "text": " We may say, okay, deprioritize this feature a little bit.", "tokens": [51116, 492, 815, 584, 11, 1392, 11, 1367, 470, 50017, 1125, 341, 4111, 257, 707, 857, 13, 51366], "temperature": 0.0, "avg_logprob": -0.1432009645410486, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.0004103664541617036}, {"id": 240, "seek": 128644, "start": 1306.48, "end": 1309.48, "text": " Let's increase the priority on this one.", "tokens": [51366, 961, 311, 3488, 264, 9365, 322, 341, 472, 13, 51516], "temperature": 0.0, "avg_logprob": -0.1432009645410486, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.0004103664541617036}, {"id": 241, "seek": 128644, "start": 1309.48, "end": 1314.48, "text": " And in that way, we are actually making moral and ethical judgments,", "tokens": [51516, 400, 294, 300, 636, 11, 321, 366, 767, 1455, 9723, 293, 18890, 40337, 11, 51766], "temperature": 0.0, "avg_logprob": -0.1432009645410486, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.0004103664541617036}, {"id": 242, "seek": 131448, "start": 1314.52, "end": 1317.52, "text": " and we're encoding them.", "tokens": [50366, 293, 321, 434, 43430, 552, 13, 50516], "temperature": 0.0, "avg_logprob": -0.11863164901733399, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.0006981394835747778}, {"id": 243, "seek": 131448, "start": 1317.52, "end": 1322.52, "text": " Now, the great thing about code is that it's very easily inspected.", "tokens": [50516, 823, 11, 264, 869, 551, 466, 3089, 307, 300, 309, 311, 588, 3612, 1028, 10729, 13, 50766], "temperature": 0.0, "avg_logprob": -0.11863164901733399, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.0006981394835747778}, {"id": 244, "seek": 131448, "start": 1322.52, "end": 1327.52, "text": " For some definition of very and some definition of easily.", "tokens": [50766, 1171, 512, 7123, 295, 588, 293, 512, 7123, 295, 3612, 13, 51016], "temperature": 0.0, "avg_logprob": -0.11863164901733399, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.0006981394835747778}, {"id": 245, "seek": 131448, "start": 1327.52, "end": 1330.52, "text": " That's an exercise for the reader.", "tokens": [51016, 663, 311, 364, 5380, 337, 264, 15149, 13, 51166], "temperature": 0.0, "avg_logprob": -0.11863164901733399, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.0006981394835747778}, {"id": 246, "seek": 131448, "start": 1330.52, "end": 1337.52, "text": " But if we dig into it, we can see where those value judgments are made.", "tokens": [51166, 583, 498, 321, 2528, 666, 309, 11, 321, 393, 536, 689, 729, 2158, 40337, 366, 1027, 13, 51516], "temperature": 0.0, "avg_logprob": -0.11863164901733399, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.0006981394835747778}, {"id": 247, "seek": 133752, "start": 1337.56, "end": 1343.56, "text": " The other stuff, this is my favorite part.", "tokens": [50366, 440, 661, 1507, 11, 341, 307, 452, 2954, 644, 13, 50666], "temperature": 0.0, "avg_logprob": -0.2147206806001209, "compression_ratio": 1.2601626016260163, "no_speech_prob": 0.00047263296437449753}, {"id": 248, "seek": 133752, "start": 1343.56, "end": 1348.56, "text": " If somebody has a better name for it, let me know.", "tokens": [50666, 759, 2618, 575, 257, 1101, 1315, 337, 309, 11, 718, 385, 458, 13, 50916], "temperature": 0.0, "avg_logprob": -0.2147206806001209, "compression_ratio": 1.2601626016260163, "no_speech_prob": 0.00047263296437449753}, {"id": 249, "seek": 133752, "start": 1348.56, "end": 1357.56, "text": " So our hardware specifications required to recompile a model.", "tokens": [50916, 407, 527, 8837, 29448, 4739, 281, 48000, 794, 257, 2316, 13, 51366], "temperature": 0.0, "avg_logprob": -0.2147206806001209, "compression_ratio": 1.2601626016260163, "no_speech_prob": 0.00047263296437449753}, {"id": 250, "seek": 135756, "start": 1357.6, "end": 1363.6, "text": " How about disclosure of training time?", "tokens": [50366, 1012, 466, 30392, 295, 3097, 565, 30, 50666], "temperature": 0.0, "avg_logprob": -0.3289606183074241, "compression_ratio": 1.3043478260869565, "no_speech_prob": 0.0016223301645368338}, {"id": 251, "seek": 135756, "start": 1363.6, "end": 1369.6, "text": " How long it was trained for?", "tokens": [50666, 1012, 938, 309, 390, 8895, 337, 30, 50966], "temperature": 0.0, "avg_logprob": -0.3289606183074241, "compression_ratio": 1.3043478260869565, "no_speech_prob": 0.0016223301645368338}, {"id": 252, "seek": 135756, "start": 1369.6, "end": 1374.6, "text": " A definition of correctness.", "tokens": [50966, 316, 7123, 295, 3006, 1287, 13, 51216], "temperature": 0.0, "avg_logprob": -0.3289606183074241, "compression_ratio": 1.3043478260869565, "no_speech_prob": 0.0016223301645368338}, {"id": 253, "seek": 135756, "start": 1374.6, "end": 1381.6, "text": " So all of these do impact what comes out of the data.", "tokens": [51216, 407, 439, 295, 613, 360, 2712, 437, 1487, 484, 295, 264, 1412, 13, 51566], "temperature": 0.0, "avg_logprob": -0.3289606183074241, "compression_ratio": 1.3043478260869565, "no_speech_prob": 0.0016223301645368338}, {"id": 254, "seek": 138160, "start": 1381.6399999999999, "end": 1389.6399999999999, "text": " All of these do impact what comes out of your machine learning algorithm.", "tokens": [50366, 1057, 295, 613, 360, 2712, 437, 1487, 484, 295, 428, 3479, 2539, 9284, 13, 50766], "temperature": 0.0, "avg_logprob": -0.20471608436713784, "compression_ratio": 1.4407894736842106, "no_speech_prob": 0.001047224272042513}, {"id": 255, "seek": 138160, "start": 1389.6399999999999, "end": 1394.6399999999999, "text": " I was doing like a one-day course,", "tokens": [50766, 286, 390, 884, 411, 257, 472, 12, 810, 1164, 11, 51016], "temperature": 0.0, "avg_logprob": -0.20471608436713784, "compression_ratio": 1.4407894736842106, "no_speech_prob": 0.001047224272042513}, {"id": 256, "seek": 138160, "start": 1394.6399999999999, "end": 1398.6399999999999, "text": " brushing up some knowledge,", "tokens": [51016, 33130, 493, 512, 3601, 11, 51216], "temperature": 0.0, "avg_logprob": -0.20471608436713784, "compression_ratio": 1.4407894736842106, "no_speech_prob": 0.001047224272042513}, {"id": 257, "seek": 138160, "start": 1398.6399999999999, "end": 1402.6399999999999, "text": " and there was a bit of a competition,", "tokens": [51216, 293, 456, 390, 257, 857, 295, 257, 6211, 11, 51416], "temperature": 0.0, "avg_logprob": -0.20471608436713784, "compression_ratio": 1.4407894736842106, "no_speech_prob": 0.001047224272042513}, {"id": 258, "seek": 138160, "start": 1402.6399999999999, "end": 1406.6399999999999, "text": " ordinarily I hate competitions in classwork.", "tokens": [51416, 25376, 3289, 286, 4700, 26185, 294, 1508, 1902, 13, 51616], "temperature": 0.0, "avg_logprob": -0.20471608436713784, "compression_ratio": 1.4407894736842106, "no_speech_prob": 0.001047224272042513}, {"id": 259, "seek": 140664, "start": 1406.68, "end": 1412.68, "text": " But the idea was, okay, let's...", "tokens": [50366, 583, 264, 1558, 390, 11, 1392, 11, 718, 311, 485, 50666], "temperature": 0.0, "avg_logprob": -0.17630916833877563, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0010812103282660246}, {"id": 260, "seek": 140664, "start": 1412.68, "end": 1414.68, "text": " Here are all the concepts.", "tokens": [50666, 1692, 366, 439, 264, 10392, 13, 50766], "temperature": 0.0, "avg_logprob": -0.17630916833877563, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0010812103282660246}, {"id": 261, "seek": 140664, "start": 1414.68, "end": 1418.68, "text": " Do some fine-tuning, play around,", "tokens": [50766, 1144, 512, 2489, 12, 83, 37726, 11, 862, 926, 11, 50966], "temperature": 0.0, "avg_logprob": -0.17630916833877563, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0010812103282660246}, {"id": 262, "seek": 140664, "start": 1418.68, "end": 1425.68, "text": " and see who can have the highest accuracy for some random task.", "tokens": [50966, 293, 536, 567, 393, 362, 264, 6343, 14170, 337, 512, 4974, 5633, 13, 51316], "temperature": 0.0, "avg_logprob": -0.17630916833877563, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0010812103282660246}, {"id": 263, "seek": 140664, "start": 1425.68, "end": 1429.68, "text": " And I thought I did a pretty good job.", "tokens": [51316, 400, 286, 1194, 286, 630, 257, 1238, 665, 1691, 13, 51516], "temperature": 0.0, "avg_logprob": -0.17630916833877563, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0010812103282660246}, {"id": 264, "seek": 140664, "start": 1429.68, "end": 1431.68, "text": " The story of my life.", "tokens": [51516, 440, 1657, 295, 452, 993, 13, 51616], "temperature": 0.0, "avg_logprob": -0.17630916833877563, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0010812103282660246}, {"id": 265, "seek": 140664, "start": 1431.68, "end": 1433.68, "text": " I thought I did a pretty good job.", "tokens": [51616, 286, 1194, 286, 630, 257, 1238, 665, 1691, 13, 51716], "temperature": 0.0, "avg_logprob": -0.17630916833877563, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0010812103282660246}, {"id": 266, "seek": 143368, "start": 1433.72, "end": 1441.72, "text": " But there was one person who achieved nearly a perfect score.", "tokens": [50366, 583, 456, 390, 472, 954, 567, 11042, 6217, 257, 2176, 6175, 13, 50766], "temperature": 0.0, "avg_logprob": -0.11643498667170492, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.000291248521534726}, {"id": 267, "seek": 143368, "start": 1441.72, "end": 1445.72, "text": " So of course, the question is, would you do?", "tokens": [50766, 407, 295, 1164, 11, 264, 1168, 307, 11, 576, 291, 360, 30, 50966], "temperature": 0.0, "avg_logprob": -0.11643498667170492, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.000291248521534726}, {"id": 268, "seek": 143368, "start": 1445.72, "end": 1453.72, "text": " And they said, oh, well, I just ran the training for, you know, two days.", "tokens": [50966, 400, 436, 848, 11, 1954, 11, 731, 11, 286, 445, 5872, 264, 3097, 337, 11, 291, 458, 11, 732, 1708, 13, 51366], "temperature": 0.0, "avg_logprob": -0.11643498667170492, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.000291248521534726}, {"id": 269, "seek": 143368, "start": 1453.72, "end": 1457.72, "text": " I'm like, oh, okay.", "tokens": [51366, 286, 478, 411, 11, 1954, 11, 1392, 13, 51566], "temperature": 0.0, "avg_logprob": -0.11643498667170492, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.000291248521534726}, {"id": 270, "seek": 143368, "start": 1457.72, "end": 1459.72, "text": " Yeah, this was a one-day course.", "tokens": [51566, 865, 11, 341, 390, 257, 472, 12, 810, 1164, 13, 51666], "temperature": 0.0, "avg_logprob": -0.11643498667170492, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.000291248521534726}, {"id": 271, "seek": 143368, "start": 1459.72, "end": 1461.72, "text": " I didn't think it was a two-day investment.", "tokens": [51666, 286, 994, 380, 519, 309, 390, 257, 732, 12, 810, 6078, 13, 51766], "temperature": 0.0, "avg_logprob": -0.11643498667170492, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.000291248521534726}, {"id": 272, "seek": 146172, "start": 1461.76, "end": 1465.76, "text": " So the training time does absolutely affect the quality", "tokens": [50366, 407, 264, 3097, 565, 775, 3122, 3345, 264, 3125, 50566], "temperature": 0.0, "avg_logprob": -0.12036491185426712, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012054999824613333}, {"id": 273, "seek": 146172, "start": 1465.76, "end": 1469.76, "text": " and the output of the resulting system,", "tokens": [50566, 293, 264, 5598, 295, 264, 16505, 1185, 11, 50766], "temperature": 0.0, "avg_logprob": -0.12036491185426712, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012054999824613333}, {"id": 274, "seek": 146172, "start": 1469.76, "end": 1473.76, "text": " as does the hardware.", "tokens": [50766, 382, 775, 264, 8837, 13, 50966], "temperature": 0.0, "avg_logprob": -0.12036491185426712, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012054999824613333}, {"id": 275, "seek": 146172, "start": 1473.76, "end": 1479.76, "text": " So they are required to recompile a model.", "tokens": [50966, 407, 436, 366, 4739, 281, 48000, 794, 257, 2316, 13, 51266], "temperature": 0.0, "avg_logprob": -0.12036491185426712, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012054999824613333}, {"id": 276, "seek": 146172, "start": 1479.76, "end": 1485.76, "text": " But similar to data, we also have the question of access.", "tokens": [51266, 583, 2531, 281, 1412, 11, 321, 611, 362, 264, 1168, 295, 2105, 13, 51566], "temperature": 0.0, "avg_logprob": -0.12036491185426712, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012054999824613333}, {"id": 277, "seek": 146172, "start": 1485.76, "end": 1488.76, "text": " Access to equitable compute.", "tokens": [51566, 17166, 281, 33730, 14722, 13, 51716], "temperature": 0.0, "avg_logprob": -0.12036491185426712, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012054999824613333}, {"id": 278, "seek": 148876, "start": 1488.8, "end": 1494.8, "text": " Access to the hardware itself.", "tokens": [50366, 17166, 281, 264, 8837, 2564, 13, 50666], "temperature": 0.0, "avg_logprob": -0.10725249496160769, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.0002611264062579721}, {"id": 279, "seek": 148876, "start": 1494.8, "end": 1501.8, "text": " And again, we have a problem of attribution.", "tokens": [50666, 400, 797, 11, 321, 362, 257, 1154, 295, 9080, 1448, 13, 51016], "temperature": 0.0, "avg_logprob": -0.10725249496160769, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.0002611264062579721}, {"id": 280, "seek": 148876, "start": 1501.8, "end": 1505.8, "text": " So finally, output.", "tokens": [51016, 407, 2721, 11, 5598, 13, 51216], "temperature": 0.0, "avg_logprob": -0.10725249496160769, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.0002611264062579721}, {"id": 281, "seek": 148876, "start": 1505.8, "end": 1512.8, "text": " And since we are focusing on models and machine learning models,", "tokens": [51216, 400, 1670, 321, 366, 8416, 322, 5245, 293, 3479, 2539, 5245, 11, 51566], "temperature": 0.0, "avg_logprob": -0.10725249496160769, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.0002611264062579721}, {"id": 282, "seek": 148876, "start": 1512.8, "end": 1517.8, "text": " we've got matrices.", "tokens": [51566, 321, 600, 658, 32284, 13, 51816], "temperature": 0.0, "avg_logprob": -0.10725249496160769, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.0002611264062579721}, {"id": 283, "seek": 151780, "start": 1517.84, "end": 1526.84, "text": " I don't really know how to make that work in an open way.", "tokens": [50366, 286, 500, 380, 534, 458, 577, 281, 652, 300, 589, 294, 364, 1269, 636, 13, 50816], "temperature": 0.0, "avg_logprob": -0.09835875779390335, "compression_ratio": 1.3076923076923077, "no_speech_prob": 0.00030046567553654313}, {"id": 284, "seek": 151780, "start": 1526.84, "end": 1529.84, "text": " Yes, I can inspect a matrix.", "tokens": [50816, 1079, 11, 286, 393, 15018, 257, 8141, 13, 50966], "temperature": 0.0, "avg_logprob": -0.09835875779390335, "compression_ratio": 1.3076923076923077, "no_speech_prob": 0.00030046567553654313}, {"id": 285, "seek": 151780, "start": 1529.84, "end": 1532.84, "text": " Can I make sense of it in isolation?", "tokens": [50966, 1664, 286, 652, 2020, 295, 309, 294, 16001, 30, 51116], "temperature": 0.0, "avg_logprob": -0.09835875779390335, "compression_ratio": 1.3076923076923077, "no_speech_prob": 0.00030046567553654313}, {"id": 286, "seek": 151780, "start": 1532.84, "end": 1535.84, "text": " Not so much.", "tokens": [51116, 1726, 370, 709, 13, 51266], "temperature": 0.0, "avg_logprob": -0.09835875779390335, "compression_ratio": 1.3076923076923077, "no_speech_prob": 0.00030046567553654313}, {"id": 287, "seek": 151780, "start": 1535.84, "end": 1543.84, "text": " So if we do a litmus test, if this is all we have,", "tokens": [51266, 407, 498, 321, 360, 257, 7997, 18761, 1500, 11, 498, 341, 307, 439, 321, 362, 11, 51666], "temperature": 0.0, "avg_logprob": -0.09835875779390335, "compression_ratio": 1.3076923076923077, "no_speech_prob": 0.00030046567553654313}, {"id": 288, "seek": 154384, "start": 1543.8799999999999, "end": 1547.8799999999999, "text": " can we do arbitrary machine learning tasks with just this?", "tokens": [50366, 393, 321, 360, 23211, 3479, 2539, 9608, 365, 445, 341, 30, 50566], "temperature": 0.0, "avg_logprob": -0.13666170835494995, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.0011688570957630873}, {"id": 289, "seek": 154384, "start": 1547.8799999999999, "end": 1549.8799999999999, "text": " Probably not.", "tokens": [50566, 9210, 406, 13, 50666], "temperature": 0.0, "avg_logprob": -0.13666170835494995, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.0011688570957630873}, {"id": 290, "seek": 154384, "start": 1549.8799999999999, "end": 1555.8799999999999, "text": " How about just the code?", "tokens": [50666, 1012, 466, 445, 264, 3089, 30, 50966], "temperature": 0.0, "avg_logprob": -0.13666170835494995, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.0011688570957630873}, {"id": 291, "seek": 154384, "start": 1555.8799999999999, "end": 1557.8799999999999, "text": " Probably not.", "tokens": [50966, 9210, 406, 13, 51066], "temperature": 0.0, "avg_logprob": -0.13666170835494995, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.0011688570957630873}, {"id": 292, "seek": 154384, "start": 1557.8799999999999, "end": 1561.8799999999999, "text": " You still need some data.", "tokens": [51066, 509, 920, 643, 512, 1412, 13, 51266], "temperature": 0.0, "avg_logprob": -0.13666170835494995, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.0011688570957630873}, {"id": 293, "seek": 154384, "start": 1561.8799999999999, "end": 1566.8799999999999, "text": " Just the hardware? Maybe. I'd like to see that.", "tokens": [51266, 1449, 264, 8837, 30, 2704, 13, 286, 1116, 411, 281, 536, 300, 13, 51516], "temperature": 0.0, "avg_logprob": -0.13666170835494995, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.0011688570957630873}, {"id": 294, "seek": 156688, "start": 1566.92, "end": 1573.92, "text": " Just the model?", "tokens": [50366, 1449, 264, 2316, 30, 50716], "temperature": 0.0, "avg_logprob": -0.15358038584391276, "compression_ratio": 1.515923566878981, "no_speech_prob": 0.0033199796453118324}, {"id": 295, "seek": 156688, "start": 1573.92, "end": 1575.92, "text": " I'm going to say no.", "tokens": [50716, 286, 478, 516, 281, 584, 572, 13, 50816], "temperature": 0.0, "avg_logprob": -0.15358038584391276, "compression_ratio": 1.515923566878981, "no_speech_prob": 0.0033199796453118324}, {"id": 296, "seek": 156688, "start": 1575.92, "end": 1578.92, "text": " That's my... I'm putting my foot down there.", "tokens": [50816, 663, 311, 452, 485, 286, 478, 3372, 452, 2671, 760, 456, 13, 50966], "temperature": 0.0, "avg_logprob": -0.15358038584391276, "compression_ratio": 1.515923566878981, "no_speech_prob": 0.0033199796453118324}, {"id": 297, "seek": 156688, "start": 1578.92, "end": 1584.92, "text": " So what do we really need to make a transparent machine learning system?", "tokens": [50966, 407, 437, 360, 321, 534, 643, 281, 652, 257, 12737, 3479, 2539, 1185, 30, 51266], "temperature": 0.0, "avg_logprob": -0.15358038584391276, "compression_ratio": 1.515923566878981, "no_speech_prob": 0.0033199796453118324}, {"id": 298, "seek": 156688, "start": 1584.92, "end": 1587.92, "text": " We need all of it.", "tokens": [51266, 492, 643, 439, 295, 309, 13, 51416], "temperature": 0.0, "avg_logprob": -0.15358038584391276, "compression_ratio": 1.515923566878981, "no_speech_prob": 0.0033199796453118324}, {"id": 299, "seek": 156688, "start": 1587.92, "end": 1589.92, "text": " It all needs to be there.", "tokens": [51416, 467, 439, 2203, 281, 312, 456, 13, 51516], "temperature": 0.0, "avg_logprob": -0.15358038584391276, "compression_ratio": 1.515923566878981, "no_speech_prob": 0.0033199796453118324}, {"id": 300, "seek": 156688, "start": 1589.92, "end": 1593.92, "text": " It all needs to be open and available.", "tokens": [51516, 467, 439, 2203, 281, 312, 1269, 293, 2435, 13, 51716], "temperature": 0.0, "avg_logprob": -0.15358038584391276, "compression_ratio": 1.515923566878981, "no_speech_prob": 0.0033199796453118324}, {"id": 301, "seek": 159392, "start": 1593.96, "end": 1599.96, "text": " And that might mean that some things are not suitable for being open.", "tokens": [50366, 400, 300, 1062, 914, 300, 512, 721, 366, 406, 12873, 337, 885, 1269, 13, 50666], "temperature": 0.0, "avg_logprob": -0.1240995911990895, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.0007663425640203059}, {"id": 302, "seek": 159392, "start": 1603.96, "end": 1607.96, "text": " So some other questions that I'd love for you to think about", "tokens": [50866, 407, 512, 661, 1651, 300, 286, 1116, 959, 337, 291, 281, 519, 466, 51066], "temperature": 0.0, "avg_logprob": -0.1240995911990895, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.0007663425640203059}, {"id": 303, "seek": 159392, "start": 1607.96, "end": 1611.96, "text": " as you're thinking about open source and machine learning", "tokens": [51066, 382, 291, 434, 1953, 466, 1269, 4009, 293, 3479, 2539, 51266], "temperature": 0.0, "avg_logprob": -0.1240995911990895, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.0007663425640203059}, {"id": 304, "seek": 159392, "start": 1611.96, "end": 1616.96, "text": " is what does contribution to a model look like?", "tokens": [51266, 307, 437, 775, 13150, 281, 257, 2316, 574, 411, 30, 51516], "temperature": 0.0, "avg_logprob": -0.1240995911990895, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.0007663425640203059}, {"id": 305, "seek": 159392, "start": 1616.96, "end": 1621.96, "text": " What does correctness of a contribution look like?", "tokens": [51516, 708, 775, 3006, 1287, 295, 257, 13150, 574, 411, 30, 51766], "temperature": 0.0, "avg_logprob": -0.1240995911990895, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.0007663425640203059}, {"id": 306, "seek": 162392, "start": 1623.96, "end": 1634.96, "text": " How do we actually verify the openness of these systems", "tokens": [50366, 1012, 360, 321, 767, 16888, 264, 36200, 295, 613, 3652, 50916], "temperature": 0.0, "avg_logprob": -0.13355198171403673, "compression_ratio": 1.290909090909091, "no_speech_prob": 0.0012385868467390537}, {"id": 307, "seek": 162392, "start": 1634.96, "end": 1641.96, "text": " in a way that doesn't require a huge amount of investment", "tokens": [50916, 294, 257, 636, 300, 1177, 380, 3651, 257, 2603, 2372, 295, 6078, 51266], "temperature": 0.0, "avg_logprob": -0.13355198171403673, "compression_ratio": 1.290909090909091, "no_speech_prob": 0.0012385868467390537}, {"id": 308, "seek": 162392, "start": 1641.96, "end": 1644.96, "text": " that only a select few have?", "tokens": [51266, 300, 787, 257, 3048, 1326, 362, 30, 51416], "temperature": 0.0, "avg_logprob": -0.13355198171403673, "compression_ratio": 1.290909090909091, "no_speech_prob": 0.0012385868467390537}], "language": "en"}