{"text": " All right. Perfect. So, let's hope that we don't get this. Perfect. So. All right. We're going away from CERN, but we stay in Taitlan, so let's welcome Victor and give him some applause. Hello, everyone. Thank you so much for being here. In today's session, we are going to see how to deploy a hyperconverting infrastructure with F across the cloud edge continuum. But first, let me introduce myself. My name is Victor Palma, cloud engineer at Open Nebula. I come from Madrid, Spain, and I've been working for Open Nebula for more than two years. Developing an innovative new feature for the cloud edge world. So, first, we are going to start with some theory and then my idea is to show you a demo of all the things that we are going to see here. Well, first, we need to do the question of what is the cloud edge continuum? The cloud edge continuum is just an environment of nodes on the edge, distributed in multiple locations and everything interconnected using the same management layer. So, some advantages of the cloud edge continuum are the deployment of low latency applications. Since we are deploying our application near and closely to the final user, we can also reduce the energy consumption that our application does since we are deploying the application in a small data center, not in a big one. We can also reduce the vendor dependency since we are not deploying our application in a standard data center like cloud or AWS. We are deploying our application in multiple locations and managed by any provider that we want at any time. Then we can improve the user experience using the cloud edge continuum for our applications. Since it's related to the first point, since we are deploying our application near to the user, the experience and the latency is going to be low. We can also expand the service ability. It's very easy to duplicate and replicate our applications in different locations, different servers. We can deploy our application, for example, one cloud edge in Paris, another one in Madrid. If we want to replicate that, it's very easy to replicate the same infrastructure in Lisbon or London and so on. Finally, we can reduce data transfers and reduce security rigs since we are running all of this in a local location near to the user. The data transfer we need to transfer from our application to the user is more closely. How can we manage all of this? How can we manage the applications in the cloud edge continuum? First, we need to have a set of clusters of bare metal servers. Ideally, running KVNC is the mostly popular hypervisor. Then we need a uniform management layer for handling and managing all these locations, the private cloud associated with that. Then we need to interconnect all these clusters. Finally, we need to provide a multi-tenancy environment in order to create several users and groups and isolate each environment in the cloud edge. We are going to see now an example of how a cloud edge is inside. This example is a scenario for a 5G radio. We have this cloud edge node connected to the 5G radio using an edge land and two servers. The idea of the cloud edge is only to have a small number of servers. In this case, we have two, running the KVNC hypervisor. We can run in this hypervisor any application that we want. For example, a Kubernetes cluster for handling all the 5G code and workloads. We can also use the virtual GPUs of the server if it is available for data processing. All the servers inside this node are connected to the Internet through this public network using a single VLAN. The idea of the cloud edge node is to be autonomous as possible. If the single management ledger that we are going to use is not accessible, this site can work alone and without any problem. It can still provide service to the user. Now we are going to bring these ideas to life and see how all these concepts match together in order to create our cluster. How can we deploy the cloud edge in the node? We are going to use three main technologies. The first one is OpenNevola. It is a platform for orchestrating virtual workloads. Then we are going to use Terraform to automate the resources deployment. Then we are going to use Ansible for the configuration site for installing the packages that we are going to need. We don't need to use Ansible and Terraform directly. We are going to use these technologies through the OneProvision portal. That is our OpenNevola tool that allows us to automate all the configuration of the nodes and deploy the cloud edge nodes in an instant with a few steps. OneProvision supports several providers in order to create our own cloud node on the edge like GCloud, AWS, or GAYJax. Some of them are currently in development so not all are available. We are going to use AWS for the example of the today's issue. I would like to see more closely what is OpenNevola in order to understand the environment that we are going to see in the today's demo. OpenNevola, as I already said, is a platform that allows you to orchestrate and manage all your virtual machines, application containers, or Kubernetes clusters, all of them in the same way, in a very easy daily operation. You can deploy all these applications or all these workloads using your own private cloud or expand your cloud to the public cloud or on the edge. OpenNevola has integrations with several third-party tools like Terraform, Kubernetes, Ansible, and Docker. It also has its own built-in tools like a Sunstone UI, the graphical user interface, the web portal that we are going to use to interact with OpenNevola. We can create workloads in different hypervisors like BingWare, KBM, LXD, or file crackers. You can handle in the same way a micro-BM or a virtual machine. It doesn't matter for OpenNevola. Then, we have the possibility in OpenNevola to expand our cloud to the multi-cloud or to the hybrid cloud. In the case we have, for example, our own data center and we need more resources, we can deploy any infrastructure with automatic provision using, for example, providers like GCloud, AWS, or Equinix. Then, we have a uniform management for that resources with a homogeneous layer for users on workload management. Then, deploy the infrastructure, any application that we want. For example, as I already said, we can create Kubernetes clusters or Docker or virtual machines and so on. Well, we are going to use for the deployment of our cluster, thev as a storage solution. But why thev? We are going to see thev basically because it's an easy way to distribute the storage of our cloud, of the file, of our virtual machines, and share all the storage between the cloud etching nodes in a very, very easy way. So, thev at OpenNevola has implemented the storage basis on thev that with simple configuration, you can add multiple clusters or pools to OpenNevola. This storage implements replication and consistency and some important features for thev data storage in OpenNevola are snapshots, clone operations, encryption, et cetera. Regarding thev at the edge, as I already said, we need first a small number of nodes in our cloud etch node. Thev storage is going to be dedicated to store VMs in the scheme image. It's not necessary to have high storage requirements because we can use a multiple option for the storage. It has lowest storage requirements. And this is ideal for run in a HCI configuration because we can create nodes everywhere with a reduced cost. So, thev storage in OpenNevola consists of three different types of servers. We have the full nodes that run the Thev OSD and the monitor daemons as well, the KVN hyperbysos as well. We have the OSD nodes to run the Thev OSD daemons and the hypervisor only nodes that, as the name said, only runs the hypervisor. Here we have a sample of a comparison side by side from the point of view of the Thev storage from the AWS and from OpenNevola. For AWS we need to configure first a VPC, defining the metal bar servers, all the reaching tables, all the configuration that we need for our infrastructure. And from the OpenNevola point of view we just only need to create our host and automatically associate it with the Thev storage and we can start to create workloads on that servers. So, we are going to do a little demo on how we can do this using OpenNevola. So, let me... I don't know. Displays, sorry. Okay, that's better. So, this is the OneProvision portal that we are going to use to create the cloud HNO. Here we can see a general overview of the clusters, hosts and data storage that we have already in our infrastructure. So, we are going to take a look to each section. For example, here we can see the providers that we have already configured. I have configured here the AWS provider for Frankfurt. It's the example that we are going to see here, but we can create any provider that we want. For example, AWS or Reqinys or on-prem. Here you can select the location for the provider and when you finish this process, a provider is just a set of credentials that it's going to... OneProvision, the thing that OneProvision do is use the API endpoint of each provider in order to create the host on the servers, on the provider. So, it's just a set of credentials in order to connect to that endpoint. Using Terraform. Then we have the provision that are the provision providers. Here I have already a provision created. I'm going to show you this provision as an example, but we are going to see how the process is. I've already one created because deploying a note on the edge can take around 30 minutes. So, in order to avoid that, we can see here, for example, this class already created and running. For creating a new edge note, it's very easy. We just need to click on the add button. We can see here a description or the different options that we have, the support of virtualization technologies, etc. And we can set here if we want to create a edge cluster or if we want to create a HCI cluster using the app, that's the option that we are going to use. Next, we can select the provider that we want to use. In this case, it's going to be AWS Frankfurt. General attributes like the name, the description. And here we can tune in our note, setting the number of instances. The number of instances for only hypervisors or for only the OECD, etc., the DINIAS servers. The image that we want to use in the host. All of this configuration is also accessible through Jamel files. And we can set, for example, here if we want to use virtual machines or micro-VNs using LXC, etc. I'm going to back scenes. This takes some time, but the result is this. A cluster created in the edge in a very, very easy way. So what is the result of this? We are going to see here this is the main OpenEvula dashboard. The OpenEvula system is the user interface, the web user interface. So here we can see all the virtual machines that we already have. We have some VMs already running here. If we go to the exception, we can see here three nodes deployed on the edge inside this cluster. That is the cluster that we have created using OneProvision. We can create and deploy VMs in a very easy way, downloading an appliance for the OpenEvula marketplace. So for the Docker marketplace, if we want to deploy containers in our cloud, we are going to deploy a VM as an example. So for example, we can, this one, one of the Pine Linux. We can set here, for example, for example, we can even set the host that we want to use inside the cluster. For example, we are going to use this one, but we can change any configuration of that VM. We are going to instantiate the VM. And here we can see that the VM is starting to be created. So we are going to take a few seconds since it's an edge location, but it's booting and it's running. It's totally running. We can see here the host where the VM is running, the start time. A lot of configuration regarding the VM, like capacity or other storage and so on. And we can even connect to that VM. Oops, sorry. Maybe it's not so totally ready. And this one? Okay, I think that is the, I don't know if the Wi-Fi or maybe something is blocking the connection to the VM, but believe me, it's working. It's working on my machine. But that's all regarding the demo. As you can see, it's very easy to create filter machines in Open Nebula on the, using a Cloud Edge node. So returning to the slide. Okay. Well, that's the demo environment that we show you. As final conclusion, the necessity of this project and this integration with the Open Nebula are support for them in spaces, support for incremental backup in theft, adopt theft image, live migration. We want also to improve HCI configurations and integrate the one provision tool with the one deploy project that we already have. It's another public project that you can visit on GitHub that automates all the configuration and install of Open Nebula. And you are more than welcome to contribute to the repository on GitHub. And I also encourage you to contribute to our community. Join to the forum and share your experience using Open Nebula and help other users in order to create our Cloud open source community. So this project is funded by the European Union. So it's very interesting, the project. It's Cognite, so you can visit here the URL. And it's Cognite tries to provide a cognitive serverless experience to the European Union. So we can, the idea of this project is to using Open Nebula and one provision create a lot of Cloud Edge nodes in Europe in order to deploy application and gains independence. So that's all. Thank you very much for your attention. Questions? How do you deal with network outage, especially in the Edge? Sorry, can you repeat the question? How do you deal with network outage, especially in the Edge where the connection might not be stable? How can we handle the network connection when it's not stable? The idea of the Edge nodes are that in this kind of a scenario, the node is totally independent. So it's still working even if you don't have a connection to the Internet, at least to the region. I don't know if that answered your question. Yeah? Just to understand the configuration of the nodes in this form. So from the diagram I see that you can deploy, as I said, the storage subsystem on the same nodes we are running as the virtual machine. Is that correct? But you need dedicated nodes for the storage side and the VMs. He's asking about if we can deploy in the same node the storage and the virtual machines and the other workloads. Yeah, you can deploy in the same node. From OpenEvola point of view, it's only one node. But behind it's handled in the splitting between the nodes and the hypervisual nodes. But for the user that uses OpenNegulites, the host where he can deploy VMs and use their storage. So yeah. Any more? Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " All right.", "tokens": [50364, 1057, 558, 13, 50714], "temperature": 0.0, "avg_logprob": -0.4931452134076287, "compression_ratio": 1.3285714285714285, "no_speech_prob": 0.5717567205429077}, {"id": 1, "seek": 0, "start": 7.0, "end": 8.0, "text": " Perfect.", "tokens": [50714, 10246, 13, 50764], "temperature": 0.0, "avg_logprob": -0.4931452134076287, "compression_ratio": 1.3285714285714285, "no_speech_prob": 0.5717567205429077}, {"id": 2, "seek": 0, "start": 8.0, "end": 14.0, "text": " So, let's hope that we don't get this.", "tokens": [50764, 407, 11, 718, 311, 1454, 300, 321, 500, 380, 483, 341, 13, 51064], "temperature": 0.0, "avg_logprob": -0.4931452134076287, "compression_ratio": 1.3285714285714285, "no_speech_prob": 0.5717567205429077}, {"id": 3, "seek": 0, "start": 14.0, "end": 15.0, "text": " Perfect.", "tokens": [51064, 10246, 13, 51114], "temperature": 0.0, "avg_logprob": -0.4931452134076287, "compression_ratio": 1.3285714285714285, "no_speech_prob": 0.5717567205429077}, {"id": 4, "seek": 0, "start": 15.0, "end": 16.0, "text": " So.", "tokens": [51114, 407, 13, 51164], "temperature": 0.0, "avg_logprob": -0.4931452134076287, "compression_ratio": 1.3285714285714285, "no_speech_prob": 0.5717567205429077}, {"id": 5, "seek": 0, "start": 16.0, "end": 17.0, "text": " All right.", "tokens": [51164, 1057, 558, 13, 51214], "temperature": 0.0, "avg_logprob": -0.4931452134076287, "compression_ratio": 1.3285714285714285, "no_speech_prob": 0.5717567205429077}, {"id": 6, "seek": 0, "start": 17.0, "end": 27.0, "text": " We're going away from CERN, but we stay in Taitlan, so let's welcome Victor and give him some applause.", "tokens": [51214, 492, 434, 516, 1314, 490, 383, 1598, 45, 11, 457, 321, 1754, 294, 314, 1001, 8658, 11, 370, 718, 311, 2928, 15777, 293, 976, 796, 512, 9969, 13, 51714], "temperature": 0.0, "avg_logprob": -0.4931452134076287, "compression_ratio": 1.3285714285714285, "no_speech_prob": 0.5717567205429077}, {"id": 7, "seek": 2700, "start": 27.0, "end": 31.0, "text": " Hello, everyone. Thank you so much for being here.", "tokens": [50364, 2425, 11, 1518, 13, 1044, 291, 370, 709, 337, 885, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11870764682167455, "compression_ratio": 1.471311475409836, "no_speech_prob": 0.2210436463356018}, {"id": 8, "seek": 2700, "start": 31.0, "end": 41.0, "text": " In today's session, we are going to see how to deploy a hyperconverting infrastructure with F across the cloud edge continuum.", "tokens": [50564, 682, 965, 311, 5481, 11, 321, 366, 516, 281, 536, 577, 281, 7274, 257, 9848, 1671, 331, 783, 6896, 365, 479, 2108, 264, 4588, 4691, 36120, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11870764682167455, "compression_ratio": 1.471311475409836, "no_speech_prob": 0.2210436463356018}, {"id": 9, "seek": 2700, "start": 41.0, "end": 44.0, "text": " But first, let me introduce myself.", "tokens": [51064, 583, 700, 11, 718, 385, 5366, 2059, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11870764682167455, "compression_ratio": 1.471311475409836, "no_speech_prob": 0.2210436463356018}, {"id": 10, "seek": 2700, "start": 44.0, "end": 48.0, "text": " My name is Victor Palma, cloud engineer at Open Nebula.", "tokens": [51214, 1222, 1315, 307, 15777, 6116, 1696, 11, 4588, 11403, 412, 7238, 1734, 37775, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11870764682167455, "compression_ratio": 1.471311475409836, "no_speech_prob": 0.2210436463356018}, {"id": 11, "seek": 2700, "start": 48.0, "end": 56.0, "text": " I come from Madrid, Spain, and I've been working for Open Nebula for more than two years.", "tokens": [51414, 286, 808, 490, 22091, 11, 12838, 11, 293, 286, 600, 668, 1364, 337, 7238, 1734, 37775, 337, 544, 813, 732, 924, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11870764682167455, "compression_ratio": 1.471311475409836, "no_speech_prob": 0.2210436463356018}, {"id": 12, "seek": 5600, "start": 56.0, "end": 62.0, "text": " Developing an innovative new feature for the cloud edge world.", "tokens": [50364, 11442, 278, 364, 12999, 777, 4111, 337, 264, 4588, 4691, 1002, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13738430870903862, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.07039746642112732}, {"id": 13, "seek": 5600, "start": 62.0, "end": 74.0, "text": " So, first, we are going to start with some theory and then my idea is to show you a demo of all the things that we are going to see here.", "tokens": [50664, 407, 11, 700, 11, 321, 366, 516, 281, 722, 365, 512, 5261, 293, 550, 452, 1558, 307, 281, 855, 291, 257, 10723, 295, 439, 264, 721, 300, 321, 366, 516, 281, 536, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13738430870903862, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.07039746642112732}, {"id": 14, "seek": 5600, "start": 74.0, "end": 82.0, "text": " Well, first, we need to do the question of what is the cloud edge continuum?", "tokens": [51264, 1042, 11, 700, 11, 321, 643, 281, 360, 264, 1168, 295, 437, 307, 264, 4588, 4691, 36120, 30, 51664], "temperature": 0.0, "avg_logprob": -0.13738430870903862, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.07039746642112732}, {"id": 15, "seek": 8200, "start": 82.0, "end": 93.0, "text": " The cloud edge continuum is just an environment of nodes on the edge,", "tokens": [50364, 440, 4588, 4691, 36120, 307, 445, 364, 2823, 295, 13891, 322, 264, 4691, 11, 50914], "temperature": 0.0, "avg_logprob": -0.12748949455492425, "compression_ratio": 1.3387096774193548, "no_speech_prob": 0.27412718534469604}, {"id": 16, "seek": 8200, "start": 93.0, "end": 104.0, "text": " distributed in multiple locations and everything interconnected using the same management layer.", "tokens": [50914, 12631, 294, 3866, 9253, 293, 1203, 36611, 1228, 264, 912, 4592, 4583, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12748949455492425, "compression_ratio": 1.3387096774193548, "no_speech_prob": 0.27412718534469604}, {"id": 17, "seek": 10400, "start": 104.0, "end": 118.0, "text": " So, some advantages of the cloud edge continuum are the deployment of low latency applications.", "tokens": [50364, 407, 11, 512, 14906, 295, 264, 4588, 4691, 36120, 366, 264, 19317, 295, 2295, 27043, 5821, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11995035893208272, "compression_ratio": 1.4049586776859504, "no_speech_prob": 0.2434770166873932}, {"id": 18, "seek": 10400, "start": 118.0, "end": 125.0, "text": " Since we are deploying our application near and closely to the final user,", "tokens": [51064, 4162, 321, 366, 34198, 527, 3861, 2651, 293, 8185, 281, 264, 2572, 4195, 11, 51414], "temperature": 0.0, "avg_logprob": -0.11995035893208272, "compression_ratio": 1.4049586776859504, "no_speech_prob": 0.2434770166873932}, {"id": 19, "seek": 12500, "start": 125.0, "end": 139.0, "text": " we can also reduce the energy consumption that our application does since we are deploying the application in a small data center, not in a big one.", "tokens": [50364, 321, 393, 611, 5407, 264, 2281, 12126, 300, 527, 3861, 775, 1670, 321, 366, 34198, 264, 3861, 294, 257, 1359, 1412, 3056, 11, 406, 294, 257, 955, 472, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11358182713136834, "compression_ratio": 1.7870967741935484, "no_speech_prob": 0.467225044965744}, {"id": 20, "seek": 12500, "start": 139.0, "end": 152.0, "text": " We can also reduce the vendor dependency since we are not deploying our application in a standard data center like cloud or AWS.", "tokens": [51064, 492, 393, 611, 5407, 264, 24321, 33621, 1670, 321, 366, 406, 34198, 527, 3861, 294, 257, 3832, 1412, 3056, 411, 4588, 420, 17650, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11358182713136834, "compression_ratio": 1.7870967741935484, "no_speech_prob": 0.467225044965744}, {"id": 21, "seek": 15200, "start": 152.0, "end": 161.0, "text": " We are deploying our application in multiple locations and managed by any provider that we want at any time.", "tokens": [50364, 492, 366, 34198, 527, 3861, 294, 3866, 9253, 293, 6453, 538, 604, 12398, 300, 321, 528, 412, 604, 565, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09881354868412018, "compression_ratio": 1.697142857142857, "no_speech_prob": 0.06202574446797371}, {"id": 22, "seek": 15200, "start": 161.0, "end": 171.0, "text": " Then we can improve the user experience using the cloud edge continuum for our applications.", "tokens": [50814, 1396, 321, 393, 3470, 264, 4195, 1752, 1228, 264, 4588, 4691, 36120, 337, 527, 5821, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09881354868412018, "compression_ratio": 1.697142857142857, "no_speech_prob": 0.06202574446797371}, {"id": 23, "seek": 15200, "start": 171.0, "end": 178.0, "text": " Since it's related to the first point, since we are deploying our application near to the user,", "tokens": [51314, 4162, 309, 311, 4077, 281, 264, 700, 935, 11, 1670, 321, 366, 34198, 527, 3861, 2651, 281, 264, 4195, 11, 51664], "temperature": 0.0, "avg_logprob": -0.09881354868412018, "compression_ratio": 1.697142857142857, "no_speech_prob": 0.06202574446797371}, {"id": 24, "seek": 17800, "start": 178.0, "end": 182.0, "text": " the experience and the latency is going to be low.", "tokens": [50364, 264, 1752, 293, 264, 27043, 307, 516, 281, 312, 2295, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15154287219047546, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.26951470971107483}, {"id": 25, "seek": 17800, "start": 182.0, "end": 195.0, "text": " We can also expand the service ability. It's very easy to duplicate and replicate our applications in different locations, different servers.", "tokens": [50564, 492, 393, 611, 5268, 264, 2643, 3485, 13, 467, 311, 588, 1858, 281, 23976, 293, 25356, 527, 5821, 294, 819, 9253, 11, 819, 15909, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15154287219047546, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.26951470971107483}, {"id": 26, "seek": 17800, "start": 195.0, "end": 201.0, "text": " We can deploy our application, for example, one cloud edge in Paris, another one in Madrid.", "tokens": [51214, 492, 393, 7274, 527, 3861, 11, 337, 1365, 11, 472, 4588, 4691, 294, 8380, 11, 1071, 472, 294, 22091, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15154287219047546, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.26951470971107483}, {"id": 27, "seek": 20100, "start": 201.0, "end": 211.0, "text": " If we want to replicate that, it's very easy to replicate the same infrastructure in Lisbon or London and so on.", "tokens": [50364, 759, 321, 528, 281, 25356, 300, 11, 309, 311, 588, 1858, 281, 25356, 264, 912, 6896, 294, 30812, 4351, 420, 7042, 293, 370, 322, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15252383280608614, "compression_ratio": 1.50920245398773, "no_speech_prob": 0.4751938283443451}, {"id": 28, "seek": 20100, "start": 211.0, "end": 227.0, "text": " Finally, we can reduce data transfers and reduce security rigs since we are running all of this in a local location near to the user.", "tokens": [50864, 6288, 11, 321, 393, 5407, 1412, 29137, 293, 5407, 3825, 8329, 82, 1670, 321, 366, 2614, 439, 295, 341, 294, 257, 2654, 4914, 2651, 281, 264, 4195, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15252383280608614, "compression_ratio": 1.50920245398773, "no_speech_prob": 0.4751938283443451}, {"id": 29, "seek": 22700, "start": 227.0, "end": 240.0, "text": " The data transfer we need to transfer from our application to the user is more closely.", "tokens": [50364, 440, 1412, 5003, 321, 643, 281, 5003, 490, 527, 3861, 281, 264, 4195, 307, 544, 8185, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1581541992897211, "compression_ratio": 1.5166666666666666, "no_speech_prob": 0.16190330684185028}, {"id": 30, "seek": 22700, "start": 240.0, "end": 252.0, "text": " How can we manage all of this? How can we manage the applications in the cloud edge continuum?", "tokens": [51014, 1012, 393, 321, 3067, 439, 295, 341, 30, 1012, 393, 321, 3067, 264, 5821, 294, 264, 4588, 4691, 36120, 30, 51614], "temperature": 0.0, "avg_logprob": -0.1581541992897211, "compression_ratio": 1.5166666666666666, "no_speech_prob": 0.16190330684185028}, {"id": 31, "seek": 25200, "start": 252.0, "end": 259.0, "text": " First, we need to have a set of clusters of bare metal servers.", "tokens": [50364, 2386, 11, 321, 643, 281, 362, 257, 992, 295, 23313, 295, 6949, 5760, 15909, 13, 50714], "temperature": 0.0, "avg_logprob": -0.21705483582060217, "compression_ratio": 1.4219653179190752, "no_speech_prob": 0.2573031485080719}, {"id": 32, "seek": 25200, "start": 259.0, "end": 268.0, "text": " Ideally, running KVNC is the mostly popular hypervisor.", "tokens": [50714, 40817, 11, 2614, 591, 53, 45, 34, 307, 264, 5240, 3743, 9848, 16457, 13, 51164], "temperature": 0.0, "avg_logprob": -0.21705483582060217, "compression_ratio": 1.4219653179190752, "no_speech_prob": 0.2573031485080719}, {"id": 33, "seek": 25200, "start": 268.0, "end": 279.0, "text": " Then we need a uniform management layer for handling and managing all these locations, the private cloud associated with that.", "tokens": [51164, 1396, 321, 643, 257, 9452, 4592, 4583, 337, 13175, 293, 11642, 439, 613, 9253, 11, 264, 4551, 4588, 6615, 365, 300, 13, 51714], "temperature": 0.0, "avg_logprob": -0.21705483582060217, "compression_ratio": 1.4219653179190752, "no_speech_prob": 0.2573031485080719}, {"id": 34, "seek": 27900, "start": 279.0, "end": 285.0, "text": " Then we need to interconnect all these clusters.", "tokens": [50364, 1396, 321, 643, 281, 26253, 439, 613, 23313, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08428282087499445, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.022093093022704124}, {"id": 35, "seek": 27900, "start": 285.0, "end": 302.0, "text": " Finally, we need to provide a multi-tenancy environment in order to create several users and groups and isolate each environment in the cloud edge.", "tokens": [50664, 6288, 11, 321, 643, 281, 2893, 257, 4825, 12, 1147, 6717, 2823, 294, 1668, 281, 1884, 2940, 5022, 293, 3935, 293, 25660, 1184, 2823, 294, 264, 4588, 4691, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08428282087499445, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.022093093022704124}, {"id": 36, "seek": 30200, "start": 303.0, "end": 313.0, "text": " We are going to see now an example of how a cloud edge is inside.", "tokens": [50414, 492, 366, 516, 281, 536, 586, 364, 1365, 295, 577, 257, 4588, 4691, 307, 1854, 13, 50914], "temperature": 0.0, "avg_logprob": -0.17298040967999082, "compression_ratio": 1.1612903225806452, "no_speech_prob": 0.2390536367893219}, {"id": 37, "seek": 30200, "start": 313.0, "end": 321.0, "text": " This example is a scenario for a 5G radio.", "tokens": [50914, 639, 1365, 307, 257, 9005, 337, 257, 1025, 38, 6477, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17298040967999082, "compression_ratio": 1.1612903225806452, "no_speech_prob": 0.2390536367893219}, {"id": 38, "seek": 32100, "start": 321.0, "end": 335.0, "text": " We have this cloud edge node connected to the 5G radio using an edge land and two servers.", "tokens": [50364, 492, 362, 341, 4588, 4691, 9984, 4582, 281, 264, 1025, 38, 6477, 1228, 364, 4691, 2117, 293, 732, 15909, 13, 51064], "temperature": 0.0, "avg_logprob": -0.25234967186337426, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.14395196735858917}, {"id": 39, "seek": 32100, "start": 335.0, "end": 342.0, "text": " The idea of the cloud edge is only to have a small number of servers.", "tokens": [51064, 440, 1558, 295, 264, 4588, 4691, 307, 787, 281, 362, 257, 1359, 1230, 295, 15909, 13, 51414], "temperature": 0.0, "avg_logprob": -0.25234967186337426, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.14395196735858917}, {"id": 40, "seek": 34200, "start": 342.0, "end": 347.0, "text": " In this case, we have two, running the KVNC hypervisor.", "tokens": [50364, 682, 341, 1389, 11, 321, 362, 732, 11, 2614, 264, 591, 53, 45, 34, 9848, 16457, 13, 50614], "temperature": 0.0, "avg_logprob": -0.21252097023857963, "compression_ratio": 1.321917808219178, "no_speech_prob": 0.5387195348739624}, {"id": 41, "seek": 34200, "start": 347.0, "end": 353.0, "text": " We can run in this hypervisor any application that we want.", "tokens": [50614, 492, 393, 1190, 294, 341, 9848, 16457, 604, 3861, 300, 321, 528, 13, 50914], "temperature": 0.0, "avg_logprob": -0.21252097023857963, "compression_ratio": 1.321917808219178, "no_speech_prob": 0.5387195348739624}, {"id": 42, "seek": 34200, "start": 353.0, "end": 363.0, "text": " For example, a Kubernetes cluster for handling all the 5G code and workloads.", "tokens": [50914, 1171, 1365, 11, 257, 23145, 13630, 337, 13175, 439, 264, 1025, 38, 3089, 293, 32452, 13, 51414], "temperature": 0.0, "avg_logprob": -0.21252097023857963, "compression_ratio": 1.321917808219178, "no_speech_prob": 0.5387195348739624}, {"id": 43, "seek": 36300, "start": 363.0, "end": 375.0, "text": " We can also use the virtual GPUs of the server if it is available for data processing.", "tokens": [50364, 492, 393, 611, 764, 264, 6374, 18407, 82, 295, 264, 7154, 498, 309, 307, 2435, 337, 1412, 9007, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2963934566663659, "compression_ratio": 1.3655172413793104, "no_speech_prob": 0.15166011452674866}, {"id": 44, "seek": 36300, "start": 375.0, "end": 386.0, "text": " All the servers inside this node are connected to the Internet through this public network using a single VLAN.", "tokens": [50964, 1057, 264, 15909, 1854, 341, 9984, 366, 4582, 281, 264, 7703, 807, 341, 1908, 3209, 1228, 257, 2167, 691, 36527, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2963934566663659, "compression_ratio": 1.3655172413793104, "no_speech_prob": 0.15166011452674866}, {"id": 45, "seek": 38600, "start": 386.0, "end": 398.0, "text": " The idea of the cloud edge node is to be autonomous as possible.", "tokens": [50364, 440, 1558, 295, 264, 4588, 4691, 9984, 307, 281, 312, 23797, 382, 1944, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1839071240341454, "compression_ratio": 1.4683544303797469, "no_speech_prob": 0.09821433573961258}, {"id": 46, "seek": 38600, "start": 398.0, "end": 411.0, "text": " If the single management ledger that we are going to use is not accessible, this site can work alone and without any problem.", "tokens": [50964, 759, 264, 2167, 4592, 4684, 1321, 300, 321, 366, 516, 281, 764, 307, 406, 9515, 11, 341, 3621, 393, 589, 3312, 293, 1553, 604, 1154, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1839071240341454, "compression_ratio": 1.4683544303797469, "no_speech_prob": 0.09821433573961258}, {"id": 47, "seek": 38600, "start": 411.0, "end": 415.0, "text": " It can still provide service to the user.", "tokens": [51614, 467, 393, 920, 2893, 2643, 281, 264, 4195, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1839071240341454, "compression_ratio": 1.4683544303797469, "no_speech_prob": 0.09821433573961258}, {"id": 48, "seek": 41500, "start": 415.0, "end": 429.0, "text": " Now we are going to bring these ideas to life and see how all these concepts match together in order to create our cluster.", "tokens": [50364, 823, 321, 366, 516, 281, 1565, 613, 3487, 281, 993, 293, 536, 577, 439, 613, 10392, 2995, 1214, 294, 1668, 281, 1884, 527, 13630, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19371259212493896, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.03488782420754433}, {"id": 49, "seek": 41500, "start": 429.0, "end": 434.0, "text": " How can we deploy the cloud edge in the node?", "tokens": [51064, 1012, 393, 321, 7274, 264, 4588, 4691, 294, 264, 9984, 30, 51314], "temperature": 0.0, "avg_logprob": -0.19371259212493896, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.03488782420754433}, {"id": 50, "seek": 41500, "start": 434.0, "end": 438.0, "text": " We are going to use three main technologies.", "tokens": [51314, 492, 366, 516, 281, 764, 1045, 2135, 7943, 13, 51514], "temperature": 0.0, "avg_logprob": -0.19371259212493896, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.03488782420754433}, {"id": 51, "seek": 41500, "start": 438.0, "end": 440.0, "text": " The first one is OpenNevola.", "tokens": [51514, 440, 700, 472, 307, 7238, 45, 13379, 4711, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19371259212493896, "compression_ratio": 1.5093167701863355, "no_speech_prob": 0.03488782420754433}, {"id": 52, "seek": 44000, "start": 441.0, "end": 449.0, "text": " It is a platform for orchestrating virtual workloads.", "tokens": [50414, 467, 307, 257, 3663, 337, 14161, 8754, 6374, 32452, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16710057692094282, "compression_ratio": 1.6066666666666667, "no_speech_prob": 0.11769910156726837}, {"id": 53, "seek": 44000, "start": 449.0, "end": 457.0, "text": " Then we are going to use Terraform to automate the resources deployment.", "tokens": [50814, 1396, 321, 366, 516, 281, 764, 25366, 837, 281, 31605, 264, 3593, 19317, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16710057692094282, "compression_ratio": 1.6066666666666667, "no_speech_prob": 0.11769910156726837}, {"id": 54, "seek": 44000, "start": 457.0, "end": 469.0, "text": " Then we are going to use Ansible for the configuration site for installing the packages that we are going to need.", "tokens": [51214, 1396, 321, 366, 516, 281, 764, 14590, 964, 337, 264, 11694, 3621, 337, 20762, 264, 17401, 300, 321, 366, 516, 281, 643, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16710057692094282, "compression_ratio": 1.6066666666666667, "no_speech_prob": 0.11769910156726837}, {"id": 55, "seek": 46900, "start": 469.0, "end": 476.0, "text": " We don't need to use Ansible and Terraform directly.", "tokens": [50364, 492, 500, 380, 643, 281, 764, 14590, 964, 293, 25366, 837, 3838, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18594935361076803, "compression_ratio": 1.4787234042553192, "no_speech_prob": 0.06445487588644028}, {"id": 56, "seek": 46900, "start": 476.0, "end": 480.0, "text": " We are going to use these technologies through the OneProvision portal.", "tokens": [50714, 492, 366, 516, 281, 764, 613, 7943, 807, 264, 1485, 12681, 6763, 14982, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18594935361076803, "compression_ratio": 1.4787234042553192, "no_speech_prob": 0.06445487588644028}, {"id": 57, "seek": 46900, "start": 480.0, "end": 495.0, "text": " That is our OpenNevola tool that allows us to automate all the configuration of the nodes and deploy the cloud edge nodes in an instant with a few steps.", "tokens": [50914, 663, 307, 527, 7238, 45, 13379, 4711, 2290, 300, 4045, 505, 281, 31605, 439, 264, 11694, 295, 264, 13891, 293, 7274, 264, 4588, 4691, 13891, 294, 364, 9836, 365, 257, 1326, 4439, 13, 51664], "temperature": 0.0, "avg_logprob": -0.18594935361076803, "compression_ratio": 1.4787234042553192, "no_speech_prob": 0.06445487588644028}, {"id": 58, "seek": 49500, "start": 496.0, "end": 512.0, "text": " OneProvision supports several providers in order to create our own cloud node on the edge like GCloud, AWS, or GAYJax.", "tokens": [50414, 1485, 12681, 6763, 9346, 2940, 11330, 294, 1668, 281, 1884, 527, 1065, 4588, 9984, 322, 264, 4691, 411, 460, 32787, 11, 17650, 11, 420, 460, 4299, 41, 2797, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3456680377324422, "compression_ratio": 1.273972602739726, "no_speech_prob": 0.4565158188343048}, {"id": 59, "seek": 49500, "start": 512.0, "end": 518.0, "text": " Some of them are currently in development so not all are available.", "tokens": [51214, 2188, 295, 552, 366, 4362, 294, 3250, 370, 406, 439, 366, 2435, 13, 51514], "temperature": 0.0, "avg_logprob": -0.3456680377324422, "compression_ratio": 1.273972602739726, "no_speech_prob": 0.4565158188343048}, {"id": 60, "seek": 51800, "start": 518.0, "end": 525.0, "text": " We are going to use AWS for the example of the today's issue.", "tokens": [50364, 492, 366, 516, 281, 764, 17650, 337, 264, 1365, 295, 264, 965, 311, 2734, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1938937260554387, "compression_ratio": 1.4452554744525548, "no_speech_prob": 0.22225090861320496}, {"id": 61, "seek": 51800, "start": 527.0, "end": 537.0, "text": " I would like to see more closely what is OpenNevola in order to understand the environment that we are going to see in the today's demo.", "tokens": [50814, 286, 576, 411, 281, 536, 544, 8185, 437, 307, 7238, 45, 13379, 4711, 294, 1668, 281, 1223, 264, 2823, 300, 321, 366, 516, 281, 536, 294, 264, 965, 311, 10723, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1938937260554387, "compression_ratio": 1.4452554744525548, "no_speech_prob": 0.22225090861320496}, {"id": 62, "seek": 53700, "start": 538.0, "end": 551.0, "text": " OpenNevola, as I already said, is a platform that allows you to orchestrate and manage all your virtual machines, application containers, or Kubernetes clusters,", "tokens": [50414, 7238, 45, 13379, 4711, 11, 382, 286, 1217, 848, 11, 307, 257, 3663, 300, 4045, 291, 281, 14161, 4404, 293, 3067, 439, 428, 6374, 8379, 11, 3861, 17089, 11, 420, 23145, 23313, 11, 51064], "temperature": 0.0, "avg_logprob": -0.16814187720969873, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.21025481820106506}, {"id": 63, "seek": 53700, "start": 551.0, "end": 558.0, "text": " all of them in the same way, in a very easy daily operation.", "tokens": [51064, 439, 295, 552, 294, 264, 912, 636, 11, 294, 257, 588, 1858, 5212, 6916, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16814187720969873, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.21025481820106506}, {"id": 64, "seek": 55800, "start": 559.0, "end": 575.0, "text": " You can deploy all these applications or all these workloads using your own private cloud or expand your cloud to the public cloud or on the edge.", "tokens": [50414, 509, 393, 7274, 439, 613, 5821, 420, 439, 613, 32452, 1228, 428, 1065, 4551, 4588, 420, 5268, 428, 4588, 281, 264, 1908, 4588, 420, 322, 264, 4691, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18207183480262756, "compression_ratio": 1.4174757281553398, "no_speech_prob": 0.08126620203256607}, {"id": 65, "seek": 57500, "start": 576.0, "end": 588.0, "text": " OpenNevola has integrations with several third-party tools like Terraform, Kubernetes, Ansible, and Docker.", "tokens": [50414, 7238, 45, 13379, 4711, 575, 3572, 763, 365, 2940, 2636, 12, 23409, 3873, 411, 25366, 837, 11, 23145, 11, 14590, 964, 11, 293, 33772, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18888727356405818, "compression_ratio": 1.4662921348314606, "no_speech_prob": 0.11510861665010452}, {"id": 66, "seek": 57500, "start": 588.0, "end": 602.0, "text": " It also has its own built-in tools like a Sunstone UI, the graphical user interface, the web portal that we are going to use to interact with OpenNevola.", "tokens": [51014, 467, 611, 575, 1080, 1065, 3094, 12, 259, 3873, 411, 257, 6163, 11243, 15682, 11, 264, 35942, 4195, 9226, 11, 264, 3670, 14982, 300, 321, 366, 516, 281, 764, 281, 4648, 365, 7238, 45, 13379, 4711, 13, 51714], "temperature": 0.0, "avg_logprob": -0.18888727356405818, "compression_ratio": 1.4662921348314606, "no_speech_prob": 0.11510861665010452}, {"id": 67, "seek": 60200, "start": 603.0, "end": 614.0, "text": " We can create workloads in different hypervisors like BingWare, KBM, LXD, or file crackers.", "tokens": [50414, 492, 393, 1884, 32452, 294, 819, 9848, 4938, 830, 411, 30755, 54, 543, 11, 591, 18345, 11, 441, 55, 35, 11, 420, 3991, 41407, 13, 50964], "temperature": 0.0, "avg_logprob": -0.28180590847082304, "compression_ratio": 1.2516556291390728, "no_speech_prob": 0.05142497643828392}, {"id": 68, "seek": 60200, "start": 614.0, "end": 622.0, "text": " You can handle in the same way a micro-BM or a virtual machine. It doesn't matter for OpenNevola.", "tokens": [50964, 509, 393, 4813, 294, 264, 912, 636, 257, 4532, 12, 18345, 420, 257, 6374, 3479, 13, 467, 1177, 380, 1871, 337, 7238, 45, 13379, 4711, 13, 51364], "temperature": 0.0, "avg_logprob": -0.28180590847082304, "compression_ratio": 1.2516556291390728, "no_speech_prob": 0.05142497643828392}, {"id": 69, "seek": 62200, "start": 623.0, "end": 637.0, "text": " Then, we have the possibility in OpenNevola to expand our cloud to the multi-cloud or to the hybrid cloud.", "tokens": [50414, 1396, 11, 321, 362, 264, 7959, 294, 7238, 45, 13379, 4711, 281, 5268, 527, 4588, 281, 264, 4825, 12, 44495, 420, 281, 264, 13051, 4588, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17166366577148437, "compression_ratio": 1.1910112359550562, "no_speech_prob": 0.09445636719465256}, {"id": 70, "seek": 63700, "start": 638.0, "end": 658.0, "text": " In the case we have, for example, our own data center and we need more resources, we can deploy any infrastructure with automatic provision using, for example, providers like GCloud, AWS, or Equinix.", "tokens": [50414, 682, 264, 1389, 321, 362, 11, 337, 1365, 11, 527, 1065, 1412, 3056, 293, 321, 643, 544, 3593, 11, 321, 393, 7274, 604, 6896, 365, 12509, 17225, 1228, 11, 337, 1365, 11, 11330, 411, 460, 32787, 11, 17650, 11, 420, 15624, 259, 970, 13, 51414], "temperature": 0.0, "avg_logprob": -0.19505498806635538, "compression_ratio": 1.3537414965986394, "no_speech_prob": 0.07755470275878906}, {"id": 71, "seek": 65800, "start": 659.0, "end": 672.0, "text": " Then, we have a uniform management for that resources with a homogeneous layer for users on workload management.", "tokens": [50414, 1396, 11, 321, 362, 257, 9452, 4592, 337, 300, 3593, 365, 257, 42632, 4583, 337, 5022, 322, 20139, 4592, 13, 51064], "temperature": 0.0, "avg_logprob": -0.35164276758829754, "compression_ratio": 1.2307692307692308, "no_speech_prob": 0.04446694254875183}, {"id": 72, "seek": 67200, "start": 672.0, "end": 688.0, "text": " Then, deploy the infrastructure, any application that we want. For example, as I already said, we can create Kubernetes clusters or Docker or virtual machines and so on.", "tokens": [50364, 1396, 11, 7274, 264, 6896, 11, 604, 3861, 300, 321, 528, 13, 1171, 1365, 11, 382, 286, 1217, 848, 11, 321, 393, 1884, 23145, 23313, 420, 33772, 420, 6374, 8379, 293, 370, 322, 13, 51164], "temperature": 0.0, "avg_logprob": -0.22968267139635587, "compression_ratio": 1.2803030303030303, "no_speech_prob": 0.05250995606184006}, {"id": 73, "seek": 68800, "start": 689.0, "end": 700.0, "text": " Well, we are going to use for the deployment of our cluster, thev as a storage solution.", "tokens": [50414, 1042, 11, 321, 366, 516, 281, 764, 337, 264, 19317, 295, 527, 13630, 11, 264, 85, 382, 257, 6725, 3827, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2366611087133014, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.03703492879867554}, {"id": 74, "seek": 68800, "start": 700.0, "end": 716.0, "text": " But why thev? We are going to see thev basically because it's an easy way to distribute the storage of our cloud, of the file, of our virtual machines,", "tokens": [50964, 583, 983, 264, 85, 30, 492, 366, 516, 281, 536, 264, 85, 1936, 570, 309, 311, 364, 1858, 636, 281, 20594, 264, 6725, 295, 527, 4588, 11, 295, 264, 3991, 11, 295, 527, 6374, 8379, 11, 51764], "temperature": 0.0, "avg_logprob": -0.2366611087133014, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.03703492879867554}, {"id": 75, "seek": 71600, "start": 716.0, "end": 724.0, "text": " and share all the storage between the cloud etching nodes in a very, very easy way.", "tokens": [50364, 293, 2073, 439, 264, 6725, 1296, 264, 4588, 1030, 17354, 13891, 294, 257, 588, 11, 588, 1858, 636, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1808940092722575, "compression_ratio": 1.5194805194805194, "no_speech_prob": 0.17570984363555908}, {"id": 76, "seek": 71600, "start": 724.0, "end": 739.0, "text": " So, thev at OpenNevola has implemented the storage basis on thev that with simple configuration, you can add multiple clusters or pools to OpenNevola.", "tokens": [50764, 407, 11, 264, 85, 412, 7238, 45, 13379, 4711, 575, 12270, 264, 6725, 5143, 322, 264, 85, 300, 365, 2199, 11694, 11, 291, 393, 909, 3866, 23313, 420, 28688, 281, 7238, 45, 13379, 4711, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1808940092722575, "compression_ratio": 1.5194805194805194, "no_speech_prob": 0.17570984363555908}, {"id": 77, "seek": 73900, "start": 739.0, "end": 758.0, "text": " This storage implements replication and consistency and some important features for thev data storage in OpenNevola are snapshots, clone operations, encryption, et cetera.", "tokens": [50364, 639, 6725, 704, 17988, 39911, 293, 14416, 293, 512, 1021, 4122, 337, 264, 85, 1412, 6725, 294, 7238, 45, 13379, 4711, 366, 19206, 27495, 11, 26506, 7705, 11, 29575, 11, 1030, 11458, 13, 51314], "temperature": 0.0, "avg_logprob": -0.30343011907629064, "compression_ratio": 1.3790322580645162, "no_speech_prob": 0.25183963775634766}, {"id": 78, "seek": 75800, "start": 758.0, "end": 772.0, "text": " Regarding thev at the edge, as I already said, we need first a small number of nodes in our cloud etch node.", "tokens": [50364, 35523, 264, 85, 412, 264, 4691, 11, 382, 286, 1217, 848, 11, 321, 643, 700, 257, 1359, 1230, 295, 13891, 294, 527, 4588, 1030, 339, 9984, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2196407699584961, "compression_ratio": 1.3533834586466165, "no_speech_prob": 0.27171558141708374}, {"id": 79, "seek": 75800, "start": 772.0, "end": 778.0, "text": " Thev storage is going to be dedicated to store VMs in the scheme image.", "tokens": [51064, 440, 85, 6725, 307, 516, 281, 312, 8374, 281, 3531, 18038, 82, 294, 264, 12232, 3256, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2196407699584961, "compression_ratio": 1.3533834586466165, "no_speech_prob": 0.27171558141708374}, {"id": 80, "seek": 77800, "start": 779.0, "end": 792.0, "text": " It's not necessary to have high storage requirements because we can use a multiple option for the storage.", "tokens": [50414, 467, 311, 406, 4818, 281, 362, 1090, 6725, 7728, 570, 321, 393, 764, 257, 3866, 3614, 337, 264, 6725, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15304484963417053, "compression_ratio": 1.392156862745098, "no_speech_prob": 0.16319607198238373}, {"id": 81, "seek": 77800, "start": 792.0, "end": 795.0, "text": " It has lowest storage requirements.", "tokens": [51064, 467, 575, 12437, 6725, 7728, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15304484963417053, "compression_ratio": 1.392156862745098, "no_speech_prob": 0.16319607198238373}, {"id": 82, "seek": 79500, "start": 795.0, "end": 811.0, "text": " And this is ideal for run in a HCI configuration because we can create nodes everywhere with a reduced cost.", "tokens": [50364, 400, 341, 307, 7157, 337, 1190, 294, 257, 389, 25240, 11694, 570, 321, 393, 1884, 13891, 5315, 365, 257, 9212, 2063, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18578696250915527, "compression_ratio": 1.1368421052631579, "no_speech_prob": 0.05440637469291687}, {"id": 83, "seek": 81100, "start": 812.0, "end": 822.0, "text": " So, thev storage in OpenNevola consists of three different types of servers.", "tokens": [50414, 407, 11, 264, 85, 6725, 294, 7238, 45, 13379, 4711, 14689, 295, 1045, 819, 3467, 295, 15909, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2824960947036743, "compression_ratio": 1.3284671532846715, "no_speech_prob": 0.10965330898761749}, {"id": 84, "seek": 81100, "start": 822.0, "end": 833.0, "text": " We have the full nodes that run the Thev OSD and the monitor daemons as well, the KVN hyperbysos as well.", "tokens": [50914, 492, 362, 264, 1577, 13891, 300, 1190, 264, 440, 85, 12731, 35, 293, 264, 6002, 1120, 443, 892, 382, 731, 11, 264, 591, 53, 45, 9848, 65, 749, 329, 382, 731, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2824960947036743, "compression_ratio": 1.3284671532846715, "no_speech_prob": 0.10965330898761749}, {"id": 85, "seek": 83300, "start": 834.0, "end": 847.0, "text": " We have the OSD nodes to run the Thev OSD daemons and the hypervisor only nodes that, as the name said, only runs the hypervisor.", "tokens": [50414, 492, 362, 264, 12731, 35, 13891, 281, 1190, 264, 440, 85, 12731, 35, 1120, 443, 892, 293, 264, 9848, 16457, 787, 13891, 300, 11, 382, 264, 1315, 848, 11, 787, 6676, 264, 9848, 16457, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21216216454139122, "compression_ratio": 1.3578947368421053, "no_speech_prob": 0.2028573602437973}, {"id": 86, "seek": 84700, "start": 848.0, "end": 862.0, "text": " Here we have a sample of a comparison side by side from the point of view of the Thev storage from the AWS and from OpenNevola.", "tokens": [50414, 1692, 321, 362, 257, 6889, 295, 257, 9660, 1252, 538, 1252, 490, 264, 935, 295, 1910, 295, 264, 440, 85, 6725, 490, 264, 17650, 293, 490, 7238, 45, 13379, 4711, 13, 51114], "temperature": 0.0, "avg_logprob": -0.27420379093715125, "compression_ratio": 1.27, "no_speech_prob": 0.4726046621799469}, {"id": 87, "seek": 86200, "start": 862.0, "end": 880.0, "text": " For AWS we need to configure first a VPC, defining the metal bar servers, all the reaching tables, all the configuration that we need for our infrastructure.", "tokens": [50364, 1171, 17650, 321, 643, 281, 22162, 700, 257, 691, 12986, 11, 17827, 264, 5760, 2159, 15909, 11, 439, 264, 9906, 8020, 11, 439, 264, 11694, 300, 321, 643, 337, 527, 6896, 13, 51264], "temperature": 0.0, "avg_logprob": -0.27621589766608345, "compression_ratio": 1.3305084745762712, "no_speech_prob": 0.41419151425361633}, {"id": 88, "seek": 88000, "start": 881.0, "end": 899.0, "text": " And from the OpenNevola point of view we just only need to create our host and automatically associate it with the Thev storage and we can start to create workloads on that servers.", "tokens": [50414, 400, 490, 264, 7238, 45, 13379, 4711, 935, 295, 1910, 321, 445, 787, 643, 281, 1884, 527, 3975, 293, 6772, 14644, 309, 365, 264, 440, 85, 6725, 293, 321, 393, 722, 281, 1884, 32452, 322, 300, 15909, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18976992652529762, "compression_ratio": 1.371212121212121, "no_speech_prob": 0.26517876982688904}, {"id": 89, "seek": 89900, "start": 900.0, "end": 908.0, "text": " So, we are going to do a little demo on how we can do this using OpenNevola.", "tokens": [50414, 407, 11, 321, 366, 516, 281, 360, 257, 707, 10723, 322, 577, 321, 393, 360, 341, 1228, 7238, 45, 13379, 4711, 13, 50814], "temperature": 0.0, "avg_logprob": -0.19411615224984977, "compression_ratio": 1.0410958904109588, "no_speech_prob": 0.4962652027606964}, {"id": 90, "seek": 90800, "start": 908.0, "end": 912.0, "text": " So, let me...", "tokens": [50364, 407, 11, 718, 385, 485, 50564], "temperature": 0.0, "avg_logprob": -0.4704395417244204, "compression_ratio": 0.9027777777777778, "no_speech_prob": 0.656767725944519}, {"id": 91, "seek": 90800, "start": 917.0, "end": 919.0, "text": " I don't know.", "tokens": [50814, 286, 500, 380, 458, 13, 50914], "temperature": 0.0, "avg_logprob": -0.4704395417244204, "compression_ratio": 0.9027777777777778, "no_speech_prob": 0.656767725944519}, {"id": 92, "seek": 90800, "start": 922.0, "end": 924.0, "text": " Displays, sorry.", "tokens": [51064, 32229, 82, 11, 2597, 13, 51164], "temperature": 0.0, "avg_logprob": -0.4704395417244204, "compression_ratio": 0.9027777777777778, "no_speech_prob": 0.656767725944519}, {"id": 93, "seek": 90800, "start": 928.0, "end": 931.0, "text": " Okay, that's better.", "tokens": [51364, 1033, 11, 300, 311, 1101, 13, 51514], "temperature": 0.0, "avg_logprob": -0.4704395417244204, "compression_ratio": 0.9027777777777778, "no_speech_prob": 0.656767725944519}, {"id": 94, "seek": 93100, "start": 931.0, "end": 945.0, "text": " So, this is the OneProvision portal that we are going to use to create the cloud HNO.", "tokens": [50364, 407, 11, 341, 307, 264, 1485, 12681, 6763, 14982, 300, 321, 366, 516, 281, 764, 281, 1884, 264, 4588, 389, 12861, 13, 51064], "temperature": 0.0, "avg_logprob": -0.20315015082265817, "compression_ratio": 1.4068965517241379, "no_speech_prob": 0.01770905964076519}, {"id": 95, "seek": 93100, "start": 948.0, "end": 957.0, "text": " Here we can see a general overview of the clusters, hosts and data storage that we have already in our infrastructure.", "tokens": [51214, 1692, 321, 393, 536, 257, 2674, 12492, 295, 264, 23313, 11, 21573, 293, 1412, 6725, 300, 321, 362, 1217, 294, 527, 6896, 13, 51664], "temperature": 0.0, "avg_logprob": -0.20315015082265817, "compression_ratio": 1.4068965517241379, "no_speech_prob": 0.01770905964076519}, {"id": 96, "seek": 95700, "start": 958.0, "end": 963.0, "text": " So, we are going to take a look to each section.", "tokens": [50414, 407, 11, 321, 366, 516, 281, 747, 257, 574, 281, 1184, 3541, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11421645709446498, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.13310764729976654}, {"id": 97, "seek": 95700, "start": 963.0, "end": 968.0, "text": " For example, here we can see the providers that we have already configured.", "tokens": [50664, 1171, 1365, 11, 510, 321, 393, 536, 264, 11330, 300, 321, 362, 1217, 30538, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11421645709446498, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.13310764729976654}, {"id": 98, "seek": 95700, "start": 968.0, "end": 974.0, "text": " I have configured here the AWS provider for Frankfurt.", "tokens": [50914, 286, 362, 30538, 510, 264, 17650, 12398, 337, 36530, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11421645709446498, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.13310764729976654}, {"id": 99, "seek": 95700, "start": 974.0, "end": 980.0, "text": " It's the example that we are going to see here, but we can create any provider that we want.", "tokens": [51214, 467, 311, 264, 1365, 300, 321, 366, 516, 281, 536, 510, 11, 457, 321, 393, 1884, 604, 12398, 300, 321, 528, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11421645709446498, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.13310764729976654}, {"id": 100, "seek": 98000, "start": 981.0, "end": 986.0, "text": " For example, AWS or Reqinys or on-prem.", "tokens": [50414, 1171, 1365, 11, 17650, 420, 1300, 80, 259, 749, 420, 322, 12, 29403, 13, 50664], "temperature": 0.0, "avg_logprob": -0.24884424209594727, "compression_ratio": 1.330935251798561, "no_speech_prob": 0.08450254797935486}, {"id": 101, "seek": 98000, "start": 986.0, "end": 1003.0, "text": " Here you can select the location for the provider and when you finish this process, a provider is just a set of credentials that it's going to...", "tokens": [50664, 1692, 291, 393, 3048, 264, 4914, 337, 264, 12398, 293, 562, 291, 2413, 341, 1399, 11, 257, 12398, 307, 445, 257, 992, 295, 27404, 300, 309, 311, 516, 281, 485, 51514], "temperature": 0.0, "avg_logprob": -0.24884424209594727, "compression_ratio": 1.330935251798561, "no_speech_prob": 0.08450254797935486}, {"id": 102, "seek": 100300, "start": 1004.0, "end": 1022.0, "text": " OneProvision, the thing that OneProvision do is use the API endpoint of each provider in order to create the host on the servers, on the provider.", "tokens": [50414, 1485, 12681, 6763, 11, 264, 551, 300, 1485, 12681, 6763, 360, 307, 764, 264, 9362, 35795, 295, 1184, 12398, 294, 1668, 281, 1884, 264, 3975, 322, 264, 15909, 11, 322, 264, 12398, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11729812622070312, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.012036600150167942}, {"id": 103, "seek": 100300, "start": 1022.0, "end": 1030.0, "text": " So, it's just a set of credentials in order to connect to that endpoint.", "tokens": [51314, 407, 11, 309, 311, 445, 257, 992, 295, 27404, 294, 1668, 281, 1745, 281, 300, 35795, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11729812622070312, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.012036600150167942}, {"id": 104, "seek": 103000, "start": 1031.0, "end": 1034.0, "text": " Using Terraform.", "tokens": [50414, 11142, 25366, 837, 13, 50564], "temperature": 0.0, "avg_logprob": -0.16638779640197754, "compression_ratio": 1.5507246376811594, "no_speech_prob": 0.023320522159337997}, {"id": 105, "seek": 103000, "start": 1034.0, "end": 1041.0, "text": " Then we have the provision that are the provision providers.", "tokens": [50564, 1396, 321, 362, 264, 17225, 300, 366, 264, 17225, 11330, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16638779640197754, "compression_ratio": 1.5507246376811594, "no_speech_prob": 0.023320522159337997}, {"id": 106, "seek": 103000, "start": 1041.0, "end": 1048.0, "text": " Here I have already a provision created.", "tokens": [50914, 1692, 286, 362, 1217, 257, 17225, 2942, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16638779640197754, "compression_ratio": 1.5507246376811594, "no_speech_prob": 0.023320522159337997}, {"id": 107, "seek": 103000, "start": 1048.0, "end": 1055.0, "text": " I'm going to show you this provision as an example, but we are going to see how the process is.", "tokens": [51264, 286, 478, 516, 281, 855, 291, 341, 17225, 382, 364, 1365, 11, 457, 321, 366, 516, 281, 536, 577, 264, 1399, 307, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16638779640197754, "compression_ratio": 1.5507246376811594, "no_speech_prob": 0.023320522159337997}, {"id": 108, "seek": 105500, "start": 1055.0, "end": 1065.0, "text": " I've already one created because deploying a note on the edge can take around 30 minutes.", "tokens": [50364, 286, 600, 1217, 472, 2942, 570, 34198, 257, 3637, 322, 264, 4691, 393, 747, 926, 2217, 2077, 13, 50864], "temperature": 0.0, "avg_logprob": -0.210405023130652, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.030460376292467117}, {"id": 109, "seek": 105500, "start": 1065.0, "end": 1073.0, "text": " So, in order to avoid that, we can see here, for example, this class already created and running.", "tokens": [50864, 407, 11, 294, 1668, 281, 5042, 300, 11, 321, 393, 536, 510, 11, 337, 1365, 11, 341, 1508, 1217, 2942, 293, 2614, 13, 51264], "temperature": 0.0, "avg_logprob": -0.210405023130652, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.030460376292467117}, {"id": 110, "seek": 105500, "start": 1073.0, "end": 1079.0, "text": " For creating a new edge note, it's very easy.", "tokens": [51264, 1171, 4084, 257, 777, 4691, 3637, 11, 309, 311, 588, 1858, 13, 51564], "temperature": 0.0, "avg_logprob": -0.210405023130652, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.030460376292467117}, {"id": 111, "seek": 105500, "start": 1079.0, "end": 1083.0, "text": " We just need to click on the add button.", "tokens": [51564, 492, 445, 643, 281, 2052, 322, 264, 909, 2960, 13, 51764], "temperature": 0.0, "avg_logprob": -0.210405023130652, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.030460376292467117}, {"id": 112, "seek": 108300, "start": 1083.0, "end": 1091.0, "text": " We can see here a description or the different options that we have, the support of virtualization technologies, etc.", "tokens": [50364, 492, 393, 536, 510, 257, 3855, 420, 264, 819, 3956, 300, 321, 362, 11, 264, 1406, 295, 6374, 2144, 7943, 11, 5183, 13, 50764], "temperature": 0.0, "avg_logprob": -0.22399582079987027, "compression_ratio": 1.656441717791411, "no_speech_prob": 0.008829333819448948}, {"id": 113, "seek": 108300, "start": 1091.0, "end": 1104.0, "text": " And we can set here if we want to create a edge cluster or if we want to create a HCI cluster using the app, that's the option that we are going to use.", "tokens": [50764, 400, 321, 393, 992, 510, 498, 321, 528, 281, 1884, 257, 4691, 13630, 420, 498, 321, 528, 281, 1884, 257, 389, 25240, 13630, 1228, 264, 724, 11, 300, 311, 264, 3614, 300, 321, 366, 516, 281, 764, 13, 51414], "temperature": 0.0, "avg_logprob": -0.22399582079987027, "compression_ratio": 1.656441717791411, "no_speech_prob": 0.008829333819448948}, {"id": 114, "seek": 110400, "start": 1104.0, "end": 1109.0, "text": " Next, we can select the provider that we want to use.", "tokens": [50364, 3087, 11, 321, 393, 3048, 264, 12398, 300, 321, 528, 281, 764, 13, 50614], "temperature": 0.0, "avg_logprob": -0.18118216627735204, "compression_ratio": 1.3821656050955413, "no_speech_prob": 0.006317766848951578}, {"id": 115, "seek": 110400, "start": 1109.0, "end": 1116.0, "text": " In this case, it's going to be AWS Frankfurt.", "tokens": [50614, 682, 341, 1389, 11, 309, 311, 516, 281, 312, 17650, 36530, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18118216627735204, "compression_ratio": 1.3821656050955413, "no_speech_prob": 0.006317766848951578}, {"id": 116, "seek": 110400, "start": 1116.0, "end": 1121.0, "text": " General attributes like the name, the description.", "tokens": [50964, 6996, 17212, 411, 264, 1315, 11, 264, 3855, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18118216627735204, "compression_ratio": 1.3821656050955413, "no_speech_prob": 0.006317766848951578}, {"id": 117, "seek": 110400, "start": 1121.0, "end": 1133.0, "text": " And here we can tune in our note, setting the number of instances.", "tokens": [51214, 400, 510, 321, 393, 10864, 294, 527, 3637, 11, 3287, 264, 1230, 295, 14519, 13, 51814], "temperature": 0.0, "avg_logprob": -0.18118216627735204, "compression_ratio": 1.3821656050955413, "no_speech_prob": 0.006317766848951578}, {"id": 118, "seek": 113300, "start": 1133.0, "end": 1142.0, "text": " The number of instances for only hypervisors or for only the OECD, etc., the DINIAS servers.", "tokens": [50364, 440, 1230, 295, 14519, 337, 787, 9848, 4938, 830, 420, 337, 787, 264, 422, 8140, 35, 11, 5183, 7933, 264, 413, 1464, 40, 3160, 15909, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23612355349356667, "compression_ratio": 1.3223684210526316, "no_speech_prob": 0.034069426357746124}, {"id": 119, "seek": 113300, "start": 1142.0, "end": 1149.0, "text": " The image that we want to use in the host.", "tokens": [50814, 440, 3256, 300, 321, 528, 281, 764, 294, 264, 3975, 13, 51164], "temperature": 0.0, "avg_logprob": -0.23612355349356667, "compression_ratio": 1.3223684210526316, "no_speech_prob": 0.034069426357746124}, {"id": 120, "seek": 113300, "start": 1149.0, "end": 1159.0, "text": " All of this configuration is also accessible through Jamel files.", "tokens": [51164, 1057, 295, 341, 11694, 307, 611, 9515, 807, 10372, 338, 7098, 13, 51664], "temperature": 0.0, "avg_logprob": -0.23612355349356667, "compression_ratio": 1.3223684210526316, "no_speech_prob": 0.034069426357746124}, {"id": 121, "seek": 115900, "start": 1159.0, "end": 1168.0, "text": " And we can set, for example, here if we want to use virtual machines or micro-VNs using LXC, etc.", "tokens": [50364, 400, 321, 393, 992, 11, 337, 1365, 11, 510, 498, 321, 528, 281, 764, 6374, 8379, 420, 4532, 12, 53, 45, 82, 1228, 441, 55, 34, 11, 5183, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2040847321631203, "compression_ratio": 1.347305389221557, "no_speech_prob": 0.039412226527929306}, {"id": 122, "seek": 115900, "start": 1168.0, "end": 1172.0, "text": " I'm going to back scenes.", "tokens": [50814, 286, 478, 516, 281, 646, 8026, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2040847321631203, "compression_ratio": 1.347305389221557, "no_speech_prob": 0.039412226527929306}, {"id": 123, "seek": 115900, "start": 1172.0, "end": 1178.0, "text": " This takes some time, but the result is this.", "tokens": [51014, 639, 2516, 512, 565, 11, 457, 264, 1874, 307, 341, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2040847321631203, "compression_ratio": 1.347305389221557, "no_speech_prob": 0.039412226527929306}, {"id": 124, "seek": 115900, "start": 1178.0, "end": 1185.0, "text": " A cluster created in the edge in a very, very easy way.", "tokens": [51314, 316, 13630, 2942, 294, 264, 4691, 294, 257, 588, 11, 588, 1858, 636, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2040847321631203, "compression_ratio": 1.347305389221557, "no_speech_prob": 0.039412226527929306}, {"id": 125, "seek": 118500, "start": 1185.0, "end": 1190.0, "text": " So what is the result of this?", "tokens": [50364, 407, 437, 307, 264, 1874, 295, 341, 30, 50614], "temperature": 0.0, "avg_logprob": -0.18275471031665802, "compression_ratio": 1.5033112582781456, "no_speech_prob": 0.01724313013255596}, {"id": 126, "seek": 118500, "start": 1190.0, "end": 1195.0, "text": " We are going to see here this is the main OpenEvula dashboard.", "tokens": [50614, 492, 366, 516, 281, 536, 510, 341, 307, 264, 2135, 7238, 36, 85, 3780, 18342, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18275471031665802, "compression_ratio": 1.5033112582781456, "no_speech_prob": 0.01724313013255596}, {"id": 127, "seek": 118500, "start": 1195.0, "end": 1203.0, "text": " The OpenEvula system is the user interface, the web user interface.", "tokens": [50864, 440, 7238, 36, 85, 3780, 1185, 307, 264, 4195, 9226, 11, 264, 3670, 4195, 9226, 13, 51264], "temperature": 0.0, "avg_logprob": -0.18275471031665802, "compression_ratio": 1.5033112582781456, "no_speech_prob": 0.01724313013255596}, {"id": 128, "seek": 118500, "start": 1203.0, "end": 1210.0, "text": " So here we can see all the virtual machines that we already have.", "tokens": [51264, 407, 510, 321, 393, 536, 439, 264, 6374, 8379, 300, 321, 1217, 362, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18275471031665802, "compression_ratio": 1.5033112582781456, "no_speech_prob": 0.01724313013255596}, {"id": 129, "seek": 121000, "start": 1210.0, "end": 1214.0, "text": " We have some VMs already running here.", "tokens": [50364, 492, 362, 512, 18038, 82, 1217, 2614, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.16261663623884612, "compression_ratio": 1.380281690140845, "no_speech_prob": 0.02419797144830227}, {"id": 130, "seek": 121000, "start": 1214.0, "end": 1229.0, "text": " If we go to the exception, we can see here three nodes deployed on the edge inside this cluster.", "tokens": [50564, 759, 321, 352, 281, 264, 11183, 11, 321, 393, 536, 510, 1045, 13891, 17826, 322, 264, 4691, 1854, 341, 13630, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16261663623884612, "compression_ratio": 1.380281690140845, "no_speech_prob": 0.02419797144830227}, {"id": 131, "seek": 121000, "start": 1229.0, "end": 1234.0, "text": " That is the cluster that we have created using OneProvision.", "tokens": [51314, 663, 307, 264, 13630, 300, 321, 362, 2942, 1228, 1485, 12681, 6763, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16261663623884612, "compression_ratio": 1.380281690140845, "no_speech_prob": 0.02419797144830227}, {"id": 132, "seek": 123400, "start": 1234.0, "end": 1244.0, "text": " We can create and deploy VMs in a very easy way, downloading an appliance for the OpenEvula marketplace.", "tokens": [50364, 492, 393, 1884, 293, 7274, 18038, 82, 294, 257, 588, 1858, 636, 11, 32529, 364, 45646, 337, 264, 7238, 36, 85, 3780, 19455, 13, 50864], "temperature": 0.0, "avg_logprob": -0.24372510274251302, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.02396509237587452}, {"id": 133, "seek": 123400, "start": 1244.0, "end": 1255.0, "text": " So for the Docker marketplace, if we want to deploy containers in our cloud, we are going to deploy a VM as an example.", "tokens": [50864, 407, 337, 264, 33772, 19455, 11, 498, 321, 528, 281, 7274, 17089, 294, 527, 4588, 11, 321, 366, 516, 281, 7274, 257, 18038, 382, 364, 1365, 13, 51414], "temperature": 0.0, "avg_logprob": -0.24372510274251302, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.02396509237587452}, {"id": 134, "seek": 123400, "start": 1255.0, "end": 1260.0, "text": " So for example, we can, this one, one of the Pine Linux.", "tokens": [51414, 407, 337, 1365, 11, 321, 393, 11, 341, 472, 11, 472, 295, 264, 33531, 18734, 13, 51664], "temperature": 0.0, "avg_logprob": -0.24372510274251302, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.02396509237587452}, {"id": 135, "seek": 126000, "start": 1260.0, "end": 1274.0, "text": " We can set here, for example, for example, we can even set the host that we want to use inside the cluster.", "tokens": [50364, 492, 393, 992, 510, 11, 337, 1365, 11, 337, 1365, 11, 321, 393, 754, 992, 264, 3975, 300, 321, 528, 281, 764, 1854, 264, 13630, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08691571652889252, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0308647733181715}, {"id": 136, "seek": 126000, "start": 1274.0, "end": 1281.0, "text": " For example, we are going to use this one, but we can change any configuration of that VM.", "tokens": [51064, 1171, 1365, 11, 321, 366, 516, 281, 764, 341, 472, 11, 457, 321, 393, 1319, 604, 11694, 295, 300, 18038, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08691571652889252, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0308647733181715}, {"id": 137, "seek": 126000, "start": 1281.0, "end": 1285.0, "text": " We are going to instantiate the VM.", "tokens": [51414, 492, 366, 516, 281, 9836, 13024, 264, 18038, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08691571652889252, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0308647733181715}, {"id": 138, "seek": 128500, "start": 1285.0, "end": 1291.0, "text": " And here we can see that the VM is starting to be created.", "tokens": [50364, 400, 510, 321, 393, 536, 300, 264, 18038, 307, 2891, 281, 312, 2942, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1354153727141904, "compression_ratio": 1.5796178343949046, "no_speech_prob": 0.04400385916233063}, {"id": 139, "seek": 128500, "start": 1291.0, "end": 1303.0, "text": " So we are going to take a few seconds since it's an edge location, but it's booting and it's running.", "tokens": [50664, 407, 321, 366, 516, 281, 747, 257, 1326, 3949, 1670, 309, 311, 364, 4691, 4914, 11, 457, 309, 311, 11450, 278, 293, 309, 311, 2614, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1354153727141904, "compression_ratio": 1.5796178343949046, "no_speech_prob": 0.04400385916233063}, {"id": 140, "seek": 128500, "start": 1303.0, "end": 1305.0, "text": " It's totally running.", "tokens": [51264, 467, 311, 3879, 2614, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1354153727141904, "compression_ratio": 1.5796178343949046, "no_speech_prob": 0.04400385916233063}, {"id": 141, "seek": 128500, "start": 1305.0, "end": 1311.0, "text": " We can see here the host where the VM is running, the start time.", "tokens": [51364, 492, 393, 536, 510, 264, 3975, 689, 264, 18038, 307, 2614, 11, 264, 722, 565, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1354153727141904, "compression_ratio": 1.5796178343949046, "no_speech_prob": 0.04400385916233063}, {"id": 142, "seek": 131100, "start": 1311.0, "end": 1320.0, "text": " A lot of configuration regarding the VM, like capacity or other storage and so on.", "tokens": [50364, 316, 688, 295, 11694, 8595, 264, 18038, 11, 411, 6042, 420, 661, 6725, 293, 370, 322, 13, 50814], "temperature": 0.0, "avg_logprob": -0.22852683950353553, "compression_ratio": 1.289855072463768, "no_speech_prob": 0.06511970609426498}, {"id": 143, "seek": 131100, "start": 1320.0, "end": 1324.0, "text": " And we can even connect to that VM.", "tokens": [50814, 400, 321, 393, 754, 1745, 281, 300, 18038, 13, 51014], "temperature": 0.0, "avg_logprob": -0.22852683950353553, "compression_ratio": 1.289855072463768, "no_speech_prob": 0.06511970609426498}, {"id": 144, "seek": 131100, "start": 1324.0, "end": 1326.0, "text": " Oops, sorry.", "tokens": [51014, 21726, 11, 2597, 13, 51114], "temperature": 0.0, "avg_logprob": -0.22852683950353553, "compression_ratio": 1.289855072463768, "no_speech_prob": 0.06511970609426498}, {"id": 145, "seek": 131100, "start": 1326.0, "end": 1330.0, "text": " Maybe it's not so totally ready.", "tokens": [51114, 2704, 309, 311, 406, 370, 3879, 1919, 13, 51314], "temperature": 0.0, "avg_logprob": -0.22852683950353553, "compression_ratio": 1.289855072463768, "no_speech_prob": 0.06511970609426498}, {"id": 146, "seek": 131100, "start": 1330.0, "end": 1333.0, "text": " And this one?", "tokens": [51314, 400, 341, 472, 30, 51464], "temperature": 0.0, "avg_logprob": -0.22852683950353553, "compression_ratio": 1.289855072463768, "no_speech_prob": 0.06511970609426498}, {"id": 147, "seek": 133300, "start": 1334.0, "end": 1346.0, "text": " Okay, I think that is the, I don't know if the Wi-Fi or maybe something is blocking the connection to the VM, but believe me, it's working.", "tokens": [50414, 1033, 11, 286, 519, 300, 307, 264, 11, 286, 500, 380, 458, 498, 264, 14035, 12, 13229, 420, 1310, 746, 307, 17776, 264, 4984, 281, 264, 18038, 11, 457, 1697, 385, 11, 309, 311, 1364, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2083742959158761, "compression_ratio": 1.3253968253968254, "no_speech_prob": 0.030083077028393745}, {"id": 148, "seek": 133300, "start": 1349.0, "end": 1351.0, "text": " It's working on my machine.", "tokens": [51164, 467, 311, 1364, 322, 452, 3479, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2083742959158761, "compression_ratio": 1.3253968253968254, "no_speech_prob": 0.030083077028393745}, {"id": 149, "seek": 135100, "start": 1352.0, "end": 1358.0, "text": " But that's all regarding the demo.", "tokens": [50414, 583, 300, 311, 439, 8595, 264, 10723, 13, 50714], "temperature": 0.0, "avg_logprob": -0.33788180351257324, "compression_ratio": 1.2671755725190839, "no_speech_prob": 0.02305370196700096}, {"id": 150, "seek": 135100, "start": 1358.0, "end": 1368.0, "text": " As you can see, it's very easy to create filter machines in Open Nebula on the, using a Cloud Edge node.", "tokens": [50714, 1018, 291, 393, 536, 11, 309, 311, 588, 1858, 281, 1884, 6608, 8379, 294, 7238, 1734, 37775, 322, 264, 11, 1228, 257, 8061, 19328, 9984, 13, 51214], "temperature": 0.0, "avg_logprob": -0.33788180351257324, "compression_ratio": 1.2671755725190839, "no_speech_prob": 0.02305370196700096}, {"id": 151, "seek": 135100, "start": 1368.0, "end": 1375.0, "text": " So returning to the slide.", "tokens": [51214, 407, 12678, 281, 264, 4137, 13, 51564], "temperature": 0.0, "avg_logprob": -0.33788180351257324, "compression_ratio": 1.2671755725190839, "no_speech_prob": 0.02305370196700096}, {"id": 152, "seek": 137500, "start": 1376.0, "end": 1378.0, "text": " Okay.", "tokens": [50414, 1033, 13, 50514], "temperature": 0.0, "avg_logprob": -0.29537728627522786, "compression_ratio": 1.4659090909090908, "no_speech_prob": 0.023718049749732018}, {"id": 153, "seek": 137500, "start": 1378.0, "end": 1383.0, "text": " Well, that's the demo environment that we show you.", "tokens": [50514, 1042, 11, 300, 311, 264, 10723, 2823, 300, 321, 855, 291, 13, 50764], "temperature": 0.0, "avg_logprob": -0.29537728627522786, "compression_ratio": 1.4659090909090908, "no_speech_prob": 0.023718049749732018}, {"id": 154, "seek": 137500, "start": 1383.0, "end": 1402.0, "text": " As final conclusion, the necessity of this project and this integration with the Open Nebula are support for them in spaces, support for incremental backup in theft, adopt theft image, live migration.", "tokens": [50764, 1018, 2572, 10063, 11, 264, 24217, 295, 341, 1716, 293, 341, 10980, 365, 264, 7238, 1734, 37775, 366, 1406, 337, 552, 294, 7673, 11, 1406, 337, 35759, 14807, 294, 28508, 11, 6878, 28508, 3256, 11, 1621, 17011, 13, 51714], "temperature": 0.0, "avg_logprob": -0.29537728627522786, "compression_ratio": 1.4659090909090908, "no_speech_prob": 0.023718049749732018}, {"id": 155, "seek": 140200, "start": 1402.0, "end": 1413.0, "text": " We want also to improve HCI configurations and integrate the one provision tool with the one deploy project that we already have.", "tokens": [50364, 492, 528, 611, 281, 3470, 389, 25240, 31493, 293, 13365, 264, 472, 17225, 2290, 365, 264, 472, 7274, 1716, 300, 321, 1217, 362, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11812665121895927, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.09718937426805496}, {"id": 156, "seek": 140200, "start": 1413.0, "end": 1423.0, "text": " It's another public project that you can visit on GitHub that automates all the configuration and install of Open Nebula.", "tokens": [50914, 467, 311, 1071, 1908, 1716, 300, 291, 393, 3441, 322, 23331, 300, 3553, 1024, 439, 264, 11694, 293, 3625, 295, 7238, 1734, 37775, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11812665121895927, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.09718937426805496}, {"id": 157, "seek": 140200, "start": 1423.0, "end": 1429.0, "text": " And you are more than welcome to contribute to the repository on GitHub.", "tokens": [51414, 400, 291, 366, 544, 813, 2928, 281, 10586, 281, 264, 25841, 322, 23331, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11812665121895927, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.09718937426805496}, {"id": 158, "seek": 142900, "start": 1429.0, "end": 1434.0, "text": " And I also encourage you to contribute to our community.", "tokens": [50364, 400, 286, 611, 5373, 291, 281, 10586, 281, 527, 1768, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17912778189015943, "compression_ratio": 1.3941605839416058, "no_speech_prob": 0.08012655377388}, {"id": 159, "seek": 142900, "start": 1434.0, "end": 1448.0, "text": " Join to the forum and share your experience using Open Nebula and help other users in order to create our Cloud open source community.", "tokens": [50614, 19642, 281, 264, 17542, 293, 2073, 428, 1752, 1228, 7238, 1734, 37775, 293, 854, 661, 5022, 294, 1668, 281, 1884, 527, 8061, 1269, 4009, 1768, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17912778189015943, "compression_ratio": 1.3941605839416058, "no_speech_prob": 0.08012655377388}, {"id": 160, "seek": 144800, "start": 1449.0, "end": 1456.0, "text": " So this project is funded by the European Union.", "tokens": [50414, 407, 341, 1716, 307, 14385, 538, 264, 6473, 8133, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2646676970691216, "compression_ratio": 1.168141592920354, "no_speech_prob": 0.2594456076622009}, {"id": 161, "seek": 144800, "start": 1456.0, "end": 1461.0, "text": " So it's very interesting, the project.", "tokens": [50764, 407, 309, 311, 588, 1880, 11, 264, 1716, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2646676970691216, "compression_ratio": 1.168141592920354, "no_speech_prob": 0.2594456076622009}, {"id": 162, "seek": 144800, "start": 1461.0, "end": 1466.0, "text": " It's Cognite, so you can visit here the URL.", "tokens": [51014, 467, 311, 383, 2912, 642, 11, 370, 291, 393, 3441, 510, 264, 12905, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2646676970691216, "compression_ratio": 1.168141592920354, "no_speech_prob": 0.2594456076622009}, {"id": 163, "seek": 146600, "start": 1467.0, "end": 1477.0, "text": " And it's Cognite tries to provide a cognitive serverless experience to the European Union.", "tokens": [50414, 400, 309, 311, 383, 2912, 642, 9898, 281, 2893, 257, 15605, 7154, 1832, 1752, 281, 264, 6473, 8133, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1916507345731141, "compression_ratio": 1.4696132596685083, "no_speech_prob": 0.053505539894104004}, {"id": 164, "seek": 146600, "start": 1477.0, "end": 1493.0, "text": " So we can, the idea of this project is to using Open Nebula and one provision create a lot of Cloud Edge nodes in Europe in order to deploy application and gains independence.", "tokens": [50914, 407, 321, 393, 11, 264, 1558, 295, 341, 1716, 307, 281, 1228, 7238, 1734, 37775, 293, 472, 17225, 1884, 257, 688, 295, 8061, 19328, 13891, 294, 3315, 294, 1668, 281, 7274, 3861, 293, 16823, 14640, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1916507345731141, "compression_ratio": 1.4696132596685083, "no_speech_prob": 0.053505539894104004}, {"id": 165, "seek": 149300, "start": 1494.0, "end": 1501.0, "text": " So that's all. Thank you very much for your attention.", "tokens": [50414, 407, 300, 311, 439, 13, 1044, 291, 588, 709, 337, 428, 3202, 13, 50764], "temperature": 0.0, "avg_logprob": -0.6324158168974376, "compression_ratio": 0.9558823529411765, "no_speech_prob": 0.2314923107624054}, {"id": 166, "seek": 149300, "start": 1509.0, "end": 1511.0, "text": " Questions?", "tokens": [51164, 27738, 30, 51264], "temperature": 0.0, "avg_logprob": -0.6324158168974376, "compression_ratio": 0.9558823529411765, "no_speech_prob": 0.2314923107624054}, {"id": 167, "seek": 151100, "start": 1512.0, "end": 1519.0, "text": " How do you deal with network outage, especially in the Edge?", "tokens": [50414, 1012, 360, 291, 2028, 365, 3209, 484, 609, 11, 2318, 294, 264, 19328, 30, 50764], "temperature": 0.0, "avg_logprob": -0.18324784671559052, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.10910341888666153}, {"id": 168, "seek": 151100, "start": 1519.0, "end": 1521.0, "text": " Sorry, can you repeat the question?", "tokens": [50764, 4919, 11, 393, 291, 7149, 264, 1168, 30, 50864], "temperature": 0.0, "avg_logprob": -0.18324784671559052, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.10910341888666153}, {"id": 169, "seek": 151100, "start": 1521.0, "end": 1527.0, "text": " How do you deal with network outage, especially in the Edge where the connection might not be stable?", "tokens": [50864, 1012, 360, 291, 2028, 365, 3209, 484, 609, 11, 2318, 294, 264, 19328, 689, 264, 4984, 1062, 406, 312, 8351, 30, 51164], "temperature": 0.0, "avg_logprob": -0.18324784671559052, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.10910341888666153}, {"id": 170, "seek": 152700, "start": 1527.0, "end": 1534.0, "text": " How can we handle the network connection when it's not stable?", "tokens": [50364, 1012, 393, 321, 4813, 264, 3209, 4984, 562, 309, 311, 406, 8351, 30, 50714], "temperature": 0.0, "avg_logprob": -0.1631367325782776, "compression_ratio": 1.2926829268292683, "no_speech_prob": 0.20377607643604279}, {"id": 171, "seek": 152700, "start": 1534.0, "end": 1546.0, "text": " The idea of the Edge nodes are that in this kind of a scenario, the node is totally independent.", "tokens": [50714, 440, 1558, 295, 264, 19328, 13891, 366, 300, 294, 341, 733, 295, 257, 9005, 11, 264, 9984, 307, 3879, 6695, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1631367325782776, "compression_ratio": 1.2926829268292683, "no_speech_prob": 0.20377607643604279}, {"id": 172, "seek": 154600, "start": 1546.0, "end": 1557.0, "text": " So it's still working even if you don't have a connection to the Internet, at least to the region.", "tokens": [50364, 407, 309, 311, 920, 1364, 754, 498, 291, 500, 380, 362, 257, 4984, 281, 264, 7703, 11, 412, 1935, 281, 264, 4458, 13, 50914], "temperature": 0.0, "avg_logprob": -0.24131105112475018, "compression_ratio": 1.2844827586206897, "no_speech_prob": 0.08603081107139587}, {"id": 173, "seek": 154600, "start": 1557.0, "end": 1560.0, "text": " I don't know if that answered your question.", "tokens": [50914, 286, 500, 380, 458, 498, 300, 10103, 428, 1168, 13, 51064], "temperature": 0.0, "avg_logprob": -0.24131105112475018, "compression_ratio": 1.2844827586206897, "no_speech_prob": 0.08603081107139587}, {"id": 174, "seek": 154600, "start": 1567.0, "end": 1569.0, "text": " Yeah?", "tokens": [51414, 865, 30, 51514], "temperature": 0.0, "avg_logprob": -0.24131105112475018, "compression_ratio": 1.2844827586206897, "no_speech_prob": 0.08603081107139587}, {"id": 175, "seek": 156900, "start": 1569.0, "end": 1575.0, "text": " Just to understand the configuration of the nodes in this form.", "tokens": [50364, 1449, 281, 1223, 264, 11694, 295, 264, 13891, 294, 341, 1254, 13, 50664], "temperature": 0.0, "avg_logprob": -0.29141264902034275, "compression_ratio": 1.530054644808743, "no_speech_prob": 0.5152752995491028}, {"id": 176, "seek": 156900, "start": 1575.0, "end": 1587.0, "text": " So from the diagram I see that you can deploy, as I said, the storage subsystem on the same nodes we are running as the virtual machine.", "tokens": [50664, 407, 490, 264, 10686, 286, 536, 300, 291, 393, 7274, 11, 382, 286, 848, 11, 264, 6725, 2090, 9321, 322, 264, 912, 13891, 321, 366, 2614, 382, 264, 6374, 3479, 13, 51264], "temperature": 0.0, "avg_logprob": -0.29141264902034275, "compression_ratio": 1.530054644808743, "no_speech_prob": 0.5152752995491028}, {"id": 177, "seek": 156900, "start": 1587.0, "end": 1589.0, "text": " Is that correct?", "tokens": [51264, 1119, 300, 3006, 30, 51364], "temperature": 0.0, "avg_logprob": -0.29141264902034275, "compression_ratio": 1.530054644808743, "no_speech_prob": 0.5152752995491028}, {"id": 178, "seek": 156900, "start": 1589.0, "end": 1594.0, "text": " But you need dedicated nodes for the storage side and the VMs.", "tokens": [51364, 583, 291, 643, 8374, 13891, 337, 264, 6725, 1252, 293, 264, 18038, 82, 13, 51614], "temperature": 0.0, "avg_logprob": -0.29141264902034275, "compression_ratio": 1.530054644808743, "no_speech_prob": 0.5152752995491028}, {"id": 179, "seek": 159400, "start": 1595.0, "end": 1610.0, "text": " He's asking about if we can deploy in the same node the storage and the virtual machines and the other workloads.", "tokens": [50414, 634, 311, 3365, 466, 498, 321, 393, 7274, 294, 264, 912, 9984, 264, 6725, 293, 264, 6374, 8379, 293, 264, 661, 32452, 13, 51164], "temperature": 0.0, "avg_logprob": -0.20208891800471715, "compression_ratio": 1.4852941176470589, "no_speech_prob": 0.022321639582514763}, {"id": 180, "seek": 159400, "start": 1610.0, "end": 1612.0, "text": " Yeah, you can deploy in the same node.", "tokens": [51164, 865, 11, 291, 393, 7274, 294, 264, 912, 9984, 13, 51264], "temperature": 0.0, "avg_logprob": -0.20208891800471715, "compression_ratio": 1.4852941176470589, "no_speech_prob": 0.022321639582514763}, {"id": 181, "seek": 159400, "start": 1612.0, "end": 1617.0, "text": " From OpenEvola point of view, it's only one node.", "tokens": [51264, 3358, 7238, 36, 85, 4711, 935, 295, 1910, 11, 309, 311, 787, 472, 9984, 13, 51514], "temperature": 0.0, "avg_logprob": -0.20208891800471715, "compression_ratio": 1.4852941176470589, "no_speech_prob": 0.022321639582514763}, {"id": 182, "seek": 161700, "start": 1618.0, "end": 1628.0, "text": " But behind it's handled in the splitting between the nodes and the hypervisual nodes.", "tokens": [50414, 583, 2261, 309, 311, 18033, 294, 264, 30348, 1296, 264, 13891, 293, 264, 9848, 4938, 901, 13891, 13, 50914], "temperature": 0.0, "avg_logprob": -0.32440208566599876, "compression_ratio": 1.393103448275862, "no_speech_prob": 0.008434654213488102}, {"id": 183, "seek": 161700, "start": 1628.0, "end": 1637.0, "text": " But for the user that uses OpenNegulites, the host where he can deploy VMs and use their storage.", "tokens": [50914, 583, 337, 264, 4195, 300, 4960, 7238, 45, 1146, 425, 3324, 11, 264, 3975, 689, 415, 393, 7274, 18038, 82, 293, 764, 641, 6725, 13, 51364], "temperature": 0.0, "avg_logprob": -0.32440208566599876, "compression_ratio": 1.393103448275862, "no_speech_prob": 0.008434654213488102}, {"id": 184, "seek": 161700, "start": 1640.0, "end": 1642.0, "text": " So yeah.", "tokens": [51514, 407, 1338, 13, 51614], "temperature": 0.0, "avg_logprob": -0.32440208566599876, "compression_ratio": 1.393103448275862, "no_speech_prob": 0.008434654213488102}, {"id": 185, "seek": 161700, "start": 1642.0, "end": 1644.0, "text": " Any more?", "tokens": [51614, 2639, 544, 30, 51714], "temperature": 0.0, "avg_logprob": -0.32440208566599876, "compression_ratio": 1.393103448275862, "no_speech_prob": 0.008434654213488102}, {"id": 186, "seek": 164700, "start": 1647.0, "end": 1649.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.40809086958567303, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.4002922773361206}], "language": "en"}