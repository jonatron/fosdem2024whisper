{"text": " Very much. So our next person is Ahel Boroy from Highland that's going to talk to us about using generative AI and content service platforms together. Thanks. I was on. I was on. I was checking the microphone. Okay, yep. Welcome to everyone. So this is another view on the same topic. So we are going on the technical side now. It's not like a final feature for a product, but it's a framework in order to help you to build all the features that we were seeing before in the context of a content service platform or a document. Okay, so we are going to review some in a stack. The next step we are going to use that is including also LLN on premise. We are going to review all the options. We are going also to describe the features we can build with this stack. And then we are going to review how to integrate that with your in our case because I'm working for Highland and we are building an open source product with the name of Fresco that is related to content management. So we are going to see how to integrate that with that content management platform and also just looking a few to the future. And obviously I need to include some AI picture because it is what it is. Anyway, so this GenAI stack that we are using includes mainly three components. So the first one is Olama. Olama is a service that is able to provide an API to interact with different LLMs. We are going to see later all the list but you can download your LLN on premise and this layer is providing the interaction with the LLM. You can even interact with different LLMs at the same time. The second one is Neo4j. So Neo4j is the vector database when you are using rack, we augmented reality and so on. Then you need to increase the information for the LLM. So you are storing all this information in this database. And finally we are using land change. So this framework is providing land change that is a framework to communicate all these different elements. So this framework is in Python but if you are not comfortable with Python there are many other languages that are including this kind of piece. Okay, so mainly what we have, if someone doesn't like Docker there is no problem so you still can deploy that without it but it is oriented to services. So you have Olama that is the one providing the services for the LLM that can be used in GPU or not. So you can, we are going to run this without the GPU. Just using my regular CPU on my computer. This is lower. I recommend you to use a GPU but you can do that. And we are piling all the models that we need so we can just use more than one model. With that we can increase the information for the operation with that Neo4j database and we can develop an API with this string LLM, with this framework. Okay, so these are the pieces. You have the project Docker.gen.a stack. Mainly these pieces is a sample. This sample is oriented to prompting. You have to reply questions. So we are going to do something a bit different from that but the first sample you can try is this one. Okay, so all the LLMs that are able to manage Olama today are these ones. Likely as this is growing every day there are more. But this was like last week. Okay, so this is what you need to understand. Obviously the larger the better but you need to take into account your resources. So these are very small. Your 4 gigabytes of RAM and 2 gigabytes of storage. So you can run that on a computer and then if you want to use something that is better is also larger in resources. And you can even use these LLMs that require, I don't know, many different computers once I say, okay. So today we are going to use this kind of LLMs. Also it's relevant to look at the license. So just was talking before about the license. So this is also something relevant if you want to build something commercial or something open source or whatever. You need to take care of the license. Also you can look that there is some weight license there because you have this LLM2 community license agreement that some people say that this open source, some other people say that it is not. So it's something different. So better to check if you don't see a patchy license or something that you can recognize. Better to check the conditions. So you have a lot of them to choose. We are going to work today on the demo with Mistral, CementB, that is a French company that is producing this kind of LLMs that are more or less the same performance as GPT 3.5. So it's good enough. And so what is open source, the LLM is free to download and to use, but the training data is not free and likely it has some copyright material on it. We don't know because it's not free. So on the next law ethical AI writing we have, sorry, yellow. I thought it was orange but it's yellow. Okay. It's more or less fine. So we are just only missing one. That was for text and for pictures. We know some LLM with a visual encoder on it. So for this part we are going to use lava. And lava really is granting all the different requirements. So we are using a green LLM for this other sample. Okay. Perfect. So all the demo is running on my computer while I'm there in the presentation. So I have everything running inside is 32 gigabytes of RAM and is AM64 architecture. So it's not AMD. It's MacBook Pro two years ago, something like that. Okay. As we were also reviewing the previous version before this GEN AI momentum, we also had some data section, test recognition, test classification, content analysis. Anyone is using content analysis for a real use case? Okay. It was not me. So it's something but you saw. But we have all the things, right? Some kind of automation. But now with the GEN AI, we have also a power classification. We could classify in the past. But now we can classify better. We can also, and when I say translate, we are going to see later the demo. Obviously we can translate. But we can also interact with the LLM in one language and to get the response in another language. Right? So that is the difference. We can also summarize a test. This is the most common use case and we can describe a picture. Prompting. Obviously we can use prompting. We can read that. So we have some new features that we can use in our documents. Okay. We are going to see some of them implemented. Okay. So what is this project about? It's not yet. Okay. The project is at some point of the slides. Okay. If not, I will give you the link. So in this project, what is created is a API by using this, all these infrastructure in order to provide different services. What we are using is some LLM embeddings. So we are just trying to avoid hallucinations. Just giving some additional information to the database from the document. So we are working with a document. Right? So we are not going with search. We are not going with some other applications of GNI. So we are focused on features of a document. So we are adding all that information so we can get a better response and more suitable to the document we are dealing with. And for that we are using Mistral. And if we are talking about a picture, then we can use the other LLM that was Java in order, for instance, to describe or classify the picture. We have also some, so we can choose the LLM. If you want to choose some other LLM than Mistral, you can do that for text. You can choose some other LLM with a vision and color enabled, like Java or some other on the list. And we can also choose the language. So we are going to see that later. We can just drop a document in Japanese and we are getting the summary in English or in the other side. Right? And also you can choose some numbers like the summary size or the number of tasks and so on. So these are parameters. Okay. So this is the API. Right? Pretty cool invocations. But let's see that leave. As always is better. Can you see the, better? Okay. Okay. So for instance, I'm going to work, let me find the, I'm going to work with this document. Right? I could be using an English document, but it should be easier for the AI. So we are using this one. And I'm also going to use this picture. So for your reference. Okay. Okay. Perfect. So for this document, we are going to ask for a summary. So give me a summary of this document that is in Japanese. So with that, if I'm able to. Okay. So this is running on my computer. So I have this ENAI stack running in this Docker deployment. And I'm getting the request. Okay. And with that, I'm getting the answer. Okay. So the test, this is a problem with kindergarten, in Japan, blah, blah, blah. Okay. That's fine. So I'm giving something in Japanese and I'm getting the summary in English. The second one, come on, note this one. I did it. Okay. The second one is just to classify. Classify a document that picking a term of a list of terms. So I want you to classify this document according to Japanese, Spanish or Vietnamese. Again, it's an easy example. Right. But you can choose whatever list of values. So if I say just classify this document into one of these three categories, the term is Japanese because the document is in Japanese. Okay. This is also a Revan for classification. And finally, we can also make some prompt on the document. What is the name of the zone or this document in Japanese document? The name of the zone is Musoku. Okay. So three different features that we can use on this, on a document. You can build more. Again, it's a Python, Python program with these three specific features, but you can grow up to include something else. And if we move to the, to the pictures that was for text, but for the pictures, we can describe this, this picture. We can also extract some, this is a person, this is, but that was done before. But describing is the, the, the new thing that GNI is providing for us. This is a bit slower, but in the end, they made so some man posting for the camera. He's wearing a green beanie, glasses, a black hoodie. And the land yall says air fraked. Well, no, it's a fresco, but more or less. Okay. The picture was not big enough, but it's fine. It's, it's something that is, is useful. And it's not that consuming internal resources, because it's running in, on my machine. So it's, it's fair enough. Okay. Once that we have all these features, and we have this, Python, just let me show you a bit. So this is the project, right? You have the Aeboroi, a fresco GNI, and you have the GNI stack, and mainly it's a Python program. Okay. With all these endpoints described, classified, prompt, and somebody. Okay. It's no more than that. Okay. If we go back to the original goal, is to integrate this kind of operations with our, with our product than in our case is a fresco. So a fresco, we can deploy that also in Docker or whatever you want. And we have two different APIs. So the first one is the classical press API. And the second one is a messages API, synchronous and asynchronous. So if we have existing content in the repository, you have a folder with 100 pictures, and you want to describe that. So you can use the recipe. Yes, to get the document, apply the operation, and update the document. And that's fine, because you can make a batch with that. Okay. You have all the operations available. And if you want to create that like more dynamically, when the people drops the document, yes, perform the action, then you have the messages API, the asynchronous API. So you can listen to the event, okay, there is a new picture, and this picture needs to be summarized. I'm going to summarize the picture, and that's updated. Okay. So these are the two different patterns we can, we can apply for it. What we are going to see now, again, live, everything is running on my laptop, just believe me, is something that allows us to classify a document. So we are going to upload a document. We are creating this rule. The rule is the same just for you to make the similarity with what is before. So we have a list of languages, Japanese, Vietnamese, English, whatever. And we are creating a rule to move the document to the right folder. So you draw a path document, and the document is moved to the right folder. Okay. Okay. So let's do that. Okay. Let's open a fresco. So there is a folder at some point. And this folder has a rule that is classifying the documents that I'm dropping on it. Okay. So if I, for instance, come for classify, no, for classify things, we are going to try with a Vietnamese one. It has to be a bit creative. Okay. Okay. So at this point, a fresco is listening to this new document, and it's classifying the document. So it's just selecting a term from the list of terms, and the document has been updated. So it has been classified. So if I refresh, what I find is that the document is on the Vietnamese folder, and you can do that with invoices, with whatever you want. And we can track that it was mistral, the LLM, that created this classification. Okay. Pretty easy, right? So you can integrate also all the other operations in that to get some automation. Okay. So I guess that I was running out. But no problem. So we have more time for questions. So again, this is a simple framework. You can deploy that on premise. You can choose your LLM. You have an initial REST API for operations. Public works are welcome. And then you need to integrate that with your product, with your organization, or whatever. Right. There is also some interesting hackathon with more use cases. So I presented you some use cases, but you have more of them on this hackathon. The slides will be, they are available on the, on Foxen. Okay. And also I'm using Olamma, but there are many other alternatives. You don't need to choose Olamma. So you have GPT4 all locally. This solution is the one used by, by next cloud, second state, high-end phase is the most known probably. But again, just, this is an initial framework. Take it as it and try some things with, with the NAA. Okay. That was all. Thanks. Thank you very much, Angel. Are there any questions? I'm going to do it in the order of the rule. Thank you, Angel. It seems to me all these operations are on one picture or one document. Are you also considering me asking a question on all my documents? No. So this, this sample is only for a single document or a single picture. But, but that is as easy as you have the database, the Neo4j database, then you can include as information as you want for a single document or for a single query. Right. So what I'm doing in the source code is to remove the previous information. You have to create something that is only for a single document. But you can modify that in order to add more than one document to one query. But on the sample is only for a document or a picture. While summarizing the Japanese PDF, why did you need to provide for context your picture Sorry. You showed the summarization of the Japanese PDF. Yeah. And then you provided for context the picture. No, no, the picture was for the last operation. So the three first operations for summarize, for classify and for prompting were related with the document in Japanese. I could use some other document. I know, but I love the document because I'm using this for testing for 15 years, something like that. So it's like my, my precious document. And, and the picture was there for the last one. It was the description of that picture that is more or less like, like yours then. Thank you. Similar to the previous question that I had, but for a single document, right. So the summarization for very large documents. Yeah. So, the problem is that again, I'm running on my lap. So I cannot use like a very large document, but I was just trying to summarize, for instance, books. Do you know the Gutenberg project? On the Gutenberg project, you have all the classics of Alice in Wonderland and so on. So I was trying to do that with that kind of documents. And it's able to do that, it takes a while, like minutes on my machine. Again, if instant adjusin, the regular CPU, you use a GPU, the tiny slide, I don't know, 100 faster, something like that. So I don't know. I need to make serious test with that. But having the right infrastructure, I guess that the, the performance is enough. It's not something like very instantaneous, right? But you can work with it. Thank you very much. Any other questions? Yes. Hi. A follow up on the previous question. Was the insertion into the vector is database taking a lot of time or was the actual query to the LLM? Because the insertion into the vector is database has to be done once, whereas the query can be done multiple times if, if you already vectorize the document, right? Yeah. So again, I was not trying to deliver a session on how to develop AI, right? It was just to create a framework. You have the AI track that can reply to you better than me in relation to that. But yeah, obviously, you can use the database. I'm not, I'm only using the database for a context of a single document, right? So you can create categories, you can add more than one document. You can add also the, the links to the response and, and so on. So yeah, sorry. Maybe I didn't understand you. Maybe you misunderstood my question. My question was when you added the Alice in Wonderland book, was it the vectorization that took time or was it the query to the LLM? No, no, it was vectorization, vectorization of the chance of the document. Okay. Sorry. That was the only one question. I'm not an expert, but I know a bit. Any other question? Okay. Thanks. Okay. One more question. Last one. I'll be around. So if someone just wants to, to catch me. Can you say a bit more about like the biggest use cases you see and if there's any open source setups of this that are out there for us to look at? In my opinion, the, the main use case of that is searching. So but this is a different world with different beasts. So but for searching AI, it's really quite relevant. So again, this is just to create a framework and then it's just to apply your imagination. Thank you very much, Angel. Thanks. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.120000000000001, "text": " Very much. So our next person is Ahel Boroy from Highland that's going to talk to us about", "tokens": [50364, 4372, 709, 13, 407, 527, 958, 954, 307, 2438, 338, 13739, 939, 490, 5229, 1661, 300, 311, 516, 281, 751, 281, 505, 466, 50870], "temperature": 0.0, "avg_logprob": -0.387837851919779, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.2205193042755127}, {"id": 1, "seek": 0, "start": 10.120000000000001, "end": 13.52, "text": " using generative AI and content service platforms together.", "tokens": [50870, 1228, 1337, 1166, 7318, 293, 2701, 2643, 9473, 1214, 13, 51040], "temperature": 0.0, "avg_logprob": -0.387837851919779, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.2205193042755127}, {"id": 2, "seek": 0, "start": 13.52, "end": 18.12, "text": " Thanks. I was on. I was on.", "tokens": [51040, 2561, 13, 286, 390, 322, 13, 286, 390, 322, 13, 51270], "temperature": 0.0, "avg_logprob": -0.387837851919779, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.2205193042755127}, {"id": 3, "seek": 0, "start": 18.12, "end": 19.28, "text": " I was checking the microphone.", "tokens": [51270, 286, 390, 8568, 264, 10952, 13, 51328], "temperature": 0.0, "avg_logprob": -0.387837851919779, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.2205193042755127}, {"id": 4, "seek": 0, "start": 19.28, "end": 26.32, "text": " Okay, yep. Welcome to everyone. So this is another view on the same topic. So we are", "tokens": [51328, 1033, 11, 18633, 13, 4027, 281, 1518, 13, 407, 341, 307, 1071, 1910, 322, 264, 912, 4829, 13, 407, 321, 366, 51680], "temperature": 0.0, "avg_logprob": -0.387837851919779, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.2205193042755127}, {"id": 5, "seek": 2632, "start": 26.32, "end": 33.16, "text": " going on the technical side now. It's not like a final feature for a product, but it's", "tokens": [50364, 516, 322, 264, 6191, 1252, 586, 13, 467, 311, 406, 411, 257, 2572, 4111, 337, 257, 1674, 11, 457, 309, 311, 50706], "temperature": 0.0, "avg_logprob": -0.28774942670549664, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.04893488436937332}, {"id": 6, "seek": 2632, "start": 33.16, "end": 38.88, "text": " a framework in order to help you to build all the features that we were seeing before", "tokens": [50706, 257, 8388, 294, 1668, 281, 854, 291, 281, 1322, 439, 264, 4122, 300, 321, 645, 2577, 949, 50992], "temperature": 0.0, "avg_logprob": -0.28774942670549664, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.04893488436937332}, {"id": 7, "seek": 2632, "start": 38.88, "end": 45.32, "text": " in the context of a content service platform or a document. Okay, so we are going to review", "tokens": [50992, 294, 264, 4319, 295, 257, 2701, 2643, 3663, 420, 257, 4166, 13, 1033, 11, 370, 321, 366, 516, 281, 3131, 51314], "temperature": 0.0, "avg_logprob": -0.28774942670549664, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.04893488436937332}, {"id": 8, "seek": 2632, "start": 45.32, "end": 52.28, "text": " some in a stack. The next step we are going to use that is including also LLN on premise.", "tokens": [51314, 512, 294, 257, 8630, 13, 440, 958, 1823, 321, 366, 516, 281, 764, 300, 307, 3009, 611, 441, 43, 45, 322, 22045, 13, 51662], "temperature": 0.0, "avg_logprob": -0.28774942670549664, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.04893488436937332}, {"id": 9, "seek": 5228, "start": 52.28, "end": 57.24, "text": " We are going to review all the options. We are going also to describe the features we", "tokens": [50364, 492, 366, 516, 281, 3131, 439, 264, 3956, 13, 492, 366, 516, 611, 281, 6786, 264, 4122, 321, 50612], "temperature": 0.0, "avg_logprob": -0.17326660526608959, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.034774042665958405}, {"id": 10, "seek": 5228, "start": 57.24, "end": 63.04, "text": " can build with this stack. And then we are going to review how to integrate that with", "tokens": [50612, 393, 1322, 365, 341, 8630, 13, 400, 550, 321, 366, 516, 281, 3131, 577, 281, 13365, 300, 365, 50902], "temperature": 0.0, "avg_logprob": -0.17326660526608959, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.034774042665958405}, {"id": 11, "seek": 5228, "start": 63.04, "end": 68.4, "text": " your in our case because I'm working for Highland and we are building an open source product", "tokens": [50902, 428, 294, 527, 1389, 570, 286, 478, 1364, 337, 5229, 1661, 293, 321, 366, 2390, 364, 1269, 4009, 1674, 51170], "temperature": 0.0, "avg_logprob": -0.17326660526608959, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.034774042665958405}, {"id": 12, "seek": 5228, "start": 68.4, "end": 72.92, "text": " with the name of Fresco that is related to content management. So we are going to see", "tokens": [51170, 365, 264, 1315, 295, 42618, 1291, 300, 307, 4077, 281, 2701, 4592, 13, 407, 321, 366, 516, 281, 536, 51396], "temperature": 0.0, "avg_logprob": -0.17326660526608959, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.034774042665958405}, {"id": 13, "seek": 5228, "start": 72.92, "end": 78.92, "text": " how to integrate that with that content management platform and also just looking a few to the", "tokens": [51396, 577, 281, 13365, 300, 365, 300, 2701, 4592, 3663, 293, 611, 445, 1237, 257, 1326, 281, 264, 51696], "temperature": 0.0, "avg_logprob": -0.17326660526608959, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.034774042665958405}, {"id": 14, "seek": 7892, "start": 79.08, "end": 85.44, "text": " future. And obviously I need to include some AI picture because it is what it is. Anyway,", "tokens": [50372, 2027, 13, 400, 2745, 286, 643, 281, 4090, 512, 7318, 3036, 570, 309, 307, 437, 309, 307, 13, 5684, 11, 50690], "temperature": 0.0, "avg_logprob": -0.25010611794211646, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.022625621408224106}, {"id": 15, "seek": 7892, "start": 85.44, "end": 92.44, "text": " so this GenAI stack that we are using includes mainly three components. So the first one", "tokens": [50690, 370, 341, 3632, 48698, 8630, 300, 321, 366, 1228, 5974, 8704, 1045, 6677, 13, 407, 264, 700, 472, 51040], "temperature": 0.0, "avg_logprob": -0.25010611794211646, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.022625621408224106}, {"id": 16, "seek": 7892, "start": 94.36, "end": 101.36, "text": " is Olama. Olama is a service that is able to provide an API to interact with different", "tokens": [51136, 307, 6141, 2404, 13, 6141, 2404, 307, 257, 2643, 300, 307, 1075, 281, 2893, 364, 9362, 281, 4648, 365, 819, 51486], "temperature": 0.0, "avg_logprob": -0.25010611794211646, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.022625621408224106}, {"id": 17, "seek": 10136, "start": 102.36, "end": 109.36, "text": " LLMs. We are going to see later all the list but you can download your LLN on premise and", "tokens": [50414, 441, 43, 26386, 13, 492, 366, 516, 281, 536, 1780, 439, 264, 1329, 457, 291, 393, 5484, 428, 441, 43, 45, 322, 22045, 293, 50764], "temperature": 0.0, "avg_logprob": -0.21342345078786215, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.03823763132095337}, {"id": 18, "seek": 10136, "start": 109.68, "end": 115.08, "text": " this layer is providing the interaction with the LLM. You can even interact with different", "tokens": [50780, 341, 4583, 307, 6530, 264, 9285, 365, 264, 441, 43, 44, 13, 509, 393, 754, 4648, 365, 819, 51050], "temperature": 0.0, "avg_logprob": -0.21342345078786215, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.03823763132095337}, {"id": 19, "seek": 10136, "start": 115.08, "end": 122.08, "text": " LLMs at the same time. The second one is Neo4j. So Neo4j is the vector database when", "tokens": [51050, 441, 43, 26386, 412, 264, 912, 565, 13, 440, 1150, 472, 307, 24458, 19, 73, 13, 407, 24458, 19, 73, 307, 264, 8062, 8149, 562, 51400], "temperature": 0.0, "avg_logprob": -0.21342345078786215, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.03823763132095337}, {"id": 20, "seek": 10136, "start": 122.76, "end": 129.76, "text": " you are using rack, we augmented reality and so on. Then you need to increase the information", "tokens": [51434, 291, 366, 1228, 14788, 11, 321, 36155, 4103, 293, 370, 322, 13, 1396, 291, 643, 281, 3488, 264, 1589, 51784], "temperature": 0.0, "avg_logprob": -0.21342345078786215, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.03823763132095337}, {"id": 21, "seek": 12976, "start": 130.76, "end": 137.76, "text": " for the LLM. So you are storing all this information in this database. And finally we are using", "tokens": [50414, 337, 264, 441, 43, 44, 13, 407, 291, 366, 26085, 439, 341, 1589, 294, 341, 8149, 13, 400, 2721, 321, 366, 1228, 50764], "temperature": 0.0, "avg_logprob": -0.2629889511480564, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.00422444986179471}, {"id": 22, "seek": 12976, "start": 137.76, "end": 144.76, "text": " land change. So this framework is providing land change that is a framework to communicate", "tokens": [50764, 2117, 1319, 13, 407, 341, 8388, 307, 6530, 2117, 1319, 300, 307, 257, 8388, 281, 7890, 51114], "temperature": 0.0, "avg_logprob": -0.2629889511480564, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.00422444986179471}, {"id": 23, "seek": 12976, "start": 145.84, "end": 150.44, "text": " all these different elements. So this framework is in Python but if you are not comfortable", "tokens": [51168, 439, 613, 819, 4959, 13, 407, 341, 8388, 307, 294, 15329, 457, 498, 291, 366, 406, 4619, 51398], "temperature": 0.0, "avg_logprob": -0.2629889511480564, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.00422444986179471}, {"id": 24, "seek": 12976, "start": 150.44, "end": 157.44, "text": " with Python there are many other languages that are including this kind of piece. Okay,", "tokens": [51398, 365, 15329, 456, 366, 867, 661, 8650, 300, 366, 3009, 341, 733, 295, 2522, 13, 1033, 11, 51748], "temperature": 0.0, "avg_logprob": -0.2629889511480564, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.00422444986179471}, {"id": 25, "seek": 15744, "start": 157.8, "end": 164.3, "text": " so mainly what we have, if someone doesn't like Docker there is no problem so you still", "tokens": [50382, 370, 8704, 437, 321, 362, 11, 498, 1580, 1177, 380, 411, 33772, 456, 307, 572, 1154, 370, 291, 920, 50707], "temperature": 0.0, "avg_logprob": -0.2759178706577846, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0038554510101675987}, {"id": 26, "seek": 15744, "start": 164.3, "end": 171.3, "text": " can deploy that without it but it is oriented to services. So you have Olama that is the", "tokens": [50707, 393, 7274, 300, 1553, 309, 457, 309, 307, 21841, 281, 3328, 13, 407, 291, 362, 6141, 2404, 300, 307, 264, 51057], "temperature": 0.0, "avg_logprob": -0.2759178706577846, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0038554510101675987}, {"id": 27, "seek": 15744, "start": 171.3, "end": 178.3, "text": " one providing the services for the LLM that can be used in GPU or not. So you can, we", "tokens": [51057, 472, 6530, 264, 3328, 337, 264, 441, 43, 44, 300, 393, 312, 1143, 294, 18407, 420, 406, 13, 407, 291, 393, 11, 321, 51407], "temperature": 0.0, "avg_logprob": -0.2759178706577846, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0038554510101675987}, {"id": 28, "seek": 15744, "start": 178.32, "end": 184.44, "text": " are going to run this without the GPU. Just using my regular CPU on my computer. This", "tokens": [51408, 366, 516, 281, 1190, 341, 1553, 264, 18407, 13, 1449, 1228, 452, 3890, 13199, 322, 452, 3820, 13, 639, 51714], "temperature": 0.0, "avg_logprob": -0.2759178706577846, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0038554510101675987}, {"id": 29, "seek": 18444, "start": 184.44, "end": 189.98, "text": " is lower. I recommend you to use a GPU but you can do that. And we are piling all the", "tokens": [50364, 307, 3126, 13, 286, 2748, 291, 281, 764, 257, 18407, 457, 291, 393, 360, 300, 13, 400, 321, 366, 280, 4883, 439, 264, 50641], "temperature": 0.0, "avg_logprob": -0.2134816957556683, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.0112002519890666}, {"id": 30, "seek": 18444, "start": 189.98, "end": 196.98, "text": " models that we need so we can just use more than one model. With that we can increase", "tokens": [50641, 5245, 300, 321, 643, 370, 321, 393, 445, 764, 544, 813, 472, 2316, 13, 2022, 300, 321, 393, 3488, 50991], "temperature": 0.0, "avg_logprob": -0.2134816957556683, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.0112002519890666}, {"id": 31, "seek": 18444, "start": 197.84, "end": 204.84, "text": " the information for the operation with that Neo4j database and we can develop an API with", "tokens": [51034, 264, 1589, 337, 264, 6916, 365, 300, 24458, 19, 73, 8149, 293, 321, 393, 1499, 364, 9362, 365, 51384], "temperature": 0.0, "avg_logprob": -0.2134816957556683, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.0112002519890666}, {"id": 32, "seek": 18444, "start": 205.68, "end": 212.18, "text": " this string LLM, with this framework. Okay, so these are the pieces. You have the project", "tokens": [51426, 341, 6798, 441, 43, 44, 11, 365, 341, 8388, 13, 1033, 11, 370, 613, 366, 264, 3755, 13, 509, 362, 264, 1716, 51751], "temperature": 0.0, "avg_logprob": -0.2134816957556683, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.0112002519890666}, {"id": 33, "seek": 21218, "start": 213.18, "end": 220.18, "text": " Docker.gen.a stack. Mainly these pieces is a sample. This sample is oriented to prompting.", "tokens": [50414, 33772, 13, 1766, 13, 64, 8630, 13, 47468, 613, 3755, 307, 257, 6889, 13, 639, 6889, 307, 21841, 281, 12391, 278, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2663428518507216, "compression_ratio": 1.454054054054054, "no_speech_prob": 0.011304933577775955}, {"id": 34, "seek": 21218, "start": 221.5, "end": 226.58, "text": " You have to reply questions. So we are going to do something a bit different from that", "tokens": [50830, 509, 362, 281, 16972, 1651, 13, 407, 321, 366, 516, 281, 360, 746, 257, 857, 819, 490, 300, 51084], "temperature": 0.0, "avg_logprob": -0.2663428518507216, "compression_ratio": 1.454054054054054, "no_speech_prob": 0.011304933577775955}, {"id": 35, "seek": 21218, "start": 226.58, "end": 233.58, "text": " but the first sample you can try is this one. Okay, so all the LLMs that are able to manage", "tokens": [51084, 457, 264, 700, 6889, 291, 393, 853, 307, 341, 472, 13, 1033, 11, 370, 439, 264, 441, 43, 26386, 300, 366, 1075, 281, 3067, 51434], "temperature": 0.0, "avg_logprob": -0.2663428518507216, "compression_ratio": 1.454054054054054, "no_speech_prob": 0.011304933577775955}, {"id": 36, "seek": 23358, "start": 233.86, "end": 240.86, "text": " Olama today are these ones. Likely as this is growing every day there are more. But this", "tokens": [50378, 6141, 2404, 965, 366, 613, 2306, 13, 1743, 356, 382, 341, 307, 4194, 633, 786, 456, 366, 544, 13, 583, 341, 50728], "temperature": 0.0, "avg_logprob": -0.2656996009055148, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.009437771514058113}, {"id": 37, "seek": 23358, "start": 243.58, "end": 249.14000000000001, "text": " was like last week. Okay, so this is what you need to understand. Obviously the larger", "tokens": [50864, 390, 411, 1036, 1243, 13, 1033, 11, 370, 341, 307, 437, 291, 643, 281, 1223, 13, 7580, 264, 4833, 51142], "temperature": 0.0, "avg_logprob": -0.2656996009055148, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.009437771514058113}, {"id": 38, "seek": 23358, "start": 249.14000000000001, "end": 256.14, "text": " the better but you need to take into account your resources. So these are very small. Your", "tokens": [51142, 264, 1101, 457, 291, 643, 281, 747, 666, 2696, 428, 3593, 13, 407, 613, 366, 588, 1359, 13, 2260, 51492], "temperature": 0.0, "avg_logprob": -0.2656996009055148, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.009437771514058113}, {"id": 39, "seek": 23358, "start": 256.38, "end": 262.98, "text": " 4 gigabytes of RAM and 2 gigabytes of storage. So you can run that on a computer and then", "tokens": [51504, 1017, 42741, 295, 14561, 293, 568, 42741, 295, 6725, 13, 407, 291, 393, 1190, 300, 322, 257, 3820, 293, 550, 51834], "temperature": 0.0, "avg_logprob": -0.2656996009055148, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.009437771514058113}, {"id": 40, "seek": 26298, "start": 262.98, "end": 268.94, "text": " if you want to use something that is better is also larger in resources. And you can even", "tokens": [50364, 498, 291, 528, 281, 764, 746, 300, 307, 1101, 307, 611, 4833, 294, 3593, 13, 400, 291, 393, 754, 50662], "temperature": 0.0, "avg_logprob": -0.19676077884176504, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.003743777982890606}, {"id": 41, "seek": 26298, "start": 268.94, "end": 275.94, "text": " use these LLMs that require, I don't know, many different computers once I say, okay.", "tokens": [50662, 764, 613, 441, 43, 26386, 300, 3651, 11, 286, 500, 380, 458, 11, 867, 819, 10807, 1564, 286, 584, 11, 1392, 13, 51012], "temperature": 0.0, "avg_logprob": -0.19676077884176504, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.003743777982890606}, {"id": 42, "seek": 26298, "start": 277.90000000000003, "end": 284.90000000000003, "text": " So today we are going to use this kind of LLMs. Also it's relevant to look at the license.", "tokens": [51110, 407, 965, 321, 366, 516, 281, 764, 341, 733, 295, 441, 43, 26386, 13, 2743, 309, 311, 7340, 281, 574, 412, 264, 10476, 13, 51460], "temperature": 0.0, "avg_logprob": -0.19676077884176504, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.003743777982890606}, {"id": 43, "seek": 26298, "start": 286.98, "end": 292.1, "text": " So just was talking before about the license. So this is also something relevant if you", "tokens": [51564, 407, 445, 390, 1417, 949, 466, 264, 10476, 13, 407, 341, 307, 611, 746, 7340, 498, 291, 51820], "temperature": 0.0, "avg_logprob": -0.19676077884176504, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.003743777982890606}, {"id": 44, "seek": 29210, "start": 292.1, "end": 296.70000000000005, "text": " want to build something commercial or something open source or whatever. You need to take care", "tokens": [50364, 528, 281, 1322, 746, 6841, 420, 746, 1269, 4009, 420, 2035, 13, 509, 643, 281, 747, 1127, 50594], "temperature": 0.0, "avg_logprob": -0.2050457000732422, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.004556282423436642}, {"id": 45, "seek": 29210, "start": 296.70000000000005, "end": 303.70000000000005, "text": " of the license. Also you can look that there is some weight license there because you have", "tokens": [50594, 295, 264, 10476, 13, 2743, 291, 393, 574, 300, 456, 307, 512, 3364, 10476, 456, 570, 291, 362, 50944], "temperature": 0.0, "avg_logprob": -0.2050457000732422, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.004556282423436642}, {"id": 46, "seek": 29210, "start": 304.22, "end": 310.14000000000004, "text": " this LLM2 community license agreement that some people say that this open source, some", "tokens": [50970, 341, 441, 43, 44, 17, 1768, 10476, 8106, 300, 512, 561, 584, 300, 341, 1269, 4009, 11, 512, 51266], "temperature": 0.0, "avg_logprob": -0.2050457000732422, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.004556282423436642}, {"id": 47, "seek": 29210, "start": 310.14000000000004, "end": 315.78000000000003, "text": " other people say that it is not. So it's something different. So better to check if you don't", "tokens": [51266, 661, 561, 584, 300, 309, 307, 406, 13, 407, 309, 311, 746, 819, 13, 407, 1101, 281, 1520, 498, 291, 500, 380, 51548], "temperature": 0.0, "avg_logprob": -0.2050457000732422, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.004556282423436642}, {"id": 48, "seek": 29210, "start": 315.78000000000003, "end": 321.78000000000003, "text": " see a patchy license or something that you can recognize. Better to check the conditions.", "tokens": [51548, 536, 257, 9972, 88, 10476, 420, 746, 300, 291, 393, 5521, 13, 15753, 281, 1520, 264, 4487, 13, 51848], "temperature": 0.0, "avg_logprob": -0.2050457000732422, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.004556282423436642}, {"id": 49, "seek": 32178, "start": 322.65999999999997, "end": 329.65999999999997, "text": " So you have a lot of them to choose. We are going to work today on the demo with Mistral,", "tokens": [50408, 407, 291, 362, 257, 688, 295, 552, 281, 2826, 13, 492, 366, 516, 281, 589, 965, 322, 264, 10723, 365, 20166, 2155, 11, 50758], "temperature": 0.0, "avg_logprob": -0.30411074534956234, "compression_ratio": 1.416243654822335, "no_speech_prob": 0.005018973257392645}, {"id": 50, "seek": 32178, "start": 330.65999999999997, "end": 337.65999999999997, "text": " CementB, that is a French company that is producing this kind of LLMs that are more or less the", "tokens": [50808, 383, 1712, 33, 11, 300, 307, 257, 5522, 2237, 300, 307, 10501, 341, 733, 295, 441, 43, 26386, 300, 366, 544, 420, 1570, 264, 51158], "temperature": 0.0, "avg_logprob": -0.30411074534956234, "compression_ratio": 1.416243654822335, "no_speech_prob": 0.005018973257392645}, {"id": 51, "seek": 32178, "start": 339.09999999999997, "end": 346.09999999999997, "text": " same performance as GPT 3.5. So it's good enough. And so what is open source, the LLM is free", "tokens": [51230, 912, 3389, 382, 26039, 51, 805, 13, 20, 13, 407, 309, 311, 665, 1547, 13, 400, 370, 437, 307, 1269, 4009, 11, 264, 441, 43, 44, 307, 1737, 51580], "temperature": 0.0, "avg_logprob": -0.30411074534956234, "compression_ratio": 1.416243654822335, "no_speech_prob": 0.005018973257392645}, {"id": 52, "seek": 34610, "start": 346.86, "end": 353.86, "text": " to download and to use, but the training data is not free and likely it has some copyright", "tokens": [50402, 281, 5484, 293, 281, 764, 11, 457, 264, 3097, 1412, 307, 406, 1737, 293, 3700, 309, 575, 512, 17996, 50752], "temperature": 0.0, "avg_logprob": -0.29747657775878905, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.04664007946848869}, {"id": 53, "seek": 34610, "start": 356.70000000000005, "end": 362.70000000000005, "text": " material on it. We don't know because it's not free. So on the next law ethical AI writing", "tokens": [50894, 2527, 322, 309, 13, 492, 500, 380, 458, 570, 309, 311, 406, 1737, 13, 407, 322, 264, 958, 2101, 18890, 7318, 3579, 51194], "temperature": 0.0, "avg_logprob": -0.29747657775878905, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.04664007946848869}, {"id": 54, "seek": 34610, "start": 362.70000000000005, "end": 369.70000000000005, "text": " we have, sorry, yellow. I thought it was orange but it's yellow. Okay. It's more or less fine.", "tokens": [51194, 321, 362, 11, 2597, 11, 5566, 13, 286, 1194, 309, 390, 7671, 457, 309, 311, 5566, 13, 1033, 13, 467, 311, 544, 420, 1570, 2489, 13, 51544], "temperature": 0.0, "avg_logprob": -0.29747657775878905, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.04664007946848869}, {"id": 55, "seek": 36970, "start": 370.7, "end": 377.7, "text": " So we are just only missing one. That was for text and for pictures. We know some LLM", "tokens": [50414, 407, 321, 366, 445, 787, 5361, 472, 13, 663, 390, 337, 2487, 293, 337, 5242, 13, 492, 458, 512, 441, 43, 44, 50764], "temperature": 0.0, "avg_logprob": -0.19722767950783313, "compression_ratio": 1.5028901734104045, "no_speech_prob": 0.01579909585416317}, {"id": 56, "seek": 36970, "start": 380.9, "end": 387.9, "text": " with a visual encoder on it. So for this part we are going to use lava. And lava really", "tokens": [50924, 365, 257, 5056, 2058, 19866, 322, 309, 13, 407, 337, 341, 644, 321, 366, 516, 281, 764, 22097, 13, 400, 22097, 534, 51274], "temperature": 0.0, "avg_logprob": -0.19722767950783313, "compression_ratio": 1.5028901734104045, "no_speech_prob": 0.01579909585416317}, {"id": 57, "seek": 36970, "start": 387.98, "end": 394.98, "text": " is granting all the different requirements. So we are using a green LLM for this other", "tokens": [51278, 307, 50204, 439, 264, 819, 7728, 13, 407, 321, 366, 1228, 257, 3092, 441, 43, 44, 337, 341, 661, 51628], "temperature": 0.0, "avg_logprob": -0.19722767950783313, "compression_ratio": 1.5028901734104045, "no_speech_prob": 0.01579909585416317}, {"id": 58, "seek": 39498, "start": 395.22, "end": 402.22, "text": " sample. Okay. Perfect. So all the demo is running on my computer while I'm there in the presentation.", "tokens": [50376, 6889, 13, 1033, 13, 10246, 13, 407, 439, 264, 10723, 307, 2614, 322, 452, 3820, 1339, 286, 478, 456, 294, 264, 5860, 13, 50726], "temperature": 0.0, "avg_logprob": -0.3289149378387021, "compression_ratio": 1.385786802030457, "no_speech_prob": 0.005162580404430628}, {"id": 59, "seek": 39498, "start": 405.14000000000004, "end": 412.14000000000004, "text": " So I have everything running inside is 32 gigabytes of RAM and is AM64 architecture.", "tokens": [50872, 407, 286, 362, 1203, 2614, 1854, 307, 8858, 42741, 295, 14561, 293, 307, 6475, 19395, 9482, 13, 51222], "temperature": 0.0, "avg_logprob": -0.3289149378387021, "compression_ratio": 1.385786802030457, "no_speech_prob": 0.005162580404430628}, {"id": 60, "seek": 39498, "start": 412.46000000000004, "end": 419.46000000000004, "text": " So it's not AMD. It's MacBook Pro two years ago, something like that. Okay. As we were", "tokens": [51238, 407, 309, 311, 406, 34808, 13, 467, 311, 31737, 1705, 732, 924, 2057, 11, 746, 411, 300, 13, 1033, 13, 1018, 321, 645, 51588], "temperature": 0.0, "avg_logprob": -0.3289149378387021, "compression_ratio": 1.385786802030457, "no_speech_prob": 0.005162580404430628}, {"id": 61, "seek": 41946, "start": 419.73999999999995, "end": 426.73999999999995, "text": " also reviewing the previous version before this GEN AI momentum, we also had some data", "tokens": [50378, 611, 19576, 264, 3894, 3037, 949, 341, 460, 2195, 7318, 11244, 11, 321, 611, 632, 512, 1412, 50728], "temperature": 0.0, "avg_logprob": -0.4089763580806672, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.002279676264151931}, {"id": 62, "seek": 41946, "start": 434.78, "end": 439.78, "text": " section, test recognition, test classification, content analysis. Anyone is using content", "tokens": [51130, 3541, 11, 1500, 11150, 11, 1500, 21538, 11, 2701, 5215, 13, 14643, 307, 1228, 2701, 51380], "temperature": 0.0, "avg_logprob": -0.4089763580806672, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.002279676264151931}, {"id": 63, "seek": 41946, "start": 439.78, "end": 446.78, "text": " analysis for a real use case? Okay. It was not me. So it's something but you saw. But", "tokens": [51380, 5215, 337, 257, 957, 764, 1389, 30, 1033, 13, 467, 390, 406, 385, 13, 407, 309, 311, 746, 457, 291, 1866, 13, 583, 51730], "temperature": 0.0, "avg_logprob": -0.4089763580806672, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.002279676264151931}, {"id": 64, "seek": 44678, "start": 446.78, "end": 452.58, "text": " we have all the things, right? Some kind of automation. But now with the GEN AI, we have", "tokens": [50364, 321, 362, 439, 264, 721, 11, 558, 30, 2188, 733, 295, 17769, 13, 583, 586, 365, 264, 460, 2195, 7318, 11, 321, 362, 50654], "temperature": 0.0, "avg_logprob": -0.2498526419362714, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.01833328790962696}, {"id": 65, "seek": 44678, "start": 452.58, "end": 459.58, "text": " also a power classification. We could classify in the past. But now we can classify better.", "tokens": [50654, 611, 257, 1347, 21538, 13, 492, 727, 33872, 294, 264, 1791, 13, 583, 586, 321, 393, 33872, 1101, 13, 51004], "temperature": 0.0, "avg_logprob": -0.2498526419362714, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.01833328790962696}, {"id": 66, "seek": 44678, "start": 460.82, "end": 465.5, "text": " We can also, and when I say translate, we are going to see later the demo. Obviously", "tokens": [51066, 492, 393, 611, 11, 293, 562, 286, 584, 13799, 11, 321, 366, 516, 281, 536, 1780, 264, 10723, 13, 7580, 51300], "temperature": 0.0, "avg_logprob": -0.2498526419362714, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.01833328790962696}, {"id": 67, "seek": 44678, "start": 465.5, "end": 471.5, "text": " we can translate. But we can also interact with the LLM in one language and to get the", "tokens": [51300, 321, 393, 13799, 13, 583, 321, 393, 611, 4648, 365, 264, 441, 43, 44, 294, 472, 2856, 293, 281, 483, 264, 51600], "temperature": 0.0, "avg_logprob": -0.2498526419362714, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.01833328790962696}, {"id": 68, "seek": 47150, "start": 471.5, "end": 476.86, "text": " response in another language. Right? So that is the difference. We can also summarize", "tokens": [50364, 4134, 294, 1071, 2856, 13, 1779, 30, 407, 300, 307, 264, 2649, 13, 492, 393, 611, 20858, 50632], "temperature": 0.0, "avg_logprob": -0.18211659141208814, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.01767110638320446}, {"id": 69, "seek": 47150, "start": 476.86, "end": 482.7, "text": " a test. This is the most common use case and we can describe a picture. Prompting. Obviously", "tokens": [50632, 257, 1500, 13, 639, 307, 264, 881, 2689, 764, 1389, 293, 321, 393, 6786, 257, 3036, 13, 15833, 662, 278, 13, 7580, 50924], "temperature": 0.0, "avg_logprob": -0.18211659141208814, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.01767110638320446}, {"id": 70, "seek": 47150, "start": 482.7, "end": 489.7, "text": " we can use prompting. We can read that. So we have some new features that we can use in", "tokens": [50924, 321, 393, 764, 12391, 278, 13, 492, 393, 1401, 300, 13, 407, 321, 362, 512, 777, 4122, 300, 321, 393, 764, 294, 51274], "temperature": 0.0, "avg_logprob": -0.18211659141208814, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.01767110638320446}, {"id": 71, "seek": 47150, "start": 490.34, "end": 497.34, "text": " our documents. Okay. We are going to see some of them implemented. Okay. So what is this", "tokens": [51306, 527, 8512, 13, 1033, 13, 492, 366, 516, 281, 536, 512, 295, 552, 12270, 13, 1033, 13, 407, 437, 307, 341, 51656], "temperature": 0.0, "avg_logprob": -0.18211659141208814, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.01767110638320446}, {"id": 72, "seek": 49734, "start": 497.34, "end": 504.34, "text": " project about? It's not yet. Okay. The project is at some point of the slides. Okay. If not,", "tokens": [50364, 1716, 466, 30, 467, 311, 406, 1939, 13, 1033, 13, 440, 1716, 307, 412, 512, 935, 295, 264, 9788, 13, 1033, 13, 759, 406, 11, 50714], "temperature": 0.0, "avg_logprob": -0.251276041006113, "compression_ratio": 1.4540816326530612, "no_speech_prob": 0.01121563371270895}, {"id": 73, "seek": 49734, "start": 508.7, "end": 515.6999999999999, "text": " I will give you the link. So in this project, what is created is a API by using this, all", "tokens": [50932, 286, 486, 976, 291, 264, 2113, 13, 407, 294, 341, 1716, 11, 437, 307, 2942, 307, 257, 9362, 538, 1228, 341, 11, 439, 51282], "temperature": 0.0, "avg_logprob": -0.251276041006113, "compression_ratio": 1.4540816326530612, "no_speech_prob": 0.01121563371270895}, {"id": 74, "seek": 49734, "start": 516.38, "end": 523.38, "text": " these infrastructure in order to provide different services. What we are using is some LLM embeddings.", "tokens": [51316, 613, 6896, 294, 1668, 281, 2893, 819, 3328, 13, 708, 321, 366, 1228, 307, 512, 441, 43, 44, 12240, 29432, 13, 51666], "temperature": 0.0, "avg_logprob": -0.251276041006113, "compression_ratio": 1.4540816326530612, "no_speech_prob": 0.01121563371270895}, {"id": 75, "seek": 52338, "start": 523.46, "end": 530.46, "text": " So we are just trying to avoid hallucinations. Just giving some additional information to", "tokens": [50368, 407, 321, 366, 445, 1382, 281, 5042, 35212, 10325, 13, 1449, 2902, 512, 4497, 1589, 281, 50718], "temperature": 0.0, "avg_logprob": -0.1938657875520637, "compression_ratio": 1.835978835978836, "no_speech_prob": 0.024409031495451927}, {"id": 76, "seek": 52338, "start": 530.46, "end": 534.46, "text": " the database from the document. So we are working with a document. Right? So we are", "tokens": [50718, 264, 8149, 490, 264, 4166, 13, 407, 321, 366, 1364, 365, 257, 4166, 13, 1779, 30, 407, 321, 366, 50918], "temperature": 0.0, "avg_logprob": -0.1938657875520637, "compression_ratio": 1.835978835978836, "no_speech_prob": 0.024409031495451927}, {"id": 77, "seek": 52338, "start": 534.46, "end": 539.46, "text": " not going with search. We are not going with some other applications of GNI. So we are", "tokens": [50918, 406, 516, 365, 3164, 13, 492, 366, 406, 516, 365, 512, 661, 5821, 295, 460, 42496, 13, 407, 321, 366, 51168], "temperature": 0.0, "avg_logprob": -0.1938657875520637, "compression_ratio": 1.835978835978836, "no_speech_prob": 0.024409031495451927}, {"id": 78, "seek": 52338, "start": 539.46, "end": 546.46, "text": " focused on features of a document. So we are adding all that information so we can get", "tokens": [51168, 5178, 322, 4122, 295, 257, 4166, 13, 407, 321, 366, 5127, 439, 300, 1589, 370, 321, 393, 483, 51518], "temperature": 0.0, "avg_logprob": -0.1938657875520637, "compression_ratio": 1.835978835978836, "no_speech_prob": 0.024409031495451927}, {"id": 79, "seek": 54646, "start": 547.34, "end": 554.34, "text": " a better response and more suitable to the document we are dealing with. And for that", "tokens": [50408, 257, 1101, 4134, 293, 544, 12873, 281, 264, 4166, 321, 366, 6260, 365, 13, 400, 337, 300, 50758], "temperature": 0.0, "avg_logprob": -0.21401324200986035, "compression_ratio": 1.5235294117647058, "no_speech_prob": 0.013027828186750412}, {"id": 80, "seek": 54646, "start": 556.86, "end": 563.38, "text": " we are using Mistral. And if we are talking about a picture, then we can use the other", "tokens": [50884, 321, 366, 1228, 20166, 2155, 13, 400, 498, 321, 366, 1417, 466, 257, 3036, 11, 550, 321, 393, 764, 264, 661, 51210], "temperature": 0.0, "avg_logprob": -0.21401324200986035, "compression_ratio": 1.5235294117647058, "no_speech_prob": 0.013027828186750412}, {"id": 81, "seek": 54646, "start": 563.38, "end": 570.38, "text": " LLM that was Java in order, for instance, to describe or classify the picture. We have", "tokens": [51210, 441, 43, 44, 300, 390, 10745, 294, 1668, 11, 337, 5197, 11, 281, 6786, 420, 33872, 264, 3036, 13, 492, 362, 51560], "temperature": 0.0, "avg_logprob": -0.21401324200986035, "compression_ratio": 1.5235294117647058, "no_speech_prob": 0.013027828186750412}, {"id": 82, "seek": 57038, "start": 570.66, "end": 576.98, "text": " also some, so we can choose the LLM. If you want to choose some other LLM than Mistral,", "tokens": [50378, 611, 512, 11, 370, 321, 393, 2826, 264, 441, 43, 44, 13, 759, 291, 528, 281, 2826, 512, 661, 441, 43, 44, 813, 20166, 2155, 11, 50694], "temperature": 0.0, "avg_logprob": -0.15810171438723195, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.022452203556895256}, {"id": 83, "seek": 57038, "start": 576.98, "end": 582.78, "text": " you can do that for text. You can choose some other LLM with a vision and color enabled,", "tokens": [50694, 291, 393, 360, 300, 337, 2487, 13, 509, 393, 2826, 512, 661, 441, 43, 44, 365, 257, 5201, 293, 2017, 15172, 11, 50984], "temperature": 0.0, "avg_logprob": -0.15810171438723195, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.022452203556895256}, {"id": 84, "seek": 57038, "start": 582.78, "end": 589.78, "text": " like Java or some other on the list. And we can also choose the language. So we are going", "tokens": [50984, 411, 10745, 420, 512, 661, 322, 264, 1329, 13, 400, 321, 393, 611, 2826, 264, 2856, 13, 407, 321, 366, 516, 51334], "temperature": 0.0, "avg_logprob": -0.15810171438723195, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.022452203556895256}, {"id": 85, "seek": 57038, "start": 589.86, "end": 596.06, "text": " to see that later. We can just drop a document in Japanese and we are getting the summary", "tokens": [51338, 281, 536, 300, 1780, 13, 492, 393, 445, 3270, 257, 4166, 294, 5433, 293, 321, 366, 1242, 264, 12691, 51648], "temperature": 0.0, "avg_logprob": -0.15810171438723195, "compression_ratio": 1.6635514018691588, "no_speech_prob": 0.022452203556895256}, {"id": 86, "seek": 59606, "start": 596.14, "end": 603.14, "text": " in English or in the other side. Right? And also you can choose some numbers like the", "tokens": [50368, 294, 3669, 420, 294, 264, 661, 1252, 13, 1779, 30, 400, 611, 291, 393, 2826, 512, 3547, 411, 264, 50718], "temperature": 0.0, "avg_logprob": -0.26389807042941243, "compression_ratio": 1.4316939890710383, "no_speech_prob": 0.0136509845033288}, {"id": 87, "seek": 59606, "start": 603.2199999999999, "end": 609.6199999999999, "text": " summary size or the number of tasks and so on. So these are parameters. Okay. So this", "tokens": [50722, 12691, 2744, 420, 264, 1230, 295, 9608, 293, 370, 322, 13, 407, 613, 366, 9834, 13, 1033, 13, 407, 341, 51042], "temperature": 0.0, "avg_logprob": -0.26389807042941243, "compression_ratio": 1.4316939890710383, "no_speech_prob": 0.0136509845033288}, {"id": 88, "seek": 59606, "start": 609.6199999999999, "end": 616.6199999999999, "text": " is the API. Right? Pretty cool invocations. But let's see that leave. As always is better.", "tokens": [51042, 307, 264, 9362, 13, 1779, 30, 10693, 1627, 1048, 905, 763, 13, 583, 718, 311, 536, 300, 1856, 13, 1018, 1009, 307, 1101, 13, 51392], "temperature": 0.0, "avg_logprob": -0.26389807042941243, "compression_ratio": 1.4316939890710383, "no_speech_prob": 0.0136509845033288}, {"id": 89, "seek": 61662, "start": 617.62, "end": 624.62, "text": " Can you see the, better? Okay. Okay. So for instance, I'm going to work, let me find the,", "tokens": [50414, 1664, 291, 536, 264, 11, 1101, 30, 1033, 13, 1033, 13, 407, 337, 5197, 11, 286, 478, 516, 281, 589, 11, 718, 385, 915, 264, 11, 50764], "temperature": 0.0, "avg_logprob": -0.24428637186686197, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.0263061486184597}, {"id": 90, "seek": 61662, "start": 635.0600000000001, "end": 639.86, "text": " I'm going to work with this document. Right? I could be using an English document, but", "tokens": [51286, 286, 478, 516, 281, 589, 365, 341, 4166, 13, 1779, 30, 286, 727, 312, 1228, 364, 3669, 4166, 11, 457, 51526], "temperature": 0.0, "avg_logprob": -0.24428637186686197, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.0263061486184597}, {"id": 91, "seek": 61662, "start": 639.86, "end": 645.86, "text": " it should be easier for the AI. So we are using this one. And I'm also going to use", "tokens": [51526, 309, 820, 312, 3571, 337, 264, 7318, 13, 407, 321, 366, 1228, 341, 472, 13, 400, 286, 478, 611, 516, 281, 764, 51826], "temperature": 0.0, "avg_logprob": -0.24428637186686197, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.0263061486184597}, {"id": 92, "seek": 64662, "start": 647.62, "end": 654.62, "text": " this picture. So for your reference. Okay. Okay. Perfect. So for this document, we are", "tokens": [50414, 341, 3036, 13, 407, 337, 428, 6408, 13, 1033, 13, 1033, 13, 10246, 13, 407, 337, 341, 4166, 11, 321, 366, 50764], "temperature": 0.0, "avg_logprob": -0.20045178814938194, "compression_ratio": 1.560693641618497, "no_speech_prob": 0.002455970970913768}, {"id": 93, "seek": 64662, "start": 655.78, "end": 661.78, "text": " going to ask for a summary. So give me a summary of this document that is in Japanese. So with", "tokens": [50822, 516, 281, 1029, 337, 257, 12691, 13, 407, 976, 385, 257, 12691, 295, 341, 4166, 300, 307, 294, 5433, 13, 407, 365, 51122], "temperature": 0.0, "avg_logprob": -0.20045178814938194, "compression_ratio": 1.560693641618497, "no_speech_prob": 0.002455970970913768}, {"id": 94, "seek": 64662, "start": 661.78, "end": 668.78, "text": " that, if I'm able to. Okay. So this is running on my computer. So I have this ENAI stack", "tokens": [51122, 300, 11, 498, 286, 478, 1075, 281, 13, 1033, 13, 407, 341, 307, 2614, 322, 452, 3820, 13, 407, 286, 362, 341, 15244, 48698, 8630, 51472], "temperature": 0.0, "avg_logprob": -0.20045178814938194, "compression_ratio": 1.560693641618497, "no_speech_prob": 0.002455970970913768}, {"id": 95, "seek": 66878, "start": 669.78, "end": 676.78, "text": " running in this Docker deployment. And I'm getting the request. Okay. And with that, I'm", "tokens": [50414, 2614, 294, 341, 33772, 19317, 13, 400, 286, 478, 1242, 264, 5308, 13, 1033, 13, 400, 365, 300, 11, 286, 478, 50764], "temperature": 0.0, "avg_logprob": -0.36150524351331925, "compression_ratio": 1.6134969325153374, "no_speech_prob": 0.020655732601881027}, {"id": 96, "seek": 66878, "start": 678.22, "end": 685.22, "text": " getting the answer. Okay. So the test, this is a problem with kindergarten, in Japan,", "tokens": [50836, 1242, 264, 1867, 13, 1033, 13, 407, 264, 1500, 11, 341, 307, 257, 1154, 365, 26671, 11, 294, 3367, 11, 51186], "temperature": 0.0, "avg_logprob": -0.36150524351331925, "compression_ratio": 1.6134969325153374, "no_speech_prob": 0.020655732601881027}, {"id": 97, "seek": 66878, "start": 687.6999999999999, "end": 692.6999999999999, "text": " blah, blah, blah. Okay. That's fine. So I'm giving something in Japanese and I'm getting", "tokens": [51310, 12288, 11, 12288, 11, 12288, 13, 1033, 13, 663, 311, 2489, 13, 407, 286, 478, 2902, 746, 294, 5433, 293, 286, 478, 1242, 51560], "temperature": 0.0, "avg_logprob": -0.36150524351331925, "compression_ratio": 1.6134969325153374, "no_speech_prob": 0.020655732601881027}, {"id": 98, "seek": 69270, "start": 692.7, "end": 699.7, "text": " the summary in English. The second one, come on, note this one. I did it. Okay. The second", "tokens": [50364, 264, 12691, 294, 3669, 13, 440, 1150, 472, 11, 808, 322, 11, 3637, 341, 472, 13, 286, 630, 309, 13, 1033, 13, 440, 1150, 50714], "temperature": 0.0, "avg_logprob": -0.259229496547154, "compression_ratio": 1.5227272727272727, "no_speech_prob": 0.016131607815623283}, {"id": 99, "seek": 69270, "start": 704.0200000000001, "end": 711.0200000000001, "text": " one is just to classify. Classify a document that picking a term of a list of terms. So", "tokens": [50930, 472, 307, 445, 281, 33872, 13, 9471, 2505, 257, 4166, 300, 8867, 257, 1433, 295, 257, 1329, 295, 2115, 13, 407, 51280], "temperature": 0.0, "avg_logprob": -0.259229496547154, "compression_ratio": 1.5227272727272727, "no_speech_prob": 0.016131607815623283}, {"id": 100, "seek": 69270, "start": 713.26, "end": 719.46, "text": " I want you to classify this document according to Japanese, Spanish or Vietnamese. Again,", "tokens": [51392, 286, 528, 291, 281, 33872, 341, 4166, 4650, 281, 5433, 11, 8058, 420, 25934, 13, 3764, 11, 51702], "temperature": 0.0, "avg_logprob": -0.259229496547154, "compression_ratio": 1.5227272727272727, "no_speech_prob": 0.016131607815623283}, {"id": 101, "seek": 71946, "start": 719.5, "end": 726.26, "text": " it's an easy example. Right. But you can choose whatever list of values. So if I say just", "tokens": [50366, 309, 311, 364, 1858, 1365, 13, 1779, 13, 583, 291, 393, 2826, 2035, 1329, 295, 4190, 13, 407, 498, 286, 584, 445, 50704], "temperature": 0.0, "avg_logprob": -0.24091702838276707, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.007664792705327272}, {"id": 102, "seek": 71946, "start": 726.26, "end": 733.26, "text": " classify this document into one of these three categories, the term is Japanese because the", "tokens": [50704, 33872, 341, 4166, 666, 472, 295, 613, 1045, 10479, 11, 264, 1433, 307, 5433, 570, 264, 51054], "temperature": 0.0, "avg_logprob": -0.24091702838276707, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.007664792705327272}, {"id": 103, "seek": 71946, "start": 734.5, "end": 741.5, "text": " document is in Japanese. Okay. This is also a Revan for classification. And finally, we", "tokens": [51116, 4166, 307, 294, 5433, 13, 1033, 13, 639, 307, 611, 257, 1300, 9768, 337, 21538, 13, 400, 2721, 11, 321, 51466], "temperature": 0.0, "avg_logprob": -0.24091702838276707, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.007664792705327272}, {"id": 104, "seek": 71946, "start": 741.98, "end": 748.98, "text": " can also make some prompt on the document. What is the name of the zone or this document", "tokens": [51490, 393, 611, 652, 512, 12391, 322, 264, 4166, 13, 708, 307, 264, 1315, 295, 264, 6668, 420, 341, 4166, 51840], "temperature": 0.0, "avg_logprob": -0.24091702838276707, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.007664792705327272}, {"id": 105, "seek": 74946, "start": 749.5400000000001, "end": 755.14, "text": " in Japanese document? The name of the zone is Musoku. Okay. So three different features", "tokens": [50368, 294, 5433, 4166, 30, 440, 1315, 295, 264, 6668, 307, 3569, 13275, 13, 1033, 13, 407, 1045, 819, 4122, 50648], "temperature": 0.0, "avg_logprob": -0.21661677567855173, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.009938209317624569}, {"id": 106, "seek": 74946, "start": 755.14, "end": 762.14, "text": " that we can use on this, on a document. You can build more. Again, it's a Python, Python", "tokens": [50648, 300, 321, 393, 764, 322, 341, 11, 322, 257, 4166, 13, 509, 393, 1322, 544, 13, 3764, 11, 309, 311, 257, 15329, 11, 15329, 50998], "temperature": 0.0, "avg_logprob": -0.21661677567855173, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.009938209317624569}, {"id": 107, "seek": 74946, "start": 762.38, "end": 769.3000000000001, "text": " program with these three specific features, but you can grow up to include something else.", "tokens": [51010, 1461, 365, 613, 1045, 2685, 4122, 11, 457, 291, 393, 1852, 493, 281, 4090, 746, 1646, 13, 51356], "temperature": 0.0, "avg_logprob": -0.21661677567855173, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.009938209317624569}, {"id": 108, "seek": 74946, "start": 769.3000000000001, "end": 775.22, "text": " And if we move to the, to the pictures that was for text, but for the pictures, we can", "tokens": [51356, 400, 498, 321, 1286, 281, 264, 11, 281, 264, 5242, 300, 390, 337, 2487, 11, 457, 337, 264, 5242, 11, 321, 393, 51652], "temperature": 0.0, "avg_logprob": -0.21661677567855173, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.009938209317624569}, {"id": 109, "seek": 77522, "start": 775.26, "end": 782.26, "text": " describe this, this picture. We can also extract some, this is a person, this is, but that", "tokens": [50366, 6786, 341, 11, 341, 3036, 13, 492, 393, 611, 8947, 512, 11, 341, 307, 257, 954, 11, 341, 307, 11, 457, 300, 50716], "temperature": 0.0, "avg_logprob": -0.34478185709240367, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.024334361776709557}, {"id": 110, "seek": 77522, "start": 783.46, "end": 789.46, "text": " was done before. But describing is the, the, the new thing that GNI is providing for us.", "tokens": [50776, 390, 1096, 949, 13, 583, 16141, 307, 264, 11, 264, 11, 264, 777, 551, 300, 460, 42496, 307, 6530, 337, 505, 13, 51076], "temperature": 0.0, "avg_logprob": -0.34478185709240367, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.024334361776709557}, {"id": 111, "seek": 77522, "start": 789.46, "end": 795.46, "text": " This is a bit slower, but in the end, they made so some man posting for the camera. He's", "tokens": [51076, 639, 307, 257, 857, 14009, 11, 457, 294, 264, 917, 11, 436, 1027, 370, 512, 587, 15978, 337, 264, 2799, 13, 634, 311, 51376], "temperature": 0.0, "avg_logprob": -0.34478185709240367, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.024334361776709557}, {"id": 112, "seek": 77522, "start": 795.46, "end": 802.46, "text": " wearing a green beanie, glasses, a black hoodie. And the land yall says air fraked. Well, no,", "tokens": [51376, 4769, 257, 3092, 312, 7155, 11, 10812, 11, 257, 2211, 41191, 13, 400, 264, 2117, 288, 336, 1619, 1988, 431, 7301, 13, 1042, 11, 572, 11, 51726], "temperature": 0.0, "avg_logprob": -0.34478185709240367, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.024334361776709557}, {"id": 113, "seek": 80246, "start": 802.46, "end": 809.46, "text": " it's a fresco, but more or less. Okay. The picture was not big enough, but it's fine.", "tokens": [50364, 309, 311, 257, 25235, 1291, 11, 457, 544, 420, 1570, 13, 1033, 13, 440, 3036, 390, 406, 955, 1547, 11, 457, 309, 311, 2489, 13, 50714], "temperature": 0.0, "avg_logprob": -0.25737962535783354, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.016058020293712616}, {"id": 114, "seek": 80246, "start": 811.02, "end": 818.02, "text": " It's, it's something that is, is useful. And it's not that consuming internal resources,", "tokens": [50792, 467, 311, 11, 309, 311, 746, 300, 307, 11, 307, 4420, 13, 400, 309, 311, 406, 300, 19867, 6920, 3593, 11, 51142], "temperature": 0.0, "avg_logprob": -0.25737962535783354, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.016058020293712616}, {"id": 115, "seek": 80246, "start": 818.0600000000001, "end": 824.1800000000001, "text": " because it's running in, on my machine. So it's, it's fair enough. Okay. Once that we", "tokens": [51144, 570, 309, 311, 2614, 294, 11, 322, 452, 3479, 13, 407, 309, 311, 11, 309, 311, 3143, 1547, 13, 1033, 13, 3443, 300, 321, 51450], "temperature": 0.0, "avg_logprob": -0.25737962535783354, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.016058020293712616}, {"id": 116, "seek": 80246, "start": 824.1800000000001, "end": 831.1800000000001, "text": " have all these features, and we have this, Python, just let me show you a bit. So this", "tokens": [51450, 362, 439, 613, 4122, 11, 293, 321, 362, 341, 11, 15329, 11, 445, 718, 385, 855, 291, 257, 857, 13, 407, 341, 51800], "temperature": 0.0, "avg_logprob": -0.25737962535783354, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.016058020293712616}, {"id": 117, "seek": 83118, "start": 831.9799999999999, "end": 838.9799999999999, "text": " is the project, right? You have the Aeboroi, a fresco GNI, and you have the GNI stack,", "tokens": [50404, 307, 264, 1716, 11, 558, 30, 509, 362, 264, 316, 68, 3918, 4869, 11, 257, 25235, 1291, 460, 42496, 11, 293, 291, 362, 264, 460, 42496, 8630, 11, 50754], "temperature": 0.0, "avg_logprob": -0.36158632620787007, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.006076085846871138}, {"id": 118, "seek": 83118, "start": 840.38, "end": 847.38, "text": " and mainly it's a Python program. Okay. With all these endpoints described, classified,", "tokens": [50824, 293, 8704, 309, 311, 257, 15329, 1461, 13, 1033, 13, 2022, 439, 613, 917, 20552, 7619, 11, 20627, 11, 51174], "temperature": 0.0, "avg_logprob": -0.36158632620787007, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.006076085846871138}, {"id": 119, "seek": 83118, "start": 850.18, "end": 857.18, "text": " prompt, and somebody. Okay. It's no more than that. Okay. If we go back to the original", "tokens": [51314, 12391, 11, 293, 2618, 13, 1033, 13, 467, 311, 572, 544, 813, 300, 13, 1033, 13, 759, 321, 352, 646, 281, 264, 3380, 51664], "temperature": 0.0, "avg_logprob": -0.36158632620787007, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.006076085846871138}, {"id": 120, "seek": 85718, "start": 858.18, "end": 865.18, "text": " goal, is to integrate this kind of operations with our, with our product than in our case", "tokens": [50414, 3387, 11, 307, 281, 13365, 341, 733, 295, 7705, 365, 527, 11, 365, 527, 1674, 813, 294, 527, 1389, 50764], "temperature": 0.0, "avg_logprob": -0.2722121857024811, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.014033634215593338}, {"id": 121, "seek": 85718, "start": 867.26, "end": 874.26, "text": " is a fresco. So a fresco, we can deploy that also in Docker or whatever you want. And we", "tokens": [50868, 307, 257, 25235, 1291, 13, 407, 257, 25235, 1291, 11, 321, 393, 7274, 300, 611, 294, 33772, 420, 2035, 291, 528, 13, 400, 321, 51218], "temperature": 0.0, "avg_logprob": -0.2722121857024811, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.014033634215593338}, {"id": 122, "seek": 85718, "start": 874.66, "end": 879.3, "text": " have two different APIs. So the first one is the classical press API. And the second", "tokens": [51238, 362, 732, 819, 21445, 13, 407, 264, 700, 472, 307, 264, 13735, 1886, 9362, 13, 400, 264, 1150, 51470], "temperature": 0.0, "avg_logprob": -0.2722121857024811, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.014033634215593338}, {"id": 123, "seek": 85718, "start": 879.3, "end": 886.3, "text": " one is a messages API, synchronous and asynchronous. So if we have existing content in the repository,", "tokens": [51470, 472, 307, 257, 7897, 9362, 11, 44743, 293, 49174, 13, 407, 498, 321, 362, 6741, 2701, 294, 264, 25841, 11, 51820], "temperature": 0.0, "avg_logprob": -0.2722121857024811, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.014033634215593338}, {"id": 124, "seek": 88630, "start": 887.3, "end": 893.66, "text": " you have a folder with 100 pictures, and you want to describe that. So you can use the", "tokens": [50414, 291, 362, 257, 10820, 365, 2319, 5242, 11, 293, 291, 528, 281, 6786, 300, 13, 407, 291, 393, 764, 264, 50732], "temperature": 0.0, "avg_logprob": -0.19934552648793097, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0033702936489135027}, {"id": 125, "seek": 88630, "start": 893.66, "end": 899.9799999999999, "text": " recipe. Yes, to get the document, apply the operation, and update the document. And that's", "tokens": [50732, 6782, 13, 1079, 11, 281, 483, 264, 4166, 11, 3079, 264, 6916, 11, 293, 5623, 264, 4166, 13, 400, 300, 311, 51048], "temperature": 0.0, "avg_logprob": -0.19934552648793097, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0033702936489135027}, {"id": 126, "seek": 88630, "start": 899.9799999999999, "end": 906.2199999999999, "text": " fine, because you can make a batch with that. Okay. You have all the operations available.", "tokens": [51048, 2489, 11, 570, 291, 393, 652, 257, 15245, 365, 300, 13, 1033, 13, 509, 362, 439, 264, 7705, 2435, 13, 51360], "temperature": 0.0, "avg_logprob": -0.19934552648793097, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0033702936489135027}, {"id": 127, "seek": 88630, "start": 906.2199999999999, "end": 912.78, "text": " And if you want to create that like more dynamically, when the people drops the document, yes, perform", "tokens": [51360, 400, 498, 291, 528, 281, 1884, 300, 411, 544, 43492, 11, 562, 264, 561, 11438, 264, 4166, 11, 2086, 11, 2042, 51688], "temperature": 0.0, "avg_logprob": -0.19934552648793097, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0033702936489135027}, {"id": 128, "seek": 91278, "start": 912.78, "end": 917.9, "text": " the action, then you have the messages API, the asynchronous API. So you can listen to", "tokens": [50364, 264, 3069, 11, 550, 291, 362, 264, 7897, 9362, 11, 264, 49174, 9362, 13, 407, 291, 393, 2140, 281, 50620], "temperature": 0.0, "avg_logprob": -0.1763831088417455, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.006218971684575081}, {"id": 129, "seek": 91278, "start": 917.9, "end": 921.5799999999999, "text": " the event, okay, there is a new picture, and this picture needs to be summarized. I'm going", "tokens": [50620, 264, 2280, 11, 1392, 11, 456, 307, 257, 777, 3036, 11, 293, 341, 3036, 2203, 281, 312, 14611, 1602, 13, 286, 478, 516, 50804], "temperature": 0.0, "avg_logprob": -0.1763831088417455, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.006218971684575081}, {"id": 130, "seek": 91278, "start": 921.5799999999999, "end": 928.5799999999999, "text": " to summarize the picture, and that's updated. Okay. So these are the two different patterns", "tokens": [50804, 281, 20858, 264, 3036, 11, 293, 300, 311, 10588, 13, 1033, 13, 407, 613, 366, 264, 732, 819, 8294, 51154], "temperature": 0.0, "avg_logprob": -0.1763831088417455, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.006218971684575081}, {"id": 131, "seek": 91278, "start": 928.5799999999999, "end": 934.78, "text": " we can, we can apply for it. What we are going to see now, again, live, everything is running", "tokens": [51154, 321, 393, 11, 321, 393, 3079, 337, 309, 13, 708, 321, 366, 516, 281, 536, 586, 11, 797, 11, 1621, 11, 1203, 307, 2614, 51464], "temperature": 0.0, "avg_logprob": -0.1763831088417455, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.006218971684575081}, {"id": 132, "seek": 93478, "start": 934.78, "end": 941.78, "text": " on my laptop, just believe me, is something that allows us to classify a document. So", "tokens": [50364, 322, 452, 10732, 11, 445, 1697, 385, 11, 307, 746, 300, 4045, 505, 281, 33872, 257, 4166, 13, 407, 50714], "temperature": 0.0, "avg_logprob": -0.19122987382867362, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.6208280920982361}, {"id": 133, "seek": 93478, "start": 943.42, "end": 950.1, "text": " we are going to upload a document. We are creating this rule. The rule is the same just", "tokens": [50796, 321, 366, 516, 281, 6580, 257, 4166, 13, 492, 366, 4084, 341, 4978, 13, 440, 4978, 307, 264, 912, 445, 51130], "temperature": 0.0, "avg_logprob": -0.19122987382867362, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.6208280920982361}, {"id": 134, "seek": 93478, "start": 950.1, "end": 957.1, "text": " for you to make the similarity with what is before. So we have a list of languages, Japanese,", "tokens": [51130, 337, 291, 281, 652, 264, 32194, 365, 437, 307, 949, 13, 407, 321, 362, 257, 1329, 295, 8650, 11, 5433, 11, 51480], "temperature": 0.0, "avg_logprob": -0.19122987382867362, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.6208280920982361}, {"id": 135, "seek": 93478, "start": 957.8199999999999, "end": 963.66, "text": " Vietnamese, English, whatever. And we are creating a rule to move the document to the right", "tokens": [51516, 25934, 11, 3669, 11, 2035, 13, 400, 321, 366, 4084, 257, 4978, 281, 1286, 264, 4166, 281, 264, 558, 51808], "temperature": 0.0, "avg_logprob": -0.19122987382867362, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.6208280920982361}, {"id": 136, "seek": 96366, "start": 963.6999999999999, "end": 970.6999999999999, "text": " folder. So you draw a path document, and the document is moved to the right folder. Okay.", "tokens": [50366, 10820, 13, 407, 291, 2642, 257, 3100, 4166, 11, 293, 264, 4166, 307, 4259, 281, 264, 558, 10820, 13, 1033, 13, 50716], "temperature": 0.0, "avg_logprob": -0.23422982142521784, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.008199122734367847}, {"id": 137, "seek": 96366, "start": 970.6999999999999, "end": 977.6999999999999, "text": " Okay. So let's do that. Okay. Let's open a fresco. So there is a folder at some point.", "tokens": [50716, 1033, 13, 407, 718, 311, 360, 300, 13, 1033, 13, 961, 311, 1269, 257, 25235, 1291, 13, 407, 456, 307, 257, 10820, 412, 512, 935, 13, 51066], "temperature": 0.0, "avg_logprob": -0.23422982142521784, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.008199122734367847}, {"id": 138, "seek": 99366, "start": 994.66, "end": 1001.66, "text": " And this folder has a rule that is classifying the documents that I'm dropping on it. Okay.", "tokens": [50414, 400, 341, 10820, 575, 257, 4978, 300, 307, 1508, 5489, 264, 8512, 300, 286, 478, 13601, 322, 309, 13, 1033, 13, 50764], "temperature": 0.0, "avg_logprob": -0.22998428344726562, "compression_ratio": 1.5197740112994351, "no_speech_prob": 0.01507454365491867}, {"id": 139, "seek": 99366, "start": 1004.9, "end": 1011.9, "text": " So if I, for instance, come for classify, no, for classify things, we are going to try", "tokens": [50926, 407, 498, 286, 11, 337, 5197, 11, 808, 337, 33872, 11, 572, 11, 337, 33872, 721, 11, 321, 366, 516, 281, 853, 51276], "temperature": 0.0, "avg_logprob": -0.22998428344726562, "compression_ratio": 1.5197740112994351, "no_speech_prob": 0.01507454365491867}, {"id": 140, "seek": 99366, "start": 1013.78, "end": 1020.78, "text": " with a Vietnamese one. It has to be a bit creative. Okay. Okay. So at this point, a fresco", "tokens": [51370, 365, 257, 25934, 472, 13, 467, 575, 281, 312, 257, 857, 5880, 13, 1033, 13, 1033, 13, 407, 412, 341, 935, 11, 257, 25235, 1291, 51720], "temperature": 0.0, "avg_logprob": -0.22998428344726562, "compression_ratio": 1.5197740112994351, "no_speech_prob": 0.01507454365491867}, {"id": 141, "seek": 102078, "start": 1021.1, "end": 1028.1, "text": " is listening to this new document, and it's classifying the document. So it's just selecting", "tokens": [50380, 307, 4764, 281, 341, 777, 4166, 11, 293, 309, 311, 1508, 5489, 264, 4166, 13, 407, 309, 311, 445, 18182, 50730], "temperature": 0.0, "avg_logprob": -0.17051962731589734, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.004842707421630621}, {"id": 142, "seek": 102078, "start": 1030.3799999999999, "end": 1037.3799999999999, "text": " a term from the list of terms, and the document has been updated. So it has been classified.", "tokens": [50844, 257, 1433, 490, 264, 1329, 295, 2115, 11, 293, 264, 4166, 575, 668, 10588, 13, 407, 309, 575, 668, 20627, 13, 51194], "temperature": 0.0, "avg_logprob": -0.17051962731589734, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.004842707421630621}, {"id": 143, "seek": 102078, "start": 1037.82, "end": 1044.82, "text": " So if I refresh, what I find is that the document is on the Vietnamese folder, and you can do", "tokens": [51216, 407, 498, 286, 15134, 11, 437, 286, 915, 307, 300, 264, 4166, 307, 322, 264, 25934, 10820, 11, 293, 291, 393, 360, 51566], "temperature": 0.0, "avg_logprob": -0.17051962731589734, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.004842707421630621}, {"id": 144, "seek": 104482, "start": 1045.82, "end": 1052.82, "text": " that with invoices, with whatever you want. And we can track that it was mistral, the LLM,", "tokens": [50414, 300, 365, 1048, 78, 1473, 11, 365, 2035, 291, 528, 13, 400, 321, 393, 2837, 300, 309, 390, 3544, 2155, 11, 264, 441, 43, 44, 11, 50764], "temperature": 0.0, "avg_logprob": -0.23553861154092326, "compression_ratio": 1.4946236559139785, "no_speech_prob": 0.011384180746972561}, {"id": 145, "seek": 104482, "start": 1054.34, "end": 1061.34, "text": " that created this classification. Okay. Pretty easy, right? So you can integrate also all", "tokens": [50840, 300, 2942, 341, 21538, 13, 1033, 13, 10693, 1858, 11, 558, 30, 407, 291, 393, 13365, 611, 439, 51190], "temperature": 0.0, "avg_logprob": -0.23553861154092326, "compression_ratio": 1.4946236559139785, "no_speech_prob": 0.011384180746972561}, {"id": 146, "seek": 104482, "start": 1061.4199999999998, "end": 1068.4199999999998, "text": " the other operations in that to get some automation. Okay. So I guess that I was running out. But", "tokens": [51194, 264, 661, 7705, 294, 300, 281, 483, 512, 17769, 13, 1033, 13, 407, 286, 2041, 300, 286, 390, 2614, 484, 13, 583, 51544], "temperature": 0.0, "avg_logprob": -0.23553861154092326, "compression_ratio": 1.4946236559139785, "no_speech_prob": 0.011384180746972561}, {"id": 147, "seek": 106842, "start": 1068.42, "end": 1075.42, "text": " no problem. So we have more time for questions. So again, this is a simple framework. You", "tokens": [50364, 572, 1154, 13, 407, 321, 362, 544, 565, 337, 1651, 13, 407, 797, 11, 341, 307, 257, 2199, 8388, 13, 509, 50714], "temperature": 0.0, "avg_logprob": -0.21229785421620245, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.01570260524749756}, {"id": 148, "seek": 106842, "start": 1077.18, "end": 1084.18, "text": " can deploy that on premise. You can choose your LLM. You have an initial REST API for", "tokens": [50802, 393, 7274, 300, 322, 22045, 13, 509, 393, 2826, 428, 441, 43, 44, 13, 509, 362, 364, 5883, 497, 14497, 9362, 337, 51152], "temperature": 0.0, "avg_logprob": -0.21229785421620245, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.01570260524749756}, {"id": 149, "seek": 106842, "start": 1085.7, "end": 1092.7, "text": " operations. Public works are welcome. And then you need to integrate that with your product,", "tokens": [51228, 7705, 13, 9489, 1985, 366, 2928, 13, 400, 550, 291, 643, 281, 13365, 300, 365, 428, 1674, 11, 51578], "temperature": 0.0, "avg_logprob": -0.21229785421620245, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.01570260524749756}, {"id": 150, "seek": 109270, "start": 1092.7, "end": 1099.7, "text": " with your organization, or whatever. Right. There is also some interesting hackathon with", "tokens": [50364, 365, 428, 4475, 11, 420, 2035, 13, 1779, 13, 821, 307, 611, 512, 1880, 10339, 18660, 365, 50714], "temperature": 0.0, "avg_logprob": -0.24761862706656407, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.01948249153792858}, {"id": 151, "seek": 109270, "start": 1099.82, "end": 1106.82, "text": " more use cases. So I presented you some use cases, but you have more of them on this hackathon.", "tokens": [50720, 544, 764, 3331, 13, 407, 286, 8212, 291, 512, 764, 3331, 11, 457, 291, 362, 544, 295, 552, 322, 341, 10339, 18660, 13, 51070], "temperature": 0.0, "avg_logprob": -0.24761862706656407, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.01948249153792858}, {"id": 152, "seek": 109270, "start": 1106.82, "end": 1113.82, "text": " The slides will be, they are available on the, on Foxen. Okay. And also I'm using Olamma,", "tokens": [51070, 440, 9788, 486, 312, 11, 436, 366, 2435, 322, 264, 11, 322, 11388, 268, 13, 1033, 13, 400, 611, 286, 478, 1228, 422, 4326, 1696, 11, 51420], "temperature": 0.0, "avg_logprob": -0.24761862706656407, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.01948249153792858}, {"id": 153, "seek": 109270, "start": 1114.5, "end": 1118.6200000000001, "text": " but there are many other alternatives. You don't need to choose Olamma. So you have GPT4", "tokens": [51454, 457, 456, 366, 867, 661, 20478, 13, 509, 500, 380, 643, 281, 2826, 422, 4326, 1696, 13, 407, 291, 362, 26039, 51, 19, 51660], "temperature": 0.0, "avg_logprob": -0.24761862706656407, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.01948249153792858}, {"id": 154, "seek": 111862, "start": 1118.62, "end": 1125.62, "text": " all locally. This solution is the one used by, by next cloud, second state, high-end", "tokens": [50364, 439, 16143, 13, 639, 3827, 307, 264, 472, 1143, 538, 11, 538, 958, 4588, 11, 1150, 1785, 11, 1090, 12, 521, 50714], "temperature": 0.0, "avg_logprob": -0.411446898324149, "compression_ratio": 1.4277456647398843, "no_speech_prob": 0.027216261252760887}, {"id": 155, "seek": 111862, "start": 1127.6999999999998, "end": 1134.6999999999998, "text": " phase is the most known probably. But again, just, this is an initial framework. Take it", "tokens": [50818, 5574, 307, 264, 881, 2570, 1391, 13, 583, 797, 11, 445, 11, 341, 307, 364, 5883, 8388, 13, 3664, 309, 51168], "temperature": 0.0, "avg_logprob": -0.411446898324149, "compression_ratio": 1.4277456647398843, "no_speech_prob": 0.027216261252760887}, {"id": 156, "seek": 111862, "start": 1134.7399999999998, "end": 1141.7399999999998, "text": " as it and try some things with, with the NAA. Okay. That was all. Thanks.", "tokens": [51170, 382, 309, 293, 853, 512, 721, 365, 11, 365, 264, 426, 5265, 13, 1033, 13, 663, 390, 439, 13, 2561, 13, 51520], "temperature": 0.0, "avg_logprob": -0.411446898324149, "compression_ratio": 1.4277456647398843, "no_speech_prob": 0.027216261252760887}, {"id": 157, "seek": 114174, "start": 1141.74, "end": 1148.74, "text": " Thank you very much, Angel. Are there any questions?", "tokens": [50364, 1044, 291, 588, 709, 11, 14902, 13, 2014, 456, 604, 1651, 30, 50714], "temperature": 0.0, "avg_logprob": -0.30970659795797095, "compression_ratio": 1.375, "no_speech_prob": 0.06027882918715477}, {"id": 158, "seek": 114174, "start": 1156.34, "end": 1159.34, "text": " I'm going to do it in the order of the rule.", "tokens": [51094, 286, 478, 516, 281, 360, 309, 294, 264, 1668, 295, 264, 4978, 13, 51244], "temperature": 0.0, "avg_logprob": -0.30970659795797095, "compression_ratio": 1.375, "no_speech_prob": 0.06027882918715477}, {"id": 159, "seek": 114174, "start": 1159.34, "end": 1166.14, "text": " Thank you, Angel. It seems to me all these operations are on one picture or one document.", "tokens": [51244, 1044, 291, 11, 14902, 13, 467, 2544, 281, 385, 439, 613, 7705, 366, 322, 472, 3036, 420, 472, 4166, 13, 51584], "temperature": 0.0, "avg_logprob": -0.30970659795797095, "compression_ratio": 1.375, "no_speech_prob": 0.06027882918715477}, {"id": 160, "seek": 116614, "start": 1166.14, "end": 1171.14, "text": " Are you also considering me asking a question on all my documents?", "tokens": [50364, 2014, 291, 611, 8079, 385, 3365, 257, 1168, 322, 439, 452, 8512, 30, 50614], "temperature": 0.0, "avg_logprob": -0.1640699540061512, "compression_ratio": 1.678391959798995, "no_speech_prob": 0.10858698189258575}, {"id": 161, "seek": 116614, "start": 1171.14, "end": 1177.8200000000002, "text": " No. So this, this sample is only for a single document or a single picture. But, but that", "tokens": [50614, 883, 13, 407, 341, 11, 341, 6889, 307, 787, 337, 257, 2167, 4166, 420, 257, 2167, 3036, 13, 583, 11, 457, 300, 50948], "temperature": 0.0, "avg_logprob": -0.1640699540061512, "compression_ratio": 1.678391959798995, "no_speech_prob": 0.10858698189258575}, {"id": 162, "seek": 116614, "start": 1177.8200000000002, "end": 1184.8200000000002, "text": " is as easy as you have the database, the Neo4j database, then you can include as information", "tokens": [50948, 307, 382, 1858, 382, 291, 362, 264, 8149, 11, 264, 24458, 19, 73, 8149, 11, 550, 291, 393, 4090, 382, 1589, 51298], "temperature": 0.0, "avg_logprob": -0.1640699540061512, "compression_ratio": 1.678391959798995, "no_speech_prob": 0.10858698189258575}, {"id": 163, "seek": 116614, "start": 1186.14, "end": 1191.94, "text": " as you want for a single document or for a single query. Right. So what I'm doing in", "tokens": [51364, 382, 291, 528, 337, 257, 2167, 4166, 420, 337, 257, 2167, 14581, 13, 1779, 13, 407, 437, 286, 478, 884, 294, 51654], "temperature": 0.0, "avg_logprob": -0.1640699540061512, "compression_ratio": 1.678391959798995, "no_speech_prob": 0.10858698189258575}, {"id": 164, "seek": 119194, "start": 1191.94, "end": 1198.94, "text": " the source code is to remove the previous information. You have to create something that is only for", "tokens": [50364, 264, 4009, 3089, 307, 281, 4159, 264, 3894, 1589, 13, 509, 362, 281, 1884, 746, 300, 307, 787, 337, 50714], "temperature": 0.0, "avg_logprob": -0.22511979598033277, "compression_ratio": 1.6267942583732058, "no_speech_prob": 0.019539423286914825}, {"id": 165, "seek": 119194, "start": 1200.02, "end": 1206.3, "text": " a single document. But you can modify that in order to add more than one document to", "tokens": [50768, 257, 2167, 4166, 13, 583, 291, 393, 16927, 300, 294, 1668, 281, 909, 544, 813, 472, 4166, 281, 51082], "temperature": 0.0, "avg_logprob": -0.22511979598033277, "compression_ratio": 1.6267942583732058, "no_speech_prob": 0.019539423286914825}, {"id": 166, "seek": 119194, "start": 1206.3, "end": 1213.3, "text": " one query. But on the sample is only for a document or a picture.", "tokens": [51082, 472, 14581, 13, 583, 322, 264, 6889, 307, 787, 337, 257, 4166, 420, 257, 3036, 13, 51432], "temperature": 0.0, "avg_logprob": -0.22511979598033277, "compression_ratio": 1.6267942583732058, "no_speech_prob": 0.019539423286914825}, {"id": 167, "seek": 119194, "start": 1214.6200000000001, "end": 1221.6200000000001, "text": " While summarizing the Japanese PDF, why did you need to provide for context your picture", "tokens": [51498, 3987, 14611, 3319, 264, 5433, 17752, 11, 983, 630, 291, 643, 281, 2893, 337, 4319, 428, 3036, 51848], "temperature": 0.0, "avg_logprob": -0.22511979598033277, "compression_ratio": 1.6267942583732058, "no_speech_prob": 0.019539423286914825}, {"id": 168, "seek": 122194, "start": 1222.3, "end": 1222.94, "text": " Sorry.", "tokens": [50382, 4919, 13, 50414], "temperature": 0.0, "avg_logprob": -0.331096066926655, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.01663898304104805}, {"id": 169, "seek": 122194, "start": 1222.94, "end": 1228.94, "text": " You showed the summarization of the Japanese PDF.", "tokens": [50414, 509, 4712, 264, 14611, 2144, 295, 264, 5433, 17752, 13, 50714], "temperature": 0.0, "avg_logprob": -0.331096066926655, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.01663898304104805}, {"id": 170, "seek": 122194, "start": 1228.94, "end": 1229.94, "text": " Yeah.", "tokens": [50714, 865, 13, 50764], "temperature": 0.0, "avg_logprob": -0.331096066926655, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.01663898304104805}, {"id": 171, "seek": 122194, "start": 1229.94, "end": 1232.42, "text": " And then you provided for context the picture.", "tokens": [50764, 400, 550, 291, 5649, 337, 4319, 264, 3036, 13, 50888], "temperature": 0.0, "avg_logprob": -0.331096066926655, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.01663898304104805}, {"id": 172, "seek": 122194, "start": 1232.42, "end": 1238.06, "text": " No, no, the picture was for the last operation. So the three first operations for summarize,", "tokens": [50888, 883, 11, 572, 11, 264, 3036, 390, 337, 264, 1036, 6916, 13, 407, 264, 1045, 700, 7705, 337, 20858, 11, 51170], "temperature": 0.0, "avg_logprob": -0.331096066926655, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.01663898304104805}, {"id": 173, "seek": 122194, "start": 1238.06, "end": 1243.5, "text": " for classify and for prompting were related with the document in Japanese. I could use", "tokens": [51170, 337, 33872, 293, 337, 12391, 278, 645, 4077, 365, 264, 4166, 294, 5433, 13, 286, 727, 764, 51442], "temperature": 0.0, "avg_logprob": -0.331096066926655, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.01663898304104805}, {"id": 174, "seek": 122194, "start": 1243.5, "end": 1247.22, "text": " some other document. I know, but I love the document because I'm using this for testing", "tokens": [51442, 512, 661, 4166, 13, 286, 458, 11, 457, 286, 959, 264, 4166, 570, 286, 478, 1228, 341, 337, 4997, 51628], "temperature": 0.0, "avg_logprob": -0.331096066926655, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.01663898304104805}, {"id": 175, "seek": 124722, "start": 1247.22, "end": 1253.74, "text": " for 15 years, something like that. So it's like my, my precious document. And, and the", "tokens": [50364, 337, 2119, 924, 11, 746, 411, 300, 13, 407, 309, 311, 411, 452, 11, 452, 12406, 4166, 13, 400, 11, 293, 264, 50690], "temperature": 0.0, "avg_logprob": -0.3531441136410362, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.01666310615837574}, {"id": 176, "seek": 124722, "start": 1253.74, "end": 1258.58, "text": " picture was there for the last one. It was the description of that picture that is more", "tokens": [50690, 3036, 390, 456, 337, 264, 1036, 472, 13, 467, 390, 264, 3855, 295, 300, 3036, 300, 307, 544, 50932], "temperature": 0.0, "avg_logprob": -0.3531441136410362, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.01666310615837574}, {"id": 177, "seek": 124722, "start": 1258.58, "end": 1262.42, "text": " or less like, like yours then.", "tokens": [50932, 420, 1570, 411, 11, 411, 6342, 550, 13, 51124], "temperature": 0.0, "avg_logprob": -0.3531441136410362, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.01666310615837574}, {"id": 178, "seek": 124722, "start": 1262.42, "end": 1266.6200000000001, "text": " Thank you.", "tokens": [51124, 1044, 291, 13, 51334], "temperature": 0.0, "avg_logprob": -0.3531441136410362, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.01666310615837574}, {"id": 179, "seek": 124722, "start": 1266.6200000000001, "end": 1272.02, "text": " Similar to the previous question that I had, but for a single document, right. So the summarization", "tokens": [51334, 10905, 281, 264, 3894, 1168, 300, 286, 632, 11, 457, 337, 257, 2167, 4166, 11, 558, 13, 407, 264, 14611, 2144, 51604], "temperature": 0.0, "avg_logprob": -0.3531441136410362, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.01666310615837574}, {"id": 180, "seek": 124722, "start": 1272.02, "end": 1273.38, "text": " for very large documents.", "tokens": [51604, 337, 588, 2416, 8512, 13, 51672], "temperature": 0.0, "avg_logprob": -0.3531441136410362, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.01666310615837574}, {"id": 181, "seek": 124722, "start": 1273.38, "end": 1274.38, "text": " Yeah.", "tokens": [51672, 865, 13, 51722], "temperature": 0.0, "avg_logprob": -0.3531441136410362, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.01666310615837574}, {"id": 182, "seek": 127438, "start": 1274.38, "end": 1281.38, "text": " So, the problem is that again, I'm running on my lap. So I cannot use like a very large", "tokens": [50364, 407, 11, 264, 1154, 307, 300, 797, 11, 286, 478, 2614, 322, 452, 13214, 13, 407, 286, 2644, 764, 411, 257, 588, 2416, 50714], "temperature": 0.0, "avg_logprob": -0.28261906338721204, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0842614620923996}, {"id": 183, "seek": 127438, "start": 1284.66, "end": 1289.1000000000001, "text": " document, but I was just trying to summarize, for instance, books. Do you know the Gutenberg", "tokens": [50878, 4166, 11, 457, 286, 390, 445, 1382, 281, 20858, 11, 337, 5197, 11, 3642, 13, 1144, 291, 458, 264, 42833, 6873, 51100], "temperature": 0.0, "avg_logprob": -0.28261906338721204, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0842614620923996}, {"id": 184, "seek": 127438, "start": 1289.1000000000001, "end": 1293.9, "text": " project? On the Gutenberg project, you have all the classics of Alice in Wonderland and", "tokens": [51100, 1716, 30, 1282, 264, 42833, 6873, 1716, 11, 291, 362, 439, 264, 36110, 295, 16004, 294, 13224, 1661, 293, 51340], "temperature": 0.0, "avg_logprob": -0.28261906338721204, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0842614620923996}, {"id": 185, "seek": 127438, "start": 1293.9, "end": 1300.0600000000002, "text": " so on. So I was trying to do that with that kind of documents. And it's able to do that,", "tokens": [51340, 370, 322, 13, 407, 286, 390, 1382, 281, 360, 300, 365, 300, 733, 295, 8512, 13, 400, 309, 311, 1075, 281, 360, 300, 11, 51648], "temperature": 0.0, "avg_logprob": -0.28261906338721204, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0842614620923996}, {"id": 186, "seek": 130006, "start": 1300.1799999999998, "end": 1307.1799999999998, "text": " it takes a while, like minutes on my machine. Again, if instant adjusin, the regular CPU,", "tokens": [50370, 309, 2516, 257, 1339, 11, 411, 2077, 322, 452, 3479, 13, 3764, 11, 498, 9836, 614, 73, 301, 259, 11, 264, 3890, 13199, 11, 50720], "temperature": 0.0, "avg_logprob": -0.2979450424512227, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.03270682319998741}, {"id": 187, "seek": 130006, "start": 1307.4199999999998, "end": 1314.4199999999998, "text": " you use a GPU, the tiny slide, I don't know, 100 faster, something like that. So I don't", "tokens": [50732, 291, 764, 257, 18407, 11, 264, 5870, 4137, 11, 286, 500, 380, 458, 11, 2319, 4663, 11, 746, 411, 300, 13, 407, 286, 500, 380, 51082], "temperature": 0.0, "avg_logprob": -0.2979450424512227, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.03270682319998741}, {"id": 188, "seek": 130006, "start": 1315.02, "end": 1320.3, "text": " know. I need to make serious test with that. But having the right infrastructure, I guess", "tokens": [51112, 458, 13, 286, 643, 281, 652, 3156, 1500, 365, 300, 13, 583, 1419, 264, 558, 6896, 11, 286, 2041, 51376], "temperature": 0.0, "avg_logprob": -0.2979450424512227, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.03270682319998741}, {"id": 189, "seek": 130006, "start": 1320.3, "end": 1327.3, "text": " that the, the performance is enough. It's not something like very instantaneous, right?", "tokens": [51376, 300, 264, 11, 264, 3389, 307, 1547, 13, 467, 311, 406, 746, 411, 588, 45596, 11, 558, 30, 51726], "temperature": 0.0, "avg_logprob": -0.2979450424512227, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.03270682319998741}, {"id": 190, "seek": 132730, "start": 1328.18, "end": 1331.62, "text": " But you can work with it.", "tokens": [50408, 583, 291, 393, 589, 365, 309, 13, 50580], "temperature": 0.0, "avg_logprob": -0.2973645873691725, "compression_ratio": 1.6748768472906403, "no_speech_prob": 0.004916646052151918}, {"id": 191, "seek": 132730, "start": 1331.62, "end": 1336.62, "text": " Thank you very much. Any other questions?", "tokens": [50580, 1044, 291, 588, 709, 13, 2639, 661, 1651, 30, 50830], "temperature": 0.0, "avg_logprob": -0.2973645873691725, "compression_ratio": 1.6748768472906403, "no_speech_prob": 0.004916646052151918}, {"id": 192, "seek": 132730, "start": 1336.62, "end": 1337.62, "text": " Yes.", "tokens": [50830, 1079, 13, 50880], "temperature": 0.0, "avg_logprob": -0.2973645873691725, "compression_ratio": 1.6748768472906403, "no_speech_prob": 0.004916646052151918}, {"id": 193, "seek": 132730, "start": 1337.62, "end": 1344.62, "text": " Hi. A follow up on the previous question. Was the insertion into the vector is database", "tokens": [50880, 2421, 13, 316, 1524, 493, 322, 264, 3894, 1168, 13, 3027, 264, 8969, 313, 666, 264, 8062, 307, 8149, 51230], "temperature": 0.0, "avg_logprob": -0.2973645873691725, "compression_ratio": 1.6748768472906403, "no_speech_prob": 0.004916646052151918}, {"id": 194, "seek": 132730, "start": 1344.62, "end": 1350.18, "text": " taking a lot of time or was the actual query to the LLM? Because the insertion into the", "tokens": [51230, 1940, 257, 688, 295, 565, 420, 390, 264, 3539, 14581, 281, 264, 441, 43, 44, 30, 1436, 264, 8969, 313, 666, 264, 51508], "temperature": 0.0, "avg_logprob": -0.2973645873691725, "compression_ratio": 1.6748768472906403, "no_speech_prob": 0.004916646052151918}, {"id": 195, "seek": 132730, "start": 1350.18, "end": 1357.18, "text": " vector is database has to be done once, whereas the query can be done multiple times if, if", "tokens": [51508, 8062, 307, 8149, 575, 281, 312, 1096, 1564, 11, 9735, 264, 14581, 393, 312, 1096, 3866, 1413, 498, 11, 498, 51858], "temperature": 0.0, "avg_logprob": -0.2973645873691725, "compression_ratio": 1.6748768472906403, "no_speech_prob": 0.004916646052151918}, {"id": 196, "seek": 135730, "start": 1357.3799999999999, "end": 1359.4199999999998, "text": " you already vectorize the document, right?", "tokens": [50368, 291, 1217, 8062, 1125, 264, 4166, 11, 558, 30, 50470], "temperature": 0.0, "avg_logprob": -0.2262692004442215, "compression_ratio": 1.7357142857142858, "no_speech_prob": 0.0074476138688623905}, {"id": 197, "seek": 135730, "start": 1359.4199999999998, "end": 1364.8999999999999, "text": " Yeah. So again, I was not trying to deliver a session on how to develop AI, right? It", "tokens": [50470, 865, 13, 407, 797, 11, 286, 390, 406, 1382, 281, 4239, 257, 5481, 322, 577, 281, 1499, 7318, 11, 558, 30, 467, 50744], "temperature": 0.0, "avg_logprob": -0.2262692004442215, "compression_ratio": 1.7357142857142858, "no_speech_prob": 0.0074476138688623905}, {"id": 198, "seek": 135730, "start": 1364.8999999999999, "end": 1370.86, "text": " was just to create a framework. You have the AI track that can reply to you better than", "tokens": [50744, 390, 445, 281, 1884, 257, 8388, 13, 509, 362, 264, 7318, 2837, 300, 393, 16972, 281, 291, 1101, 813, 51042], "temperature": 0.0, "avg_logprob": -0.2262692004442215, "compression_ratio": 1.7357142857142858, "no_speech_prob": 0.0074476138688623905}, {"id": 199, "seek": 135730, "start": 1370.86, "end": 1376.06, "text": " me in relation to that. But yeah, obviously, you can use the database. I'm not, I'm only", "tokens": [51042, 385, 294, 9721, 281, 300, 13, 583, 1338, 11, 2745, 11, 291, 393, 764, 264, 8149, 13, 286, 478, 406, 11, 286, 478, 787, 51302], "temperature": 0.0, "avg_logprob": -0.2262692004442215, "compression_ratio": 1.7357142857142858, "no_speech_prob": 0.0074476138688623905}, {"id": 200, "seek": 135730, "start": 1376.06, "end": 1380.46, "text": " using the database for a context of a single document, right? So you can create categories,", "tokens": [51302, 1228, 264, 8149, 337, 257, 4319, 295, 257, 2167, 4166, 11, 558, 30, 407, 291, 393, 1884, 10479, 11, 51522], "temperature": 0.0, "avg_logprob": -0.2262692004442215, "compression_ratio": 1.7357142857142858, "no_speech_prob": 0.0074476138688623905}, {"id": 201, "seek": 135730, "start": 1380.46, "end": 1386.62, "text": " you can add more than one document. You can add also the, the links to the response and,", "tokens": [51522, 291, 393, 909, 544, 813, 472, 4166, 13, 509, 393, 909, 611, 264, 11, 264, 6123, 281, 264, 4134, 293, 11, 51830], "temperature": 0.0, "avg_logprob": -0.2262692004442215, "compression_ratio": 1.7357142857142858, "no_speech_prob": 0.0074476138688623905}, {"id": 202, "seek": 138662, "start": 1386.62, "end": 1393.4599999999998, "text": " and so on. So yeah, sorry. Maybe I didn't understand you.", "tokens": [50364, 293, 370, 322, 13, 407, 1338, 11, 2597, 13, 2704, 286, 994, 380, 1223, 291, 13, 50706], "temperature": 0.0, "avg_logprob": -0.2863272589606208, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.01340236235409975}, {"id": 203, "seek": 138662, "start": 1393.4599999999998, "end": 1397.34, "text": " Maybe you misunderstood my question. My question was when you added the Alice in Wonderland", "tokens": [50706, 2704, 291, 33870, 452, 1168, 13, 1222, 1168, 390, 562, 291, 3869, 264, 16004, 294, 13224, 1661, 50900], "temperature": 0.0, "avg_logprob": -0.2863272589606208, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.01340236235409975}, {"id": 204, "seek": 138662, "start": 1397.34, "end": 1403.1399999999999, "text": " book, was it the vectorization that took time or was it the query to the LLM?", "tokens": [50900, 1446, 11, 390, 309, 264, 8062, 2144, 300, 1890, 565, 420, 390, 309, 264, 14581, 281, 264, 441, 43, 44, 30, 51190], "temperature": 0.0, "avg_logprob": -0.2863272589606208, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.01340236235409975}, {"id": 205, "seek": 138662, "start": 1403.1399999999999, "end": 1407.26, "text": " No, no, it was vectorization, vectorization of the chance of the document.", "tokens": [51190, 883, 11, 572, 11, 309, 390, 8062, 2144, 11, 8062, 2144, 295, 264, 2931, 295, 264, 4166, 13, 51396], "temperature": 0.0, "avg_logprob": -0.2863272589606208, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.01340236235409975}, {"id": 206, "seek": 138662, "start": 1407.26, "end": 1408.26, "text": " Okay. Sorry.", "tokens": [51396, 1033, 13, 4919, 13, 51446], "temperature": 0.0, "avg_logprob": -0.2863272589606208, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.01340236235409975}, {"id": 207, "seek": 138662, "start": 1408.26, "end": 1410.5, "text": " That was the only one question.", "tokens": [51446, 663, 390, 264, 787, 472, 1168, 13, 51558], "temperature": 0.0, "avg_logprob": -0.2863272589606208, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.01340236235409975}, {"id": 208, "seek": 138662, "start": 1410.5, "end": 1416.4599999999998, "text": " I'm not an expert, but I know a bit.", "tokens": [51558, 286, 478, 406, 364, 5844, 11, 457, 286, 458, 257, 857, 13, 51856], "temperature": 0.0, "avg_logprob": -0.2863272589606208, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.01340236235409975}, {"id": 209, "seek": 141646, "start": 1416.46, "end": 1422.7, "text": " Any other question? Okay.", "tokens": [50364, 2639, 661, 1168, 30, 1033, 13, 50676], "temperature": 0.0, "avg_logprob": -0.3845320322427405, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.10180982947349548}, {"id": 210, "seek": 141646, "start": 1422.7, "end": 1423.7, "text": " Thanks. Okay.", "tokens": [50676, 2561, 13, 1033, 13, 50726], "temperature": 0.0, "avg_logprob": -0.3845320322427405, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.10180982947349548}, {"id": 211, "seek": 141646, "start": 1423.7, "end": 1424.7, "text": " One more question.", "tokens": [50726, 1485, 544, 1168, 13, 50776], "temperature": 0.0, "avg_logprob": -0.3845320322427405, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.10180982947349548}, {"id": 212, "seek": 141646, "start": 1424.7, "end": 1425.7, "text": " Last one.", "tokens": [50776, 5264, 472, 13, 50826], "temperature": 0.0, "avg_logprob": -0.3845320322427405, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.10180982947349548}, {"id": 213, "seek": 141646, "start": 1425.7, "end": 1431.1000000000001, "text": " I'll be around. So if someone just wants to, to catch me.", "tokens": [50826, 286, 603, 312, 926, 13, 407, 498, 1580, 445, 2738, 281, 11, 281, 3745, 385, 13, 51096], "temperature": 0.0, "avg_logprob": -0.3845320322427405, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.10180982947349548}, {"id": 214, "seek": 141646, "start": 1431.1000000000001, "end": 1436.42, "text": " Can you say a bit more about like the biggest use cases you see and if there's any open source", "tokens": [51096, 1664, 291, 584, 257, 857, 544, 466, 411, 264, 3880, 764, 3331, 291, 536, 293, 498, 456, 311, 604, 1269, 4009, 51362], "temperature": 0.0, "avg_logprob": -0.3845320322427405, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.10180982947349548}, {"id": 215, "seek": 141646, "start": 1436.42, "end": 1441.22, "text": " setups of this that are out there for us to look at?", "tokens": [51362, 46832, 295, 341, 300, 366, 484, 456, 337, 505, 281, 574, 412, 30, 51602], "temperature": 0.0, "avg_logprob": -0.3845320322427405, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.10180982947349548}, {"id": 216, "seek": 144122, "start": 1441.22, "end": 1446.98, "text": " In my opinion, the, the main use case of that is searching.", "tokens": [50364, 682, 452, 4800, 11, 264, 11, 264, 2135, 764, 1389, 295, 300, 307, 10808, 13, 50652], "temperature": 0.0, "avg_logprob": -0.2638506491978963, "compression_ratio": 1.6994818652849741, "no_speech_prob": 0.08043301105499268}, {"id": 217, "seek": 144122, "start": 1446.98, "end": 1450.42, "text": " So but this is a different world with different beasts.", "tokens": [50652, 407, 457, 341, 307, 257, 819, 1002, 365, 819, 37386, 13, 50824], "temperature": 0.0, "avg_logprob": -0.2638506491978963, "compression_ratio": 1.6994818652849741, "no_speech_prob": 0.08043301105499268}, {"id": 218, "seek": 144122, "start": 1450.42, "end": 1456.22, "text": " So but for searching AI, it's really quite relevant.", "tokens": [50824, 407, 457, 337, 10808, 7318, 11, 309, 311, 534, 1596, 7340, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2638506491978963, "compression_ratio": 1.6994818652849741, "no_speech_prob": 0.08043301105499268}, {"id": 219, "seek": 144122, "start": 1456.22, "end": 1465.82, "text": " So again, this is just to create a framework and then it's just to apply your imagination.", "tokens": [51114, 407, 797, 11, 341, 307, 445, 281, 1884, 257, 8388, 293, 550, 309, 311, 445, 281, 3079, 428, 12938, 13, 51594], "temperature": 0.0, "avg_logprob": -0.2638506491978963, "compression_ratio": 1.6994818652849741, "no_speech_prob": 0.08043301105499268}, {"id": 220, "seek": 144122, "start": 1465.82, "end": 1466.82, "text": " Thank you very much, Angel.", "tokens": [51594, 1044, 291, 588, 709, 11, 14902, 13, 51644], "temperature": 0.0, "avg_logprob": -0.2638506491978963, "compression_ratio": 1.6994818652849741, "no_speech_prob": 0.08043301105499268}, {"id": 221, "seek": 144122, "start": 1466.82, "end": 1467.82, "text": " Thanks.", "tokens": [51644, 2561, 13, 51694], "temperature": 0.0, "avg_logprob": -0.2638506491978963, "compression_ratio": 1.6994818652849741, "no_speech_prob": 0.08043301105499268}, {"id": 222, "seek": 144122, "start": 1467.82, "end": 1468.82, "text": " Thank you.", "tokens": [51694, 1044, 291, 13, 51744], "temperature": 0.0, "avg_logprob": -0.2638506491978963, "compression_ratio": 1.6994818652849741, "no_speech_prob": 0.08043301105499268}, {"id": 223, "seek": 144122, "start": 1468.82, "end": 1469.82, "text": " Thank you.", "tokens": [51744, 1044, 291, 13, 51794], "temperature": 0.0, "avg_logprob": -0.2638506491978963, "compression_ratio": 1.6994818652849741, "no_speech_prob": 0.08043301105499268}, {"id": 224, "seek": 144122, "start": 1469.82, "end": 1470.82, "text": " Thank you.", "tokens": [51794, 1044, 291, 13, 51844], "temperature": 0.0, "avg_logprob": -0.2638506491978963, "compression_ratio": 1.6994818652849741, "no_speech_prob": 0.08043301105499268}, {"id": 225, "seek": 147082, "start": 1470.82, "end": 1471.82, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 226, "seek": 147082, "start": 1471.82, "end": 1472.82, "text": " Thank you.", "tokens": [50414, 1044, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 227, "seek": 147082, "start": 1472.82, "end": 1473.82, "text": " Thank you.", "tokens": [50464, 1044, 291, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 228, "seek": 147082, "start": 1473.82, "end": 1474.82, "text": " Thank you.", "tokens": [50514, 1044, 291, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 229, "seek": 147082, "start": 1474.82, "end": 1475.82, "text": " Thank you.", "tokens": [50564, 1044, 291, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 230, "seek": 147082, "start": 1475.82, "end": 1476.82, "text": " Thank you.", "tokens": [50614, 1044, 291, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 231, "seek": 147082, "start": 1476.82, "end": 1477.82, "text": " Thank you.", "tokens": [50664, 1044, 291, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 232, "seek": 147082, "start": 1477.82, "end": 1478.82, "text": " Thank you.", "tokens": [50714, 1044, 291, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 233, "seek": 147082, "start": 1478.82, "end": 1479.82, "text": " Thank you.", "tokens": [50764, 1044, 291, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 234, "seek": 147082, "start": 1479.82, "end": 1480.82, "text": " Thank you.", "tokens": [50814, 1044, 291, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 235, "seek": 147082, "start": 1480.82, "end": 1481.82, "text": " Thank you.", "tokens": [50864, 1044, 291, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 236, "seek": 147082, "start": 1481.82, "end": 1482.82, "text": " Thank you.", "tokens": [50914, 1044, 291, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 237, "seek": 147082, "start": 1482.82, "end": 1483.82, "text": " Thank you.", "tokens": [50964, 1044, 291, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 238, "seek": 147082, "start": 1483.82, "end": 1484.82, "text": " Thank you.", "tokens": [51014, 1044, 291, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 239, "seek": 147082, "start": 1484.82, "end": 1485.82, "text": " Thank you.", "tokens": [51064, 1044, 291, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 240, "seek": 147082, "start": 1485.82, "end": 1486.82, "text": " Thank you.", "tokens": [51114, 1044, 291, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 241, "seek": 147082, "start": 1486.82, "end": 1487.82, "text": " Thank you.", "tokens": [51164, 1044, 291, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 242, "seek": 147082, "start": 1487.82, "end": 1488.82, "text": " Thank you.", "tokens": [51214, 1044, 291, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 243, "seek": 147082, "start": 1488.82, "end": 1489.82, "text": " Thank you.", "tokens": [51264, 1044, 291, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 244, "seek": 147082, "start": 1489.82, "end": 1490.82, "text": " Thank you.", "tokens": [51314, 1044, 291, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 245, "seek": 147082, "start": 1490.82, "end": 1491.82, "text": " Thank you.", "tokens": [51364, 1044, 291, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 246, "seek": 147082, "start": 1491.82, "end": 1492.82, "text": " Thank you.", "tokens": [51414, 1044, 291, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 247, "seek": 147082, "start": 1492.82, "end": 1493.82, "text": " Thank you.", "tokens": [51464, 1044, 291, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 248, "seek": 147082, "start": 1493.82, "end": 1494.82, "text": " Thank you.", "tokens": [51514, 1044, 291, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 249, "seek": 147082, "start": 1494.82, "end": 1495.82, "text": " Thank you.", "tokens": [51564, 1044, 291, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 250, "seek": 147082, "start": 1495.82, "end": 1496.82, "text": " Thank you.", "tokens": [51614, 1044, 291, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 251, "seek": 147082, "start": 1496.82, "end": 1497.82, "text": " Thank you.", "tokens": [51664, 1044, 291, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 252, "seek": 147082, "start": 1497.82, "end": 1498.82, "text": " Thank you.", "tokens": [51714, 1044, 291, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}, {"id": 253, "seek": 147082, "start": 1498.82, "end": 1499.82, "text": " Thank you.", "tokens": [51764, 1044, 291, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1018906223530672, "compression_ratio": 13.25, "no_speech_prob": 0.9912434816360474}], "language": "en"}