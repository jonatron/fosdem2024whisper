{"text": " Thank you. So I want to evolve my application. I'm going to use Gleam and Erlang. The organization asked for slides, but this is a luster application. So I can't give you the slides. My name is Kiro van Gelder. I'm a freelancer from my own company. I've been using software for up to 30 years since I don't know how many platforms, languages, environments. And I happen to like to beam Erlang, Gleam, etc. So that's why I picked it. That's not only why I picked it. So recently I was at Langdev, model-driven stuff, all kinds of things that people showed us. One talk that spiked my interest was about, okay, well, we have a game. We have a description of a game. And while we're running the game, there's people interacting with the game. Well, there's things we don't quite like, so we're going to change the rules. And then we have to keep, the game has to keep running. So, yeah, well, okay, that looked awesome. So I thought I can build something like that, but I'm going to do it on Beam. Beam has these superpowers, all the reloading and things are, it seems really suited for it, so why not? And then the other thing I thought, well, if I want to do that with a game, and I have to build some kind of infrastructure and things, let's start with a simple game. So I picked out Holland's Ganserbord. You have a little goose spawn, and you have to reach the very middle spot. And if you look at it from the model and the rules, there's actually quite a few of interesting exceptions for all the kinds of places that you can run around on. And land on a goose, you have to move twice as far. You can skip it during, if you're in prison, well, you can only get released when someone else releases you. It's kind of special. So what will I talk about? Tiny introduction on model-driven development. I'm going to tell you why I do the modeling game. I'm going to explain a little bit about dynamic reloading as the Beam provides it for those who might not know it. I'm going to tell you why the game itself, the instance will be running in core Erlang, and that will be demos all along. Model-driven development. You want to have some model. It needs to have a very precise description, and at some point, me as an editor, sometimes in a computer editor, will make that description. And from that description, we generate an instance that is running what we described. Why would you want to do that? Instances sometimes are, well, they're compiled targets, and they know the things that they have to do, but they don't know things about themselves. A model is a description that does know it. A very nice example that was given on that Langdef was about Dutch income tax. This is described in laws. Computers do not interpret laws, but they went through the effort to take the law, adjust it a little bit so they did it together with lawyers to make sure you got something that a computer could interpret. So laws might have ambiguities or vagueness things in there. Computers don't. Well, then they had these more precise versions of the law, and they, from that model, they generated Java code. Dutch income tax is running on Java code at the moment. Additional things you can do once you have it in the model is you can start reasoning about it. So by now, I understand that even if they want to introduce a new law, they can, you can just plug it in the system and see is there any contradiction here. If so, please adjust this law proposal. Often you'll have a domain specific language like the adjusted law. For my conserboard, you could have stuff like this. Often people will want to edit these DSLs by hand, but I'm very much interested in small deltas. If I have a running game and I make a big modification in here, I don't have a game. So I need to make sure I do a small step, a small delta. So I have to, with what I do, I have to restrict the possible edits that can be done, which I call deltas. How does that look? Building a system called Eagle. It's running on a server. That is, I have a browser. That's my client. From the client, I do the editing of the model. From there, I generate my instance, my game, and then from the same or a different browser, I view the game and I play the game. Why would I want to do a modeling GLEAM? The model should be as precise as possible as I just explained. GLEAM gives us types. Type safety is more than non-type safety. So my name is GLEAM. Other benefit, a superpower of GLEAM, you can compile it both to Erlang and to JavaScript. So I can, if I have my model and I can somehow transmit it, I can just use the exact same code on both sides of my client server and I know that it's the same thing. It saves me work. What does it look like? I have my model here. So in my LusterUp, I now have an iframe. This iframe is the model client that I showed in the previous picture. It's an iframe, so it's a web page, but as you might guess, that web page again is a LusterUp. And I want to grow the Hanzer board from as minimal a thing as possible in small deltas. So let's see where we can start. My model is very simple. It's a bit too simple here, so I need a bit more of a model. I can make a list of an int. I could even make a list of a list of an int and it's represented here. And I not only want to describe that I have some type, it also needs to have some default value. In addition to that, we have a cell. My Hanzer board isn't quite finished yet. Sorry. I have a plain cell starting to finish and I might even use the goose. And right now what I also have then is a game type. It has a board which has a list of cells and it has pawns which are a list of numbers indexing where the pawns are. It's a choice. There are other choices. And the default of my Hanzer board then has one cell at the moment which is plain, which is a bit ridiculous. So let's add a few more cells and say we're going to start at some of these things. And of course the default that I gave to the int turned out to be a bit awkward because I wanted it to be zero. So I mess a bit with my model. In very small steps, I modify my description to get hopefully to a better place. Dynamic reload. Erlang and beam provides us the tools to do these timing upload. This is a loop. Usually there's a process running on the beam that is executing this loop. It had been started by another process. And generally it receives messages, handles them, sends back some results. That's what the from exclamation mark is. It sends back some result. Could be errors. Could be part of the new state. And then it loops. And another possible message could be well just stop and then you don't loop. All there is to it. What Erlang and beam also provide is a way to reload, sorry, to load new code into your, into the beam for a specific module. That's the code module in the Erlang kernel package. You call it binary here in the middle. Target is the target module. The thing in square brackets is the file name that it would be coming from. We don't care. And the object code is compiled core Erlang code. At that point when you do that, you have your old version of the code in the beam and you have a new version of the code in the beam. But the existing processes, my game, my Hansbord is still running the old stuff. So now I'm going to send a message upgrade to my Hansbord process. And roughly like that this is the same loop as before but now I have the relevant upgrade part. Instead of just looping, which would loop in my old code, I have to say format this module for my target module, my Hansbord module, explicitly specify the module with the loop and then it's guaranteed to use the new version of the code. And then I can happily play along my game with upgraded code. Now why do we do that in Erlang? The difference between the local loop and the exported loop is something that Gleam does not know. So I can't do it. Why did I pick core Erlang? Because we can do this all in memory. No need to use file systems and other things. There is an Erlang C-E-R-L library that can generate these things. I wrapped it for Gleam. That's called Gencore Erlang and you can find it on the hex. So let's start a game. I already made the type properly. So I'm ready to create a game. If I connect to the server it won't do anything here, but if I create an instance it's now and it will connect to the game. So as you can see, it picked up the start plain plain from the definitions that I had at the left. It also picked up a move button. The rules, there are some implicit rules that I did not edit things about. I did not tell anything about. I need to be able to do something in my game. So there's a hard code that moves. I will just move the pawn one forward. And there's also a check for the win condition. So where the pawn is, it has to check on the board whether that's a finished location. And that rule is going on continuously. I have not made nice deltas and things for that in the UI and when at some point I will. But these things are running in the background in the instance. Now at least I'll show you the move. If I move, my pawn moves one forward. So yay, getting closer to the finish in my GUNS board. All right. So a little bit more about the bits and pieces that are happening. There's the Asian communication from the browser to the server for the model. Whenever there is a delta that's made, we just recreate the entire instance module in Core Erlang, reloaded and upgraded. And the instance just keeps talking to it. It doesn't even notice it from the client and also talks in terms of chasing with it. State and rules. The initial state is something that should be adopted, adjusted and rules are... So yeah, that's right. The rule is something that is in the model. I talk about a conceptual thing there. My instance doesn't know the rule. It just has code. And my client doesn't know what the rule is either. It just knows whether it can do something or cannot do something. So what is important that I wanted to make as two kind of changes. Some changes that when I do that, the instance will in the end see it. So I'm going to change the board. And that when I change the board it's going to compile the change into the Core Erlang, reload it, do some small migration and then also pass that information to our client. So, here we have it again. If I turn this into a Goose for instance, it becomes a Goose. So that was one recompilation of Erlang in the back button. Now I make another plane. I can add another one if I want to. And at some point I'm going to have to reach the finish here. So let's make that. So yeah, that was three, four recompilations of things in memory and moving on. Involving. But another one thing that I might want to change, because I can also create multiple instances, is my starting state. And that would mean that the only thing that happens if I change that from my client, it changes my model, but nothing else, unless I don't start a new instance, nothing happens with it. And that looks like this. So I have a game one and a game two. Game two hasn't been started yet. And if I change this one from zero to one, now I start at position one. Then we notice that in game one nothing happened. But if I start a new instance, then this one will now start at position one instead of position zero. And just to show that even though it shows two, it didn't change to one, but didn't show it. If I move, it will go to three. Well, and where did I put the finish today? On start, zero, one, three, on number four. I'll just move to four. There. I finished my game. I won. So things I want to do in the future. I'm very much interested in what kind of deltas are usable, sensible. You saw me adding cells to a board. You saw me change the type of the board. Okay. What if I remove a cell from the board? Yeah. What if the pawn is on there? It quickly becomes like, you could think of a couple of solutions for when you remove the cell from the board. You move the pawn to the previous or the next cell, or you remove it, or you put it on cell zero. But why would, why, how do you pick one? Depends on your application. So I really want to look more into that. Another thing is that I don't think the Hanselboard UI that I had looks very nice. It would be much better if it looked like the second slide that I showed. But if I do that, then the client really knows about Hanselboard. But what if I want to make a slightly different game? So, okay. And what if my Hanselboard knows about most of the things I do, but I add that labyrinth thing? Now I want to render a nice labyrinth. Okay. Can I make something that knows its Hanselboard, but can also adjust to changes that I make in the model that expands on what was already there that they didn't know about in the start? And obviously, it needs to be multiplayer because playing in my own is, I want you all to play with me. All right. The code that you saw in the iframes is in the top link. While reasoning about this, I also wrote a little, a start of a Gleam library that generates Gleam code, which is the second link, the one that generates Core Erlang. It's the third link, my own web page, the fourth. And if you want to know what the Dutch income text is looking like, it's all in there. Thank you. We have time for a couple of questions. Anyone? Any? Okay. Thank you. That was really quite amazing technology there. I was wondering, do you have thoughts on like when you might decide to apply these sorts of techniques to a problem? When it would be a good question. Yeah. Okay. Question is when this kind of solution would apply to a problem. It's, I might not be the best person to answer this because I'm somewhat new to model driven. It helps when you, when that description is going to give you something, whether that is checking that something is coherent or correct. When, yeah, the model should give you something. When not to do it, well, if you just want to play Hansenboard, just make a Hansenboard server and a Hansenboard client because it's much faster. Much quicker to build. So it's, it's, it is an investment. It's quite an investment. It's not just, not 10% extra. It's a factor much, possibly 5, 10 extra to make sure that you can really do that kind of stuff. Other question? It was the same question. Because it's fun. Okay. Yeah, that was really, really cool. Really interesting. When you changed like the initial state, you showed like the running client didn't update, right? Like it didn't, it didn't update the client because all these updates are triggered through messages. Is it possible that you could replay message history from the beginning with a running client? So like if I updated the initial state of a running process, could I then replay all of the messages it's received to change propagates? There's, there's, there's two answers to that, I guess. Yeah. Is it possible to replay all the events that happened both to the model and the instance or just to the instance game? Just to the instance. Just to the instance. At the moment, no. Would you? Would be, would be interesting. One way in which at least the part of the answer would be yes is if you look at the model when I, when I say please change this in this way with this delta, the server will respond by just giving you back, yep, I applied this delta, now you do too. Okay, cool. Any other questions? Okay, thank you then. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.72, "text": " Thank you. So I want to evolve my application. I'm going to use Gleam and Erlang. The organization", "tokens": [50364, 1044, 291, 13, 407, 286, 528, 281, 16693, 452, 3861, 13, 286, 478, 516, 281, 764, 460, 306, 335, 293, 3300, 25241, 13, 440, 4475, 51000], "temperature": 0.0, "avg_logprob": -0.26111047479170785, "compression_ratio": 1.4427083333333333, "no_speech_prob": 0.47420185804367065}, {"id": 1, "seek": 0, "start": 12.72, "end": 19.64, "text": " asked for slides, but this is a luster application. So I can't give you the slides. My name is", "tokens": [51000, 2351, 337, 9788, 11, 457, 341, 307, 257, 287, 8393, 3861, 13, 407, 286, 393, 380, 976, 291, 264, 9788, 13, 1222, 1315, 307, 51346], "temperature": 0.0, "avg_logprob": -0.26111047479170785, "compression_ratio": 1.4427083333333333, "no_speech_prob": 0.47420185804367065}, {"id": 2, "seek": 0, "start": 19.64, "end": 25.400000000000002, "text": " Kiro van Gelder. I'm a freelancer from my own company. I've been using software for", "tokens": [51346, 591, 5182, 3161, 16142, 1068, 13, 286, 478, 257, 27931, 28347, 490, 452, 1065, 2237, 13, 286, 600, 668, 1228, 4722, 337, 51634], "temperature": 0.0, "avg_logprob": -0.26111047479170785, "compression_ratio": 1.4427083333333333, "no_speech_prob": 0.47420185804367065}, {"id": 3, "seek": 2540, "start": 25.4, "end": 31.96, "text": " up to 30 years since I don't know how many platforms, languages, environments. And I", "tokens": [50364, 493, 281, 2217, 924, 1670, 286, 500, 380, 458, 577, 867, 9473, 11, 8650, 11, 12388, 13, 400, 286, 50692], "temperature": 0.0, "avg_logprob": -0.1831756341652792, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.3194763660430908}, {"id": 4, "seek": 2540, "start": 31.96, "end": 36.839999999999996, "text": " happen to like to beam Erlang, Gleam, etc. So that's why I picked it. That's not only", "tokens": [50692, 1051, 281, 411, 281, 14269, 3300, 25241, 11, 460, 306, 335, 11, 5183, 13, 407, 300, 311, 983, 286, 6183, 309, 13, 663, 311, 406, 787, 50936], "temperature": 0.0, "avg_logprob": -0.1831756341652792, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.3194763660430908}, {"id": 5, "seek": 2540, "start": 36.839999999999996, "end": 44.239999999999995, "text": " why I picked it. So recently I was at Langdev, model-driven stuff, all kinds of things that", "tokens": [50936, 983, 286, 6183, 309, 13, 407, 3938, 286, 390, 412, 13313, 40343, 11, 2316, 12, 25456, 1507, 11, 439, 3685, 295, 721, 300, 51306], "temperature": 0.0, "avg_logprob": -0.1831756341652792, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.3194763660430908}, {"id": 6, "seek": 2540, "start": 44.239999999999995, "end": 50.0, "text": " people showed us. One talk that spiked my interest was about, okay, well, we have a", "tokens": [51306, 561, 4712, 505, 13, 1485, 751, 300, 637, 44070, 452, 1179, 390, 466, 11, 1392, 11, 731, 11, 321, 362, 257, 51594], "temperature": 0.0, "avg_logprob": -0.1831756341652792, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.3194763660430908}, {"id": 7, "seek": 2540, "start": 50.0, "end": 53.76, "text": " game. We have a description of a game. And while we're running the game, there's people", "tokens": [51594, 1216, 13, 492, 362, 257, 3855, 295, 257, 1216, 13, 400, 1339, 321, 434, 2614, 264, 1216, 11, 456, 311, 561, 51782], "temperature": 0.0, "avg_logprob": -0.1831756341652792, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.3194763660430908}, {"id": 8, "seek": 5376, "start": 53.76, "end": 57.72, "text": " interacting with the game. Well, there's things we don't quite like, so we're going", "tokens": [50364, 18017, 365, 264, 1216, 13, 1042, 11, 456, 311, 721, 321, 500, 380, 1596, 411, 11, 370, 321, 434, 516, 50562], "temperature": 0.0, "avg_logprob": -0.1735229019291145, "compression_ratio": 1.7244094488188977, "no_speech_prob": 0.10009578615427017}, {"id": 9, "seek": 5376, "start": 57.72, "end": 63.4, "text": " to change the rules. And then we have to keep, the game has to keep running. So, yeah, well,", "tokens": [50562, 281, 1319, 264, 4474, 13, 400, 550, 321, 362, 281, 1066, 11, 264, 1216, 575, 281, 1066, 2614, 13, 407, 11, 1338, 11, 731, 11, 50846], "temperature": 0.0, "avg_logprob": -0.1735229019291145, "compression_ratio": 1.7244094488188977, "no_speech_prob": 0.10009578615427017}, {"id": 10, "seek": 5376, "start": 63.4, "end": 67.96, "text": " okay, that looked awesome. So I thought I can build something like that, but I'm going", "tokens": [50846, 1392, 11, 300, 2956, 3476, 13, 407, 286, 1194, 286, 393, 1322, 746, 411, 300, 11, 457, 286, 478, 516, 51074], "temperature": 0.0, "avg_logprob": -0.1735229019291145, "compression_ratio": 1.7244094488188977, "no_speech_prob": 0.10009578615427017}, {"id": 11, "seek": 5376, "start": 67.96, "end": 73.24, "text": " to do it on Beam. Beam has these superpowers, all the reloading and things are, it seems", "tokens": [51074, 281, 360, 309, 322, 40916, 13, 40916, 575, 613, 1687, 47953, 11, 439, 264, 25628, 278, 293, 721, 366, 11, 309, 2544, 51338], "temperature": 0.0, "avg_logprob": -0.1735229019291145, "compression_ratio": 1.7244094488188977, "no_speech_prob": 0.10009578615427017}, {"id": 12, "seek": 5376, "start": 73.24, "end": 77.64, "text": " really suited for it, so why not? And then the other thing I thought, well, if I want", "tokens": [51338, 534, 24736, 337, 309, 11, 370, 983, 406, 30, 400, 550, 264, 661, 551, 286, 1194, 11, 731, 11, 498, 286, 528, 51558], "temperature": 0.0, "avg_logprob": -0.1735229019291145, "compression_ratio": 1.7244094488188977, "no_speech_prob": 0.10009578615427017}, {"id": 13, "seek": 7764, "start": 77.64, "end": 83.76, "text": " to do that with a game, and I have to build some kind of infrastructure and things, let's", "tokens": [50364, 281, 360, 300, 365, 257, 1216, 11, 293, 286, 362, 281, 1322, 512, 733, 295, 6896, 293, 721, 11, 718, 311, 50670], "temperature": 0.0, "avg_logprob": -0.20426787500796112, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.5738871097564697}, {"id": 14, "seek": 7764, "start": 83.76, "end": 90.0, "text": " start with a simple game. So I picked out Holland's Ganserbord. You have a little goose", "tokens": [50670, 722, 365, 257, 2199, 1216, 13, 407, 286, 6183, 484, 27201, 311, 460, 599, 260, 65, 765, 13, 509, 362, 257, 707, 24717, 50982], "temperature": 0.0, "avg_logprob": -0.20426787500796112, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.5738871097564697}, {"id": 15, "seek": 7764, "start": 90.0, "end": 97.0, "text": " spawn, and you have to reach the very middle spot. And if you look at it from the model", "tokens": [50982, 17088, 11, 293, 291, 362, 281, 2524, 264, 588, 2808, 4008, 13, 400, 498, 291, 574, 412, 309, 490, 264, 2316, 51332], "temperature": 0.0, "avg_logprob": -0.20426787500796112, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.5738871097564697}, {"id": 16, "seek": 7764, "start": 97.0, "end": 105.4, "text": " and the rules, there's actually quite a few of interesting exceptions for all the kinds", "tokens": [51332, 293, 264, 4474, 11, 456, 311, 767, 1596, 257, 1326, 295, 1880, 22847, 337, 439, 264, 3685, 51752], "temperature": 0.0, "avg_logprob": -0.20426787500796112, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.5738871097564697}, {"id": 17, "seek": 10540, "start": 105.4, "end": 110.96000000000001, "text": " of places that you can run around on. And land on a goose, you have to move twice as", "tokens": [50364, 295, 3190, 300, 291, 393, 1190, 926, 322, 13, 400, 2117, 322, 257, 24717, 11, 291, 362, 281, 1286, 6091, 382, 50642], "temperature": 0.0, "avg_logprob": -0.19195894224453816, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.049554068595170975}, {"id": 18, "seek": 10540, "start": 110.96000000000001, "end": 116.4, "text": " far. You can skip it during, if you're in prison, well, you can only get released when", "tokens": [50642, 1400, 13, 509, 393, 10023, 309, 1830, 11, 498, 291, 434, 294, 6168, 11, 731, 11, 291, 393, 787, 483, 4736, 562, 50914], "temperature": 0.0, "avg_logprob": -0.19195894224453816, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.049554068595170975}, {"id": 19, "seek": 10540, "start": 116.4, "end": 124.88000000000001, "text": " someone else releases you. It's kind of special. So what will I talk about? Tiny introduction", "tokens": [50914, 1580, 1646, 16952, 291, 13, 467, 311, 733, 295, 2121, 13, 407, 437, 486, 286, 751, 466, 30, 39992, 9339, 51338], "temperature": 0.0, "avg_logprob": -0.19195894224453816, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.049554068595170975}, {"id": 20, "seek": 10540, "start": 124.88000000000001, "end": 130.44, "text": " on model-driven development. I'm going to tell you why I do the modeling game. I'm", "tokens": [51338, 322, 2316, 12, 25456, 3250, 13, 286, 478, 516, 281, 980, 291, 983, 286, 360, 264, 15983, 1216, 13, 286, 478, 51616], "temperature": 0.0, "avg_logprob": -0.19195894224453816, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.049554068595170975}, {"id": 21, "seek": 10540, "start": 130.44, "end": 133.84, "text": " going to explain a little bit about dynamic reloading as the Beam provides it for those", "tokens": [51616, 516, 281, 2903, 257, 707, 857, 466, 8546, 25628, 278, 382, 264, 40916, 6417, 309, 337, 729, 51786], "temperature": 0.0, "avg_logprob": -0.19195894224453816, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.049554068595170975}, {"id": 22, "seek": 13384, "start": 133.88, "end": 138.68, "text": " who might not know it. I'm going to tell you why the game itself, the instance will be", "tokens": [50366, 567, 1062, 406, 458, 309, 13, 286, 478, 516, 281, 980, 291, 983, 264, 1216, 2564, 11, 264, 5197, 486, 312, 50606], "temperature": 0.0, "avg_logprob": -0.16133610407511392, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.07994253933429718}, {"id": 23, "seek": 13384, "start": 138.68, "end": 145.68, "text": " running in core Erlang, and that will be demos all along. Model-driven development. You want", "tokens": [50606, 2614, 294, 4965, 3300, 25241, 11, 293, 300, 486, 312, 33788, 439, 2051, 13, 17105, 12, 25456, 3250, 13, 509, 528, 50956], "temperature": 0.0, "avg_logprob": -0.16133610407511392, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.07994253933429718}, {"id": 24, "seek": 13384, "start": 145.68, "end": 150.12, "text": " to have some model. It needs to have a very precise description, and at some point, me", "tokens": [50956, 281, 362, 512, 2316, 13, 467, 2203, 281, 362, 257, 588, 13600, 3855, 11, 293, 412, 512, 935, 11, 385, 51178], "temperature": 0.0, "avg_logprob": -0.16133610407511392, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.07994253933429718}, {"id": 25, "seek": 13384, "start": 150.12, "end": 156.0, "text": " as an editor, sometimes in a computer editor, will make that description. And from that", "tokens": [51178, 382, 364, 9839, 11, 2171, 294, 257, 3820, 9839, 11, 486, 652, 300, 3855, 13, 400, 490, 300, 51472], "temperature": 0.0, "avg_logprob": -0.16133610407511392, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.07994253933429718}, {"id": 26, "seek": 13384, "start": 156.0, "end": 162.24, "text": " description, we generate an instance that is running what we described. Why would you", "tokens": [51472, 3855, 11, 321, 8460, 364, 5197, 300, 307, 2614, 437, 321, 7619, 13, 1545, 576, 291, 51784], "temperature": 0.0, "avg_logprob": -0.16133610407511392, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.07994253933429718}, {"id": 27, "seek": 16224, "start": 162.28, "end": 172.28, "text": " want to do that? Instances sometimes are, well, they're compiled targets, and they know the", "tokens": [50366, 528, 281, 360, 300, 30, 2730, 2676, 2171, 366, 11, 731, 11, 436, 434, 36548, 12911, 11, 293, 436, 458, 264, 50866], "temperature": 0.0, "avg_logprob": -0.18257209692108498, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.021584805101156235}, {"id": 28, "seek": 16224, "start": 172.28, "end": 175.84, "text": " things that they have to do, but they don't know things about themselves. A model is a", "tokens": [50866, 721, 300, 436, 362, 281, 360, 11, 457, 436, 500, 380, 458, 721, 466, 2969, 13, 316, 2316, 307, 257, 51044], "temperature": 0.0, "avg_logprob": -0.18257209692108498, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.021584805101156235}, {"id": 29, "seek": 16224, "start": 175.84, "end": 181.64000000000001, "text": " description that does know it. A very nice example that was given on that Langdef was", "tokens": [51044, 3855, 300, 775, 458, 309, 13, 316, 588, 1481, 1365, 300, 390, 2212, 322, 300, 13313, 1479, 69, 390, 51334], "temperature": 0.0, "avg_logprob": -0.18257209692108498, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.021584805101156235}, {"id": 30, "seek": 16224, "start": 181.64000000000001, "end": 190.24, "text": " about Dutch income tax. This is described in laws. Computers do not interpret laws, but", "tokens": [51334, 466, 15719, 5742, 3366, 13, 639, 307, 7619, 294, 6064, 13, 37804, 433, 360, 406, 7302, 6064, 11, 457, 51764], "temperature": 0.0, "avg_logprob": -0.18257209692108498, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.021584805101156235}, {"id": 31, "seek": 19024, "start": 190.36, "end": 201.20000000000002, "text": " they went through the effort to take the law, adjust it a little bit so they did it together", "tokens": [50370, 436, 1437, 807, 264, 4630, 281, 747, 264, 2101, 11, 4369, 309, 257, 707, 857, 370, 436, 630, 309, 1214, 50912], "temperature": 0.0, "avg_logprob": -0.2308308330934439, "compression_ratio": 1.5082872928176796, "no_speech_prob": 0.013146823272109032}, {"id": 32, "seek": 19024, "start": 201.20000000000002, "end": 209.20000000000002, "text": " with lawyers to make sure you got something that a computer could interpret. So laws might", "tokens": [50912, 365, 16219, 281, 652, 988, 291, 658, 746, 300, 257, 3820, 727, 7302, 13, 407, 6064, 1062, 51312], "temperature": 0.0, "avg_logprob": -0.2308308330934439, "compression_ratio": 1.5082872928176796, "no_speech_prob": 0.013146823272109032}, {"id": 33, "seek": 19024, "start": 209.4, "end": 215.52, "text": " have ambiguities or vagueness things in there. Computers don't. Well, then they had these", "tokens": [51322, 362, 40390, 1088, 420, 13501, 7801, 442, 721, 294, 456, 13, 37804, 433, 500, 380, 13, 1042, 11, 550, 436, 632, 613, 51628], "temperature": 0.0, "avg_logprob": -0.2308308330934439, "compression_ratio": 1.5082872928176796, "no_speech_prob": 0.013146823272109032}, {"id": 34, "seek": 21552, "start": 215.52, "end": 222.52, "text": " more precise versions of the law, and they, from that model, they generated Java code.", "tokens": [50364, 544, 13600, 9606, 295, 264, 2101, 11, 293, 436, 11, 490, 300, 2316, 11, 436, 10833, 10745, 3089, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16261367444638852, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.03320777788758278}, {"id": 35, "seek": 21552, "start": 222.64000000000001, "end": 228.64000000000001, "text": " Dutch income tax is running on Java code at the moment. Additional things you can do", "tokens": [50720, 15719, 5742, 3366, 307, 2614, 322, 10745, 3089, 412, 264, 1623, 13, 44272, 721, 291, 393, 360, 51020], "temperature": 0.0, "avg_logprob": -0.16261367444638852, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.03320777788758278}, {"id": 36, "seek": 21552, "start": 228.64000000000001, "end": 234.4, "text": " once you have it in the model is you can start reasoning about it. So by now, I understand", "tokens": [51020, 1564, 291, 362, 309, 294, 264, 2316, 307, 291, 393, 722, 21577, 466, 309, 13, 407, 538, 586, 11, 286, 1223, 51308], "temperature": 0.0, "avg_logprob": -0.16261367444638852, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.03320777788758278}, {"id": 37, "seek": 21552, "start": 234.4, "end": 238.12, "text": " that even if they want to introduce a new law, they can, you can just plug it in the", "tokens": [51308, 300, 754, 498, 436, 528, 281, 5366, 257, 777, 2101, 11, 436, 393, 11, 291, 393, 445, 5452, 309, 294, 264, 51494], "temperature": 0.0, "avg_logprob": -0.16261367444638852, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.03320777788758278}, {"id": 38, "seek": 21552, "start": 238.12, "end": 244.12, "text": " system and see is there any contradiction here. If so, please adjust this law proposal.", "tokens": [51494, 1185, 293, 536, 307, 456, 604, 34937, 510, 13, 759, 370, 11, 1767, 4369, 341, 2101, 11494, 13, 51794], "temperature": 0.0, "avg_logprob": -0.16261367444638852, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.03320777788758278}, {"id": 39, "seek": 24412, "start": 245.12, "end": 252.12, "text": " Often you'll have a domain specific language like the adjusted law. For my conserboard,", "tokens": [50414, 20043, 291, 603, 362, 257, 9274, 2685, 2856, 411, 264, 19871, 2101, 13, 1171, 452, 1014, 260, 3787, 11, 50764], "temperature": 0.0, "avg_logprob": -0.200464549817537, "compression_ratio": 1.53125, "no_speech_prob": 0.00522506283596158}, {"id": 40, "seek": 24412, "start": 255.32, "end": 262.32, "text": " you could have stuff like this. Often people will want to edit these DSLs by hand, but", "tokens": [50924, 291, 727, 362, 1507, 411, 341, 13, 20043, 561, 486, 528, 281, 8129, 613, 15816, 43, 82, 538, 1011, 11, 457, 51274], "temperature": 0.0, "avg_logprob": -0.200464549817537, "compression_ratio": 1.53125, "no_speech_prob": 0.00522506283596158}, {"id": 41, "seek": 24412, "start": 262.48, "end": 266.92, "text": " I'm very much interested in small deltas. If I have a running game and I make a big", "tokens": [51282, 286, 478, 588, 709, 3102, 294, 1359, 1103, 83, 296, 13, 759, 286, 362, 257, 2614, 1216, 293, 286, 652, 257, 955, 51504], "temperature": 0.0, "avg_logprob": -0.200464549817537, "compression_ratio": 1.53125, "no_speech_prob": 0.00522506283596158}, {"id": 42, "seek": 24412, "start": 266.92, "end": 271.68, "text": " modification in here, I don't have a game. So I need to make sure I do a small step,", "tokens": [51504, 26747, 294, 510, 11, 286, 500, 380, 362, 257, 1216, 13, 407, 286, 643, 281, 652, 988, 286, 360, 257, 1359, 1823, 11, 51742], "temperature": 0.0, "avg_logprob": -0.200464549817537, "compression_ratio": 1.53125, "no_speech_prob": 0.00522506283596158}, {"id": 43, "seek": 27168, "start": 271.72, "end": 278.24, "text": " a small delta. So I have to, with what I do, I have to restrict the possible edits that", "tokens": [50366, 257, 1359, 8289, 13, 407, 286, 362, 281, 11, 365, 437, 286, 360, 11, 286, 362, 281, 7694, 264, 1944, 41752, 300, 50692], "temperature": 0.0, "avg_logprob": -0.1622862575030086, "compression_ratio": 1.6555023923444976, "no_speech_prob": 0.01206760574132204}, {"id": 44, "seek": 27168, "start": 278.24, "end": 285.24, "text": " can be done, which I call deltas. How does that look? Building a system called Eagle.", "tokens": [50692, 393, 312, 1096, 11, 597, 286, 818, 1103, 83, 296, 13, 1012, 775, 300, 574, 30, 18974, 257, 1185, 1219, 27926, 13, 51042], "temperature": 0.0, "avg_logprob": -0.1622862575030086, "compression_ratio": 1.6555023923444976, "no_speech_prob": 0.01206760574132204}, {"id": 45, "seek": 27168, "start": 287.64, "end": 294.64, "text": " It's running on a server. That is, I have a browser. That's my client. From the client,", "tokens": [51162, 467, 311, 2614, 322, 257, 7154, 13, 663, 307, 11, 286, 362, 257, 11185, 13, 663, 311, 452, 6423, 13, 3358, 264, 6423, 11, 51512], "temperature": 0.0, "avg_logprob": -0.1622862575030086, "compression_ratio": 1.6555023923444976, "no_speech_prob": 0.01206760574132204}, {"id": 46, "seek": 27168, "start": 295.96000000000004, "end": 300.56, "text": " I do the editing of the model. From there, I generate my instance, my game, and then", "tokens": [51578, 286, 360, 264, 10000, 295, 264, 2316, 13, 3358, 456, 11, 286, 8460, 452, 5197, 11, 452, 1216, 11, 293, 550, 51808], "temperature": 0.0, "avg_logprob": -0.1622862575030086, "compression_ratio": 1.6555023923444976, "no_speech_prob": 0.01206760574132204}, {"id": 47, "seek": 30056, "start": 300.6, "end": 307.6, "text": " from the same or a different browser, I view the game and I play the game. Why would I", "tokens": [50366, 490, 264, 912, 420, 257, 819, 11185, 11, 286, 1910, 264, 1216, 293, 286, 862, 264, 1216, 13, 1545, 576, 286, 50716], "temperature": 0.0, "avg_logprob": -0.21829341888427733, "compression_ratio": 1.5042372881355932, "no_speech_prob": 0.0019334614044055343}, {"id": 48, "seek": 30056, "start": 311.4, "end": 316.52, "text": " want to do a modeling GLEAM? The model should be as precise as possible as I just explained.", "tokens": [50906, 528, 281, 360, 257, 15983, 460, 2634, 2865, 30, 440, 2316, 820, 312, 382, 13600, 382, 1944, 382, 286, 445, 8825, 13, 51162], "temperature": 0.0, "avg_logprob": -0.21829341888427733, "compression_ratio": 1.5042372881355932, "no_speech_prob": 0.0019334614044055343}, {"id": 49, "seek": 30056, "start": 316.52, "end": 322.92, "text": " GLEAM gives us types. Type safety is more than non-type safety. So my name is GLEAM.", "tokens": [51162, 460, 2634, 2865, 2709, 505, 3467, 13, 15576, 4514, 307, 544, 813, 2107, 12, 20467, 4514, 13, 407, 452, 1315, 307, 460, 2634, 2865, 13, 51482], "temperature": 0.0, "avg_logprob": -0.21829341888427733, "compression_ratio": 1.5042372881355932, "no_speech_prob": 0.0019334614044055343}, {"id": 50, "seek": 30056, "start": 322.92, "end": 329.92, "text": " Other benefit, a superpower of GLEAM, you can compile it both to Erlang and to JavaScript.", "tokens": [51482, 5358, 5121, 11, 257, 45765, 295, 460, 2634, 2865, 11, 291, 393, 31413, 309, 1293, 281, 3300, 25241, 293, 281, 15778, 13, 51832], "temperature": 0.0, "avg_logprob": -0.21829341888427733, "compression_ratio": 1.5042372881355932, "no_speech_prob": 0.0019334614044055343}, {"id": 51, "seek": 32992, "start": 329.92, "end": 335.72, "text": " So I can, if I have my model and I can somehow transmit it, I can just use the exact same", "tokens": [50364, 407, 286, 393, 11, 498, 286, 362, 452, 2316, 293, 286, 393, 6063, 17831, 309, 11, 286, 393, 445, 764, 264, 1900, 912, 50654], "temperature": 0.0, "avg_logprob": -0.16820884931205524, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.0027064108289778233}, {"id": 52, "seek": 32992, "start": 335.72, "end": 341.72, "text": " code on both sides of my client server and I know that it's the same thing. It saves", "tokens": [50654, 3089, 322, 1293, 4881, 295, 452, 6423, 7154, 293, 286, 458, 300, 309, 311, 264, 912, 551, 13, 467, 19155, 50954], "temperature": 0.0, "avg_logprob": -0.16820884931205524, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.0027064108289778233}, {"id": 53, "seek": 32992, "start": 341.72, "end": 348.72, "text": " me work. What does it look like? I have my model here. So in my LusterUp, I now have", "tokens": [50954, 385, 589, 13, 708, 775, 309, 574, 411, 30, 286, 362, 452, 2316, 510, 13, 407, 294, 452, 441, 8393, 22164, 11, 286, 586, 362, 51304], "temperature": 0.0, "avg_logprob": -0.16820884931205524, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.0027064108289778233}, {"id": 54, "seek": 32992, "start": 349.16, "end": 355.8, "text": " an iframe. This iframe is the model client that I showed in the previous picture. It's", "tokens": [51326, 364, 498, 81, 529, 13, 639, 498, 81, 529, 307, 264, 2316, 6423, 300, 286, 4712, 294, 264, 3894, 3036, 13, 467, 311, 51658], "temperature": 0.0, "avg_logprob": -0.16820884931205524, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.0027064108289778233}, {"id": 55, "seek": 35580, "start": 355.8, "end": 362.8, "text": " an iframe, so it's a web page, but as you might guess, that web page again is a LusterUp.", "tokens": [50364, 364, 498, 81, 529, 11, 370, 309, 311, 257, 3670, 3028, 11, 457, 382, 291, 1062, 2041, 11, 300, 3670, 3028, 797, 307, 257, 441, 8393, 22164, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16698695541521824, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.006523603107780218}, {"id": 56, "seek": 35580, "start": 364.04, "end": 369.28000000000003, "text": " And I want to grow the Hanzer board from as minimal a thing as possible in small deltas.", "tokens": [50776, 400, 286, 528, 281, 1852, 264, 389, 3910, 260, 3150, 490, 382, 13206, 257, 551, 382, 1944, 294, 1359, 1103, 83, 296, 13, 51038], "temperature": 0.0, "avg_logprob": -0.16698695541521824, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.006523603107780218}, {"id": 57, "seek": 35580, "start": 369.28000000000003, "end": 374.84000000000003, "text": " So let's see where we can start. My model is very simple. It's a bit too simple here,", "tokens": [51038, 407, 718, 311, 536, 689, 321, 393, 722, 13, 1222, 2316, 307, 588, 2199, 13, 467, 311, 257, 857, 886, 2199, 510, 11, 51316], "temperature": 0.0, "avg_logprob": -0.16698695541521824, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.006523603107780218}, {"id": 58, "seek": 35580, "start": 374.84000000000003, "end": 381.16, "text": " so I need a bit more of a model. I can make a list of an int. I could even make a list", "tokens": [51316, 370, 286, 643, 257, 857, 544, 295, 257, 2316, 13, 286, 393, 652, 257, 1329, 295, 364, 560, 13, 286, 727, 754, 652, 257, 1329, 51632], "temperature": 0.0, "avg_logprob": -0.16698695541521824, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.006523603107780218}, {"id": 59, "seek": 38116, "start": 381.16, "end": 386.56, "text": " of a list of an int and it's represented here. And I not only want to describe that I have", "tokens": [50364, 295, 257, 1329, 295, 364, 560, 293, 309, 311, 10379, 510, 13, 400, 286, 406, 787, 528, 281, 6786, 300, 286, 362, 50634], "temperature": 0.0, "avg_logprob": -0.1856679818064896, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.042312610894441605}, {"id": 60, "seek": 38116, "start": 386.56, "end": 393.56, "text": " some type, it also needs to have some default value. In addition to that, we have a cell.", "tokens": [50634, 512, 2010, 11, 309, 611, 2203, 281, 362, 512, 7576, 2158, 13, 682, 4500, 281, 300, 11, 321, 362, 257, 2815, 13, 50984], "temperature": 0.0, "avg_logprob": -0.1856679818064896, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.042312610894441605}, {"id": 61, "seek": 38116, "start": 395.56, "end": 400.32000000000005, "text": " My Hanzer board isn't quite finished yet. Sorry. I have a plain cell starting to finish", "tokens": [51084, 1222, 389, 3910, 260, 3150, 1943, 380, 1596, 4335, 1939, 13, 4919, 13, 286, 362, 257, 11121, 2815, 2891, 281, 2413, 51322], "temperature": 0.0, "avg_logprob": -0.1856679818064896, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.042312610894441605}, {"id": 62, "seek": 38116, "start": 400.32000000000005, "end": 407.04, "text": " and I might even use the goose. And right now what I also have then is a game type. It", "tokens": [51322, 293, 286, 1062, 754, 764, 264, 24717, 13, 400, 558, 586, 437, 286, 611, 362, 550, 307, 257, 1216, 2010, 13, 467, 51658], "temperature": 0.0, "avg_logprob": -0.1856679818064896, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.042312610894441605}, {"id": 63, "seek": 40704, "start": 407.04, "end": 413.68, "text": " has a board which has a list of cells and it has pawns which are a list of numbers indexing", "tokens": [50364, 575, 257, 3150, 597, 575, 257, 1329, 295, 5438, 293, 309, 575, 30905, 82, 597, 366, 257, 1329, 295, 3547, 8186, 278, 50696], "temperature": 0.0, "avg_logprob": -0.1734967329064194, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00857498124241829}, {"id": 64, "seek": 40704, "start": 413.68, "end": 420.68, "text": " where the pawns are. It's a choice. There are other choices. And the default of my Hanzer", "tokens": [50696, 689, 264, 30905, 82, 366, 13, 467, 311, 257, 3922, 13, 821, 366, 661, 7994, 13, 400, 264, 7576, 295, 452, 389, 3910, 260, 51046], "temperature": 0.0, "avg_logprob": -0.1734967329064194, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00857498124241829}, {"id": 65, "seek": 40704, "start": 422.52000000000004, "end": 426.72, "text": " board then has one cell at the moment which is plain, which is a bit ridiculous. So let's", "tokens": [51138, 3150, 550, 575, 472, 2815, 412, 264, 1623, 597, 307, 11121, 11, 597, 307, 257, 857, 11083, 13, 407, 718, 311, 51348], "temperature": 0.0, "avg_logprob": -0.1734967329064194, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00857498124241829}, {"id": 66, "seek": 40704, "start": 426.72, "end": 433.0, "text": " add a few more cells and say we're going to start at some of these things. And of course", "tokens": [51348, 909, 257, 1326, 544, 5438, 293, 584, 321, 434, 516, 281, 722, 412, 512, 295, 613, 721, 13, 400, 295, 1164, 51662], "temperature": 0.0, "avg_logprob": -0.1734967329064194, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00857498124241829}, {"id": 67, "seek": 43300, "start": 433.0, "end": 436.16, "text": " the default that I gave to the int turned out to be a bit awkward because I wanted it to", "tokens": [50364, 264, 7576, 300, 286, 2729, 281, 264, 560, 3574, 484, 281, 312, 257, 857, 11411, 570, 286, 1415, 309, 281, 50522], "temperature": 0.0, "avg_logprob": -0.20314657423231336, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.004568112548440695}, {"id": 68, "seek": 43300, "start": 436.16, "end": 442.16, "text": " be zero. So I mess a bit with my model. In very small steps, I modify my description", "tokens": [50522, 312, 4018, 13, 407, 286, 2082, 257, 857, 365, 452, 2316, 13, 682, 588, 1359, 4439, 11, 286, 16927, 452, 3855, 50822], "temperature": 0.0, "avg_logprob": -0.20314657423231336, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.004568112548440695}, {"id": 69, "seek": 43300, "start": 442.16, "end": 449.16, "text": " to get hopefully to a better place. Dynamic reload. Erlang and beam provides us the tools", "tokens": [50822, 281, 483, 4696, 281, 257, 1101, 1081, 13, 45440, 25628, 13, 3300, 25241, 293, 14269, 6417, 505, 264, 3873, 51172], "temperature": 0.0, "avg_logprob": -0.20314657423231336, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.004568112548440695}, {"id": 70, "seek": 43300, "start": 453.04, "end": 460.04, "text": " to do these timing upload. This is a loop. Usually there's a process running on the beam", "tokens": [51366, 281, 360, 613, 10822, 6580, 13, 639, 307, 257, 6367, 13, 11419, 456, 311, 257, 1399, 2614, 322, 264, 14269, 51716], "temperature": 0.0, "avg_logprob": -0.20314657423231336, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.004568112548440695}, {"id": 71, "seek": 46004, "start": 460.92, "end": 467.92, "text": " that is executing this loop. It had been started by another process. And generally it receives", "tokens": [50408, 300, 307, 32368, 341, 6367, 13, 467, 632, 668, 1409, 538, 1071, 1399, 13, 400, 5101, 309, 20717, 50758], "temperature": 0.0, "avg_logprob": -0.23928376998024425, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.031988054513931274}, {"id": 72, "seek": 46004, "start": 468.36, "end": 473.36, "text": " messages, handles them, sends back some results. That's what the from exclamation mark is.", "tokens": [50780, 7897, 11, 18722, 552, 11, 14790, 646, 512, 3542, 13, 663, 311, 437, 264, 490, 1624, 43233, 1491, 307, 13, 51030], "temperature": 0.0, "avg_logprob": -0.23928376998024425, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.031988054513931274}, {"id": 73, "seek": 46004, "start": 473.36, "end": 478.84000000000003, "text": " It sends back some result. Could be errors. Could be part of the new state. And then it", "tokens": [51030, 467, 14790, 646, 512, 1874, 13, 7497, 312, 13603, 13, 7497, 312, 644, 295, 264, 777, 1785, 13, 400, 550, 309, 51304], "temperature": 0.0, "avg_logprob": -0.23928376998024425, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.031988054513931274}, {"id": 74, "seek": 46004, "start": 478.84000000000003, "end": 485.84000000000003, "text": " loops. And another possible message could be well just stop and then you don't loop.", "tokens": [51304, 16121, 13, 400, 1071, 1944, 3636, 727, 312, 731, 445, 1590, 293, 550, 291, 500, 380, 6367, 13, 51654], "temperature": 0.0, "avg_logprob": -0.23928376998024425, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.031988054513931274}, {"id": 75, "seek": 48584, "start": 486.84, "end": 493.84, "text": " All there is to it. What Erlang and beam also provide is a way to reload, sorry, to load", "tokens": [50414, 1057, 456, 307, 281, 309, 13, 708, 3300, 25241, 293, 14269, 611, 2893, 307, 257, 636, 281, 25628, 11, 2597, 11, 281, 3677, 50764], "temperature": 0.0, "avg_logprob": -0.22258302900526258, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.007585741113871336}, {"id": 76, "seek": 48584, "start": 497.64, "end": 504.64, "text": " new code into your, into the beam for a specific module. That's the code module in the Erlang", "tokens": [50954, 777, 3089, 666, 428, 11, 666, 264, 14269, 337, 257, 2685, 10088, 13, 663, 311, 264, 3089, 10088, 294, 264, 3300, 25241, 51304], "temperature": 0.0, "avg_logprob": -0.22258302900526258, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.007585741113871336}, {"id": 77, "seek": 48584, "start": 507.28, "end": 514.28, "text": " kernel package. You call it binary here in the middle. Target is the target module. The", "tokens": [51436, 28256, 7372, 13, 509, 818, 309, 17434, 510, 294, 264, 2808, 13, 24586, 307, 264, 3779, 10088, 13, 440, 51786], "temperature": 0.0, "avg_logprob": -0.22258302900526258, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.007585741113871336}, {"id": 78, "seek": 51428, "start": 515.28, "end": 519.56, "text": " thing in square brackets is the file name that it would be coming from. We don't care.", "tokens": [50414, 551, 294, 3732, 26179, 307, 264, 3991, 1315, 300, 309, 576, 312, 1348, 490, 13, 492, 500, 380, 1127, 13, 50628], "temperature": 0.0, "avg_logprob": -0.16633874437083368, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0014586664037778974}, {"id": 79, "seek": 51428, "start": 519.56, "end": 526.56, "text": " And the object code is compiled core Erlang code. At that point when you do that, you", "tokens": [50628, 400, 264, 2657, 3089, 307, 36548, 4965, 3300, 25241, 3089, 13, 1711, 300, 935, 562, 291, 360, 300, 11, 291, 50978], "temperature": 0.0, "avg_logprob": -0.16633874437083368, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0014586664037778974}, {"id": 80, "seek": 51428, "start": 529.68, "end": 533.0, "text": " have your old version of the code in the beam and you have a new version of the code in", "tokens": [51134, 362, 428, 1331, 3037, 295, 264, 3089, 294, 264, 14269, 293, 291, 362, 257, 777, 3037, 295, 264, 3089, 294, 51300], "temperature": 0.0, "avg_logprob": -0.16633874437083368, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0014586664037778974}, {"id": 81, "seek": 51428, "start": 533.0, "end": 539.04, "text": " the beam. But the existing processes, my game, my Hansbord is still running the old stuff.", "tokens": [51300, 264, 14269, 13, 583, 264, 6741, 7555, 11, 452, 1216, 11, 452, 17926, 65, 765, 307, 920, 2614, 264, 1331, 1507, 13, 51602], "temperature": 0.0, "avg_logprob": -0.16633874437083368, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0014586664037778974}, {"id": 82, "seek": 53904, "start": 539.04, "end": 546.04, "text": " So now I'm going to send a message upgrade to my Hansbord process. And roughly like that", "tokens": [50364, 407, 586, 286, 478, 516, 281, 2845, 257, 3636, 11484, 281, 452, 17926, 65, 765, 1399, 13, 400, 9810, 411, 300, 50714], "temperature": 0.0, "avg_logprob": -0.15266456184806404, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.0048933192156255245}, {"id": 83, "seek": 53904, "start": 548.28, "end": 552.28, "text": " this is the same loop as before but now I have the relevant upgrade part. Instead of", "tokens": [50826, 341, 307, 264, 912, 6367, 382, 949, 457, 586, 286, 362, 264, 7340, 11484, 644, 13, 7156, 295, 51026], "temperature": 0.0, "avg_logprob": -0.15266456184806404, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.0048933192156255245}, {"id": 84, "seek": 53904, "start": 552.28, "end": 557.64, "text": " just looping, which would loop in my old code, I have to say format this module for my target", "tokens": [51026, 445, 6367, 278, 11, 597, 576, 6367, 294, 452, 1331, 3089, 11, 286, 362, 281, 584, 7877, 341, 10088, 337, 452, 3779, 51294], "temperature": 0.0, "avg_logprob": -0.15266456184806404, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.0048933192156255245}, {"id": 85, "seek": 53904, "start": 557.64, "end": 564.0, "text": " module, my Hansbord module, explicitly specify the module with the loop and then it's guaranteed", "tokens": [51294, 10088, 11, 452, 17926, 65, 765, 10088, 11, 20803, 16500, 264, 10088, 365, 264, 6367, 293, 550, 309, 311, 18031, 51612], "temperature": 0.0, "avg_logprob": -0.15266456184806404, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.0048933192156255245}, {"id": 86, "seek": 56400, "start": 564.0, "end": 571.0, "text": " to use the new version of the code. And then I can happily play along my game with upgraded", "tokens": [50364, 281, 764, 264, 777, 3037, 295, 264, 3089, 13, 400, 550, 286, 393, 19909, 862, 2051, 452, 1216, 365, 24133, 50714], "temperature": 0.0, "avg_logprob": -0.17508167916155876, "compression_ratio": 1.5394736842105263, "no_speech_prob": 0.004661184269934893}, {"id": 87, "seek": 56400, "start": 572.08, "end": 579.08, "text": " code. Now why do we do that in Erlang? The difference between the local loop and the", "tokens": [50768, 3089, 13, 823, 983, 360, 321, 360, 300, 294, 3300, 25241, 30, 440, 2649, 1296, 264, 2654, 6367, 293, 264, 51118], "temperature": 0.0, "avg_logprob": -0.17508167916155876, "compression_ratio": 1.5394736842105263, "no_speech_prob": 0.004661184269934893}, {"id": 88, "seek": 56400, "start": 581.56, "end": 587.68, "text": " exported loop is something that Gleam does not know. So I can't do it. Why did I pick", "tokens": [51242, 42055, 6367, 307, 746, 300, 460, 306, 335, 775, 406, 458, 13, 407, 286, 393, 380, 360, 309, 13, 1545, 630, 286, 1888, 51548], "temperature": 0.0, "avg_logprob": -0.17508167916155876, "compression_ratio": 1.5394736842105263, "no_speech_prob": 0.004661184269934893}, {"id": 89, "seek": 56400, "start": 587.68, "end": 593.68, "text": " core Erlang? Because we can do this all in memory. No need to use file systems and other", "tokens": [51548, 4965, 3300, 25241, 30, 1436, 321, 393, 360, 341, 439, 294, 4675, 13, 883, 643, 281, 764, 3991, 3652, 293, 661, 51848], "temperature": 0.0, "avg_logprob": -0.17508167916155876, "compression_ratio": 1.5394736842105263, "no_speech_prob": 0.004661184269934893}, {"id": 90, "seek": 59368, "start": 593.7199999999999, "end": 600.7199999999999, "text": " things. There is an Erlang C-E-R-L library that can generate these things. I wrapped it for", "tokens": [50366, 721, 13, 821, 307, 364, 3300, 25241, 383, 12, 36, 12, 49, 12, 43, 6405, 300, 393, 8460, 613, 721, 13, 286, 14226, 309, 337, 50716], "temperature": 0.0, "avg_logprob": -0.22181620248934117, "compression_ratio": 1.4731182795698925, "no_speech_prob": 0.008965961635112762}, {"id": 91, "seek": 59368, "start": 600.8, "end": 607.8, "text": " Gleam. That's called Gencore Erlang and you can find it on the hex. So let's start a game.", "tokens": [50720, 460, 306, 335, 13, 663, 311, 1219, 3632, 12352, 3300, 25241, 293, 291, 393, 915, 309, 322, 264, 23291, 13, 407, 718, 311, 722, 257, 1216, 13, 51070], "temperature": 0.0, "avg_logprob": -0.22181620248934117, "compression_ratio": 1.4731182795698925, "no_speech_prob": 0.008965961635112762}, {"id": 92, "seek": 59368, "start": 615.52, "end": 622.52, "text": " I already made the type properly. So I'm ready to create a game. If I connect to the server", "tokens": [51456, 286, 1217, 1027, 264, 2010, 6108, 13, 407, 286, 478, 1919, 281, 1884, 257, 1216, 13, 759, 286, 1745, 281, 264, 7154, 51806], "temperature": 0.0, "avg_logprob": -0.22181620248934117, "compression_ratio": 1.4731182795698925, "no_speech_prob": 0.008965961635112762}, {"id": 93, "seek": 62252, "start": 622.52, "end": 629.3199999999999, "text": " it won't do anything here, but if I create an instance it's now and it will connect to", "tokens": [50364, 309, 1582, 380, 360, 1340, 510, 11, 457, 498, 286, 1884, 364, 5197, 309, 311, 586, 293, 309, 486, 1745, 281, 50704], "temperature": 0.0, "avg_logprob": -0.19178597645093037, "compression_ratio": 1.6372093023255814, "no_speech_prob": 0.013502378016710281}, {"id": 94, "seek": 62252, "start": 629.3199999999999, "end": 636.3199999999999, "text": " the game. So as you can see, it picked up the start plain plain from the definitions that", "tokens": [50704, 264, 1216, 13, 407, 382, 291, 393, 536, 11, 309, 6183, 493, 264, 722, 11121, 11121, 490, 264, 21988, 300, 51054], "temperature": 0.0, "avg_logprob": -0.19178597645093037, "compression_ratio": 1.6372093023255814, "no_speech_prob": 0.013502378016710281}, {"id": 95, "seek": 62252, "start": 636.6, "end": 643.6, "text": " I had at the left. It also picked up a move button. The rules, there are some implicit", "tokens": [51068, 286, 632, 412, 264, 1411, 13, 467, 611, 6183, 493, 257, 1286, 2960, 13, 440, 4474, 11, 456, 366, 512, 26947, 51418], "temperature": 0.0, "avg_logprob": -0.19178597645093037, "compression_ratio": 1.6372093023255814, "no_speech_prob": 0.013502378016710281}, {"id": 96, "seek": 62252, "start": 643.68, "end": 650.68, "text": " rules that I did not edit things about. I did not tell anything about. I need to be able", "tokens": [51422, 4474, 300, 286, 630, 406, 8129, 721, 466, 13, 286, 630, 406, 980, 1340, 466, 13, 286, 643, 281, 312, 1075, 51772], "temperature": 0.0, "avg_logprob": -0.19178597645093037, "compression_ratio": 1.6372093023255814, "no_speech_prob": 0.013502378016710281}, {"id": 97, "seek": 65068, "start": 651.28, "end": 654.92, "text": " to do something in my game. So there's a hard code that moves. I will just move the pawn", "tokens": [50394, 281, 360, 746, 294, 452, 1216, 13, 407, 456, 311, 257, 1152, 3089, 300, 6067, 13, 286, 486, 445, 1286, 264, 30905, 50576], "temperature": 0.0, "avg_logprob": -0.273926980716666, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.014317348599433899}, {"id": 98, "seek": 65068, "start": 654.92, "end": 661.42, "text": " one forward. And there's also a check for the win condition. So where the pawn is, it has", "tokens": [50576, 472, 2128, 13, 400, 456, 311, 611, 257, 1520, 337, 264, 1942, 4188, 13, 407, 689, 264, 30905, 307, 11, 309, 575, 50901], "temperature": 0.0, "avg_logprob": -0.273926980716666, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.014317348599433899}, {"id": 99, "seek": 65068, "start": 661.42, "end": 667.76, "text": " to check on the board whether that's a finished location. And that rule is going on continuously.", "tokens": [50901, 281, 1520, 322, 264, 3150, 1968, 300, 311, 257, 4335, 4914, 13, 400, 300, 4978, 307, 516, 322, 15684, 13, 51218], "temperature": 0.0, "avg_logprob": -0.273926980716666, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.014317348599433899}, {"id": 100, "seek": 65068, "start": 667.76, "end": 674.76, "text": " I have not made nice deltas and things for that in the UI and when at some point I will.", "tokens": [51218, 286, 362, 406, 1027, 1481, 1103, 83, 296, 293, 721, 337, 300, 294, 264, 15682, 293, 562, 412, 512, 935, 286, 486, 13, 51568], "temperature": 0.0, "avg_logprob": -0.273926980716666, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.014317348599433899}, {"id": 101, "seek": 67476, "start": 675.12, "end": 681.24, "text": " But these things are running in the background in the instance. Now at least I'll show you", "tokens": [50382, 583, 613, 721, 366, 2614, 294, 264, 3678, 294, 264, 5197, 13, 823, 412, 1935, 286, 603, 855, 291, 50688], "temperature": 0.0, "avg_logprob": -0.3370106750064426, "compression_ratio": 1.433862433862434, "no_speech_prob": 0.00978597067296505}, {"id": 102, "seek": 67476, "start": 681.24, "end": 688.24, "text": " the move. If I move, my pawn moves one forward. So yay, getting closer to the finish in my", "tokens": [50688, 264, 1286, 13, 759, 286, 1286, 11, 452, 30905, 6067, 472, 2128, 13, 407, 23986, 11, 1242, 4966, 281, 264, 2413, 294, 452, 51038], "temperature": 0.0, "avg_logprob": -0.3370106750064426, "compression_ratio": 1.433862433862434, "no_speech_prob": 0.00978597067296505}, {"id": 103, "seek": 67476, "start": 690.36, "end": 697.36, "text": " GUNS board. All right. So a little bit more about the bits and pieces that are happening.", "tokens": [51144, 460, 3979, 50, 3150, 13, 1057, 558, 13, 407, 257, 707, 857, 544, 466, 264, 9239, 293, 3755, 300, 366, 2737, 13, 51494], "temperature": 0.0, "avg_logprob": -0.3370106750064426, "compression_ratio": 1.433862433862434, "no_speech_prob": 0.00978597067296505}, {"id": 104, "seek": 69736, "start": 698.36, "end": 705.36, "text": " There's the Asian communication from the browser to the server for the model. Whenever", "tokens": [50414, 821, 311, 264, 10645, 6101, 490, 264, 11185, 281, 264, 7154, 337, 264, 2316, 13, 14159, 50764], "temperature": 0.0, "avg_logprob": -0.29874146874271224, "compression_ratio": 1.5434782608695652, "no_speech_prob": 0.05415863171219826}, {"id": 105, "seek": 69736, "start": 709.12, "end": 716.12, "text": " there is a delta that's made, we just recreate the entire instance module in Core Erlang, reloaded", "tokens": [50952, 456, 307, 257, 8289, 300, 311, 1027, 11, 321, 445, 25833, 264, 2302, 5197, 10088, 294, 14798, 3300, 25241, 11, 25628, 292, 51302], "temperature": 0.0, "avg_logprob": -0.29874146874271224, "compression_ratio": 1.5434782608695652, "no_speech_prob": 0.05415863171219826}, {"id": 106, "seek": 69736, "start": 716.12, "end": 723.12, "text": " and upgraded. And the instance just keeps talking to it. It doesn't even notice it from the client", "tokens": [51302, 293, 24133, 13, 400, 264, 5197, 445, 5965, 1417, 281, 309, 13, 467, 1177, 380, 754, 3449, 309, 490, 264, 6423, 51652], "temperature": 0.0, "avg_logprob": -0.29874146874271224, "compression_ratio": 1.5434782608695652, "no_speech_prob": 0.05415863171219826}, {"id": 107, "seek": 72312, "start": 723.96, "end": 730.96, "text": " and also talks in terms of chasing with it. State and rules. The initial state is something", "tokens": [50406, 293, 611, 6686, 294, 2115, 295, 17876, 365, 309, 13, 4533, 293, 4474, 13, 440, 5883, 1785, 307, 746, 50756], "temperature": 0.0, "avg_logprob": -0.19597393077808423, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.01660669781267643}, {"id": 108, "seek": 72312, "start": 733.64, "end": 740.64, "text": " that should be adopted, adjusted and rules are... So yeah, that's right. The rule is", "tokens": [50890, 300, 820, 312, 12175, 11, 19871, 293, 4474, 366, 485, 407, 1338, 11, 300, 311, 558, 13, 440, 4978, 307, 51240], "temperature": 0.0, "avg_logprob": -0.19597393077808423, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.01660669781267643}, {"id": 109, "seek": 72312, "start": 741.16, "end": 745.44, "text": " something that is in the model. I talk about a conceptual thing there. My instance doesn't", "tokens": [51266, 746, 300, 307, 294, 264, 2316, 13, 286, 751, 466, 257, 24106, 551, 456, 13, 1222, 5197, 1177, 380, 51480], "temperature": 0.0, "avg_logprob": -0.19597393077808423, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.01660669781267643}, {"id": 110, "seek": 72312, "start": 745.44, "end": 752.2, "text": " know the rule. It just has code. And my client doesn't know what the rule is either. It just", "tokens": [51480, 458, 264, 4978, 13, 467, 445, 575, 3089, 13, 400, 452, 6423, 1177, 380, 458, 437, 264, 4978, 307, 2139, 13, 467, 445, 51818], "temperature": 0.0, "avg_logprob": -0.19597393077808423, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.01660669781267643}, {"id": 111, "seek": 75220, "start": 752.2, "end": 759.2, "text": " knows whether it can do something or cannot do something. So what is important that I", "tokens": [50364, 3255, 1968, 309, 393, 360, 746, 420, 2644, 360, 746, 13, 407, 437, 307, 1021, 300, 286, 50714], "temperature": 0.0, "avg_logprob": -0.15409222077787593, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0025815193075686693}, {"id": 112, "seek": 75220, "start": 761.24, "end": 766.9200000000001, "text": " wanted to make as two kind of changes. Some changes that when I do that, the instance will", "tokens": [50816, 1415, 281, 652, 382, 732, 733, 295, 2962, 13, 2188, 2962, 300, 562, 286, 360, 300, 11, 264, 5197, 486, 51100], "temperature": 0.0, "avg_logprob": -0.15409222077787593, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0025815193075686693}, {"id": 113, "seek": 75220, "start": 766.9200000000001, "end": 773.9200000000001, "text": " in the end see it. So I'm going to change the board. And that when I change the board", "tokens": [51100, 294, 264, 917, 536, 309, 13, 407, 286, 478, 516, 281, 1319, 264, 3150, 13, 400, 300, 562, 286, 1319, 264, 3150, 51450], "temperature": 0.0, "avg_logprob": -0.15409222077787593, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0025815193075686693}, {"id": 114, "seek": 75220, "start": 773.9200000000001, "end": 780.6, "text": " it's going to compile the change into the Core Erlang, reload it, do some small migration", "tokens": [51450, 309, 311, 516, 281, 31413, 264, 1319, 666, 264, 14798, 3300, 25241, 11, 25628, 309, 11, 360, 512, 1359, 17011, 51784], "temperature": 0.0, "avg_logprob": -0.15409222077787593, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0025815193075686693}, {"id": 115, "seek": 78060, "start": 780.6, "end": 787.6, "text": " and then also pass that information to our client. So, here we have it again. If I turn", "tokens": [50364, 293, 550, 611, 1320, 300, 1589, 281, 527, 6423, 13, 407, 11, 510, 321, 362, 309, 797, 13, 759, 286, 1261, 50714], "temperature": 0.0, "avg_logprob": -0.18868746902003433, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004028240218758583}, {"id": 116, "seek": 78060, "start": 789.2, "end": 795.48, "text": " this into a Goose for instance, it becomes a Goose. So that was one recompilation of", "tokens": [50794, 341, 666, 257, 1037, 541, 337, 5197, 11, 309, 3643, 257, 1037, 541, 13, 407, 300, 390, 472, 48000, 16067, 295, 51108], "temperature": 0.0, "avg_logprob": -0.18868746902003433, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004028240218758583}, {"id": 117, "seek": 78060, "start": 795.48, "end": 800.84, "text": " Erlang in the back button. Now I make another plane. I can add another one if I want to.", "tokens": [51108, 3300, 25241, 294, 264, 646, 2960, 13, 823, 286, 652, 1071, 5720, 13, 286, 393, 909, 1071, 472, 498, 286, 528, 281, 13, 51376], "temperature": 0.0, "avg_logprob": -0.18868746902003433, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004028240218758583}, {"id": 118, "seek": 78060, "start": 800.84, "end": 807.84, "text": " And at some point I'm going to have to reach the finish here. So let's make that. So yeah,", "tokens": [51376, 400, 412, 512, 935, 286, 478, 516, 281, 362, 281, 2524, 264, 2413, 510, 13, 407, 718, 311, 652, 300, 13, 407, 1338, 11, 51726], "temperature": 0.0, "avg_logprob": -0.18868746902003433, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004028240218758583}, {"id": 119, "seek": 80784, "start": 808.84, "end": 815.84, "text": " that was three, four recompilations of things in memory and moving on. Involving. But another", "tokens": [50414, 300, 390, 1045, 11, 1451, 48000, 388, 763, 295, 721, 294, 4675, 293, 2684, 322, 13, 682, 9646, 798, 13, 583, 1071, 50764], "temperature": 0.0, "avg_logprob": -0.1966432742218473, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.0014725816436111927}, {"id": 120, "seek": 80784, "start": 819.1600000000001, "end": 826.1600000000001, "text": " one thing that I might want to change, because I can also create multiple instances, is my", "tokens": [50930, 472, 551, 300, 286, 1062, 528, 281, 1319, 11, 570, 286, 393, 611, 1884, 3866, 14519, 11, 307, 452, 51280], "temperature": 0.0, "avg_logprob": -0.1966432742218473, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.0014725816436111927}, {"id": 121, "seek": 80784, "start": 826.4, "end": 833.12, "text": " starting state. And that would mean that the only thing that happens if I change that from", "tokens": [51292, 2891, 1785, 13, 400, 300, 576, 914, 300, 264, 787, 551, 300, 2314, 498, 286, 1319, 300, 490, 51628], "temperature": 0.0, "avg_logprob": -0.1966432742218473, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.0014725816436111927}, {"id": 122, "seek": 83312, "start": 833.12, "end": 838.8, "text": " my client, it changes my model, but nothing else, unless I don't start a new instance,", "tokens": [50364, 452, 6423, 11, 309, 2962, 452, 2316, 11, 457, 1825, 1646, 11, 5969, 286, 500, 380, 722, 257, 777, 5197, 11, 50648], "temperature": 0.0, "avg_logprob": -0.18554320084421258, "compression_ratio": 1.6794258373205742, "no_speech_prob": 0.00952828023582697}, {"id": 123, "seek": 83312, "start": 838.8, "end": 845.8, "text": " nothing happens with it. And that looks like this. So I have a game one and a game two.", "tokens": [50648, 1825, 2314, 365, 309, 13, 400, 300, 1542, 411, 341, 13, 407, 286, 362, 257, 1216, 472, 293, 257, 1216, 732, 13, 50998], "temperature": 0.0, "avg_logprob": -0.18554320084421258, "compression_ratio": 1.6794258373205742, "no_speech_prob": 0.00952828023582697}, {"id": 124, "seek": 83312, "start": 848.0, "end": 853.52, "text": " Game two hasn't been started yet. And if I change this one from zero to one, now I start", "tokens": [51108, 7522, 732, 6132, 380, 668, 1409, 1939, 13, 400, 498, 286, 1319, 341, 472, 490, 4018, 281, 472, 11, 586, 286, 722, 51384], "temperature": 0.0, "avg_logprob": -0.18554320084421258, "compression_ratio": 1.6794258373205742, "no_speech_prob": 0.00952828023582697}, {"id": 125, "seek": 83312, "start": 853.52, "end": 860.12, "text": " at position one. Then we notice that in game one nothing happened. But if I start a new", "tokens": [51384, 412, 2535, 472, 13, 1396, 321, 3449, 300, 294, 1216, 472, 1825, 2011, 13, 583, 498, 286, 722, 257, 777, 51714], "temperature": 0.0, "avg_logprob": -0.18554320084421258, "compression_ratio": 1.6794258373205742, "no_speech_prob": 0.00952828023582697}, {"id": 126, "seek": 86012, "start": 860.12, "end": 867.12, "text": " instance, then this one will now start at position one instead of position zero. And", "tokens": [50364, 5197, 11, 550, 341, 472, 486, 586, 722, 412, 2535, 472, 2602, 295, 2535, 4018, 13, 400, 50714], "temperature": 0.0, "avg_logprob": -0.18449550204806858, "compression_ratio": 1.505813953488372, "no_speech_prob": 0.01238136738538742}, {"id": 127, "seek": 86012, "start": 869.52, "end": 875.04, "text": " just to show that even though it shows two, it didn't change to one, but didn't show it.", "tokens": [50834, 445, 281, 855, 300, 754, 1673, 309, 3110, 732, 11, 309, 994, 380, 1319, 281, 472, 11, 457, 994, 380, 855, 309, 13, 51110], "temperature": 0.0, "avg_logprob": -0.18449550204806858, "compression_ratio": 1.505813953488372, "no_speech_prob": 0.01238136738538742}, {"id": 128, "seek": 86012, "start": 875.04, "end": 882.04, "text": " If I move, it will go to three. Well, and where did I put the finish today? On start,", "tokens": [51110, 759, 286, 1286, 11, 309, 486, 352, 281, 1045, 13, 1042, 11, 293, 689, 630, 286, 829, 264, 2413, 965, 30, 1282, 722, 11, 51460], "temperature": 0.0, "avg_logprob": -0.18449550204806858, "compression_ratio": 1.505813953488372, "no_speech_prob": 0.01238136738538742}, {"id": 129, "seek": 88204, "start": 883.04, "end": 890.04, "text": " zero, one, three, on number four. I'll just move to four. There. I finished my game. I", "tokens": [50414, 4018, 11, 472, 11, 1045, 11, 322, 1230, 1451, 13, 286, 603, 445, 1286, 281, 1451, 13, 821, 13, 286, 4335, 452, 1216, 13, 286, 50764], "temperature": 0.0, "avg_logprob": -0.17143472035725912, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.011389272287487984}, {"id": 130, "seek": 88204, "start": 890.04, "end": 897.04, "text": " won. So things I want to do in the future. I'm very much interested in what kind of deltas", "tokens": [50764, 1582, 13, 407, 721, 286, 528, 281, 360, 294, 264, 2027, 13, 286, 478, 588, 709, 3102, 294, 437, 733, 295, 1103, 83, 296, 51114], "temperature": 0.0, "avg_logprob": -0.17143472035725912, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.011389272287487984}, {"id": 131, "seek": 88204, "start": 902.4399999999999, "end": 909.4399999999999, "text": " are usable, sensible. You saw me adding cells to a board. You saw me change the type of", "tokens": [51384, 366, 29975, 11, 25380, 13, 509, 1866, 385, 5127, 5438, 281, 257, 3150, 13, 509, 1866, 385, 1319, 264, 2010, 295, 51734], "temperature": 0.0, "avg_logprob": -0.17143472035725912, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.011389272287487984}, {"id": 132, "seek": 90944, "start": 910.4000000000001, "end": 917.4000000000001, "text": " the board. Okay. What if I remove a cell from the board? Yeah. What if the pawn is on there?", "tokens": [50412, 264, 3150, 13, 1033, 13, 708, 498, 286, 4159, 257, 2815, 490, 264, 3150, 30, 865, 13, 708, 498, 264, 30905, 307, 322, 456, 30, 50762], "temperature": 0.0, "avg_logprob": -0.20325092354206123, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.01908893696963787}, {"id": 133, "seek": 90944, "start": 918.0400000000001, "end": 925.0400000000001, "text": " It quickly becomes like, you could think of a couple of solutions for when you remove", "tokens": [50794, 467, 2661, 3643, 411, 11, 291, 727, 519, 295, 257, 1916, 295, 6547, 337, 562, 291, 4159, 51144], "temperature": 0.0, "avg_logprob": -0.20325092354206123, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.01908893696963787}, {"id": 134, "seek": 90944, "start": 925.32, "end": 929.72, "text": " the cell from the board. You move the pawn to the previous or the next cell, or you remove", "tokens": [51158, 264, 2815, 490, 264, 3150, 13, 509, 1286, 264, 30905, 281, 264, 3894, 420, 264, 958, 2815, 11, 420, 291, 4159, 51378], "temperature": 0.0, "avg_logprob": -0.20325092354206123, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.01908893696963787}, {"id": 135, "seek": 90944, "start": 929.72, "end": 934.72, "text": " it, or you put it on cell zero. But why would, why, how do you pick one? Depends on your", "tokens": [51378, 309, 11, 420, 291, 829, 309, 322, 2815, 4018, 13, 583, 983, 576, 11, 983, 11, 577, 360, 291, 1888, 472, 30, 4056, 2581, 322, 428, 51628], "temperature": 0.0, "avg_logprob": -0.20325092354206123, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.01908893696963787}, {"id": 136, "seek": 93472, "start": 934.72, "end": 941.72, "text": " application. So I really want to look more into that. Another thing is that I don't think", "tokens": [50364, 3861, 13, 407, 286, 534, 528, 281, 574, 544, 666, 300, 13, 3996, 551, 307, 300, 286, 500, 380, 519, 50714], "temperature": 0.0, "avg_logprob": -0.17792259294962146, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.011430867947638035}, {"id": 137, "seek": 93472, "start": 944.88, "end": 949.12, "text": " the Hanselboard UI that I had looks very nice. It would be much better if it looked like", "tokens": [50872, 264, 17926, 338, 3787, 15682, 300, 286, 632, 1542, 588, 1481, 13, 467, 576, 312, 709, 1101, 498, 309, 2956, 411, 51084], "temperature": 0.0, "avg_logprob": -0.17792259294962146, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.011430867947638035}, {"id": 138, "seek": 93472, "start": 949.12, "end": 956.12, "text": " the second slide that I showed. But if I do that, then the client really knows about Hanselboard.", "tokens": [51084, 264, 1150, 4137, 300, 286, 4712, 13, 583, 498, 286, 360, 300, 11, 550, 264, 6423, 534, 3255, 466, 17926, 338, 3787, 13, 51434], "temperature": 0.0, "avg_logprob": -0.17792259294962146, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.011430867947638035}, {"id": 139, "seek": 93472, "start": 957.2, "end": 961.6800000000001, "text": " But what if I want to make a slightly different game? So, okay. And what if my Hanselboard", "tokens": [51488, 583, 437, 498, 286, 528, 281, 652, 257, 4748, 819, 1216, 30, 407, 11, 1392, 13, 400, 437, 498, 452, 17926, 338, 3787, 51712], "temperature": 0.0, "avg_logprob": -0.17792259294962146, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.011430867947638035}, {"id": 140, "seek": 96168, "start": 961.68, "end": 967.0799999999999, "text": " knows about most of the things I do, but I add that labyrinth thing? Now I want to render", "tokens": [50364, 3255, 466, 881, 295, 264, 721, 286, 360, 11, 457, 286, 909, 300, 287, 46800, 392, 551, 30, 823, 286, 528, 281, 15529, 50634], "temperature": 0.0, "avg_logprob": -0.22272238330306293, "compression_ratio": 1.625, "no_speech_prob": 0.05701400712132454}, {"id": 141, "seek": 96168, "start": 967.0799999999999, "end": 973.16, "text": " a nice labyrinth. Okay. Can I make something that knows its Hanselboard, but can also adjust", "tokens": [50634, 257, 1481, 287, 46800, 392, 13, 1033, 13, 1664, 286, 652, 746, 300, 3255, 1080, 17926, 338, 3787, 11, 457, 393, 611, 4369, 50938], "temperature": 0.0, "avg_logprob": -0.22272238330306293, "compression_ratio": 1.625, "no_speech_prob": 0.05701400712132454}, {"id": 142, "seek": 96168, "start": 973.16, "end": 978.2399999999999, "text": " to changes that I make in the model that expands on what was already there that they didn't", "tokens": [50938, 281, 2962, 300, 286, 652, 294, 264, 2316, 300, 33706, 322, 437, 390, 1217, 456, 300, 436, 994, 380, 51192], "temperature": 0.0, "avg_logprob": -0.22272238330306293, "compression_ratio": 1.625, "no_speech_prob": 0.05701400712132454}, {"id": 143, "seek": 96168, "start": 978.2399999999999, "end": 984.4399999999999, "text": " know about in the start? And obviously, it needs to be multiplayer because playing in", "tokens": [51192, 458, 466, 294, 264, 722, 30, 400, 2745, 11, 309, 2203, 281, 312, 27325, 570, 2433, 294, 51502], "temperature": 0.0, "avg_logprob": -0.22272238330306293, "compression_ratio": 1.625, "no_speech_prob": 0.05701400712132454}, {"id": 144, "seek": 96168, "start": 984.4399999999999, "end": 988.88, "text": " my own is, I want you all to play with me.", "tokens": [51502, 452, 1065, 307, 11, 286, 528, 291, 439, 281, 862, 365, 385, 13, 51724], "temperature": 0.0, "avg_logprob": -0.22272238330306293, "compression_ratio": 1.625, "no_speech_prob": 0.05701400712132454}, {"id": 145, "seek": 98888, "start": 988.88, "end": 995.88, "text": " All right. The code that you saw in the iframes is in the top link. While reasoning about", "tokens": [50364, 1057, 558, 13, 440, 3089, 300, 291, 1866, 294, 264, 498, 81, 1632, 307, 294, 264, 1192, 2113, 13, 3987, 21577, 466, 50714], "temperature": 0.0, "avg_logprob": -0.2401784844354752, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.039735328406095505}, {"id": 146, "seek": 98888, "start": 996.28, "end": 1000.96, "text": " this, I also wrote a little, a start of a Gleam library that generates Gleam code, which", "tokens": [50734, 341, 11, 286, 611, 4114, 257, 707, 11, 257, 722, 295, 257, 460, 306, 335, 6405, 300, 23815, 460, 306, 335, 3089, 11, 597, 50968], "temperature": 0.0, "avg_logprob": -0.2401784844354752, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.039735328406095505}, {"id": 147, "seek": 98888, "start": 1000.96, "end": 1005.64, "text": " is the second link, the one that generates Core Erlang. It's the third link, my own", "tokens": [50968, 307, 264, 1150, 2113, 11, 264, 472, 300, 23815, 14798, 3300, 25241, 13, 467, 311, 264, 2636, 2113, 11, 452, 1065, 51202], "temperature": 0.0, "avg_logprob": -0.2401784844354752, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.039735328406095505}, {"id": 148, "seek": 98888, "start": 1005.64, "end": 1011.48, "text": " web page, the fourth. And if you want to know what the Dutch income text is looking like,", "tokens": [51202, 3670, 3028, 11, 264, 6409, 13, 400, 498, 291, 528, 281, 458, 437, 264, 15719, 5742, 2487, 307, 1237, 411, 11, 51494], "temperature": 0.0, "avg_logprob": -0.2401784844354752, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.039735328406095505}, {"id": 149, "seek": 98888, "start": 1011.48, "end": 1014.48, "text": " it's all in there. Thank you.", "tokens": [51494, 309, 311, 439, 294, 456, 13, 1044, 291, 13, 51644], "temperature": 0.0, "avg_logprob": -0.2401784844354752, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.039735328406095505}, {"id": 150, "seek": 101448, "start": 1014.48, "end": 1021.48, "text": " We have time for a couple of questions. Anyone? Any? Okay.", "tokens": [50364, 492, 362, 565, 337, 257, 1916, 295, 1651, 13, 14643, 30, 2639, 30, 1033, 13, 50714], "temperature": 0.0, "avg_logprob": -0.5188299627865062, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.10117710381746292}, {"id": 151, "seek": 101448, "start": 1021.48, "end": 1028.48, "text": " Thank you. That was really quite amazing technology there. I was wondering, do you have", "tokens": [50714, 1044, 291, 13, 663, 390, 534, 1596, 2243, 2899, 456, 13, 286, 390, 6359, 11, 360, 291, 362, 51064], "temperature": 0.0, "avg_logprob": -0.5188299627865062, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.10117710381746292}, {"id": 152, "seek": 101448, "start": 1031.92, "end": 1036.92, "text": " thoughts on like when you might decide to apply these sorts of techniques to a problem?", "tokens": [51236, 4598, 322, 411, 562, 291, 1062, 4536, 281, 3079, 613, 7527, 295, 7512, 281, 257, 1154, 30, 51486], "temperature": 0.0, "avg_logprob": -0.5188299627865062, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.10117710381746292}, {"id": 153, "seek": 101448, "start": 1036.92, "end": 1039.6, "text": " When it would be a good question.", "tokens": [51486, 1133, 309, 576, 312, 257, 665, 1168, 13, 51620], "temperature": 0.0, "avg_logprob": -0.5188299627865062, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.10117710381746292}, {"id": 154, "seek": 103960, "start": 1039.6799999999998, "end": 1047.6799999999998, "text": " Yeah. Okay. Question is when this kind of solution would apply to a problem. It's, I might not", "tokens": [50368, 865, 13, 1033, 13, 14464, 307, 562, 341, 733, 295, 3827, 576, 3079, 281, 257, 1154, 13, 467, 311, 11, 286, 1062, 406, 50768], "temperature": 0.0, "avg_logprob": -0.1728824166690602, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.010611478239297867}, {"id": 155, "seek": 103960, "start": 1047.6799999999998, "end": 1053.32, "text": " be the best person to answer this because I'm somewhat new to model driven. It helps", "tokens": [50768, 312, 264, 1151, 954, 281, 1867, 341, 570, 286, 478, 8344, 777, 281, 2316, 9555, 13, 467, 3665, 51050], "temperature": 0.0, "avg_logprob": -0.1728824166690602, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.010611478239297867}, {"id": 156, "seek": 103960, "start": 1053.32, "end": 1058.36, "text": " when you, when that description is going to give you something, whether that is checking", "tokens": [51050, 562, 291, 11, 562, 300, 3855, 307, 516, 281, 976, 291, 746, 11, 1968, 300, 307, 8568, 51302], "temperature": 0.0, "avg_logprob": -0.1728824166690602, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.010611478239297867}, {"id": 157, "seek": 103960, "start": 1058.36, "end": 1065.36, "text": " that something is coherent or correct. When, yeah, the model should give you something.", "tokens": [51302, 300, 746, 307, 36239, 420, 3006, 13, 1133, 11, 1338, 11, 264, 2316, 820, 976, 291, 746, 13, 51652], "temperature": 0.0, "avg_logprob": -0.1728824166690602, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.010611478239297867}, {"id": 158, "seek": 106960, "start": 1070.6, "end": 1074.6399999999999, "text": " When not to do it, well, if you just want to play Hansenboard, just make a Hansenboard", "tokens": [50414, 1133, 406, 281, 360, 309, 11, 731, 11, 498, 291, 445, 528, 281, 862, 17926, 268, 3787, 11, 445, 652, 257, 17926, 268, 3787, 50616], "temperature": 0.0, "avg_logprob": -0.26770907181959885, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.011119932867586613}, {"id": 159, "seek": 106960, "start": 1074.6399999999999, "end": 1079.84, "text": " server and a Hansenboard client because it's much faster. Much quicker to build. So it's,", "tokens": [50616, 7154, 293, 257, 17926, 268, 3787, 6423, 570, 309, 311, 709, 4663, 13, 12313, 16255, 281, 1322, 13, 407, 309, 311, 11, 50876], "temperature": 0.0, "avg_logprob": -0.26770907181959885, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.011119932867586613}, {"id": 160, "seek": 106960, "start": 1079.84, "end": 1086.84, "text": " it's, it is an investment. It's quite an investment. It's not just, not 10% extra. It's a factor", "tokens": [50876, 309, 311, 11, 309, 307, 364, 6078, 13, 467, 311, 1596, 364, 6078, 13, 467, 311, 406, 445, 11, 406, 1266, 4, 2857, 13, 467, 311, 257, 5952, 51226], "temperature": 0.0, "avg_logprob": -0.26770907181959885, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.011119932867586613}, {"id": 161, "seek": 106960, "start": 1090.0, "end": 1097.0, "text": " much, possibly 5, 10 extra to make sure that you can really do that kind of stuff.", "tokens": [51384, 709, 11, 6264, 1025, 11, 1266, 2857, 281, 652, 988, 300, 291, 393, 534, 360, 300, 733, 295, 1507, 13, 51734], "temperature": 0.0, "avg_logprob": -0.26770907181959885, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.011119932867586613}, {"id": 162, "seek": 109700, "start": 1097.4, "end": 1101.4, "text": " Other question? It was the same question.", "tokens": [50384, 5358, 1168, 30, 467, 390, 264, 912, 1168, 13, 50584], "temperature": 0.0, "avg_logprob": -0.4843887441298541, "compression_ratio": 1.5030674846625767, "no_speech_prob": 0.008381177671253681}, {"id": 163, "seek": 109700, "start": 1101.4, "end": 1103.4, "text": " Because it's fun.", "tokens": [50584, 1436, 309, 311, 1019, 13, 50684], "temperature": 0.0, "avg_logprob": -0.4843887441298541, "compression_ratio": 1.5030674846625767, "no_speech_prob": 0.008381177671253681}, {"id": 164, "seek": 109700, "start": 1103.4, "end": 1105.4, "text": " Okay.", "tokens": [50684, 1033, 13, 50784], "temperature": 0.0, "avg_logprob": -0.4843887441298541, "compression_ratio": 1.5030674846625767, "no_speech_prob": 0.008381177671253681}, {"id": 165, "seek": 109700, "start": 1105.4, "end": 1112.4, "text": " Yeah, that was really, really cool. Really interesting. When you changed like the initial", "tokens": [50784, 865, 11, 300, 390, 534, 11, 534, 1627, 13, 4083, 1880, 13, 1133, 291, 3105, 411, 264, 5883, 51134], "temperature": 0.0, "avg_logprob": -0.4843887441298541, "compression_ratio": 1.5030674846625767, "no_speech_prob": 0.008381177671253681}, {"id": 166, "seek": 109700, "start": 1113.04, "end": 1120.04, "text": " state, you showed like the running client didn't update, right? Like it didn't, it didn't", "tokens": [51166, 1785, 11, 291, 4712, 411, 264, 2614, 6423, 994, 380, 5623, 11, 558, 30, 1743, 309, 994, 380, 11, 309, 994, 380, 51516], "temperature": 0.0, "avg_logprob": -0.4843887441298541, "compression_ratio": 1.5030674846625767, "no_speech_prob": 0.008381177671253681}, {"id": 167, "seek": 112004, "start": 1120.04, "end": 1127.04, "text": " update the client because all these updates are triggered through messages. Is it possible", "tokens": [50364, 5623, 264, 6423, 570, 439, 613, 9205, 366, 21710, 807, 7897, 13, 1119, 309, 1944, 50714], "temperature": 0.0, "avg_logprob": -0.31055604710298423, "compression_ratio": 1.5989583333333333, "no_speech_prob": 0.1334342360496521}, {"id": 168, "seek": 112004, "start": 1128.04, "end": 1134.04, "text": " that you could replay message history from the beginning with a running client? So like", "tokens": [50764, 300, 291, 727, 23836, 3636, 2503, 490, 264, 2863, 365, 257, 2614, 6423, 30, 407, 411, 51064], "temperature": 0.0, "avg_logprob": -0.31055604710298423, "compression_ratio": 1.5989583333333333, "no_speech_prob": 0.1334342360496521}, {"id": 169, "seek": 112004, "start": 1134.04, "end": 1140.04, "text": " if I updated the initial state of a running process, could I then replay all of the messages", "tokens": [51064, 498, 286, 10588, 264, 5883, 1785, 295, 257, 2614, 1399, 11, 727, 286, 550, 23836, 439, 295, 264, 7897, 51364], "temperature": 0.0, "avg_logprob": -0.31055604710298423, "compression_ratio": 1.5989583333333333, "no_speech_prob": 0.1334342360496521}, {"id": 170, "seek": 112004, "start": 1140.04, "end": 1145.04, "text": " it's received to change propagates?", "tokens": [51364, 309, 311, 4613, 281, 1319, 12425, 1024, 30, 51614], "temperature": 0.0, "avg_logprob": -0.31055604710298423, "compression_ratio": 1.5989583333333333, "no_speech_prob": 0.1334342360496521}, {"id": 171, "seek": 114504, "start": 1145.04, "end": 1152.04, "text": " There's, there's, there's two answers to that, I guess. Yeah. Is it possible to replay all", "tokens": [50364, 821, 311, 11, 456, 311, 11, 456, 311, 732, 6338, 281, 300, 11, 286, 2041, 13, 865, 13, 1119, 309, 1944, 281, 23836, 439, 50714], "temperature": 0.0, "avg_logprob": -0.25677331288655597, "compression_ratio": 1.6875, "no_speech_prob": 0.02043042704463005}, {"id": 172, "seek": 114504, "start": 1156.34, "end": 1163.34, "text": " the events that happened both to the model and the instance or just to the instance game?", "tokens": [50929, 264, 3931, 300, 2011, 1293, 281, 264, 2316, 293, 264, 5197, 420, 445, 281, 264, 5197, 1216, 30, 51279], "temperature": 0.0, "avg_logprob": -0.25677331288655597, "compression_ratio": 1.6875, "no_speech_prob": 0.02043042704463005}, {"id": 173, "seek": 114504, "start": 1164.04, "end": 1171.04, "text": " Just to the instance. Just to the instance. At the moment, no.", "tokens": [51314, 1449, 281, 264, 5197, 13, 1449, 281, 264, 5197, 13, 1711, 264, 1623, 11, 572, 13, 51664], "temperature": 0.0, "avg_logprob": -0.25677331288655597, "compression_ratio": 1.6875, "no_speech_prob": 0.02043042704463005}, {"id": 174, "seek": 117104, "start": 1172.04, "end": 1173.04, "text": " Would you?", "tokens": [50414, 6068, 291, 30, 50464], "temperature": 0.0, "avg_logprob": -0.22362882784097501, "compression_ratio": 1.6376146788990826, "no_speech_prob": 0.006857102271169424}, {"id": 175, "seek": 117104, "start": 1173.04, "end": 1178.04, "text": " Would be, would be interesting. One way in which at least the part of the answer would be yes", "tokens": [50464, 6068, 312, 11, 576, 312, 1880, 13, 1485, 636, 294, 597, 412, 1935, 264, 644, 295, 264, 1867, 576, 312, 2086, 50714], "temperature": 0.0, "avg_logprob": -0.22362882784097501, "compression_ratio": 1.6376146788990826, "no_speech_prob": 0.006857102271169424}, {"id": 176, "seek": 117104, "start": 1178.04, "end": 1182.04, "text": " is if you look at the model when I, when I say please change this in this way with this", "tokens": [50714, 307, 498, 291, 574, 412, 264, 2316, 562, 286, 11, 562, 286, 584, 1767, 1319, 341, 294, 341, 636, 365, 341, 50914], "temperature": 0.0, "avg_logprob": -0.22362882784097501, "compression_ratio": 1.6376146788990826, "no_speech_prob": 0.006857102271169424}, {"id": 177, "seek": 117104, "start": 1182.04, "end": 1187.04, "text": " delta, the server will respond by just giving you back, yep, I applied this delta, now you", "tokens": [50914, 8289, 11, 264, 7154, 486, 4196, 538, 445, 2902, 291, 646, 11, 18633, 11, 286, 6456, 341, 8289, 11, 586, 291, 51164], "temperature": 0.0, "avg_logprob": -0.22362882784097501, "compression_ratio": 1.6376146788990826, "no_speech_prob": 0.006857102271169424}, {"id": 178, "seek": 117104, "start": 1187.04, "end": 1189.04, "text": " do too.", "tokens": [51164, 360, 886, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22362882784097501, "compression_ratio": 1.6376146788990826, "no_speech_prob": 0.006857102271169424}, {"id": 179, "seek": 117104, "start": 1189.04, "end": 1196.04, "text": " Okay, cool. Any other questions? Okay, thank you then.", "tokens": [51264, 1033, 11, 1627, 13, 2639, 661, 1651, 30, 1033, 11, 1309, 291, 550, 13, 51614], "temperature": 0.0, "avg_logprob": -0.22362882784097501, "compression_ratio": 1.6376146788990826, "no_speech_prob": 0.006857102271169424}, {"id": 180, "seek": 117104, "start": 1196.04, "end": 1197.04, "text": " Thank you.", "tokens": [51614, 1044, 291, 13, 51664], "temperature": 0.0, "avg_logprob": -0.22362882784097501, "compression_ratio": 1.6376146788990826, "no_speech_prob": 0.006857102271169424}, {"id": 181, "seek": 120104, "start": 1201.04, "end": 1202.04, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.5159457921981812, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9929677248001099}], "language": "en"}