{"text": " Hello everyone. Thank you for having us here. It's our first time here, so please be kind. It's like for both of us, it's the first time here, so we're a little bit nervous. And we're here to talk about practical CI, CID observability with open telemetry. This is the abstract we have submitted and of course we don't expect you to go through a whole thing because it's just enormous. But like if we could abstract the abstract in a way, this talk is about enhancing your pipeline's ability and performance by bringing observability to every stage of software delivery. So we're going to answer two questions like how we can identify flakiness and bottlenecks during our CI-CB process and envision a future of effortless visibility. And again, we're going to talk about what's the role of open telemetry on that and also how, with the role it plays in shaping CI-CB's future and explore all the challenges and opportunities ahead. So if I go to the next slide, a little bit about us, I'm Dimitris, a software engineer at Grafana Labs at the Platform Productivity Squad. And I'm Giordano, Gio for France. I'm a software engineer for the X-Port Squad at Grafana Labs. So this is the agenda. As I said, we're going to start by defining what CI really is and then talk about current issues we have with CI-CB systems. And then we're going to do a small intro to open telemetry and how we use it really, why it is important to own our data, and then practical use cases where we are and what's next. So all that being said, we can proceed to the next slide with a question to you all. So what is CI? We're looking for a definition of CI here. Anyone there to guess? It's fine if not. Okay, I'll proceed. So, sorry, say again? Yeah, we're not looking for continuous integration. We're looking for the definition of that. So CI, I guess, thank you very much, is continuous integration. But the definition of that is, you know, as some experts have defined it in a couple of books, the thing is that continuous integration definition can mean different things to different people. It's all a matter of a perspective where you're looking that from what CI means to everyone. But one thing is for sure, continuous is the only thing that's going to be there all the time because we're talking about a never-ending feedback loop, you know, which keeps improving stuff and gives us like visibility over our CI CD processes. So we can move to the next question, which is what is CI like for real this time? And I would like again, if someone dares to guess. It's a black box. Yes, right. Yeah, go on. Yes, running test could be. So again, CI is a list of things like it's a mechanism to, for example, reduce repetitive manual processes or generate deployable software at any time and any place. And of course, like, you know, scores, flaky tests and flaky builds and prevent people from, you know, getting paid at 3am in the morning because we don't want to like spend human hours during the night. They're really important for us. So the next slide is about, I mean, if you think that this is complicated, I think not. I think, you know, this happens to this has happened to at least everyone of us, at least like once, I think. So, you know, it starts from testing, building, deploying and then waiting for changes or wait for 3am maintenance windows. You can see here to errors and downtime and panicking. And also when we have resolved all those issues, we all go to LinkedIn after that and say, you know, I'm a troubleshooting expert or like DevOps expert, automation expert. So that's what we do because that's who we are. Yes, true. Exactly. So the next slide is another question. Yeah. So another question, the next slide, what is CI like for real, real this time? And what we're looking for is a single word. If anyone, there's to guess that single word. Pipelines, automation. Alertings. Yes, that's what we're looking for. We're looking for alerting. So CI and alerting serve a common person, a purpose or at least they try to serve a common purpose. So they work closely together as essential components of continuous automated monitoring. So you can see that both of them are practically identifying issues or, you know, we have like continuous system monitoring or like, you know, all those things. And then CI, we're looking at alerting as the left shift of CI basically, which means that if we have affecting alerting within CI ensures that, you know, threshold bridges and like potential problems are going to vanish. And we see I needs to focus on like robust build for new releases. So together, CI and alerting serve a common goal like prompt problem identification, fortifying system, reliability and sustainability. As you can see from the picture, they need to be like holding hands forever because that's what they do. And we'll go to the next slide. So a few things about continuous integration. We already talked about some of them, but like continuous integration is the guard in early stages, like we can detect changes, maintain bills, health and constantly monitor system signals. And like CI is used to catch issues before they breathe really. So if we go to the next slide, we're having alerting next to that, but we're not actually comparing those two. We just want to show you how closely, like how tightly coupled they are together. So alerting is like, you know, is our alerting system, like for later stages, they identify as problems as well, maintenance, allows and monitor system, just like CI does. Just but we should see that alerting is a mechanism to be used just in case CI has something has slipped through CI and we didn't catch it. So when we have alerting in place and CI in place as well, we need to know that there are not two components running in parallel, right? They are like CI lanes that lays the groundwork and then alerting response to threats. So they are like, you know, unstoppable working together to serve the same purpose. So an important thing to remember about alerting is that every time we need to create actionable alerts. So if something slips through CI, we need to know, we need to get the alert, have a runbook, have some documentation, automatically resolve some alerts if we don't think that are important enough to wake up someone in the middle of the night and all that. So where we are now with CI-CD systems and like, what is this whole talk amounted to? So observability so far, as you can see here, is about like, you know, all the all-time classic concepts we know, like from printing here, we have all done this, like in our early stages, I guess, from printing that, we're still doing that? Yeah, okay, we're still doing that. And then from paging the platform team or from having three different platforms, like we can have GitHub or GitHub or Atlassian or Bitbucket, whatever you use and then find a broken test, go from there to your favorite CI vendor and then go from there to Grafana or Data Dog or your favorite visualization tool to try and correlate those errors together. So focusing, as you can see down there, if we focus, if like the sole focus of observability is at the run part of things, this neglects valuable insights from earlier phases like code review or building or testing and like incomplete observability across the CI pipeline leads to limited visibility during, you know, earlier stages. We don't know what happened during the build phase, for example, or the test phase or we have difficulty in root cause analysis or increased mean time to recovery. Gio is going to talk to you more about that and how this is related to Dora metrics and also missed optimization opportunities. Like we know that our CI pipelines take a lot to run, but we don't actually know what to improve if we want to make them sort of make them faster. So next question, typical, this is fine meme. You know, we know we deploy something, everything catches fire, we are happy and what we do basically is that we try to mitigate the fire. But when the observability part of things is so late in the deployment and development and life cycle, I think it's too late. So there was no reason to let it last this long and get this bad. So how we can be more proactive? If we shift our focus a little bit to the left, we can address issues before they escalate and be proactive. We can enhance the efficiency by catching problems early in the process. We can have, we can ensure robustness by focusing on like the integrity of our builds and tests and also be mindful about the cost reduction because this is also a really important topic and minimize expenses associated with post deployment troubleshooting at downtime. So the next slide, if we assume that we have focused our shift left, the other, so you know, things turn the other way around. So instead of having the fire everywhere and then us in the middle like being agnostic of what's happening is the other way around. So we have a lot of time to mitigate the fire. We can actually be proactive and as we prioritize observability earlier in the development process, we are identifying and addressing issues actually before they become fire. We tried many tools. We tried to find the best way to set up like such a system so we can proactively like react to all those problems. So the tool we found easier to use and address like all those issues in CI CD pipeline is open telemetry because it helps us create like standard patterns and some underconventions. Jir is going to talk to you more about that in a little bit. So in the next slide, we're going to show how we use open telemetry to get to exactly this point where it's even if something appears, it's still too early and we can, you know, act and fix the issue before we wake up people in the middle of the night. So stage is yours. Thank you. Can you hear me? Okay. Thank you. So first question. What is open telemetry? Does anyone know what to work with? One? So no one. Okay. A few people. But as a short definition of open telemetry is it's an observability framework which is designed to manage and create telemetry data such as metrics, logs, traces, events, whatever. There is of course a more comprehensive definition of open telemetry which is way longer, way more complex, which you can find on the open telemetry website. For this case, though, our, what I want to focus on here is two bits of definition which is semantic convention and owning your own data. Now, semantic conventions, we can think about them as a standard. Like it's a standard way of naming things, of a standard way of defining attributes for your logs, for your metrics, for your traces. And I mean, we know, we all know this. If we think about semantic conventions as standards, we can divide them by two different areas or we can categorize them by two different, in two different ways. By signal type such as metrics, logs, traces, events, whatever. And by AIA. Now, by AIA means we have telemetry, there are semantic conventions for databases, we have semantic conventions for cloud providers, we have semantic conventions for a lot of different things, really, for log files. Something that is not there yet, though, is semantic conventions for continuous integration and continuous delivery. Now, what is important in my opinion? This is important because, I mean, we use some CI tool, I guess, everyone here uses a different CI tool. But regardless of what we use, we can see that at the end of the day, the data that is behind its CI tool is the same. Regardless of whether someone calls it stage, someone calls it job, someone calls it status or outcome or whatever, the underlying data, such as the job name, the outcome of a CI system is the same. Now, I'm not extremely familiar with every CI system of there, but at some point I was trying to figure out why in our CI we had a test that sometimes was taking three minutes to a test, like a pipeline, that sometimes was taking only three minutes to complete, while some others up to nine or ten, which, I mean, without any code changes. So, like, if you talk about flakiness, yeah, that's part of, like, test failing, failing, other parties, why sometimes they take too long. Easy peasy, I think. I wrote some totally reliable Go code. No? No, okay. It was very good code. So, what was code was doing was getting stuff out of the ground database and pushing it to log a template in here for later analysis. It worked great. Worked perfectly. Now, what happened is that we were able, at the end of the day, even if the code wasn't very good, we were able to at least look at something outside of our CI system. Why this? Because our CI system didn't provide us with the UI, with the tools to query for the data we were looking for. So, we were trying to analyze why something was happening. And our UI wasn't able to do so. So, okay. I share the news with my team. And, I mean, I guess every one of you has been there at some point in your life. They got too excited. Ivana wanted us to have the log data on Elasticsearch. Piotr was, which is not a colleague, wanted to get tracing data from Git action, this is a lot of drone. And, yeah, no, that code was not good enough. I mean, back to the drawing board. What happens now? We need to figure out a way of getting data out of GitHub CI, drone, GitHub actions, whatever. I don't know. What else? Bit bucket. I don't know if they have a CI system. I'm not sure about. And, we need to push it to every database out there, from Graphite, Tempo, Elasticsearch, I don't know, whatever. Yeager. I mean, it sounds like a very silly question. I bet there is no one of us that really uses 10 different databases to match their telemetry data. Also, because then, you know, this is what was going to happen. We had to write code to get data out of every CI system to push it to every other database system. And, I say no, like, I wasn't going to do that. But, this, I like some very important point that's owning your data. When I think about, when I started thinking about owning my data, what I thought about was mostly owning the hardware in which the data was going to be stored. So, like, owning the drive or having it stored on one of my machines. I think that that's not exactly the point we need to make here. I think owning your data means you being able to decide where the data goes, where and how to store the data. We can very, very well be using a cloud database provider to store our data. The important bit is that we own, we know, we decide where the data is going to be stored and we decide and we have the ability to use the data however we want. So, the reason why open telemetry is important and fits very well with the picture is that by defining standards and by defining a specification for which data can be transferred and stored, not stored but transferred, we are able to only take care of the first part of the equation here. We take data out of the systems and then open telemetry is going to take care about inverting and sending to the database we need. What we did was we built an open telemetry collector who, does any of you know what our collector distribution is? A few. Okay, so an open telemetry collector distribution is basically a set of pre-built components. It's a binary that you can run, of course, and you can configure to do things. It consists very reductively of receivers, processors and exporters. Receivers are the components that allow you to get data in, can be like watching at log files, can be, I don't know, even listening on Bluetooth 1.0 and check for things over the air, can be extracting metrics from some running services. There are processors that transfer this data in the format that you need. They add attributes, they modify attributes or remove them. And exporters that send this data out to your database of choice. The thing here is that for those exporters, we didn't write anything. Those are already open source exporters. There are more for elastic search for whatever, like really Jager or you name it. So the only bit we had to do was writing a drone receiver, which was getting data out of drone to pipe into open telemetry and then push it to log in, tempo and prometheus. There are some practical examples. There is a Jenkins plugin that gets traces data out of Jenkins and sends it via the OTLP format. Irokinz brought these other get-up functions that run commands and exports the execution of these commands as trace data. And of course, our own experiment, which is very complete but very well free to take a look at. Now, what is unlocked? What is unlocked for us? As I said, first of all, there were these performance issues with our first test. Second thing, at some point we had this test here that was a bit flaky. Failing sometimes, sometimes not. Of course, worked on my machine, worked on my advanced machine probably. But yeah, we couldn't figure out what to do with it because we would disable it but then when we were going to enable it, if you cannot really reproduce it locally. Now, by getting this data out of our CI and pushing the build logs into our observability system into our log instance, we were able to trace back from the build that you see on the right to the logs for that build, trace back to the first time that failed test in our CI. And from there, if you look down here, this is an attribute that we thought was valuable. We had a build number, which is our unique ID for drone, which then pointed out to the first pull request that introduced that test or that flakiness. With that, we were able to identify what was causing the actual issue, which was a test which was totally unrelated, running a different suite. But turns out that was causing the flakiness. Something else that we were able to do was, so first of all, like one, I don't know, silly thing that we did but we liked, was to create a custom UI within Grafana to mimic sort of like the UI that you have when you look at the output of your system. I mean, there is some value maybe near but the important bit here is that we own the data. We were able to do something which was funny. We spent maybe one day on it. And yeah, it will look good. The second thing, however, is more important. Now, in Grafana, we have a very complex release system. Very complex. We maintain a set of different release branches that need, in theory, should need to be released at an even time. Of course, like everyone, like for everyone, something things breaks. Sometimes things break and you don't know why because you are not looking at it. Sometimes a commit you make in main breaks something else somewhere else because you back ported it but you didn't really test it. What we were able to do with this was keep getting metrics and stats out of our system, out of our builds so that we could be the timeline of our deployment branches. This means that at any given time, we had a single pane of glass to look at what was the status of our release processes so that our release team could just go here and check whether something was broken they needed to act upon before trying to do our release. We also had visibility over the stats over the number of running pipelines or failed pipelines. We can dig into builds. We can do a lot of different things which we didn't feel like they were possible in our CI system UI. What is Unlocks? Really anything. The point here is that we are trying to define standards. We are trying to get into this space. It's a very early stage concept but what it may unlock given that you own your data, it's really up to you. We can talk about Dora metrics so having ways of reducing mean time to restoring services, we can talk about generating red like metrics or requests for your CI. How long did it take? Did something that happened start at the rate of our failing test? The duration started going up because of something we did on our environment. That's something that for us it unlocked quite well. Some other example, caught coverage over time. There is no reason why you cannot export test results as JUnit maybe and then graph them on Grafana and keep in track of your coverage. You can do flakiness detection like we did before. You start seeing that the test started failing at some point. You can detect that. You can create an alert on flakiness. You can trace back to where the test started flaking. At that point we think it's for us and we think it was way easier to identify what was the actual root cause of the flakiness. Then we have security, whatever. Really, the data is yours. You decide what to do with it. Again, all in all, what is unlocked for us at this point, I think there are three different CI systems. We are using three different systems for different reasons. All in all, what is unlocked for us was bringing all the data into there to work with Grafana and to have our production metrics together with the pre-production metrics. Now, what's next? We have formed an open telemetry working group about CI security observability. There are more stuff to come. Join the discussion. If you have your own issue that you want to fix or your own use case that you want to bring up to the group, please join the Cloud Native Computing Foundation Slack channel. This is the proposal for the standard. That's it. If you have any questions. Any questions? Yeah, up there. Anything you need? Yeah, we'll try. What kind of? Sampling. So far, we're not sampling anything. We are collecting a trace for every build that goes through the CI system. For PRs, it's a bit different because we don't want to create bad data, like useless data. It costs money. Data costs money. What we do is we generate data only for pipelines that happen on those branches we care about. So if you make a PR and the PR is okay, it gets merged into main. After it gets merged, we run another pipeline, the same one before the PR, and that one we collect data from. That way, basically, we have the flakiness on our list branch and not on the PRs because in PRs, I mean, it's not flaky. I mean, okay, we can reflect it as in PRs, but maybe we are doing something and it breaks the build, but maybe it's not a vital point. Yeah, if we did that for every branch, basically, we would face cardinality explosion and it's going to be so expensive. So you have to define which branches you're interested in. For example, in Grafana, we have the main branch, which is like the main branch of our repo, and then some version branches for all the different versions that Grafana have, for example. And this is what we're interested in. But again, you're on the data. You can decide to do it all your way. Any other? Yeah. How many flaky tests or else have you found by exporting data from this? What's the case? Yeah, number of unnecessary... Oh, yeah, sorry. Apart from flaky tests, what other metrics can we get, like useful metrics we can get out of that? So I think... No, no, no. What other problems have you encountered? Like you found out that you didn't find what you're just looking at? For example, stack runners. Runners were stuck in unused repositories. We didn't have a way to know that there was a runner running all the time. We're getting timeouts and all that. This one problem. Then another problem is the number of restarts in builds, which is basically related to flaky tests. But there was no way for us to know how many, like for Geo, for example, went and restarted his build because it was problematic, because there was a bug, or because there was an actual issue with runner. It doesn't have to be necessarily code related. So we needed to know how big was the number of the restarts and then try to find the root cause of what caused this, basically. There was also something I want to talk about, maybe, is that we are also able to improve a bit of the performance of our pipeline. And by performance, I mean just allocating more resources. By doing that, we were also maybe able to reduce the cost of the bit because the runner where pipelines were running for shorter, there was less queue. So it's also like improving performance also comes from having the data about how long they take. And also, last thing is that we also had issues where we used extremely powerful runners to build docs, for example. And docs builds took, I don't know, a minute where if the docs build took like five minutes, it was not going to be the end of the world, because there are docs, they're just small changes, really important changes, don't get me wrong, but small. So we could move away from really powerful runners to something smaller just to help with some cost reduction and stuff. Any other questions? Do we have time? Do we have one up there? Do we have time? No? Come join us at the Grafana booth, please. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.76, "text": " Hello everyone. Thank you for having us here. It's our first time here, so please be kind.", "tokens": [50364, 2425, 1518, 13, 1044, 291, 337, 1419, 505, 510, 13, 467, 311, 527, 700, 565, 510, 11, 370, 1767, 312, 733, 13, 51052], "temperature": 0.0, "avg_logprob": -0.3004942293520327, "compression_ratio": 1.4409448818897639, "no_speech_prob": 0.519980251789093}, {"id": 1, "seek": 0, "start": 13.76, "end": 18.76, "text": " It's like for both of us, it's the first time here, so we're a little bit nervous. And we're", "tokens": [51052, 467, 311, 411, 337, 1293, 295, 505, 11, 309, 311, 264, 700, 565, 510, 11, 370, 321, 434, 257, 707, 857, 6296, 13, 400, 321, 434, 51302], "temperature": 0.0, "avg_logprob": -0.3004942293520327, "compression_ratio": 1.4409448818897639, "no_speech_prob": 0.519980251789093}, {"id": 2, "seek": 1876, "start": 18.76, "end": 33.6, "text": " here to talk about practical CI, CID observability with open telemetry. This is the abstract we", "tokens": [50364, 510, 281, 751, 466, 8496, 37777, 11, 383, 2777, 9951, 2310, 365, 1269, 4304, 5537, 627, 13, 639, 307, 264, 12649, 321, 51106], "temperature": 0.0, "avg_logprob": -0.27160818680473, "compression_ratio": 1.4764397905759161, "no_speech_prob": 0.18800662457942963}, {"id": 3, "seek": 1876, "start": 33.6, "end": 37.84, "text": " have submitted and of course we don't expect you to go through a whole thing because it's", "tokens": [51106, 362, 14405, 293, 295, 1164, 321, 500, 380, 2066, 291, 281, 352, 807, 257, 1379, 551, 570, 309, 311, 51318], "temperature": 0.0, "avg_logprob": -0.27160818680473, "compression_ratio": 1.4764397905759161, "no_speech_prob": 0.18800662457942963}, {"id": 4, "seek": 1876, "start": 37.84, "end": 44.68000000000001, "text": " just enormous. But like if we could abstract the abstract in a way, this talk is about enhancing", "tokens": [51318, 445, 11322, 13, 583, 411, 498, 321, 727, 12649, 264, 12649, 294, 257, 636, 11, 341, 751, 307, 466, 36579, 51660], "temperature": 0.0, "avg_logprob": -0.27160818680473, "compression_ratio": 1.4764397905759161, "no_speech_prob": 0.18800662457942963}, {"id": 5, "seek": 4468, "start": 44.68, "end": 50.16, "text": " your pipeline's ability and performance by bringing observability to every stage of software", "tokens": [50364, 428, 15517, 311, 3485, 293, 3389, 538, 5062, 9951, 2310, 281, 633, 3233, 295, 4722, 50638], "temperature": 0.0, "avg_logprob": -0.21157241904217264, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.20405231416225433}, {"id": 6, "seek": 4468, "start": 50.16, "end": 56.28, "text": " delivery. So we're going to answer two questions like how we can identify flakiness and bottlenecks", "tokens": [50638, 8982, 13, 407, 321, 434, 516, 281, 1867, 732, 1651, 411, 577, 321, 393, 5876, 932, 514, 1324, 293, 44641, 2761, 50944], "temperature": 0.0, "avg_logprob": -0.21157241904217264, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.20405231416225433}, {"id": 7, "seek": 4468, "start": 56.28, "end": 64.36, "text": " during our CI-CB process and envision a future of effortless visibility. And again, we're going to", "tokens": [50944, 1830, 527, 37777, 12, 34, 33, 1399, 293, 24739, 257, 2027, 295, 4630, 1832, 19883, 13, 400, 797, 11, 321, 434, 516, 281, 51348], "temperature": 0.0, "avg_logprob": -0.21157241904217264, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.20405231416225433}, {"id": 8, "seek": 4468, "start": 64.36, "end": 72.16, "text": " talk about what's the role of open telemetry on that and also how, with the role it plays in", "tokens": [51348, 751, 466, 437, 311, 264, 3090, 295, 1269, 4304, 5537, 627, 322, 300, 293, 611, 577, 11, 365, 264, 3090, 309, 5749, 294, 51738], "temperature": 0.0, "avg_logprob": -0.21157241904217264, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.20405231416225433}, {"id": 9, "seek": 7216, "start": 72.16, "end": 78.32, "text": " shaping CI-CB's future and explore all the challenges and opportunities ahead. So if I go to the", "tokens": [50364, 25945, 37777, 12, 34, 33, 311, 2027, 293, 6839, 439, 264, 4759, 293, 4786, 2286, 13, 407, 498, 286, 352, 281, 264, 50672], "temperature": 0.0, "avg_logprob": -0.24305280049641928, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.06336271017789841}, {"id": 10, "seek": 7216, "start": 78.32, "end": 84.2, "text": " next slide, a little bit about us, I'm Dimitris, a software engineer at Grafana Labs at the", "tokens": [50672, 958, 4137, 11, 257, 707, 857, 466, 505, 11, 286, 478, 20975, 270, 5714, 11, 257, 4722, 11403, 412, 8985, 69, 2095, 40047, 412, 264, 50966], "temperature": 0.0, "avg_logprob": -0.24305280049641928, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.06336271017789841}, {"id": 11, "seek": 7216, "start": 84.2, "end": 92.6, "text": " Platform Productivity Squad. And I'm Giordano, Gio for France. I'm a software engineer for the", "tokens": [50966, 28707, 22005, 4253, 26596, 13, 400, 286, 478, 15334, 765, 3730, 11, 460, 1004, 337, 6190, 13, 286, 478, 257, 4722, 11403, 337, 264, 51386], "temperature": 0.0, "avg_logprob": -0.24305280049641928, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.06336271017789841}, {"id": 12, "seek": 7216, "start": 92.6, "end": 100.36, "text": " X-Port Squad at Grafana Labs. So this is the agenda. As I said, we're going to start by", "tokens": [51386, 1783, 12, 47, 477, 26596, 412, 8985, 69, 2095, 40047, 13, 407, 341, 307, 264, 9829, 13, 1018, 286, 848, 11, 321, 434, 516, 281, 722, 538, 51774], "temperature": 0.0, "avg_logprob": -0.24305280049641928, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.06336271017789841}, {"id": 13, "seek": 10036, "start": 100.4, "end": 106.48, "text": " defining what CI really is and then talk about current issues we have with CI-CB systems. And then", "tokens": [50366, 17827, 437, 37777, 534, 307, 293, 550, 751, 466, 2190, 2663, 321, 362, 365, 37777, 12, 34, 33, 3652, 13, 400, 550, 50670], "temperature": 0.0, "avg_logprob": -0.1295322821690486, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.02799653634428978}, {"id": 14, "seek": 10036, "start": 106.48, "end": 113.12, "text": " we're going to do a small intro to open telemetry and how we use it really, why it is important to", "tokens": [50670, 321, 434, 516, 281, 360, 257, 1359, 12897, 281, 1269, 4304, 5537, 627, 293, 577, 321, 764, 309, 534, 11, 983, 309, 307, 1021, 281, 51002], "temperature": 0.0, "avg_logprob": -0.1295322821690486, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.02799653634428978}, {"id": 15, "seek": 10036, "start": 113.12, "end": 118.6, "text": " own our data, and then practical use cases where we are and what's next. So all that being said,", "tokens": [51002, 1065, 527, 1412, 11, 293, 550, 8496, 764, 3331, 689, 321, 366, 293, 437, 311, 958, 13, 407, 439, 300, 885, 848, 11, 51276], "temperature": 0.0, "avg_logprob": -0.1295322821690486, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.02799653634428978}, {"id": 16, "seek": 10036, "start": 118.6, "end": 124.88, "text": " we can proceed to the next slide with a question to you all. So what is CI? We're looking for a", "tokens": [51276, 321, 393, 8991, 281, 264, 958, 4137, 365, 257, 1168, 281, 291, 439, 13, 407, 437, 307, 37777, 30, 492, 434, 1237, 337, 257, 51590], "temperature": 0.0, "avg_logprob": -0.1295322821690486, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.02799653634428978}, {"id": 17, "seek": 12488, "start": 125.52, "end": 138.32, "text": " definition of CI here. Anyone there to guess? It's fine if not. Okay, I'll proceed. So, sorry,", "tokens": [50396, 7123, 295, 37777, 510, 13, 14643, 456, 281, 2041, 30, 467, 311, 2489, 498, 406, 13, 1033, 11, 286, 603, 8991, 13, 407, 11, 2597, 11, 51036], "temperature": 0.0, "avg_logprob": -0.25651031494140625, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.029010673984885216}, {"id": 18, "seek": 12488, "start": 138.32, "end": 143.44, "text": " say again? Yeah, we're not looking for continuous integration. We're looking for the definition of", "tokens": [51036, 584, 797, 30, 865, 11, 321, 434, 406, 1237, 337, 10957, 10980, 13, 492, 434, 1237, 337, 264, 7123, 295, 51292], "temperature": 0.0, "avg_logprob": -0.25651031494140625, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.029010673984885216}, {"id": 19, "seek": 12488, "start": 143.44, "end": 149.12, "text": " that. So CI, I guess, thank you very much, is continuous integration. But the definition of", "tokens": [51292, 300, 13, 407, 37777, 11, 286, 2041, 11, 1309, 291, 588, 709, 11, 307, 10957, 10980, 13, 583, 264, 7123, 295, 51576], "temperature": 0.0, "avg_logprob": -0.25651031494140625, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.029010673984885216}, {"id": 20, "seek": 14912, "start": 149.20000000000002, "end": 157.08, "text": " that is, you know, as some experts have defined it in a couple of books, the thing is that continuous", "tokens": [50368, 300, 307, 11, 291, 458, 11, 382, 512, 8572, 362, 7642, 309, 294, 257, 1916, 295, 3642, 11, 264, 551, 307, 300, 10957, 50762], "temperature": 0.0, "avg_logprob": -0.19169158935546876, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.04343174025416374}, {"id": 21, "seek": 14912, "start": 157.08, "end": 163.0, "text": " integration definition can mean different things to different people. It's all a matter of a", "tokens": [50762, 10980, 7123, 393, 914, 819, 721, 281, 819, 561, 13, 467, 311, 439, 257, 1871, 295, 257, 51058], "temperature": 0.0, "avg_logprob": -0.19169158935546876, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.04343174025416374}, {"id": 22, "seek": 14912, "start": 163.0, "end": 170.0, "text": " perspective where you're looking that from what CI means to everyone. But one thing is for sure,", "tokens": [51058, 4585, 689, 291, 434, 1237, 300, 490, 437, 37777, 1355, 281, 1518, 13, 583, 472, 551, 307, 337, 988, 11, 51408], "temperature": 0.0, "avg_logprob": -0.19169158935546876, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.04343174025416374}, {"id": 23, "seek": 14912, "start": 170.0, "end": 175.48000000000002, "text": " continuous is the only thing that's going to be there all the time because we're talking about a", "tokens": [51408, 10957, 307, 264, 787, 551, 300, 311, 516, 281, 312, 456, 439, 264, 565, 570, 321, 434, 1417, 466, 257, 51682], "temperature": 0.0, "avg_logprob": -0.19169158935546876, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.04343174025416374}, {"id": 24, "seek": 17548, "start": 175.84, "end": 183.23999999999998, "text": " never-ending feedback loop, you know, which keeps improving stuff and gives us like visibility", "tokens": [50382, 1128, 12, 2029, 5824, 6367, 11, 291, 458, 11, 597, 5965, 11470, 1507, 293, 2709, 505, 411, 19883, 50752], "temperature": 0.0, "avg_logprob": -0.2939224745097913, "compression_ratio": 1.4097560975609755, "no_speech_prob": 0.012807329185307026}, {"id": 25, "seek": 17548, "start": 183.23999999999998, "end": 191.2, "text": " over our CI CD processes. So we can move to the next question, which is what is CI like for real", "tokens": [50752, 670, 527, 37777, 6743, 7555, 13, 407, 321, 393, 1286, 281, 264, 958, 1168, 11, 597, 307, 437, 307, 37777, 411, 337, 957, 51150], "temperature": 0.0, "avg_logprob": -0.2939224745097913, "compression_ratio": 1.4097560975609755, "no_speech_prob": 0.012807329185307026}, {"id": 26, "seek": 17548, "start": 191.2, "end": 200.12, "text": " this time? And I would like again, if someone dares to guess. It's a black box. Yes, right. Yeah,", "tokens": [51150, 341, 565, 30, 400, 286, 576, 411, 797, 11, 498, 1580, 50213, 281, 2041, 13, 467, 311, 257, 2211, 2424, 13, 1079, 11, 558, 13, 865, 11, 51596], "temperature": 0.0, "avg_logprob": -0.2939224745097913, "compression_ratio": 1.4097560975609755, "no_speech_prob": 0.012807329185307026}, {"id": 27, "seek": 20012, "start": 200.44, "end": 208.92000000000002, "text": " go on. Yes, running test could be. So again, CI is a list of things like it's a mechanism to,", "tokens": [50380, 352, 322, 13, 1079, 11, 2614, 1500, 727, 312, 13, 407, 797, 11, 37777, 307, 257, 1329, 295, 721, 411, 309, 311, 257, 7513, 281, 11, 50804], "temperature": 0.0, "avg_logprob": -0.17724029463951033, "compression_ratio": 1.544, "no_speech_prob": 0.026896197348833084}, {"id": 28, "seek": 20012, "start": 208.92000000000002, "end": 215.0, "text": " for example, reduce repetitive manual processes or generate deployable software at any time and", "tokens": [50804, 337, 1365, 11, 5407, 29404, 9688, 7555, 420, 8460, 7274, 712, 4722, 412, 604, 565, 293, 51108], "temperature": 0.0, "avg_logprob": -0.17724029463951033, "compression_ratio": 1.544, "no_speech_prob": 0.026896197348833084}, {"id": 29, "seek": 20012, "start": 215.0, "end": 220.44, "text": " any place. And of course, like, you know, scores, flaky tests and flaky builds and prevent people", "tokens": [51108, 604, 1081, 13, 400, 295, 1164, 11, 411, 11, 291, 458, 11, 13444, 11, 932, 15681, 6921, 293, 932, 15681, 15182, 293, 4871, 561, 51380], "temperature": 0.0, "avg_logprob": -0.17724029463951033, "compression_ratio": 1.544, "no_speech_prob": 0.026896197348833084}, {"id": 30, "seek": 20012, "start": 220.44, "end": 225.64000000000001, "text": " from, you know, getting paid at 3am in the morning because we don't want to like spend human hours", "tokens": [51380, 490, 11, 291, 458, 11, 1242, 4835, 412, 805, 335, 294, 264, 2446, 570, 321, 500, 380, 528, 281, 411, 3496, 1952, 2496, 51640], "temperature": 0.0, "avg_logprob": -0.17724029463951033, "compression_ratio": 1.544, "no_speech_prob": 0.026896197348833084}, {"id": 31, "seek": 22564, "start": 225.72, "end": 233.0, "text": " during the night. They're really important for us. So the next slide is about, I mean, if you think", "tokens": [50368, 1830, 264, 1818, 13, 814, 434, 534, 1021, 337, 505, 13, 407, 264, 958, 4137, 307, 466, 11, 286, 914, 11, 498, 291, 519, 50732], "temperature": 0.0, "avg_logprob": -0.11724891662597656, "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.047322798520326614}, {"id": 32, "seek": 22564, "start": 233.0, "end": 237.55999999999997, "text": " that this is complicated, I think not. I think, you know, this happens to this has happened to at", "tokens": [50732, 300, 341, 307, 6179, 11, 286, 519, 406, 13, 286, 519, 11, 291, 458, 11, 341, 2314, 281, 341, 575, 2011, 281, 412, 50960], "temperature": 0.0, "avg_logprob": -0.11724891662597656, "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.047322798520326614}, {"id": 33, "seek": 22564, "start": 237.55999999999997, "end": 243.0, "text": " least everyone of us, at least like once, I think. So, you know, it starts from testing, building,", "tokens": [50960, 1935, 1518, 295, 505, 11, 412, 1935, 411, 1564, 11, 286, 519, 13, 407, 11, 291, 458, 11, 309, 3719, 490, 4997, 11, 2390, 11, 51232], "temperature": 0.0, "avg_logprob": -0.11724891662597656, "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.047322798520326614}, {"id": 34, "seek": 22564, "start": 243.0, "end": 249.0, "text": " deploying and then waiting for changes or wait for 3am maintenance windows. You can see here", "tokens": [51232, 34198, 293, 550, 3806, 337, 2962, 420, 1699, 337, 805, 335, 11258, 9309, 13, 509, 393, 536, 510, 51532], "temperature": 0.0, "avg_logprob": -0.11724891662597656, "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.047322798520326614}, {"id": 35, "seek": 24900, "start": 249.0, "end": 255.24, "text": " to errors and downtime and panicking. And also when we have resolved all those issues, we all go", "tokens": [50364, 281, 13603, 293, 49648, 293, 2462, 10401, 13, 400, 611, 562, 321, 362, 20772, 439, 729, 2663, 11, 321, 439, 352, 50676], "temperature": 0.0, "avg_logprob": -0.19344381332397462, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.062077708542346954}, {"id": 36, "seek": 24900, "start": 255.24, "end": 260.2, "text": " to LinkedIn after that and say, you know, I'm a troubleshooting expert or like DevOps expert,", "tokens": [50676, 281, 20657, 934, 300, 293, 584, 11, 291, 458, 11, 286, 478, 257, 15379, 47011, 5844, 420, 411, 43051, 5844, 11, 50924], "temperature": 0.0, "avg_logprob": -0.19344381332397462, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.062077708542346954}, {"id": 37, "seek": 24900, "start": 260.2, "end": 268.12, "text": " automation expert. So that's what we do because that's who we are. Yes, true. Exactly. So the next", "tokens": [50924, 17769, 5844, 13, 407, 300, 311, 437, 321, 360, 570, 300, 311, 567, 321, 366, 13, 1079, 11, 2074, 13, 7587, 13, 407, 264, 958, 51320], "temperature": 0.0, "avg_logprob": -0.19344381332397462, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.062077708542346954}, {"id": 38, "seek": 24900, "start": 268.12, "end": 275.16, "text": " slide is another question. Yeah. So another question, the next slide, what is CI like for real, real", "tokens": [51320, 4137, 307, 1071, 1168, 13, 865, 13, 407, 1071, 1168, 11, 264, 958, 4137, 11, 437, 307, 37777, 411, 337, 957, 11, 957, 51672], "temperature": 0.0, "avg_logprob": -0.19344381332397462, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.062077708542346954}, {"id": 39, "seek": 27516, "start": 275.24, "end": 281.88000000000005, "text": " this time? And what we're looking for is a single word. If anyone, there's to guess that single word.", "tokens": [50368, 341, 565, 30, 400, 437, 321, 434, 1237, 337, 307, 257, 2167, 1349, 13, 759, 2878, 11, 456, 311, 281, 2041, 300, 2167, 1349, 13, 50700], "temperature": 0.0, "avg_logprob": -0.19457502568021734, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.02217109128832817}, {"id": 40, "seek": 27516, "start": 283.64000000000004, "end": 288.36, "text": " Pipelines, automation. Alertings. Yes, that's what we're looking for. We're looking for", "tokens": [50788, 35396, 9173, 11, 17769, 13, 44939, 1109, 13, 1079, 11, 300, 311, 437, 321, 434, 1237, 337, 13, 492, 434, 1237, 337, 51024], "temperature": 0.0, "avg_logprob": -0.19457502568021734, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.02217109128832817}, {"id": 41, "seek": 27516, "start": 288.36, "end": 295.8, "text": " alerting. So CI and alerting serve a common person, a purpose or at least they try to serve a common", "tokens": [51024, 419, 27187, 13, 407, 37777, 293, 419, 27187, 4596, 257, 2689, 954, 11, 257, 4334, 420, 412, 1935, 436, 853, 281, 4596, 257, 2689, 51396], "temperature": 0.0, "avg_logprob": -0.19457502568021734, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.02217109128832817}, {"id": 42, "seek": 27516, "start": 295.8, "end": 301.40000000000003, "text": " purpose. So they work closely together as essential components of continuous automated", "tokens": [51396, 4334, 13, 407, 436, 589, 8185, 1214, 382, 7115, 6677, 295, 10957, 18473, 51676], "temperature": 0.0, "avg_logprob": -0.19457502568021734, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.02217109128832817}, {"id": 43, "seek": 30140, "start": 301.4, "end": 307.4, "text": " monitoring. So you can see that both of them are practically identifying issues or, you know,", "tokens": [50364, 11028, 13, 407, 291, 393, 536, 300, 1293, 295, 552, 366, 15667, 16696, 2663, 420, 11, 291, 458, 11, 50664], "temperature": 0.0, "avg_logprob": -0.13066796695484834, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.06420911103487015}, {"id": 44, "seek": 30140, "start": 307.4, "end": 314.2, "text": " we have like continuous system monitoring or like, you know, all those things. And then CI,", "tokens": [50664, 321, 362, 411, 10957, 1185, 11028, 420, 411, 11, 291, 458, 11, 439, 729, 721, 13, 400, 550, 37777, 11, 51004], "temperature": 0.0, "avg_logprob": -0.13066796695484834, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.06420911103487015}, {"id": 45, "seek": 30140, "start": 314.2, "end": 320.2, "text": " we're looking at alerting as the left shift of CI basically, which means that if we have", "tokens": [51004, 321, 434, 1237, 412, 419, 27187, 382, 264, 1411, 5513, 295, 37777, 1936, 11, 597, 1355, 300, 498, 321, 362, 51304], "temperature": 0.0, "avg_logprob": -0.13066796695484834, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.06420911103487015}, {"id": 46, "seek": 30140, "start": 320.2, "end": 324.59999999999997, "text": " affecting alerting within CI ensures that, you know, threshold bridges and like potential", "tokens": [51304, 17476, 419, 27187, 1951, 37777, 28111, 300, 11, 291, 458, 11, 14678, 21114, 293, 411, 3995, 51524], "temperature": 0.0, "avg_logprob": -0.13066796695484834, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.06420911103487015}, {"id": 47, "seek": 32460, "start": 324.6, "end": 330.92, "text": " problems are going to vanish. And we see I needs to focus on like robust build for new", "tokens": [50364, 2740, 366, 516, 281, 43584, 13, 400, 321, 536, 286, 2203, 281, 1879, 322, 411, 13956, 1322, 337, 777, 50680], "temperature": 0.0, "avg_logprob": -0.1366118493481217, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.033769458532333374}, {"id": 48, "seek": 32460, "start": 330.92, "end": 336.6, "text": " releases. So together, CI and alerting serve a common goal like prompt problem identification,", "tokens": [50680, 16952, 13, 407, 1214, 11, 37777, 293, 419, 27187, 4596, 257, 2689, 3387, 411, 12391, 1154, 22065, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1366118493481217, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.033769458532333374}, {"id": 49, "seek": 32460, "start": 336.6, "end": 342.44, "text": " fortifying system, reliability and sustainability. As you can see from the picture, they need to be", "tokens": [50964, 5009, 5489, 1185, 11, 24550, 293, 16360, 13, 1018, 291, 393, 536, 490, 264, 3036, 11, 436, 643, 281, 312, 51256], "temperature": 0.0, "avg_logprob": -0.1366118493481217, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.033769458532333374}, {"id": 50, "seek": 32460, "start": 342.44, "end": 347.96000000000004, "text": " like holding hands forever because that's what they do. And we'll go to the next slide. So", "tokens": [51256, 411, 5061, 2377, 5680, 570, 300, 311, 437, 436, 360, 13, 400, 321, 603, 352, 281, 264, 958, 4137, 13, 407, 51532], "temperature": 0.0, "avg_logprob": -0.1366118493481217, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.033769458532333374}, {"id": 51, "seek": 32460, "start": 348.84000000000003, "end": 353.64000000000004, "text": " a few things about continuous integration. We already talked about some of them, but like", "tokens": [51576, 257, 1326, 721, 466, 10957, 10980, 13, 492, 1217, 2825, 466, 512, 295, 552, 11, 457, 411, 51816], "temperature": 0.0, "avg_logprob": -0.1366118493481217, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.033769458532333374}, {"id": 52, "seek": 35364, "start": 353.64, "end": 358.84, "text": " continuous integration is the guard in early stages, like we can detect changes, maintain", "tokens": [50364, 10957, 10980, 307, 264, 6290, 294, 2440, 10232, 11, 411, 321, 393, 5531, 2962, 11, 6909, 50624], "temperature": 0.0, "avg_logprob": -0.11903078611506972, "compression_ratio": 1.5622317596566524, "no_speech_prob": 0.0148590337485075}, {"id": 53, "seek": 35364, "start": 358.84, "end": 365.96, "text": " bills, health and constantly monitor system signals. And like CI is used to catch issues", "tokens": [50624, 12433, 11, 1585, 293, 6460, 6002, 1185, 12354, 13, 400, 411, 37777, 307, 1143, 281, 3745, 2663, 50980], "temperature": 0.0, "avg_logprob": -0.11903078611506972, "compression_ratio": 1.5622317596566524, "no_speech_prob": 0.0148590337485075}, {"id": 54, "seek": 35364, "start": 365.96, "end": 370.28, "text": " before they breathe really. So if we go to the next slide, we're having alerting next to that,", "tokens": [50980, 949, 436, 10192, 534, 13, 407, 498, 321, 352, 281, 264, 958, 4137, 11, 321, 434, 1419, 419, 27187, 958, 281, 300, 11, 51196], "temperature": 0.0, "avg_logprob": -0.11903078611506972, "compression_ratio": 1.5622317596566524, "no_speech_prob": 0.0148590337485075}, {"id": 55, "seek": 35364, "start": 370.28, "end": 376.52, "text": " but we're not actually comparing those two. We just want to show you how closely, like how", "tokens": [51196, 457, 321, 434, 406, 767, 15763, 729, 732, 13, 492, 445, 528, 281, 855, 291, 577, 8185, 11, 411, 577, 51508], "temperature": 0.0, "avg_logprob": -0.11903078611506972, "compression_ratio": 1.5622317596566524, "no_speech_prob": 0.0148590337485075}, {"id": 56, "seek": 37652, "start": 376.52, "end": 383.88, "text": " tightly coupled they are together. So alerting is like, you know, is our alerting system,", "tokens": [50364, 21952, 29482, 436, 366, 1214, 13, 407, 419, 27187, 307, 411, 11, 291, 458, 11, 307, 527, 419, 27187, 1185, 11, 50732], "temperature": 0.0, "avg_logprob": -0.19528912483377658, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.08520317077636719}, {"id": 57, "seek": 37652, "start": 383.88, "end": 389.0, "text": " like for later stages, they identify as problems as well, maintenance, allows and monitor system,", "tokens": [50732, 411, 337, 1780, 10232, 11, 436, 5876, 382, 2740, 382, 731, 11, 11258, 11, 4045, 293, 6002, 1185, 11, 50988], "temperature": 0.0, "avg_logprob": -0.19528912483377658, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.08520317077636719}, {"id": 58, "seek": 37652, "start": 389.0, "end": 395.24, "text": " just like CI does. Just but we should see that alerting is a mechanism to be used just in case", "tokens": [50988, 445, 411, 37777, 775, 13, 1449, 457, 321, 820, 536, 300, 419, 27187, 307, 257, 7513, 281, 312, 1143, 445, 294, 1389, 51300], "temperature": 0.0, "avg_logprob": -0.19528912483377658, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.08520317077636719}, {"id": 59, "seek": 37652, "start": 395.24, "end": 402.84, "text": " CI has something has slipped through CI and we didn't catch it. So when we have alerting in place", "tokens": [51300, 37777, 575, 746, 575, 28989, 807, 37777, 293, 321, 994, 380, 3745, 309, 13, 407, 562, 321, 362, 419, 27187, 294, 1081, 51680], "temperature": 0.0, "avg_logprob": -0.19528912483377658, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.08520317077636719}, {"id": 60, "seek": 40284, "start": 402.84, "end": 408.59999999999997, "text": " and CI in place as well, we need to know that there are not two components running in parallel,", "tokens": [50364, 293, 37777, 294, 1081, 382, 731, 11, 321, 643, 281, 458, 300, 456, 366, 406, 732, 6677, 2614, 294, 8952, 11, 50652], "temperature": 0.0, "avg_logprob": -0.13279718881124977, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.015114394016563892}, {"id": 61, "seek": 40284, "start": 408.59999999999997, "end": 415.08, "text": " right? They are like CI lanes that lays the groundwork and then alerting response to threats. So they", "tokens": [50652, 558, 30, 814, 366, 411, 37777, 25397, 300, 32714, 264, 2727, 1902, 293, 550, 419, 27187, 4134, 281, 14909, 13, 407, 436, 50976], "temperature": 0.0, "avg_logprob": -0.13279718881124977, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.015114394016563892}, {"id": 62, "seek": 40284, "start": 415.08, "end": 422.44, "text": " are like, you know, unstoppable working together to serve the same purpose. So an important thing", "tokens": [50976, 366, 411, 11, 291, 458, 11, 48261, 1364, 1214, 281, 4596, 264, 912, 4334, 13, 407, 364, 1021, 551, 51344], "temperature": 0.0, "avg_logprob": -0.13279718881124977, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.015114394016563892}, {"id": 63, "seek": 40284, "start": 422.44, "end": 427.55999999999995, "text": " to remember about alerting is that every time we need to create actionable alerts. So if something", "tokens": [51344, 281, 1604, 466, 419, 27187, 307, 300, 633, 565, 321, 643, 281, 1884, 45098, 28061, 13, 407, 498, 746, 51600], "temperature": 0.0, "avg_logprob": -0.13279718881124977, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.015114394016563892}, {"id": 64, "seek": 42756, "start": 427.56, "end": 433.48, "text": " slips through CI, we need to know, we need to get the alert, have a runbook, have some documentation,", "tokens": [50364, 44690, 807, 37777, 11, 321, 643, 281, 458, 11, 321, 643, 281, 483, 264, 9615, 11, 362, 257, 1190, 2939, 11, 362, 512, 14333, 11, 50660], "temperature": 0.0, "avg_logprob": -0.1427969978851022, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.018999457359313965}, {"id": 65, "seek": 42756, "start": 433.48, "end": 437.8, "text": " automatically resolve some alerts if we don't think that are important enough to wake up someone in", "tokens": [50660, 6772, 14151, 512, 28061, 498, 321, 500, 380, 519, 300, 366, 1021, 1547, 281, 6634, 493, 1580, 294, 50876], "temperature": 0.0, "avg_logprob": -0.1427969978851022, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.018999457359313965}, {"id": 66, "seek": 42756, "start": 437.8, "end": 445.16, "text": " the middle of the night and all that. So where we are now with CI-CD systems and like, what is this", "tokens": [50876, 264, 2808, 295, 264, 1818, 293, 439, 300, 13, 407, 689, 321, 366, 586, 365, 37777, 12, 16508, 3652, 293, 411, 11, 437, 307, 341, 51244], "temperature": 0.0, "avg_logprob": -0.1427969978851022, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.018999457359313965}, {"id": 67, "seek": 42756, "start": 445.16, "end": 451.0, "text": " whole talk amounted to? So observability so far, as you can see here, is about like, you know,", "tokens": [51244, 1379, 751, 2372, 292, 281, 30, 407, 9951, 2310, 370, 1400, 11, 382, 291, 393, 536, 510, 11, 307, 466, 411, 11, 291, 458, 11, 51536], "temperature": 0.0, "avg_logprob": -0.1427969978851022, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.018999457359313965}, {"id": 68, "seek": 45100, "start": 451.64, "end": 458.28, "text": " all the all-time classic concepts we know, like from printing here, we have all done this,", "tokens": [50396, 439, 264, 439, 12, 3766, 7230, 10392, 321, 458, 11, 411, 490, 14699, 510, 11, 321, 362, 439, 1096, 341, 11, 50728], "temperature": 0.0, "avg_logprob": -0.20835868599488563, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.12160786241292953}, {"id": 69, "seek": 45100, "start": 458.28, "end": 462.76, "text": " like in our early stages, I guess, from printing that, we're still doing that? Yeah, okay, we're", "tokens": [50728, 411, 294, 527, 2440, 10232, 11, 286, 2041, 11, 490, 14699, 300, 11, 321, 434, 920, 884, 300, 30, 865, 11, 1392, 11, 321, 434, 50952], "temperature": 0.0, "avg_logprob": -0.20835868599488563, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.12160786241292953}, {"id": 70, "seek": 45100, "start": 462.76, "end": 469.56, "text": " still doing that. And then from paging the platform team or from having three different", "tokens": [50952, 920, 884, 300, 13, 400, 550, 490, 280, 3568, 264, 3663, 1469, 420, 490, 1419, 1045, 819, 51292], "temperature": 0.0, "avg_logprob": -0.20835868599488563, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.12160786241292953}, {"id": 71, "seek": 45100, "start": 470.28, "end": 475.24, "text": " platforms, like we can have GitHub or GitHub or Atlassian or Bitbucket, whatever you use and then", "tokens": [51328, 9473, 11, 411, 321, 393, 362, 23331, 420, 23331, 420, 11000, 640, 952, 420, 9101, 65, 1134, 302, 11, 2035, 291, 764, 293, 550, 51576], "temperature": 0.0, "avg_logprob": -0.20835868599488563, "compression_ratio": 1.7429906542056075, "no_speech_prob": 0.12160786241292953}, {"id": 72, "seek": 47524, "start": 476.04, "end": 481.88, "text": " find a broken test, go from there to your favorite CI vendor and then go from there to Grafana or", "tokens": [50404, 915, 257, 5463, 1500, 11, 352, 490, 456, 281, 428, 2954, 37777, 24321, 293, 550, 352, 490, 456, 281, 8985, 69, 2095, 420, 50696], "temperature": 0.0, "avg_logprob": -0.12098639351981026, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.02927052043378353}, {"id": 73, "seek": 47524, "start": 481.88, "end": 488.36, "text": " Data Dog or your favorite visualization tool to try and correlate those errors together. So", "tokens": [50696, 11888, 13472, 420, 428, 2954, 25801, 2290, 281, 853, 293, 48742, 729, 13603, 1214, 13, 407, 51020], "temperature": 0.0, "avg_logprob": -0.12098639351981026, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.02927052043378353}, {"id": 74, "seek": 47524, "start": 489.32, "end": 495.32, "text": " focusing, as you can see down there, if we focus, if like the sole focus of observability is at the", "tokens": [51068, 8416, 11, 382, 291, 393, 536, 760, 456, 11, 498, 321, 1879, 11, 498, 411, 264, 12321, 1879, 295, 9951, 2310, 307, 412, 264, 51368], "temperature": 0.0, "avg_logprob": -0.12098639351981026, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.02927052043378353}, {"id": 75, "seek": 47524, "start": 495.32, "end": 501.08, "text": " run part of things, this neglects valuable insights from earlier phases like code review or building", "tokens": [51368, 1190, 644, 295, 721, 11, 341, 17745, 82, 8263, 14310, 490, 3071, 18764, 411, 3089, 3131, 420, 2390, 51656], "temperature": 0.0, "avg_logprob": -0.12098639351981026, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.02927052043378353}, {"id": 76, "seek": 50108, "start": 501.08, "end": 508.35999999999996, "text": " or testing and like incomplete observability across the CI pipeline leads to limited visibility", "tokens": [50364, 420, 4997, 293, 411, 31709, 9951, 2310, 2108, 264, 37777, 15517, 6689, 281, 5567, 19883, 50728], "temperature": 0.0, "avg_logprob": -0.1332603347635715, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.017746886238455772}, {"id": 77, "seek": 50108, "start": 508.35999999999996, "end": 513.0, "text": " during, you know, earlier stages. We don't know what happened during the build phase, for example,", "tokens": [50728, 1830, 11, 291, 458, 11, 3071, 10232, 13, 492, 500, 380, 458, 437, 2011, 1830, 264, 1322, 5574, 11, 337, 1365, 11, 50960], "temperature": 0.0, "avg_logprob": -0.1332603347635715, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.017746886238455772}, {"id": 78, "seek": 50108, "start": 513.0, "end": 519.4, "text": " or the test phase or we have difficulty in root cause analysis or increased mean time to recovery.", "tokens": [50960, 420, 264, 1500, 5574, 420, 321, 362, 10360, 294, 5593, 3082, 5215, 420, 6505, 914, 565, 281, 8597, 13, 51280], "temperature": 0.0, "avg_logprob": -0.1332603347635715, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.017746886238455772}, {"id": 79, "seek": 50108, "start": 519.4, "end": 523.24, "text": " Gio is going to talk to you more about that and how this is related to Dora metrics and", "tokens": [51280, 460, 1004, 307, 516, 281, 751, 281, 291, 544, 466, 300, 293, 577, 341, 307, 4077, 281, 413, 3252, 16367, 293, 51472], "temperature": 0.0, "avg_logprob": -0.1332603347635715, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.017746886238455772}, {"id": 80, "seek": 50108, "start": 523.24, "end": 528.28, "text": " also missed optimization opportunities. Like we know that our CI pipelines take a lot to run,", "tokens": [51472, 611, 6721, 19618, 4786, 13, 1743, 321, 458, 300, 527, 37777, 40168, 747, 257, 688, 281, 1190, 11, 51724], "temperature": 0.0, "avg_logprob": -0.1332603347635715, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.017746886238455772}, {"id": 81, "seek": 52828, "start": 528.36, "end": 534.8399999999999, "text": " but we don't actually know what to improve if we want to make them sort of make them faster.", "tokens": [50368, 457, 321, 500, 380, 767, 458, 437, 281, 3470, 498, 321, 528, 281, 652, 552, 1333, 295, 652, 552, 4663, 13, 50692], "temperature": 0.0, "avg_logprob": -0.13360634278715328, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.009792842902243137}, {"id": 82, "seek": 52828, "start": 534.8399999999999, "end": 541.3199999999999, "text": " So next question, typical, this is fine meme. You know, we know we deploy something,", "tokens": [50692, 407, 958, 1168, 11, 7476, 11, 341, 307, 2489, 21701, 13, 509, 458, 11, 321, 458, 321, 7274, 746, 11, 51016], "temperature": 0.0, "avg_logprob": -0.13360634278715328, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.009792842902243137}, {"id": 83, "seek": 52828, "start": 541.3199999999999, "end": 547.4, "text": " everything catches fire, we are happy and what we do basically is that we try to mitigate the fire.", "tokens": [51016, 1203, 25496, 2610, 11, 321, 366, 2055, 293, 437, 321, 360, 1936, 307, 300, 321, 853, 281, 27336, 264, 2610, 13, 51320], "temperature": 0.0, "avg_logprob": -0.13360634278715328, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.009792842902243137}, {"id": 84, "seek": 52828, "start": 547.4, "end": 553.4, "text": " But when the observability part of things is so late in the deployment and development and", "tokens": [51320, 583, 562, 264, 9951, 2310, 644, 295, 721, 307, 370, 3469, 294, 264, 19317, 293, 3250, 293, 51620], "temperature": 0.0, "avg_logprob": -0.13360634278715328, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.009792842902243137}, {"id": 85, "seek": 55340, "start": 553.9599999999999, "end": 560.92, "text": " life cycle, I think it's too late. So there was no reason to let it last this long and get this bad.", "tokens": [50392, 993, 6586, 11, 286, 519, 309, 311, 886, 3469, 13, 407, 456, 390, 572, 1778, 281, 718, 309, 1036, 341, 938, 293, 483, 341, 1578, 13, 50740], "temperature": 0.0, "avg_logprob": -0.10106300078716475, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0321454256772995}, {"id": 86, "seek": 55340, "start": 560.92, "end": 569.16, "text": " So how we can be more proactive? If we shift our focus a little bit to the left, we can address", "tokens": [50740, 407, 577, 321, 393, 312, 544, 28028, 30, 759, 321, 5513, 527, 1879, 257, 707, 857, 281, 264, 1411, 11, 321, 393, 2985, 51152], "temperature": 0.0, "avg_logprob": -0.10106300078716475, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0321454256772995}, {"id": 87, "seek": 55340, "start": 569.16, "end": 575.0799999999999, "text": " issues before they escalate and be proactive. We can enhance the efficiency by catching problems", "tokens": [51152, 2663, 949, 436, 17871, 473, 293, 312, 28028, 13, 492, 393, 11985, 264, 10493, 538, 16124, 2740, 51448], "temperature": 0.0, "avg_logprob": -0.10106300078716475, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0321454256772995}, {"id": 88, "seek": 55340, "start": 575.0799999999999, "end": 580.6, "text": " early in the process. We can have, we can ensure robustness by focusing on like the integrity of", "tokens": [51448, 2440, 294, 264, 1399, 13, 492, 393, 362, 11, 321, 393, 5586, 13956, 1287, 538, 8416, 322, 411, 264, 16000, 295, 51724], "temperature": 0.0, "avg_logprob": -0.10106300078716475, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0321454256772995}, {"id": 89, "seek": 58060, "start": 580.6, "end": 585.96, "text": " our builds and tests and also be mindful about the cost reduction because this is also a really", "tokens": [50364, 527, 15182, 293, 6921, 293, 611, 312, 14618, 466, 264, 2063, 11004, 570, 341, 307, 611, 257, 534, 50632], "temperature": 0.0, "avg_logprob": -0.13445890517461867, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.014634593389928341}, {"id": 90, "seek": 58060, "start": 585.96, "end": 592.0400000000001, "text": " important topic and minimize expenses associated with post deployment troubleshooting at downtime.", "tokens": [50632, 1021, 4829, 293, 17522, 15506, 6615, 365, 2183, 19317, 15379, 47011, 412, 49648, 13, 50936], "temperature": 0.0, "avg_logprob": -0.13445890517461867, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.014634593389928341}, {"id": 91, "seek": 58060, "start": 592.84, "end": 600.9200000000001, "text": " So the next slide, if we assume that we have focused our shift left, the other, so you know,", "tokens": [50976, 407, 264, 958, 4137, 11, 498, 321, 6552, 300, 321, 362, 5178, 527, 5513, 1411, 11, 264, 661, 11, 370, 291, 458, 11, 51380], "temperature": 0.0, "avg_logprob": -0.13445890517461867, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.014634593389928341}, {"id": 92, "seek": 58060, "start": 600.9200000000001, "end": 605.88, "text": " things turn the other way around. So instead of having the fire everywhere and then us in the", "tokens": [51380, 721, 1261, 264, 661, 636, 926, 13, 407, 2602, 295, 1419, 264, 2610, 5315, 293, 550, 505, 294, 264, 51628], "temperature": 0.0, "avg_logprob": -0.13445890517461867, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.014634593389928341}, {"id": 93, "seek": 60588, "start": 605.88, "end": 611.0, "text": " middle like being agnostic of what's happening is the other way around. So we have a lot of time", "tokens": [50364, 2808, 411, 885, 623, 77, 19634, 295, 437, 311, 2737, 307, 264, 661, 636, 926, 13, 407, 321, 362, 257, 688, 295, 565, 50620], "temperature": 0.0, "avg_logprob": -0.08040148488591227, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.04232468828558922}, {"id": 94, "seek": 60588, "start": 611.0, "end": 616.36, "text": " to mitigate the fire. We can actually be proactive and as we prioritize observability earlier in", "tokens": [50620, 281, 27336, 264, 2610, 13, 492, 393, 767, 312, 28028, 293, 382, 321, 25164, 9951, 2310, 3071, 294, 50888], "temperature": 0.0, "avg_logprob": -0.08040148488591227, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.04232468828558922}, {"id": 95, "seek": 60588, "start": 616.36, "end": 622.2, "text": " the development process, we are identifying and addressing issues actually before they become", "tokens": [50888, 264, 3250, 1399, 11, 321, 366, 16696, 293, 14329, 2663, 767, 949, 436, 1813, 51180], "temperature": 0.0, "avg_logprob": -0.08040148488591227, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.04232468828558922}, {"id": 96, "seek": 60588, "start": 622.2, "end": 628.36, "text": " fire. We tried many tools. We tried to find the best way to set up like such a system so we can", "tokens": [51180, 2610, 13, 492, 3031, 867, 3873, 13, 492, 3031, 281, 915, 264, 1151, 636, 281, 992, 493, 411, 1270, 257, 1185, 370, 321, 393, 51488], "temperature": 0.0, "avg_logprob": -0.08040148488591227, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.04232468828558922}, {"id": 97, "seek": 62836, "start": 628.36, "end": 636.12, "text": " proactively like react to all those problems. So the tool we found easier to use and address like", "tokens": [50364, 447, 45679, 411, 4515, 281, 439, 729, 2740, 13, 407, 264, 2290, 321, 1352, 3571, 281, 764, 293, 2985, 411, 50752], "temperature": 0.0, "avg_logprob": -0.1594764788945516, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.03494618088006973}, {"id": 98, "seek": 62836, "start": 636.12, "end": 642.04, "text": " all those issues in CI CD pipeline is open telemetry because it helps us create like standard", "tokens": [50752, 439, 729, 2663, 294, 37777, 6743, 15517, 307, 1269, 4304, 5537, 627, 570, 309, 3665, 505, 1884, 411, 3832, 51048], "temperature": 0.0, "avg_logprob": -0.1594764788945516, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.03494618088006973}, {"id": 99, "seek": 62836, "start": 642.04, "end": 646.36, "text": " patterns and some underconventions. Jir is going to talk to you more about that in a little bit.", "tokens": [51048, 8294, 293, 512, 833, 1671, 15051, 13, 508, 347, 307, 516, 281, 751, 281, 291, 544, 466, 300, 294, 257, 707, 857, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1594764788945516, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.03494618088006973}, {"id": 100, "seek": 62836, "start": 646.36, "end": 653.5600000000001, "text": " So in the next slide, we're going to show how we use open telemetry to get to exactly this point", "tokens": [51264, 407, 294, 264, 958, 4137, 11, 321, 434, 516, 281, 855, 577, 321, 764, 1269, 4304, 5537, 627, 281, 483, 281, 2293, 341, 935, 51624], "temperature": 0.0, "avg_logprob": -0.1594764788945516, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.03494618088006973}, {"id": 101, "seek": 65356, "start": 653.56, "end": 661.0799999999999, "text": " where it's even if something appears, it's still too early and we can, you know, act and fix the", "tokens": [50364, 689, 309, 311, 754, 498, 746, 7038, 11, 309, 311, 920, 886, 2440, 293, 321, 393, 11, 291, 458, 11, 605, 293, 3191, 264, 50740], "temperature": 0.0, "avg_logprob": -0.1571422088436964, "compression_ratio": 1.4974874371859297, "no_speech_prob": 0.04181400686502457}, {"id": 102, "seek": 65356, "start": 661.0799999999999, "end": 667.4799999999999, "text": " issue before we wake up people in the middle of the night. So stage is yours. Thank you. Can you hear", "tokens": [50740, 2734, 949, 321, 6634, 493, 561, 294, 264, 2808, 295, 264, 1818, 13, 407, 3233, 307, 6342, 13, 1044, 291, 13, 1664, 291, 1568, 51060], "temperature": 0.0, "avg_logprob": -0.1571422088436964, "compression_ratio": 1.4974874371859297, "no_speech_prob": 0.04181400686502457}, {"id": 103, "seek": 65356, "start": 667.4799999999999, "end": 675.16, "text": " me? Okay. Thank you. So first question. What is open telemetry? Does anyone know what to work with?", "tokens": [51060, 385, 30, 1033, 13, 1044, 291, 13, 407, 700, 1168, 13, 708, 307, 1269, 4304, 5537, 627, 30, 4402, 2878, 458, 437, 281, 589, 365, 30, 51444], "temperature": 0.0, "avg_logprob": -0.1571422088436964, "compression_ratio": 1.4974874371859297, "no_speech_prob": 0.04181400686502457}, {"id": 104, "seek": 67516, "start": 675.56, "end": 687.16, "text": " One? So no one. Okay. A few people. But as a short definition of open telemetry is it's an observability", "tokens": [50384, 1485, 30, 407, 572, 472, 13, 1033, 13, 316, 1326, 561, 13, 583, 382, 257, 2099, 7123, 295, 1269, 4304, 5537, 627, 307, 309, 311, 364, 9951, 2310, 50964], "temperature": 0.0, "avg_logprob": -0.19882535934448242, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.04808700084686279}, {"id": 105, "seek": 67516, "start": 687.16, "end": 693.8, "text": " framework which is designed to manage and create telemetry data such as metrics, logs, traces,", "tokens": [50964, 8388, 597, 307, 4761, 281, 3067, 293, 1884, 4304, 5537, 627, 1412, 1270, 382, 16367, 11, 20820, 11, 26076, 11, 51296], "temperature": 0.0, "avg_logprob": -0.19882535934448242, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.04808700084686279}, {"id": 106, "seek": 67516, "start": 693.8, "end": 700.8399999999999, "text": " events, whatever. There is of course a more comprehensive definition of open telemetry which", "tokens": [51296, 3931, 11, 2035, 13, 821, 307, 295, 1164, 257, 544, 13914, 7123, 295, 1269, 4304, 5537, 627, 597, 51648], "temperature": 0.0, "avg_logprob": -0.19882535934448242, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.04808700084686279}, {"id": 107, "seek": 70084, "start": 700.84, "end": 707.08, "text": " is way longer, way more complex, which you can find on the open telemetry website. For this case,", "tokens": [50364, 307, 636, 2854, 11, 636, 544, 3997, 11, 597, 291, 393, 915, 322, 264, 1269, 4304, 5537, 627, 3144, 13, 1171, 341, 1389, 11, 50676], "temperature": 0.0, "avg_logprob": -0.14532881667933514, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.025876959785819054}, {"id": 108, "seek": 70084, "start": 707.08, "end": 713.72, "text": " though, our, what I want to focus on here is two bits of definition which is semantic convention", "tokens": [50676, 1673, 11, 527, 11, 437, 286, 528, 281, 1879, 322, 510, 307, 732, 9239, 295, 7123, 597, 307, 47982, 10286, 51008], "temperature": 0.0, "avg_logprob": -0.14532881667933514, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.025876959785819054}, {"id": 109, "seek": 70084, "start": 714.36, "end": 721.4, "text": " and owning your own data. Now, semantic conventions, we can think about them as a standard. Like it's", "tokens": [51040, 293, 29820, 428, 1065, 1412, 13, 823, 11, 47982, 33520, 11, 321, 393, 519, 466, 552, 382, 257, 3832, 13, 1743, 309, 311, 51392], "temperature": 0.0, "avg_logprob": -0.14532881667933514, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.025876959785819054}, {"id": 110, "seek": 70084, "start": 721.4, "end": 726.9200000000001, "text": " a standard way of naming things, of a standard way of defining attributes for your logs, for your", "tokens": [51392, 257, 3832, 636, 295, 25290, 721, 11, 295, 257, 3832, 636, 295, 17827, 17212, 337, 428, 20820, 11, 337, 428, 51668], "temperature": 0.0, "avg_logprob": -0.14532881667933514, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.025876959785819054}, {"id": 111, "seek": 72692, "start": 726.92, "end": 736.68, "text": " metrics, for your traces. And I mean, we know, we all know this. If we think about semantic", "tokens": [50364, 16367, 11, 337, 428, 26076, 13, 400, 286, 914, 11, 321, 458, 11, 321, 439, 458, 341, 13, 759, 321, 519, 466, 47982, 50852], "temperature": 0.0, "avg_logprob": -0.17814335757738922, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.03715057671070099}, {"id": 112, "seek": 72692, "start": 736.68, "end": 744.04, "text": " conventions as standards, we can divide them by two different areas or we can categorize them by two", "tokens": [50852, 33520, 382, 7787, 11, 321, 393, 9845, 552, 538, 732, 819, 3179, 420, 321, 393, 19250, 1125, 552, 538, 732, 51220], "temperature": 0.0, "avg_logprob": -0.17814335757738922, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.03715057671070099}, {"id": 113, "seek": 72692, "start": 744.04, "end": 750.76, "text": " different, in two different ways. By signal type such as metrics, logs, traces, events, whatever.", "tokens": [51220, 819, 11, 294, 732, 819, 2098, 13, 3146, 6358, 2010, 1270, 382, 16367, 11, 20820, 11, 26076, 11, 3931, 11, 2035, 13, 51556], "temperature": 0.0, "avg_logprob": -0.17814335757738922, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.03715057671070099}, {"id": 114, "seek": 75076, "start": 751.4, "end": 758.04, "text": " And by AIA. Now, by AIA means we have telemetry, there are semantic conventions for databases,", "tokens": [50396, 400, 538, 316, 6914, 13, 823, 11, 538, 316, 6914, 1355, 321, 362, 4304, 5537, 627, 11, 456, 366, 47982, 33520, 337, 22380, 11, 50728], "temperature": 0.0, "avg_logprob": -0.18200008685772234, "compression_ratio": 1.8594594594594596, "no_speech_prob": 0.03777313232421875}, {"id": 115, "seek": 75076, "start": 758.04, "end": 763.24, "text": " we have semantic conventions for cloud providers, we have semantic conventions for", "tokens": [50728, 321, 362, 47982, 33520, 337, 4588, 11330, 11, 321, 362, 47982, 33520, 337, 50988], "temperature": 0.0, "avg_logprob": -0.18200008685772234, "compression_ratio": 1.8594594594594596, "no_speech_prob": 0.03777313232421875}, {"id": 116, "seek": 75076, "start": 763.24, "end": 768.04, "text": " a lot of different things, really, for log files. Something that is not there yet, though,", "tokens": [50988, 257, 688, 295, 819, 721, 11, 534, 11, 337, 3565, 7098, 13, 6595, 300, 307, 406, 456, 1939, 11, 1673, 11, 51228], "temperature": 0.0, "avg_logprob": -0.18200008685772234, "compression_ratio": 1.8594594594594596, "no_speech_prob": 0.03777313232421875}, {"id": 117, "seek": 75076, "start": 768.04, "end": 771.64, "text": " is semantic conventions for continuous integration and continuous delivery.", "tokens": [51228, 307, 47982, 33520, 337, 10957, 10980, 293, 10957, 8982, 13, 51408], "temperature": 0.0, "avg_logprob": -0.18200008685772234, "compression_ratio": 1.8594594594594596, "no_speech_prob": 0.03777313232421875}, {"id": 118, "seek": 77164, "start": 772.36, "end": 780.36, "text": " Now, what is important in my opinion? This is important because, I mean, we use some", "tokens": [50400, 823, 11, 437, 307, 1021, 294, 452, 4800, 30, 639, 307, 1021, 570, 11, 286, 914, 11, 321, 764, 512, 50800], "temperature": 0.0, "avg_logprob": -0.24587897036937958, "compression_ratio": 1.7644230769230769, "no_speech_prob": 0.04611450433731079}, {"id": 119, "seek": 77164, "start": 780.36, "end": 787.4, "text": " CI tool, I guess, everyone here uses a different CI tool. But regardless of what we use, we can see", "tokens": [50800, 37777, 2290, 11, 286, 2041, 11, 1518, 510, 4960, 257, 819, 37777, 2290, 13, 583, 10060, 295, 437, 321, 764, 11, 321, 393, 536, 51152], "temperature": 0.0, "avg_logprob": -0.24587897036937958, "compression_ratio": 1.7644230769230769, "no_speech_prob": 0.04611450433731079}, {"id": 120, "seek": 77164, "start": 787.4, "end": 793.4, "text": " that at the end of the day, the data that is behind its CI tool is the same. Regardless of", "tokens": [51152, 300, 412, 264, 917, 295, 264, 786, 11, 264, 1412, 300, 307, 2261, 1080, 37777, 2290, 307, 264, 912, 13, 25148, 295, 51452], "temperature": 0.0, "avg_logprob": -0.24587897036937958, "compression_ratio": 1.7644230769230769, "no_speech_prob": 0.04611450433731079}, {"id": 121, "seek": 77164, "start": 793.4, "end": 800.84, "text": " whether someone calls it stage, someone calls it job, someone calls it status or outcome or", "tokens": [51452, 1968, 1580, 5498, 309, 3233, 11, 1580, 5498, 309, 1691, 11, 1580, 5498, 309, 6558, 420, 9700, 420, 51824], "temperature": 0.0, "avg_logprob": -0.24587897036937958, "compression_ratio": 1.7644230769230769, "no_speech_prob": 0.04611450433731079}, {"id": 122, "seek": 80084, "start": 801.48, "end": 809.24, "text": " whatever, the underlying data, such as the job name, the outcome of a CI system is the same.", "tokens": [50396, 2035, 11, 264, 14217, 1412, 11, 1270, 382, 264, 1691, 1315, 11, 264, 9700, 295, 257, 37777, 1185, 307, 264, 912, 13, 50784], "temperature": 0.0, "avg_logprob": -0.16973696584286896, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.014792254194617271}, {"id": 123, "seek": 80084, "start": 811.0, "end": 817.08, "text": " Now, I'm not extremely familiar with every CI system of there, but at some point I was trying", "tokens": [50872, 823, 11, 286, 478, 406, 4664, 4963, 365, 633, 37777, 1185, 295, 456, 11, 457, 412, 512, 935, 286, 390, 1382, 51176], "temperature": 0.0, "avg_logprob": -0.16973696584286896, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.014792254194617271}, {"id": 124, "seek": 80084, "start": 817.08, "end": 822.76, "text": " to figure out why in our CI we had a test that sometimes was taking three minutes to a test,", "tokens": [51176, 281, 2573, 484, 983, 294, 527, 37777, 321, 632, 257, 1500, 300, 2171, 390, 1940, 1045, 2077, 281, 257, 1500, 11, 51460], "temperature": 0.0, "avg_logprob": -0.16973696584286896, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.014792254194617271}, {"id": 125, "seek": 80084, "start": 822.76, "end": 828.0400000000001, "text": " like a pipeline, that sometimes was taking only three minutes to complete, while some others up", "tokens": [51460, 411, 257, 15517, 11, 300, 2171, 390, 1940, 787, 1045, 2077, 281, 3566, 11, 1339, 512, 2357, 493, 51724], "temperature": 0.0, "avg_logprob": -0.16973696584286896, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.014792254194617271}, {"id": 126, "seek": 82804, "start": 828.04, "end": 832.28, "text": " to nine or ten, which, I mean, without any code changes. So, like, if you talk about flakiness,", "tokens": [50364, 281, 4949, 420, 2064, 11, 597, 11, 286, 914, 11, 1553, 604, 3089, 2962, 13, 407, 11, 411, 11, 498, 291, 751, 466, 932, 514, 1324, 11, 50576], "temperature": 0.0, "avg_logprob": -0.22507301596708076, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.021893877536058426}, {"id": 127, "seek": 82804, "start": 832.28, "end": 836.76, "text": " yeah, that's part of, like, test failing, failing, other parties, why sometimes they take too long.", "tokens": [50576, 1338, 11, 300, 311, 644, 295, 11, 411, 11, 1500, 18223, 11, 18223, 11, 661, 8265, 11, 983, 2171, 436, 747, 886, 938, 13, 50800], "temperature": 0.0, "avg_logprob": -0.22507301596708076, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.021893877536058426}, {"id": 128, "seek": 82804, "start": 839.4, "end": 849.9599999999999, "text": " Easy peasy, I think. I wrote some totally reliable Go code. No? No, okay. It was very good code.", "tokens": [50932, 16002, 520, 5871, 11, 286, 519, 13, 286, 4114, 512, 3879, 12924, 1037, 3089, 13, 883, 30, 883, 11, 1392, 13, 467, 390, 588, 665, 3089, 13, 51460], "temperature": 0.0, "avg_logprob": -0.22507301596708076, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.021893877536058426}, {"id": 129, "seek": 84996, "start": 850.9200000000001, "end": 858.36, "text": " So, what was code was doing was getting stuff out of the ground database and pushing it to", "tokens": [50412, 407, 11, 437, 390, 3089, 390, 884, 390, 1242, 1507, 484, 295, 264, 2727, 8149, 293, 7380, 309, 281, 50784], "temperature": 0.0, "avg_logprob": -0.20603719049570512, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.022442590445280075}, {"id": 130, "seek": 84996, "start": 858.36, "end": 866.76, "text": " log a template in here for later analysis. It worked great. Worked perfectly. Now, what happened", "tokens": [50784, 3565, 257, 12379, 294, 510, 337, 1780, 5215, 13, 467, 2732, 869, 13, 6603, 292, 6239, 13, 823, 11, 437, 2011, 51204], "temperature": 0.0, "avg_logprob": -0.20603719049570512, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.022442590445280075}, {"id": 131, "seek": 84996, "start": 866.76, "end": 871.48, "text": " is that we were able, at the end of the day, even if the code wasn't very good, we were able to at", "tokens": [51204, 307, 300, 321, 645, 1075, 11, 412, 264, 917, 295, 264, 786, 11, 754, 498, 264, 3089, 2067, 380, 588, 665, 11, 321, 645, 1075, 281, 412, 51440], "temperature": 0.0, "avg_logprob": -0.20603719049570512, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.022442590445280075}, {"id": 132, "seek": 84996, "start": 871.48, "end": 877.48, "text": " least look at something outside of our CI system. Why this? Because our CI system didn't provide us", "tokens": [51440, 1935, 574, 412, 746, 2380, 295, 527, 37777, 1185, 13, 1545, 341, 30, 1436, 527, 37777, 1185, 994, 380, 2893, 505, 51740], "temperature": 0.0, "avg_logprob": -0.20603719049570512, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.022442590445280075}, {"id": 133, "seek": 87748, "start": 878.04, "end": 883.4, "text": " with the UI, with the tools to query for the data we were looking for. So, we were trying to", "tokens": [50392, 365, 264, 15682, 11, 365, 264, 3873, 281, 14581, 337, 264, 1412, 321, 645, 1237, 337, 13, 407, 11, 321, 645, 1382, 281, 50660], "temperature": 0.0, "avg_logprob": -0.12823185920715333, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.011927167885005474}, {"id": 134, "seek": 87748, "start": 883.4, "end": 892.12, "text": " analyze why something was happening. And our UI wasn't able to do so. So, okay. I share the news", "tokens": [50660, 12477, 983, 746, 390, 2737, 13, 400, 527, 15682, 2067, 380, 1075, 281, 360, 370, 13, 407, 11, 1392, 13, 286, 2073, 264, 2583, 51096], "temperature": 0.0, "avg_logprob": -0.12823185920715333, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.011927167885005474}, {"id": 135, "seek": 87748, "start": 892.12, "end": 899.72, "text": " with my team. And, I mean, I guess every one of you has been there at some point in your life.", "tokens": [51096, 365, 452, 1469, 13, 400, 11, 286, 914, 11, 286, 2041, 633, 472, 295, 291, 575, 668, 456, 412, 512, 935, 294, 428, 993, 13, 51476], "temperature": 0.0, "avg_logprob": -0.12823185920715333, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.011927167885005474}, {"id": 136, "seek": 89972, "start": 900.6800000000001, "end": 908.76, "text": " They got too excited. Ivana wanted us to have the log data on Elasticsearch. Piotr was,", "tokens": [50412, 814, 658, 886, 2919, 13, 26546, 2095, 1415, 505, 281, 362, 264, 3565, 1412, 322, 2699, 2750, 405, 1178, 13, 430, 6471, 81, 390, 11, 50816], "temperature": 0.0, "avg_logprob": -0.25103390342310855, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.011971008963882923}, {"id": 137, "seek": 89972, "start": 908.76, "end": 913.48, "text": " which is not a colleague, wanted to get tracing data from Git action, this is a lot of drone.", "tokens": [50816, 597, 307, 406, 257, 13532, 11, 1415, 281, 483, 25262, 1412, 490, 16939, 3069, 11, 341, 307, 257, 688, 295, 13852, 13, 51052], "temperature": 0.0, "avg_logprob": -0.25103390342310855, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.011971008963882923}, {"id": 138, "seek": 89972, "start": 914.6800000000001, "end": 918.12, "text": " And, yeah, no, that code was not good enough.", "tokens": [51112, 400, 11, 1338, 11, 572, 11, 300, 3089, 390, 406, 665, 1547, 13, 51284], "temperature": 0.0, "avg_logprob": -0.25103390342310855, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.011971008963882923}, {"id": 139, "seek": 89972, "start": 921.4, "end": 928.6, "text": " I mean, back to the drawing board. What happens now? We need to figure out a way of getting data out of", "tokens": [51448, 286, 914, 11, 646, 281, 264, 6316, 3150, 13, 708, 2314, 586, 30, 492, 643, 281, 2573, 484, 257, 636, 295, 1242, 1412, 484, 295, 51808], "temperature": 0.0, "avg_logprob": -0.25103390342310855, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.011971008963882923}, {"id": 140, "seek": 92860, "start": 929.32, "end": 937.48, "text": " GitHub CI, drone, GitHub actions, whatever. I don't know. What else? Bit bucket. I don't know", "tokens": [50400, 23331, 37777, 11, 13852, 11, 23331, 5909, 11, 2035, 13, 286, 500, 380, 458, 13, 708, 1646, 30, 9101, 13058, 13, 286, 500, 380, 458, 50808], "temperature": 0.0, "avg_logprob": -0.18107034592401414, "compression_ratio": 1.5575221238938053, "no_speech_prob": 0.00481896847486496}, {"id": 141, "seek": 92860, "start": 937.48, "end": 943.72, "text": " if they have a CI system. I'm not sure about. And, we need to push it to every database out there,", "tokens": [50808, 498, 436, 362, 257, 37777, 1185, 13, 286, 478, 406, 988, 466, 13, 400, 11, 321, 643, 281, 2944, 309, 281, 633, 8149, 484, 456, 11, 51120], "temperature": 0.0, "avg_logprob": -0.18107034592401414, "compression_ratio": 1.5575221238938053, "no_speech_prob": 0.00481896847486496}, {"id": 142, "seek": 92860, "start": 944.28, "end": 950.12, "text": " from Graphite, Tempo, Elasticsearch, I don't know, whatever. Yeager.", "tokens": [51148, 490, 21884, 642, 11, 8095, 2259, 11, 2699, 2750, 405, 1178, 11, 286, 500, 380, 458, 11, 2035, 13, 835, 3557, 13, 51440], "temperature": 0.0, "avg_logprob": -0.18107034592401414, "compression_ratio": 1.5575221238938053, "no_speech_prob": 0.00481896847486496}, {"id": 143, "seek": 92860, "start": 952.44, "end": 958.2, "text": " I mean, it sounds like a very silly question. I bet there is no one of us that really uses", "tokens": [51556, 286, 914, 11, 309, 3263, 411, 257, 588, 11774, 1168, 13, 286, 778, 456, 307, 572, 472, 295, 505, 300, 534, 4960, 51844], "temperature": 0.0, "avg_logprob": -0.18107034592401414, "compression_ratio": 1.5575221238938053, "no_speech_prob": 0.00481896847486496}, {"id": 144, "seek": 95820, "start": 958.2, "end": 965.48, "text": " 10 different databases to match their telemetry data. Also, because then, you know, this is what", "tokens": [50364, 1266, 819, 22380, 281, 2995, 641, 4304, 5537, 627, 1412, 13, 2743, 11, 570, 550, 11, 291, 458, 11, 341, 307, 437, 50728], "temperature": 0.0, "avg_logprob": -0.18648534718126353, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.009686310775578022}, {"id": 145, "seek": 95820, "start": 965.48, "end": 969.72, "text": " was going to happen. We had to write code to get data out of every CI system to push it to every", "tokens": [50728, 390, 516, 281, 1051, 13, 492, 632, 281, 2464, 3089, 281, 483, 1412, 484, 295, 633, 37777, 1185, 281, 2944, 309, 281, 633, 50940], "temperature": 0.0, "avg_logprob": -0.18648534718126353, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.009686310775578022}, {"id": 146, "seek": 95820, "start": 969.72, "end": 980.44, "text": " other database system. And, I say no, like, I wasn't going to do that. But, this, I like some very", "tokens": [50940, 661, 8149, 1185, 13, 400, 11, 286, 584, 572, 11, 411, 11, 286, 2067, 380, 516, 281, 360, 300, 13, 583, 11, 341, 11, 286, 411, 512, 588, 51476], "temperature": 0.0, "avg_logprob": -0.18648534718126353, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.009686310775578022}, {"id": 147, "seek": 95820, "start": 980.44, "end": 987.48, "text": " important point that's owning your data. When I think about, when I started thinking about", "tokens": [51476, 1021, 935, 300, 311, 29820, 428, 1412, 13, 1133, 286, 519, 466, 11, 562, 286, 1409, 1953, 466, 51828], "temperature": 0.0, "avg_logprob": -0.18648534718126353, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.009686310775578022}, {"id": 148, "seek": 98748, "start": 987.48, "end": 993.32, "text": " owning my data, what I thought about was mostly owning the hardware in which the data was going", "tokens": [50364, 29820, 452, 1412, 11, 437, 286, 1194, 466, 390, 5240, 29820, 264, 8837, 294, 597, 264, 1412, 390, 516, 50656], "temperature": 0.0, "avg_logprob": -0.11733468373616536, "compression_ratio": 1.7236842105263157, "no_speech_prob": 0.010843733325600624}, {"id": 149, "seek": 98748, "start": 993.32, "end": 999.72, "text": " to be stored. So, like, owning the drive or having it stored on one of my machines. I think that", "tokens": [50656, 281, 312, 12187, 13, 407, 11, 411, 11, 29820, 264, 3332, 420, 1419, 309, 12187, 322, 472, 295, 452, 8379, 13, 286, 519, 300, 50976], "temperature": 0.0, "avg_logprob": -0.11733468373616536, "compression_ratio": 1.7236842105263157, "no_speech_prob": 0.010843733325600624}, {"id": 150, "seek": 98748, "start": 999.72, "end": 1007.16, "text": " that's not exactly the point we need to make here. I think owning your data means you being able to", "tokens": [50976, 300, 311, 406, 2293, 264, 935, 321, 643, 281, 652, 510, 13, 286, 519, 29820, 428, 1412, 1355, 291, 885, 1075, 281, 51348], "temperature": 0.0, "avg_logprob": -0.11733468373616536, "compression_ratio": 1.7236842105263157, "no_speech_prob": 0.010843733325600624}, {"id": 151, "seek": 98748, "start": 1007.16, "end": 1013.24, "text": " decide where the data goes, where and how to store the data. We can very, very well be using a cloud", "tokens": [51348, 4536, 689, 264, 1412, 1709, 11, 689, 293, 577, 281, 3531, 264, 1412, 13, 492, 393, 588, 11, 588, 731, 312, 1228, 257, 4588, 51652], "temperature": 0.0, "avg_logprob": -0.11733468373616536, "compression_ratio": 1.7236842105263157, "no_speech_prob": 0.010843733325600624}, {"id": 152, "seek": 101324, "start": 1013.48, "end": 1020.2, "text": " database provider to store our data. The important bit is that we own, we know, we decide where the", "tokens": [50376, 8149, 12398, 281, 3531, 527, 1412, 13, 440, 1021, 857, 307, 300, 321, 1065, 11, 321, 458, 11, 321, 4536, 689, 264, 50712], "temperature": 0.0, "avg_logprob": -0.14342918395996093, "compression_ratio": 1.8314606741573034, "no_speech_prob": 0.018087666481733322}, {"id": 153, "seek": 101324, "start": 1020.2, "end": 1025.08, "text": " data is going to be stored and we decide and we have the ability to use the data however we want.", "tokens": [50712, 1412, 307, 516, 281, 312, 12187, 293, 321, 4536, 293, 321, 362, 264, 3485, 281, 764, 264, 1412, 4461, 321, 528, 13, 50956], "temperature": 0.0, "avg_logprob": -0.14342918395996093, "compression_ratio": 1.8314606741573034, "no_speech_prob": 0.018087666481733322}, {"id": 154, "seek": 101324, "start": 1027.32, "end": 1031.4, "text": " So, the reason why open telemetry is important and fits very well with the picture is that by defining", "tokens": [51068, 407, 11, 264, 1778, 983, 1269, 4304, 5537, 627, 307, 1021, 293, 9001, 588, 731, 365, 264, 3036, 307, 300, 538, 17827, 51272], "temperature": 0.0, "avg_logprob": -0.14342918395996093, "compression_ratio": 1.8314606741573034, "no_speech_prob": 0.018087666481733322}, {"id": 155, "seek": 101324, "start": 1031.4, "end": 1036.36, "text": " standards and by defining a specification for which data can be transferred and stored, not", "tokens": [51272, 7787, 293, 538, 17827, 257, 31256, 337, 597, 1412, 393, 312, 15809, 293, 12187, 11, 406, 51520], "temperature": 0.0, "avg_logprob": -0.14342918395996093, "compression_ratio": 1.8314606741573034, "no_speech_prob": 0.018087666481733322}, {"id": 156, "seek": 101324, "start": 1036.36, "end": 1042.84, "text": " stored but transferred, we are able to only take care of the first part of the equation here. We", "tokens": [51520, 12187, 457, 15809, 11, 321, 366, 1075, 281, 787, 747, 1127, 295, 264, 700, 644, 295, 264, 5367, 510, 13, 492, 51844], "temperature": 0.0, "avg_logprob": -0.14342918395996093, "compression_ratio": 1.8314606741573034, "no_speech_prob": 0.018087666481733322}, {"id": 157, "seek": 104284, "start": 1043.6399999999999, "end": 1049.72, "text": " take data out of the systems and then open telemetry is going to take care about", "tokens": [50404, 747, 1412, 484, 295, 264, 3652, 293, 550, 1269, 4304, 5537, 627, 307, 516, 281, 747, 1127, 466, 50708], "temperature": 0.0, "avg_logprob": -0.2108436975723658, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0023737859446555376}, {"id": 158, "seek": 104284, "start": 1050.12, "end": 1053.48, "text": " inverting and sending to the database we need.", "tokens": [50728, 28653, 783, 293, 7750, 281, 264, 8149, 321, 643, 13, 50896], "temperature": 0.0, "avg_logprob": -0.2108436975723658, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0023737859446555376}, {"id": 159, "seek": 104284, "start": 1056.1999999999998, "end": 1062.6, "text": " What we did was we built an open telemetry collector who, does any of you know what", "tokens": [51032, 708, 321, 630, 390, 321, 3094, 364, 1269, 4304, 5537, 627, 23960, 567, 11, 775, 604, 295, 291, 458, 437, 51352], "temperature": 0.0, "avg_logprob": -0.2108436975723658, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0023737859446555376}, {"id": 160, "seek": 104284, "start": 1063.8, "end": 1070.4399999999998, "text": " our collector distribution is? A few. Okay, so an open telemetry collector distribution is basically", "tokens": [51412, 527, 23960, 7316, 307, 30, 316, 1326, 13, 1033, 11, 370, 364, 1269, 4304, 5537, 627, 23960, 7316, 307, 1936, 51744], "temperature": 0.0, "avg_logprob": -0.2108436975723658, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0023737859446555376}, {"id": 161, "seek": 107044, "start": 1070.44, "end": 1077.96, "text": " a set of pre-built components. It's a binary that you can run, of course, and you can configure to", "tokens": [50364, 257, 992, 295, 659, 12, 23018, 6677, 13, 467, 311, 257, 17434, 300, 291, 393, 1190, 11, 295, 1164, 11, 293, 291, 393, 22162, 281, 50740], "temperature": 0.0, "avg_logprob": -0.1818621527288378, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.028854085132479668}, {"id": 162, "seek": 107044, "start": 1077.96, "end": 1085.56, "text": " do things. It consists very reductively of receivers, processors and exporters. Receivers", "tokens": [50740, 360, 721, 13, 467, 14689, 588, 2783, 349, 3413, 295, 49196, 11, 27751, 293, 1278, 12168, 13, 1300, 384, 1762, 51120], "temperature": 0.0, "avg_logprob": -0.1818621527288378, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.028854085132479668}, {"id": 163, "seek": 107044, "start": 1085.56, "end": 1091.0, "text": " are the components that allow you to get data in, can be like watching at log files, can be,", "tokens": [51120, 366, 264, 6677, 300, 2089, 291, 281, 483, 1412, 294, 11, 393, 312, 411, 1976, 412, 3565, 7098, 11, 393, 312, 11, 51392], "temperature": 0.0, "avg_logprob": -0.1818621527288378, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.028854085132479668}, {"id": 164, "seek": 107044, "start": 1091.96, "end": 1097.64, "text": " I don't know, even listening on Bluetooth 1.0 and check for things over the air,", "tokens": [51440, 286, 500, 380, 458, 11, 754, 4764, 322, 20286, 502, 13, 15, 293, 1520, 337, 721, 670, 264, 1988, 11, 51724], "temperature": 0.0, "avg_logprob": -0.1818621527288378, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.028854085132479668}, {"id": 165, "seek": 109764, "start": 1097.64, "end": 1103.0, "text": " can be extracting metrics from some running services. There are processors that transfer this", "tokens": [50364, 393, 312, 49844, 16367, 490, 512, 2614, 3328, 13, 821, 366, 27751, 300, 5003, 341, 50632], "temperature": 0.0, "avg_logprob": -0.1662182383017965, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.037315208464860916}, {"id": 166, "seek": 109764, "start": 1103.0, "end": 1108.2800000000002, "text": " data in the format that you need. They add attributes, they modify attributes or remove them.", "tokens": [50632, 1412, 294, 264, 7877, 300, 291, 643, 13, 814, 909, 17212, 11, 436, 16927, 17212, 420, 4159, 552, 13, 50896], "temperature": 0.0, "avg_logprob": -0.1662182383017965, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.037315208464860916}, {"id": 167, "seek": 109764, "start": 1109.0800000000002, "end": 1115.16, "text": " And exporters that send this data out to your database of choice. The thing here is that", "tokens": [50936, 400, 1278, 12168, 300, 2845, 341, 1412, 484, 281, 428, 8149, 295, 3922, 13, 440, 551, 510, 307, 300, 51240], "temperature": 0.0, "avg_logprob": -0.1662182383017965, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.037315208464860916}, {"id": 168, "seek": 109764, "start": 1115.16, "end": 1119.64, "text": " for those exporters, we didn't write anything. Those are already open source exporters.", "tokens": [51240, 337, 729, 1278, 12168, 11, 321, 994, 380, 2464, 1340, 13, 3950, 366, 1217, 1269, 4009, 1278, 12168, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1662182383017965, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.037315208464860916}, {"id": 169, "seek": 109764, "start": 1119.64, "end": 1126.1200000000001, "text": " There are more for elastic search for whatever, like really Jager or you name it.", "tokens": [51464, 821, 366, 544, 337, 17115, 3164, 337, 2035, 11, 411, 534, 508, 3557, 420, 291, 1315, 309, 13, 51788], "temperature": 0.0, "avg_logprob": -0.1662182383017965, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.037315208464860916}, {"id": 170, "seek": 112764, "start": 1127.72, "end": 1132.3600000000001, "text": " So the only bit we had to do was writing a drone receiver, which was getting data out of drone", "tokens": [50368, 407, 264, 787, 857, 321, 632, 281, 360, 390, 3579, 257, 13852, 20086, 11, 597, 390, 1242, 1412, 484, 295, 13852, 50600], "temperature": 0.0, "avg_logprob": -0.24571193348277698, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.00790576171129942}, {"id": 171, "seek": 112764, "start": 1134.0400000000002, "end": 1138.92, "text": " to pipe into open telemetry and then push it to log in, tempo and prometheus.", "tokens": [50684, 281, 11240, 666, 1269, 4304, 5537, 627, 293, 550, 2944, 309, 281, 3565, 294, 11, 8972, 293, 37786, 42209, 13, 50928], "temperature": 0.0, "avg_logprob": -0.24571193348277698, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.00790576171129942}, {"id": 172, "seek": 112764, "start": 1141.72, "end": 1148.0400000000002, "text": " There are some practical examples. There is a Jenkins plugin that gets traces data out of", "tokens": [51068, 821, 366, 512, 8496, 5110, 13, 821, 307, 257, 41273, 23407, 300, 2170, 26076, 1412, 484, 295, 51384], "temperature": 0.0, "avg_logprob": -0.24571193348277698, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.00790576171129942}, {"id": 173, "seek": 114804, "start": 1149.0, "end": 1158.36, "text": " Jenkins and sends it via the OTLP format. Irokinz brought these other", "tokens": [50412, 41273, 293, 14790, 309, 5766, 264, 38617, 45196, 7877, 13, 286, 340, 5843, 89, 3038, 613, 661, 50880], "temperature": 0.0, "avg_logprob": -0.38809676537146937, "compression_ratio": 1.4388888888888889, "no_speech_prob": 0.026677967980504036}, {"id": 174, "seek": 114804, "start": 1159.0, "end": 1165.96, "text": " get-up functions that run commands and exports the execution of these commands as trace data.", "tokens": [50912, 483, 12, 1010, 6828, 300, 1190, 16901, 293, 31428, 264, 15058, 295, 613, 16901, 382, 13508, 1412, 13, 51260], "temperature": 0.0, "avg_logprob": -0.38809676537146937, "compression_ratio": 1.4388888888888889, "no_speech_prob": 0.026677967980504036}, {"id": 175, "seek": 114804, "start": 1167.0, "end": 1172.6, "text": " And of course, our own experiment, which is very complete but very well free to take a look at.", "tokens": [51312, 400, 295, 1164, 11, 527, 1065, 5120, 11, 597, 307, 588, 3566, 457, 588, 731, 1737, 281, 747, 257, 574, 412, 13, 51592], "temperature": 0.0, "avg_logprob": -0.38809676537146937, "compression_ratio": 1.4388888888888889, "no_speech_prob": 0.026677967980504036}, {"id": 176, "seek": 117260, "start": 1173.3999999999999, "end": 1180.6, "text": " Now, what is unlocked? What is unlocked for us? As I said, first of all, there were these", "tokens": [50404, 823, 11, 437, 307, 30180, 30, 708, 307, 30180, 337, 505, 30, 1018, 286, 848, 11, 700, 295, 439, 11, 456, 645, 613, 50764], "temperature": 0.0, "avg_logprob": -0.22414876336920753, "compression_ratio": 1.5359116022099448, "no_speech_prob": 0.01384642068296671}, {"id": 177, "seek": 117260, "start": 1180.6, "end": 1186.6799999999998, "text": " performance issues with our first test. Second thing, at some point we had this test here", "tokens": [50764, 3389, 2663, 365, 527, 700, 1500, 13, 5736, 551, 11, 412, 512, 935, 321, 632, 341, 1500, 510, 51068], "temperature": 0.0, "avg_logprob": -0.22414876336920753, "compression_ratio": 1.5359116022099448, "no_speech_prob": 0.01384642068296671}, {"id": 178, "seek": 117260, "start": 1188.28, "end": 1195.56, "text": " that was a bit flaky. Failing sometimes, sometimes not. Of course, worked on my machine, worked on", "tokens": [51148, 300, 390, 257, 857, 932, 15681, 13, 479, 23315, 2171, 11, 2171, 406, 13, 2720, 1164, 11, 2732, 322, 452, 3479, 11, 2732, 322, 51512], "temperature": 0.0, "avg_logprob": -0.22414876336920753, "compression_ratio": 1.5359116022099448, "no_speech_prob": 0.01384642068296671}, {"id": 179, "seek": 119556, "start": 1196.52, "end": 1203.56, "text": " my advanced machine probably. But yeah, we couldn't figure out what to do with it because we would", "tokens": [50412, 452, 7339, 3479, 1391, 13, 583, 1338, 11, 321, 2809, 380, 2573, 484, 437, 281, 360, 365, 309, 570, 321, 576, 50764], "temperature": 0.0, "avg_logprob": -0.2063956790500217, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.04959643632173538}, {"id": 180, "seek": 119556, "start": 1203.56, "end": 1209.56, "text": " disable it but then when we were going to enable it, if you cannot really reproduce it locally.", "tokens": [50764, 28362, 309, 457, 550, 562, 321, 645, 516, 281, 9528, 309, 11, 498, 291, 2644, 534, 29501, 309, 16143, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2063956790500217, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.04959643632173538}, {"id": 181, "seek": 119556, "start": 1210.6799999999998, "end": 1218.6799999999998, "text": " Now, by getting this data out of our CI and pushing the build logs into our observability", "tokens": [51120, 823, 11, 538, 1242, 341, 1412, 484, 295, 527, 37777, 293, 7380, 264, 1322, 20820, 666, 527, 9951, 2310, 51520], "temperature": 0.0, "avg_logprob": -0.2063956790500217, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.04959643632173538}, {"id": 182, "seek": 119556, "start": 1218.6799999999998, "end": 1225.08, "text": " system into our log instance, we were able to trace back from the build that you see on the right", "tokens": [51520, 1185, 666, 527, 3565, 5197, 11, 321, 645, 1075, 281, 13508, 646, 490, 264, 1322, 300, 291, 536, 322, 264, 558, 51840], "temperature": 0.0, "avg_logprob": -0.2063956790500217, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.04959643632173538}, {"id": 183, "seek": 122556, "start": 1225.56, "end": 1232.2, "text": " to the logs for that build, trace back to the first time that failed test in our CI.", "tokens": [50364, 281, 264, 20820, 337, 300, 1322, 11, 13508, 646, 281, 264, 700, 565, 300, 7612, 1500, 294, 527, 37777, 13, 50696], "temperature": 0.0, "avg_logprob": -0.12760360424335188, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.007759209722280502}, {"id": 184, "seek": 122556, "start": 1233.72, "end": 1240.44, "text": " And from there, if you look down here, this is an attribute that we thought was valuable.", "tokens": [50772, 400, 490, 456, 11, 498, 291, 574, 760, 510, 11, 341, 307, 364, 19667, 300, 321, 1194, 390, 8263, 13, 51108], "temperature": 0.0, "avg_logprob": -0.12760360424335188, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.007759209722280502}, {"id": 185, "seek": 122556, "start": 1240.44, "end": 1246.9199999999998, "text": " We had a build number, which is our unique ID for drone, which then pointed out to the first", "tokens": [51108, 492, 632, 257, 1322, 1230, 11, 597, 307, 527, 3845, 7348, 337, 13852, 11, 597, 550, 10932, 484, 281, 264, 700, 51432], "temperature": 0.0, "avg_logprob": -0.12760360424335188, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.007759209722280502}, {"id": 186, "seek": 122556, "start": 1247.8799999999999, "end": 1254.44, "text": " pull request that introduced that test or that flakiness. With that, we were able to identify", "tokens": [51480, 2235, 5308, 300, 7268, 300, 1500, 420, 300, 932, 514, 1324, 13, 2022, 300, 11, 321, 645, 1075, 281, 5876, 51808], "temperature": 0.0, "avg_logprob": -0.12760360424335188, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.007759209722280502}, {"id": 187, "seek": 125444, "start": 1254.44, "end": 1258.52, "text": " what was causing the actual issue, which was a test which was totally unrelated,", "tokens": [50364, 437, 390, 9853, 264, 3539, 2734, 11, 597, 390, 257, 1500, 597, 390, 3879, 38967, 11, 50568], "temperature": 0.0, "avg_logprob": -0.16784167027735447, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.0084994463250041}, {"id": 188, "seek": 125444, "start": 1258.52, "end": 1263.96, "text": " running a different suite. But turns out that was causing the flakiness.", "tokens": [50568, 2614, 257, 819, 14205, 13, 583, 4523, 484, 300, 390, 9853, 264, 932, 514, 1324, 13, 50840], "temperature": 0.0, "avg_logprob": -0.16784167027735447, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.0084994463250041}, {"id": 189, "seek": 125444, "start": 1265.8, "end": 1271.64, "text": " Something else that we were able to do was, so first of all, like one, I don't know, silly thing", "tokens": [50932, 6595, 1646, 300, 321, 645, 1075, 281, 360, 390, 11, 370, 700, 295, 439, 11, 411, 472, 11, 286, 500, 380, 458, 11, 11774, 551, 51224], "temperature": 0.0, "avg_logprob": -0.16784167027735447, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.0084994463250041}, {"id": 190, "seek": 125444, "start": 1271.64, "end": 1280.92, "text": " that we did but we liked, was to create a custom UI within Grafana to mimic sort of like the UI", "tokens": [51224, 300, 321, 630, 457, 321, 4501, 11, 390, 281, 1884, 257, 2375, 15682, 1951, 8985, 69, 2095, 281, 31075, 1333, 295, 411, 264, 15682, 51688], "temperature": 0.0, "avg_logprob": -0.16784167027735447, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.0084994463250041}, {"id": 191, "seek": 128092, "start": 1280.92, "end": 1286.76, "text": " that you have when you look at the output of your system. I mean, there is some value maybe", "tokens": [50364, 300, 291, 362, 562, 291, 574, 412, 264, 5598, 295, 428, 1185, 13, 286, 914, 11, 456, 307, 512, 2158, 1310, 50656], "temperature": 0.0, "avg_logprob": -0.1752228834191147, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.021083995699882507}, {"id": 192, "seek": 128092, "start": 1286.76, "end": 1291.16, "text": " near but the important bit here is that we own the data. We were able to do something which was", "tokens": [50656, 2651, 457, 264, 1021, 857, 510, 307, 300, 321, 1065, 264, 1412, 13, 492, 645, 1075, 281, 360, 746, 597, 390, 50876], "temperature": 0.0, "avg_logprob": -0.1752228834191147, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.021083995699882507}, {"id": 193, "seek": 128092, "start": 1291.16, "end": 1297.48, "text": " funny. We spent maybe one day on it. And yeah, it will look good. The second thing, however,", "tokens": [50876, 4074, 13, 492, 4418, 1310, 472, 786, 322, 309, 13, 400, 1338, 11, 309, 486, 574, 665, 13, 440, 1150, 551, 11, 4461, 11, 51192], "temperature": 0.0, "avg_logprob": -0.1752228834191147, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.021083995699882507}, {"id": 194, "seek": 128092, "start": 1297.48, "end": 1306.04, "text": " is more important. Now, in Grafana, we have a very complex release system. Very complex.", "tokens": [51192, 307, 544, 1021, 13, 823, 11, 294, 8985, 69, 2095, 11, 321, 362, 257, 588, 3997, 4374, 1185, 13, 4372, 3997, 13, 51620], "temperature": 0.0, "avg_logprob": -0.1752228834191147, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.021083995699882507}, {"id": 195, "seek": 130604, "start": 1306.12, "end": 1314.36, "text": " We maintain a set of different release branches that need, in theory, should need to be", "tokens": [50368, 492, 6909, 257, 992, 295, 819, 4374, 14770, 300, 643, 11, 294, 5261, 11, 820, 643, 281, 312, 50780], "temperature": 0.0, "avg_logprob": -0.23648432965548533, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.01354729849845171}, {"id": 196, "seek": 130604, "start": 1314.36, "end": 1321.96, "text": " released at an even time. Of course, like everyone, like for everyone, something things breaks.", "tokens": [50780, 4736, 412, 364, 754, 565, 13, 2720, 1164, 11, 411, 1518, 11, 411, 337, 1518, 11, 746, 721, 9857, 13, 51160], "temperature": 0.0, "avg_logprob": -0.23648432965548533, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.01354729849845171}, {"id": 197, "seek": 130604, "start": 1321.96, "end": 1325.3999999999999, "text": " Sometimes things break and you don't know why because you are not looking at it. Sometimes", "tokens": [51160, 4803, 721, 1821, 293, 291, 500, 380, 458, 983, 570, 291, 366, 406, 1237, 412, 309, 13, 4803, 51332], "temperature": 0.0, "avg_logprob": -0.23648432965548533, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.01354729849845171}, {"id": 198, "seek": 130604, "start": 1325.3999999999999, "end": 1331.0, "text": " a commit you make in main breaks something else somewhere else because you back ported it but", "tokens": [51332, 257, 5599, 291, 652, 294, 2135, 9857, 746, 1646, 4079, 1646, 570, 291, 646, 2436, 292, 309, 457, 51612], "temperature": 0.0, "avg_logprob": -0.23648432965548533, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.01354729849845171}, {"id": 199, "seek": 130604, "start": 1331.0, "end": 1335.8, "text": " you didn't really test it. What we were able to do with this was keep getting metrics and", "tokens": [51612, 291, 994, 380, 534, 1500, 309, 13, 708, 321, 645, 1075, 281, 360, 365, 341, 390, 1066, 1242, 16367, 293, 51852], "temperature": 0.0, "avg_logprob": -0.23648432965548533, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.01354729849845171}, {"id": 200, "seek": 133580, "start": 1335.8, "end": 1342.12, "text": " stats out of our system, out of our builds so that we could be the timeline of our deployment", "tokens": [50364, 18152, 484, 295, 527, 1185, 11, 484, 295, 527, 15182, 370, 300, 321, 727, 312, 264, 12933, 295, 527, 19317, 50680], "temperature": 0.0, "avg_logprob": -0.13666556863223805, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.013297811150550842}, {"id": 201, "seek": 133580, "start": 1342.12, "end": 1347.24, "text": " branches. This means that at any given time, we had a single pane of glass to look at what was", "tokens": [50680, 14770, 13, 639, 1355, 300, 412, 604, 2212, 565, 11, 321, 632, 257, 2167, 32605, 295, 4276, 281, 574, 412, 437, 390, 50936], "temperature": 0.0, "avg_logprob": -0.13666556863223805, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.013297811150550842}, {"id": 202, "seek": 133580, "start": 1347.24, "end": 1352.36, "text": " the status of our release processes so that our release team could just go here and check", "tokens": [50936, 264, 6558, 295, 527, 4374, 7555, 370, 300, 527, 4374, 1469, 727, 445, 352, 510, 293, 1520, 51192], "temperature": 0.0, "avg_logprob": -0.13666556863223805, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.013297811150550842}, {"id": 203, "seek": 133580, "start": 1353.72, "end": 1359.32, "text": " whether something was broken they needed to act upon before trying to do our release.", "tokens": [51260, 1968, 746, 390, 5463, 436, 2978, 281, 605, 3564, 949, 1382, 281, 360, 527, 4374, 13, 51540], "temperature": 0.0, "avg_logprob": -0.13666556863223805, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.013297811150550842}, {"id": 204, "seek": 135932, "start": 1359.96, "end": 1368.6, "text": " We also had visibility over the stats over the number of running pipelines or failed pipelines.", "tokens": [50396, 492, 611, 632, 19883, 670, 264, 18152, 670, 264, 1230, 295, 2614, 40168, 420, 7612, 40168, 13, 50828], "temperature": 0.0, "avg_logprob": -0.20534190846912898, "compression_ratio": 1.4946236559139785, "no_speech_prob": 0.02785203978419304}, {"id": 205, "seek": 135932, "start": 1370.28, "end": 1375.96, "text": " We can dig into builds. We can do a lot of different things which we didn't feel like", "tokens": [50912, 492, 393, 2528, 666, 15182, 13, 492, 393, 360, 257, 688, 295, 819, 721, 597, 321, 994, 380, 841, 411, 51196], "temperature": 0.0, "avg_logprob": -0.20534190846912898, "compression_ratio": 1.4946236559139785, "no_speech_prob": 0.02785203978419304}, {"id": 206, "seek": 135932, "start": 1375.96, "end": 1385.48, "text": " they were possible in our CI system UI. What is Unlocks? Really anything. The point here is that", "tokens": [51196, 436, 645, 1944, 294, 527, 37777, 1185, 15682, 13, 708, 307, 1156, 34896, 30, 4083, 1340, 13, 440, 935, 510, 307, 300, 51672], "temperature": 0.0, "avg_logprob": -0.20534190846912898, "compression_ratio": 1.4946236559139785, "no_speech_prob": 0.02785203978419304}, {"id": 207, "seek": 138548, "start": 1386.1200000000001, "end": 1393.24, "text": " we are trying to define standards. We are trying to get into this space. It's a very early stage", "tokens": [50396, 321, 366, 1382, 281, 6964, 7787, 13, 492, 366, 1382, 281, 483, 666, 341, 1901, 13, 467, 311, 257, 588, 2440, 3233, 50752], "temperature": 0.0, "avg_logprob": -0.18576969040764701, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.020707666873931885}, {"id": 208, "seek": 138548, "start": 1393.24, "end": 1399.8, "text": " concept but what it may unlock given that you own your data, it's really up to you. We can talk", "tokens": [50752, 3410, 457, 437, 309, 815, 11634, 2212, 300, 291, 1065, 428, 1412, 11, 309, 311, 534, 493, 281, 291, 13, 492, 393, 751, 51080], "temperature": 0.0, "avg_logprob": -0.18576969040764701, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.020707666873931885}, {"id": 209, "seek": 138548, "start": 1399.8, "end": 1409.4, "text": " about Dora metrics so having ways of reducing mean time to restoring services, we can talk about", "tokens": [51080, 466, 413, 3252, 16367, 370, 1419, 2098, 295, 12245, 914, 565, 281, 36349, 3328, 11, 321, 393, 751, 466, 51560], "temperature": 0.0, "avg_logprob": -0.18576969040764701, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.020707666873931885}, {"id": 210, "seek": 140940, "start": 1410.3600000000001, "end": 1417.0, "text": " generating red like metrics or requests for your CI. How long did it take? Did something that", "tokens": [50412, 17746, 2182, 411, 16367, 420, 12475, 337, 428, 37777, 13, 1012, 938, 630, 309, 747, 30, 2589, 746, 300, 50744], "temperature": 0.0, "avg_logprob": -0.34155421103200606, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.008636855520308018}, {"id": 211, "seek": 140940, "start": 1418.6000000000001, "end": 1426.52, "text": " happened start at the rate of our failing test? The duration started going up because of something", "tokens": [50824, 2011, 722, 412, 264, 3314, 295, 527, 18223, 1500, 30, 440, 16365, 1409, 516, 493, 570, 295, 746, 51220], "temperature": 0.0, "avg_logprob": -0.34155421103200606, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.008636855520308018}, {"id": 212, "seek": 140940, "start": 1426.52, "end": 1431.24, "text": " we did on our environment. That's something that for us it unlocked quite well.", "tokens": [51220, 321, 630, 322, 527, 2823, 13, 663, 311, 746, 300, 337, 505, 309, 30180, 1596, 731, 13, 51456], "temperature": 0.0, "avg_logprob": -0.34155421103200606, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.008636855520308018}, {"id": 213, "seek": 143124, "start": 1431.72, "end": 1440.52, "text": " Some other example, caught coverage over time. There is no reason why you cannot export test", "tokens": [50388, 2188, 661, 1365, 11, 5415, 9645, 670, 565, 13, 821, 307, 572, 1778, 983, 291, 2644, 10725, 1500, 50828], "temperature": 0.0, "avg_logprob": -0.20388521773091864, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.006371587514877319}, {"id": 214, "seek": 143124, "start": 1440.52, "end": 1445.64, "text": " results as JUnit maybe and then graph them on Grafana and keep in track of your coverage.", "tokens": [50828, 3542, 382, 508, 12405, 270, 1310, 293, 550, 4295, 552, 322, 8985, 69, 2095, 293, 1066, 294, 2837, 295, 428, 9645, 13, 51084], "temperature": 0.0, "avg_logprob": -0.20388521773091864, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.006371587514877319}, {"id": 215, "seek": 143124, "start": 1447.8, "end": 1452.44, "text": " You can do flakiness detection like we did before. You start seeing that the test started", "tokens": [51192, 509, 393, 360, 932, 514, 1324, 17784, 411, 321, 630, 949, 13, 509, 722, 2577, 300, 264, 1500, 1409, 51424], "temperature": 0.0, "avg_logprob": -0.20388521773091864, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.006371587514877319}, {"id": 216, "seek": 143124, "start": 1452.44, "end": 1457.72, "text": " failing at some point. You can detect that. You can create an alert on flakiness.", "tokens": [51424, 18223, 412, 512, 935, 13, 509, 393, 5531, 300, 13, 509, 393, 1884, 364, 9615, 322, 932, 514, 1324, 13, 51688], "temperature": 0.0, "avg_logprob": -0.20388521773091864, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.006371587514877319}, {"id": 217, "seek": 145772, "start": 1457.72, "end": 1466.52, "text": " You can trace back to where the test started flaking. At that point we think it's for us", "tokens": [50364, 509, 393, 13508, 646, 281, 689, 264, 1500, 1409, 932, 2456, 13, 1711, 300, 935, 321, 519, 309, 311, 337, 505, 50804], "temperature": 0.0, "avg_logprob": -0.24656986437345807, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.016707004979252815}, {"id": 218, "seek": 145772, "start": 1466.52, "end": 1471.64, "text": " and we think it was way easier to identify what was the actual root cause of the flakiness.", "tokens": [50804, 293, 321, 519, 309, 390, 636, 3571, 281, 5876, 437, 390, 264, 3539, 5593, 3082, 295, 264, 932, 514, 1324, 13, 51060], "temperature": 0.0, "avg_logprob": -0.24656986437345807, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.016707004979252815}, {"id": 219, "seek": 145772, "start": 1473.0, "end": 1478.1200000000001, "text": " Then we have security, whatever. Really, the data is yours. You decide what to do with it.", "tokens": [51128, 1396, 321, 362, 3825, 11, 2035, 13, 4083, 11, 264, 1412, 307, 6342, 13, 509, 4536, 437, 281, 360, 365, 309, 13, 51384], "temperature": 0.0, "avg_logprob": -0.24656986437345807, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.016707004979252815}, {"id": 220, "seek": 145772, "start": 1480.68, "end": 1486.28, "text": " Again, all in all, what is unlocked for us at this point, I think there are three different", "tokens": [51512, 3764, 11, 439, 294, 439, 11, 437, 307, 30180, 337, 505, 412, 341, 935, 11, 286, 519, 456, 366, 1045, 819, 51792], "temperature": 0.0, "avg_logprob": -0.24656986437345807, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.016707004979252815}, {"id": 221, "seek": 148628, "start": 1486.36, "end": 1493.32, "text": " CI systems. We are using three different systems for different reasons. All in all,", "tokens": [50368, 37777, 3652, 13, 492, 366, 1228, 1045, 819, 3652, 337, 819, 4112, 13, 1057, 294, 439, 11, 50716], "temperature": 0.0, "avg_logprob": -0.22887779349711404, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.01234075054526329}, {"id": 222, "seek": 148628, "start": 1493.32, "end": 1500.44, "text": " what is unlocked for us was bringing all the data into there to work with Grafana and to have our", "tokens": [50716, 437, 307, 30180, 337, 505, 390, 5062, 439, 264, 1412, 666, 456, 281, 589, 365, 8985, 69, 2095, 293, 281, 362, 527, 51072], "temperature": 0.0, "avg_logprob": -0.22887779349711404, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.01234075054526329}, {"id": 223, "seek": 148628, "start": 1500.44, "end": 1509.3999999999999, "text": " production metrics together with the pre-production metrics. Now, what's next? We have formed an", "tokens": [51072, 4265, 16367, 1214, 365, 264, 659, 12, 40827, 16367, 13, 823, 11, 437, 311, 958, 30, 492, 362, 8693, 364, 51520], "temperature": 0.0, "avg_logprob": -0.22887779349711404, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.01234075054526329}, {"id": 224, "seek": 150940, "start": 1509.48, "end": 1514.76, "text": " open telemetry working group about CI security observability. There are more stuff to come.", "tokens": [50368, 1269, 4304, 5537, 627, 1364, 1594, 466, 37777, 3825, 9951, 2310, 13, 821, 366, 544, 1507, 281, 808, 13, 50632], "temperature": 0.0, "avg_logprob": -0.1990649149968074, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.079961396753788}, {"id": 225, "seek": 150940, "start": 1517.24, "end": 1521.96, "text": " Join the discussion. If you have your own issue that you want to fix or your own", "tokens": [50756, 19642, 264, 5017, 13, 759, 291, 362, 428, 1065, 2734, 300, 291, 528, 281, 3191, 420, 428, 1065, 50992], "temperature": 0.0, "avg_logprob": -0.1990649149968074, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.079961396753788}, {"id": 226, "seek": 150940, "start": 1521.96, "end": 1524.3600000000001, "text": " use case that you want to bring up to the group, please join the", "tokens": [50992, 764, 1389, 300, 291, 528, 281, 1565, 493, 281, 264, 1594, 11, 1767, 3917, 264, 51112], "temperature": 0.0, "avg_logprob": -0.1990649149968074, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.079961396753788}, {"id": 227, "seek": 150940, "start": 1525.5600000000002, "end": 1531.8000000000002, "text": " Cloud Native Computing Foundation Slack channel. This is the proposal for the standard.", "tokens": [51172, 8061, 15093, 37804, 278, 10335, 37211, 2269, 13, 639, 307, 264, 11494, 337, 264, 3832, 13, 51484], "temperature": 0.0, "avg_logprob": -0.1990649149968074, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.079961396753788}, {"id": 228, "seek": 153180, "start": 1532.76, "end": 1538.6, "text": " That's it. If you have any questions.", "tokens": [50412, 663, 311, 309, 13, 759, 291, 362, 604, 1651, 13, 50704], "temperature": 0.0, "avg_logprob": -0.6756359736124674, "compression_ratio": 1.0303030303030303, "no_speech_prob": 0.04423537477850914}, {"id": 229, "seek": 153180, "start": 1553.56, "end": 1554.9199999999998, "text": " Any questions? Yeah, up there.", "tokens": [51452, 2639, 1651, 30, 865, 11, 493, 456, 13, 51520], "temperature": 0.0, "avg_logprob": -0.6756359736124674, "compression_ratio": 1.0303030303030303, "no_speech_prob": 0.04423537477850914}, {"id": 230, "seek": 155492, "start": 1555.88, "end": 1556.92, "text": " Anything you need?", "tokens": [50412, 11998, 291, 643, 30, 50464], "temperature": 0.0, "avg_logprob": -0.6651257001436673, "compression_ratio": 0.8939393939393939, "no_speech_prob": 0.04445990175008774}, {"id": 231, "seek": 155492, "start": 1558.68, "end": 1560.68, "text": " Yeah, we'll try.", "tokens": [50552, 865, 11, 321, 603, 853, 13, 50652], "temperature": 0.0, "avg_logprob": -0.6651257001436673, "compression_ratio": 0.8939393939393939, "no_speech_prob": 0.04445990175008774}, {"id": 232, "seek": 155492, "start": 1572.92, "end": 1573.5600000000002, "text": " What kind of?", "tokens": [51264, 708, 733, 295, 30, 51296], "temperature": 0.0, "avg_logprob": -0.6651257001436673, "compression_ratio": 0.8939393939393939, "no_speech_prob": 0.04445990175008774}, {"id": 233, "seek": 155492, "start": 1576.52, "end": 1577.0, "text": " Sampling.", "tokens": [51444, 4832, 11970, 13, 51468], "temperature": 0.0, "avg_logprob": -0.6651257001436673, "compression_ratio": 0.8939393939393939, "no_speech_prob": 0.04445990175008774}, {"id": 234, "seek": 158492, "start": 1585.88, "end": 1595.0, "text": " So far, we're not sampling anything. We are collecting a trace for every build that goes", "tokens": [50412, 407, 1400, 11, 321, 434, 406, 21179, 1340, 13, 492, 366, 12510, 257, 13508, 337, 633, 1322, 300, 1709, 50868], "temperature": 0.0, "avg_logprob": -0.2562744516721914, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.09915314614772797}, {"id": 235, "seek": 158492, "start": 1595.0, "end": 1602.1200000000001, "text": " through the CI system. For PRs, it's a bit different because we don't want to create", "tokens": [50868, 807, 264, 37777, 1185, 13, 1171, 11568, 82, 11, 309, 311, 257, 857, 819, 570, 321, 500, 380, 528, 281, 1884, 51224], "temperature": 0.0, "avg_logprob": -0.2562744516721914, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.09915314614772797}, {"id": 236, "seek": 158492, "start": 1602.8400000000001, "end": 1614.68, "text": " bad data, like useless data. It costs money. Data costs money. What we do is we generate data only", "tokens": [51260, 1578, 1412, 11, 411, 14115, 1412, 13, 467, 5497, 1460, 13, 11888, 5497, 1460, 13, 708, 321, 360, 307, 321, 8460, 1412, 787, 51852], "temperature": 0.0, "avg_logprob": -0.2562744516721914, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.09915314614772797}, {"id": 237, "seek": 161468, "start": 1615.0800000000002, "end": 1621.16, "text": " for pipelines that happen on those branches we care about. So if you make a PR and the PR is okay,", "tokens": [50384, 337, 40168, 300, 1051, 322, 729, 14770, 321, 1127, 466, 13, 407, 498, 291, 652, 257, 11568, 293, 264, 11568, 307, 1392, 11, 50688], "temperature": 0.0, "avg_logprob": -0.1903395562801721, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.10909415781497955}, {"id": 238, "seek": 161468, "start": 1622.04, "end": 1628.1200000000001, "text": " it gets merged into main. After it gets merged, we run another pipeline, the same one before the PR,", "tokens": [50732, 309, 2170, 36427, 666, 2135, 13, 2381, 309, 2170, 36427, 11, 321, 1190, 1071, 15517, 11, 264, 912, 472, 949, 264, 11568, 11, 51036], "temperature": 0.0, "avg_logprob": -0.1903395562801721, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.10909415781497955}, {"id": 239, "seek": 161468, "start": 1628.1200000000001, "end": 1634.44, "text": " and that one we collect data from. That way, basically, we have the flakiness on our list", "tokens": [51036, 293, 300, 472, 321, 2500, 1412, 490, 13, 663, 636, 11, 1936, 11, 321, 362, 264, 932, 514, 1324, 322, 527, 1329, 51352], "temperature": 0.0, "avg_logprob": -0.1903395562801721, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.10909415781497955}, {"id": 240, "seek": 161468, "start": 1634.44, "end": 1639.0800000000002, "text": " branch and not on the PRs because in PRs, I mean, it's not flaky. I mean, okay, we can", "tokens": [51352, 9819, 293, 406, 322, 264, 11568, 82, 570, 294, 11568, 82, 11, 286, 914, 11, 309, 311, 406, 932, 15681, 13, 286, 914, 11, 1392, 11, 321, 393, 51584], "temperature": 0.0, "avg_logprob": -0.1903395562801721, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.10909415781497955}, {"id": 241, "seek": 163908, "start": 1639.08, "end": 1643.96, "text": " reflect it as in PRs, but maybe we are doing something and it breaks the build, but maybe it's", "tokens": [50364, 5031, 309, 382, 294, 11568, 82, 11, 457, 1310, 321, 366, 884, 746, 293, 309, 9857, 264, 1322, 11, 457, 1310, 309, 311, 50608], "temperature": 0.0, "avg_logprob": -0.20499824896091368, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.035263534635305405}, {"id": 242, "seek": 163908, "start": 1643.96, "end": 1651.8, "text": " not a vital point. Yeah, if we did that for every branch, basically, we would face cardinality", "tokens": [50608, 406, 257, 11707, 935, 13, 865, 11, 498, 321, 630, 300, 337, 633, 9819, 11, 1936, 11, 321, 576, 1851, 2920, 259, 1860, 51000], "temperature": 0.0, "avg_logprob": -0.20499824896091368, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.035263534635305405}, {"id": 243, "seek": 163908, "start": 1651.8, "end": 1657.08, "text": " explosion and it's going to be so expensive. So you have to define which branches you're interested", "tokens": [51000, 15673, 293, 309, 311, 516, 281, 312, 370, 5124, 13, 407, 291, 362, 281, 6964, 597, 14770, 291, 434, 3102, 51264], "temperature": 0.0, "avg_logprob": -0.20499824896091368, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.035263534635305405}, {"id": 244, "seek": 163908, "start": 1657.08, "end": 1661.56, "text": " in. For example, in Grafana, we have the main branch, which is like the main branch of our repo,", "tokens": [51264, 294, 13, 1171, 1365, 11, 294, 8985, 69, 2095, 11, 321, 362, 264, 2135, 9819, 11, 597, 307, 411, 264, 2135, 9819, 295, 527, 49040, 11, 51488], "temperature": 0.0, "avg_logprob": -0.20499824896091368, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.035263534635305405}, {"id": 245, "seek": 163908, "start": 1661.56, "end": 1666.4399999999998, "text": " and then some version branches for all the different versions that Grafana have, for example.", "tokens": [51488, 293, 550, 512, 3037, 14770, 337, 439, 264, 819, 9606, 300, 8985, 69, 2095, 362, 11, 337, 1365, 13, 51732], "temperature": 0.0, "avg_logprob": -0.20499824896091368, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.035263534635305405}, {"id": 246, "seek": 166644, "start": 1666.44, "end": 1672.04, "text": " And this is what we're interested in. But again, you're on the data. You can decide to do it all", "tokens": [50364, 400, 341, 307, 437, 321, 434, 3102, 294, 13, 583, 797, 11, 291, 434, 322, 264, 1412, 13, 509, 393, 4536, 281, 360, 309, 439, 50644], "temperature": 0.0, "avg_logprob": -0.3660203116280692, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.01922479271888733}, {"id": 247, "seek": 166644, "start": 1672.04, "end": 1675.56, "text": " your way. Any other? Yeah.", "tokens": [50644, 428, 636, 13, 2639, 661, 30, 865, 13, 50820], "temperature": 0.0, "avg_logprob": -0.3660203116280692, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.01922479271888733}, {"id": 248, "seek": 166644, "start": 1675.56, "end": 1680.52, "text": " How many flaky tests or else have you found by exporting data from this?", "tokens": [50820, 1012, 867, 932, 15681, 6921, 420, 1646, 362, 291, 1352, 538, 44686, 1412, 490, 341, 30, 51068], "temperature": 0.0, "avg_logprob": -0.3660203116280692, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.01922479271888733}, {"id": 249, "seek": 166644, "start": 1681.88, "end": 1689.24, "text": " What's the case? Yeah, number of unnecessary... Oh, yeah, sorry. Apart from flaky tests,", "tokens": [51136, 708, 311, 264, 1389, 30, 865, 11, 1230, 295, 19350, 485, 876, 11, 1338, 11, 2597, 13, 24111, 490, 932, 15681, 6921, 11, 51504], "temperature": 0.0, "avg_logprob": -0.3660203116280692, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.01922479271888733}, {"id": 250, "seek": 166644, "start": 1689.24, "end": 1693.24, "text": " what other metrics can we get, like useful metrics we can get out of that? So I think...", "tokens": [51504, 437, 661, 16367, 393, 321, 483, 11, 411, 4420, 16367, 321, 393, 483, 484, 295, 300, 30, 407, 286, 519, 485, 51704], "temperature": 0.0, "avg_logprob": -0.3660203116280692, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.01922479271888733}, {"id": 251, "seek": 169324, "start": 1694.04, "end": 1698.36, "text": " No, no, no. What other problems have you encountered?", "tokens": [50404, 883, 11, 572, 11, 572, 13, 708, 661, 2740, 362, 291, 20381, 30, 50620], "temperature": 0.0, "avg_logprob": -0.35514124702004823, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.014457006007432938}, {"id": 252, "seek": 169324, "start": 1698.36, "end": 1702.36, "text": " Like you found out that you didn't find what you're just looking at?", "tokens": [50620, 1743, 291, 1352, 484, 300, 291, 994, 380, 915, 437, 291, 434, 445, 1237, 412, 30, 50820], "temperature": 0.0, "avg_logprob": -0.35514124702004823, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.014457006007432938}, {"id": 253, "seek": 169324, "start": 1702.36, "end": 1712.28, "text": " For example, stack runners. Runners were stuck in unused repositories. We didn't have a way to know", "tokens": [50820, 1171, 1365, 11, 8630, 33892, 13, 8950, 2999, 645, 5541, 294, 44383, 22283, 2083, 13, 492, 994, 380, 362, 257, 636, 281, 458, 51316], "temperature": 0.0, "avg_logprob": -0.35514124702004823, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.014457006007432938}, {"id": 254, "seek": 169324, "start": 1712.28, "end": 1717.56, "text": " that there was a runner running all the time. We're getting timeouts and all that. This one problem.", "tokens": [51316, 300, 456, 390, 257, 24376, 2614, 439, 264, 565, 13, 492, 434, 1242, 565, 7711, 293, 439, 300, 13, 639, 472, 1154, 13, 51580], "temperature": 0.0, "avg_logprob": -0.35514124702004823, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.014457006007432938}, {"id": 255, "seek": 171756, "start": 1717.6399999999999, "end": 1724.44, "text": " Then another problem is the number of restarts in builds, which is basically related to flaky tests.", "tokens": [50368, 1396, 1071, 1154, 307, 264, 1230, 295, 1472, 11814, 294, 15182, 11, 597, 307, 1936, 4077, 281, 932, 15681, 6921, 13, 50708], "temperature": 0.0, "avg_logprob": -0.13082031408945718, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.02987075224518776}, {"id": 256, "seek": 171756, "start": 1724.44, "end": 1732.76, "text": " But there was no way for us to know how many, like for Geo, for example, went and restarted his", "tokens": [50708, 583, 456, 390, 572, 636, 337, 505, 281, 458, 577, 867, 11, 411, 337, 2876, 78, 11, 337, 1365, 11, 1437, 293, 21022, 292, 702, 51124], "temperature": 0.0, "avg_logprob": -0.13082031408945718, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.02987075224518776}, {"id": 257, "seek": 171756, "start": 1732.76, "end": 1736.9199999999998, "text": " build because it was problematic, because there was a bug, or because there was an actual issue", "tokens": [51124, 1322, 570, 309, 390, 19011, 11, 570, 456, 390, 257, 7426, 11, 420, 570, 456, 390, 364, 3539, 2734, 51332], "temperature": 0.0, "avg_logprob": -0.13082031408945718, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.02987075224518776}, {"id": 258, "seek": 171756, "start": 1736.9199999999998, "end": 1743.24, "text": " with runner. It doesn't have to be necessarily code related. So we needed to know how big was", "tokens": [51332, 365, 24376, 13, 467, 1177, 380, 362, 281, 312, 4725, 3089, 4077, 13, 407, 321, 2978, 281, 458, 577, 955, 390, 51648], "temperature": 0.0, "avg_logprob": -0.13082031408945718, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.02987075224518776}, {"id": 259, "seek": 174324, "start": 1743.24, "end": 1749.56, "text": " the number of the restarts and then try to find the root cause of what caused this, basically.", "tokens": [50364, 264, 1230, 295, 264, 1472, 11814, 293, 550, 853, 281, 915, 264, 5593, 3082, 295, 437, 7008, 341, 11, 1936, 13, 50680], "temperature": 0.0, "avg_logprob": -0.18992117549596207, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.03802594169974327}, {"id": 260, "seek": 174324, "start": 1750.36, "end": 1756.2, "text": " There was also something I want to talk about, maybe, is that we are also able to improve a bit", "tokens": [50720, 821, 390, 611, 746, 286, 528, 281, 751, 466, 11, 1310, 11, 307, 300, 321, 366, 611, 1075, 281, 3470, 257, 857, 51012], "temperature": 0.0, "avg_logprob": -0.18992117549596207, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.03802594169974327}, {"id": 261, "seek": 174324, "start": 1756.2, "end": 1762.04, "text": " of the performance of our pipeline. And by performance, I mean just allocating more resources.", "tokens": [51012, 295, 264, 3389, 295, 527, 15517, 13, 400, 538, 3389, 11, 286, 914, 445, 12660, 990, 544, 3593, 13, 51304], "temperature": 0.0, "avg_logprob": -0.18992117549596207, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.03802594169974327}, {"id": 262, "seek": 174324, "start": 1763.0, "end": 1766.68, "text": " By doing that, we were also maybe able to reduce the cost of the bit because", "tokens": [51352, 3146, 884, 300, 11, 321, 645, 611, 1310, 1075, 281, 5407, 264, 2063, 295, 264, 857, 570, 51536], "temperature": 0.0, "avg_logprob": -0.18992117549596207, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.03802594169974327}, {"id": 263, "seek": 176668, "start": 1767.24, "end": 1773.24, "text": " the runner where pipelines were running for shorter, there was less queue.", "tokens": [50392, 264, 24376, 689, 40168, 645, 2614, 337, 11639, 11, 456, 390, 1570, 18639, 13, 50692], "temperature": 0.0, "avg_logprob": -0.2594701549674891, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.02145226299762726}, {"id": 264, "seek": 176668, "start": 1774.44, "end": 1779.0, "text": " So it's also like improving performance also comes from having the data about", "tokens": [50752, 407, 309, 311, 611, 411, 11470, 3389, 611, 1487, 490, 1419, 264, 1412, 466, 50980], "temperature": 0.0, "avg_logprob": -0.2594701549674891, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.02145226299762726}, {"id": 265, "seek": 176668, "start": 1780.68, "end": 1789.16, "text": " how long they take. And also, last thing is that we also had issues where we used extremely", "tokens": [51064, 577, 938, 436, 747, 13, 400, 611, 11, 1036, 551, 307, 300, 321, 611, 632, 2663, 689, 321, 1143, 4664, 51488], "temperature": 0.0, "avg_logprob": -0.2594701549674891, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.02145226299762726}, {"id": 266, "seek": 176668, "start": 1789.16, "end": 1796.2, "text": " powerful runners to build docs, for example. And docs builds took, I don't know, a minute", "tokens": [51488, 4005, 33892, 281, 1322, 45623, 11, 337, 1365, 13, 400, 45623, 15182, 1890, 11, 286, 500, 380, 458, 11, 257, 3456, 51840], "temperature": 0.0, "avg_logprob": -0.2594701549674891, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.02145226299762726}, {"id": 267, "seek": 179620, "start": 1796.28, "end": 1800.1200000000001, "text": " where if the docs build took like five minutes, it was not going to be the end of the world,", "tokens": [50368, 689, 498, 264, 45623, 1322, 1890, 411, 1732, 2077, 11, 309, 390, 406, 516, 281, 312, 264, 917, 295, 264, 1002, 11, 50560], "temperature": 0.0, "avg_logprob": -0.20400423752634148, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.008712416514754295}, {"id": 268, "seek": 179620, "start": 1800.1200000000001, "end": 1804.2, "text": " because there are docs, they're just small changes, really important changes, don't get me wrong,", "tokens": [50560, 570, 456, 366, 45623, 11, 436, 434, 445, 1359, 2962, 11, 534, 1021, 2962, 11, 500, 380, 483, 385, 2085, 11, 50764], "temperature": 0.0, "avg_logprob": -0.20400423752634148, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.008712416514754295}, {"id": 269, "seek": 179620, "start": 1804.2, "end": 1809.96, "text": " but small. So we could move away from really powerful runners to something smaller just to", "tokens": [50764, 457, 1359, 13, 407, 321, 727, 1286, 1314, 490, 534, 4005, 33892, 281, 746, 4356, 445, 281, 51052], "temperature": 0.0, "avg_logprob": -0.20400423752634148, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.008712416514754295}, {"id": 270, "seek": 179620, "start": 1810.68, "end": 1818.04, "text": " help with some cost reduction and stuff. Any other questions? Do we have time?", "tokens": [51088, 854, 365, 512, 2063, 11004, 293, 1507, 13, 2639, 661, 1651, 30, 1144, 321, 362, 565, 30, 51456], "temperature": 0.0, "avg_logprob": -0.20400423752634148, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.008712416514754295}, {"id": 271, "seek": 179620, "start": 1818.04, "end": 1825.32, "text": " Do we have one up there? Do we have time? No? Come join us at the Grafana booth, please.", "tokens": [51456, 1144, 321, 362, 472, 493, 456, 30, 1144, 321, 362, 565, 30, 883, 30, 2492, 3917, 505, 412, 264, 8985, 69, 2095, 20912, 11, 1767, 13, 51820], "temperature": 0.0, "avg_logprob": -0.20400423752634148, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.008712416514754295}, {"id": 272, "seek": 182620, "start": 1826.2, "end": 1829.4, "text": " Thank you.", "tokens": [50380, 1044, 291, 13, 50524], "temperature": 0.0, "avg_logprob": -0.8442519505818685, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.8662745952606201}], "language": "en"}