{"text": " So, thank you. My name is Ion Blackus and in this presentation we're going to talk about the Unikernels and their integration challenges in cloud native environments. So, first we're going to set the scene, talk about containers and some books mechanism and then we're going to read the Unikernels and talk about our own OCI compatible container runtime, Uran-C. And after that some demos and evaluation results sections. So, yeah, we're a team of researchers with mixed industrial and academic background, mostly focused in virtualization, container untimes and hardware acceleration. So, yeah, containers is the standard right way to deploy and package your application. They're portable, they can be run both in cloud and net. They're easy to scale with the support of a wide ecosystem and they have super fast phone times. But they come with major risk when it comes to multi-tendent scenarios. Multiple containers serve the same kernel and rely on software components for their isolation. So, yeah, in a malicious workload scenario, in a previous escalation, container impact the entire host. So, what about vendors did was to either use software-assisted solutions with tools like SACOM or a Parmore or hardware-assisted solutions, VM type solutions with tools like Farcracker and Deviser. So, yeah, the thing becomes like this. We have now, we deploy our application inside the container, inside the VM, totally isolated from the last system and we get to keep actually the benefits of container, the portability and the scalability and we also kind of resolve the isolation issue. But of course, this comes with side effects, since higher overhead because of CPU and memory provisioning for the VM and the much more complex system stack that needs to be maintained. By taking a step back, we can see that the application does not need all these parts, not run. It just needs the run time, the libraries and some parts of the OS, like the drivers. And, yeah, this kind of modeling is enabled by technology called the Unikanal. So, what's Unikanal? It's a specialized single-hatter space that contains exactly the necessary parts for the application to run. So, yeah, this leads to reduced attack surface and faster boot times, which is especially crucial in serverless scenarios where responsiveness matters. And, yeah, but Unikanals are not widely adopted yet. And why is that? We identified too many issues. The first one is packaging and, yeah, Unikanals should look like an OS image in order to utilize because this is support. And the second one, the more, more than, is deployment and the execution of Unikanals. Container run times need to be extended. Additional logic is needed in order to execute Unikanals. And with this, I would like to give a floor to your viewers to talk about your ANSI. Thank you, Yanis. Hello, everyone. So, to solve the deployment challenge of Unikanals, we introduce URAN-C, which is a Unikanal container runtime. It is fully CRI-compatible. It's written in Go. Actually, it's a CLI tool which makes use of interconnected Go packages to actually spawn the Unikanals. It reads Unikanals as processes. So, in a way, it directly manages the application and not the system in which the application runs. The images, the Unikanal images required to run these Unikanals are typical OSI artifacts. And in order to actually spawn these Unikanal VMs, we make use of underlying hypervisors. So, first, let's take a look at how Unikanal images looks like. First of all, they are standard OSI images, so they can be managed via standard tooling and can be distributed using already existing registries. But there is one differentiating factor. URAN-C needs some specific annotations to function. These annotations are the Unikanal binary path inside the root of S, the Unikanal type, the hypervisor type, the command line that we need to pass to the Unikanal. And optionally, if we are using IneterD, the path to the IneterD file. So, to facilitate the packaging of the Unikanal, we created a simple image builder called BIMMA, which uses simple Docker file like syntax to create the images. As you can see, it's pretty typical for anyone who has used Docker. It's practically the same thing. So, now we have seen how an OSI Unikanal image looks. Let's take a closer look at how Unikanal actually spawns a Unikanal. First of all, container.dc invokes URAN-C create. URAN-C create then sets up a new network name space, create a new name space, sets up a pseudo terminal if it's required, and spawns URAN-C reexec process inside that name space. The reexec process then notifies the parent process that had started. Then, URAN-C, the URAN-C create, the original process, saves the state, the PID of the reexec process, etc. Executes any create runtime hooks, and then sends an OK IPC message to the reexec process, executes any create container hooks, and then exits. Then, container.dc invokes URAN-C start, which sends an IPC message to the reexec process, executes post start hooks, and exits as well. So, now it's the most interesting part. The reexec process actually sets up any necessary network and storage components, for example, the W device, etc. Executes any start container hooks, and actually spawns the Unikanal VM. So, as you can see, this is a pretty typical life cycle for any container runtime, with just some minor adjustments to facilitate the Unikanal execution. So, to actually spawn the Unikanals, we use hypervisors. We made it really easy to integrate any new hypervisors you want to implement in the system. So, I'm going to show you how to do this with the Unikanal VM. So, in this case, you can just implement this interface, which is mostly just the exec V function. So, it's really easy. Currently, we have support for Solo 5, Kimu, and Firecracker. For storage, we have support for block device via the map or snapshotter. We have support for InitrD, which is packed inside the image. And we also have support for shared effects coming soon. In the diagram, you can see how an image looks like, the layers look like. So, for the network part, we followed the every simple approach that is also used by sandboxed runtimes, like Cata containers. We create a new top device inside the container network loading space. And then we breed all the data, all the traffic to the VF endpoint provided by CNI. We do this using traffic control. To integrate Unikanals in Kubernetes, we had the Asphalt Challenge. That's because we need to actually spawn non-Unikanal containers inside the same pod. For example, the POS container or any other side-con containers. To achieve this, we use RunC to spawn the generic containers. And then, RunC handles the Unikanal containers inside the network namespace of the pod. So, there are some really interesting use cases, for example, KN80, in which we need to have intrapod Unikanal container communication. In KN80, for example, the QPROXY container needs to be able to communicate with a user function, which is Unikanal. To achieve this, we implement a static network configuration. We provide the static IP to the top device. So, we handle it that way. So, now, let's see RunC in action. We will see a simple deployment using an HDL. We will pull the image from the registry. And using an HDL, we will actually spawn an NGNX Unikanal inside VM. So, as we can see, there are no containers running right now. We pull the image from our registry. Okay, it's already existing. And now, we can run it using an HDL. We have to define the runtime. So, we do that. Okay, it spawned. And now, we can see that it started six seconds ago. It was created. Perfect. So, now, we can inspect the container to find the IP address. Okay. And if we curl it, we can see that it's an NGNX server built using Unicraft. Pretty typical. So, now, we can see the actual run. And we can see that it's running. And the container is in the RunC process. That's also running. Okay. And now, with that, I will give the floor back to Janis to show you a more elaborate example with K-nate. Okay. So, now, just... Okay, that's bad. Now, let's deploy a serverless workload with RunC. So, what we first do here is that we see that we have another RunC process running in the cluster. And after this, what we need to do is to define the RunC class for the K-H cluster. You can see here, we apply the RunC class. And then, it's time to define the K-native service. We can see that your RunC container around them is specified. And a simple HTTP-reply-server workload is used as the workload of the serverless function. We apply the K-native service. And then, we will retrieve back the URL endpoint, which by triggering it with a simple HTTP request, a simple HTTP get, essentially, we start the execution of the serverless workload. So, here, we can see the curve. And after this, the pods are going to be running. And underneath, there will be the RunC process with the K-mode hypervisors with the sandbox workload. So, yeah. That's it. And... So, the evaluation section. In order to evaluate your RunC, we convert with other container runtimes, such as divisor and other containers. And, yeah, in that process, we utilize the tool called K-perf responsible for generating and triggering K-native services via HTTP request, as we saw in the demo. And also, responsible for reporting the service latencies. So, yeah, the scale from zero, evaluation scenarios like this, for a number of iterations, we scale. It's a K-native service. And we report at the end of the number of executions, we report the responsible latency. We do this for every other container runtime. So, these are the results. We can see on the X axis the different container runtimes used for that process. And on the Y axis, the service response latency, seconds. And, yeah, of course, lower is better. So, on the blog post, with the experiment setup and all the parameters setting for K-perf. That's all. Thank you. Thank you. Okay. So, the question is about memory benchmarking, right? Yeah. Memory benchmarking is not yet on our work, but we have plans on that also. Yeah. Something that we can do. Sorry. So, the question is if we have run in Germany, AWS, I don't know. Actually, this experiment was on the Prime Service. We have not yet experienced any big lab vendors and deployments. So, hopefully, maybe the next evaluation will be also part with major vendors. That was the end. Okay. So, okay. I heard something about paravirtualization, right? But, yeah. Okay. I'm not sure that that's. Okay. So, I think that's it. Thank you. Thank you so much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.0, "text": " So, thank you.", "tokens": [50364, 407, 11, 1309, 291, 13, 51014], "temperature": 0.0, "avg_logprob": -0.5601956194097345, "compression_ratio": 1.2769230769230768, "no_speech_prob": 0.543345034122467}, {"id": 1, "seek": 0, "start": 13.0, "end": 20.0, "text": " My name is Ion Blackus and in this presentation we're going to talk about the Unikernels and", "tokens": [51014, 1222, 1315, 307, 286, 266, 4076, 301, 293, 294, 341, 5860, 321, 434, 516, 281, 751, 466, 264, 1156, 1035, 1248, 1625, 293, 51364], "temperature": 0.0, "avg_logprob": -0.5601956194097345, "compression_ratio": 1.2769230769230768, "no_speech_prob": 0.543345034122467}, {"id": 2, "seek": 0, "start": 20.0, "end": 24.0, "text": " their integration challenges in cloud native environments.", "tokens": [51364, 641, 10980, 4759, 294, 4588, 8470, 12388, 13, 51564], "temperature": 0.0, "avg_logprob": -0.5601956194097345, "compression_ratio": 1.2769230769230768, "no_speech_prob": 0.543345034122467}, {"id": 3, "seek": 2400, "start": 24.0, "end": 30.0, "text": " So, first we're going to set the scene, talk about containers and some books mechanism", "tokens": [50364, 407, 11, 700, 321, 434, 516, 281, 992, 264, 4145, 11, 751, 466, 17089, 293, 512, 3642, 7513, 50664], "temperature": 0.0, "avg_logprob": -0.24094721782638365, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.1713929921388626}, {"id": 4, "seek": 2400, "start": 30.0, "end": 37.0, "text": " and then we're going to read the Unikernels and talk about our own OCI compatible container", "tokens": [50664, 293, 550, 321, 434, 516, 281, 1401, 264, 1156, 1035, 1248, 1625, 293, 751, 466, 527, 1065, 422, 25240, 18218, 10129, 51014], "temperature": 0.0, "avg_logprob": -0.24094721782638365, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.1713929921388626}, {"id": 5, "seek": 2400, "start": 37.0, "end": 44.0, "text": " runtime, Uran-C. And after that some demos and evaluation results sections.", "tokens": [51014, 34474, 11, 624, 4257, 12, 34, 13, 400, 934, 300, 512, 33788, 293, 13344, 3542, 10863, 13, 51364], "temperature": 0.0, "avg_logprob": -0.24094721782638365, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.1713929921388626}, {"id": 6, "seek": 2400, "start": 44.0, "end": 49.0, "text": " So, yeah, we're a team of researchers with mixed industrial and academic background,", "tokens": [51364, 407, 11, 1338, 11, 321, 434, 257, 1469, 295, 10309, 365, 7467, 9987, 293, 7778, 3678, 11, 51614], "temperature": 0.0, "avg_logprob": -0.24094721782638365, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.1713929921388626}, {"id": 7, "seek": 4900, "start": 49.0, "end": 55.0, "text": " mostly focused in virtualization, container untimes and hardware acceleration.", "tokens": [50364, 5240, 5178, 294, 6374, 2144, 11, 10129, 1701, 1532, 293, 8837, 17162, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2371705820862676, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.07002335041761398}, {"id": 8, "seek": 4900, "start": 55.0, "end": 62.0, "text": " So, yeah, containers is the standard right way to deploy and package your application.", "tokens": [50664, 407, 11, 1338, 11, 17089, 307, 264, 3832, 558, 636, 281, 7274, 293, 7372, 428, 3861, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2371705820862676, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.07002335041761398}, {"id": 9, "seek": 4900, "start": 62.0, "end": 65.0, "text": " They're portable, they can be run both in cloud and net.", "tokens": [51014, 814, 434, 21800, 11, 436, 393, 312, 1190, 1293, 294, 4588, 293, 2533, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2371705820862676, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.07002335041761398}, {"id": 10, "seek": 4900, "start": 65.0, "end": 71.0, "text": " They're easy to scale with the support of a wide ecosystem and they have super fast", "tokens": [51164, 814, 434, 1858, 281, 4373, 365, 264, 1406, 295, 257, 4874, 11311, 293, 436, 362, 1687, 2370, 51464], "temperature": 0.0, "avg_logprob": -0.2371705820862676, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.07002335041761398}, {"id": 11, "seek": 7100, "start": 71.0, "end": 78.0, "text": " phone times. But they come with major risk when it comes to multi-tendent scenarios.", "tokens": [50364, 2593, 1413, 13, 583, 436, 808, 365, 2563, 3148, 562, 309, 1487, 281, 4825, 12, 83, 521, 317, 15077, 13, 50714], "temperature": 0.0, "avg_logprob": -0.25908075968424477, "compression_ratio": 1.5235294117647058, "no_speech_prob": 0.18705572187900543}, {"id": 12, "seek": 7100, "start": 78.0, "end": 84.0, "text": " Multiple containers serve the same kernel and rely on software components for their", "tokens": [50714, 40056, 17089, 4596, 264, 912, 28256, 293, 10687, 322, 4722, 6677, 337, 641, 51014], "temperature": 0.0, "avg_logprob": -0.25908075968424477, "compression_ratio": 1.5235294117647058, "no_speech_prob": 0.18705572187900543}, {"id": 13, "seek": 7100, "start": 84.0, "end": 95.0, "text": " isolation. So, yeah, in a malicious workload scenario, in a previous escalation, container", "tokens": [51014, 16001, 13, 407, 11, 1338, 11, 294, 257, 33496, 20139, 9005, 11, 294, 257, 3894, 17871, 399, 11, 10129, 51564], "temperature": 0.0, "avg_logprob": -0.25908075968424477, "compression_ratio": 1.5235294117647058, "no_speech_prob": 0.18705572187900543}, {"id": 14, "seek": 9500, "start": 95.0, "end": 101.0, "text": " impact the entire host. So, what about vendors did was to either use software-assisted", "tokens": [50364, 2712, 264, 2302, 3975, 13, 407, 11, 437, 466, 22056, 630, 390, 281, 2139, 764, 4722, 12, 640, 33250, 50664], "temperature": 0.0, "avg_logprob": -0.3704336339777166, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.4837913513183594}, {"id": 15, "seek": 9500, "start": 101.0, "end": 107.0, "text": " solutions with tools like SACOM or a Parmore or hardware-assisted solutions, VM type solutions", "tokens": [50664, 6547, 365, 3873, 411, 318, 4378, 5251, 420, 257, 3457, 3138, 420, 8837, 12, 640, 33250, 6547, 11, 18038, 2010, 6547, 50964], "temperature": 0.0, "avg_logprob": -0.3704336339777166, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.4837913513183594}, {"id": 16, "seek": 9500, "start": 107.0, "end": 116.0, "text": " with tools like Farcracker and Deviser. So, yeah, the thing becomes like this.", "tokens": [50964, 365, 3873, 411, 9067, 10757, 23599, 293, 9096, 6694, 13, 407, 11, 1338, 11, 264, 551, 3643, 411, 341, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3704336339777166, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.4837913513183594}, {"id": 17, "seek": 9500, "start": 116.0, "end": 123.0, "text": " We have now, we deploy our application inside the container, inside the VM, totally isolated", "tokens": [51414, 492, 362, 586, 11, 321, 7274, 527, 3861, 1854, 264, 10129, 11, 1854, 264, 18038, 11, 3879, 14621, 51764], "temperature": 0.0, "avg_logprob": -0.3704336339777166, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.4837913513183594}, {"id": 18, "seek": 12300, "start": 123.0, "end": 131.0, "text": " from the last system and we get to keep actually the benefits of container, the portability", "tokens": [50364, 490, 264, 1036, 1185, 293, 321, 483, 281, 1066, 767, 264, 5311, 295, 10129, 11, 264, 2436, 2310, 50764], "temperature": 0.0, "avg_logprob": -0.1945387673756433, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.10093846172094345}, {"id": 19, "seek": 12300, "start": 131.0, "end": 138.0, "text": " and the scalability and we also kind of resolve the isolation issue. But of course, this comes", "tokens": [50764, 293, 264, 15664, 2310, 293, 321, 611, 733, 295, 14151, 264, 16001, 2734, 13, 583, 295, 1164, 11, 341, 1487, 51114], "temperature": 0.0, "avg_logprob": -0.1945387673756433, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.10093846172094345}, {"id": 20, "seek": 12300, "start": 138.0, "end": 146.0, "text": " with side effects, since higher overhead because of CPU and memory provisioning for the VM", "tokens": [51114, 365, 1252, 5065, 11, 1670, 2946, 19922, 570, 295, 13199, 293, 4675, 17225, 278, 337, 264, 18038, 51514], "temperature": 0.0, "avg_logprob": -0.1945387673756433, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.10093846172094345}, {"id": 21, "seek": 14600, "start": 146.0, "end": 156.0, "text": " and the much more complex system stack that needs to be maintained. By taking a step back,", "tokens": [50364, 293, 264, 709, 544, 3997, 1185, 8630, 300, 2203, 281, 312, 17578, 13, 3146, 1940, 257, 1823, 646, 11, 50864], "temperature": 0.0, "avg_logprob": -0.17921714506287506, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.20610129833221436}, {"id": 22, "seek": 14600, "start": 156.0, "end": 162.0, "text": " we can see that the application does not need all these parts, not run. It just needs the", "tokens": [50864, 321, 393, 536, 300, 264, 3861, 775, 406, 643, 439, 613, 3166, 11, 406, 1190, 13, 467, 445, 2203, 264, 51164], "temperature": 0.0, "avg_logprob": -0.17921714506287506, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.20610129833221436}, {"id": 23, "seek": 14600, "start": 162.0, "end": 169.0, "text": " run time, the libraries and some parts of the OS, like the drivers. And, yeah, this kind", "tokens": [51164, 1190, 565, 11, 264, 15148, 293, 512, 3166, 295, 264, 12731, 11, 411, 264, 11590, 13, 400, 11, 1338, 11, 341, 733, 51514], "temperature": 0.0, "avg_logprob": -0.17921714506287506, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.20610129833221436}, {"id": 24, "seek": 16900, "start": 169.0, "end": 177.0, "text": " of modeling is enabled by technology called the Unikanal. So, what's Unikanal? It's a", "tokens": [50364, 295, 15983, 307, 15172, 538, 2899, 1219, 264, 1156, 16048, 304, 13, 407, 11, 437, 311, 1156, 16048, 304, 30, 467, 311, 257, 50764], "temperature": 0.0, "avg_logprob": -0.263578301045432, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.10998008400201797}, {"id": 25, "seek": 16900, "start": 177.0, "end": 183.0, "text": " specialized single-hatter space that contains exactly the necessary parts for the application", "tokens": [50764, 19813, 2167, 12, 71, 1161, 1901, 300, 8306, 2293, 264, 4818, 3166, 337, 264, 3861, 51064], "temperature": 0.0, "avg_logprob": -0.263578301045432, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.10998008400201797}, {"id": 26, "seek": 16900, "start": 183.0, "end": 191.0, "text": " to run. So, yeah, this leads to reduced attack surface and faster boot times, which is especially", "tokens": [51064, 281, 1190, 13, 407, 11, 1338, 11, 341, 6689, 281, 9212, 2690, 3753, 293, 4663, 11450, 1413, 11, 597, 307, 2318, 51464], "temperature": 0.0, "avg_logprob": -0.263578301045432, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.10998008400201797}, {"id": 27, "seek": 19100, "start": 191.0, "end": 200.0, "text": " crucial in serverless scenarios where responsiveness matters. And, yeah, but Unikanals are not", "tokens": [50364, 11462, 294, 7154, 1832, 15077, 689, 2914, 8477, 7001, 13, 400, 11, 1338, 11, 457, 1156, 16048, 1124, 366, 406, 50814], "temperature": 0.0, "avg_logprob": -0.25087686018510297, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.14646866917610168}, {"id": 28, "seek": 19100, "start": 200.0, "end": 209.0, "text": " widely adopted yet. And why is that? We identified too many issues. The first one is", "tokens": [50814, 13371, 12175, 1939, 13, 400, 983, 307, 300, 30, 492, 9234, 886, 867, 2663, 13, 440, 700, 472, 307, 51264], "temperature": 0.0, "avg_logprob": -0.25087686018510297, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.14646866917610168}, {"id": 29, "seek": 19100, "start": 209.0, "end": 215.0, "text": " packaging and, yeah, Unikanals should look like an OS image in order to utilize because", "tokens": [51264, 16836, 293, 11, 1338, 11, 1156, 16048, 1124, 820, 574, 411, 364, 12731, 3256, 294, 1668, 281, 16117, 570, 51564], "temperature": 0.0, "avg_logprob": -0.25087686018510297, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.14646866917610168}, {"id": 30, "seek": 21500, "start": 215.0, "end": 222.0, "text": " this is support. And the second one, the more, more than, is deployment and the execution of", "tokens": [50364, 341, 307, 1406, 13, 400, 264, 1150, 472, 11, 264, 544, 11, 544, 813, 11, 307, 19317, 293, 264, 15058, 295, 50714], "temperature": 0.0, "avg_logprob": -0.213695911446003, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.06380791962146759}, {"id": 31, "seek": 21500, "start": 222.0, "end": 228.0, "text": " Unikanals. Container run times need to be extended. Additional logic is needed in order to", "tokens": [50714, 1156, 16048, 1124, 13, 43732, 260, 1190, 1413, 643, 281, 312, 10913, 13, 44272, 9952, 307, 2978, 294, 1668, 281, 51014], "temperature": 0.0, "avg_logprob": -0.213695911446003, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.06380791962146759}, {"id": 32, "seek": 21500, "start": 228.0, "end": 234.0, "text": " execute Unikanals. And with this, I would like to give a floor to your viewers to talk about", "tokens": [51014, 14483, 1156, 16048, 1124, 13, 400, 365, 341, 11, 286, 576, 411, 281, 976, 257, 4123, 281, 428, 8499, 281, 751, 466, 51314], "temperature": 0.0, "avg_logprob": -0.213695911446003, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.06380791962146759}, {"id": 33, "seek": 21500, "start": 234.0, "end": 244.0, "text": " your ANSI. Thank you, Yanis. Hello, everyone. So, to solve the deployment challenge of Unikanals,", "tokens": [51314, 428, 5252, 20262, 13, 1044, 291, 11, 13633, 271, 13, 2425, 11, 1518, 13, 407, 11, 281, 5039, 264, 19317, 3430, 295, 1156, 16048, 1124, 11, 51814], "temperature": 0.0, "avg_logprob": -0.213695911446003, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.06380791962146759}, {"id": 34, "seek": 24400, "start": 244.0, "end": 252.0, "text": " we introduce URAN-C, which is a Unikanal container runtime. It is fully CRI-compatible. It's written", "tokens": [50364, 321, 5366, 624, 49, 1770, 12, 34, 11, 597, 307, 257, 1156, 16048, 304, 10129, 34474, 13, 467, 307, 4498, 383, 5577, 12, 1112, 11584, 964, 13, 467, 311, 3720, 50764], "temperature": 0.0, "avg_logprob": -0.13211075310568207, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.1321895569562912}, {"id": 35, "seek": 24400, "start": 252.0, "end": 259.0, "text": " in Go. Actually, it's a CLI tool which makes use of interconnected Go packages to actually", "tokens": [50764, 294, 1037, 13, 5135, 11, 309, 311, 257, 12855, 40, 2290, 597, 1669, 764, 295, 36611, 1037, 17401, 281, 767, 51114], "temperature": 0.0, "avg_logprob": -0.13211075310568207, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.1321895569562912}, {"id": 36, "seek": 24400, "start": 259.0, "end": 266.0, "text": " spawn the Unikanals. It reads Unikanals as processes. So, in a way, it directly manages the", "tokens": [51114, 17088, 264, 1156, 16048, 1124, 13, 467, 15700, 1156, 16048, 1124, 382, 7555, 13, 407, 11, 294, 257, 636, 11, 309, 3838, 22489, 264, 51464], "temperature": 0.0, "avg_logprob": -0.13211075310568207, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.1321895569562912}, {"id": 37, "seek": 24400, "start": 266.0, "end": 273.0, "text": " application and not the system in which the application runs. The images, the Unikanal images", "tokens": [51464, 3861, 293, 406, 264, 1185, 294, 597, 264, 3861, 6676, 13, 440, 5267, 11, 264, 1156, 16048, 304, 5267, 51814], "temperature": 0.0, "avg_logprob": -0.13211075310568207, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.1321895569562912}, {"id": 38, "seek": 27300, "start": 273.0, "end": 280.0, "text": " required to run these Unikanals are typical OSI artifacts. And in order to actually spawn these", "tokens": [50364, 4739, 281, 1190, 613, 1156, 16048, 1124, 366, 7476, 12731, 40, 24617, 13, 400, 294, 1668, 281, 767, 17088, 613, 50714], "temperature": 0.0, "avg_logprob": -0.12550094723701477, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.004080377984791994}, {"id": 39, "seek": 27300, "start": 280.0, "end": 288.0, "text": " Unikanal VMs, we make use of underlying hypervisors. So, first, let's take a look at how Unikanal", "tokens": [50714, 1156, 16048, 304, 18038, 82, 11, 321, 652, 764, 295, 14217, 9848, 4938, 830, 13, 407, 11, 700, 11, 718, 311, 747, 257, 574, 412, 577, 1156, 16048, 304, 51114], "temperature": 0.0, "avg_logprob": -0.12550094723701477, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.004080377984791994}, {"id": 40, "seek": 27300, "start": 288.0, "end": 294.0, "text": " images looks like. First of all, they are standard OSI images, so they can be managed via", "tokens": [51114, 5267, 1542, 411, 13, 2386, 295, 439, 11, 436, 366, 3832, 12731, 40, 5267, 11, 370, 436, 393, 312, 6453, 5766, 51414], "temperature": 0.0, "avg_logprob": -0.12550094723701477, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.004080377984791994}, {"id": 41, "seek": 27300, "start": 294.0, "end": 301.0, "text": " standard tooling and can be distributed using already existing registries. But there is one", "tokens": [51414, 3832, 46593, 293, 393, 312, 12631, 1228, 1217, 6741, 11376, 2244, 13, 583, 456, 307, 472, 51764], "temperature": 0.0, "avg_logprob": -0.12550094723701477, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.004080377984791994}, {"id": 42, "seek": 30100, "start": 301.0, "end": 308.0, "text": " differentiating factor. URAN-C needs some specific annotations to function. These annotations are", "tokens": [50364, 27372, 990, 5952, 13, 624, 49, 1770, 12, 34, 2203, 512, 2685, 25339, 763, 281, 2445, 13, 1981, 25339, 763, 366, 50714], "temperature": 0.0, "avg_logprob": -0.11471807731772368, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.03488924354314804}, {"id": 43, "seek": 30100, "start": 308.0, "end": 314.0, "text": " the Unikanal binary path inside the root of S, the Unikanal type, the hypervisor type, the command", "tokens": [50714, 264, 1156, 16048, 304, 17434, 3100, 1854, 264, 5593, 295, 318, 11, 264, 1156, 16048, 304, 2010, 11, 264, 9848, 16457, 2010, 11, 264, 5622, 51014], "temperature": 0.0, "avg_logprob": -0.11471807731772368, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.03488924354314804}, {"id": 44, "seek": 30100, "start": 314.0, "end": 320.0, "text": " line that we need to pass to the Unikanal. And optionally, if we are using IneterD, the path to", "tokens": [51014, 1622, 300, 321, 643, 281, 1320, 281, 264, 1156, 16048, 304, 13, 400, 3614, 379, 11, 498, 321, 366, 1228, 682, 2398, 35, 11, 264, 3100, 281, 51314], "temperature": 0.0, "avg_logprob": -0.11471807731772368, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.03488924354314804}, {"id": 45, "seek": 30100, "start": 320.0, "end": 328.0, "text": " the IneterD file. So, to facilitate the packaging of the Unikanal, we created a simple image", "tokens": [51314, 264, 682, 2398, 35, 3991, 13, 407, 11, 281, 20207, 264, 16836, 295, 264, 1156, 16048, 304, 11, 321, 2942, 257, 2199, 3256, 51714], "temperature": 0.0, "avg_logprob": -0.11471807731772368, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.03488924354314804}, {"id": 46, "seek": 32800, "start": 328.0, "end": 336.0, "text": " builder called BIMMA, which uses simple Docker file like syntax to create the images. As you can see, it's", "tokens": [50364, 27377, 1219, 363, 6324, 9998, 11, 597, 4960, 2199, 33772, 3991, 411, 28431, 281, 1884, 264, 5267, 13, 1018, 291, 393, 536, 11, 309, 311, 50764], "temperature": 0.0, "avg_logprob": -0.16982237497965494, "compression_ratio": 1.4855769230769231, "no_speech_prob": 0.011836269870400429}, {"id": 47, "seek": 32800, "start": 336.0, "end": 345.0, "text": " pretty typical for anyone who has used Docker. It's practically the same thing. So, now we have seen how", "tokens": [50764, 1238, 7476, 337, 2878, 567, 575, 1143, 33772, 13, 467, 311, 15667, 264, 912, 551, 13, 407, 11, 586, 321, 362, 1612, 577, 51214], "temperature": 0.0, "avg_logprob": -0.16982237497965494, "compression_ratio": 1.4855769230769231, "no_speech_prob": 0.011836269870400429}, {"id": 48, "seek": 32800, "start": 345.0, "end": 353.0, "text": " an OSI Unikanal image looks. Let's take a closer look at how Unikanal actually spawns a Unikanal.", "tokens": [51214, 364, 12731, 40, 1156, 16048, 304, 3256, 1542, 13, 961, 311, 747, 257, 4966, 574, 412, 577, 1156, 16048, 304, 767, 17088, 82, 257, 1156, 16048, 304, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16982237497965494, "compression_ratio": 1.4855769230769231, "no_speech_prob": 0.011836269870400429}, {"id": 49, "seek": 35300, "start": 353.0, "end": 361.0, "text": " First of all, container.dc invokes URAN-C create. URAN-C create then sets up a new network name space, create a", "tokens": [50364, 2386, 295, 439, 11, 10129, 13, 67, 66, 1048, 8606, 624, 49, 1770, 12, 34, 1884, 13, 624, 49, 1770, 12, 34, 1884, 550, 6352, 493, 257, 777, 3209, 1315, 1901, 11, 1884, 257, 50764], "temperature": 0.0, "avg_logprob": -0.15602444667442172, "compression_ratio": 1.7554347826086956, "no_speech_prob": 0.019229883328080177}, {"id": 50, "seek": 35300, "start": 361.0, "end": 369.0, "text": " new name space, sets up a pseudo terminal if it's required, and spawns URAN-C reexec process inside that name", "tokens": [50764, 777, 1315, 1901, 11, 6352, 493, 257, 35899, 14709, 498, 309, 311, 4739, 11, 293, 17088, 82, 624, 49, 1770, 12, 34, 319, 3121, 3045, 1399, 1854, 300, 1315, 51164], "temperature": 0.0, "avg_logprob": -0.15602444667442172, "compression_ratio": 1.7554347826086956, "no_speech_prob": 0.019229883328080177}, {"id": 51, "seek": 35300, "start": 369.0, "end": 377.0, "text": " space. The reexec process then notifies the parent process that had started. Then, URAN-C, the URAN-C", "tokens": [51164, 1901, 13, 440, 319, 3121, 3045, 1399, 550, 406, 11221, 264, 2596, 1399, 300, 632, 1409, 13, 1396, 11, 624, 49, 1770, 12, 34, 11, 264, 624, 49, 1770, 12, 34, 51564], "temperature": 0.0, "avg_logprob": -0.15602444667442172, "compression_ratio": 1.7554347826086956, "no_speech_prob": 0.019229883328080177}, {"id": 52, "seek": 37700, "start": 377.0, "end": 386.0, "text": " create, the original process, saves the state, the PID of the reexec process, etc. Executes any create runtime hooks, and", "tokens": [50364, 1884, 11, 264, 3380, 1399, 11, 19155, 264, 1785, 11, 264, 430, 2777, 295, 264, 319, 3121, 3045, 1399, 11, 5183, 13, 17662, 1819, 604, 1884, 34474, 26485, 11, 293, 50814], "temperature": 0.0, "avg_logprob": -0.1392145351487763, "compression_ratio": 1.9885714285714287, "no_speech_prob": 0.016634777188301086}, {"id": 53, "seek": 37700, "start": 386.0, "end": 397.0, "text": " then sends an OK IPC message to the reexec process, executes any create container hooks, and then exits. Then,", "tokens": [50814, 550, 14790, 364, 2264, 8671, 34, 3636, 281, 264, 319, 3121, 3045, 1399, 11, 4454, 1819, 604, 1884, 10129, 26485, 11, 293, 550, 44183, 13, 1396, 11, 51364], "temperature": 0.0, "avg_logprob": -0.1392145351487763, "compression_ratio": 1.9885714285714287, "no_speech_prob": 0.016634777188301086}, {"id": 54, "seek": 37700, "start": 397.0, "end": 405.0, "text": " container.dc invokes URAN-C start, which sends an IPC message to the reexec process, executes post start hooks, and", "tokens": [51364, 10129, 13, 67, 66, 1048, 8606, 624, 49, 1770, 12, 34, 722, 11, 597, 14790, 364, 8671, 34, 3636, 281, 264, 319, 3121, 3045, 1399, 11, 4454, 1819, 2183, 722, 26485, 11, 293, 51764], "temperature": 0.0, "avg_logprob": -0.1392145351487763, "compression_ratio": 1.9885714285714287, "no_speech_prob": 0.016634777188301086}, {"id": 55, "seek": 40500, "start": 405.0, "end": 414.0, "text": " exits as well. So, now it's the most interesting part. The reexec process actually sets up any necessary network and storage", "tokens": [50364, 44183, 382, 731, 13, 407, 11, 586, 309, 311, 264, 881, 1880, 644, 13, 440, 319, 3121, 3045, 1399, 767, 6352, 493, 604, 4818, 3209, 293, 6725, 50814], "temperature": 0.0, "avg_logprob": -0.19999967721792367, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.01369926705956459}, {"id": 56, "seek": 40500, "start": 414.0, "end": 425.0, "text": " components, for example, the W device, etc. Executes any start container hooks, and actually spawns the Unikanal VM. So, as you", "tokens": [50814, 6677, 11, 337, 1365, 11, 264, 343, 4302, 11, 5183, 13, 17662, 1819, 604, 722, 10129, 26485, 11, 293, 767, 17088, 82, 264, 1156, 16048, 304, 18038, 13, 407, 11, 382, 291, 51364], "temperature": 0.0, "avg_logprob": -0.19999967721792367, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.01369926705956459}, {"id": 57, "seek": 42500, "start": 425.0, "end": 435.0, "text": " can see, this is a pretty typical life cycle for any container runtime, with just some minor adjustments to facilitate the Unikanal", "tokens": [50364, 393, 536, 11, 341, 307, 257, 1238, 7476, 993, 6586, 337, 604, 10129, 34474, 11, 365, 445, 512, 6696, 18624, 281, 20207, 264, 1156, 16048, 304, 50864], "temperature": 0.0, "avg_logprob": -0.11385515653170072, "compression_ratio": 1.5433526011560694, "no_speech_prob": 0.04610691964626312}, {"id": 58, "seek": 42500, "start": 435.0, "end": 448.0, "text": " execution. So, to actually spawn the Unikanals, we use hypervisors. We made it really easy to integrate any new hypervisors you want to", "tokens": [50864, 15058, 13, 407, 11, 281, 767, 17088, 264, 1156, 16048, 1124, 11, 321, 764, 9848, 4938, 830, 13, 492, 1027, 309, 534, 1858, 281, 13365, 604, 777, 9848, 4938, 830, 291, 528, 281, 51514], "temperature": 0.0, "avg_logprob": -0.11385515653170072, "compression_ratio": 1.5433526011560694, "no_speech_prob": 0.04610691964626312}, {"id": 59, "seek": 44800, "start": 448.0, "end": 454.0, "text": " implement in the system. So, I'm going to show you how to do this with the Unikanal VM. So, in this case, you can just", "tokens": [50364, 4445, 294, 264, 1185, 13, 407, 11, 286, 478, 516, 281, 855, 291, 577, 281, 360, 341, 365, 264, 1156, 16048, 304, 18038, 13, 407, 11, 294, 341, 1389, 11, 291, 393, 445, 50664], "temperature": 0.4, "avg_logprob": -0.7624277806544042, "compression_ratio": 1.5324074074074074, "no_speech_prob": 0.26801297068595886}, {"id": 60, "seek": 44800, "start": 454.0, "end": 461.0, "text": " implement this interface, which is mostly just the exec V function. So, it's really easy. Currently, we have support for", "tokens": [50664, 4445, 341, 9226, 11, 597, 307, 5240, 445, 264, 4454, 691, 2445, 13, 407, 11, 309, 311, 534, 1858, 13, 19964, 11, 321, 362, 1406, 337, 51014], "temperature": 0.4, "avg_logprob": -0.7624277806544042, "compression_ratio": 1.5324074074074074, "no_speech_prob": 0.26801297068595886}, {"id": 61, "seek": 44800, "start": 461.0, "end": 469.0, "text": " Solo 5, Kimu, and Firecracker. For storage, we have support for block device via the map or", "tokens": [51014, 26452, 1025, 11, 5652, 84, 11, 293, 7652, 10757, 23599, 13, 1171, 6725, 11, 321, 362, 1406, 337, 3461, 4302, 5766, 264, 4471, 420, 51414], "temperature": 0.4, "avg_logprob": -0.7624277806544042, "compression_ratio": 1.5324074074074074, "no_speech_prob": 0.26801297068595886}, {"id": 62, "seek": 46900, "start": 469.0, "end": 479.0, "text": " snapshotter. We have support for InitrD, which is packed inside the image. And we also have support for shared effects coming soon.", "tokens": [50364, 30163, 391, 13, 492, 362, 1406, 337, 682, 270, 81, 35, 11, 597, 307, 13265, 1854, 264, 3256, 13, 400, 321, 611, 362, 1406, 337, 5507, 5065, 1348, 2321, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3112107768203273, "compression_ratio": 1.4914285714285713, "no_speech_prob": 0.03801726549863815}, {"id": 63, "seek": 46900, "start": 479.0, "end": 493.0, "text": " In the diagram, you can see how an image looks like, the layers look like. So, for the network part, we followed the every simple", "tokens": [50864, 682, 264, 10686, 11, 291, 393, 536, 577, 364, 3256, 1542, 411, 11, 264, 7914, 574, 411, 13, 407, 11, 337, 264, 3209, 644, 11, 321, 6263, 264, 633, 2199, 51564], "temperature": 0.0, "avg_logprob": -0.3112107768203273, "compression_ratio": 1.4914285714285713, "no_speech_prob": 0.03801726549863815}, {"id": 64, "seek": 49300, "start": 493.0, "end": 503.0, "text": " approach that is also used by sandboxed runtimes, like Cata containers. We create a new top device inside the container network", "tokens": [50364, 3109, 300, 307, 611, 1143, 538, 42115, 292, 49435, 1532, 11, 411, 383, 3274, 17089, 13, 492, 1884, 257, 777, 1192, 4302, 1854, 264, 10129, 3209, 50864], "temperature": 0.0, "avg_logprob": -0.28315385182698566, "compression_ratio": 1.233009708737864, "no_speech_prob": 0.03444480150938034}, {"id": 65, "seek": 50300, "start": 503.0, "end": 516.0, "text": " loading space. And then we breed all the data, all the traffic to the VF endpoint provided by CNI. We do this using traffic control.", "tokens": [50364, 15114, 1901, 13, 400, 550, 321, 18971, 439, 264, 1412, 11, 439, 264, 6419, 281, 264, 691, 37, 35795, 5649, 538, 14589, 40, 13, 492, 360, 341, 1228, 6419, 1969, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2931702833909255, "compression_ratio": 1.4269662921348314, "no_speech_prob": 0.10200878977775574}, {"id": 66, "seek": 50300, "start": 516.0, "end": 527.0, "text": " To integrate Unikanals in Kubernetes, we had the Asphalt Challenge. That's because we need to actually spawn non-Unikanal", "tokens": [51014, 1407, 13365, 1156, 16048, 1124, 294, 23145, 11, 321, 632, 264, 1018, 39910, 17517, 13, 663, 311, 570, 321, 643, 281, 767, 17088, 2107, 12, 12405, 16048, 304, 51564], "temperature": 0.0, "avg_logprob": -0.2931702833909255, "compression_ratio": 1.4269662921348314, "no_speech_prob": 0.10200878977775574}, {"id": 67, "seek": 52700, "start": 527.0, "end": 538.0, "text": " containers inside the same pod. For example, the POS container or any other side-con containers. To achieve this, we use RunC to spawn the", "tokens": [50364, 17089, 1854, 264, 912, 2497, 13, 1171, 1365, 11, 264, 430, 4367, 10129, 420, 604, 661, 1252, 12, 1671, 17089, 13, 1407, 4584, 341, 11, 321, 764, 8950, 34, 281, 17088, 264, 50914], "temperature": 0.0, "avg_logprob": -0.21419845410247348, "compression_ratio": 1.6287425149700598, "no_speech_prob": 0.07171065360307693}, {"id": 68, "seek": 52700, "start": 538.0, "end": 549.0, "text": " generic containers. And then, RunC handles the Unikanal containers inside the network namespace of the pod. So, there are some really", "tokens": [50914, 19577, 17089, 13, 400, 550, 11, 8950, 34, 18722, 264, 1156, 16048, 304, 17089, 1854, 264, 3209, 5288, 17940, 295, 264, 2497, 13, 407, 11, 456, 366, 512, 534, 51464], "temperature": 0.0, "avg_logprob": -0.21419845410247348, "compression_ratio": 1.6287425149700598, "no_speech_prob": 0.07171065360307693}, {"id": 69, "seek": 54900, "start": 549.0, "end": 561.0, "text": " interesting use cases, for example, KN80, in which we need to have intrapod Unikanal container communication. In KN80, for example, the QPROXY", "tokens": [50364, 1880, 764, 3331, 11, 337, 1365, 11, 26967, 4702, 11, 294, 597, 321, 643, 281, 362, 560, 4007, 378, 1156, 16048, 304, 10129, 6101, 13, 682, 26967, 4702, 11, 337, 1365, 11, 264, 1249, 47, 7142, 55, 56, 50964], "temperature": 0.0, "avg_logprob": -0.16419569203551387, "compression_ratio": 1.5027624309392265, "no_speech_prob": 0.09832330048084259}, {"id": 70, "seek": 54900, "start": 561.0, "end": 571.0, "text": " container needs to be able to communicate with a user function, which is Unikanal. To achieve this, we implement a static network", "tokens": [50964, 10129, 2203, 281, 312, 1075, 281, 7890, 365, 257, 4195, 2445, 11, 597, 307, 1156, 16048, 304, 13, 1407, 4584, 341, 11, 321, 4445, 257, 13437, 3209, 51464], "temperature": 0.0, "avg_logprob": -0.16419569203551387, "compression_ratio": 1.5027624309392265, "no_speech_prob": 0.09832330048084259}, {"id": 71, "seek": 57100, "start": 571.0, "end": 588.0, "text": " configuration. We provide the static IP to the top device. So, we handle it that way. So, now, let's see RunC in action. We will see a simple", "tokens": [50364, 11694, 13, 492, 2893, 264, 13437, 8671, 281, 264, 1192, 4302, 13, 407, 11, 321, 4813, 309, 300, 636, 13, 407, 11, 586, 11, 718, 311, 536, 8950, 34, 294, 3069, 13, 492, 486, 536, 257, 2199, 51214], "temperature": 0.0, "avg_logprob": -0.1541307379559773, "compression_ratio": 1.236842105263158, "no_speech_prob": 0.011946522630751133}, {"id": 72, "seek": 58800, "start": 588.0, "end": 598.0, "text": " deployment using an HDL. We will pull the image from the registry. And using an HDL, we will actually spawn an NGNX Unikanal inside", "tokens": [50364, 19317, 1228, 364, 12149, 43, 13, 492, 486, 2235, 264, 3256, 490, 264, 36468, 13, 400, 1228, 364, 12149, 43, 11, 321, 486, 767, 17088, 364, 426, 38, 45, 55, 1156, 16048, 304, 1854, 50864], "temperature": 0.0, "avg_logprob": -0.2747263657419305, "compression_ratio": 1.1801801801801801, "no_speech_prob": 0.052857425063848495}, {"id": 73, "seek": 59800, "start": 598.0, "end": 623.0, "text": " VM. So, as we can see, there are no containers running right now. We pull the image from our registry. Okay, it's already existing. And now, we can run it using", "tokens": [50364, 18038, 13, 407, 11, 382, 321, 393, 536, 11, 456, 366, 572, 17089, 2614, 558, 586, 13, 492, 2235, 264, 3256, 490, 527, 36468, 13, 1033, 11, 309, 311, 1217, 6741, 13, 400, 586, 11, 321, 393, 1190, 309, 1228, 51614], "temperature": 0.0, "avg_logprob": -0.2012417749925093, "compression_ratio": 1.25, "no_speech_prob": 0.1082521602511406}, {"id": 74, "seek": 62300, "start": 623.0, "end": 640.0, "text": " an HDL. We have to define the runtime. So, we do that. Okay, it spawned. And now, we can see that it started six seconds ago. It was created. Perfect. So, now, we can", "tokens": [50364, 364, 12149, 43, 13, 492, 362, 281, 6964, 264, 34474, 13, 407, 11, 321, 360, 300, 13, 1033, 11, 309, 17088, 292, 13, 400, 586, 11, 321, 393, 536, 300, 309, 1409, 2309, 3949, 2057, 13, 467, 390, 2942, 13, 10246, 13, 407, 11, 586, 11, 321, 393, 51214], "temperature": 0.0, "avg_logprob": -0.1121214169722337, "compression_ratio": 1.3070866141732282, "no_speech_prob": 0.04729827865958214}, {"id": 75, "seek": 64000, "start": 640.0, "end": 656.0, "text": " inspect the container to find the IP address. Okay. And if we curl it, we can see that it's an NGNX server built using Unicraft. Pretty typical. So, now, we can see the actual", "tokens": [50364, 15018, 264, 10129, 281, 915, 264, 8671, 2985, 13, 1033, 13, 400, 498, 321, 22591, 309, 11, 321, 393, 536, 300, 309, 311, 364, 426, 38, 45, 55, 7154, 3094, 1228, 1156, 299, 4469, 13, 10693, 7476, 13, 407, 11, 586, 11, 321, 393, 536, 264, 3539, 51164], "temperature": 0.0, "avg_logprob": -0.09869253869150199, "compression_ratio": 1.3059701492537314, "no_speech_prob": 0.07542455941438675}, {"id": 76, "seek": 65600, "start": 656.0, "end": 669.0, "text": " run. And we can see that it's running. And the container is in the RunC process. That's also running. Okay. And now, with that, I will give the floor back to", "tokens": [50364, 1190, 13, 400, 321, 393, 536, 300, 309, 311, 2614, 13, 400, 264, 10129, 307, 294, 264, 8950, 34, 1399, 13, 663, 311, 611, 2614, 13, 1033, 13, 400, 586, 11, 365, 300, 11, 286, 486, 976, 264, 4123, 646, 281, 51014], "temperature": 0.2, "avg_logprob": -0.5631625281439887, "compression_ratio": 1.2868852459016393, "no_speech_prob": 0.10526653379201889}, {"id": 77, "seek": 66900, "start": 669.0, "end": 698.0, "text": " Janis to show you a more elaborate example with K-nate. Okay. So, now, just... Okay, that's bad.", "tokens": [50364, 4956, 271, 281, 855, 291, 257, 544, 20945, 1365, 365, 591, 12, 77, 473, 13, 1033, 13, 407, 11, 586, 11, 445, 485, 1033, 11, 300, 311, 1578, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3520204948656487, "compression_ratio": 1.0786516853932584, "no_speech_prob": 0.2570541501045227}, {"id": 78, "seek": 69800, "start": 698.0, "end": 711.0, "text": " Now, let's deploy a serverless workload with RunC. So, what we first do here is that we see that we have another RunC process running in the", "tokens": [50364, 823, 11, 718, 311, 7274, 257, 7154, 1832, 20139, 365, 8950, 34, 13, 407, 11, 437, 321, 700, 360, 510, 307, 300, 321, 536, 300, 321, 362, 1071, 8950, 34, 1399, 2614, 294, 264, 51014], "temperature": 0.0, "avg_logprob": -0.1838247023130718, "compression_ratio": 1.2612612612612613, "no_speech_prob": 0.19316868484020233}, {"id": 79, "seek": 71100, "start": 711.0, "end": 729.0, "text": " cluster. And after this, what we need to do is to define the RunC class for the K-H cluster. You can see here, we apply the RunC class. And then, it's time to define the K-native", "tokens": [50364, 13630, 13, 400, 934, 341, 11, 437, 321, 643, 281, 360, 307, 281, 6964, 264, 8950, 34, 1508, 337, 264, 591, 12, 39, 13630, 13, 509, 393, 536, 510, 11, 321, 3079, 264, 8950, 34, 1508, 13, 400, 550, 11, 309, 311, 565, 281, 6964, 264, 591, 12, 77, 1166, 51264], "temperature": 0.0, "avg_logprob": -0.1853455967373318, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.44292688369750977}, {"id": 80, "seek": 72900, "start": 729.0, "end": 745.0, "text": " service. We can see that your RunC container around them is specified. And a simple HTTP-reply-server workload is used as the workload of the serverless function. We apply the", "tokens": [50364, 2643, 13, 492, 393, 536, 300, 428, 8950, 34, 10129, 926, 552, 307, 22206, 13, 400, 257, 2199, 33283, 12, 265, 2724, 12, 12484, 331, 20139, 307, 1143, 382, 264, 20139, 295, 264, 7154, 1832, 2445, 13, 492, 3079, 264, 51164], "temperature": 0.0, "avg_logprob": -0.28411414406516333, "compression_ratio": 1.3157894736842106, "no_speech_prob": 0.6213293671607971}, {"id": 81, "seek": 74500, "start": 745.0, "end": 764.0, "text": " K-native service. And then, we will retrieve back the URL endpoint, which by triggering it with a simple HTTP request, a simple HTTP get, essentially, we start the execution of the serverless workload. So, here, we can see the", "tokens": [50364, 591, 12, 77, 1166, 2643, 13, 400, 550, 11, 321, 486, 30254, 646, 264, 12905, 35795, 11, 597, 538, 40406, 309, 365, 257, 2199, 33283, 5308, 11, 257, 2199, 33283, 483, 11, 4476, 11, 321, 722, 264, 15058, 295, 264, 7154, 1832, 20139, 13, 407, 11, 510, 11, 321, 393, 536, 264, 51314], "temperature": 0.0, "avg_logprob": -0.2985892636435373, "compression_ratio": 1.4125, "no_speech_prob": 0.6065576672554016}, {"id": 82, "seek": 76400, "start": 764.0, "end": 790.0, "text": " curve. And after this, the pods are going to be running. And underneath, there will be the RunC process with the K-mode hypervisors with the sandbox workload. So, yeah. That's it. And...", "tokens": [50364, 7605, 13, 400, 934, 341, 11, 264, 31925, 366, 516, 281, 312, 2614, 13, 400, 7223, 11, 456, 486, 312, 264, 8950, 34, 1399, 365, 264, 591, 12, 76, 1429, 9848, 4938, 830, 365, 264, 42115, 20139, 13, 407, 11, 1338, 13, 663, 311, 309, 13, 400, 485, 51664], "temperature": 0.0, "avg_logprob": -0.2790378057039701, "compression_ratio": 1.3285714285714285, "no_speech_prob": 0.30156126618385315}, {"id": 83, "seek": 79000, "start": 790.0, "end": 817.0, "text": " So, the evaluation section. In order to evaluate your RunC, we convert with other container runtimes, such as divisor and other containers. And, yeah, in that process, we utilize the tool called K-perf responsible for generating and triggering K-native services via HTTP request, as we saw in the demo. And also, responsible for reporting the service latencies.", "tokens": [50364, 407, 11, 264, 13344, 3541, 13, 682, 1668, 281, 13059, 428, 8950, 34, 11, 321, 7620, 365, 661, 10129, 49435, 1532, 11, 1270, 382, 25974, 284, 293, 661, 17089, 13, 400, 11, 1338, 11, 294, 300, 1399, 11, 321, 16117, 264, 2290, 1219, 591, 12, 610, 69, 6250, 337, 17746, 293, 40406, 591, 12, 77, 1166, 3328, 5766, 33283, 5308, 11, 382, 321, 1866, 294, 264, 10723, 13, 400, 611, 11, 6250, 337, 10031, 264, 2643, 4465, 6464, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2543789966996894, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.1871931105852127}, {"id": 84, "seek": 81700, "start": 818.0, "end": 841.0, "text": " So, yeah, the scale from zero, evaluation scenarios like this, for a number of iterations, we scale. It's a K-native service. And we report at the end of the number of executions, we report the responsible latency. We do this for every other container", "tokens": [50414, 407, 11, 1338, 11, 264, 4373, 490, 4018, 11, 13344, 15077, 411, 341, 11, 337, 257, 1230, 295, 36540, 11, 321, 4373, 13, 467, 311, 257, 591, 12, 77, 1166, 2643, 13, 400, 321, 2275, 412, 264, 917, 295, 264, 1230, 295, 4454, 3666, 11, 321, 2275, 264, 6250, 27043, 13, 492, 360, 341, 337, 633, 661, 10129, 51564], "temperature": 0.0, "avg_logprob": -0.18263996801068705, "compression_ratio": 1.5304878048780488, "no_speech_prob": 0.24888506531715393}, {"id": 85, "seek": 84100, "start": 841.0, "end": 867.0, "text": " runtime. So, these are the results. We can see on the X axis the different container runtimes used for that process. And on the Y axis, the service response latency, seconds. And, yeah, of course, lower is better. So, on the blog post, with the experiment setup and all the parameters setting for K-perf. That's all. Thank you.", "tokens": [50364, 34474, 13, 407, 11, 613, 366, 264, 3542, 13, 492, 393, 536, 322, 264, 1783, 10298, 264, 819, 10129, 49435, 1532, 1143, 337, 300, 1399, 13, 400, 322, 264, 398, 10298, 11, 264, 2643, 4134, 27043, 11, 3949, 13, 400, 11, 1338, 11, 295, 1164, 11, 3126, 307, 1101, 13, 407, 11, 322, 264, 6968, 2183, 11, 365, 264, 5120, 8657, 293, 439, 264, 9834, 3287, 337, 591, 12, 610, 69, 13, 663, 311, 439, 13, 1044, 291, 13, 51664], "temperature": 0.0, "avg_logprob": -0.20730462888392007, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.2184431105852127}, {"id": 86, "seek": 87100, "start": 871.0, "end": 873.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.7064870198567709, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.4189120829105377}, {"id": 87, "seek": 90100, "start": 901.0, "end": 923.0, "text": " Okay. So, the question is about memory benchmarking, right? Yeah. Memory benchmarking is not yet on our work, but we have plans on that also. Yeah. Something that we can do.", "tokens": [50364, 1033, 13, 407, 11, 264, 1168, 307, 466, 4675, 18927, 278, 11, 558, 30, 865, 13, 38203, 18927, 278, 307, 406, 1939, 322, 527, 589, 11, 457, 321, 362, 5482, 322, 300, 611, 13, 865, 13, 6595, 300, 321, 393, 360, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3128754085964627, "compression_ratio": 1.3622047244094488, "no_speech_prob": 0.22325126826763153}, {"id": 88, "seek": 93100, "start": 932.0, "end": 934.0, "text": " Sorry.", "tokens": [50414, 4919, 13, 50514], "temperature": 0.0, "avg_logprob": -0.4803028459902163, "compression_ratio": 0.9230769230769231, "no_speech_prob": 0.1678512990474701}, {"id": 89, "seek": 93100, "start": 939.0, "end": 944.0, "text": " So, the question is if we have run in Germany, AWS, I don't know.", "tokens": [50764, 407, 11, 264, 1168, 307, 498, 321, 362, 1190, 294, 7244, 11, 17650, 11, 286, 500, 380, 458, 13, 51014], "temperature": 0.0, "avg_logprob": -0.4803028459902163, "compression_ratio": 0.9230769230769231, "no_speech_prob": 0.1678512990474701}, {"id": 90, "seek": 94400, "start": 944.0, "end": 965.0, "text": " Actually, this experiment was on the Prime Service. We have not yet experienced any big lab vendors and deployments. So, hopefully, maybe the next evaluation will be also part with major vendors.", "tokens": [50364, 5135, 11, 341, 5120, 390, 322, 264, 9655, 9561, 13, 492, 362, 406, 1939, 6751, 604, 955, 2715, 22056, 293, 7274, 1117, 13, 407, 11, 4696, 11, 1310, 264, 958, 13344, 486, 312, 611, 644, 365, 2563, 22056, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3141042845589774, "compression_ratio": 1.3541666666666667, "no_speech_prob": 0.09502014517784119}, {"id": 91, "seek": 97400, "start": 975.0, "end": 977.0, "text": " That was the end.", "tokens": [50414, 663, 390, 264, 917, 13, 50514], "temperature": 0.4, "avg_logprob": -0.6430122712079216, "compression_ratio": 1.0326086956521738, "no_speech_prob": 0.2621212601661682}, {"id": 92, "seek": 97400, "start": 980.0, "end": 982.0, "text": " Okay.", "tokens": [50664, 1033, 13, 50764], "temperature": 0.4, "avg_logprob": -0.6430122712079216, "compression_ratio": 1.0326086956521738, "no_speech_prob": 0.2621212601661682}, {"id": 93, "seek": 97400, "start": 996.0, "end": 1003.0, "text": " So, okay. I heard something about paravirtualization, right? But, yeah.", "tokens": [51464, 407, 11, 1392, 13, 286, 2198, 746, 466, 971, 706, 2498, 901, 2144, 11, 558, 30, 583, 11, 1338, 13, 51814], "temperature": 0.4, "avg_logprob": -0.6430122712079216, "compression_ratio": 1.0326086956521738, "no_speech_prob": 0.2621212601661682}, {"id": 94, "seek": 100400, "start": 1004.0, "end": 1006.0, "text": " Okay.", "tokens": [50364, 1033, 13, 50464], "temperature": 0.0, "avg_logprob": -0.5578259150187175, "compression_ratio": 0.8857142857142857, "no_speech_prob": 0.16893643140792847}, {"id": 95, "seek": 100400, "start": 1015.0, "end": 1017.0, "text": " I'm not sure that that's.", "tokens": [50914, 286, 478, 406, 988, 300, 300, 311, 13, 51014], "temperature": 0.0, "avg_logprob": -0.5578259150187175, "compression_ratio": 0.8857142857142857, "no_speech_prob": 0.16893643140792847}, {"id": 96, "seek": 103400, "start": 1034.0, "end": 1036.0, "text": " Okay.", "tokens": [50364, 1033, 13, 50464], "temperature": 0.0, "avg_logprob": -0.7325738430023193, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.5272992253303528}, {"id": 97, "seek": 106400, "start": 1064.0, "end": 1084.0, "text": " So, I think that's it. Thank you. Thank you so much.", "tokens": [50364, 407, 11, 286, 519, 300, 311, 309, 13, 1044, 291, 13, 1044, 291, 370, 709, 13, 51364], "temperature": 0.2, "avg_logprob": -0.7211661589773077, "compression_ratio": 1.0833333333333333, "no_speech_prob": 0.04061267152428627}], "language": "en"}