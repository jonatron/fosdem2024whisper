{"text": " All right, well, welcome back everybody. Up next, the one and only Dan Jenkins is going to tell us all about G-Streamer and Golang. Take it away, please. Thank you. Hello, everyone. Can everyone hear me okay? Yeah? Good. Great. Cool. Okay. I forgot my clicker. Number one rookie thing to do. No, no, I've got my phone. So I'm good. But yeah, that's why I've got my phone. And it's going to look a little bit weird. I also forgot to buy I brought two European plugs with me. But one wasn't European. One was American. So my day did not start off well. So yes, G-Streamer and Golang. So a little bit about me. Very, oh, that's just going to get really annoying. I'm just going to click. Cool. Okay, so a little bit about me. So yes, I'm Dan Jenkins. I run a couple of companies. One called Everycast Labs, one called Nimbleape, and another one called Comcom. So Everycast Labs does broadcast stuff, bringing in remote talent into broadcast workflows. Nimbleape is a consultancy service, consultancy company based in the UK. And then Comcom is an event that we put on for open source people, our way of kind of giving back to the ecosystem that we build from. I was the very first Google developer expert in the world when it comes to WebRTC. I'm not saying I'm the best at WebRTC, but I'm the first that actually got accredited by Google's developer program. I love Lego, and I love real-time media. So yeah, Nimbleape, we're a consultancy, and if you've got hard problems that you want solved, come talk to us. And Everycast Labs, we've got that product that I was just talking about called Broadcast Bridge. And then Comcom. So Comcom is dear to my heart. Historically, it's been a residential event where we bring everyone, everyone stays in the same place. And then we've got three days of awesome real-time and open media content. And then we're back in 2024. Dates are still up in the air because of contracts, but it's not going to be residential this year. We're going to go on tour, so we're not just going to be in the UK. And that's quite exciting. So to the actual topic, GStreamer building real-time applications with Golang. So what are we actually going to talk about? We're going to talk about GStreamer, obviously. We're going to talk about Golang, obviously. But I want to introduce you to something called GoGST. GoGST has been around for a long time now, but kind of got itself into a bad state where it was not un-maintained, but there were lots of little forks and lots of little patches everywhere. And so we've kind of changed how that project's being managed now. And then also I want to introduce you to something called Pion. So let's take a look at GStreamer first. Who in the room has heard about GStreamer? Good. That's the answer I was looking for. So open source multimedia framework basically does everything that you chuck at it in some form. And I absolutely love GStreamer. So a lot of you might know GStreamer as something like this. I'm not going to ask you to tell me what that is, because I know that it's kind of taking in an RTSP source and then doing something with it and then outputting something at the end, via UDP, but all the stuff in the middle now. But GStreamer is actually super powerful and ultimately lets you do ingress, do something with it, and then egress. And it kind of boils down to something that's simple, right? GStreamer can do it all and can do a lot of things. So for us at everycast labs with our broadcast bridge product, we care about certain things. So GStreamer can do NDI, GStreamer can do WebRTC, GStreamer can do SRT, can do RTP, it can do HLS, it can do RTMP and RTSP, right? I'm not telling you anything that you don't know at this point. But for us, at least with broadcast bridge, GStreamer has a superpower and that superpower is app source and app sync. How many people in the room know about app source and app sync? Okay, good. That means like 60% of you are going to learn something now. The rest of you just sit and be happy. So yeah, this is what we use at broadcast bridge, in our broadcast bridge product. And that's because we don't write C. And so ultimately kind of adding code to plugins within GStreamer is really difficult for us. I know that's changing as time is going on. There are more and more Rust plugins, but at its core there are a load of stuff that we don't feel able to kind of contribute to if we find a problem. And so a lot of the time we don't like writing C like this, but we do like writing a lot of Go. And so we end up writing something like this. And this is Go GST. It was originally created by a guy with the GitHub handle, Tinyzimmer. I love the name. But now it's in its own GitHub organization. So it's under github.com. So it's under the new GitHub org and there's three main contributors. I think there's something like 17 total, but there's three main ones. Tinyzimmer me and R.S. Willy. So Lesfawkes is better for everyone. So this other one, Big, Little Ben. That's from the LiveKit team. And the LiveKit team had their own fork of Go GST. And they had put a load of work into fixing bugs, but they were never getting merged back into the project as it was under the Tinyzimmer GitHub. So now it's forked out. Well, it's not actually forked. We forked it into its own organization and then did the GitHub magic where we unforked it and then the Tinyzimmer one is now a fork of us. So there's a lot of GitHub kind of organization going on to make it easier for everyone. Did you know that GitHub forks don't turn up in Google SEO results and they don't turn up in GitHub search results either? And search doesn't work in the repo. So yeah, and search doesn't work in the repo. So basically forks are dumb. I mean, they're not dumb. But forks are bad. We should not be relying on forks for a long-term thing whatsoever. So yeah, this is actually really great for everyone now. So less forks is better for everyone. And like I said earlier, BroadcastBridge uses a mixture of SRT, NDI, WebRTC, among a load of other things as well. And so why, you're probably asking, why would we even need to use AppSync and AppSync when the modules, the plugins are already in Gstreamer? Like Gstreamer already knows how to take in an SRT feed. It already knows how to output an NDI feed and it knows how to do WebRTC stuff. So why are we building on top of AppSync and AppSync? And it comes down to greater control like I was kind of alluding to earlier. We use Pion to do WebRTC. And that's not because the Gstreamer implementation isn't good. It's just that if we want to be able to do anything that isn't implemented into the Gstreamer implementation, then we'd need to get someone to actually go and change that code. And that's something that we're able to do. My team aren't capable of doing, but we don't go really, really, really well. And so we can definitely kind of go and take that greater control. Like I said, this means we're handling WebRTC in something that we really know. Like, ultimately, very few people in this room know about transcoding something from one codec to another. And we just rely on FFMpeg or Gstreamer or whatever to do it for us. It's the same with WebRTC for us. We really know what we're doing with WebRTC and we want to be able to kind of tweak things that we can't necessarily tweak with the Gstreamer implementation. But Pion is hugely, hugely powerful. And this is the other key thing and it's easily upgradeable. So when we actually find a bug in Pion, we can a Gstreamer pipeline and never leaving the C level. But cost isn't just measured in terms of compute. Cost is everything from building the feature all the way through to deploying the feature and running the feature. And you've got to look at the whole picture. Pion gives us huge, huge flexibility and we can move fast and we can add new features and ultimately that means that we win business. So let's take a quick look at AppSource. How many people are actually familiar with AppSource? Right. So AppSource is just another plugin, module, whatever they're called. And ultimately, you can put it inside of your pipeline and you can push data into Gstreamer using AppSource. You set a load of capabilities on that element, that AppSource element, telling it, oh, well, this media that I'm just about to push into you is this format and this frame rate and whatever else. And you can push in data or you can make, so you have to push data in obviously, but you can also make Gstreamer ask you for the data. So instead of just going, oh, I've got data, data, data, data, data. And then Gstreamer goes, oh, hold on. I can't do anything with this. Why are you sending me so much data? You can, Gstreamer can actually ask for it. Now, that's not hugely helpful when it comes to real-time applications because real-time applications, in the case of Pion, sending us web, getting RTP data from Pion, for example, that's real-time. And so we want to get that data from Pion and we want to pass it into Gstreamer straight away. Because we're getting it in this constant flow from Pion. Whereas if you were reading a, if you were reading a file and then you were passing those chunks into Gstreamer, well, you've got control over how fast you push those chunks in. And so why not let Gstreamer go, ah, I want a bit more data. I want a bit more data. I want a bit more data. Right. App sync is absolutely no different. It's a, it's a plug-in, it's a module. And, and when you put it into the pipeline, it becomes an element. And ultimately you get push data out of AppSync. And so imagine you've got AppSource and then you've got something in the middle, whether or not that's transforming it or transcoding it. And then you've got AppSync and you're connecting all these bits together. And so you're pushing data in. Gstreamer is then doing something with it. And then it's pushing it, it's passing it over to AppSync. And then AppSync sends it out to your application as data. Not as UDP, not, not via report or anything. It's giving you the, the, the raw buffer of data. So you get pushed your data from AppSync via the, the, the, the new sample signal and event. I've got some data here you go. Notice how this is all go lang. So, yeah, let's take a very quick look. So we've got our sync. So that's an AppSource, AppSync element that I've made. And I'm setting some callbacks on it. And then we've got new sample funk. And then that gives me, that gives me my, my sync. And then I'm going to tell it as a return. I'm going to tell it what the return, what the flow state is. And so I pull the sample. And then if the sample isn't end of, isn't nil, then, then we carry on. If it is nil, then I'm returning that we are at the end of the stream. And then buffer. So we get this, our sample. So we're pulling the sample. And then, and then we're getting the buffer out of that. And then ultimately reading some, some, some information from that, from that buffer map, changing it from big Indian to little Indian, I think, or something. And then, and then doing some stuff on it, doing some maths on it. Not a lot of like useful information there. Like in terms of like, what am I actually then going to go and do with it? Well, at the moment, it's just printing out RMS. But then you can go off and do whatever you want with it. For us, that means getting a video and audio data out of G streamer and chucking it into NDI. Oh, Dan, why are you not using NDI within G streamer? Well, I tell you number one, when we did our NDI integration, G streamer didn't have NDI. It was, it was completely separate. It was, it was a different repo. And it wasn't part of the G streamer rust plugins. And then B, we do extra stuff that G streamer doesn't know how to do yet. So we, we grab tally information from, from NDI. And to be able to do that, you need access to the underlying NDI sender. And, and so there's stuff that G streamer can't do yet. Something that we actually want to add in to G streamer. So that we can stop sending stuff via the NDI SDK directly and we can just let G streamer deal with it for us. But again, goes back to that cost analysis, right? At the moment, we can get that data out of G streamer using app sync and chuck it out via NDI. We can do that. And it's relatively cheap. But then there's a load of extra work for us to be able to kind of go in and figure out the right way of doing it in G streamer so that like tally information becomes available as a signal. So yeah, for us, this means that we have to handle RTP and RTCP from Pion. Because Pion, within WebRTC, WebRTC is made up of lots of standards. But ultimately the media is RTP. And the bit that tells you what the quality is and everything else that goes with it along with it is RTCP. So it's very easy to forget about things that are very important when you don't deal with them. Like RTCP. SFU people in the room will go, ah, you could never forget about RTCP. But as a web developer, the browser deals with all of this for us. And so it's very easy for us to go, ah, RTP, I'm going to get my media. I'm going to get my media. And then everything works really, really well when you're in a really nice network environment. But then you chuck in real life scenario and the audio in the video goes terrible. Why did the audio and video go terrible? Because there's no RTCP feedback mechanism to go, ah, something's going wrong. But yeah, GStreamer makes all of this easy. And very quickly on this very specific thing, we use RTP bin within GStreamer. So that's that middle bit for us. We use app source, chuck it into RTP bin, and then we do a load of transcoding and stuff as well. And then we get app sync. RTP bin is magical. If you deal with RTP at all with GStreamer, then you need to be using RTP bin. There's a lot of text there. But ultimately, it implements everything you need to be able to handle RTP and RTCP and demuxing of payloads. And it's just a very nice all in all thing that deals with everything using all of the separate, all the separate plugins. But it forces it all together nicely for you. So for us, that's connecting the app source sync pads to RTP bin. And you'll notice I say pads. So for us, you can see up the top there RTP bin. So we're requesting a pad from RTP bin in that format. So it's a receive RTCP sync. And then we're also requesting a pad of send RTCP sync source as well. We then go and make a new app sync and a new app source. And you can see they're labeled RTCP app sync and RTCP app source. We then add those to our pipeline because otherwise nothing works. All of your elements have got to be in a pipeline. And then we link our RTCP app source pad RTCP app source, get static pad source, link RTCP sync pad. Yes. So I'm getting the app, sorry. I'm grabbing the RTCP sync pad from the RTP bin. And I'm linking it over to the RTCP app source. So that's basically just saying RTP bin is going to give me some information up to RTCP information via a pad. And I'm connecting to that pad so that I can then grab that information and send it over, send it back via Pion up to my web RTCP. So you'll get RTP in, in this case, you'll get RTP in into RTP bin, but you'll get RTCP in and out. So you'll get told RTCP and you'll also send it back out as well. And like I say, don't forget about the RTCP. As you can tell, I forgot about the RTCP and ended up doing certain demos and going, ah, look, it's really great. And then someone went and tried it on a really crappy internet connection and went, no, Dan, it doesn't work. And, and made me look rather foolish. So you end up looking something like this. So does everyone know about the dot graphs that you can generate from GStreamer? A couple of nods, not that many. So you can, within GStreamer, you can tell it, I want you to export a dot graph file on anything, on, on a state change or whatever. You, you've got control over when it generates it. And so for, for me, we, when we've got debugging enabled, we enable a dot graph generation whenever state changes. And so ultimately, this looks really small and dumb. It's a PDF. So you can go in and, and look at it in high quality detail. Um, because it's not a PNG. So you've got lots of options. You can, the dot graph can be converted into lots of different formats. But the really cool thing about dot graphs is it tells you what's connected to what. And so it's really great for debugging. And so for us, we've got our app source, um, our app source and our, our two app sources. So one is, um, one is RTP, which is this one. And then this one is RTCP. And you can see, I'm coming off the camera. I'm sorry. Um, so you can see that this one's set with, um, with capabilities to say that this is RTCP. And this one is set with capabilities to say this is RTP. And so you can see those are linked to a pad within a GST bin, a GST RTP bin. And so those pads are then connected to an RTP session. The RTP session is then, um, connected to a demuxer. The demuxer is then connected to a jitter buffer. And the jitter buffer is then able to go. Oh, well, in this, in this RTP stream that I'm receiving, that's both audio and video, where it's demuxed it and then it automatically goes, ah, here's the video and here's the audio. Right. And then it chucks it back out, chucks it back out, creates some pads for me, which I then connect over to, well, there's an app sync up there and that's my RTCP app sync. But then you could see here that it's then connecting out Opus and VPA into my pipeline. And then this is like the rest of the pipeline, which we don't care about, but like, I get told it's Opus and I get told it's VPA. And so I'm able to decode it and do stuff with it, whether or not that's outputting to NDI or whatever. At the end of the, um, at the end of it is, um, is an app source, uh, sorry, an app, an app sync for sending out via NDI. So we, we got into go purely because of Pion and Pion gives us loads of control. It's basically WebRTC in pure Golang. If you ignore the fact that WebRTC does lots of like actual media stuff, but when you look at, say, the, just the, the network portion of it of sending, sending data from here and sending it there, then it's pure Golang. So yeah, you can do any of this with any of the G streamer bindings or you can just, you know, do it with actual G streamer C. I mean, who actually want to do that? I don't know. But you can go and use whatever bindings you want. And so there's really nice bindings for Python, Rust, um, and I haven't used any of the others. Um, I've definitely used the Python one and the, and the Rust one myself. Um, and the Golang one, I went on there this morning to take the screenshot and I was like, Oh, where's the Golang one? Um, so here's the pull request to add it to the list. So if you've got a problem and G streamer doesn't quite solve that problem, that's what this talks about. This talk is about the fact that you can make G streamer do what you want it to do using app source and app sync. You can build it yourself with app source and app sync. So why G streamer? Why not FFM peg? Whatever. G streamer does everything that we need it to do. It has a fantastic community, super friendly community. And ultimately it's just super flexible and does exactly what we need it to do. Um, which is not something that we felt as a team. FFM peg would give us, for example, G streamer has a lot of scaffolding, let's say, um, and, and gives us an awful lot, um, for free. Whereas G, uh, FFM pegs a little bit more, more work, right? So my last message is G streamer for the win. Um, so yeah, don't wait for others. Don't wait for others to build your plugin for you. You can go and build with G streamer, app source and sync. And that's me. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.6, "text": " All right, well, welcome back everybody. Up next, the one and only Dan Jenkins is going", "tokens": [50364, 1057, 558, 11, 731, 11, 2928, 646, 2201, 13, 5858, 958, 11, 264, 472, 293, 787, 3394, 41273, 307, 516, 51044], "temperature": 0.0, "avg_logprob": -0.32601815881863444, "compression_ratio": 1.3657142857142857, "no_speech_prob": 0.6187078356742859}, {"id": 1, "seek": 0, "start": 13.6, "end": 18.76, "text": " to tell us all about G-Streamer and Golang. Take it away, please.", "tokens": [51044, 281, 980, 505, 439, 466, 460, 12, 4520, 1572, 260, 293, 36319, 656, 13, 3664, 309, 1314, 11, 1767, 13, 51302], "temperature": 0.0, "avg_logprob": -0.32601815881863444, "compression_ratio": 1.3657142857142857, "no_speech_prob": 0.6187078356742859}, {"id": 2, "seek": 0, "start": 18.76, "end": 25.6, "text": " Thank you. Hello, everyone. Can everyone hear me okay? Yeah? Good. Great. Cool. Okay.", "tokens": [51302, 1044, 291, 13, 2425, 11, 1518, 13, 1664, 1518, 1568, 385, 1392, 30, 865, 30, 2205, 13, 3769, 13, 8561, 13, 1033, 13, 51644], "temperature": 0.0, "avg_logprob": -0.32601815881863444, "compression_ratio": 1.3657142857142857, "no_speech_prob": 0.6187078356742859}, {"id": 3, "seek": 2560, "start": 25.6, "end": 32.800000000000004, "text": " I forgot my clicker. Number one rookie thing to do. No, no, I've got my phone. So I'm good.", "tokens": [50364, 286, 5298, 452, 2052, 260, 13, 5118, 472, 36299, 551, 281, 360, 13, 883, 11, 572, 11, 286, 600, 658, 452, 2593, 13, 407, 286, 478, 665, 13, 50724], "temperature": 0.0, "avg_logprob": -0.23421946593693324, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.3973580002784729}, {"id": 4, "seek": 2560, "start": 32.800000000000004, "end": 36.72, "text": " But yeah, that's why I've got my phone. And it's going to look a little bit weird.", "tokens": [50724, 583, 1338, 11, 300, 311, 983, 286, 600, 658, 452, 2593, 13, 400, 309, 311, 516, 281, 574, 257, 707, 857, 3657, 13, 50920], "temperature": 0.0, "avg_logprob": -0.23421946593693324, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.3973580002784729}, {"id": 5, "seek": 2560, "start": 36.72, "end": 44.16, "text": " I also forgot to buy I brought two European plugs with me. But one wasn't European. One was American.", "tokens": [50920, 286, 611, 5298, 281, 2256, 286, 3038, 732, 6473, 33899, 365, 385, 13, 583, 472, 2067, 380, 6473, 13, 1485, 390, 2665, 13, 51292], "temperature": 0.0, "avg_logprob": -0.23421946593693324, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.3973580002784729}, {"id": 6, "seek": 2560, "start": 44.16, "end": 52.32, "text": " So my day did not start off well. So yes, G-Streamer and Golang. So a little bit about me.", "tokens": [51292, 407, 452, 786, 630, 406, 722, 766, 731, 13, 407, 2086, 11, 460, 12, 4520, 1572, 260, 293, 36319, 656, 13, 407, 257, 707, 857, 466, 385, 13, 51700], "temperature": 0.0, "avg_logprob": -0.23421946593693324, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.3973580002784729}, {"id": 7, "seek": 5232, "start": 53.04, "end": 58.480000000000004, "text": " Very, oh, that's just going to get really annoying. I'm just going to click. Cool.", "tokens": [50400, 4372, 11, 1954, 11, 300, 311, 445, 516, 281, 483, 534, 11304, 13, 286, 478, 445, 516, 281, 2052, 13, 8561, 13, 50672], "temperature": 0.0, "avg_logprob": -0.21290416507930546, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.0339960940182209}, {"id": 8, "seek": 5232, "start": 58.480000000000004, "end": 64.64, "text": " Okay, so a little bit about me. So yes, I'm Dan Jenkins. I run a couple of companies.", "tokens": [50672, 1033, 11, 370, 257, 707, 857, 466, 385, 13, 407, 2086, 11, 286, 478, 3394, 41273, 13, 286, 1190, 257, 1916, 295, 3431, 13, 50980], "temperature": 0.0, "avg_logprob": -0.21290416507930546, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.0339960940182209}, {"id": 9, "seek": 5232, "start": 65.36, "end": 70.24000000000001, "text": " One called Everycast Labs, one called Nimbleape, and another one called Comcom.", "tokens": [51016, 1485, 1219, 2048, 3734, 40047, 11, 472, 1219, 45251, 638, 41153, 11, 293, 1071, 472, 1219, 2432, 1112, 13, 51260], "temperature": 0.0, "avg_logprob": -0.21290416507930546, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.0339960940182209}, {"id": 10, "seek": 5232, "start": 71.03999999999999, "end": 78.24000000000001, "text": " So Everycast Labs does broadcast stuff, bringing in remote talent into broadcast workflows.", "tokens": [51300, 407, 2048, 3734, 40047, 775, 9975, 1507, 11, 5062, 294, 8607, 8301, 666, 9975, 43461, 13, 51660], "temperature": 0.0, "avg_logprob": -0.21290416507930546, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.0339960940182209}, {"id": 11, "seek": 7824, "start": 78.8, "end": 85.75999999999999, "text": " Nimbleape is a consultancy service, consultancy company based in the UK. And then Comcom is an", "tokens": [50392, 45251, 638, 41153, 307, 257, 7189, 6717, 2643, 11, 7189, 6717, 2237, 2361, 294, 264, 7051, 13, 400, 550, 2432, 1112, 307, 364, 50740], "temperature": 0.0, "avg_logprob": -0.08938987509718219, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.008888657204806805}, {"id": 12, "seek": 7824, "start": 85.75999999999999, "end": 92.72, "text": " event that we put on for open source people, our way of kind of giving back to the ecosystem that", "tokens": [50740, 2280, 300, 321, 829, 322, 337, 1269, 4009, 561, 11, 527, 636, 295, 733, 295, 2902, 646, 281, 264, 11311, 300, 51088], "temperature": 0.0, "avg_logprob": -0.08938987509718219, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.008888657204806805}, {"id": 13, "seek": 7824, "start": 92.72, "end": 99.6, "text": " we build from. I was the very first Google developer expert in the world when it comes to WebRTC.", "tokens": [51088, 321, 1322, 490, 13, 286, 390, 264, 588, 700, 3329, 10754, 5844, 294, 264, 1002, 562, 309, 1487, 281, 9573, 49, 18238, 13, 51432], "temperature": 0.0, "avg_logprob": -0.08938987509718219, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.008888657204806805}, {"id": 14, "seek": 7824, "start": 100.24, "end": 106.72, "text": " I'm not saying I'm the best at WebRTC, but I'm the first that actually got accredited by Google's", "tokens": [51464, 286, 478, 406, 1566, 286, 478, 264, 1151, 412, 9573, 49, 18238, 11, 457, 286, 478, 264, 700, 300, 767, 658, 33877, 1226, 538, 3329, 311, 51788], "temperature": 0.0, "avg_logprob": -0.08938987509718219, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.008888657204806805}, {"id": 15, "seek": 10672, "start": 106.72, "end": 113.6, "text": " developer program. I love Lego, and I love real-time media. So yeah, Nimbleape, we're a consultancy,", "tokens": [50364, 10754, 1461, 13, 286, 959, 28761, 11, 293, 286, 959, 957, 12, 3766, 3021, 13, 407, 1338, 11, 45251, 638, 41153, 11, 321, 434, 257, 7189, 6717, 11, 50708], "temperature": 0.0, "avg_logprob": -0.09582968319163603, "compression_ratio": 1.5284552845528456, "no_speech_prob": 0.00447061937302351}, {"id": 16, "seek": 10672, "start": 114.56, "end": 121.6, "text": " and if you've got hard problems that you want solved, come talk to us. And Everycast Labs,", "tokens": [50756, 293, 498, 291, 600, 658, 1152, 2740, 300, 291, 528, 13041, 11, 808, 751, 281, 505, 13, 400, 2048, 3734, 40047, 11, 51108], "temperature": 0.0, "avg_logprob": -0.09582968319163603, "compression_ratio": 1.5284552845528456, "no_speech_prob": 0.00447061937302351}, {"id": 17, "seek": 10672, "start": 121.6, "end": 125.44, "text": " we've got that product that I was just talking about called Broadcast Bridge. And then Comcom.", "tokens": [51108, 321, 600, 658, 300, 1674, 300, 286, 390, 445, 1417, 466, 1219, 14074, 3734, 18917, 13, 400, 550, 2432, 1112, 13, 51300], "temperature": 0.0, "avg_logprob": -0.09582968319163603, "compression_ratio": 1.5284552845528456, "no_speech_prob": 0.00447061937302351}, {"id": 18, "seek": 10672, "start": 126.08, "end": 133.2, "text": " So Comcom is dear to my heart. Historically, it's been a residential event where we bring", "tokens": [51332, 407, 2432, 1112, 307, 6875, 281, 452, 1917, 13, 25108, 984, 11, 309, 311, 668, 257, 17389, 2280, 689, 321, 1565, 51688], "temperature": 0.0, "avg_logprob": -0.09582968319163603, "compression_ratio": 1.5284552845528456, "no_speech_prob": 0.00447061937302351}, {"id": 19, "seek": 13320, "start": 133.2, "end": 141.11999999999998, "text": " everyone, everyone stays in the same place. And then we've got three days of awesome real-time and", "tokens": [50364, 1518, 11, 1518, 10834, 294, 264, 912, 1081, 13, 400, 550, 321, 600, 658, 1045, 1708, 295, 3476, 957, 12, 3766, 293, 50760], "temperature": 0.0, "avg_logprob": -0.08815926703337197, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.014061731286346912}, {"id": 20, "seek": 13320, "start": 141.11999999999998, "end": 149.76, "text": " open media content. And then we're back in 2024. Dates are still up in the air because of contracts,", "tokens": [50760, 1269, 3021, 2701, 13, 400, 550, 321, 434, 646, 294, 45237, 13, 413, 1024, 366, 920, 493, 294, 264, 1988, 570, 295, 13952, 11, 51192], "temperature": 0.0, "avg_logprob": -0.08815926703337197, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.014061731286346912}, {"id": 21, "seek": 13320, "start": 150.72, "end": 155.2, "text": " but it's not going to be residential this year. We're going to go on tour, so we're not just", "tokens": [51240, 457, 309, 311, 406, 516, 281, 312, 17389, 341, 1064, 13, 492, 434, 516, 281, 352, 322, 3512, 11, 370, 321, 434, 406, 445, 51464], "temperature": 0.0, "avg_logprob": -0.08815926703337197, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.014061731286346912}, {"id": 22, "seek": 13320, "start": 155.2, "end": 162.48, "text": " going to be in the UK. And that's quite exciting. So to the actual topic, GStreamer building real-time", "tokens": [51464, 516, 281, 312, 294, 264, 7051, 13, 400, 300, 311, 1596, 4670, 13, 407, 281, 264, 3539, 4829, 11, 460, 4520, 1572, 260, 2390, 957, 12, 3766, 51828], "temperature": 0.0, "avg_logprob": -0.08815926703337197, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.014061731286346912}, {"id": 23, "seek": 16248, "start": 162.48, "end": 166.23999999999998, "text": " applications with Golang. So what are we actually going to talk about? We're going to talk about", "tokens": [50364, 5821, 365, 36319, 656, 13, 407, 437, 366, 321, 767, 516, 281, 751, 466, 30, 492, 434, 516, 281, 751, 466, 50552], "temperature": 0.0, "avg_logprob": -0.08487221854073661, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.004406983498483896}, {"id": 24, "seek": 16248, "start": 166.23999999999998, "end": 172.23999999999998, "text": " GStreamer, obviously. We're going to talk about Golang, obviously. But I want to introduce you to", "tokens": [50552, 460, 4520, 1572, 260, 11, 2745, 13, 492, 434, 516, 281, 751, 466, 36319, 656, 11, 2745, 13, 583, 286, 528, 281, 5366, 291, 281, 50852], "temperature": 0.0, "avg_logprob": -0.08487221854073661, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.004406983498483896}, {"id": 25, "seek": 16248, "start": 172.23999999999998, "end": 181.44, "text": " something called GoGST. GoGST has been around for a long time now, but kind of got itself into a", "tokens": [50852, 746, 1219, 1037, 38, 6840, 13, 1037, 38, 6840, 575, 668, 926, 337, 257, 938, 565, 586, 11, 457, 733, 295, 658, 2564, 666, 257, 51312], "temperature": 0.0, "avg_logprob": -0.08487221854073661, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.004406983498483896}, {"id": 26, "seek": 16248, "start": 181.44, "end": 187.51999999999998, "text": " bad state where it was not un-maintained, but there were lots of little forks and lots of little", "tokens": [51312, 1578, 1785, 689, 309, 390, 406, 517, 12, 76, 5114, 3563, 11, 457, 456, 645, 3195, 295, 707, 337, 1694, 293, 3195, 295, 707, 51616], "temperature": 0.0, "avg_logprob": -0.08487221854073661, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.004406983498483896}, {"id": 27, "seek": 18752, "start": 187.52, "end": 196.48000000000002, "text": " patches everywhere. And so we've kind of changed how that project's being managed now. And then", "tokens": [50364, 26531, 5315, 13, 400, 370, 321, 600, 733, 295, 3105, 577, 300, 1716, 311, 885, 6453, 586, 13, 400, 550, 50812], "temperature": 0.0, "avg_logprob": -0.09952153657612048, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.004690797068178654}, {"id": 28, "seek": 18752, "start": 196.48000000000002, "end": 202.08, "text": " also I want to introduce you to something called Pion. So let's take a look at GStreamer first.", "tokens": [50812, 611, 286, 528, 281, 5366, 291, 281, 746, 1219, 430, 313, 13, 407, 718, 311, 747, 257, 574, 412, 460, 4520, 1572, 260, 700, 13, 51092], "temperature": 0.0, "avg_logprob": -0.09952153657612048, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.004690797068178654}, {"id": 29, "seek": 18752, "start": 202.08, "end": 206.88, "text": " Who in the room has heard about GStreamer? Good. That's the answer I was looking for.", "tokens": [51092, 2102, 294, 264, 1808, 575, 2198, 466, 460, 4520, 1572, 260, 30, 2205, 13, 663, 311, 264, 1867, 286, 390, 1237, 337, 13, 51332], "temperature": 0.0, "avg_logprob": -0.09952153657612048, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.004690797068178654}, {"id": 30, "seek": 18752, "start": 207.92000000000002, "end": 216.08, "text": " So open source multimedia framework basically does everything that you chuck at it in some form.", "tokens": [51384, 407, 1269, 4009, 49202, 8388, 1936, 775, 1203, 300, 291, 20870, 412, 309, 294, 512, 1254, 13, 51792], "temperature": 0.0, "avg_logprob": -0.09952153657612048, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.004690797068178654}, {"id": 31, "seek": 21608, "start": 217.04000000000002, "end": 224.08, "text": " And I absolutely love GStreamer. So a lot of you might know GStreamer as something like this.", "tokens": [50412, 400, 286, 3122, 959, 460, 4520, 1572, 260, 13, 407, 257, 688, 295, 291, 1062, 458, 460, 4520, 1572, 260, 382, 746, 411, 341, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11039499803022905, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.0022544516250491142}, {"id": 32, "seek": 21608, "start": 226.88000000000002, "end": 232.24, "text": " I'm not going to ask you to tell me what that is, because I know that it's kind of taking in an RTSP", "tokens": [50904, 286, 478, 406, 516, 281, 1029, 291, 281, 980, 385, 437, 300, 307, 11, 570, 286, 458, 300, 309, 311, 733, 295, 1940, 294, 364, 497, 7327, 47, 51172], "temperature": 0.0, "avg_logprob": -0.11039499803022905, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.0022544516250491142}, {"id": 33, "seek": 21608, "start": 233.28, "end": 236.64000000000001, "text": " source and then doing something with it and then outputting something at the end,", "tokens": [51224, 4009, 293, 550, 884, 746, 365, 309, 293, 550, 5598, 783, 746, 412, 264, 917, 11, 51392], "temperature": 0.0, "avg_logprob": -0.11039499803022905, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.0022544516250491142}, {"id": 34, "seek": 23664, "start": 236.72, "end": 247.35999999999999, "text": " via UDP, but all the stuff in the middle now. But GStreamer is actually super powerful and", "tokens": [50368, 5766, 624, 11373, 11, 457, 439, 264, 1507, 294, 264, 2808, 586, 13, 583, 460, 4520, 1572, 260, 307, 767, 1687, 4005, 293, 50900], "temperature": 0.0, "avg_logprob": -0.13843728274833866, "compression_ratio": 1.5105263157894737, "no_speech_prob": 0.019751379266381264}, {"id": 35, "seek": 23664, "start": 247.35999999999999, "end": 254.0, "text": " ultimately lets you do ingress, do something with it, and then egress. And it kind of boils down to", "tokens": [50900, 6284, 6653, 291, 360, 3957, 735, 11, 360, 746, 365, 309, 11, 293, 550, 308, 3091, 13, 400, 309, 733, 295, 35049, 760, 281, 51232], "temperature": 0.0, "avg_logprob": -0.13843728274833866, "compression_ratio": 1.5105263157894737, "no_speech_prob": 0.019751379266381264}, {"id": 36, "seek": 23664, "start": 254.0, "end": 262.71999999999997, "text": " something that's simple, right? GStreamer can do it all and can do a lot of things. So for us at", "tokens": [51232, 746, 300, 311, 2199, 11, 558, 30, 460, 4520, 1572, 260, 393, 360, 309, 439, 293, 393, 360, 257, 688, 295, 721, 13, 407, 337, 505, 412, 51668], "temperature": 0.0, "avg_logprob": -0.13843728274833866, "compression_ratio": 1.5105263157894737, "no_speech_prob": 0.019751379266381264}, {"id": 37, "seek": 26272, "start": 262.72, "end": 268.88000000000005, "text": " everycast labs with our broadcast bridge product, we care about certain things. So GStreamer can do", "tokens": [50364, 633, 3734, 20339, 365, 527, 9975, 7283, 1674, 11, 321, 1127, 466, 1629, 721, 13, 407, 460, 4520, 1572, 260, 393, 360, 50672], "temperature": 0.0, "avg_logprob": -0.10998882165476054, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.0020176172256469727}, {"id": 38, "seek": 26272, "start": 268.88000000000005, "end": 277.6, "text": " NDI, GStreamer can do WebRTC, GStreamer can do SRT, can do RTP, it can do HLS, it can do RTMP and", "tokens": [50672, 426, 3085, 11, 460, 4520, 1572, 260, 393, 360, 9573, 49, 18238, 11, 460, 4520, 1572, 260, 393, 360, 20840, 51, 11, 393, 360, 497, 16804, 11, 309, 393, 360, 389, 19198, 11, 309, 393, 360, 21797, 12224, 293, 51108], "temperature": 0.0, "avg_logprob": -0.10998882165476054, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.0020176172256469727}, {"id": 39, "seek": 26272, "start": 277.6, "end": 285.84000000000003, "text": " RTSP, right? I'm not telling you anything that you don't know at this point. But for us, at least", "tokens": [51108, 497, 7327, 47, 11, 558, 30, 286, 478, 406, 3585, 291, 1340, 300, 291, 500, 380, 458, 412, 341, 935, 13, 583, 337, 505, 11, 412, 1935, 51520], "temperature": 0.0, "avg_logprob": -0.10998882165476054, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.0020176172256469727}, {"id": 40, "seek": 26272, "start": 285.84000000000003, "end": 292.48, "text": " with broadcast bridge, GStreamer has a superpower and that superpower is app source and app sync.", "tokens": [51520, 365, 9975, 7283, 11, 460, 4520, 1572, 260, 575, 257, 45765, 293, 300, 45765, 307, 724, 4009, 293, 724, 20271, 13, 51852], "temperature": 0.0, "avg_logprob": -0.10998882165476054, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.0020176172256469727}, {"id": 41, "seek": 29248, "start": 292.48, "end": 300.32, "text": " How many people in the room know about app source and app sync? Okay, good. That means like 60%", "tokens": [50364, 1012, 867, 561, 294, 264, 1808, 458, 466, 724, 4009, 293, 724, 20271, 30, 1033, 11, 665, 13, 663, 1355, 411, 4060, 4, 50756], "temperature": 0.0, "avg_logprob": -0.12928789312189276, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.00036795256892219186}, {"id": 42, "seek": 29248, "start": 300.32, "end": 307.12, "text": " of you are going to learn something now. The rest of you just sit and be happy. So yeah, this is", "tokens": [50756, 295, 291, 366, 516, 281, 1466, 746, 586, 13, 440, 1472, 295, 291, 445, 1394, 293, 312, 2055, 13, 407, 1338, 11, 341, 307, 51096], "temperature": 0.0, "avg_logprob": -0.12928789312189276, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.00036795256892219186}, {"id": 43, "seek": 29248, "start": 307.12, "end": 314.16, "text": " what we use at broadcast bridge, in our broadcast bridge product. And that's because we don't write", "tokens": [51096, 437, 321, 764, 412, 9975, 7283, 11, 294, 527, 9975, 7283, 1674, 13, 400, 300, 311, 570, 321, 500, 380, 2464, 51448], "temperature": 0.0, "avg_logprob": -0.12928789312189276, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.00036795256892219186}, {"id": 44, "seek": 29248, "start": 314.16, "end": 321.6, "text": " C. And so ultimately kind of adding code to plugins within GStreamer is really difficult for us.", "tokens": [51448, 383, 13, 400, 370, 6284, 733, 295, 5127, 3089, 281, 33759, 1951, 460, 4520, 1572, 260, 307, 534, 2252, 337, 505, 13, 51820], "temperature": 0.0, "avg_logprob": -0.12928789312189276, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.00036795256892219186}, {"id": 45, "seek": 32248, "start": 322.48, "end": 328.16, "text": " I know that's changing as time is going on. There are more and more Rust plugins, but at its core", "tokens": [50364, 286, 458, 300, 311, 4473, 382, 565, 307, 516, 322, 13, 821, 366, 544, 293, 544, 34952, 33759, 11, 457, 412, 1080, 4965, 50648], "temperature": 0.0, "avg_logprob": -0.13273826837539673, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0008001524256542325}, {"id": 46, "seek": 32248, "start": 328.16, "end": 334.0, "text": " there are a load of stuff that we don't feel able to kind of contribute to if we find a problem.", "tokens": [50648, 456, 366, 257, 3677, 295, 1507, 300, 321, 500, 380, 841, 1075, 281, 733, 295, 10586, 281, 498, 321, 915, 257, 1154, 13, 50940], "temperature": 0.0, "avg_logprob": -0.13273826837539673, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0008001524256542325}, {"id": 47, "seek": 32248, "start": 334.88, "end": 344.16, "text": " And so a lot of the time we don't like writing C like this, but we do like writing a lot of Go.", "tokens": [50984, 400, 370, 257, 688, 295, 264, 565, 321, 500, 380, 411, 3579, 383, 411, 341, 11, 457, 321, 360, 411, 3579, 257, 688, 295, 1037, 13, 51448], "temperature": 0.0, "avg_logprob": -0.13273826837539673, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0008001524256542325}, {"id": 48, "seek": 34416, "start": 345.12, "end": 351.04, "text": " And so we end up writing something like this. And this is Go GST.", "tokens": [50412, 400, 370, 321, 917, 493, 3579, 746, 411, 341, 13, 400, 341, 307, 1037, 460, 6840, 13, 50708], "temperature": 0.0, "avg_logprob": -0.26747983874696674, "compression_ratio": 1.3742331288343559, "no_speech_prob": 0.016144905239343643}, {"id": 49, "seek": 34416, "start": 353.68, "end": 359.76000000000005, "text": " It was originally created by a guy with the GitHub handle, Tinyzimmer. I love the name.", "tokens": [50840, 467, 390, 7993, 2942, 538, 257, 2146, 365, 264, 23331, 4813, 11, 39992, 89, 14477, 13, 286, 959, 264, 1315, 13, 51144], "temperature": 0.0, "avg_logprob": -0.26747983874696674, "compression_ratio": 1.3742331288343559, "no_speech_prob": 0.016144905239343643}, {"id": 50, "seek": 34416, "start": 360.88, "end": 366.24, "text": " But now it's in its own GitHub organization. So it's under github.com.", "tokens": [51200, 583, 586, 309, 311, 294, 1080, 1065, 23331, 4475, 13, 407, 309, 311, 833, 290, 355, 836, 13, 1112, 13, 51468], "temperature": 0.0, "avg_logprob": -0.26747983874696674, "compression_ratio": 1.3742331288343559, "no_speech_prob": 0.016144905239343643}, {"id": 51, "seek": 36624, "start": 366.8, "end": 381.28000000000003, "text": " So it's under the new GitHub org and there's three main contributors. I think there's something like", "tokens": [50392, 407, 309, 311, 833, 264, 777, 23331, 14045, 293, 456, 311, 1045, 2135, 45627, 13, 286, 519, 456, 311, 746, 411, 51116], "temperature": 0.0, "avg_logprob": -0.2600456476211548, "compression_ratio": 1.3611111111111112, "no_speech_prob": 0.06345850229263306}, {"id": 52, "seek": 36624, "start": 381.28000000000003, "end": 392.64, "text": " 17 total, but there's three main ones. Tinyzimmer me and R.S. Willy. So Lesfawkes is better for", "tokens": [51116, 3282, 3217, 11, 457, 456, 311, 1045, 2135, 2306, 13, 39992, 89, 14477, 385, 293, 497, 13, 50, 13, 42238, 13, 407, 6965, 69, 1607, 5993, 307, 1101, 337, 51684], "temperature": 0.0, "avg_logprob": -0.2600456476211548, "compression_ratio": 1.3611111111111112, "no_speech_prob": 0.06345850229263306}, {"id": 53, "seek": 39264, "start": 392.64, "end": 402.32, "text": " everyone. So this other one, Big, Little Ben. That's from the LiveKit team. And the LiveKit", "tokens": [50364, 1518, 13, 407, 341, 661, 472, 11, 5429, 11, 8022, 3964, 13, 663, 311, 490, 264, 10385, 45626, 1469, 13, 400, 264, 10385, 45626, 50848], "temperature": 0.0, "avg_logprob": -0.15200241191967115, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.011168443597853184}, {"id": 54, "seek": 39264, "start": 402.32, "end": 412.15999999999997, "text": " team had their own fork of Go GST. And they had put a load of work into fixing bugs, but", "tokens": [50848, 1469, 632, 641, 1065, 17716, 295, 1037, 460, 6840, 13, 400, 436, 632, 829, 257, 3677, 295, 589, 666, 19442, 15120, 11, 457, 51340], "temperature": 0.0, "avg_logprob": -0.15200241191967115, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.011168443597853184}, {"id": 55, "seek": 39264, "start": 413.36, "end": 420.32, "text": " they were never getting merged back into the project as it was under the Tinyzimmer GitHub.", "tokens": [51400, 436, 645, 1128, 1242, 36427, 646, 666, 264, 1716, 382, 309, 390, 833, 264, 39992, 89, 14477, 23331, 13, 51748], "temperature": 0.0, "avg_logprob": -0.15200241191967115, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.011168443597853184}, {"id": 56, "seek": 42032, "start": 421.12, "end": 427.44, "text": " So now it's forked out. Well, it's not actually forked. We forked it into its own organization", "tokens": [50404, 407, 586, 309, 311, 17716, 292, 484, 13, 1042, 11, 309, 311, 406, 767, 17716, 292, 13, 492, 17716, 292, 309, 666, 1080, 1065, 4475, 50720], "temperature": 0.0, "avg_logprob": -0.09818055629730224, "compression_ratio": 1.598901098901099, "no_speech_prob": 0.001992168603464961}, {"id": 57, "seek": 42032, "start": 427.44, "end": 432.8, "text": " and then did the GitHub magic where we unforked it and then the Tinyzimmer one is now a fork of us.", "tokens": [50720, 293, 550, 630, 264, 23331, 5585, 689, 321, 3971, 1284, 292, 309, 293, 550, 264, 39992, 89, 14477, 472, 307, 586, 257, 17716, 295, 505, 13, 50988], "temperature": 0.0, "avg_logprob": -0.09818055629730224, "compression_ratio": 1.598901098901099, "no_speech_prob": 0.001992168603464961}, {"id": 58, "seek": 42032, "start": 433.6, "end": 440.8, "text": " So there's a lot of GitHub kind of organization going on to make it easier for everyone. Did you", "tokens": [51028, 407, 456, 311, 257, 688, 295, 23331, 733, 295, 4475, 516, 322, 281, 652, 309, 3571, 337, 1518, 13, 2589, 291, 51388], "temperature": 0.0, "avg_logprob": -0.09818055629730224, "compression_ratio": 1.598901098901099, "no_speech_prob": 0.001992168603464961}, {"id": 59, "seek": 44080, "start": 440.8, "end": 450.64, "text": " know that GitHub forks don't turn up in Google SEO results and they don't turn up in GitHub", "tokens": [50364, 458, 300, 23331, 337, 1694, 500, 380, 1261, 493, 294, 3329, 22964, 3542, 293, 436, 500, 380, 1261, 493, 294, 23331, 50856], "temperature": 0.0, "avg_logprob": -0.14179504973978935, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.11757290363311768}, {"id": 60, "seek": 44080, "start": 450.64, "end": 456.64, "text": " search results either? And search doesn't work in the repo. So yeah, and search doesn't work in the", "tokens": [50856, 3164, 3542, 2139, 30, 400, 3164, 1177, 380, 589, 294, 264, 49040, 13, 407, 1338, 11, 293, 3164, 1177, 380, 589, 294, 264, 51156], "temperature": 0.0, "avg_logprob": -0.14179504973978935, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.11757290363311768}, {"id": 61, "seek": 44080, "start": 456.64, "end": 465.76, "text": " repo. So basically forks are dumb. I mean, they're not dumb. But forks are bad. We should not be", "tokens": [51156, 49040, 13, 407, 1936, 337, 1694, 366, 10316, 13, 286, 914, 11, 436, 434, 406, 10316, 13, 583, 337, 1694, 366, 1578, 13, 492, 820, 406, 312, 51612], "temperature": 0.0, "avg_logprob": -0.14179504973978935, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.11757290363311768}, {"id": 62, "seek": 46576, "start": 465.76, "end": 473.03999999999996, "text": " relying on forks for a long-term thing whatsoever. So yeah, this is actually really great for everyone", "tokens": [50364, 24140, 322, 337, 1694, 337, 257, 938, 12, 7039, 551, 17076, 13, 407, 1338, 11, 341, 307, 767, 534, 869, 337, 1518, 50728], "temperature": 0.0, "avg_logprob": -0.09668470865272615, "compression_ratio": 1.458128078817734, "no_speech_prob": 0.02754225954413414}, {"id": 63, "seek": 46576, "start": 473.03999999999996, "end": 480.48, "text": " now. So less forks is better for everyone. And like I said earlier, BroadcastBridge uses a mixture", "tokens": [50728, 586, 13, 407, 1570, 337, 1694, 307, 1101, 337, 1518, 13, 400, 411, 286, 848, 3071, 11, 14074, 3734, 33, 15804, 4960, 257, 9925, 51100], "temperature": 0.0, "avg_logprob": -0.09668470865272615, "compression_ratio": 1.458128078817734, "no_speech_prob": 0.02754225954413414}, {"id": 64, "seek": 46576, "start": 480.48, "end": 489.59999999999997, "text": " of SRT, NDI, WebRTC, among a load of other things as well. And so why, you're probably asking,", "tokens": [51100, 295, 20840, 51, 11, 426, 3085, 11, 9573, 49, 18238, 11, 3654, 257, 3677, 295, 661, 721, 382, 731, 13, 400, 370, 983, 11, 291, 434, 1391, 3365, 11, 51556], "temperature": 0.0, "avg_logprob": -0.09668470865272615, "compression_ratio": 1.458128078817734, "no_speech_prob": 0.02754225954413414}, {"id": 65, "seek": 48960, "start": 490.56, "end": 497.68, "text": " why would we even need to use AppSync and AppSync when the modules, the plugins are already in Gstreamer?", "tokens": [50412, 983, 576, 321, 754, 643, 281, 764, 3132, 50, 34015, 293, 3132, 50, 34015, 562, 264, 16679, 11, 264, 33759, 366, 1217, 294, 460, 9291, 260, 30, 50768], "temperature": 0.0, "avg_logprob": -0.11378290461397719, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.03007536381483078}, {"id": 66, "seek": 48960, "start": 498.88, "end": 504.48, "text": " Like Gstreamer already knows how to take in an SRT feed. It already knows how to output an NDI", "tokens": [50828, 1743, 460, 9291, 260, 1217, 3255, 577, 281, 747, 294, 364, 20840, 51, 3154, 13, 467, 1217, 3255, 577, 281, 5598, 364, 426, 3085, 51108], "temperature": 0.0, "avg_logprob": -0.11378290461397719, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.03007536381483078}, {"id": 67, "seek": 48960, "start": 504.48, "end": 512.16, "text": " feed and it knows how to do WebRTC stuff. So why are we building on top of AppSync and AppSync?", "tokens": [51108, 3154, 293, 309, 3255, 577, 281, 360, 9573, 49, 18238, 1507, 13, 407, 983, 366, 321, 2390, 322, 1192, 295, 3132, 50, 34015, 293, 3132, 50, 34015, 30, 51492], "temperature": 0.0, "avg_logprob": -0.11378290461397719, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.03007536381483078}, {"id": 68, "seek": 51216, "start": 513.12, "end": 516.9599999999999, "text": " And it comes down to greater control like I was kind of alluding to earlier.", "tokens": [50412, 400, 309, 1487, 760, 281, 5044, 1969, 411, 286, 390, 733, 295, 439, 33703, 281, 3071, 13, 50604], "temperature": 0.0, "avg_logprob": -0.09754976630210876, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.10414116829633713}, {"id": 69, "seek": 51216, "start": 518.7199999999999, "end": 526.56, "text": " We use Pion to do WebRTC. And that's not because the Gstreamer implementation isn't good. It's just", "tokens": [50692, 492, 764, 430, 313, 281, 360, 9573, 49, 18238, 13, 400, 300, 311, 406, 570, 264, 460, 9291, 260, 11420, 1943, 380, 665, 13, 467, 311, 445, 51084], "temperature": 0.0, "avg_logprob": -0.09754976630210876, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.10414116829633713}, {"id": 70, "seek": 51216, "start": 526.56, "end": 535.1999999999999, "text": " that if we want to be able to do anything that isn't implemented into the Gstreamer implementation,", "tokens": [51084, 300, 498, 321, 528, 281, 312, 1075, 281, 360, 1340, 300, 1943, 380, 12270, 666, 264, 460, 9291, 260, 11420, 11, 51516], "temperature": 0.0, "avg_logprob": -0.09754976630210876, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.10414116829633713}, {"id": 71, "seek": 51216, "start": 535.92, "end": 541.8399999999999, "text": " then we'd need to get someone to actually go and change that code. And that's something that we're", "tokens": [51552, 550, 321, 1116, 643, 281, 483, 1580, 281, 767, 352, 293, 1319, 300, 3089, 13, 400, 300, 311, 746, 300, 321, 434, 51848], "temperature": 0.0, "avg_logprob": -0.09754976630210876, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.10414116829633713}, {"id": 72, "seek": 54216, "start": 542.7199999999999, "end": 548.3199999999999, "text": " able to do. My team aren't capable of doing, but we don't go really, really, really well.", "tokens": [50392, 1075, 281, 360, 13, 1222, 1469, 3212, 380, 8189, 295, 884, 11, 457, 321, 500, 380, 352, 534, 11, 534, 11, 534, 731, 13, 50672], "temperature": 0.0, "avg_logprob": -0.1563050124956214, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.0010046070674434304}, {"id": 73, "seek": 54216, "start": 549.04, "end": 552.8, "text": " And so we can definitely kind of go and take that greater control.", "tokens": [50708, 400, 370, 321, 393, 2138, 733, 295, 352, 293, 747, 300, 5044, 1969, 13, 50896], "temperature": 0.0, "avg_logprob": -0.1563050124956214, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.0010046070674434304}, {"id": 74, "seek": 54216, "start": 555.28, "end": 560.7199999999999, "text": " Like I said, this means we're handling WebRTC in something that we really know. Like, ultimately,", "tokens": [51020, 1743, 286, 848, 11, 341, 1355, 321, 434, 13175, 9573, 49, 18238, 294, 746, 300, 321, 534, 458, 13, 1743, 11, 6284, 11, 51292], "temperature": 0.0, "avg_logprob": -0.1563050124956214, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.0010046070674434304}, {"id": 75, "seek": 54216, "start": 564.16, "end": 569.52, "text": " very few people in this room know about transcoding something from one codec to another. And we just", "tokens": [51464, 588, 1326, 561, 294, 341, 1808, 458, 466, 43800, 8616, 746, 490, 472, 3089, 66, 281, 1071, 13, 400, 321, 445, 51732], "temperature": 0.0, "avg_logprob": -0.1563050124956214, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.0010046070674434304}, {"id": 76, "seek": 56952, "start": 569.52, "end": 578.3199999999999, "text": " rely on FFMpeg or Gstreamer or whatever to do it for us. It's the same with WebRTC for us. We", "tokens": [50364, 10687, 322, 479, 37, 44, 494, 70, 420, 460, 9291, 260, 420, 2035, 281, 360, 309, 337, 505, 13, 467, 311, 264, 912, 365, 9573, 49, 18238, 337, 505, 13, 492, 50804], "temperature": 0.0, "avg_logprob": -0.0881877172560919, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.00165462342556566}, {"id": 77, "seek": 56952, "start": 578.3199999999999, "end": 585.04, "text": " really know what we're doing with WebRTC and we want to be able to kind of tweak things that we", "tokens": [50804, 534, 458, 437, 321, 434, 884, 365, 9573, 49, 18238, 293, 321, 528, 281, 312, 1075, 281, 733, 295, 29879, 721, 300, 321, 51140], "temperature": 0.0, "avg_logprob": -0.0881877172560919, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.00165462342556566}, {"id": 78, "seek": 56952, "start": 585.04, "end": 591.6, "text": " can't necessarily tweak with the Gstreamer implementation. But Pion is hugely, hugely", "tokens": [51140, 393, 380, 4725, 29879, 365, 264, 460, 9291, 260, 11420, 13, 583, 430, 313, 307, 27417, 11, 27417, 51468], "temperature": 0.0, "avg_logprob": -0.0881877172560919, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.00165462342556566}, {"id": 79, "seek": 56952, "start": 591.6, "end": 597.84, "text": " powerful. And this is the other key thing and it's easily upgradeable. So when we actually find a bug", "tokens": [51468, 4005, 13, 400, 341, 307, 264, 661, 2141, 551, 293, 309, 311, 3612, 11484, 712, 13, 407, 562, 321, 767, 915, 257, 7426, 51780], "temperature": 0.0, "avg_logprob": -0.0881877172560919, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.00165462342556566}, {"id": 80, "seek": 59784, "start": 597.84, "end": 599.2800000000001, "text": " in Pion, we can", "tokens": [50364, 294, 430, 313, 11, 321, 393, 50436], "temperature": 0.0, "avg_logprob": -0.19949671957227919, "compression_ratio": 0.6521739130434783, "no_speech_prob": 0.08387020975351334}, {"id": 81, "seek": 71784, "start": 718.1600000000001, "end": 730.0, "text": " a Gstreamer pipeline and never leaving the C level. But cost isn't just measured in terms of", "tokens": [50380, 257, 460, 9291, 260, 15517, 293, 1128, 5012, 264, 383, 1496, 13, 583, 2063, 1943, 380, 445, 12690, 294, 2115, 295, 50972], "temperature": 0.0, "avg_logprob": -0.17958781298469095, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.4295978546142578}, {"id": 82, "seek": 71784, "start": 730.0, "end": 737.9200000000001, "text": " compute. Cost is everything from building the feature all the way through to deploying the", "tokens": [50972, 14722, 13, 20863, 307, 1203, 490, 2390, 264, 4111, 439, 264, 636, 807, 281, 34198, 264, 51368], "temperature": 0.0, "avg_logprob": -0.17958781298469095, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.4295978546142578}, {"id": 83, "seek": 71784, "start": 737.9200000000001, "end": 745.6800000000001, "text": " feature and running the feature. And you've got to look at the whole picture. Pion gives us huge,", "tokens": [51368, 4111, 293, 2614, 264, 4111, 13, 400, 291, 600, 658, 281, 574, 412, 264, 1379, 3036, 13, 430, 313, 2709, 505, 2603, 11, 51756], "temperature": 0.0, "avg_logprob": -0.17958781298469095, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.4295978546142578}, {"id": 84, "seek": 74568, "start": 745.76, "end": 750.4, "text": " huge flexibility and we can move fast and we can add new features and ultimately that means", "tokens": [50368, 2603, 12635, 293, 321, 393, 1286, 2370, 293, 321, 393, 909, 777, 4122, 293, 6284, 300, 1355, 50600], "temperature": 0.0, "avg_logprob": -0.1341832098753556, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.0024840361438691616}, {"id": 85, "seek": 74568, "start": 750.4, "end": 756.3199999999999, "text": " that we win business. So let's take a quick look at AppSource. How many people are actually familiar", "tokens": [50600, 300, 321, 1942, 1606, 13, 407, 718, 311, 747, 257, 1702, 574, 412, 3132, 50, 2948, 13, 1012, 867, 561, 366, 767, 4963, 50896], "temperature": 0.0, "avg_logprob": -0.1341832098753556, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.0024840361438691616}, {"id": 86, "seek": 74568, "start": 756.3199999999999, "end": 765.76, "text": " with AppSource? Right. So AppSource is just another plugin, module, whatever they're called.", "tokens": [50896, 365, 3132, 50, 2948, 30, 1779, 13, 407, 3132, 50, 2948, 307, 445, 1071, 23407, 11, 10088, 11, 2035, 436, 434, 1219, 13, 51368], "temperature": 0.0, "avg_logprob": -0.1341832098753556, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.0024840361438691616}, {"id": 87, "seek": 74568, "start": 767.52, "end": 775.12, "text": " And ultimately, you can put it inside of your pipeline and you can push data into Gstreamer", "tokens": [51456, 400, 6284, 11, 291, 393, 829, 309, 1854, 295, 428, 15517, 293, 291, 393, 2944, 1412, 666, 460, 9291, 260, 51836], "temperature": 0.0, "avg_logprob": -0.1341832098753556, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.0024840361438691616}, {"id": 88, "seek": 77568, "start": 776.56, "end": 784.4799999999999, "text": " using AppSource. You set a load of capabilities on that element, that AppSource element,", "tokens": [50408, 1228, 3132, 50, 2948, 13, 509, 992, 257, 3677, 295, 10862, 322, 300, 4478, 11, 300, 3132, 50, 2948, 4478, 11, 50804], "temperature": 0.0, "avg_logprob": -0.08995912551879882, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.000936825352255255}, {"id": 89, "seek": 77568, "start": 785.28, "end": 791.1999999999999, "text": " telling it, oh, well, this media that I'm just about to push into you is this format and this", "tokens": [50844, 3585, 309, 11, 1954, 11, 731, 11, 341, 3021, 300, 286, 478, 445, 466, 281, 2944, 666, 291, 307, 341, 7877, 293, 341, 51140], "temperature": 0.0, "avg_logprob": -0.08995912551879882, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.000936825352255255}, {"id": 90, "seek": 77568, "start": 791.1999999999999, "end": 797.92, "text": " frame rate and whatever else. And you can push in data or you can make, so you have to push", "tokens": [51140, 3920, 3314, 293, 2035, 1646, 13, 400, 291, 393, 2944, 294, 1412, 420, 291, 393, 652, 11, 370, 291, 362, 281, 2944, 51476], "temperature": 0.0, "avg_logprob": -0.08995912551879882, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.000936825352255255}, {"id": 91, "seek": 77568, "start": 797.92, "end": 803.3599999999999, "text": " data in obviously, but you can also make Gstreamer ask you for the data. So instead of just going,", "tokens": [51476, 1412, 294, 2745, 11, 457, 291, 393, 611, 652, 460, 9291, 260, 1029, 291, 337, 264, 1412, 13, 407, 2602, 295, 445, 516, 11, 51748], "temperature": 0.0, "avg_logprob": -0.08995912551879882, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.000936825352255255}, {"id": 92, "seek": 80336, "start": 803.44, "end": 808.8000000000001, "text": " oh, I've got data, data, data, data, data. And then Gstreamer goes, oh, hold on. I can't do anything", "tokens": [50368, 1954, 11, 286, 600, 658, 1412, 11, 1412, 11, 1412, 11, 1412, 11, 1412, 13, 400, 550, 460, 9291, 260, 1709, 11, 1954, 11, 1797, 322, 13, 286, 393, 380, 360, 1340, 50636], "temperature": 0.0, "avg_logprob": -0.11322078704833985, "compression_ratio": 1.7933579335793357, "no_speech_prob": 0.009909914806485176}, {"id": 93, "seek": 80336, "start": 808.8000000000001, "end": 814.96, "text": " with this. Why are you sending me so much data? You can, Gstreamer can actually ask for it. Now,", "tokens": [50636, 365, 341, 13, 1545, 366, 291, 7750, 385, 370, 709, 1412, 30, 509, 393, 11, 460, 9291, 260, 393, 767, 1029, 337, 309, 13, 823, 11, 50944], "temperature": 0.0, "avg_logprob": -0.11322078704833985, "compression_ratio": 1.7933579335793357, "no_speech_prob": 0.009909914806485176}, {"id": 94, "seek": 80336, "start": 814.96, "end": 819.84, "text": " that's not hugely helpful when it comes to real-time applications because real-time applications,", "tokens": [50944, 300, 311, 406, 27417, 4961, 562, 309, 1487, 281, 957, 12, 3766, 5821, 570, 957, 12, 3766, 5821, 11, 51188], "temperature": 0.0, "avg_logprob": -0.11322078704833985, "compression_ratio": 1.7933579335793357, "no_speech_prob": 0.009909914806485176}, {"id": 95, "seek": 80336, "start": 819.84, "end": 828.16, "text": " in the case of Pion, sending us web, getting RTP data from Pion, for example, that's real-time.", "tokens": [51188, 294, 264, 1389, 295, 430, 313, 11, 7750, 505, 3670, 11, 1242, 497, 16804, 1412, 490, 430, 313, 11, 337, 1365, 11, 300, 311, 957, 12, 3766, 13, 51604], "temperature": 0.0, "avg_logprob": -0.11322078704833985, "compression_ratio": 1.7933579335793357, "no_speech_prob": 0.009909914806485176}, {"id": 96, "seek": 80336, "start": 828.16, "end": 832.4, "text": " And so we want to get that data from Pion and we want to pass it into Gstreamer straight away.", "tokens": [51604, 400, 370, 321, 528, 281, 483, 300, 1412, 490, 430, 313, 293, 321, 528, 281, 1320, 309, 666, 460, 9291, 260, 2997, 1314, 13, 51816], "temperature": 0.0, "avg_logprob": -0.11322078704833985, "compression_ratio": 1.7933579335793357, "no_speech_prob": 0.009909914806485176}, {"id": 97, "seek": 83336, "start": 833.52, "end": 840.08, "text": " Because we're getting it in this constant flow from Pion. Whereas if you were reading a,", "tokens": [50372, 1436, 321, 434, 1242, 309, 294, 341, 5754, 3095, 490, 430, 313, 13, 13813, 498, 291, 645, 3760, 257, 11, 50700], "temperature": 0.0, "avg_logprob": -0.12143281102180481, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.0006265127449296415}, {"id": 98, "seek": 83336, "start": 841.2, "end": 847.2, "text": " if you were reading a file and then you were passing those chunks into Gstreamer,", "tokens": [50756, 498, 291, 645, 3760, 257, 3991, 293, 550, 291, 645, 8437, 729, 24004, 666, 460, 9291, 260, 11, 51056], "temperature": 0.0, "avg_logprob": -0.12143281102180481, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.0006265127449296415}, {"id": 99, "seek": 83336, "start": 847.76, "end": 853.36, "text": " well, you've got control over how fast you push those chunks in. And so why not let Gstreamer go,", "tokens": [51084, 731, 11, 291, 600, 658, 1969, 670, 577, 2370, 291, 2944, 729, 24004, 294, 13, 400, 370, 983, 406, 718, 460, 9291, 260, 352, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12143281102180481, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.0006265127449296415}, {"id": 100, "seek": 83336, "start": 853.36, "end": 856.5600000000001, "text": " ah, I want a bit more data. I want a bit more data. I want a bit more data.", "tokens": [51364, 3716, 11, 286, 528, 257, 857, 544, 1412, 13, 286, 528, 257, 857, 544, 1412, 13, 286, 528, 257, 857, 544, 1412, 13, 51524], "temperature": 0.0, "avg_logprob": -0.12143281102180481, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.0006265127449296415}, {"id": 101, "seek": 85656, "start": 857.04, "end": 864.3199999999999, "text": " Right. App sync is absolutely no different. It's a, it's a plug-in, it's a module. And,", "tokens": [50388, 1779, 13, 3132, 20271, 307, 3122, 572, 819, 13, 467, 311, 257, 11, 309, 311, 257, 5452, 12, 259, 11, 309, 311, 257, 10088, 13, 400, 11, 50752], "temperature": 0.0, "avg_logprob": -0.14629330173615487, "compression_ratio": 1.765625, "no_speech_prob": 0.016774162650108337}, {"id": 102, "seek": 85656, "start": 864.3199999999999, "end": 867.04, "text": " and when you put it into the pipeline, it becomes an element.", "tokens": [50752, 293, 562, 291, 829, 309, 666, 264, 15517, 11, 309, 3643, 364, 4478, 13, 50888], "temperature": 0.0, "avg_logprob": -0.14629330173615487, "compression_ratio": 1.765625, "no_speech_prob": 0.016774162650108337}, {"id": 103, "seek": 85656, "start": 869.3599999999999, "end": 875.28, "text": " And ultimately you get push data out of AppSync. And so imagine you've got AppSource and then you've", "tokens": [51004, 400, 6284, 291, 483, 2944, 1412, 484, 295, 3132, 50, 34015, 13, 400, 370, 3811, 291, 600, 658, 3132, 50, 2948, 293, 550, 291, 600, 51300], "temperature": 0.0, "avg_logprob": -0.14629330173615487, "compression_ratio": 1.765625, "no_speech_prob": 0.016774162650108337}, {"id": 104, "seek": 85656, "start": 875.28, "end": 880.56, "text": " got something in the middle, whether or not that's transforming it or transcoding it. And then you've", "tokens": [51300, 658, 746, 294, 264, 2808, 11, 1968, 420, 406, 300, 311, 27210, 309, 420, 43800, 8616, 309, 13, 400, 550, 291, 600, 51564], "temperature": 0.0, "avg_logprob": -0.14629330173615487, "compression_ratio": 1.765625, "no_speech_prob": 0.016774162650108337}, {"id": 105, "seek": 85656, "start": 880.56, "end": 886.16, "text": " got AppSync and you're connecting all these bits together. And so you're pushing data in. Gstreamer", "tokens": [51564, 658, 3132, 50, 34015, 293, 291, 434, 11015, 439, 613, 9239, 1214, 13, 400, 370, 291, 434, 7380, 1412, 294, 13, 460, 9291, 260, 51844], "temperature": 0.0, "avg_logprob": -0.14629330173615487, "compression_ratio": 1.765625, "no_speech_prob": 0.016774162650108337}, {"id": 106, "seek": 88616, "start": 886.16, "end": 891.6, "text": " is then doing something with it. And then it's pushing it, it's passing it over to AppSync.", "tokens": [50364, 307, 550, 884, 746, 365, 309, 13, 400, 550, 309, 311, 7380, 309, 11, 309, 311, 8437, 309, 670, 281, 3132, 50, 34015, 13, 50636], "temperature": 0.0, "avg_logprob": -0.11717548193754973, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.0010429902467876673}, {"id": 107, "seek": 88616, "start": 891.6, "end": 899.28, "text": " And then AppSync sends it out to your application as data. Not as UDP, not, not via report or anything.", "tokens": [50636, 400, 550, 3132, 50, 34015, 14790, 309, 484, 281, 428, 3861, 382, 1412, 13, 1726, 382, 624, 11373, 11, 406, 11, 406, 5766, 2275, 420, 1340, 13, 51020], "temperature": 0.0, "avg_logprob": -0.11717548193754973, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.0010429902467876673}, {"id": 108, "seek": 88616, "start": 899.28, "end": 906.48, "text": " It's giving you the, the, the raw buffer of data. So you get pushed your data from AppSync via the,", "tokens": [51020, 467, 311, 2902, 291, 264, 11, 264, 11, 264, 8936, 21762, 295, 1412, 13, 407, 291, 483, 9152, 428, 1412, 490, 3132, 50, 34015, 5766, 264, 11, 51380], "temperature": 0.0, "avg_logprob": -0.11717548193754973, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.0010429902467876673}, {"id": 109, "seek": 88616, "start": 906.48, "end": 912.0799999999999, "text": " the, the, the new sample signal and event. I've got some data here you go.", "tokens": [51380, 264, 11, 264, 11, 264, 777, 6889, 6358, 293, 2280, 13, 286, 600, 658, 512, 1412, 510, 291, 352, 13, 51660], "temperature": 0.0, "avg_logprob": -0.11717548193754973, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.0010429902467876673}, {"id": 110, "seek": 91208, "start": 912.1600000000001, "end": 922.8000000000001, "text": " Notice how this is all go lang. So, yeah, let's take a very quick look. So we've got our sync.", "tokens": [50368, 13428, 577, 341, 307, 439, 352, 2265, 13, 407, 11, 1338, 11, 718, 311, 747, 257, 588, 1702, 574, 13, 407, 321, 600, 658, 527, 20271, 13, 50900], "temperature": 0.0, "avg_logprob": -0.1527750239652746, "compression_ratio": 1.5449438202247192, "no_speech_prob": 0.0005672958213835955}, {"id": 111, "seek": 91208, "start": 922.8000000000001, "end": 928.4000000000001, "text": " So that's an AppSource, AppSync element that I've made. And I'm setting some callbacks on it.", "tokens": [50900, 407, 300, 311, 364, 3132, 50, 2948, 11, 3132, 50, 34015, 4478, 300, 286, 600, 1027, 13, 400, 286, 478, 3287, 512, 818, 17758, 322, 309, 13, 51180], "temperature": 0.0, "avg_logprob": -0.1527750239652746, "compression_ratio": 1.5449438202247192, "no_speech_prob": 0.0005672958213835955}, {"id": 112, "seek": 91208, "start": 928.96, "end": 938.08, "text": " And then we've got new sample funk. And then that gives me, that gives me my, my sync.", "tokens": [51208, 400, 550, 321, 600, 658, 777, 6889, 26476, 13, 400, 550, 300, 2709, 385, 11, 300, 2709, 385, 452, 11, 452, 20271, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1527750239652746, "compression_ratio": 1.5449438202247192, "no_speech_prob": 0.0005672958213835955}, {"id": 113, "seek": 93808, "start": 938.64, "end": 943.9200000000001, "text": " And then I'm going to tell it as a return. I'm going to tell it what the return, what the flow", "tokens": [50392, 400, 550, 286, 478, 516, 281, 980, 309, 382, 257, 2736, 13, 286, 478, 516, 281, 980, 309, 437, 264, 2736, 11, 437, 264, 3095, 50656], "temperature": 0.0, "avg_logprob": -0.09411026449764476, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.011754543520510197}, {"id": 114, "seek": 93808, "start": 943.9200000000001, "end": 953.76, "text": " state is. And so I pull the sample. And then if the sample isn't end of, isn't nil, then,", "tokens": [50656, 1785, 307, 13, 400, 370, 286, 2235, 264, 6889, 13, 400, 550, 498, 264, 6889, 1943, 380, 917, 295, 11, 1943, 380, 297, 388, 11, 550, 11, 51148], "temperature": 0.0, "avg_logprob": -0.09411026449764476, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.011754543520510197}, {"id": 115, "seek": 93808, "start": 953.76, "end": 959.76, "text": " then we carry on. If it is nil, then I'm returning that we are at the end of the stream.", "tokens": [51148, 550, 321, 3985, 322, 13, 759, 309, 307, 297, 388, 11, 550, 286, 478, 12678, 300, 321, 366, 412, 264, 917, 295, 264, 4309, 13, 51448], "temperature": 0.0, "avg_logprob": -0.09411026449764476, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.011754543520510197}, {"id": 116, "seek": 95976, "start": 960.64, "end": 966.0, "text": " And then buffer. So we get this, our sample. So we're pulling the sample.", "tokens": [50408, 400, 550, 21762, 13, 407, 321, 483, 341, 11, 527, 6889, 13, 407, 321, 434, 8407, 264, 6889, 13, 50676], "temperature": 0.0, "avg_logprob": -0.13102289346548227, "compression_ratio": 1.9240506329113924, "no_speech_prob": 0.01362611260265112}, {"id": 117, "seek": 95976, "start": 967.04, "end": 972.4, "text": " And then, and then we're getting the buffer out of that. And then ultimately reading some, some,", "tokens": [50728, 400, 550, 11, 293, 550, 321, 434, 1242, 264, 21762, 484, 295, 300, 13, 400, 550, 6284, 3760, 512, 11, 512, 11, 50996], "temperature": 0.0, "avg_logprob": -0.13102289346548227, "compression_ratio": 1.9240506329113924, "no_speech_prob": 0.01362611260265112}, {"id": 118, "seek": 95976, "start": 972.4, "end": 979.2, "text": " some information from that, from that buffer map, changing it from big Indian to little Indian,", "tokens": [50996, 512, 1589, 490, 300, 11, 490, 300, 21762, 4471, 11, 4473, 309, 490, 955, 6427, 281, 707, 6427, 11, 51336], "temperature": 0.0, "avg_logprob": -0.13102289346548227, "compression_ratio": 1.9240506329113924, "no_speech_prob": 0.01362611260265112}, {"id": 119, "seek": 95976, "start": 979.2, "end": 983.84, "text": " I think, or something. And then, and then doing some stuff on it, doing some maths on it.", "tokens": [51336, 286, 519, 11, 420, 746, 13, 400, 550, 11, 293, 550, 884, 512, 1507, 322, 309, 11, 884, 512, 36287, 322, 309, 13, 51568], "temperature": 0.0, "avg_logprob": -0.13102289346548227, "compression_ratio": 1.9240506329113924, "no_speech_prob": 0.01362611260265112}, {"id": 120, "seek": 95976, "start": 983.84, "end": 988.64, "text": " Not a lot of like useful information there. Like in terms of like, what am I actually then going to", "tokens": [51568, 1726, 257, 688, 295, 411, 4420, 1589, 456, 13, 1743, 294, 2115, 295, 411, 11, 437, 669, 286, 767, 550, 516, 281, 51808], "temperature": 0.0, "avg_logprob": -0.13102289346548227, "compression_ratio": 1.9240506329113924, "no_speech_prob": 0.01362611260265112}, {"id": 121, "seek": 98864, "start": 988.64, "end": 996.4, "text": " go and do with it? Well, at the moment, it's just printing out RMS. But then you can go off and do", "tokens": [50364, 352, 293, 360, 365, 309, 30, 1042, 11, 412, 264, 1623, 11, 309, 311, 445, 14699, 484, 497, 10288, 13, 583, 550, 291, 393, 352, 766, 293, 360, 50752], "temperature": 0.0, "avg_logprob": -0.12191898521335645, "compression_ratio": 1.5025906735751295, "no_speech_prob": 0.0022552968002855778}, {"id": 122, "seek": 98864, "start": 996.4, "end": 1004.3199999999999, "text": " whatever you want with it. For us, that means getting a video and audio data out of G streamer", "tokens": [50752, 2035, 291, 528, 365, 309, 13, 1171, 505, 11, 300, 1355, 1242, 257, 960, 293, 6278, 1412, 484, 295, 460, 4309, 260, 51148], "temperature": 0.0, "avg_logprob": -0.12191898521335645, "compression_ratio": 1.5025906735751295, "no_speech_prob": 0.0022552968002855778}, {"id": 123, "seek": 98864, "start": 1004.3199999999999, "end": 1013.2, "text": " and chucking it into NDI. Oh, Dan, why are you not using NDI within G streamer? Well, I tell you", "tokens": [51148, 293, 20870, 278, 309, 666, 426, 3085, 13, 876, 11, 3394, 11, 983, 366, 291, 406, 1228, 426, 3085, 1951, 460, 4309, 260, 30, 1042, 11, 286, 980, 291, 51592], "temperature": 0.0, "avg_logprob": -0.12191898521335645, "compression_ratio": 1.5025906735751295, "no_speech_prob": 0.0022552968002855778}, {"id": 124, "seek": 101320, "start": 1013.2, "end": 1018.8000000000001, "text": " number one, when we did our NDI integration, G streamer didn't have NDI. It was, it was completely", "tokens": [50364, 1230, 472, 11, 562, 321, 630, 527, 426, 3085, 10980, 11, 460, 4309, 260, 994, 380, 362, 426, 3085, 13, 467, 390, 11, 309, 390, 2584, 50644], "temperature": 0.0, "avg_logprob": -0.11660164858387635, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0019856798462569714}, {"id": 125, "seek": 101320, "start": 1018.8000000000001, "end": 1023.9200000000001, "text": " separate. It was, it was a different repo. And it wasn't part of the G streamer rust plugins.", "tokens": [50644, 4994, 13, 467, 390, 11, 309, 390, 257, 819, 49040, 13, 400, 309, 2067, 380, 644, 295, 264, 460, 4309, 260, 15259, 33759, 13, 50900], "temperature": 0.0, "avg_logprob": -0.11660164858387635, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0019856798462569714}, {"id": 126, "seek": 101320, "start": 1025.04, "end": 1031.92, "text": " And then B, we do extra stuff that G streamer doesn't know how to do yet. So we, we grab tally", "tokens": [50956, 400, 550, 363, 11, 321, 360, 2857, 1507, 300, 460, 4309, 260, 1177, 380, 458, 577, 281, 360, 1939, 13, 407, 321, 11, 321, 4444, 256, 379, 51300], "temperature": 0.0, "avg_logprob": -0.11660164858387635, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0019856798462569714}, {"id": 127, "seek": 101320, "start": 1031.92, "end": 1037.76, "text": " information from, from NDI. And to be able to do that, you need access to the underlying NDI sender.", "tokens": [51300, 1589, 490, 11, 490, 426, 3085, 13, 400, 281, 312, 1075, 281, 360, 300, 11, 291, 643, 2105, 281, 264, 14217, 426, 3085, 2845, 260, 13, 51592], "temperature": 0.0, "avg_logprob": -0.11660164858387635, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0019856798462569714}, {"id": 128, "seek": 103776, "start": 1038.72, "end": 1044.48, "text": " And, and so there's stuff that G streamer can't do yet. Something that we actually want to add in to", "tokens": [50412, 400, 11, 293, 370, 456, 311, 1507, 300, 460, 4309, 260, 393, 380, 360, 1939, 13, 6595, 300, 321, 767, 528, 281, 909, 294, 281, 50700], "temperature": 0.0, "avg_logprob": -0.12465635367802211, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.0044682943262159824}, {"id": 129, "seek": 103776, "start": 1044.48, "end": 1051.04, "text": " G streamer. So that we can stop sending stuff via the NDI SDK directly and we can just let G streamer", "tokens": [50700, 460, 4309, 260, 13, 407, 300, 321, 393, 1590, 7750, 1507, 5766, 264, 426, 3085, 37135, 3838, 293, 321, 393, 445, 718, 460, 4309, 260, 51028], "temperature": 0.0, "avg_logprob": -0.12465635367802211, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.0044682943262159824}, {"id": 130, "seek": 103776, "start": 1051.04, "end": 1057.52, "text": " deal with it for us. But again, goes back to that cost analysis, right? At the moment, we can get that", "tokens": [51028, 2028, 365, 309, 337, 505, 13, 583, 797, 11, 1709, 646, 281, 300, 2063, 5215, 11, 558, 30, 1711, 264, 1623, 11, 321, 393, 483, 300, 51352], "temperature": 0.0, "avg_logprob": -0.12465635367802211, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.0044682943262159824}, {"id": 131, "seek": 103776, "start": 1057.52, "end": 1065.2, "text": " data out of G streamer using app sync and chuck it out via NDI. We can do that. And it's relatively", "tokens": [51352, 1412, 484, 295, 460, 4309, 260, 1228, 724, 20271, 293, 20870, 309, 484, 5766, 426, 3085, 13, 492, 393, 360, 300, 13, 400, 309, 311, 7226, 51736], "temperature": 0.0, "avg_logprob": -0.12465635367802211, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.0044682943262159824}, {"id": 132, "seek": 106520, "start": 1065.28, "end": 1070.8, "text": " cheap. But then there's a load of extra work for us to be able to kind of go in and figure out the", "tokens": [50368, 7084, 13, 583, 550, 456, 311, 257, 3677, 295, 2857, 589, 337, 505, 281, 312, 1075, 281, 733, 295, 352, 294, 293, 2573, 484, 264, 50644], "temperature": 0.0, "avg_logprob": -0.11874627621374398, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.002717734081670642}, {"id": 133, "seek": 106520, "start": 1070.8, "end": 1076.64, "text": " right way of doing it in G streamer so that like tally information becomes available as a signal.", "tokens": [50644, 558, 636, 295, 884, 309, 294, 460, 4309, 260, 370, 300, 411, 256, 379, 1589, 3643, 2435, 382, 257, 6358, 13, 50936], "temperature": 0.0, "avg_logprob": -0.11874627621374398, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.002717734081670642}, {"id": 134, "seek": 106520, "start": 1077.92, "end": 1086.32, "text": " So yeah, for us, this means that we have to handle RTP and RTCP from Pion. Because Pion,", "tokens": [51000, 407, 1338, 11, 337, 505, 11, 341, 1355, 300, 321, 362, 281, 4813, 497, 16804, 293, 497, 18238, 47, 490, 430, 313, 13, 1436, 430, 313, 11, 51420], "temperature": 0.0, "avg_logprob": -0.11874627621374398, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.002717734081670642}, {"id": 135, "seek": 106520, "start": 1087.3600000000001, "end": 1093.92, "text": " within WebRTC, WebRTC is made up of lots of standards. But ultimately the media is RTP.", "tokens": [51472, 1951, 9573, 49, 18238, 11, 9573, 49, 18238, 307, 1027, 493, 295, 3195, 295, 7787, 13, 583, 6284, 264, 3021, 307, 497, 16804, 13, 51800], "temperature": 0.0, "avg_logprob": -0.11874627621374398, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.002717734081670642}, {"id": 136, "seek": 109392, "start": 1094.64, "end": 1101.2, "text": " And the bit that tells you what the quality is and everything else that goes with it along with it", "tokens": [50400, 400, 264, 857, 300, 5112, 291, 437, 264, 3125, 307, 293, 1203, 1646, 300, 1709, 365, 309, 2051, 365, 309, 50728], "temperature": 0.0, "avg_logprob": -0.09341323084947539, "compression_ratio": 1.572192513368984, "no_speech_prob": 0.0006927769863978028}, {"id": 137, "seek": 109392, "start": 1101.2, "end": 1110.0800000000002, "text": " is RTCP. So it's very easy to forget about things that are very important when you don't deal with", "tokens": [50728, 307, 497, 18238, 47, 13, 407, 309, 311, 588, 1858, 281, 2870, 466, 721, 300, 366, 588, 1021, 562, 291, 500, 380, 2028, 365, 51172], "temperature": 0.0, "avg_logprob": -0.09341323084947539, "compression_ratio": 1.572192513368984, "no_speech_prob": 0.0006927769863978028}, {"id": 138, "seek": 109392, "start": 1110.0800000000002, "end": 1119.04, "text": " them. Like RTCP. SFU people in the room will go, ah, you could never forget about RTCP. But as a", "tokens": [51172, 552, 13, 1743, 497, 18238, 47, 13, 31095, 52, 561, 294, 264, 1808, 486, 352, 11, 3716, 11, 291, 727, 1128, 2870, 466, 497, 18238, 47, 13, 583, 382, 257, 51620], "temperature": 0.0, "avg_logprob": -0.09341323084947539, "compression_ratio": 1.572192513368984, "no_speech_prob": 0.0006927769863978028}, {"id": 139, "seek": 111904, "start": 1119.04, "end": 1126.0, "text": " web developer, the browser deals with all of this for us. And so it's very easy for us to go, ah,", "tokens": [50364, 3670, 10754, 11, 264, 11185, 11215, 365, 439, 295, 341, 337, 505, 13, 400, 370, 309, 311, 588, 1858, 337, 505, 281, 352, 11, 3716, 11, 50712], "temperature": 0.0, "avg_logprob": -0.07277173042297364, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.01913769729435444}, {"id": 140, "seek": 111904, "start": 1126.0, "end": 1131.04, "text": " RTP, I'm going to get my media. I'm going to get my media. And then everything works really,", "tokens": [50712, 497, 16804, 11, 286, 478, 516, 281, 483, 452, 3021, 13, 286, 478, 516, 281, 483, 452, 3021, 13, 400, 550, 1203, 1985, 534, 11, 50964], "temperature": 0.0, "avg_logprob": -0.07277173042297364, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.01913769729435444}, {"id": 141, "seek": 111904, "start": 1131.04, "end": 1136.8799999999999, "text": " really well when you're in a really nice network environment. But then you chuck in real life", "tokens": [50964, 534, 731, 562, 291, 434, 294, 257, 534, 1481, 3209, 2823, 13, 583, 550, 291, 20870, 294, 957, 993, 51256], "temperature": 0.0, "avg_logprob": -0.07277173042297364, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.01913769729435444}, {"id": 142, "seek": 111904, "start": 1136.8799999999999, "end": 1143.44, "text": " scenario and the audio in the video goes terrible. Why did the audio and video go terrible? Because", "tokens": [51256, 9005, 293, 264, 6278, 294, 264, 960, 1709, 6237, 13, 1545, 630, 264, 6278, 293, 960, 352, 6237, 30, 1436, 51584], "temperature": 0.0, "avg_logprob": -0.07277173042297364, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.01913769729435444}, {"id": 143, "seek": 114344, "start": 1143.44, "end": 1151.28, "text": " there's no RTCP feedback mechanism to go, ah, something's going wrong. But yeah, GStreamer", "tokens": [50364, 456, 311, 572, 497, 18238, 47, 5824, 7513, 281, 352, 11, 3716, 11, 746, 311, 516, 2085, 13, 583, 1338, 11, 460, 4520, 1572, 260, 50756], "temperature": 0.0, "avg_logprob": -0.11884038588579963, "compression_ratio": 1.4517766497461928, "no_speech_prob": 0.008139535784721375}, {"id": 144, "seek": 114344, "start": 1151.28, "end": 1158.96, "text": " makes all of this easy. And very quickly on this very specific thing, we use RTP bin within GStreamer.", "tokens": [50756, 1669, 439, 295, 341, 1858, 13, 400, 588, 2661, 322, 341, 588, 2685, 551, 11, 321, 764, 497, 16804, 5171, 1951, 460, 4520, 1572, 260, 13, 51140], "temperature": 0.0, "avg_logprob": -0.11884038588579963, "compression_ratio": 1.4517766497461928, "no_speech_prob": 0.008139535784721375}, {"id": 145, "seek": 114344, "start": 1159.52, "end": 1167.1200000000001, "text": " So that's that middle bit for us. We use app source, chuck it into RTP bin, and then we do a", "tokens": [51168, 407, 300, 311, 300, 2808, 857, 337, 505, 13, 492, 764, 724, 4009, 11, 20870, 309, 666, 497, 16804, 5171, 11, 293, 550, 321, 360, 257, 51548], "temperature": 0.0, "avg_logprob": -0.11884038588579963, "compression_ratio": 1.4517766497461928, "no_speech_prob": 0.008139535784721375}, {"id": 146, "seek": 116712, "start": 1167.1999999999998, "end": 1176.1599999999999, "text": " load of transcoding and stuff as well. And then we get app sync. RTP bin is magical. If you deal", "tokens": [50368, 3677, 295, 43800, 8616, 293, 1507, 382, 731, 13, 400, 550, 321, 483, 724, 20271, 13, 497, 16804, 5171, 307, 12066, 13, 759, 291, 2028, 50816], "temperature": 0.0, "avg_logprob": -0.09703152935679366, "compression_ratio": 1.4838709677419355, "no_speech_prob": 0.014149080030620098}, {"id": 147, "seek": 116712, "start": 1176.1599999999999, "end": 1186.1599999999999, "text": " with RTP at all with GStreamer, then you need to be using RTP bin. There's a lot of text there.", "tokens": [50816, 365, 497, 16804, 412, 439, 365, 460, 4520, 1572, 260, 11, 550, 291, 643, 281, 312, 1228, 497, 16804, 5171, 13, 821, 311, 257, 688, 295, 2487, 456, 13, 51316], "temperature": 0.0, "avg_logprob": -0.09703152935679366, "compression_ratio": 1.4838709677419355, "no_speech_prob": 0.014149080030620098}, {"id": 148, "seek": 116712, "start": 1187.36, "end": 1193.76, "text": " But ultimately, it implements everything you need to be able to handle RTP and RTCP", "tokens": [51376, 583, 6284, 11, 309, 704, 17988, 1203, 291, 643, 281, 312, 1075, 281, 4813, 497, 16804, 293, 497, 18238, 47, 51696], "temperature": 0.0, "avg_logprob": -0.09703152935679366, "compression_ratio": 1.4838709677419355, "no_speech_prob": 0.014149080030620098}, {"id": 149, "seek": 119376, "start": 1194.72, "end": 1205.04, "text": " and demuxing of payloads. And it's just a very nice all in all thing that deals with", "tokens": [50412, 293, 1371, 2449, 278, 295, 30918, 82, 13, 400, 309, 311, 445, 257, 588, 1481, 439, 294, 439, 551, 300, 11215, 365, 50928], "temperature": 0.0, "avg_logprob": -0.160590296206267, "compression_ratio": 1.4126984126984128, "no_speech_prob": 0.0006638942868448794}, {"id": 150, "seek": 119376, "start": 1205.04, "end": 1213.12, "text": " everything using all of the separate, all the separate plugins. But it forces it all together", "tokens": [50928, 1203, 1228, 439, 295, 264, 4994, 11, 439, 264, 4994, 33759, 13, 583, 309, 5874, 309, 439, 1214, 51332], "temperature": 0.0, "avg_logprob": -0.160590296206267, "compression_ratio": 1.4126984126984128, "no_speech_prob": 0.0006638942868448794}, {"id": 151, "seek": 121312, "start": 1213.12, "end": 1223.9199999999998, "text": " nicely for you. So for us, that's connecting the app source sync pads to RTP bin. And you'll notice", "tokens": [50364, 9594, 337, 291, 13, 407, 337, 505, 11, 300, 311, 11015, 264, 724, 4009, 20271, 19179, 281, 497, 16804, 5171, 13, 400, 291, 603, 3449, 50904], "temperature": 0.0, "avg_logprob": -0.11378116821974851, "compression_ratio": 1.68, "no_speech_prob": 0.04725269228219986}, {"id": 152, "seek": 121312, "start": 1223.9199999999998, "end": 1233.04, "text": " I say pads. So for us, you can see up the top there RTP bin. So we're requesting a pad from RTP bin", "tokens": [50904, 286, 584, 19179, 13, 407, 337, 505, 11, 291, 393, 536, 493, 264, 1192, 456, 497, 16804, 5171, 13, 407, 321, 434, 31937, 257, 6887, 490, 497, 16804, 5171, 51360], "temperature": 0.0, "avg_logprob": -0.11378116821974851, "compression_ratio": 1.68, "no_speech_prob": 0.04725269228219986}, {"id": 153, "seek": 121312, "start": 1233.6799999999998, "end": 1240.1599999999999, "text": " in that format. So it's a receive RTCP sync. And then we're also requesting a pad of send RTCP", "tokens": [51392, 294, 300, 7877, 13, 407, 309, 311, 257, 4774, 497, 18238, 47, 20271, 13, 400, 550, 321, 434, 611, 31937, 257, 6887, 295, 2845, 497, 18238, 47, 51716], "temperature": 0.0, "avg_logprob": -0.11378116821974851, "compression_ratio": 1.68, "no_speech_prob": 0.04725269228219986}, {"id": 154, "seek": 124016, "start": 1240.24, "end": 1249.8400000000001, "text": " sync source as well. We then go and make a new app sync and a new app source. And you can see", "tokens": [50368, 20271, 4009, 382, 731, 13, 492, 550, 352, 293, 652, 257, 777, 724, 20271, 293, 257, 777, 724, 4009, 13, 400, 291, 393, 536, 50848], "temperature": 0.0, "avg_logprob": -0.1250543123410072, "compression_ratio": 1.7085714285714286, "no_speech_prob": 0.008574583567678928}, {"id": 155, "seek": 124016, "start": 1249.8400000000001, "end": 1258.96, "text": " they're labeled RTCP app sync and RTCP app source. We then add those to our pipeline because otherwise", "tokens": [50848, 436, 434, 21335, 497, 18238, 47, 724, 20271, 293, 497, 18238, 47, 724, 4009, 13, 492, 550, 909, 729, 281, 527, 15517, 570, 5911, 51304], "temperature": 0.0, "avg_logprob": -0.1250543123410072, "compression_ratio": 1.7085714285714286, "no_speech_prob": 0.008574583567678928}, {"id": 156, "seek": 124016, "start": 1258.96, "end": 1266.96, "text": " nothing works. All of your elements have got to be in a pipeline. And then we link our RTCP app source", "tokens": [51304, 1825, 1985, 13, 1057, 295, 428, 4959, 362, 658, 281, 312, 294, 257, 15517, 13, 400, 550, 321, 2113, 527, 497, 18238, 47, 724, 4009, 51704], "temperature": 0.0, "avg_logprob": -0.1250543123410072, "compression_ratio": 1.7085714285714286, "no_speech_prob": 0.008574583567678928}, {"id": 157, "seek": 126696, "start": 1267.8400000000001, "end": 1278.72, "text": " pad RTCP app source, get static pad source, link RTCP sync pad. Yes. So I'm getting the app,", "tokens": [50408, 6887, 497, 18238, 47, 724, 4009, 11, 483, 13437, 6887, 4009, 11, 2113, 497, 18238, 47, 20271, 6887, 13, 1079, 13, 407, 286, 478, 1242, 264, 724, 11, 50952], "temperature": 0.0, "avg_logprob": -0.1535045342011885, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0058515737764537334}, {"id": 158, "seek": 126696, "start": 1278.72, "end": 1288.88, "text": " sorry. I'm grabbing the RTCP sync pad from the RTP bin. And I'm linking it over to the RTCP app source.", "tokens": [50952, 2597, 13, 286, 478, 23771, 264, 497, 18238, 47, 20271, 6887, 490, 264, 497, 16804, 5171, 13, 400, 286, 478, 25775, 309, 670, 281, 264, 497, 18238, 47, 724, 4009, 13, 51460], "temperature": 0.0, "avg_logprob": -0.1535045342011885, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0058515737764537334}, {"id": 159, "seek": 126696, "start": 1290.24, "end": 1295.8400000000001, "text": " So that's basically just saying RTP bin is going to give me some information up to RTCP", "tokens": [51528, 407, 300, 311, 1936, 445, 1566, 497, 16804, 5171, 307, 516, 281, 976, 385, 512, 1589, 493, 281, 497, 18238, 47, 51808], "temperature": 0.0, "avg_logprob": -0.1535045342011885, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0058515737764537334}, {"id": 160, "seek": 129584, "start": 1295.9199999999998, "end": 1301.28, "text": " information via a pad. And I'm connecting to that pad so that I can then grab that information and", "tokens": [50368, 1589, 5766, 257, 6887, 13, 400, 286, 478, 11015, 281, 300, 6887, 370, 300, 286, 393, 550, 4444, 300, 1589, 293, 50636], "temperature": 0.0, "avg_logprob": -0.12108372627420629, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.0038054874166846275}, {"id": 161, "seek": 129584, "start": 1301.28, "end": 1310.08, "text": " send it over, send it back via Pion up to my web RTCP. So you'll get RTP in, in this case, you'll get", "tokens": [50636, 2845, 309, 670, 11, 2845, 309, 646, 5766, 430, 313, 493, 281, 452, 3670, 497, 18238, 47, 13, 407, 291, 603, 483, 497, 16804, 294, 11, 294, 341, 1389, 11, 291, 603, 483, 51076], "temperature": 0.0, "avg_logprob": -0.12108372627420629, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.0038054874166846275}, {"id": 162, "seek": 129584, "start": 1310.08, "end": 1318.8, "text": " RTP in into RTP bin, but you'll get RTCP in and out. So you'll get told RTCP and you'll also send", "tokens": [51076, 497, 16804, 294, 666, 497, 16804, 5171, 11, 457, 291, 603, 483, 497, 18238, 47, 294, 293, 484, 13, 407, 291, 603, 483, 1907, 497, 18238, 47, 293, 291, 603, 611, 2845, 51512], "temperature": 0.0, "avg_logprob": -0.12108372627420629, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.0038054874166846275}, {"id": 163, "seek": 131880, "start": 1318.8, "end": 1325.12, "text": " it back out as well. And like I say, don't forget about the RTCP. As you can tell, I forgot about the", "tokens": [50364, 309, 646, 484, 382, 731, 13, 400, 411, 286, 584, 11, 500, 380, 2870, 466, 264, 497, 18238, 47, 13, 1018, 291, 393, 980, 11, 286, 5298, 466, 264, 50680], "temperature": 0.0, "avg_logprob": -0.10528118810921072, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.1335853487253189}, {"id": 164, "seek": 131880, "start": 1325.12, "end": 1333.68, "text": " RTCP and ended up doing certain demos and going, ah, look, it's really great. And then someone went", "tokens": [50680, 497, 18238, 47, 293, 4590, 493, 884, 1629, 33788, 293, 516, 11, 3716, 11, 574, 11, 309, 311, 534, 869, 13, 400, 550, 1580, 1437, 51108], "temperature": 0.0, "avg_logprob": -0.10528118810921072, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.1335853487253189}, {"id": 165, "seek": 131880, "start": 1333.68, "end": 1339.84, "text": " and tried it on a really crappy internet connection and went, no, Dan, it doesn't work. And, and made", "tokens": [51108, 293, 3031, 309, 322, 257, 534, 36531, 4705, 4984, 293, 1437, 11, 572, 11, 3394, 11, 309, 1177, 380, 589, 13, 400, 11, 293, 1027, 51416], "temperature": 0.0, "avg_logprob": -0.10528118810921072, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.1335853487253189}, {"id": 166, "seek": 131880, "start": 1339.84, "end": 1348.56, "text": " me look rather foolish. So you end up looking something like this. So does everyone know about", "tokens": [51416, 385, 574, 2831, 23478, 13, 407, 291, 917, 493, 1237, 746, 411, 341, 13, 407, 775, 1518, 458, 466, 51852], "temperature": 0.0, "avg_logprob": -0.10528118810921072, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.1335853487253189}, {"id": 167, "seek": 134880, "start": 1349.2, "end": 1351.6, "text": " the dot graphs that you can generate from GStreamer?", "tokens": [50384, 264, 5893, 24877, 300, 291, 393, 8460, 490, 460, 4520, 1572, 260, 30, 50504], "temperature": 0.0, "avg_logprob": -0.09984850415996477, "compression_ratio": 1.661904761904762, "no_speech_prob": 0.0010595140047371387}, {"id": 168, "seek": 134880, "start": 1353.9199999999998, "end": 1361.04, "text": " A couple of nods, not that many. So you can, within GStreamer, you can tell it, I want you to export", "tokens": [50620, 316, 1916, 295, 15224, 82, 11, 406, 300, 867, 13, 407, 291, 393, 11, 1951, 460, 4520, 1572, 260, 11, 291, 393, 980, 309, 11, 286, 528, 291, 281, 10725, 50976], "temperature": 0.0, "avg_logprob": -0.09984850415996477, "compression_ratio": 1.661904761904762, "no_speech_prob": 0.0010595140047371387}, {"id": 169, "seek": 134880, "start": 1361.04, "end": 1368.6399999999999, "text": " a dot graph file on anything, on, on a state change or whatever. You, you've got control over when it", "tokens": [50976, 257, 5893, 4295, 3991, 322, 1340, 11, 322, 11, 322, 257, 1785, 1319, 420, 2035, 13, 509, 11, 291, 600, 658, 1969, 670, 562, 309, 51356], "temperature": 0.0, "avg_logprob": -0.09984850415996477, "compression_ratio": 1.661904761904762, "no_speech_prob": 0.0010595140047371387}, {"id": 170, "seek": 134880, "start": 1368.6399999999999, "end": 1375.6, "text": " generates it. And so for, for me, we, when we've got debugging enabled, we enable a dot graph", "tokens": [51356, 23815, 309, 13, 400, 370, 337, 11, 337, 385, 11, 321, 11, 562, 321, 600, 658, 45592, 15172, 11, 321, 9528, 257, 5893, 4295, 51704], "temperature": 0.0, "avg_logprob": -0.09984850415996477, "compression_ratio": 1.661904761904762, "no_speech_prob": 0.0010595140047371387}, {"id": 171, "seek": 137560, "start": 1376.1599999999999, "end": 1386.08, "text": " generation whenever state changes. And so ultimately, this looks really small and dumb. It's a PDF. So", "tokens": [50392, 5125, 5699, 1785, 2962, 13, 400, 370, 6284, 11, 341, 1542, 534, 1359, 293, 10316, 13, 467, 311, 257, 17752, 13, 407, 50888], "temperature": 0.0, "avg_logprob": -0.10289161633222531, "compression_ratio": 1.4264705882352942, "no_speech_prob": 0.0014748277608305216}, {"id": 172, "seek": 137560, "start": 1386.08, "end": 1394.24, "text": " you can go in and, and look at it in high quality detail. Um, because it's not a PNG. So you've got", "tokens": [50888, 291, 393, 352, 294, 293, 11, 293, 574, 412, 309, 294, 1090, 3125, 2607, 13, 3301, 11, 570, 309, 311, 406, 257, 430, 30237, 13, 407, 291, 600, 658, 51296], "temperature": 0.0, "avg_logprob": -0.10289161633222531, "compression_ratio": 1.4264705882352942, "no_speech_prob": 0.0014748277608305216}, {"id": 173, "seek": 137560, "start": 1394.24, "end": 1398.6399999999999, "text": " lots of options. You can, the dot graph can be converted into lots of different formats.", "tokens": [51296, 3195, 295, 3956, 13, 509, 393, 11, 264, 5893, 4295, 393, 312, 16424, 666, 3195, 295, 819, 25879, 13, 51516], "temperature": 0.0, "avg_logprob": -0.10289161633222531, "compression_ratio": 1.4264705882352942, "no_speech_prob": 0.0014748277608305216}, {"id": 174, "seek": 139864, "start": 1399.3600000000001, "end": 1406.5600000000002, "text": " But the really cool thing about dot graphs is it tells you what's connected to what. And so it's", "tokens": [50400, 583, 264, 534, 1627, 551, 466, 5893, 24877, 307, 309, 5112, 291, 437, 311, 4582, 281, 437, 13, 400, 370, 309, 311, 50760], "temperature": 0.0, "avg_logprob": -0.12405681610107422, "compression_ratio": 1.64, "no_speech_prob": 0.007347886450588703}, {"id": 175, "seek": 139864, "start": 1406.5600000000002, "end": 1414.24, "text": " really great for debugging. And so for us, we've got our app source, um, our app source and our,", "tokens": [50760, 534, 869, 337, 45592, 13, 400, 370, 337, 505, 11, 321, 600, 658, 527, 724, 4009, 11, 1105, 11, 527, 724, 4009, 293, 527, 11, 51144], "temperature": 0.0, "avg_logprob": -0.12405681610107422, "compression_ratio": 1.64, "no_speech_prob": 0.007347886450588703}, {"id": 176, "seek": 139864, "start": 1415.44, "end": 1421.8400000000001, "text": " our two app sources. So one is, um, one is RTP, which is this one. And then this one is RTCP.", "tokens": [51204, 527, 732, 724, 7139, 13, 407, 472, 307, 11, 1105, 11, 472, 307, 497, 16804, 11, 597, 307, 341, 472, 13, 400, 550, 341, 472, 307, 497, 18238, 47, 13, 51524], "temperature": 0.0, "avg_logprob": -0.12405681610107422, "compression_ratio": 1.64, "no_speech_prob": 0.007347886450588703}, {"id": 177, "seek": 142184, "start": 1422.48, "end": 1428.1599999999999, "text": " And you can see, I'm coming off the camera. I'm sorry. Um, so you can see that this one's set", "tokens": [50396, 400, 291, 393, 536, 11, 286, 478, 1348, 766, 264, 2799, 13, 286, 478, 2597, 13, 3301, 11, 370, 291, 393, 536, 300, 341, 472, 311, 992, 50680], "temperature": 0.0, "avg_logprob": -0.09053573608398438, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.059732720255851746}, {"id": 178, "seek": 142184, "start": 1428.72, "end": 1435.1999999999998, "text": " with, um, with capabilities to say that this is RTCP. And this one is set with capabilities to say", "tokens": [50708, 365, 11, 1105, 11, 365, 10862, 281, 584, 300, 341, 307, 497, 18238, 47, 13, 400, 341, 472, 307, 992, 365, 10862, 281, 584, 51032], "temperature": 0.0, "avg_logprob": -0.09053573608398438, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.059732720255851746}, {"id": 179, "seek": 142184, "start": 1435.1999999999998, "end": 1446.8, "text": " this is RTP. And so you can see those are linked to a pad within a GST bin, a GST RTP bin. And so", "tokens": [51032, 341, 307, 497, 16804, 13, 400, 370, 291, 393, 536, 729, 366, 9408, 281, 257, 6887, 1951, 257, 460, 6840, 5171, 11, 257, 460, 6840, 497, 16804, 5171, 13, 400, 370, 51612], "temperature": 0.0, "avg_logprob": -0.09053573608398438, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.059732720255851746}, {"id": 180, "seek": 144680, "start": 1446.8, "end": 1452.8, "text": " those pads are then connected to an RTP session. The RTP session is then, um, connected to a", "tokens": [50364, 729, 19179, 366, 550, 4582, 281, 364, 497, 16804, 5481, 13, 440, 497, 16804, 5481, 307, 550, 11, 1105, 11, 4582, 281, 257, 50664], "temperature": 0.0, "avg_logprob": -0.12756887782703746, "compression_ratio": 1.8480392156862746, "no_speech_prob": 0.01505244616419077}, {"id": 181, "seek": 144680, "start": 1452.8, "end": 1459.52, "text": " demuxer. The demuxer is then connected to a jitter buffer. And the jitter buffer is then able to go.", "tokens": [50664, 1371, 2449, 260, 13, 440, 1371, 2449, 260, 307, 550, 4582, 281, 257, 361, 3904, 21762, 13, 400, 264, 361, 3904, 21762, 307, 550, 1075, 281, 352, 13, 51000], "temperature": 0.0, "avg_logprob": -0.12756887782703746, "compression_ratio": 1.8480392156862746, "no_speech_prob": 0.01505244616419077}, {"id": 182, "seek": 144680, "start": 1459.52, "end": 1465.76, "text": " Oh, well, in this, in this RTP stream that I'm receiving, that's both audio and video,", "tokens": [51000, 876, 11, 731, 11, 294, 341, 11, 294, 341, 497, 16804, 4309, 300, 286, 478, 10040, 11, 300, 311, 1293, 6278, 293, 960, 11, 51312], "temperature": 0.0, "avg_logprob": -0.12756887782703746, "compression_ratio": 1.8480392156862746, "no_speech_prob": 0.01505244616419077}, {"id": 183, "seek": 144680, "start": 1466.48, "end": 1471.9199999999998, "text": " where it's demuxed it and then it automatically goes, ah, here's the video and here's the audio.", "tokens": [51348, 689, 309, 311, 1371, 2449, 292, 309, 293, 550, 309, 6772, 1709, 11, 3716, 11, 510, 311, 264, 960, 293, 510, 311, 264, 6278, 13, 51620], "temperature": 0.0, "avg_logprob": -0.12756887782703746, "compression_ratio": 1.8480392156862746, "no_speech_prob": 0.01505244616419077}, {"id": 184, "seek": 147192, "start": 1472.5600000000002, "end": 1474.0800000000002, "text": " Right. And then it chucks it back out,", "tokens": [50396, 1779, 13, 400, 550, 309, 417, 15493, 309, 646, 484, 11, 50472], "temperature": 0.0, "avg_logprob": -0.12802237272262573, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.004274564795196056}, {"id": 185, "seek": 147192, "start": 1476.88, "end": 1482.8000000000002, "text": " chucks it back out, creates some pads for me, which I then connect over to, well,", "tokens": [50612, 417, 15493, 309, 646, 484, 11, 7829, 512, 19179, 337, 385, 11, 597, 286, 550, 1745, 670, 281, 11, 731, 11, 50908], "temperature": 0.0, "avg_logprob": -0.12802237272262573, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.004274564795196056}, {"id": 186, "seek": 147192, "start": 1482.8000000000002, "end": 1487.76, "text": " there's an app sync up there and that's my RTCP app sync. But then you could see here", "tokens": [50908, 456, 311, 364, 724, 20271, 493, 456, 293, 300, 311, 452, 497, 18238, 47, 724, 20271, 13, 583, 550, 291, 727, 536, 510, 51156], "temperature": 0.0, "avg_logprob": -0.12802237272262573, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.004274564795196056}, {"id": 187, "seek": 147192, "start": 1488.8000000000002, "end": 1493.68, "text": " that it's then connecting out Opus and VPA into my pipeline.", "tokens": [51208, 300, 309, 311, 550, 11015, 484, 12011, 301, 293, 691, 10297, 666, 452, 15517, 13, 51452], "temperature": 0.0, "avg_logprob": -0.12802237272262573, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.004274564795196056}, {"id": 188, "seek": 149368, "start": 1494.0, "end": 1502.72, "text": " And then this is like the rest of the pipeline, which we don't care about, but like, I get told", "tokens": [50380, 400, 550, 341, 307, 411, 264, 1472, 295, 264, 15517, 11, 597, 321, 500, 380, 1127, 466, 11, 457, 411, 11, 286, 483, 1907, 50816], "temperature": 0.0, "avg_logprob": -0.12281516393025717, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.001892589032649994}, {"id": 189, "seek": 149368, "start": 1502.72, "end": 1508.3200000000002, "text": " it's Opus and I get told it's VPA. And so I'm able to decode it and do stuff with it,", "tokens": [50816, 309, 311, 12011, 301, 293, 286, 483, 1907, 309, 311, 691, 10297, 13, 400, 370, 286, 478, 1075, 281, 979, 1429, 309, 293, 360, 1507, 365, 309, 11, 51096], "temperature": 0.0, "avg_logprob": -0.12281516393025717, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.001892589032649994}, {"id": 190, "seek": 149368, "start": 1509.2, "end": 1517.76, "text": " whether or not that's outputting to NDI or whatever. At the end of the, um, at the end of it is, um,", "tokens": [51140, 1968, 420, 406, 300, 311, 5598, 783, 281, 426, 3085, 420, 2035, 13, 1711, 264, 917, 295, 264, 11, 1105, 11, 412, 264, 917, 295, 309, 307, 11, 1105, 11, 51568], "temperature": 0.0, "avg_logprob": -0.12281516393025717, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.001892589032649994}, {"id": 191, "seek": 151776, "start": 1517.84, "end": 1522.56, "text": " is an app source, uh, sorry, an app, an app sync for sending out via NDI.", "tokens": [50368, 307, 364, 724, 4009, 11, 2232, 11, 2597, 11, 364, 724, 11, 364, 724, 20271, 337, 7750, 484, 5766, 426, 3085, 13, 50604], "temperature": 0.0, "avg_logprob": -0.14154517416860543, "compression_ratio": 1.5198237885462555, "no_speech_prob": 0.0026202150620520115}, {"id": 192, "seek": 151776, "start": 1524.4, "end": 1530.16, "text": " So we, we got into go purely because of Pion and Pion gives us loads of control.", "tokens": [50696, 407, 321, 11, 321, 658, 666, 352, 17491, 570, 295, 430, 313, 293, 430, 313, 2709, 505, 12668, 295, 1969, 13, 50984], "temperature": 0.0, "avg_logprob": -0.14154517416860543, "compression_ratio": 1.5198237885462555, "no_speech_prob": 0.0026202150620520115}, {"id": 193, "seek": 151776, "start": 1531.2, "end": 1538.64, "text": " It's basically WebRTC in pure Golang. If you ignore the fact that WebRTC does lots of like", "tokens": [51036, 467, 311, 1936, 9573, 49, 18238, 294, 6075, 36319, 656, 13, 759, 291, 11200, 264, 1186, 300, 9573, 49, 18238, 775, 3195, 295, 411, 51408], "temperature": 0.0, "avg_logprob": -0.14154517416860543, "compression_ratio": 1.5198237885462555, "no_speech_prob": 0.0026202150620520115}, {"id": 194, "seek": 151776, "start": 1538.64, "end": 1545.04, "text": " actual media stuff, but when you look at, say, the, just the, the network portion of it of sending,", "tokens": [51408, 3539, 3021, 1507, 11, 457, 562, 291, 574, 412, 11, 584, 11, 264, 11, 445, 264, 11, 264, 3209, 8044, 295, 309, 295, 7750, 11, 51728], "temperature": 0.0, "avg_logprob": -0.14154517416860543, "compression_ratio": 1.5198237885462555, "no_speech_prob": 0.0026202150620520115}, {"id": 195, "seek": 154504, "start": 1545.04, "end": 1548.8, "text": " sending data from here and sending it there, then it's pure Golang.", "tokens": [50364, 7750, 1412, 490, 510, 293, 7750, 309, 456, 11, 550, 309, 311, 6075, 36319, 656, 13, 50552], "temperature": 0.0, "avg_logprob": -0.08894880550114188, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.004899390507489443}, {"id": 196, "seek": 154504, "start": 1550.3999999999999, "end": 1555.2, "text": " So yeah, you can do any of this with any of the G streamer bindings or you can just, you know,", "tokens": [50632, 407, 1338, 11, 291, 393, 360, 604, 295, 341, 365, 604, 295, 264, 460, 4309, 260, 14786, 1109, 420, 291, 393, 445, 11, 291, 458, 11, 50872], "temperature": 0.0, "avg_logprob": -0.08894880550114188, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.004899390507489443}, {"id": 197, "seek": 154504, "start": 1555.2, "end": 1559.52, "text": " do it with actual G streamer C. I mean, who actually want to do that? I don't know.", "tokens": [50872, 360, 309, 365, 3539, 460, 4309, 260, 383, 13, 286, 914, 11, 567, 767, 528, 281, 360, 300, 30, 286, 500, 380, 458, 13, 51088], "temperature": 0.0, "avg_logprob": -0.08894880550114188, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.004899390507489443}, {"id": 198, "seek": 154504, "start": 1560.56, "end": 1565.92, "text": " But you can go and use whatever bindings you want. And so there's really nice bindings for Python,", "tokens": [51140, 583, 291, 393, 352, 293, 764, 2035, 14786, 1109, 291, 528, 13, 400, 370, 456, 311, 534, 1481, 14786, 1109, 337, 15329, 11, 51408], "temperature": 0.0, "avg_logprob": -0.08894880550114188, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.004899390507489443}, {"id": 199, "seek": 154504, "start": 1565.92, "end": 1570.6399999999999, "text": " Rust, um, and I haven't used any of the others. Um, I've definitely used the Python one and the,", "tokens": [51408, 34952, 11, 1105, 11, 293, 286, 2378, 380, 1143, 604, 295, 264, 2357, 13, 3301, 11, 286, 600, 2138, 1143, 264, 15329, 472, 293, 264, 11, 51644], "temperature": 0.0, "avg_logprob": -0.08894880550114188, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.004899390507489443}, {"id": 200, "seek": 157064, "start": 1570.64, "end": 1576.24, "text": " and the Rust one myself. Um, and the Golang one, I went on there this morning to take the screenshot", "tokens": [50364, 293, 264, 34952, 472, 2059, 13, 3301, 11, 293, 264, 36319, 656, 472, 11, 286, 1437, 322, 456, 341, 2446, 281, 747, 264, 27712, 50644], "temperature": 0.0, "avg_logprob": -0.07328907648722331, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.002816189080476761}, {"id": 201, "seek": 157064, "start": 1576.24, "end": 1580.64, "text": " and I was like, Oh, where's the Golang one? Um, so here's the pull request to add it to the list.", "tokens": [50644, 293, 286, 390, 411, 11, 876, 11, 689, 311, 264, 36319, 656, 472, 30, 3301, 11, 370, 510, 311, 264, 2235, 5308, 281, 909, 309, 281, 264, 1329, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07328907648722331, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.002816189080476761}, {"id": 202, "seek": 157064, "start": 1581.92, "end": 1587.0400000000002, "text": " So if you've got a problem and G streamer doesn't quite solve that problem,", "tokens": [50928, 407, 498, 291, 600, 658, 257, 1154, 293, 460, 4309, 260, 1177, 380, 1596, 5039, 300, 1154, 11, 51184], "temperature": 0.0, "avg_logprob": -0.07328907648722331, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.002816189080476761}, {"id": 203, "seek": 157064, "start": 1587.76, "end": 1592.8000000000002, "text": " that's what this talks about. This talk is about the fact that you can make G streamer do what", "tokens": [51220, 300, 311, 437, 341, 6686, 466, 13, 639, 751, 307, 466, 264, 1186, 300, 291, 393, 652, 460, 4309, 260, 360, 437, 51472], "temperature": 0.0, "avg_logprob": -0.07328907648722331, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.002816189080476761}, {"id": 204, "seek": 157064, "start": 1592.8000000000002, "end": 1598.96, "text": " you want it to do using app source and app sync. You can build it yourself with app source and", "tokens": [51472, 291, 528, 309, 281, 360, 1228, 724, 4009, 293, 724, 20271, 13, 509, 393, 1322, 309, 1803, 365, 724, 4009, 293, 51780], "temperature": 0.0, "avg_logprob": -0.07328907648722331, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.002816189080476761}, {"id": 205, "seek": 159896, "start": 1598.96, "end": 1605.68, "text": " app sync. So why G streamer? Why not FFM peg? Whatever. G streamer does everything that we", "tokens": [50364, 724, 20271, 13, 407, 983, 460, 4309, 260, 30, 1545, 406, 479, 37, 44, 17199, 30, 8541, 13, 460, 4309, 260, 775, 1203, 300, 321, 50700], "temperature": 0.0, "avg_logprob": -0.1325866154261998, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0007219305261969566}, {"id": 206, "seek": 159896, "start": 1605.68, "end": 1613.8400000000001, "text": " need it to do. It has a fantastic community, super friendly community. And ultimately it's just", "tokens": [50700, 643, 309, 281, 360, 13, 467, 575, 257, 5456, 1768, 11, 1687, 9208, 1768, 13, 400, 6284, 309, 311, 445, 51108], "temperature": 0.0, "avg_logprob": -0.1325866154261998, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0007219305261969566}, {"id": 207, "seek": 159896, "start": 1613.8400000000001, "end": 1619.8400000000001, "text": " super flexible and does exactly what we need it to do. Um, which is not something that we felt as", "tokens": [51108, 1687, 11358, 293, 775, 2293, 437, 321, 643, 309, 281, 360, 13, 3301, 11, 597, 307, 406, 746, 300, 321, 2762, 382, 51408], "temperature": 0.0, "avg_logprob": -0.1325866154261998, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0007219305261969566}, {"id": 208, "seek": 159896, "start": 1619.8400000000001, "end": 1626.64, "text": " a team. FFM peg would give us, for example, G streamer has a lot of scaffolding, let's say,", "tokens": [51408, 257, 1469, 13, 479, 37, 44, 17199, 576, 976, 505, 11, 337, 1365, 11, 460, 4309, 260, 575, 257, 688, 295, 44094, 278, 11, 718, 311, 584, 11, 51748], "temperature": 0.0, "avg_logprob": -0.1325866154261998, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0007219305261969566}, {"id": 209, "seek": 162664, "start": 1626.8000000000002, "end": 1634.24, "text": " um, and, and gives us an awful lot, um, for free. Whereas G, uh, FFM pegs a little bit more, more", "tokens": [50372, 1105, 11, 293, 11, 293, 2709, 505, 364, 11232, 688, 11, 1105, 11, 337, 1737, 13, 13813, 460, 11, 2232, 11, 479, 37, 44, 17199, 82, 257, 707, 857, 544, 11, 544, 50744], "temperature": 0.0, "avg_logprob": -0.17170809791201636, "compression_ratio": 1.5201793721973094, "no_speech_prob": 0.0010065127862617373}, {"id": 210, "seek": 162664, "start": 1634.24, "end": 1641.92, "text": " work, right? So my last message is G streamer for the win. Um, so yeah, don't wait for others.", "tokens": [50744, 589, 11, 558, 30, 407, 452, 1036, 3636, 307, 460, 4309, 260, 337, 264, 1942, 13, 3301, 11, 370, 1338, 11, 500, 380, 1699, 337, 2357, 13, 51128], "temperature": 0.0, "avg_logprob": -0.17170809791201636, "compression_ratio": 1.5201793721973094, "no_speech_prob": 0.0010065127862617373}, {"id": 211, "seek": 162664, "start": 1641.92, "end": 1646.5600000000002, "text": " Don't wait for others to build your plugin for you. You can go and build with G streamer,", "tokens": [51128, 1468, 380, 1699, 337, 2357, 281, 1322, 428, 23407, 337, 291, 13, 509, 393, 352, 293, 1322, 365, 460, 4309, 260, 11, 51360], "temperature": 0.0, "avg_logprob": -0.17170809791201636, "compression_ratio": 1.5201793721973094, "no_speech_prob": 0.0010065127862617373}, {"id": 212, "seek": 162664, "start": 1646.5600000000002, "end": 1650.64, "text": " app source and sync. And that's me. Thank you very much.", "tokens": [51360, 724, 4009, 293, 20271, 13, 400, 300, 311, 385, 13, 1044, 291, 588, 709, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17170809791201636, "compression_ratio": 1.5201793721973094, "no_speech_prob": 0.0010065127862617373}], "language": "en"}