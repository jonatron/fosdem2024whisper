{"text": " in the morning on Sunday. It's nice to see all here, looking very bright and early. So we shall get straight into it. Let me welcome the first presenter of the day, Maria and Christian from CrateDB, who are going to be talking about privacy and generative AI. Thank you. Good morning from our side. Pleasure to open the Dev Room today and thanks for being here that early on a Sunday morning. We're going to talk about a very interesting topic, generative AI, how to use your own data and how we can build such applications also based on open source software. I think everyone is used to open AI and chat GPT, but you never know what happens with your data in these cases. So very, very brief overview. This is gen AI. I think everyone in the room played around with it already. Just a very quick summary of the basics here. You have your source data of any kind of sort of data. It can be text, it can be code, images, audio, videos. Everything is transformed. We are encoders, but billions of parameters that we use, a lot of text, a lot of input to train the so-called foundational models. We as users formulate some prompts against it. We ask the models some questions. It does its job and it generates the output and a language model does nothing else than predicting the most likely next token that it should generate. That's all the magic behind. We see a very, very big potential. When I first tried chat GPT more than a year ago, it was amazing. It started to write code for me. It starts to generate articles. I even went to some tools out there, took 30 seconds of my video and all of a sudden I can be a virtual speaker. Very, very impressive, super fast, but there's also a bot assigned to it. Obviously, some quality issues. All of you heard of hallucinations. Last week we had the example of what color is the water. Is it blue or is it really transparent? Depending on your training data, if you use children's books, the water is obviously blue. If you use the real-world training data, water should be transparent. Same as snowflakes or not white. They are transparent technically. Also, a lot of ethical questions, a lot of governance questions. Official government people talking to deep fakes, not realizing it. Also, a big threat that we have in the future. We have to be aware of also some environmental impact. The key thing we want to talk about today is quality and reliability with the importance of current, of accurate, and also of private data that is not available publicly. Because all of these foundation and models have been trained on public data. What's in Github, what's on the internet, what is in the documentation. Yesterday I watched a presentation with a clear message to everyone writing docs. We are responsible for what these models tell us. If you write bad documentation, we get bad results from GEPT or other models. It has been trained on not so good training data. Here, for example, Maria figured out promo code, open AIS web. If you register there and put the code 20% off. But unfortunately it was not working. So asking GEPT, hey, how can I apply the promo code? I'm sorry I know about this promotion. That's something you don't want to happen if it's a company chat bot. You want to avoid this. So perfect example why we need this current and accurate data up to the minute, maybe even up to the second. We need this current data. And obviously non-public data, private data, it's internal documents, it's confidential documents, documentation that is not public. And also if you are working with, they use legal documents, they use the technical documentations, vectorize it, put it to a language model and then for the maintenance workers, they have an application ready. But this is information that also must not leak. And this brings us also into a little bit of a dilemma because there are multiple options to bring this private data into the foundation and models or to enhance this foundation and models. First thing, again, I think everyone in the room heard about it, is fine-tuning. Where you give some input data, you really change the parameters, the weights in the foundation and model so that the knowledge gets incorporated into your fine-tuned LLM. Very good. You put the domain knowledge in there, but there are also challenges, right? You don't solve the frequency issue of the data. It's still some static knowledge. So there's research out there that one single wrong training data record can kill the overall performance. One guy says the water is blue and all of a sudden the response of the chat, but it's all water is light blue or something like this. And it doesn't solve the problem of hallucinations. You might still get a lot of hallucinations and not talking about the resources that we need. So second option, retrieval augmented generation, which is kind of developed into kind of a standard when you want to work with your own data. So first step is you really need to make the existing data, whether it's videos, it's data from internal database documents available to create the embeddings, to calculate the vectors, how this knowledge is internally represented. And then as soon as your user asks a question in the knowledge assistant or the chatbot, there's a called retriever is then asked, hey, please give me the relevant context. And this can now be a similarity search in the vector database, or it can be a combination of various searches, a full text search, to your spatial search, a regular SQL query to get information out of your databases. This context is returned back to the retriever. It is put into a special prompt, as context, as additional information to the prompt, and together with the question, and this additional context, not a large language model can generate your answer. And you can put into the prompt, as we will see in the demo also, please use only this contextual data. If you don't know the answer, please say you don't know. Limits the hallucinations a lot, doesn't prevent them 100%. Good. I think I talked about disadvantages and challenges already. And one advantage I forgot to mention is access control. Now that you really get this context from either vector store a different database, maybe create, you can put fine-grained privileges there. The example application that I mentioned before, some of the maintenance workers are not allowed to use legal documents, for example. So they don't use the index, use the embeddings of the legal documents, but they are obviously allowed to use the technical documentation. And someone from the legal department, oh, what is the support contract with XYZ? Are we now in liability? Et cetera. Obviously, they need then different indexes, different search indexes. How to do this? How semantics represented? Key is the vectors. So, or embeddings. And the vector is nothing else than a series of decimal values or an array of decimal values with a lot of different embedding models out there already. And every model has its strengths and weaknesses. Some are more optimal if you use, for example, German text, if you use Chinese text or Indian text, right? A very different way how to come up with the semantics and to analyze how the attention mechanisms internally work, right? Because the sentences are built in a very, very different way. So you see different performance there or highly specialized models. You do an image recognition. Oh, it's a sleeping cat. And this can then be vectorized as well. And you can search for this context in your vector store. And now, if we think this one step further, how could an architecture look like for such a knowledge assistant or a chatbot? Prototype is always easy to build, but you need to think about a lot of a lot of additional topics. First of all, it starts with the data, right? The data that you want to train, that you want to vectorize, that you want to make available for your search. So we've shown here a landing zone from different sources, can be the original sources. You might copy it, depends on the architecture you want to build. And the important thing is the processing layer. How do you chunk your data? How do you create the vectors? And obviously, you need to store these chunks of information together with the vectors and provide proper data access control. Second part here, the LLM part, talked about it now multiple times. You need access to the embeddings, you need access to the large language models, and then also needs to be some logging. What do do you use a query? How much cost does it incur? Is the performance okay? A lot of logging that also occurs here. And intentionally, an LLM gateway put in front of it because it needs to be changeable. Chatbots with a lot of functionalities don't want to go into all the details, obviously monitoring and reporting. And the beauty of it, you can build all of that with open source tools nowadays. And also the embeddings and language models can be open sourced, a lot of alternatives out there. Now, why create a long chain? You need a robust data management. As we have seen, there's a lot of different data sources involved here, data stores, whether it's logging, whether it's semantics, your agents communicate in JSON. So you need to store all of this information, ideally in one store, not five, six different databases here that you need to operate, you need to learn the language, et cetera. And also long chain, other opportunities are also out there. Think of Haystack and others that you could use. But all of these frameworks give you a very good set of building blocks. You can just use them. It's available in Python, JavaScript, there are also Java ports out there, ports to other languages are now available. Everything you need is already in these libraries to come up with your overall architecture. And that's now the point to hand it over to Maria. She will guide you through a demo where we want to use it, try to simulate how you can use support tickets, internal data. Here we took some Twitter posts from Microsoft. We will vectorize them and we'll show how a support or a customer can then interact with this chat bot, ask certain questions. It won't demonstrate it's not such a big effort. You can get started right away. And all the demo, we put the link here on the slide. You find also the link to the demo in the app or on the website for the talk. Thank you. Do you hear me? Okay. Awesome. Thank you. So you have heard a lot of theoretical aspects of the drug and how it works. I have a little bit more than 10 minutes to show you a practical example. But believe me, we can have hours long workshop on this topic. So essentially, the idea today is to show you how to augment some of the existing LLAMs with the private data and how to use it for the context of some specific questions that this LLAM has not seen so far. So we actually use data that capture customer interactions on Twitter and these customer interactions involve different questions from the users about Microsoft, Amazon, all these different products today and how actually the support from these big companies actually answer to these user questions. So this is not something that you usually see on the Internet very easily. So if you have maybe some problem with some Microsoft product, yeah, very often you can actually find the solution out there. But some very specific questions that are asked directly to customer support is probably a very good reason why it sells to the customer support. So you didn't find the answer to this out of the box. And we will use CradyBee as a vector store to support this example. So I think Kristina already gave you a good overview of what the CradyBee is. What is the long chain? Long chain is an open source Python project that actually is used to facilitate the development of LLAM applications. It's a pretty cool project that integrates a lot of large language models, a lot of models for calculating embeddings and actually something that helps you integrate some data source with some language model without thinking out of the box how the full engineering pipeline should look like. Actually you can just do this in a couple of lines of code. May I add one point here that I forgot to mention. Although you use long chains, very good starting point. What we have also seen for very advanced purposes, you want to directly interact with your data, with your source data, with your vector store and all of that is available in standard SQL, no matter which data model you're using. And CradyBee is an open source store, one of the easiest ways to run CradyBee is actually to use a Docker image. So a vector support in CradyBee has been available since 5.5 version, but if you actually always pull the latest image, you should not actually think about this. So once you run this Docker run command, we actually run the instance of CradyBee cluster and then we can access the admin UI in the local host. So currently I think because of the resolution of this screen, yes, not everything is available, but actually in this admin UI you have a couple of tabs that you can use actually to monitor your cluster to run some query in the console and also to have overview of the tables and the views that are available in your database. So let's go back to the example because the time is flying very fast. So what we need is the first step, we need a couple of import statements to make sure that the long chain and all libraries that we use in this example are available. What is also important is that you import CradyBee vector search interface that is available in one of the long chain versions, let's say, which is used to interact with the CradyBee. And as a next step, because we need to interact with the CradyBee instance, we need to specify how we interact. So this is done by specifying connection string. We are using open source version running on local host, but you also have option, for example, if you want to deploy CradyBee cloud cluster and at this point we also give option for all users to deploy one cluster that is free forever so you can just run it and use it for testing purposes. Finally, we need to specify the collection name that we are going to use in this notebook session. So if we run this piece of code, the connection string is now available and then we can start interacting with the CradyBee. So for purpose of this notebook, I rely on open AI models. Of course, there are long chain supports, so many different models, you can actually integrate many of them, but if you choose to use open AI, make sure that you have open AI key as a part of your environment variable. So now let's take a look at how the dataset looks like. This dataset is also available on our CradyBee dataset repository, which is also open source and it contains the customer interaction about Microsoft products. So essentially we would like to now kind of narrow the scope of this notebook for the for the illustration reasons and time reasons. So essentially this dataset has some information like who is the author of this question, whether it's unbound, outbound question, when it was created, what was the context of the question or the answer and actually whether this text is response to something or is it response tweet or is it created in response to something else. So essentially all this information and now the idea is to feed them to the large language model and to ask questions that could be for example seen in this dataset. So as a first step, if you remember this big rug image is to create embeddings. Embeddings is actually the representation of your data that is suitable for machine learning and AI purposes. So first as a first step we need to load the data from this dataset and for this we use CSV loader interface that is available in Longchain and now in this like few pieces of code we already we already creating embeddings for all the data set for all the entries in our dataset. So if I go back to the admin UI I can see two tables. So in the first table actually gives me a collection of entries. So as we as we define the the first collection we created is called customer data but essentially what is interesting now is to see like embeddings created for all the entries in this in this collection. So for example this is the instance of the of the document that we are actually using for the training purpose or for the context purposes and you can actually see how the embeddings look like. So if you use open AI embeddings usually the length of your of your vector is going to be 1040 something yes it would be size of 1040 something but you can also for example choose some other embedding algorithm for example hugging face as you can see suggested here which is which is open source and it can easily be used out of the box in just two lines of code. Now once we have these embeddings let's define our question and our question today is like okay I have some I have some order on my Microsoft Store but I want to update the shipping address and how I do this. I also here put alternative questions so like when you play with this notebook you can also put your own questions and see actually whether this data set has enough information to answer this question. So once the question is answered what we want to do is actually we want to find the context that is relevant to this question and this context is done by doing similarity search of the vector representation of our question compared to the vectors actually that we stored in the creative instance and this is actually done in just one line of code. As Christian suggested vector search is one way to find the relevant context of course kdb supports other types of searches like full text search or geospatial search or just key search keyword search so like you can use different type of searches combined together to find what is what is the relevant context for your question. Once we do this we are now ready to actually ask our LLM to answer our question and how we do this. First we need to create a prompt that explains LLM what his purpose is. So his purpose is today to be expert about Microsoft products and services and should use the context that you are going to actually give to the LLM to answer relevant questions but if the answer is not fine in the context it should reply with I don't know and this is very simple way to create a prompt that actually gives instructions to LLM how it should answer specific questions and finally we just need to create small chatbot by using some of the available models that are integrated with the long chain and also passing this context together with the user question. Once this is completed we can access the answer and in this case it says to update the shipping address you will need to cancel your current order and place a new one. Maybe that's something that is still up to date that is relevant maybe it's not relevant anymore but it's actually something we learned only from the dataset we provided so this is a way how to actually how you actually use your private data to teach LLM actually what should what should be the context for any incoming questions. So I hope you like this demo you can play with this notebook it's on our creative B examples repository and you also can see there are other similar notebooks for different different different types of examples for different prompt engineering examples or like how to create another another form of chatbots how to use another embedding algorithms so please let us know what you think give us a feedback open a new issue on this repository and we are looking forward actually to work with you on these topics. So I think that is all from us thank you for being part of this session maybe we have time for one question okay awesome do we have questions anyone thank you for the talk I have a question about the embeddings model because if you encode prompt with language model and use external embeddings model they cannot be in different spaces and if you do similarity search have you tested it and do you see the effect of different embeddings I mean it's a very important question now if you the way you create these embeddings is super important and you're usually limited to one embedding algorithm because you need to they need to have the same length and obviously they need to capture the same semantics simplifying a bit and this is also what I meant with the customers that we work with they were able to create different indexes right and then the retriever gets more and more complex as you've seen on this architecture slide this is a simplified example you maybe you need to query different different indexes created by different embedding algorithms you know so that you can search your images you can search your textual data right obviously you might use different things there and then re-rank the results come up with the really relevant context maybe from different indexes and maybe you also want to combine it with a full text search or limit it to customer support tickets from Europe trying to come up with a good example there are or to customers support tickets from the US with some geospatial inhibition but this is then the re-ranking of the results that really identifies the particular context that is really relevant for the question okay thanks a lot any more questions no so thank you very much for the very nice talk thank you you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.48, "text": " in the morning on Sunday. It's nice to see all here, looking very bright and early. So", "tokens": [50364, 294, 264, 2446, 322, 7776, 13, 467, 311, 1481, 281, 536, 439, 510, 11, 1237, 588, 4730, 293, 2440, 13, 407, 50888], "temperature": 0.0, "avg_logprob": -0.2818012237548828, "compression_ratio": 1.4153005464480874, "no_speech_prob": 0.4530014395713806}, {"id": 1, "seek": 0, "start": 10.48, "end": 15.92, "text": " we shall get straight into it. Let me welcome the first presenter of the day, Maria and", "tokens": [50888, 321, 4393, 483, 2997, 666, 309, 13, 961, 385, 2928, 264, 700, 35594, 295, 264, 786, 11, 12734, 293, 51160], "temperature": 0.0, "avg_logprob": -0.2818012237548828, "compression_ratio": 1.4153005464480874, "no_speech_prob": 0.4530014395713806}, {"id": 2, "seek": 0, "start": 15.92, "end": 26.84, "text": " Christian from CrateDB, who are going to be talking about privacy and generative AI.", "tokens": [51160, 5778, 490, 383, 4404, 27735, 11, 567, 366, 516, 281, 312, 1417, 466, 11427, 293, 1337, 1166, 7318, 13, 51706], "temperature": 0.0, "avg_logprob": -0.2818012237548828, "compression_ratio": 1.4153005464480874, "no_speech_prob": 0.4530014395713806}, {"id": 3, "seek": 2684, "start": 26.84, "end": 34.4, "text": " Thank you. Good morning from our side. Pleasure to open the Dev Room today and thanks for", "tokens": [50364, 1044, 291, 13, 2205, 2446, 490, 527, 1252, 13, 25658, 2508, 281, 1269, 264, 9096, 19190, 965, 293, 3231, 337, 50742], "temperature": 0.0, "avg_logprob": -0.20992392793707892, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.08230578899383545}, {"id": 4, "seek": 2684, "start": 34.4, "end": 38.88, "text": " being here that early on a Sunday morning. We're going to talk about a very interesting", "tokens": [50742, 885, 510, 300, 2440, 322, 257, 7776, 2446, 13, 492, 434, 516, 281, 751, 466, 257, 588, 1880, 50966], "temperature": 0.0, "avg_logprob": -0.20992392793707892, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.08230578899383545}, {"id": 5, "seek": 2684, "start": 38.88, "end": 44.480000000000004, "text": " topic, generative AI, how to use your own data and how we can build such applications", "tokens": [50966, 4829, 11, 1337, 1166, 7318, 11, 577, 281, 764, 428, 1065, 1412, 293, 577, 321, 393, 1322, 1270, 5821, 51246], "temperature": 0.0, "avg_logprob": -0.20992392793707892, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.08230578899383545}, {"id": 6, "seek": 2684, "start": 44.480000000000004, "end": 49.480000000000004, "text": " also based on open source software. I think everyone is used to open AI and chat GPT,", "tokens": [51246, 611, 2361, 322, 1269, 4009, 4722, 13, 286, 519, 1518, 307, 1143, 281, 1269, 7318, 293, 5081, 26039, 51, 11, 51496], "temperature": 0.0, "avg_logprob": -0.20992392793707892, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.08230578899383545}, {"id": 7, "seek": 2684, "start": 49.480000000000004, "end": 55.56, "text": " but you never know what happens with your data in these cases. So very, very brief overview.", "tokens": [51496, 457, 291, 1128, 458, 437, 2314, 365, 428, 1412, 294, 613, 3331, 13, 407, 588, 11, 588, 5353, 12492, 13, 51800], "temperature": 0.0, "avg_logprob": -0.20992392793707892, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.08230578899383545}, {"id": 8, "seek": 5556, "start": 55.56, "end": 61.080000000000005, "text": " This is gen AI. I think everyone in the room played around with it already. Just a very", "tokens": [50364, 639, 307, 1049, 7318, 13, 286, 519, 1518, 294, 264, 1808, 3737, 926, 365, 309, 1217, 13, 1449, 257, 588, 50640], "temperature": 0.0, "avg_logprob": -0.20715701454564145, "compression_ratio": 1.5391304347826087, "no_speech_prob": 0.012729685753583908}, {"id": 9, "seek": 5556, "start": 61.080000000000005, "end": 69.2, "text": " quick summary of the basics here. You have your source data of any kind of sort of data.", "tokens": [50640, 1702, 12691, 295, 264, 14688, 510, 13, 509, 362, 428, 4009, 1412, 295, 604, 733, 295, 1333, 295, 1412, 13, 51046], "temperature": 0.0, "avg_logprob": -0.20715701454564145, "compression_ratio": 1.5391304347826087, "no_speech_prob": 0.012729685753583908}, {"id": 10, "seek": 5556, "start": 69.2, "end": 76.36, "text": " It can be text, it can be code, images, audio, videos. Everything is transformed. We are", "tokens": [51046, 467, 393, 312, 2487, 11, 309, 393, 312, 3089, 11, 5267, 11, 6278, 11, 2145, 13, 5471, 307, 16894, 13, 492, 366, 51404], "temperature": 0.0, "avg_logprob": -0.20715701454564145, "compression_ratio": 1.5391304347826087, "no_speech_prob": 0.012729685753583908}, {"id": 11, "seek": 5556, "start": 76.36, "end": 82.56, "text": " encoders, but billions of parameters that we use, a lot of text, a lot of input to train", "tokens": [51404, 2058, 378, 433, 11, 457, 17375, 295, 9834, 300, 321, 764, 11, 257, 688, 295, 2487, 11, 257, 688, 295, 4846, 281, 3847, 51714], "temperature": 0.0, "avg_logprob": -0.20715701454564145, "compression_ratio": 1.5391304347826087, "no_speech_prob": 0.012729685753583908}, {"id": 12, "seek": 8256, "start": 82.56, "end": 88.4, "text": " the so-called foundational models. We as users formulate some prompts against it. We ask", "tokens": [50364, 264, 370, 12, 11880, 32195, 5245, 13, 492, 382, 5022, 47881, 512, 41095, 1970, 309, 13, 492, 1029, 50656], "temperature": 0.0, "avg_logprob": -0.17705176516277035, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.1642158478498459}, {"id": 13, "seek": 8256, "start": 88.4, "end": 94.32000000000001, "text": " the models some questions. It does its job and it generates the output and a language", "tokens": [50656, 264, 5245, 512, 1651, 13, 467, 775, 1080, 1691, 293, 309, 23815, 264, 5598, 293, 257, 2856, 50952], "temperature": 0.0, "avg_logprob": -0.17705176516277035, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.1642158478498459}, {"id": 14, "seek": 8256, "start": 94.32000000000001, "end": 102.04, "text": " model does nothing else than predicting the most likely next token that it should generate.", "tokens": [50952, 2316, 775, 1825, 1646, 813, 32884, 264, 881, 3700, 958, 14862, 300, 309, 820, 8460, 13, 51338], "temperature": 0.0, "avg_logprob": -0.17705176516277035, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.1642158478498459}, {"id": 15, "seek": 8256, "start": 102.04, "end": 110.88, "text": " That's all the magic behind. We see a very, very big potential. When I first tried chat", "tokens": [51338, 663, 311, 439, 264, 5585, 2261, 13, 492, 536, 257, 588, 11, 588, 955, 3995, 13, 1133, 286, 700, 3031, 5081, 51780], "temperature": 0.0, "avg_logprob": -0.17705176516277035, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.1642158478498459}, {"id": 16, "seek": 11088, "start": 110.88, "end": 116.96, "text": " GPT more than a year ago, it was amazing. It started to write code for me. It starts to generate", "tokens": [50364, 26039, 51, 544, 813, 257, 1064, 2057, 11, 309, 390, 2243, 13, 467, 1409, 281, 2464, 3089, 337, 385, 13, 467, 3719, 281, 8460, 50668], "temperature": 0.0, "avg_logprob": -0.18772666931152343, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.03464173525571823}, {"id": 17, "seek": 11088, "start": 116.96, "end": 123.6, "text": " articles. I even went to some tools out there, took 30 seconds of my video and all of a sudden I", "tokens": [50668, 11290, 13, 286, 754, 1437, 281, 512, 3873, 484, 456, 11, 1890, 2217, 3949, 295, 452, 960, 293, 439, 295, 257, 3990, 286, 51000], "temperature": 0.0, "avg_logprob": -0.18772666931152343, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.03464173525571823}, {"id": 18, "seek": 11088, "start": 123.6, "end": 131.4, "text": " can be a virtual speaker. Very, very impressive, super fast, but there's also a bot assigned to", "tokens": [51000, 393, 312, 257, 6374, 8145, 13, 4372, 11, 588, 8992, 11, 1687, 2370, 11, 457, 456, 311, 611, 257, 10592, 13279, 281, 51390], "temperature": 0.0, "avg_logprob": -0.18772666931152343, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.03464173525571823}, {"id": 19, "seek": 11088, "start": 131.4, "end": 137.48, "text": " it. Obviously, some quality issues. All of you heard of hallucinations. Last week we had the", "tokens": [51390, 309, 13, 7580, 11, 512, 3125, 2663, 13, 1057, 295, 291, 2198, 295, 35212, 10325, 13, 5264, 1243, 321, 632, 264, 51694], "temperature": 0.0, "avg_logprob": -0.18772666931152343, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.03464173525571823}, {"id": 20, "seek": 13748, "start": 137.51999999999998, "end": 142.67999999999998, "text": " example of what color is the water. Is it blue or is it really transparent? Depending on your", "tokens": [50366, 1365, 295, 437, 2017, 307, 264, 1281, 13, 1119, 309, 3344, 420, 307, 309, 534, 12737, 30, 22539, 322, 428, 50624], "temperature": 0.0, "avg_logprob": -0.19229883807046072, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.018865372985601425}, {"id": 21, "seek": 13748, "start": 142.67999999999998, "end": 148.39999999999998, "text": " training data, if you use children's books, the water is obviously blue. If you use the real-world", "tokens": [50624, 3097, 1412, 11, 498, 291, 764, 2227, 311, 3642, 11, 264, 1281, 307, 2745, 3344, 13, 759, 291, 764, 264, 957, 12, 13217, 50910], "temperature": 0.0, "avg_logprob": -0.19229883807046072, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.018865372985601425}, {"id": 22, "seek": 13748, "start": 148.39999999999998, "end": 152.2, "text": " training data, water should be transparent. Same as snowflakes or not white. They are", "tokens": [50910, 3097, 1412, 11, 1281, 820, 312, 12737, 13, 10635, 382, 44124, 3419, 420, 406, 2418, 13, 814, 366, 51100], "temperature": 0.0, "avg_logprob": -0.19229883807046072, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.018865372985601425}, {"id": 23, "seek": 13748, "start": 152.2, "end": 160.79999999999998, "text": " transparent technically. Also, a lot of ethical questions, a lot of governance questions. Official", "tokens": [51100, 12737, 12120, 13, 2743, 11, 257, 688, 295, 18890, 1651, 11, 257, 688, 295, 17449, 1651, 13, 38577, 51530], "temperature": 0.0, "avg_logprob": -0.19229883807046072, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.018865372985601425}, {"id": 24, "seek": 13748, "start": 160.79999999999998, "end": 166.23999999999998, "text": " government people talking to deep fakes, not realizing it. Also, a big threat that we have in", "tokens": [51530, 2463, 561, 1417, 281, 2452, 283, 3419, 11, 406, 16734, 309, 13, 2743, 11, 257, 955, 4734, 300, 321, 362, 294, 51802], "temperature": 0.0, "avg_logprob": -0.19229883807046072, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.018865372985601425}, {"id": 25, "seek": 16624, "start": 166.32000000000002, "end": 172.48000000000002, "text": " the future. We have to be aware of also some environmental impact. The key thing we want to", "tokens": [50368, 264, 2027, 13, 492, 362, 281, 312, 3650, 295, 611, 512, 8303, 2712, 13, 440, 2141, 551, 321, 528, 281, 50676], "temperature": 0.0, "avg_logprob": -0.37795400619506836, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.02057126723229885}, {"id": 26, "seek": 16624, "start": 172.48000000000002, "end": 181.20000000000002, "text": " talk about today is quality and reliability with the importance of current, of accurate, and also", "tokens": [50676, 751, 466, 965, 307, 3125, 293, 24550, 365, 264, 7379, 295, 2190, 11, 295, 8559, 11, 293, 611, 51112], "temperature": 0.0, "avg_logprob": -0.37795400619506836, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.02057126723229885}, {"id": 27, "seek": 16624, "start": 181.20000000000002, "end": 186.84, "text": " of private data that is not available publicly. Because all of these foundation and models have", "tokens": [51112, 295, 4551, 1412, 300, 307, 406, 2435, 14843, 13, 1436, 439, 295, 613, 7030, 293, 5245, 362, 51394], "temperature": 0.0, "avg_logprob": -0.37795400619506836, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.02057126723229885}, {"id": 28, "seek": 16624, "start": 186.84, "end": 191.28, "text": " been trained on public data. What's in Github, what's on the internet, what is in the", "tokens": [51394, 668, 8895, 322, 1908, 1412, 13, 708, 311, 294, 460, 355, 836, 11, 437, 311, 322, 264, 4705, 11, 437, 307, 294, 264, 51616], "temperature": 0.0, "avg_logprob": -0.37795400619506836, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.02057126723229885}, {"id": 29, "seek": 19128, "start": 191.36, "end": 197.72, "text": " documentation. Yesterday I watched a presentation with a clear message to everyone writing docs. We", "tokens": [50368, 14333, 13, 19765, 286, 6337, 257, 5860, 365, 257, 1850, 3636, 281, 1518, 3579, 45623, 13, 492, 50686], "temperature": 0.0, "avg_logprob": -0.3390537430258358, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.05446644499897957}, {"id": 30, "seek": 19128, "start": 197.72, "end": 202.96, "text": " are responsible for what these models tell us. If you write bad documentation, we get bad results", "tokens": [50686, 366, 6250, 337, 437, 613, 5245, 980, 505, 13, 759, 291, 2464, 1578, 14333, 11, 321, 483, 1578, 3542, 50948], "temperature": 0.0, "avg_logprob": -0.3390537430258358, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.05446644499897957}, {"id": 31, "seek": 19128, "start": 202.96, "end": 213.76, "text": " from GEPT or other models. It has been trained on not so good training data. Here, for example,", "tokens": [50948, 490, 460, 8929, 51, 420, 661, 5245, 13, 467, 575, 668, 8895, 322, 406, 370, 665, 3097, 1412, 13, 1692, 11, 337, 1365, 11, 51488], "temperature": 0.0, "avg_logprob": -0.3390537430258358, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.05446644499897957}, {"id": 32, "seek": 21376, "start": 213.76, "end": 225.12, "text": " Maria figured out promo code, open AIS web. If you register there and put the code 20% off.", "tokens": [50364, 12734, 8932, 484, 26750, 3089, 11, 1269, 316, 2343, 3670, 13, 759, 291, 7280, 456, 293, 829, 264, 3089, 945, 4, 766, 13, 50932], "temperature": 0.0, "avg_logprob": -0.39385184263571715, "compression_ratio": 1.395121951219512, "no_speech_prob": 0.24660444259643555}, {"id": 33, "seek": 21376, "start": 225.12, "end": 231.6, "text": " But unfortunately it was not working. So asking GEPT, hey, how can I apply the promo code? I'm", "tokens": [50932, 583, 7015, 309, 390, 406, 1364, 13, 407, 3365, 460, 8929, 51, 11, 4177, 11, 577, 393, 286, 3079, 264, 26750, 3089, 30, 286, 478, 51256], "temperature": 0.0, "avg_logprob": -0.39385184263571715, "compression_ratio": 1.395121951219512, "no_speech_prob": 0.24660444259643555}, {"id": 34, "seek": 21376, "start": 231.6, "end": 237.44, "text": " sorry I know about this promotion. That's something you don't want to happen if it's a company chat", "tokens": [51256, 2597, 286, 458, 466, 341, 15783, 13, 663, 311, 746, 291, 500, 380, 528, 281, 1051, 498, 309, 311, 257, 2237, 5081, 51548], "temperature": 0.0, "avg_logprob": -0.39385184263571715, "compression_ratio": 1.395121951219512, "no_speech_prob": 0.24660444259643555}, {"id": 35, "seek": 23744, "start": 237.52, "end": 245.04, "text": " bot. You want to avoid this. So perfect example why we need this current and accurate data up to the", "tokens": [50368, 10592, 13, 509, 528, 281, 5042, 341, 13, 407, 2176, 1365, 983, 321, 643, 341, 2190, 293, 8559, 1412, 493, 281, 264, 50744], "temperature": 0.0, "avg_logprob": -0.2841564995901925, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.015651937574148178}, {"id": 36, "seek": 23744, "start": 245.04, "end": 252.04, "text": " minute, maybe even up to the second. We need this current data. And obviously non-public data,", "tokens": [50744, 3456, 11, 1310, 754, 493, 281, 264, 1150, 13, 492, 643, 341, 2190, 1412, 13, 400, 2745, 2107, 12, 79, 3865, 1412, 11, 51094], "temperature": 0.0, "avg_logprob": -0.2841564995901925, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.015651937574148178}, {"id": 37, "seek": 23744, "start": 252.04, "end": 257.6, "text": " private data, it's internal documents, it's confidential documents, documentation that is not", "tokens": [51094, 4551, 1412, 11, 309, 311, 6920, 8512, 11, 309, 311, 27054, 8512, 11, 14333, 300, 307, 406, 51372], "temperature": 0.0, "avg_logprob": -0.2841564995901925, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.015651937574148178}, {"id": 38, "seek": 25760, "start": 257.68, "end": 267.68, "text": " public. And also if you are working with, they use legal documents, they use the technical", "tokens": [50368, 1908, 13, 400, 611, 498, 291, 366, 1364, 365, 11, 436, 764, 5089, 8512, 11, 436, 764, 264, 6191, 50868], "temperature": 0.0, "avg_logprob": -0.2688591480255127, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.027626866474747658}, {"id": 39, "seek": 25760, "start": 267.68, "end": 272.08000000000004, "text": " documentations, vectorize it, put it to a language model and then for the maintenance workers, they", "tokens": [50868, 4166, 763, 11, 8062, 1125, 309, 11, 829, 309, 281, 257, 2856, 2316, 293, 550, 337, 264, 11258, 5600, 11, 436, 51088], "temperature": 0.0, "avg_logprob": -0.2688591480255127, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.027626866474747658}, {"id": 40, "seek": 25760, "start": 272.08000000000004, "end": 277.92, "text": " have an application ready. But this is information that also must not leak. And this brings us also", "tokens": [51088, 362, 364, 3861, 1919, 13, 583, 341, 307, 1589, 300, 611, 1633, 406, 17143, 13, 400, 341, 5607, 505, 611, 51380], "temperature": 0.0, "avg_logprob": -0.2688591480255127, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.027626866474747658}, {"id": 41, "seek": 25760, "start": 277.92, "end": 284.32000000000005, "text": " into a little bit of a dilemma because there are multiple options to bring this private data into", "tokens": [51380, 666, 257, 707, 857, 295, 257, 34312, 570, 456, 366, 3866, 3956, 281, 1565, 341, 4551, 1412, 666, 51700], "temperature": 0.0, "avg_logprob": -0.2688591480255127, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.027626866474747658}, {"id": 42, "seek": 28432, "start": 284.4, "end": 289.92, "text": " the foundation and models or to enhance this foundation and models. First thing, again, I think", "tokens": [50368, 264, 7030, 293, 5245, 420, 281, 11985, 341, 7030, 293, 5245, 13, 2386, 551, 11, 797, 11, 286, 519, 50644], "temperature": 0.0, "avg_logprob": -0.15847985287930103, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.01344522088766098}, {"id": 43, "seek": 28432, "start": 289.92, "end": 296.88, "text": " everyone in the room heard about it, is fine-tuning. Where you give some input data, you really change", "tokens": [50644, 1518, 294, 264, 1808, 2198, 466, 309, 11, 307, 2489, 12, 83, 37726, 13, 2305, 291, 976, 512, 4846, 1412, 11, 291, 534, 1319, 50992], "temperature": 0.0, "avg_logprob": -0.15847985287930103, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.01344522088766098}, {"id": 44, "seek": 28432, "start": 296.88, "end": 302.56, "text": " the parameters, the weights in the foundation and model so that the knowledge gets incorporated", "tokens": [50992, 264, 9834, 11, 264, 17443, 294, 264, 7030, 293, 2316, 370, 300, 264, 3601, 2170, 21654, 51276], "temperature": 0.0, "avg_logprob": -0.15847985287930103, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.01344522088766098}, {"id": 45, "seek": 28432, "start": 302.56, "end": 310.48, "text": " into your fine-tuned LLM. Very good. You put the domain knowledge in there, but there are also", "tokens": [51276, 666, 428, 2489, 12, 83, 43703, 441, 43, 44, 13, 4372, 665, 13, 509, 829, 264, 9274, 3601, 294, 456, 11, 457, 456, 366, 611, 51672], "temperature": 0.0, "avg_logprob": -0.15847985287930103, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.01344522088766098}, {"id": 46, "seek": 31048, "start": 310.56, "end": 316.56, "text": " challenges, right? You don't solve the frequency issue of the data. It's still some static knowledge.", "tokens": [50368, 4759, 11, 558, 30, 509, 500, 380, 5039, 264, 7893, 2734, 295, 264, 1412, 13, 467, 311, 920, 512, 13437, 3601, 13, 50668], "temperature": 0.0, "avg_logprob": -0.18456886088953608, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.012669073417782784}, {"id": 47, "seek": 31048, "start": 316.56, "end": 323.52000000000004, "text": " So there's research out there that one single wrong training data record can kill the overall", "tokens": [50668, 407, 456, 311, 2132, 484, 456, 300, 472, 2167, 2085, 3097, 1412, 2136, 393, 1961, 264, 4787, 51016], "temperature": 0.0, "avg_logprob": -0.18456886088953608, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.012669073417782784}, {"id": 48, "seek": 31048, "start": 323.52000000000004, "end": 328.24, "text": " performance. One guy says the water is blue and all of a sudden the response of the chat, but it's", "tokens": [51016, 3389, 13, 1485, 2146, 1619, 264, 1281, 307, 3344, 293, 439, 295, 257, 3990, 264, 4134, 295, 264, 5081, 11, 457, 309, 311, 51252], "temperature": 0.0, "avg_logprob": -0.18456886088953608, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.012669073417782784}, {"id": 49, "seek": 31048, "start": 328.24, "end": 335.36, "text": " all water is light blue or something like this. And it doesn't solve the problem of hallucinations.", "tokens": [51252, 439, 1281, 307, 1442, 3344, 420, 746, 411, 341, 13, 400, 309, 1177, 380, 5039, 264, 1154, 295, 35212, 10325, 13, 51608], "temperature": 0.0, "avg_logprob": -0.18456886088953608, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.012669073417782784}, {"id": 50, "seek": 31048, "start": 335.36, "end": 340.16, "text": " You might still get a lot of hallucinations and not talking about the resources that we need.", "tokens": [51608, 509, 1062, 920, 483, 257, 688, 295, 35212, 10325, 293, 406, 1417, 466, 264, 3593, 300, 321, 643, 13, 51848], "temperature": 0.0, "avg_logprob": -0.18456886088953608, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.012669073417782784}, {"id": 51, "seek": 34048, "start": 341.28000000000003, "end": 350.32, "text": " So second option, retrieval augmented generation, which is kind of developed into kind of a", "tokens": [50404, 407, 1150, 3614, 11, 19817, 3337, 36155, 5125, 11, 597, 307, 733, 295, 4743, 666, 733, 295, 257, 50856], "temperature": 0.0, "avg_logprob": -0.19315564772661994, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0003710724995471537}, {"id": 52, "seek": 34048, "start": 350.32, "end": 357.12, "text": " standard when you want to work with your own data. So first step is you really need to make the", "tokens": [50856, 3832, 562, 291, 528, 281, 589, 365, 428, 1065, 1412, 13, 407, 700, 1823, 307, 291, 534, 643, 281, 652, 264, 51196], "temperature": 0.0, "avg_logprob": -0.19315564772661994, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0003710724995471537}, {"id": 53, "seek": 34048, "start": 357.12, "end": 363.52000000000004, "text": " existing data, whether it's videos, it's data from internal database documents available to create", "tokens": [51196, 6741, 1412, 11, 1968, 309, 311, 2145, 11, 309, 311, 1412, 490, 6920, 8149, 8512, 2435, 281, 1884, 51516], "temperature": 0.0, "avg_logprob": -0.19315564772661994, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0003710724995471537}, {"id": 54, "seek": 34048, "start": 363.52000000000004, "end": 368.8, "text": " the embeddings, to calculate the vectors, how this knowledge is internally represented. And then", "tokens": [51516, 264, 12240, 29432, 11, 281, 8873, 264, 18875, 11, 577, 341, 3601, 307, 19501, 10379, 13, 400, 550, 51780], "temperature": 0.0, "avg_logprob": -0.19315564772661994, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0003710724995471537}, {"id": 55, "seek": 36880, "start": 369.04, "end": 373.76, "text": " as soon as your user asks a question in the knowledge assistant or the chatbot,", "tokens": [50376, 382, 2321, 382, 428, 4195, 8962, 257, 1168, 294, 264, 3601, 10994, 420, 264, 5081, 18870, 11, 50612], "temperature": 0.0, "avg_logprob": -0.17273287339643997, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.000711973465513438}, {"id": 56, "seek": 36880, "start": 375.04, "end": 379.6, "text": " there's a called retriever is then asked, hey, please give me the relevant context. And this", "tokens": [50676, 456, 311, 257, 1219, 19817, 331, 307, 550, 2351, 11, 4177, 11, 1767, 976, 385, 264, 7340, 4319, 13, 400, 341, 50904], "temperature": 0.0, "avg_logprob": -0.17273287339643997, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.000711973465513438}, {"id": 57, "seek": 36880, "start": 379.6, "end": 386.88, "text": " can now be a similarity search in the vector database, or it can be a combination of various", "tokens": [50904, 393, 586, 312, 257, 32194, 3164, 294, 264, 8062, 8149, 11, 420, 309, 393, 312, 257, 6562, 295, 3683, 51268], "temperature": 0.0, "avg_logprob": -0.17273287339643997, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.000711973465513438}, {"id": 58, "seek": 36880, "start": 386.88, "end": 392.24, "text": " searches, a full text search, to your spatial search, a regular SQL query to get information out of", "tokens": [51268, 26701, 11, 257, 1577, 2487, 3164, 11, 281, 428, 23598, 3164, 11, 257, 3890, 19200, 14581, 281, 483, 1589, 484, 295, 51536], "temperature": 0.0, "avg_logprob": -0.17273287339643997, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.000711973465513438}, {"id": 59, "seek": 36880, "start": 392.24, "end": 398.32, "text": " your databases. This context is returned back to the retriever. It is put into a special prompt,", "tokens": [51536, 428, 22380, 13, 639, 4319, 307, 8752, 646, 281, 264, 19817, 331, 13, 467, 307, 829, 666, 257, 2121, 12391, 11, 51840], "temperature": 0.0, "avg_logprob": -0.17273287339643997, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.000711973465513438}, {"id": 60, "seek": 39880, "start": 398.8, "end": 403.2, "text": " as context, as additional information to the prompt, and together with the question,", "tokens": [50364, 382, 4319, 11, 382, 4497, 1589, 281, 264, 12391, 11, 293, 1214, 365, 264, 1168, 11, 50584], "temperature": 0.0, "avg_logprob": -0.1412206973348345, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0022274681832641363}, {"id": 61, "seek": 39880, "start": 403.92, "end": 408.64, "text": " and this additional context, not a large language model can generate your answer. And you can put", "tokens": [50620, 293, 341, 4497, 4319, 11, 406, 257, 2416, 2856, 2316, 393, 8460, 428, 1867, 13, 400, 291, 393, 829, 50856], "temperature": 0.0, "avg_logprob": -0.1412206973348345, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0022274681832641363}, {"id": 62, "seek": 39880, "start": 408.64, "end": 413.52000000000004, "text": " into the prompt, as we will see in the demo also, please use only this contextual data. If you", "tokens": [50856, 666, 264, 12391, 11, 382, 321, 486, 536, 294, 264, 10723, 611, 11, 1767, 764, 787, 341, 35526, 1412, 13, 759, 291, 51100], "temperature": 0.0, "avg_logprob": -0.1412206973348345, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0022274681832641363}, {"id": 63, "seek": 39880, "start": 413.52000000000004, "end": 418.8, "text": " don't know the answer, please say you don't know. Limits the hallucinations a lot, doesn't prevent", "tokens": [51100, 500, 380, 458, 264, 1867, 11, 1767, 584, 291, 500, 380, 458, 13, 16406, 1208, 264, 35212, 10325, 257, 688, 11, 1177, 380, 4871, 51364], "temperature": 0.0, "avg_logprob": -0.1412206973348345, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0022274681832641363}, {"id": 64, "seek": 39880, "start": 418.8, "end": 427.84000000000003, "text": " them 100%. Good. I think I talked about disadvantages and challenges already. And one advantage I", "tokens": [51364, 552, 2319, 6856, 2205, 13, 286, 519, 286, 2825, 466, 37431, 293, 4759, 1217, 13, 400, 472, 5002, 286, 51816], "temperature": 0.0, "avg_logprob": -0.1412206973348345, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0022274681832641363}, {"id": 65, "seek": 42784, "start": 427.84, "end": 434.15999999999997, "text": " forgot to mention is access control. Now that you really get this context from either vector store", "tokens": [50364, 5298, 281, 2152, 307, 2105, 1969, 13, 823, 300, 291, 534, 483, 341, 4319, 490, 2139, 8062, 3531, 50680], "temperature": 0.0, "avg_logprob": -0.13475719718045967, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0016002976335585117}, {"id": 66, "seek": 42784, "start": 434.15999999999997, "end": 442.15999999999997, "text": " a different database, maybe create, you can put fine-grained privileges there. The example", "tokens": [50680, 257, 819, 8149, 11, 1310, 1884, 11, 291, 393, 829, 2489, 12, 20735, 2001, 32588, 456, 13, 440, 1365, 51080], "temperature": 0.0, "avg_logprob": -0.13475719718045967, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0016002976335585117}, {"id": 67, "seek": 42784, "start": 442.15999999999997, "end": 445.91999999999996, "text": " application that I mentioned before, some of the maintenance workers are not allowed to use", "tokens": [51080, 3861, 300, 286, 2835, 949, 11, 512, 295, 264, 11258, 5600, 366, 406, 4350, 281, 764, 51268], "temperature": 0.0, "avg_logprob": -0.13475719718045967, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0016002976335585117}, {"id": 68, "seek": 42784, "start": 445.91999999999996, "end": 453.76, "text": " legal documents, for example. So they don't use the index, use the embeddings of the legal documents,", "tokens": [51268, 5089, 8512, 11, 337, 1365, 13, 407, 436, 500, 380, 764, 264, 8186, 11, 764, 264, 12240, 29432, 295, 264, 5089, 8512, 11, 51660], "temperature": 0.0, "avg_logprob": -0.13475719718045967, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0016002976335585117}, {"id": 69, "seek": 45376, "start": 453.76, "end": 458.71999999999997, "text": " but they are obviously allowed to use the technical documentation. And someone from the legal", "tokens": [50364, 457, 436, 366, 2745, 4350, 281, 764, 264, 6191, 14333, 13, 400, 1580, 490, 264, 5089, 50612], "temperature": 0.0, "avg_logprob": -0.17405526356030537, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0015866964822635055}, {"id": 70, "seek": 45376, "start": 458.71999999999997, "end": 465.28, "text": " department, oh, what is the support contract with XYZ? Are we now in liability? Et cetera. Obviously,", "tokens": [50612, 5882, 11, 1954, 11, 437, 307, 264, 1406, 4364, 365, 48826, 57, 30, 2014, 321, 586, 294, 25196, 30, 3790, 11458, 13, 7580, 11, 50940], "temperature": 0.0, "avg_logprob": -0.17405526356030537, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0015866964822635055}, {"id": 71, "seek": 45376, "start": 465.28, "end": 473.2, "text": " they need then different indexes, different search indexes. How to do this? How semantics", "tokens": [50940, 436, 643, 550, 819, 8186, 279, 11, 819, 3164, 8186, 279, 13, 1012, 281, 360, 341, 30, 1012, 4361, 45298, 51336], "temperature": 0.0, "avg_logprob": -0.17405526356030537, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0015866964822635055}, {"id": 72, "seek": 45376, "start": 473.2, "end": 482.96, "text": " represented? Key is the vectors. So, or embeddings. And the vector is nothing else than a series of", "tokens": [51336, 10379, 30, 12759, 307, 264, 18875, 13, 407, 11, 420, 12240, 29432, 13, 400, 264, 8062, 307, 1825, 1646, 813, 257, 2638, 295, 51824], "temperature": 0.0, "avg_logprob": -0.17405526356030537, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0015866964822635055}, {"id": 73, "seek": 48296, "start": 482.96, "end": 488.56, "text": " decimal values or an array of decimal values with a lot of different embedding models out there", "tokens": [50364, 26601, 4190, 420, 364, 10225, 295, 26601, 4190, 365, 257, 688, 295, 819, 12240, 3584, 5245, 484, 456, 50644], "temperature": 0.0, "avg_logprob": -0.12128800543669228, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0049373493529856205}, {"id": 74, "seek": 48296, "start": 488.56, "end": 495.91999999999996, "text": " already. And every model has its strengths and weaknesses. Some are more optimal if you use,", "tokens": [50644, 1217, 13, 400, 633, 2316, 575, 1080, 16986, 293, 24381, 13, 2188, 366, 544, 16252, 498, 291, 764, 11, 51012], "temperature": 0.0, "avg_logprob": -0.12128800543669228, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0049373493529856205}, {"id": 75, "seek": 48296, "start": 495.91999999999996, "end": 502.47999999999996, "text": " for example, German text, if you use Chinese text or Indian text, right? A very different way how", "tokens": [51012, 337, 1365, 11, 6521, 2487, 11, 498, 291, 764, 4649, 2487, 420, 6427, 2487, 11, 558, 30, 316, 588, 819, 636, 577, 51340], "temperature": 0.0, "avg_logprob": -0.12128800543669228, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0049373493529856205}, {"id": 76, "seek": 48296, "start": 502.47999999999996, "end": 507.84, "text": " to come up with the semantics and to analyze how the attention mechanisms internally work,", "tokens": [51340, 281, 808, 493, 365, 264, 4361, 45298, 293, 281, 12477, 577, 264, 3202, 15902, 19501, 589, 11, 51608], "temperature": 0.0, "avg_logprob": -0.12128800543669228, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0049373493529856205}, {"id": 77, "seek": 48296, "start": 507.84, "end": 512.48, "text": " right? Because the sentences are built in a very, very different way. So you see different", "tokens": [51608, 558, 30, 1436, 264, 16579, 366, 3094, 294, 257, 588, 11, 588, 819, 636, 13, 407, 291, 536, 819, 51840], "temperature": 0.0, "avg_logprob": -0.12128800543669228, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0049373493529856205}, {"id": 78, "seek": 51248, "start": 512.5600000000001, "end": 517.6, "text": " performance there or highly specialized models. You do an image recognition. Oh, it's a sleeping cat.", "tokens": [50368, 3389, 456, 420, 5405, 19813, 5245, 13, 509, 360, 364, 3256, 11150, 13, 876, 11, 309, 311, 257, 8296, 3857, 13, 50620], "temperature": 0.0, "avg_logprob": -0.12220041530648458, "compression_ratio": 1.5753968253968254, "no_speech_prob": 0.0015132081462070346}, {"id": 79, "seek": 51248, "start": 518.72, "end": 524.72, "text": " And this can then be vectorized as well. And you can search for this context in your vector store.", "tokens": [50676, 400, 341, 393, 550, 312, 8062, 1602, 382, 731, 13, 400, 291, 393, 3164, 337, 341, 4319, 294, 428, 8062, 3531, 13, 50976], "temperature": 0.0, "avg_logprob": -0.12220041530648458, "compression_ratio": 1.5753968253968254, "no_speech_prob": 0.0015132081462070346}, {"id": 80, "seek": 51248, "start": 527.76, "end": 533.2, "text": " And now, if we think this one step further, how could an architecture look like for such a knowledge", "tokens": [51128, 400, 586, 11, 498, 321, 519, 341, 472, 1823, 3052, 11, 577, 727, 364, 9482, 574, 411, 337, 1270, 257, 3601, 51400], "temperature": 0.0, "avg_logprob": -0.12220041530648458, "compression_ratio": 1.5753968253968254, "no_speech_prob": 0.0015132081462070346}, {"id": 81, "seek": 51248, "start": 533.2, "end": 538.4, "text": " assistant or a chatbot? Prototype is always easy to build, but you need to think about a lot of", "tokens": [51400, 10994, 420, 257, 5081, 18870, 30, 10019, 13108, 307, 1009, 1858, 281, 1322, 11, 457, 291, 643, 281, 519, 466, 257, 688, 295, 51660], "temperature": 0.0, "avg_logprob": -0.12220041530648458, "compression_ratio": 1.5753968253968254, "no_speech_prob": 0.0015132081462070346}, {"id": 82, "seek": 53840, "start": 539.36, "end": 546.3199999999999, "text": " a lot of additional topics. First of all, it starts with the data, right? The data that you want to", "tokens": [50412, 257, 688, 295, 4497, 8378, 13, 2386, 295, 439, 11, 309, 3719, 365, 264, 1412, 11, 558, 30, 440, 1412, 300, 291, 528, 281, 50760], "temperature": 0.0, "avg_logprob": -0.10212337176005046, "compression_ratio": 1.776978417266187, "no_speech_prob": 0.0021385103464126587}, {"id": 83, "seek": 53840, "start": 546.3199999999999, "end": 551.52, "text": " train, that you want to vectorize, that you want to make available for your search. So we've shown", "tokens": [50760, 3847, 11, 300, 291, 528, 281, 8062, 1125, 11, 300, 291, 528, 281, 652, 2435, 337, 428, 3164, 13, 407, 321, 600, 4898, 51020], "temperature": 0.0, "avg_logprob": -0.10212337176005046, "compression_ratio": 1.776978417266187, "no_speech_prob": 0.0021385103464126587}, {"id": 84, "seek": 53840, "start": 551.52, "end": 557.4399999999999, "text": " here a landing zone from different sources, can be the original sources. You might copy it, depends", "tokens": [51020, 510, 257, 11202, 6668, 490, 819, 7139, 11, 393, 312, 264, 3380, 7139, 13, 509, 1062, 5055, 309, 11, 5946, 51316], "temperature": 0.0, "avg_logprob": -0.10212337176005046, "compression_ratio": 1.776978417266187, "no_speech_prob": 0.0021385103464126587}, {"id": 85, "seek": 53840, "start": 557.4399999999999, "end": 562.24, "text": " on the architecture you want to build. And the important thing is the processing layer. How do", "tokens": [51316, 322, 264, 9482, 291, 528, 281, 1322, 13, 400, 264, 1021, 551, 307, 264, 9007, 4583, 13, 1012, 360, 51556], "temperature": 0.0, "avg_logprob": -0.10212337176005046, "compression_ratio": 1.776978417266187, "no_speech_prob": 0.0021385103464126587}, {"id": 86, "seek": 53840, "start": 562.24, "end": 567.6, "text": " you chunk your data? How do you create the vectors? And obviously, you need to store these chunks of", "tokens": [51556, 291, 16635, 428, 1412, 30, 1012, 360, 291, 1884, 264, 18875, 30, 400, 2745, 11, 291, 643, 281, 3531, 613, 24004, 295, 51824], "temperature": 0.0, "avg_logprob": -0.10212337176005046, "compression_ratio": 1.776978417266187, "no_speech_prob": 0.0021385103464126587}, {"id": 87, "seek": 56760, "start": 567.6800000000001, "end": 574.88, "text": " information together with the vectors and provide proper data access control. Second part here,", "tokens": [50368, 1589, 1214, 365, 264, 18875, 293, 2893, 2296, 1412, 2105, 1969, 13, 5736, 644, 510, 11, 50728], "temperature": 0.0, "avg_logprob": -0.11894548204210069, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0011779243359342217}, {"id": 88, "seek": 56760, "start": 574.88, "end": 580.64, "text": " the LLM part, talked about it now multiple times. You need access to the embeddings,", "tokens": [50728, 264, 441, 43, 44, 644, 11, 2825, 466, 309, 586, 3866, 1413, 13, 509, 643, 2105, 281, 264, 12240, 29432, 11, 51016], "temperature": 0.0, "avg_logprob": -0.11894548204210069, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0011779243359342217}, {"id": 89, "seek": 56760, "start": 580.64, "end": 585.76, "text": " you need access to the large language models, and then also needs to be some logging. What do", "tokens": [51016, 291, 643, 2105, 281, 264, 2416, 2856, 5245, 11, 293, 550, 611, 2203, 281, 312, 512, 27991, 13, 708, 360, 51272], "temperature": 0.0, "avg_logprob": -0.11894548204210069, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0011779243359342217}, {"id": 90, "seek": 56760, "start": 585.76, "end": 591.76, "text": " do you use a query? How much cost does it incur? Is the performance okay? A lot of logging that", "tokens": [51272, 360, 291, 764, 257, 14581, 30, 1012, 709, 2063, 775, 309, 35774, 30, 1119, 264, 3389, 1392, 30, 316, 688, 295, 27991, 300, 51572], "temperature": 0.0, "avg_logprob": -0.11894548204210069, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0011779243359342217}, {"id": 91, "seek": 59176, "start": 591.76, "end": 598.64, "text": " also occurs here. And intentionally, an LLM gateway put in front of it because it needs to be", "tokens": [50364, 611, 11843, 510, 13, 400, 22062, 11, 364, 441, 43, 44, 28532, 829, 294, 1868, 295, 309, 570, 309, 2203, 281, 312, 50708], "temperature": 0.0, "avg_logprob": -0.10675210450824939, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0024478931445628405}, {"id": 92, "seek": 59176, "start": 599.28, "end": 606.48, "text": " changeable. Chatbots with a lot of functionalities don't want to go into all the details, obviously", "tokens": [50740, 1319, 712, 13, 27503, 65, 1971, 365, 257, 688, 295, 11745, 1088, 500, 380, 528, 281, 352, 666, 439, 264, 4365, 11, 2745, 51100], "temperature": 0.0, "avg_logprob": -0.10675210450824939, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0024478931445628405}, {"id": 93, "seek": 59176, "start": 606.48, "end": 611.36, "text": " monitoring and reporting. And the beauty of it, you can build all of that with open source tools", "tokens": [51100, 11028, 293, 10031, 13, 400, 264, 6643, 295, 309, 11, 291, 393, 1322, 439, 295, 300, 365, 1269, 4009, 3873, 51344], "temperature": 0.0, "avg_logprob": -0.10675210450824939, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0024478931445628405}, {"id": 94, "seek": 59176, "start": 611.36, "end": 617.52, "text": " nowadays. And also the embeddings and language models can be open sourced, a lot of alternatives", "tokens": [51344, 13434, 13, 400, 611, 264, 12240, 29432, 293, 2856, 5245, 393, 312, 1269, 11006, 1232, 11, 257, 688, 295, 20478, 51652], "temperature": 0.0, "avg_logprob": -0.10675210450824939, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0024478931445628405}, {"id": 95, "seek": 61752, "start": 617.52, "end": 625.04, "text": " out there. Now, why create a long chain? You need a robust data management. As we have seen,", "tokens": [50364, 484, 456, 13, 823, 11, 983, 1884, 257, 938, 5021, 30, 509, 643, 257, 13956, 1412, 4592, 13, 1018, 321, 362, 1612, 11, 50740], "temperature": 0.0, "avg_logprob": -0.14929867634731056, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.002967562759295106}, {"id": 96, "seek": 61752, "start": 625.04, "end": 628.96, "text": " there's a lot of different data sources involved here, data stores, whether it's logging, whether", "tokens": [50740, 456, 311, 257, 688, 295, 819, 1412, 7139, 3288, 510, 11, 1412, 9512, 11, 1968, 309, 311, 27991, 11, 1968, 50936], "temperature": 0.0, "avg_logprob": -0.14929867634731056, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.002967562759295106}, {"id": 97, "seek": 61752, "start": 628.96, "end": 634.48, "text": " it's semantics, your agents communicate in JSON. So you need to store all of this information,", "tokens": [50936, 309, 311, 4361, 45298, 11, 428, 12554, 7890, 294, 31828, 13, 407, 291, 643, 281, 3531, 439, 295, 341, 1589, 11, 51212], "temperature": 0.0, "avg_logprob": -0.14929867634731056, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.002967562759295106}, {"id": 98, "seek": 61752, "start": 634.48, "end": 639.6, "text": " ideally in one store, not five, six different databases here that you need to operate,", "tokens": [51212, 22915, 294, 472, 3531, 11, 406, 1732, 11, 2309, 819, 22380, 510, 300, 291, 643, 281, 9651, 11, 51468], "temperature": 0.0, "avg_logprob": -0.14929867634731056, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.002967562759295106}, {"id": 99, "seek": 61752, "start": 639.6, "end": 645.6, "text": " you need to learn the language, et cetera. And also long chain, other opportunities are also", "tokens": [51468, 291, 643, 281, 1466, 264, 2856, 11, 1030, 11458, 13, 400, 611, 938, 5021, 11, 661, 4786, 366, 611, 51768], "temperature": 0.0, "avg_logprob": -0.14929867634731056, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.002967562759295106}, {"id": 100, "seek": 64560, "start": 645.6, "end": 650.24, "text": " out there. Think of Haystack and others that you could use. But all of these frameworks give you", "tokens": [50364, 484, 456, 13, 6557, 295, 8721, 372, 501, 293, 2357, 300, 291, 727, 764, 13, 583, 439, 295, 613, 29834, 976, 291, 50596], "temperature": 0.0, "avg_logprob": -0.1408731183435163, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.008274354040622711}, {"id": 101, "seek": 64560, "start": 650.24, "end": 655.0400000000001, "text": " a very good set of building blocks. You can just use them. It's available in Python, JavaScript,", "tokens": [50596, 257, 588, 665, 992, 295, 2390, 8474, 13, 509, 393, 445, 764, 552, 13, 467, 311, 2435, 294, 15329, 11, 15778, 11, 50836], "temperature": 0.0, "avg_logprob": -0.1408731183435163, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.008274354040622711}, {"id": 102, "seek": 64560, "start": 655.0400000000001, "end": 661.52, "text": " there are also Java ports out there, ports to other languages are now available. Everything you", "tokens": [50836, 456, 366, 611, 10745, 18160, 484, 456, 11, 18160, 281, 661, 8650, 366, 586, 2435, 13, 5471, 291, 51160], "temperature": 0.0, "avg_logprob": -0.1408731183435163, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.008274354040622711}, {"id": 103, "seek": 64560, "start": 661.52, "end": 667.28, "text": " need is already in these libraries to come up with your overall architecture. And that's now the", "tokens": [51160, 643, 307, 1217, 294, 613, 15148, 281, 808, 493, 365, 428, 4787, 9482, 13, 400, 300, 311, 586, 264, 51448], "temperature": 0.0, "avg_logprob": -0.1408731183435163, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.008274354040622711}, {"id": 104, "seek": 64560, "start": 667.28, "end": 674.4, "text": " point to hand it over to Maria. She will guide you through a demo where we want to use it, try to", "tokens": [51448, 935, 281, 1011, 309, 670, 281, 12734, 13, 1240, 486, 5934, 291, 807, 257, 10723, 689, 321, 528, 281, 764, 309, 11, 853, 281, 51804], "temperature": 0.0, "avg_logprob": -0.1408731183435163, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.008274354040622711}, {"id": 105, "seek": 67440, "start": 674.4, "end": 680.72, "text": " simulate how you can use support tickets, internal data. Here we took some Twitter posts from Microsoft.", "tokens": [50364, 27817, 577, 291, 393, 764, 1406, 12628, 11, 6920, 1412, 13, 1692, 321, 1890, 512, 5794, 12300, 490, 8116, 13, 50680], "temperature": 0.0, "avg_logprob": -0.14400490043089562, "compression_ratio": 1.5737051792828685, "no_speech_prob": 0.002528005512431264}, {"id": 106, "seek": 67440, "start": 680.72, "end": 686.88, "text": " We will vectorize them and we'll show how a support or a customer can then interact with this chat", "tokens": [50680, 492, 486, 8062, 1125, 552, 293, 321, 603, 855, 577, 257, 1406, 420, 257, 5474, 393, 550, 4648, 365, 341, 5081, 50988], "temperature": 0.0, "avg_logprob": -0.14400490043089562, "compression_ratio": 1.5737051792828685, "no_speech_prob": 0.002528005512431264}, {"id": 107, "seek": 67440, "start": 686.88, "end": 693.12, "text": " bot, ask certain questions. It won't demonstrate it's not such a big effort. You can get started", "tokens": [50988, 10592, 11, 1029, 1629, 1651, 13, 467, 1582, 380, 11698, 309, 311, 406, 1270, 257, 955, 4630, 13, 509, 393, 483, 1409, 51300], "temperature": 0.0, "avg_logprob": -0.14400490043089562, "compression_ratio": 1.5737051792828685, "no_speech_prob": 0.002528005512431264}, {"id": 108, "seek": 67440, "start": 693.12, "end": 699.6, "text": " right away. And all the demo, we put the link here on the slide. You find also the link to the", "tokens": [51300, 558, 1314, 13, 400, 439, 264, 10723, 11, 321, 829, 264, 2113, 510, 322, 264, 4137, 13, 509, 915, 611, 264, 2113, 281, 264, 51624], "temperature": 0.0, "avg_logprob": -0.14400490043089562, "compression_ratio": 1.5737051792828685, "no_speech_prob": 0.002528005512431264}, {"id": 109, "seek": 69960, "start": 699.6800000000001, "end": 703.0400000000001, "text": " demo in the app or on the website for the talk.", "tokens": [50368, 10723, 294, 264, 724, 420, 322, 264, 3144, 337, 264, 751, 13, 50536], "temperature": 0.0, "avg_logprob": -0.19209333397876258, "compression_ratio": 1.5492957746478873, "no_speech_prob": 0.003790449583902955}, {"id": 110, "seek": 69960, "start": 709.12, "end": 717.2, "text": " Thank you. Do you hear me? Okay. Awesome. Thank you. So you have heard a lot of theoretical", "tokens": [50840, 1044, 291, 13, 1144, 291, 1568, 385, 30, 1033, 13, 10391, 13, 1044, 291, 13, 407, 291, 362, 2198, 257, 688, 295, 20864, 51244], "temperature": 0.0, "avg_logprob": -0.19209333397876258, "compression_ratio": 1.5492957746478873, "no_speech_prob": 0.003790449583902955}, {"id": 111, "seek": 69960, "start": 717.2, "end": 722.08, "text": " aspects of the drug and how it works. I have a little bit more than 10 minutes to show you a", "tokens": [51244, 7270, 295, 264, 4110, 293, 577, 309, 1985, 13, 286, 362, 257, 707, 857, 544, 813, 1266, 2077, 281, 855, 291, 257, 51488], "temperature": 0.0, "avg_logprob": -0.19209333397876258, "compression_ratio": 1.5492957746478873, "no_speech_prob": 0.003790449583902955}, {"id": 112, "seek": 69960, "start": 722.08, "end": 728.8000000000001, "text": " practical example. But believe me, we can have hours long workshop on this topic. So essentially,", "tokens": [51488, 8496, 1365, 13, 583, 1697, 385, 11, 321, 393, 362, 2496, 938, 13541, 322, 341, 4829, 13, 407, 4476, 11, 51824], "temperature": 0.0, "avg_logprob": -0.19209333397876258, "compression_ratio": 1.5492957746478873, "no_speech_prob": 0.003790449583902955}, {"id": 113, "seek": 72880, "start": 728.8, "end": 736.24, "text": " the idea today is to show you how to augment some of the existing LLAMs with the private data and how", "tokens": [50364, 264, 1558, 965, 307, 281, 855, 291, 577, 281, 29919, 512, 295, 264, 6741, 441, 43, 2865, 82, 365, 264, 4551, 1412, 293, 577, 50736], "temperature": 0.0, "avg_logprob": -0.12332334863134177, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.0008494873181916773}, {"id": 114, "seek": 72880, "start": 736.24, "end": 742.88, "text": " to use it for the context of some specific questions that this LLAM has not seen so far.", "tokens": [50736, 281, 764, 309, 337, 264, 4319, 295, 512, 2685, 1651, 300, 341, 441, 43, 2865, 575, 406, 1612, 370, 1400, 13, 51068], "temperature": 0.0, "avg_logprob": -0.12332334863134177, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.0008494873181916773}, {"id": 115, "seek": 72880, "start": 743.5999999999999, "end": 750.64, "text": " So we actually use data that capture customer interactions on Twitter and these customer", "tokens": [51104, 407, 321, 767, 764, 1412, 300, 7983, 5474, 13280, 322, 5794, 293, 613, 5474, 51456], "temperature": 0.0, "avg_logprob": -0.12332334863134177, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.0008494873181916773}, {"id": 116, "seek": 72880, "start": 750.64, "end": 756.56, "text": " interactions involve different questions from the users about Microsoft, Amazon, all these", "tokens": [51456, 13280, 9494, 819, 1651, 490, 264, 5022, 466, 8116, 11, 6795, 11, 439, 613, 51752], "temperature": 0.0, "avg_logprob": -0.12332334863134177, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.0008494873181916773}, {"id": 117, "seek": 75656, "start": 757.3599999999999, "end": 762.2399999999999, "text": " different products today and how actually the support from these big companies actually", "tokens": [50404, 819, 3383, 965, 293, 577, 767, 264, 1406, 490, 613, 955, 3431, 767, 50648], "temperature": 0.0, "avg_logprob": -0.10203164033215455, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0026502669788897038}, {"id": 118, "seek": 75656, "start": 762.2399999999999, "end": 767.3599999999999, "text": " answer to these user questions. So this is not something that you usually see on the Internet", "tokens": [50648, 1867, 281, 613, 4195, 1651, 13, 407, 341, 307, 406, 746, 300, 291, 2673, 536, 322, 264, 7703, 50904], "temperature": 0.0, "avg_logprob": -0.10203164033215455, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0026502669788897038}, {"id": 119, "seek": 75656, "start": 767.3599999999999, "end": 771.8399999999999, "text": " very easily. So if you have maybe some problem with some Microsoft product, yeah, very often you", "tokens": [50904, 588, 3612, 13, 407, 498, 291, 362, 1310, 512, 1154, 365, 512, 8116, 1674, 11, 1338, 11, 588, 2049, 291, 51128], "temperature": 0.0, "avg_logprob": -0.10203164033215455, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0026502669788897038}, {"id": 120, "seek": 75656, "start": 771.8399999999999, "end": 778.56, "text": " can actually find the solution out there. But some very specific questions that are asked directly", "tokens": [51128, 393, 767, 915, 264, 3827, 484, 456, 13, 583, 512, 588, 2685, 1651, 300, 366, 2351, 3838, 51464], "temperature": 0.0, "avg_logprob": -0.10203164033215455, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0026502669788897038}, {"id": 121, "seek": 75656, "start": 778.56, "end": 782.64, "text": " to customer support is probably a very good reason why it sells to the customer support. So you", "tokens": [51464, 281, 5474, 1406, 307, 1391, 257, 588, 665, 1778, 983, 309, 20897, 281, 264, 5474, 1406, 13, 407, 291, 51668], "temperature": 0.0, "avg_logprob": -0.10203164033215455, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0026502669788897038}, {"id": 122, "seek": 78264, "start": 782.64, "end": 791.4399999999999, "text": " didn't find the answer to this out of the box. And we will use CradyBee as a vector store", "tokens": [50364, 994, 380, 915, 264, 1867, 281, 341, 484, 295, 264, 2424, 13, 400, 321, 486, 764, 4779, 880, 33, 1653, 382, 257, 8062, 3531, 50804], "temperature": 0.0, "avg_logprob": -0.17368289402553014, "compression_ratio": 1.5443548387096775, "no_speech_prob": 0.0024036867544054985}, {"id": 123, "seek": 78264, "start": 792.16, "end": 798.96, "text": " to support this example. So I think Kristina already gave you a good overview of what the CradyBee", "tokens": [50840, 281, 1406, 341, 1365, 13, 407, 286, 519, 19562, 1426, 1217, 2729, 291, 257, 665, 12492, 295, 437, 264, 4779, 880, 33, 1653, 51180], "temperature": 0.0, "avg_logprob": -0.17368289402553014, "compression_ratio": 1.5443548387096775, "no_speech_prob": 0.0024036867544054985}, {"id": 124, "seek": 78264, "start": 799.84, "end": 804.4, "text": " is. What is the long chain? Long chain is an open source Python project that actually is used to", "tokens": [51224, 307, 13, 708, 307, 264, 938, 5021, 30, 8282, 5021, 307, 364, 1269, 4009, 15329, 1716, 300, 767, 307, 1143, 281, 51452], "temperature": 0.0, "avg_logprob": -0.17368289402553014, "compression_ratio": 1.5443548387096775, "no_speech_prob": 0.0024036867544054985}, {"id": 125, "seek": 78264, "start": 804.4, "end": 810.8, "text": " facilitate the development of LLAM applications. It's a pretty cool project that integrates a lot", "tokens": [51452, 20207, 264, 3250, 295, 441, 43, 2865, 5821, 13, 467, 311, 257, 1238, 1627, 1716, 300, 3572, 1024, 257, 688, 51772], "temperature": 0.0, "avg_logprob": -0.17368289402553014, "compression_ratio": 1.5443548387096775, "no_speech_prob": 0.0024036867544054985}, {"id": 126, "seek": 81080, "start": 810.8, "end": 816.24, "text": " of large language models, a lot of models for calculating embeddings and actually something", "tokens": [50364, 295, 2416, 2856, 5245, 11, 257, 688, 295, 5245, 337, 28258, 12240, 29432, 293, 767, 746, 50636], "temperature": 0.0, "avg_logprob": -0.14459245772588822, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0005789304268546402}, {"id": 127, "seek": 81080, "start": 816.24, "end": 823.5999999999999, "text": " that helps you integrate some data source with some language model without thinking out of the box", "tokens": [50636, 300, 3665, 291, 13365, 512, 1412, 4009, 365, 512, 2856, 2316, 1553, 1953, 484, 295, 264, 2424, 51004], "temperature": 0.0, "avg_logprob": -0.14459245772588822, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0005789304268546402}, {"id": 128, "seek": 81080, "start": 823.5999999999999, "end": 828.4, "text": " how the full engineering pipeline should look like. Actually you can just do this in a couple of", "tokens": [51004, 577, 264, 1577, 7043, 15517, 820, 574, 411, 13, 5135, 291, 393, 445, 360, 341, 294, 257, 1916, 295, 51244], "temperature": 0.0, "avg_logprob": -0.14459245772588822, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0005789304268546402}, {"id": 129, "seek": 81080, "start": 828.4, "end": 834.16, "text": " lines of code. May I add one point here that I forgot to mention. Although you use long chains,", "tokens": [51244, 3876, 295, 3089, 13, 1891, 286, 909, 472, 935, 510, 300, 286, 5298, 281, 2152, 13, 5780, 291, 764, 938, 12626, 11, 51532], "temperature": 0.0, "avg_logprob": -0.14459245772588822, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0005789304268546402}, {"id": 130, "seek": 81080, "start": 834.16, "end": 840.4799999999999, "text": " very good starting point. What we have also seen for very advanced purposes, you want to directly", "tokens": [51532, 588, 665, 2891, 935, 13, 708, 321, 362, 611, 1612, 337, 588, 7339, 9932, 11, 291, 528, 281, 3838, 51848], "temperature": 0.0, "avg_logprob": -0.14459245772588822, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0005789304268546402}, {"id": 131, "seek": 84048, "start": 840.48, "end": 844.24, "text": " interact with your data, with your source data, with your vector store and all of that is available", "tokens": [50364, 4648, 365, 428, 1412, 11, 365, 428, 4009, 1412, 11, 365, 428, 8062, 3531, 293, 439, 295, 300, 307, 2435, 50552], "temperature": 0.0, "avg_logprob": -0.12440316983968905, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0003712130128405988}, {"id": 132, "seek": 84048, "start": 844.24, "end": 849.6, "text": " in standard SQL, no matter which data model you're using. And CradyBee is an open source store,", "tokens": [50552, 294, 3832, 19200, 11, 572, 1871, 597, 1412, 2316, 291, 434, 1228, 13, 400, 4779, 880, 33, 1653, 307, 364, 1269, 4009, 3531, 11, 50820], "temperature": 0.0, "avg_logprob": -0.12440316983968905, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0003712130128405988}, {"id": 133, "seek": 84048, "start": 849.6, "end": 855.9200000000001, "text": " one of the easiest ways to run CradyBee is actually to use a Docker image. So a vector support in", "tokens": [50820, 472, 295, 264, 12889, 2098, 281, 1190, 4779, 880, 33, 1653, 307, 767, 281, 764, 257, 33772, 3256, 13, 407, 257, 8062, 1406, 294, 51136], "temperature": 0.0, "avg_logprob": -0.12440316983968905, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0003712130128405988}, {"id": 134, "seek": 84048, "start": 855.9200000000001, "end": 861.28, "text": " CradyBee has been available since 5.5 version, but if you actually always pull the latest image,", "tokens": [51136, 4779, 880, 33, 1653, 575, 668, 2435, 1670, 1025, 13, 20, 3037, 11, 457, 498, 291, 767, 1009, 2235, 264, 6792, 3256, 11, 51404], "temperature": 0.0, "avg_logprob": -0.12440316983968905, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0003712130128405988}, {"id": 135, "seek": 86128, "start": 861.36, "end": 868.48, "text": " you should not actually think about this. So once you run this Docker run command,", "tokens": [50368, 291, 820, 406, 767, 519, 466, 341, 13, 407, 1564, 291, 1190, 341, 33772, 1190, 5622, 11, 50724], "temperature": 0.0, "avg_logprob": -0.13000795420478373, "compression_ratio": 1.546448087431694, "no_speech_prob": 0.004495699889957905}, {"id": 136, "seek": 86128, "start": 869.6, "end": 876.3199999999999, "text": " we actually run the instance of CradyBee cluster and then we can access the admin UI in the local", "tokens": [50780, 321, 767, 1190, 264, 5197, 295, 4779, 880, 33, 1653, 13630, 293, 550, 321, 393, 2105, 264, 24236, 15682, 294, 264, 2654, 51116], "temperature": 0.0, "avg_logprob": -0.13000795420478373, "compression_ratio": 1.546448087431694, "no_speech_prob": 0.004495699889957905}, {"id": 137, "seek": 86128, "start": 876.3199999999999, "end": 886.64, "text": " host. So currently I think because of the resolution of this screen, yes, not everything is available,", "tokens": [51116, 3975, 13, 407, 4362, 286, 519, 570, 295, 264, 8669, 295, 341, 2568, 11, 2086, 11, 406, 1203, 307, 2435, 11, 51632], "temperature": 0.0, "avg_logprob": -0.13000795420478373, "compression_ratio": 1.546448087431694, "no_speech_prob": 0.004495699889957905}, {"id": 138, "seek": 88664, "start": 886.64, "end": 894.0, "text": " but actually in this admin UI you have a couple of tabs that you can use actually to monitor your", "tokens": [50364, 457, 767, 294, 341, 24236, 15682, 291, 362, 257, 1916, 295, 20743, 300, 291, 393, 764, 767, 281, 6002, 428, 50732], "temperature": 0.0, "avg_logprob": -0.12052515719799285, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.00043916128925047815}, {"id": 139, "seek": 88664, "start": 894.0, "end": 899.92, "text": " cluster to run some query in the console and also to have overview of the tables and the views that", "tokens": [50732, 13630, 281, 1190, 512, 14581, 294, 264, 11076, 293, 611, 281, 362, 12492, 295, 264, 8020, 293, 264, 6809, 300, 51028], "temperature": 0.0, "avg_logprob": -0.12052515719799285, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.00043916128925047815}, {"id": 140, "seek": 88664, "start": 899.92, "end": 906.16, "text": " are available in your database. So let's go back to the example because the time is flying very fast.", "tokens": [51028, 366, 2435, 294, 428, 8149, 13, 407, 718, 311, 352, 646, 281, 264, 1365, 570, 264, 565, 307, 7137, 588, 2370, 13, 51340], "temperature": 0.0, "avg_logprob": -0.12052515719799285, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.00043916128925047815}, {"id": 141, "seek": 88664, "start": 906.16, "end": 911.6, "text": " So what we need is the first step, we need a couple of import statements to make sure that the long", "tokens": [51340, 407, 437, 321, 643, 307, 264, 700, 1823, 11, 321, 643, 257, 1916, 295, 974, 12363, 281, 652, 988, 300, 264, 938, 51612], "temperature": 0.0, "avg_logprob": -0.12052515719799285, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.00043916128925047815}, {"id": 142, "seek": 91160, "start": 911.6, "end": 918.16, "text": " chain and all libraries that we use in this example are available. What is also important is that", "tokens": [50364, 5021, 293, 439, 15148, 300, 321, 764, 294, 341, 1365, 366, 2435, 13, 708, 307, 611, 1021, 307, 300, 50692], "temperature": 0.0, "avg_logprob": -0.10763572321997748, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.002243214985355735}, {"id": 143, "seek": 91160, "start": 919.12, "end": 931.6, "text": " you import CradyBee vector search interface that is available in one of the long chain versions,", "tokens": [50740, 291, 974, 4779, 880, 33, 1653, 8062, 3164, 9226, 300, 307, 2435, 294, 472, 295, 264, 938, 5021, 9606, 11, 51364], "temperature": 0.0, "avg_logprob": -0.10763572321997748, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.002243214985355735}, {"id": 144, "seek": 91160, "start": 931.6, "end": 939.6, "text": " let's say, which is used to interact with the CradyBee. And as a next step, because we need", "tokens": [51364, 718, 311, 584, 11, 597, 307, 1143, 281, 4648, 365, 264, 4779, 880, 33, 1653, 13, 400, 382, 257, 958, 1823, 11, 570, 321, 643, 51764], "temperature": 0.0, "avg_logprob": -0.10763572321997748, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.002243214985355735}, {"id": 145, "seek": 93960, "start": 939.6, "end": 944.5600000000001, "text": " to interact with the CradyBee instance, we need to specify how we interact. So this is done", "tokens": [50364, 281, 4648, 365, 264, 4779, 880, 33, 1653, 5197, 11, 321, 643, 281, 16500, 577, 321, 4648, 13, 407, 341, 307, 1096, 50612], "temperature": 0.0, "avg_logprob": -0.12841139669003693, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0009266667766496539}, {"id": 146, "seek": 93960, "start": 946.0, "end": 952.4, "text": " by specifying connection string. We are using open source version running on local host,", "tokens": [50684, 538, 1608, 5489, 4984, 6798, 13, 492, 366, 1228, 1269, 4009, 3037, 2614, 322, 2654, 3975, 11, 51004], "temperature": 0.0, "avg_logprob": -0.12841139669003693, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0009266667766496539}, {"id": 147, "seek": 93960, "start": 952.4, "end": 958.5600000000001, "text": " but you also have option, for example, if you want to deploy CradyBee cloud cluster and at this", "tokens": [51004, 457, 291, 611, 362, 3614, 11, 337, 1365, 11, 498, 291, 528, 281, 7274, 4779, 880, 33, 1653, 4588, 13630, 293, 412, 341, 51312], "temperature": 0.0, "avg_logprob": -0.12841139669003693, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0009266667766496539}, {"id": 148, "seek": 93960, "start": 958.5600000000001, "end": 965.36, "text": " point we also give option for all users to deploy one cluster that is free forever so you can just", "tokens": [51312, 935, 321, 611, 976, 3614, 337, 439, 5022, 281, 7274, 472, 13630, 300, 307, 1737, 5680, 370, 291, 393, 445, 51652], "temperature": 0.0, "avg_logprob": -0.12841139669003693, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0009266667766496539}, {"id": 149, "seek": 96536, "start": 965.36, "end": 973.28, "text": " run it and use it for testing purposes. Finally, we need to specify the collection name that we are", "tokens": [50364, 1190, 309, 293, 764, 309, 337, 4997, 9932, 13, 6288, 11, 321, 643, 281, 16500, 264, 5765, 1315, 300, 321, 366, 50760], "temperature": 0.0, "avg_logprob": -0.09661908400686163, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0008977938559837639}, {"id": 150, "seek": 96536, "start": 973.28, "end": 979.12, "text": " going to use in this notebook session. So if we run this piece of code, the connection string is", "tokens": [50760, 516, 281, 764, 294, 341, 21060, 5481, 13, 407, 498, 321, 1190, 341, 2522, 295, 3089, 11, 264, 4984, 6798, 307, 51052], "temperature": 0.0, "avg_logprob": -0.09661908400686163, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0008977938559837639}, {"id": 151, "seek": 96536, "start": 979.12, "end": 986.24, "text": " now available and then we can start interacting with the CradyBee. So for purpose of this notebook,", "tokens": [51052, 586, 2435, 293, 550, 321, 393, 722, 18017, 365, 264, 4779, 880, 33, 1653, 13, 407, 337, 4334, 295, 341, 21060, 11, 51408], "temperature": 0.0, "avg_logprob": -0.09661908400686163, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0008977938559837639}, {"id": 152, "seek": 96536, "start": 987.12, "end": 993.12, "text": " I rely on open AI models. Of course, there are long chain supports, so many different models,", "tokens": [51452, 286, 10687, 322, 1269, 7318, 5245, 13, 2720, 1164, 11, 456, 366, 938, 5021, 9346, 11, 370, 867, 819, 5245, 11, 51752], "temperature": 0.0, "avg_logprob": -0.09661908400686163, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0008977938559837639}, {"id": 153, "seek": 99312, "start": 993.12, "end": 998.72, "text": " you can actually integrate many of them, but if you choose to use open AI, make sure that you have", "tokens": [50364, 291, 393, 767, 13365, 867, 295, 552, 11, 457, 498, 291, 2826, 281, 764, 1269, 7318, 11, 652, 988, 300, 291, 362, 50644], "temperature": 0.0, "avg_logprob": -0.11771807177313443, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.0005529398913495243}, {"id": 154, "seek": 99312, "start": 998.72, "end": 1005.28, "text": " open AI key as a part of your environment variable. So now let's take a look at how the dataset looks", "tokens": [50644, 1269, 7318, 2141, 382, 257, 644, 295, 428, 2823, 7006, 13, 407, 586, 718, 311, 747, 257, 574, 412, 577, 264, 28872, 1542, 50972], "temperature": 0.0, "avg_logprob": -0.11771807177313443, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.0005529398913495243}, {"id": 155, "seek": 99312, "start": 1005.28, "end": 1010.88, "text": " like. This dataset is also available on our CradyBee dataset repository, which is also", "tokens": [50972, 411, 13, 639, 28872, 307, 611, 2435, 322, 527, 4779, 880, 33, 1653, 28872, 25841, 11, 597, 307, 611, 51252], "temperature": 0.0, "avg_logprob": -0.11771807177313443, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.0005529398913495243}, {"id": 156, "seek": 99312, "start": 1011.84, "end": 1021.76, "text": " open source and it contains the customer interaction about Microsoft products. So", "tokens": [51300, 1269, 4009, 293, 309, 8306, 264, 5474, 9285, 466, 8116, 3383, 13, 407, 51796], "temperature": 0.0, "avg_logprob": -0.11771807177313443, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.0005529398913495243}, {"id": 157, "seek": 102176, "start": 1021.76, "end": 1027.76, "text": " essentially we would like to now kind of narrow the scope of this notebook for the", "tokens": [50364, 4476, 321, 576, 411, 281, 586, 733, 295, 9432, 264, 11923, 295, 341, 21060, 337, 264, 50664], "temperature": 0.0, "avg_logprob": -0.15493972213179977, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.0018247893312945962}, {"id": 158, "seek": 102176, "start": 1027.76, "end": 1036.24, "text": " for the illustration reasons and time reasons. So essentially this dataset has some information like", "tokens": [50664, 337, 264, 22645, 4112, 293, 565, 4112, 13, 407, 4476, 341, 28872, 575, 512, 1589, 411, 51088], "temperature": 0.0, "avg_logprob": -0.15493972213179977, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.0018247893312945962}, {"id": 159, "seek": 102176, "start": 1038.0, "end": 1043.92, "text": " who is the author of this question, whether it's unbound, outbound question, when it was created,", "tokens": [51176, 567, 307, 264, 3793, 295, 341, 1168, 11, 1968, 309, 311, 517, 18767, 11, 484, 18767, 1168, 11, 562, 309, 390, 2942, 11, 51472], "temperature": 0.0, "avg_logprob": -0.15493972213179977, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.0018247893312945962}, {"id": 160, "seek": 102176, "start": 1043.92, "end": 1051.04, "text": " what was the context of the question or the answer and actually whether this text is response", "tokens": [51472, 437, 390, 264, 4319, 295, 264, 1168, 420, 264, 1867, 293, 767, 1968, 341, 2487, 307, 4134, 51828], "temperature": 0.0, "avg_logprob": -0.15493972213179977, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.0018247893312945962}, {"id": 161, "seek": 105176, "start": 1052.0, "end": 1058.96, "text": " to something or is it response tweet or is it created in response to something else.", "tokens": [50376, 281, 746, 420, 307, 309, 4134, 15258, 420, 307, 309, 2942, 294, 4134, 281, 746, 1646, 13, 50724], "temperature": 0.0, "avg_logprob": -0.10869581946011247, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0003961625916417688}, {"id": 162, "seek": 105176, "start": 1059.52, "end": 1064.64, "text": " So essentially all this information and now the idea is to feed them to the large language model", "tokens": [50752, 407, 4476, 439, 341, 1589, 293, 586, 264, 1558, 307, 281, 3154, 552, 281, 264, 2416, 2856, 2316, 51008], "temperature": 0.0, "avg_logprob": -0.10869581946011247, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0003961625916417688}, {"id": 163, "seek": 105176, "start": 1064.64, "end": 1073.36, "text": " and to ask questions that could be for example seen in this dataset. So as a first step, if you", "tokens": [51008, 293, 281, 1029, 1651, 300, 727, 312, 337, 1365, 1612, 294, 341, 28872, 13, 407, 382, 257, 700, 1823, 11, 498, 291, 51444], "temperature": 0.0, "avg_logprob": -0.10869581946011247, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0003961625916417688}, {"id": 164, "seek": 105176, "start": 1073.36, "end": 1081.2, "text": " remember this big rug image is to create embeddings. Embeddings is actually the representation of your", "tokens": [51444, 1604, 341, 955, 18329, 3256, 307, 281, 1884, 12240, 29432, 13, 24234, 292, 29432, 307, 767, 264, 10290, 295, 428, 51836], "temperature": 0.0, "avg_logprob": -0.10869581946011247, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0003961625916417688}, {"id": 165, "seek": 108120, "start": 1081.2, "end": 1090.0, "text": " data that is suitable for machine learning and AI purposes. So first as a first step we need to", "tokens": [50364, 1412, 300, 307, 12873, 337, 3479, 2539, 293, 7318, 9932, 13, 407, 700, 382, 257, 700, 1823, 321, 643, 281, 50804], "temperature": 0.0, "avg_logprob": -0.14715369779672197, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.00040317614912055433}, {"id": 166, "seek": 108120, "start": 1090.0, "end": 1097.28, "text": " load the data from this dataset and for this we use CSV loader interface that is available in", "tokens": [50804, 3677, 264, 1412, 490, 341, 28872, 293, 337, 341, 321, 764, 48814, 3677, 260, 9226, 300, 307, 2435, 294, 51168], "temperature": 0.0, "avg_logprob": -0.14715369779672197, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.00040317614912055433}, {"id": 167, "seek": 108120, "start": 1097.28, "end": 1108.48, "text": " Longchain and now in this like few pieces of code we already we already creating embeddings for all", "tokens": [51168, 8282, 11509, 293, 586, 294, 341, 411, 1326, 3755, 295, 3089, 321, 1217, 321, 1217, 4084, 12240, 29432, 337, 439, 51728], "temperature": 0.0, "avg_logprob": -0.14715369779672197, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.00040317614912055433}, {"id": 168, "seek": 110848, "start": 1108.48, "end": 1115.44, "text": " the data set for all the entries in our dataset. So if I go back to the admin UI I can see two tables.", "tokens": [50364, 264, 1412, 992, 337, 439, 264, 23041, 294, 527, 28872, 13, 407, 498, 286, 352, 646, 281, 264, 24236, 15682, 286, 393, 536, 732, 8020, 13, 50712], "temperature": 0.0, "avg_logprob": -0.1530700871642207, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.000879425962921232}, {"id": 169, "seek": 110848, "start": 1116.0, "end": 1126.24, "text": " So in the first table actually gives me a collection of entries. So as we as we define the", "tokens": [50740, 407, 294, 264, 700, 3199, 767, 2709, 385, 257, 5765, 295, 23041, 13, 407, 382, 321, 382, 321, 6964, 264, 51252], "temperature": 0.0, "avg_logprob": -0.1530700871642207, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.000879425962921232}, {"id": 170, "seek": 110848, "start": 1126.24, "end": 1131.84, "text": " the first collection we created is called customer data but essentially what is interesting now is to", "tokens": [51252, 264, 700, 5765, 321, 2942, 307, 1219, 5474, 1412, 457, 4476, 437, 307, 1880, 586, 307, 281, 51532], "temperature": 0.0, "avg_logprob": -0.1530700871642207, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.000879425962921232}, {"id": 171, "seek": 113184, "start": 1131.9199999999998, "end": 1140.56, "text": " see like embeddings created for all the entries in this in this collection. So for example this is", "tokens": [50368, 536, 411, 12240, 29432, 2942, 337, 439, 264, 23041, 294, 341, 294, 341, 5765, 13, 407, 337, 1365, 341, 307, 50800], "temperature": 0.0, "avg_logprob": -0.12290070661857946, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.003079928457736969}, {"id": 172, "seek": 113184, "start": 1140.56, "end": 1146.72, "text": " the instance of the of the document that we are actually using for the training purpose or for the", "tokens": [50800, 264, 5197, 295, 264, 295, 264, 4166, 300, 321, 366, 767, 1228, 337, 264, 3097, 4334, 420, 337, 264, 51108], "temperature": 0.0, "avg_logprob": -0.12290070661857946, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.003079928457736969}, {"id": 173, "seek": 113184, "start": 1147.52, "end": 1155.76, "text": " context purposes and you can actually see how the embeddings look like. So if you use open AI", "tokens": [51148, 4319, 9932, 293, 291, 393, 767, 536, 577, 264, 12240, 29432, 574, 411, 13, 407, 498, 291, 764, 1269, 7318, 51560], "temperature": 0.0, "avg_logprob": -0.12290070661857946, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.003079928457736969}, {"id": 174, "seek": 115576, "start": 1155.76, "end": 1166.48, "text": " embeddings usually the length of your of your vector is going to be 1040 something yes it would", "tokens": [50364, 12240, 29432, 2673, 264, 4641, 295, 428, 295, 428, 8062, 307, 516, 281, 312, 1266, 5254, 746, 2086, 309, 576, 50900], "temperature": 0.0, "avg_logprob": -0.12349389883188101, "compression_ratio": 1.7005988023952097, "no_speech_prob": 0.00107672275044024}, {"id": 175, "seek": 115576, "start": 1166.48, "end": 1171.84, "text": " be size of 1040 something but you can also for example choose some other embedding algorithm for", "tokens": [50900, 312, 2744, 295, 1266, 5254, 746, 457, 291, 393, 611, 337, 1365, 2826, 512, 661, 12240, 3584, 9284, 337, 51168], "temperature": 0.0, "avg_logprob": -0.12349389883188101, "compression_ratio": 1.7005988023952097, "no_speech_prob": 0.00107672275044024}, {"id": 176, "seek": 115576, "start": 1171.84, "end": 1180.24, "text": " example hugging face as you can see suggested here which is which is open source and it can", "tokens": [51168, 1365, 41706, 1851, 382, 291, 393, 536, 10945, 510, 597, 307, 597, 307, 1269, 4009, 293, 309, 393, 51588], "temperature": 0.0, "avg_logprob": -0.12349389883188101, "compression_ratio": 1.7005988023952097, "no_speech_prob": 0.00107672275044024}, {"id": 177, "seek": 118024, "start": 1180.8, "end": 1187.28, "text": " easily be used out of the box in just two lines of code. Now once we have these embeddings let's", "tokens": [50392, 3612, 312, 1143, 484, 295, 264, 2424, 294, 445, 732, 3876, 295, 3089, 13, 823, 1564, 321, 362, 613, 12240, 29432, 718, 311, 50716], "temperature": 0.0, "avg_logprob": -0.06454290662493024, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0005756574682891369}, {"id": 178, "seek": 118024, "start": 1187.28, "end": 1195.6, "text": " define our question and our question today is like okay I have some I have some order on my", "tokens": [50716, 6964, 527, 1168, 293, 527, 1168, 965, 307, 411, 1392, 286, 362, 512, 286, 362, 512, 1668, 322, 452, 51132], "temperature": 0.0, "avg_logprob": -0.06454290662493024, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0005756574682891369}, {"id": 179, "seek": 118024, "start": 1195.6, "end": 1203.52, "text": " Microsoft Store but I want to update the shipping address and how I do this. I also here put alternative", "tokens": [51132, 8116, 17242, 457, 286, 528, 281, 5623, 264, 14122, 2985, 293, 577, 286, 360, 341, 13, 286, 611, 510, 829, 8535, 51528], "temperature": 0.0, "avg_logprob": -0.06454290662493024, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0005756574682891369}, {"id": 180, "seek": 118024, "start": 1203.52, "end": 1208.32, "text": " questions so like when you play with this notebook you can also put your own questions and see", "tokens": [51528, 1651, 370, 411, 562, 291, 862, 365, 341, 21060, 291, 393, 611, 829, 428, 1065, 1651, 293, 536, 51768], "temperature": 0.0, "avg_logprob": -0.06454290662493024, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0005756574682891369}, {"id": 181, "seek": 120832, "start": 1208.3999999999999, "end": 1215.28, "text": " actually whether this data set has enough information to answer this question. So once the", "tokens": [50368, 767, 1968, 341, 1412, 992, 575, 1547, 1589, 281, 1867, 341, 1168, 13, 407, 1564, 264, 50712], "temperature": 0.0, "avg_logprob": -0.11007659042937847, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.00025014806305989623}, {"id": 182, "seek": 120832, "start": 1215.28, "end": 1220.8799999999999, "text": " question is answered what we want to do is actually we want to find the context that is relevant to", "tokens": [50712, 1168, 307, 10103, 437, 321, 528, 281, 360, 307, 767, 321, 528, 281, 915, 264, 4319, 300, 307, 7340, 281, 50992], "temperature": 0.0, "avg_logprob": -0.11007659042937847, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.00025014806305989623}, {"id": 183, "seek": 120832, "start": 1220.8799999999999, "end": 1227.2, "text": " this question and this context is done by doing similarity search of the vector representation of", "tokens": [50992, 341, 1168, 293, 341, 4319, 307, 1096, 538, 884, 32194, 3164, 295, 264, 8062, 10290, 295, 51308], "temperature": 0.0, "avg_logprob": -0.11007659042937847, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.00025014806305989623}, {"id": 184, "seek": 120832, "start": 1227.2, "end": 1234.0, "text": " our question compared to the vectors actually that we stored in the creative instance and this is", "tokens": [51308, 527, 1168, 5347, 281, 264, 18875, 767, 300, 321, 12187, 294, 264, 5880, 5197, 293, 341, 307, 51648], "temperature": 0.0, "avg_logprob": -0.11007659042937847, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.00025014806305989623}, {"id": 185, "seek": 123400, "start": 1234.08, "end": 1241.68, "text": " actually done in just one line of code. As Christian suggested vector search is one way to find the", "tokens": [50368, 767, 1096, 294, 445, 472, 1622, 295, 3089, 13, 1018, 5778, 10945, 8062, 3164, 307, 472, 636, 281, 915, 264, 50748], "temperature": 0.0, "avg_logprob": -0.10062743316997182, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.000669891363941133}, {"id": 186, "seek": 123400, "start": 1241.68, "end": 1248.16, "text": " relevant context of course kdb supports other types of searches like full text search or geospatial", "tokens": [50748, 7340, 4319, 295, 1164, 350, 67, 65, 9346, 661, 3467, 295, 26701, 411, 1577, 2487, 3164, 420, 1519, 2763, 267, 831, 51072], "temperature": 0.0, "avg_logprob": -0.10062743316997182, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.000669891363941133}, {"id": 187, "seek": 123400, "start": 1248.16, "end": 1253.68, "text": " search or just key search keyword search so like you can use different type of searches combined", "tokens": [51072, 3164, 420, 445, 2141, 3164, 20428, 3164, 370, 411, 291, 393, 764, 819, 2010, 295, 26701, 9354, 51348], "temperature": 0.0, "avg_logprob": -0.10062743316997182, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.000669891363941133}, {"id": 188, "seek": 123400, "start": 1253.68, "end": 1260.4, "text": " together to find what is what is the relevant context for your question. Once we do this we are", "tokens": [51348, 1214, 281, 915, 437, 307, 437, 307, 264, 7340, 4319, 337, 428, 1168, 13, 3443, 321, 360, 341, 321, 366, 51684], "temperature": 0.0, "avg_logprob": -0.10062743316997182, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.000669891363941133}, {"id": 189, "seek": 126040, "start": 1260.4, "end": 1268.24, "text": " now ready to actually ask our LLM to answer our question and how we do this. First we need to create", "tokens": [50364, 586, 1919, 281, 767, 1029, 527, 441, 43, 44, 281, 1867, 527, 1168, 293, 577, 321, 360, 341, 13, 2386, 321, 643, 281, 1884, 50756], "temperature": 0.0, "avg_logprob": -0.06640542071798573, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0006395501550287008}, {"id": 190, "seek": 126040, "start": 1268.24, "end": 1276.48, "text": " a prompt that explains LLM what his purpose is. So his purpose is today to be expert about", "tokens": [50756, 257, 12391, 300, 13948, 441, 43, 44, 437, 702, 4334, 307, 13, 407, 702, 4334, 307, 965, 281, 312, 5844, 466, 51168], "temperature": 0.0, "avg_logprob": -0.06640542071798573, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0006395501550287008}, {"id": 191, "seek": 126040, "start": 1276.48, "end": 1282.0800000000002, "text": " Microsoft products and services and should use the context that you are going to actually give", "tokens": [51168, 8116, 3383, 293, 3328, 293, 820, 764, 264, 4319, 300, 291, 366, 516, 281, 767, 976, 51448], "temperature": 0.0, "avg_logprob": -0.06640542071798573, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0006395501550287008}, {"id": 192, "seek": 126040, "start": 1282.0800000000002, "end": 1288.16, "text": " to the LLM to answer relevant questions but if the answer is not fine in the context it should", "tokens": [51448, 281, 264, 441, 43, 44, 281, 1867, 7340, 1651, 457, 498, 264, 1867, 307, 406, 2489, 294, 264, 4319, 309, 820, 51752], "temperature": 0.0, "avg_logprob": -0.06640542071798573, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0006395501550287008}, {"id": 193, "seek": 128816, "start": 1288.16, "end": 1295.6000000000001, "text": " reply with I don't know and this is very simple way to create a prompt that actually gives", "tokens": [50364, 16972, 365, 286, 500, 380, 458, 293, 341, 307, 588, 2199, 636, 281, 1884, 257, 12391, 300, 767, 2709, 50736], "temperature": 0.0, "avg_logprob": -0.1453702300786972, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0006098280427977443}, {"id": 194, "seek": 128816, "start": 1295.6000000000001, "end": 1303.1200000000001, "text": " instructions to LLM how it should answer specific questions and finally we just need to create", "tokens": [50736, 9415, 281, 441, 43, 44, 577, 309, 820, 1867, 2685, 1651, 293, 2721, 321, 445, 643, 281, 1884, 51112], "temperature": 0.0, "avg_logprob": -0.1453702300786972, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0006098280427977443}, {"id": 195, "seek": 128816, "start": 1304.48, "end": 1311.44, "text": " small chatbot by using some of the available models that are integrated with the long chain and also", "tokens": [51180, 1359, 5081, 18870, 538, 1228, 512, 295, 264, 2435, 5245, 300, 366, 10919, 365, 264, 938, 5021, 293, 611, 51528], "temperature": 0.0, "avg_logprob": -0.1453702300786972, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0006098280427977443}, {"id": 196, "seek": 131144, "start": 1312.0800000000002, "end": 1318.24, "text": " passing this context together with the user question. Once this is completed we can access", "tokens": [50396, 8437, 341, 4319, 1214, 365, 264, 4195, 1168, 13, 3443, 341, 307, 7365, 321, 393, 2105, 50704], "temperature": 0.0, "avg_logprob": -0.07193735099974133, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.0025896956212818623}, {"id": 197, "seek": 131144, "start": 1319.1200000000001, "end": 1325.04, "text": " the answer and in this case it says to update the shipping address you will need to cancel your", "tokens": [50748, 264, 1867, 293, 294, 341, 1389, 309, 1619, 281, 5623, 264, 14122, 2985, 291, 486, 643, 281, 10373, 428, 51044], "temperature": 0.0, "avg_logprob": -0.07193735099974133, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.0025896956212818623}, {"id": 198, "seek": 131144, "start": 1325.04, "end": 1330.48, "text": " current order and place a new one. Maybe that's something that is still up to date that is relevant", "tokens": [51044, 2190, 1668, 293, 1081, 257, 777, 472, 13, 2704, 300, 311, 746, 300, 307, 920, 493, 281, 4002, 300, 307, 7340, 51316], "temperature": 0.0, "avg_logprob": -0.07193735099974133, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.0025896956212818623}, {"id": 199, "seek": 131144, "start": 1330.48, "end": 1334.96, "text": " maybe it's not relevant anymore but it's actually something we learned only from the dataset we", "tokens": [51316, 1310, 309, 311, 406, 7340, 3602, 457, 309, 311, 767, 746, 321, 3264, 787, 490, 264, 28872, 321, 51540], "temperature": 0.0, "avg_logprob": -0.07193735099974133, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.0025896956212818623}, {"id": 200, "seek": 133496, "start": 1334.96, "end": 1342.56, "text": " provided so this is a way how to actually how you actually use your private data to teach LLM", "tokens": [50364, 5649, 370, 341, 307, 257, 636, 577, 281, 767, 577, 291, 767, 764, 428, 4551, 1412, 281, 2924, 441, 43, 44, 50744], "temperature": 0.0, "avg_logprob": -0.14610704253701604, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.003833622671663761}, {"id": 201, "seek": 133496, "start": 1342.56, "end": 1352.16, "text": " actually what should what should be the context for any incoming questions. So I hope you like this", "tokens": [50744, 767, 437, 820, 437, 820, 312, 264, 4319, 337, 604, 22341, 1651, 13, 407, 286, 1454, 291, 411, 341, 51224], "temperature": 0.0, "avg_logprob": -0.14610704253701604, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.003833622671663761}, {"id": 202, "seek": 133496, "start": 1352.16, "end": 1361.04, "text": " demo you can play with this notebook it's on our creative B examples repository and you also can see", "tokens": [51224, 10723, 291, 393, 862, 365, 341, 21060, 309, 311, 322, 527, 5880, 363, 5110, 25841, 293, 291, 611, 393, 536, 51668], "temperature": 0.0, "avg_logprob": -0.14610704253701604, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.003833622671663761}, {"id": 203, "seek": 136104, "start": 1361.12, "end": 1367.2, "text": " there are other similar notebooks for different different different types of examples for different", "tokens": [50368, 456, 366, 661, 2531, 43782, 337, 819, 819, 819, 3467, 295, 5110, 337, 819, 50672], "temperature": 0.0, "avg_logprob": -0.1142772697820896, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0013509112177416682}, {"id": 204, "seek": 136104, "start": 1367.92, "end": 1374.8799999999999, "text": " prompt engineering examples or like how to create another another form of chatbots how to use another", "tokens": [50708, 12391, 7043, 5110, 420, 411, 577, 281, 1884, 1071, 1071, 1254, 295, 5081, 65, 1971, 577, 281, 764, 1071, 51056], "temperature": 0.0, "avg_logprob": -0.1142772697820896, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0013509112177416682}, {"id": 205, "seek": 136104, "start": 1374.8799999999999, "end": 1382.8, "text": " embedding algorithms so please let us know what you think give us a feedback open a new issue on", "tokens": [51056, 12240, 3584, 14642, 370, 1767, 718, 505, 458, 437, 291, 519, 976, 505, 257, 5824, 1269, 257, 777, 2734, 322, 51452], "temperature": 0.0, "avg_logprob": -0.1142772697820896, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0013509112177416682}, {"id": 206, "seek": 136104, "start": 1382.8, "end": 1390.8799999999999, "text": " this repository and we are looking forward actually to work with you on these topics. So I think that", "tokens": [51452, 341, 25841, 293, 321, 366, 1237, 2128, 767, 281, 589, 365, 291, 322, 613, 8378, 13, 407, 286, 519, 300, 51856], "temperature": 0.0, "avg_logprob": -0.1142772697820896, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0013509112177416682}, {"id": 207, "seek": 139104, "start": 1391.76, "end": 1401.2, "text": " is all from us thank you for being part of this session maybe we have time for one question", "tokens": [50400, 307, 439, 490, 505, 1309, 291, 337, 885, 644, 295, 341, 5481, 1310, 321, 362, 565, 337, 472, 1168, 50872], "temperature": 0.0, "avg_logprob": -0.30455875396728516, "compression_ratio": 1.358695652173913, "no_speech_prob": 0.0036134275142103434}, {"id": 208, "seek": 139104, "start": 1403.84, "end": 1404.72, "text": " okay awesome", "tokens": [51004, 1392, 3476, 51048], "temperature": 0.0, "avg_logprob": -0.30455875396728516, "compression_ratio": 1.358695652173913, "no_speech_prob": 0.0036134275142103434}, {"id": 209, "seek": 139104, "start": 1416.96, "end": 1417.84, "text": " do we have questions", "tokens": [51660, 360, 321, 362, 1651, 51704], "temperature": 0.0, "avg_logprob": -0.30455875396728516, "compression_ratio": 1.358695652173913, "no_speech_prob": 0.0036134275142103434}, {"id": 210, "seek": 142104, "start": 1421.52, "end": 1423.04, "text": " anyone", "tokens": [50388, 2878, 50464], "temperature": 0.0, "avg_logprob": -0.30207440966651555, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.00532869528979063}, {"id": 211, "seek": 142104, "start": 1434.3999999999999, "end": 1440.0, "text": " thank you for the talk I have a question about the embeddings model because if you", "tokens": [51032, 1309, 291, 337, 264, 751, 286, 362, 257, 1168, 466, 264, 12240, 29432, 2316, 570, 498, 291, 51312], "temperature": 0.0, "avg_logprob": -0.30207440966651555, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.00532869528979063}, {"id": 212, "seek": 142104, "start": 1440.8799999999999, "end": 1447.12, "text": " encode prompt with language model and use external embeddings model they cannot be in", "tokens": [51356, 2058, 1429, 12391, 365, 2856, 2316, 293, 764, 8320, 12240, 29432, 2316, 436, 2644, 312, 294, 51668], "temperature": 0.0, "avg_logprob": -0.30207440966651555, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.00532869528979063}, {"id": 213, "seek": 144712, "start": 1447.12, "end": 1455.6799999999998, "text": " different spaces and if you do similarity search have you tested it and do you see the effect of", "tokens": [50364, 819, 7673, 293, 498, 291, 360, 32194, 3164, 362, 291, 8246, 309, 293, 360, 291, 536, 264, 1802, 295, 50792], "temperature": 0.0, "avg_logprob": -0.12283799171447754, "compression_ratio": 1.5401459854014599, "no_speech_prob": 0.006512598134577274}, {"id": 214, "seek": 144712, "start": 1456.3999999999999, "end": 1457.36, "text": " different embeddings", "tokens": [50828, 819, 12240, 29432, 50876], "temperature": 0.0, "avg_logprob": -0.12283799171447754, "compression_ratio": 1.5401459854014599, "no_speech_prob": 0.006512598134577274}, {"id": 215, "seek": 144712, "start": 1465.36, "end": 1471.04, "text": " I mean it's a very important question now if you the way you create these embeddings is super", "tokens": [51276, 286, 914, 309, 311, 257, 588, 1021, 1168, 586, 498, 291, 264, 636, 291, 1884, 613, 12240, 29432, 307, 1687, 51560], "temperature": 0.0, "avg_logprob": -0.12283799171447754, "compression_ratio": 1.5401459854014599, "no_speech_prob": 0.006512598134577274}, {"id": 216, "seek": 147104, "start": 1471.04, "end": 1476.32, "text": " important and you're usually limited to one embedding algorithm because you need to", "tokens": [50364, 1021, 293, 291, 434, 2673, 5567, 281, 472, 12240, 3584, 9284, 570, 291, 643, 281, 50628], "temperature": 0.0, "avg_logprob": -0.13635817612751877, "compression_ratio": 1.856, "no_speech_prob": 0.028684094548225403}, {"id": 217, "seek": 147104, "start": 1477.44, "end": 1481.84, "text": " they need to have the same length and obviously they need to capture the same semantics", "tokens": [50684, 436, 643, 281, 362, 264, 912, 4641, 293, 2745, 436, 643, 281, 7983, 264, 912, 4361, 45298, 50904], "temperature": 0.0, "avg_logprob": -0.13635817612751877, "compression_ratio": 1.856, "no_speech_prob": 0.028684094548225403}, {"id": 218, "seek": 147104, "start": 1481.84, "end": 1486.72, "text": " simplifying a bit and this is also what I meant with the customers that we work with they were able", "tokens": [50904, 6883, 5489, 257, 857, 293, 341, 307, 611, 437, 286, 4140, 365, 264, 4581, 300, 321, 589, 365, 436, 645, 1075, 51148], "temperature": 0.0, "avg_logprob": -0.13635817612751877, "compression_ratio": 1.856, "no_speech_prob": 0.028684094548225403}, {"id": 219, "seek": 147104, "start": 1486.72, "end": 1492.24, "text": " to create different indexes right and then the retriever gets more and more complex as you've seen", "tokens": [51148, 281, 1884, 819, 8186, 279, 558, 293, 550, 264, 19817, 331, 2170, 544, 293, 544, 3997, 382, 291, 600, 1612, 51424], "temperature": 0.0, "avg_logprob": -0.13635817612751877, "compression_ratio": 1.856, "no_speech_prob": 0.028684094548225403}, {"id": 220, "seek": 147104, "start": 1492.24, "end": 1497.04, "text": " on this architecture slide this is a simplified example you maybe you need to query different", "tokens": [51424, 322, 341, 9482, 4137, 341, 307, 257, 26335, 1365, 291, 1310, 291, 643, 281, 14581, 819, 51664], "temperature": 0.0, "avg_logprob": -0.13635817612751877, "compression_ratio": 1.856, "no_speech_prob": 0.028684094548225403}, {"id": 221, "seek": 149704, "start": 1497.36, "end": 1503.04, "text": " different indexes created by different embedding algorithms you know so that you can search your", "tokens": [50380, 819, 8186, 279, 2942, 538, 819, 12240, 3584, 14642, 291, 458, 370, 300, 291, 393, 3164, 428, 50664], "temperature": 0.0, "avg_logprob": -0.21432125325105628, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.0017705319914966822}, {"id": 222, "seek": 149704, "start": 1503.04, "end": 1507.52, "text": " images you can search your textual data right obviously you might use different things there", "tokens": [50664, 5267, 291, 393, 3164, 428, 2487, 901, 1412, 558, 2745, 291, 1062, 764, 819, 721, 456, 50888], "temperature": 0.0, "avg_logprob": -0.21432125325105628, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.0017705319914966822}, {"id": 223, "seek": 149704, "start": 1508.3999999999999, "end": 1514.3999999999999, "text": " and then re-rank the results come up with the really relevant context maybe from different", "tokens": [50932, 293, 550, 319, 12, 20479, 264, 3542, 808, 493, 365, 264, 534, 7340, 4319, 1310, 490, 819, 51232], "temperature": 0.0, "avg_logprob": -0.21432125325105628, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.0017705319914966822}, {"id": 224, "seek": 149704, "start": 1514.3999999999999, "end": 1518.6399999999999, "text": " indexes and maybe you also want to combine it with a full text search or limit it to", "tokens": [51232, 8186, 279, 293, 1310, 291, 611, 528, 281, 10432, 309, 365, 257, 1577, 2487, 3164, 420, 4948, 309, 281, 51444], "temperature": 0.0, "avg_logprob": -0.21432125325105628, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.0017705319914966822}, {"id": 225, "seek": 149704, "start": 1519.2, "end": 1523.76, "text": " customer support tickets from Europe trying to come up with a good example there are or to", "tokens": [51472, 5474, 1406, 12628, 490, 3315, 1382, 281, 808, 493, 365, 257, 665, 1365, 456, 366, 420, 281, 51700], "temperature": 0.0, "avg_logprob": -0.21432125325105628, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.0017705319914966822}, {"id": 226, "seek": 152376, "start": 1524.32, "end": 1530.32, "text": " customers support tickets from the US with some geospatial inhibition but this is then the re-ranking", "tokens": [50392, 4581, 1406, 12628, 490, 264, 2546, 365, 512, 1519, 2763, 267, 831, 20406, 849, 457, 341, 307, 550, 264, 319, 12, 20479, 278, 50692], "temperature": 0.0, "avg_logprob": -0.24707941327776228, "compression_ratio": 1.6611111111111112, "no_speech_prob": 0.009126225486397743}, {"id": 227, "seek": 152376, "start": 1530.32, "end": 1537.76, "text": " of the results that really identifies the particular context that is really relevant for the question", "tokens": [50692, 295, 264, 3542, 300, 534, 34597, 264, 1729, 4319, 300, 307, 534, 7340, 337, 264, 1168, 51064], "temperature": 0.0, "avg_logprob": -0.24707941327776228, "compression_ratio": 1.6611111111111112, "no_speech_prob": 0.009126225486397743}, {"id": 228, "seek": 152376, "start": 1538.8, "end": 1539.68, "text": " okay thanks a lot", "tokens": [51116, 1392, 3231, 257, 688, 51160], "temperature": 0.0, "avg_logprob": -0.24707941327776228, "compression_ratio": 1.6611111111111112, "no_speech_prob": 0.009126225486397743}, {"id": 229, "seek": 152376, "start": 1542.48, "end": 1543.28, "text": " any more questions", "tokens": [51300, 604, 544, 1651, 51340], "temperature": 0.0, "avg_logprob": -0.24707941327776228, "compression_ratio": 1.6611111111111112, "no_speech_prob": 0.009126225486397743}, {"id": 230, "seek": 152376, "start": 1546.8799999999999, "end": 1551.44, "text": " no so thank you very much for the very nice talk thank you", "tokens": [51520, 572, 370, 1309, 291, 588, 709, 337, 264, 588, 1481, 751, 1309, 291, 51748], "temperature": 0.0, "avg_logprob": -0.24707941327776228, "compression_ratio": 1.6611111111111112, "no_speech_prob": 0.009126225486397743}, {"id": 231, "seek": 155376, "start": 1553.76, "end": 1554.98, "text": " you", "tokens": [50400, 291, 50425], "temperature": 0.0, "avg_logprob": -0.7815516591072083, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.3599032461643219}], "language": "en"}