{"text": " Hi everyone. Thank you for having some patience with me. Computers are not my strong suit, although I am in IT. So my name is Sunita Rijder. I am the Community Manager of Open Staff. And I work at Alieander. So let's get into a little bit of background. So Alieander is a distributed grid operator. So we are responsible for the distribution of energy in both electricity and gas in about a third of the Netherlands. So I think we all know these kinds of gas. So this is energy consumption on some place in the Netherlands. However, we have no idea what's going to happen in the future. Well, this is where Open Staff comes in. Open Staff stands for Open Short-term Energy Forecast. So instead of our question mark, we actually know what's going to happen. So after this very short introduction, let's tell, let's talk about what I'm going to talk about today. So first of all, I'll start with the challenges on the grid and why we actually need Open Staff. Then I'll talk about Open Staff, of course. And finally, I really want to discuss our recent developments and collaborations. So the challenges on the grid. So when everything was still good and easy on the electricity grid, it looks like this. So on the left, you see one big producer, just one direction energy flow, and then we have our consumers. Fairly easy. However, due to the energy transition, I think you're all aware, it looks like this. So very chaotic. So on the production side, we have a distributed production due to our solar and wind, both on the mid and low voltage, but also at our consumers. And on the consumption side, we have the issue that our consumption has exponentially increased. We had a lot about EV charging over here. Well, those electrical vehicles need electricity through the grid. And this is where our capacity issues start. So this is a map of the Netherlands. And I think you can all guess that red is bad. So on the red parts, we actually have no capacity available. So let's say that you want to start a company of one of these areas, we cannot connect you. So you get no power from us because we just simply have none to give. But of course, we're all very smart and over the people. So we have some solutions. So one of these solutions is actually to shave the peak. If we expect grid limitations to be surpassed. So on this left image, you see a forecast on the load on, for example, a transformer. We see a very clear peak. And this is where our grid limitations are surpassed. So our solution is just to say shave the peak. So for example, if this is production, we just ask one of our solar farms to just shut off for a little while. Of course, they can money for this, but that's something else. And then this is the result. So our grid limitations are not surpassed and nothing breaks. So great. But then able to do this, we do need to know just left image. So we actually need accurate forecasts. And this is where we have open step. So again, open step stands for open short term energy forecasting. And let me a little bit explain a little bit more about it. So first of all, what the hell is it? Well, it's a complete software stack to forecast the load on the electricity grid. But it's energy forecast. So it could also do it for heat. And it's automated machine learning pipelines. So it's a step by step process, which is automated to make a forecast. So in these dark blue boxes is all everything that open stuff can do. And I'll talk a little bit more about it. So what does the software look like? So first of all, you need a database. This is one that you have to make yourself, of course. But we do have open step DBC, open step database connector. And this is able to get all of your data from your database. And then we get into open step already talked about pipelines. So of course, these are in the software overview. And these are part of the tasks orchestration. Then we have data preprocessing, which includes data validation. So for example, if you see a little flat line, as we're able to cancel these out of your input data, and there was something very interesting, feature engineering. So in this feature engineering, we're, for example, able to calculate the wind speed at the height of a windmill from the wind speed on the ground. And we're also able to calculate the lag load for one time stamp. And then of course, we're machine learning pipelines. So we have some machine learning in there. So we're using open source models such as XG boost to make our machine learning models. So we're able to train, optimize hyper parameters, of course, make a forecast. And we're also able to make a split forecast to our Dazzles model. And finally, we are able to evaluate our forecasts, store our model, and do some post processing. So let's look into the methodology on a really high level. So on the left, we have our target load. This is where we actually want to forecast. Then we have some external predictors. So we have our weather forecast, market prices, and typical profiles of companies and households. From these external predictors, we can actually calculate our derived features. So this is the feature engineering I just talked about. So we're able to calculate lag loads for each time stamp, but also to have some derived weather features, such as, for example, the wind speed at the height of a windmill. And for the more calendar info, it really matters if you're are forecasting on a Sunday or Christmas compared to a Monday. And then we can train a single model for all our lead times. So here you can see what the data, for example, looks like. So if a daytime with increments of 15 minutes, our targets, and external predictors, you can also see here that we have the Dutch energy prices in there. So if you have multiple training horizons, we just simply do pick late our data and use this for our training horizons. And then if there are questions about it, please ask me in the break, but I don't have time to go into this in 15 minutes. We can with our trained model now actually make this forecast. And of course, we want it to look nice. So we have this beautiful Grafana dashboard, which actually summarizes all of the information that you need for your forecast. So let's look into it. First and foremost, our forecast. So the red line on the left is actually the low that has been historically measured. And then we see here the yellow lines is our forecast. Well, now you see that there are a lot of yellow lines. What do those mean? Well, those are actually the quantiles. So you have actually a certainty in your forecast. And this can be actually useful if we're a certain location. You're quite sure what your forecast is going to be. You can go into another quantile. Then if you have a location where you have a lot of factors that you actually don't know anything about. And also very nice our feature importance plot. So here in the feature importance plot, we can see our lag loads and some other features. And this is actually nice. So you can see for every location, which features are important for your forecast. So for example, here we see radiation. I don't think it's readable for you, but it says radiation. So you know that there are quite some solar parks or solar panels behind, for example, your substation. Wind speeds nowhere to be seen. So probably no windmills in that area. So this was really short about open staff. Let me see how much time I have left. Six minutes. Perfect. Okay. So community and upcoming events. One of the main things that has really changed in open staff this last year is our community. So before it was just Alliander who actually created it together with RTE, working on open staff. And now it looks like this. So let me go over every company really quick. So Alliander, that's where I'm from, talked about that enough. RT actually working on open staff for quite a while and they're actually ready to implement it very soon. RT International just joined us this year. They have a very nice proof of concept and they're going to work on it further. Fidel has actually been using open staff quite a long. I've heard some terms, leeches this today. Well, that was a feed on up to actually a month ago. So we contacted them and they were like, oh yeah, we found some bugs. We fixed it. We can implement this. So they actually joined our community as of this year. Sigelman still working on a proof of concept and seeing if they want to replace their own forecasting model with open staff. And Shell is working on open staff DBC and seeing if they can use their method of data important. Now, I hope everyone feels like they want to try open staff. Well, you're in luck because we are organizing a workshop. So on the Friday, the first of March from two to four, we were organizing a workshop. And I would like everyone who's interested to join. So you'll get a better introduction to open staff and also a little bit more of the technical details. It will be virtual. And you will get really a hands on experience. So you get some example notebooks from us where you have to make your own exercises and you can actually make your own forecast with open staff and see how easy it is. If you want to sign up, just scan the QR card over here. And it will be very nice. I also have it on the next slide for people who are too slow. So want to know more about open staff, maybe even before you sign up for the workshop, we of course have our GitHub website documentation, etc. You're only one command away from using open staff. And if there's anything you want to ask or give some comments or anything, you can just send me an email or send me a message on LinkedIn. So thank you for your time and I welcome any questions. Who's running the microphone? I'll try to do my best. Please feel free to guess to find the best path. Hello. First of all, thank you so much. This was very interesting. And I have no experience, I have never heard of open staff before reading on the FOSTA website. I have one question about the data collection. Do you provide like some examples or standards on how and where to fetch data because the data source is very, I tried, I looked. So very good question, I think this is something that a community indeed struggles with. So for the Netherlands, we actually do have those sources because we are using them ourselves for other countries who are working on it to see if we can find some open data for everyone. But if you're interested, you can always send me an email and I'll see what we have. Yeah, great. Hi, it's Min\u00e9. I'm from Red Hat. So obviously I will ask the question about scaling this, right? How will you standardize and scale this because it's a project. It sounds super interesting. But how are we going to scale this to 49,000 substations or millions of smart meters at home? Very good question. This is actually something we're working on right now. So we are actually employing our open step stack on Dexter probably anytime soon and seeing if you can actually scale from that. Currently we have it scaled up at I think 100 substations. And if you're curious how we have a reference implementation on our GitHub and you can see all the information there on how we deploy this. Thanks. Yeah, yeah, sure. I have a question about the data sources. Is there any thought given to adding geographical information systems data into the system for forecasting models? Because especially stuff like wind and solar radiation also not just depend on the time of day and the wind speeds, but the location itself. Great question. Yeah, actually for our system, it just connects to the closest K and MI. So that's the Royal Dutch Weather Organization. So it's able to find the closest station to where you actually want to forecast. So it definitely takes a location to account. We have a prediction job class where you can put in all of the information for your forecast and in there you also put the latitude and longitude of your location. So it does take that into account. Question over there. Thanks for the question about the geographic data because I was thinking about an approach of just using cheap raspberry weather stations in Austria and distributing them across some locations to fetch the data because I have the Google Weather API and the Open Weather API or whatever as comparison values. And for the geographic thing, thanks for the question. How would you connect that? Like is this a plan of open stuff? Did I miss this? Yeah, thanks for the kind of difficult question because I don't know the answer. So I'll ask my colleagues who actually made this part of the open stuff and I'll get back to you if we connect afterwards. So then you'll know. But it's very interesting to do with the Raspberry Price things. Thanks.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.76, "text": " Hi everyone. Thank you for having some patience with me. Computers are not my strong suit,", "tokens": [50364, 2421, 1518, 13, 1044, 291, 337, 1419, 512, 14826, 365, 385, 13, 37804, 433, 366, 406, 452, 2068, 5722, 11, 50902], "temperature": 0.0, "avg_logprob": -0.30240208615538894, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.17493285238742828}, {"id": 1, "seek": 0, "start": 10.76, "end": 17.240000000000002, "text": " although I am in IT. So my name is Sunita Rijder. I am the Community Manager of Open", "tokens": [50902, 4878, 286, 669, 294, 6783, 13, 407, 452, 1315, 307, 6163, 2786, 497, 1718, 1068, 13, 286, 669, 264, 10421, 13821, 295, 7238, 51226], "temperature": 0.0, "avg_logprob": -0.30240208615538894, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.17493285238742828}, {"id": 2, "seek": 0, "start": 17.240000000000002, "end": 23.64, "text": " Staff. And I work at Alieander. So let's get into a little bit of background. So Alieander", "tokens": [51226, 16440, 13, 400, 286, 589, 412, 967, 414, 4483, 13, 407, 718, 311, 483, 666, 257, 707, 857, 295, 3678, 13, 407, 967, 414, 4483, 51546], "temperature": 0.0, "avg_logprob": -0.30240208615538894, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.17493285238742828}, {"id": 3, "seek": 0, "start": 23.64, "end": 29.16, "text": " is a distributed grid operator. So we are responsible for the distribution of energy", "tokens": [51546, 307, 257, 12631, 10748, 12973, 13, 407, 321, 366, 6250, 337, 264, 7316, 295, 2281, 51822], "temperature": 0.0, "avg_logprob": -0.30240208615538894, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.17493285238742828}, {"id": 4, "seek": 2916, "start": 29.16, "end": 36.92, "text": " in both electricity and gas in about a third of the Netherlands. So I think we all know", "tokens": [50364, 294, 1293, 10356, 293, 4211, 294, 466, 257, 2636, 295, 264, 20873, 13, 407, 286, 519, 321, 439, 458, 50752], "temperature": 0.0, "avg_logprob": -0.153535626151345, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.029923424124717712}, {"id": 5, "seek": 2916, "start": 36.92, "end": 41.96, "text": " these kinds of gas. So this is energy consumption on some place in the Netherlands. However,", "tokens": [50752, 613, 3685, 295, 4211, 13, 407, 341, 307, 2281, 12126, 322, 512, 1081, 294, 264, 20873, 13, 2908, 11, 51004], "temperature": 0.0, "avg_logprob": -0.153535626151345, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.029923424124717712}, {"id": 6, "seek": 2916, "start": 41.96, "end": 46.44, "text": " we have no idea what's going to happen in the future. Well, this is where Open Staff", "tokens": [51004, 321, 362, 572, 1558, 437, 311, 516, 281, 1051, 294, 264, 2027, 13, 1042, 11, 341, 307, 689, 7238, 16440, 51228], "temperature": 0.0, "avg_logprob": -0.153535626151345, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.029923424124717712}, {"id": 7, "seek": 2916, "start": 46.44, "end": 53.400000000000006, "text": " comes in. Open Staff stands for Open Short-term Energy Forecast. So instead of our question", "tokens": [51228, 1487, 294, 13, 7238, 16440, 7382, 337, 7238, 16881, 12, 7039, 14939, 9018, 3734, 13, 407, 2602, 295, 527, 1168, 51576], "temperature": 0.0, "avg_logprob": -0.153535626151345, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.029923424124717712}, {"id": 8, "seek": 5340, "start": 53.4, "end": 59.64, "text": " mark, we actually know what's going to happen. So after this very short introduction, let's", "tokens": [50364, 1491, 11, 321, 767, 458, 437, 311, 516, 281, 1051, 13, 407, 934, 341, 588, 2099, 9339, 11, 718, 311, 50676], "temperature": 0.0, "avg_logprob": -0.13298848697117396, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.04449200630187988}, {"id": 9, "seek": 5340, "start": 59.64, "end": 63.879999999999995, "text": " tell, let's talk about what I'm going to talk about today. So first of all, I'll start with", "tokens": [50676, 980, 11, 718, 311, 751, 466, 437, 286, 478, 516, 281, 751, 466, 965, 13, 407, 700, 295, 439, 11, 286, 603, 722, 365, 50888], "temperature": 0.0, "avg_logprob": -0.13298848697117396, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.04449200630187988}, {"id": 10, "seek": 5340, "start": 63.879999999999995, "end": 69.16, "text": " the challenges on the grid and why we actually need Open Staff. Then I'll talk about Open", "tokens": [50888, 264, 4759, 322, 264, 10748, 293, 983, 321, 767, 643, 7238, 16440, 13, 1396, 286, 603, 751, 466, 7238, 51152], "temperature": 0.0, "avg_logprob": -0.13298848697117396, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.04449200630187988}, {"id": 11, "seek": 5340, "start": 69.16, "end": 76.72, "text": " Staff, of course. And finally, I really want to discuss our recent developments and collaborations.", "tokens": [51152, 16440, 11, 295, 1164, 13, 400, 2721, 11, 286, 534, 528, 281, 2248, 527, 5162, 20862, 293, 36908, 13, 51530], "temperature": 0.0, "avg_logprob": -0.13298848697117396, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.04449200630187988}, {"id": 12, "seek": 5340, "start": 76.72, "end": 82.6, "text": " So the challenges on the grid. So when everything was still good and easy on the electricity", "tokens": [51530, 407, 264, 4759, 322, 264, 10748, 13, 407, 562, 1203, 390, 920, 665, 293, 1858, 322, 264, 10356, 51824], "temperature": 0.0, "avg_logprob": -0.13298848697117396, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.04449200630187988}, {"id": 13, "seek": 8260, "start": 82.6, "end": 87.32, "text": " grid, it looks like this. So on the left, you see one big producer, just one direction", "tokens": [50364, 10748, 11, 309, 1542, 411, 341, 13, 407, 322, 264, 1411, 11, 291, 536, 472, 955, 12314, 11, 445, 472, 3513, 50600], "temperature": 0.0, "avg_logprob": -0.10034780666745942, "compression_ratio": 1.889344262295082, "no_speech_prob": 0.018606455996632576}, {"id": 14, "seek": 8260, "start": 87.32, "end": 93.91999999999999, "text": " energy flow, and then we have our consumers. Fairly easy. However, due to the energy transition,", "tokens": [50600, 2281, 3095, 11, 293, 550, 321, 362, 527, 11883, 13, 12157, 356, 1858, 13, 2908, 11, 3462, 281, 264, 2281, 6034, 11, 50930], "temperature": 0.0, "avg_logprob": -0.10034780666745942, "compression_ratio": 1.889344262295082, "no_speech_prob": 0.018606455996632576}, {"id": 15, "seek": 8260, "start": 93.91999999999999, "end": 100.19999999999999, "text": " I think you're all aware, it looks like this. So very chaotic. So on the production side,", "tokens": [50930, 286, 519, 291, 434, 439, 3650, 11, 309, 1542, 411, 341, 13, 407, 588, 27013, 13, 407, 322, 264, 4265, 1252, 11, 51244], "temperature": 0.0, "avg_logprob": -0.10034780666745942, "compression_ratio": 1.889344262295082, "no_speech_prob": 0.018606455996632576}, {"id": 16, "seek": 8260, "start": 100.19999999999999, "end": 105.88, "text": " we have a distributed production due to our solar and wind, both on the mid and low voltage,", "tokens": [51244, 321, 362, 257, 12631, 4265, 3462, 281, 527, 7936, 293, 2468, 11, 1293, 322, 264, 2062, 293, 2295, 8352, 11, 51528], "temperature": 0.0, "avg_logprob": -0.10034780666745942, "compression_ratio": 1.889344262295082, "no_speech_prob": 0.018606455996632576}, {"id": 17, "seek": 8260, "start": 105.88, "end": 111.24, "text": " but also at our consumers. And on the consumption side, we have the issue that our consumption", "tokens": [51528, 457, 611, 412, 527, 11883, 13, 400, 322, 264, 12126, 1252, 11, 321, 362, 264, 2734, 300, 527, 12126, 51796], "temperature": 0.0, "avg_logprob": -0.10034780666745942, "compression_ratio": 1.889344262295082, "no_speech_prob": 0.018606455996632576}, {"id": 18, "seek": 11124, "start": 111.24, "end": 116.44, "text": " has exponentially increased. We had a lot about EV charging over here. Well, those electrical", "tokens": [50364, 575, 37330, 6505, 13, 492, 632, 257, 688, 466, 15733, 11379, 670, 510, 13, 1042, 11, 729, 12147, 50624], "temperature": 0.0, "avg_logprob": -0.1193252541553015, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.01466019544750452}, {"id": 19, "seek": 11124, "start": 116.44, "end": 125.36, "text": " vehicles need electricity through the grid. And this is where our capacity issues start.", "tokens": [50624, 8948, 643, 10356, 807, 264, 10748, 13, 400, 341, 307, 689, 527, 6042, 2663, 722, 13, 51070], "temperature": 0.0, "avg_logprob": -0.1193252541553015, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.01466019544750452}, {"id": 20, "seek": 11124, "start": 125.36, "end": 130.6, "text": " So this is a map of the Netherlands. And I think you can all guess that red is bad. So", "tokens": [51070, 407, 341, 307, 257, 4471, 295, 264, 20873, 13, 400, 286, 519, 291, 393, 439, 2041, 300, 2182, 307, 1578, 13, 407, 51332], "temperature": 0.0, "avg_logprob": -0.1193252541553015, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.01466019544750452}, {"id": 21, "seek": 11124, "start": 130.6, "end": 135.95999999999998, "text": " on the red parts, we actually have no capacity available. So let's say that you want to start", "tokens": [51332, 322, 264, 2182, 3166, 11, 321, 767, 362, 572, 6042, 2435, 13, 407, 718, 311, 584, 300, 291, 528, 281, 722, 51600], "temperature": 0.0, "avg_logprob": -0.1193252541553015, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.01466019544750452}, {"id": 22, "seek": 13596, "start": 136.04000000000002, "end": 141.52, "text": " a company of one of these areas, we cannot connect you. So you get no power from us because", "tokens": [50368, 257, 2237, 295, 472, 295, 613, 3179, 11, 321, 2644, 1745, 291, 13, 407, 291, 483, 572, 1347, 490, 505, 570, 50642], "temperature": 0.0, "avg_logprob": -0.15773215915845787, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.10417205840349197}, {"id": 23, "seek": 13596, "start": 141.52, "end": 147.28, "text": " we just simply have none to give. But of course, we're all very smart and over the people.", "tokens": [50642, 321, 445, 2935, 362, 6022, 281, 976, 13, 583, 295, 1164, 11, 321, 434, 439, 588, 4069, 293, 670, 264, 561, 13, 50930], "temperature": 0.0, "avg_logprob": -0.15773215915845787, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.10417205840349197}, {"id": 24, "seek": 13596, "start": 147.28, "end": 153.08, "text": " So we have some solutions. So one of these solutions is actually to shave the peak. If", "tokens": [50930, 407, 321, 362, 512, 6547, 13, 407, 472, 295, 613, 6547, 307, 767, 281, 25544, 264, 10651, 13, 759, 51220], "temperature": 0.0, "avg_logprob": -0.15773215915845787, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.10417205840349197}, {"id": 25, "seek": 13596, "start": 153.08, "end": 160.0, "text": " we expect grid limitations to be surpassed. So on this left image, you see a forecast", "tokens": [51220, 321, 2066, 10748, 15705, 281, 312, 27650, 292, 13, 407, 322, 341, 1411, 3256, 11, 291, 536, 257, 14330, 51566], "temperature": 0.0, "avg_logprob": -0.15773215915845787, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.10417205840349197}, {"id": 26, "seek": 13596, "start": 160.0, "end": 165.20000000000002, "text": " on the load on, for example, a transformer. We see a very clear peak. And this is where", "tokens": [51566, 322, 264, 3677, 322, 11, 337, 1365, 11, 257, 31782, 13, 492, 536, 257, 588, 1850, 10651, 13, 400, 341, 307, 689, 51826], "temperature": 0.0, "avg_logprob": -0.15773215915845787, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.10417205840349197}, {"id": 27, "seek": 16520, "start": 165.2, "end": 172.16, "text": " our grid limitations are surpassed. So our solution is just to say shave the peak. So", "tokens": [50364, 527, 10748, 15705, 366, 27650, 292, 13, 407, 527, 3827, 307, 445, 281, 584, 25544, 264, 10651, 13, 407, 50712], "temperature": 0.0, "avg_logprob": -0.19333443274864784, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.004735610447824001}, {"id": 28, "seek": 16520, "start": 172.16, "end": 177.04, "text": " for example, if this is production, we just ask one of our solar farms to just shut off", "tokens": [50712, 337, 1365, 11, 498, 341, 307, 4265, 11, 321, 445, 1029, 472, 295, 527, 7936, 20366, 281, 445, 5309, 766, 50956], "temperature": 0.0, "avg_logprob": -0.19333443274864784, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.004735610447824001}, {"id": 29, "seek": 16520, "start": 177.04, "end": 182.23999999999998, "text": " for a little while. Of course, they can money for this, but that's something else. And then", "tokens": [50956, 337, 257, 707, 1339, 13, 2720, 1164, 11, 436, 393, 1460, 337, 341, 11, 457, 300, 311, 746, 1646, 13, 400, 550, 51216], "temperature": 0.0, "avg_logprob": -0.19333443274864784, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.004735610447824001}, {"id": 30, "seek": 16520, "start": 182.23999999999998, "end": 189.67999999999998, "text": " this is the result. So our grid limitations are not surpassed and nothing breaks. So great.", "tokens": [51216, 341, 307, 264, 1874, 13, 407, 527, 10748, 15705, 366, 406, 27650, 292, 293, 1825, 9857, 13, 407, 869, 13, 51588], "temperature": 0.0, "avg_logprob": -0.19333443274864784, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.004735610447824001}, {"id": 31, "seek": 18968, "start": 190.64000000000001, "end": 198.32, "text": " But then able to do this, we do need to know just left image. So we actually need accurate forecasts.", "tokens": [50412, 583, 550, 1075, 281, 360, 341, 11, 321, 360, 643, 281, 458, 445, 1411, 3256, 13, 407, 321, 767, 643, 8559, 49421, 13, 50796], "temperature": 0.0, "avg_logprob": -0.13757162905753928, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.009145929478108883}, {"id": 32, "seek": 18968, "start": 200.08, "end": 208.0, "text": " And this is where we have open step. So again, open step stands for open short term energy", "tokens": [50884, 400, 341, 307, 689, 321, 362, 1269, 1823, 13, 407, 797, 11, 1269, 1823, 7382, 337, 1269, 2099, 1433, 2281, 51280], "temperature": 0.0, "avg_logprob": -0.13757162905753928, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.009145929478108883}, {"id": 33, "seek": 18968, "start": 208.0, "end": 213.92000000000002, "text": " forecasting. And let me a little bit explain a little bit more about it. So first of all,", "tokens": [51280, 44331, 13, 400, 718, 385, 257, 707, 857, 2903, 257, 707, 857, 544, 466, 309, 13, 407, 700, 295, 439, 11, 51576], "temperature": 0.0, "avg_logprob": -0.13757162905753928, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.009145929478108883}, {"id": 34, "seek": 18968, "start": 213.92000000000002, "end": 218.8, "text": " what the hell is it? Well, it's a complete software stack to forecast the load on the", "tokens": [51576, 437, 264, 4921, 307, 309, 30, 1042, 11, 309, 311, 257, 3566, 4722, 8630, 281, 14330, 264, 3677, 322, 264, 51820], "temperature": 0.0, "avg_logprob": -0.13757162905753928, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.009145929478108883}, {"id": 35, "seek": 21880, "start": 218.88000000000002, "end": 222.56, "text": " electricity grid. But it's energy forecast. So it could also do it for heat.", "tokens": [50368, 10356, 10748, 13, 583, 309, 311, 2281, 14330, 13, 407, 309, 727, 611, 360, 309, 337, 3738, 13, 50552], "temperature": 0.0, "avg_logprob": -0.11240904906700397, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.002358795143663883}, {"id": 36, "seek": 21880, "start": 223.76000000000002, "end": 228.96, "text": " And it's automated machine learning pipelines. So it's a step by step process, which is automated", "tokens": [50612, 400, 309, 311, 18473, 3479, 2539, 40168, 13, 407, 309, 311, 257, 1823, 538, 1823, 1399, 11, 597, 307, 18473, 50872], "temperature": 0.0, "avg_logprob": -0.11240904906700397, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.002358795143663883}, {"id": 37, "seek": 21880, "start": 228.96, "end": 237.04000000000002, "text": " to make a forecast. So in these dark blue boxes is all everything that open stuff can do. And", "tokens": [50872, 281, 652, 257, 14330, 13, 407, 294, 613, 2877, 3344, 9002, 307, 439, 1203, 300, 1269, 1507, 393, 360, 13, 400, 51276], "temperature": 0.0, "avg_logprob": -0.11240904906700397, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.002358795143663883}, {"id": 38, "seek": 21880, "start": 237.04000000000002, "end": 244.0, "text": " I'll talk a little bit more about it. So what does the software look like? So first of all,", "tokens": [51276, 286, 603, 751, 257, 707, 857, 544, 466, 309, 13, 407, 437, 775, 264, 4722, 574, 411, 30, 407, 700, 295, 439, 11, 51624], "temperature": 0.0, "avg_logprob": -0.11240904906700397, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.002358795143663883}, {"id": 39, "seek": 21880, "start": 244.0, "end": 248.4, "text": " you need a database. This is one that you have to make yourself, of course. But we do have", "tokens": [51624, 291, 643, 257, 8149, 13, 639, 307, 472, 300, 291, 362, 281, 652, 1803, 11, 295, 1164, 13, 583, 321, 360, 362, 51844], "temperature": 0.0, "avg_logprob": -0.11240904906700397, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.002358795143663883}, {"id": 40, "seek": 24840, "start": 248.48000000000002, "end": 254.96, "text": " open step DBC, open step database connector. And this is able to get all of your data from your", "tokens": [50368, 1269, 1823, 413, 7869, 11, 1269, 1823, 8149, 19127, 13, 400, 341, 307, 1075, 281, 483, 439, 295, 428, 1412, 490, 428, 50692], "temperature": 0.0, "avg_logprob": -0.16325499086963888, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.001980820205062628}, {"id": 41, "seek": 24840, "start": 254.96, "end": 262.0, "text": " database. And then we get into open step already talked about pipelines. So of course, these are", "tokens": [50692, 8149, 13, 400, 550, 321, 483, 666, 1269, 1823, 1217, 2825, 466, 40168, 13, 407, 295, 1164, 11, 613, 366, 51044], "temperature": 0.0, "avg_logprob": -0.16325499086963888, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.001980820205062628}, {"id": 42, "seek": 24840, "start": 262.0, "end": 269.04, "text": " in the software overview. And these are part of the tasks orchestration. Then we have data preprocessing,", "tokens": [51044, 294, 264, 4722, 12492, 13, 400, 613, 366, 644, 295, 264, 9608, 14161, 2405, 13, 1396, 321, 362, 1412, 2666, 340, 780, 278, 11, 51396], "temperature": 0.0, "avg_logprob": -0.16325499086963888, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.001980820205062628}, {"id": 43, "seek": 24840, "start": 269.92, "end": 275.04, "text": " which includes data validation. So for example, if you see a little flat line, as we're able to", "tokens": [51440, 597, 5974, 1412, 24071, 13, 407, 337, 1365, 11, 498, 291, 536, 257, 707, 4962, 1622, 11, 382, 321, 434, 1075, 281, 51696], "temperature": 0.0, "avg_logprob": -0.16325499086963888, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.001980820205062628}, {"id": 44, "seek": 27504, "start": 275.68, "end": 281.12, "text": " cancel these out of your input data, and there was something very interesting, feature engineering.", "tokens": [50396, 10373, 613, 484, 295, 428, 4846, 1412, 11, 293, 456, 390, 746, 588, 1880, 11, 4111, 7043, 13, 50668], "temperature": 0.0, "avg_logprob": -0.14186905978018777, "compression_ratio": 1.85, "no_speech_prob": 0.0031121657229959965}, {"id": 45, "seek": 27504, "start": 281.12, "end": 285.68, "text": " So in this feature engineering, we're, for example, able to calculate the wind speed at the height of", "tokens": [50668, 407, 294, 341, 4111, 7043, 11, 321, 434, 11, 337, 1365, 11, 1075, 281, 8873, 264, 2468, 3073, 412, 264, 6681, 295, 50896], "temperature": 0.0, "avg_logprob": -0.14186905978018777, "compression_ratio": 1.85, "no_speech_prob": 0.0031121657229959965}, {"id": 46, "seek": 27504, "start": 285.68, "end": 291.92, "text": " a windmill from the wind speed on the ground. And we're also able to calculate the lag load", "tokens": [50896, 257, 2468, 18841, 490, 264, 2468, 3073, 322, 264, 2727, 13, 400, 321, 434, 611, 1075, 281, 8873, 264, 8953, 3677, 51208], "temperature": 0.0, "avg_logprob": -0.14186905978018777, "compression_ratio": 1.85, "no_speech_prob": 0.0031121657229959965}, {"id": 47, "seek": 27504, "start": 291.92, "end": 298.08000000000004, "text": " for one time stamp. And then of course, we're machine learning pipelines. So we have some", "tokens": [51208, 337, 472, 565, 9921, 13, 400, 550, 295, 1164, 11, 321, 434, 3479, 2539, 40168, 13, 407, 321, 362, 512, 51516], "temperature": 0.0, "avg_logprob": -0.14186905978018777, "compression_ratio": 1.85, "no_speech_prob": 0.0031121657229959965}, {"id": 48, "seek": 27504, "start": 298.08000000000004, "end": 303.36, "text": " machine learning in there. So we're using open source models such as XG boost to make our machine", "tokens": [51516, 3479, 2539, 294, 456, 13, 407, 321, 434, 1228, 1269, 4009, 5245, 1270, 382, 1783, 38, 9194, 281, 652, 527, 3479, 51780], "temperature": 0.0, "avg_logprob": -0.14186905978018777, "compression_ratio": 1.85, "no_speech_prob": 0.0031121657229959965}, {"id": 49, "seek": 30336, "start": 303.36, "end": 309.6, "text": " learning models. So we're able to train, optimize hyper parameters, of course, make a forecast.", "tokens": [50364, 2539, 5245, 13, 407, 321, 434, 1075, 281, 3847, 11, 19719, 9848, 9834, 11, 295, 1164, 11, 652, 257, 14330, 13, 50676], "temperature": 0.0, "avg_logprob": -0.11119076640335555, "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.0012080171145498753}, {"id": 50, "seek": 30336, "start": 309.6, "end": 315.2, "text": " And we're also able to make a split forecast to our Dazzles model. And finally, we are able to", "tokens": [50676, 400, 321, 434, 611, 1075, 281, 652, 257, 7472, 14330, 281, 527, 413, 9112, 904, 2316, 13, 400, 2721, 11, 321, 366, 1075, 281, 50956], "temperature": 0.0, "avg_logprob": -0.11119076640335555, "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.0012080171145498753}, {"id": 51, "seek": 30336, "start": 315.2, "end": 323.28000000000003, "text": " evaluate our forecasts, store our model, and do some post processing. So let's look into the", "tokens": [50956, 13059, 527, 49421, 11, 3531, 527, 2316, 11, 293, 360, 512, 2183, 9007, 13, 407, 718, 311, 574, 666, 264, 51360], "temperature": 0.0, "avg_logprob": -0.11119076640335555, "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.0012080171145498753}, {"id": 52, "seek": 30336, "start": 323.28000000000003, "end": 330.56, "text": " methodology on a really high level. So on the left, we have our target load. This is where we", "tokens": [51360, 24850, 322, 257, 534, 1090, 1496, 13, 407, 322, 264, 1411, 11, 321, 362, 527, 3779, 3677, 13, 639, 307, 689, 321, 51724], "temperature": 0.0, "avg_logprob": -0.11119076640335555, "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.0012080171145498753}, {"id": 53, "seek": 33056, "start": 330.64, "end": 336.24, "text": " actually want to forecast. Then we have some external predictors. So we have our weather forecast,", "tokens": [50368, 767, 528, 281, 14330, 13, 1396, 321, 362, 512, 8320, 6069, 830, 13, 407, 321, 362, 527, 5503, 14330, 11, 50648], "temperature": 0.0, "avg_logprob": -0.07085866061123934, "compression_ratio": 1.7627737226277371, "no_speech_prob": 0.002927243709564209}, {"id": 54, "seek": 33056, "start": 336.24, "end": 342.88, "text": " market prices, and typical profiles of companies and households. From these external predictors,", "tokens": [50648, 2142, 7901, 11, 293, 7476, 23693, 295, 3431, 293, 22850, 13, 3358, 613, 8320, 6069, 830, 11, 50980], "temperature": 0.0, "avg_logprob": -0.07085866061123934, "compression_ratio": 1.7627737226277371, "no_speech_prob": 0.002927243709564209}, {"id": 55, "seek": 33056, "start": 342.88, "end": 347.84000000000003, "text": " we can actually calculate our derived features. So this is the feature engineering I just talked", "tokens": [50980, 321, 393, 767, 8873, 527, 18949, 4122, 13, 407, 341, 307, 264, 4111, 7043, 286, 445, 2825, 51228], "temperature": 0.0, "avg_logprob": -0.07085866061123934, "compression_ratio": 1.7627737226277371, "no_speech_prob": 0.002927243709564209}, {"id": 56, "seek": 33056, "start": 347.84000000000003, "end": 353.36, "text": " about. So we're able to calculate lag loads for each time stamp, but also to have some derived", "tokens": [51228, 466, 13, 407, 321, 434, 1075, 281, 8873, 8953, 12668, 337, 1184, 565, 9921, 11, 457, 611, 281, 362, 512, 18949, 51504], "temperature": 0.0, "avg_logprob": -0.07085866061123934, "compression_ratio": 1.7627737226277371, "no_speech_prob": 0.002927243709564209}, {"id": 57, "seek": 33056, "start": 353.36, "end": 359.12, "text": " weather features, such as, for example, the wind speed at the height of a windmill. And for the", "tokens": [51504, 5503, 4122, 11, 1270, 382, 11, 337, 1365, 11, 264, 2468, 3073, 412, 264, 6681, 295, 257, 2468, 18841, 13, 400, 337, 264, 51792], "temperature": 0.0, "avg_logprob": -0.07085866061123934, "compression_ratio": 1.7627737226277371, "no_speech_prob": 0.002927243709564209}, {"id": 58, "seek": 35912, "start": 359.12, "end": 364.8, "text": " more calendar info, it really matters if you're are forecasting on a Sunday or Christmas compared", "tokens": [50364, 544, 12183, 13614, 11, 309, 534, 7001, 498, 291, 434, 366, 44331, 322, 257, 7776, 420, 5272, 5347, 50648], "temperature": 0.0, "avg_logprob": -0.11432153802168997, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0017052341718226671}, {"id": 59, "seek": 35912, "start": 364.8, "end": 372.96, "text": " to a Monday. And then we can train a single model for all our lead times. So here you can see what", "tokens": [50648, 281, 257, 8138, 13, 400, 550, 321, 393, 3847, 257, 2167, 2316, 337, 439, 527, 1477, 1413, 13, 407, 510, 291, 393, 536, 437, 51056], "temperature": 0.0, "avg_logprob": -0.11432153802168997, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0017052341718226671}, {"id": 60, "seek": 35912, "start": 372.96, "end": 378.96, "text": " the data, for example, looks like. So if a daytime with increments of 15 minutes, our targets,", "tokens": [51056, 264, 1412, 11, 337, 1365, 11, 1542, 411, 13, 407, 498, 257, 31908, 365, 1946, 1117, 295, 2119, 2077, 11, 527, 12911, 11, 51356], "temperature": 0.0, "avg_logprob": -0.11432153802168997, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0017052341718226671}, {"id": 61, "seek": 35912, "start": 380.0, "end": 385.28000000000003, "text": " and external predictors, you can also see here that we have the Dutch energy prices in there.", "tokens": [51408, 293, 8320, 6069, 830, 11, 291, 393, 611, 536, 510, 300, 321, 362, 264, 15719, 2281, 7901, 294, 456, 13, 51672], "temperature": 0.0, "avg_logprob": -0.11432153802168997, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0017052341718226671}, {"id": 62, "seek": 38528, "start": 385.28, "end": 392.96, "text": " So if you have multiple training horizons, we just simply do pick late our data and use this", "tokens": [50364, 407, 498, 291, 362, 3866, 3097, 7937, 892, 11, 321, 445, 2935, 360, 1888, 3469, 527, 1412, 293, 764, 341, 50748], "temperature": 0.0, "avg_logprob": -0.159034950564606, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0033512820955365896}, {"id": 63, "seek": 38528, "start": 392.96, "end": 400.08, "text": " for our training horizons. And then if there are questions about it, please ask me in the break,", "tokens": [50748, 337, 527, 3097, 7937, 892, 13, 400, 550, 498, 456, 366, 1651, 466, 309, 11, 1767, 1029, 385, 294, 264, 1821, 11, 51104], "temperature": 0.0, "avg_logprob": -0.159034950564606, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0033512820955365896}, {"id": 64, "seek": 38528, "start": 400.08, "end": 406.32, "text": " but I don't have time to go into this in 15 minutes. We can with our trained model now actually make", "tokens": [51104, 457, 286, 500, 380, 362, 565, 281, 352, 666, 341, 294, 2119, 2077, 13, 492, 393, 365, 527, 8895, 2316, 586, 767, 652, 51416], "temperature": 0.0, "avg_logprob": -0.159034950564606, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0033512820955365896}, {"id": 65, "seek": 38528, "start": 406.32, "end": 414.88, "text": " this forecast. And of course, we want it to look nice. So we have this beautiful Grafana dashboard,", "tokens": [51416, 341, 14330, 13, 400, 295, 1164, 11, 321, 528, 309, 281, 574, 1481, 13, 407, 321, 362, 341, 2238, 8985, 69, 2095, 18342, 11, 51844], "temperature": 0.0, "avg_logprob": -0.159034950564606, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0033512820955365896}, {"id": 66, "seek": 41488, "start": 414.88, "end": 420.64, "text": " which actually summarizes all of the information that you need for your forecast. So let's look", "tokens": [50364, 597, 767, 14611, 5660, 439, 295, 264, 1589, 300, 291, 643, 337, 428, 14330, 13, 407, 718, 311, 574, 50652], "temperature": 0.0, "avg_logprob": -0.09532476897932525, "compression_ratio": 1.9209486166007905, "no_speech_prob": 0.003225114895030856}, {"id": 67, "seek": 41488, "start": 420.64, "end": 427.76, "text": " into it. First and foremost, our forecast. So the red line on the left is actually the low that", "tokens": [50652, 666, 309, 13, 2386, 293, 18864, 11, 527, 14330, 13, 407, 264, 2182, 1622, 322, 264, 1411, 307, 767, 264, 2295, 300, 51008], "temperature": 0.0, "avg_logprob": -0.09532476897932525, "compression_ratio": 1.9209486166007905, "no_speech_prob": 0.003225114895030856}, {"id": 68, "seek": 41488, "start": 427.76, "end": 433.36, "text": " has been historically measured. And then we see here the yellow lines is our forecast. Well,", "tokens": [51008, 575, 668, 16180, 12690, 13, 400, 550, 321, 536, 510, 264, 5566, 3876, 307, 527, 14330, 13, 1042, 11, 51288], "temperature": 0.0, "avg_logprob": -0.09532476897932525, "compression_ratio": 1.9209486166007905, "no_speech_prob": 0.003225114895030856}, {"id": 69, "seek": 41488, "start": 433.36, "end": 438.0, "text": " now you see that there are a lot of yellow lines. What do those mean? Well, those are actually the", "tokens": [51288, 586, 291, 536, 300, 456, 366, 257, 688, 295, 5566, 3876, 13, 708, 360, 729, 914, 30, 1042, 11, 729, 366, 767, 264, 51520], "temperature": 0.0, "avg_logprob": -0.09532476897932525, "compression_ratio": 1.9209486166007905, "no_speech_prob": 0.003225114895030856}, {"id": 70, "seek": 41488, "start": 438.0, "end": 444.24, "text": " quantiles. So you have actually a certainty in your forecast. And this can be actually useful if we're", "tokens": [51520, 4426, 4680, 13, 407, 291, 362, 767, 257, 27022, 294, 428, 14330, 13, 400, 341, 393, 312, 767, 4420, 498, 321, 434, 51832], "temperature": 0.0, "avg_logprob": -0.09532476897932525, "compression_ratio": 1.9209486166007905, "no_speech_prob": 0.003225114895030856}, {"id": 71, "seek": 44424, "start": 444.32, "end": 449.44, "text": " a certain location. You're quite sure what your forecast is going to be. You can go into", "tokens": [50368, 257, 1629, 4914, 13, 509, 434, 1596, 988, 437, 428, 14330, 307, 516, 281, 312, 13, 509, 393, 352, 666, 50624], "temperature": 0.0, "avg_logprob": -0.10583327033303001, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.001521242200396955}, {"id": 72, "seek": 44424, "start": 449.44, "end": 453.28000000000003, "text": " another quantile. Then if you have a location where you have a lot of factors that you actually", "tokens": [50624, 1071, 4426, 794, 13, 1396, 498, 291, 362, 257, 4914, 689, 291, 362, 257, 688, 295, 6771, 300, 291, 767, 50816], "temperature": 0.0, "avg_logprob": -0.10583327033303001, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.001521242200396955}, {"id": 73, "seek": 44424, "start": 453.28000000000003, "end": 461.6, "text": " don't know anything about. And also very nice our feature importance plot. So here in the feature", "tokens": [50816, 500, 380, 458, 1340, 466, 13, 400, 611, 588, 1481, 527, 4111, 7379, 7542, 13, 407, 510, 294, 264, 4111, 51232], "temperature": 0.0, "avg_logprob": -0.10583327033303001, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.001521242200396955}, {"id": 74, "seek": 44424, "start": 461.6, "end": 468.8, "text": " importance plot, we can see our lag loads and some other features. And this is actually", "tokens": [51232, 7379, 7542, 11, 321, 393, 536, 527, 8953, 12668, 293, 512, 661, 4122, 13, 400, 341, 307, 767, 51592], "temperature": 0.0, "avg_logprob": -0.10583327033303001, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.001521242200396955}, {"id": 75, "seek": 46880, "start": 469.76, "end": 474.32, "text": " nice. So you can see for every location, which features are important for your forecast.", "tokens": [50412, 1481, 13, 407, 291, 393, 536, 337, 633, 4914, 11, 597, 4122, 366, 1021, 337, 428, 14330, 13, 50640], "temperature": 0.0, "avg_logprob": -0.11305787013127254, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.004276918713003397}, {"id": 76, "seek": 46880, "start": 474.32, "end": 478.8, "text": " So for example, here we see radiation. I don't think it's readable for you, but it says radiation.", "tokens": [50640, 407, 337, 1365, 11, 510, 321, 536, 12420, 13, 286, 500, 380, 519, 309, 311, 49857, 337, 291, 11, 457, 309, 1619, 12420, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11305787013127254, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.004276918713003397}, {"id": 77, "seek": 46880, "start": 478.8, "end": 485.28000000000003, "text": " So you know that there are quite some solar parks or solar panels behind, for example, your substation.", "tokens": [50864, 407, 291, 458, 300, 456, 366, 1596, 512, 7936, 16213, 420, 7936, 13419, 2261, 11, 337, 1365, 11, 428, 4594, 399, 13, 51188], "temperature": 0.0, "avg_logprob": -0.11305787013127254, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.004276918713003397}, {"id": 78, "seek": 46880, "start": 487.68, "end": 491.6, "text": " Wind speeds nowhere to be seen. So probably no windmills in that area.", "tokens": [51308, 6320, 16411, 11159, 281, 312, 1612, 13, 407, 1391, 572, 2468, 76, 2565, 294, 300, 1859, 13, 51504], "temperature": 0.0, "avg_logprob": -0.11305787013127254, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.004276918713003397}, {"id": 79, "seek": 49160, "start": 492.40000000000003, "end": 498.88, "text": " So this was really short about open staff. Let me see how much time I have left.", "tokens": [50404, 407, 341, 390, 534, 2099, 466, 1269, 3525, 13, 961, 385, 536, 577, 709, 565, 286, 362, 1411, 13, 50728], "temperature": 0.0, "avg_logprob": -0.23246999996811596, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.005872685927897692}, {"id": 80, "seek": 49160, "start": 501.20000000000005, "end": 507.12, "text": " Six minutes. Perfect. Okay. So community and upcoming events. One of the main things that has", "tokens": [50844, 11678, 2077, 13, 10246, 13, 1033, 13, 407, 1768, 293, 11500, 3931, 13, 1485, 295, 264, 2135, 721, 300, 575, 51140], "temperature": 0.0, "avg_logprob": -0.23246999996811596, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.005872685927897692}, {"id": 81, "seek": 49160, "start": 507.84000000000003, "end": 514.48, "text": " really changed in open staff this last year is our community. So before it was just Alliander", "tokens": [51176, 534, 3105, 294, 1269, 3525, 341, 1036, 1064, 307, 527, 1768, 13, 407, 949, 309, 390, 445, 1057, 72, 4483, 51508], "temperature": 0.0, "avg_logprob": -0.23246999996811596, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.005872685927897692}, {"id": 82, "seek": 51448, "start": 514.48, "end": 521.6800000000001, "text": " who actually created it together with RTE, working on open staff. And now it looks like this. So let", "tokens": [50364, 567, 767, 2942, 309, 1214, 365, 497, 13639, 11, 1364, 322, 1269, 3525, 13, 400, 586, 309, 1542, 411, 341, 13, 407, 718, 50724], "temperature": 0.0, "avg_logprob": -0.15065888889500353, "compression_ratio": 1.702054794520548, "no_speech_prob": 0.08015773445367813}, {"id": 83, "seek": 51448, "start": 521.6800000000001, "end": 527.04, "text": " me go over every company really quick. So Alliander, that's where I'm from, talked about that enough.", "tokens": [50724, 385, 352, 670, 633, 2237, 534, 1702, 13, 407, 1057, 72, 4483, 11, 300, 311, 689, 286, 478, 490, 11, 2825, 466, 300, 1547, 13, 50992], "temperature": 0.0, "avg_logprob": -0.15065888889500353, "compression_ratio": 1.702054794520548, "no_speech_prob": 0.08015773445367813}, {"id": 84, "seek": 51448, "start": 527.04, "end": 531.52, "text": " RT actually working on open staff for quite a while and they're actually ready to implement it very", "tokens": [50992, 21797, 767, 1364, 322, 1269, 3525, 337, 1596, 257, 1339, 293, 436, 434, 767, 1919, 281, 4445, 309, 588, 51216], "temperature": 0.0, "avg_logprob": -0.15065888889500353, "compression_ratio": 1.702054794520548, "no_speech_prob": 0.08015773445367813}, {"id": 85, "seek": 51448, "start": 531.52, "end": 536.72, "text": " soon. RT International just joined us this year. They have a very nice proof of concept and they're", "tokens": [51216, 2321, 13, 21797, 9157, 445, 6869, 505, 341, 1064, 13, 814, 362, 257, 588, 1481, 8177, 295, 3410, 293, 436, 434, 51476], "temperature": 0.0, "avg_logprob": -0.15065888889500353, "compression_ratio": 1.702054794520548, "no_speech_prob": 0.08015773445367813}, {"id": 86, "seek": 51448, "start": 536.72, "end": 542.64, "text": " going to work on it further. Fidel has actually been using open staff quite a long. I've heard", "tokens": [51476, 516, 281, 589, 322, 309, 3052, 13, 479, 16189, 575, 767, 668, 1228, 1269, 3525, 1596, 257, 938, 13, 286, 600, 2198, 51772], "temperature": 0.0, "avg_logprob": -0.15065888889500353, "compression_ratio": 1.702054794520548, "no_speech_prob": 0.08015773445367813}, {"id": 87, "seek": 54264, "start": 542.64, "end": 550.72, "text": " some terms, leeches this today. Well, that was a feed on up to actually a month ago. So we contacted", "tokens": [50364, 512, 2115, 11, 476, 27789, 341, 965, 13, 1042, 11, 300, 390, 257, 3154, 322, 493, 281, 767, 257, 1618, 2057, 13, 407, 321, 21546, 50768], "temperature": 0.0, "avg_logprob": -0.19187184037833377, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.006865587085485458}, {"id": 88, "seek": 54264, "start": 550.72, "end": 555.28, "text": " them and they were like, oh yeah, we found some bugs. We fixed it. We can implement this. So they", "tokens": [50768, 552, 293, 436, 645, 411, 11, 1954, 1338, 11, 321, 1352, 512, 15120, 13, 492, 6806, 309, 13, 492, 393, 4445, 341, 13, 407, 436, 50996], "temperature": 0.0, "avg_logprob": -0.19187184037833377, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.006865587085485458}, {"id": 89, "seek": 54264, "start": 555.28, "end": 560.16, "text": " actually joined our community as of this year. Sigelman still working on a proof of concept", "tokens": [50996, 767, 6869, 527, 1768, 382, 295, 341, 1064, 13, 37763, 338, 1601, 920, 1364, 322, 257, 8177, 295, 3410, 51240], "temperature": 0.0, "avg_logprob": -0.19187184037833377, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.006865587085485458}, {"id": 90, "seek": 54264, "start": 560.16, "end": 564.88, "text": " and seeing if they want to replace their own forecasting model with open staff. And Shell is", "tokens": [51240, 293, 2577, 498, 436, 528, 281, 7406, 641, 1065, 44331, 2316, 365, 1269, 3525, 13, 400, 22863, 307, 51476], "temperature": 0.0, "avg_logprob": -0.19187184037833377, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.006865587085485458}, {"id": 91, "seek": 54264, "start": 564.88, "end": 570.3199999999999, "text": " working on open staff DBC and seeing if they can use their method of data important.", "tokens": [51476, 1364, 322, 1269, 3525, 413, 7869, 293, 2577, 498, 436, 393, 764, 641, 3170, 295, 1412, 1021, 13, 51748], "temperature": 0.0, "avg_logprob": -0.19187184037833377, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.006865587085485458}, {"id": 92, "seek": 57264, "start": 572.72, "end": 579.12, "text": " Now, I hope everyone feels like they want to try open staff. Well, you're in luck because we are", "tokens": [50368, 823, 11, 286, 1454, 1518, 3417, 411, 436, 528, 281, 853, 1269, 3525, 13, 1042, 11, 291, 434, 294, 3668, 570, 321, 366, 50688], "temperature": 0.0, "avg_logprob": -0.11632844564077016, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.0037383229937404394}, {"id": 93, "seek": 57264, "start": 579.12, "end": 586.16, "text": " organizing a workshop. So on the Friday, the first of March from two to four, we were organizing a", "tokens": [50688, 17608, 257, 13541, 13, 407, 322, 264, 6984, 11, 264, 700, 295, 6129, 490, 732, 281, 1451, 11, 321, 645, 17608, 257, 51040], "temperature": 0.0, "avg_logprob": -0.11632844564077016, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.0037383229937404394}, {"id": 94, "seek": 57264, "start": 586.16, "end": 591.68, "text": " workshop. And I would like everyone who's interested to join. So you'll get a better", "tokens": [51040, 13541, 13, 400, 286, 576, 411, 1518, 567, 311, 3102, 281, 3917, 13, 407, 291, 603, 483, 257, 1101, 51316], "temperature": 0.0, "avg_logprob": -0.11632844564077016, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.0037383229937404394}, {"id": 95, "seek": 57264, "start": 591.68, "end": 594.8, "text": " introduction to open staff and also a little bit more of the technical details.", "tokens": [51316, 9339, 281, 1269, 3525, 293, 611, 257, 707, 857, 544, 295, 264, 6191, 4365, 13, 51472], "temperature": 0.0, "avg_logprob": -0.11632844564077016, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.0037383229937404394}, {"id": 96, "seek": 57264, "start": 596.56, "end": 601.4399999999999, "text": " It will be virtual. And you will get really a hands on experience. So you get some example", "tokens": [51560, 467, 486, 312, 6374, 13, 400, 291, 486, 483, 534, 257, 2377, 322, 1752, 13, 407, 291, 483, 512, 1365, 51804], "temperature": 0.0, "avg_logprob": -0.11632844564077016, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.0037383229937404394}, {"id": 97, "seek": 60144, "start": 601.44, "end": 605.84, "text": " notebooks from us where you have to make your own exercises and you can actually make your own", "tokens": [50364, 43782, 490, 505, 689, 291, 362, 281, 652, 428, 1065, 11900, 293, 291, 393, 767, 652, 428, 1065, 50584], "temperature": 0.0, "avg_logprob": -0.1367823282877604, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.008339134976267815}, {"id": 98, "seek": 60144, "start": 605.84, "end": 612.0, "text": " forecast with open staff and see how easy it is. If you want to sign up, just scan the QR", "tokens": [50584, 14330, 365, 1269, 3525, 293, 536, 577, 1858, 309, 307, 13, 759, 291, 528, 281, 1465, 493, 11, 445, 11049, 264, 32784, 50892], "temperature": 0.0, "avg_logprob": -0.1367823282877604, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.008339134976267815}, {"id": 99, "seek": 60144, "start": 612.0, "end": 617.12, "text": " card over here. And it will be very nice. I also have it on the next slide for people who are too", "tokens": [50892, 2920, 670, 510, 13, 400, 309, 486, 312, 588, 1481, 13, 286, 611, 362, 309, 322, 264, 958, 4137, 337, 561, 567, 366, 886, 51148], "temperature": 0.0, "avg_logprob": -0.1367823282877604, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.008339134976267815}, {"id": 100, "seek": 60144, "start": 617.12, "end": 624.4000000000001, "text": " slow. So want to know more about open staff, maybe even before you sign up for the workshop,", "tokens": [51148, 2964, 13, 407, 528, 281, 458, 544, 466, 1269, 3525, 11, 1310, 754, 949, 291, 1465, 493, 337, 264, 13541, 11, 51512], "temperature": 0.0, "avg_logprob": -0.1367823282877604, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.008339134976267815}, {"id": 101, "seek": 60144, "start": 624.4000000000001, "end": 630.8800000000001, "text": " we of course have our GitHub website documentation, etc. You're only one command away from using open", "tokens": [51512, 321, 295, 1164, 362, 527, 23331, 3144, 14333, 11, 5183, 13, 509, 434, 787, 472, 5622, 1314, 490, 1228, 1269, 51836], "temperature": 0.0, "avg_logprob": -0.1367823282877604, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.008339134976267815}, {"id": 102, "seek": 63088, "start": 630.88, "end": 637.4399999999999, "text": " staff. And if there's anything you want to ask or give some comments or anything, you can just", "tokens": [50364, 3525, 13, 400, 498, 456, 311, 1340, 291, 528, 281, 1029, 420, 976, 512, 3053, 420, 1340, 11, 291, 393, 445, 50692], "temperature": 0.0, "avg_logprob": -0.1759672400392132, "compression_ratio": 1.4829268292682927, "no_speech_prob": 0.012192729860544205}, {"id": 103, "seek": 63088, "start": 637.4399999999999, "end": 643.92, "text": " send me an email or send me a message on LinkedIn. So thank you for your time and I welcome any", "tokens": [50692, 2845, 385, 364, 3796, 420, 2845, 385, 257, 3636, 322, 20657, 13, 407, 1309, 291, 337, 428, 565, 293, 286, 2928, 604, 51016], "temperature": 0.0, "avg_logprob": -0.1759672400392132, "compression_ratio": 1.4829268292682927, "no_speech_prob": 0.012192729860544205}, {"id": 104, "seek": 63088, "start": 643.92, "end": 644.4, "text": " questions.", "tokens": [51016, 1651, 13, 51040], "temperature": 0.0, "avg_logprob": -0.1759672400392132, "compression_ratio": 1.4829268292682927, "no_speech_prob": 0.012192729860544205}, {"id": 105, "seek": 63088, "start": 650.8, "end": 652.08, "text": " Who's running the microphone?", "tokens": [51360, 2102, 311, 2614, 264, 10952, 30, 51424], "temperature": 0.0, "avg_logprob": -0.1759672400392132, "compression_ratio": 1.4829268292682927, "no_speech_prob": 0.012192729860544205}, {"id": 106, "seek": 63088, "start": 652.88, "end": 656.8, "text": " I'll try to do my best. Please feel free to guess to find the best path.", "tokens": [51464, 286, 603, 853, 281, 360, 452, 1151, 13, 2555, 841, 1737, 281, 2041, 281, 915, 264, 1151, 3100, 13, 51660], "temperature": 0.0, "avg_logprob": -0.1759672400392132, "compression_ratio": 1.4829268292682927, "no_speech_prob": 0.012192729860544205}, {"id": 107, "seek": 66088, "start": 660.88, "end": 668.4, "text": " Hello. First of all, thank you so much. This was very interesting. And I have no experience,", "tokens": [50364, 2425, 13, 2386, 295, 439, 11, 1309, 291, 370, 709, 13, 639, 390, 588, 1880, 13, 400, 286, 362, 572, 1752, 11, 50740], "temperature": 0.0, "avg_logprob": -0.2149082819620768, "compression_ratio": 1.5201793721973094, "no_speech_prob": 0.004189327824860811}, {"id": 108, "seek": 66088, "start": 668.4, "end": 673.4399999999999, "text": " I have never heard of open staff before reading on the FOSTA website. I have one question about", "tokens": [50740, 286, 362, 1128, 2198, 295, 1269, 3525, 949, 3760, 322, 264, 479, 4367, 8241, 3144, 13, 286, 362, 472, 1168, 466, 50992], "temperature": 0.0, "avg_logprob": -0.2149082819620768, "compression_ratio": 1.5201793721973094, "no_speech_prob": 0.004189327824860811}, {"id": 109, "seek": 66088, "start": 673.4399999999999, "end": 678.96, "text": " the data collection. Do you provide like some examples or standards on how and where to fetch", "tokens": [50992, 264, 1412, 5765, 13, 1144, 291, 2893, 411, 512, 5110, 420, 7787, 322, 577, 293, 689, 281, 23673, 51268], "temperature": 0.0, "avg_logprob": -0.2149082819620768, "compression_ratio": 1.5201793721973094, "no_speech_prob": 0.004189327824860811}, {"id": 110, "seek": 66088, "start": 678.96, "end": 682.08, "text": " data because the data source is very, I tried, I looked.", "tokens": [51268, 1412, 570, 264, 1412, 4009, 307, 588, 11, 286, 3031, 11, 286, 2956, 13, 51424], "temperature": 0.0, "avg_logprob": -0.2149082819620768, "compression_ratio": 1.5201793721973094, "no_speech_prob": 0.004189327824860811}, {"id": 111, "seek": 69088, "start": 691.2, "end": 700.4, "text": " So very good question, I think this is something that a community indeed struggles with. So for the", "tokens": [50380, 407, 588, 665, 1168, 11, 286, 519, 341, 307, 746, 300, 257, 1768, 6451, 17592, 365, 13, 407, 337, 264, 50840], "temperature": 0.0, "avg_logprob": -0.1545776195740432, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.007731741294264793}, {"id": 112, "seek": 69088, "start": 700.4, "end": 704.64, "text": " Netherlands, we actually do have those sources because we are using them ourselves for other", "tokens": [50840, 20873, 11, 321, 767, 360, 362, 729, 7139, 570, 321, 366, 1228, 552, 4175, 337, 661, 51052], "temperature": 0.0, "avg_logprob": -0.1545776195740432, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.007731741294264793}, {"id": 113, "seek": 69088, "start": 704.64, "end": 709.12, "text": " countries who are working on it to see if we can find some open data for everyone.", "tokens": [51052, 3517, 567, 366, 1364, 322, 309, 281, 536, 498, 321, 393, 915, 512, 1269, 1412, 337, 1518, 13, 51276], "temperature": 0.0, "avg_logprob": -0.1545776195740432, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.007731741294264793}, {"id": 114, "seek": 69088, "start": 710.32, "end": 714.88, "text": " But if you're interested, you can always send me an email and I'll see what we have. Yeah, great.", "tokens": [51336, 583, 498, 291, 434, 3102, 11, 291, 393, 1009, 2845, 385, 364, 3796, 293, 286, 603, 536, 437, 321, 362, 13, 865, 11, 869, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1545776195740432, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.007731741294264793}, {"id": 115, "seek": 71488, "start": 715.68, "end": 722.24, "text": " Hi, it's Min\u00e9. I'm from Red Hat. So obviously I will ask the question about scaling this, right? How", "tokens": [50404, 2421, 11, 309, 311, 2829, 526, 13, 286, 478, 490, 4477, 15867, 13, 407, 2745, 286, 486, 1029, 264, 1168, 466, 21589, 341, 11, 558, 30, 1012, 50732], "temperature": 0.0, "avg_logprob": -0.2724246184031169, "compression_ratio": 1.5, "no_speech_prob": 0.018551524728536606}, {"id": 116, "seek": 71488, "start": 722.24, "end": 727.04, "text": " will you standardize and scale this because it's a project. It sounds super interesting. But how", "tokens": [50732, 486, 291, 3832, 1125, 293, 4373, 341, 570, 309, 311, 257, 1716, 13, 467, 3263, 1687, 1880, 13, 583, 577, 50972], "temperature": 0.0, "avg_logprob": -0.2724246184031169, "compression_ratio": 1.5, "no_speech_prob": 0.018551524728536606}, {"id": 117, "seek": 71488, "start": 727.04, "end": 733.84, "text": " are we going to scale this to 49,000 substations or millions of smart meters at home?", "tokens": [50972, 366, 321, 516, 281, 4373, 341, 281, 16513, 11, 1360, 4594, 763, 420, 6803, 295, 4069, 8146, 412, 1280, 30, 51312], "temperature": 0.0, "avg_logprob": -0.2724246184031169, "compression_ratio": 1.5, "no_speech_prob": 0.018551524728536606}, {"id": 118, "seek": 71488, "start": 735.6, "end": 739.76, "text": " Very good question. This is actually something we're working on right now. So we are actually", "tokens": [51400, 4372, 665, 1168, 13, 639, 307, 767, 746, 321, 434, 1364, 322, 558, 586, 13, 407, 321, 366, 767, 51608], "temperature": 0.0, "avg_logprob": -0.2724246184031169, "compression_ratio": 1.5, "no_speech_prob": 0.018551524728536606}, {"id": 119, "seek": 73976, "start": 740.4, "end": 746.0, "text": " employing our open step stack on Dexter probably anytime soon and seeing if you can actually scale", "tokens": [50396, 3188, 278, 527, 1269, 1823, 8630, 322, 1346, 36671, 1391, 13038, 2321, 293, 2577, 498, 291, 393, 767, 4373, 50676], "temperature": 0.0, "avg_logprob": -0.2834741087520824, "compression_ratio": 1.5051020408163265, "no_speech_prob": 0.035081978887319565}, {"id": 120, "seek": 73976, "start": 746.0, "end": 751.6, "text": " from that. Currently we have it scaled up at I think 100 substations. And if you're curious how we", "tokens": [50676, 490, 300, 13, 19964, 321, 362, 309, 36039, 493, 412, 286, 519, 2319, 4594, 763, 13, 400, 498, 291, 434, 6369, 577, 321, 50956], "temperature": 0.0, "avg_logprob": -0.2834741087520824, "compression_ratio": 1.5051020408163265, "no_speech_prob": 0.035081978887319565}, {"id": 121, "seek": 73976, "start": 751.6, "end": 756.96, "text": " have a reference implementation on our GitHub and you can see all the information there on how we", "tokens": [50956, 362, 257, 6408, 11420, 322, 527, 23331, 293, 291, 393, 536, 439, 264, 1589, 456, 322, 577, 321, 51224], "temperature": 0.0, "avg_logprob": -0.2834741087520824, "compression_ratio": 1.5051020408163265, "no_speech_prob": 0.035081978887319565}, {"id": 122, "seek": 75696, "start": 757.12, "end": 759.9200000000001, "text": " deploy this. Thanks.", "tokens": [50372, 7274, 341, 13, 2561, 13, 50512], "temperature": 0.0, "avg_logprob": -0.28250929888556986, "compression_ratio": 1.3836477987421383, "no_speech_prob": 0.0031894007697701454}, {"id": 123, "seek": 75696, "start": 761.9200000000001, "end": 762.8000000000001, "text": " Yeah, yeah, sure.", "tokens": [50612, 865, 11, 1338, 11, 988, 13, 50656], "temperature": 0.0, "avg_logprob": -0.28250929888556986, "compression_ratio": 1.3836477987421383, "no_speech_prob": 0.0031894007697701454}, {"id": 124, "seek": 75696, "start": 769.9200000000001, "end": 778.1600000000001, "text": " I have a question about the data sources. Is there any thought given to adding geographical", "tokens": [51012, 286, 362, 257, 1168, 466, 264, 1412, 7139, 13, 1119, 456, 604, 1194, 2212, 281, 5127, 39872, 51424], "temperature": 0.0, "avg_logprob": -0.28250929888556986, "compression_ratio": 1.3836477987421383, "no_speech_prob": 0.0031894007697701454}, {"id": 125, "seek": 75696, "start": 778.1600000000001, "end": 783.6800000000001, "text": " information systems data into the system for forecasting models? Because especially stuff", "tokens": [51424, 1589, 3652, 1412, 666, 264, 1185, 337, 44331, 5245, 30, 1436, 2318, 1507, 51700], "temperature": 0.0, "avg_logprob": -0.28250929888556986, "compression_ratio": 1.3836477987421383, "no_speech_prob": 0.0031894007697701454}, {"id": 126, "seek": 78368, "start": 783.68, "end": 789.68, "text": " like wind and solar radiation also not just depend on the time of day and the wind speeds,", "tokens": [50364, 411, 2468, 293, 7936, 12420, 611, 406, 445, 5672, 322, 264, 565, 295, 786, 293, 264, 2468, 16411, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10250511765480042, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.003833214985206723}, {"id": 127, "seek": 78368, "start": 789.68, "end": 791.12, "text": " but the location itself.", "tokens": [50664, 457, 264, 4914, 2564, 13, 50736], "temperature": 0.0, "avg_logprob": -0.10250511765480042, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.003833214985206723}, {"id": 128, "seek": 78368, "start": 796.2399999999999, "end": 801.5999999999999, "text": " Great question. Yeah, actually for our system, it just connects to the closest K and MI. So that's", "tokens": [50992, 3769, 1168, 13, 865, 11, 767, 337, 527, 1185, 11, 309, 445, 16967, 281, 264, 13699, 591, 293, 13696, 13, 407, 300, 311, 51260], "temperature": 0.0, "avg_logprob": -0.10250511765480042, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.003833214985206723}, {"id": 129, "seek": 78368, "start": 801.5999999999999, "end": 807.52, "text": " the Royal Dutch Weather Organization. So it's able to find the closest station to where you actually", "tokens": [51260, 264, 12717, 15719, 34441, 23979, 13, 407, 309, 311, 1075, 281, 915, 264, 13699, 5214, 281, 689, 291, 767, 51556], "temperature": 0.0, "avg_logprob": -0.10250511765480042, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.003833214985206723}, {"id": 130, "seek": 78368, "start": 807.52, "end": 812.8, "text": " want to forecast. So it definitely takes a location to account. We have a prediction job", "tokens": [51556, 528, 281, 14330, 13, 407, 309, 2138, 2516, 257, 4914, 281, 2696, 13, 492, 362, 257, 17630, 1691, 51820], "temperature": 0.0, "avg_logprob": -0.10250511765480042, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.003833214985206723}, {"id": 131, "seek": 81280, "start": 812.8, "end": 816.64, "text": " class where you can put in all of the information for your forecast and in there you also put the", "tokens": [50364, 1508, 689, 291, 393, 829, 294, 439, 295, 264, 1589, 337, 428, 14330, 293, 294, 456, 291, 611, 829, 264, 50556], "temperature": 0.0, "avg_logprob": -0.12385113537311554, "compression_ratio": 1.6, "no_speech_prob": 0.0031174453906714916}, {"id": 132, "seek": 81280, "start": 816.64, "end": 822.8, "text": " latitude and longitude of your location. So it does take that into account. Question over there.", "tokens": [50556, 45436, 293, 938, 4377, 295, 428, 4914, 13, 407, 309, 775, 747, 300, 666, 2696, 13, 14464, 670, 456, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12385113537311554, "compression_ratio": 1.6, "no_speech_prob": 0.0031174453906714916}, {"id": 133, "seek": 81280, "start": 833.68, "end": 838.0, "text": " Thanks for the question about the geographic data because I was thinking about an approach of", "tokens": [51408, 2561, 337, 264, 1168, 466, 264, 32318, 1412, 570, 286, 390, 1953, 466, 364, 3109, 295, 51624], "temperature": 0.0, "avg_logprob": -0.12385113537311554, "compression_ratio": 1.6, "no_speech_prob": 0.0031174453906714916}, {"id": 134, "seek": 83800, "start": 838.0, "end": 844.08, "text": " just using cheap raspberry weather stations in Austria and distributing them across some locations", "tokens": [50364, 445, 1228, 7084, 41468, 5503, 13390, 294, 26501, 293, 41406, 552, 2108, 512, 9253, 50668], "temperature": 0.0, "avg_logprob": -0.17904962991413317, "compression_ratio": 1.5330396475770924, "no_speech_prob": 0.019768400117754936}, {"id": 135, "seek": 83800, "start": 844.08, "end": 848.72, "text": " to fetch the data because I have the Google Weather API and the Open Weather API or whatever as", "tokens": [50668, 281, 23673, 264, 1412, 570, 286, 362, 264, 3329, 34441, 9362, 293, 264, 7238, 34441, 9362, 420, 2035, 382, 50900], "temperature": 0.0, "avg_logprob": -0.17904962991413317, "compression_ratio": 1.5330396475770924, "no_speech_prob": 0.019768400117754936}, {"id": 136, "seek": 83800, "start": 848.72, "end": 854.96, "text": " comparison values. And for the geographic thing, thanks for the question. How would you connect", "tokens": [50900, 9660, 4190, 13, 400, 337, 264, 32318, 551, 11, 3231, 337, 264, 1168, 13, 1012, 576, 291, 1745, 51212], "temperature": 0.0, "avg_logprob": -0.17904962991413317, "compression_ratio": 1.5330396475770924, "no_speech_prob": 0.019768400117754936}, {"id": 137, "seek": 83800, "start": 854.96, "end": 859.44, "text": " that? Like is this a plan of open stuff? Did I miss this?", "tokens": [51212, 300, 30, 1743, 307, 341, 257, 1393, 295, 1269, 1507, 30, 2589, 286, 1713, 341, 30, 51436], "temperature": 0.0, "avg_logprob": -0.17904962991413317, "compression_ratio": 1.5330396475770924, "no_speech_prob": 0.019768400117754936}, {"id": 138, "seek": 86800, "start": 868.56, "end": 872.72, "text": " Yeah, thanks for the kind of difficult question because I don't know the answer.", "tokens": [50392, 865, 11, 3231, 337, 264, 733, 295, 2252, 1168, 570, 286, 500, 380, 458, 264, 1867, 13, 50600], "temperature": 0.0, "avg_logprob": -0.2641636358725058, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.012048676609992981}, {"id": 139, "seek": 86800, "start": 873.6, "end": 878.56, "text": " So I'll ask my colleagues who actually made this part of the open stuff and I'll get back to you", "tokens": [50644, 407, 286, 603, 1029, 452, 7734, 567, 767, 1027, 341, 644, 295, 264, 1269, 1507, 293, 286, 603, 483, 646, 281, 291, 50892], "temperature": 0.0, "avg_logprob": -0.2641636358725058, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.012048676609992981}, {"id": 140, "seek": 86800, "start": 878.56, "end": 882.72, "text": " if we connect afterwards. So then you'll know. But it's very interesting to do with the Raspberry", "tokens": [50892, 498, 321, 1745, 10543, 13, 407, 550, 291, 603, 458, 13, 583, 309, 311, 588, 1880, 281, 360, 365, 264, 41154, 51100], "temperature": 0.0, "avg_logprob": -0.2641636358725058, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.012048676609992981}, {"id": 141, "seek": 86800, "start": 882.72, "end": 884.8, "text": " Price things. Thanks.", "tokens": [51100, 25803, 721, 13, 2561, 13, 51204], "temperature": 0.0, "avg_logprob": -0.2641636358725058, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.012048676609992981}], "language": "en"}