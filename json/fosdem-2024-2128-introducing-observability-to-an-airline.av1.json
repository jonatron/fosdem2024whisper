{"text": " Hello everybody. Can you hear me? Is it working? Awesome. This is my first talk, so I'm just going to do this because hello, FOSTA! Yes! Great to hear some energy in here at this time. Right, so my name's James and my major client currently is a, so my major client, my only client currently is a major European airline. Get that right? And I wanted to talk to you today about some of the challenges that we're facing in introducing observability to that client, a framework that I've kind of put together to overcome those challenges and some thoughts that I have overall about observability. This talk should be applicable to any big organization. So there's not really anything that's specific to an airline, but if you think about the scale of not only the size, but the amount of different tasks an airline would be doing and the kind of vintage of most major airlines, you'll kind of get an idea of what we're talking about here. By the way, just as an idea, who here works for like a company that's got more than a thousand people in it? Okay, fair enough. Okay. And how many of those people are actually using observability on any scale? Okay, some of you, awesome. You should be doing this instead. In this talk, I want to walk you through three steps I'm taking to introduce observability. One, I'm calling an observability transformation. We're not going to be talking about anything too technically exciting here, and we're certainly not going to be talking about anything like introducing observability to like the cockpit or anything like that. This talk is about helping you get your company or client or whoever else on board with observability. It's about making that transition successful and it's about making it sustainable. And of course, the associated love and adoration of your peers for making their lives a whole hell of a lot easier. So, first thing I want us to do is align us on what observability is. So, that'll be easy. Does anyone want to, I tell you what, we're running late, so I'll just tell you what observability is. Firstly, I think that what we've got to remember when we're talking about observability is that a lot of people don't really know what to think of, but they're probably thinking of something like this, like a big ten-foot view of everything that's going on. Obviously most people in here won't think that that's observability. Why not? Can anyone say, like, is this observable? Is this an observable system by our definition? This is what I think of when I think of observability. And when I speak to anybody that, you know, may be lay technical or non-technical, this is the kind of thing that I'll introduce to them. I know that I'm putting a definition on something that, you know, and that's a little bit controversial, but this is what I think of. So, this will help you kind of ground in what this talk is about. So, you can imagine, like, as we went through that previous thing, like, there's this cake being made. And so, you know, I can describe quite easily that previously, with a, like, monitoring process, we would monitor, get the metrics and the logs from each individual component of that system. But now what we're going to do is we're going to collect the request for a cake through that system. And this has some clear value if we start talking about this. There's this other way of talking about it that's like, you know, observability is how we understand something by the internal something, I can't say, but it doesn't really kind of get across the value to people that may be a bit skeptical about this. And I think that this kind of does. So, let's just pocket that idea for a second. This idea basically describes observability as recording work done to satisfy a request. So, a request is completely observable when you can see all the work done to that request, and a system is completely observable when you can see all the work done to all requests moving through a system. This to me is much more tangible. It does tie it specifically to requests or events. However, I do note that when we talk about observability and making long running processes observable, most people try and arbitrarily or otherwise find ways of cutting them up into individual traces anyway. So, I think that this is fairly close to, like, how we're doing observability in practice. So, in my view, an observability transformation fits alongside other transformations which, when done right, leads to much more productive organizations. So, with Agile, we move from waterfall to more incremental development. With DevOps, DevSecOps, all of that, we move from silos to more cross-functional teams with cloud, like it or load it. We move from buying things up front and hoping that they were the right things to buying things on tap as and when we needed it. So, with observability, we're really talking about moving everything 90 degrees. So, instead of observing individual systems, we're going to observe requests as they go through them. This should also act as a warning. Just, who has gone through an Agile transformation? Keep your hand up if you think that that went really, really well. Yeah. And I'm using this word very, very specifically because this is another thing I want us to pocket as we're going through this. You do need to think of this as a transformation and you need to think about the kind of pitfalls of other types of transformations and how to overcome them if you want to introduce observability to your company, client, whoever. Okay. So, we're all aligned, please, on what observability is. We know we want it, but we don't get to decide. So, we need to think about who we need to convince. Although you could probably get away, especially in smaller or more agile companies, we're just convincing a couple of people and going ahead, often with this sort of thing, you're going to have to convince a lot of people. And so, this is me capturing three broad groups of stakeholders here that you're going to want to convince if you want to bring people along with this observability transformation. And you want to get everyone on board because if you only get, for example, the C-suite on board, like the higher ups, if you like, on board, then engineers will just make your product fail, make your transformation fail so that they can get back to their work, like with any other thing. And then management will just say, right, I've just lost a load of productivity, we can solve this by getting rid of this observability thing. Similarly, if you get your engineers on board and they keep pushing towards it, you'll land up with them being burnt out because they're not being given the time and the resources that they need to actually make it work. So, it's worth thinking through very quickly here, wary of time. I can spend ages on this slide, by the way, because thinking about stakeholders is really, really interesting, but I'm just going to pick up a few highlights. As an example, anyone here a skeptic would describe themselves as an observability skeptic? I'd imagine, maybe, do you have any reasoning? No, that's fair enough. But it's worth noting that even in here, and I think that there's lots of people outside, the thing I compare it to is kind of transforming towards test-driven development. A lot of places will introduce test-driven development and the way that they'll do it, for example, is their experience will be that some manager somewhere will insist on 100% test coverage. So, they've gone through that, they have to do all these ridiculous things to jump through hoops to get this transformation to be complete, and then they come out at the end of it saying, well, test-driven development's crap, we're not doing this. They managed to get rid of it and they managed to dump it. So, you might think that of these three people, the engineers would be the easiest to convince, but there are lots of people that are out there that have gone through three or four of these now and really need to be sold on whether this is going to help them. So, really, don't think that they're going to be automatically on your side just because you're convinced. Also, I'll note all the disagreement that we have just in this one conference about what the best tooling and the best approaches are anyway. Quickly on things like management, management will want to be convinced that it's not going to break down productivity. One example I'll give when we're looking at, for example, higher ups like the C-suite, they're going to be interested, you're going to be asking them to spend money because you can't just say, oh, we're going to do this, you want to actually resource a team. With my client, what we did was we actually went through the outages over the last 12 months and we did some estimates, we said there are estimates and we caveated like what the caveats are. We went through and we worked out how much time we think would be saved on outages, on each of these outages, if they had good instrumentation of their code and if we could identify the issues more quickly. They could go away and they could calculate that in terms of a cost which they could use to justify it. So, don't forget about your stakeholders. One thing that you didn't hear is in all of that, is what tool to use. That's because, sorry, everyone that makes a tool, it largely doesn't matter at this point. People want traces because they want less downtime, they want more clarity, they want to capture lost revenue or whatever else. But you can do that with pretty much any observability tool right now. So, the one thing you don't want to do as part of convincing people is to try and sell them on a specific tool. That can come later. In my engagement, we're focusing on tempo. And the reason that we're doing that, I'll introduce some of the other reasons in a bit, but the main reason is because we always use Grafana, we already use Prometheus, it slots right in. And we don't really have to discuss it much. There's another thing which is because tempo is open source, we don't have to involve as part of selling this project, a new vendor, and new commercials and stuff like that. So, open source to the rescue with that. But really, you want to get your project approved so you can go and start instrumenting code. Last thing I'll say on this is team topology. This is an example of the sort of team that I'd expect to go and start an observability transformation. You'd want, I prefer smaller, more agile teams. So, you might look at this and think, well, based on my business, I might have two or three of your software engineers, two or three of your operations engineers. That might be an anti-pattern. You can go and look up all the reasons why bigger teams tend to do work more slowly. I'm not going to cover that now. So, I'm looking at a kind of crack team. Software engineers are going to get in and go and instrument the code. We've got an operations engineer that's going to make sure that we clear the pathways to actually get those spans out into tracing databases. And finally, we've got somebody that's kind of in a product owner position that's going to protect that team, make sure that they're not answering inane questions all the time. And that is also going to be working with the business and with the other product delivery teams and the platform team and whoever else to make sure that concerns are raised, that they're heard, that they pivot when they realize that actually they've made a mistake. So, that's an important role as well. But remember, this is a transformation and we're trying to do new things. So, we're changing cultures here. So, you do need to be responsive to feedback and you need to be responsive to feelings. Otherwise, your engineers here are going to make the best system that never gets used, which is another pitfall of transformations. Okay. Those are my thoughts on convincing people to do an observability transformation. Now, let's imagine you've got the thumbs up. Let's move towards implementation. Most important thing is to not get bogged down in the details of the infrastructure. You need to move to instrumentation. But, you are going to have to need some sort of tracing database. You are going to need some sort of tooling. If you have something already, so for example, if you're already using a provider of some sort and they have it, then great. Consider that. However, one of the ways that you can make sure that these things move faster is by moving your tracing database into where the data is that you're collecting. You think about big, old companies, big and or old companies. They get really nervous when you say, right, we're going to collect all this data and we're going to go put in this cloud provider over here. Now, that can take months to agree. And so what you can do is you can short circuit that, start that process, start discussing how you're going to do this. But you can also at the same time move your tracing databases into maybe the accounts or the cloud provider that's actually already been agreed to use this. There is a downside here, which some of you might be thinking is, well, doesn't that mean, James, that you'd have maybe multiple tracing databases, which means that you wouldn't have all your spans in the same place? And that is true, but it means that you can move on to instrumentation. It means that you can move to the point where you have like maybe two traces that somebody has to look to, and then you can get other people in the business to say, hey, wouldn't it be useful if, and then you can start having the discussions. Don't try and boil the ocean on these things. And we're being pragmatic here. So as an example here, this is if your client is in AWS, you can quickly get Tempo. There's a good article on the Grafana website deploying Tempo on Fargate, which means that you can get that up nice and quickly. So again, that's an advantage of using these things. And more importantly, you can deploy it. You can find out it's the wrong thing to do, and you can go do something else. And it's a great thing about using these open source tools is you can really work it out as you're moving. With that in mind, get instrumenting. And know that to start with that team that I put together earlier is going to be doing a lot of the work themselves. Automatic instrumentation is your friend. Get your software engineer to go and find the code bases that are across the system, especially on your hotpaths, and start raising PRs to auto instrument. You know how best to do these in your company. Some companies, they want to start the conversation with a PR. Sometimes they want to start with a meeting or something like this. But getting auto instrumentation in to these code bases will mean that you will start being able to build up the shallow layer of these traces. Then if any teams start becoming interested in this, opportunistically pair your software engineer with those teams. Pairing mobbing is a great way to share knowledge. Remember, a lot of these software engineers will not have done this kind of thing before, and doing it's kind of hard if you don't know how to do it. You don't want them to get frustrated to throw in the towel and say, no, this is dumb. This is hard. This is not the way we used to do things. Whereas if you can put your software engineer in with a pair as a pairing or a mobbing situation, they will have happy times and everything will be lovely. Also, make sure that you point out the value when you see it. It's very easy for us to see these things and to go, oh, it's great. And so obviously it's great. But this is new to people. So point out the 10% of their queries that has like this weird choke point. Point out all these advantages you're getting from this instrumentation and from all these spans as you're collecting them. Find, when there's an issue, when there's downtime, get your team to go and see if they can race the people that are doing incident response to finding where the issue is based on the tracing. Once these teams realize that they can see through walls with this stuff, they'll soon start instrumenting their own code. But you need to get them to look. Another trap is to get bogged down on the problems that are harder to instrument. Airlines and banks and other places have a bad habit and that bad habit is Fortran. Or like Zidark or some mainframe thing or whatever. If anyone here, has anyone here just put your hand up if you do any development on like COBOL, Fortran, anything like that? Awesome, awesome. If you go an instrument something like that, please come and talk about it. That sounds awesome. That sounds like a lot of people are going to talk about this one. I'll be fascinated by it. But if you're doing this kind of project, now is not the time. Something like that is not really going to correct me if I'm wrong, anybody out there. I don't think there's any instrumentation for Fortran code or anything like that. Treat it as a third party system. And also don't try and instrument other people's code. I've seen this happen. People will go, right, okay, we've got this third party and it's this third party code that we deploy. How are we going to instrument that? Do not instrument the stuff that is there and then accept that you're going to get to a point where it's going to roll over to logs and metrics. If the tool that you're using allows you to be able to connect up logs and metrics to your traces, that's really handy because remember in these big organizations, you might never reach the golden sunlit uplands of traces for everything. So you're always going to have to go back. You can think of it sort of like fast travelling through the infrastructure as that you are not going to be able to get to the point where necessarily you're going to be able to get into the point in the Oracle database that you're really trying to kill that has actually had this problem. But you will be able to fast travel to the bit in the code where it makes a query to an Oracle database and then you'll know which logs and traces to look at. So the goal really is for wide coverage, especially of hot paths. And that brings us to another thing which is culture change. So you've been working on this for maybe six months or so. It's fairly short projects. You've gotten traces. You've got end to end on many of the request pass through the systems. People kind of get observability now. So those three people will come out with a few others and build an observability engineering team, right? I would say that for most organizations, that's the wrong way to do things. There are companies for which observability engineering having separate teams stood up for that kind of thing does make sense. But for most places, you're really going to be looking at creating this kind of. This is one of my favorite slides ever, which is weird. I have weird favorite things. But this talks about like a DevOps transformation where what you do is you create a DevOps team and the best DevOps team disappears after like six or twelve months because what it's done is it's created this thing where they come together. And you should, you know, this is a valid way of doing things for observability as well. Ultimately, you may have an enablement team. However, instrumentation should be being done by devs as part of their day to day work The tooling needs to, oh five minutes. Oh, slow down. Enablement should be sharing best practices and doing training and stuff. The tooling really needs to be absorbed into an existing platform team. And this is the really cool part is that now, if you think about it, you've gotten to this point where you've got all this instrumentation into your codes. You can start thinking about what kind of tooling makes sense for your organization. Whereas when you started, it's very hard to do. That wasn't five minutes. Okay, I'll stop. So, yeah, if you've done your job well, hopefully these people won't need you anymore. And you can go and absorb back into teams and you can call that project complete. You might be able to do some kind of enablement team. But as I said, that wasn't meant to have a question mark at the end of it. Go and effect change. I'm going to end it there. There wasn't much time. I've got so much more I want to talk about on this subject. So I might do a follow up thing. If you have any questions, I'd be happy to answer them and you can find where to find me at that website. Thanks, James. We have still five minutes for Q&A. Some questions here. Okay. There is one. I answered almost everything. Hi. How long has it taken for you to convince a big org and an old company to move from no observability to some sort of observability? Completely convinced. I'll let you know. So I maybe joined back in May with this client and was helping them with a previous project that was getting wrapped up. So I'd say it's been eight or nine months working on other things and identifying this as a need where it's been working through. Yeah, it can take time. So because you've got all, as I said, you go back to that stakeholder slide. I could have spent a whole 20 minutes just on that because you've got to kind of get everybody aligned. I've done lots of like meetings. I've shown the people off, shown things off to people. I've shown off all these slides and stuff, gotten everybody on board. And yeah, so I think by the time, you know, I'd say that everyone's actually in lockstep, probably about now actually. I should say though, by the way, is we didn't, you know, just not do anything until that point. So there's been lots of opportunities to like seed things as we've been doing other work as well. So yeah. All right. Some questions? No, then thanks, James. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.24, "text": " Hello everybody. Can you hear me? Is it working? Awesome. This is my first talk, so I'm just", "tokens": [50364, 2425, 2201, 13, 1664, 291, 1568, 385, 30, 1119, 309, 1364, 30, 10391, 13, 639, 307, 452, 700, 751, 11, 370, 286, 478, 445, 50976], "temperature": 0.0, "avg_logprob": -0.3170408700641833, "compression_ratio": 1.4, "no_speech_prob": 0.5850480198860168}, {"id": 1, "seek": 0, "start": 12.24, "end": 20.400000000000002, "text": " going to do this because hello, FOSTA! Yes! Great to hear some energy in here at this", "tokens": [50976, 516, 281, 360, 341, 570, 7751, 11, 479, 4367, 8241, 0, 1079, 0, 3769, 281, 1568, 512, 2281, 294, 510, 412, 341, 51384], "temperature": 0.0, "avg_logprob": -0.3170408700641833, "compression_ratio": 1.4, "no_speech_prob": 0.5850480198860168}, {"id": 2, "seek": 0, "start": 20.400000000000002, "end": 28.400000000000002, "text": " time. Right, so my name's James and my major client currently is a, so my major client,", "tokens": [51384, 565, 13, 1779, 11, 370, 452, 1315, 311, 5678, 293, 452, 2563, 6423, 4362, 307, 257, 11, 370, 452, 2563, 6423, 11, 51784], "temperature": 0.0, "avg_logprob": -0.3170408700641833, "compression_ratio": 1.4, "no_speech_prob": 0.5850480198860168}, {"id": 3, "seek": 2840, "start": 28.4, "end": 33.68, "text": " my only client currently is a major European airline. Get that right? And I wanted to talk", "tokens": [50364, 452, 787, 6423, 4362, 307, 257, 2563, 6473, 29528, 13, 3240, 300, 558, 30, 400, 286, 1415, 281, 751, 50628], "temperature": 0.0, "avg_logprob": -0.15881477702747693, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0605580098927021}, {"id": 4, "seek": 2840, "start": 33.68, "end": 39.32, "text": " to you today about some of the challenges that we're facing in introducing observability", "tokens": [50628, 281, 291, 965, 466, 512, 295, 264, 4759, 300, 321, 434, 7170, 294, 15424, 9951, 2310, 50910], "temperature": 0.0, "avg_logprob": -0.15881477702747693, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0605580098927021}, {"id": 5, "seek": 2840, "start": 39.32, "end": 44.56, "text": " to that client, a framework that I've kind of put together to overcome those challenges", "tokens": [50910, 281, 300, 6423, 11, 257, 8388, 300, 286, 600, 733, 295, 829, 1214, 281, 10473, 729, 4759, 51172], "temperature": 0.0, "avg_logprob": -0.15881477702747693, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0605580098927021}, {"id": 6, "seek": 2840, "start": 44.56, "end": 51.16, "text": " and some thoughts that I have overall about observability. This talk should be applicable", "tokens": [51172, 293, 512, 4598, 300, 286, 362, 4787, 466, 9951, 2310, 13, 639, 751, 820, 312, 21142, 51502], "temperature": 0.0, "avg_logprob": -0.15881477702747693, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0605580098927021}, {"id": 7, "seek": 2840, "start": 51.16, "end": 55.72, "text": " to any big organization. So there's not really anything that's specific to an airline, but", "tokens": [51502, 281, 604, 955, 4475, 13, 407, 456, 311, 406, 534, 1340, 300, 311, 2685, 281, 364, 29528, 11, 457, 51730], "temperature": 0.0, "avg_logprob": -0.15881477702747693, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0605580098927021}, {"id": 8, "seek": 5572, "start": 55.72, "end": 60.16, "text": " if you think about the scale of not only the size, but the amount of different tasks", "tokens": [50364, 498, 291, 519, 466, 264, 4373, 295, 406, 787, 264, 2744, 11, 457, 264, 2372, 295, 819, 9608, 50586], "temperature": 0.0, "avg_logprob": -0.14185322182519094, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.017284706234931946}, {"id": 9, "seek": 5572, "start": 60.16, "end": 64.76, "text": " an airline would be doing and the kind of vintage of most major airlines, you'll kind", "tokens": [50586, 364, 29528, 576, 312, 884, 293, 264, 733, 295, 23050, 295, 881, 2563, 37147, 11, 291, 603, 733, 50816], "temperature": 0.0, "avg_logprob": -0.14185322182519094, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.017284706234931946}, {"id": 10, "seek": 5572, "start": 64.76, "end": 70.03999999999999, "text": " of get an idea of what we're talking about here. By the way, just as an idea, who here", "tokens": [50816, 295, 483, 364, 1558, 295, 437, 321, 434, 1417, 466, 510, 13, 3146, 264, 636, 11, 445, 382, 364, 1558, 11, 567, 510, 51080], "temperature": 0.0, "avg_logprob": -0.14185322182519094, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.017284706234931946}, {"id": 11, "seek": 5572, "start": 70.03999999999999, "end": 77.44, "text": " works for like a company that's got more than a thousand people in it? Okay, fair enough.", "tokens": [51080, 1985, 337, 411, 257, 2237, 300, 311, 658, 544, 813, 257, 4714, 561, 294, 309, 30, 1033, 11, 3143, 1547, 13, 51450], "temperature": 0.0, "avg_logprob": -0.14185322182519094, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.017284706234931946}, {"id": 12, "seek": 5572, "start": 77.44, "end": 84.68, "text": " Okay. And how many of those people are actually using observability on any scale? Okay, some", "tokens": [51450, 1033, 13, 400, 577, 867, 295, 729, 561, 366, 767, 1228, 9951, 2310, 322, 604, 4373, 30, 1033, 11, 512, 51812], "temperature": 0.0, "avg_logprob": -0.14185322182519094, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.017284706234931946}, {"id": 13, "seek": 8468, "start": 84.72000000000001, "end": 89.12, "text": " of you, awesome. You should be doing this instead. In this talk, I want to walk you", "tokens": [50366, 295, 291, 11, 3476, 13, 509, 820, 312, 884, 341, 2602, 13, 682, 341, 751, 11, 286, 528, 281, 1792, 291, 50586], "temperature": 0.0, "avg_logprob": -0.12394223314650515, "compression_ratio": 1.9390243902439024, "no_speech_prob": 0.45750442147254944}, {"id": 14, "seek": 8468, "start": 89.12, "end": 93.44000000000001, "text": " through three steps I'm taking to introduce observability. One, I'm calling an observability", "tokens": [50586, 807, 1045, 4439, 286, 478, 1940, 281, 5366, 9951, 2310, 13, 1485, 11, 286, 478, 5141, 364, 9951, 2310, 50802], "temperature": 0.0, "avg_logprob": -0.12394223314650515, "compression_ratio": 1.9390243902439024, "no_speech_prob": 0.45750442147254944}, {"id": 15, "seek": 8468, "start": 93.44000000000001, "end": 97.84, "text": " transformation. We're not going to be talking about anything too technically exciting here,", "tokens": [50802, 9887, 13, 492, 434, 406, 516, 281, 312, 1417, 466, 1340, 886, 12120, 4670, 510, 11, 51022], "temperature": 0.0, "avg_logprob": -0.12394223314650515, "compression_ratio": 1.9390243902439024, "no_speech_prob": 0.45750442147254944}, {"id": 16, "seek": 8468, "start": 97.84, "end": 100.52000000000001, "text": " and we're certainly not going to be talking about anything like introducing observability", "tokens": [51022, 293, 321, 434, 3297, 406, 516, 281, 312, 1417, 466, 1340, 411, 15424, 9951, 2310, 51156], "temperature": 0.0, "avg_logprob": -0.12394223314650515, "compression_ratio": 1.9390243902439024, "no_speech_prob": 0.45750442147254944}, {"id": 17, "seek": 8468, "start": 100.52000000000001, "end": 104.44000000000001, "text": " to like the cockpit or anything like that. This talk is about helping you get your company", "tokens": [51156, 281, 411, 264, 35990, 420, 1340, 411, 300, 13, 639, 751, 307, 466, 4315, 291, 483, 428, 2237, 51352], "temperature": 0.0, "avg_logprob": -0.12394223314650515, "compression_ratio": 1.9390243902439024, "no_speech_prob": 0.45750442147254944}, {"id": 18, "seek": 8468, "start": 104.44000000000001, "end": 109.44000000000001, "text": " or client or whoever else on board with observability. It's about making that transition successful", "tokens": [51352, 420, 6423, 420, 11387, 1646, 322, 3150, 365, 9951, 2310, 13, 467, 311, 466, 1455, 300, 6034, 4406, 51602], "temperature": 0.0, "avg_logprob": -0.12394223314650515, "compression_ratio": 1.9390243902439024, "no_speech_prob": 0.45750442147254944}, {"id": 19, "seek": 8468, "start": 109.44000000000001, "end": 114.4, "text": " and it's about making it sustainable. And of course, the associated love and adoration", "tokens": [51602, 293, 309, 311, 466, 1455, 309, 11235, 13, 400, 295, 1164, 11, 264, 6615, 959, 293, 614, 9357, 51850], "temperature": 0.0, "avg_logprob": -0.12394223314650515, "compression_ratio": 1.9390243902439024, "no_speech_prob": 0.45750442147254944}, {"id": 20, "seek": 11440, "start": 114.44000000000001, "end": 120.16000000000001, "text": " of your peers for making their lives a whole hell of a lot easier. So, first thing I want", "tokens": [50366, 295, 428, 16739, 337, 1455, 641, 2909, 257, 1379, 4921, 295, 257, 688, 3571, 13, 407, 11, 700, 551, 286, 528, 50652], "temperature": 0.0, "avg_logprob": -0.14925629517127728, "compression_ratio": 1.7804878048780488, "no_speech_prob": 0.00816174317151308}, {"id": 21, "seek": 11440, "start": 120.16000000000001, "end": 127.16000000000001, "text": " us to do is align us on what observability is. So, that'll be easy. Does anyone want", "tokens": [50652, 505, 281, 360, 307, 7975, 505, 322, 437, 9951, 2310, 307, 13, 407, 11, 300, 603, 312, 1858, 13, 4402, 2878, 528, 51002], "temperature": 0.0, "avg_logprob": -0.14925629517127728, "compression_ratio": 1.7804878048780488, "no_speech_prob": 0.00816174317151308}, {"id": 22, "seek": 11440, "start": 127.28, "end": 131.52, "text": " to, I tell you what, we're running late, so I'll just tell you what observability is.", "tokens": [51008, 281, 11, 286, 980, 291, 437, 11, 321, 434, 2614, 3469, 11, 370, 286, 603, 445, 980, 291, 437, 9951, 2310, 307, 13, 51220], "temperature": 0.0, "avg_logprob": -0.14925629517127728, "compression_ratio": 1.7804878048780488, "no_speech_prob": 0.00816174317151308}, {"id": 23, "seek": 11440, "start": 131.52, "end": 136.72, "text": " Firstly, I think that what we've got to remember when we're talking about observability is", "tokens": [51220, 20042, 11, 286, 519, 300, 437, 321, 600, 658, 281, 1604, 562, 321, 434, 1417, 466, 9951, 2310, 307, 51480], "temperature": 0.0, "avg_logprob": -0.14925629517127728, "compression_ratio": 1.7804878048780488, "no_speech_prob": 0.00816174317151308}, {"id": 24, "seek": 11440, "start": 136.72, "end": 140.04000000000002, "text": " that a lot of people don't really know what to think of, but they're probably thinking", "tokens": [51480, 300, 257, 688, 295, 561, 500, 380, 534, 458, 437, 281, 519, 295, 11, 457, 436, 434, 1391, 1953, 51646], "temperature": 0.0, "avg_logprob": -0.14925629517127728, "compression_ratio": 1.7804878048780488, "no_speech_prob": 0.00816174317151308}, {"id": 25, "seek": 14004, "start": 140.04, "end": 147.04, "text": " of something like this, like a big ten-foot view of everything that's going on. Obviously", "tokens": [50364, 295, 746, 411, 341, 11, 411, 257, 955, 2064, 12, 13498, 1910, 295, 1203, 300, 311, 516, 322, 13, 7580, 50714], "temperature": 0.0, "avg_logprob": -0.15203864600068778, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.11514902114868164}, {"id": 26, "seek": 14004, "start": 147.04, "end": 152.68, "text": " most people in here won't think that that's observability. Why not? Can anyone say, like,", "tokens": [50714, 881, 561, 294, 510, 1582, 380, 519, 300, 300, 311, 9951, 2310, 13, 1545, 406, 30, 1664, 2878, 584, 11, 411, 11, 50996], "temperature": 0.0, "avg_logprob": -0.15203864600068778, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.11514902114868164}, {"id": 27, "seek": 14004, "start": 152.68, "end": 159.68, "text": " is this observable? Is this an observable system by our definition? This is what I think", "tokens": [50996, 307, 341, 9951, 712, 30, 1119, 341, 364, 9951, 712, 1185, 538, 527, 7123, 30, 639, 307, 437, 286, 519, 51346], "temperature": 0.0, "avg_logprob": -0.15203864600068778, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.11514902114868164}, {"id": 28, "seek": 14004, "start": 160.88, "end": 165.28, "text": " of when I think of observability. And when I speak to anybody that, you know, may be", "tokens": [51406, 295, 562, 286, 519, 295, 9951, 2310, 13, 400, 562, 286, 1710, 281, 4472, 300, 11, 291, 458, 11, 815, 312, 51626], "temperature": 0.0, "avg_logprob": -0.15203864600068778, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.11514902114868164}, {"id": 29, "seek": 16528, "start": 165.36, "end": 169.64000000000001, "text": " lay technical or non-technical, this is the kind of thing that I'll introduce to them.", "tokens": [50368, 2360, 6191, 420, 2107, 12, 29113, 804, 11, 341, 307, 264, 733, 295, 551, 300, 286, 603, 5366, 281, 552, 13, 50582], "temperature": 0.0, "avg_logprob": -0.12510997708104238, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.015156280249357224}, {"id": 30, "seek": 16528, "start": 169.64000000000001, "end": 174.36, "text": " I know that I'm putting a definition on something that, you know, and that's a little bit controversial,", "tokens": [50582, 286, 458, 300, 286, 478, 3372, 257, 7123, 322, 746, 300, 11, 291, 458, 11, 293, 300, 311, 257, 707, 857, 17323, 11, 50818], "temperature": 0.0, "avg_logprob": -0.12510997708104238, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.015156280249357224}, {"id": 31, "seek": 16528, "start": 174.36, "end": 178.48, "text": " but this is what I think of. So, this will help you kind of ground in what this talk", "tokens": [50818, 457, 341, 307, 437, 286, 519, 295, 13, 407, 11, 341, 486, 854, 291, 733, 295, 2727, 294, 437, 341, 751, 51024], "temperature": 0.0, "avg_logprob": -0.12510997708104238, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.015156280249357224}, {"id": 32, "seek": 16528, "start": 178.48, "end": 183.48, "text": " is about. So, you can imagine, like, as we went through that previous thing, like, there's", "tokens": [51024, 307, 466, 13, 407, 11, 291, 393, 3811, 11, 411, 11, 382, 321, 1437, 807, 300, 3894, 551, 11, 411, 11, 456, 311, 51274], "temperature": 0.0, "avg_logprob": -0.12510997708104238, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.015156280249357224}, {"id": 33, "seek": 16528, "start": 183.48, "end": 190.08, "text": " this cake being made. And so, you know, I can describe quite easily that previously,", "tokens": [51274, 341, 5908, 885, 1027, 13, 400, 370, 11, 291, 458, 11, 286, 393, 6786, 1596, 3612, 300, 8046, 11, 51604], "temperature": 0.0, "avg_logprob": -0.12510997708104238, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.015156280249357224}, {"id": 34, "seek": 19008, "start": 190.08, "end": 195.60000000000002, "text": " with a, like, monitoring process, we would monitor, get the metrics and the logs from", "tokens": [50364, 365, 257, 11, 411, 11, 11028, 1399, 11, 321, 576, 6002, 11, 483, 264, 16367, 293, 264, 20820, 490, 50640], "temperature": 0.0, "avg_logprob": -0.14338899339948383, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.010080892592668533}, {"id": 35, "seek": 19008, "start": 195.60000000000002, "end": 200.88000000000002, "text": " each individual component of that system. But now what we're going to do is we're going", "tokens": [50640, 1184, 2609, 6542, 295, 300, 1185, 13, 583, 586, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 50904], "temperature": 0.0, "avg_logprob": -0.14338899339948383, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.010080892592668533}, {"id": 36, "seek": 19008, "start": 200.88000000000002, "end": 207.88000000000002, "text": " to collect the request for a cake through that system. And this has some clear value", "tokens": [50904, 281, 2500, 264, 5308, 337, 257, 5908, 807, 300, 1185, 13, 400, 341, 575, 512, 1850, 2158, 51254], "temperature": 0.0, "avg_logprob": -0.14338899339948383, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.010080892592668533}, {"id": 37, "seek": 19008, "start": 208.24, "end": 211.56, "text": " if we start talking about this. There's this other way of talking about it that's like,", "tokens": [51272, 498, 321, 722, 1417, 466, 341, 13, 821, 311, 341, 661, 636, 295, 1417, 466, 309, 300, 311, 411, 11, 51438], "temperature": 0.0, "avg_logprob": -0.14338899339948383, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.010080892592668533}, {"id": 38, "seek": 19008, "start": 211.56, "end": 218.56, "text": " you know, observability is how we understand something by the internal something, I can't", "tokens": [51438, 291, 458, 11, 9951, 2310, 307, 577, 321, 1223, 746, 538, 264, 6920, 746, 11, 286, 393, 380, 51788], "temperature": 0.0, "avg_logprob": -0.14338899339948383, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.010080892592668533}, {"id": 39, "seek": 21856, "start": 219.28, "end": 223.76, "text": " say, but it doesn't really kind of get across the value to people that may be a bit skeptical", "tokens": [50400, 584, 11, 457, 309, 1177, 380, 534, 733, 295, 483, 2108, 264, 2158, 281, 561, 300, 815, 312, 257, 857, 28601, 50624], "temperature": 0.0, "avg_logprob": -0.15850393121892756, "compression_ratio": 1.9, "no_speech_prob": 0.002071500290185213}, {"id": 40, "seek": 21856, "start": 223.76, "end": 227.32, "text": " about this. And I think that this kind of does. So, let's just pocket that idea for", "tokens": [50624, 466, 341, 13, 400, 286, 519, 300, 341, 733, 295, 775, 13, 407, 11, 718, 311, 445, 8963, 300, 1558, 337, 50802], "temperature": 0.0, "avg_logprob": -0.15850393121892756, "compression_ratio": 1.9, "no_speech_prob": 0.002071500290185213}, {"id": 41, "seek": 21856, "start": 227.32, "end": 234.32, "text": " a second. This idea basically describes observability as recording work done to satisfy a request.", "tokens": [50802, 257, 1150, 13, 639, 1558, 1936, 15626, 9951, 2310, 382, 6613, 589, 1096, 281, 19319, 257, 5308, 13, 51152], "temperature": 0.0, "avg_logprob": -0.15850393121892756, "compression_ratio": 1.9, "no_speech_prob": 0.002071500290185213}, {"id": 42, "seek": 21856, "start": 236.6, "end": 242.6, "text": " So, a request is completely observable when you can see all the work done to that request,", "tokens": [51266, 407, 11, 257, 5308, 307, 2584, 9951, 712, 562, 291, 393, 536, 439, 264, 589, 1096, 281, 300, 5308, 11, 51566], "temperature": 0.0, "avg_logprob": -0.15850393121892756, "compression_ratio": 1.9, "no_speech_prob": 0.002071500290185213}, {"id": 43, "seek": 21856, "start": 242.6, "end": 246.08, "text": " and a system is completely observable when you can see all the work done to all requests", "tokens": [51566, 293, 257, 1185, 307, 2584, 9951, 712, 562, 291, 393, 536, 439, 264, 589, 1096, 281, 439, 12475, 51740], "temperature": 0.0, "avg_logprob": -0.15850393121892756, "compression_ratio": 1.9, "no_speech_prob": 0.002071500290185213}, {"id": 44, "seek": 24608, "start": 246.08, "end": 250.56, "text": " moving through a system. This to me is much more tangible. It does tie it specifically", "tokens": [50364, 2684, 807, 257, 1185, 13, 639, 281, 385, 307, 709, 544, 27094, 13, 467, 775, 7582, 309, 4682, 50588], "temperature": 0.0, "avg_logprob": -0.11197183699834914, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.024013517424464226}, {"id": 45, "seek": 24608, "start": 250.56, "end": 254.52, "text": " to requests or events. However, I do note that when we talk about observability and", "tokens": [50588, 281, 12475, 420, 3931, 13, 2908, 11, 286, 360, 3637, 300, 562, 321, 751, 466, 9951, 2310, 293, 50786], "temperature": 0.0, "avg_logprob": -0.11197183699834914, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.024013517424464226}, {"id": 46, "seek": 24608, "start": 254.52, "end": 260.0, "text": " making long running processes observable, most people try and arbitrarily or otherwise", "tokens": [50786, 1455, 938, 2614, 7555, 9951, 712, 11, 881, 561, 853, 293, 19071, 3289, 420, 5911, 51060], "temperature": 0.0, "avg_logprob": -0.11197183699834914, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.024013517424464226}, {"id": 47, "seek": 24608, "start": 260.0, "end": 264.56, "text": " find ways of cutting them up into individual traces anyway. So, I think that this is fairly", "tokens": [51060, 915, 2098, 295, 6492, 552, 493, 666, 2609, 26076, 4033, 13, 407, 11, 286, 519, 300, 341, 307, 6457, 51288], "temperature": 0.0, "avg_logprob": -0.11197183699834914, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.024013517424464226}, {"id": 48, "seek": 24608, "start": 264.56, "end": 270.96000000000004, "text": " close to, like, how we're doing observability in practice. So, in my view, an observability", "tokens": [51288, 1998, 281, 11, 411, 11, 577, 321, 434, 884, 9951, 2310, 294, 3124, 13, 407, 11, 294, 452, 1910, 11, 364, 9951, 2310, 51608], "temperature": 0.0, "avg_logprob": -0.11197183699834914, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.024013517424464226}, {"id": 49, "seek": 27096, "start": 270.96, "end": 277.35999999999996, "text": " transformation fits alongside other transformations which, when done right, leads to much more", "tokens": [50364, 9887, 9001, 12385, 661, 34852, 597, 11, 562, 1096, 558, 11, 6689, 281, 709, 544, 50684], "temperature": 0.0, "avg_logprob": -0.18389014477999704, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.23439763486385345}, {"id": 50, "seek": 27096, "start": 277.35999999999996, "end": 282.12, "text": " productive organizations. So, with Agile, we move from waterfall to more incremental", "tokens": [50684, 13304, 6150, 13, 407, 11, 365, 2725, 794, 11, 321, 1286, 490, 27848, 281, 544, 35759, 50922], "temperature": 0.0, "avg_logprob": -0.18389014477999704, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.23439763486385345}, {"id": 51, "seek": 27096, "start": 282.12, "end": 288.12, "text": " development. With DevOps, DevSecOps, all of that, we move from silos to more cross-functional", "tokens": [50922, 3250, 13, 2022, 43051, 11, 9096, 29511, 36179, 11, 439, 295, 300, 11, 321, 1286, 490, 48893, 281, 544, 3278, 12, 22845, 304, 51222], "temperature": 0.0, "avg_logprob": -0.18389014477999704, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.23439763486385345}, {"id": 52, "seek": 27096, "start": 288.12, "end": 295.24, "text": " teams with cloud, like it or load it. We move from buying things up front and hoping that", "tokens": [51222, 5491, 365, 4588, 11, 411, 309, 420, 3677, 309, 13, 492, 1286, 490, 6382, 721, 493, 1868, 293, 7159, 300, 51578], "temperature": 0.0, "avg_logprob": -0.18389014477999704, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.23439763486385345}, {"id": 53, "seek": 27096, "start": 295.24, "end": 300.76, "text": " they were the right things to buying things on tap as and when we needed it. So, with", "tokens": [51578, 436, 645, 264, 558, 721, 281, 6382, 721, 322, 5119, 382, 293, 562, 321, 2978, 309, 13, 407, 11, 365, 51854], "temperature": 0.0, "avg_logprob": -0.18389014477999704, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.23439763486385345}, {"id": 54, "seek": 30076, "start": 300.76, "end": 305.48, "text": " observability, we're really talking about moving everything 90 degrees. So, instead", "tokens": [50364, 9951, 2310, 11, 321, 434, 534, 1417, 466, 2684, 1203, 4289, 5310, 13, 407, 11, 2602, 50600], "temperature": 0.0, "avg_logprob": -0.15815835907345727, "compression_ratio": 1.537117903930131, "no_speech_prob": 0.026508115231990814}, {"id": 55, "seek": 30076, "start": 305.48, "end": 313.56, "text": " of observing individual systems, we're going to observe requests as they go through them.", "tokens": [50600, 295, 22107, 2609, 3652, 11, 321, 434, 516, 281, 11441, 12475, 382, 436, 352, 807, 552, 13, 51004], "temperature": 0.0, "avg_logprob": -0.15815835907345727, "compression_ratio": 1.537117903930131, "no_speech_prob": 0.026508115231990814}, {"id": 56, "seek": 30076, "start": 313.56, "end": 320.28, "text": " This should also act as a warning. Just, who has gone through an Agile transformation?", "tokens": [51004, 639, 820, 611, 605, 382, 257, 9164, 13, 1449, 11, 567, 575, 2780, 807, 364, 2725, 794, 9887, 30, 51340], "temperature": 0.0, "avg_logprob": -0.15815835907345727, "compression_ratio": 1.537117903930131, "no_speech_prob": 0.026508115231990814}, {"id": 57, "seek": 30076, "start": 320.28, "end": 329.08, "text": " Keep your hand up if you think that that went really, really well. Yeah. And I'm using this", "tokens": [51340, 5527, 428, 1011, 493, 498, 291, 519, 300, 300, 1437, 534, 11, 534, 731, 13, 865, 13, 400, 286, 478, 1228, 341, 51780], "temperature": 0.0, "avg_logprob": -0.15815835907345727, "compression_ratio": 1.537117903930131, "no_speech_prob": 0.026508115231990814}, {"id": 58, "seek": 32908, "start": 329.08, "end": 333.2, "text": " word very, very specifically because this is another thing I want us to pocket as we're", "tokens": [50364, 1349, 588, 11, 588, 4682, 570, 341, 307, 1071, 551, 286, 528, 505, 281, 8963, 382, 321, 434, 50570], "temperature": 0.0, "avg_logprob": -0.1507767318585597, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.6038935780525208}, {"id": 59, "seek": 32908, "start": 333.2, "end": 337.88, "text": " going through this. You do need to think of this as a transformation and you need to think", "tokens": [50570, 516, 807, 341, 13, 509, 360, 643, 281, 519, 295, 341, 382, 257, 9887, 293, 291, 643, 281, 519, 50804], "temperature": 0.0, "avg_logprob": -0.1507767318585597, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.6038935780525208}, {"id": 60, "seek": 32908, "start": 337.88, "end": 343.08, "text": " about the kind of pitfalls of other types of transformations and how to overcome them", "tokens": [50804, 466, 264, 733, 295, 10147, 18542, 295, 661, 3467, 295, 34852, 293, 577, 281, 10473, 552, 51064], "temperature": 0.0, "avg_logprob": -0.1507767318585597, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.6038935780525208}, {"id": 61, "seek": 32908, "start": 343.08, "end": 351.96, "text": " if you want to introduce observability to your company, client, whoever. Okay. So, we're", "tokens": [51064, 498, 291, 528, 281, 5366, 9951, 2310, 281, 428, 2237, 11, 6423, 11, 11387, 13, 1033, 13, 407, 11, 321, 434, 51508], "temperature": 0.0, "avg_logprob": -0.1507767318585597, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.6038935780525208}, {"id": 62, "seek": 32908, "start": 351.96, "end": 357.47999999999996, "text": " all aligned, please, on what observability is. We know we want it, but we don't get to", "tokens": [51508, 439, 17962, 11, 1767, 11, 322, 437, 9951, 2310, 307, 13, 492, 458, 321, 528, 309, 11, 457, 321, 500, 380, 483, 281, 51784], "temperature": 0.0, "avg_logprob": -0.1507767318585597, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.6038935780525208}, {"id": 63, "seek": 35748, "start": 357.48, "end": 363.16, "text": " decide. So, we need to think about who we need to convince. Although you could probably", "tokens": [50364, 4536, 13, 407, 11, 321, 643, 281, 519, 466, 567, 321, 643, 281, 13447, 13, 5780, 291, 727, 1391, 50648], "temperature": 0.0, "avg_logprob": -0.12484941936674572, "compression_ratio": 1.792, "no_speech_prob": 0.4318794906139374}, {"id": 64, "seek": 35748, "start": 363.16, "end": 368.28000000000003, "text": " get away, especially in smaller or more agile companies, we're just convincing a couple", "tokens": [50648, 483, 1314, 11, 2318, 294, 4356, 420, 544, 30072, 3431, 11, 321, 434, 445, 24823, 257, 1916, 50904], "temperature": 0.0, "avg_logprob": -0.12484941936674572, "compression_ratio": 1.792, "no_speech_prob": 0.4318794906139374}, {"id": 65, "seek": 35748, "start": 368.28000000000003, "end": 371.40000000000003, "text": " of people and going ahead, often with this sort of thing, you're going to have to convince", "tokens": [50904, 295, 561, 293, 516, 2286, 11, 2049, 365, 341, 1333, 295, 551, 11, 291, 434, 516, 281, 362, 281, 13447, 51060], "temperature": 0.0, "avg_logprob": -0.12484941936674572, "compression_ratio": 1.792, "no_speech_prob": 0.4318794906139374}, {"id": 66, "seek": 35748, "start": 371.40000000000003, "end": 377.52000000000004, "text": " a lot of people. And so, this is me capturing three broad groups of stakeholders here that", "tokens": [51060, 257, 688, 295, 561, 13, 400, 370, 11, 341, 307, 385, 23384, 1045, 4152, 3935, 295, 17779, 510, 300, 51366], "temperature": 0.0, "avg_logprob": -0.12484941936674572, "compression_ratio": 1.792, "no_speech_prob": 0.4318794906139374}, {"id": 67, "seek": 35748, "start": 377.52000000000004, "end": 382.24, "text": " you're going to want to convince if you want to bring people along with this observability", "tokens": [51366, 291, 434, 516, 281, 528, 281, 13447, 498, 291, 528, 281, 1565, 561, 2051, 365, 341, 9951, 2310, 51602], "temperature": 0.0, "avg_logprob": -0.12484941936674572, "compression_ratio": 1.792, "no_speech_prob": 0.4318794906139374}, {"id": 68, "seek": 38224, "start": 382.24, "end": 387.96000000000004, "text": " transformation. And you want to get everyone on board because if you only get, for example,", "tokens": [50364, 9887, 13, 400, 291, 528, 281, 483, 1518, 322, 3150, 570, 498, 291, 787, 483, 11, 337, 1365, 11, 50650], "temperature": 0.0, "avg_logprob": -0.15527515751974924, "compression_ratio": 1.7547892720306513, "no_speech_prob": 0.7404835820198059}, {"id": 69, "seek": 38224, "start": 387.96000000000004, "end": 393.28000000000003, "text": " the C-suite on board, like the higher ups, if you like, on board, then engineers will", "tokens": [50650, 264, 383, 12, 33136, 322, 3150, 11, 411, 264, 2946, 15497, 11, 498, 291, 411, 11, 322, 3150, 11, 550, 11955, 486, 50916], "temperature": 0.0, "avg_logprob": -0.15527515751974924, "compression_ratio": 1.7547892720306513, "no_speech_prob": 0.7404835820198059}, {"id": 70, "seek": 38224, "start": 393.28000000000003, "end": 400.84000000000003, "text": " just make your product fail, make your transformation fail so that they can get back to their work,", "tokens": [50916, 445, 652, 428, 1674, 3061, 11, 652, 428, 9887, 3061, 370, 300, 436, 393, 483, 646, 281, 641, 589, 11, 51294], "temperature": 0.0, "avg_logprob": -0.15527515751974924, "compression_ratio": 1.7547892720306513, "no_speech_prob": 0.7404835820198059}, {"id": 71, "seek": 38224, "start": 400.84000000000003, "end": 404.64, "text": " like with any other thing. And then management will just say, right, I've just lost a load", "tokens": [51294, 411, 365, 604, 661, 551, 13, 400, 550, 4592, 486, 445, 584, 11, 558, 11, 286, 600, 445, 2731, 257, 3677, 51484], "temperature": 0.0, "avg_logprob": -0.15527515751974924, "compression_ratio": 1.7547892720306513, "no_speech_prob": 0.7404835820198059}, {"id": 72, "seek": 38224, "start": 404.64, "end": 409.88, "text": " of productivity, we can solve this by getting rid of this observability thing. Similarly,", "tokens": [51484, 295, 15604, 11, 321, 393, 5039, 341, 538, 1242, 3973, 295, 341, 9951, 2310, 551, 13, 13157, 11, 51746], "temperature": 0.0, "avg_logprob": -0.15527515751974924, "compression_ratio": 1.7547892720306513, "no_speech_prob": 0.7404835820198059}, {"id": 73, "seek": 40988, "start": 409.88, "end": 413.28, "text": " if you get your engineers on board and they keep pushing towards it, you'll land up with", "tokens": [50364, 498, 291, 483, 428, 11955, 322, 3150, 293, 436, 1066, 7380, 3030, 309, 11, 291, 603, 2117, 493, 365, 50534], "temperature": 0.0, "avg_logprob": -0.1513790907683196, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.01028046477586031}, {"id": 74, "seek": 40988, "start": 413.28, "end": 416.4, "text": " them being burnt out because they're not being given the time and the resources that", "tokens": [50534, 552, 885, 18901, 484, 570, 436, 434, 406, 885, 2212, 264, 565, 293, 264, 3593, 300, 50690], "temperature": 0.0, "avg_logprob": -0.1513790907683196, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.01028046477586031}, {"id": 75, "seek": 40988, "start": 416.4, "end": 423.36, "text": " they need to actually make it work. So, it's worth thinking through very quickly here,", "tokens": [50690, 436, 643, 281, 767, 652, 309, 589, 13, 407, 11, 309, 311, 3163, 1953, 807, 588, 2661, 510, 11, 51038], "temperature": 0.0, "avg_logprob": -0.1513790907683196, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.01028046477586031}, {"id": 76, "seek": 40988, "start": 423.36, "end": 429.24, "text": " wary of time. I can spend ages on this slide, by the way, because thinking about stakeholders", "tokens": [51038, 46585, 295, 565, 13, 286, 393, 3496, 12357, 322, 341, 4137, 11, 538, 264, 636, 11, 570, 1953, 466, 17779, 51332], "temperature": 0.0, "avg_logprob": -0.1513790907683196, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.01028046477586031}, {"id": 77, "seek": 40988, "start": 429.24, "end": 434.2, "text": " is really, really interesting, but I'm just going to pick up a few highlights. As an example,", "tokens": [51332, 307, 534, 11, 534, 1880, 11, 457, 286, 478, 445, 516, 281, 1888, 493, 257, 1326, 14254, 13, 1018, 364, 1365, 11, 51580], "temperature": 0.0, "avg_logprob": -0.1513790907683196, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.01028046477586031}, {"id": 78, "seek": 43420, "start": 435.2, "end": 442.84, "text": " anyone here a skeptic would describe themselves as an observability skeptic? I'd imagine,", "tokens": [50414, 2878, 510, 257, 19128, 299, 576, 6786, 2969, 382, 364, 9951, 2310, 19128, 299, 30, 286, 1116, 3811, 11, 50796], "temperature": 0.0, "avg_logprob": -0.2510119257746516, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.2535322308540344}, {"id": 79, "seek": 43420, "start": 442.84, "end": 448.2, "text": " maybe, do you have any reasoning? No, that's fair enough. But it's worth noting that even", "tokens": [50796, 1310, 11, 360, 291, 362, 604, 21577, 30, 883, 11, 300, 311, 3143, 1547, 13, 583, 309, 311, 3163, 26801, 300, 754, 51064], "temperature": 0.0, "avg_logprob": -0.2510119257746516, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.2535322308540344}, {"id": 80, "seek": 43420, "start": 448.2, "end": 453.12, "text": " in here, and I think that there's lots of people outside, the thing I compare it to is", "tokens": [51064, 294, 510, 11, 293, 286, 519, 300, 456, 311, 3195, 295, 561, 2380, 11, 264, 551, 286, 6794, 309, 281, 307, 51310], "temperature": 0.0, "avg_logprob": -0.2510119257746516, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.2535322308540344}, {"id": 81, "seek": 43420, "start": 453.12, "end": 458.8, "text": " kind of transforming towards test-driven development. A lot of places will introduce test-driven", "tokens": [51310, 733, 295, 27210, 3030, 1500, 12, 25456, 3250, 13, 316, 688, 295, 3190, 486, 5366, 1500, 12, 25456, 51594], "temperature": 0.0, "avg_logprob": -0.2510119257746516, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.2535322308540344}, {"id": 82, "seek": 43420, "start": 458.8, "end": 463.0, "text": " development and the way that they'll do it, for example, is their experience will be that", "tokens": [51594, 3250, 293, 264, 636, 300, 436, 603, 360, 309, 11, 337, 1365, 11, 307, 641, 1752, 486, 312, 300, 51804], "temperature": 0.0, "avg_logprob": -0.2510119257746516, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.2535322308540344}, {"id": 83, "seek": 46300, "start": 463.8, "end": 469.44, "text": " some manager somewhere will insist on 100% test coverage. So, they've gone through that,", "tokens": [50404, 512, 6598, 4079, 486, 13466, 322, 2319, 4, 1500, 9645, 13, 407, 11, 436, 600, 2780, 807, 300, 11, 50686], "temperature": 0.0, "avg_logprob": -0.1610231541875583, "compression_ratio": 1.8163934426229509, "no_speech_prob": 0.008065951988101006}, {"id": 84, "seek": 46300, "start": 469.44, "end": 472.96, "text": " they have to do all these ridiculous things to jump through hoops to get this transformation", "tokens": [50686, 436, 362, 281, 360, 439, 613, 11083, 721, 281, 3012, 807, 1106, 3370, 281, 483, 341, 9887, 50862], "temperature": 0.0, "avg_logprob": -0.1610231541875583, "compression_ratio": 1.8163934426229509, "no_speech_prob": 0.008065951988101006}, {"id": 85, "seek": 46300, "start": 472.96, "end": 477.8, "text": " to be complete, and then they come out at the end of it saying, well, test-driven development's", "tokens": [50862, 281, 312, 3566, 11, 293, 550, 436, 808, 484, 412, 264, 917, 295, 309, 1566, 11, 731, 11, 1500, 12, 25456, 3250, 311, 51104], "temperature": 0.0, "avg_logprob": -0.1610231541875583, "compression_ratio": 1.8163934426229509, "no_speech_prob": 0.008065951988101006}, {"id": 86, "seek": 46300, "start": 477.8, "end": 482.56, "text": " crap, we're not doing this. They managed to get rid of it and they managed to dump it.", "tokens": [51104, 12426, 11, 321, 434, 406, 884, 341, 13, 814, 6453, 281, 483, 3973, 295, 309, 293, 436, 6453, 281, 11430, 309, 13, 51342], "temperature": 0.0, "avg_logprob": -0.1610231541875583, "compression_ratio": 1.8163934426229509, "no_speech_prob": 0.008065951988101006}, {"id": 87, "seek": 46300, "start": 482.56, "end": 486.84, "text": " So, you might think that of these three people, the engineers would be the easiest to convince,", "tokens": [51342, 407, 11, 291, 1062, 519, 300, 295, 613, 1045, 561, 11, 264, 11955, 576, 312, 264, 12889, 281, 13447, 11, 51556], "temperature": 0.0, "avg_logprob": -0.1610231541875583, "compression_ratio": 1.8163934426229509, "no_speech_prob": 0.008065951988101006}, {"id": 88, "seek": 46300, "start": 486.84, "end": 490.8, "text": " but there are lots of people that are out there that have gone through three or four of these", "tokens": [51556, 457, 456, 366, 3195, 295, 561, 300, 366, 484, 456, 300, 362, 2780, 807, 1045, 420, 1451, 295, 613, 51754], "temperature": 0.0, "avg_logprob": -0.1610231541875583, "compression_ratio": 1.8163934426229509, "no_speech_prob": 0.008065951988101006}, {"id": 89, "seek": 49080, "start": 490.84000000000003, "end": 496.56, "text": " now and really need to be sold on whether this is going to help them. So, really, don't think", "tokens": [50366, 586, 293, 534, 643, 281, 312, 3718, 322, 1968, 341, 307, 516, 281, 854, 552, 13, 407, 11, 534, 11, 500, 380, 519, 50652], "temperature": 0.0, "avg_logprob": -0.1668241818745931, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.04961057007312775}, {"id": 90, "seek": 49080, "start": 496.56, "end": 501.08, "text": " that they're going to be automatically on your side just because you're convinced. Also,", "tokens": [50652, 300, 436, 434, 516, 281, 312, 6772, 322, 428, 1252, 445, 570, 291, 434, 12561, 13, 2743, 11, 50878], "temperature": 0.0, "avg_logprob": -0.1668241818745931, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.04961057007312775}, {"id": 91, "seek": 49080, "start": 501.08, "end": 505.56, "text": " I'll note all the disagreement that we have just in this one conference about what the best", "tokens": [50878, 286, 603, 3637, 439, 264, 38947, 300, 321, 362, 445, 294, 341, 472, 7586, 466, 437, 264, 1151, 51102], "temperature": 0.0, "avg_logprob": -0.1668241818745931, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.04961057007312775}, {"id": 92, "seek": 49080, "start": 505.56, "end": 512.24, "text": " tooling and the best approaches are anyway. Quickly on things like management, management", "tokens": [51102, 46593, 293, 264, 1151, 11587, 366, 4033, 13, 31800, 322, 721, 411, 4592, 11, 4592, 51436], "temperature": 0.0, "avg_logprob": -0.1668241818745931, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.04961057007312775}, {"id": 93, "seek": 51224, "start": 512.32, "end": 521.0, "text": " will want to be convinced that it's not going to break down productivity. One example I'll", "tokens": [50368, 486, 528, 281, 312, 12561, 300, 309, 311, 406, 516, 281, 1821, 760, 15604, 13, 1485, 1365, 286, 603, 50802], "temperature": 0.0, "avg_logprob": -0.1571923617658944, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.7796388864517212}, {"id": 94, "seek": 51224, "start": 521.0, "end": 524.6800000000001, "text": " give when we're looking at, for example, higher ups like the C-suite, they're going to be", "tokens": [50802, 976, 562, 321, 434, 1237, 412, 11, 337, 1365, 11, 2946, 15497, 411, 264, 383, 12, 33136, 11, 436, 434, 516, 281, 312, 50986], "temperature": 0.0, "avg_logprob": -0.1571923617658944, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.7796388864517212}, {"id": 95, "seek": 51224, "start": 524.6800000000001, "end": 528.24, "text": " interested, you're going to be asking them to spend money because you can't just say,", "tokens": [50986, 3102, 11, 291, 434, 516, 281, 312, 3365, 552, 281, 3496, 1460, 570, 291, 393, 380, 445, 584, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1571923617658944, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.7796388864517212}, {"id": 96, "seek": 51224, "start": 528.24, "end": 533.08, "text": " oh, we're going to do this, you want to actually resource a team. With my client, what we did", "tokens": [51164, 1954, 11, 321, 434, 516, 281, 360, 341, 11, 291, 528, 281, 767, 7684, 257, 1469, 13, 2022, 452, 6423, 11, 437, 321, 630, 51406], "temperature": 0.0, "avg_logprob": -0.1571923617658944, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.7796388864517212}, {"id": 97, "seek": 51224, "start": 533.08, "end": 539.24, "text": " was we actually went through the outages over the last 12 months and we did some estimates,", "tokens": [51406, 390, 321, 767, 1437, 807, 264, 484, 1660, 670, 264, 1036, 2272, 2493, 293, 321, 630, 512, 20561, 11, 51714], "temperature": 0.0, "avg_logprob": -0.1571923617658944, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.7796388864517212}, {"id": 98, "seek": 53924, "start": 539.48, "end": 543.5600000000001, "text": " we said there are estimates and we caveated like what the caveats are. We went through and", "tokens": [50376, 321, 848, 456, 366, 20561, 293, 321, 11730, 770, 411, 437, 264, 11730, 1720, 366, 13, 492, 1437, 807, 293, 50580], "temperature": 0.0, "avg_logprob": -0.18171724498781383, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.05047980323433876}, {"id": 99, "seek": 53924, "start": 543.5600000000001, "end": 548.84, "text": " we worked out how much time we think would be saved on outages, on each of these outages,", "tokens": [50580, 321, 2732, 484, 577, 709, 565, 321, 519, 576, 312, 6624, 322, 484, 1660, 11, 322, 1184, 295, 613, 484, 1660, 11, 50844], "temperature": 0.0, "avg_logprob": -0.18171724498781383, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.05047980323433876}, {"id": 100, "seek": 53924, "start": 548.84, "end": 554.04, "text": " if they had good instrumentation of their code and if we could identify the issues more quickly.", "tokens": [50844, 498, 436, 632, 665, 7198, 399, 295, 641, 3089, 293, 498, 321, 727, 5876, 264, 2663, 544, 2661, 13, 51104], "temperature": 0.0, "avg_logprob": -0.18171724498781383, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.05047980323433876}, {"id": 101, "seek": 53924, "start": 554.04, "end": 561.04, "text": " They could go away and they could calculate that in terms of a cost which they could use to justify", "tokens": [51104, 814, 727, 352, 1314, 293, 436, 727, 8873, 300, 294, 2115, 295, 257, 2063, 597, 436, 727, 764, 281, 20833, 51454], "temperature": 0.0, "avg_logprob": -0.18171724498781383, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.05047980323433876}, {"id": 102, "seek": 53924, "start": 561.04, "end": 568.52, "text": " it. So, don't forget about your stakeholders. One thing that you didn't hear is in all of that,", "tokens": [51454, 309, 13, 407, 11, 500, 380, 2870, 466, 428, 17779, 13, 1485, 551, 300, 291, 994, 380, 1568, 307, 294, 439, 295, 300, 11, 51828], "temperature": 0.0, "avg_logprob": -0.18171724498781383, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.05047980323433876}, {"id": 103, "seek": 56852, "start": 568.6, "end": 574.52, "text": " is what tool to use. That's because, sorry, everyone that makes a tool, it largely doesn't", "tokens": [50368, 307, 437, 2290, 281, 764, 13, 663, 311, 570, 11, 2597, 11, 1518, 300, 1669, 257, 2290, 11, 309, 11611, 1177, 380, 50664], "temperature": 0.0, "avg_logprob": -0.13208612504896228, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.01657511107623577}, {"id": 104, "seek": 56852, "start": 574.52, "end": 579.6, "text": " matter at this point. People want traces because they want less downtime, they want more clarity,", "tokens": [50664, 1871, 412, 341, 935, 13, 3432, 528, 26076, 570, 436, 528, 1570, 49648, 11, 436, 528, 544, 16992, 11, 50918], "temperature": 0.0, "avg_logprob": -0.13208612504896228, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.01657511107623577}, {"id": 105, "seek": 56852, "start": 579.6, "end": 584.4, "text": " they want to capture lost revenue or whatever else. But you can do that with pretty much", "tokens": [50918, 436, 528, 281, 7983, 2731, 9324, 420, 2035, 1646, 13, 583, 291, 393, 360, 300, 365, 1238, 709, 51158], "temperature": 0.0, "avg_logprob": -0.13208612504896228, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.01657511107623577}, {"id": 106, "seek": 56852, "start": 584.4, "end": 591.56, "text": " any observability tool right now. So, the one thing you don't want to do as part of convincing", "tokens": [51158, 604, 9951, 2310, 2290, 558, 586, 13, 407, 11, 264, 472, 551, 291, 500, 380, 528, 281, 360, 382, 644, 295, 24823, 51516], "temperature": 0.0, "avg_logprob": -0.13208612504896228, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.01657511107623577}, {"id": 107, "seek": 59156, "start": 591.5999999999999, "end": 598.5999999999999, "text": " people is to try and sell them on a specific tool. That can come later. In my engagement,", "tokens": [50366, 561, 307, 281, 853, 293, 3607, 552, 322, 257, 2685, 2290, 13, 663, 393, 808, 1780, 13, 682, 452, 8742, 11, 50716], "temperature": 0.0, "avg_logprob": -0.1630618311777836, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.7040976881980896}, {"id": 108, "seek": 59156, "start": 598.5999999999999, "end": 603.9599999999999, "text": " we're focusing on tempo. And the reason that we're doing that, I'll introduce some of the", "tokens": [50716, 321, 434, 8416, 322, 8972, 13, 400, 264, 1778, 300, 321, 434, 884, 300, 11, 286, 603, 5366, 512, 295, 264, 50984], "temperature": 0.0, "avg_logprob": -0.1630618311777836, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.7040976881980896}, {"id": 109, "seek": 59156, "start": 603.9599999999999, "end": 608.8, "text": " other reasons in a bit, but the main reason is because we always use Grafana, we already", "tokens": [50984, 661, 4112, 294, 257, 857, 11, 457, 264, 2135, 1778, 307, 570, 321, 1009, 764, 8985, 69, 2095, 11, 321, 1217, 51226], "temperature": 0.0, "avg_logprob": -0.1630618311777836, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.7040976881980896}, {"id": 110, "seek": 59156, "start": 608.8, "end": 613.8399999999999, "text": " use Prometheus, it slots right in. And we don't really have to discuss it much. There's another", "tokens": [51226, 764, 2114, 649, 42209, 11, 309, 24266, 558, 294, 13, 400, 321, 500, 380, 534, 362, 281, 2248, 309, 709, 13, 821, 311, 1071, 51478], "temperature": 0.0, "avg_logprob": -0.1630618311777836, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.7040976881980896}, {"id": 111, "seek": 59156, "start": 613.8399999999999, "end": 618.5999999999999, "text": " thing which is because tempo is open source, we don't have to involve as part of selling this", "tokens": [51478, 551, 597, 307, 570, 8972, 307, 1269, 4009, 11, 321, 500, 380, 362, 281, 9494, 382, 644, 295, 6511, 341, 51716], "temperature": 0.0, "avg_logprob": -0.1630618311777836, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.7040976881980896}, {"id": 112, "seek": 61860, "start": 618.64, "end": 625.16, "text": " project, a new vendor, and new commercials and stuff like that. So, open source to the", "tokens": [50366, 1716, 11, 257, 777, 24321, 11, 293, 777, 33666, 293, 1507, 411, 300, 13, 407, 11, 1269, 4009, 281, 264, 50692], "temperature": 0.0, "avg_logprob": -0.18962849270213733, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.028042826801538467}, {"id": 113, "seek": 61860, "start": 625.16, "end": 631.66, "text": " rescue with that. But really, you want to get your project approved so you can go and", "tokens": [50692, 13283, 365, 300, 13, 583, 534, 11, 291, 528, 281, 483, 428, 1716, 10826, 370, 291, 393, 352, 293, 51017], "temperature": 0.0, "avg_logprob": -0.18962849270213733, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.028042826801538467}, {"id": 114, "seek": 61860, "start": 631.66, "end": 638.66, "text": " start instrumenting code. Last thing I'll say on this is team topology. This is an example", "tokens": [51017, 722, 7198, 278, 3089, 13, 5264, 551, 286, 603, 584, 322, 341, 307, 1469, 1192, 1793, 13, 639, 307, 364, 1365, 51367], "temperature": 0.0, "avg_logprob": -0.18962849270213733, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.028042826801538467}, {"id": 115, "seek": 61860, "start": 638.8000000000001, "end": 643.72, "text": " of the sort of team that I'd expect to go and start an observability transformation.", "tokens": [51374, 295, 264, 1333, 295, 1469, 300, 286, 1116, 2066, 281, 352, 293, 722, 364, 9951, 2310, 9887, 13, 51620], "temperature": 0.0, "avg_logprob": -0.18962849270213733, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.028042826801538467}, {"id": 116, "seek": 64372, "start": 643.84, "end": 649.84, "text": " You'd want, I prefer smaller, more agile teams. So, you might look at this and think, well,", "tokens": [50370, 509, 1116, 528, 11, 286, 4382, 4356, 11, 544, 30072, 5491, 13, 407, 11, 291, 1062, 574, 412, 341, 293, 519, 11, 731, 11, 50670], "temperature": 0.0, "avg_logprob": -0.20004411177201706, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.15359057486057281}, {"id": 117, "seek": 64372, "start": 649.84, "end": 656.84, "text": " based on my business, I might have two or three of your software engineers, two or three", "tokens": [50670, 2361, 322, 452, 1606, 11, 286, 1062, 362, 732, 420, 1045, 295, 428, 4722, 11955, 11, 732, 420, 1045, 51020], "temperature": 0.0, "avg_logprob": -0.20004411177201706, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.15359057486057281}, {"id": 118, "seek": 64372, "start": 657.1600000000001, "end": 661.28, "text": " of your operations engineers. That might be an anti-pattern. You can go and look up all", "tokens": [51036, 295, 428, 7705, 11955, 13, 663, 1062, 312, 364, 6061, 12, 79, 1161, 77, 13, 509, 393, 352, 293, 574, 493, 439, 51242], "temperature": 0.0, "avg_logprob": -0.20004411177201706, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.15359057486057281}, {"id": 119, "seek": 64372, "start": 661.28, "end": 666.44, "text": " the reasons why bigger teams tend to do work more slowly. I'm not going to cover that now.", "tokens": [51242, 264, 4112, 983, 3801, 5491, 3928, 281, 360, 589, 544, 5692, 13, 286, 478, 406, 516, 281, 2060, 300, 586, 13, 51500], "temperature": 0.0, "avg_logprob": -0.20004411177201706, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.15359057486057281}, {"id": 120, "seek": 64372, "start": 666.44, "end": 670.96, "text": " So, I'm looking at a kind of crack team. Software engineers are going to get in and go and instrument", "tokens": [51500, 407, 11, 286, 478, 1237, 412, 257, 733, 295, 6226, 1469, 13, 27428, 11955, 366, 516, 281, 483, 294, 293, 352, 293, 7198, 51726], "temperature": 0.0, "avg_logprob": -0.20004411177201706, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.15359057486057281}, {"id": 121, "seek": 67096, "start": 671.0, "end": 674.9200000000001, "text": " the code. We've got an operations engineer that's going to make sure that we clear the", "tokens": [50366, 264, 3089, 13, 492, 600, 658, 364, 7705, 11403, 300, 311, 516, 281, 652, 988, 300, 321, 1850, 264, 50562], "temperature": 0.0, "avg_logprob": -0.12976024596671748, "compression_ratio": 1.9181494661921707, "no_speech_prob": 0.07264653593301773}, {"id": 122, "seek": 67096, "start": 674.9200000000001, "end": 681.5600000000001, "text": " pathways to actually get those spans out into tracing databases. And finally, we've got", "tokens": [50562, 22988, 281, 767, 483, 729, 44086, 484, 666, 25262, 22380, 13, 400, 2721, 11, 321, 600, 658, 50894], "temperature": 0.0, "avg_logprob": -0.12976024596671748, "compression_ratio": 1.9181494661921707, "no_speech_prob": 0.07264653593301773}, {"id": 123, "seek": 67096, "start": 681.5600000000001, "end": 685.96, "text": " somebody that's kind of in a product owner position that's going to protect that team,", "tokens": [50894, 2618, 300, 311, 733, 295, 294, 257, 1674, 7289, 2535, 300, 311, 516, 281, 2371, 300, 1469, 11, 51114], "temperature": 0.0, "avg_logprob": -0.12976024596671748, "compression_ratio": 1.9181494661921707, "no_speech_prob": 0.07264653593301773}, {"id": 124, "seek": 67096, "start": 685.96, "end": 689.48, "text": " make sure that they're not answering inane questions all the time. And that is also going", "tokens": [51114, 652, 988, 300, 436, 434, 406, 13430, 294, 1929, 1651, 439, 264, 565, 13, 400, 300, 307, 611, 516, 51290], "temperature": 0.0, "avg_logprob": -0.12976024596671748, "compression_ratio": 1.9181494661921707, "no_speech_prob": 0.07264653593301773}, {"id": 125, "seek": 67096, "start": 689.48, "end": 693.5600000000001, "text": " to be working with the business and with the other product delivery teams and the platform", "tokens": [51290, 281, 312, 1364, 365, 264, 1606, 293, 365, 264, 661, 1674, 8982, 5491, 293, 264, 3663, 51494], "temperature": 0.0, "avg_logprob": -0.12976024596671748, "compression_ratio": 1.9181494661921707, "no_speech_prob": 0.07264653593301773}, {"id": 126, "seek": 67096, "start": 693.5600000000001, "end": 700.2800000000001, "text": " team and whoever else to make sure that concerns are raised, that they're heard, that they pivot", "tokens": [51494, 1469, 293, 11387, 1646, 281, 652, 988, 300, 7389, 366, 6005, 11, 300, 436, 434, 2198, 11, 300, 436, 14538, 51830], "temperature": 0.0, "avg_logprob": -0.12976024596671748, "compression_ratio": 1.9181494661921707, "no_speech_prob": 0.07264653593301773}, {"id": 127, "seek": 70028, "start": 700.28, "end": 704.3199999999999, "text": " when they realize that actually they've made a mistake. So, that's an important role as", "tokens": [50364, 562, 436, 4325, 300, 767, 436, 600, 1027, 257, 6146, 13, 407, 11, 300, 311, 364, 1021, 3090, 382, 50566], "temperature": 0.0, "avg_logprob": -0.15675318123090384, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.004658875521272421}, {"id": 128, "seek": 70028, "start": 704.3199999999999, "end": 709.28, "text": " well. But remember, this is a transformation and we're trying to do new things. So, we're", "tokens": [50566, 731, 13, 583, 1604, 11, 341, 307, 257, 9887, 293, 321, 434, 1382, 281, 360, 777, 721, 13, 407, 11, 321, 434, 50814], "temperature": 0.0, "avg_logprob": -0.15675318123090384, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.004658875521272421}, {"id": 129, "seek": 70028, "start": 709.28, "end": 714.1999999999999, "text": " changing cultures here. So, you do need to be responsive to feedback and you need to", "tokens": [50814, 4473, 12951, 510, 13, 407, 11, 291, 360, 643, 281, 312, 21826, 281, 5824, 293, 291, 643, 281, 51060], "temperature": 0.0, "avg_logprob": -0.15675318123090384, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.004658875521272421}, {"id": 130, "seek": 70028, "start": 714.1999999999999, "end": 720.4, "text": " be responsive to feelings. Otherwise, your engineers here are going to make the best", "tokens": [51060, 312, 21826, 281, 6640, 13, 10328, 11, 428, 11955, 510, 366, 516, 281, 652, 264, 1151, 51370], "temperature": 0.0, "avg_logprob": -0.15675318123090384, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.004658875521272421}, {"id": 131, "seek": 70028, "start": 720.4, "end": 725.4, "text": " system that never gets used, which is another pitfall of transformations.", "tokens": [51370, 1185, 300, 1128, 2170, 1143, 11, 597, 307, 1071, 10147, 6691, 295, 34852, 13, 51620], "temperature": 0.0, "avg_logprob": -0.15675318123090384, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.004658875521272421}, {"id": 132, "seek": 72540, "start": 726.4, "end": 732.4, "text": " Okay. Those are my thoughts on convincing people to do an observability transformation.", "tokens": [50414, 1033, 13, 3950, 366, 452, 4598, 322, 24823, 561, 281, 360, 364, 9951, 2310, 9887, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1758755956377302, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.014182306826114655}, {"id": 133, "seek": 72540, "start": 732.4, "end": 738.4, "text": " Now, let's imagine you've got the thumbs up. Let's move towards implementation. Most important", "tokens": [50714, 823, 11, 718, 311, 3811, 291, 600, 658, 264, 8838, 493, 13, 961, 311, 1286, 3030, 11420, 13, 4534, 1021, 51014], "temperature": 0.0, "avg_logprob": -0.1758755956377302, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.014182306826114655}, {"id": 134, "seek": 72540, "start": 738.4, "end": 743.4, "text": " thing is to not get bogged down in the details of the infrastructure. You need to move to", "tokens": [51014, 551, 307, 281, 406, 483, 26132, 3004, 760, 294, 264, 4365, 295, 264, 6896, 13, 509, 643, 281, 1286, 281, 51264], "temperature": 0.0, "avg_logprob": -0.1758755956377302, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.014182306826114655}, {"id": 135, "seek": 72540, "start": 743.4, "end": 749.4, "text": " instrumentation. But, you are going to have to need some sort of tracing database. You are going to", "tokens": [51264, 7198, 399, 13, 583, 11, 291, 366, 516, 281, 362, 281, 643, 512, 1333, 295, 25262, 8149, 13, 509, 366, 516, 281, 51564], "temperature": 0.0, "avg_logprob": -0.1758755956377302, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.014182306826114655}, {"id": 136, "seek": 72540, "start": 749.4, "end": 754.4, "text": " need some sort of tooling. If you have something already, so for example, if you're already using a", "tokens": [51564, 643, 512, 1333, 295, 46593, 13, 759, 291, 362, 746, 1217, 11, 370, 337, 1365, 11, 498, 291, 434, 1217, 1228, 257, 51814], "temperature": 0.0, "avg_logprob": -0.1758755956377302, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.014182306826114655}, {"id": 137, "seek": 75440, "start": 754.4, "end": 760.4, "text": " provider of some sort and they have it, then great. Consider that. However, one of the ways that you can", "tokens": [50364, 12398, 295, 512, 1333, 293, 436, 362, 309, 11, 550, 869, 13, 17416, 300, 13, 2908, 11, 472, 295, 264, 2098, 300, 291, 393, 50664], "temperature": 0.0, "avg_logprob": -0.09140021129719263, "compression_ratio": 1.7269076305220883, "no_speech_prob": 0.03198632225394249}, {"id": 138, "seek": 75440, "start": 760.4, "end": 767.4, "text": " make sure that these things move faster is by moving your tracing database into where the data is that", "tokens": [50664, 652, 988, 300, 613, 721, 1286, 4663, 307, 538, 2684, 428, 25262, 8149, 666, 689, 264, 1412, 307, 300, 51014], "temperature": 0.0, "avg_logprob": -0.09140021129719263, "compression_ratio": 1.7269076305220883, "no_speech_prob": 0.03198632225394249}, {"id": 139, "seek": 75440, "start": 767.4, "end": 773.4, "text": " you're collecting. You think about big, old companies, big and or old companies. They get really", "tokens": [51014, 291, 434, 12510, 13, 509, 519, 466, 955, 11, 1331, 3431, 11, 955, 293, 420, 1331, 3431, 13, 814, 483, 534, 51314], "temperature": 0.0, "avg_logprob": -0.09140021129719263, "compression_ratio": 1.7269076305220883, "no_speech_prob": 0.03198632225394249}, {"id": 140, "seek": 75440, "start": 773.4, "end": 778.4, "text": " nervous when you say, right, we're going to collect all this data and we're going to go put in this cloud provider over here.", "tokens": [51314, 6296, 562, 291, 584, 11, 558, 11, 321, 434, 516, 281, 2500, 439, 341, 1412, 293, 321, 434, 516, 281, 352, 829, 294, 341, 4588, 12398, 670, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09140021129719263, "compression_ratio": 1.7269076305220883, "no_speech_prob": 0.03198632225394249}, {"id": 141, "seek": 77840, "start": 778.4, "end": 785.4, "text": " Now, that can take months to agree. And so what you can do is you can short circuit that, start that process,", "tokens": [50364, 823, 11, 300, 393, 747, 2493, 281, 3986, 13, 400, 370, 437, 291, 393, 360, 307, 291, 393, 2099, 9048, 300, 11, 722, 300, 1399, 11, 50714], "temperature": 0.0, "avg_logprob": -0.108882985025082, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.01880546286702156}, {"id": 142, "seek": 77840, "start": 785.4, "end": 791.4, "text": " start discussing how you're going to do this. But you can also at the same time move your tracing databases into", "tokens": [50714, 722, 10850, 577, 291, 434, 516, 281, 360, 341, 13, 583, 291, 393, 611, 412, 264, 912, 565, 1286, 428, 25262, 22380, 666, 51014], "temperature": 0.0, "avg_logprob": -0.108882985025082, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.01880546286702156}, {"id": 143, "seek": 77840, "start": 791.4, "end": 798.4, "text": " maybe the accounts or the cloud provider that's actually already been agreed to use this. There is a downside", "tokens": [51014, 1310, 264, 9402, 420, 264, 4588, 12398, 300, 311, 767, 1217, 668, 9166, 281, 764, 341, 13, 821, 307, 257, 25060, 51364], "temperature": 0.0, "avg_logprob": -0.108882985025082, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.01880546286702156}, {"id": 144, "seek": 77840, "start": 798.4, "end": 803.4, "text": " here, which some of you might be thinking is, well, doesn't that mean, James, that you'd have maybe multiple", "tokens": [51364, 510, 11, 597, 512, 295, 291, 1062, 312, 1953, 307, 11, 731, 11, 1177, 380, 300, 914, 11, 5678, 11, 300, 291, 1116, 362, 1310, 3866, 51614], "temperature": 0.0, "avg_logprob": -0.108882985025082, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.01880546286702156}, {"id": 145, "seek": 80340, "start": 803.4, "end": 811.4, "text": " tracing databases, which means that you wouldn't have all your spans in the same place? And that is true, but it", "tokens": [50364, 25262, 22380, 11, 597, 1355, 300, 291, 2759, 380, 362, 439, 428, 44086, 294, 264, 912, 1081, 30, 400, 300, 307, 2074, 11, 457, 309, 50764], "temperature": 0.0, "avg_logprob": -0.10833362947430528, "compression_ratio": 1.8125, "no_speech_prob": 0.4534803330898285}, {"id": 146, "seek": 80340, "start": 811.4, "end": 816.4, "text": " means that you can move on to instrumentation. It means that you can move to the point where you have like maybe two", "tokens": [50764, 1355, 300, 291, 393, 1286, 322, 281, 7198, 399, 13, 467, 1355, 300, 291, 393, 1286, 281, 264, 935, 689, 291, 362, 411, 1310, 732, 51014], "temperature": 0.0, "avg_logprob": -0.10833362947430528, "compression_ratio": 1.8125, "no_speech_prob": 0.4534803330898285}, {"id": 147, "seek": 80340, "start": 816.4, "end": 823.4, "text": " traces that somebody has to look to, and then you can get other people in the business to say, hey, wouldn't it be useful if,", "tokens": [51014, 26076, 300, 2618, 575, 281, 574, 281, 11, 293, 550, 291, 393, 483, 661, 561, 294, 264, 1606, 281, 584, 11, 4177, 11, 2759, 380, 309, 312, 4420, 498, 11, 51364], "temperature": 0.0, "avg_logprob": -0.10833362947430528, "compression_ratio": 1.8125, "no_speech_prob": 0.4534803330898285}, {"id": 148, "seek": 80340, "start": 823.4, "end": 831.4, "text": " and then you can start having the discussions. Don't try and boil the ocean on these things. And we're being", "tokens": [51364, 293, 550, 291, 393, 722, 1419, 264, 11088, 13, 1468, 380, 853, 293, 13329, 264, 7810, 322, 613, 721, 13, 400, 321, 434, 885, 51764], "temperature": 0.0, "avg_logprob": -0.10833362947430528, "compression_ratio": 1.8125, "no_speech_prob": 0.4534803330898285}, {"id": 149, "seek": 83140, "start": 831.4, "end": 838.4, "text": " pragmatic here. So as an example here, this is if your client is in AWS, you can quickly get Tempo. There's a good article on", "tokens": [50364, 46904, 510, 13, 407, 382, 364, 1365, 510, 11, 341, 307, 498, 428, 6423, 307, 294, 17650, 11, 291, 393, 2661, 483, 8095, 2259, 13, 821, 311, 257, 665, 7222, 322, 50714], "temperature": 0.0, "avg_logprob": -0.0895146718100896, "compression_ratio": 1.74822695035461, "no_speech_prob": 0.017701152712106705}, {"id": 150, "seek": 83140, "start": 838.4, "end": 847.4, "text": " the Grafana website deploying Tempo on Fargate, which means that you can get that up nice and quickly. So again, that's an", "tokens": [50714, 264, 8985, 69, 2095, 3144, 34198, 8095, 2259, 322, 9067, 22514, 11, 597, 1355, 300, 291, 393, 483, 300, 493, 1481, 293, 2661, 13, 407, 797, 11, 300, 311, 364, 51164], "temperature": 0.0, "avg_logprob": -0.0895146718100896, "compression_ratio": 1.74822695035461, "no_speech_prob": 0.017701152712106705}, {"id": 151, "seek": 83140, "start": 847.4, "end": 852.4, "text": " advantage of using these things. And more importantly, you can deploy it. You can find out it's the wrong thing to do,", "tokens": [51164, 5002, 295, 1228, 613, 721, 13, 400, 544, 8906, 11, 291, 393, 7274, 309, 13, 509, 393, 915, 484, 309, 311, 264, 2085, 551, 281, 360, 11, 51414], "temperature": 0.0, "avg_logprob": -0.0895146718100896, "compression_ratio": 1.74822695035461, "no_speech_prob": 0.017701152712106705}, {"id": 152, "seek": 83140, "start": 852.4, "end": 858.4, "text": " and you can go do something else. And it's a great thing about using these open source tools is you can really work it out as", "tokens": [51414, 293, 291, 393, 352, 360, 746, 1646, 13, 400, 309, 311, 257, 869, 551, 466, 1228, 613, 1269, 4009, 3873, 307, 291, 393, 534, 589, 309, 484, 382, 51714], "temperature": 0.0, "avg_logprob": -0.0895146718100896, "compression_ratio": 1.74822695035461, "no_speech_prob": 0.017701152712106705}, {"id": 153, "seek": 85840, "start": 858.4, "end": 866.4, "text": " you're moving. With that in mind, get instrumenting. And know that to start with that team that I put together earlier is going to be", "tokens": [50364, 291, 434, 2684, 13, 2022, 300, 294, 1575, 11, 483, 7198, 278, 13, 400, 458, 300, 281, 722, 365, 300, 1469, 300, 286, 829, 1214, 3071, 307, 516, 281, 312, 50764], "temperature": 0.0, "avg_logprob": -0.10346925386818506, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.09088985621929169}, {"id": 154, "seek": 85840, "start": 866.4, "end": 876.4, "text": " doing a lot of the work themselves. Automatic instrumentation is your friend. Get your software engineer to go and find the code", "tokens": [50764, 884, 257, 688, 295, 264, 589, 2969, 13, 6049, 13143, 7198, 399, 307, 428, 1277, 13, 3240, 428, 4722, 11403, 281, 352, 293, 915, 264, 3089, 51264], "temperature": 0.0, "avg_logprob": -0.10346925386818506, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.09088985621929169}, {"id": 155, "seek": 85840, "start": 876.4, "end": 886.4, "text": " bases that are across the system, especially on your hotpaths, and start raising PRs to auto instrument. You know how best to do", "tokens": [51264, 17949, 300, 366, 2108, 264, 1185, 11, 2318, 322, 428, 2368, 31852, 82, 11, 293, 722, 11225, 11568, 82, 281, 8399, 7198, 13, 509, 458, 577, 1151, 281, 360, 51764], "temperature": 0.0, "avg_logprob": -0.10346925386818506, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.09088985621929169}, {"id": 156, "seek": 88640, "start": 886.4, "end": 891.4, "text": " these in your company. Some companies, they want to start the conversation with a PR. Sometimes they want to start with a meeting", "tokens": [50364, 613, 294, 428, 2237, 13, 2188, 3431, 11, 436, 528, 281, 722, 264, 3761, 365, 257, 11568, 13, 4803, 436, 528, 281, 722, 365, 257, 3440, 50614], "temperature": 0.0, "avg_logprob": -0.10007338296799433, "compression_ratio": 1.7136752136752136, "no_speech_prob": 0.05218207836151123}, {"id": 157, "seek": 88640, "start": 891.4, "end": 900.4, "text": " or something like this. But getting auto instrumentation in to these code bases will mean that you will start being able to build up the", "tokens": [50614, 420, 746, 411, 341, 13, 583, 1242, 8399, 7198, 399, 294, 281, 613, 3089, 17949, 486, 914, 300, 291, 486, 722, 885, 1075, 281, 1322, 493, 264, 51064], "temperature": 0.0, "avg_logprob": -0.10007338296799433, "compression_ratio": 1.7136752136752136, "no_speech_prob": 0.05218207836151123}, {"id": 158, "seek": 88640, "start": 900.4, "end": 913.4, "text": " shallow layer of these traces. Then if any teams start becoming interested in this, opportunistically pair your software engineer with", "tokens": [51064, 20488, 4583, 295, 613, 26076, 13, 1396, 498, 604, 5491, 722, 5617, 3102, 294, 341, 11, 2070, 20458, 6119, 428, 4722, 11403, 365, 51714], "temperature": 0.0, "avg_logprob": -0.10007338296799433, "compression_ratio": 1.7136752136752136, "no_speech_prob": 0.05218207836151123}, {"id": 159, "seek": 91340, "start": 913.4, "end": 920.4, "text": " those teams. Pairing mobbing is a great way to share knowledge. Remember, a lot of these software engineers will not have done this kind of", "tokens": [50364, 729, 5491, 13, 430, 1246, 278, 4298, 4324, 307, 257, 869, 636, 281, 2073, 3601, 13, 5459, 11, 257, 688, 295, 613, 4722, 11955, 486, 406, 362, 1096, 341, 733, 295, 50714], "temperature": 0.0, "avg_logprob": -0.09461275136695718, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.5256286859512329}, {"id": 160, "seek": 91340, "start": 920.4, "end": 927.4, "text": " thing before, and doing it's kind of hard if you don't know how to do it. You don't want them to get frustrated to throw in the towel", "tokens": [50714, 551, 949, 11, 293, 884, 309, 311, 733, 295, 1152, 498, 291, 500, 380, 458, 577, 281, 360, 309, 13, 509, 500, 380, 528, 552, 281, 483, 15751, 281, 3507, 294, 264, 15755, 51064], "temperature": 0.0, "avg_logprob": -0.09461275136695718, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.5256286859512329}, {"id": 161, "seek": 91340, "start": 927.4, "end": 935.4, "text": " and say, no, this is dumb. This is hard. This is not the way we used to do things. Whereas if you can put your software engineer in with", "tokens": [51064, 293, 584, 11, 572, 11, 341, 307, 10316, 13, 639, 307, 1152, 13, 639, 307, 406, 264, 636, 321, 1143, 281, 360, 721, 13, 13813, 498, 291, 393, 829, 428, 4722, 11403, 294, 365, 51464], "temperature": 0.0, "avg_logprob": -0.09461275136695718, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.5256286859512329}, {"id": 162, "seek": 93540, "start": 935.4, "end": 944.4, "text": " a pair as a pairing or a mobbing situation, they will have happy times and everything will be lovely. Also, make sure that you point out the", "tokens": [50364, 257, 6119, 382, 257, 32735, 420, 257, 4298, 4324, 2590, 11, 436, 486, 362, 2055, 1413, 293, 1203, 486, 312, 7496, 13, 2743, 11, 652, 988, 300, 291, 935, 484, 264, 50814], "temperature": 0.0, "avg_logprob": -0.1603558378399543, "compression_ratio": 1.640625, "no_speech_prob": 0.7808340787887573}, {"id": 163, "seek": 93540, "start": 944.4, "end": 952.4, "text": " value when you see it. It's very easy for us to see these things and to go, oh, it's great. And so obviously it's great. But this is new to", "tokens": [50814, 2158, 562, 291, 536, 309, 13, 467, 311, 588, 1858, 337, 505, 281, 536, 613, 721, 293, 281, 352, 11, 1954, 11, 309, 311, 869, 13, 400, 370, 2745, 309, 311, 869, 13, 583, 341, 307, 777, 281, 51214], "temperature": 0.0, "avg_logprob": -0.1603558378399543, "compression_ratio": 1.640625, "no_speech_prob": 0.7808340787887573}, {"id": 164, "seek": 93540, "start": 952.4, "end": 959.4, "text": " people. So point out the 10% of their queries that has like this weird choke point. Point out all these advantages you're getting from this", "tokens": [51214, 561, 13, 407, 935, 484, 264, 1266, 4, 295, 641, 24109, 300, 575, 411, 341, 3657, 34427, 935, 13, 12387, 484, 439, 613, 14906, 291, 434, 1242, 490, 341, 51564], "temperature": 0.0, "avg_logprob": -0.1603558378399543, "compression_ratio": 1.640625, "no_speech_prob": 0.7808340787887573}, {"id": 165, "seek": 95940, "start": 959.4, "end": 967.4, "text": " instrumentation and from all these spans as you're collecting them. Find, when there's an issue, when there's downtime, get your team to go and", "tokens": [50364, 7198, 399, 293, 490, 439, 613, 44086, 382, 291, 434, 12510, 552, 13, 11809, 11, 562, 456, 311, 364, 2734, 11, 562, 456, 311, 49648, 11, 483, 428, 1469, 281, 352, 293, 50764], "temperature": 0.0, "avg_logprob": -0.10436971755254837, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.5942327380180359}, {"id": 166, "seek": 95940, "start": 967.4, "end": 978.4, "text": " see if they can race the people that are doing incident response to finding where the issue is based on the tracing. Once these teams realize that they can see", "tokens": [50764, 536, 498, 436, 393, 4569, 264, 561, 300, 366, 884, 9348, 4134, 281, 5006, 689, 264, 2734, 307, 2361, 322, 264, 25262, 13, 3443, 613, 5491, 4325, 300, 436, 393, 536, 51314], "temperature": 0.0, "avg_logprob": -0.10436971755254837, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.5942327380180359}, {"id": 167, "seek": 95940, "start": 978.4, "end": 988.4, "text": " through walls with this stuff, they'll soon start instrumenting their own code. But you need to get them to look. Another trap is to get bogged down on", "tokens": [51314, 807, 7920, 365, 341, 1507, 11, 436, 603, 2321, 722, 7198, 278, 641, 1065, 3089, 13, 583, 291, 643, 281, 483, 552, 281, 574, 13, 3996, 11487, 307, 281, 483, 26132, 3004, 760, 322, 51814], "temperature": 0.0, "avg_logprob": -0.10436971755254837, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.5942327380180359}, {"id": 168, "seek": 98840, "start": 988.4, "end": 999.4, "text": " the problems that are harder to instrument. Airlines and banks and other places have a bad habit and that bad habit is Fortran. Or like Zidark or some", "tokens": [50364, 264, 2740, 300, 366, 6081, 281, 7198, 13, 38788, 293, 10237, 293, 661, 3190, 362, 257, 1578, 7164, 293, 300, 1578, 7164, 307, 11002, 4257, 13, 1610, 411, 1176, 327, 809, 420, 512, 50914], "temperature": 0.0, "avg_logprob": -0.20713028540978065, "compression_ratio": 1.748, "no_speech_prob": 0.3771943747997284}, {"id": 169, "seek": 98840, "start": 999.4, "end": 1007.4, "text": " mainframe thing or whatever. If anyone here, has anyone here just put your hand up if you do any development on like COBOL, Fortran,", "tokens": [50914, 2135, 17265, 551, 420, 2035, 13, 759, 2878, 510, 11, 575, 2878, 510, 445, 829, 428, 1011, 493, 498, 291, 360, 604, 3250, 322, 411, 34812, 5046, 11, 11002, 4257, 11, 51314], "temperature": 0.0, "avg_logprob": -0.20713028540978065, "compression_ratio": 1.748, "no_speech_prob": 0.3771943747997284}, {"id": 170, "seek": 98840, "start": 1007.4, "end": 1016.4, "text": " anything like that? Awesome, awesome. If you go an instrument something like that, please come and talk about it. That sounds awesome. That sounds like a", "tokens": [51314, 1340, 411, 300, 30, 10391, 11, 3476, 13, 759, 291, 352, 364, 7198, 746, 411, 300, 11, 1767, 808, 293, 751, 466, 309, 13, 663, 3263, 3476, 13, 663, 3263, 411, 257, 51764], "temperature": 0.0, "avg_logprob": -0.20713028540978065, "compression_ratio": 1.748, "no_speech_prob": 0.3771943747997284}, {"id": 171, "seek": 101640, "start": 1016.4, "end": 1025.4, "text": " lot of people are going to talk about this one. I'll be fascinated by it. But if you're doing this kind of project, now is not the time. Something like that is not", "tokens": [50364, 688, 295, 561, 366, 516, 281, 751, 466, 341, 472, 13, 286, 603, 312, 24597, 538, 309, 13, 583, 498, 291, 434, 884, 341, 733, 295, 1716, 11, 586, 307, 406, 264, 565, 13, 6595, 411, 300, 307, 406, 50814], "temperature": 0.0, "avg_logprob": -0.1857434100792056, "compression_ratio": 1.7153024911032029, "no_speech_prob": 0.1936698853969574}, {"id": 172, "seek": 101640, "start": 1025.4, "end": 1033.4, "text": " really going to correct me if I'm wrong, anybody out there. I don't think there's any instrumentation for Fortran code or anything like that. Treat it as a third", "tokens": [50814, 534, 516, 281, 3006, 385, 498, 286, 478, 2085, 11, 4472, 484, 456, 13, 286, 500, 380, 519, 456, 311, 604, 7198, 399, 337, 11002, 4257, 3089, 420, 1340, 411, 300, 13, 20298, 309, 382, 257, 2636, 51214], "temperature": 0.0, "avg_logprob": -0.1857434100792056, "compression_ratio": 1.7153024911032029, "no_speech_prob": 0.1936698853969574}, {"id": 173, "seek": 101640, "start": 1033.4, "end": 1042.4, "text": " party system. And also don't try and instrument other people's code. I've seen this happen. People will go, right, okay, we've got this third party and it's", "tokens": [51214, 3595, 1185, 13, 400, 611, 500, 380, 853, 293, 7198, 661, 561, 311, 3089, 13, 286, 600, 1612, 341, 1051, 13, 3432, 486, 352, 11, 558, 11, 1392, 11, 321, 600, 658, 341, 2636, 3595, 293, 309, 311, 51664], "temperature": 0.0, "avg_logprob": -0.1857434100792056, "compression_ratio": 1.7153024911032029, "no_speech_prob": 0.1936698853969574}, {"id": 174, "seek": 104240, "start": 1042.4, "end": 1057.4, "text": " this third party code that we deploy. How are we going to instrument that? Do not instrument the stuff that is there and then accept that you're going to get to a point where it's going to roll over to logs and", "tokens": [50364, 341, 2636, 3595, 3089, 300, 321, 7274, 13, 1012, 366, 321, 516, 281, 7198, 300, 30, 1144, 406, 7198, 264, 1507, 300, 307, 456, 293, 550, 3241, 300, 291, 434, 516, 281, 483, 281, 257, 935, 689, 309, 311, 516, 281, 3373, 670, 281, 20820, 293, 51114], "temperature": 0.0, "avg_logprob": -0.1725601073234312, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.10482795536518097}, {"id": 175, "seek": 104240, "start": 1057.4, "end": 1068.4, "text": " metrics. If the tool that you're using allows you to be able to connect up logs and metrics to your traces, that's really handy because remember in these big organizations, you might never reach the golden", "tokens": [51114, 16367, 13, 759, 264, 2290, 300, 291, 434, 1228, 4045, 291, 281, 312, 1075, 281, 1745, 493, 20820, 293, 16367, 281, 428, 26076, 11, 300, 311, 534, 13239, 570, 1604, 294, 613, 955, 6150, 11, 291, 1062, 1128, 2524, 264, 9729, 51664], "temperature": 0.0, "avg_logprob": -0.1725601073234312, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.10482795536518097}, {"id": 176, "seek": 106840, "start": 1068.4, "end": 1080.4, "text": " sunlit uplands of traces for everything. So you're always going to have to go back. You can think of it sort of like fast travelling through the infrastructure as that you are not going to be able to get to the point where", "tokens": [50364, 3295, 23062, 493, 10230, 295, 26076, 337, 1203, 13, 407, 291, 434, 1009, 516, 281, 362, 281, 352, 646, 13, 509, 393, 519, 295, 309, 1333, 295, 411, 2370, 20515, 807, 264, 6896, 382, 300, 291, 366, 406, 516, 281, 312, 1075, 281, 483, 281, 264, 935, 689, 50964], "temperature": 0.0, "avg_logprob": -0.12470433371407645, "compression_ratio": 1.8663967611336032, "no_speech_prob": 0.30968305468559265}, {"id": 177, "seek": 106840, "start": 1080.4, "end": 1093.4, "text": " necessarily you're going to be able to get into the point in the Oracle database that you're really trying to kill that has actually had this problem. But you will be able to fast travel to the bit in the code where it makes a query to an", "tokens": [50964, 4725, 291, 434, 516, 281, 312, 1075, 281, 483, 666, 264, 935, 294, 264, 25654, 8149, 300, 291, 434, 534, 1382, 281, 1961, 300, 575, 767, 632, 341, 1154, 13, 583, 291, 486, 312, 1075, 281, 2370, 3147, 281, 264, 857, 294, 264, 3089, 689, 309, 1669, 257, 14581, 281, 364, 51614], "temperature": 0.0, "avg_logprob": -0.12470433371407645, "compression_ratio": 1.8663967611336032, "no_speech_prob": 0.30968305468559265}, {"id": 178, "seek": 109340, "start": 1093.4, "end": 1111.4, "text": " Oracle database and then you'll know which logs and traces to look at. So the goal really is for wide coverage, especially of hot paths. And that brings us to another thing which is culture change. So you've been working on this for maybe", "tokens": [50364, 25654, 8149, 293, 550, 291, 603, 458, 597, 20820, 293, 26076, 281, 574, 412, 13, 407, 264, 3387, 534, 307, 337, 4874, 9645, 11, 2318, 295, 2368, 14518, 13, 400, 300, 5607, 505, 281, 1071, 551, 597, 307, 3713, 1319, 13, 407, 291, 600, 668, 1364, 322, 341, 337, 1310, 51264], "temperature": 0.0, "avg_logprob": -0.11791245142618816, "compression_ratio": 1.4691358024691359, "no_speech_prob": 0.13727670907974243}, {"id": 179, "seek": 111140, "start": 1111.4, "end": 1124.4, "text": " six months or so. It's fairly short projects. You've gotten traces. You've got end to end on many of the request pass through the systems. People kind of get observability now. So those three people will come out with a few others and build an observability", "tokens": [50364, 2309, 2493, 420, 370, 13, 467, 311, 6457, 2099, 4455, 13, 509, 600, 5768, 26076, 13, 509, 600, 658, 917, 281, 917, 322, 867, 295, 264, 5308, 1320, 807, 264, 3652, 13, 3432, 733, 295, 483, 9951, 2310, 586, 13, 407, 729, 1045, 561, 486, 808, 484, 365, 257, 1326, 2357, 293, 1322, 364, 9951, 2310, 51014], "temperature": 0.0, "avg_logprob": -0.11298146614661583, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.8569977879524231}, {"id": 180, "seek": 111140, "start": 1124.4, "end": 1138.4, "text": " engineering team, right? I would say that for most organizations, that's the wrong way to do things. There are companies for which observability engineering having separate teams stood up for that kind of thing does make", "tokens": [51014, 7043, 1469, 11, 558, 30, 286, 576, 584, 300, 337, 881, 6150, 11, 300, 311, 264, 2085, 636, 281, 360, 721, 13, 821, 366, 3431, 337, 597, 9951, 2310, 7043, 1419, 4994, 5491, 9371, 493, 337, 300, 733, 295, 551, 775, 652, 51714], "temperature": 0.0, "avg_logprob": -0.11298146614661583, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.8569977879524231}, {"id": 181, "seek": 113840, "start": 1138.4, "end": 1158.4, "text": " sense. But for most places, you're really going to be looking at creating this kind of. This is one of my favorite slides ever, which is weird. I have weird favorite things. But this talks about like a DevOps transformation where what you do is you create a DevOps team and the best DevOps team disappears after like six or", "tokens": [50364, 2020, 13, 583, 337, 881, 3190, 11, 291, 434, 534, 516, 281, 312, 1237, 412, 4084, 341, 733, 295, 13, 639, 307, 472, 295, 452, 2954, 9788, 1562, 11, 597, 307, 3657, 13, 286, 362, 3657, 2954, 721, 13, 583, 341, 6686, 466, 411, 257, 43051, 9887, 689, 437, 291, 360, 307, 291, 1884, 257, 43051, 1469, 293, 264, 1151, 43051, 1469, 25527, 934, 411, 2309, 420, 51364], "temperature": 0.0, "avg_logprob": -0.19248591678243288, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.14609485864639282}, {"id": 182, "seek": 115840, "start": 1158.4, "end": 1177.4, "text": " twelve months because what it's done is it's created this thing where they come together. And you should, you know, this is a valid way of doing things for observability as well. Ultimately, you may have an enablement team. However, instrumentation should be being done by devs as part of their day to day work", "tokens": [50364, 14390, 2493, 570, 437, 309, 311, 1096, 307, 309, 311, 2942, 341, 551, 689, 436, 808, 1214, 13, 400, 291, 820, 11, 291, 458, 11, 341, 307, 257, 7363, 636, 295, 884, 721, 337, 9951, 2310, 382, 731, 13, 23921, 11, 291, 815, 362, 364, 9528, 518, 1469, 13, 2908, 11, 7198, 399, 820, 312, 885, 1096, 538, 1905, 82, 382, 644, 295, 641, 786, 281, 786, 589, 51314], "temperature": 0.0, "avg_logprob": -0.13214715321858725, "compression_ratio": 1.5346534653465347, "no_speech_prob": 0.8034475445747375}, {"id": 183, "seek": 117740, "start": 1177.4, "end": 1206.4, "text": " The tooling needs to, oh five minutes. Oh, slow down. Enablement should be sharing best practices and doing training and stuff. The tooling really needs to be absorbed into an existing platform team. And this is the really cool part is that now, if you think about it, you've gotten to this point where you've got all this instrumentation into your codes. You can start thinking about what kind of tooling makes sense for your organization. Whereas when you started, it's very hard to do.", "tokens": [50414, 440, 46593, 2203, 281, 11, 1954, 1732, 2077, 13, 876, 11, 2964, 760, 13, 2193, 712, 518, 820, 312, 5414, 1151, 7525, 293, 884, 3097, 293, 1507, 13, 440, 46593, 534, 2203, 281, 312, 20799, 666, 364, 6741, 3663, 1469, 13, 400, 341, 307, 264, 534, 1627, 644, 307, 300, 586, 11, 498, 291, 519, 466, 309, 11, 291, 600, 5768, 281, 341, 935, 689, 291, 600, 658, 439, 341, 7198, 399, 666, 428, 14211, 13, 509, 393, 722, 1953, 466, 437, 733, 295, 46593, 1669, 2020, 337, 428, 4475, 13, 13813, 562, 291, 1409, 11, 309, 311, 588, 1152, 281, 360, 13, 51814], "temperature": 0.0, "avg_logprob": -0.18198578312711897, "compression_ratio": 1.6885813148788926, "no_speech_prob": 0.8189054131507874}, {"id": 184, "seek": 120740, "start": 1207.4, "end": 1231.4, "text": " That wasn't five minutes. Okay, I'll stop. So, yeah, if you've done your job well, hopefully these people won't need you anymore. And you can go and absorb back into teams and you can call that project complete. You might be able to do some kind of enablement team. But as I said, that wasn't meant to have a question mark at the end of it.", "tokens": [50364, 663, 2067, 380, 1732, 2077, 13, 1033, 11, 286, 603, 1590, 13, 407, 11, 1338, 11, 498, 291, 600, 1096, 428, 1691, 731, 11, 4696, 613, 561, 1582, 380, 643, 291, 3602, 13, 400, 291, 393, 352, 293, 15631, 646, 666, 5491, 293, 291, 393, 818, 300, 1716, 3566, 13, 509, 1062, 312, 1075, 281, 360, 512, 733, 295, 9528, 518, 1469, 13, 583, 382, 286, 848, 11, 300, 2067, 380, 4140, 281, 362, 257, 1168, 1491, 412, 264, 917, 295, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12482486374076755, "compression_ratio": 1.5044247787610618, "no_speech_prob": 0.040680743753910065}, {"id": 185, "seek": 123140, "start": 1232.4, "end": 1245.4, "text": " Go and effect change. I'm going to end it there. There wasn't much time. I've got so much more I want to talk about on this subject. So I might do a follow up thing. If you have any questions, I'd be happy to answer them and you can find where to find me at that website.", "tokens": [50414, 1037, 293, 1802, 1319, 13, 286, 478, 516, 281, 917, 309, 456, 13, 821, 2067, 380, 709, 565, 13, 286, 600, 658, 370, 709, 544, 286, 528, 281, 751, 466, 322, 341, 3983, 13, 407, 286, 1062, 360, 257, 1524, 493, 551, 13, 759, 291, 362, 604, 1651, 11, 286, 1116, 312, 2055, 281, 1867, 552, 293, 291, 393, 915, 689, 281, 915, 385, 412, 300, 3144, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12117939204960079, "compression_ratio": 1.5244444444444445, "no_speech_prob": 0.07258695363998413}, {"id": 186, "seek": 123140, "start": 1252.4, "end": 1259.4, "text": " Thanks, James. We have still five minutes for Q&A. Some questions here.", "tokens": [51414, 2561, 11, 5678, 13, 492, 362, 920, 1732, 2077, 337, 1249, 5, 32, 13, 2188, 1651, 510, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12117939204960079, "compression_ratio": 1.5244444444444445, "no_speech_prob": 0.07258695363998413}, {"id": 187, "seek": 126140, "start": 1262.4, "end": 1266.4, "text": " Okay.", "tokens": [50414, 1033, 13, 50614], "temperature": 0.0, "avg_logprob": -0.22560216510106648, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.082683265209198}, {"id": 188, "seek": 126140, "start": 1266.4, "end": 1269.4, "text": " There is one.", "tokens": [50614, 821, 307, 472, 13, 50764], "temperature": 0.0, "avg_logprob": -0.22560216510106648, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.082683265209198}, {"id": 189, "seek": 126140, "start": 1269.4, "end": 1272.4, "text": " I answered almost everything.", "tokens": [50764, 286, 10103, 1920, 1203, 13, 50914], "temperature": 0.0, "avg_logprob": -0.22560216510106648, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.082683265209198}, {"id": 190, "seek": 126140, "start": 1272.4, "end": 1285.4, "text": " Hi. How long has it taken for you to convince a big org and an old company to move from no observability to some sort of observability?", "tokens": [50914, 2421, 13, 1012, 938, 575, 309, 2726, 337, 291, 281, 13447, 257, 955, 14045, 293, 364, 1331, 2237, 281, 1286, 490, 572, 9951, 2310, 281, 512, 1333, 295, 9951, 2310, 30, 51564], "temperature": 0.0, "avg_logprob": -0.22560216510106648, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.082683265209198}, {"id": 191, "seek": 126140, "start": 1285.4, "end": 1288.4, "text": " Completely convinced. I'll let you know.", "tokens": [51564, 39978, 12561, 13, 286, 603, 718, 291, 458, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22560216510106648, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.082683265209198}, {"id": 192, "seek": 128840, "start": 1289.4, "end": 1300.4, "text": " So I maybe joined back in May with this client and was helping them with a previous project that was getting wrapped up.", "tokens": [50414, 407, 286, 1310, 6869, 646, 294, 1891, 365, 341, 6423, 293, 390, 4315, 552, 365, 257, 3894, 1716, 300, 390, 1242, 14226, 493, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12872539349456333, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.07215122133493423}, {"id": 193, "seek": 128840, "start": 1300.4, "end": 1308.4, "text": " So I'd say it's been eight or nine months working on other things and identifying this as a need where it's been working through.", "tokens": [50964, 407, 286, 1116, 584, 309, 311, 668, 3180, 420, 4949, 2493, 1364, 322, 661, 721, 293, 16696, 341, 382, 257, 643, 689, 309, 311, 668, 1364, 807, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12872539349456333, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.07215122133493423}, {"id": 194, "seek": 128840, "start": 1308.4, "end": 1310.4, "text": " Yeah, it can take time.", "tokens": [51364, 865, 11, 309, 393, 747, 565, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12872539349456333, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.07215122133493423}, {"id": 195, "seek": 131040, "start": 1311.4, "end": 1315.4, "text": " So because you've got all, as I said, you go back to that stakeholder slide.", "tokens": [50414, 407, 570, 291, 600, 658, 439, 11, 382, 286, 848, 11, 291, 352, 646, 281, 300, 43406, 4137, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13968180795001167, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.16326706111431122}, {"id": 196, "seek": 131040, "start": 1315.4, "end": 1319.4, "text": " I could have spent a whole 20 minutes just on that because you've got to kind of get everybody aligned.", "tokens": [50614, 286, 727, 362, 4418, 257, 1379, 945, 2077, 445, 322, 300, 570, 291, 600, 658, 281, 733, 295, 483, 2201, 17962, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13968180795001167, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.16326706111431122}, {"id": 197, "seek": 131040, "start": 1319.4, "end": 1324.4, "text": " I've done lots of like meetings. I've shown the people off, shown things off to people.", "tokens": [50814, 286, 600, 1096, 3195, 295, 411, 8410, 13, 286, 600, 4898, 264, 561, 766, 11, 4898, 721, 766, 281, 561, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13968180795001167, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.16326706111431122}, {"id": 198, "seek": 131040, "start": 1324.4, "end": 1327.4, "text": " I've shown off all these slides and stuff, gotten everybody on board.", "tokens": [51064, 286, 600, 4898, 766, 439, 613, 9788, 293, 1507, 11, 5768, 2201, 322, 3150, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13968180795001167, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.16326706111431122}, {"id": 199, "seek": 131040, "start": 1327.4, "end": 1337.4, "text": " And yeah, so I think by the time, you know, I'd say that everyone's actually in lockstep, probably about now actually.", "tokens": [51214, 400, 1338, 11, 370, 286, 519, 538, 264, 565, 11, 291, 458, 11, 286, 1116, 584, 300, 1518, 311, 767, 294, 4017, 16792, 11, 1391, 466, 586, 767, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13968180795001167, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.16326706111431122}, {"id": 200, "seek": 134040, "start": 1341.4, "end": 1347.4, "text": " I should say though, by the way, is we didn't, you know, just not do anything until that point.", "tokens": [50414, 286, 820, 584, 1673, 11, 538, 264, 636, 11, 307, 321, 994, 380, 11, 291, 458, 11, 445, 406, 360, 1340, 1826, 300, 935, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18095349010668302, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.0041212281212210655}, {"id": 201, "seek": 134040, "start": 1347.4, "end": 1352.4, "text": " So there's been lots of opportunities to like seed things as we've been doing other work as well.", "tokens": [50714, 407, 456, 311, 668, 3195, 295, 4786, 281, 411, 8871, 721, 382, 321, 600, 668, 884, 661, 589, 382, 731, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18095349010668302, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.0041212281212210655}, {"id": 202, "seek": 134040, "start": 1352.4, "end": 1354.4, "text": " So yeah.", "tokens": [50964, 407, 1338, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18095349010668302, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.0041212281212210655}, {"id": 203, "seek": 134040, "start": 1356.4, "end": 1363.4, "text": " All right. Some questions? No, then thanks, James.", "tokens": [51164, 1057, 558, 13, 2188, 1651, 30, 883, 11, 550, 3231, 11, 5678, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18095349010668302, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.0041212281212210655}, {"id": 204, "seek": 134040, "start": 1363.4, "end": 1365.4, "text": " Thank you.", "tokens": [51514, 1044, 291, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18095349010668302, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.0041212281212210655}, {"id": 205, "seek": 137040, "start": 1370.4, "end": 1372.4, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.49532536665598553, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9448984265327454}], "language": "en"}