{"text": " Hi, everyone. So today I'm going to talk about the state of the rest SDK in 2023. So all the things that we've accomplished as of last year and some of our future plans as well. So first of all, who am I and how did I get into the rest SDK? Well, I'm Benjamin Bouvi\u00e9. I'm a software engineer in the Rust team at Element. Prior to that, I worked in a game engine, well, a game dev company on a game engine that was written in Rust and WebAssembly. And prior to that, I was a compiler engineer in the SpiderMonkey team, which is the JavaScript engine, powering Firefox, where I did Rust and WebAssembly. So you can sense that there is a common theme here. And back in the days at Mozilla, we were using IRC. And so I wrote a few bots that were just pulling out jokes from the internet and posting them on the channels. And then at some point, we decided to use this new cool thing called Matrix. And so I rewrote my bots so that they could also run on the Matrix using JavaScript at the time, because when you work at Mozilla, you have to bet on JavaScript all the time. And then a few years later, I decided to rewrite it in Rust because I like Rust. And I made this framework system called Trinity that would use Rust for interacting with the Matrix system. And then you can actually write the bot comments themselves using WebAssembly, which is pretty sweet. And I experimented to neutralize it in production. It's mostly a fun project. And that's how I started to use the Rust SDK. So what is the Rust SDK? Very good question. So it's a Rust library implementing the client server API to allow you to implement clients easily if you want to use Rust in your project. So the code is available on GitHub under the FHT2 license. And it does all the things that you would expect from a Matrix client. Logging in, logging out, sending messages, receiving messages. But I guess the most interesting thing is that you get into an encryption for free. And you don't have to worry about the, excuse my French, gory details in the sense that you don't have to learn about Olm, Meg Olm, like sending, uploading your keys, claiming keys, querying keys and all of that stuff, which is very fine. And that we handle for you. Some history for this Rust SDK. So there was in the past one project that was called Ruma for Rust Matrix, which modeled all the events that are happening in a Matrix room timeline. And also all the requests and responses to the endpoints. And the goal at the time, I think, was to try to create a home server in Rust. Eventually that didn't happen for the Ruma project itself. But people realized that it was a good idea to actually model all those events, request and responses, and reuse them across other projects. And there was another Rust home server that started to be written and that is conduit. And like in another timeline in the world, so there was Damir, who is now the team leader at the Rust SDK team at Element. He was doing Rust on his part like free time. And he maintained a small plugin so that you can use WeChat with Matrix. And that was written in Python. And so as he was trying to learn Rust, he decided to rewrite it in Rust. And the thing is, well, he did so. He searched for library written in Rust to do that. And there was none. So he decided to start one. And that's how the Matrix Rust SDK started. And from the outset, it started to use Ruma because it made sense. And that allowed to reuse massive amounts of code, which was very nice. And Damir, being a crypto engineer, he also implemented all the crypto stack, which was very sweet. And then that was first in the Matrix Rust SDK. And all of that code was pulled out and extracted as an independent library called Vodose Mac, which apparently in question means amphibian. And it's like a big pun across languages like Olm, Megolm, and like all of these just refers to amphibians, it seems. And yeah, so that's how it goes. All right. So why Rust, you would ask, well, this is my minute for the Rust evangelizing taskforce. So I mean, you're probably convinced if you're here already, but it's like at the same time high level and super fast. It allows you to write code in a very fast fashion without having to worry about lots of low level details and issues. It is secure and memory safe, which is very nice for a library because you want to have something very robust. It has an amazing tooling and ecosystem, like all the packages, the crates that are published on craze.io give you all the things that you want to have. And like the cargo, the tool that does it all is just wonderful. You can run tests and, you know, the documentation and all of that. You just want to also very important for the rest of this talk. It is compatible with foreign function interfaces. So you can call into other native languages that speak the CABI. So it's quite important to see. And one of the things that is maybe a bit undervalued in the Rust community is that it's actually also in trying to empower you to write a multithreaded code without you having to know too much about it, trying to make it very accessible. And it's a value that was in the community first, and you can find it in all the places. It transpires in translates to all the places in Rust from the error messages that just hold your hands and try to explain you what you did wrong and try to tell you how to fix the problem that you run into, et cetera, et cetera. So it's very sweet to use. And yeah, being a former C++ programmer, so there was this notice in one of the offices where I worked before that read, you must be this tall to write multithreaded code. And it's apparently at three meters high on the wall. So this is something of the past. Like with Rust, you can just be fearless when you're writing multithreaded code because there is this thing called the ownership model. And that makes it really easy to also model concurrent implementation of anything really. So that's really, really nice. So why the Rust SDK? Well, there was this story where we had three apps, basically Android apps, the iOS apps and the web version that is also powering the desktop version. And they all were using a different SDK and a different crypto stack. So that means that if you are serious about your security, and you want to, for instance, audit your cryptography, now you have to do it in three places and make sure that every single implementation actually does what it's supposed to do, which is a bit of a nightmare. And now you have also per platform issues. You can have a bug in one stack, and then you need to check whether the other stacks also have it, et cetera, et cetera. Well, now we are saying, no, we have only a single stack for the element apps, and it's written in Rust. In particular, it's a single crypto stack. You have very high test coverage. As I'm speaking, it's like more than 83% of test coverage in the Rust SDK. The VodoZemac library, the crypto stack is being first as well, which is very important in terms of finding issues, security issues. So that means it's a single place where you can add features, you can code once, where you use everywhere the old Java Dream that everybody knows and loves about. All right, who's using it? So there is Fractal, the GTK-based Matrix client, which is using it. There is IMB, terminal UI client, if you like, Veeam bindings and all of that. There's the new generation of element apps. The element X apps are only using that, which is pretty sweet. And also the crypto stack, as it could be extracted, and it's also like there are specific bindings just for the crypto stack. And so it could be used in the current generation of element apps. And it's another codename element R. And I guess that you can imagine what the R stands for at this point. Rust. All right. So what happened since the last first time? Well, the previous release of the Rust SDK was in October 22. So we made a new release this year. Yay! At the beginning of this month. Thank you. So it's still not 1.0, still quite experimental. We're breaking APIs all the time, but trying to do a better job at writing, changing logs and all of that. And we'll see how it goes. So new features. So you probably heard about sliding sync last year. And this year, the new kind of sync synchronization that makes it so that logging into a new device and retrieving events is always instant, even if you haven't opened the app for months or years. So we entirely support that. There is the basic feature that you can subscribe to specific rooms and list of rooms of which we get a sliding window that is computed by the server. But we're getting rid of that, as Matthew said. And it also implements a modular design in the sense that you have opt-in extensions for read receipts and typing notices and many other things. And all of that is supported in the SDK. As you can see on the right, it's quite verbose because, well, it's a very versatile and general like API to give you the most control so that you can build higher level primitives on top of that. We'll get back to that. And it's vitrugated behind the experimental sliding sync cargo feature. And you can, we basically use it in production in element X. So it's quite stable, actually. There's also support for OIDC, so OpenID Connect. It's a cross stack effort moving from the custom metrics authentication to OpenID Connect. If you have a metrics authentication service running, so it's another service running on your server alongside the Synapse or your own server, it can act as an actual OIDC provider or specialized proxy to an upstream provider. So if you have a GitLab instance, for instance, you can connect it to the metrics authentication system and then have your GitLab users log into matrix for free, like that. And so that's the server side part. It's also written in REST, which is pretty sweet because that means that the request and responses can be actually reused in the client, the REST metrics SDK. And the SDK implements all of that already. And we are also using it in production in element X. So it gives you all the things that you would like to do with OIDC, create, reload, metadata, register on your OIDC client, do the login flow in all the steps and all of that. And it's also behind the cargo feature at this point. Among the big news, we have a new default storage backend. So the storage backend are implemented using traits, which are REST for interfaces. The previous defaults when you wanted to persist things on disk was sled. And now it's been replaced to SQLite because, well, pretty much everybody knows about SQL. And it's also much faster for our use case. We still have an in memory backend if you don't care about losing states and an index DB backend that is used when you're compiling for the web to WebAssembly. Some new cryptography features. So there is this new thing called secret storage. And it's mostly an implementation detail, but it gives you an encrypted key valley store that is backed in the user account data. And where you can put any information that you would like to share across all your devices in a secure way. Like the server doesn't know about this information. It cannot peek into it and know what is in there because it's also encrypted. On top of that, we implemented key backup and restoration. So that means that when you have a new device, well, when you're using elementics, for instance, it will store all the room keys that are used for decrypting room messages in encrypted rooms in the secret storage. And then another device can restore them so that you can actually see the history of events before you joined with that new device. Also, in addition to that, we made it so that the cross-signing automatically happens and you don't have to worry about this at all. That's what's used to verify your own devices and other people's devices. And it's also like some of the private keys are stored in that secret storage as well. And speaking of high-level primitives, so we made a new crate, new package called the Matrix SDK UI. It is highly experimental and also highly opinionated in the sense that we're enabling a few cargo features by default. And we are trying to make it so that we implement the best practices in terms of user experience and performance. And it's also as robust and tested as the rest of the SDK, which is very sweet. And we use sliding sync as a foundation for all these new high-level features. One of these features is the room list service, which, as its name suggests, gives you a list of the rooms. Yes, it does it so in a way that we try to make it to show something to the user as soon as possible. So that's how you feel that the app is kind of instant when you open the app, because it will try to load just one event for all the rooms you were in, or for no, a few rooms you were in. So you have something to display. And then in the background, once that's done, it will try to fetch more events. And also you can configure it to say, this is a set of visible rooms in my apps. So because when you have an app, you cannot show like a thousand rooms, you will only show a subset, right? So you can configure it to say, this is the ones that are actually rendered on the screen. And those are prioritized so that you get more events for these rooms. Another thing we added was the encryption service. So it's basically a sliding thing that is just running encryption on the side, and it gives you access to more concurrency with the other one. So think of it this way, the room list service, the one I just talked about, when you're scrolling on a mobile app, it will change the list of rooms that are shown on the screen, right? So that now means that it's sending new requests to ask for things. And if we did the encryption in the same request, but it's getting a bit technical, but that would mean that we would need to abort those requests and delay encryption. So now we have basically more concurrency and more performance. And we can do the encryption task in the background while you're still scrolling on the room list using this encryption service. And we also have a notification service. So that's very specialized client that just handles push notifications. So if you're given an event and a room identifier, we want to retrieve the event and maybe a bit of context, like what's your name, what's the name of the person who sent the message to you and all of that. It's also using a sliding thing for that. And it makes use of the encryption service because on an encrypted room, of course, you would get a push notification for an encrypted event and the server cannot know if it's a meaningful event, right? Maybe it's just a reaction, putting a thumbs up on one of your messages. So we decrypt the event in the client itself and then we decide whether it's worth sharing as a notification. The one fun thing, if you can call it fun, is on iOS, if you want to modify the notification in case it's encrypted, it's running a separate process. And that makes our life very hard because even if you're just decrypting data, the state of the cryptography keys is mutably changed, right? So now we have multiple states that is global across two processes that are sharing the same database. So we had to be a bit creative to solve that issue and we are basically enabling the writer head log in SQLite and using some data in the database to indicate who's the process that currently tries to read and write to the database. So basically implementing a text like that. All right. And since we added those two services, the encryption service and the room service, we wanted to make it very simple to just fire synchronization and forget about it. So we made some nice high-level service that just wraps the other two and you can just build it and start it and it will do all those things for you and implement all the best practices and you don't have to worry about any of this. And then you can just take listeners on that service and get information that is meaningful to do when we're rendering for a client. Now that we have a list of rooms and decrypted events, what do we do? Well, we want to display them and we have an API for that called the Timeline API. It's basically a room view MVC, so model view components on steroids. The thing is that in the matrix protocol, events are actually like atomic. It's an app and only database. So let's say you have a thumbs up reaction to a message that is a response to something else that would be two events, like the reaction itself and the message itself. So the timeline will aggregate all those different events into a single timeline item that is much more what you want to render as a client on the screen. So it makes it much simpler to render a single timeline like that. And it does a lot of things for you too. It can enter local echoes. So basically when you're sending a message to a room, you want to show it even before the server has returned that it received it. So it will do that and then reconcile the response from the server with the local state and all of that. So it's pretty sweet. And it's all observable, very reactive. So that's nice. You just get, like, as a user of that API, you get notification that one item has been added or removed or updated. And you can just, like, react according to that. So how is this all used in ElementX? We're using a Mozilla project called Unify, Unify FFI. It will automatically create bindings for you for calling interest from other languages. So at this point, we generate bindings for Swift on iOS and Kotlin and Android. It can also generate bindings for other languages. And we use that for Go, for testing purposes, I think. It requires a bit of integration with the foreign languages runtime. And over the years, we've contributed a few PRs to this project. So we made it so that you can just use procedural macros for exporting your types and your input blocks to other languages. And we also added this year's support for async code. So you don't have to block when calling into an async function on the Rust side. It will just look as an async function on the Kotlin or Swift side. And you have actual concurrency and background processing happening, which is pretty sweet for performance. And reactive programming in Rust. How do we do it? Well, the principle of reactive programming is you have some data and you want to make it observable so people can subscribe to it. And then they will get notifications. And I mentioned the Timeline API that will notify you when there is a new Timeline item that has been added, removed, et cetera. So we're using crates that we created ourselves, IBOL. And there's also an extension that is div-based for collections because when you have a vector with a thousand entries in it, you don't want to say, oh, now there's a new thing that has been pushed into the vector. I hear all the 1,001 entries for that vector. No, you just want to hear that there's a new entry and that's its position, right? It also has some extra querying facilities. So you can batch all these updates, div updates. So you don't have to cross the FFI language boundary too often. That has an inherent cost that we want to avoid, some overhead that we want to avoid. And for your batch transaction, well, for your batch to be quite precise, you need to have also transactions to say, this is the beginning of the batch, this is the end of the batch. And also you can do some filtering on these stream of events, limiting, sorting. So it's kind of mapping to things that you would do on SQL in general. It's pretty sweet and that's what we're using, for instance, to filter the rooms in the room list immediately on the client side. All right. So some of the future work that we're going to do, well, I intentionally remain a bit vague here, but we're going to eventually support all the major features a matrix client would expect. We are already working on Scrantum cryptography. And as of today, I think there has been a PR against Voters and Mac to have something that is compatible with Libsignal and with what they do. So that's pretty exciting. And there is a general theme of doing more things client side. When you have end to end encryption, your server kind of becomes dumb sometimes because it cannot peak into the encrypted event. And so you have to resolve a lot of things on the client side. If you get a new event in a room, does that trigger a notification for an encrypted room? Well, you have to push a notification and it's the clients that will decide whether or not it resolves into an actual notification. And even for sorting the room list, you have to do it client side because if there is some room activity, you want to sort by room activity, just show me the room that have some activity. Well, it's the same thing. If the event was encrypted, you don't know if it was just a thumbs up reaction. Maybe that doesn't justify putting the room at the top. If it was something meaningful like an actual message. So that means that this task has to be done on the client now. And yeah, we're also computing the other badges client side in the rest SDK. So we are trying to be very careful to not get into static notifications situations because it's a pain for everyone, us included. And yeah, that's pretty much it. All right. Just a few things. Well, first to all the contributors of the rest SDK special shout out to Kevin Komei from the fractal community. It's done like a bunch of work in the rest SDK, including most of the support for OIDC on the client side, which was MaceFPR. And if you want to be on this slide next year, you can contribute to we have a few issues that are tagged as good first issues or help pointed if you want. And I would like to take this opportunity also to thank elements for donating all of my work to the matrix organization. You can also be a supporter of matrix if you want by following one of these two links. Thank you for listening. And I would be happy to answer any questions if you have any. To the internet asking why have you moved away from sled? Why have we moved away from sled? That's a good question. So I think in terms of performance, so slay this, if I recall correctly, I wasn't there when that happened. So it's kind of hard to answer this precisely, but I think that it's a key value store embedded key value store. And the performance was not great, especially on mobile devices. And we just figured that using SQLite that has been like performance tested and improved and tuned over the years was a right thing to do. And also the way you structure your data using a SQL database is quite different from the way you would structure it with a key value store. So it's just like slightly easier to perform requests when you have a SQL database because you know all of that. Yeah. Any other question? The internet also asks how is your developer experience when using UniFi, UniFFI in general? Are there any hard edges? That's a good question. So there's, oh yes, so when using UniFi across, for calling rest across other languages, have there been hard edges? Yes. It's been a few cases where we have a memory leak that is identified. Well, Kotlin uses the GVM and GVM as a garbage collector. And so we accidentally, and when I say we, I think it's like the UniFi group in general, introduced some leaks by having the equivalent of premises or futures leak sometimes. So that was a problem, but usually it's, I would say it's 90% of the time it's stable. And the 10% of the time where there is an issue, it's high priority for us because obviously it breaks our apps. So we fix it, we try to fix it as quick as possible and we contribute back. But most of the time it works fine for Kotlin and Swift. So the support and stability is also per language, I suppose, since you have to create bindings for each language. So yeah, I cannot speak for the Python or Go generation on the UniFi side. But usually, since Mozilla also use UniFi, they have to provide high stability guarantees as well. So they are pretty reactive and also fixing bugs. So it's working well. Yes. I was wondering about the startup times. Yeah, so the question was, what about startup times for the rest of the time? I think there were two questions. The first one was just starting the SDK itself and then when you're syncing a list of rooms, do you get instant and response and all of that? And well, it's native code, so you don't have to boot up an entire VM for the SDK itself. So it's pretty fast. It will restore the state from the disk. So that can be a slow step. But even like for users who have thousands and thousands of rooms open and I'm looking at Matthew on the side of the room, our general benchmark runner, it's pretty fast. And for receiving a room list, we are also tracking these performance of our time. Pretty much instant. And every time there is an improvement that needs to be done, we'll do it. Yeah. I mean, we are in sync with the synchronization times where about between five to 20 minutes, if you are a very heavy weight user of Matrix, now it's really up to three seconds. So consider that an improvement. Any other questions? Yes? What's the state of supporting extensible events in the rest SDK? So I think that's a question for Ruma. And since we're using Ruma for passing the events, and I'm pretty sure that so the rest type system is quite extensive in the sense that you can have union types. And for each event that can be extended, I suppose that you can have there is a variant in that unit tab that says it's a custom event. If you're referring to a specific MSC, I don't know what it is. And I'm sorry about that. Was that a custom MSC or? No. No. Okay. Just events in general. So yes, you will end up in these case where we have, you will match on this union type or the event and it will say, well, it's something I don't know about. So I'm just ending it over to you and you do something with this. Yes? Can we also provide a question for the rest of the SDK? I'll rephrase this question as are there plans to use the rest SDK for web? Because it's not used. So right now, as we are speaking, as of last week, people have started, well, have enabled by default for new logins on Element Web, I think. So that may be the nightly version using the rest cryptography for Element Web. We have a separate repository for bindings that are for WebAssembly because there's no meaning in using Unify for that. We can directly compile the rest to WebAssembly. So no need to have an intermediary in the middle. And I think the long-term goal is to use the rest SDK everywhere for the Element apps at least. So don't, don't take my word as granted, but I think that this is going to happen. Yeah. Any other question? Yes? That's a very good question. So the question is, is search in scope for the rest SDK and what kind of features would be out of scope for the rest SDK? So to respond for search, that depends if you mean room search or message search, full text search. And well, actually it doesn't depend because the answer for both is yes. We're going to try to take care of that. For full text, we, there was a previous client made by Element called Hydrogen. That was a web client and that could do that and had the fancy system to actually index the messages on your client and then share parts of the index with your other clients devices. So we're probably going to reuse and reimplement some of that in the rest SDK at some point. Yeah. In terms of what features are out of scope for the rest SDK, it's kind of hard to tell, but I think that everything that is like high level UI related like rendering widgets, but not in the sense of widget API, but actually UI widgets and stuff like that is not something that we want to implement or provide. And then I think that, well, the MSCs that have proven to be features that have been proven to be not very useful will probably not be implemented. It's not clear what's not in the roadmap at this point. Sorry, it's not a very satisfying answer, but yes, question here. So the question was using the rest SDK can store a lot of data if you're listening to lots of events and is there any way to limit that amount of data that is stored on this? Well, as I was saying, the storage is implemented as a trait. So one could always implement a different version of the Scallot backend and decide to drop items at some point. One thing that we wanted to make it is the ability to store events locally. And that's connected to the previous question. If you want to be able to do full text search, well, you have no other choice, but decrypting all the events and storing them locally, at least in memory for some time to do the indexing. And then the indexes have to go to the disk. And that means that, yeah, the size of the index can grow a lot. And so we would probably have to implement some kind of garbage collection and say, well, we kind of forget about like old data, older than like a month, year or something like that. And we only care about the most recent data. All right, thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.24, "text": " Hi, everyone. So today I'm going to talk about the state of the rest SDK in 2023. So all", "tokens": [50364, 2421, 11, 1518, 13, 407, 965, 286, 478, 516, 281, 751, 466, 264, 1785, 295, 264, 1472, 37135, 294, 44377, 13, 407, 439, 50926], "temperature": 0.0, "avg_logprob": -0.2540592384338379, "compression_ratio": 1.4489795918367347, "no_speech_prob": 0.33920589089393616}, {"id": 1, "seek": 0, "start": 11.24, "end": 16.4, "text": " the things that we've accomplished as of last year and some of our future plans as well.", "tokens": [50926, 264, 721, 300, 321, 600, 15419, 382, 295, 1036, 1064, 293, 512, 295, 527, 2027, 5482, 382, 731, 13, 51184], "temperature": 0.0, "avg_logprob": -0.2540592384338379, "compression_ratio": 1.4489795918367347, "no_speech_prob": 0.33920589089393616}, {"id": 2, "seek": 0, "start": 16.4, "end": 22.32, "text": " So first of all, who am I and how did I get into the rest SDK? Well, I'm Benjamin Bouvi\u00e9.", "tokens": [51184, 407, 700, 295, 439, 11, 567, 669, 286, 293, 577, 630, 286, 483, 666, 264, 1472, 37135, 30, 1042, 11, 286, 478, 22231, 43833, 4917, 526, 13, 51480], "temperature": 0.0, "avg_logprob": -0.2540592384338379, "compression_ratio": 1.4489795918367347, "no_speech_prob": 0.33920589089393616}, {"id": 3, "seek": 0, "start": 22.32, "end": 27.8, "text": " I'm a software engineer in the Rust team at Element. Prior to that, I worked in a game", "tokens": [51480, 286, 478, 257, 4722, 11403, 294, 264, 34952, 1469, 412, 20900, 13, 24032, 281, 300, 11, 286, 2732, 294, 257, 1216, 51754], "temperature": 0.0, "avg_logprob": -0.2540592384338379, "compression_ratio": 1.4489795918367347, "no_speech_prob": 0.33920589089393616}, {"id": 4, "seek": 2780, "start": 27.8, "end": 33.6, "text": " engine, well, a game dev company on a game engine that was written in Rust and WebAssembly.", "tokens": [50364, 2848, 11, 731, 11, 257, 1216, 1905, 2237, 322, 257, 1216, 2848, 300, 390, 3720, 294, 34952, 293, 9573, 10884, 19160, 13, 50654], "temperature": 0.0, "avg_logprob": -0.24235761283647897, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.020728614181280136}, {"id": 5, "seek": 2780, "start": 33.6, "end": 38.84, "text": " And prior to that, I was a compiler engineer in the SpiderMonkey team, which is the JavaScript", "tokens": [50654, 400, 4059, 281, 300, 11, 286, 390, 257, 31958, 11403, 294, 264, 17733, 32498, 4119, 1469, 11, 597, 307, 264, 15778, 50916], "temperature": 0.0, "avg_logprob": -0.24235761283647897, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.020728614181280136}, {"id": 6, "seek": 2780, "start": 38.84, "end": 44.480000000000004, "text": " engine, powering Firefox, where I did Rust and WebAssembly. So you can sense that there is a", "tokens": [50916, 2848, 11, 1347, 278, 46613, 11, 689, 286, 630, 34952, 293, 9573, 10884, 19160, 13, 407, 291, 393, 2020, 300, 456, 307, 257, 51198], "temperature": 0.0, "avg_logprob": -0.24235761283647897, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.020728614181280136}, {"id": 7, "seek": 2780, "start": 44.480000000000004, "end": 52.24, "text": " common theme here. And back in the days at Mozilla, we were using IRC. And so I wrote a few", "tokens": [51198, 2689, 6314, 510, 13, 400, 646, 294, 264, 1708, 412, 3335, 26403, 11, 321, 645, 1228, 16486, 34, 13, 400, 370, 286, 4114, 257, 1326, 51586], "temperature": 0.0, "avg_logprob": -0.24235761283647897, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.020728614181280136}, {"id": 8, "seek": 5224, "start": 52.24, "end": 58.68, "text": " bots that were just pulling out jokes from the internet and posting them on the channels. And", "tokens": [50364, 35410, 300, 645, 445, 8407, 484, 14439, 490, 264, 4705, 293, 15978, 552, 322, 264, 9235, 13, 400, 50686], "temperature": 0.0, "avg_logprob": -0.14706625687448602, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.09924035519361496}, {"id": 9, "seek": 5224, "start": 58.68, "end": 64.36, "text": " then at some point, we decided to use this new cool thing called Matrix. And so I rewrote my", "tokens": [50686, 550, 412, 512, 935, 11, 321, 3047, 281, 764, 341, 777, 1627, 551, 1219, 36274, 13, 400, 370, 286, 319, 7449, 1370, 452, 50970], "temperature": 0.0, "avg_logprob": -0.14706625687448602, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.09924035519361496}, {"id": 10, "seek": 5224, "start": 64.36, "end": 70.92, "text": " bots so that they could also run on the Matrix using JavaScript at the time, because when you", "tokens": [50970, 35410, 370, 300, 436, 727, 611, 1190, 322, 264, 36274, 1228, 15778, 412, 264, 565, 11, 570, 562, 291, 51298], "temperature": 0.0, "avg_logprob": -0.14706625687448602, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.09924035519361496}, {"id": 11, "seek": 5224, "start": 70.92, "end": 76.44, "text": " work at Mozilla, you have to bet on JavaScript all the time. And then a few years later, I decided to", "tokens": [51298, 589, 412, 3335, 26403, 11, 291, 362, 281, 778, 322, 15778, 439, 264, 565, 13, 400, 550, 257, 1326, 924, 1780, 11, 286, 3047, 281, 51574], "temperature": 0.0, "avg_logprob": -0.14706625687448602, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.09924035519361496}, {"id": 12, "seek": 7644, "start": 76.48, "end": 83.03999999999999, "text": " rewrite it in Rust because I like Rust. And I made this framework system called Trinity that would", "tokens": [50366, 28132, 309, 294, 34952, 570, 286, 411, 34952, 13, 400, 286, 1027, 341, 8388, 1185, 1219, 33121, 300, 576, 50694], "temperature": 0.0, "avg_logprob": -0.18630357630112593, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0453016497194767}, {"id": 13, "seek": 7644, "start": 83.03999999999999, "end": 89.16, "text": " use Rust for interacting with the Matrix system. And then you can actually write the bot", "tokens": [50694, 764, 34952, 337, 18017, 365, 264, 36274, 1185, 13, 400, 550, 291, 393, 767, 2464, 264, 10592, 51000], "temperature": 0.0, "avg_logprob": -0.18630357630112593, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0453016497194767}, {"id": 14, "seek": 7644, "start": 89.16, "end": 94.6, "text": " comments themselves using WebAssembly, which is pretty sweet. And I experimented to", "tokens": [51000, 3053, 2969, 1228, 9573, 10884, 19160, 11, 597, 307, 1238, 3844, 13, 400, 286, 5120, 292, 281, 51272], "temperature": 0.0, "avg_logprob": -0.18630357630112593, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0453016497194767}, {"id": 15, "seek": 7644, "start": 94.6, "end": 100.32, "text": " neutralize it in production. It's mostly a fun project. And that's how I started to use the", "tokens": [51272, 10598, 1125, 309, 294, 4265, 13, 467, 311, 5240, 257, 1019, 1716, 13, 400, 300, 311, 577, 286, 1409, 281, 764, 264, 51558], "temperature": 0.0, "avg_logprob": -0.18630357630112593, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0453016497194767}, {"id": 16, "seek": 10032, "start": 100.32, "end": 107.44, "text": " Rust SDK. So what is the Rust SDK? Very good question. So it's a Rust library implementing the", "tokens": [50364, 34952, 37135, 13, 407, 437, 307, 264, 34952, 37135, 30, 4372, 665, 1168, 13, 407, 309, 311, 257, 34952, 6405, 18114, 264, 50720], "temperature": 0.0, "avg_logprob": -0.2000245011371115, "compression_ratio": 1.5355648535564854, "no_speech_prob": 0.018942734226584435}, {"id": 17, "seek": 10032, "start": 107.44, "end": 114.67999999999999, "text": " client server API to allow you to implement clients easily if you want to use Rust in your", "tokens": [50720, 6423, 7154, 9362, 281, 2089, 291, 281, 4445, 6982, 3612, 498, 291, 528, 281, 764, 34952, 294, 428, 51082], "temperature": 0.0, "avg_logprob": -0.2000245011371115, "compression_ratio": 1.5355648535564854, "no_speech_prob": 0.018942734226584435}, {"id": 18, "seek": 10032, "start": 114.67999999999999, "end": 120.6, "text": " project. So the code is available on GitHub under the FHT2 license. And it does all the things", "tokens": [51082, 1716, 13, 407, 264, 3089, 307, 2435, 322, 23331, 833, 264, 479, 39, 51, 17, 10476, 13, 400, 309, 775, 439, 264, 721, 51378], "temperature": 0.0, "avg_logprob": -0.2000245011371115, "compression_ratio": 1.5355648535564854, "no_speech_prob": 0.018942734226584435}, {"id": 19, "seek": 10032, "start": 120.6, "end": 125.16, "text": " that you would expect from a Matrix client. Logging in, logging out, sending messages,", "tokens": [51378, 300, 291, 576, 2066, 490, 257, 36274, 6423, 13, 10824, 3249, 294, 11, 27991, 484, 11, 7750, 7897, 11, 51606], "temperature": 0.0, "avg_logprob": -0.2000245011371115, "compression_ratio": 1.5355648535564854, "no_speech_prob": 0.018942734226584435}, {"id": 20, "seek": 12516, "start": 125.52, "end": 131.48, "text": " receiving messages. But I guess the most interesting thing is that you get into an encryption for", "tokens": [50382, 10040, 7897, 13, 583, 286, 2041, 264, 881, 1880, 551, 307, 300, 291, 483, 666, 364, 29575, 337, 50680], "temperature": 0.0, "avg_logprob": -0.20665070445267195, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.0214620903134346}, {"id": 21, "seek": 12516, "start": 131.48, "end": 136.96, "text": " free. And you don't have to worry about the, excuse my French, gory details in the sense that you", "tokens": [50680, 1737, 13, 400, 291, 500, 380, 362, 281, 3292, 466, 264, 11, 8960, 452, 5522, 11, 290, 827, 4365, 294, 264, 2020, 300, 291, 50954], "temperature": 0.0, "avg_logprob": -0.20665070445267195, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.0214620903134346}, {"id": 22, "seek": 12516, "start": 136.96, "end": 142.0, "text": " don't have to learn about Olm, Meg Olm, like sending, uploading your keys, claiming keys,", "tokens": [50954, 500, 380, 362, 281, 1466, 466, 6141, 76, 11, 9986, 6141, 76, 11, 411, 7750, 11, 27301, 428, 9317, 11, 19232, 9317, 11, 51206], "temperature": 0.0, "avg_logprob": -0.20665070445267195, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.0214620903134346}, {"id": 23, "seek": 12516, "start": 142.0, "end": 148.64, "text": " querying keys and all of that stuff, which is very fine. And that we handle for you. Some", "tokens": [51206, 7083, 1840, 9317, 293, 439, 295, 300, 1507, 11, 597, 307, 588, 2489, 13, 400, 300, 321, 4813, 337, 291, 13, 2188, 51538], "temperature": 0.0, "avg_logprob": -0.20665070445267195, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.0214620903134346}, {"id": 24, "seek": 14864, "start": 148.67999999999998, "end": 156.0, "text": " history for this Rust SDK. So there was in the past one project that was called Ruma for Rust", "tokens": [50366, 2503, 337, 341, 34952, 37135, 13, 407, 456, 390, 294, 264, 1791, 472, 1716, 300, 390, 1219, 497, 5544, 337, 34952, 50732], "temperature": 0.0, "avg_logprob": -0.16917580746589822, "compression_ratio": 1.6297872340425532, "no_speech_prob": 0.032791607081890106}, {"id": 25, "seek": 14864, "start": 156.0, "end": 165.0, "text": " Matrix, which modeled all the events that are happening in a Matrix room timeline. And also all", "tokens": [50732, 36274, 11, 597, 37140, 439, 264, 3931, 300, 366, 2737, 294, 257, 36274, 1808, 12933, 13, 400, 611, 439, 51182], "temperature": 0.0, "avg_logprob": -0.16917580746589822, "compression_ratio": 1.6297872340425532, "no_speech_prob": 0.032791607081890106}, {"id": 26, "seek": 14864, "start": 165.0, "end": 170.44, "text": " the requests and responses to the endpoints. And the goal at the time, I think, was to try to create", "tokens": [51182, 264, 12475, 293, 13019, 281, 264, 917, 20552, 13, 400, 264, 3387, 412, 264, 565, 11, 286, 519, 11, 390, 281, 853, 281, 1884, 51454], "temperature": 0.0, "avg_logprob": -0.16917580746589822, "compression_ratio": 1.6297872340425532, "no_speech_prob": 0.032791607081890106}, {"id": 27, "seek": 14864, "start": 170.44, "end": 176.11999999999998, "text": " a home server in Rust. Eventually that didn't happen for the Ruma project itself. But people", "tokens": [51454, 257, 1280, 7154, 294, 34952, 13, 17586, 300, 994, 380, 1051, 337, 264, 497, 5544, 1716, 2564, 13, 583, 561, 51738], "temperature": 0.0, "avg_logprob": -0.16917580746589822, "compression_ratio": 1.6297872340425532, "no_speech_prob": 0.032791607081890106}, {"id": 28, "seek": 17612, "start": 176.16, "end": 180.92000000000002, "text": " realized that it was a good idea to actually model all those events, request and responses, and", "tokens": [50366, 5334, 300, 309, 390, 257, 665, 1558, 281, 767, 2316, 439, 729, 3931, 11, 5308, 293, 13019, 11, 293, 50604], "temperature": 0.0, "avg_logprob": -0.25842778822954965, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.01554170809686184}, {"id": 29, "seek": 17612, "start": 180.92000000000002, "end": 187.8, "text": " reuse them across other projects. And there was another Rust home server that started to be", "tokens": [50604, 26225, 552, 2108, 661, 4455, 13, 400, 456, 390, 1071, 34952, 1280, 7154, 300, 1409, 281, 312, 50948], "temperature": 0.0, "avg_logprob": -0.25842778822954965, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.01554170809686184}, {"id": 30, "seek": 17612, "start": 187.8, "end": 197.28, "text": " written and that is conduit. And like in another timeline in the world, so there was Damir, who is", "tokens": [50948, 3720, 293, 300, 307, 15504, 270, 13, 400, 411, 294, 1071, 12933, 294, 264, 1002, 11, 370, 456, 390, 5885, 347, 11, 567, 307, 51422], "temperature": 0.0, "avg_logprob": -0.25842778822954965, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.01554170809686184}, {"id": 31, "seek": 19728, "start": 197.32, "end": 206.24, "text": " now the team leader at the Rust SDK team at Element. He was doing Rust on his part like free", "tokens": [50366, 586, 264, 1469, 5263, 412, 264, 34952, 37135, 1469, 412, 20900, 13, 634, 390, 884, 34952, 322, 702, 644, 411, 1737, 50812], "temperature": 0.0, "avg_logprob": -0.19938204447428384, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.043302860110998154}, {"id": 32, "seek": 19728, "start": 206.24, "end": 213.84, "text": " time. And he maintained a small plugin so that you can use WeChat with Matrix. And that was written", "tokens": [50812, 565, 13, 400, 415, 17578, 257, 1359, 23407, 370, 300, 291, 393, 764, 492, 41683, 365, 36274, 13, 400, 300, 390, 3720, 51192], "temperature": 0.0, "avg_logprob": -0.19938204447428384, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.043302860110998154}, {"id": 33, "seek": 19728, "start": 213.84, "end": 220.24, "text": " in Python. And so as he was trying to learn Rust, he decided to rewrite it in Rust. And the thing", "tokens": [51192, 294, 15329, 13, 400, 370, 382, 415, 390, 1382, 281, 1466, 34952, 11, 415, 3047, 281, 28132, 309, 294, 34952, 13, 400, 264, 551, 51512], "temperature": 0.0, "avg_logprob": -0.19938204447428384, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.043302860110998154}, {"id": 34, "seek": 22024, "start": 220.32000000000002, "end": 227.12, "text": " is, well, he did so. He searched for library written in Rust to do that. And there was none. So he", "tokens": [50368, 307, 11, 731, 11, 415, 630, 370, 13, 634, 22961, 337, 6405, 3720, 294, 34952, 281, 360, 300, 13, 400, 456, 390, 6022, 13, 407, 415, 50708], "temperature": 0.0, "avg_logprob": -0.16355587454403148, "compression_ratio": 1.6569037656903767, "no_speech_prob": 0.06785723567008972}, {"id": 35, "seek": 22024, "start": 227.12, "end": 232.52, "text": " decided to start one. And that's how the Matrix Rust SDK started. And from the outset, it started", "tokens": [50708, 3047, 281, 722, 472, 13, 400, 300, 311, 577, 264, 36274, 34952, 37135, 1409, 13, 400, 490, 264, 44618, 11, 309, 1409, 50978], "temperature": 0.0, "avg_logprob": -0.16355587454403148, "compression_ratio": 1.6569037656903767, "no_speech_prob": 0.06785723567008972}, {"id": 36, "seek": 22024, "start": 232.52, "end": 238.92000000000002, "text": " to use Ruma because it made sense. And that allowed to reuse massive amounts of code, which was very", "tokens": [50978, 281, 764, 497, 5544, 570, 309, 1027, 2020, 13, 400, 300, 4350, 281, 26225, 5994, 11663, 295, 3089, 11, 597, 390, 588, 51298], "temperature": 0.0, "avg_logprob": -0.16355587454403148, "compression_ratio": 1.6569037656903767, "no_speech_prob": 0.06785723567008972}, {"id": 37, "seek": 22024, "start": 238.92000000000002, "end": 245.12, "text": " nice. And Damir, being a crypto engineer, he also implemented all the crypto stack, which was very", "tokens": [51298, 1481, 13, 400, 5885, 347, 11, 885, 257, 17240, 11403, 11, 415, 611, 12270, 439, 264, 17240, 8630, 11, 597, 390, 588, 51608], "temperature": 0.0, "avg_logprob": -0.16355587454403148, "compression_ratio": 1.6569037656903767, "no_speech_prob": 0.06785723567008972}, {"id": 38, "seek": 24512, "start": 245.12, "end": 252.48000000000002, "text": " sweet. And then that was first in the Matrix Rust SDK. And all of that code was pulled out and", "tokens": [50364, 3844, 13, 400, 550, 300, 390, 700, 294, 264, 36274, 34952, 37135, 13, 400, 439, 295, 300, 3089, 390, 7373, 484, 293, 50732], "temperature": 0.0, "avg_logprob": -0.22758215376474325, "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.04940856620669365}, {"id": 39, "seek": 24512, "start": 252.48000000000002, "end": 258.68, "text": " extracted as an independent library called Vodose Mac, which apparently in question means", "tokens": [50732, 34086, 382, 364, 6695, 6405, 1219, 691, 378, 541, 5707, 11, 597, 7970, 294, 1168, 1355, 51042], "temperature": 0.0, "avg_logprob": -0.22758215376474325, "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.04940856620669365}, {"id": 40, "seek": 24512, "start": 258.72, "end": 266.36, "text": " amphibian. And it's like a big pun across languages like Olm, Megolm, and like all of these just", "tokens": [51044, 40077, 897, 952, 13, 400, 309, 311, 411, 257, 955, 4468, 2108, 8650, 411, 6141, 76, 11, 9986, 401, 76, 11, 293, 411, 439, 295, 613, 445, 51426], "temperature": 0.0, "avg_logprob": -0.22758215376474325, "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.04940856620669365}, {"id": 41, "seek": 24512, "start": 266.36, "end": 274.68, "text": " refers to amphibians, it seems. And yeah, so that's how it goes. All right. So why Rust, you would", "tokens": [51426, 14942, 281, 40077, 897, 2567, 11, 309, 2544, 13, 400, 1338, 11, 370, 300, 311, 577, 309, 1709, 13, 1057, 558, 13, 407, 983, 34952, 11, 291, 576, 51842], "temperature": 0.0, "avg_logprob": -0.22758215376474325, "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.04940856620669365}, {"id": 42, "seek": 27468, "start": 274.72, "end": 282.92, "text": " ask, well, this is my minute for the Rust evangelizing taskforce. So I mean, you're probably convinced", "tokens": [50366, 1029, 11, 731, 11, 341, 307, 452, 3456, 337, 264, 34952, 24546, 3319, 5633, 5156, 13, 407, 286, 914, 11, 291, 434, 1391, 12561, 50776], "temperature": 0.0, "avg_logprob": -0.12112990113877758, "compression_ratio": 1.582995951417004, "no_speech_prob": 0.015179249458014965}, {"id": 43, "seek": 27468, "start": 282.92, "end": 288.68, "text": " if you're here already, but it's like at the same time high level and super fast. It allows you to", "tokens": [50776, 498, 291, 434, 510, 1217, 11, 457, 309, 311, 411, 412, 264, 912, 565, 1090, 1496, 293, 1687, 2370, 13, 467, 4045, 291, 281, 51064], "temperature": 0.0, "avg_logprob": -0.12112990113877758, "compression_ratio": 1.582995951417004, "no_speech_prob": 0.015179249458014965}, {"id": 44, "seek": 27468, "start": 288.68, "end": 295.52, "text": " write code in a very fast fashion without having to worry about lots of low level details and", "tokens": [51064, 2464, 3089, 294, 257, 588, 2370, 6700, 1553, 1419, 281, 3292, 466, 3195, 295, 2295, 1496, 4365, 293, 51406], "temperature": 0.0, "avg_logprob": -0.12112990113877758, "compression_ratio": 1.582995951417004, "no_speech_prob": 0.015179249458014965}, {"id": 45, "seek": 27468, "start": 295.52, "end": 300.92, "text": " issues. It is secure and memory safe, which is very nice for a library because you want to have", "tokens": [51406, 2663, 13, 467, 307, 7144, 293, 4675, 3273, 11, 597, 307, 588, 1481, 337, 257, 6405, 570, 291, 528, 281, 362, 51676], "temperature": 0.0, "avg_logprob": -0.12112990113877758, "compression_ratio": 1.582995951417004, "no_speech_prob": 0.015179249458014965}, {"id": 46, "seek": 30092, "start": 300.96000000000004, "end": 307.48, "text": " something very robust. It has an amazing tooling and ecosystem, like all the packages, the crates that", "tokens": [50366, 746, 588, 13956, 13, 467, 575, 364, 2243, 46593, 293, 11311, 11, 411, 439, 264, 17401, 11, 264, 941, 1024, 300, 50692], "temperature": 0.0, "avg_logprob": -0.16403714815775552, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.010777464136481285}, {"id": 47, "seek": 30092, "start": 307.48, "end": 313.76, "text": " are published on craze.io give you all the things that you want to have. And like the cargo, the", "tokens": [50692, 366, 6572, 322, 2094, 1381, 13, 1004, 976, 291, 439, 264, 721, 300, 291, 528, 281, 362, 13, 400, 411, 264, 19449, 11, 264, 51006], "temperature": 0.0, "avg_logprob": -0.16403714815775552, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.010777464136481285}, {"id": 48, "seek": 30092, "start": 314.04, "end": 318.72, "text": " tool that does it all is just wonderful. You can run tests and, you know, the documentation and all", "tokens": [51020, 2290, 300, 775, 309, 439, 307, 445, 3715, 13, 509, 393, 1190, 6921, 293, 11, 291, 458, 11, 264, 14333, 293, 439, 51254], "temperature": 0.0, "avg_logprob": -0.16403714815775552, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.010777464136481285}, {"id": 49, "seek": 30092, "start": 318.72, "end": 323.72, "text": " of that. You just want to also very important for the rest of this talk. It is compatible with", "tokens": [51254, 295, 300, 13, 509, 445, 528, 281, 611, 588, 1021, 337, 264, 1472, 295, 341, 751, 13, 467, 307, 18218, 365, 51504], "temperature": 0.0, "avg_logprob": -0.16403714815775552, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.010777464136481285}, {"id": 50, "seek": 30092, "start": 324.08000000000004, "end": 330.28000000000003, "text": " foreign function interfaces. So you can call into other native languages that speak the CABI. So", "tokens": [51522, 5329, 2445, 28416, 13, 407, 291, 393, 818, 666, 661, 8470, 8650, 300, 1710, 264, 383, 13868, 40, 13, 407, 51832], "temperature": 0.0, "avg_logprob": -0.16403714815775552, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.010777464136481285}, {"id": 51, "seek": 33028, "start": 330.32, "end": 335.96, "text": " it's quite important to see. And one of the things that is maybe a bit undervalued in the Rust", "tokens": [50366, 309, 311, 1596, 1021, 281, 536, 13, 400, 472, 295, 264, 721, 300, 307, 1310, 257, 857, 833, 3337, 5827, 294, 264, 34952, 50648], "temperature": 0.0, "avg_logprob": -0.15030227723668832, "compression_ratio": 1.8036363636363637, "no_speech_prob": 0.007829098962247372}, {"id": 52, "seek": 33028, "start": 335.96, "end": 342.79999999999995, "text": " community is that it's actually also in trying to empower you to write a multithreaded code without", "tokens": [50648, 1768, 307, 300, 309, 311, 767, 611, 294, 1382, 281, 11071, 291, 281, 2464, 257, 2120, 355, 2538, 292, 3089, 1553, 50990], "temperature": 0.0, "avg_logprob": -0.15030227723668832, "compression_ratio": 1.8036363636363637, "no_speech_prob": 0.007829098962247372}, {"id": 53, "seek": 33028, "start": 343.03999999999996, "end": 347.91999999999996, "text": " you having to know too much about it, trying to make it very accessible. And it's a value that was in", "tokens": [51002, 291, 1419, 281, 458, 886, 709, 466, 309, 11, 1382, 281, 652, 309, 588, 9515, 13, 400, 309, 311, 257, 2158, 300, 390, 294, 51246], "temperature": 0.0, "avg_logprob": -0.15030227723668832, "compression_ratio": 1.8036363636363637, "no_speech_prob": 0.007829098962247372}, {"id": 54, "seek": 33028, "start": 347.91999999999996, "end": 353.23999999999995, "text": " the community first, and you can find it in all the places. It transpires in translates to all the", "tokens": [51246, 264, 1768, 700, 11, 293, 291, 393, 915, 309, 294, 439, 264, 3190, 13, 467, 7132, 3145, 294, 28468, 281, 439, 264, 51512], "temperature": 0.0, "avg_logprob": -0.15030227723668832, "compression_ratio": 1.8036363636363637, "no_speech_prob": 0.007829098962247372}, {"id": 55, "seek": 33028, "start": 353.52, "end": 359.2, "text": " places in Rust from the error messages that just hold your hands and try to explain you what you did", "tokens": [51526, 3190, 294, 34952, 490, 264, 6713, 7897, 300, 445, 1797, 428, 2377, 293, 853, 281, 2903, 291, 437, 291, 630, 51810], "temperature": 0.0, "avg_logprob": -0.15030227723668832, "compression_ratio": 1.8036363636363637, "no_speech_prob": 0.007829098962247372}, {"id": 56, "seek": 35920, "start": 359.24, "end": 365.44, "text": " wrong and try to tell you how to fix the problem that you run into, et cetera, et cetera. So it's very", "tokens": [50366, 2085, 293, 853, 281, 980, 291, 577, 281, 3191, 264, 1154, 300, 291, 1190, 666, 11, 1030, 11458, 11, 1030, 11458, 13, 407, 309, 311, 588, 50676], "temperature": 0.0, "avg_logprob": -0.15065363021123976, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.01516097504645586}, {"id": 57, "seek": 35920, "start": 365.44, "end": 374.2, "text": " sweet to use. And yeah, being a former C++ programmer, so there was this notice in one of the", "tokens": [50676, 3844, 281, 764, 13, 400, 1338, 11, 885, 257, 5819, 383, 25472, 32116, 11, 370, 456, 390, 341, 3449, 294, 472, 295, 264, 51114], "temperature": 0.0, "avg_logprob": -0.15065363021123976, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.01516097504645586}, {"id": 58, "seek": 35920, "start": 374.2, "end": 380.88, "text": " offices where I worked before that read, you must be this tall to write multithreaded code. And it's", "tokens": [51114, 14434, 689, 286, 2732, 949, 300, 1401, 11, 291, 1633, 312, 341, 6764, 281, 2464, 2120, 355, 2538, 292, 3089, 13, 400, 309, 311, 51448], "temperature": 0.0, "avg_logprob": -0.15065363021123976, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.01516097504645586}, {"id": 59, "seek": 35920, "start": 380.96, "end": 388.12, "text": " apparently at three meters high on the wall. So this is something of the past. Like with Rust, you", "tokens": [51452, 7970, 412, 1045, 8146, 1090, 322, 264, 2929, 13, 407, 341, 307, 746, 295, 264, 1791, 13, 1743, 365, 34952, 11, 291, 51810], "temperature": 0.0, "avg_logprob": -0.15065363021123976, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.01516097504645586}, {"id": 60, "seek": 38812, "start": 388.16, "end": 392.52, "text": " can just be fearless when you're writing multithreaded code because there is this thing called the", "tokens": [50366, 393, 445, 312, 44139, 562, 291, 434, 3579, 2120, 355, 2538, 292, 3089, 570, 456, 307, 341, 551, 1219, 264, 50584], "temperature": 0.0, "avg_logprob": -0.15642094612121582, "compression_ratio": 1.6260162601626016, "no_speech_prob": 0.006945575121790171}, {"id": 61, "seek": 38812, "start": 392.52, "end": 400.12, "text": " ownership model. And that makes it really easy to also model concurrent implementation of anything", "tokens": [50584, 15279, 2316, 13, 400, 300, 1669, 309, 534, 1858, 281, 611, 2316, 37702, 11420, 295, 1340, 50964], "temperature": 0.0, "avg_logprob": -0.15642094612121582, "compression_ratio": 1.6260162601626016, "no_speech_prob": 0.006945575121790171}, {"id": 62, "seek": 38812, "start": 400.16, "end": 409.64, "text": " really. So that's really, really nice. So why the Rust SDK? Well, there was this story where we had", "tokens": [50966, 534, 13, 407, 300, 311, 534, 11, 534, 1481, 13, 407, 983, 264, 34952, 37135, 30, 1042, 11, 456, 390, 341, 1657, 689, 321, 632, 51440], "temperature": 0.0, "avg_logprob": -0.15642094612121582, "compression_ratio": 1.6260162601626016, "no_speech_prob": 0.006945575121790171}, {"id": 63, "seek": 38812, "start": 409.72, "end": 415.08, "text": " three apps, basically Android apps, the iOS apps and the web version that is also powering the desktop", "tokens": [51444, 1045, 7733, 11, 1936, 8853, 7733, 11, 264, 17430, 7733, 293, 264, 3670, 3037, 300, 307, 611, 1347, 278, 264, 14502, 51712], "temperature": 0.0, "avg_logprob": -0.15642094612121582, "compression_ratio": 1.6260162601626016, "no_speech_prob": 0.006945575121790171}, {"id": 64, "seek": 41508, "start": 415.12, "end": 422.12, "text": " version. And they all were using a different SDK and a different crypto stack. So that means that if", "tokens": [50366, 3037, 13, 400, 436, 439, 645, 1228, 257, 819, 37135, 293, 257, 819, 17240, 8630, 13, 407, 300, 1355, 300, 498, 50716], "temperature": 0.0, "avg_logprob": -0.09370704524773212, "compression_ratio": 1.7310344827586206, "no_speech_prob": 0.02684595249593258}, {"id": 65, "seek": 41508, "start": 422.12, "end": 426.56, "text": " you are serious about your security, and you want to, for instance, audit your cryptography, now you", "tokens": [50716, 291, 366, 3156, 466, 428, 3825, 11, 293, 291, 528, 281, 11, 337, 5197, 11, 17748, 428, 9844, 5820, 11, 586, 291, 50938], "temperature": 0.0, "avg_logprob": -0.09370704524773212, "compression_ratio": 1.7310344827586206, "no_speech_prob": 0.02684595249593258}, {"id": 66, "seek": 41508, "start": 426.56, "end": 431.15999999999997, "text": " have to do it in three places and make sure that every single implementation actually does what it's", "tokens": [50938, 362, 281, 360, 309, 294, 1045, 3190, 293, 652, 988, 300, 633, 2167, 11420, 767, 775, 437, 309, 311, 51168], "temperature": 0.0, "avg_logprob": -0.09370704524773212, "compression_ratio": 1.7310344827586206, "no_speech_prob": 0.02684595249593258}, {"id": 67, "seek": 41508, "start": 431.15999999999997, "end": 436.96, "text": " supposed to do, which is a bit of a nightmare. And now you have also per platform issues. You can have", "tokens": [51168, 3442, 281, 360, 11, 597, 307, 257, 857, 295, 257, 18724, 13, 400, 586, 291, 362, 611, 680, 3663, 2663, 13, 509, 393, 362, 51458], "temperature": 0.0, "avg_logprob": -0.09370704524773212, "compression_ratio": 1.7310344827586206, "no_speech_prob": 0.02684595249593258}, {"id": 68, "seek": 41508, "start": 436.96, "end": 441.68, "text": " a bug in one stack, and then you need to check whether the other stacks also have it, et cetera,", "tokens": [51458, 257, 7426, 294, 472, 8630, 11, 293, 550, 291, 643, 281, 1520, 1968, 264, 661, 30792, 611, 362, 309, 11, 1030, 11458, 11, 51694], "temperature": 0.0, "avg_logprob": -0.09370704524773212, "compression_ratio": 1.7310344827586206, "no_speech_prob": 0.02684595249593258}, {"id": 69, "seek": 44168, "start": 441.72, "end": 449.40000000000003, "text": " et cetera. Well, now we are saying, no, we have only a single stack for the element apps, and it's", "tokens": [50366, 1030, 11458, 13, 1042, 11, 586, 321, 366, 1566, 11, 572, 11, 321, 362, 787, 257, 2167, 8630, 337, 264, 4478, 7733, 11, 293, 309, 311, 50750], "temperature": 0.0, "avg_logprob": -0.19835630144391742, "compression_ratio": 1.5494071146245059, "no_speech_prob": 0.005323429591953754}, {"id": 70, "seek": 44168, "start": 449.40000000000003, "end": 455.92, "text": " written in Rust. In particular, it's a single crypto stack. You have very high test coverage. As I'm", "tokens": [50750, 3720, 294, 34952, 13, 682, 1729, 11, 309, 311, 257, 2167, 17240, 8630, 13, 509, 362, 588, 1090, 1500, 9645, 13, 1018, 286, 478, 51076], "temperature": 0.0, "avg_logprob": -0.19835630144391742, "compression_ratio": 1.5494071146245059, "no_speech_prob": 0.005323429591953754}, {"id": 71, "seek": 44168, "start": 455.92, "end": 464.56, "text": " speaking, it's like more than 83% of test coverage in the Rust SDK. The VodoZemac library, the", "tokens": [51076, 4124, 11, 309, 311, 411, 544, 813, 30997, 4, 295, 1500, 9645, 294, 264, 34952, 37135, 13, 440, 691, 17423, 57, 443, 326, 6405, 11, 264, 51508], "temperature": 0.0, "avg_logprob": -0.19835630144391742, "compression_ratio": 1.5494071146245059, "no_speech_prob": 0.005323429591953754}, {"id": 72, "seek": 44168, "start": 464.6, "end": 469.8, "text": " crypto stack is being first as well, which is very important in terms of finding issues, security", "tokens": [51510, 17240, 8630, 307, 885, 700, 382, 731, 11, 597, 307, 588, 1021, 294, 2115, 295, 5006, 2663, 11, 3825, 51770], "temperature": 0.0, "avg_logprob": -0.19835630144391742, "compression_ratio": 1.5494071146245059, "no_speech_prob": 0.005323429591953754}, {"id": 73, "seek": 46980, "start": 469.88, "end": 474.28000000000003, "text": " issues. So that means it's a single place where you can add features, you can code once, where you use", "tokens": [50368, 2663, 13, 407, 300, 1355, 309, 311, 257, 2167, 1081, 689, 291, 393, 909, 4122, 11, 291, 393, 3089, 1564, 11, 689, 291, 764, 50588], "temperature": 0.0, "avg_logprob": -0.2372708407315341, "compression_ratio": 1.632, "no_speech_prob": 0.005109137389808893}, {"id": 74, "seek": 46980, "start": 474.28000000000003, "end": 482.24, "text": " everywhere the old Java Dream that everybody knows and loves about. All right, who's using it? So there", "tokens": [50588, 5315, 264, 1331, 10745, 12105, 300, 2201, 3255, 293, 6752, 466, 13, 1057, 558, 11, 567, 311, 1228, 309, 30, 407, 456, 50986], "temperature": 0.0, "avg_logprob": -0.2372708407315341, "compression_ratio": 1.632, "no_speech_prob": 0.005109137389808893}, {"id": 75, "seek": 46980, "start": 482.24, "end": 488.76, "text": " is Fractal, the GTK-based Matrix client, which is using it. There is IMB, terminal UI client, if you", "tokens": [50986, 307, 1526, 578, 304, 11, 264, 17530, 42, 12, 6032, 36274, 6423, 11, 597, 307, 1228, 309, 13, 821, 307, 21463, 33, 11, 14709, 15682, 6423, 11, 498, 291, 51312], "temperature": 0.0, "avg_logprob": -0.2372708407315341, "compression_ratio": 1.632, "no_speech_prob": 0.005109137389808893}, {"id": 76, "seek": 46980, "start": 488.76, "end": 494.88, "text": " like, Veeam bindings and all of that. There's the new generation of element apps. The element X apps", "tokens": [51312, 411, 11, 691, 1653, 335, 14786, 1109, 293, 439, 295, 300, 13, 821, 311, 264, 777, 5125, 295, 4478, 7733, 13, 440, 4478, 1783, 7733, 51618], "temperature": 0.0, "avg_logprob": -0.2372708407315341, "compression_ratio": 1.632, "no_speech_prob": 0.005109137389808893}, {"id": 77, "seek": 49488, "start": 495.2, "end": 501.32, "text": " are only using that, which is pretty sweet. And also the crypto stack, as it could be extracted, and it's", "tokens": [50380, 366, 787, 1228, 300, 11, 597, 307, 1238, 3844, 13, 400, 611, 264, 17240, 8630, 11, 382, 309, 727, 312, 34086, 11, 293, 309, 311, 50686], "temperature": 0.0, "avg_logprob": -0.17956233719020215, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.018875727429986}, {"id": 78, "seek": 49488, "start": 501.32, "end": 508.4, "text": " also like there are specific bindings just for the crypto stack. And so it could be used in the current", "tokens": [50686, 611, 411, 456, 366, 2685, 14786, 1109, 445, 337, 264, 17240, 8630, 13, 400, 370, 309, 727, 312, 1143, 294, 264, 2190, 51040], "temperature": 0.0, "avg_logprob": -0.17956233719020215, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.018875727429986}, {"id": 79, "seek": 49488, "start": 508.4, "end": 515.56, "text": " generation of element apps. And it's another codename element R. And I guess that you can imagine what", "tokens": [51040, 5125, 295, 4478, 7733, 13, 400, 309, 311, 1071, 17656, 268, 529, 4478, 497, 13, 400, 286, 2041, 300, 291, 393, 3811, 437, 51398], "temperature": 0.0, "avg_logprob": -0.17956233719020215, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.018875727429986}, {"id": 80, "seek": 49488, "start": 515.56, "end": 524.8, "text": " the R stands for at this point. Rust. All right. So what happened since the last first time? Well,", "tokens": [51398, 264, 497, 7382, 337, 412, 341, 935, 13, 34952, 13, 1057, 558, 13, 407, 437, 2011, 1670, 264, 1036, 700, 565, 30, 1042, 11, 51860], "temperature": 0.0, "avg_logprob": -0.17956233719020215, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.018875727429986}, {"id": 81, "seek": 52488, "start": 525.0, "end": 531.96, "text": " the previous release of the Rust SDK was in October 22. So we made a new release this year. Yay! At the", "tokens": [50370, 264, 3894, 4374, 295, 264, 34952, 37135, 390, 294, 7617, 5853, 13, 407, 321, 1027, 257, 777, 4374, 341, 1064, 13, 13268, 0, 1711, 264, 50718], "temperature": 0.0, "avg_logprob": -0.14893063103280416, "compression_ratio": 1.412037037037037, "no_speech_prob": 0.00474027544260025}, {"id": 82, "seek": 52488, "start": 531.96, "end": 542.4, "text": " beginning of this month. Thank you. So it's still not 1.0, still quite experimental. We're breaking", "tokens": [50718, 2863, 295, 341, 1618, 13, 1044, 291, 13, 407, 309, 311, 920, 406, 502, 13, 15, 11, 920, 1596, 17069, 13, 492, 434, 7697, 51240], "temperature": 0.0, "avg_logprob": -0.14893063103280416, "compression_ratio": 1.412037037037037, "no_speech_prob": 0.00474027544260025}, {"id": 83, "seek": 52488, "start": 542.4, "end": 548.32, "text": " APIs all the time, but trying to do a better job at writing, changing logs and all of that. And we'll", "tokens": [51240, 21445, 439, 264, 565, 11, 457, 1382, 281, 360, 257, 1101, 1691, 412, 3579, 11, 4473, 20820, 293, 439, 295, 300, 13, 400, 321, 603, 51536], "temperature": 0.0, "avg_logprob": -0.14893063103280416, "compression_ratio": 1.412037037037037, "no_speech_prob": 0.00474027544260025}, {"id": 84, "seek": 54832, "start": 548.36, "end": 555.5200000000001, "text": " see how it goes. So new features. So you probably heard about sliding sync last year. And this year,", "tokens": [50366, 536, 577, 309, 1709, 13, 407, 777, 4122, 13, 407, 291, 1391, 2198, 466, 21169, 20271, 1036, 1064, 13, 400, 341, 1064, 11, 50724], "temperature": 0.0, "avg_logprob": -0.15268714453584403, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.04095209017395973}, {"id": 85, "seek": 54832, "start": 555.5600000000001, "end": 562.32, "text": " the new kind of sync synchronization that makes it so that logging into a new device and retrieving", "tokens": [50726, 264, 777, 733, 295, 20271, 19331, 2144, 300, 1669, 309, 370, 300, 27991, 666, 257, 777, 4302, 293, 19817, 798, 51064], "temperature": 0.0, "avg_logprob": -0.15268714453584403, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.04095209017395973}, {"id": 86, "seek": 54832, "start": 562.32, "end": 570.12, "text": " events is always instant, even if you haven't opened the app for months or years. So we entirely", "tokens": [51064, 3931, 307, 1009, 9836, 11, 754, 498, 291, 2378, 380, 5625, 264, 724, 337, 2493, 420, 924, 13, 407, 321, 7696, 51454], "temperature": 0.0, "avg_logprob": -0.15268714453584403, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.04095209017395973}, {"id": 87, "seek": 54832, "start": 570.4000000000001, "end": 577.0400000000001, "text": " support that. There is the basic feature that you can subscribe to specific rooms and list of rooms", "tokens": [51468, 1406, 300, 13, 821, 307, 264, 3875, 4111, 300, 291, 393, 3022, 281, 2685, 9396, 293, 1329, 295, 9396, 51800], "temperature": 0.0, "avg_logprob": -0.15268714453584403, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.04095209017395973}, {"id": 88, "seek": 57704, "start": 577.24, "end": 582.48, "text": " of which we get a sliding window that is computed by the server. But we're getting rid of that, as", "tokens": [50374, 295, 597, 321, 483, 257, 21169, 4910, 300, 307, 40610, 538, 264, 7154, 13, 583, 321, 434, 1242, 3973, 295, 300, 11, 382, 50636], "temperature": 0.0, "avg_logprob": -0.15015029907226562, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.010975923389196396}, {"id": 89, "seek": 57704, "start": 582.52, "end": 588.0799999999999, "text": " Matthew said. And it also implements a modular design in the sense that you have opt-in extensions for", "tokens": [50638, 12434, 848, 13, 400, 309, 611, 704, 17988, 257, 31111, 1715, 294, 264, 2020, 300, 291, 362, 2427, 12, 259, 25129, 337, 50916], "temperature": 0.0, "avg_logprob": -0.15015029907226562, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.010975923389196396}, {"id": 90, "seek": 57704, "start": 588.0799999999999, "end": 593.28, "text": " read receipts and typing notices and many other things. And all of that is supported in the SDK. As you", "tokens": [50916, 1401, 2268, 48908, 293, 18444, 32978, 293, 867, 661, 721, 13, 400, 439, 295, 300, 307, 8104, 294, 264, 37135, 13, 1018, 291, 51176], "temperature": 0.0, "avg_logprob": -0.15015029907226562, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.010975923389196396}, {"id": 91, "seek": 57704, "start": 593.28, "end": 599.88, "text": " can see on the right, it's quite verbose because, well, it's a very versatile and general like API to", "tokens": [51176, 393, 536, 322, 264, 558, 11, 309, 311, 1596, 9595, 541, 570, 11, 731, 11, 309, 311, 257, 588, 25057, 293, 2674, 411, 9362, 281, 51506], "temperature": 0.0, "avg_logprob": -0.15015029907226562, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.010975923389196396}, {"id": 92, "seek": 57704, "start": 599.88, "end": 605.28, "text": " give you the most control so that you can build higher level primitives on top of that. We'll get back", "tokens": [51506, 976, 291, 264, 881, 1969, 370, 300, 291, 393, 1322, 2946, 1496, 2886, 38970, 322, 1192, 295, 300, 13, 492, 603, 483, 646, 51776], "temperature": 0.0, "avg_logprob": -0.15015029907226562, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.010975923389196396}, {"id": 93, "seek": 60528, "start": 605.36, "end": 612.4, "text": " to that. And it's vitrugated behind the experimental sliding sync cargo feature. And you can, we basically", "tokens": [50368, 281, 300, 13, 400, 309, 311, 9467, 81, 697, 770, 2261, 264, 17069, 21169, 20271, 19449, 4111, 13, 400, 291, 393, 11, 321, 1936, 50720], "temperature": 0.0, "avg_logprob": -0.22320463260014853, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004847679752856493}, {"id": 94, "seek": 60528, "start": 612.4, "end": 620.12, "text": " use it in production in element X. So it's quite stable, actually. There's also support for OIDC,", "tokens": [50720, 764, 309, 294, 4265, 294, 4478, 1783, 13, 407, 309, 311, 1596, 8351, 11, 767, 13, 821, 311, 611, 1406, 337, 422, 2777, 34, 11, 51106], "temperature": 0.0, "avg_logprob": -0.22320463260014853, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004847679752856493}, {"id": 95, "seek": 60528, "start": 620.12, "end": 625.92, "text": " so OpenID Connect. It's a cross stack effort moving from the custom metrics authentication to", "tokens": [51106, 370, 7238, 2777, 11653, 13, 467, 311, 257, 3278, 8630, 4630, 2684, 490, 264, 2375, 16367, 26643, 281, 51396], "temperature": 0.0, "avg_logprob": -0.22320463260014853, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004847679752856493}, {"id": 96, "seek": 60528, "start": 625.92, "end": 632.9599999999999, "text": " OpenID Connect. If you have a metrics authentication service running, so it's another service running", "tokens": [51396, 7238, 2777, 11653, 13, 759, 291, 362, 257, 16367, 26643, 2643, 2614, 11, 370, 309, 311, 1071, 2643, 2614, 51748], "temperature": 0.0, "avg_logprob": -0.22320463260014853, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004847679752856493}, {"id": 97, "seek": 63296, "start": 632.96, "end": 640.44, "text": " on your server alongside the Synapse or your own server, it can act as an actual OIDC provider or", "tokens": [50364, 322, 428, 7154, 12385, 264, 26155, 11145, 420, 428, 1065, 7154, 11, 309, 393, 605, 382, 364, 3539, 422, 2777, 34, 12398, 420, 50738], "temperature": 0.0, "avg_logprob": -0.1747082694102142, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.004258979111909866}, {"id": 98, "seek": 63296, "start": 640.44, "end": 644.5600000000001, "text": " specialized proxy to an upstream provider. So if you have a GitLab instance, for instance, you can", "tokens": [50738, 19813, 29690, 281, 364, 33915, 12398, 13, 407, 498, 291, 362, 257, 16939, 37880, 5197, 11, 337, 5197, 11, 291, 393, 50944], "temperature": 0.0, "avg_logprob": -0.1747082694102142, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.004258979111909866}, {"id": 99, "seek": 63296, "start": 645.0, "end": 650.9200000000001, "text": " connect it to the metrics authentication system and then have your GitLab users log into matrix for", "tokens": [50966, 1745, 309, 281, 264, 16367, 26643, 1185, 293, 550, 362, 428, 16939, 37880, 5022, 3565, 666, 8141, 337, 51262], "temperature": 0.0, "avg_logprob": -0.1747082694102142, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.004258979111909866}, {"id": 100, "seek": 63296, "start": 650.9200000000001, "end": 655.9200000000001, "text": " free, like that. And so that's the server side part. It's also written in REST, which is pretty", "tokens": [51262, 1737, 11, 411, 300, 13, 400, 370, 300, 311, 264, 7154, 1252, 644, 13, 467, 311, 611, 3720, 294, 497, 14497, 11, 597, 307, 1238, 51512], "temperature": 0.0, "avg_logprob": -0.1747082694102142, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.004258979111909866}, {"id": 101, "seek": 63296, "start": 655.9200000000001, "end": 661.72, "text": " sweet because that means that the request and responses can be actually reused in the client, the", "tokens": [51512, 3844, 570, 300, 1355, 300, 264, 5308, 293, 13019, 393, 312, 767, 319, 4717, 294, 264, 6423, 11, 264, 51802], "temperature": 0.0, "avg_logprob": -0.1747082694102142, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.004258979111909866}, {"id": 102, "seek": 66172, "start": 662.48, "end": 670.36, "text": " REST metrics SDK. And the SDK implements all of that already. And we are also using it in production", "tokens": [50402, 497, 14497, 16367, 37135, 13, 400, 264, 37135, 704, 17988, 439, 295, 300, 1217, 13, 400, 321, 366, 611, 1228, 309, 294, 4265, 50796], "temperature": 0.0, "avg_logprob": -0.1495754605247861, "compression_ratio": 1.5887096774193548, "no_speech_prob": 0.0013753013918176293}, {"id": 103, "seek": 66172, "start": 670.36, "end": 677.48, "text": " in element X. So it gives you all the things that you would like to do with OIDC, create, reload,", "tokens": [50796, 294, 4478, 1783, 13, 407, 309, 2709, 291, 439, 264, 721, 300, 291, 576, 411, 281, 360, 365, 422, 2777, 34, 11, 1884, 11, 25628, 11, 51152], "temperature": 0.0, "avg_logprob": -0.1495754605247861, "compression_ratio": 1.5887096774193548, "no_speech_prob": 0.0013753013918176293}, {"id": 104, "seek": 66172, "start": 677.48, "end": 683.1600000000001, "text": " metadata, register on your OIDC client, do the login flow in all the steps and all of that. And", "tokens": [51152, 26603, 11, 7280, 322, 428, 422, 2777, 34, 6423, 11, 360, 264, 24276, 3095, 294, 439, 264, 4439, 293, 439, 295, 300, 13, 400, 51436], "temperature": 0.0, "avg_logprob": -0.1495754605247861, "compression_ratio": 1.5887096774193548, "no_speech_prob": 0.0013753013918176293}, {"id": 105, "seek": 66172, "start": 683.1600000000001, "end": 690.84, "text": " it's also behind the cargo feature at this point. Among the big news, we have a new default storage", "tokens": [51436, 309, 311, 611, 2261, 264, 19449, 4111, 412, 341, 935, 13, 16119, 264, 955, 2583, 11, 321, 362, 257, 777, 7576, 6725, 51820], "temperature": 0.0, "avg_logprob": -0.1495754605247861, "compression_ratio": 1.5887096774193548, "no_speech_prob": 0.0013753013918176293}, {"id": 106, "seek": 69084, "start": 690.88, "end": 698.24, "text": " backend. So the storage backend are implemented using traits, which are REST for interfaces. The", "tokens": [50366, 38087, 13, 407, 264, 6725, 38087, 366, 12270, 1228, 19526, 11, 597, 366, 497, 14497, 337, 28416, 13, 440, 50734], "temperature": 0.0, "avg_logprob": -0.19706014407578334, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.013000287115573883}, {"id": 107, "seek": 69084, "start": 698.24, "end": 704.72, "text": " previous defaults when you wanted to persist things on disk was sled. And now it's been replaced to", "tokens": [50734, 3894, 7576, 82, 562, 291, 1415, 281, 13233, 721, 322, 12355, 390, 46242, 13, 400, 586, 309, 311, 668, 10772, 281, 51058], "temperature": 0.0, "avg_logprob": -0.19706014407578334, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.013000287115573883}, {"id": 108, "seek": 69084, "start": 704.72, "end": 712.6800000000001, "text": " SQLite because, well, pretty much everybody knows about SQL. And it's also much faster for our", "tokens": [51058, 19200, 642, 570, 11, 731, 11, 1238, 709, 2201, 3255, 466, 19200, 13, 400, 309, 311, 611, 709, 4663, 337, 527, 51456], "temperature": 0.0, "avg_logprob": -0.19706014407578334, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.013000287115573883}, {"id": 109, "seek": 69084, "start": 712.6800000000001, "end": 718.52, "text": " use case. We still have an in memory backend if you don't care about losing states and an index DB", "tokens": [51456, 764, 1389, 13, 492, 920, 362, 364, 294, 4675, 38087, 498, 291, 500, 380, 1127, 466, 7027, 4368, 293, 364, 8186, 26754, 51748], "temperature": 0.0, "avg_logprob": -0.19706014407578334, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.013000287115573883}, {"id": 110, "seek": 71852, "start": 718.64, "end": 727.12, "text": " backend that is used when you're compiling for the web to WebAssembly. Some new cryptography", "tokens": [50370, 38087, 300, 307, 1143, 562, 291, 434, 715, 4883, 337, 264, 3670, 281, 9573, 10884, 19160, 13, 2188, 777, 9844, 5820, 50794], "temperature": 0.0, "avg_logprob": -0.21389421686395868, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.0028036145959049463}, {"id": 111, "seek": 71852, "start": 727.12, "end": 731.96, "text": " features. So there is this new thing called secret storage. And it's mostly an implementation", "tokens": [50794, 4122, 13, 407, 456, 307, 341, 777, 551, 1219, 4054, 6725, 13, 400, 309, 311, 5240, 364, 11420, 51036], "temperature": 0.0, "avg_logprob": -0.21389421686395868, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.0028036145959049463}, {"id": 112, "seek": 71852, "start": 731.96, "end": 738.6, "text": " detail, but it gives you an encrypted key valley store that is backed in the user account data. And", "tokens": [51036, 2607, 11, 457, 309, 2709, 291, 364, 36663, 2141, 17636, 3531, 300, 307, 20391, 294, 264, 4195, 2696, 1412, 13, 400, 51368], "temperature": 0.0, "avg_logprob": -0.21389421686395868, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.0028036145959049463}, {"id": 113, "seek": 71852, "start": 738.6, "end": 744.3199999999999, "text": " where you can put any information that you would like to share across all your devices in a secure", "tokens": [51368, 689, 291, 393, 829, 604, 1589, 300, 291, 576, 411, 281, 2073, 2108, 439, 428, 5759, 294, 257, 7144, 51654], "temperature": 0.0, "avg_logprob": -0.21389421686395868, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.0028036145959049463}, {"id": 114, "seek": 71852, "start": 744.3199999999999, "end": 748.4399999999999, "text": " way. Like the server doesn't know about this information. It cannot peek into it and know what", "tokens": [51654, 636, 13, 1743, 264, 7154, 1177, 380, 458, 466, 341, 1589, 13, 467, 2644, 19604, 666, 309, 293, 458, 437, 51860], "temperature": 0.0, "avg_logprob": -0.21389421686395868, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.0028036145959049463}, {"id": 115, "seek": 74844, "start": 748.8800000000001, "end": 755.7600000000001, "text": " is in there because it's also encrypted. On top of that, we implemented key backup and restoration. So", "tokens": [50386, 307, 294, 456, 570, 309, 311, 611, 36663, 13, 1282, 1192, 295, 300, 11, 321, 12270, 2141, 14807, 293, 23722, 13, 407, 50730], "temperature": 0.0, "avg_logprob": -0.1879195346627184, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0020994956139475107}, {"id": 116, "seek": 74844, "start": 755.7600000000001, "end": 761.6400000000001, "text": " that means that when you have a new device, well, when you're using elementics, for instance, it", "tokens": [50730, 300, 1355, 300, 562, 291, 362, 257, 777, 4302, 11, 731, 11, 562, 291, 434, 1228, 4478, 1167, 11, 337, 5197, 11, 309, 51024], "temperature": 0.0, "avg_logprob": -0.1879195346627184, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0020994956139475107}, {"id": 117, "seek": 74844, "start": 761.6400000000001, "end": 769.4000000000001, "text": " will store all the room keys that are used for decrypting room messages in encrypted rooms in the", "tokens": [51024, 486, 3531, 439, 264, 1808, 9317, 300, 366, 1143, 337, 979, 627, 662, 278, 1808, 7897, 294, 36663, 9396, 294, 264, 51412], "temperature": 0.0, "avg_logprob": -0.1879195346627184, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0020994956139475107}, {"id": 118, "seek": 74844, "start": 769.4000000000001, "end": 774.32, "text": " secret storage. And then another device can restore them so that you can actually see the", "tokens": [51412, 4054, 6725, 13, 400, 550, 1071, 4302, 393, 15227, 552, 370, 300, 291, 393, 767, 536, 264, 51658], "temperature": 0.0, "avg_logprob": -0.1879195346627184, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0020994956139475107}, {"id": 119, "seek": 77432, "start": 774.4000000000001, "end": 780.6400000000001, "text": " history of events before you joined with that new device. Also, in addition to that, we made it so", "tokens": [50368, 2503, 295, 3931, 949, 291, 6869, 365, 300, 777, 4302, 13, 2743, 11, 294, 4500, 281, 300, 11, 321, 1027, 309, 370, 50680], "temperature": 0.0, "avg_logprob": -0.11587377921822145, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.004023783374577761}, {"id": 120, "seek": 77432, "start": 780.6400000000001, "end": 787.36, "text": " that the cross-signing automatically happens and you don't have to worry about this at all. That's", "tokens": [50680, 300, 264, 3278, 12, 82, 9676, 6772, 2314, 293, 291, 500, 380, 362, 281, 3292, 466, 341, 412, 439, 13, 663, 311, 51016], "temperature": 0.0, "avg_logprob": -0.11587377921822145, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.004023783374577761}, {"id": 121, "seek": 77432, "start": 787.36, "end": 794.08, "text": " what's used to verify your own devices and other people's devices. And it's also like some of the", "tokens": [51016, 437, 311, 1143, 281, 16888, 428, 1065, 5759, 293, 661, 561, 311, 5759, 13, 400, 309, 311, 611, 411, 512, 295, 264, 51352], "temperature": 0.0, "avg_logprob": -0.11587377921822145, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.004023783374577761}, {"id": 122, "seek": 77432, "start": 794.08, "end": 803.5200000000001, "text": " private keys are stored in that secret storage as well. And speaking of high-level primitives, so", "tokens": [51352, 4551, 9317, 366, 12187, 294, 300, 4054, 6725, 382, 731, 13, 400, 4124, 295, 1090, 12, 12418, 2886, 38970, 11, 370, 51824], "temperature": 0.0, "avg_logprob": -0.11587377921822145, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.004023783374577761}, {"id": 123, "seek": 80352, "start": 803.56, "end": 810.6, "text": " we made a new crate, new package called the Matrix SDK UI. It is highly experimental and also highly", "tokens": [50366, 321, 1027, 257, 777, 42426, 11, 777, 7372, 1219, 264, 36274, 37135, 15682, 13, 467, 307, 5405, 17069, 293, 611, 5405, 50718], "temperature": 0.0, "avg_logprob": -0.1786507545633519, "compression_ratio": 1.584, "no_speech_prob": 0.005117957480251789}, {"id": 124, "seek": 80352, "start": 810.6, "end": 817.04, "text": " opinionated in the sense that we're enabling a few cargo features by default. And we are trying to", "tokens": [50718, 4800, 770, 294, 264, 2020, 300, 321, 434, 23148, 257, 1326, 19449, 4122, 538, 7576, 13, 400, 321, 366, 1382, 281, 51040], "temperature": 0.0, "avg_logprob": -0.1786507545633519, "compression_ratio": 1.584, "no_speech_prob": 0.005117957480251789}, {"id": 125, "seek": 80352, "start": 817.04, "end": 823.68, "text": " make it so that we implement the best practices in terms of user experience and performance. And", "tokens": [51040, 652, 309, 370, 300, 321, 4445, 264, 1151, 7525, 294, 2115, 295, 4195, 1752, 293, 3389, 13, 400, 51372], "temperature": 0.0, "avg_logprob": -0.1786507545633519, "compression_ratio": 1.584, "no_speech_prob": 0.005117957480251789}, {"id": 126, "seek": 80352, "start": 823.68, "end": 828.6, "text": " it's also as robust and tested as the rest of the SDK, which is very sweet. And we use sliding sync", "tokens": [51372, 309, 311, 611, 382, 13956, 293, 8246, 382, 264, 1472, 295, 264, 37135, 11, 597, 307, 588, 3844, 13, 400, 321, 764, 21169, 20271, 51618], "temperature": 0.0, "avg_logprob": -0.1786507545633519, "compression_ratio": 1.584, "no_speech_prob": 0.005117957480251789}, {"id": 127, "seek": 82860, "start": 828.64, "end": 834.44, "text": " as a foundation for all these new high-level features. One of these features is the room list", "tokens": [50366, 382, 257, 7030, 337, 439, 613, 777, 1090, 12, 12418, 4122, 13, 1485, 295, 613, 4122, 307, 264, 1808, 1329, 50656], "temperature": 0.0, "avg_logprob": -0.12819831206066773, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.027720872312784195}, {"id": 128, "seek": 82860, "start": 834.44, "end": 841.12, "text": " service, which, as its name suggests, gives you a list of the rooms. Yes, it does it so in a way", "tokens": [50656, 2643, 11, 597, 11, 382, 1080, 1315, 13409, 11, 2709, 291, 257, 1329, 295, 264, 9396, 13, 1079, 11, 309, 775, 309, 370, 294, 257, 636, 50990], "temperature": 0.0, "avg_logprob": -0.12819831206066773, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.027720872312784195}, {"id": 129, "seek": 82860, "start": 841.12, "end": 849.88, "text": " that we try to make it to show something to the user as soon as possible. So that's how you feel", "tokens": [50990, 300, 321, 853, 281, 652, 309, 281, 855, 746, 281, 264, 4195, 382, 2321, 382, 1944, 13, 407, 300, 311, 577, 291, 841, 51428], "temperature": 0.0, "avg_logprob": -0.12819831206066773, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.027720872312784195}, {"id": 130, "seek": 82860, "start": 849.88, "end": 856.8000000000001, "text": " that the app is kind of instant when you open the app, because it will try to load just one event", "tokens": [51428, 300, 264, 724, 307, 733, 295, 9836, 562, 291, 1269, 264, 724, 11, 570, 309, 486, 853, 281, 3677, 445, 472, 2280, 51774], "temperature": 0.0, "avg_logprob": -0.12819831206066773, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.027720872312784195}, {"id": 131, "seek": 85680, "start": 856.88, "end": 862.4, "text": " for all the rooms you were in, or for no, a few rooms you were in. So you have something to display.", "tokens": [50368, 337, 439, 264, 9396, 291, 645, 294, 11, 420, 337, 572, 11, 257, 1326, 9396, 291, 645, 294, 13, 407, 291, 362, 746, 281, 4674, 13, 50644], "temperature": 0.0, "avg_logprob": -0.15362664156181868, "compression_ratio": 1.8161764705882353, "no_speech_prob": 0.014416398480534554}, {"id": 132, "seek": 85680, "start": 862.4, "end": 869.28, "text": " And then in the background, once that's done, it will try to fetch more events. And also you can", "tokens": [50644, 400, 550, 294, 264, 3678, 11, 1564, 300, 311, 1096, 11, 309, 486, 853, 281, 23673, 544, 3931, 13, 400, 611, 291, 393, 50988], "temperature": 0.0, "avg_logprob": -0.15362664156181868, "compression_ratio": 1.8161764705882353, "no_speech_prob": 0.014416398480534554}, {"id": 133, "seek": 85680, "start": 869.28, "end": 874.28, "text": " configure it to say, this is a set of visible rooms in my apps. So because when you have an app,", "tokens": [50988, 22162, 309, 281, 584, 11, 341, 307, 257, 992, 295, 8974, 9396, 294, 452, 7733, 13, 407, 570, 562, 291, 362, 364, 724, 11, 51238], "temperature": 0.0, "avg_logprob": -0.15362664156181868, "compression_ratio": 1.8161764705882353, "no_speech_prob": 0.014416398480534554}, {"id": 134, "seek": 85680, "start": 874.28, "end": 880.0799999999999, "text": " you cannot show like a thousand rooms, you will only show a subset, right? So you can configure it", "tokens": [51238, 291, 2644, 855, 411, 257, 4714, 9396, 11, 291, 486, 787, 855, 257, 25993, 11, 558, 30, 407, 291, 393, 22162, 309, 51528], "temperature": 0.0, "avg_logprob": -0.15362664156181868, "compression_ratio": 1.8161764705882353, "no_speech_prob": 0.014416398480534554}, {"id": 135, "seek": 85680, "start": 880.0799999999999, "end": 886.1999999999999, "text": " to say, this is the ones that are actually rendered on the screen. And those are prioritized so that", "tokens": [51528, 281, 584, 11, 341, 307, 264, 2306, 300, 366, 767, 28748, 322, 264, 2568, 13, 400, 729, 366, 14846, 1602, 370, 300, 51834], "temperature": 0.0, "avg_logprob": -0.15362664156181868, "compression_ratio": 1.8161764705882353, "no_speech_prob": 0.014416398480534554}, {"id": 136, "seek": 88620, "start": 886.24, "end": 894.9200000000001, "text": " you get more events for these rooms. Another thing we added was the encryption service. So it's", "tokens": [50366, 291, 483, 544, 3931, 337, 613, 9396, 13, 3996, 551, 321, 3869, 390, 264, 29575, 2643, 13, 407, 309, 311, 50800], "temperature": 0.0, "avg_logprob": -0.12137524420473755, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0032180773559957743}, {"id": 137, "seek": 88620, "start": 894.9200000000001, "end": 899.12, "text": " basically a sliding thing that is just running encryption on the side, and it gives you access", "tokens": [50800, 1936, 257, 21169, 551, 300, 307, 445, 2614, 29575, 322, 264, 1252, 11, 293, 309, 2709, 291, 2105, 51010], "temperature": 0.0, "avg_logprob": -0.12137524420473755, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0032180773559957743}, {"id": 138, "seek": 88620, "start": 899.12, "end": 904.32, "text": " to more concurrency with the other one. So think of it this way, the room list service, the one I", "tokens": [51010, 281, 544, 23702, 10457, 365, 264, 661, 472, 13, 407, 519, 295, 309, 341, 636, 11, 264, 1808, 1329, 2643, 11, 264, 472, 286, 51270], "temperature": 0.0, "avg_logprob": -0.12137524420473755, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0032180773559957743}, {"id": 139, "seek": 88620, "start": 904.32, "end": 909.2, "text": " just talked about, when you're scrolling on a mobile app, it will change the list of rooms that are", "tokens": [51270, 445, 2825, 466, 11, 562, 291, 434, 29053, 322, 257, 6013, 724, 11, 309, 486, 1319, 264, 1329, 295, 9396, 300, 366, 51514], "temperature": 0.0, "avg_logprob": -0.12137524420473755, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0032180773559957743}, {"id": 140, "seek": 88620, "start": 909.2, "end": 914.6400000000001, "text": " shown on the screen, right? So that now means that it's sending new requests to ask for things. And", "tokens": [51514, 4898, 322, 264, 2568, 11, 558, 30, 407, 300, 586, 1355, 300, 309, 311, 7750, 777, 12475, 281, 1029, 337, 721, 13, 400, 51786], "temperature": 0.0, "avg_logprob": -0.12137524420473755, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0032180773559957743}, {"id": 141, "seek": 91464, "start": 915.24, "end": 920.0, "text": " if we did the encryption in the same request, but it's getting a bit technical, but that would", "tokens": [50394, 498, 321, 630, 264, 29575, 294, 264, 912, 5308, 11, 457, 309, 311, 1242, 257, 857, 6191, 11, 457, 300, 576, 50632], "temperature": 0.0, "avg_logprob": -0.11730874259516878, "compression_ratio": 1.8505747126436782, "no_speech_prob": 0.004379352554678917}, {"id": 142, "seek": 91464, "start": 920.0, "end": 925.24, "text": " mean that we would need to abort those requests and delay encryption. So now we have basically more", "tokens": [50632, 914, 300, 321, 576, 643, 281, 38117, 729, 12475, 293, 8577, 29575, 13, 407, 586, 321, 362, 1936, 544, 50894], "temperature": 0.0, "avg_logprob": -0.11730874259516878, "compression_ratio": 1.8505747126436782, "no_speech_prob": 0.004379352554678917}, {"id": 143, "seek": 91464, "start": 925.24, "end": 931.72, "text": " concurrency and more performance. And we can do the encryption task in the background while you're", "tokens": [50894, 23702, 10457, 293, 544, 3389, 13, 400, 321, 393, 360, 264, 29575, 5633, 294, 264, 3678, 1339, 291, 434, 51218], "temperature": 0.0, "avg_logprob": -0.11730874259516878, "compression_ratio": 1.8505747126436782, "no_speech_prob": 0.004379352554678917}, {"id": 144, "seek": 91464, "start": 931.72, "end": 937.52, "text": " still scrolling on the room list using this encryption service. And we also have a notification", "tokens": [51218, 920, 29053, 322, 264, 1808, 1329, 1228, 341, 29575, 2643, 13, 400, 321, 611, 362, 257, 11554, 51508], "temperature": 0.0, "avg_logprob": -0.11730874259516878, "compression_ratio": 1.8505747126436782, "no_speech_prob": 0.004379352554678917}, {"id": 145, "seek": 91464, "start": 937.52, "end": 943.72, "text": " service. So that's very specialized client that just handles push notifications. So if you're", "tokens": [51508, 2643, 13, 407, 300, 311, 588, 19813, 6423, 300, 445, 18722, 2944, 13426, 13, 407, 498, 291, 434, 51818], "temperature": 0.0, "avg_logprob": -0.11730874259516878, "compression_ratio": 1.8505747126436782, "no_speech_prob": 0.004379352554678917}, {"id": 146, "seek": 94372, "start": 943.76, "end": 948.72, "text": " given an event and a room identifier, we want to retrieve the event and maybe a bit of context,", "tokens": [50366, 2212, 364, 2280, 293, 257, 1808, 45690, 11, 321, 528, 281, 30254, 264, 2280, 293, 1310, 257, 857, 295, 4319, 11, 50614], "temperature": 0.0, "avg_logprob": -0.15391021079205452, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.008066261187195778}, {"id": 147, "seek": 94372, "start": 948.72, "end": 957.32, "text": " like what's your name, what's the name of the person who sent the message to you and all of that.", "tokens": [50614, 411, 437, 311, 428, 1315, 11, 437, 311, 264, 1315, 295, 264, 954, 567, 2279, 264, 3636, 281, 291, 293, 439, 295, 300, 13, 51044], "temperature": 0.0, "avg_logprob": -0.15391021079205452, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.008066261187195778}, {"id": 148, "seek": 94372, "start": 957.32, "end": 964.1600000000001, "text": " It's also using a sliding thing for that. And it makes use of the encryption service because on", "tokens": [51044, 467, 311, 611, 1228, 257, 21169, 551, 337, 300, 13, 400, 309, 1669, 764, 295, 264, 29575, 2643, 570, 322, 51386], "temperature": 0.0, "avg_logprob": -0.15391021079205452, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.008066261187195778}, {"id": 149, "seek": 94372, "start": 964.1600000000001, "end": 969.0400000000001, "text": " an encrypted room, of course, you would get a push notification for an encrypted event and the", "tokens": [51386, 364, 36663, 1808, 11, 295, 1164, 11, 291, 576, 483, 257, 2944, 11554, 337, 364, 36663, 2280, 293, 264, 51630], "temperature": 0.0, "avg_logprob": -0.15391021079205452, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.008066261187195778}, {"id": 150, "seek": 96904, "start": 969.0799999999999, "end": 974.64, "text": " server cannot know if it's a meaningful event, right? Maybe it's just a reaction, putting a", "tokens": [50366, 7154, 2644, 458, 498, 309, 311, 257, 10995, 2280, 11, 558, 30, 2704, 309, 311, 445, 257, 5480, 11, 3372, 257, 50644], "temperature": 0.0, "avg_logprob": -0.13491205825019129, "compression_ratio": 1.625, "no_speech_prob": 0.007529101800173521}, {"id": 151, "seek": 96904, "start": 974.64, "end": 981.4399999999999, "text": " thumbs up on one of your messages. So we decrypt the event in the client itself and then we decide", "tokens": [50644, 8838, 493, 322, 472, 295, 428, 7897, 13, 407, 321, 979, 627, 662, 264, 2280, 294, 264, 6423, 2564, 293, 550, 321, 4536, 50984], "temperature": 0.0, "avg_logprob": -0.13491205825019129, "compression_ratio": 1.625, "no_speech_prob": 0.007529101800173521}, {"id": 152, "seek": 96904, "start": 981.4399999999999, "end": 990.56, "text": " whether it's worth sharing as a notification. The one fun thing, if you can call it fun, is on", "tokens": [50984, 1968, 309, 311, 3163, 5414, 382, 257, 11554, 13, 440, 472, 1019, 551, 11, 498, 291, 393, 818, 309, 1019, 11, 307, 322, 51440], "temperature": 0.0, "avg_logprob": -0.13491205825019129, "compression_ratio": 1.625, "no_speech_prob": 0.007529101800173521}, {"id": 153, "seek": 96904, "start": 990.56, "end": 998.1999999999999, "text": " iOS, if you want to modify the notification in case it's encrypted, it's running a separate", "tokens": [51440, 17430, 11, 498, 291, 528, 281, 16927, 264, 11554, 294, 1389, 309, 311, 36663, 11, 309, 311, 2614, 257, 4994, 51822], "temperature": 0.0, "avg_logprob": -0.13491205825019129, "compression_ratio": 1.625, "no_speech_prob": 0.007529101800173521}, {"id": 154, "seek": 99820, "start": 998.2800000000001, "end": 1004.9200000000001, "text": " process. And that makes our life very hard because even if you're just decrypting data, the state of", "tokens": [50368, 1399, 13, 400, 300, 1669, 527, 993, 588, 1152, 570, 754, 498, 291, 434, 445, 979, 627, 662, 278, 1412, 11, 264, 1785, 295, 50700], "temperature": 0.0, "avg_logprob": -0.16874569718555738, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.028027815744280815}, {"id": 155, "seek": 99820, "start": 1004.9200000000001, "end": 1012.5200000000001, "text": " the cryptography keys is mutably changed, right? So now we have multiple states that is global", "tokens": [50700, 264, 9844, 5820, 9317, 307, 5839, 1188, 3105, 11, 558, 30, 407, 586, 321, 362, 3866, 4368, 300, 307, 4338, 51080], "temperature": 0.0, "avg_logprob": -0.16874569718555738, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.028027815744280815}, {"id": 156, "seek": 99820, "start": 1012.5200000000001, "end": 1018.48, "text": " across two processes that are sharing the same database. So we had to be a bit creative to solve", "tokens": [51080, 2108, 732, 7555, 300, 366, 5414, 264, 912, 8149, 13, 407, 321, 632, 281, 312, 257, 857, 5880, 281, 5039, 51378], "temperature": 0.0, "avg_logprob": -0.16874569718555738, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.028027815744280815}, {"id": 157, "seek": 99820, "start": 1018.48, "end": 1024.3600000000001, "text": " that issue and we are basically enabling the writer head log in SQLite and using some data in", "tokens": [51378, 300, 2734, 293, 321, 366, 1936, 23148, 264, 9936, 1378, 3565, 294, 19200, 642, 293, 1228, 512, 1412, 294, 51672], "temperature": 0.0, "avg_logprob": -0.16874569718555738, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.028027815744280815}, {"id": 158, "seek": 102436, "start": 1024.3999999999999, "end": 1030.9599999999998, "text": " the database to indicate who's the process that currently tries to read and write to the database.", "tokens": [50366, 264, 8149, 281, 13330, 567, 311, 264, 1399, 300, 4362, 9898, 281, 1401, 293, 2464, 281, 264, 8149, 13, 50694], "temperature": 0.0, "avg_logprob": -0.20103705911075367, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.007262409199029207}, {"id": 159, "seek": 102436, "start": 1030.9599999999998, "end": 1039.6, "text": " So basically implementing a text like that. All right. And since we added those two services,", "tokens": [50694, 407, 1936, 18114, 257, 2487, 411, 300, 13, 1057, 558, 13, 400, 1670, 321, 3869, 729, 732, 3328, 11, 51126], "temperature": 0.0, "avg_logprob": -0.20103705911075367, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.007262409199029207}, {"id": 160, "seek": 102436, "start": 1039.6, "end": 1044.28, "text": " the encryption service and the room service, we wanted to make it very simple to just fire", "tokens": [51126, 264, 29575, 2643, 293, 264, 1808, 2643, 11, 321, 1415, 281, 652, 309, 588, 2199, 281, 445, 2610, 51360], "temperature": 0.0, "avg_logprob": -0.20103705911075367, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.007262409199029207}, {"id": 161, "seek": 102436, "start": 1044.28, "end": 1050.84, "text": " synchronization and forget about it. So we made some nice high-level service that just wraps", "tokens": [51360, 19331, 2144, 293, 2870, 466, 309, 13, 407, 321, 1027, 512, 1481, 1090, 12, 12418, 2643, 300, 445, 25831, 51688], "temperature": 0.0, "avg_logprob": -0.20103705911075367, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.007262409199029207}, {"id": 162, "seek": 105084, "start": 1051.4399999999998, "end": 1057.76, "text": " the other two and you can just build it and start it and it will do all those things for you and", "tokens": [50394, 264, 661, 732, 293, 291, 393, 445, 1322, 309, 293, 722, 309, 293, 309, 486, 360, 439, 729, 721, 337, 291, 293, 50710], "temperature": 0.0, "avg_logprob": -0.13011778394381204, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.011208470910787582}, {"id": 163, "seek": 105084, "start": 1057.76, "end": 1061.8799999999999, "text": " implement all the best practices and you don't have to worry about any of this. And then you can", "tokens": [50710, 4445, 439, 264, 1151, 7525, 293, 291, 500, 380, 362, 281, 3292, 466, 604, 295, 341, 13, 400, 550, 291, 393, 50916], "temperature": 0.0, "avg_logprob": -0.13011778394381204, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.011208470910787582}, {"id": 164, "seek": 105084, "start": 1061.8799999999999, "end": 1067.6, "text": " just take listeners on that service and get information that is meaningful to do when we're", "tokens": [50916, 445, 747, 23274, 322, 300, 2643, 293, 483, 1589, 300, 307, 10995, 281, 360, 562, 321, 434, 51202], "temperature": 0.0, "avg_logprob": -0.13011778394381204, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.011208470910787582}, {"id": 165, "seek": 105084, "start": 1067.6, "end": 1075.56, "text": " rendering for a client. Now that we have a list of rooms and decrypted events, what do we do? Well,", "tokens": [51202, 22407, 337, 257, 6423, 13, 823, 300, 321, 362, 257, 1329, 295, 9396, 293, 979, 627, 25383, 3931, 11, 437, 360, 321, 360, 30, 1042, 11, 51600], "temperature": 0.0, "avg_logprob": -0.13011778394381204, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.011208470910787582}, {"id": 166, "seek": 107556, "start": 1075.6, "end": 1080.8799999999999, "text": " we want to display them and we have an API for that called the Timeline API. It's basically a", "tokens": [50366, 321, 528, 281, 4674, 552, 293, 321, 362, 364, 9362, 337, 300, 1219, 264, 7172, 5440, 9362, 13, 467, 311, 1936, 257, 50630], "temperature": 0.0, "avg_logprob": -0.1958256771689967, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.011542215012013912}, {"id": 167, "seek": 107556, "start": 1080.8799999999999, "end": 1086.9199999999998, "text": " room view MVC, so model view components on steroids. The thing is that in the matrix protocol,", "tokens": [50630, 1808, 1910, 17663, 34, 11, 370, 2316, 1910, 6677, 322, 45717, 13, 440, 551, 307, 300, 294, 264, 8141, 10336, 11, 50932], "temperature": 0.0, "avg_logprob": -0.1958256771689967, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.011542215012013912}, {"id": 168, "seek": 107556, "start": 1086.9199999999998, "end": 1093.2, "text": " events are actually like atomic. It's an app and only database. So let's say you have a thumbs up", "tokens": [50932, 3931, 366, 767, 411, 22275, 13, 467, 311, 364, 724, 293, 787, 8149, 13, 407, 718, 311, 584, 291, 362, 257, 8838, 493, 51246], "temperature": 0.0, "avg_logprob": -0.1958256771689967, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.011542215012013912}, {"id": 169, "seek": 107556, "start": 1093.2, "end": 1100.24, "text": " reaction to a message that is a response to something else that would be two events, like the", "tokens": [51246, 5480, 281, 257, 3636, 300, 307, 257, 4134, 281, 746, 1646, 300, 576, 312, 732, 3931, 11, 411, 264, 51598], "temperature": 0.0, "avg_logprob": -0.1958256771689967, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.011542215012013912}, {"id": 170, "seek": 110024, "start": 1100.28, "end": 1105.28, "text": " reaction itself and the message itself. So the timeline will aggregate all those different", "tokens": [50366, 5480, 2564, 293, 264, 3636, 2564, 13, 407, 264, 12933, 486, 26118, 439, 729, 819, 50616], "temperature": 0.0, "avg_logprob": -0.15858281742442737, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.03213324397802353}, {"id": 171, "seek": 110024, "start": 1105.28, "end": 1113.04, "text": " events into a single timeline item that is much more what you want to render as a client on the", "tokens": [50616, 3931, 666, 257, 2167, 12933, 3174, 300, 307, 709, 544, 437, 291, 528, 281, 15529, 382, 257, 6423, 322, 264, 51004], "temperature": 0.0, "avg_logprob": -0.15858281742442737, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.03213324397802353}, {"id": 172, "seek": 110024, "start": 1113.04, "end": 1118.92, "text": " screen. So it makes it much simpler to render a single timeline like that. And it does a lot of", "tokens": [51004, 2568, 13, 407, 309, 1669, 309, 709, 18587, 281, 15529, 257, 2167, 12933, 411, 300, 13, 400, 309, 775, 257, 688, 295, 51298], "temperature": 0.0, "avg_logprob": -0.15858281742442737, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.03213324397802353}, {"id": 173, "seek": 110024, "start": 1118.92, "end": 1124.1200000000001, "text": " things for you too. It can enter local echoes. So basically when you're sending a message to a", "tokens": [51298, 721, 337, 291, 886, 13, 467, 393, 3242, 2654, 47051, 13, 407, 1936, 562, 291, 434, 7750, 257, 3636, 281, 257, 51558], "temperature": 0.0, "avg_logprob": -0.15858281742442737, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.03213324397802353}, {"id": 174, "seek": 112412, "start": 1124.1599999999999, "end": 1132.4399999999998, "text": " room, you want to show it even before the server has returned that it received it. So it will do", "tokens": [50366, 1808, 11, 291, 528, 281, 855, 309, 754, 949, 264, 7154, 575, 8752, 300, 309, 4613, 309, 13, 407, 309, 486, 360, 50780], "temperature": 0.0, "avg_logprob": -0.17104734693254744, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.01563308946788311}, {"id": 175, "seek": 112412, "start": 1132.4399999999998, "end": 1137.28, "text": " that and then reconcile the response from the server with the local state and all of that. So", "tokens": [50780, 300, 293, 550, 41059, 264, 4134, 490, 264, 7154, 365, 264, 2654, 1785, 293, 439, 295, 300, 13, 407, 51022], "temperature": 0.0, "avg_logprob": -0.17104734693254744, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.01563308946788311}, {"id": 176, "seek": 112412, "start": 1137.28, "end": 1143.0, "text": " it's pretty sweet. And it's all observable, very reactive. So that's nice. You just get, like,", "tokens": [51022, 309, 311, 1238, 3844, 13, 400, 309, 311, 439, 9951, 712, 11, 588, 28897, 13, 407, 300, 311, 1481, 13, 509, 445, 483, 11, 411, 11, 51308], "temperature": 0.0, "avg_logprob": -0.17104734693254744, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.01563308946788311}, {"id": 177, "seek": 112412, "start": 1143.0, "end": 1149.6, "text": " as a user of that API, you get notification that one item has been added or removed or updated.", "tokens": [51308, 382, 257, 4195, 295, 300, 9362, 11, 291, 483, 11554, 300, 472, 3174, 575, 668, 3869, 420, 7261, 420, 10588, 13, 51638], "temperature": 0.0, "avg_logprob": -0.17104734693254744, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.01563308946788311}, {"id": 178, "seek": 114960, "start": 1150.04, "end": 1159.9599999999998, "text": " And you can just, like, react according to that. So how is this all used in ElementX? We're using a", "tokens": [50386, 400, 291, 393, 445, 11, 411, 11, 4515, 4650, 281, 300, 13, 407, 577, 307, 341, 439, 1143, 294, 20900, 55, 30, 492, 434, 1228, 257, 50882], "temperature": 0.0, "avg_logprob": -0.2100436966140549, "compression_ratio": 1.4509803921568627, "no_speech_prob": 0.009887902066111565}, {"id": 179, "seek": 114960, "start": 1159.9599999999998, "end": 1167.9199999999998, "text": " Mozilla project called Unify, Unify FFI. It will automatically create bindings for you for calling", "tokens": [50882, 3335, 26403, 1716, 1219, 1156, 2505, 11, 1156, 2505, 479, 38568, 13, 467, 486, 6772, 1884, 14786, 1109, 337, 291, 337, 5141, 51280], "temperature": 0.0, "avg_logprob": -0.2100436966140549, "compression_ratio": 1.4509803921568627, "no_speech_prob": 0.009887902066111565}, {"id": 180, "seek": 114960, "start": 1167.9199999999998, "end": 1174.52, "text": " interest from other languages. So at this point, we generate bindings for Swift on iOS and Kotlin", "tokens": [51280, 1179, 490, 661, 8650, 13, 407, 412, 341, 935, 11, 321, 8460, 14786, 1109, 337, 25539, 322, 17430, 293, 30123, 5045, 51610], "temperature": 0.0, "avg_logprob": -0.2100436966140549, "compression_ratio": 1.4509803921568627, "no_speech_prob": 0.009887902066111565}, {"id": 181, "seek": 117452, "start": 1174.6, "end": 1181.08, "text": " and Android. It can also generate bindings for other languages. And we use that for Go, for testing", "tokens": [50368, 293, 8853, 13, 467, 393, 611, 8460, 14786, 1109, 337, 661, 8650, 13, 400, 321, 764, 300, 337, 1037, 11, 337, 4997, 50692], "temperature": 0.0, "avg_logprob": -0.1703674049787624, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.01132616214454174}, {"id": 182, "seek": 117452, "start": 1181.08, "end": 1187.12, "text": " purposes, I think. It requires a bit of integration with the foreign languages runtime. And over the", "tokens": [50692, 9932, 11, 286, 519, 13, 467, 7029, 257, 857, 295, 10980, 365, 264, 5329, 8650, 34474, 13, 400, 670, 264, 50994], "temperature": 0.0, "avg_logprob": -0.1703674049787624, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.01132616214454174}, {"id": 183, "seek": 117452, "start": 1187.12, "end": 1192.84, "text": " years, we've contributed a few PRs to this project. So we made it so that you can just use", "tokens": [50994, 924, 11, 321, 600, 18434, 257, 1326, 11568, 82, 281, 341, 1716, 13, 407, 321, 1027, 309, 370, 300, 291, 393, 445, 764, 51280], "temperature": 0.0, "avg_logprob": -0.1703674049787624, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.01132616214454174}, {"id": 184, "seek": 117452, "start": 1192.84, "end": 1201.6, "text": " procedural macros for exporting your types and your input blocks to other languages. And we also", "tokens": [51280, 43951, 7912, 2635, 337, 44686, 428, 3467, 293, 428, 4846, 8474, 281, 661, 8650, 13, 400, 321, 611, 51718], "temperature": 0.0, "avg_logprob": -0.1703674049787624, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.01132616214454174}, {"id": 185, "seek": 120160, "start": 1201.6799999999998, "end": 1207.76, "text": " added this year's support for async code. So you don't have to block when calling into an async", "tokens": [50368, 3869, 341, 1064, 311, 1406, 337, 382, 34015, 3089, 13, 407, 291, 500, 380, 362, 281, 3461, 562, 5141, 666, 364, 382, 34015, 50672], "temperature": 0.0, "avg_logprob": -0.15394701750382134, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009730402380228043}, {"id": 186, "seek": 120160, "start": 1207.76, "end": 1213.84, "text": " function on the Rust side. It will just look as an async function on the Kotlin or Swift side. And", "tokens": [50672, 2445, 322, 264, 34952, 1252, 13, 467, 486, 445, 574, 382, 364, 382, 34015, 2445, 322, 264, 30123, 5045, 420, 25539, 1252, 13, 400, 50976], "temperature": 0.0, "avg_logprob": -0.15394701750382134, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009730402380228043}, {"id": 187, "seek": 120160, "start": 1213.84, "end": 1218.6399999999999, "text": " you have actual concurrency and background processing happening, which is pretty sweet for", "tokens": [50976, 291, 362, 3539, 23702, 10457, 293, 3678, 9007, 2737, 11, 597, 307, 1238, 3844, 337, 51216], "temperature": 0.0, "avg_logprob": -0.15394701750382134, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009730402380228043}, {"id": 188, "seek": 120160, "start": 1218.6399999999999, "end": 1225.6, "text": " performance. And reactive programming in Rust. How do we do it? Well, the principle of reactive", "tokens": [51216, 3389, 13, 400, 28897, 9410, 294, 34952, 13, 1012, 360, 321, 360, 309, 30, 1042, 11, 264, 8665, 295, 28897, 51564], "temperature": 0.0, "avg_logprob": -0.15394701750382134, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009730402380228043}, {"id": 189, "seek": 120160, "start": 1225.6, "end": 1231.28, "text": " programming is you have some data and you want to make it observable so people can subscribe to it.", "tokens": [51564, 9410, 307, 291, 362, 512, 1412, 293, 291, 528, 281, 652, 309, 9951, 712, 370, 561, 393, 3022, 281, 309, 13, 51848], "temperature": 0.0, "avg_logprob": -0.15394701750382134, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.009730402380228043}, {"id": 190, "seek": 123128, "start": 1231.36, "end": 1236.32, "text": " And then they will get notifications. And I mentioned the Timeline API that will notify you", "tokens": [50368, 400, 550, 436, 486, 483, 13426, 13, 400, 286, 2835, 264, 7172, 5440, 9362, 300, 486, 36560, 291, 50616], "temperature": 0.0, "avg_logprob": -0.1766632625034877, "compression_ratio": 1.6577946768060836, "no_speech_prob": 0.010307331569492817}, {"id": 191, "seek": 123128, "start": 1236.32, "end": 1240.32, "text": " when there is a new Timeline item that has been added, removed, et cetera. So we're using", "tokens": [50616, 562, 456, 307, 257, 777, 7172, 5440, 3174, 300, 575, 668, 3869, 11, 7261, 11, 1030, 11458, 13, 407, 321, 434, 1228, 50816], "temperature": 0.0, "avg_logprob": -0.1766632625034877, "compression_ratio": 1.6577946768060836, "no_speech_prob": 0.010307331569492817}, {"id": 192, "seek": 123128, "start": 1241.12, "end": 1247.2, "text": " crates that we created ourselves, IBOL. And there's also an extension that is", "tokens": [50856, 941, 1024, 300, 321, 2942, 4175, 11, 286, 33, 5046, 13, 400, 456, 311, 611, 364, 10320, 300, 307, 51160], "temperature": 0.0, "avg_logprob": -0.1766632625034877, "compression_ratio": 1.6577946768060836, "no_speech_prob": 0.010307331569492817}, {"id": 193, "seek": 123128, "start": 1248.32, "end": 1253.52, "text": " div-based for collections because when you have a vector with a thousand entries in it,", "tokens": [51216, 3414, 12, 6032, 337, 16641, 570, 562, 291, 362, 257, 8062, 365, 257, 4714, 23041, 294, 309, 11, 51476], "temperature": 0.0, "avg_logprob": -0.1766632625034877, "compression_ratio": 1.6577946768060836, "no_speech_prob": 0.010307331569492817}, {"id": 194, "seek": 123128, "start": 1253.52, "end": 1258.48, "text": " you don't want to say, oh, now there's a new thing that has been pushed into the vector.", "tokens": [51476, 291, 500, 380, 528, 281, 584, 11, 1954, 11, 586, 456, 311, 257, 777, 551, 300, 575, 668, 9152, 666, 264, 8062, 13, 51724], "temperature": 0.0, "avg_logprob": -0.1766632625034877, "compression_ratio": 1.6577946768060836, "no_speech_prob": 0.010307331569492817}, {"id": 195, "seek": 125848, "start": 1258.56, "end": 1264.56, "text": " I hear all the 1,001 entries for that vector. No, you just want to hear that", "tokens": [50368, 286, 1568, 439, 264, 502, 11, 628, 16, 23041, 337, 300, 8062, 13, 883, 11, 291, 445, 528, 281, 1568, 300, 50668], "temperature": 0.0, "avg_logprob": -0.1416218380133311, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0044999923557043076}, {"id": 196, "seek": 125848, "start": 1265.44, "end": 1272.24, "text": " there's a new entry and that's its position, right? It also has some extra querying facilities.", "tokens": [50712, 456, 311, 257, 777, 8729, 293, 300, 311, 1080, 2535, 11, 558, 30, 467, 611, 575, 512, 2857, 7083, 1840, 9406, 13, 51052], "temperature": 0.0, "avg_logprob": -0.1416218380133311, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0044999923557043076}, {"id": 197, "seek": 125848, "start": 1272.24, "end": 1279.3600000000001, "text": " So you can batch all these updates, div updates. So you don't have to cross the FFI language", "tokens": [51052, 407, 291, 393, 15245, 439, 613, 9205, 11, 3414, 9205, 13, 407, 291, 500, 380, 362, 281, 3278, 264, 479, 38568, 2856, 51408], "temperature": 0.0, "avg_logprob": -0.1416218380133311, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0044999923557043076}, {"id": 198, "seek": 125848, "start": 1279.3600000000001, "end": 1284.0, "text": " boundary too often. That has an inherent cost that we want to avoid, some overhead that we want to", "tokens": [51408, 12866, 886, 2049, 13, 663, 575, 364, 26387, 2063, 300, 321, 528, 281, 5042, 11, 512, 19922, 300, 321, 528, 281, 51640], "temperature": 0.0, "avg_logprob": -0.1416218380133311, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0044999923557043076}, {"id": 199, "seek": 128400, "start": 1284.0, "end": 1290.96, "text": " avoid. And for your batch transaction, well, for your batch to be quite precise, you need to have", "tokens": [50364, 5042, 13, 400, 337, 428, 15245, 14425, 11, 731, 11, 337, 428, 15245, 281, 312, 1596, 13600, 11, 291, 643, 281, 362, 50712], "temperature": 0.0, "avg_logprob": -0.1290760630184842, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.007145297713577747}, {"id": 200, "seek": 128400, "start": 1290.96, "end": 1294.4, "text": " also transactions to say, this is the beginning of the batch, this is the end of the batch.", "tokens": [50712, 611, 16856, 281, 584, 11, 341, 307, 264, 2863, 295, 264, 15245, 11, 341, 307, 264, 917, 295, 264, 15245, 13, 50884], "temperature": 0.0, "avg_logprob": -0.1290760630184842, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.007145297713577747}, {"id": 201, "seek": 128400, "start": 1295.52, "end": 1303.28, "text": " And also you can do some filtering on these stream of events, limiting, sorting. So it's kind of", "tokens": [50940, 400, 611, 291, 393, 360, 512, 30822, 322, 613, 4309, 295, 3931, 11, 22083, 11, 32411, 13, 407, 309, 311, 733, 295, 51328], "temperature": 0.0, "avg_logprob": -0.1290760630184842, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.007145297713577747}, {"id": 202, "seek": 128400, "start": 1303.84, "end": 1309.52, "text": " mapping to things that you would do on SQL in general. It's pretty sweet and that's what we're", "tokens": [51356, 18350, 281, 721, 300, 291, 576, 360, 322, 19200, 294, 2674, 13, 467, 311, 1238, 3844, 293, 300, 311, 437, 321, 434, 51640], "temperature": 0.0, "avg_logprob": -0.1290760630184842, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.007145297713577747}, {"id": 203, "seek": 130952, "start": 1309.52, "end": 1316.24, "text": " using, for instance, to filter the rooms in the room list immediately on the client side.", "tokens": [50364, 1228, 11, 337, 5197, 11, 281, 6608, 264, 9396, 294, 264, 1808, 1329, 4258, 322, 264, 6423, 1252, 13, 50700], "temperature": 0.0, "avg_logprob": -0.17688052528782894, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.004173177760094404}, {"id": 204, "seek": 130952, "start": 1318.32, "end": 1324.8799999999999, "text": " All right. So some of the future work that we're going to do, well, I intentionally remain a bit", "tokens": [50804, 1057, 558, 13, 407, 512, 295, 264, 2027, 589, 300, 321, 434, 516, 281, 360, 11, 731, 11, 286, 22062, 6222, 257, 857, 51132], "temperature": 0.0, "avg_logprob": -0.17688052528782894, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.004173177760094404}, {"id": 205, "seek": 130952, "start": 1324.8799999999999, "end": 1329.44, "text": " vague here, but we're going to eventually support all the major features a matrix client would expect.", "tokens": [51132, 24247, 510, 11, 457, 321, 434, 516, 281, 4728, 1406, 439, 264, 2563, 4122, 257, 8141, 6423, 576, 2066, 13, 51360], "temperature": 0.0, "avg_logprob": -0.17688052528782894, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.004173177760094404}, {"id": 206, "seek": 130952, "start": 1331.12, "end": 1338.0, "text": " We are already working on Scrantum cryptography. And as of today, I think there has been a PR", "tokens": [51444, 492, 366, 1217, 1364, 322, 2747, 7541, 449, 9844, 5820, 13, 400, 382, 295, 965, 11, 286, 519, 456, 575, 668, 257, 11568, 51788], "temperature": 0.0, "avg_logprob": -0.17688052528782894, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.004173177760094404}, {"id": 207, "seek": 133800, "start": 1338.4, "end": 1344.08, "text": " against Voters and Mac to have something that is compatible with Libsignal and with what they do. So", "tokens": [50384, 1970, 691, 310, 433, 293, 5707, 281, 362, 746, 300, 307, 18218, 365, 15834, 82, 788, 304, 293, 365, 437, 436, 360, 13, 407, 50668], "temperature": 0.0, "avg_logprob": -0.22852176867033305, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.011901452206075191}, {"id": 208, "seek": 133800, "start": 1344.08, "end": 1349.52, "text": " that's pretty exciting. And there is a general theme of doing more things client side. When you have", "tokens": [50668, 300, 311, 1238, 4670, 13, 400, 456, 307, 257, 2674, 6314, 295, 884, 544, 721, 6423, 1252, 13, 1133, 291, 362, 50940], "temperature": 0.0, "avg_logprob": -0.22852176867033305, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.011901452206075191}, {"id": 209, "seek": 133800, "start": 1350.08, "end": 1355.36, "text": " end to end encryption, your server kind of becomes dumb sometimes because it cannot peak into the", "tokens": [50968, 917, 281, 917, 29575, 11, 428, 7154, 733, 295, 3643, 10316, 2171, 570, 309, 2644, 10651, 666, 264, 51232], "temperature": 0.0, "avg_logprob": -0.22852176867033305, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.011901452206075191}, {"id": 210, "seek": 133800, "start": 1355.36, "end": 1362.32, "text": " encrypted event. And so you have to resolve a lot of things on the client side. If you get a", "tokens": [51232, 36663, 2280, 13, 400, 370, 291, 362, 281, 14151, 257, 688, 295, 721, 322, 264, 6423, 1252, 13, 759, 291, 483, 257, 51580], "temperature": 0.0, "avg_logprob": -0.22852176867033305, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.011901452206075191}, {"id": 211, "seek": 136232, "start": 1362.96, "end": 1367.4399999999998, "text": " new event in a room, does that trigger a notification for an encrypted room? Well,", "tokens": [50396, 777, 2280, 294, 257, 1808, 11, 775, 300, 7875, 257, 11554, 337, 364, 36663, 1808, 30, 1042, 11, 50620], "temperature": 0.0, "avg_logprob": -0.10402121586082257, "compression_ratio": 1.889344262295082, "no_speech_prob": 0.014987063594162464}, {"id": 212, "seek": 136232, "start": 1367.4399999999998, "end": 1371.6, "text": " you have to push a notification and it's the clients that will decide whether or not it", "tokens": [50620, 291, 362, 281, 2944, 257, 11554, 293, 309, 311, 264, 6982, 300, 486, 4536, 1968, 420, 406, 309, 50828], "temperature": 0.0, "avg_logprob": -0.10402121586082257, "compression_ratio": 1.889344262295082, "no_speech_prob": 0.014987063594162464}, {"id": 213, "seek": 136232, "start": 1371.6, "end": 1377.6799999999998, "text": " resolves into an actual notification. And even for sorting the room list, you have to do it", "tokens": [50828, 7923, 977, 666, 364, 3539, 11554, 13, 400, 754, 337, 32411, 264, 1808, 1329, 11, 291, 362, 281, 360, 309, 51132], "temperature": 0.0, "avg_logprob": -0.10402121586082257, "compression_ratio": 1.889344262295082, "no_speech_prob": 0.014987063594162464}, {"id": 214, "seek": 136232, "start": 1377.6799999999998, "end": 1382.32, "text": " client side because if there is some room activity, you want to sort by room activity, just show me", "tokens": [51132, 6423, 1252, 570, 498, 456, 307, 512, 1808, 5191, 11, 291, 528, 281, 1333, 538, 1808, 5191, 11, 445, 855, 385, 51364], "temperature": 0.0, "avg_logprob": -0.10402121586082257, "compression_ratio": 1.889344262295082, "no_speech_prob": 0.014987063594162464}, {"id": 215, "seek": 136232, "start": 1382.32, "end": 1388.48, "text": " the room that have some activity. Well, it's the same thing. If the event was encrypted, you don't", "tokens": [51364, 264, 1808, 300, 362, 512, 5191, 13, 1042, 11, 309, 311, 264, 912, 551, 13, 759, 264, 2280, 390, 36663, 11, 291, 500, 380, 51672], "temperature": 0.0, "avg_logprob": -0.10402121586082257, "compression_ratio": 1.889344262295082, "no_speech_prob": 0.014987063594162464}, {"id": 216, "seek": 138848, "start": 1388.96, "end": 1393.28, "text": " know if it was just a thumbs up reaction. Maybe that doesn't justify putting the room at the top.", "tokens": [50388, 458, 498, 309, 390, 445, 257, 8838, 493, 5480, 13, 2704, 300, 1177, 380, 20833, 3372, 264, 1808, 412, 264, 1192, 13, 50604], "temperature": 0.0, "avg_logprob": -0.2106579341245501, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.008790436200797558}, {"id": 217, "seek": 138848, "start": 1394.08, "end": 1400.0, "text": " If it was something meaningful like an actual message. So that means that this task has to be", "tokens": [50644, 759, 309, 390, 746, 10995, 411, 364, 3539, 3636, 13, 407, 300, 1355, 300, 341, 5633, 575, 281, 312, 50940], "temperature": 0.0, "avg_logprob": -0.2106579341245501, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.008790436200797558}, {"id": 218, "seek": 138848, "start": 1400.0, "end": 1405.3600000000001, "text": " done on the client now. And yeah, we're also computing the other badges client side in the", "tokens": [50940, 1096, 322, 264, 6423, 586, 13, 400, 1338, 11, 321, 434, 611, 15866, 264, 661, 43894, 6423, 1252, 294, 264, 51208], "temperature": 0.0, "avg_logprob": -0.2106579341245501, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.008790436200797558}, {"id": 219, "seek": 138848, "start": 1405.3600000000001, "end": 1412.72, "text": " rest SDK. So we are trying to be very careful to not get into static notifications situations", "tokens": [51208, 1472, 37135, 13, 407, 321, 366, 1382, 281, 312, 588, 5026, 281, 406, 483, 666, 13437, 13426, 6851, 51576], "temperature": 0.0, "avg_logprob": -0.2106579341245501, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.008790436200797558}, {"id": 220, "seek": 141272, "start": 1412.72, "end": 1418.72, "text": " because it's a pain for everyone, us included. And yeah, that's pretty much it. All right.", "tokens": [50364, 570, 309, 311, 257, 1822, 337, 1518, 11, 505, 5556, 13, 400, 1338, 11, 300, 311, 1238, 709, 309, 13, 1057, 558, 13, 50664], "temperature": 0.0, "avg_logprob": -0.19974610357001277, "compression_ratio": 1.4796747967479675, "no_speech_prob": 0.033527787774801254}, {"id": 221, "seek": 141272, "start": 1419.44, "end": 1424.72, "text": " Just a few things. Well, first to all the contributors of the rest SDK special shout out", "tokens": [50700, 1449, 257, 1326, 721, 13, 1042, 11, 700, 281, 439, 264, 45627, 295, 264, 1472, 37135, 2121, 8043, 484, 50964], "temperature": 0.0, "avg_logprob": -0.19974610357001277, "compression_ratio": 1.4796747967479675, "no_speech_prob": 0.033527787774801254}, {"id": 222, "seek": 141272, "start": 1424.72, "end": 1432.32, "text": " to Kevin Komei from the fractal community. It's done like a bunch of work in the rest SDK,", "tokens": [50964, 281, 9954, 591, 423, 72, 490, 264, 17948, 304, 1768, 13, 467, 311, 1096, 411, 257, 3840, 295, 589, 294, 264, 1472, 37135, 11, 51344], "temperature": 0.0, "avg_logprob": -0.19974610357001277, "compression_ratio": 1.4796747967479675, "no_speech_prob": 0.033527787774801254}, {"id": 223, "seek": 141272, "start": 1432.32, "end": 1439.28, "text": " including most of the support for OIDC on the client side, which was MaceFPR. And if you want", "tokens": [51344, 3009, 881, 295, 264, 1406, 337, 422, 2777, 34, 322, 264, 6423, 1252, 11, 597, 390, 376, 617, 37, 15958, 13, 400, 498, 291, 528, 51692], "temperature": 0.0, "avg_logprob": -0.19974610357001277, "compression_ratio": 1.4796747967479675, "no_speech_prob": 0.033527787774801254}, {"id": 224, "seek": 143928, "start": 1439.28, "end": 1445.6, "text": " to be on this slide next year, you can contribute to we have a few issues that are tagged as good", "tokens": [50364, 281, 312, 322, 341, 4137, 958, 1064, 11, 291, 393, 10586, 281, 321, 362, 257, 1326, 2663, 300, 366, 40239, 382, 665, 50680], "temperature": 0.0, "avg_logprob": -0.13889320330186325, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.03692277520895004}, {"id": 225, "seek": 143928, "start": 1445.6, "end": 1452.16, "text": " first issues or help pointed if you want. And I would like to take this opportunity also to", "tokens": [50680, 700, 2663, 420, 854, 10932, 498, 291, 528, 13, 400, 286, 576, 411, 281, 747, 341, 2650, 611, 281, 51008], "temperature": 0.0, "avg_logprob": -0.13889320330186325, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.03692277520895004}, {"id": 226, "seek": 143928, "start": 1452.16, "end": 1457.76, "text": " thank elements for donating all of my work to the matrix organization. You can also be a supporter", "tokens": [51008, 1309, 4959, 337, 36686, 439, 295, 452, 589, 281, 264, 8141, 4475, 13, 509, 393, 611, 312, 257, 28600, 51288], "temperature": 0.0, "avg_logprob": -0.13889320330186325, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.03692277520895004}, {"id": 227, "seek": 143928, "start": 1457.76, "end": 1463.84, "text": " of matrix if you want by following one of these two links. Thank you for listening.", "tokens": [51288, 295, 8141, 498, 291, 528, 538, 3480, 472, 295, 613, 732, 6123, 13, 1044, 291, 337, 4764, 13, 51592], "temperature": 0.0, "avg_logprob": -0.13889320330186325, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.03692277520895004}, {"id": 228, "seek": 146384, "start": 1464.48, "end": 1467.28, "text": " And I would be happy to answer any questions if you have any.", "tokens": [50396, 400, 286, 576, 312, 2055, 281, 1867, 604, 1651, 498, 291, 362, 604, 13, 50536], "temperature": 0.0, "avg_logprob": -0.21614563465118408, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.00994837936013937}, {"id": 229, "seek": 146384, "start": 1476.6399999999999, "end": 1479.84, "text": " To the internet asking why have you moved away from sled?", "tokens": [51004, 1407, 264, 4705, 3365, 983, 362, 291, 4259, 1314, 490, 46242, 30, 51164], "temperature": 0.0, "avg_logprob": -0.21614563465118408, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.00994837936013937}, {"id": 230, "seek": 146384, "start": 1481.12, "end": 1487.1999999999998, "text": " Why have we moved away from sled? That's a good question. So I think in terms of performance,", "tokens": [51228, 1545, 362, 321, 4259, 1314, 490, 46242, 30, 663, 311, 257, 665, 1168, 13, 407, 286, 519, 294, 2115, 295, 3389, 11, 51532], "temperature": 0.0, "avg_logprob": -0.21614563465118408, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.00994837936013937}, {"id": 231, "seek": 148720, "start": 1487.8400000000001, "end": 1494.4, "text": " so slay this, if I recall correctly, I wasn't there when that happened. So it's kind of hard to", "tokens": [50396, 370, 1061, 320, 341, 11, 498, 286, 9901, 8944, 11, 286, 2067, 380, 456, 562, 300, 2011, 13, 407, 309, 311, 733, 295, 1152, 281, 50724], "temperature": 0.0, "avg_logprob": -0.1713527115908536, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.012922211550176144}, {"id": 232, "seek": 148720, "start": 1494.4, "end": 1500.0, "text": " answer this precisely, but I think that it's a key value store embedded key value store. And", "tokens": [50724, 1867, 341, 13402, 11, 457, 286, 519, 300, 309, 311, 257, 2141, 2158, 3531, 16741, 2141, 2158, 3531, 13, 400, 51004], "temperature": 0.0, "avg_logprob": -0.1713527115908536, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.012922211550176144}, {"id": 233, "seek": 148720, "start": 1500.0, "end": 1509.8400000000001, "text": " the performance was not great, especially on mobile devices. And we just figured that using", "tokens": [51004, 264, 3389, 390, 406, 869, 11, 2318, 322, 6013, 5759, 13, 400, 321, 445, 8932, 300, 1228, 51496], "temperature": 0.0, "avg_logprob": -0.1713527115908536, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.012922211550176144}, {"id": 234, "seek": 148720, "start": 1509.8400000000001, "end": 1515.28, "text": " SQLite that has been like performance tested and improved and tuned over the years was", "tokens": [51496, 19200, 642, 300, 575, 668, 411, 3389, 8246, 293, 9689, 293, 10870, 670, 264, 924, 390, 51768], "temperature": 0.0, "avg_logprob": -0.1713527115908536, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.012922211550176144}, {"id": 235, "seek": 151528, "start": 1516.0, "end": 1523.12, "text": " a right thing to do. And also the way you structure your data using a SQL database is quite", "tokens": [50400, 257, 558, 551, 281, 360, 13, 400, 611, 264, 636, 291, 3877, 428, 1412, 1228, 257, 19200, 8149, 307, 1596, 50756], "temperature": 0.0, "avg_logprob": -0.14395007633027576, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.006550498306751251}, {"id": 236, "seek": 151528, "start": 1523.12, "end": 1528.0, "text": " different from the way you would structure it with a key value store. So it's just like", "tokens": [50756, 819, 490, 264, 636, 291, 576, 3877, 309, 365, 257, 2141, 2158, 3531, 13, 407, 309, 311, 445, 411, 51000], "temperature": 0.0, "avg_logprob": -0.14395007633027576, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.006550498306751251}, {"id": 237, "seek": 151528, "start": 1528.72, "end": 1533.92, "text": " slightly easier to perform requests when you have a SQL database because you know all of that.", "tokens": [51036, 4748, 3571, 281, 2042, 12475, 562, 291, 362, 257, 19200, 8149, 570, 291, 458, 439, 295, 300, 13, 51296], "temperature": 0.0, "avg_logprob": -0.14395007633027576, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.006550498306751251}, {"id": 238, "seek": 151528, "start": 1536.3999999999999, "end": 1544.8, "text": " Yeah. Any other question? The internet also asks how is your developer experience when using", "tokens": [51420, 865, 13, 2639, 661, 1168, 30, 440, 4705, 611, 8962, 577, 307, 428, 10754, 1752, 562, 1228, 51840], "temperature": 0.0, "avg_logprob": -0.14395007633027576, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.006550498306751251}, {"id": 239, "seek": 154480, "start": 1544.8, "end": 1552.32, "text": " UniFi, UniFFI in general? Are there any hard edges? That's a good question. So there's, oh yes,", "tokens": [50364, 35191, 13229, 11, 35191, 37, 38568, 294, 2674, 30, 2014, 456, 604, 1152, 8819, 30, 663, 311, 257, 665, 1168, 13, 407, 456, 311, 11, 1954, 2086, 11, 50740], "temperature": 0.0, "avg_logprob": -0.20901978015899658, "compression_ratio": 1.4896907216494846, "no_speech_prob": 0.00165656884200871}, {"id": 240, "seek": 154480, "start": 1552.32, "end": 1561.44, "text": " so when using UniFi across, for calling rest across other languages, have there been hard edges?", "tokens": [50740, 370, 562, 1228, 35191, 13229, 2108, 11, 337, 5141, 1472, 2108, 661, 8650, 11, 362, 456, 668, 1152, 8819, 30, 51196], "temperature": 0.0, "avg_logprob": -0.20901978015899658, "compression_ratio": 1.4896907216494846, "no_speech_prob": 0.00165656884200871}, {"id": 241, "seek": 154480, "start": 1561.44, "end": 1568.6399999999999, "text": " Yes. It's been a few cases where we have a memory leak that is identified. Well, Kotlin uses the", "tokens": [51196, 1079, 13, 467, 311, 668, 257, 1326, 3331, 689, 321, 362, 257, 4675, 17143, 300, 307, 9234, 13, 1042, 11, 30123, 5045, 4960, 264, 51556], "temperature": 0.0, "avg_logprob": -0.20901978015899658, "compression_ratio": 1.4896907216494846, "no_speech_prob": 0.00165656884200871}, {"id": 242, "seek": 156864, "start": 1568.64, "end": 1578.0800000000002, "text": " GVM and GVM as a garbage collector. And so we accidentally, and when I say we, I think it's", "tokens": [50364, 460, 53, 44, 293, 460, 53, 44, 382, 257, 14150, 23960, 13, 400, 370, 321, 15715, 11, 293, 562, 286, 584, 321, 11, 286, 519, 309, 311, 50836], "temperature": 0.0, "avg_logprob": -0.13202818846091247, "compression_ratio": 1.42, "no_speech_prob": 0.007486122194677591}, {"id": 243, "seek": 156864, "start": 1578.0800000000002, "end": 1585.8400000000001, "text": " like the UniFi group in general, introduced some leaks by having the equivalent of premises or", "tokens": [50836, 411, 264, 35191, 13229, 1594, 294, 2674, 11, 7268, 512, 28885, 538, 1419, 264, 10344, 295, 34266, 420, 51224], "temperature": 0.0, "avg_logprob": -0.13202818846091247, "compression_ratio": 1.42, "no_speech_prob": 0.007486122194677591}, {"id": 244, "seek": 156864, "start": 1585.8400000000001, "end": 1593.5200000000002, "text": " futures leak sometimes. So that was a problem, but usually it's, I would say it's 90% of the time", "tokens": [51224, 26071, 17143, 2171, 13, 407, 300, 390, 257, 1154, 11, 457, 2673, 309, 311, 11, 286, 576, 584, 309, 311, 4289, 4, 295, 264, 565, 51608], "temperature": 0.0, "avg_logprob": -0.13202818846091247, "compression_ratio": 1.42, "no_speech_prob": 0.007486122194677591}, {"id": 245, "seek": 159352, "start": 1593.52, "end": 1599.28, "text": " it's stable. And the 10% of the time where there is an issue, it's high priority for us because", "tokens": [50364, 309, 311, 8351, 13, 400, 264, 1266, 4, 295, 264, 565, 689, 456, 307, 364, 2734, 11, 309, 311, 1090, 9365, 337, 505, 570, 50652], "temperature": 0.0, "avg_logprob": -0.06817802940447305, "compression_ratio": 1.5672268907563025, "no_speech_prob": 0.006607349496334791}, {"id": 246, "seek": 159352, "start": 1599.28, "end": 1606.08, "text": " obviously it breaks our apps. So we fix it, we try to fix it as quick as possible and we", "tokens": [50652, 2745, 309, 9857, 527, 7733, 13, 407, 321, 3191, 309, 11, 321, 853, 281, 3191, 309, 382, 1702, 382, 1944, 293, 321, 50992], "temperature": 0.0, "avg_logprob": -0.06817802940447305, "compression_ratio": 1.5672268907563025, "no_speech_prob": 0.006607349496334791}, {"id": 247, "seek": 159352, "start": 1606.08, "end": 1613.2, "text": " contribute back. But most of the time it works fine for Kotlin and Swift. So the support and", "tokens": [50992, 10586, 646, 13, 583, 881, 295, 264, 565, 309, 1985, 2489, 337, 30123, 5045, 293, 25539, 13, 407, 264, 1406, 293, 51348], "temperature": 0.0, "avg_logprob": -0.06817802940447305, "compression_ratio": 1.5672268907563025, "no_speech_prob": 0.006607349496334791}, {"id": 248, "seek": 159352, "start": 1613.2, "end": 1621.04, "text": " stability is also per language, I suppose, since you have to create bindings for each language.", "tokens": [51348, 11826, 307, 611, 680, 2856, 11, 286, 7297, 11, 1670, 291, 362, 281, 1884, 14786, 1109, 337, 1184, 2856, 13, 51740], "temperature": 0.0, "avg_logprob": -0.06817802940447305, "compression_ratio": 1.5672268907563025, "no_speech_prob": 0.006607349496334791}, {"id": 249, "seek": 162104, "start": 1621.04, "end": 1628.6399999999999, "text": " So yeah, I cannot speak for the Python or Go generation on the UniFi side. But usually,", "tokens": [50364, 407, 1338, 11, 286, 2644, 1710, 337, 264, 15329, 420, 1037, 5125, 322, 264, 35191, 13229, 1252, 13, 583, 2673, 11, 50744], "temperature": 0.0, "avg_logprob": -0.1832871363713191, "compression_ratio": 1.404494382022472, "no_speech_prob": 0.006697186268866062}, {"id": 250, "seek": 162104, "start": 1628.6399999999999, "end": 1635.04, "text": " since Mozilla also use UniFi, they have to provide high stability guarantees as well. So they are", "tokens": [50744, 1670, 3335, 26403, 611, 764, 35191, 13229, 11, 436, 362, 281, 2893, 1090, 11826, 32567, 382, 731, 13, 407, 436, 366, 51064], "temperature": 0.0, "avg_logprob": -0.1832871363713191, "compression_ratio": 1.404494382022472, "no_speech_prob": 0.006697186268866062}, {"id": 251, "seek": 162104, "start": 1635.04, "end": 1642.32, "text": " pretty reactive and also fixing bugs. So it's working well. Yes.", "tokens": [51064, 1238, 28897, 293, 611, 19442, 15120, 13, 407, 309, 311, 1364, 731, 13, 1079, 13, 51428], "temperature": 0.0, "avg_logprob": -0.1832871363713191, "compression_ratio": 1.404494382022472, "no_speech_prob": 0.006697186268866062}, {"id": 252, "seek": 164232, "start": 1642.6399999999999, "end": 1646.0, "text": " I was wondering about the startup times.", "tokens": [50380, 286, 390, 6359, 466, 264, 18578, 1413, 13, 50548], "temperature": 0.0, "avg_logprob": -0.701225757598877, "compression_ratio": 0.8333333333333334, "no_speech_prob": 0.09502141922712326}, {"id": 253, "seek": 164600, "start": 1646.4, "end": 1673.92, "text": " Yeah, so the question was, what about startup times for the rest of the time? I think there were", "tokens": [50384, 865, 11, 370, 264, 1168, 390, 11, 437, 466, 18578, 1413, 337, 264, 1472, 295, 264, 565, 30, 286, 519, 456, 645, 51760], "temperature": 0.0, "avg_logprob": -0.36291892711932844, "compression_ratio": 1.1566265060240963, "no_speech_prob": 0.026554416865110397}, {"id": 254, "seek": 167392, "start": 1674.0, "end": 1680.72, "text": " two questions. The first one was just starting the SDK itself and then when you're syncing a list", "tokens": [50368, 732, 1651, 13, 440, 700, 472, 390, 445, 2891, 264, 37135, 2564, 293, 550, 562, 291, 434, 5451, 2175, 257, 1329, 50704], "temperature": 0.0, "avg_logprob": -0.16249985788382737, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.11303886771202087}, {"id": 255, "seek": 167392, "start": 1680.72, "end": 1687.44, "text": " of rooms, do you get instant and response and all of that? And well, it's native code, so you don't", "tokens": [50704, 295, 9396, 11, 360, 291, 483, 9836, 293, 4134, 293, 439, 295, 300, 30, 400, 731, 11, 309, 311, 8470, 3089, 11, 370, 291, 500, 380, 51040], "temperature": 0.0, "avg_logprob": -0.16249985788382737, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.11303886771202087}, {"id": 256, "seek": 167392, "start": 1687.44, "end": 1694.8000000000002, "text": " have to boot up an entire VM for the SDK itself. So it's pretty fast. It will restore the state", "tokens": [51040, 362, 281, 11450, 493, 364, 2302, 18038, 337, 264, 37135, 2564, 13, 407, 309, 311, 1238, 2370, 13, 467, 486, 15227, 264, 1785, 51408], "temperature": 0.0, "avg_logprob": -0.16249985788382737, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.11303886771202087}, {"id": 257, "seek": 167392, "start": 1694.8000000000002, "end": 1703.1200000000001, "text": " from the disk. So that can be a slow step. But even like for users who have thousands and thousands", "tokens": [51408, 490, 264, 12355, 13, 407, 300, 393, 312, 257, 2964, 1823, 13, 583, 754, 411, 337, 5022, 567, 362, 5383, 293, 5383, 51824], "temperature": 0.0, "avg_logprob": -0.16249985788382737, "compression_ratio": 1.5975609756097562, "no_speech_prob": 0.11303886771202087}, {"id": 258, "seek": 170312, "start": 1703.12, "end": 1711.36, "text": " of rooms open and I'm looking at Matthew on the side of the room, our general benchmark runner,", "tokens": [50364, 295, 9396, 1269, 293, 286, 478, 1237, 412, 12434, 322, 264, 1252, 295, 264, 1808, 11, 527, 2674, 18927, 24376, 11, 50776], "temperature": 0.0, "avg_logprob": -0.18009713577897582, "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.020604010671377182}, {"id": 259, "seek": 170312, "start": 1712.8, "end": 1719.9199999999998, "text": " it's pretty fast. And for receiving a room list, we are also tracking these performance of our time.", "tokens": [50848, 309, 311, 1238, 2370, 13, 400, 337, 10040, 257, 1808, 1329, 11, 321, 366, 611, 11603, 613, 3389, 295, 527, 565, 13, 51204], "temperature": 0.0, "avg_logprob": -0.18009713577897582, "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.020604010671377182}, {"id": 260, "seek": 170312, "start": 1721.4399999999998, "end": 1727.12, "text": " Pretty much instant. And every time there is an improvement that needs to be done, we'll do it.", "tokens": [51280, 10693, 709, 9836, 13, 400, 633, 565, 456, 307, 364, 10444, 300, 2203, 281, 312, 1096, 11, 321, 603, 360, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18009713577897582, "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.020604010671377182}, {"id": 261, "seek": 172712, "start": 1728.08, "end": 1732.9599999999998, "text": " Yeah. I mean, we are in sync with the synchronization times where about", "tokens": [50412, 865, 13, 286, 914, 11, 321, 366, 294, 20271, 365, 264, 19331, 2144, 1413, 689, 466, 50656], "temperature": 0.0, "avg_logprob": -0.2428310333736359, "compression_ratio": 1.3444444444444446, "no_speech_prob": 0.01005760207772255}, {"id": 262, "seek": 172712, "start": 1733.84, "end": 1740.0, "text": " between five to 20 minutes, if you are a very heavy weight user of Matrix,", "tokens": [50700, 1296, 1732, 281, 945, 2077, 11, 498, 291, 366, 257, 588, 4676, 3364, 4195, 295, 36274, 11, 51008], "temperature": 0.0, "avg_logprob": -0.2428310333736359, "compression_ratio": 1.3444444444444446, "no_speech_prob": 0.01005760207772255}, {"id": 263, "seek": 172712, "start": 1740.0, "end": 1745.6, "text": " now it's really up to three seconds. So consider that an improvement.", "tokens": [51008, 586, 309, 311, 534, 493, 281, 1045, 3949, 13, 407, 1949, 300, 364, 10444, 13, 51288], "temperature": 0.0, "avg_logprob": -0.2428310333736359, "compression_ratio": 1.3444444444444446, "no_speech_prob": 0.01005760207772255}, {"id": 264, "seek": 172712, "start": 1750.6399999999999, "end": 1752.8, "text": " Any other questions? Yes?", "tokens": [51540, 2639, 661, 1651, 30, 1079, 30, 51648], "temperature": 0.0, "avg_logprob": -0.2428310333736359, "compression_ratio": 1.3444444444444446, "no_speech_prob": 0.01005760207772255}, {"id": 265, "seek": 175712, "start": 1757.84, "end": 1767.4399999999998, "text": " What's the state of supporting extensible events in the rest SDK? So I think that's a question for", "tokens": [50400, 708, 311, 264, 1785, 295, 7231, 1279, 30633, 3931, 294, 264, 1472, 37135, 30, 407, 286, 519, 300, 311, 257, 1168, 337, 50880], "temperature": 0.0, "avg_logprob": -0.17994683583577473, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.003281045239418745}, {"id": 266, "seek": 175712, "start": 1767.4399999999998, "end": 1776.0, "text": " Ruma. And since we're using Ruma for passing the events, and I'm pretty sure that so the rest type", "tokens": [50880, 497, 5544, 13, 400, 1670, 321, 434, 1228, 497, 5544, 337, 8437, 264, 3931, 11, 293, 286, 478, 1238, 988, 300, 370, 264, 1472, 2010, 51308], "temperature": 0.0, "avg_logprob": -0.17994683583577473, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.003281045239418745}, {"id": 267, "seek": 175712, "start": 1776.0, "end": 1782.08, "text": " system is quite extensive in the sense that you can have union types. And for each event that can", "tokens": [51308, 1185, 307, 1596, 13246, 294, 264, 2020, 300, 291, 393, 362, 11671, 3467, 13, 400, 337, 1184, 2280, 300, 393, 51612], "temperature": 0.0, "avg_logprob": -0.17994683583577473, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.003281045239418745}, {"id": 268, "seek": 178208, "start": 1782.08, "end": 1788.32, "text": " be extended, I suppose that you can have there is a variant in that unit tab that says it's a custom", "tokens": [50364, 312, 10913, 11, 286, 7297, 300, 291, 393, 362, 456, 307, 257, 17501, 294, 300, 4985, 4421, 300, 1619, 309, 311, 257, 2375, 50676], "temperature": 0.0, "avg_logprob": -0.1738943170618128, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.039060674607753754}, {"id": 269, "seek": 178208, "start": 1788.32, "end": 1793.52, "text": " event. If you're referring to a specific MSC, I don't know what it is. And I'm sorry about that.", "tokens": [50676, 2280, 13, 759, 291, 434, 13761, 281, 257, 2685, 7395, 34, 11, 286, 500, 380, 458, 437, 309, 307, 13, 400, 286, 478, 2597, 466, 300, 13, 50936], "temperature": 0.0, "avg_logprob": -0.1738943170618128, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.039060674607753754}, {"id": 270, "seek": 178208, "start": 1794.24, "end": 1801.52, "text": " Was that a custom MSC or? No. No. Okay. Just events in general. So yes, you will end up in", "tokens": [50972, 3027, 300, 257, 2375, 7395, 34, 420, 30, 883, 13, 883, 13, 1033, 13, 1449, 3931, 294, 2674, 13, 407, 2086, 11, 291, 486, 917, 493, 294, 51336], "temperature": 0.0, "avg_logprob": -0.1738943170618128, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.039060674607753754}, {"id": 271, "seek": 178208, "start": 1801.52, "end": 1807.12, "text": " these case where we have, you will match on this union type or the event and it will say,", "tokens": [51336, 613, 1389, 689, 321, 362, 11, 291, 486, 2995, 322, 341, 11671, 2010, 420, 264, 2280, 293, 309, 486, 584, 11, 51616], "temperature": 0.0, "avg_logprob": -0.1738943170618128, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.039060674607753754}, {"id": 272, "seek": 180712, "start": 1807.12, "end": 1812.3999999999999, "text": " well, it's something I don't know about. So I'm just ending it over to you and you do something with this.", "tokens": [50364, 731, 11, 309, 311, 746, 286, 500, 380, 458, 466, 13, 407, 286, 478, 445, 8121, 309, 670, 281, 291, 293, 291, 360, 746, 365, 341, 13, 50628], "temperature": 0.0, "avg_logprob": -0.19196559401119456, "compression_ratio": 1.168421052631579, "no_speech_prob": 0.033345501869916916}, {"id": 273, "seek": 180712, "start": 1814.6399999999999, "end": 1816.6399999999999, "text": " Yes?", "tokens": [50740, 1079, 30, 50840], "temperature": 0.0, "avg_logprob": -0.19196559401119456, "compression_ratio": 1.168421052631579, "no_speech_prob": 0.033345501869916916}, {"id": 274, "seek": 183712, "start": 1837.28, "end": 1840.8, "text": " Can we also provide a question for the rest of the SDK?", "tokens": [50372, 1664, 321, 611, 2893, 257, 1168, 337, 264, 1472, 295, 264, 37135, 30, 50548], "temperature": 0.0, "avg_logprob": -0.2902534908718533, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.007307091727852821}, {"id": 275, "seek": 183712, "start": 1840.8, "end": 1845.4399999999998, "text": " I'll rephrase this question as are there plans to use the rest SDK for web?", "tokens": [50548, 286, 603, 319, 44598, 651, 341, 1168, 382, 366, 456, 5482, 281, 764, 264, 1472, 37135, 337, 3670, 30, 50780], "temperature": 0.0, "avg_logprob": -0.2902534908718533, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.007307091727852821}, {"id": 276, "seek": 183712, "start": 1846.3999999999999, "end": 1852.32, "text": " Because it's not used. So right now, as we are speaking, as of last week, people have started,", "tokens": [50828, 1436, 309, 311, 406, 1143, 13, 407, 558, 586, 11, 382, 321, 366, 4124, 11, 382, 295, 1036, 1243, 11, 561, 362, 1409, 11, 51124], "temperature": 0.0, "avg_logprob": -0.2902534908718533, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.007307091727852821}, {"id": 277, "seek": 183712, "start": 1854.32, "end": 1861.4399999999998, "text": " well, have enabled by default for new logins on Element Web, I think. So that may be the nightly", "tokens": [51224, 731, 11, 362, 15172, 538, 7576, 337, 777, 3565, 1292, 322, 20900, 9573, 11, 286, 519, 13, 407, 300, 815, 312, 264, 1818, 356, 51580], "temperature": 0.0, "avg_logprob": -0.2902534908718533, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.007307091727852821}, {"id": 278, "seek": 186144, "start": 1861.44, "end": 1872.16, "text": " version using the rest cryptography for Element Web. We have a separate repository for bindings", "tokens": [50364, 3037, 1228, 264, 1472, 9844, 5820, 337, 20900, 9573, 13, 492, 362, 257, 4994, 25841, 337, 14786, 1109, 50900], "temperature": 0.0, "avg_logprob": -0.16201230883598328, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.020632974803447723}, {"id": 279, "seek": 186144, "start": 1872.16, "end": 1878.16, "text": " that are for WebAssembly because there's no meaning in using Unify for that. We can directly compile", "tokens": [50900, 300, 366, 337, 9573, 10884, 19160, 570, 456, 311, 572, 3620, 294, 1228, 1156, 2505, 337, 300, 13, 492, 393, 3838, 31413, 51200], "temperature": 0.0, "avg_logprob": -0.16201230883598328, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.020632974803447723}, {"id": 280, "seek": 186144, "start": 1878.16, "end": 1883.2, "text": " the rest to WebAssembly. So no need to have an intermediary in the middle. And I think the", "tokens": [51200, 264, 1472, 281, 9573, 10884, 19160, 13, 407, 572, 643, 281, 362, 364, 15184, 822, 294, 264, 2808, 13, 400, 286, 519, 264, 51452], "temperature": 0.0, "avg_logprob": -0.16201230883598328, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.020632974803447723}, {"id": 281, "seek": 186144, "start": 1883.2, "end": 1889.8400000000001, "text": " long-term goal is to use the rest SDK everywhere for the Element apps at least. So don't, don't", "tokens": [51452, 938, 12, 7039, 3387, 307, 281, 764, 264, 1472, 37135, 5315, 337, 264, 20900, 7733, 412, 1935, 13, 407, 500, 380, 11, 500, 380, 51784], "temperature": 0.0, "avg_logprob": -0.16201230883598328, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.020632974803447723}, {"id": 282, "seek": 189144, "start": 1891.52, "end": 1895.44, "text": " take my word as granted, but I think that this is going to happen. Yeah.", "tokens": [50368, 747, 452, 1349, 382, 12344, 11, 457, 286, 519, 300, 341, 307, 516, 281, 1051, 13, 865, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1216725331765634, "compression_ratio": 1.3356643356643356, "no_speech_prob": 0.005220472346991301}, {"id": 283, "seek": 189144, "start": 1898.4, "end": 1900.4, "text": " Any other question? Yes?", "tokens": [50712, 2639, 661, 1168, 30, 1079, 30, 50812], "temperature": 0.0, "avg_logprob": -0.1216725331765634, "compression_ratio": 1.3356643356643356, "no_speech_prob": 0.005220472346991301}, {"id": 284, "seek": 189144, "start": 1915.3600000000001, "end": 1920.64, "text": " That's a very good question. So the question is, is search in scope for the rest SDK and what", "tokens": [51560, 663, 311, 257, 588, 665, 1168, 13, 407, 264, 1168, 307, 11, 307, 3164, 294, 11923, 337, 264, 1472, 37135, 293, 437, 51824], "temperature": 0.0, "avg_logprob": -0.1216725331765634, "compression_ratio": 1.3356643356643356, "no_speech_prob": 0.005220472346991301}, {"id": 285, "seek": 192064, "start": 1920.64, "end": 1926.88, "text": " kind of features would be out of scope for the rest SDK? So to respond for search, that depends", "tokens": [50364, 733, 295, 4122, 576, 312, 484, 295, 11923, 337, 264, 1472, 37135, 30, 407, 281, 4196, 337, 3164, 11, 300, 5946, 50676], "temperature": 0.0, "avg_logprob": -0.10965976909715303, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.006836786866188049}, {"id": 286, "seek": 192064, "start": 1926.88, "end": 1933.1200000000001, "text": " if you mean room search or message search, full text search. And well, actually it doesn't depend", "tokens": [50676, 498, 291, 914, 1808, 3164, 420, 3636, 3164, 11, 1577, 2487, 3164, 13, 400, 731, 11, 767, 309, 1177, 380, 5672, 50988], "temperature": 0.0, "avg_logprob": -0.10965976909715303, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.006836786866188049}, {"id": 287, "seek": 192064, "start": 1933.1200000000001, "end": 1940.8000000000002, "text": " because the answer for both is yes. We're going to try to take care of that. For full text, we,", "tokens": [50988, 570, 264, 1867, 337, 1293, 307, 2086, 13, 492, 434, 516, 281, 853, 281, 747, 1127, 295, 300, 13, 1171, 1577, 2487, 11, 321, 11, 51372], "temperature": 0.0, "avg_logprob": -0.10965976909715303, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.006836786866188049}, {"id": 288, "seek": 192064, "start": 1940.8000000000002, "end": 1947.68, "text": " there was a previous client made by Element called Hydrogen. That was a web client and that could do", "tokens": [51372, 456, 390, 257, 3894, 6423, 1027, 538, 20900, 1219, 24231, 7747, 13, 663, 390, 257, 3670, 6423, 293, 300, 727, 360, 51716], "temperature": 0.0, "avg_logprob": -0.10965976909715303, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.006836786866188049}, {"id": 289, "seek": 194768, "start": 1947.76, "end": 1953.92, "text": " that and had the fancy system to actually index the messages on your client and then share", "tokens": [50368, 300, 293, 632, 264, 10247, 1185, 281, 767, 8186, 264, 7897, 322, 428, 6423, 293, 550, 2073, 50676], "temperature": 0.0, "avg_logprob": -0.09457470575968424, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.006850189529359341}, {"id": 290, "seek": 194768, "start": 1955.3600000000001, "end": 1961.1200000000001, "text": " parts of the index with your other clients devices. So we're probably going to reuse and", "tokens": [50748, 3166, 295, 264, 8186, 365, 428, 661, 6982, 5759, 13, 407, 321, 434, 1391, 516, 281, 26225, 293, 51036], "temperature": 0.0, "avg_logprob": -0.09457470575968424, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.006850189529359341}, {"id": 291, "seek": 194768, "start": 1961.1200000000001, "end": 1966.3200000000002, "text": " reimplement some of that in the rest SDK at some point. Yeah. In terms of what features are out", "tokens": [51036, 33433, 43704, 512, 295, 300, 294, 264, 1472, 37135, 412, 512, 935, 13, 865, 13, 682, 2115, 295, 437, 4122, 366, 484, 51296], "temperature": 0.0, "avg_logprob": -0.09457470575968424, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.006850189529359341}, {"id": 292, "seek": 194768, "start": 1966.3200000000002, "end": 1975.52, "text": " of scope for the rest SDK, it's kind of hard to tell, but I think that everything that is like", "tokens": [51296, 295, 11923, 337, 264, 1472, 37135, 11, 309, 311, 733, 295, 1152, 281, 980, 11, 457, 286, 519, 300, 1203, 300, 307, 411, 51756], "temperature": 0.0, "avg_logprob": -0.09457470575968424, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.006850189529359341}, {"id": 293, "seek": 197552, "start": 1976.48, "end": 1983.52, "text": " high level UI related like rendering widgets, but not in the sense of widget API, but actually", "tokens": [50412, 1090, 1496, 15682, 4077, 411, 22407, 43355, 11, 457, 406, 294, 264, 2020, 295, 34047, 9362, 11, 457, 767, 50764], "temperature": 0.0, "avg_logprob": -0.107345602978235, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.001937219058163464}, {"id": 294, "seek": 197552, "start": 1983.52, "end": 1988.16, "text": " UI widgets and stuff like that is not something that we want to implement or provide.", "tokens": [50764, 15682, 43355, 293, 1507, 411, 300, 307, 406, 746, 300, 321, 528, 281, 4445, 420, 2893, 13, 50996], "temperature": 0.0, "avg_logprob": -0.107345602978235, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.001937219058163464}, {"id": 295, "seek": 197552, "start": 1990.24, "end": 1997.92, "text": " And then I think that, well, the MSCs that have proven to be features that have been", "tokens": [51100, 400, 550, 286, 519, 300, 11, 731, 11, 264, 7395, 33290, 300, 362, 12785, 281, 312, 4122, 300, 362, 668, 51484], "temperature": 0.0, "avg_logprob": -0.107345602978235, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.001937219058163464}, {"id": 296, "seek": 197552, "start": 1997.92, "end": 2003.6, "text": " proven to be not very useful will probably not be implemented. It's not clear what's not in the", "tokens": [51484, 12785, 281, 312, 406, 588, 4420, 486, 1391, 406, 312, 12270, 13, 467, 311, 406, 1850, 437, 311, 406, 294, 264, 51768], "temperature": 0.0, "avg_logprob": -0.107345602978235, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.001937219058163464}, {"id": 297, "seek": 200360, "start": 2003.6, "end": 2011.4399999999998, "text": " roadmap at this point. Sorry, it's not a very satisfying answer, but yes, question here.", "tokens": [50364, 35738, 412, 341, 935, 13, 4919, 11, 309, 311, 406, 257, 588, 18348, 1867, 11, 457, 2086, 11, 1168, 510, 13, 50756], "temperature": 0.0, "avg_logprob": -0.21910375356674194, "compression_ratio": 1.1, "no_speech_prob": 0.0059929764829576015}, {"id": 298, "seek": 203360, "start": 2034.1599999999999, "end": 2050.08, "text": " So the question was using the rest SDK can store a lot of data if you're listening to lots of events", "tokens": [50392, 407, 264, 1168, 390, 1228, 264, 1472, 37135, 393, 3531, 257, 688, 295, 1412, 498, 291, 434, 4764, 281, 3195, 295, 3931, 51188], "temperature": 0.0, "avg_logprob": -0.14819974369472927, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.001526674604974687}, {"id": 299, "seek": 203360, "start": 2050.08, "end": 2056.7999999999997, "text": " and is there any way to limit that amount of data that is stored on this? Well, as I was saying,", "tokens": [51188, 293, 307, 456, 604, 636, 281, 4948, 300, 2372, 295, 1412, 300, 307, 12187, 322, 341, 30, 1042, 11, 382, 286, 390, 1566, 11, 51524], "temperature": 0.0, "avg_logprob": -0.14819974369472927, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.001526674604974687}, {"id": 300, "seek": 203360, "start": 2056.7999999999997, "end": 2062.72, "text": " the storage is implemented as a trait. So one could always implement a different version of the", "tokens": [51524, 264, 6725, 307, 12270, 382, 257, 22538, 13, 407, 472, 727, 1009, 4445, 257, 819, 3037, 295, 264, 51820], "temperature": 0.0, "avg_logprob": -0.14819974369472927, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.001526674604974687}, {"id": 301, "seek": 206272, "start": 2062.72, "end": 2068.3199999999997, "text": " Scallot backend and decide to drop items at some point. One thing that we wanted to make", "tokens": [50364, 2747, 336, 310, 38087, 293, 4536, 281, 3270, 4754, 412, 512, 935, 13, 1485, 551, 300, 321, 1415, 281, 652, 50644], "temperature": 0.0, "avg_logprob": -0.13458650207519532, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.007351133041083813}, {"id": 302, "seek": 206272, "start": 2068.3199999999997, "end": 2074.16, "text": " it is the ability to store events locally. And that's connected to the previous question. If you", "tokens": [50644, 309, 307, 264, 3485, 281, 3531, 3931, 16143, 13, 400, 300, 311, 4582, 281, 264, 3894, 1168, 13, 759, 291, 50936], "temperature": 0.0, "avg_logprob": -0.13458650207519532, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.007351133041083813}, {"id": 303, "seek": 206272, "start": 2074.16, "end": 2080.24, "text": " want to be able to do full text search, well, you have no other choice, but decrypting all the events", "tokens": [50936, 528, 281, 312, 1075, 281, 360, 1577, 2487, 3164, 11, 731, 11, 291, 362, 572, 661, 3922, 11, 457, 979, 627, 662, 278, 439, 264, 3931, 51240], "temperature": 0.0, "avg_logprob": -0.13458650207519532, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.007351133041083813}, {"id": 304, "seek": 206272, "start": 2080.24, "end": 2085.2799999999997, "text": " and storing them locally, at least in memory for some time to do the indexing. And then the indexes", "tokens": [51240, 293, 26085, 552, 16143, 11, 412, 1935, 294, 4675, 337, 512, 565, 281, 360, 264, 8186, 278, 13, 400, 550, 264, 8186, 279, 51492], "temperature": 0.0, "avg_logprob": -0.13458650207519532, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.007351133041083813}, {"id": 305, "seek": 206272, "start": 2085.2799999999997, "end": 2091.2, "text": " have to go to the disk. And that means that, yeah, the size of the index can grow a lot.", "tokens": [51492, 362, 281, 352, 281, 264, 12355, 13, 400, 300, 1355, 300, 11, 1338, 11, 264, 2744, 295, 264, 8186, 393, 1852, 257, 688, 13, 51788], "temperature": 0.0, "avg_logprob": -0.13458650207519532, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.007351133041083813}, {"id": 306, "seek": 209120, "start": 2091.7599999999998, "end": 2097.04, "text": " And so we would probably have to implement some kind of garbage collection and say, well, we kind", "tokens": [50392, 400, 370, 321, 576, 1391, 362, 281, 4445, 512, 733, 295, 14150, 5765, 293, 584, 11, 731, 11, 321, 733, 50656], "temperature": 0.0, "avg_logprob": -0.19370272755622864, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.006986833643168211}, {"id": 307, "seek": 209120, "start": 2097.04, "end": 2104.3199999999997, "text": " of forget about like old data, older than like a month, year or something like that. And we only care", "tokens": [50656, 295, 2870, 466, 411, 1331, 1412, 11, 4906, 813, 411, 257, 1618, 11, 1064, 420, 746, 411, 300, 13, 400, 321, 787, 1127, 51020], "temperature": 0.0, "avg_logprob": -0.19370272755622864, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.006986833643168211}, {"id": 308, "seek": 209120, "start": 2104.3199999999997, "end": 2116.3199999999997, "text": " about the most recent data. All right, thank you very much.", "tokens": [51020, 466, 264, 881, 5162, 1412, 13, 1057, 558, 11, 1309, 291, 588, 709, 13, 51620], "temperature": 0.0, "avg_logprob": -0.19370272755622864, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.006986833643168211}], "language": "en"}