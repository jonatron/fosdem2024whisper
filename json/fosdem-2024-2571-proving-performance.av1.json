{"text": " So now we have Nikolai Vasquez who's come all the way from Atlanta to tell us about how we can improve performance in our Rust programs and give him your attention and it's going to be a really good talk. Take it away. Thank you very much. So, yep. Hi, I'm Nikolai Vasquez. Some of you might be familiar with my work in the Rust community such as the static assertions crate or recently Devon which is a benchmarking library that I'll be discussing in this talk. And so this title I realize is a bit of a misnomer. You can't really prove performance. Like there's just various factors that make this impossible such as for example there's various system conditions that could affect performance depending on machines and you could be working over different data sets. And so rather than considering this as proving performance, this is more like getting a vibe for performance. And so by show of hands how many people are familiar with measuring performance of their code? All right, so the vast majority. Great. All right, so you're all experts and you don't need me. So I know you probably know this but when we discuss what performance means in software, we're usually talking about how fast it is but to me in broader terms performance is more about how software uses resources to achieve a desired result. So along with thinking about the time that's being spent in our software, we should also be considering the amount of memory that it's using. I think that's a very important aspect of performance. And so making good software can be a balancing act of trade-offs between time and space and so it can be a bit difficult. As developers, the way that we write code can have a pretty direct impact on the performance of our software. So for example, we could be using really inefficient algorithms with a time or space complexity of O of n squared O of 2 to the n or whatever that yellow line might be. We might also be repeating computations instead of saving previous results. We could be choosing to use slower operating system APIs. So for example, waiting on sockets in Linux with the select system call versus ePoll. But also, performance can be bad for reasons that are out of your direct control as a developer. So at a micro level, for example, the CPU might not have cached the data that you're requesting and instead it will have to reach for main memory. The CPU might also expect the wrong branch to be taken and it won't speculatively execute the correct branch as well as the CPU might be waiting on previous computations before executing subsequent code. And then at the macro level, looking out, other cases might be that the network has really poor latency or bandwidth. Other processes could be using excessive amounts of RAM and that can cause DOS to swap memory to disk or your storage might be on a slow device like spinning hard drives instead of SSDs. So when it comes to performance, why do people choose Rust? I believe that the central reason to pick Rust for performance is it's in its community. I find that the community's culture of performance has led to many zero cost abstractions ranging from async away in the compiler to very fast standard library APIs. And we also see this culture in third party libraries. So people will try to make their code work really well and constrain environments in the embedded space or people will focus their attention on how well they're using time and space. So how fast their code is and how much memory it's using. And as well as the community has developed many tools to measure performance. So this really does speak to the culture. And now that we have a sense for what performance is, how do we go about measuring it? So for the sake of simplicity, I'll only be covering things that can be implemented with the Rust standard library. I'm not going to be covering. For example, hardware performance counters because each operating system has different APIs for that and usually accessing them requires root privileges and that can be difficult. So let's consider a simple operation such as allocating a vector of 180 integers. We could try timing it by using the standard libraries instant type and this is generally considered an all right approach. But the results may be surprising. It might just say zero nanoseconds. And so why is this happening? Well, it turns out that the compiler is smart enough to realize that the value wasn't actually used and so it optimizes out the allocation. And so when you're benchmarking, you really should pretend or at least trick the compiler into believing that the value is being used and so the standard library provides a black box function and you can use that to prevent the compiler from optimizing code that you want to run. And I find that a lot of people don't reach for this when they should. And so now that we're using this, we're actually getting higher timings that are more realistic and this is evidence that we're actually now measuring the allocation time. But why 500 nanoseconds? How consistent or accurate is this timing? Well, it turns out that if we run our code repeatedly, the times can fluctuate greatly. So the numbers might vary because of noisy system conditions or some of the things that I mentioned earlier. And you might wonder, well, OK, then how can we get a better sense of our code speed? And you could dive into existing solutions. What I generally recommend for practicality's sake is you should use an existing library that implements this correctly. And so recently I created this library, Devon, that is for exactly this. I wanted to make a tool that makes it very easy to do correct measurements and be able to compare various pieces of Rust code. And so to me, I would say Devon is so easy that it's a comfy bench marking library because a Devon sofa is like a comfy bench. And so that's why I named it that. You can read a bit more about it on the announcement blog post that I have on my website. But I'll also dive into what Devon can do for us today. And so I wanted to make Devon really easy to use. And I wanted the way to register benchmarks to be very simple. So I came up with this very simple yet powerful attribute macro that behind the scenes will generate the code that's needed to benchmark a function. And this might look familiar because this is also how you register unit tests in Rust. And like unit tests, registering benchmarks with Devon can also be done anywhere, not just in the same crate as your benchmarking runner. And you can also, well, I also take advantage of this feature within Devon by measuring internals of Devon with Devon, which is kind of meta. And so given the previous benchmark that we wrote, it's pretty straightforward to adapt it to Devon. We just stick it in a function and then mark it as bench. And then Devon will be able to run that. And after executing our benchmark, Devon presents us with pretty succinct information about how it ran. On this, we can see that the best speed was measured at about 70 nanoseconds. And this realistically represents how fast the function would perform under ideal conditions. And we also see that the worst case was measured at about 200 nanoseconds. And so there's various things that could play into that. It might not be necessarily the code itself, but the situation around the code. And then we also have median and mean, which represent the average time that the function took to run. And we can also see that these values are pretty close to the fastest sample. So we can be fairly confident that this function will generally perform at this speed, at least on this machine. And so to give some insight into how Devon is running this code, we see that it's reporting the number of samples and total iterations. And this represents how many timings, samples represents how many timings Devon has measured. And then iterations is the number of repetitions across all the timings or all the samples. And if we divide the iteration count by the sample count, we end up getting what I call the sample size, which is how many iterations per sample. And so we see that each sample took about 64 iterations. This is chosen dynamically at runtime based on how much time is spent in earlier samples. And this number can be higher for faster functions, or it can be as low as just one iteration per sample for really slow functions. But if we want to measure not only the time to allocate a vector, or sorry, if we only want to measure the time to allocate a vector and not the time to deallocate it, then the way this would work in Devon is you simply return the created value from the benchmark function. And this will defer freeing the vector until after the sample is finished being timed. And since Devon will automatically black box the returned value, we can actually remove the black box from our function. And this just makes it a lot easier to read. And so since we're measuring vector allocation but not deallocation, now our benchmark results are about half the time that we measured before. And so far we've only been benchmarking allocating vectors that contain 100 integers, but we can also benchmark across other cases. So we can use the args option to measure across one, five, 10, 1,000, you name it, any value that can be provided as an input. And this, I find it's generally very good practice to measure across various cases to get a better sense of how your code's running. And we can see that generally as expected, as the size increases, the benchmark also slows down. But interestingly enough, for cases that are at 10 or smaller, there's not really a difference in performance. And so really the differences, I would say, are more like systemic noise because it doesn't really make sense that creating five values in a vector takes longer than creating 10, at least not consistently so. And we also notice that this function really starts to slow down a lot at bigger sizes. And so that aligns with whatever hypothesis we might have had about this benchmark before. But we can also compare the performance across multiple types by making the function generic. And then we can provide a types option to pass all the cases. So now this benchmark is not only running the standard libraries vector type, but it's also comparing that against SmallVec using all the same cases as before. And for those who aren't familiar, SmallVec is a type that's designed to be faster for smaller sizes. And it does this by storing values on the stack instead of doing a heap allocation. But once there's not enough space on the stack, it'll fall back to using the standard libraries vector, or rather it'll use the heap like the standard libraries vector. And so to make what's happening a bit clearer, Devon's not actually doing anything special to the function. This is just normal generic code that's pretty common to write. Instead Devon is using the function as is to generate the benchmarking code for each type that's passed into the attribute. And so once we run this, we have this nice tree output and table where we see that Devon has grouped the types as separate trees under the benchmark function's name. And we can also see from these measurements that, at least for this specific operation collecting from a range, SmallVec is faster than the standard libraries vector when the number of items fits on the stack. However, once a size grows beyond fitting on the stack, once SmallVec needs to do heap allocations, interestingly enough the standard libraries vector is faster. And I imagine this is because the standard libraries vector can do nice optimizations like specialization, which if any of you can make that stable, please. I've been waiting forever. But also when we think about software performance, like I mentioned earlier, we shouldn't only be considering speed and we should also be considering the amount of space that's being used. And normally if you're profiling a long running program, keeping track of allocations with a tool like DHAT, the cost there is relatively low because it gets amortized generally over the life of the program. And the nice thing about tools like DHAT is that it'll collect back traces to tell you exactly where your allocations are happening. So it does give you a lot of information. However, in microbenchmarks, when the time spent tracking allocations, like that can have a noticeable impact. So taking back traces can take microseconds, whereas the code we want to measure may just be a few nanoseconds. And so we would be totally blowing out the timings. And in a sense, by observing the behavior of our program, we've now also affected our measurements. So is it possible to gather insights without affecting measurements? Is it possible to reduce the amount of time spent here? Well, I actually managed to do that. So Devon has a custom allocator that will only track the number of bytes allocated and the number of allocations during benchmarks. This applies to allocations, the allocation, reallocation of grow or shrink. And the way that you use this is you override the global allocator with Devon's allocrofiler. But you can also pass a custom allocator if in reality you are going to be using a faster allocator such as meme alloc. And so it's fairly flexible. So once we've registered this allocator and we rerun the same benchmarks as before, we can see which cases are allocating and how many times. And notice that we are not seeing the allocation listed here because, like I mentioned before, we're returning the created value from the benchmark. And so that's being dropped after the sample is run. And I also want to note that the timings here are the same as before we did any allocation profiling. I managed to optimize this to a point that its footprint is pretty indistinguishable front noise by using thread local storage and then optimizing that further, at least in the case of Mac OS. So if we look a little closer, we can see that, yeah, for smaller sizes, indeed, small back is not going to be performing any heap allocations and is strictly doing its operations with the stack. We can also tell Devon the number of bytes we're processing or number of items we're processing. And this allows us to get a pretty different perspective. The way we do this gets a little more complicated. We change our function to take a venture argument and then we call the counter method on that and we pass it an instance of bytes count. In this case, we're saying that we're counting n 32-bit integers and then we pass a closure to benchmark our function from iterator implementation. So, we then see that Devon will output the number of bytes being processed in terms of, in this case, megabytes or gigabytes per second. And for a lot of people, this might be an easier data point to get an intuition for the speed rather than just the strict timing numbers. For some people, saying growing numbers for better performance is just easier to think about. So to recap what I just covered, Devon has various features that really set it apart from existing solutions. I find that its API is just a lot simpler. It's easier to remember. I also really like how the compact output makes it pretty easy to consider various cases. And as well as because you can parameterize your benchmarks across various cases, you can really just get a sense for the difference in performance depending on the scenario. So I also really like that by going with an attribute macro, I realize that, oh, well, if you make the function generic, you can just pass the types in because you're just parsing whatever you want as the options. And so you can have benchmarks over various collections of the standard libraries, so linked list, VEC, hash set, et cetera. And you can see how different operations really differ between those collections. So such operations that are pretty common like clear might be a lot slower on a linked list whereas on a VEC, it's pretty instant. And another feature that helps me a lot is thinking of the numbers in terms of throughput. I find that it tends to just be easier to understand than durations. As well as something that I find no existing tool out there does is you can track the number of heap allocations at the same time that you're measuring the time being spent running your benchmark. As well as one feature I didn't mention here because I thought it might be a little complex to cover is you can do some interesting things like run benchmarks over multiple threads and this allows you to measure the effects of contention on atomics and locks. So if you're developing a low-level synchronization library, you might find this to be pretty useful. I also want to cover what motivated me to pursue this. I found that a lot of existing tools in space were pretty good but their APIs had some, in my opinion, unnecessary complexity and so I wanted an API that didn't go too far beyond the complexity of the code that you're benchmarking itself. And I really appreciated that by trying this new API, open up new possibilities such as what I mentioned before with benchmarking generic functions, it was relatively straightforward to implement that which was a bit of a surprise. So some food for thought if you're developing your own libraries. And I also found that the default way that some other tools run is pretty slow and I get why they're trying to do a lot of statistical analysis to remove outliers. But there are some cases where you do actually want to know when the code was especially slow because if you're benchmarking over various inputs, it's possible that one case just happened to create a really large string. And so you want to be able to get a sense for everything that happened, not just the best case scenarios in my opinion. And if you do want to run your benchmarks for longer, have larger sample sizes, more samples, there's also options to do that. So you're not restricted. I also want to mention some other Rust performance measuring tools that I think are very much worth considering. So criterion is obviously the current go to Rust benchmarking library. A feature that I really particularly like about it is its graph output because I'm a very visual person. I also do graphic design. Another newer micro benchmarking library is Tango. And what I find unique about it is that it has this technique called paired benchmarking where the execution gets interleaved between benchmarks. And what this does is it spreads whatever systemic negative conditions evenly across your benchmarks. And so this is certainly a feature I eventually want to have in Devon. Currently my APIs tries to prevent requiring ownership of the closure you're passing in. I might have to change that to make this work. I don't know. I think if we had co-routines, I could make it work. But I don't know. Maybe if someone knows how to abuse asynchol weight to get co-routines, please talk to me. Another very useful tool is flame graphs. This is more of a general technique that's well known across the industry. There's plenty of blog posts about this. But for those who aren't familiar, it's a visualization tool that really helps you find where to focus your time. And I think it's very important to find where the bottleneck in your code is before you actually start picking at specific places to optimize and do microbenchmarks on. So try to reach for profiling with flame graphs before you do microbenchmarking, if you can. As well as there's the DHAT crate. And like I mentioned before, every single time an allocation operation happens, it takes a back trace. And so it's able to give you pretty deep insights about where you're allocating memory and how you're allocating memory. It's also able to do some other stuff such as tracking max heap usage at a time. I'm going to try to add that to Devon, but unfortunately it adds a bit of overhead. So maybe it's possible to subtract that overhead from the timings. We'll see. And so some thoughts I want to leave you with is if you're going to be doing reaching for microbenchmarking tools like criterion, Devon, Tango, really figure out if it's worth microoptimizing that kind of code, just try to find the meteor performance issues in your program. So like I mentioned, flame graphs are particularly good for that. And also rather than having standalone benchmarks, you should be comparing it between different cases so you can measure across different inputs and implementations. So like I showed before, with Devon, you can benchmark origin error functions. And this also, for example, in the case of small vec versus vec, really gives you a better sense of is it really worth it to optimize your code using unsafe? And so try to find the specific scenarios where you actually are getting those wins because no one likes nasal demons. And also when making your own projects, since I imagine many people here are contributors to open source and have their own stuff that they're proud of, I really strongly advise that you don't let perfect be the enemy of good. Devon has a lot of features that criterion doesn't have, but also vice versa. Devon doesn't have graphs or machine readable output yet. I do eventually want to get there, but I didn't let that stop me from publishing something that I felt was good that people might want to use. And so try to focus on the features that matter to you most or at least are the most academically interesting. Not everything needs to be a full-fledged product. Definitely try to pursue your interests when making your own projects. Always remember that you can fill in the gaps later if you want. So that's all I had for this. I plan to have questions, but also in the meantime, you can read more about me. And currently I just have one blog post on there about Devon. I plan to publish another thing on kind of like std-conditional T in C++, but in Rust, which is as cursed as it sounds if you're familiar with std-conditional T. You can also follow me on mastodon or Twitter if I refuse to call it X. You can check out the code for Devon. Please give it a star. Play around with it. And yeah, if you want to reach out to me, I'm pretty accessible through mastodon. So there I'm hacky-derm at Nicolai. So yeah, any questions? We do have ten minutes for questions, so I'll plant you. Just raise your hand. I'm going to come to you. So Nicolai, thanks for your talk first. Very nice. And I have two questions. The first question is, have you thought about integrating into CI CD, so continuous integration things? That like, to me it seemed like this is a very handy tool with that, which I can use if I have a problem at hand, which I want to analyze. I quickly can do some benchmark and then dig deeper. But I think if I have found an issue in a very specific place, I might also want to have a test case out of it that I can monitor or be alarmed in my CI CD if there is an issue again. So that was the first question. And the second question would be, is it possible to run all those benchmarks in parallel, or do you have to sequentialize them in order to get the measurements right? Both great questions. So right now, what's blocking getting a lot of value out of running your benchmarks in CI is that Devon doesn't yet have programmatic output. My plans have JSON output and maybe some other format, if that makes sense. But yeah, as well as, so if you have programmatic output, then Devon can then consume previous iterations of that if you're benchmarking across different branches. As well as the author of Tango was exploring some ideas of using a shared library to compile it against four different branches and to make that pretty straightforward with get-of-actions. So yes, I'm definitely very interested in that. Sorry, repeat the second question. The second question was regarding the execution. If you are able to execute more than one benchmark in parallel, and whether there's some impact on the measurement itself. Yeah, so while technically you can, I find that putting the current process under more load could just negatively affect your timings. And so it didn't seem reasonable to do that, although I haven't actually measured if that would actually have as big of a negative impact as I would expect. Thank you. One question I had is, is there a way to compare the execution time with and without the warm cache? That is, the impact of cache on some data structures can be huge. And sometimes in benchmarking, in micro benchmarking especially, you have the problem that you're reusing the same lines. So the second benchmark is going to be faster always. But maybe your use case is actually the one in which the cache is called, for instance. Yeah, great question as well. So I considered having a helper effect function to evict the CPU caches, although I haven't thought of a good way of documenting when this is best to use. But in lieu of that, you can apply as a method on the Bencher type. You can pass a closure to generate inputs for every single iteration. And so if you wanted to, you could create a new buffer for every single time that function is being run at your benchmarking. So since that would be in a different place in memory, the cache effects wouldn't make the benchmark seem so much faster than it might be in a real world case. So we have a question from the matrix. So people are... Oh, Neo has a question. People are following online. So it's a really good topic. It was a very good talk. The question is, thanks. Devan looks very interesting and the API looks much cleaner, simpler than Criterions. Now Criterion can compare across different runs or implementations and then summarize whether performance improved or got worse. Within some confidence interval. Does Devan have something similar or plan to? Yeah, so it currently does not. I found that I kind of shoehorned myself a bit with this output format in that it's not super easy to add a lot more information to that. And so it's kind of like has become a bit of a UI problem in a way, which I find interesting given that's a command line. But yeah, I would very much like to just directly tell the user that this is faster or slower than previous runs. There's also the issue that, for example, if I have my laptop plugged in, now my benchmark runs faster. If I don't have it plugged in, then it's slower. It gets throttled. So it's not always obvious that there was a change in the implementation that caused the function to get slower. And I believe that Criterion's documentation has like a big warning section about this issue. But yeah, I do think that is valuable to have and I would like to have it. And also, if you all are actually very interested in this project, feel free to submit ideas to the GitHub page for it or better pull requests, implement the features that you'd like to see. I'm only one person and only have so many hours in a day. Yeah, I have two questions. The first one was you mentioned that some of the flaws or design differences with Criterion was that it focused a lot on very, I don't know, horrible. It's a lot in statistics and instead of just giving you the fastest, the average and all of that. I was wondering if there is a mechanism in Devon to output, like for example, percentiles or something like that. And my second question was when your benchmarking memory, if the function you're benchmarking the allocates instead of returning all the memory that it allocated, would the output show the total memory allocated or just the memory remaining when the function returned? Yeah, so any allocations that happen before the benchmark runs or not, they will or they will not be visible to Devon in a sense. It will have recorded that but it won't be associated with the current benchmark. It will just get cleared before the benchmark runs. So in that case, you would see that the number of the allocations would be greater than the number of allocations in the benchmark. And to answer your first question, when you say percentiles, are you talking about confidence intervals? Well, no, I mean, it would be also an option but the first thing that came to mind to me was percentiles. Like when you order the outputs, like the timings in the sending order, which for example, I don't know how to describe it right now, but yes, which was the 95th. If you did 100 runs, which was the 95th fastest or slowest, for example. Yeah. So I would like to have more interesting statistics output. Right now, I was just focused on having what I considered was the most important for your average benchmarks. Like I'd also like to output what the variance is across all the samples. So again, I kind of painted myself into a bit of a corner in that people usually only have so many columns in their terminal. And so this table output will be interesting to see how I add to it. I think what I'll end up doing is have options for emitting the columns that you're interested in having and just have certain columns by default. So when I do end up getting around to having more interesting statistics, that'll probably lead the way to make a user configurable of whether you end up with a giant table or not. Okay. Thank you very much for your talk and your answers. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.6, "text": " So now we have Nikolai Vasquez who's come all the way from Atlanta to tell us about", "tokens": [50364, 407, 586, 321, 362, 13969, 401, 1301, 23299, 39009, 567, 311, 808, 439, 264, 636, 490, 20225, 281, 980, 505, 466, 50894], "temperature": 0.0, "avg_logprob": -0.3269579943488626, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.15376991033554077}, {"id": 1, "seek": 0, "start": 10.6, "end": 16.68, "text": " how we can improve performance in our Rust programs and give him your attention and it's", "tokens": [50894, 577, 321, 393, 3470, 3389, 294, 527, 34952, 4268, 293, 976, 796, 428, 3202, 293, 309, 311, 51198], "temperature": 0.0, "avg_logprob": -0.3269579943488626, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.15376991033554077}, {"id": 2, "seek": 0, "start": 16.68, "end": 18.36, "text": " going to be a really good talk.", "tokens": [51198, 516, 281, 312, 257, 534, 665, 751, 13, 51282], "temperature": 0.0, "avg_logprob": -0.3269579943488626, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.15376991033554077}, {"id": 3, "seek": 0, "start": 18.36, "end": 19.36, "text": " Take it away.", "tokens": [51282, 3664, 309, 1314, 13, 51332], "temperature": 0.0, "avg_logprob": -0.3269579943488626, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.15376991033554077}, {"id": 4, "seek": 0, "start": 19.36, "end": 21.0, "text": " Thank you very much.", "tokens": [51332, 1044, 291, 588, 709, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3269579943488626, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.15376991033554077}, {"id": 5, "seek": 0, "start": 21.0, "end": 22.0, "text": " So, yep.", "tokens": [51414, 407, 11, 18633, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3269579943488626, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.15376991033554077}, {"id": 6, "seek": 0, "start": 22.0, "end": 24.32, "text": " Hi, I'm Nikolai Vasquez.", "tokens": [51464, 2421, 11, 286, 478, 13969, 401, 1301, 23299, 39009, 13, 51580], "temperature": 0.0, "avg_logprob": -0.3269579943488626, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.15376991033554077}, {"id": 7, "seek": 2432, "start": 24.32, "end": 32.96, "text": " Some of you might be familiar with my work in the Rust community such as the static assertions", "tokens": [50364, 2188, 295, 291, 1062, 312, 4963, 365, 452, 589, 294, 264, 34952, 1768, 1270, 382, 264, 13437, 19810, 626, 50796], "temperature": 0.0, "avg_logprob": -0.2211043325702796, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.001344373682513833}, {"id": 8, "seek": 2432, "start": 32.96, "end": 40.6, "text": " crate or recently Devon which is a benchmarking library that I'll be discussing in this talk.", "tokens": [50796, 42426, 420, 3938, 9096, 266, 597, 307, 257, 18927, 278, 6405, 300, 286, 603, 312, 10850, 294, 341, 751, 13, 51178], "temperature": 0.0, "avg_logprob": -0.2211043325702796, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.001344373682513833}, {"id": 9, "seek": 2432, "start": 40.6, "end": 45.72, "text": " And so this title I realize is a bit of a misnomer.", "tokens": [51178, 400, 370, 341, 4876, 286, 4325, 307, 257, 857, 295, 257, 3346, 77, 14301, 13, 51434], "temperature": 0.0, "avg_logprob": -0.2211043325702796, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.001344373682513833}, {"id": 10, "seek": 2432, "start": 45.72, "end": 48.400000000000006, "text": " You can't really prove performance.", "tokens": [51434, 509, 393, 380, 534, 7081, 3389, 13, 51568], "temperature": 0.0, "avg_logprob": -0.2211043325702796, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.001344373682513833}, {"id": 11, "seek": 2432, "start": 48.400000000000006, "end": 53.68, "text": " Like there's just various factors that make this impossible such as for example there's", "tokens": [51568, 1743, 456, 311, 445, 3683, 6771, 300, 652, 341, 6243, 1270, 382, 337, 1365, 456, 311, 51832], "temperature": 0.0, "avg_logprob": -0.2211043325702796, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.001344373682513833}, {"id": 12, "seek": 5368, "start": 53.68, "end": 61.04, "text": " various system conditions that could affect performance depending on machines and you", "tokens": [50364, 3683, 1185, 4487, 300, 727, 3345, 3389, 5413, 322, 8379, 293, 291, 50732], "temperature": 0.0, "avg_logprob": -0.15751479148864747, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.000983408885076642}, {"id": 13, "seek": 5368, "start": 61.04, "end": 64.12, "text": " could be working over different data sets.", "tokens": [50732, 727, 312, 1364, 670, 819, 1412, 6352, 13, 50886], "temperature": 0.0, "avg_logprob": -0.15751479148864747, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.000983408885076642}, {"id": 14, "seek": 5368, "start": 64.12, "end": 70.2, "text": " And so rather than considering this as proving performance, this is more like getting a vibe", "tokens": [50886, 400, 370, 2831, 813, 8079, 341, 382, 27221, 3389, 11, 341, 307, 544, 411, 1242, 257, 14606, 51190], "temperature": 0.0, "avg_logprob": -0.15751479148864747, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.000983408885076642}, {"id": 15, "seek": 5368, "start": 70.2, "end": 75.88, "text": " for performance.", "tokens": [51190, 337, 3389, 13, 51474], "temperature": 0.0, "avg_logprob": -0.15751479148864747, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.000983408885076642}, {"id": 16, "seek": 7588, "start": 75.88, "end": 84.52, "text": " And so by show of hands how many people are familiar with measuring performance of their", "tokens": [50364, 400, 370, 538, 855, 295, 2377, 577, 867, 561, 366, 4963, 365, 13389, 3389, 295, 641, 50796], "temperature": 0.0, "avg_logprob": -0.25853815386372225, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.00068729865597561}, {"id": 17, "seek": 7588, "start": 84.52, "end": 85.52, "text": " code?", "tokens": [50796, 3089, 30, 50846], "temperature": 0.0, "avg_logprob": -0.25853815386372225, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.00068729865597561}, {"id": 18, "seek": 7588, "start": 85.52, "end": 88.08, "text": " All right, so the vast majority.", "tokens": [50846, 1057, 558, 11, 370, 264, 8369, 6286, 13, 50974], "temperature": 0.0, "avg_logprob": -0.25853815386372225, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.00068729865597561}, {"id": 19, "seek": 7588, "start": 88.08, "end": 89.08, "text": " Great.", "tokens": [50974, 3769, 13, 51024], "temperature": 0.0, "avg_logprob": -0.25853815386372225, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.00068729865597561}, {"id": 20, "seek": 7588, "start": 89.08, "end": 93.75999999999999, "text": " All right, so you're all experts and you don't need me.", "tokens": [51024, 1057, 558, 11, 370, 291, 434, 439, 8572, 293, 291, 500, 380, 643, 385, 13, 51258], "temperature": 0.0, "avg_logprob": -0.25853815386372225, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.00068729865597561}, {"id": 21, "seek": 7588, "start": 93.75999999999999, "end": 100.16, "text": " So I know you probably know this but when we discuss what performance means in software,", "tokens": [51258, 407, 286, 458, 291, 1391, 458, 341, 457, 562, 321, 2248, 437, 3389, 1355, 294, 4722, 11, 51578], "temperature": 0.0, "avg_logprob": -0.25853815386372225, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.00068729865597561}, {"id": 22, "seek": 7588, "start": 100.16, "end": 104.56, "text": " we're usually talking about how fast it is but to me in broader terms performance is", "tokens": [51578, 321, 434, 2673, 1417, 466, 577, 2370, 309, 307, 457, 281, 385, 294, 13227, 2115, 3389, 307, 51798], "temperature": 0.0, "avg_logprob": -0.25853815386372225, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.00068729865597561}, {"id": 23, "seek": 10456, "start": 104.56, "end": 109.4, "text": " more about how software uses resources to achieve a desired result.", "tokens": [50364, 544, 466, 577, 4722, 4960, 3593, 281, 4584, 257, 14721, 1874, 13, 50606], "temperature": 0.0, "avg_logprob": -0.11943656142039966, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.00830806139856577}, {"id": 24, "seek": 10456, "start": 109.4, "end": 114.92, "text": " So along with thinking about the time that's being spent in our software, we should also", "tokens": [50606, 407, 2051, 365, 1953, 466, 264, 565, 300, 311, 885, 4418, 294, 527, 4722, 11, 321, 820, 611, 50882], "temperature": 0.0, "avg_logprob": -0.11943656142039966, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.00830806139856577}, {"id": 25, "seek": 10456, "start": 114.92, "end": 117.84, "text": " be considering the amount of memory that it's using.", "tokens": [50882, 312, 8079, 264, 2372, 295, 4675, 300, 309, 311, 1228, 13, 51028], "temperature": 0.0, "avg_logprob": -0.11943656142039966, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.00830806139856577}, {"id": 26, "seek": 10456, "start": 117.84, "end": 122.28, "text": " I think that's a very important aspect of performance.", "tokens": [51028, 286, 519, 300, 311, 257, 588, 1021, 4171, 295, 3389, 13, 51250], "temperature": 0.0, "avg_logprob": -0.11943656142039966, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.00830806139856577}, {"id": 27, "seek": 10456, "start": 122.28, "end": 129.44, "text": " And so making good software can be a balancing act of trade-offs between time and space and", "tokens": [51250, 400, 370, 1455, 665, 4722, 393, 312, 257, 22495, 605, 295, 4923, 12, 19231, 1296, 565, 293, 1901, 293, 51608], "temperature": 0.0, "avg_logprob": -0.11943656142039966, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.00830806139856577}, {"id": 28, "seek": 10456, "start": 129.44, "end": 133.12, "text": " so it can be a bit difficult.", "tokens": [51608, 370, 309, 393, 312, 257, 857, 2252, 13, 51792], "temperature": 0.0, "avg_logprob": -0.11943656142039966, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.00830806139856577}, {"id": 29, "seek": 13312, "start": 133.48000000000002, "end": 139.0, "text": " As developers, the way that we write code can have a pretty direct impact on the performance", "tokens": [50382, 1018, 8849, 11, 264, 636, 300, 321, 2464, 3089, 393, 362, 257, 1238, 2047, 2712, 322, 264, 3389, 50658], "temperature": 0.0, "avg_logprob": -0.13730121652285257, "compression_ratio": 1.5907335907335907, "no_speech_prob": 0.004460844211280346}, {"id": 30, "seek": 13312, "start": 139.0, "end": 140.48000000000002, "text": " of our software.", "tokens": [50658, 295, 527, 4722, 13, 50732], "temperature": 0.0, "avg_logprob": -0.13730121652285257, "compression_ratio": 1.5907335907335907, "no_speech_prob": 0.004460844211280346}, {"id": 31, "seek": 13312, "start": 140.48000000000002, "end": 145.04, "text": " So for example, we could be using really inefficient algorithms with a time or space complexity", "tokens": [50732, 407, 337, 1365, 11, 321, 727, 312, 1228, 534, 43495, 14642, 365, 257, 565, 420, 1901, 14024, 50960], "temperature": 0.0, "avg_logprob": -0.13730121652285257, "compression_ratio": 1.5907335907335907, "no_speech_prob": 0.004460844211280346}, {"id": 32, "seek": 13312, "start": 145.04, "end": 152.36, "text": " of O of n squared O of 2 to the n or whatever that yellow line might be.", "tokens": [50960, 295, 422, 295, 297, 8889, 422, 295, 568, 281, 264, 297, 420, 2035, 300, 5566, 1622, 1062, 312, 13, 51326], "temperature": 0.0, "avg_logprob": -0.13730121652285257, "compression_ratio": 1.5907335907335907, "no_speech_prob": 0.004460844211280346}, {"id": 33, "seek": 13312, "start": 152.36, "end": 158.96, "text": " We might also be repeating computations instead of saving previous results.", "tokens": [51326, 492, 1062, 611, 312, 18617, 2807, 763, 2602, 295, 6816, 3894, 3542, 13, 51656], "temperature": 0.0, "avg_logprob": -0.13730121652285257, "compression_ratio": 1.5907335907335907, "no_speech_prob": 0.004460844211280346}, {"id": 34, "seek": 13312, "start": 158.96, "end": 161.8, "text": " We could be choosing to use slower operating system APIs.", "tokens": [51656, 492, 727, 312, 10875, 281, 764, 14009, 7447, 1185, 21445, 13, 51798], "temperature": 0.0, "avg_logprob": -0.13730121652285257, "compression_ratio": 1.5907335907335907, "no_speech_prob": 0.004460844211280346}, {"id": 35, "seek": 16180, "start": 161.8, "end": 169.64000000000001, "text": " So for example, waiting on sockets in Linux with the select system call versus ePoll.", "tokens": [50364, 407, 337, 1365, 11, 3806, 322, 370, 11984, 294, 18734, 365, 264, 3048, 1185, 818, 5717, 308, 47, 1833, 13, 50756], "temperature": 0.0, "avg_logprob": -0.17362309651202465, "compression_ratio": 1.5302325581395348, "no_speech_prob": 0.00016335771942976862}, {"id": 36, "seek": 16180, "start": 169.64000000000001, "end": 176.04000000000002, "text": " But also, performance can be bad for reasons that are out of your direct control as a developer.", "tokens": [50756, 583, 611, 11, 3389, 393, 312, 1578, 337, 4112, 300, 366, 484, 295, 428, 2047, 1969, 382, 257, 10754, 13, 51076], "temperature": 0.0, "avg_logprob": -0.17362309651202465, "compression_ratio": 1.5302325581395348, "no_speech_prob": 0.00016335771942976862}, {"id": 37, "seek": 16180, "start": 176.04000000000002, "end": 181.8, "text": " So at a micro level, for example, the CPU might not have cached the data that you're", "tokens": [51076, 407, 412, 257, 4532, 1496, 11, 337, 1365, 11, 264, 13199, 1062, 406, 362, 269, 15095, 264, 1412, 300, 291, 434, 51364], "temperature": 0.0, "avg_logprob": -0.17362309651202465, "compression_ratio": 1.5302325581395348, "no_speech_prob": 0.00016335771942976862}, {"id": 38, "seek": 16180, "start": 181.8, "end": 186.44, "text": " requesting and instead it will have to reach for main memory.", "tokens": [51364, 31937, 293, 2602, 309, 486, 362, 281, 2524, 337, 2135, 4675, 13, 51596], "temperature": 0.0, "avg_logprob": -0.17362309651202465, "compression_ratio": 1.5302325581395348, "no_speech_prob": 0.00016335771942976862}, {"id": 39, "seek": 18644, "start": 186.44, "end": 191.88, "text": " The CPU might also expect the wrong branch to be taken and it won't speculatively execute", "tokens": [50364, 440, 13199, 1062, 611, 2066, 264, 2085, 9819, 281, 312, 2726, 293, 309, 1582, 380, 1608, 425, 19020, 14483, 50636], "temperature": 0.0, "avg_logprob": -0.10969678243001302, "compression_ratio": 1.5432692307692308, "no_speech_prob": 0.0004727152409031987}, {"id": 40, "seek": 18644, "start": 191.88, "end": 198.6, "text": " the correct branch as well as the CPU might be waiting on previous computations before", "tokens": [50636, 264, 3006, 9819, 382, 731, 382, 264, 13199, 1062, 312, 3806, 322, 3894, 2807, 763, 949, 50972], "temperature": 0.0, "avg_logprob": -0.10969678243001302, "compression_ratio": 1.5432692307692308, "no_speech_prob": 0.0004727152409031987}, {"id": 41, "seek": 18644, "start": 198.6, "end": 202.8, "text": " executing subsequent code.", "tokens": [50972, 32368, 19962, 3089, 13, 51182], "temperature": 0.0, "avg_logprob": -0.10969678243001302, "compression_ratio": 1.5432692307692308, "no_speech_prob": 0.0004727152409031987}, {"id": 42, "seek": 18644, "start": 202.8, "end": 209.36, "text": " And then at the macro level, looking out, other cases might be that the network has", "tokens": [51182, 400, 550, 412, 264, 18887, 1496, 11, 1237, 484, 11, 661, 3331, 1062, 312, 300, 264, 3209, 575, 51510], "temperature": 0.0, "avg_logprob": -0.10969678243001302, "compression_ratio": 1.5432692307692308, "no_speech_prob": 0.0004727152409031987}, {"id": 43, "seek": 18644, "start": 209.36, "end": 212.28, "text": " really poor latency or bandwidth.", "tokens": [51510, 534, 4716, 27043, 420, 23647, 13, 51656], "temperature": 0.0, "avg_logprob": -0.10969678243001302, "compression_ratio": 1.5432692307692308, "no_speech_prob": 0.0004727152409031987}, {"id": 44, "seek": 21228, "start": 212.28, "end": 217.56, "text": " Other processes could be using excessive amounts of RAM and that can cause DOS to swap memory", "tokens": [50364, 5358, 7555, 727, 312, 1228, 22704, 11663, 295, 14561, 293, 300, 393, 3082, 413, 4367, 281, 18135, 4675, 50628], "temperature": 0.0, "avg_logprob": -0.1648204709276741, "compression_ratio": 1.469298245614035, "no_speech_prob": 0.005383302923291922}, {"id": 45, "seek": 21228, "start": 217.56, "end": 223.44, "text": " to disk or your storage might be on a slow device like spinning hard drives instead of", "tokens": [50628, 281, 12355, 420, 428, 6725, 1062, 312, 322, 257, 2964, 4302, 411, 15640, 1152, 11754, 2602, 295, 50922], "temperature": 0.0, "avg_logprob": -0.1648204709276741, "compression_ratio": 1.469298245614035, "no_speech_prob": 0.005383302923291922}, {"id": 46, "seek": 21228, "start": 223.44, "end": 226.96, "text": " SSDs.", "tokens": [50922, 30262, 82, 13, 51098], "temperature": 0.0, "avg_logprob": -0.1648204709276741, "compression_ratio": 1.469298245614035, "no_speech_prob": 0.005383302923291922}, {"id": 47, "seek": 21228, "start": 226.96, "end": 232.08, "text": " So when it comes to performance, why do people choose Rust?", "tokens": [51098, 407, 562, 309, 1487, 281, 3389, 11, 983, 360, 561, 2826, 34952, 30, 51354], "temperature": 0.0, "avg_logprob": -0.1648204709276741, "compression_ratio": 1.469298245614035, "no_speech_prob": 0.005383302923291922}, {"id": 48, "seek": 21228, "start": 232.08, "end": 237.24, "text": " I believe that the central reason to pick Rust for performance is it's in its community.", "tokens": [51354, 286, 1697, 300, 264, 5777, 1778, 281, 1888, 34952, 337, 3389, 307, 309, 311, 294, 1080, 1768, 13, 51612], "temperature": 0.0, "avg_logprob": -0.1648204709276741, "compression_ratio": 1.469298245614035, "no_speech_prob": 0.005383302923291922}, {"id": 49, "seek": 23724, "start": 237.72, "end": 243.72, "text": " I find that the community's culture of performance has led to many zero cost abstractions ranging", "tokens": [50388, 286, 915, 300, 264, 1768, 311, 3713, 295, 3389, 575, 4684, 281, 867, 4018, 2063, 12649, 626, 25532, 50688], "temperature": 0.0, "avg_logprob": -0.15932755884916885, "compression_ratio": 1.625, "no_speech_prob": 0.0057234675623476505}, {"id": 50, "seek": 23724, "start": 243.72, "end": 249.92000000000002, "text": " from async away in the compiler to very fast standard library APIs.", "tokens": [50688, 490, 382, 34015, 1314, 294, 264, 31958, 281, 588, 2370, 3832, 6405, 21445, 13, 50998], "temperature": 0.0, "avg_logprob": -0.15932755884916885, "compression_ratio": 1.625, "no_speech_prob": 0.0057234675623476505}, {"id": 51, "seek": 23724, "start": 249.92000000000002, "end": 252.68, "text": " And we also see this culture in third party libraries.", "tokens": [50998, 400, 321, 611, 536, 341, 3713, 294, 2636, 3595, 15148, 13, 51136], "temperature": 0.0, "avg_logprob": -0.15932755884916885, "compression_ratio": 1.625, "no_speech_prob": 0.0057234675623476505}, {"id": 52, "seek": 23724, "start": 252.68, "end": 257.44, "text": " So people will try to make their code work really well and constrain environments in", "tokens": [51136, 407, 561, 486, 853, 281, 652, 641, 3089, 589, 534, 731, 293, 1817, 7146, 12388, 294, 51374], "temperature": 0.0, "avg_logprob": -0.15932755884916885, "compression_ratio": 1.625, "no_speech_prob": 0.0057234675623476505}, {"id": 53, "seek": 23724, "start": 257.44, "end": 263.64, "text": " the embedded space or people will focus their attention on how well they're using time and", "tokens": [51374, 264, 16741, 1901, 420, 561, 486, 1879, 641, 3202, 322, 577, 731, 436, 434, 1228, 565, 293, 51684], "temperature": 0.0, "avg_logprob": -0.15932755884916885, "compression_ratio": 1.625, "no_speech_prob": 0.0057234675623476505}, {"id": 54, "seek": 23724, "start": 263.64, "end": 265.04, "text": " space.", "tokens": [51684, 1901, 13, 51754], "temperature": 0.0, "avg_logprob": -0.15932755884916885, "compression_ratio": 1.625, "no_speech_prob": 0.0057234675623476505}, {"id": 55, "seek": 26504, "start": 265.04, "end": 269.76000000000005, "text": " So how fast their code is and how much memory it's using.", "tokens": [50364, 407, 577, 2370, 641, 3089, 307, 293, 577, 709, 4675, 309, 311, 1228, 13, 50600], "temperature": 0.0, "avg_logprob": -0.10256766355954684, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.252503564814106e-05}, {"id": 56, "seek": 26504, "start": 269.76000000000005, "end": 273.68, "text": " And as well as the community has developed many tools to measure performance.", "tokens": [50600, 400, 382, 731, 382, 264, 1768, 575, 4743, 867, 3873, 281, 3481, 3389, 13, 50796], "temperature": 0.0, "avg_logprob": -0.10256766355954684, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.252503564814106e-05}, {"id": 57, "seek": 26504, "start": 273.68, "end": 279.32000000000005, "text": " So this really does speak to the culture.", "tokens": [50796, 407, 341, 534, 775, 1710, 281, 264, 3713, 13, 51078], "temperature": 0.0, "avg_logprob": -0.10256766355954684, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.252503564814106e-05}, {"id": 58, "seek": 26504, "start": 279.32000000000005, "end": 285.68, "text": " And now that we have a sense for what performance is, how do we go about measuring it?", "tokens": [51078, 400, 586, 300, 321, 362, 257, 2020, 337, 437, 3389, 307, 11, 577, 360, 321, 352, 466, 13389, 309, 30, 51396], "temperature": 0.0, "avg_logprob": -0.10256766355954684, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.252503564814106e-05}, {"id": 59, "seek": 26504, "start": 285.68, "end": 291.16, "text": " So for the sake of simplicity, I'll only be covering things that can be implemented with", "tokens": [51396, 407, 337, 264, 9717, 295, 25632, 11, 286, 603, 787, 312, 10322, 721, 300, 393, 312, 12270, 365, 51670], "temperature": 0.0, "avg_logprob": -0.10256766355954684, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.252503564814106e-05}, {"id": 60, "seek": 26504, "start": 291.16, "end": 292.52000000000004, "text": " the Rust standard library.", "tokens": [51670, 264, 34952, 3832, 6405, 13, 51738], "temperature": 0.0, "avg_logprob": -0.10256766355954684, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.252503564814106e-05}, {"id": 61, "seek": 26504, "start": 292.52000000000004, "end": 294.6, "text": " I'm not going to be covering.", "tokens": [51738, 286, 478, 406, 516, 281, 312, 10322, 13, 51842], "temperature": 0.0, "avg_logprob": -0.10256766355954684, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.252503564814106e-05}, {"id": 62, "seek": 29460, "start": 294.64000000000004, "end": 299.56, "text": " For example, hardware performance counters because each operating system has different", "tokens": [50366, 1171, 1365, 11, 8837, 3389, 39338, 570, 1184, 7447, 1185, 575, 819, 50612], "temperature": 0.0, "avg_logprob": -0.17032358863136984, "compression_ratio": 1.5789473684210527, "no_speech_prob": 8.880603854777291e-05}, {"id": 63, "seek": 29460, "start": 299.56, "end": 307.52000000000004, "text": " APIs for that and usually accessing them requires root privileges and that can be difficult.", "tokens": [50612, 21445, 337, 300, 293, 2673, 26440, 552, 7029, 5593, 32588, 293, 300, 393, 312, 2252, 13, 51010], "temperature": 0.0, "avg_logprob": -0.17032358863136984, "compression_ratio": 1.5789473684210527, "no_speech_prob": 8.880603854777291e-05}, {"id": 64, "seek": 29460, "start": 307.52000000000004, "end": 313.96000000000004, "text": " So let's consider a simple operation such as allocating a vector of 180 integers.", "tokens": [51010, 407, 718, 311, 1949, 257, 2199, 6916, 1270, 382, 12660, 990, 257, 8062, 295, 11971, 41674, 13, 51332], "temperature": 0.0, "avg_logprob": -0.17032358863136984, "compression_ratio": 1.5789473684210527, "no_speech_prob": 8.880603854777291e-05}, {"id": 65, "seek": 29460, "start": 313.96000000000004, "end": 318.04, "text": " We could try timing it by using the standard libraries instant type and this is generally", "tokens": [51332, 492, 727, 853, 10822, 309, 538, 1228, 264, 3832, 15148, 9836, 2010, 293, 341, 307, 5101, 51536], "temperature": 0.0, "avg_logprob": -0.17032358863136984, "compression_ratio": 1.5789473684210527, "no_speech_prob": 8.880603854777291e-05}, {"id": 66, "seek": 29460, "start": 318.04, "end": 320.56, "text": " considered an all right approach.", "tokens": [51536, 4888, 364, 439, 558, 3109, 13, 51662], "temperature": 0.0, "avg_logprob": -0.17032358863136984, "compression_ratio": 1.5789473684210527, "no_speech_prob": 8.880603854777291e-05}, {"id": 67, "seek": 29460, "start": 320.56, "end": 322.52000000000004, "text": " But the results may be surprising.", "tokens": [51662, 583, 264, 3542, 815, 312, 8830, 13, 51760], "temperature": 0.0, "avg_logprob": -0.17032358863136984, "compression_ratio": 1.5789473684210527, "no_speech_prob": 8.880603854777291e-05}, {"id": 68, "seek": 32252, "start": 322.52, "end": 325.2, "text": " It might just say zero nanoseconds.", "tokens": [50364, 467, 1062, 445, 584, 4018, 14067, 541, 28750, 13, 50498], "temperature": 0.0, "avg_logprob": -0.12116131415733924, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.0001794223499018699}, {"id": 69, "seek": 32252, "start": 325.2, "end": 327.68, "text": " And so why is this happening?", "tokens": [50498, 400, 370, 983, 307, 341, 2737, 30, 50622], "temperature": 0.0, "avg_logprob": -0.12116131415733924, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.0001794223499018699}, {"id": 70, "seek": 32252, "start": 327.68, "end": 332.03999999999996, "text": " Well, it turns out that the compiler is smart enough to realize that the value wasn't actually", "tokens": [50622, 1042, 11, 309, 4523, 484, 300, 264, 31958, 307, 4069, 1547, 281, 4325, 300, 264, 2158, 2067, 380, 767, 50840], "temperature": 0.0, "avg_logprob": -0.12116131415733924, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.0001794223499018699}, {"id": 71, "seek": 32252, "start": 332.03999999999996, "end": 335.79999999999995, "text": " used and so it optimizes out the allocation.", "tokens": [50840, 1143, 293, 370, 309, 5028, 5660, 484, 264, 27599, 13, 51028], "temperature": 0.0, "avg_logprob": -0.12116131415733924, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.0001794223499018699}, {"id": 72, "seek": 32252, "start": 335.79999999999995, "end": 341.2, "text": " And so when you're benchmarking, you really should pretend or at least trick the compiler", "tokens": [51028, 400, 370, 562, 291, 434, 18927, 278, 11, 291, 534, 820, 11865, 420, 412, 1935, 4282, 264, 31958, 51298], "temperature": 0.0, "avg_logprob": -0.12116131415733924, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.0001794223499018699}, {"id": 73, "seek": 32252, "start": 341.2, "end": 345.03999999999996, "text": " into believing that the value is being used and so the standard library provides a black", "tokens": [51298, 666, 16594, 300, 264, 2158, 307, 885, 1143, 293, 370, 264, 3832, 6405, 6417, 257, 2211, 51490], "temperature": 0.0, "avg_logprob": -0.12116131415733924, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.0001794223499018699}, {"id": 74, "seek": 32252, "start": 345.03999999999996, "end": 350.96, "text": " box function and you can use that to prevent the compiler from optimizing code that you", "tokens": [51490, 2424, 2445, 293, 291, 393, 764, 300, 281, 4871, 264, 31958, 490, 40425, 3089, 300, 291, 51786], "temperature": 0.0, "avg_logprob": -0.12116131415733924, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.0001794223499018699}, {"id": 75, "seek": 32252, "start": 350.96, "end": 352.08, "text": " want to run.", "tokens": [51786, 528, 281, 1190, 13, 51842], "temperature": 0.0, "avg_logprob": -0.12116131415733924, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.0001794223499018699}, {"id": 76, "seek": 35208, "start": 352.08, "end": 356.91999999999996, "text": " And I find that a lot of people don't reach for this when they should.", "tokens": [50364, 400, 286, 915, 300, 257, 688, 295, 561, 500, 380, 2524, 337, 341, 562, 436, 820, 13, 50606], "temperature": 0.0, "avg_logprob": -0.11021341323852539, "compression_ratio": 1.6048387096774193, "no_speech_prob": 3.3209998946404085e-05}, {"id": 77, "seek": 35208, "start": 356.91999999999996, "end": 362.03999999999996, "text": " And so now that we're using this, we're actually getting higher timings that are more realistic", "tokens": [50606, 400, 370, 586, 300, 321, 434, 1228, 341, 11, 321, 434, 767, 1242, 2946, 524, 1109, 300, 366, 544, 12465, 50862], "temperature": 0.0, "avg_logprob": -0.11021341323852539, "compression_ratio": 1.6048387096774193, "no_speech_prob": 3.3209998946404085e-05}, {"id": 78, "seek": 35208, "start": 362.03999999999996, "end": 368.91999999999996, "text": " and this is evidence that we're actually now measuring the allocation time.", "tokens": [50862, 293, 341, 307, 4467, 300, 321, 434, 767, 586, 13389, 264, 27599, 565, 13, 51206], "temperature": 0.0, "avg_logprob": -0.11021341323852539, "compression_ratio": 1.6048387096774193, "no_speech_prob": 3.3209998946404085e-05}, {"id": 79, "seek": 35208, "start": 368.91999999999996, "end": 370.71999999999997, "text": " But why 500 nanoseconds?", "tokens": [51206, 583, 983, 5923, 14067, 541, 28750, 30, 51296], "temperature": 0.0, "avg_logprob": -0.11021341323852539, "compression_ratio": 1.6048387096774193, "no_speech_prob": 3.3209998946404085e-05}, {"id": 80, "seek": 35208, "start": 370.71999999999997, "end": 373.56, "text": " How consistent or accurate is this timing?", "tokens": [51296, 1012, 8398, 420, 8559, 307, 341, 10822, 30, 51438], "temperature": 0.0, "avg_logprob": -0.11021341323852539, "compression_ratio": 1.6048387096774193, "no_speech_prob": 3.3209998946404085e-05}, {"id": 81, "seek": 35208, "start": 373.56, "end": 378.64, "text": " Well, it turns out that if we run our code repeatedly, the times can fluctuate greatly.", "tokens": [51438, 1042, 11, 309, 4523, 484, 300, 498, 321, 1190, 527, 3089, 18227, 11, 264, 1413, 393, 23448, 10107, 14147, 13, 51692], "temperature": 0.0, "avg_logprob": -0.11021341323852539, "compression_ratio": 1.6048387096774193, "no_speech_prob": 3.3209998946404085e-05}, {"id": 82, "seek": 37864, "start": 378.64, "end": 389.03999999999996, "text": " So the numbers might vary because of noisy system conditions or some of the things that", "tokens": [50364, 407, 264, 3547, 1062, 10559, 570, 295, 24518, 1185, 4487, 420, 512, 295, 264, 721, 300, 50884], "temperature": 0.0, "avg_logprob": -0.13305985650350882, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00019711875938810408}, {"id": 83, "seek": 37864, "start": 389.03999999999996, "end": 390.96, "text": " I mentioned earlier.", "tokens": [50884, 286, 2835, 3071, 13, 50980], "temperature": 0.0, "avg_logprob": -0.13305985650350882, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00019711875938810408}, {"id": 84, "seek": 37864, "start": 390.96, "end": 397.08, "text": " And you might wonder, well, OK, then how can we get a better sense of our code speed?", "tokens": [50980, 400, 291, 1062, 2441, 11, 731, 11, 2264, 11, 550, 577, 393, 321, 483, 257, 1101, 2020, 295, 527, 3089, 3073, 30, 51286], "temperature": 0.0, "avg_logprob": -0.13305985650350882, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00019711875938810408}, {"id": 85, "seek": 37864, "start": 397.08, "end": 401.52, "text": " And you could dive into existing solutions.", "tokens": [51286, 400, 291, 727, 9192, 666, 6741, 6547, 13, 51508], "temperature": 0.0, "avg_logprob": -0.13305985650350882, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00019711875938810408}, {"id": 86, "seek": 37864, "start": 401.52, "end": 405.59999999999997, "text": " What I generally recommend for practicality's sake is you should use an existing library", "tokens": [51508, 708, 286, 5101, 2748, 337, 8496, 507, 311, 9717, 307, 291, 820, 764, 364, 6741, 6405, 51712], "temperature": 0.0, "avg_logprob": -0.13305985650350882, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00019711875938810408}, {"id": 87, "seek": 37864, "start": 405.59999999999997, "end": 408.59999999999997, "text": " that implements this correctly.", "tokens": [51712, 300, 704, 17988, 341, 8944, 13, 51862], "temperature": 0.0, "avg_logprob": -0.13305985650350882, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.00019711875938810408}, {"id": 88, "seek": 40860, "start": 408.6, "end": 413.48, "text": " And so recently I created this library, Devon, that is for exactly this.", "tokens": [50364, 400, 370, 3938, 286, 2942, 341, 6405, 11, 9096, 266, 11, 300, 307, 337, 2293, 341, 13, 50608], "temperature": 0.0, "avg_logprob": -0.17016607342344342, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.0006160051561892033}, {"id": 89, "seek": 40860, "start": 413.48, "end": 419.88, "text": " I wanted to make a tool that makes it very easy to do correct measurements and be able", "tokens": [50608, 286, 1415, 281, 652, 257, 2290, 300, 1669, 309, 588, 1858, 281, 360, 3006, 15383, 293, 312, 1075, 50928], "temperature": 0.0, "avg_logprob": -0.17016607342344342, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.0006160051561892033}, {"id": 90, "seek": 40860, "start": 419.88, "end": 424.16, "text": " to compare various pieces of Rust code.", "tokens": [50928, 281, 6794, 3683, 3755, 295, 34952, 3089, 13, 51142], "temperature": 0.0, "avg_logprob": -0.17016607342344342, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.0006160051561892033}, {"id": 91, "seek": 40860, "start": 424.16, "end": 430.08000000000004, "text": " And so to me, I would say Devon is so easy that it's a comfy bench marking library because", "tokens": [51142, 400, 370, 281, 385, 11, 286, 576, 584, 9096, 266, 307, 370, 1858, 300, 309, 311, 257, 34523, 10638, 25482, 6405, 570, 51438], "temperature": 0.0, "avg_logprob": -0.17016607342344342, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.0006160051561892033}, {"id": 92, "seek": 40860, "start": 430.08000000000004, "end": 434.96000000000004, "text": " a Devon sofa is like a comfy bench.", "tokens": [51438, 257, 9096, 266, 28668, 307, 411, 257, 34523, 10638, 13, 51682], "temperature": 0.0, "avg_logprob": -0.17016607342344342, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.0006160051561892033}, {"id": 93, "seek": 40860, "start": 434.96000000000004, "end": 438.56, "text": " And so that's why I named it that.", "tokens": [51682, 400, 370, 300, 311, 983, 286, 4926, 309, 300, 13, 51862], "temperature": 0.0, "avg_logprob": -0.17016607342344342, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.0006160051561892033}, {"id": 94, "seek": 43856, "start": 438.56, "end": 444.36, "text": " You can read a bit more about it on the announcement blog post that I have on my website.", "tokens": [50364, 509, 393, 1401, 257, 857, 544, 466, 309, 322, 264, 12847, 6968, 2183, 300, 286, 362, 322, 452, 3144, 13, 50654], "temperature": 0.0, "avg_logprob": -0.11170853508843316, "compression_ratio": 1.5765765765765767, "no_speech_prob": 0.00040431669913232327}, {"id": 95, "seek": 43856, "start": 444.36, "end": 452.88, "text": " But I'll also dive into what Devon can do for us today.", "tokens": [50654, 583, 286, 603, 611, 9192, 666, 437, 9096, 266, 393, 360, 337, 505, 965, 13, 51080], "temperature": 0.0, "avg_logprob": -0.11170853508843316, "compression_ratio": 1.5765765765765767, "no_speech_prob": 0.00040431669913232327}, {"id": 96, "seek": 43856, "start": 452.88, "end": 456.36, "text": " And so I wanted to make Devon really easy to use.", "tokens": [51080, 400, 370, 286, 1415, 281, 652, 9096, 266, 534, 1858, 281, 764, 13, 51254], "temperature": 0.0, "avg_logprob": -0.11170853508843316, "compression_ratio": 1.5765765765765767, "no_speech_prob": 0.00040431669913232327}, {"id": 97, "seek": 43856, "start": 456.36, "end": 460.44, "text": " And I wanted the way to register benchmarks to be very simple.", "tokens": [51254, 400, 286, 1415, 264, 636, 281, 7280, 43751, 281, 312, 588, 2199, 13, 51458], "temperature": 0.0, "avg_logprob": -0.11170853508843316, "compression_ratio": 1.5765765765765767, "no_speech_prob": 0.00040431669913232327}, {"id": 98, "seek": 43856, "start": 460.44, "end": 466.76, "text": " So I came up with this very simple yet powerful attribute macro that behind the scenes will", "tokens": [51458, 407, 286, 1361, 493, 365, 341, 588, 2199, 1939, 4005, 19667, 18887, 300, 2261, 264, 8026, 486, 51774], "temperature": 0.0, "avg_logprob": -0.11170853508843316, "compression_ratio": 1.5765765765765767, "no_speech_prob": 0.00040431669913232327}, {"id": 99, "seek": 46676, "start": 466.76, "end": 470.36, "text": " generate the code that's needed to benchmark a function.", "tokens": [50364, 8460, 264, 3089, 300, 311, 2978, 281, 18927, 257, 2445, 13, 50544], "temperature": 0.0, "avg_logprob": -0.10177196545547314, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.033045947551727295}, {"id": 100, "seek": 46676, "start": 470.36, "end": 476.28, "text": " And this might look familiar because this is also how you register unit tests in Rust.", "tokens": [50544, 400, 341, 1062, 574, 4963, 570, 341, 307, 611, 577, 291, 7280, 4985, 6921, 294, 34952, 13, 50840], "temperature": 0.0, "avg_logprob": -0.10177196545547314, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.033045947551727295}, {"id": 101, "seek": 46676, "start": 476.28, "end": 481.59999999999997, "text": " And like unit tests, registering benchmarks with Devon can also be done anywhere, not", "tokens": [50840, 400, 411, 4985, 6921, 11, 47329, 43751, 365, 9096, 266, 393, 611, 312, 1096, 4992, 11, 406, 51106], "temperature": 0.0, "avg_logprob": -0.10177196545547314, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.033045947551727295}, {"id": 102, "seek": 46676, "start": 481.59999999999997, "end": 487.64, "text": " just in the same crate as your benchmarking runner.", "tokens": [51106, 445, 294, 264, 912, 42426, 382, 428, 18927, 278, 24376, 13, 51408], "temperature": 0.0, "avg_logprob": -0.10177196545547314, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.033045947551727295}, {"id": 103, "seek": 46676, "start": 487.64, "end": 493.4, "text": " And you can also, well, I also take advantage of this feature within Devon by measuring internals", "tokens": [51408, 400, 291, 393, 611, 11, 731, 11, 286, 611, 747, 5002, 295, 341, 4111, 1951, 9096, 266, 538, 13389, 2154, 1124, 51696], "temperature": 0.0, "avg_logprob": -0.10177196545547314, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.033045947551727295}, {"id": 104, "seek": 49340, "start": 493.4, "end": 500.12, "text": " of Devon with Devon, which is kind of meta.", "tokens": [50364, 295, 9096, 266, 365, 9096, 266, 11, 597, 307, 733, 295, 19616, 13, 50700], "temperature": 0.0, "avg_logprob": -0.09330382096140008, "compression_ratio": 1.6291079812206573, "no_speech_prob": 0.014942141249775887}, {"id": 105, "seek": 49340, "start": 500.12, "end": 503.76, "text": " And so given the previous benchmark that we wrote, it's pretty straightforward to adapt", "tokens": [50700, 400, 370, 2212, 264, 3894, 18927, 300, 321, 4114, 11, 309, 311, 1238, 15325, 281, 6231, 50882], "temperature": 0.0, "avg_logprob": -0.09330382096140008, "compression_ratio": 1.6291079812206573, "no_speech_prob": 0.014942141249775887}, {"id": 106, "seek": 49340, "start": 503.76, "end": 504.76, "text": " it to Devon.", "tokens": [50882, 309, 281, 9096, 266, 13, 50932], "temperature": 0.0, "avg_logprob": -0.09330382096140008, "compression_ratio": 1.6291079812206573, "no_speech_prob": 0.014942141249775887}, {"id": 107, "seek": 49340, "start": 504.76, "end": 507.59999999999997, "text": " We just stick it in a function and then mark it as bench.", "tokens": [50932, 492, 445, 2897, 309, 294, 257, 2445, 293, 550, 1491, 309, 382, 10638, 13, 51074], "temperature": 0.0, "avg_logprob": -0.09330382096140008, "compression_ratio": 1.6291079812206573, "no_speech_prob": 0.014942141249775887}, {"id": 108, "seek": 49340, "start": 507.59999999999997, "end": 510.91999999999996, "text": " And then Devon will be able to run that.", "tokens": [51074, 400, 550, 9096, 266, 486, 312, 1075, 281, 1190, 300, 13, 51240], "temperature": 0.0, "avg_logprob": -0.09330382096140008, "compression_ratio": 1.6291079812206573, "no_speech_prob": 0.014942141249775887}, {"id": 109, "seek": 49340, "start": 510.91999999999996, "end": 515.36, "text": " And after executing our benchmark, Devon presents us with pretty succinct information about", "tokens": [51240, 400, 934, 32368, 527, 18927, 11, 9096, 266, 13533, 505, 365, 1238, 21578, 5460, 1589, 466, 51462], "temperature": 0.0, "avg_logprob": -0.09330382096140008, "compression_ratio": 1.6291079812206573, "no_speech_prob": 0.014942141249775887}, {"id": 110, "seek": 49340, "start": 515.36, "end": 517.72, "text": " how it ran.", "tokens": [51462, 577, 309, 5872, 13, 51580], "temperature": 0.0, "avg_logprob": -0.09330382096140008, "compression_ratio": 1.6291079812206573, "no_speech_prob": 0.014942141249775887}, {"id": 111, "seek": 51772, "start": 517.72, "end": 522.84, "text": " On this, we can see that the best speed was measured at about 70 nanoseconds.", "tokens": [50364, 1282, 341, 11, 321, 393, 536, 300, 264, 1151, 3073, 390, 12690, 412, 466, 5285, 14067, 541, 28750, 13, 50620], "temperature": 0.0, "avg_logprob": -0.12017405827840169, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.01321254763752222}, {"id": 112, "seek": 51772, "start": 522.84, "end": 530.32, "text": " And this realistically represents how fast the function would perform under ideal conditions.", "tokens": [50620, 400, 341, 40734, 8855, 577, 2370, 264, 2445, 576, 2042, 833, 7157, 4487, 13, 50994], "temperature": 0.0, "avg_logprob": -0.12017405827840169, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.01321254763752222}, {"id": 113, "seek": 51772, "start": 530.32, "end": 534.48, "text": " And we also see that the worst case was measured at about 200 nanoseconds.", "tokens": [50994, 400, 321, 611, 536, 300, 264, 5855, 1389, 390, 12690, 412, 466, 2331, 14067, 541, 28750, 13, 51202], "temperature": 0.0, "avg_logprob": -0.12017405827840169, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.01321254763752222}, {"id": 114, "seek": 51772, "start": 534.48, "end": 536.9200000000001, "text": " And so there's various things that could play into that.", "tokens": [51202, 400, 370, 456, 311, 3683, 721, 300, 727, 862, 666, 300, 13, 51324], "temperature": 0.0, "avg_logprob": -0.12017405827840169, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.01321254763752222}, {"id": 115, "seek": 51772, "start": 536.9200000000001, "end": 543.48, "text": " It might not be necessarily the code itself, but the situation around the code.", "tokens": [51324, 467, 1062, 406, 312, 4725, 264, 3089, 2564, 11, 457, 264, 2590, 926, 264, 3089, 13, 51652], "temperature": 0.0, "avg_logprob": -0.12017405827840169, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.01321254763752222}, {"id": 116, "seek": 54348, "start": 543.48, "end": 548.6, "text": " And then we also have median and mean, which represent the average time that the function", "tokens": [50364, 400, 550, 321, 611, 362, 26779, 293, 914, 11, 597, 2906, 264, 4274, 565, 300, 264, 2445, 50620], "temperature": 0.0, "avg_logprob": -0.10892661074374585, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.005382150411605835}, {"id": 117, "seek": 54348, "start": 548.6, "end": 551.16, "text": " took to run.", "tokens": [50620, 1890, 281, 1190, 13, 50748], "temperature": 0.0, "avg_logprob": -0.10892661074374585, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.005382150411605835}, {"id": 118, "seek": 54348, "start": 551.16, "end": 556.6800000000001, "text": " And we can also see that these values are pretty close to the fastest sample.", "tokens": [50748, 400, 321, 393, 611, 536, 300, 613, 4190, 366, 1238, 1998, 281, 264, 14573, 6889, 13, 51024], "temperature": 0.0, "avg_logprob": -0.10892661074374585, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.005382150411605835}, {"id": 119, "seek": 54348, "start": 556.6800000000001, "end": 563.28, "text": " So we can be fairly confident that this function will generally perform at this speed, at least", "tokens": [51024, 407, 321, 393, 312, 6457, 6679, 300, 341, 2445, 486, 5101, 2042, 412, 341, 3073, 11, 412, 1935, 51354], "temperature": 0.0, "avg_logprob": -0.10892661074374585, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.005382150411605835}, {"id": 120, "seek": 54348, "start": 563.28, "end": 566.48, "text": " on this machine.", "tokens": [51354, 322, 341, 3479, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10892661074374585, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.005382150411605835}, {"id": 121, "seek": 54348, "start": 566.48, "end": 573.44, "text": " And so to give some insight into how Devon is running this code, we see that it's reporting", "tokens": [51514, 400, 370, 281, 976, 512, 11269, 666, 577, 9096, 266, 307, 2614, 341, 3089, 11, 321, 536, 300, 309, 311, 10031, 51862], "temperature": 0.0, "avg_logprob": -0.10892661074374585, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.005382150411605835}, {"id": 122, "seek": 57344, "start": 573.44, "end": 576.12, "text": " the number of samples and total iterations.", "tokens": [50364, 264, 1230, 295, 10938, 293, 3217, 36540, 13, 50498], "temperature": 0.0, "avg_logprob": -0.1174056414261605, "compression_ratio": 2.0093023255813955, "no_speech_prob": 0.00021645554807037115}, {"id": 123, "seek": 57344, "start": 576.12, "end": 583.12, "text": " And this represents how many timings, samples represents how many timings Devon has measured.", "tokens": [50498, 400, 341, 8855, 577, 867, 524, 1109, 11, 10938, 8855, 577, 867, 524, 1109, 9096, 266, 575, 12690, 13, 50848], "temperature": 0.0, "avg_logprob": -0.1174056414261605, "compression_ratio": 2.0093023255813955, "no_speech_prob": 0.00021645554807037115}, {"id": 124, "seek": 57344, "start": 583.12, "end": 590.08, "text": " And then iterations is the number of repetitions across all the timings or all the samples.", "tokens": [50848, 400, 550, 36540, 307, 264, 1230, 295, 13645, 2451, 2108, 439, 264, 524, 1109, 420, 439, 264, 10938, 13, 51196], "temperature": 0.0, "avg_logprob": -0.1174056414261605, "compression_ratio": 2.0093023255813955, "no_speech_prob": 0.00021645554807037115}, {"id": 125, "seek": 57344, "start": 590.08, "end": 594.6400000000001, "text": " And if we divide the iteration count by the sample count, we end up getting what I call", "tokens": [51196, 400, 498, 321, 9845, 264, 24784, 1207, 538, 264, 6889, 1207, 11, 321, 917, 493, 1242, 437, 286, 818, 51424], "temperature": 0.0, "avg_logprob": -0.1174056414261605, "compression_ratio": 2.0093023255813955, "no_speech_prob": 0.00021645554807037115}, {"id": 126, "seek": 57344, "start": 594.6400000000001, "end": 597.4000000000001, "text": " the sample size, which is how many iterations per sample.", "tokens": [51424, 264, 6889, 2744, 11, 597, 307, 577, 867, 36540, 680, 6889, 13, 51562], "temperature": 0.0, "avg_logprob": -0.1174056414261605, "compression_ratio": 2.0093023255813955, "no_speech_prob": 0.00021645554807037115}, {"id": 127, "seek": 57344, "start": 597.4000000000001, "end": 602.84, "text": " And so we see that each sample took about 64 iterations.", "tokens": [51562, 400, 370, 321, 536, 300, 1184, 6889, 1890, 466, 12145, 36540, 13, 51834], "temperature": 0.0, "avg_logprob": -0.1174056414261605, "compression_ratio": 2.0093023255813955, "no_speech_prob": 0.00021645554807037115}, {"id": 128, "seek": 60284, "start": 603.12, "end": 609.44, "text": " This is chosen dynamically at runtime based on how much time is spent in earlier samples.", "tokens": [50378, 639, 307, 8614, 43492, 412, 34474, 2361, 322, 577, 709, 565, 307, 4418, 294, 3071, 10938, 13, 50694], "temperature": 0.0, "avg_logprob": -0.1261709976196289, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.0020503767300397158}, {"id": 129, "seek": 60284, "start": 609.44, "end": 616.48, "text": " And this number can be higher for faster functions, or it can be as low as just one iteration", "tokens": [50694, 400, 341, 1230, 393, 312, 2946, 337, 4663, 6828, 11, 420, 309, 393, 312, 382, 2295, 382, 445, 472, 24784, 51046], "temperature": 0.0, "avg_logprob": -0.1261709976196289, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.0020503767300397158}, {"id": 130, "seek": 60284, "start": 616.48, "end": 622.5600000000001, "text": " per sample for really slow functions.", "tokens": [51046, 680, 6889, 337, 534, 2964, 6828, 13, 51350], "temperature": 0.0, "avg_logprob": -0.1261709976196289, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.0020503767300397158}, {"id": 131, "seek": 60284, "start": 622.5600000000001, "end": 629.1600000000001, "text": " But if we want to measure not only the time to allocate a vector, or sorry, if we only", "tokens": [51350, 583, 498, 321, 528, 281, 3481, 406, 787, 264, 565, 281, 35713, 257, 8062, 11, 420, 2597, 11, 498, 321, 787, 51680], "temperature": 0.0, "avg_logprob": -0.1261709976196289, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.0020503767300397158}, {"id": 132, "seek": 62916, "start": 629.16, "end": 634.24, "text": " want to measure the time to allocate a vector and not the time to deallocate it, then the", "tokens": [50364, 528, 281, 3481, 264, 565, 281, 35713, 257, 8062, 293, 406, 264, 565, 281, 368, 336, 42869, 309, 11, 550, 264, 50618], "temperature": 0.0, "avg_logprob": -0.169465359126296, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.0004044070083182305}, {"id": 133, "seek": 62916, "start": 634.24, "end": 638.76, "text": " way this would work in Devon is you simply return the created value from the benchmark", "tokens": [50618, 636, 341, 576, 589, 294, 9096, 266, 307, 291, 2935, 2736, 264, 2942, 2158, 490, 264, 18927, 50844], "temperature": 0.0, "avg_logprob": -0.169465359126296, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.0004044070083182305}, {"id": 134, "seek": 62916, "start": 638.76, "end": 640.0799999999999, "text": " function.", "tokens": [50844, 2445, 13, 50910], "temperature": 0.0, "avg_logprob": -0.169465359126296, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.0004044070083182305}, {"id": 135, "seek": 62916, "start": 640.0799999999999, "end": 646.9599999999999, "text": " And this will defer freeing the vector until after the sample is finished being timed.", "tokens": [50910, 400, 341, 486, 25704, 1737, 278, 264, 8062, 1826, 934, 264, 6889, 307, 4335, 885, 44696, 13, 51254], "temperature": 0.0, "avg_logprob": -0.169465359126296, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.0004044070083182305}, {"id": 136, "seek": 62916, "start": 646.9599999999999, "end": 652.0799999999999, "text": " And since Devon will automatically black box the returned value, we can actually remove", "tokens": [51254, 400, 1670, 9096, 266, 486, 6772, 2211, 2424, 264, 8752, 2158, 11, 321, 393, 767, 4159, 51510], "temperature": 0.0, "avg_logprob": -0.169465359126296, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.0004044070083182305}, {"id": 137, "seek": 62916, "start": 652.0799999999999, "end": 654.1999999999999, "text": " the black box from our function.", "tokens": [51510, 264, 2211, 2424, 490, 527, 2445, 13, 51616], "temperature": 0.0, "avg_logprob": -0.169465359126296, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.0004044070083182305}, {"id": 138, "seek": 62916, "start": 654.1999999999999, "end": 658.28, "text": " And this just makes it a lot easier to read.", "tokens": [51616, 400, 341, 445, 1669, 309, 257, 688, 3571, 281, 1401, 13, 51820], "temperature": 0.0, "avg_logprob": -0.169465359126296, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.0004044070083182305}, {"id": 139, "seek": 65828, "start": 659.28, "end": 664.3199999999999, "text": " And so since we're measuring vector allocation but not deallocation, now our benchmark results", "tokens": [50414, 400, 370, 1670, 321, 434, 13389, 8062, 27599, 457, 406, 368, 336, 27943, 11, 586, 527, 18927, 3542, 50666], "temperature": 0.0, "avg_logprob": -0.17202670872211456, "compression_ratio": 1.6272189349112427, "no_speech_prob": 0.00014422893582377583}, {"id": 140, "seek": 65828, "start": 664.3199999999999, "end": 673.28, "text": " are about half the time that we measured before.", "tokens": [50666, 366, 466, 1922, 264, 565, 300, 321, 12690, 949, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17202670872211456, "compression_ratio": 1.6272189349112427, "no_speech_prob": 0.00014422893582377583}, {"id": 141, "seek": 65828, "start": 673.28, "end": 680.0, "text": " And so far we've only been benchmarking allocating vectors that contain 100 integers, but we", "tokens": [51114, 400, 370, 1400, 321, 600, 787, 668, 18927, 278, 12660, 990, 18875, 300, 5304, 2319, 41674, 11, 457, 321, 51450], "temperature": 0.0, "avg_logprob": -0.17202670872211456, "compression_ratio": 1.6272189349112427, "no_speech_prob": 0.00014422893582377583}, {"id": 142, "seek": 65828, "start": 680.0, "end": 683.0799999999999, "text": " can also benchmark across other cases.", "tokens": [51450, 393, 611, 18927, 2108, 661, 3331, 13, 51604], "temperature": 0.0, "avg_logprob": -0.17202670872211456, "compression_ratio": 1.6272189349112427, "no_speech_prob": 0.00014422893582377583}, {"id": 143, "seek": 68308, "start": 683.08, "end": 693.8000000000001, "text": " So we can use the args option to measure across one, five, 10, 1,000, you name it, any value", "tokens": [50364, 407, 321, 393, 764, 264, 3882, 82, 3614, 281, 3481, 2108, 472, 11, 1732, 11, 1266, 11, 502, 11, 1360, 11, 291, 1315, 309, 11, 604, 2158, 50900], "temperature": 0.0, "avg_logprob": -0.15702345599866893, "compression_ratio": 1.4715909090909092, "no_speech_prob": 6.0123391449451447e-05}, {"id": 144, "seek": 68308, "start": 693.8000000000001, "end": 696.9200000000001, "text": " that can be provided as an input.", "tokens": [50900, 300, 393, 312, 5649, 382, 364, 4846, 13, 51056], "temperature": 0.0, "avg_logprob": -0.15702345599866893, "compression_ratio": 1.4715909090909092, "no_speech_prob": 6.0123391449451447e-05}, {"id": 145, "seek": 68308, "start": 696.9200000000001, "end": 704.84, "text": " And this, I find it's generally very good practice to measure across various cases to", "tokens": [51056, 400, 341, 11, 286, 915, 309, 311, 5101, 588, 665, 3124, 281, 3481, 2108, 3683, 3331, 281, 51452], "temperature": 0.0, "avg_logprob": -0.15702345599866893, "compression_ratio": 1.4715909090909092, "no_speech_prob": 6.0123391449451447e-05}, {"id": 146, "seek": 68308, "start": 704.84, "end": 708.9200000000001, "text": " get a better sense of how your code's running.", "tokens": [51452, 483, 257, 1101, 2020, 295, 577, 428, 3089, 311, 2614, 13, 51656], "temperature": 0.0, "avg_logprob": -0.15702345599866893, "compression_ratio": 1.4715909090909092, "no_speech_prob": 6.0123391449451447e-05}, {"id": 147, "seek": 70892, "start": 708.9599999999999, "end": 717.76, "text": " And we can see that generally as expected, as the size increases, the benchmark also slows", "tokens": [50366, 400, 321, 393, 536, 300, 5101, 382, 5176, 11, 382, 264, 2744, 8637, 11, 264, 18927, 611, 35789, 50806], "temperature": 0.0, "avg_logprob": -0.17833175397899054, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0006876461557112634}, {"id": 148, "seek": 70892, "start": 717.76, "end": 718.92, "text": " down.", "tokens": [50806, 760, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17833175397899054, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0006876461557112634}, {"id": 149, "seek": 70892, "start": 718.92, "end": 725.76, "text": " But interestingly enough, for cases that are at 10 or smaller, there's not really a difference", "tokens": [50864, 583, 25873, 1547, 11, 337, 3331, 300, 366, 412, 1266, 420, 4356, 11, 456, 311, 406, 534, 257, 2649, 51206], "temperature": 0.0, "avg_logprob": -0.17833175397899054, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0006876461557112634}, {"id": 150, "seek": 70892, "start": 725.76, "end": 727.52, "text": " in performance.", "tokens": [51206, 294, 3389, 13, 51294], "temperature": 0.0, "avg_logprob": -0.17833175397899054, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0006876461557112634}, {"id": 151, "seek": 70892, "start": 727.52, "end": 734.56, "text": " And so really the differences, I would say, are more like systemic noise because it doesn't", "tokens": [51294, 400, 370, 534, 264, 7300, 11, 286, 576, 584, 11, 366, 544, 411, 23789, 5658, 570, 309, 1177, 380, 51646], "temperature": 0.0, "avg_logprob": -0.17833175397899054, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0006876461557112634}, {"id": 152, "seek": 73456, "start": 734.56, "end": 741.76, "text": " really make sense that creating five values in a vector takes longer than creating 10,", "tokens": [50364, 534, 652, 2020, 300, 4084, 1732, 4190, 294, 257, 8062, 2516, 2854, 813, 4084, 1266, 11, 50724], "temperature": 0.0, "avg_logprob": -0.13760282032525362, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.0001511539303464815}, {"id": 153, "seek": 73456, "start": 741.76, "end": 746.68, "text": " at least not consistently so.", "tokens": [50724, 412, 1935, 406, 14961, 370, 13, 50970], "temperature": 0.0, "avg_logprob": -0.13760282032525362, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.0001511539303464815}, {"id": 154, "seek": 73456, "start": 746.68, "end": 751.4, "text": " And we also notice that this function really starts to slow down a lot at bigger sizes.", "tokens": [50970, 400, 321, 611, 3449, 300, 341, 2445, 534, 3719, 281, 2964, 760, 257, 688, 412, 3801, 11602, 13, 51206], "temperature": 0.0, "avg_logprob": -0.13760282032525362, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.0001511539303464815}, {"id": 155, "seek": 73456, "start": 751.4, "end": 759.3199999999999, "text": " And so that aligns with whatever hypothesis we might have had about this benchmark before.", "tokens": [51206, 400, 370, 300, 7975, 82, 365, 2035, 17291, 321, 1062, 362, 632, 466, 341, 18927, 949, 13, 51602], "temperature": 0.0, "avg_logprob": -0.13760282032525362, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.0001511539303464815}, {"id": 156, "seek": 75932, "start": 759.32, "end": 764.8000000000001, "text": " But we can also compare the performance across multiple types by making the function generic.", "tokens": [50364, 583, 321, 393, 611, 6794, 264, 3389, 2108, 3866, 3467, 538, 1455, 264, 2445, 19577, 13, 50638], "temperature": 0.0, "avg_logprob": -0.10846372604370118, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.03842125087976456}, {"id": 157, "seek": 75932, "start": 764.8000000000001, "end": 769.7600000000001, "text": " And then we can provide a types option to pass all the cases.", "tokens": [50638, 400, 550, 321, 393, 2893, 257, 3467, 3614, 281, 1320, 439, 264, 3331, 13, 50886], "temperature": 0.0, "avg_logprob": -0.10846372604370118, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.03842125087976456}, {"id": 158, "seek": 75932, "start": 769.7600000000001, "end": 776.6400000000001, "text": " So now this benchmark is not only running the standard libraries vector type, but it's", "tokens": [50886, 407, 586, 341, 18927, 307, 406, 787, 2614, 264, 3832, 15148, 8062, 2010, 11, 457, 309, 311, 51230], "temperature": 0.0, "avg_logprob": -0.10846372604370118, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.03842125087976456}, {"id": 159, "seek": 75932, "start": 776.6400000000001, "end": 782.24, "text": " also comparing that against SmallVec using all the same cases as before.", "tokens": [51230, 611, 15763, 300, 1970, 15287, 53, 3045, 1228, 439, 264, 912, 3331, 382, 949, 13, 51510], "temperature": 0.0, "avg_logprob": -0.10846372604370118, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.03842125087976456}, {"id": 160, "seek": 75932, "start": 782.24, "end": 786.4000000000001, "text": " And for those who aren't familiar, SmallVec is a type that's designed to be faster for", "tokens": [51510, 400, 337, 729, 567, 3212, 380, 4963, 11, 15287, 53, 3045, 307, 257, 2010, 300, 311, 4761, 281, 312, 4663, 337, 51718], "temperature": 0.0, "avg_logprob": -0.10846372604370118, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.03842125087976456}, {"id": 161, "seek": 75932, "start": 786.4000000000001, "end": 788.32, "text": " smaller sizes.", "tokens": [51718, 4356, 11602, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10846372604370118, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.03842125087976456}, {"id": 162, "seek": 78832, "start": 788.32, "end": 793.72, "text": " And it does this by storing values on the stack instead of doing a heap allocation.", "tokens": [50364, 400, 309, 775, 341, 538, 26085, 4190, 322, 264, 8630, 2602, 295, 884, 257, 33591, 27599, 13, 50634], "temperature": 0.0, "avg_logprob": -0.14168191423603133, "compression_ratio": 1.7028112449799198, "no_speech_prob": 0.00013550539733842015}, {"id": 163, "seek": 78832, "start": 793.72, "end": 799.9200000000001, "text": " But once there's not enough space on the stack, it'll fall back to using the standard libraries", "tokens": [50634, 583, 1564, 456, 311, 406, 1547, 1901, 322, 264, 8630, 11, 309, 603, 2100, 646, 281, 1228, 264, 3832, 15148, 50944], "temperature": 0.0, "avg_logprob": -0.14168191423603133, "compression_ratio": 1.7028112449799198, "no_speech_prob": 0.00013550539733842015}, {"id": 164, "seek": 78832, "start": 799.9200000000001, "end": 807.1600000000001, "text": " vector, or rather it'll use the heap like the standard libraries vector.", "tokens": [50944, 8062, 11, 420, 2831, 309, 603, 764, 264, 33591, 411, 264, 3832, 15148, 8062, 13, 51306], "temperature": 0.0, "avg_logprob": -0.14168191423603133, "compression_ratio": 1.7028112449799198, "no_speech_prob": 0.00013550539733842015}, {"id": 165, "seek": 78832, "start": 807.1600000000001, "end": 812.48, "text": " And so to make what's happening a bit clearer, Devon's not actually doing anything special", "tokens": [51306, 400, 370, 281, 652, 437, 311, 2737, 257, 857, 26131, 11, 9096, 266, 311, 406, 767, 884, 1340, 2121, 51572], "temperature": 0.0, "avg_logprob": -0.14168191423603133, "compression_ratio": 1.7028112449799198, "no_speech_prob": 0.00013550539733842015}, {"id": 166, "seek": 78832, "start": 812.48, "end": 814.0, "text": " to the function.", "tokens": [51572, 281, 264, 2445, 13, 51648], "temperature": 0.0, "avg_logprob": -0.14168191423603133, "compression_ratio": 1.7028112449799198, "no_speech_prob": 0.00013550539733842015}, {"id": 167, "seek": 78832, "start": 814.0, "end": 817.6, "text": " This is just normal generic code that's pretty common to write.", "tokens": [51648, 639, 307, 445, 2710, 19577, 3089, 300, 311, 1238, 2689, 281, 2464, 13, 51828], "temperature": 0.0, "avg_logprob": -0.14168191423603133, "compression_ratio": 1.7028112449799198, "no_speech_prob": 0.00013550539733842015}, {"id": 168, "seek": 81760, "start": 818.48, "end": 823.48, "text": " Instead Devon is using the function as is to generate the benchmarking code for each", "tokens": [50408, 7156, 9096, 266, 307, 1228, 264, 2445, 382, 307, 281, 8460, 264, 18927, 278, 3089, 337, 1184, 50658], "temperature": 0.0, "avg_logprob": -0.11621191842215402, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.0014543173601850867}, {"id": 169, "seek": 81760, "start": 823.48, "end": 828.12, "text": " type that's passed into the attribute.", "tokens": [50658, 2010, 300, 311, 4678, 666, 264, 19667, 13, 50890], "temperature": 0.0, "avg_logprob": -0.11621191842215402, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.0014543173601850867}, {"id": 170, "seek": 81760, "start": 828.12, "end": 835.72, "text": " And so once we run this, we have this nice tree output and table where we see that Devon", "tokens": [50890, 400, 370, 1564, 321, 1190, 341, 11, 321, 362, 341, 1481, 4230, 5598, 293, 3199, 689, 321, 536, 300, 9096, 266, 51270], "temperature": 0.0, "avg_logprob": -0.11621191842215402, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.0014543173601850867}, {"id": 171, "seek": 81760, "start": 835.72, "end": 842.76, "text": " has grouped the types as separate trees under the benchmark function's name.", "tokens": [51270, 575, 41877, 264, 3467, 382, 4994, 5852, 833, 264, 18927, 2445, 311, 1315, 13, 51622], "temperature": 0.0, "avg_logprob": -0.11621191842215402, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.0014543173601850867}, {"id": 172, "seek": 84276, "start": 842.76, "end": 847.76, "text": " And we can also see from these measurements that, at least for this specific operation", "tokens": [50364, 400, 321, 393, 611, 536, 490, 613, 15383, 300, 11, 412, 1935, 337, 341, 2685, 6916, 50614], "temperature": 0.0, "avg_logprob": -0.15402342852424172, "compression_ratio": 1.7255813953488373, "no_speech_prob": 0.0002779988863039762}, {"id": 173, "seek": 84276, "start": 847.76, "end": 853.48, "text": " collecting from a range, SmallVec is faster than the standard libraries vector when the", "tokens": [50614, 12510, 490, 257, 3613, 11, 15287, 53, 3045, 307, 4663, 813, 264, 3832, 15148, 8062, 562, 264, 50900], "temperature": 0.0, "avg_logprob": -0.15402342852424172, "compression_ratio": 1.7255813953488373, "no_speech_prob": 0.0002779988863039762}, {"id": 174, "seek": 84276, "start": 853.48, "end": 857.24, "text": " number of items fits on the stack.", "tokens": [50900, 1230, 295, 4754, 9001, 322, 264, 8630, 13, 51088], "temperature": 0.0, "avg_logprob": -0.15402342852424172, "compression_ratio": 1.7255813953488373, "no_speech_prob": 0.0002779988863039762}, {"id": 175, "seek": 84276, "start": 857.24, "end": 863.92, "text": " However, once a size grows beyond fitting on the stack, once SmallVec needs to do heap", "tokens": [51088, 2908, 11, 1564, 257, 2744, 13156, 4399, 15669, 322, 264, 8630, 11, 1564, 15287, 53, 3045, 2203, 281, 360, 33591, 51422], "temperature": 0.0, "avg_logprob": -0.15402342852424172, "compression_ratio": 1.7255813953488373, "no_speech_prob": 0.0002779988863039762}, {"id": 176, "seek": 84276, "start": 863.92, "end": 868.84, "text": " allocations, interestingly enough the standard libraries vector is faster.", "tokens": [51422, 12660, 763, 11, 25873, 1547, 264, 3832, 15148, 8062, 307, 4663, 13, 51668], "temperature": 0.0, "avg_logprob": -0.15402342852424172, "compression_ratio": 1.7255813953488373, "no_speech_prob": 0.0002779988863039762}, {"id": 177, "seek": 86884, "start": 868.84, "end": 873.12, "text": " And I imagine this is because the standard libraries vector can do nice optimizations", "tokens": [50364, 400, 286, 3811, 341, 307, 570, 264, 3832, 15148, 8062, 393, 360, 1481, 5028, 14455, 50578], "temperature": 0.0, "avg_logprob": -0.17591320783242412, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0002453122870065272}, {"id": 178, "seek": 86884, "start": 873.12, "end": 878.72, "text": " like specialization, which if any of you can make that stable, please.", "tokens": [50578, 411, 2121, 2144, 11, 597, 498, 604, 295, 291, 393, 652, 300, 8351, 11, 1767, 13, 50858], "temperature": 0.0, "avg_logprob": -0.17591320783242412, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0002453122870065272}, {"id": 179, "seek": 86884, "start": 878.72, "end": 884.4, "text": " I've been waiting forever.", "tokens": [50858, 286, 600, 668, 3806, 5680, 13, 51142], "temperature": 0.0, "avg_logprob": -0.17591320783242412, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0002453122870065272}, {"id": 180, "seek": 86884, "start": 884.4, "end": 889.2800000000001, "text": " But also when we think about software performance, like I mentioned earlier, we shouldn't only", "tokens": [51142, 583, 611, 562, 321, 519, 466, 4722, 3389, 11, 411, 286, 2835, 3071, 11, 321, 4659, 380, 787, 51386], "temperature": 0.0, "avg_logprob": -0.17591320783242412, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0002453122870065272}, {"id": 181, "seek": 86884, "start": 889.2800000000001, "end": 892.9200000000001, "text": " be considering speed and we should also be considering the amount of space that's being", "tokens": [51386, 312, 8079, 3073, 293, 321, 820, 611, 312, 8079, 264, 2372, 295, 1901, 300, 311, 885, 51568], "temperature": 0.0, "avg_logprob": -0.17591320783242412, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0002453122870065272}, {"id": 182, "seek": 86884, "start": 892.9200000000001, "end": 894.5600000000001, "text": " used.", "tokens": [51568, 1143, 13, 51650], "temperature": 0.0, "avg_logprob": -0.17591320783242412, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0002453122870065272}, {"id": 183, "seek": 89456, "start": 894.56, "end": 901.52, "text": " And normally if you're profiling a long running program, keeping track of allocations with", "tokens": [50364, 400, 5646, 498, 291, 434, 1740, 4883, 257, 938, 2614, 1461, 11, 5145, 2837, 295, 12660, 763, 365, 50712], "temperature": 0.0, "avg_logprob": -0.16340398286518298, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0032708991784602404}, {"id": 184, "seek": 89456, "start": 901.52, "end": 907.76, "text": " a tool like DHAT, the cost there is relatively low because it gets amortized generally over", "tokens": [50712, 257, 2290, 411, 28606, 2218, 11, 264, 2063, 456, 307, 7226, 2295, 570, 309, 2170, 669, 477, 1602, 5101, 670, 51024], "temperature": 0.0, "avg_logprob": -0.16340398286518298, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0032708991784602404}, {"id": 185, "seek": 89456, "start": 907.76, "end": 909.9599999999999, "text": " the life of the program.", "tokens": [51024, 264, 993, 295, 264, 1461, 13, 51134], "temperature": 0.0, "avg_logprob": -0.16340398286518298, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0032708991784602404}, {"id": 186, "seek": 89456, "start": 909.9599999999999, "end": 915.28, "text": " And the nice thing about tools like DHAT is that it'll collect back traces to tell you", "tokens": [51134, 400, 264, 1481, 551, 466, 3873, 411, 28606, 2218, 307, 300, 309, 603, 2500, 646, 26076, 281, 980, 291, 51400], "temperature": 0.0, "avg_logprob": -0.16340398286518298, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0032708991784602404}, {"id": 187, "seek": 89456, "start": 915.28, "end": 917.64, "text": " exactly where your allocations are happening.", "tokens": [51400, 2293, 689, 428, 12660, 763, 366, 2737, 13, 51518], "temperature": 0.0, "avg_logprob": -0.16340398286518298, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0032708991784602404}, {"id": 188, "seek": 89456, "start": 917.64, "end": 921.7199999999999, "text": " So it does give you a lot of information.", "tokens": [51518, 407, 309, 775, 976, 291, 257, 688, 295, 1589, 13, 51722], "temperature": 0.0, "avg_logprob": -0.16340398286518298, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0032708991784602404}, {"id": 189, "seek": 92172, "start": 922.44, "end": 928.72, "text": " However, in microbenchmarks, when the time spent tracking allocations, like that can", "tokens": [50400, 2908, 11, 294, 4532, 47244, 37307, 11, 562, 264, 565, 4418, 11603, 12660, 763, 11, 411, 300, 393, 50714], "temperature": 0.0, "avg_logprob": -0.16495436838228408, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.0006876258994452655}, {"id": 190, "seek": 92172, "start": 928.72, "end": 930.5600000000001, "text": " have a noticeable impact.", "tokens": [50714, 362, 257, 26041, 2712, 13, 50806], "temperature": 0.0, "avg_logprob": -0.16495436838228408, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.0006876258994452655}, {"id": 191, "seek": 92172, "start": 930.5600000000001, "end": 938.52, "text": " So taking back traces can take microseconds, whereas the code we want to measure may just", "tokens": [50806, 407, 1940, 646, 26076, 393, 747, 3123, 37841, 28750, 11, 9735, 264, 3089, 321, 528, 281, 3481, 815, 445, 51204], "temperature": 0.0, "avg_logprob": -0.16495436838228408, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.0006876258994452655}, {"id": 192, "seek": 92172, "start": 938.52, "end": 940.64, "text": " be a few nanoseconds.", "tokens": [51204, 312, 257, 1326, 14067, 541, 28750, 13, 51310], "temperature": 0.0, "avg_logprob": -0.16495436838228408, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.0006876258994452655}, {"id": 193, "seek": 92172, "start": 940.64, "end": 945.9200000000001, "text": " And so we would be totally blowing out the timings.", "tokens": [51310, 400, 370, 321, 576, 312, 3879, 15068, 484, 264, 524, 1109, 13, 51574], "temperature": 0.0, "avg_logprob": -0.16495436838228408, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.0006876258994452655}, {"id": 194, "seek": 94592, "start": 945.92, "end": 952.0799999999999, "text": " And in a sense, by observing the behavior of our program, we've now also affected our", "tokens": [50364, 400, 294, 257, 2020, 11, 538, 22107, 264, 5223, 295, 527, 1461, 11, 321, 600, 586, 611, 8028, 527, 50672], "temperature": 0.0, "avg_logprob": -0.1575185775756836, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.0040673138573765755}, {"id": 195, "seek": 94592, "start": 952.0799999999999, "end": 953.88, "text": " measurements.", "tokens": [50672, 15383, 13, 50762], "temperature": 0.0, "avg_logprob": -0.1575185775756836, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.0040673138573765755}, {"id": 196, "seek": 94592, "start": 953.88, "end": 959.4, "text": " So is it possible to gather insights without affecting measurements?", "tokens": [50762, 407, 307, 309, 1944, 281, 5448, 14310, 1553, 17476, 15383, 30, 51038], "temperature": 0.0, "avg_logprob": -0.1575185775756836, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.0040673138573765755}, {"id": 197, "seek": 94592, "start": 959.4, "end": 962.68, "text": " Is it possible to reduce the amount of time spent here?", "tokens": [51038, 1119, 309, 1944, 281, 5407, 264, 2372, 295, 565, 4418, 510, 30, 51202], "temperature": 0.0, "avg_logprob": -0.1575185775756836, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.0040673138573765755}, {"id": 198, "seek": 94592, "start": 962.68, "end": 965.5999999999999, "text": " Well, I actually managed to do that.", "tokens": [51202, 1042, 11, 286, 767, 6453, 281, 360, 300, 13, 51348], "temperature": 0.0, "avg_logprob": -0.1575185775756836, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.0040673138573765755}, {"id": 199, "seek": 94592, "start": 965.5999999999999, "end": 971.76, "text": " So Devon has a custom allocator that will only track the number of bytes allocated and", "tokens": [51348, 407, 9096, 266, 575, 257, 2375, 12660, 1639, 300, 486, 787, 2837, 264, 1230, 295, 36088, 29772, 293, 51656], "temperature": 0.0, "avg_logprob": -0.1575185775756836, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.0040673138573765755}, {"id": 200, "seek": 97176, "start": 971.76, "end": 974.52, "text": " the number of allocations during benchmarks.", "tokens": [50364, 264, 1230, 295, 12660, 763, 1830, 43751, 13, 50502], "temperature": 0.0, "avg_logprob": -0.1795683409038343, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0003099823952652514}, {"id": 201, "seek": 97176, "start": 974.52, "end": 981.4, "text": " This applies to allocations, the allocation, reallocation of grow or shrink.", "tokens": [50502, 639, 13165, 281, 12660, 763, 11, 264, 27599, 11, 319, 336, 27943, 295, 1852, 420, 23060, 13, 50846], "temperature": 0.0, "avg_logprob": -0.1795683409038343, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0003099823952652514}, {"id": 202, "seek": 97176, "start": 981.4, "end": 988.4, "text": " And the way that you use this is you override the global allocator with Devon's allocrofiler.", "tokens": [50846, 400, 264, 636, 300, 291, 764, 341, 307, 291, 42321, 264, 4338, 12660, 1639, 365, 9096, 266, 311, 12660, 340, 69, 5441, 13, 51196], "temperature": 0.0, "avg_logprob": -0.1795683409038343, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0003099823952652514}, {"id": 203, "seek": 97176, "start": 988.4, "end": 995.04, "text": " But you can also pass a custom allocator if in reality you are going to be using a faster", "tokens": [51196, 583, 291, 393, 611, 1320, 257, 2375, 12660, 1639, 498, 294, 4103, 291, 366, 516, 281, 312, 1228, 257, 4663, 51528], "temperature": 0.0, "avg_logprob": -0.1795683409038343, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0003099823952652514}, {"id": 204, "seek": 97176, "start": 995.04, "end": 997.4, "text": " allocator such as meme alloc.", "tokens": [51528, 12660, 1639, 1270, 382, 21701, 12660, 13, 51646], "temperature": 0.0, "avg_logprob": -0.1795683409038343, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0003099823952652514}, {"id": 205, "seek": 97176, "start": 997.4, "end": 999.4, "text": " And so it's fairly flexible.", "tokens": [51646, 400, 370, 309, 311, 6457, 11358, 13, 51746], "temperature": 0.0, "avg_logprob": -0.1795683409038343, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0003099823952652514}, {"id": 206, "seek": 100176, "start": 1002.2, "end": 1008.3199999999999, "text": " So once we've registered this allocator and we rerun the same benchmarks as before, we", "tokens": [50386, 407, 1564, 321, 600, 13968, 341, 12660, 1639, 293, 321, 43819, 409, 264, 912, 43751, 382, 949, 11, 321, 50692], "temperature": 0.0, "avg_logprob": -0.1552295455013413, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.00026115216314792633}, {"id": 207, "seek": 100176, "start": 1008.3199999999999, "end": 1014.8, "text": " can see which cases are allocating and how many times.", "tokens": [50692, 393, 536, 597, 3331, 366, 12660, 990, 293, 577, 867, 1413, 13, 51016], "temperature": 0.0, "avg_logprob": -0.1552295455013413, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.00026115216314792633}, {"id": 208, "seek": 100176, "start": 1014.8, "end": 1020.88, "text": " And notice that we are not seeing the allocation listed here because, like I mentioned before,", "tokens": [51016, 400, 3449, 300, 321, 366, 406, 2577, 264, 27599, 10052, 510, 570, 11, 411, 286, 2835, 949, 11, 51320], "temperature": 0.0, "avg_logprob": -0.1552295455013413, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.00026115216314792633}, {"id": 209, "seek": 100176, "start": 1020.88, "end": 1024.36, "text": " we're returning the created value from the benchmark.", "tokens": [51320, 321, 434, 12678, 264, 2942, 2158, 490, 264, 18927, 13, 51494], "temperature": 0.0, "avg_logprob": -0.1552295455013413, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.00026115216314792633}, {"id": 210, "seek": 100176, "start": 1024.36, "end": 1030.32, "text": " And so that's being dropped after the sample is run.", "tokens": [51494, 400, 370, 300, 311, 885, 8119, 934, 264, 6889, 307, 1190, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1552295455013413, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.00026115216314792633}, {"id": 211, "seek": 103032, "start": 1030.32, "end": 1035.6, "text": " And I also want to note that the timings here are the same as before we did any allocation", "tokens": [50364, 400, 286, 611, 528, 281, 3637, 300, 264, 524, 1109, 510, 366, 264, 912, 382, 949, 321, 630, 604, 27599, 50628], "temperature": 0.0, "avg_logprob": -0.19261260986328124, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.0005882080877199769}, {"id": 212, "seek": 103032, "start": 1035.6, "end": 1037.48, "text": " profiling.", "tokens": [50628, 1740, 4883, 13, 50722], "temperature": 0.0, "avg_logprob": -0.19261260986328124, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.0005882080877199769}, {"id": 213, "seek": 103032, "start": 1037.48, "end": 1044.4399999999998, "text": " I managed to optimize this to a point that its footprint is pretty indistinguishable front", "tokens": [50722, 286, 6453, 281, 19719, 341, 281, 257, 935, 300, 1080, 24222, 307, 1238, 1016, 468, 7050, 742, 712, 1868, 51070], "temperature": 0.0, "avg_logprob": -0.19261260986328124, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.0005882080877199769}, {"id": 214, "seek": 103032, "start": 1044.4399999999998, "end": 1049.84, "text": " noise by using thread local storage and then optimizing that further, at least in the case", "tokens": [51070, 5658, 538, 1228, 7207, 2654, 6725, 293, 550, 40425, 300, 3052, 11, 412, 1935, 294, 264, 1389, 51340], "temperature": 0.0, "avg_logprob": -0.19261260986328124, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.0005882080877199769}, {"id": 215, "seek": 103032, "start": 1049.84, "end": 1053.6399999999999, "text": " of Mac OS.", "tokens": [51340, 295, 5707, 12731, 13, 51530], "temperature": 0.0, "avg_logprob": -0.19261260986328124, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.0005882080877199769}, {"id": 216, "seek": 105364, "start": 1053.64, "end": 1061.1200000000001, "text": " So if we look a little closer, we can see that, yeah, for smaller sizes, indeed, small", "tokens": [50364, 407, 498, 321, 574, 257, 707, 4966, 11, 321, 393, 536, 300, 11, 1338, 11, 337, 4356, 11602, 11, 6451, 11, 1359, 50738], "temperature": 0.0, "avg_logprob": -0.16707452495446365, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0002304678491782397}, {"id": 217, "seek": 105364, "start": 1061.1200000000001, "end": 1066.4, "text": " back is not going to be performing any heap allocations and is strictly doing its operations", "tokens": [50738, 646, 307, 406, 516, 281, 312, 10205, 604, 33591, 12660, 763, 293, 307, 20792, 884, 1080, 7705, 51002], "temperature": 0.0, "avg_logprob": -0.16707452495446365, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0002304678491782397}, {"id": 218, "seek": 105364, "start": 1066.4, "end": 1071.76, "text": " with the stack.", "tokens": [51002, 365, 264, 8630, 13, 51270], "temperature": 0.0, "avg_logprob": -0.16707452495446365, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0002304678491782397}, {"id": 219, "seek": 105364, "start": 1071.76, "end": 1076.4, "text": " We can also tell Devon the number of bytes we're processing or number of items we're", "tokens": [51270, 492, 393, 611, 980, 9096, 266, 264, 1230, 295, 36088, 321, 434, 9007, 420, 1230, 295, 4754, 321, 434, 51502], "temperature": 0.0, "avg_logprob": -0.16707452495446365, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0002304678491782397}, {"id": 220, "seek": 105364, "start": 1076.4, "end": 1078.0, "text": " processing.", "tokens": [51502, 9007, 13, 51582], "temperature": 0.0, "avg_logprob": -0.16707452495446365, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0002304678491782397}, {"id": 221, "seek": 105364, "start": 1078.0, "end": 1082.44, "text": " And this allows us to get a pretty different perspective.", "tokens": [51582, 400, 341, 4045, 505, 281, 483, 257, 1238, 819, 4585, 13, 51804], "temperature": 0.0, "avg_logprob": -0.16707452495446365, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0002304678491782397}, {"id": 222, "seek": 108244, "start": 1083.44, "end": 1086.64, "text": " The way we do this gets a little more complicated.", "tokens": [50414, 440, 636, 321, 360, 341, 2170, 257, 707, 544, 6179, 13, 50574], "temperature": 0.0, "avg_logprob": -0.2036761060173129, "compression_ratio": 1.6262135922330097, "no_speech_prob": 0.0010983716929331422}, {"id": 223, "seek": 108244, "start": 1086.64, "end": 1092.3600000000001, "text": " We change our function to take a venture argument and then we call the counter method on that", "tokens": [50574, 492, 1319, 527, 2445, 281, 747, 257, 18474, 6770, 293, 550, 321, 818, 264, 5682, 3170, 322, 300, 50860], "temperature": 0.0, "avg_logprob": -0.2036761060173129, "compression_ratio": 1.6262135922330097, "no_speech_prob": 0.0010983716929331422}, {"id": 224, "seek": 108244, "start": 1092.3600000000001, "end": 1096.1200000000001, "text": " and we pass it an instance of bytes count.", "tokens": [50860, 293, 321, 1320, 309, 364, 5197, 295, 36088, 1207, 13, 51048], "temperature": 0.0, "avg_logprob": -0.2036761060173129, "compression_ratio": 1.6262135922330097, "no_speech_prob": 0.0010983716929331422}, {"id": 225, "seek": 108244, "start": 1096.1200000000001, "end": 1105.6000000000001, "text": " In this case, we're saying that we're counting n 32-bit integers and then we pass a closure", "tokens": [51048, 682, 341, 1389, 11, 321, 434, 1566, 300, 321, 434, 13251, 297, 8858, 12, 5260, 41674, 293, 550, 321, 1320, 257, 24653, 51522], "temperature": 0.0, "avg_logprob": -0.2036761060173129, "compression_ratio": 1.6262135922330097, "no_speech_prob": 0.0010983716929331422}, {"id": 226, "seek": 108244, "start": 1105.6000000000001, "end": 1110.2, "text": " to benchmark our function from iterator implementation.", "tokens": [51522, 281, 18927, 527, 2445, 490, 17138, 1639, 11420, 13, 51752], "temperature": 0.0, "avg_logprob": -0.2036761060173129, "compression_ratio": 1.6262135922330097, "no_speech_prob": 0.0010983716929331422}, {"id": 227, "seek": 111244, "start": 1113.2, "end": 1125.56, "text": " So, we then see that Devon will output the number of bytes being processed in terms of,", "tokens": [50402, 407, 11, 321, 550, 536, 300, 9096, 266, 486, 5598, 264, 1230, 295, 36088, 885, 18846, 294, 2115, 295, 11, 51020], "temperature": 0.0, "avg_logprob": -0.15592554637363978, "compression_ratio": 1.5393258426966292, "no_speech_prob": 9.605133527657017e-05}, {"id": 228, "seek": 111244, "start": 1125.56, "end": 1129.1200000000001, "text": " in this case, megabytes or gigabytes per second.", "tokens": [51020, 294, 341, 1389, 11, 10816, 24538, 420, 42741, 680, 1150, 13, 51198], "temperature": 0.0, "avg_logprob": -0.15592554637363978, "compression_ratio": 1.5393258426966292, "no_speech_prob": 9.605133527657017e-05}, {"id": 229, "seek": 111244, "start": 1129.1200000000001, "end": 1135.16, "text": " And for a lot of people, this might be an easier data point to get an intuition for", "tokens": [51198, 400, 337, 257, 688, 295, 561, 11, 341, 1062, 312, 364, 3571, 1412, 935, 281, 483, 364, 24002, 337, 51500], "temperature": 0.0, "avg_logprob": -0.15592554637363978, "compression_ratio": 1.5393258426966292, "no_speech_prob": 9.605133527657017e-05}, {"id": 230, "seek": 111244, "start": 1135.16, "end": 1139.28, "text": " the speed rather than just the strict timing numbers.", "tokens": [51500, 264, 3073, 2831, 813, 445, 264, 10910, 10822, 3547, 13, 51706], "temperature": 0.0, "avg_logprob": -0.15592554637363978, "compression_ratio": 1.5393258426966292, "no_speech_prob": 9.605133527657017e-05}, {"id": 231, "seek": 113928, "start": 1139.28, "end": 1146.04, "text": " For some people, saying growing numbers for better performance is just easier to think", "tokens": [50364, 1171, 512, 561, 11, 1566, 4194, 3547, 337, 1101, 3389, 307, 445, 3571, 281, 519, 50702], "temperature": 0.0, "avg_logprob": -0.17622623443603516, "compression_ratio": 1.4491978609625669, "no_speech_prob": 0.00017948952154256403}, {"id": 232, "seek": 113928, "start": 1146.04, "end": 1150.04, "text": " about.", "tokens": [50702, 466, 13, 50902], "temperature": 0.0, "avg_logprob": -0.17622623443603516, "compression_ratio": 1.4491978609625669, "no_speech_prob": 0.00017948952154256403}, {"id": 233, "seek": 113928, "start": 1150.04, "end": 1156.56, "text": " So to recap what I just covered, Devon has various features that really set it apart", "tokens": [50902, 407, 281, 20928, 437, 286, 445, 5343, 11, 9096, 266, 575, 3683, 4122, 300, 534, 992, 309, 4936, 51228], "temperature": 0.0, "avg_logprob": -0.17622623443603516, "compression_ratio": 1.4491978609625669, "no_speech_prob": 0.00017948952154256403}, {"id": 234, "seek": 113928, "start": 1156.56, "end": 1158.92, "text": " from existing solutions.", "tokens": [51228, 490, 6741, 6547, 13, 51346], "temperature": 0.0, "avg_logprob": -0.17622623443603516, "compression_ratio": 1.4491978609625669, "no_speech_prob": 0.00017948952154256403}, {"id": 235, "seek": 113928, "start": 1158.92, "end": 1164.24, "text": " I find that its API is just a lot simpler.", "tokens": [51346, 286, 915, 300, 1080, 9362, 307, 445, 257, 688, 18587, 13, 51612], "temperature": 0.0, "avg_logprob": -0.17622623443603516, "compression_ratio": 1.4491978609625669, "no_speech_prob": 0.00017948952154256403}, {"id": 236, "seek": 113928, "start": 1164.24, "end": 1168.28, "text": " It's easier to remember.", "tokens": [51612, 467, 311, 3571, 281, 1604, 13, 51814], "temperature": 0.0, "avg_logprob": -0.17622623443603516, "compression_ratio": 1.4491978609625669, "no_speech_prob": 0.00017948952154256403}, {"id": 237, "seek": 116828, "start": 1168.28, "end": 1177.48, "text": " I also really like how the compact output makes it pretty easy to consider various cases.", "tokens": [50364, 286, 611, 534, 411, 577, 264, 14679, 5598, 1669, 309, 1238, 1858, 281, 1949, 3683, 3331, 13, 50824], "temperature": 0.0, "avg_logprob": -0.1660106893171344, "compression_ratio": 1.5172413793103448, "no_speech_prob": 5.7371373259229586e-05}, {"id": 238, "seek": 116828, "start": 1177.48, "end": 1185.68, "text": " And as well as because you can parameterize your benchmarks across various cases, you", "tokens": [50824, 400, 382, 731, 382, 570, 291, 393, 13075, 1125, 428, 43751, 2108, 3683, 3331, 11, 291, 51234], "temperature": 0.0, "avg_logprob": -0.1660106893171344, "compression_ratio": 1.5172413793103448, "no_speech_prob": 5.7371373259229586e-05}, {"id": 239, "seek": 116828, "start": 1185.68, "end": 1196.08, "text": " can really just get a sense for the difference in performance depending on the scenario.", "tokens": [51234, 393, 534, 445, 483, 257, 2020, 337, 264, 2649, 294, 3389, 5413, 322, 264, 9005, 13, 51754], "temperature": 0.0, "avg_logprob": -0.1660106893171344, "compression_ratio": 1.5172413793103448, "no_speech_prob": 5.7371373259229586e-05}, {"id": 240, "seek": 119608, "start": 1196.08, "end": 1204.1599999999999, "text": " So I also really like that by going with an attribute macro, I realize that, oh, well,", "tokens": [50364, 407, 286, 611, 534, 411, 300, 538, 516, 365, 364, 19667, 18887, 11, 286, 4325, 300, 11, 1954, 11, 731, 11, 50768], "temperature": 0.0, "avg_logprob": -0.1657962929712583, "compression_ratio": 1.541237113402062, "no_speech_prob": 0.00012531258107628673}, {"id": 241, "seek": 119608, "start": 1204.1599999999999, "end": 1209.76, "text": " if you make the function generic, you can just pass the types in because you're just", "tokens": [50768, 498, 291, 652, 264, 2445, 19577, 11, 291, 393, 445, 1320, 264, 3467, 294, 570, 291, 434, 445, 51048], "temperature": 0.0, "avg_logprob": -0.1657962929712583, "compression_ratio": 1.541237113402062, "no_speech_prob": 0.00012531258107628673}, {"id": 242, "seek": 119608, "start": 1209.76, "end": 1213.6799999999998, "text": " parsing whatever you want as the options.", "tokens": [51048, 21156, 278, 2035, 291, 528, 382, 264, 3956, 13, 51244], "temperature": 0.0, "avg_logprob": -0.1657962929712583, "compression_ratio": 1.541237113402062, "no_speech_prob": 0.00012531258107628673}, {"id": 243, "seek": 119608, "start": 1213.6799999999998, "end": 1222.28, "text": " And so you can have benchmarks over various collections of the standard libraries, so", "tokens": [51244, 400, 370, 291, 393, 362, 43751, 670, 3683, 16641, 295, 264, 3832, 15148, 11, 370, 51674], "temperature": 0.0, "avg_logprob": -0.1657962929712583, "compression_ratio": 1.541237113402062, "no_speech_prob": 0.00012531258107628673}, {"id": 244, "seek": 122228, "start": 1222.28, "end": 1226.68, "text": " linked list, VEC, hash set, et cetera.", "tokens": [50364, 9408, 1329, 11, 691, 8140, 11, 22019, 992, 11, 1030, 11458, 13, 50584], "temperature": 0.0, "avg_logprob": -0.18443566276913598, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.033054836094379425}, {"id": 245, "seek": 122228, "start": 1226.68, "end": 1231.12, "text": " And you can see how different operations really differ between those collections.", "tokens": [50584, 400, 291, 393, 536, 577, 819, 7705, 534, 743, 1296, 729, 16641, 13, 50806], "temperature": 0.0, "avg_logprob": -0.18443566276913598, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.033054836094379425}, {"id": 246, "seek": 122228, "start": 1231.12, "end": 1235.92, "text": " So such operations that are pretty common like clear might be a lot slower on a linked", "tokens": [50806, 407, 1270, 7705, 300, 366, 1238, 2689, 411, 1850, 1062, 312, 257, 688, 14009, 322, 257, 9408, 51046], "temperature": 0.0, "avg_logprob": -0.18443566276913598, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.033054836094379425}, {"id": 247, "seek": 122228, "start": 1235.92, "end": 1241.68, "text": " list whereas on a VEC, it's pretty instant.", "tokens": [51046, 1329, 9735, 322, 257, 691, 8140, 11, 309, 311, 1238, 9836, 13, 51334], "temperature": 0.0, "avg_logprob": -0.18443566276913598, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.033054836094379425}, {"id": 248, "seek": 122228, "start": 1241.68, "end": 1250.6399999999999, "text": " And another feature that helps me a lot is thinking of the numbers in terms of throughput.", "tokens": [51334, 400, 1071, 4111, 300, 3665, 385, 257, 688, 307, 1953, 295, 264, 3547, 294, 2115, 295, 44629, 13, 51782], "temperature": 0.0, "avg_logprob": -0.18443566276913598, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.033054836094379425}, {"id": 249, "seek": 125064, "start": 1250.64, "end": 1257.72, "text": " I find that it tends to just be easier to understand than durations.", "tokens": [50364, 286, 915, 300, 309, 12258, 281, 445, 312, 3571, 281, 1223, 813, 4861, 763, 13, 50718], "temperature": 0.0, "avg_logprob": -0.07452643899356617, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.000588230206631124}, {"id": 250, "seek": 125064, "start": 1257.72, "end": 1264.64, "text": " As well as something that I find no existing tool out there does is you can track the number", "tokens": [50718, 1018, 731, 382, 746, 300, 286, 915, 572, 6741, 2290, 484, 456, 775, 307, 291, 393, 2837, 264, 1230, 51064], "temperature": 0.0, "avg_logprob": -0.07452643899356617, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.000588230206631124}, {"id": 251, "seek": 125064, "start": 1264.64, "end": 1271.48, "text": " of heap allocations at the same time that you're measuring the time being spent running", "tokens": [51064, 295, 33591, 12660, 763, 412, 264, 912, 565, 300, 291, 434, 13389, 264, 565, 885, 4418, 2614, 51406], "temperature": 0.0, "avg_logprob": -0.07452643899356617, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.000588230206631124}, {"id": 252, "seek": 125064, "start": 1271.48, "end": 1274.48, "text": " your benchmark.", "tokens": [51406, 428, 18927, 13, 51556], "temperature": 0.0, "avg_logprob": -0.07452643899356617, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.000588230206631124}, {"id": 253, "seek": 125064, "start": 1274.48, "end": 1279.5200000000002, "text": " As well as one feature I didn't mention here because I thought it might be a little complex", "tokens": [51556, 1018, 731, 382, 472, 4111, 286, 994, 380, 2152, 510, 570, 286, 1194, 309, 1062, 312, 257, 707, 3997, 51808], "temperature": 0.0, "avg_logprob": -0.07452643899356617, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.000588230206631124}, {"id": 254, "seek": 127952, "start": 1279.52, "end": 1285.2, "text": " to cover is you can do some interesting things like run benchmarks over multiple threads", "tokens": [50364, 281, 2060, 307, 291, 393, 360, 512, 1880, 721, 411, 1190, 43751, 670, 3866, 19314, 50648], "temperature": 0.0, "avg_logprob": -0.11661933597765471, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.002510607475414872}, {"id": 255, "seek": 127952, "start": 1285.2, "end": 1291.16, "text": " and this allows you to measure the effects of contention on atomics and locks.", "tokens": [50648, 293, 341, 4045, 291, 281, 3481, 264, 5065, 295, 660, 1251, 322, 12018, 1167, 293, 20703, 13, 50946], "temperature": 0.0, "avg_logprob": -0.11661933597765471, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.002510607475414872}, {"id": 256, "seek": 127952, "start": 1291.16, "end": 1301.52, "text": " So if you're developing a low-level synchronization library, you might find this to be pretty useful.", "tokens": [50946, 407, 498, 291, 434, 6416, 257, 2295, 12, 12418, 19331, 2144, 6405, 11, 291, 1062, 915, 341, 281, 312, 1238, 4420, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11661933597765471, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.002510607475414872}, {"id": 257, "seek": 127952, "start": 1301.52, "end": 1307.68, "text": " I also want to cover what motivated me to pursue this.", "tokens": [51464, 286, 611, 528, 281, 2060, 437, 14515, 385, 281, 12392, 341, 13, 51772], "temperature": 0.0, "avg_logprob": -0.11661933597765471, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.002510607475414872}, {"id": 258, "seek": 130768, "start": 1307.68, "end": 1314.8400000000001, "text": " I found that a lot of existing tools in space were pretty good but their APIs had some, in", "tokens": [50364, 286, 1352, 300, 257, 688, 295, 6741, 3873, 294, 1901, 645, 1238, 665, 457, 641, 21445, 632, 512, 11, 294, 50722], "temperature": 0.0, "avg_logprob": -0.15119379216974432, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.0012060570297762752}, {"id": 259, "seek": 130768, "start": 1314.8400000000001, "end": 1321.64, "text": " my opinion, unnecessary complexity and so I wanted an API that didn't go too far beyond", "tokens": [50722, 452, 4800, 11, 19350, 14024, 293, 370, 286, 1415, 364, 9362, 300, 994, 380, 352, 886, 1400, 4399, 51062], "temperature": 0.0, "avg_logprob": -0.15119379216974432, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.0012060570297762752}, {"id": 260, "seek": 130768, "start": 1321.64, "end": 1326.68, "text": " the complexity of the code that you're benchmarking itself.", "tokens": [51062, 264, 14024, 295, 264, 3089, 300, 291, 434, 18927, 278, 2564, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15119379216974432, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.0012060570297762752}, {"id": 261, "seek": 130768, "start": 1326.68, "end": 1334.72, "text": " And I really appreciated that by trying this new API, open up new possibilities such as", "tokens": [51314, 400, 286, 534, 17169, 300, 538, 1382, 341, 777, 9362, 11, 1269, 493, 777, 12178, 1270, 382, 51716], "temperature": 0.0, "avg_logprob": -0.15119379216974432, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.0012060570297762752}, {"id": 262, "seek": 133472, "start": 1334.72, "end": 1340.24, "text": " what I mentioned before with benchmarking generic functions, it was relatively straightforward", "tokens": [50364, 437, 286, 2835, 949, 365, 18927, 278, 19577, 6828, 11, 309, 390, 7226, 15325, 50640], "temperature": 0.0, "avg_logprob": -0.16729798461451675, "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.0008420728845521808}, {"id": 263, "seek": 133472, "start": 1340.24, "end": 1344.1200000000001, "text": " to implement that which was a bit of a surprise.", "tokens": [50640, 281, 4445, 300, 597, 390, 257, 857, 295, 257, 6365, 13, 50834], "temperature": 0.0, "avg_logprob": -0.16729798461451675, "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.0008420728845521808}, {"id": 264, "seek": 133472, "start": 1344.1200000000001, "end": 1350.24, "text": " So some food for thought if you're developing your own libraries.", "tokens": [50834, 407, 512, 1755, 337, 1194, 498, 291, 434, 6416, 428, 1065, 15148, 13, 51140], "temperature": 0.0, "avg_logprob": -0.16729798461451675, "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.0008420728845521808}, {"id": 265, "seek": 133472, "start": 1350.24, "end": 1361.0, "text": " And I also found that the default way that some other tools run is pretty slow and I", "tokens": [51140, 400, 286, 611, 1352, 300, 264, 7576, 636, 300, 512, 661, 3873, 1190, 307, 1238, 2964, 293, 286, 51678], "temperature": 0.0, "avg_logprob": -0.16729798461451675, "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.0008420728845521808}, {"id": 266, "seek": 136100, "start": 1361.08, "end": 1368.72, "text": " get why they're trying to do a lot of statistical analysis to remove outliers.", "tokens": [50368, 483, 983, 436, 434, 1382, 281, 360, 257, 688, 295, 22820, 5215, 281, 4159, 484, 23646, 13, 50750], "temperature": 0.0, "avg_logprob": -0.12803688049316406, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.01080839242786169}, {"id": 267, "seek": 136100, "start": 1368.72, "end": 1377.36, "text": " But there are some cases where you do actually want to know when the code was especially", "tokens": [50750, 583, 456, 366, 512, 3331, 689, 291, 360, 767, 528, 281, 458, 562, 264, 3089, 390, 2318, 51182], "temperature": 0.0, "avg_logprob": -0.12803688049316406, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.01080839242786169}, {"id": 268, "seek": 136100, "start": 1377.36, "end": 1383.04, "text": " slow because if you're benchmarking over various inputs, it's possible that one case", "tokens": [51182, 2964, 570, 498, 291, 434, 18927, 278, 670, 3683, 15743, 11, 309, 311, 1944, 300, 472, 1389, 51466], "temperature": 0.0, "avg_logprob": -0.12803688049316406, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.01080839242786169}, {"id": 269, "seek": 136100, "start": 1383.04, "end": 1385.84, "text": " just happened to create a really large string.", "tokens": [51466, 445, 2011, 281, 1884, 257, 534, 2416, 6798, 13, 51606], "temperature": 0.0, "avg_logprob": -0.12803688049316406, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.01080839242786169}, {"id": 270, "seek": 138584, "start": 1385.84, "end": 1392.72, "text": " And so you want to be able to get a sense for everything that happened, not just the", "tokens": [50364, 400, 370, 291, 528, 281, 312, 1075, 281, 483, 257, 2020, 337, 1203, 300, 2011, 11, 406, 445, 264, 50708], "temperature": 0.0, "avg_logprob": -0.15620006455315483, "compression_ratio": 1.4863387978142077, "no_speech_prob": 0.001097861328162253}, {"id": 271, "seek": 138584, "start": 1392.72, "end": 1397.8, "text": " best case scenarios in my opinion.", "tokens": [50708, 1151, 1389, 15077, 294, 452, 4800, 13, 50962], "temperature": 0.0, "avg_logprob": -0.15620006455315483, "compression_ratio": 1.4863387978142077, "no_speech_prob": 0.001097861328162253}, {"id": 272, "seek": 138584, "start": 1397.8, "end": 1403.56, "text": " And if you do want to run your benchmarks for longer, have larger sample sizes, more", "tokens": [50962, 400, 498, 291, 360, 528, 281, 1190, 428, 43751, 337, 2854, 11, 362, 4833, 6889, 11602, 11, 544, 51250], "temperature": 0.0, "avg_logprob": -0.15620006455315483, "compression_ratio": 1.4863387978142077, "no_speech_prob": 0.001097861328162253}, {"id": 273, "seek": 138584, "start": 1403.56, "end": 1406.36, "text": " samples, there's also options to do that.", "tokens": [51250, 10938, 11, 456, 311, 611, 3956, 281, 360, 300, 13, 51390], "temperature": 0.0, "avg_logprob": -0.15620006455315483, "compression_ratio": 1.4863387978142077, "no_speech_prob": 0.001097861328162253}, {"id": 274, "seek": 138584, "start": 1406.36, "end": 1410.72, "text": " So you're not restricted.", "tokens": [51390, 407, 291, 434, 406, 20608, 13, 51608], "temperature": 0.0, "avg_logprob": -0.15620006455315483, "compression_ratio": 1.4863387978142077, "no_speech_prob": 0.001097861328162253}, {"id": 275, "seek": 141072, "start": 1410.72, "end": 1417.52, "text": " I also want to mention some other Rust performance measuring tools that I think are very much", "tokens": [50364, 286, 611, 528, 281, 2152, 512, 661, 34952, 3389, 13389, 3873, 300, 286, 519, 366, 588, 709, 50704], "temperature": 0.0, "avg_logprob": -0.17008960569227063, "compression_ratio": 1.5023923444976077, "no_speech_prob": 0.0019242082489654422}, {"id": 276, "seek": 141072, "start": 1417.52, "end": 1419.24, "text": " worth considering.", "tokens": [50704, 3163, 8079, 13, 50790], "temperature": 0.0, "avg_logprob": -0.17008960569227063, "compression_ratio": 1.5023923444976077, "no_speech_prob": 0.0019242082489654422}, {"id": 277, "seek": 141072, "start": 1419.24, "end": 1425.68, "text": " So criterion is obviously the current go to Rust benchmarking library.", "tokens": [50790, 407, 46691, 307, 2745, 264, 2190, 352, 281, 34952, 18927, 278, 6405, 13, 51112], "temperature": 0.0, "avg_logprob": -0.17008960569227063, "compression_ratio": 1.5023923444976077, "no_speech_prob": 0.0019242082489654422}, {"id": 278, "seek": 141072, "start": 1425.68, "end": 1431.16, "text": " A feature that I really particularly like about it is its graph output because I'm a", "tokens": [51112, 316, 4111, 300, 286, 534, 4098, 411, 466, 309, 307, 1080, 4295, 5598, 570, 286, 478, 257, 51386], "temperature": 0.0, "avg_logprob": -0.17008960569227063, "compression_ratio": 1.5023923444976077, "no_speech_prob": 0.0019242082489654422}, {"id": 279, "seek": 141072, "start": 1431.16, "end": 1432.48, "text": " very visual person.", "tokens": [51386, 588, 5056, 954, 13, 51452], "temperature": 0.0, "avg_logprob": -0.17008960569227063, "compression_ratio": 1.5023923444976077, "no_speech_prob": 0.0019242082489654422}, {"id": 280, "seek": 141072, "start": 1432.48, "end": 1436.92, "text": " I also do graphic design.", "tokens": [51452, 286, 611, 360, 14089, 1715, 13, 51674], "temperature": 0.0, "avg_logprob": -0.17008960569227063, "compression_ratio": 1.5023923444976077, "no_speech_prob": 0.0019242082489654422}, {"id": 281, "seek": 143692, "start": 1436.92, "end": 1441.04, "text": " Another newer micro benchmarking library is Tango.", "tokens": [50364, 3996, 17628, 4532, 18927, 278, 6405, 307, 314, 17150, 13, 50570], "temperature": 0.0, "avg_logprob": -0.1509361120370718, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.033550065010786057}, {"id": 282, "seek": 143692, "start": 1441.04, "end": 1448.1200000000001, "text": " And what I find unique about it is that it has this technique called paired benchmarking", "tokens": [50570, 400, 437, 286, 915, 3845, 466, 309, 307, 300, 309, 575, 341, 6532, 1219, 25699, 18927, 278, 50924], "temperature": 0.0, "avg_logprob": -0.1509361120370718, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.033550065010786057}, {"id": 283, "seek": 143692, "start": 1448.1200000000001, "end": 1453.92, "text": " where the execution gets interleaved between benchmarks.", "tokens": [50924, 689, 264, 15058, 2170, 728, 306, 12865, 1296, 43751, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1509361120370718, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.033550065010786057}, {"id": 284, "seek": 143692, "start": 1453.92, "end": 1462.0, "text": " And what this does is it spreads whatever systemic negative conditions evenly across", "tokens": [51214, 400, 437, 341, 775, 307, 309, 25728, 2035, 23789, 3671, 4487, 17658, 2108, 51618], "temperature": 0.0, "avg_logprob": -0.1509361120370718, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.033550065010786057}, {"id": 285, "seek": 143692, "start": 1462.0, "end": 1463.52, "text": " your benchmarks.", "tokens": [51618, 428, 43751, 13, 51694], "temperature": 0.0, "avg_logprob": -0.1509361120370718, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.033550065010786057}, {"id": 286, "seek": 146352, "start": 1463.52, "end": 1468.92, "text": " And so this is certainly a feature I eventually want to have in Devon.", "tokens": [50364, 400, 370, 341, 307, 3297, 257, 4111, 286, 4728, 528, 281, 362, 294, 9096, 266, 13, 50634], "temperature": 0.0, "avg_logprob": -0.1934143304824829, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.011327113024890423}, {"id": 287, "seek": 146352, "start": 1468.92, "end": 1477.6, "text": " Currently my APIs tries to prevent requiring ownership of the closure you're passing in.", "tokens": [50634, 19964, 452, 21445, 9898, 281, 4871, 24165, 15279, 295, 264, 24653, 291, 434, 8437, 294, 13, 51068], "temperature": 0.0, "avg_logprob": -0.1934143304824829, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.011327113024890423}, {"id": 288, "seek": 146352, "start": 1477.6, "end": 1479.8799999999999, "text": " I might have to change that to make this work.", "tokens": [51068, 286, 1062, 362, 281, 1319, 300, 281, 652, 341, 589, 13, 51182], "temperature": 0.0, "avg_logprob": -0.1934143304824829, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.011327113024890423}, {"id": 289, "seek": 146352, "start": 1479.8799999999999, "end": 1480.8799999999999, "text": " I don't know.", "tokens": [51182, 286, 500, 380, 458, 13, 51232], "temperature": 0.0, "avg_logprob": -0.1934143304824829, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.011327113024890423}, {"id": 290, "seek": 146352, "start": 1480.8799999999999, "end": 1483.48, "text": " I think if we had co-routines, I could make it work.", "tokens": [51232, 286, 519, 498, 321, 632, 598, 12, 81, 346, 1652, 11, 286, 727, 652, 309, 589, 13, 51362], "temperature": 0.0, "avg_logprob": -0.1934143304824829, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.011327113024890423}, {"id": 291, "seek": 146352, "start": 1483.48, "end": 1484.84, "text": " But I don't know.", "tokens": [51362, 583, 286, 500, 380, 458, 13, 51430], "temperature": 0.0, "avg_logprob": -0.1934143304824829, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.011327113024890423}, {"id": 292, "seek": 146352, "start": 1484.84, "end": 1493.32, "text": " Maybe if someone knows how to abuse asynchol weight to get co-routines, please talk to me.", "tokens": [51430, 2704, 498, 1580, 3255, 577, 281, 9852, 382, 2534, 339, 401, 3364, 281, 483, 598, 12, 81, 346, 1652, 11, 1767, 751, 281, 385, 13, 51854], "temperature": 0.0, "avg_logprob": -0.1934143304824829, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.011327113024890423}, {"id": 293, "seek": 149332, "start": 1493.32, "end": 1496.6399999999999, "text": " Another very useful tool is flame graphs.", "tokens": [50364, 3996, 588, 4420, 2290, 307, 13287, 24877, 13, 50530], "temperature": 0.0, "avg_logprob": -0.12203940916597174, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.006687261164188385}, {"id": 294, "seek": 149332, "start": 1496.6399999999999, "end": 1500.36, "text": " This is more of a general technique that's well known across the industry.", "tokens": [50530, 639, 307, 544, 295, 257, 2674, 6532, 300, 311, 731, 2570, 2108, 264, 3518, 13, 50716], "temperature": 0.0, "avg_logprob": -0.12203940916597174, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.006687261164188385}, {"id": 295, "seek": 149332, "start": 1500.36, "end": 1503.56, "text": " There's plenty of blog posts about this.", "tokens": [50716, 821, 311, 7140, 295, 6968, 12300, 466, 341, 13, 50876], "temperature": 0.0, "avg_logprob": -0.12203940916597174, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.006687261164188385}, {"id": 296, "seek": 149332, "start": 1503.56, "end": 1509.24, "text": " But for those who aren't familiar, it's a visualization tool that really helps you find", "tokens": [50876, 583, 337, 729, 567, 3212, 380, 4963, 11, 309, 311, 257, 25801, 2290, 300, 534, 3665, 291, 915, 51160], "temperature": 0.0, "avg_logprob": -0.12203940916597174, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.006687261164188385}, {"id": 297, "seek": 149332, "start": 1509.24, "end": 1511.8799999999999, "text": " where to focus your time.", "tokens": [51160, 689, 281, 1879, 428, 565, 13, 51292], "temperature": 0.0, "avg_logprob": -0.12203940916597174, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.006687261164188385}, {"id": 298, "seek": 149332, "start": 1511.8799999999999, "end": 1518.2, "text": " And I think it's very important to find where the bottleneck in your code is before you", "tokens": [51292, 400, 286, 519, 309, 311, 588, 1021, 281, 915, 689, 264, 44641, 547, 294, 428, 3089, 307, 949, 291, 51608], "temperature": 0.0, "avg_logprob": -0.12203940916597174, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.006687261164188385}, {"id": 299, "seek": 151820, "start": 1518.2, "end": 1526.48, "text": " actually start picking at specific places to optimize and do microbenchmarks on.", "tokens": [50364, 767, 722, 8867, 412, 2685, 3190, 281, 19719, 293, 360, 4532, 47244, 37307, 322, 13, 50778], "temperature": 0.0, "avg_logprob": -0.1375815901037765, "compression_ratio": 1.5, "no_speech_prob": 0.0023587706964462996}, {"id": 300, "seek": 151820, "start": 1526.48, "end": 1535.92, "text": " So try to reach for profiling with flame graphs before you do microbenchmarking, if you can.", "tokens": [50778, 407, 853, 281, 2524, 337, 1740, 4883, 365, 13287, 24877, 949, 291, 360, 4532, 47244, 5638, 278, 11, 498, 291, 393, 13, 51250], "temperature": 0.0, "avg_logprob": -0.1375815901037765, "compression_ratio": 1.5, "no_speech_prob": 0.0023587706964462996}, {"id": 301, "seek": 151820, "start": 1535.92, "end": 1540.04, "text": " As well as there's the DHAT crate.", "tokens": [51250, 1018, 731, 382, 456, 311, 264, 28606, 2218, 42426, 13, 51456], "temperature": 0.0, "avg_logprob": -0.1375815901037765, "compression_ratio": 1.5, "no_speech_prob": 0.0023587706964462996}, {"id": 302, "seek": 151820, "start": 1540.04, "end": 1547.28, "text": " And like I mentioned before, every single time an allocation operation happens, it takes", "tokens": [51456, 400, 411, 286, 2835, 949, 11, 633, 2167, 565, 364, 27599, 6916, 2314, 11, 309, 2516, 51818], "temperature": 0.0, "avg_logprob": -0.1375815901037765, "compression_ratio": 1.5, "no_speech_prob": 0.0023587706964462996}, {"id": 303, "seek": 154728, "start": 1547.28, "end": 1548.32, "text": " a back trace.", "tokens": [50364, 257, 646, 13508, 13, 50416], "temperature": 0.0, "avg_logprob": -0.17583839709942156, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.01564810611307621}, {"id": 304, "seek": 154728, "start": 1548.32, "end": 1554.8, "text": " And so it's able to give you pretty deep insights about where you're allocating memory and how", "tokens": [50416, 400, 370, 309, 311, 1075, 281, 976, 291, 1238, 2452, 14310, 466, 689, 291, 434, 12660, 990, 4675, 293, 577, 50740], "temperature": 0.0, "avg_logprob": -0.17583839709942156, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.01564810611307621}, {"id": 305, "seek": 154728, "start": 1554.8, "end": 1556.24, "text": " you're allocating memory.", "tokens": [50740, 291, 434, 12660, 990, 4675, 13, 50812], "temperature": 0.0, "avg_logprob": -0.17583839709942156, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.01564810611307621}, {"id": 306, "seek": 154728, "start": 1556.24, "end": 1563.92, "text": " It's also able to do some other stuff such as tracking max heap usage at a time.", "tokens": [50812, 467, 311, 611, 1075, 281, 360, 512, 661, 1507, 1270, 382, 11603, 11469, 33591, 14924, 412, 257, 565, 13, 51196], "temperature": 0.0, "avg_logprob": -0.17583839709942156, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.01564810611307621}, {"id": 307, "seek": 154728, "start": 1563.92, "end": 1568.96, "text": " I'm going to try to add that to Devon, but unfortunately it adds a bit of overhead.", "tokens": [51196, 286, 478, 516, 281, 853, 281, 909, 300, 281, 9096, 266, 11, 457, 7015, 309, 10860, 257, 857, 295, 19922, 13, 51448], "temperature": 0.0, "avg_logprob": -0.17583839709942156, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.01564810611307621}, {"id": 308, "seek": 154728, "start": 1568.96, "end": 1573.2, "text": " So maybe it's possible to subtract that overhead from the timings.", "tokens": [51448, 407, 1310, 309, 311, 1944, 281, 16390, 300, 19922, 490, 264, 524, 1109, 13, 51660], "temperature": 0.0, "avg_logprob": -0.17583839709942156, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.01564810611307621}, {"id": 309, "seek": 154728, "start": 1573.2, "end": 1576.44, "text": " We'll see.", "tokens": [51660, 492, 603, 536, 13, 51822], "temperature": 0.0, "avg_logprob": -0.17583839709942156, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.01564810611307621}, {"id": 310, "seek": 157644, "start": 1576.44, "end": 1584.8, "text": " And so some thoughts I want to leave you with is if you're going to be doing reaching for", "tokens": [50364, 400, 370, 512, 4598, 286, 528, 281, 1856, 291, 365, 307, 498, 291, 434, 516, 281, 312, 884, 9906, 337, 50782], "temperature": 0.0, "avg_logprob": -0.1468517575945173, "compression_ratio": 1.4345549738219896, "no_speech_prob": 0.0002377507626079023}, {"id": 311, "seek": 157644, "start": 1584.8, "end": 1595.68, "text": " microbenchmarking tools like criterion, Devon, Tango, really figure out if it's worth microoptimizing", "tokens": [50782, 4532, 47244, 5638, 278, 3873, 411, 46691, 11, 9096, 266, 11, 314, 17150, 11, 534, 2573, 484, 498, 309, 311, 3163, 4532, 5747, 332, 3319, 51326], "temperature": 0.0, "avg_logprob": -0.1468517575945173, "compression_ratio": 1.4345549738219896, "no_speech_prob": 0.0002377507626079023}, {"id": 312, "seek": 157644, "start": 1595.68, "end": 1604.28, "text": " that kind of code, just try to find the meteor performance issues in your program.", "tokens": [51326, 300, 733, 295, 3089, 11, 445, 853, 281, 915, 264, 25313, 3389, 2663, 294, 428, 1461, 13, 51756], "temperature": 0.0, "avg_logprob": -0.1468517575945173, "compression_ratio": 1.4345549738219896, "no_speech_prob": 0.0002377507626079023}, {"id": 313, "seek": 160428, "start": 1604.28, "end": 1608.8, "text": " So like I mentioned, flame graphs are particularly good for that.", "tokens": [50364, 407, 411, 286, 2835, 11, 13287, 24877, 366, 4098, 665, 337, 300, 13, 50590], "temperature": 0.0, "avg_logprob": -0.17601558343688053, "compression_ratio": 1.5422885572139304, "no_speech_prob": 0.025159059092402458}, {"id": 314, "seek": 160428, "start": 1608.8, "end": 1614.6, "text": " And also rather than having standalone benchmarks, you should be comparing it between different", "tokens": [50590, 400, 611, 2831, 813, 1419, 37454, 43751, 11, 291, 820, 312, 15763, 309, 1296, 819, 50880], "temperature": 0.0, "avg_logprob": -0.17601558343688053, "compression_ratio": 1.5422885572139304, "no_speech_prob": 0.025159059092402458}, {"id": 315, "seek": 160428, "start": 1614.6, "end": 1620.8, "text": " cases so you can measure across different inputs and implementations.", "tokens": [50880, 3331, 370, 291, 393, 3481, 2108, 819, 15743, 293, 4445, 763, 13, 51190], "temperature": 0.0, "avg_logprob": -0.17601558343688053, "compression_ratio": 1.5422885572139304, "no_speech_prob": 0.025159059092402458}, {"id": 316, "seek": 160428, "start": 1620.8, "end": 1629.6399999999999, "text": " So like I showed before, with Devon, you can benchmark origin error functions.", "tokens": [51190, 407, 411, 286, 4712, 949, 11, 365, 9096, 266, 11, 291, 393, 18927, 4957, 6713, 6828, 13, 51632], "temperature": 0.0, "avg_logprob": -0.17601558343688053, "compression_ratio": 1.5422885572139304, "no_speech_prob": 0.025159059092402458}, {"id": 317, "seek": 162964, "start": 1629.64, "end": 1637.8000000000002, "text": " And this also, for example, in the case of small vec versus vec, really gives you a better", "tokens": [50364, 400, 341, 611, 11, 337, 1365, 11, 294, 264, 1389, 295, 1359, 42021, 5717, 42021, 11, 534, 2709, 291, 257, 1101, 50772], "temperature": 0.0, "avg_logprob": -0.17129933656151616, "compression_ratio": 1.5, "no_speech_prob": 0.017966054379940033}, {"id": 318, "seek": 162964, "start": 1637.8000000000002, "end": 1643.44, "text": " sense of is it really worth it to optimize your code using unsafe?", "tokens": [50772, 2020, 295, 307, 309, 534, 3163, 309, 281, 19719, 428, 3089, 1228, 35948, 30, 51054], "temperature": 0.0, "avg_logprob": -0.17129933656151616, "compression_ratio": 1.5, "no_speech_prob": 0.017966054379940033}, {"id": 319, "seek": 162964, "start": 1643.44, "end": 1649.68, "text": " And so try to find the specific scenarios where you actually are getting those wins", "tokens": [51054, 400, 370, 853, 281, 915, 264, 2685, 15077, 689, 291, 767, 366, 1242, 729, 10641, 51366], "temperature": 0.0, "avg_logprob": -0.17129933656151616, "compression_ratio": 1.5, "no_speech_prob": 0.017966054379940033}, {"id": 320, "seek": 162964, "start": 1649.68, "end": 1656.72, "text": " because no one likes nasal demons.", "tokens": [51366, 570, 572, 472, 5902, 41575, 19733, 13, 51718], "temperature": 0.0, "avg_logprob": -0.17129933656151616, "compression_ratio": 1.5, "no_speech_prob": 0.017966054379940033}, {"id": 321, "seek": 165672, "start": 1656.8, "end": 1664.3600000000001, "text": " And also when making your own projects, since I imagine many people here are contributors", "tokens": [50368, 400, 611, 562, 1455, 428, 1065, 4455, 11, 1670, 286, 3811, 867, 561, 510, 366, 45627, 50746], "temperature": 0.0, "avg_logprob": -0.17778187448328192, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.0007790733361616731}, {"id": 322, "seek": 165672, "start": 1664.3600000000001, "end": 1671.3600000000001, "text": " to open source and have their own stuff that they're proud of, I really strongly advise", "tokens": [50746, 281, 1269, 4009, 293, 362, 641, 1065, 1507, 300, 436, 434, 4570, 295, 11, 286, 534, 10613, 18312, 51096], "temperature": 0.0, "avg_logprob": -0.17778187448328192, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.0007790733361616731}, {"id": 323, "seek": 165672, "start": 1671.3600000000001, "end": 1675.52, "text": " that you don't let perfect be the enemy of good.", "tokens": [51096, 300, 291, 500, 380, 718, 2176, 312, 264, 5945, 295, 665, 13, 51304], "temperature": 0.0, "avg_logprob": -0.17778187448328192, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.0007790733361616731}, {"id": 324, "seek": 165672, "start": 1675.52, "end": 1680.52, "text": " Devon has a lot of features that criterion doesn't have, but also vice versa.", "tokens": [51304, 9096, 266, 575, 257, 688, 295, 4122, 300, 46691, 1177, 380, 362, 11, 457, 611, 11964, 25650, 13, 51554], "temperature": 0.0, "avg_logprob": -0.17778187448328192, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.0007790733361616731}, {"id": 325, "seek": 165672, "start": 1680.52, "end": 1686.04, "text": " Devon doesn't have graphs or machine readable output yet.", "tokens": [51554, 9096, 266, 1177, 380, 362, 24877, 420, 3479, 49857, 5598, 1939, 13, 51830], "temperature": 0.0, "avg_logprob": -0.17778187448328192, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.0007790733361616731}, {"id": 326, "seek": 168604, "start": 1686.36, "end": 1691.1599999999999, "text": " I do eventually want to get there, but I didn't let that stop me from publishing something", "tokens": [50380, 286, 360, 4728, 528, 281, 483, 456, 11, 457, 286, 994, 380, 718, 300, 1590, 385, 490, 17832, 746, 50620], "temperature": 0.0, "avg_logprob": -0.12897342184315558, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.00024131439568009228}, {"id": 327, "seek": 168604, "start": 1691.1599999999999, "end": 1696.72, "text": " that I felt was good that people might want to use.", "tokens": [50620, 300, 286, 2762, 390, 665, 300, 561, 1062, 528, 281, 764, 13, 50898], "temperature": 0.0, "avg_logprob": -0.12897342184315558, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.00024131439568009228}, {"id": 328, "seek": 168604, "start": 1696.72, "end": 1704.1599999999999, "text": " And so try to focus on the features that matter to you most or at least are the most academically", "tokens": [50898, 400, 370, 853, 281, 1879, 322, 264, 4122, 300, 1871, 281, 291, 881, 420, 412, 1935, 366, 264, 881, 48944, 51270], "temperature": 0.0, "avg_logprob": -0.12897342184315558, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.00024131439568009228}, {"id": 329, "seek": 168604, "start": 1704.1599999999999, "end": 1705.92, "text": " interesting.", "tokens": [51270, 1880, 13, 51358], "temperature": 0.0, "avg_logprob": -0.12897342184315558, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.00024131439568009228}, {"id": 330, "seek": 168604, "start": 1705.92, "end": 1708.76, "text": " Not everything needs to be a full-fledged product.", "tokens": [51358, 1726, 1203, 2203, 281, 312, 257, 1577, 12, 69, 1493, 3004, 1674, 13, 51500], "temperature": 0.0, "avg_logprob": -0.12897342184315558, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.00024131439568009228}, {"id": 331, "seek": 168604, "start": 1708.76, "end": 1713.56, "text": " Definitely try to pursue your interests when making your own projects.", "tokens": [51500, 12151, 853, 281, 12392, 428, 8847, 562, 1455, 428, 1065, 4455, 13, 51740], "temperature": 0.0, "avg_logprob": -0.12897342184315558, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.00024131439568009228}, {"id": 332, "seek": 171356, "start": 1713.9199999999998, "end": 1717.9199999999998, "text": " Always remember that you can fill in the gaps later if you want.", "tokens": [50382, 11270, 1604, 300, 291, 393, 2836, 294, 264, 15031, 1780, 498, 291, 528, 13, 50582], "temperature": 0.0, "avg_logprob": -0.26404685974121095, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.004195055924355984}, {"id": 333, "seek": 171356, "start": 1720.36, "end": 1723.8, "text": " So that's all I had for this.", "tokens": [50704, 407, 300, 311, 439, 286, 632, 337, 341, 13, 50876], "temperature": 0.0, "avg_logprob": -0.26404685974121095, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.004195055924355984}, {"id": 334, "seek": 171356, "start": 1723.8, "end": 1733.32, "text": " I plan to have questions, but also in the meantime, you can read more about me.", "tokens": [50876, 286, 1393, 281, 362, 1651, 11, 457, 611, 294, 264, 14991, 11, 291, 393, 1401, 544, 466, 385, 13, 51352], "temperature": 0.0, "avg_logprob": -0.26404685974121095, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.004195055924355984}, {"id": 335, "seek": 171356, "start": 1733.32, "end": 1736.76, "text": " And currently I just have one blog post on there about Devon.", "tokens": [51352, 400, 4362, 286, 445, 362, 472, 6968, 2183, 322, 456, 466, 9096, 266, 13, 51524], "temperature": 0.0, "avg_logprob": -0.26404685974121095, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.004195055924355984}, {"id": 336, "seek": 171356, "start": 1736.76, "end": 1743.52, "text": " I plan to publish another thing on kind of like std-conditional T in C++, but in", "tokens": [51524, 286, 1393, 281, 11374, 1071, 551, 322, 733, 295, 411, 342, 67, 12, 18882, 2628, 314, 294, 383, 25472, 11, 457, 294, 51862], "temperature": 0.0, "avg_logprob": -0.26404685974121095, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.004195055924355984}, {"id": 337, "seek": 174352, "start": 1743.52, "end": 1750.52, "text": " Rust, which is as cursed as it sounds if you're familiar with std-conditional T.", "tokens": [50364, 34952, 11, 597, 307, 382, 29498, 382, 309, 3263, 498, 291, 434, 4963, 365, 342, 67, 12, 18882, 2628, 314, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18414815266927084, "compression_ratio": 1.490566037735849, "no_speech_prob": 0.0003148489922750741}, {"id": 338, "seek": 174352, "start": 1750.52, "end": 1758.96, "text": " You can also follow me on mastodon or Twitter if I refuse to call it X.", "tokens": [50714, 509, 393, 611, 1524, 385, 322, 27055, 378, 266, 420, 5794, 498, 286, 16791, 281, 818, 309, 1783, 13, 51136], "temperature": 0.0, "avg_logprob": -0.18414815266927084, "compression_ratio": 1.490566037735849, "no_speech_prob": 0.0003148489922750741}, {"id": 339, "seek": 174352, "start": 1758.96, "end": 1761.72, "text": " You can check out the code for Devon.", "tokens": [51136, 509, 393, 1520, 484, 264, 3089, 337, 9096, 266, 13, 51274], "temperature": 0.0, "avg_logprob": -0.18414815266927084, "compression_ratio": 1.490566037735849, "no_speech_prob": 0.0003148489922750741}, {"id": 340, "seek": 174352, "start": 1761.72, "end": 1763.56, "text": " Please give it a star.", "tokens": [51274, 2555, 976, 309, 257, 3543, 13, 51366], "temperature": 0.0, "avg_logprob": -0.18414815266927084, "compression_ratio": 1.490566037735849, "no_speech_prob": 0.0003148489922750741}, {"id": 341, "seek": 174352, "start": 1763.56, "end": 1765.84, "text": " Play around with it.", "tokens": [51366, 5506, 926, 365, 309, 13, 51480], "temperature": 0.0, "avg_logprob": -0.18414815266927084, "compression_ratio": 1.490566037735849, "no_speech_prob": 0.0003148489922750741}, {"id": 342, "seek": 174352, "start": 1765.84, "end": 1771.92, "text": " And yeah, if you want to reach out to me, I'm pretty accessible through mastodon.", "tokens": [51480, 400, 1338, 11, 498, 291, 528, 281, 2524, 484, 281, 385, 11, 286, 478, 1238, 9515, 807, 27055, 378, 266, 13, 51784], "temperature": 0.0, "avg_logprob": -0.18414815266927084, "compression_ratio": 1.490566037735849, "no_speech_prob": 0.0003148489922750741}, {"id": 343, "seek": 177192, "start": 1771.92, "end": 1778.48, "text": " So there I'm hacky-derm at Nicolai.", "tokens": [50364, 407, 456, 286, 478, 10339, 88, 12, 67, 966, 412, 14776, 401, 1301, 13, 50692], "temperature": 0.0, "avg_logprob": -0.47033134909237134, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.004735805559903383}, {"id": 344, "seek": 177192, "start": 1778.48, "end": 1780.52, "text": " So yeah, any questions?", "tokens": [50692, 407, 1338, 11, 604, 1651, 30, 50794], "temperature": 0.0, "avg_logprob": -0.47033134909237134, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.004735805559903383}, {"id": 345, "seek": 177192, "start": 1780.52, "end": 1788.2, "text": " We do have ten minutes for questions, so I'll plant you.", "tokens": [50794, 492, 360, 362, 2064, 2077, 337, 1651, 11, 370, 286, 603, 3709, 291, 13, 51178], "temperature": 0.0, "avg_logprob": -0.47033134909237134, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.004735805559903383}, {"id": 346, "seek": 177192, "start": 1788.2, "end": 1789.2, "text": " Just raise your hand.", "tokens": [51178, 1449, 5300, 428, 1011, 13, 51228], "temperature": 0.0, "avg_logprob": -0.47033134909237134, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.004735805559903383}, {"id": 347, "seek": 177192, "start": 1789.2, "end": 1790.2, "text": " I'm going to come to you.", "tokens": [51228, 286, 478, 516, 281, 808, 281, 291, 13, 51278], "temperature": 0.0, "avg_logprob": -0.47033134909237134, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.004735805559903383}, {"id": 348, "seek": 177192, "start": 1796.44, "end": 1798.24, "text": " So Nicolai, thanks for your talk first.", "tokens": [51590, 407, 14776, 401, 1301, 11, 3231, 337, 428, 751, 700, 13, 51680], "temperature": 0.0, "avg_logprob": -0.47033134909237134, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.004735805559903383}, {"id": 349, "seek": 177192, "start": 1798.24, "end": 1799.4, "text": " Very nice.", "tokens": [51680, 4372, 1481, 13, 51738], "temperature": 0.0, "avg_logprob": -0.47033134909237134, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.004735805559903383}, {"id": 350, "seek": 177192, "start": 1799.4, "end": 1801.52, "text": " And I have two questions.", "tokens": [51738, 400, 286, 362, 732, 1651, 13, 51844], "temperature": 0.0, "avg_logprob": -0.47033134909237134, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.004735805559903383}, {"id": 351, "seek": 180152, "start": 1801.52, "end": 1806.8, "text": " The first question is, have you thought about integrating into CI CD, so continuous integration", "tokens": [50364, 440, 700, 1168, 307, 11, 362, 291, 1194, 466, 26889, 666, 37777, 6743, 11, 370, 10957, 10980, 50628], "temperature": 0.0, "avg_logprob": -0.22294374214586363, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.12014911323785782}, {"id": 352, "seek": 180152, "start": 1806.8, "end": 1807.8, "text": " things?", "tokens": [50628, 721, 30, 50678], "temperature": 0.0, "avg_logprob": -0.22294374214586363, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.12014911323785782}, {"id": 353, "seek": 180152, "start": 1807.8, "end": 1811.92, "text": " That like, to me it seemed like this is a very handy tool with that, which I can use if I", "tokens": [50678, 663, 411, 11, 281, 385, 309, 6576, 411, 341, 307, 257, 588, 13239, 2290, 365, 300, 11, 597, 286, 393, 764, 498, 286, 50884], "temperature": 0.0, "avg_logprob": -0.22294374214586363, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.12014911323785782}, {"id": 354, "seek": 180152, "start": 1811.92, "end": 1814.76, "text": " have a problem at hand, which I want to analyze.", "tokens": [50884, 362, 257, 1154, 412, 1011, 11, 597, 286, 528, 281, 12477, 13, 51026], "temperature": 0.0, "avg_logprob": -0.22294374214586363, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.12014911323785782}, {"id": 355, "seek": 180152, "start": 1814.76, "end": 1820.04, "text": " I quickly can do some benchmark and then dig deeper.", "tokens": [51026, 286, 2661, 393, 360, 512, 18927, 293, 550, 2528, 7731, 13, 51290], "temperature": 0.0, "avg_logprob": -0.22294374214586363, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.12014911323785782}, {"id": 356, "seek": 180152, "start": 1820.04, "end": 1824.76, "text": " But I think if I have found an issue in a very specific place, I might also want to", "tokens": [51290, 583, 286, 519, 498, 286, 362, 1352, 364, 2734, 294, 257, 588, 2685, 1081, 11, 286, 1062, 611, 528, 281, 51526], "temperature": 0.0, "avg_logprob": -0.22294374214586363, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.12014911323785782}, {"id": 357, "seek": 180152, "start": 1824.76, "end": 1829.8799999999999, "text": " have a test case out of it that I can monitor or be alarmed in my CI CD if there is an issue", "tokens": [51526, 362, 257, 1500, 1389, 484, 295, 309, 300, 286, 393, 6002, 420, 312, 27597, 1912, 294, 452, 37777, 6743, 498, 456, 307, 364, 2734, 51782], "temperature": 0.0, "avg_logprob": -0.22294374214586363, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.12014911323785782}, {"id": 358, "seek": 180152, "start": 1829.8799999999999, "end": 1830.8799999999999, "text": " again.", "tokens": [51782, 797, 13, 51832], "temperature": 0.0, "avg_logprob": -0.22294374214586363, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.12014911323785782}, {"id": 359, "seek": 183088, "start": 1831.24, "end": 1832.8400000000001, "text": " So that was the first question.", "tokens": [50382, 407, 300, 390, 264, 700, 1168, 13, 50462], "temperature": 0.0, "avg_logprob": -0.27224854363335504, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.0033035618253052235}, {"id": 360, "seek": 183088, "start": 1832.8400000000001, "end": 1837.88, "text": " And the second question would be, is it possible to run all those benchmarks in parallel, or", "tokens": [50462, 400, 264, 1150, 1168, 576, 312, 11, 307, 309, 1944, 281, 1190, 439, 729, 43751, 294, 8952, 11, 420, 50714], "temperature": 0.0, "avg_logprob": -0.27224854363335504, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.0033035618253052235}, {"id": 361, "seek": 183088, "start": 1837.88, "end": 1844.8400000000001, "text": " do you have to sequentialize them in order to get the measurements right?", "tokens": [50714, 360, 291, 362, 281, 42881, 1125, 552, 294, 1668, 281, 483, 264, 15383, 558, 30, 51062], "temperature": 0.0, "avg_logprob": -0.27224854363335504, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.0033035618253052235}, {"id": 362, "seek": 183088, "start": 1844.8400000000001, "end": 1846.5600000000002, "text": " Both great questions.", "tokens": [51062, 6767, 869, 1651, 13, 51148], "temperature": 0.0, "avg_logprob": -0.27224854363335504, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.0033035618253052235}, {"id": 363, "seek": 183088, "start": 1846.5600000000002, "end": 1853.64, "text": " So right now, what's blocking getting a lot of value out of running your benchmarks in", "tokens": [51148, 407, 558, 586, 11, 437, 311, 17776, 1242, 257, 688, 295, 2158, 484, 295, 2614, 428, 43751, 294, 51502], "temperature": 0.0, "avg_logprob": -0.27224854363335504, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.0033035618253052235}, {"id": 364, "seek": 183088, "start": 1853.64, "end": 1857.0400000000002, "text": " CI is that Devon doesn't yet have programmatic output.", "tokens": [51502, 37777, 307, 300, 9096, 266, 1177, 380, 1939, 362, 1461, 25915, 5598, 13, 51672], "temperature": 0.0, "avg_logprob": -0.27224854363335504, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.0033035618253052235}, {"id": 365, "seek": 185704, "start": 1857.04, "end": 1863.6399999999999, "text": " My plans have JSON output and maybe some other format, if that makes sense.", "tokens": [50364, 1222, 5482, 362, 31828, 5598, 293, 1310, 512, 661, 7877, 11, 498, 300, 1669, 2020, 13, 50694], "temperature": 0.0, "avg_logprob": -0.2405784376736345, "compression_ratio": 1.4058823529411764, "no_speech_prob": 0.004607194568961859}, {"id": 366, "seek": 185704, "start": 1863.6399999999999, "end": 1873.6399999999999, "text": " But yeah, as well as, so if you have programmatic output, then Devon can then consume previous", "tokens": [50694, 583, 1338, 11, 382, 731, 382, 11, 370, 498, 291, 362, 1461, 25915, 5598, 11, 550, 9096, 266, 393, 550, 14732, 3894, 51194], "temperature": 0.0, "avg_logprob": -0.2405784376736345, "compression_ratio": 1.4058823529411764, "no_speech_prob": 0.004607194568961859}, {"id": 367, "seek": 185704, "start": 1873.6399999999999, "end": 1880.36, "text": " iterations of that if you're benchmarking across different branches.", "tokens": [51194, 36540, 295, 300, 498, 291, 434, 18927, 278, 2108, 819, 14770, 13, 51530], "temperature": 0.0, "avg_logprob": -0.2405784376736345, "compression_ratio": 1.4058823529411764, "no_speech_prob": 0.004607194568961859}, {"id": 368, "seek": 188036, "start": 1880.36, "end": 1892.6399999999999, "text": " As well as the author of Tango was exploring some ideas of using a shared library to compile", "tokens": [50364, 1018, 731, 382, 264, 3793, 295, 314, 17150, 390, 12736, 512, 3487, 295, 1228, 257, 5507, 6405, 281, 31413, 50978], "temperature": 0.0, "avg_logprob": -0.25476699216025217, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.0008688977686688304}, {"id": 369, "seek": 188036, "start": 1892.6399999999999, "end": 1902.32, "text": " it against four different branches and to make that pretty straightforward with get-of-actions.", "tokens": [50978, 309, 1970, 1451, 819, 14770, 293, 281, 652, 300, 1238, 15325, 365, 483, 12, 2670, 12, 12299, 13, 51462], "temperature": 0.0, "avg_logprob": -0.25476699216025217, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.0008688977686688304}, {"id": 370, "seek": 188036, "start": 1902.32, "end": 1906.36, "text": " So yes, I'm definitely very interested in that.", "tokens": [51462, 407, 2086, 11, 286, 478, 2138, 588, 3102, 294, 300, 13, 51664], "temperature": 0.0, "avg_logprob": -0.25476699216025217, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.0008688977686688304}, {"id": 371, "seek": 190636, "start": 1907.36, "end": 1910.36, "text": " Sorry, repeat the second question.", "tokens": [50414, 4919, 11, 7149, 264, 1150, 1168, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2667838825899012, "compression_ratio": 1.5502645502645502, "no_speech_prob": 0.0009511299431324005}, {"id": 372, "seek": 190636, "start": 1912.36, "end": 1914.36, "text": " The second question was regarding the execution.", "tokens": [50664, 440, 1150, 1168, 390, 8595, 264, 15058, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2667838825899012, "compression_ratio": 1.5502645502645502, "no_speech_prob": 0.0009511299431324005}, {"id": 373, "seek": 190636, "start": 1914.36, "end": 1920.36, "text": " If you are able to execute more than one benchmark in parallel, and whether there's some impact", "tokens": [50764, 759, 291, 366, 1075, 281, 14483, 544, 813, 472, 18927, 294, 8952, 11, 293, 1968, 456, 311, 512, 2712, 51064], "temperature": 0.0, "avg_logprob": -0.2667838825899012, "compression_ratio": 1.5502645502645502, "no_speech_prob": 0.0009511299431324005}, {"id": 374, "seek": 190636, "start": 1920.36, "end": 1922.36, "text": " on the measurement itself.", "tokens": [51064, 322, 264, 13160, 2564, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2667838825899012, "compression_ratio": 1.5502645502645502, "no_speech_prob": 0.0009511299431324005}, {"id": 375, "seek": 190636, "start": 1923.36, "end": 1935.36, "text": " Yeah, so while technically you can, I find that putting the current process under more", "tokens": [51214, 865, 11, 370, 1339, 12120, 291, 393, 11, 286, 915, 300, 3372, 264, 2190, 1399, 833, 544, 51814], "temperature": 0.0, "avg_logprob": -0.2667838825899012, "compression_ratio": 1.5502645502645502, "no_speech_prob": 0.0009511299431324005}, {"id": 376, "seek": 193536, "start": 1935.36, "end": 1939.36, "text": " load could just negatively affect your timings.", "tokens": [50364, 3677, 727, 445, 29519, 3345, 428, 524, 1109, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11727286252108488, "compression_ratio": 1.4429530201342282, "no_speech_prob": 0.011369286105036736}, {"id": 377, "seek": 193536, "start": 1939.36, "end": 1949.36, "text": " And so it didn't seem reasonable to do that, although I haven't actually measured if that", "tokens": [50564, 400, 370, 309, 994, 380, 1643, 10585, 281, 360, 300, 11, 4878, 286, 2378, 380, 767, 12690, 498, 300, 51064], "temperature": 0.0, "avg_logprob": -0.11727286252108488, "compression_ratio": 1.4429530201342282, "no_speech_prob": 0.011369286105036736}, {"id": 378, "seek": 193536, "start": 1949.36, "end": 1952.36, "text": " would actually have as big of a negative impact as I would expect.", "tokens": [51064, 576, 767, 362, 382, 955, 295, 257, 3671, 2712, 382, 286, 576, 2066, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11727286252108488, "compression_ratio": 1.4429530201342282, "no_speech_prob": 0.011369286105036736}, {"id": 379, "seek": 193536, "start": 1960.36, "end": 1961.36, "text": " Thank you.", "tokens": [51614, 1044, 291, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11727286252108488, "compression_ratio": 1.4429530201342282, "no_speech_prob": 0.011369286105036736}, {"id": 380, "seek": 196136, "start": 1961.36, "end": 1973.36, "text": " One question I had is, is there a way to compare the execution time with and without the warm", "tokens": [50364, 1485, 1168, 286, 632, 307, 11, 307, 456, 257, 636, 281, 6794, 264, 15058, 565, 365, 293, 1553, 264, 4561, 50964], "temperature": 0.0, "avg_logprob": -0.11967186025671057, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.010842193849384785}, {"id": 381, "seek": 196136, "start": 1973.36, "end": 1974.36, "text": " cache?", "tokens": [50964, 19459, 30, 51014], "temperature": 0.0, "avg_logprob": -0.11967186025671057, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.010842193849384785}, {"id": 382, "seek": 196136, "start": 1974.36, "end": 1979.36, "text": " That is, the impact of cache on some data structures can be huge.", "tokens": [51014, 663, 307, 11, 264, 2712, 295, 19459, 322, 512, 1412, 9227, 393, 312, 2603, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11967186025671057, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.010842193849384785}, {"id": 383, "seek": 196136, "start": 1979.36, "end": 1985.36, "text": " And sometimes in benchmarking, in micro benchmarking especially, you have the problem that you're", "tokens": [51264, 400, 2171, 294, 18927, 278, 11, 294, 4532, 18927, 278, 2318, 11, 291, 362, 264, 1154, 300, 291, 434, 51564], "temperature": 0.0, "avg_logprob": -0.11967186025671057, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.010842193849384785}, {"id": 384, "seek": 196136, "start": 1985.36, "end": 1988.36, "text": " reusing the same lines.", "tokens": [51564, 319, 7981, 264, 912, 3876, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11967186025671057, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.010842193849384785}, {"id": 385, "seek": 198836, "start": 1988.36, "end": 1991.36, "text": " So the second benchmark is going to be faster always.", "tokens": [50364, 407, 264, 1150, 18927, 307, 516, 281, 312, 4663, 1009, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10280032274199695, "compression_ratio": 1.4751131221719458, "no_speech_prob": 0.001871429500170052}, {"id": 386, "seek": 198836, "start": 1991.36, "end": 1999.36, "text": " But maybe your use case is actually the one in which the cache is called, for instance.", "tokens": [50514, 583, 1310, 428, 764, 1389, 307, 767, 264, 472, 294, 597, 264, 19459, 307, 1219, 11, 337, 5197, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10280032274199695, "compression_ratio": 1.4751131221719458, "no_speech_prob": 0.001871429500170052}, {"id": 387, "seek": 198836, "start": 1999.36, "end": 2002.36, "text": " Yeah, great question as well.", "tokens": [50914, 865, 11, 869, 1168, 382, 731, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10280032274199695, "compression_ratio": 1.4751131221719458, "no_speech_prob": 0.001871429500170052}, {"id": 388, "seek": 198836, "start": 2002.36, "end": 2012.36, "text": " So I considered having a helper effect function to evict the CPU caches, although I haven't", "tokens": [51064, 407, 286, 4888, 1419, 257, 36133, 1802, 2445, 281, 1073, 985, 264, 13199, 269, 13272, 11, 4878, 286, 2378, 380, 51564], "temperature": 0.0, "avg_logprob": -0.10280032274199695, "compression_ratio": 1.4751131221719458, "no_speech_prob": 0.001871429500170052}, {"id": 389, "seek": 198836, "start": 2012.36, "end": 2017.36, "text": " thought of a good way of documenting when this is best to use.", "tokens": [51564, 1194, 295, 257, 665, 636, 295, 42360, 562, 341, 307, 1151, 281, 764, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10280032274199695, "compression_ratio": 1.4751131221719458, "no_speech_prob": 0.001871429500170052}, {"id": 390, "seek": 201836, "start": 2018.36, "end": 2027.36, "text": " But in lieu of that, you can apply as a method on the Bencher type.", "tokens": [50364, 583, 294, 26036, 295, 300, 11, 291, 393, 3079, 382, 257, 3170, 322, 264, 3964, 6759, 2010, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06973342334522921, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.0009244795073755085}, {"id": 391, "seek": 201836, "start": 2027.36, "end": 2032.36, "text": " You can pass a closure to generate inputs for every single iteration.", "tokens": [50814, 509, 393, 1320, 257, 24653, 281, 8460, 15743, 337, 633, 2167, 24784, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06973342334522921, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.0009244795073755085}, {"id": 392, "seek": 201836, "start": 2032.36, "end": 2039.36, "text": " And so if you wanted to, you could create a new buffer for every single time that function", "tokens": [51064, 400, 370, 498, 291, 1415, 281, 11, 291, 727, 1884, 257, 777, 21762, 337, 633, 2167, 565, 300, 2445, 51414], "temperature": 0.0, "avg_logprob": -0.06973342334522921, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.0009244795073755085}, {"id": 393, "seek": 201836, "start": 2039.36, "end": 2042.36, "text": " is being run at your benchmarking.", "tokens": [51414, 307, 885, 1190, 412, 428, 18927, 278, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06973342334522921, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.0009244795073755085}, {"id": 394, "seek": 204236, "start": 2042.36, "end": 2051.3599999999997, "text": " So since that would be in a different place in memory, the cache effects wouldn't make", "tokens": [50364, 407, 1670, 300, 576, 312, 294, 257, 819, 1081, 294, 4675, 11, 264, 19459, 5065, 2759, 380, 652, 50814], "temperature": 0.0, "avg_logprob": -0.11975750715836235, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.0009525878704153001}, {"id": 395, "seek": 204236, "start": 2051.3599999999997, "end": 2059.3599999999997, "text": " the benchmark seem so much faster than it might be in a real world case.", "tokens": [50814, 264, 18927, 1643, 370, 709, 4663, 813, 309, 1062, 312, 294, 257, 957, 1002, 1389, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11975750715836235, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.0009525878704153001}, {"id": 396, "seek": 204236, "start": 2059.3599999999997, "end": 2062.3599999999997, "text": " So we have a question from the matrix.", "tokens": [51214, 407, 321, 362, 257, 1168, 490, 264, 8141, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11975750715836235, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.0009525878704153001}, {"id": 397, "seek": 204236, "start": 2062.3599999999997, "end": 2063.3599999999997, "text": " So people are...", "tokens": [51364, 407, 561, 366, 485, 51414], "temperature": 0.0, "avg_logprob": -0.11975750715836235, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.0009525878704153001}, {"id": 398, "seek": 204236, "start": 2063.3599999999997, "end": 2065.3599999999997, "text": " Oh, Neo has a question.", "tokens": [51414, 876, 11, 24458, 575, 257, 1168, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11975750715836235, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.0009525878704153001}, {"id": 399, "seek": 204236, "start": 2065.3599999999997, "end": 2067.3599999999997, "text": " People are following online.", "tokens": [51514, 3432, 366, 3480, 2950, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11975750715836235, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.0009525878704153001}, {"id": 400, "seek": 204236, "start": 2067.3599999999997, "end": 2070.3599999999997, "text": " So it's a really good topic.", "tokens": [51614, 407, 309, 311, 257, 534, 665, 4829, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11975750715836235, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.0009525878704153001}, {"id": 401, "seek": 204236, "start": 2070.3599999999997, "end": 2071.3599999999997, "text": " It was a very good talk.", "tokens": [51764, 467, 390, 257, 588, 665, 751, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11975750715836235, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.0009525878704153001}, {"id": 402, "seek": 207136, "start": 2071.36, "end": 2073.36, "text": " The question is, thanks.", "tokens": [50364, 440, 1168, 307, 11, 3231, 13, 50464], "temperature": 0.0, "avg_logprob": -0.14552778356215534, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.008160005323588848}, {"id": 403, "seek": 207136, "start": 2073.36, "end": 2079.36, "text": " Devan looks very interesting and the API looks much cleaner, simpler than Criterions.", "tokens": [50464, 9096, 282, 1542, 588, 1880, 293, 264, 9362, 1542, 709, 16532, 11, 18587, 813, 4779, 1681, 626, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14552778356215534, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.008160005323588848}, {"id": 404, "seek": 207136, "start": 2079.36, "end": 2083.36, "text": " Now Criterion can compare across different runs or implementations and then summarize", "tokens": [50764, 823, 4779, 1681, 313, 393, 6794, 2108, 819, 6676, 420, 4445, 763, 293, 550, 20858, 50964], "temperature": 0.0, "avg_logprob": -0.14552778356215534, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.008160005323588848}, {"id": 405, "seek": 207136, "start": 2083.36, "end": 2086.36, "text": " whether performance improved or got worse.", "tokens": [50964, 1968, 3389, 9689, 420, 658, 5324, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14552778356215534, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.008160005323588848}, {"id": 406, "seek": 207136, "start": 2086.36, "end": 2088.36, "text": " Within some confidence interval.", "tokens": [51114, 15996, 512, 6687, 15035, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14552778356215534, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.008160005323588848}, {"id": 407, "seek": 207136, "start": 2088.36, "end": 2092.36, "text": " Does Devan have something similar or plan to?", "tokens": [51214, 4402, 9096, 282, 362, 746, 2531, 420, 1393, 281, 30, 51414], "temperature": 0.0, "avg_logprob": -0.14552778356215534, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.008160005323588848}, {"id": 408, "seek": 207136, "start": 2092.36, "end": 2095.36, "text": " Yeah, so it currently does not.", "tokens": [51414, 865, 11, 370, 309, 4362, 775, 406, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14552778356215534, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.008160005323588848}, {"id": 409, "seek": 209536, "start": 2095.36, "end": 2103.36, "text": " I found that I kind of shoehorned myself a bit with this output format in that it's", "tokens": [50364, 286, 1352, 300, 286, 733, 295, 12796, 31990, 292, 2059, 257, 857, 365, 341, 5598, 7877, 294, 300, 309, 311, 50764], "temperature": 0.0, "avg_logprob": -0.1064496702618069, "compression_ratio": 1.4971098265895955, "no_speech_prob": 0.019090497866272926}, {"id": 410, "seek": 209536, "start": 2103.36, "end": 2108.36, "text": " not super easy to add a lot more information to that.", "tokens": [50764, 406, 1687, 1858, 281, 909, 257, 688, 544, 1589, 281, 300, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1064496702618069, "compression_ratio": 1.4971098265895955, "no_speech_prob": 0.019090497866272926}, {"id": 411, "seek": 209536, "start": 2108.36, "end": 2114.36, "text": " And so it's kind of like has become a bit of a UI problem in a way, which I find interesting", "tokens": [51014, 400, 370, 309, 311, 733, 295, 411, 575, 1813, 257, 857, 295, 257, 15682, 1154, 294, 257, 636, 11, 597, 286, 915, 1880, 51314], "temperature": 0.0, "avg_logprob": -0.1064496702618069, "compression_ratio": 1.4971098265895955, "no_speech_prob": 0.019090497866272926}, {"id": 412, "seek": 209536, "start": 2114.36, "end": 2118.36, "text": " given that's a command line.", "tokens": [51314, 2212, 300, 311, 257, 5622, 1622, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1064496702618069, "compression_ratio": 1.4971098265895955, "no_speech_prob": 0.019090497866272926}, {"id": 413, "seek": 211836, "start": 2118.36, "end": 2128.36, "text": " But yeah, I would very much like to just directly tell the user that this is faster", "tokens": [50364, 583, 1338, 11, 286, 576, 588, 709, 411, 281, 445, 3838, 980, 264, 4195, 300, 341, 307, 4663, 50864], "temperature": 0.0, "avg_logprob": -0.10730889948402964, "compression_ratio": 1.5, "no_speech_prob": 0.01939903199672699}, {"id": 414, "seek": 211836, "start": 2128.36, "end": 2131.36, "text": " or slower than previous runs.", "tokens": [50864, 420, 14009, 813, 3894, 6676, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10730889948402964, "compression_ratio": 1.5, "no_speech_prob": 0.01939903199672699}, {"id": 415, "seek": 211836, "start": 2131.36, "end": 2140.36, "text": " There's also the issue that, for example, if I have my laptop plugged in, now my benchmark", "tokens": [51014, 821, 311, 611, 264, 2734, 300, 11, 337, 1365, 11, 498, 286, 362, 452, 10732, 25679, 294, 11, 586, 452, 18927, 51464], "temperature": 0.0, "avg_logprob": -0.10730889948402964, "compression_ratio": 1.5, "no_speech_prob": 0.01939903199672699}, {"id": 416, "seek": 211836, "start": 2140.36, "end": 2141.36, "text": " runs faster.", "tokens": [51464, 6676, 4663, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10730889948402964, "compression_ratio": 1.5, "no_speech_prob": 0.01939903199672699}, {"id": 417, "seek": 211836, "start": 2141.36, "end": 2144.36, "text": " If I don't have it plugged in, then it's slower.", "tokens": [51514, 759, 286, 500, 380, 362, 309, 25679, 294, 11, 550, 309, 311, 14009, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10730889948402964, "compression_ratio": 1.5, "no_speech_prob": 0.01939903199672699}, {"id": 418, "seek": 211836, "start": 2144.36, "end": 2145.36, "text": " It gets throttled.", "tokens": [51664, 467, 2170, 739, 1521, 1493, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10730889948402964, "compression_ratio": 1.5, "no_speech_prob": 0.01939903199672699}, {"id": 419, "seek": 214536, "start": 2145.36, "end": 2154.36, "text": " So it's not always obvious that there was a change in the implementation that caused", "tokens": [50364, 407, 309, 311, 406, 1009, 6322, 300, 456, 390, 257, 1319, 294, 264, 11420, 300, 7008, 50814], "temperature": 0.0, "avg_logprob": -0.10339699188868205, "compression_ratio": 1.53551912568306, "no_speech_prob": 0.0003682025708258152}, {"id": 420, "seek": 214536, "start": 2154.36, "end": 2159.36, "text": " the function to get slower.", "tokens": [50814, 264, 2445, 281, 483, 14009, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10339699188868205, "compression_ratio": 1.53551912568306, "no_speech_prob": 0.0003682025708258152}, {"id": 421, "seek": 214536, "start": 2159.36, "end": 2164.36, "text": " And I believe that Criterion's documentation has like a big warning section about this", "tokens": [51064, 400, 286, 1697, 300, 4779, 1681, 313, 311, 14333, 575, 411, 257, 955, 9164, 3541, 466, 341, 51314], "temperature": 0.0, "avg_logprob": -0.10339699188868205, "compression_ratio": 1.53551912568306, "no_speech_prob": 0.0003682025708258152}, {"id": 422, "seek": 214536, "start": 2164.36, "end": 2166.36, "text": " issue.", "tokens": [51314, 2734, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10339699188868205, "compression_ratio": 1.53551912568306, "no_speech_prob": 0.0003682025708258152}, {"id": 423, "seek": 214536, "start": 2166.36, "end": 2171.36, "text": " But yeah, I do think that is valuable to have and I would like to have it.", "tokens": [51414, 583, 1338, 11, 286, 360, 519, 300, 307, 8263, 281, 362, 293, 286, 576, 411, 281, 362, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10339699188868205, "compression_ratio": 1.53551912568306, "no_speech_prob": 0.0003682025708258152}, {"id": 424, "seek": 217136, "start": 2172.36, "end": 2182.36, "text": " And also, if you all are actually very interested in this project, feel free to submit ideas", "tokens": [50414, 400, 611, 11, 498, 291, 439, 366, 767, 588, 3102, 294, 341, 1716, 11, 841, 1737, 281, 10315, 3487, 50914], "temperature": 0.0, "avg_logprob": -0.11638199537992477, "compression_ratio": 1.4252873563218391, "no_speech_prob": 0.016138507053256035}, {"id": 425, "seek": 217136, "start": 2182.36, "end": 2188.36, "text": " to the GitHub page for it or better pull requests, implement the features that you'd like to", "tokens": [50914, 281, 264, 23331, 3028, 337, 309, 420, 1101, 2235, 12475, 11, 4445, 264, 4122, 300, 291, 1116, 411, 281, 51214], "temperature": 0.0, "avg_logprob": -0.11638199537992477, "compression_ratio": 1.4252873563218391, "no_speech_prob": 0.016138507053256035}, {"id": 426, "seek": 217136, "start": 2188.36, "end": 2189.36, "text": " see.", "tokens": [51214, 536, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11638199537992477, "compression_ratio": 1.4252873563218391, "no_speech_prob": 0.016138507053256035}, {"id": 427, "seek": 217136, "start": 2189.36, "end": 2193.36, "text": " I'm only one person and only have so many hours in a day.", "tokens": [51264, 286, 478, 787, 472, 954, 293, 787, 362, 370, 867, 2496, 294, 257, 786, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11638199537992477, "compression_ratio": 1.4252873563218391, "no_speech_prob": 0.016138507053256035}, {"id": 428, "seek": 220136, "start": 2201.36, "end": 2212.36, "text": " Yeah, I have two questions.", "tokens": [50364, 865, 11, 286, 362, 732, 1651, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14905395815449377, "compression_ratio": 1.1666666666666667, "no_speech_prob": 0.030234219506382942}, {"id": 429, "seek": 220136, "start": 2212.36, "end": 2220.36, "text": " The first one was you mentioned that some of the flaws or design differences with Criterion", "tokens": [50914, 440, 700, 472, 390, 291, 2835, 300, 512, 295, 264, 27108, 420, 1715, 7300, 365, 4779, 1681, 313, 51314], "temperature": 0.0, "avg_logprob": -0.14905395815449377, "compression_ratio": 1.1666666666666667, "no_speech_prob": 0.030234219506382942}, {"id": 430, "seek": 222036, "start": 2220.36, "end": 2228.36, "text": " was that it focused a lot on very, I don't know, horrible.", "tokens": [50364, 390, 300, 309, 5178, 257, 688, 322, 588, 11, 286, 500, 380, 458, 11, 9263, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20983461711717688, "compression_ratio": 1.3109243697478992, "no_speech_prob": 0.4792824387550354}, {"id": 431, "seek": 222036, "start": 2228.36, "end": 2240.36, "text": " It's a lot in statistics and instead of just giving you the fastest, the average and all", "tokens": [50764, 467, 311, 257, 688, 294, 12523, 293, 2602, 295, 445, 2902, 291, 264, 14573, 11, 264, 4274, 293, 439, 51364], "temperature": 0.0, "avg_logprob": -0.20983461711717688, "compression_ratio": 1.3109243697478992, "no_speech_prob": 0.4792824387550354}, {"id": 432, "seek": 222036, "start": 2240.36, "end": 2241.36, "text": " of that.", "tokens": [51364, 295, 300, 13, 51414], "temperature": 0.0, "avg_logprob": -0.20983461711717688, "compression_ratio": 1.3109243697478992, "no_speech_prob": 0.4792824387550354}, {"id": 433, "seek": 224136, "start": 2241.36, "end": 2252.36, "text": " I was wondering if there is a mechanism in Devon to output, like for example, percentiles", "tokens": [50364, 286, 390, 6359, 498, 456, 307, 257, 7513, 294, 9096, 266, 281, 5598, 11, 411, 337, 1365, 11, 3043, 4680, 50914], "temperature": 0.0, "avg_logprob": -0.18223195917466106, "compression_ratio": 1.4081632653061225, "no_speech_prob": 0.060719557106494904}, {"id": 434, "seek": 224136, "start": 2252.36, "end": 2254.36, "text": " or something like that.", "tokens": [50914, 420, 746, 411, 300, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18223195917466106, "compression_ratio": 1.4081632653061225, "no_speech_prob": 0.060719557106494904}, {"id": 435, "seek": 224136, "start": 2254.36, "end": 2265.36, "text": " And my second question was when your benchmarking memory, if the function you're benchmarking", "tokens": [51014, 400, 452, 1150, 1168, 390, 562, 428, 18927, 278, 4675, 11, 498, 264, 2445, 291, 434, 18927, 278, 51564], "temperature": 0.0, "avg_logprob": -0.18223195917466106, "compression_ratio": 1.4081632653061225, "no_speech_prob": 0.060719557106494904}, {"id": 436, "seek": 226536, "start": 2265.36, "end": 2270.36, "text": " the allocates instead of returning all the memory that it allocated, would the output", "tokens": [50364, 264, 12660, 1024, 2602, 295, 12678, 439, 264, 4675, 300, 309, 29772, 11, 576, 264, 5598, 50614], "temperature": 0.0, "avg_logprob": -0.1494448102753738, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.03404638543725014}, {"id": 437, "seek": 226536, "start": 2270.36, "end": 2277.36, "text": " show the total memory allocated or just the memory remaining when the function returned?", "tokens": [50614, 855, 264, 3217, 4675, 29772, 420, 445, 264, 4675, 8877, 562, 264, 2445, 8752, 30, 50964], "temperature": 0.0, "avg_logprob": -0.1494448102753738, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.03404638543725014}, {"id": 438, "seek": 226536, "start": 2277.36, "end": 2290.36, "text": " Yeah, so any allocations that happen before the benchmark runs or not, they will or they", "tokens": [50964, 865, 11, 370, 604, 12660, 763, 300, 1051, 949, 264, 18927, 6676, 420, 406, 11, 436, 486, 420, 436, 51614], "temperature": 0.0, "avg_logprob": -0.1494448102753738, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.03404638543725014}, {"id": 439, "seek": 229036, "start": 2290.36, "end": 2294.36, "text": " will not be visible to Devon in a sense.", "tokens": [50364, 486, 406, 312, 8974, 281, 9096, 266, 294, 257, 2020, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10315745638817855, "compression_ratio": 1.7094017094017093, "no_speech_prob": 0.06453461199998856}, {"id": 440, "seek": 229036, "start": 2294.36, "end": 2299.36, "text": " It will have recorded that but it won't be associated with the current benchmark.", "tokens": [50564, 467, 486, 362, 8287, 300, 457, 309, 1582, 380, 312, 6615, 365, 264, 2190, 18927, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10315745638817855, "compression_ratio": 1.7094017094017093, "no_speech_prob": 0.06453461199998856}, {"id": 441, "seek": 229036, "start": 2299.36, "end": 2304.36, "text": " It will just get cleared before the benchmark runs.", "tokens": [50814, 467, 486, 445, 483, 19725, 949, 264, 18927, 6676, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10315745638817855, "compression_ratio": 1.7094017094017093, "no_speech_prob": 0.06453461199998856}, {"id": 442, "seek": 229036, "start": 2304.36, "end": 2310.36, "text": " So in that case, you would see that the number of the allocations would be greater than the", "tokens": [51064, 407, 294, 300, 1389, 11, 291, 576, 536, 300, 264, 1230, 295, 264, 12660, 763, 576, 312, 5044, 813, 264, 51364], "temperature": 0.0, "avg_logprob": -0.10315745638817855, "compression_ratio": 1.7094017094017093, "no_speech_prob": 0.06453461199998856}, {"id": 443, "seek": 229036, "start": 2310.36, "end": 2313.36, "text": " number of allocations in the benchmark.", "tokens": [51364, 1230, 295, 12660, 763, 294, 264, 18927, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10315745638817855, "compression_ratio": 1.7094017094017093, "no_speech_prob": 0.06453461199998856}, {"id": 444, "seek": 229036, "start": 2313.36, "end": 2319.36, "text": " And to answer your first question, when you say percentiles, are you talking about confidence", "tokens": [51514, 400, 281, 1867, 428, 700, 1168, 11, 562, 291, 584, 3043, 4680, 11, 366, 291, 1417, 466, 6687, 51814], "temperature": 0.0, "avg_logprob": -0.10315745638817855, "compression_ratio": 1.7094017094017093, "no_speech_prob": 0.06453461199998856}, {"id": 445, "seek": 231936, "start": 2319.36, "end": 2320.36, "text": " intervals?", "tokens": [50364, 26651, 30, 50414], "temperature": 0.0, "avg_logprob": -0.18338664961449894, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.1661258041858673}, {"id": 446, "seek": 231936, "start": 2320.36, "end": 2328.36, "text": " Well, no, I mean, it would be also an option but the first thing that came to mind to me", "tokens": [50414, 1042, 11, 572, 11, 286, 914, 11, 309, 576, 312, 611, 364, 3614, 457, 264, 700, 551, 300, 1361, 281, 1575, 281, 385, 50814], "temperature": 0.0, "avg_logprob": -0.18338664961449894, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.1661258041858673}, {"id": 447, "seek": 231936, "start": 2328.36, "end": 2329.36, "text": " was percentiles.", "tokens": [50814, 390, 3043, 4680, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18338664961449894, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.1661258041858673}, {"id": 448, "seek": 231936, "start": 2329.36, "end": 2340.36, "text": " Like when you order the outputs, like the timings in the sending order, which for example, I", "tokens": [50864, 1743, 562, 291, 1668, 264, 23930, 11, 411, 264, 524, 1109, 294, 264, 7750, 1668, 11, 597, 337, 1365, 11, 286, 51414], "temperature": 0.0, "avg_logprob": -0.18338664961449894, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.1661258041858673}, {"id": 449, "seek": 231936, "start": 2340.36, "end": 2348.36, "text": " don't know how to describe it right now, but yes, which was the 95th.", "tokens": [51414, 500, 380, 458, 577, 281, 6786, 309, 558, 586, 11, 457, 2086, 11, 597, 390, 264, 13420, 392, 13, 51814], "temperature": 0.0, "avg_logprob": -0.18338664961449894, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.1661258041858673}, {"id": 450, "seek": 234836, "start": 2348.36, "end": 2355.36, "text": " If you did 100 runs, which was the 95th fastest or slowest, for example.", "tokens": [50364, 759, 291, 630, 2319, 6676, 11, 597, 390, 264, 13420, 392, 14573, 420, 2964, 377, 11, 337, 1365, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1853079636891683, "compression_ratio": 1.349112426035503, "no_speech_prob": 0.023932715877890587}, {"id": 451, "seek": 234836, "start": 2355.36, "end": 2358.36, "text": " Yeah.", "tokens": [50714, 865, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1853079636891683, "compression_ratio": 1.349112426035503, "no_speech_prob": 0.023932715877890587}, {"id": 452, "seek": 234836, "start": 2358.36, "end": 2370.36, "text": " So I would like to have more interesting statistics output.", "tokens": [50864, 407, 286, 576, 411, 281, 362, 544, 1880, 12523, 5598, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1853079636891683, "compression_ratio": 1.349112426035503, "no_speech_prob": 0.023932715877890587}, {"id": 453, "seek": 234836, "start": 2370.36, "end": 2377.36, "text": " Right now, I was just focused on having what I considered was the most important for your", "tokens": [51464, 1779, 586, 11, 286, 390, 445, 5178, 322, 1419, 437, 286, 4888, 390, 264, 881, 1021, 337, 428, 51814], "temperature": 0.0, "avg_logprob": -0.1853079636891683, "compression_ratio": 1.349112426035503, "no_speech_prob": 0.023932715877890587}, {"id": 454, "seek": 237736, "start": 2377.36, "end": 2378.36, "text": " average benchmarks.", "tokens": [50364, 4274, 43751, 13, 50414], "temperature": 0.0, "avg_logprob": -0.12929445505142212, "compression_ratio": 1.3860759493670887, "no_speech_prob": 0.0054624383337795734}, {"id": 455, "seek": 237736, "start": 2378.36, "end": 2390.36, "text": " Like I'd also like to output what the variance is across all the samples.", "tokens": [50414, 1743, 286, 1116, 611, 411, 281, 5598, 437, 264, 21977, 307, 2108, 439, 264, 10938, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12929445505142212, "compression_ratio": 1.3860759493670887, "no_speech_prob": 0.0054624383337795734}, {"id": 456, "seek": 237736, "start": 2390.36, "end": 2401.36, "text": " So again, I kind of painted myself into a bit of a corner in that people usually only have", "tokens": [51014, 407, 797, 11, 286, 733, 295, 11797, 2059, 666, 257, 857, 295, 257, 4538, 294, 300, 561, 2673, 787, 362, 51564], "temperature": 0.0, "avg_logprob": -0.12929445505142212, "compression_ratio": 1.3860759493670887, "no_speech_prob": 0.0054624383337795734}, {"id": 457, "seek": 237736, "start": 2401.36, "end": 2403.36, "text": " so many columns in their terminal.", "tokens": [51564, 370, 867, 13766, 294, 641, 14709, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12929445505142212, "compression_ratio": 1.3860759493670887, "no_speech_prob": 0.0054624383337795734}, {"id": 458, "seek": 240336, "start": 2403.36, "end": 2412.36, "text": " And so this table output will be interesting to see how I add to it.", "tokens": [50364, 400, 370, 341, 3199, 5598, 486, 312, 1880, 281, 536, 577, 286, 909, 281, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06471187774449179, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.024030635133385658}, {"id": 459, "seek": 240336, "start": 2412.36, "end": 2419.36, "text": " I think what I'll end up doing is have options for emitting the columns that you're interested", "tokens": [50814, 286, 519, 437, 286, 603, 917, 493, 884, 307, 362, 3956, 337, 846, 2414, 264, 13766, 300, 291, 434, 3102, 51164], "temperature": 0.0, "avg_logprob": -0.06471187774449179, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.024030635133385658}, {"id": 460, "seek": 240336, "start": 2419.36, "end": 2423.36, "text": " in having and just have certain columns by default.", "tokens": [51164, 294, 1419, 293, 445, 362, 1629, 13766, 538, 7576, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06471187774449179, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.024030635133385658}, {"id": 461, "seek": 240336, "start": 2423.36, "end": 2429.36, "text": " So when I do end up getting around to having more interesting statistics, that'll probably", "tokens": [51364, 407, 562, 286, 360, 917, 493, 1242, 926, 281, 1419, 544, 1880, 12523, 11, 300, 603, 1391, 51664], "temperature": 0.0, "avg_logprob": -0.06471187774449179, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.024030635133385658}, {"id": 462, "seek": 242936, "start": 2429.36, "end": 2441.36, "text": " lead the way to make a user configurable of whether you end up with a giant table or not.", "tokens": [50364, 1477, 264, 636, 281, 652, 257, 4195, 22192, 712, 295, 1968, 291, 917, 493, 365, 257, 7410, 3199, 420, 406, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1313471379487411, "compression_ratio": 1.373913043478261, "no_speech_prob": 0.07921573519706726}, {"id": 463, "seek": 242936, "start": 2441.36, "end": 2442.36, "text": " Okay.", "tokens": [50964, 1033, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1313471379487411, "compression_ratio": 1.373913043478261, "no_speech_prob": 0.07921573519706726}, {"id": 464, "seek": 242936, "start": 2442.36, "end": 2444.36, "text": " Thank you very much for your talk and your answers.", "tokens": [51014, 1044, 291, 588, 709, 337, 428, 751, 293, 428, 6338, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1313471379487411, "compression_ratio": 1.373913043478261, "no_speech_prob": 0.07921573519706726}, {"id": 465, "seek": 242936, "start": 2444.36, "end": 2445.36, "text": " Thank you.", "tokens": [51114, 1044, 291, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1313471379487411, "compression_ratio": 1.373913043478261, "no_speech_prob": 0.07921573519706726}], "language": "en"}