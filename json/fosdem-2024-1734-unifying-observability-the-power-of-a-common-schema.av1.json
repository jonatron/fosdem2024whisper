{"text": " So, up next, we have Christos and Alex and unifying observability in the power of common schema. Okay, thanks everyone and welcome to our talk. We will in this presentation talk about the conversion story of two schemas of open telemetry in the elastic common schema. But let's first introduce ourselves. My name is Alex. I'm leading the open telemetry initiative at Elastic and I'm a co-maintenor of the open telemetry semantic conventions project. Hi, I'm Christos. I work on elastic as well and I'm software engineer focusing on observability and specifically open telemetry where I am a contributor and a prover on the semantic convention project. Okay, we would like to start with a quite easy and simple question. How many of you do know exactly what open telemetry is? That's great. I can skip some slides later. How many of you do know what semantic conventions is about? That's what I expected. And how many of you do know what elastic common schema is? Okay, thanks everyone. So let's deep dive a bit on the history of open source tools and standards in observability to give us a picture where the standards come from. Let me. Okay. No. Does that work? Okay, around, do you hear me? That works well. Okay. Around or a bit more than 10 years ago when microservice emerged that also changed the observability market and industry. That's when like big tech companies started building their own open source tools for collecting observability data. So tools like Zipkin, Jega for distributor traces emerged, the Elk stack for logging, Prometheus for metrics. We heard a lot about this in previous talks. And based on this defective standard tools, then actual standards emerged like open tracing, open sensors later for distributed tracing, open sensors also covered metrics and the open metrics as a derivative of Prometheus format emerged and Elastic has its own ECS that defines the semantics of structured logging data. Since we will talk a bit more about ECS, a quick introduction what that is. So ECS stands for the Elastic Com Schema and it's basically just a definition of a set of fields that describe the semantics in structured logging data. So for example, if you're collecting a service name with your observability data, the Com Schema tells you that you should put this value into a field that is called service.name, not app.name or application.name. So you have common names that you can later on search for and this also allows you to correlate data across different signals. Now as you can see, we already have at least four standards here that are partially competing, partially complementary. Plus we have all the tools that also create some defective standards for collecting data. So it's ridiculous to have so many standards, right? We need one more that covers all of them. And usually what happens is we have one more that is competing with all the others. And yes, we have one more standard for observability. OpenTelemedia will come back to the comic later again. This is the slide that I can skip based on the Paul. So OpenTelemedia provides not just a standard but a full ecosystem and framework for observability. For collecting data, sending it protocol. One thing that I want to highlight here, there is a specification in OpenTelemedia that defines what data you can collect, like traces, metrics, logs. OpenTelemedia working group is also working on a profiling signal. And what we will talk more about in this presentation is the semantic conventions. Semantic conventions are very similar to what I've shown for ECS. And basically defines, yeah, attribute names and their semantics. Let's have a concrete example of how the data structure in OpenTelemedia looks like here with some logging data. Very simplified view here, it's a bit more complex. But let's say we have a set of log records, right? The OpenTelemedia protocol defines like the core structure of that signal with fields like severity text, which is basically the log level and body, which is basically the log message. In addition, you can collect with your observability data additional context information. This is usually represented in so-called attributes, and that's where semantic conventions come into play. The semantic conventions define which attributes exist, their names, types, and also the semantics behind this. For example, if you're collecting an HTTP access log, right, and you want to capture the HTTP request method, this is the attribute name that you would use for it. Now observability data is usually also captured in a broader context for some resource like a concrete service, a host, or other resources. That's why OTLP wraps the actual observability data into a resource wrapper, and a resource again has a set of attributes, so-called resource attributes, that describe the resource, something like the service name, host name, and so on. So this is the structure in OpenTelemedia for collecting observability data, and semantic conventions is just about the attributes basically in their meaning in this data. Now let's come back to our timeline of standards. There's one important thing I didn't mention before. Actually OpenTelemedia, and we heard this in the previous talk, is the result of a merger between open tracing and open sensors. OpenTelemedia also supports Prometheus metrics and OpenMetrics that we have heard in some of the previous talks, and just last year, Elastic also announced the donation of ECS into OpenTelemedia. So coming back to this, the question is, is it really that we have one more competing standard? I would say actually not. With OTLP we have less competing standards, and OTLP really succeeds in reducing the amount of competing standards and becoming the one and single standard for observability. Now as I said before, Elastic announced the donation of ECS into the OTLP's semantic conventions project. Why? Yeah, because there are great benefits to this. First of all, there are complementary parts and strengths in both schemas that we now merge into one single schema. And second, we grow two different communities by merging them and providing a bigger network effect. So it's a huge win I think for the community, but there are not only benefits, there are also challenges, right? First of all, the overlap between the two schemas is a potential for schema conflicts. And to resolve these conflicts might mean that we need to have either breaking changes in the one schema or in the other. We have seen the structure of observability data in OpenTelemedia, which consists of the protocol with the nested structure plus the semantic conventions. It's quite different to how ECS defines the fields because ECS is just a plain definition of fields without like nested structures or so. So there's some difference resolving that is a bit of a challenge. Another interesting thing that we discovered when we started merging ECS is that in OpenTelemedia before the merger, many times attributes have been defined in a concrete context. For example, we have here an HTTP server span and the attribute HTTP route is basically defined under the semantic conventions for HTTP server spans. The problem is now if I want to use the same attribute in a different context like let's say HTTP access logs, I mean there was always a means just to reference the other attribute, but it feels sort of weird because in the one context is a first class, right, attribute and the other one is just a reference that overrides some semantics. So learning from ECS, what we already achieved with the merger is that now we have in OpenTelemedia a dedicated attributes registry that serves the case of just defining attributes with their types, with their meaning and in the different semantic conventions and their use cases we are just referencing those attributes. So we have clear separation between defining attributes and using them in a concrete context. And finally another challenge is metrics. Metrics formats in OpenTelemedia follow the TSTB model. So we have a concrete metric name like system disk IO in this case with a type, with a unit and we have a set of dimensions modeled as attributes. In this case direction for example for disk IO read or write. In ECS previously the metrics were basically modeled as numerical fields on documents and you can have multiple numerical fields in the documents so you can have multiple metrics. That's the reason why often some of these dimensions that we have in OpenTelemedia are just encoded into the metric name on ECS side. So we have things like disk read bytes or disk write bytes. This is quite a big difference in modeling. This is a case where we are learning basically from OpenTelemedia and adopting this at Elastic now also with Elastic Search supporting TSTB. So we see we are learning from both sides which is a great thing and we are coming to the best solution possible for the community. And Chris will tell you how this actual merger is happening in practice. Thank you. Can you hear me? Okay. So as Alex mentioned there are a lot of things going on so the question is when is time to celebrate the merger that everything has been completed and the truth is that we are not there yet. There are things that needs to be done and actually everyone believed in the beginning that once the merger was announced that that's all. I mean we have not anything to add there but yeah the truth is that the actual work started right after the merger was announced. So yeah let's see some examples of how the merger is happening and how things are moving forward. So I have some real examples here from the upstream repository on GitHub with issues and pull requests. So this one for example is trying to add some new resource attributes for the container images and specifically the digest of the image. So as we can see that PR was filed on the 4th of July I think yes and it took it some time to get seen right. So it took us like many review cycles more than 20 blocker comments actually there so lots of back and forth lots of discussions but that one was actually merged after almost two months. And another example is about a very important attribute the IP of the host hosted IP as we call it and this one was really unique really interesting actually because this PR was filed by a non ECS contributor. So actually that contributor used to work for a company that it's I would say completely unrelated to the ECS project but it was quite nice because in that case the existence of the ECS project was taken into account and there were very interesting conversations and it took us like almost three months to have it in. So yeah it's quite obvious with these examples that the merger was not something trivial not something straightforward that can happen from one day to the other by for example writing a script that will transfer everything from one project to the other or something like that. So we have decided to take an approach to move let's say not so fast and pay attention to the detail and have the proper people work on specific areas so as to leverage their expertise and be sure that what we are merging to the up seem to the final project which is actually the sematic convention of open telemetry will stay there and everyone will be happy with that in the future. So that's more or less the areas of the sematic conventions. We have areas in area about databases cloud containers Kubernetes HTTP system metric system resource attributes and many others. And yeah so we have started focusing on specific areas some examples is the effort that we are doing on the system metrics area we have a working group working there focusing on the stability of the area. We are in a really good position now we are moving towards the ability really soon and the same for the process namespace the process area the process resource attributes and the same for container area we are close to achieving the 100 percent converges there the recent going PR that will add the final attributes final metrics excuse me same for HTTP and network areas we have good coverage HTTP sematic conventions were declared as stable really recently so we are adding on top now which is quite nice and yeah we have work in progress in databases mobile areas cloud Kubernetes so we have working groups getting started and focusing on these areas and yeah over the past months we are focusing on making the project as good as possible it's a community driven way so we as ECS contributes to the contributors donating this project we are not only focusing on the merger itself but we want also to ensure that the sematic conventions project will be there and will can serve us in the future so we are also focusing on other things as well like improving the tooling of the project working on the guidelines this is quite important because there are many times that the guidelines of the one project are in conflict with the guidelines of the other projects so in that case we need to take a step back and reconsider the guidelines and see what we want to have there as a final result and yeah also we work on restructuring the project before it was the sematic conventions within the project were grouped by signal logs metrics traces and so on but now we have a better organized organization there and we group the attributes by topic and yeah as Alex mentioned already we have introduced the global attributes registry it's actually a very big list with all the attributes there and then within the actual specification you can reference the attributes from there so yeah that's quite useful and we're also working on adding a new concept from ECS which actually the attribute nesting or reusing some namespaces that means that if you have a namespace for example always dot whatever you can nest it attach it as it is under the host namespace for example and you don't need to redefine it again so yeah these are some examples from the upstream most of them are closed some of them are really let's say close to be completed but we have some small blockers there but the work is moving forward that's a that's the point and yeah how the community is organized around these so as I mentioned before we want to have proper people working on specific areas leveraging their expertise so we have working groups working on each area and we're trying to first declare their the areas of the semantic attribute the sematic conventions as stable which means that all the semantic conventions that we will have there will be stable and then we can use them in the actual implementations so the next step is to tune the implementations accordingly which means essentially the open telemetry collector and the language SDKs and yeah some examples the system metrics working group the working group around databases we have a security semantic conventions working group which is getting started now we have also approvers areas for the mobile area containers Kubernetes and many others that I don't mention here and the process looks like this first once you want to create a working group or a specific project you propose the working group area and you mentioned there what issues you want to work on and then you will have people expressing their interest to join this effort you will need to find a sponsor from the technical committee and yeah once everything is decided we have a specific project board we have regular meetings we have people getting assigned to the issues there and yeah the work is happening like this and yeah regarding the merger itself in yeah technically it happens like this we follow this process so once we have to either introduce some new fields some new semantic conventions or we want to move something from ECS to the semantic conventions of open telemetry we first check obviously what we have in these two projects and we also check what implementations have so far essentially the open telemetry collector or the SDKs because there are cases that the for example the collector already uses some some let's say metrics there or some semantic events some resource attributes for example but those are not yet part of the semantic conventions of open telemetry so in that case we also check what there is there so we might find something interesting so we can use it and once we have everything considered we have a final proposal we raise an issue or a pull request directly and we start the discussion within the community we yeah particularly focusing on measuring the breaking changes because you can imagine that we want to avoid bringing frustration to our users on both sides so yeah that's really unique really important thing to consider and we go through the review process and then once we have a conclusion we merge and then of course we need to handle the breaking changes because they are there most of the times and yeah the summary for today is that the merger is happening feel free to join us contributors are more than welcome everything happens in the app stream so if you are interested please join and you will see that you will find that you will have real impact from day one there and the goal of everyone is to make the semantic convention of open telemetry the one unique straight one unique and straightforward standard for observability and security that will be there for the future so yeah with that you can find us on csf slack channels or by using our github handles and some project meetings on Mondays we have the semantic events working group meeting same our next day Tuesdays we have the specification sig meeting and on Thursdays we have the system metrics working group 530 30 central time and yeah without any questions I think we're out of time do we have any questions hi thank you for the talk this this was really interesting and clarified some things for me I have one question about what's how what are the benefits of these semantic conventions in terms of like front-end tooling that that we are using because I know that you know there's this idea in open telemetry project that you have semantic conventions and you have common attributes for different signals and then we collect all this data in all these different signals in some observability tools and I imagine in like front-end we could automatically correlate different signals if we have this like common attributes I'm not up to date with the current state of this this area so yeah this is my question what are the main benefits of following this semantic conventions yeah I would say there are two actually one is I mean open telemetry is an open source standard right and there are many vendors adopting this so we need common semantics of what the data represents to build features higher level features on top this is the first thing and the other one is correlation as you already mentioned cross like different signals to also have correlation cross or through the resource attributes for example so you can drill down basically on different signals into the same resource and yeah I would say these two things and also cross signal correlation not only through resources but things like trace ID to have them you know both on locks and traces and later maybe in profiling data this kind of things okay thank you so are you doing something like that in elastic like in front-end at the moment is there any work going on in this area like correlation of different signals yeah of course like I think that's that's the goal for for every observability vendor to bring all all these different signals together yeah okay great thank you very much any other questions going once okay cool then bingo plus okay", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.120000000000001, "text": " So, up next, we have Christos and Alex and unifying observability in the power of common", "tokens": [50364, 407, 11, 493, 958, 11, 321, 362, 2040, 329, 293, 5202, 293, 517, 5489, 9951, 2310, 294, 264, 1347, 295, 2689, 51120], "temperature": 0.0, "avg_logprob": -0.4820704576445789, "compression_ratio": 1.2016806722689075, "no_speech_prob": 0.2686777114868164}, {"id": 1, "seek": 0, "start": 15.120000000000001, "end": 16.12, "text": " schema.", "tokens": [51120, 34078, 13, 51170], "temperature": 0.0, "avg_logprob": -0.4820704576445789, "compression_ratio": 1.2016806722689075, "no_speech_prob": 0.2686777114868164}, {"id": 2, "seek": 0, "start": 16.12, "end": 25.64, "text": " Okay, thanks everyone and welcome to our talk.", "tokens": [51170, 1033, 11, 3231, 1518, 293, 2928, 281, 527, 751, 13, 51646], "temperature": 0.0, "avg_logprob": -0.4820704576445789, "compression_ratio": 1.2016806722689075, "no_speech_prob": 0.2686777114868164}, {"id": 3, "seek": 2564, "start": 25.64, "end": 31.68, "text": " We will in this presentation talk about the conversion story of two schemas of open telemetry", "tokens": [50364, 492, 486, 294, 341, 5860, 751, 466, 264, 14298, 1657, 295, 732, 22627, 296, 295, 1269, 4304, 5537, 627, 50666], "temperature": 0.0, "avg_logprob": -0.24155660109086471, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.385060578584671}, {"id": 4, "seek": 2564, "start": 31.68, "end": 34.28, "text": " in the elastic common schema.", "tokens": [50666, 294, 264, 17115, 2689, 34078, 13, 50796], "temperature": 0.0, "avg_logprob": -0.24155660109086471, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.385060578584671}, {"id": 5, "seek": 2564, "start": 34.28, "end": 36.16, "text": " But let's first introduce ourselves.", "tokens": [50796, 583, 718, 311, 700, 5366, 4175, 13, 50890], "temperature": 0.0, "avg_logprob": -0.24155660109086471, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.385060578584671}, {"id": 6, "seek": 2564, "start": 36.16, "end": 37.16, "text": " My name is Alex.", "tokens": [50890, 1222, 1315, 307, 5202, 13, 50940], "temperature": 0.0, "avg_logprob": -0.24155660109086471, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.385060578584671}, {"id": 7, "seek": 2564, "start": 37.16, "end": 42.120000000000005, "text": " I'm leading the open telemetry initiative at Elastic and I'm a co-maintenor of the open", "tokens": [50940, 286, 478, 5775, 264, 1269, 4304, 5537, 627, 11552, 412, 2699, 2750, 293, 286, 478, 257, 598, 12, 49417, 1147, 284, 295, 264, 1269, 51188], "temperature": 0.0, "avg_logprob": -0.24155660109086471, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.385060578584671}, {"id": 8, "seek": 2564, "start": 42.120000000000005, "end": 45.6, "text": " telemetry semantic conventions project.", "tokens": [51188, 4304, 5537, 627, 47982, 33520, 1716, 13, 51362], "temperature": 0.0, "avg_logprob": -0.24155660109086471, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.385060578584671}, {"id": 9, "seek": 2564, "start": 45.6, "end": 48.480000000000004, "text": " Hi, I'm Christos.", "tokens": [51362, 2421, 11, 286, 478, 2040, 329, 13, 51506], "temperature": 0.0, "avg_logprob": -0.24155660109086471, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.385060578584671}, {"id": 10, "seek": 2564, "start": 48.480000000000004, "end": 54.68, "text": " I work on elastic as well and I'm software engineer focusing on observability and specifically", "tokens": [51506, 286, 589, 322, 17115, 382, 731, 293, 286, 478, 4722, 11403, 8416, 322, 9951, 2310, 293, 4682, 51816], "temperature": 0.0, "avg_logprob": -0.24155660109086471, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.385060578584671}, {"id": 11, "seek": 5468, "start": 54.68, "end": 60.4, "text": " open telemetry where I am a contributor and a prover on the semantic convention project.", "tokens": [50364, 1269, 4304, 5537, 627, 689, 286, 669, 257, 42859, 293, 257, 447, 331, 322, 264, 47982, 10286, 1716, 13, 50650], "temperature": 0.0, "avg_logprob": -0.1946099394111223, "compression_ratio": 1.6748768472906403, "no_speech_prob": 0.018002767115831375}, {"id": 12, "seek": 5468, "start": 60.4, "end": 66.12, "text": " Okay, we would like to start with a quite easy and simple question.", "tokens": [50650, 1033, 11, 321, 576, 411, 281, 722, 365, 257, 1596, 1858, 293, 2199, 1168, 13, 50936], "temperature": 0.0, "avg_logprob": -0.1946099394111223, "compression_ratio": 1.6748768472906403, "no_speech_prob": 0.018002767115831375}, {"id": 13, "seek": 5468, "start": 66.12, "end": 71.16, "text": " How many of you do know exactly what open telemetry is?", "tokens": [50936, 1012, 867, 295, 291, 360, 458, 2293, 437, 1269, 4304, 5537, 627, 307, 30, 51188], "temperature": 0.0, "avg_logprob": -0.1946099394111223, "compression_ratio": 1.6748768472906403, "no_speech_prob": 0.018002767115831375}, {"id": 14, "seek": 5468, "start": 71.16, "end": 72.16, "text": " That's great.", "tokens": [51188, 663, 311, 869, 13, 51238], "temperature": 0.0, "avg_logprob": -0.1946099394111223, "compression_ratio": 1.6748768472906403, "no_speech_prob": 0.018002767115831375}, {"id": 15, "seek": 5468, "start": 72.16, "end": 74.36, "text": " I can skip some slides later.", "tokens": [51238, 286, 393, 10023, 512, 9788, 1780, 13, 51348], "temperature": 0.0, "avg_logprob": -0.1946099394111223, "compression_ratio": 1.6748768472906403, "no_speech_prob": 0.018002767115831375}, {"id": 16, "seek": 5468, "start": 74.36, "end": 78.96000000000001, "text": " How many of you do know what semantic conventions is about?", "tokens": [51348, 1012, 867, 295, 291, 360, 458, 437, 47982, 33520, 307, 466, 30, 51578], "temperature": 0.0, "avg_logprob": -0.1946099394111223, "compression_ratio": 1.6748768472906403, "no_speech_prob": 0.018002767115831375}, {"id": 17, "seek": 5468, "start": 78.96000000000001, "end": 81.08, "text": " That's what I expected.", "tokens": [51578, 663, 311, 437, 286, 5176, 13, 51684], "temperature": 0.0, "avg_logprob": -0.1946099394111223, "compression_ratio": 1.6748768472906403, "no_speech_prob": 0.018002767115831375}, {"id": 18, "seek": 8108, "start": 81.08, "end": 84.8, "text": " And how many of you do know what elastic common schema is?", "tokens": [50364, 400, 577, 867, 295, 291, 360, 458, 437, 17115, 2689, 34078, 307, 30, 50550], "temperature": 0.0, "avg_logprob": -0.438587287803749, "compression_ratio": 1.4, "no_speech_prob": 0.08090582489967346}, {"id": 19, "seek": 8108, "start": 84.8, "end": 89.28, "text": " Okay, thanks everyone.", "tokens": [50550, 1033, 11, 3231, 1518, 13, 50774], "temperature": 0.0, "avg_logprob": -0.438587287803749, "compression_ratio": 1.4, "no_speech_prob": 0.08090582489967346}, {"id": 20, "seek": 8108, "start": 89.28, "end": 96.03999999999999, "text": " So let's deep dive a bit on the history of open source tools and standards in observability", "tokens": [50774, 407, 718, 311, 2452, 9192, 257, 857, 322, 264, 2503, 295, 1269, 4009, 3873, 293, 7787, 294, 9951, 2310, 51112], "temperature": 0.0, "avg_logprob": -0.438587287803749, "compression_ratio": 1.4, "no_speech_prob": 0.08090582489967346}, {"id": 21, "seek": 8108, "start": 96.03999999999999, "end": 99.44, "text": " to give us a picture where the standards come from.", "tokens": [51112, 281, 976, 505, 257, 3036, 689, 264, 7787, 808, 490, 13, 51282], "temperature": 0.0, "avg_logprob": -0.438587287803749, "compression_ratio": 1.4, "no_speech_prob": 0.08090582489967346}, {"id": 22, "seek": 8108, "start": 99.44, "end": 102.44, "text": " Let me.", "tokens": [51282, 961, 385, 13, 51432], "temperature": 0.0, "avg_logprob": -0.438587287803749, "compression_ratio": 1.4, "no_speech_prob": 0.08090582489967346}, {"id": 23, "seek": 8108, "start": 102.44, "end": 104.44, "text": " Okay.", "tokens": [51432, 1033, 13, 51532], "temperature": 0.0, "avg_logprob": -0.438587287803749, "compression_ratio": 1.4, "no_speech_prob": 0.08090582489967346}, {"id": 24, "seek": 8108, "start": 104.44, "end": 106.44, "text": " No.", "tokens": [51532, 883, 13, 51632], "temperature": 0.0, "avg_logprob": -0.438587287803749, "compression_ratio": 1.4, "no_speech_prob": 0.08090582489967346}, {"id": 25, "seek": 8108, "start": 106.44, "end": 109.92, "text": " Does that work?", "tokens": [51632, 4402, 300, 589, 30, 51806], "temperature": 0.0, "avg_logprob": -0.438587287803749, "compression_ratio": 1.4, "no_speech_prob": 0.08090582489967346}, {"id": 26, "seek": 10992, "start": 110.92, "end": 114.72, "text": " Okay, around, do you hear me?", "tokens": [50414, 1033, 11, 926, 11, 360, 291, 1568, 385, 30, 50604], "temperature": 0.0, "avg_logprob": -0.31675425264024243, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.003737381426617503}, {"id": 27, "seek": 10992, "start": 114.72, "end": 115.72, "text": " That works well.", "tokens": [50604, 663, 1985, 731, 13, 50654], "temperature": 0.0, "avg_logprob": -0.31675425264024243, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.003737381426617503}, {"id": 28, "seek": 10992, "start": 115.72, "end": 116.72, "text": " Okay.", "tokens": [50654, 1033, 13, 50704], "temperature": 0.0, "avg_logprob": -0.31675425264024243, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.003737381426617503}, {"id": 29, "seek": 10992, "start": 116.72, "end": 121.64, "text": " Around or a bit more than 10 years ago when microservice emerged that also changed the", "tokens": [50704, 17633, 420, 257, 857, 544, 813, 1266, 924, 2057, 562, 15547, 25006, 20178, 300, 611, 3105, 264, 50950], "temperature": 0.0, "avg_logprob": -0.31675425264024243, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.003737381426617503}, {"id": 30, "seek": 10992, "start": 121.64, "end": 124.76, "text": " observability market and industry.", "tokens": [50950, 9951, 2310, 2142, 293, 3518, 13, 51106], "temperature": 0.0, "avg_logprob": -0.31675425264024243, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.003737381426617503}, {"id": 31, "seek": 10992, "start": 124.76, "end": 130.12, "text": " That's when like big tech companies started building their own open source tools for collecting", "tokens": [51106, 663, 311, 562, 411, 955, 7553, 3431, 1409, 2390, 641, 1065, 1269, 4009, 3873, 337, 12510, 51374], "temperature": 0.0, "avg_logprob": -0.31675425264024243, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.003737381426617503}, {"id": 32, "seek": 10992, "start": 130.12, "end": 131.12, "text": " observability data.", "tokens": [51374, 9951, 2310, 1412, 13, 51424], "temperature": 0.0, "avg_logprob": -0.31675425264024243, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.003737381426617503}, {"id": 33, "seek": 10992, "start": 131.12, "end": 137.08, "text": " So tools like Zipkin, Jega for distributor traces emerged, the Elk stack for logging,", "tokens": [51424, 407, 3873, 411, 1176, 647, 5843, 11, 508, 6335, 337, 49192, 26076, 20178, 11, 264, 2699, 74, 8630, 337, 27991, 11, 51722], "temperature": 0.0, "avg_logprob": -0.31675425264024243, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.003737381426617503}, {"id": 34, "seek": 13708, "start": 137.08, "end": 138.08, "text": " Prometheus for metrics.", "tokens": [50364, 2114, 649, 42209, 337, 16367, 13, 50414], "temperature": 0.0, "avg_logprob": -0.26679172515869143, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.016865432262420654}, {"id": 35, "seek": 13708, "start": 138.08, "end": 141.36, "text": " We heard a lot about this in previous talks.", "tokens": [50414, 492, 2198, 257, 688, 466, 341, 294, 3894, 6686, 13, 50578], "temperature": 0.0, "avg_logprob": -0.26679172515869143, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.016865432262420654}, {"id": 36, "seek": 13708, "start": 141.36, "end": 148.44, "text": " And based on this defective standard tools, then actual standards emerged like open tracing,", "tokens": [50578, 400, 2361, 322, 341, 16445, 488, 3832, 3873, 11, 550, 3539, 7787, 20178, 411, 1269, 25262, 11, 50932], "temperature": 0.0, "avg_logprob": -0.26679172515869143, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.016865432262420654}, {"id": 37, "seek": 13708, "start": 148.44, "end": 153.96, "text": " open sensors later for distributed tracing, open sensors also covered metrics and the", "tokens": [50932, 1269, 14840, 1780, 337, 12631, 25262, 11, 1269, 14840, 611, 5343, 16367, 293, 264, 51208], "temperature": 0.0, "avg_logprob": -0.26679172515869143, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.016865432262420654}, {"id": 38, "seek": 13708, "start": 153.96, "end": 162.72000000000003, "text": " open metrics as a derivative of Prometheus format emerged and Elastic has its own ECS", "tokens": [51208, 1269, 16367, 382, 257, 13760, 295, 2114, 649, 42209, 7877, 20178, 293, 2699, 2750, 575, 1080, 1065, 19081, 50, 51646], "temperature": 0.0, "avg_logprob": -0.26679172515869143, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.016865432262420654}, {"id": 39, "seek": 13708, "start": 162.72000000000003, "end": 166.8, "text": " that defines the semantics of structured logging data.", "tokens": [51646, 300, 23122, 264, 4361, 45298, 295, 18519, 27991, 1412, 13, 51850], "temperature": 0.0, "avg_logprob": -0.26679172515869143, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.016865432262420654}, {"id": 40, "seek": 16680, "start": 166.8, "end": 171.76000000000002, "text": " Since we will talk a bit more about ECS, a quick introduction what that is.", "tokens": [50364, 4162, 321, 486, 751, 257, 857, 544, 466, 19081, 50, 11, 257, 1702, 9339, 437, 300, 307, 13, 50612], "temperature": 0.0, "avg_logprob": -0.17310662363089768, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00814667996019125}, {"id": 41, "seek": 16680, "start": 171.76000000000002, "end": 177.52, "text": " So ECS stands for the Elastic Com Schema and it's basically just a definition of a set", "tokens": [50612, 407, 19081, 50, 7382, 337, 264, 2699, 2750, 2432, 2065, 5619, 293, 309, 311, 1936, 445, 257, 7123, 295, 257, 992, 50900], "temperature": 0.0, "avg_logprob": -0.17310662363089768, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00814667996019125}, {"id": 42, "seek": 16680, "start": 177.52, "end": 182.88000000000002, "text": " of fields that describe the semantics in structured logging data.", "tokens": [50900, 295, 7909, 300, 6786, 264, 4361, 45298, 294, 18519, 27991, 1412, 13, 51168], "temperature": 0.0, "avg_logprob": -0.17310662363089768, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00814667996019125}, {"id": 43, "seek": 16680, "start": 182.88000000000002, "end": 188.96, "text": " So for example, if you're collecting a service name with your observability data, the Com", "tokens": [51168, 407, 337, 1365, 11, 498, 291, 434, 12510, 257, 2643, 1315, 365, 428, 9951, 2310, 1412, 11, 264, 2432, 51472], "temperature": 0.0, "avg_logprob": -0.17310662363089768, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00814667996019125}, {"id": 44, "seek": 16680, "start": 188.96, "end": 195.52, "text": " Schema tells you that you should put this value into a field that is called service.name,", "tokens": [51472, 2065, 5619, 5112, 291, 300, 291, 820, 829, 341, 2158, 666, 257, 2519, 300, 307, 1219, 2643, 13, 16344, 11, 51800], "temperature": 0.0, "avg_logprob": -0.17310662363089768, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00814667996019125}, {"id": 45, "seek": 19552, "start": 195.52, "end": 198.44, "text": " not app.name or application.name.", "tokens": [50364, 406, 724, 13, 16344, 420, 3861, 13, 16344, 13, 50510], "temperature": 0.0, "avg_logprob": -0.21935522833535837, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00529546570032835}, {"id": 46, "seek": 19552, "start": 198.44, "end": 204.12, "text": " So you have common names that you can later on search for and this also allows you to", "tokens": [50510, 407, 291, 362, 2689, 5288, 300, 291, 393, 1780, 322, 3164, 337, 293, 341, 611, 4045, 291, 281, 50794], "temperature": 0.0, "avg_logprob": -0.21935522833535837, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00529546570032835}, {"id": 47, "seek": 19552, "start": 204.12, "end": 209.20000000000002, "text": " correlate data across different signals.", "tokens": [50794, 48742, 1412, 2108, 819, 12354, 13, 51048], "temperature": 0.0, "avg_logprob": -0.21935522833535837, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00529546570032835}, {"id": 48, "seek": 19552, "start": 209.20000000000002, "end": 215.44, "text": " Now as you can see, we already have at least four standards here that are partially competing,", "tokens": [51048, 823, 382, 291, 393, 536, 11, 321, 1217, 362, 412, 1935, 1451, 7787, 510, 300, 366, 18886, 15439, 11, 51360], "temperature": 0.0, "avg_logprob": -0.21935522833535837, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00529546570032835}, {"id": 49, "seek": 19552, "start": 215.44, "end": 217.44, "text": " partially complementary.", "tokens": [51360, 18886, 40705, 13, 51460], "temperature": 0.0, "avg_logprob": -0.21935522833535837, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00529546570032835}, {"id": 50, "seek": 19552, "start": 217.44, "end": 223.92000000000002, "text": " Plus we have all the tools that also create some defective standards for collecting data.", "tokens": [51460, 7721, 321, 362, 439, 264, 3873, 300, 611, 1884, 512, 16445, 488, 7787, 337, 12510, 1412, 13, 51784], "temperature": 0.0, "avg_logprob": -0.21935522833535837, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00529546570032835}, {"id": 51, "seek": 22392, "start": 223.92, "end": 227.55999999999997, "text": " So it's ridiculous to have so many standards, right?", "tokens": [50364, 407, 309, 311, 11083, 281, 362, 370, 867, 7787, 11, 558, 30, 50546], "temperature": 0.0, "avg_logprob": -0.19897057906441065, "compression_ratio": 1.71875, "no_speech_prob": 0.004812194965779781}, {"id": 52, "seek": 22392, "start": 227.55999999999997, "end": 231.83999999999997, "text": " We need one more that covers all of them.", "tokens": [50546, 492, 643, 472, 544, 300, 10538, 439, 295, 552, 13, 50760], "temperature": 0.0, "avg_logprob": -0.19897057906441065, "compression_ratio": 1.71875, "no_speech_prob": 0.004812194965779781}, {"id": 53, "seek": 22392, "start": 231.83999999999997, "end": 237.23999999999998, "text": " And usually what happens is we have one more that is competing with all the others.", "tokens": [50760, 400, 2673, 437, 2314, 307, 321, 362, 472, 544, 300, 307, 15439, 365, 439, 264, 2357, 13, 51030], "temperature": 0.0, "avg_logprob": -0.19897057906441065, "compression_ratio": 1.71875, "no_speech_prob": 0.004812194965779781}, {"id": 54, "seek": 22392, "start": 237.23999999999998, "end": 240.44, "text": " And yes, we have one more standard for observability.", "tokens": [51030, 400, 2086, 11, 321, 362, 472, 544, 3832, 337, 9951, 2310, 13, 51190], "temperature": 0.0, "avg_logprob": -0.19897057906441065, "compression_ratio": 1.71875, "no_speech_prob": 0.004812194965779781}, {"id": 55, "seek": 22392, "start": 240.44, "end": 244.79999999999998, "text": " OpenTelemedia will come back to the comic later again.", "tokens": [51190, 7238, 14233, 306, 1912, 654, 486, 808, 646, 281, 264, 13900, 1780, 797, 13, 51408], "temperature": 0.0, "avg_logprob": -0.19897057906441065, "compression_ratio": 1.71875, "no_speech_prob": 0.004812194965779781}, {"id": 56, "seek": 22392, "start": 244.79999999999998, "end": 247.39999999999998, "text": " This is the slide that I can skip based on the Paul.", "tokens": [51408, 639, 307, 264, 4137, 300, 286, 393, 10023, 2361, 322, 264, 4552, 13, 51538], "temperature": 0.0, "avg_logprob": -0.19897057906441065, "compression_ratio": 1.71875, "no_speech_prob": 0.004812194965779781}, {"id": 57, "seek": 22392, "start": 247.39999999999998, "end": 253.88, "text": " So OpenTelemedia provides not just a standard but a full ecosystem and framework for observability.", "tokens": [51538, 407, 7238, 14233, 306, 1912, 654, 6417, 406, 445, 257, 3832, 457, 257, 1577, 11311, 293, 8388, 337, 9951, 2310, 13, 51862], "temperature": 0.0, "avg_logprob": -0.19897057906441065, "compression_ratio": 1.71875, "no_speech_prob": 0.004812194965779781}, {"id": 58, "seek": 25388, "start": 253.88, "end": 257.04, "text": " For collecting data, sending it protocol.", "tokens": [50364, 1171, 12510, 1412, 11, 7750, 309, 10336, 13, 50522], "temperature": 0.0, "avg_logprob": -0.17265532998477712, "compression_ratio": 1.6254980079681276, "no_speech_prob": 0.017833204939961433}, {"id": 59, "seek": 25388, "start": 257.04, "end": 261.6, "text": " One thing that I want to highlight here, there is a specification in OpenTelemedia that defines", "tokens": [50522, 1485, 551, 300, 286, 528, 281, 5078, 510, 11, 456, 307, 257, 31256, 294, 7238, 14233, 306, 1912, 654, 300, 23122, 50750], "temperature": 0.0, "avg_logprob": -0.17265532998477712, "compression_ratio": 1.6254980079681276, "no_speech_prob": 0.017833204939961433}, {"id": 60, "seek": 25388, "start": 261.6, "end": 265.96, "text": " what data you can collect, like traces, metrics, logs.", "tokens": [50750, 437, 1412, 291, 393, 2500, 11, 411, 26076, 11, 16367, 11, 20820, 13, 50968], "temperature": 0.0, "avg_logprob": -0.17265532998477712, "compression_ratio": 1.6254980079681276, "no_speech_prob": 0.017833204939961433}, {"id": 61, "seek": 25388, "start": 265.96, "end": 270.48, "text": " OpenTelemedia working group is also working on a profiling signal.", "tokens": [50968, 7238, 14233, 306, 1912, 654, 1364, 1594, 307, 611, 1364, 322, 257, 1740, 4883, 6358, 13, 51194], "temperature": 0.0, "avg_logprob": -0.17265532998477712, "compression_ratio": 1.6254980079681276, "no_speech_prob": 0.017833204939961433}, {"id": 62, "seek": 25388, "start": 270.48, "end": 275.28, "text": " And what we will talk more about in this presentation is the semantic conventions.", "tokens": [51194, 400, 437, 321, 486, 751, 544, 466, 294, 341, 5860, 307, 264, 47982, 33520, 13, 51434], "temperature": 0.0, "avg_logprob": -0.17265532998477712, "compression_ratio": 1.6254980079681276, "no_speech_prob": 0.017833204939961433}, {"id": 63, "seek": 25388, "start": 275.28, "end": 280.96, "text": " Semantic conventions are very similar to what I've shown for ECS.", "tokens": [51434, 14421, 7128, 33520, 366, 588, 2531, 281, 437, 286, 600, 4898, 337, 19081, 50, 13, 51718], "temperature": 0.0, "avg_logprob": -0.17265532998477712, "compression_ratio": 1.6254980079681276, "no_speech_prob": 0.017833204939961433}, {"id": 64, "seek": 28096, "start": 280.96, "end": 288.64, "text": " And basically defines, yeah, attribute names and their semantics.", "tokens": [50364, 400, 1936, 23122, 11, 1338, 11, 19667, 5288, 293, 641, 4361, 45298, 13, 50748], "temperature": 0.0, "avg_logprob": -0.21144302769711143, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.004297544714063406}, {"id": 65, "seek": 28096, "start": 288.64, "end": 292.88, "text": " Let's have a concrete example of how the data structure in OpenTelemedia looks like here", "tokens": [50748, 961, 311, 362, 257, 9859, 1365, 295, 577, 264, 1412, 3877, 294, 7238, 14233, 306, 1912, 654, 1542, 411, 510, 50960], "temperature": 0.0, "avg_logprob": -0.21144302769711143, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.004297544714063406}, {"id": 66, "seek": 28096, "start": 292.88, "end": 294.76, "text": " with some logging data.", "tokens": [50960, 365, 512, 27991, 1412, 13, 51054], "temperature": 0.0, "avg_logprob": -0.21144302769711143, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.004297544714063406}, {"id": 67, "seek": 28096, "start": 294.76, "end": 297.03999999999996, "text": " Very simplified view here, it's a bit more complex.", "tokens": [51054, 4372, 26335, 1910, 510, 11, 309, 311, 257, 857, 544, 3997, 13, 51168], "temperature": 0.0, "avg_logprob": -0.21144302769711143, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.004297544714063406}, {"id": 68, "seek": 28096, "start": 297.03999999999996, "end": 300.47999999999996, "text": " But let's say we have a set of log records, right?", "tokens": [51168, 583, 718, 311, 584, 321, 362, 257, 992, 295, 3565, 7724, 11, 558, 30, 51340], "temperature": 0.0, "avg_logprob": -0.21144302769711143, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.004297544714063406}, {"id": 69, "seek": 28096, "start": 300.47999999999996, "end": 306.67999999999995, "text": " The OpenTelemedia protocol defines like the core structure of that signal with fields", "tokens": [51340, 440, 7238, 14233, 306, 1912, 654, 10336, 23122, 411, 264, 4965, 3877, 295, 300, 6358, 365, 7909, 51650], "temperature": 0.0, "avg_logprob": -0.21144302769711143, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.004297544714063406}, {"id": 70, "seek": 30668, "start": 306.68, "end": 311.72, "text": " like severity text, which is basically the log level and body, which is basically the", "tokens": [50364, 411, 35179, 2487, 11, 597, 307, 1936, 264, 3565, 1496, 293, 1772, 11, 597, 307, 1936, 264, 50616], "temperature": 0.0, "avg_logprob": -0.1620578090701483, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.00460051791742444}, {"id": 71, "seek": 30668, "start": 311.72, "end": 313.24, "text": " log message.", "tokens": [50616, 3565, 3636, 13, 50692], "temperature": 0.0, "avg_logprob": -0.1620578090701483, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.00460051791742444}, {"id": 72, "seek": 30668, "start": 313.24, "end": 318.56, "text": " In addition, you can collect with your observability data additional context information.", "tokens": [50692, 682, 4500, 11, 291, 393, 2500, 365, 428, 9951, 2310, 1412, 4497, 4319, 1589, 13, 50958], "temperature": 0.0, "avg_logprob": -0.1620578090701483, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.00460051791742444}, {"id": 73, "seek": 30668, "start": 318.56, "end": 322.88, "text": " This is usually represented in so-called attributes, and that's where semantic conventions come", "tokens": [50958, 639, 307, 2673, 10379, 294, 370, 12, 11880, 17212, 11, 293, 300, 311, 689, 47982, 33520, 808, 51174], "temperature": 0.0, "avg_logprob": -0.1620578090701483, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.00460051791742444}, {"id": 74, "seek": 30668, "start": 322.88, "end": 323.88, "text": " into play.", "tokens": [51174, 666, 862, 13, 51224], "temperature": 0.0, "avg_logprob": -0.1620578090701483, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.00460051791742444}, {"id": 75, "seek": 30668, "start": 323.88, "end": 329.6, "text": " The semantic conventions define which attributes exist, their names, types, and also the semantics", "tokens": [51224, 440, 47982, 33520, 6964, 597, 17212, 2514, 11, 641, 5288, 11, 3467, 11, 293, 611, 264, 4361, 45298, 51510], "temperature": 0.0, "avg_logprob": -0.1620578090701483, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.00460051791742444}, {"id": 76, "seek": 30668, "start": 329.6, "end": 330.6, "text": " behind this.", "tokens": [51510, 2261, 341, 13, 51560], "temperature": 0.0, "avg_logprob": -0.1620578090701483, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.00460051791742444}, {"id": 77, "seek": 30668, "start": 330.6, "end": 336.0, "text": " For example, if you're collecting an HTTP access log, right, and you want to capture", "tokens": [51560, 1171, 1365, 11, 498, 291, 434, 12510, 364, 33283, 2105, 3565, 11, 558, 11, 293, 291, 528, 281, 7983, 51830], "temperature": 0.0, "avg_logprob": -0.1620578090701483, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.00460051791742444}, {"id": 78, "seek": 33600, "start": 336.0, "end": 343.48, "text": " the HTTP request method, this is the attribute name that you would use for it.", "tokens": [50364, 264, 33283, 5308, 3170, 11, 341, 307, 264, 19667, 1315, 300, 291, 576, 764, 337, 309, 13, 50738], "temperature": 0.0, "avg_logprob": -0.17025145439252462, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.0027913276571780443}, {"id": 79, "seek": 33600, "start": 343.48, "end": 350.12, "text": " Now observability data is usually also captured in a broader context for some resource like", "tokens": [50738, 823, 9951, 2310, 1412, 307, 2673, 611, 11828, 294, 257, 13227, 4319, 337, 512, 7684, 411, 51070], "temperature": 0.0, "avg_logprob": -0.17025145439252462, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.0027913276571780443}, {"id": 80, "seek": 33600, "start": 350.12, "end": 354.0, "text": " a concrete service, a host, or other resources.", "tokens": [51070, 257, 9859, 2643, 11, 257, 3975, 11, 420, 661, 3593, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17025145439252462, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.0027913276571780443}, {"id": 81, "seek": 33600, "start": 354.0, "end": 362.04, "text": " That's why OTLP wraps the actual observability data into a resource wrapper, and a resource", "tokens": [51264, 663, 311, 983, 38617, 45196, 25831, 264, 3539, 9951, 2310, 1412, 666, 257, 7684, 46906, 11, 293, 257, 7684, 51666], "temperature": 0.0, "avg_logprob": -0.17025145439252462, "compression_ratio": 1.5736040609137056, "no_speech_prob": 0.0027913276571780443}, {"id": 82, "seek": 36204, "start": 362.04, "end": 367.12, "text": " again has a set of attributes, so-called resource attributes, that describe the resource,", "tokens": [50364, 797, 575, 257, 992, 295, 17212, 11, 370, 12, 11880, 7684, 17212, 11, 300, 6786, 264, 7684, 11, 50618], "temperature": 0.0, "avg_logprob": -0.17956164787555562, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0032158666290342808}, {"id": 83, "seek": 36204, "start": 367.12, "end": 371.64000000000004, "text": " something like the service name, host name, and so on.", "tokens": [50618, 746, 411, 264, 2643, 1315, 11, 3975, 1315, 11, 293, 370, 322, 13, 50844], "temperature": 0.0, "avg_logprob": -0.17956164787555562, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0032158666290342808}, {"id": 84, "seek": 36204, "start": 371.64000000000004, "end": 378.04, "text": " So this is the structure in OpenTelemedia for collecting observability data, and semantic", "tokens": [50844, 407, 341, 307, 264, 3877, 294, 7238, 14233, 306, 1912, 654, 337, 12510, 9951, 2310, 1412, 11, 293, 47982, 51164], "temperature": 0.0, "avg_logprob": -0.17956164787555562, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0032158666290342808}, {"id": 85, "seek": 36204, "start": 378.04, "end": 386.16, "text": " conventions is just about the attributes basically in their meaning in this data.", "tokens": [51164, 33520, 307, 445, 466, 264, 17212, 1936, 294, 641, 3620, 294, 341, 1412, 13, 51570], "temperature": 0.0, "avg_logprob": -0.17956164787555562, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0032158666290342808}, {"id": 86, "seek": 36204, "start": 386.16, "end": 390.96000000000004, "text": " Now let's come back to our timeline of standards.", "tokens": [51570, 823, 718, 311, 808, 646, 281, 527, 12933, 295, 7787, 13, 51810], "temperature": 0.0, "avg_logprob": -0.17956164787555562, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0032158666290342808}, {"id": 87, "seek": 39096, "start": 390.96, "end": 394.59999999999997, "text": " There's one important thing I didn't mention before.", "tokens": [50364, 821, 311, 472, 1021, 551, 286, 994, 380, 2152, 949, 13, 50546], "temperature": 0.0, "avg_logprob": -0.18895347445618874, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.07008815556764603}, {"id": 88, "seek": 39096, "start": 394.59999999999997, "end": 400.28, "text": " Actually OpenTelemedia, and we heard this in the previous talk, is the result of a merger", "tokens": [50546, 5135, 7238, 14233, 306, 1912, 654, 11, 293, 321, 2198, 341, 294, 264, 3894, 751, 11, 307, 264, 1874, 295, 257, 48002, 50830], "temperature": 0.0, "avg_logprob": -0.18895347445618874, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.07008815556764603}, {"id": 89, "seek": 39096, "start": 400.28, "end": 403.47999999999996, "text": " between open tracing and open sensors.", "tokens": [50830, 1296, 1269, 25262, 293, 1269, 14840, 13, 50990], "temperature": 0.0, "avg_logprob": -0.18895347445618874, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.07008815556764603}, {"id": 90, "seek": 39096, "start": 403.47999999999996, "end": 408.59999999999997, "text": " OpenTelemedia also supports Prometheus metrics and OpenMetrics that we have heard in some", "tokens": [50990, 7238, 14233, 306, 1912, 654, 611, 9346, 2114, 649, 42209, 16367, 293, 7238, 44, 302, 10716, 300, 321, 362, 2198, 294, 512, 51246], "temperature": 0.0, "avg_logprob": -0.18895347445618874, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.07008815556764603}, {"id": 91, "seek": 39096, "start": 408.59999999999997, "end": 415.56, "text": " of the previous talks, and just last year, Elastic also announced the donation of ECS", "tokens": [51246, 295, 264, 3894, 6686, 11, 293, 445, 1036, 1064, 11, 2699, 2750, 611, 7548, 264, 19724, 295, 19081, 50, 51594], "temperature": 0.0, "avg_logprob": -0.18895347445618874, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.07008815556764603}, {"id": 92, "seek": 39096, "start": 415.56, "end": 417.59999999999997, "text": " into OpenTelemedia.", "tokens": [51594, 666, 7238, 14233, 306, 1912, 654, 13, 51696], "temperature": 0.0, "avg_logprob": -0.18895347445618874, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.07008815556764603}, {"id": 93, "seek": 41760, "start": 417.6, "end": 421.76000000000005, "text": " So coming back to this, the question is, is it really that we have one more competing", "tokens": [50364, 407, 1348, 646, 281, 341, 11, 264, 1168, 307, 11, 307, 309, 534, 300, 321, 362, 472, 544, 15439, 50572], "temperature": 0.0, "avg_logprob": -0.15735044787006994, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0031040715985000134}, {"id": 94, "seek": 41760, "start": 421.76000000000005, "end": 422.76000000000005, "text": " standard?", "tokens": [50572, 3832, 30, 50622], "temperature": 0.0, "avg_logprob": -0.15735044787006994, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0031040715985000134}, {"id": 95, "seek": 41760, "start": 422.76000000000005, "end": 424.40000000000003, "text": " I would say actually not.", "tokens": [50622, 286, 576, 584, 767, 406, 13, 50704], "temperature": 0.0, "avg_logprob": -0.15735044787006994, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0031040715985000134}, {"id": 96, "seek": 41760, "start": 424.40000000000003, "end": 429.56, "text": " With OTLP we have less competing standards, and OTLP really succeeds in reducing the amount", "tokens": [50704, 2022, 38617, 45196, 321, 362, 1570, 15439, 7787, 11, 293, 38617, 45196, 534, 49263, 294, 12245, 264, 2372, 50962], "temperature": 0.0, "avg_logprob": -0.15735044787006994, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0031040715985000134}, {"id": 97, "seek": 41760, "start": 429.56, "end": 437.16, "text": " of competing standards and becoming the one and single standard for observability.", "tokens": [50962, 295, 15439, 7787, 293, 5617, 264, 472, 293, 2167, 3832, 337, 9951, 2310, 13, 51342], "temperature": 0.0, "avg_logprob": -0.15735044787006994, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0031040715985000134}, {"id": 98, "seek": 41760, "start": 437.16, "end": 444.6, "text": " Now as I said before, Elastic announced the donation of ECS into the OTLP's semantic", "tokens": [51342, 823, 382, 286, 848, 949, 11, 2699, 2750, 7548, 264, 19724, 295, 19081, 50, 666, 264, 38617, 45196, 311, 47982, 51714], "temperature": 0.0, "avg_logprob": -0.15735044787006994, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0031040715985000134}, {"id": 99, "seek": 44460, "start": 444.6, "end": 447.6, "text": " conventions project.", "tokens": [50364, 33520, 1716, 13, 50514], "temperature": 0.0, "avg_logprob": -0.19414710998535156, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.015837347134947777}, {"id": 100, "seek": 44460, "start": 447.6, "end": 448.6, "text": " Why?", "tokens": [50514, 1545, 30, 50564], "temperature": 0.0, "avg_logprob": -0.19414710998535156, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.015837347134947777}, {"id": 101, "seek": 44460, "start": 448.6, "end": 451.08000000000004, "text": " Yeah, because there are great benefits to this.", "tokens": [50564, 865, 11, 570, 456, 366, 869, 5311, 281, 341, 13, 50688], "temperature": 0.0, "avg_logprob": -0.19414710998535156, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.015837347134947777}, {"id": 102, "seek": 44460, "start": 451.08000000000004, "end": 457.32000000000005, "text": " First of all, there are complementary parts and strengths in both schemas that we now", "tokens": [50688, 2386, 295, 439, 11, 456, 366, 40705, 3166, 293, 16986, 294, 1293, 22627, 296, 300, 321, 586, 51000], "temperature": 0.0, "avg_logprob": -0.19414710998535156, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.015837347134947777}, {"id": 103, "seek": 44460, "start": 457.32000000000005, "end": 461.44, "text": " merge into one single schema.", "tokens": [51000, 22183, 666, 472, 2167, 34078, 13, 51206], "temperature": 0.0, "avg_logprob": -0.19414710998535156, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.015837347134947777}, {"id": 104, "seek": 44460, "start": 461.44, "end": 466.84000000000003, "text": " And second, we grow two different communities by merging them and providing a bigger network", "tokens": [51206, 400, 1150, 11, 321, 1852, 732, 819, 4456, 538, 44559, 552, 293, 6530, 257, 3801, 3209, 51476], "temperature": 0.0, "avg_logprob": -0.19414710998535156, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.015837347134947777}, {"id": 105, "seek": 44460, "start": 466.84000000000003, "end": 467.84000000000003, "text": " effect.", "tokens": [51476, 1802, 13, 51526], "temperature": 0.0, "avg_logprob": -0.19414710998535156, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.015837347134947777}, {"id": 106, "seek": 44460, "start": 467.84000000000003, "end": 473.04, "text": " So it's a huge win I think for the community, but there are not only benefits, there are", "tokens": [51526, 407, 309, 311, 257, 2603, 1942, 286, 519, 337, 264, 1768, 11, 457, 456, 366, 406, 787, 5311, 11, 456, 366, 51786], "temperature": 0.0, "avg_logprob": -0.19414710998535156, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.015837347134947777}, {"id": 107, "seek": 47304, "start": 473.04, "end": 475.24, "text": " also challenges, right?", "tokens": [50364, 611, 4759, 11, 558, 30, 50474], "temperature": 0.0, "avg_logprob": -0.141389857167783, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.012772849760949612}, {"id": 108, "seek": 47304, "start": 475.24, "end": 481.20000000000005, "text": " First of all, the overlap between the two schemas is a potential for schema conflicts.", "tokens": [50474, 2386, 295, 439, 11, 264, 19959, 1296, 264, 732, 22627, 296, 307, 257, 3995, 337, 34078, 19807, 13, 50772], "temperature": 0.0, "avg_logprob": -0.141389857167783, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.012772849760949612}, {"id": 109, "seek": 47304, "start": 481.20000000000005, "end": 484.88, "text": " And to resolve these conflicts might mean that we need to have either breaking changes", "tokens": [50772, 400, 281, 14151, 613, 19807, 1062, 914, 300, 321, 643, 281, 362, 2139, 7697, 2962, 50956], "temperature": 0.0, "avg_logprob": -0.141389857167783, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.012772849760949612}, {"id": 110, "seek": 47304, "start": 484.88, "end": 489.68, "text": " in the one schema or in the other.", "tokens": [50956, 294, 264, 472, 34078, 420, 294, 264, 661, 13, 51196], "temperature": 0.0, "avg_logprob": -0.141389857167783, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.012772849760949612}, {"id": 111, "seek": 47304, "start": 489.68, "end": 494.96000000000004, "text": " We have seen the structure of observability data in OpenTelemedia, which consists of the", "tokens": [51196, 492, 362, 1612, 264, 3877, 295, 9951, 2310, 1412, 294, 7238, 14233, 306, 1912, 654, 11, 597, 14689, 295, 264, 51460], "temperature": 0.0, "avg_logprob": -0.141389857167783, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.012772849760949612}, {"id": 112, "seek": 47304, "start": 494.96000000000004, "end": 499.8, "text": " protocol with the nested structure plus the semantic conventions.", "tokens": [51460, 10336, 365, 264, 15646, 292, 3877, 1804, 264, 47982, 33520, 13, 51702], "temperature": 0.0, "avg_logprob": -0.141389857167783, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.012772849760949612}, {"id": 113, "seek": 49980, "start": 499.8, "end": 506.16, "text": " It's quite different to how ECS defines the fields because ECS is just a plain definition", "tokens": [50364, 467, 311, 1596, 819, 281, 577, 19081, 50, 23122, 264, 7909, 570, 19081, 50, 307, 445, 257, 11121, 7123, 50682], "temperature": 0.0, "avg_logprob": -0.16416234970092775, "compression_ratio": 1.6134453781512605, "no_speech_prob": 0.006880940869450569}, {"id": 114, "seek": 49980, "start": 506.16, "end": 510.0, "text": " of fields without like nested structures or so.", "tokens": [50682, 295, 7909, 1553, 411, 15646, 292, 9227, 420, 370, 13, 50874], "temperature": 0.0, "avg_logprob": -0.16416234970092775, "compression_ratio": 1.6134453781512605, "no_speech_prob": 0.006880940869450569}, {"id": 115, "seek": 49980, "start": 510.0, "end": 516.48, "text": " So there's some difference resolving that is a bit of a challenge.", "tokens": [50874, 407, 456, 311, 512, 2649, 49940, 300, 307, 257, 857, 295, 257, 3430, 13, 51198], "temperature": 0.0, "avg_logprob": -0.16416234970092775, "compression_ratio": 1.6134453781512605, "no_speech_prob": 0.006880940869450569}, {"id": 116, "seek": 49980, "start": 516.48, "end": 521.76, "text": " Another interesting thing that we discovered when we started merging ECS is that in OpenTelemedia", "tokens": [51198, 3996, 1880, 551, 300, 321, 6941, 562, 321, 1409, 44559, 19081, 50, 307, 300, 294, 7238, 14233, 306, 1912, 654, 51462], "temperature": 0.0, "avg_logprob": -0.16416234970092775, "compression_ratio": 1.6134453781512605, "no_speech_prob": 0.006880940869450569}, {"id": 117, "seek": 49980, "start": 521.76, "end": 527.64, "text": " before the merger, many times attributes have been defined in a concrete context.", "tokens": [51462, 949, 264, 48002, 11, 867, 1413, 17212, 362, 668, 7642, 294, 257, 9859, 4319, 13, 51756], "temperature": 0.0, "avg_logprob": -0.16416234970092775, "compression_ratio": 1.6134453781512605, "no_speech_prob": 0.006880940869450569}, {"id": 118, "seek": 52764, "start": 527.64, "end": 532.52, "text": " For example, we have here an HTTP server span and the attribute HTTP route is basically", "tokens": [50364, 1171, 1365, 11, 321, 362, 510, 364, 33283, 7154, 16174, 293, 264, 19667, 33283, 7955, 307, 1936, 50608], "temperature": 0.0, "avg_logprob": -0.18446491161982217, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.021223017945885658}, {"id": 119, "seek": 52764, "start": 532.52, "end": 537.16, "text": " defined under the semantic conventions for HTTP server spans.", "tokens": [50608, 7642, 833, 264, 47982, 33520, 337, 33283, 7154, 44086, 13, 50840], "temperature": 0.0, "avg_logprob": -0.18446491161982217, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.021223017945885658}, {"id": 120, "seek": 52764, "start": 537.16, "end": 541.68, "text": " The problem is now if I want to use the same attribute in a different context like let's", "tokens": [50840, 440, 1154, 307, 586, 498, 286, 528, 281, 764, 264, 912, 19667, 294, 257, 819, 4319, 411, 718, 311, 51066], "temperature": 0.0, "avg_logprob": -0.18446491161982217, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.021223017945885658}, {"id": 121, "seek": 52764, "start": 541.68, "end": 548.52, "text": " say HTTP access logs, I mean there was always a means just to reference the other attribute,", "tokens": [51066, 584, 33283, 2105, 20820, 11, 286, 914, 456, 390, 1009, 257, 1355, 445, 281, 6408, 264, 661, 19667, 11, 51408], "temperature": 0.0, "avg_logprob": -0.18446491161982217, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.021223017945885658}, {"id": 122, "seek": 52764, "start": 548.52, "end": 554.36, "text": " but it feels sort of weird because in the one context is a first class, right, attribute", "tokens": [51408, 457, 309, 3417, 1333, 295, 3657, 570, 294, 264, 472, 4319, 307, 257, 700, 1508, 11, 558, 11, 19667, 51700], "temperature": 0.0, "avg_logprob": -0.18446491161982217, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.021223017945885658}, {"id": 123, "seek": 55436, "start": 554.36, "end": 559.8000000000001, "text": " and the other one is just a reference that overrides some semantics.", "tokens": [50364, 293, 264, 661, 472, 307, 445, 257, 6408, 300, 670, 81, 1875, 512, 4361, 45298, 13, 50636], "temperature": 0.0, "avg_logprob": -0.16764628887176514, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.0023520872928202152}, {"id": 124, "seek": 55436, "start": 559.8000000000001, "end": 566.44, "text": " So learning from ECS, what we already achieved with the merger is that now we have in OpenTelemedia", "tokens": [50636, 407, 2539, 490, 19081, 50, 11, 437, 321, 1217, 11042, 365, 264, 48002, 307, 300, 586, 321, 362, 294, 7238, 14233, 306, 1912, 654, 50968], "temperature": 0.0, "avg_logprob": -0.16764628887176514, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.0023520872928202152}, {"id": 125, "seek": 55436, "start": 566.44, "end": 573.96, "text": " a dedicated attributes registry that serves the case of just defining attributes with", "tokens": [50968, 257, 8374, 17212, 36468, 300, 13451, 264, 1389, 295, 445, 17827, 17212, 365, 51344], "temperature": 0.0, "avg_logprob": -0.16764628887176514, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.0023520872928202152}, {"id": 126, "seek": 55436, "start": 573.96, "end": 579.08, "text": " their types, with their meaning and in the different semantic conventions and their use", "tokens": [51344, 641, 3467, 11, 365, 641, 3620, 293, 294, 264, 819, 47982, 33520, 293, 641, 764, 51600], "temperature": 0.0, "avg_logprob": -0.16764628887176514, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.0023520872928202152}, {"id": 127, "seek": 55436, "start": 579.08, "end": 582.36, "text": " cases we are just referencing those attributes.", "tokens": [51600, 3331, 321, 366, 445, 40582, 729, 17212, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16764628887176514, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.0023520872928202152}, {"id": 128, "seek": 58236, "start": 582.36, "end": 589.4, "text": " So we have clear separation between defining attributes and using them in a concrete context.", "tokens": [50364, 407, 321, 362, 1850, 14634, 1296, 17827, 17212, 293, 1228, 552, 294, 257, 9859, 4319, 13, 50716], "temperature": 0.0, "avg_logprob": -0.1912592347845974, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0005191125092096627}, {"id": 129, "seek": 58236, "start": 589.4, "end": 594.32, "text": " And finally another challenge is metrics.", "tokens": [50716, 400, 2721, 1071, 3430, 307, 16367, 13, 50962], "temperature": 0.0, "avg_logprob": -0.1912592347845974, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0005191125092096627}, {"id": 130, "seek": 58236, "start": 594.32, "end": 598.64, "text": " Metrics formats in OpenTelemedia follow the TSTB model.", "tokens": [50962, 6377, 10716, 25879, 294, 7238, 14233, 306, 1912, 654, 1524, 264, 314, 6840, 33, 2316, 13, 51178], "temperature": 0.0, "avg_logprob": -0.1912592347845974, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0005191125092096627}, {"id": 131, "seek": 58236, "start": 598.64, "end": 604.36, "text": " So we have a concrete metric name like system disk IO in this case with a type, with a unit", "tokens": [51178, 407, 321, 362, 257, 9859, 20678, 1315, 411, 1185, 12355, 39839, 294, 341, 1389, 365, 257, 2010, 11, 365, 257, 4985, 51464], "temperature": 0.0, "avg_logprob": -0.1912592347845974, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0005191125092096627}, {"id": 132, "seek": 58236, "start": 604.36, "end": 607.76, "text": " and we have a set of dimensions modeled as attributes.", "tokens": [51464, 293, 321, 362, 257, 992, 295, 12819, 37140, 382, 17212, 13, 51634], "temperature": 0.0, "avg_logprob": -0.1912592347845974, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0005191125092096627}, {"id": 133, "seek": 60776, "start": 607.76, "end": 613.0, "text": " In this case direction for example for disk IO read or write.", "tokens": [50364, 682, 341, 1389, 3513, 337, 1365, 337, 12355, 39839, 1401, 420, 2464, 13, 50626], "temperature": 0.0, "avg_logprob": -0.17977525971152566, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.013386995531618595}, {"id": 134, "seek": 60776, "start": 613.0, "end": 622.48, "text": " In ECS previously the metrics were basically modeled as numerical fields on documents and", "tokens": [50626, 682, 19081, 50, 8046, 264, 16367, 645, 1936, 37140, 382, 29054, 7909, 322, 8512, 293, 51100], "temperature": 0.0, "avg_logprob": -0.17977525971152566, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.013386995531618595}, {"id": 135, "seek": 60776, "start": 622.48, "end": 627.28, "text": " you can have multiple numerical fields in the documents so you can have multiple metrics.", "tokens": [51100, 291, 393, 362, 3866, 29054, 7909, 294, 264, 8512, 370, 291, 393, 362, 3866, 16367, 13, 51340], "temperature": 0.0, "avg_logprob": -0.17977525971152566, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.013386995531618595}, {"id": 136, "seek": 60776, "start": 627.28, "end": 631.4399999999999, "text": " That's the reason why often some of these dimensions that we have in OpenTelemedia are", "tokens": [51340, 663, 311, 264, 1778, 983, 2049, 512, 295, 613, 12819, 300, 321, 362, 294, 7238, 14233, 306, 1912, 654, 366, 51548], "temperature": 0.0, "avg_logprob": -0.17977525971152566, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.013386995531618595}, {"id": 137, "seek": 60776, "start": 631.4399999999999, "end": 634.0, "text": " just encoded into the metric name on ECS side.", "tokens": [51548, 445, 2058, 12340, 666, 264, 20678, 1315, 322, 19081, 50, 1252, 13, 51676], "temperature": 0.0, "avg_logprob": -0.17977525971152566, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.013386995531618595}, {"id": 138, "seek": 63400, "start": 634.0, "end": 639.2, "text": " So we have things like disk read bytes or disk write bytes.", "tokens": [50364, 407, 321, 362, 721, 411, 12355, 1401, 36088, 420, 12355, 2464, 36088, 13, 50624], "temperature": 0.0, "avg_logprob": -0.16182770980031866, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.020544566214084625}, {"id": 139, "seek": 63400, "start": 639.2, "end": 642.68, "text": " This is quite a big difference in modeling.", "tokens": [50624, 639, 307, 1596, 257, 955, 2649, 294, 15983, 13, 50798], "temperature": 0.0, "avg_logprob": -0.16182770980031866, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.020544566214084625}, {"id": 140, "seek": 63400, "start": 642.68, "end": 648.6, "text": " This is a case where we are learning basically from OpenTelemedia and adopting this at Elastic", "tokens": [50798, 639, 307, 257, 1389, 689, 321, 366, 2539, 1936, 490, 7238, 14233, 306, 1912, 654, 293, 32328, 341, 412, 2699, 2750, 51094], "temperature": 0.0, "avg_logprob": -0.16182770980031866, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.020544566214084625}, {"id": 141, "seek": 63400, "start": 648.6, "end": 652.56, "text": " now also with Elastic Search supporting TSTB.", "tokens": [51094, 586, 611, 365, 2699, 2750, 17180, 7231, 314, 6840, 33, 13, 51292], "temperature": 0.0, "avg_logprob": -0.16182770980031866, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.020544566214084625}, {"id": 142, "seek": 63400, "start": 652.56, "end": 657.72, "text": " So we see we are learning from both sides which is a great thing and we are coming to", "tokens": [51292, 407, 321, 536, 321, 366, 2539, 490, 1293, 4881, 597, 307, 257, 869, 551, 293, 321, 366, 1348, 281, 51550], "temperature": 0.0, "avg_logprob": -0.16182770980031866, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.020544566214084625}, {"id": 143, "seek": 63400, "start": 657.72, "end": 660.76, "text": " the best solution possible for the community.", "tokens": [51550, 264, 1151, 3827, 1944, 337, 264, 1768, 13, 51702], "temperature": 0.0, "avg_logprob": -0.16182770980031866, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.020544566214084625}, {"id": 144, "seek": 66076, "start": 661.76, "end": 665.76, "text": " And Chris will tell you how this actual merger is happening in practice.", "tokens": [50414, 400, 6688, 486, 980, 291, 577, 341, 3539, 48002, 307, 2737, 294, 3124, 13, 50614], "temperature": 0.0, "avg_logprob": -0.3164611543927874, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.01259889267385006}, {"id": 145, "seek": 66076, "start": 678.76, "end": 679.76, "text": " Thank you.", "tokens": [51264, 1044, 291, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3164611543927874, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.01259889267385006}, {"id": 146, "seek": 66076, "start": 679.76, "end": 680.76, "text": " Can you hear me?", "tokens": [51314, 1664, 291, 1568, 385, 30, 51364], "temperature": 0.0, "avg_logprob": -0.3164611543927874, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.01259889267385006}, {"id": 147, "seek": 66076, "start": 680.76, "end": 681.76, "text": " Okay.", "tokens": [51364, 1033, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3164611543927874, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.01259889267385006}, {"id": 148, "seek": 66076, "start": 682.76, "end": 689.12, "text": " So as Alex mentioned there are a lot of things going on so the question is when is time to", "tokens": [51464, 407, 382, 5202, 2835, 456, 366, 257, 688, 295, 721, 516, 322, 370, 264, 1168, 307, 562, 307, 565, 281, 51782], "temperature": 0.0, "avg_logprob": -0.3164611543927874, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.01259889267385006}, {"id": 149, "seek": 68912, "start": 689.12, "end": 694.52, "text": " celebrate the merger that everything has been completed and the truth is that we are", "tokens": [50364, 8098, 264, 48002, 300, 1203, 575, 668, 7365, 293, 264, 3494, 307, 300, 321, 366, 50634], "temperature": 0.0, "avg_logprob": -0.15671438329360066, "compression_ratio": 1.868020304568528, "no_speech_prob": 0.0038054913748055696}, {"id": 150, "seek": 68912, "start": 694.52, "end": 695.52, "text": " not there yet.", "tokens": [50634, 406, 456, 1939, 13, 50684], "temperature": 0.0, "avg_logprob": -0.15671438329360066, "compression_ratio": 1.868020304568528, "no_speech_prob": 0.0038054913748055696}, {"id": 151, "seek": 68912, "start": 695.52, "end": 700.04, "text": " There are things that needs to be done and actually everyone believed in the beginning", "tokens": [50684, 821, 366, 721, 300, 2203, 281, 312, 1096, 293, 767, 1518, 7847, 294, 264, 2863, 50910], "temperature": 0.0, "avg_logprob": -0.15671438329360066, "compression_ratio": 1.868020304568528, "no_speech_prob": 0.0038054913748055696}, {"id": 152, "seek": 68912, "start": 700.04, "end": 704.0, "text": " that once the merger was announced that that's all.", "tokens": [50910, 300, 1564, 264, 48002, 390, 7548, 300, 300, 311, 439, 13, 51108], "temperature": 0.0, "avg_logprob": -0.15671438329360066, "compression_ratio": 1.868020304568528, "no_speech_prob": 0.0038054913748055696}, {"id": 153, "seek": 68912, "start": 704.0, "end": 711.32, "text": " I mean we have not anything to add there but yeah the truth is that the actual work started", "tokens": [51108, 286, 914, 321, 362, 406, 1340, 281, 909, 456, 457, 1338, 264, 3494, 307, 300, 264, 3539, 589, 1409, 51474], "temperature": 0.0, "avg_logprob": -0.15671438329360066, "compression_ratio": 1.868020304568528, "no_speech_prob": 0.0038054913748055696}, {"id": 154, "seek": 68912, "start": 711.32, "end": 714.24, "text": " right after the merger was announced.", "tokens": [51474, 558, 934, 264, 48002, 390, 7548, 13, 51620], "temperature": 0.0, "avg_logprob": -0.15671438329360066, "compression_ratio": 1.868020304568528, "no_speech_prob": 0.0038054913748055696}, {"id": 155, "seek": 71424, "start": 714.24, "end": 719.8, "text": " So yeah let's see some examples of how the merger is happening and how things are moving", "tokens": [50364, 407, 1338, 718, 311, 536, 512, 5110, 295, 577, 264, 48002, 307, 2737, 293, 577, 721, 366, 2684, 50642], "temperature": 0.0, "avg_logprob": -0.13327870002159706, "compression_ratio": 1.6172248803827751, "no_speech_prob": 0.0017496927175670862}, {"id": 156, "seek": 71424, "start": 719.8, "end": 720.8, "text": " forward.", "tokens": [50642, 2128, 13, 50692], "temperature": 0.0, "avg_logprob": -0.13327870002159706, "compression_ratio": 1.6172248803827751, "no_speech_prob": 0.0017496927175670862}, {"id": 157, "seek": 71424, "start": 720.8, "end": 726.04, "text": " So I have some real examples here from the upstream repository on GitHub with issues and", "tokens": [50692, 407, 286, 362, 512, 957, 5110, 510, 490, 264, 33915, 25841, 322, 23331, 365, 2663, 293, 50954], "temperature": 0.0, "avg_logprob": -0.13327870002159706, "compression_ratio": 1.6172248803827751, "no_speech_prob": 0.0017496927175670862}, {"id": 158, "seek": 71424, "start": 726.04, "end": 727.24, "text": " pull requests.", "tokens": [50954, 2235, 12475, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13327870002159706, "compression_ratio": 1.6172248803827751, "no_speech_prob": 0.0017496927175670862}, {"id": 159, "seek": 71424, "start": 727.24, "end": 732.92, "text": " So this one for example is trying to add some new resource attributes for the container", "tokens": [51014, 407, 341, 472, 337, 1365, 307, 1382, 281, 909, 512, 777, 7684, 17212, 337, 264, 10129, 51298], "temperature": 0.0, "avg_logprob": -0.13327870002159706, "compression_ratio": 1.6172248803827751, "no_speech_prob": 0.0017496927175670862}, {"id": 160, "seek": 71424, "start": 732.92, "end": 737.04, "text": " images and specifically the digest of the image.", "tokens": [51298, 5267, 293, 4682, 264, 13884, 295, 264, 3256, 13, 51504], "temperature": 0.0, "avg_logprob": -0.13327870002159706, "compression_ratio": 1.6172248803827751, "no_speech_prob": 0.0017496927175670862}, {"id": 161, "seek": 73704, "start": 737.04, "end": 745.1999999999999, "text": " So as we can see that PR was filed on the 4th of July I think yes and it took it some", "tokens": [50364, 407, 382, 321, 393, 536, 300, 11568, 390, 18789, 322, 264, 1017, 392, 295, 7370, 286, 519, 2086, 293, 309, 1890, 309, 512, 50772], "temperature": 0.0, "avg_logprob": -0.1550418184949206, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.006326151546090841}, {"id": 162, "seek": 73704, "start": 745.1999999999999, "end": 746.52, "text": " time to get seen right.", "tokens": [50772, 565, 281, 483, 1612, 558, 13, 50838], "temperature": 0.0, "avg_logprob": -0.1550418184949206, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.006326151546090841}, {"id": 163, "seek": 73704, "start": 746.52, "end": 753.68, "text": " So it took us like many review cycles more than 20 blocker comments actually there so", "tokens": [50838, 407, 309, 1890, 505, 411, 867, 3131, 17796, 544, 813, 945, 3461, 260, 3053, 767, 456, 370, 51196], "temperature": 0.0, "avg_logprob": -0.1550418184949206, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.006326151546090841}, {"id": 164, "seek": 73704, "start": 753.68, "end": 759.1999999999999, "text": " lots of back and forth lots of discussions but that one was actually merged after almost", "tokens": [51196, 3195, 295, 646, 293, 5220, 3195, 295, 11088, 457, 300, 472, 390, 767, 36427, 934, 1920, 51472], "temperature": 0.0, "avg_logprob": -0.1550418184949206, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.006326151546090841}, {"id": 165, "seek": 73704, "start": 759.1999999999999, "end": 760.7199999999999, "text": " two months.", "tokens": [51472, 732, 2493, 13, 51548], "temperature": 0.0, "avg_logprob": -0.1550418184949206, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.006326151546090841}, {"id": 166, "seek": 76072, "start": 760.72, "end": 767.64, "text": " And another example is about a very important attribute the IP of the host hosted IP as", "tokens": [50364, 400, 1071, 1365, 307, 466, 257, 588, 1021, 19667, 264, 8671, 295, 264, 3975, 19204, 8671, 382, 50710], "temperature": 0.0, "avg_logprob": -0.1572406617077914, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.038632139563560486}, {"id": 167, "seek": 76072, "start": 767.64, "end": 773.24, "text": " we call it and this one was really unique really interesting actually because this PR", "tokens": [50710, 321, 818, 309, 293, 341, 472, 390, 534, 3845, 534, 1880, 767, 570, 341, 11568, 50990], "temperature": 0.0, "avg_logprob": -0.1572406617077914, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.038632139563560486}, {"id": 168, "seek": 76072, "start": 773.24, "end": 775.88, "text": " was filed by a non ECS contributor.", "tokens": [50990, 390, 18789, 538, 257, 2107, 19081, 50, 42859, 13, 51122], "temperature": 0.0, "avg_logprob": -0.1572406617077914, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.038632139563560486}, {"id": 169, "seek": 76072, "start": 775.88, "end": 780.88, "text": " So actually that contributor used to work for a company that it's I would say completely", "tokens": [51122, 407, 767, 300, 42859, 1143, 281, 589, 337, 257, 2237, 300, 309, 311, 286, 576, 584, 2584, 51372], "temperature": 0.0, "avg_logprob": -0.1572406617077914, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.038632139563560486}, {"id": 170, "seek": 76072, "start": 780.88, "end": 785.64, "text": " unrelated to the ECS project but it was quite nice because in that case the existence of", "tokens": [51372, 38967, 281, 264, 19081, 50, 1716, 457, 309, 390, 1596, 1481, 570, 294, 300, 1389, 264, 9123, 295, 51610], "temperature": 0.0, "avg_logprob": -0.1572406617077914, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.038632139563560486}, {"id": 171, "seek": 78564, "start": 785.64, "end": 790.64, "text": " the ECS project was taken into account and there were very interesting conversations", "tokens": [50364, 264, 19081, 50, 1716, 390, 2726, 666, 2696, 293, 456, 645, 588, 1880, 7315, 50614], "temperature": 0.0, "avg_logprob": -0.09981097115410699, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.03515647351741791}, {"id": 172, "seek": 78564, "start": 790.64, "end": 795.1999999999999, "text": " and it took us like almost three months to have it in.", "tokens": [50614, 293, 309, 1890, 505, 411, 1920, 1045, 2493, 281, 362, 309, 294, 13, 50842], "temperature": 0.0, "avg_logprob": -0.09981097115410699, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.03515647351741791}, {"id": 173, "seek": 78564, "start": 795.1999999999999, "end": 801.6, "text": " So yeah it's quite obvious with these examples that the merger was not something trivial", "tokens": [50842, 407, 1338, 309, 311, 1596, 6322, 365, 613, 5110, 300, 264, 48002, 390, 406, 746, 26703, 51162], "temperature": 0.0, "avg_logprob": -0.09981097115410699, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.03515647351741791}, {"id": 174, "seek": 78564, "start": 801.6, "end": 806.36, "text": " not something straightforward that can happen from one day to the other by for example writing", "tokens": [51162, 406, 746, 15325, 300, 393, 1051, 490, 472, 786, 281, 264, 661, 538, 337, 1365, 3579, 51400], "temperature": 0.0, "avg_logprob": -0.09981097115410699, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.03515647351741791}, {"id": 175, "seek": 78564, "start": 806.36, "end": 810.2, "text": " a script that will transfer everything from one project to the other or something like", "tokens": [51400, 257, 5755, 300, 486, 5003, 1203, 490, 472, 1716, 281, 264, 661, 420, 746, 411, 51592], "temperature": 0.0, "avg_logprob": -0.09981097115410699, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.03515647351741791}, {"id": 176, "seek": 78564, "start": 810.2, "end": 811.2, "text": " that.", "tokens": [51592, 300, 13, 51642], "temperature": 0.0, "avg_logprob": -0.09981097115410699, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.03515647351741791}, {"id": 177, "seek": 81120, "start": 811.2800000000001, "end": 818.9200000000001, "text": " So we have decided to take an approach to move let's say not so fast and pay attention to", "tokens": [50368, 407, 321, 362, 3047, 281, 747, 364, 3109, 281, 1286, 718, 311, 584, 406, 370, 2370, 293, 1689, 3202, 281, 50750], "temperature": 0.0, "avg_logprob": -0.22327272184602506, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.003253278089687228}, {"id": 178, "seek": 81120, "start": 818.9200000000001, "end": 824.5400000000001, "text": " the detail and have the proper people work on specific areas so as to leverage their", "tokens": [50750, 264, 2607, 293, 362, 264, 2296, 561, 589, 322, 2685, 3179, 370, 382, 281, 13982, 641, 51031], "temperature": 0.0, "avg_logprob": -0.22327272184602506, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.003253278089687228}, {"id": 179, "seek": 81120, "start": 824.5400000000001, "end": 830.44, "text": " expertise and be sure that what we are merging to the up seem to the final project which", "tokens": [51031, 11769, 293, 312, 988, 300, 437, 321, 366, 44559, 281, 264, 493, 1643, 281, 264, 2572, 1716, 597, 51326], "temperature": 0.0, "avg_logprob": -0.22327272184602506, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.003253278089687228}, {"id": 180, "seek": 81120, "start": 830.44, "end": 834.6400000000001, "text": " is actually the sematic convention of open telemetry will stay there and everyone will", "tokens": [51326, 307, 767, 264, 4361, 2399, 10286, 295, 1269, 4304, 5537, 627, 486, 1754, 456, 293, 1518, 486, 51536], "temperature": 0.0, "avg_logprob": -0.22327272184602506, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.003253278089687228}, {"id": 181, "seek": 81120, "start": 834.6400000000001, "end": 837.2800000000001, "text": " be happy with that in the future.", "tokens": [51536, 312, 2055, 365, 300, 294, 264, 2027, 13, 51668], "temperature": 0.0, "avg_logprob": -0.22327272184602506, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.003253278089687228}, {"id": 182, "seek": 83728, "start": 837.36, "end": 842.16, "text": " So that's more or less the areas of the sematic conventions.", "tokens": [50368, 407, 300, 311, 544, 420, 1570, 264, 3179, 295, 264, 4361, 2399, 33520, 13, 50608], "temperature": 0.0, "avg_logprob": -0.20398830545359645, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0018101334571838379}, {"id": 183, "seek": 83728, "start": 842.16, "end": 849.0, "text": " We have areas in area about databases cloud containers Kubernetes HTTP system metric", "tokens": [50608, 492, 362, 3179, 294, 1859, 466, 22380, 4588, 17089, 23145, 33283, 1185, 20678, 50950], "temperature": 0.0, "avg_logprob": -0.20398830545359645, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0018101334571838379}, {"id": 184, "seek": 83728, "start": 849.0, "end": 852.68, "text": " system resource attributes and many others.", "tokens": [50950, 1185, 7684, 17212, 293, 867, 2357, 13, 51134], "temperature": 0.0, "avg_logprob": -0.20398830545359645, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0018101334571838379}, {"id": 185, "seek": 83728, "start": 852.68, "end": 859.76, "text": " And yeah so we have started focusing on specific areas some examples is the effort that we", "tokens": [51134, 400, 1338, 370, 321, 362, 1409, 8416, 322, 2685, 3179, 512, 5110, 307, 264, 4630, 300, 321, 51488], "temperature": 0.0, "avg_logprob": -0.20398830545359645, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0018101334571838379}, {"id": 186, "seek": 83728, "start": 859.76, "end": 865.24, "text": " are doing on the system metrics area we have a working group working there focusing on the", "tokens": [51488, 366, 884, 322, 264, 1185, 16367, 1859, 321, 362, 257, 1364, 1594, 1364, 456, 8416, 322, 264, 51762], "temperature": 0.0, "avg_logprob": -0.20398830545359645, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0018101334571838379}, {"id": 187, "seek": 83728, "start": 865.24, "end": 867.0799999999999, "text": " stability of the area.", "tokens": [51762, 11826, 295, 264, 1859, 13, 51854], "temperature": 0.0, "avg_logprob": -0.20398830545359645, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0018101334571838379}, {"id": 188, "seek": 86708, "start": 867.08, "end": 872.44, "text": " We are in a really good position now we are moving towards the ability really soon and", "tokens": [50364, 492, 366, 294, 257, 534, 665, 2535, 586, 321, 366, 2684, 3030, 264, 3485, 534, 2321, 293, 50632], "temperature": 0.0, "avg_logprob": -0.23260914382114206, "compression_ratio": 1.8630705394190872, "no_speech_prob": 0.004694899544119835}, {"id": 189, "seek": 86708, "start": 872.44, "end": 877.64, "text": " the same for the process namespace the process area the process resource attributes and the", "tokens": [50632, 264, 912, 337, 264, 1399, 5288, 17940, 264, 1399, 1859, 264, 1399, 7684, 17212, 293, 264, 50892], "temperature": 0.0, "avg_logprob": -0.23260914382114206, "compression_ratio": 1.8630705394190872, "no_speech_prob": 0.004694899544119835}, {"id": 190, "seek": 86708, "start": 877.64, "end": 883.6800000000001, "text": " same for container area we are close to achieving the 100 percent converges there the recent", "tokens": [50892, 912, 337, 10129, 1859, 321, 366, 1998, 281, 19626, 264, 2319, 3043, 9652, 2880, 456, 264, 5162, 51194], "temperature": 0.0, "avg_logprob": -0.23260914382114206, "compression_ratio": 1.8630705394190872, "no_speech_prob": 0.004694899544119835}, {"id": 191, "seek": 86708, "start": 883.6800000000001, "end": 890.24, "text": " going PR that will add the final attributes final metrics excuse me same for HTTP and", "tokens": [51194, 516, 11568, 300, 486, 909, 264, 2572, 17212, 2572, 16367, 8960, 385, 912, 337, 33283, 293, 51522], "temperature": 0.0, "avg_logprob": -0.23260914382114206, "compression_ratio": 1.8630705394190872, "no_speech_prob": 0.004694899544119835}, {"id": 192, "seek": 86708, "start": 890.24, "end": 896.6, "text": " network areas we have good coverage HTTP sematic conventions were declared as stable really", "tokens": [51522, 3209, 3179, 321, 362, 665, 9645, 33283, 4361, 2399, 33520, 645, 15489, 382, 8351, 534, 51840], "temperature": 0.0, "avg_logprob": -0.23260914382114206, "compression_ratio": 1.8630705394190872, "no_speech_prob": 0.004694899544119835}, {"id": 193, "seek": 89660, "start": 896.6, "end": 903.32, "text": " recently so we are adding on top now which is quite nice and yeah we have work in progress", "tokens": [50364, 3938, 370, 321, 366, 5127, 322, 1192, 586, 597, 307, 1596, 1481, 293, 1338, 321, 362, 589, 294, 4205, 50700], "temperature": 0.0, "avg_logprob": -0.2618245344895583, "compression_ratio": 1.6794258373205742, "no_speech_prob": 0.013029796071350574}, {"id": 194, "seek": 89660, "start": 903.32, "end": 910.52, "text": " in databases mobile areas cloud Kubernetes so we have working groups getting started", "tokens": [50700, 294, 22380, 6013, 3179, 4588, 23145, 370, 321, 362, 1364, 3935, 1242, 1409, 51060], "temperature": 0.0, "avg_logprob": -0.2618245344895583, "compression_ratio": 1.6794258373205742, "no_speech_prob": 0.013029796071350574}, {"id": 195, "seek": 89660, "start": 910.52, "end": 919.16, "text": " and focusing on these areas and yeah over the past months we are focusing on making", "tokens": [51060, 293, 8416, 322, 613, 3179, 293, 1338, 670, 264, 1791, 2493, 321, 366, 8416, 322, 1455, 51492], "temperature": 0.0, "avg_logprob": -0.2618245344895583, "compression_ratio": 1.6794258373205742, "no_speech_prob": 0.013029796071350574}, {"id": 196, "seek": 89660, "start": 919.16, "end": 926.4, "text": " the project as good as possible it's a community driven way so we as ECS contributes to the", "tokens": [51492, 264, 1716, 382, 665, 382, 1944, 309, 311, 257, 1768, 9555, 636, 370, 321, 382, 19081, 50, 32035, 281, 264, 51854], "temperature": 0.0, "avg_logprob": -0.2618245344895583, "compression_ratio": 1.6794258373205742, "no_speech_prob": 0.013029796071350574}, {"id": 197, "seek": 92640, "start": 926.4, "end": 931.4399999999999, "text": " contributors donating this project we are not only focusing on the merger itself but", "tokens": [50364, 45627, 36686, 341, 1716, 321, 366, 406, 787, 8416, 322, 264, 48002, 2564, 457, 50616], "temperature": 0.0, "avg_logprob": -0.15338017078156166, "compression_ratio": 2.0044444444444443, "no_speech_prob": 0.041575100272893906}, {"id": 198, "seek": 92640, "start": 931.4399999999999, "end": 937.8, "text": " we want also to ensure that the sematic conventions project will be there and will can serve us", "tokens": [50616, 321, 528, 611, 281, 5586, 300, 264, 4361, 2399, 33520, 1716, 486, 312, 456, 293, 486, 393, 4596, 505, 50934], "temperature": 0.0, "avg_logprob": -0.15338017078156166, "compression_ratio": 2.0044444444444443, "no_speech_prob": 0.041575100272893906}, {"id": 199, "seek": 92640, "start": 937.8, "end": 944.88, "text": " in the future so we are also focusing on other things as well like improving the tooling", "tokens": [50934, 294, 264, 2027, 370, 321, 366, 611, 8416, 322, 661, 721, 382, 731, 411, 11470, 264, 46593, 51288], "temperature": 0.0, "avg_logprob": -0.15338017078156166, "compression_ratio": 2.0044444444444443, "no_speech_prob": 0.041575100272893906}, {"id": 200, "seek": 92640, "start": 944.88, "end": 950.24, "text": " of the project working on the guidelines this is quite important because there are many times", "tokens": [51288, 295, 264, 1716, 1364, 322, 264, 12470, 341, 307, 1596, 1021, 570, 456, 366, 867, 1413, 51556], "temperature": 0.0, "avg_logprob": -0.15338017078156166, "compression_ratio": 2.0044444444444443, "no_speech_prob": 0.041575100272893906}, {"id": 201, "seek": 92640, "start": 950.24, "end": 954.72, "text": " that the guidelines of the one project are in conflict with the guidelines of the other", "tokens": [51556, 300, 264, 12470, 295, 264, 472, 1716, 366, 294, 6596, 365, 264, 12470, 295, 264, 661, 51780], "temperature": 0.0, "avg_logprob": -0.15338017078156166, "compression_ratio": 2.0044444444444443, "no_speech_prob": 0.041575100272893906}, {"id": 202, "seek": 95472, "start": 954.72, "end": 959.88, "text": " projects so in that case we need to take a step back and reconsider the guidelines and", "tokens": [50364, 4455, 370, 294, 300, 1389, 321, 643, 281, 747, 257, 1823, 646, 293, 40497, 264, 12470, 293, 50622], "temperature": 0.0, "avg_logprob": -0.16875369548797609, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.012344383634626865}, {"id": 203, "seek": 95472, "start": 959.88, "end": 967.5600000000001, "text": " see what we want to have there as a final result and yeah also we work on restructuring the", "tokens": [50622, 536, 437, 321, 528, 281, 362, 456, 382, 257, 2572, 1874, 293, 1338, 611, 321, 589, 322, 1472, 1757, 1345, 264, 51006], "temperature": 0.0, "avg_logprob": -0.16875369548797609, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.012344383634626865}, {"id": 204, "seek": 95472, "start": 967.5600000000001, "end": 972.8000000000001, "text": " project before it was the sematic conventions within the project were grouped by signal", "tokens": [51006, 1716, 949, 309, 390, 264, 4361, 2399, 33520, 1951, 264, 1716, 645, 41877, 538, 6358, 51268], "temperature": 0.0, "avg_logprob": -0.16875369548797609, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.012344383634626865}, {"id": 205, "seek": 95472, "start": 972.8000000000001, "end": 979.4, "text": " logs metrics traces and so on but now we have a better organized organization there and we", "tokens": [51268, 20820, 16367, 26076, 293, 370, 322, 457, 586, 321, 362, 257, 1101, 9983, 4475, 456, 293, 321, 51598], "temperature": 0.0, "avg_logprob": -0.16875369548797609, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.012344383634626865}, {"id": 206, "seek": 97940, "start": 979.48, "end": 985.36, "text": " group the attributes by topic and yeah as Alex mentioned already we have introduced the", "tokens": [50368, 1594, 264, 17212, 538, 4829, 293, 1338, 382, 5202, 2835, 1217, 321, 362, 7268, 264, 50662], "temperature": 0.0, "avg_logprob": -0.12893227527016088, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.004878698382526636}, {"id": 207, "seek": 97940, "start": 985.36, "end": 990.64, "text": " global attributes registry it's actually a very big list with all the attributes there and then", "tokens": [50662, 4338, 17212, 36468, 309, 311, 767, 257, 588, 955, 1329, 365, 439, 264, 17212, 456, 293, 550, 50926], "temperature": 0.0, "avg_logprob": -0.12893227527016088, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.004878698382526636}, {"id": 208, "seek": 97940, "start": 990.64, "end": 996.72, "text": " within the actual specification you can reference the attributes from there so yeah that's quite", "tokens": [50926, 1951, 264, 3539, 31256, 291, 393, 6408, 264, 17212, 490, 456, 370, 1338, 300, 311, 1596, 51230], "temperature": 0.0, "avg_logprob": -0.12893227527016088, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.004878698382526636}, {"id": 209, "seek": 97940, "start": 996.72, "end": 1004.88, "text": " useful and we're also working on adding a new concept from ECS which actually the attribute", "tokens": [51230, 4420, 293, 321, 434, 611, 1364, 322, 5127, 257, 777, 3410, 490, 19081, 50, 597, 767, 264, 19667, 51638], "temperature": 0.0, "avg_logprob": -0.12893227527016088, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.004878698382526636}, {"id": 210, "seek": 100488, "start": 1004.92, "end": 1009.84, "text": " nesting or reusing some namespaces that means that if you have a namespace for example always", "tokens": [50366, 297, 8714, 420, 319, 7981, 512, 5288, 79, 2116, 300, 1355, 300, 498, 291, 362, 257, 5288, 17940, 337, 1365, 1009, 50612], "temperature": 0.0, "avg_logprob": -0.13306781520014224, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0377984344959259}, {"id": 211, "seek": 100488, "start": 1009.84, "end": 1016.16, "text": " dot whatever you can nest it attach it as it is under the host namespace for example and you", "tokens": [50612, 5893, 2035, 291, 393, 15646, 309, 5085, 309, 382, 309, 307, 833, 264, 3975, 5288, 17940, 337, 1365, 293, 291, 50928], "temperature": 0.0, "avg_logprob": -0.13306781520014224, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0377984344959259}, {"id": 212, "seek": 100488, "start": 1016.16, "end": 1022.76, "text": " don't need to redefine it again so yeah these are some examples from the upstream most of them are", "tokens": [50928, 500, 380, 643, 281, 38818, 533, 309, 797, 370, 1338, 613, 366, 512, 5110, 490, 264, 33915, 881, 295, 552, 366, 51258], "temperature": 0.0, "avg_logprob": -0.13306781520014224, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0377984344959259}, {"id": 213, "seek": 100488, "start": 1022.76, "end": 1030.64, "text": " closed some of them are really let's say close to be completed but we have some small blockers", "tokens": [51258, 5395, 512, 295, 552, 366, 534, 718, 311, 584, 1998, 281, 312, 7365, 457, 321, 362, 512, 1359, 3461, 433, 51652], "temperature": 0.0, "avg_logprob": -0.13306781520014224, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0377984344959259}, {"id": 214, "seek": 103064, "start": 1030.68, "end": 1036.88, "text": " there but the work is moving forward that's a that's the point and yeah how the community is", "tokens": [50366, 456, 457, 264, 589, 307, 2684, 2128, 300, 311, 257, 300, 311, 264, 935, 293, 1338, 577, 264, 1768, 307, 50676], "temperature": 0.0, "avg_logprob": -0.1726309498654136, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.05249451473355293}, {"id": 215, "seek": 103064, "start": 1036.88, "end": 1043.2800000000002, "text": " organized around these so as I mentioned before we want to have proper people working on specific", "tokens": [50676, 9983, 926, 613, 370, 382, 286, 2835, 949, 321, 528, 281, 362, 2296, 561, 1364, 322, 2685, 50996], "temperature": 0.0, "avg_logprob": -0.1726309498654136, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.05249451473355293}, {"id": 216, "seek": 103064, "start": 1043.2800000000002, "end": 1049.8400000000001, "text": " areas leveraging their expertise so we have working groups working on each area and we're", "tokens": [50996, 3179, 32666, 641, 11769, 370, 321, 362, 1364, 3935, 1364, 322, 1184, 1859, 293, 321, 434, 51324], "temperature": 0.0, "avg_logprob": -0.1726309498654136, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.05249451473355293}, {"id": 217, "seek": 103064, "start": 1049.8400000000001, "end": 1056.0400000000002, "text": " trying to first declare their the areas of the semantic attribute the sematic conventions as", "tokens": [51324, 1382, 281, 700, 19710, 641, 264, 3179, 295, 264, 47982, 19667, 264, 4361, 2399, 33520, 382, 51634], "temperature": 0.0, "avg_logprob": -0.1726309498654136, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.05249451473355293}, {"id": 218, "seek": 105604, "start": 1056.08, "end": 1060.48, "text": " stable which means that all the semantic conventions that we will have there will be", "tokens": [50366, 8351, 597, 1355, 300, 439, 264, 47982, 33520, 300, 321, 486, 362, 456, 486, 312, 50586], "temperature": 0.0, "avg_logprob": -0.1366917042250044, "compression_ratio": 1.9429824561403508, "no_speech_prob": 0.0011126099852845073}, {"id": 219, "seek": 105604, "start": 1060.48, "end": 1065.6399999999999, "text": " stable and then we can use them in the actual implementations so the next step is to tune", "tokens": [50586, 8351, 293, 550, 321, 393, 764, 552, 294, 264, 3539, 4445, 763, 370, 264, 958, 1823, 307, 281, 10864, 50844], "temperature": 0.0, "avg_logprob": -0.1366917042250044, "compression_ratio": 1.9429824561403508, "no_speech_prob": 0.0011126099852845073}, {"id": 220, "seek": 105604, "start": 1065.6399999999999, "end": 1070.0, "text": " the implementations accordingly which means essentially the open telemetry collector and", "tokens": [50844, 264, 4445, 763, 19717, 597, 1355, 4476, 264, 1269, 4304, 5537, 627, 23960, 293, 51062], "temperature": 0.0, "avg_logprob": -0.1366917042250044, "compression_ratio": 1.9429824561403508, "no_speech_prob": 0.0011126099852845073}, {"id": 221, "seek": 105604, "start": 1070.0, "end": 1079.76, "text": " the language SDKs and yeah some examples the system metrics working group the working group", "tokens": [51062, 264, 2856, 37135, 82, 293, 1338, 512, 5110, 264, 1185, 16367, 1364, 1594, 264, 1364, 1594, 51550], "temperature": 0.0, "avg_logprob": -0.1366917042250044, "compression_ratio": 1.9429824561403508, "no_speech_prob": 0.0011126099852845073}, {"id": 222, "seek": 105604, "start": 1079.76, "end": 1085.8799999999999, "text": " around databases we have a security semantic conventions working group which is getting", "tokens": [51550, 926, 22380, 321, 362, 257, 3825, 47982, 33520, 1364, 1594, 597, 307, 1242, 51856], "temperature": 0.0, "avg_logprob": -0.1366917042250044, "compression_ratio": 1.9429824561403508, "no_speech_prob": 0.0011126099852845073}, {"id": 223, "seek": 108588, "start": 1085.92, "end": 1092.5600000000002, "text": " started now we have also approvers areas for the mobile area containers Kubernetes and many others", "tokens": [50366, 1409, 586, 321, 362, 611, 2075, 840, 3179, 337, 264, 6013, 1859, 17089, 23145, 293, 867, 2357, 50698], "temperature": 0.0, "avg_logprob": -0.13032861403477045, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00502353860065341}, {"id": 224, "seek": 108588, "start": 1092.5600000000002, "end": 1099.8400000000001, "text": " that I don't mention here and the process looks like this first once you want to create a working", "tokens": [50698, 300, 286, 500, 380, 2152, 510, 293, 264, 1399, 1542, 411, 341, 700, 1564, 291, 528, 281, 1884, 257, 1364, 51062], "temperature": 0.0, "avg_logprob": -0.13032861403477045, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00502353860065341}, {"id": 225, "seek": 108588, "start": 1099.8400000000001, "end": 1106.1200000000001, "text": " group or a specific project you propose the working group area and you mentioned there what", "tokens": [51062, 1594, 420, 257, 2685, 1716, 291, 17421, 264, 1364, 1594, 1859, 293, 291, 2835, 456, 437, 51376], "temperature": 0.0, "avg_logprob": -0.13032861403477045, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00502353860065341}, {"id": 226, "seek": 108588, "start": 1106.1200000000001, "end": 1112.0, "text": " issues you want to work on and then you will have people expressing their interest to join this", "tokens": [51376, 2663, 291, 528, 281, 589, 322, 293, 550, 291, 486, 362, 561, 22171, 641, 1179, 281, 3917, 341, 51670], "temperature": 0.0, "avg_logprob": -0.13032861403477045, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00502353860065341}, {"id": 227, "seek": 111200, "start": 1112.04, "end": 1117.84, "text": " effort you will need to find a sponsor from the technical committee and yeah once everything is", "tokens": [50366, 4630, 291, 486, 643, 281, 915, 257, 16198, 490, 264, 6191, 7482, 293, 1338, 1564, 1203, 307, 50656], "temperature": 0.0, "avg_logprob": -0.1388700191791241, "compression_ratio": 1.8317307692307692, "no_speech_prob": 0.017781421542167664}, {"id": 228, "seek": 111200, "start": 1117.84, "end": 1125.24, "text": " decided we have a specific project board we have regular meetings we have people getting assigned", "tokens": [50656, 3047, 321, 362, 257, 2685, 1716, 3150, 321, 362, 3890, 8410, 321, 362, 561, 1242, 13279, 51026], "temperature": 0.0, "avg_logprob": -0.1388700191791241, "compression_ratio": 1.8317307692307692, "no_speech_prob": 0.017781421542167664}, {"id": 229, "seek": 111200, "start": 1125.24, "end": 1132.8, "text": " to the issues there and yeah the work is happening like this and yeah regarding the merger itself", "tokens": [51026, 281, 264, 2663, 456, 293, 1338, 264, 589, 307, 2737, 411, 341, 293, 1338, 8595, 264, 48002, 2564, 51404], "temperature": 0.0, "avg_logprob": -0.1388700191791241, "compression_ratio": 1.8317307692307692, "no_speech_prob": 0.017781421542167664}, {"id": 230, "seek": 111200, "start": 1132.8, "end": 1140.4, "text": " in yeah technically it happens like this we follow this process so once we have to either", "tokens": [51404, 294, 1338, 12120, 309, 2314, 411, 341, 321, 1524, 341, 1399, 370, 1564, 321, 362, 281, 2139, 51784], "temperature": 0.0, "avg_logprob": -0.1388700191791241, "compression_ratio": 1.8317307692307692, "no_speech_prob": 0.017781421542167664}, {"id": 231, "seek": 114040, "start": 1140.44, "end": 1144.68, "text": " introduce some new fields some new semantic conventions or we want to move something from", "tokens": [50366, 5366, 512, 777, 7909, 512, 777, 47982, 33520, 420, 321, 528, 281, 1286, 746, 490, 50578], "temperature": 0.0, "avg_logprob": -0.14870727185121516, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.010674571618437767}, {"id": 232, "seek": 114040, "start": 1144.68, "end": 1150.64, "text": " ECS to the semantic conventions of open telemetry we first check obviously what we have in these", "tokens": [50578, 19081, 50, 281, 264, 47982, 33520, 295, 1269, 4304, 5537, 627, 321, 700, 1520, 2745, 437, 321, 362, 294, 613, 50876], "temperature": 0.0, "avg_logprob": -0.14870727185121516, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.010674571618437767}, {"id": 233, "seek": 114040, "start": 1150.64, "end": 1156.6000000000001, "text": " two projects and we also check what implementations have so far essentially the open telemetry", "tokens": [50876, 732, 4455, 293, 321, 611, 1520, 437, 4445, 763, 362, 370, 1400, 4476, 264, 1269, 4304, 5537, 627, 51174], "temperature": 0.0, "avg_logprob": -0.14870727185121516, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.010674571618437767}, {"id": 234, "seek": 114040, "start": 1156.6000000000001, "end": 1162.52, "text": " collector or the SDKs because there are cases that the for example the collector already", "tokens": [51174, 23960, 420, 264, 37135, 82, 570, 456, 366, 3331, 300, 264, 337, 1365, 264, 23960, 1217, 51470], "temperature": 0.0, "avg_logprob": -0.14870727185121516, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.010674571618437767}, {"id": 235, "seek": 114040, "start": 1162.52, "end": 1167.48, "text": " uses some some let's say metrics there or some semantic events some resource attributes for", "tokens": [51470, 4960, 512, 512, 718, 311, 584, 16367, 456, 420, 512, 47982, 3931, 512, 7684, 17212, 337, 51718], "temperature": 0.0, "avg_logprob": -0.14870727185121516, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.010674571618437767}, {"id": 236, "seek": 116748, "start": 1167.52, "end": 1173.52, "text": " example but those are not yet part of the semantic conventions of open telemetry so in that case we", "tokens": [50366, 1365, 457, 729, 366, 406, 1939, 644, 295, 264, 47982, 33520, 295, 1269, 4304, 5537, 627, 370, 294, 300, 1389, 321, 50666], "temperature": 0.0, "avg_logprob": -0.1479943240130389, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.002527018543332815}, {"id": 237, "seek": 116748, "start": 1173.52, "end": 1181.48, "text": " also check what there is there so we might find something interesting so we can use it and once", "tokens": [50666, 611, 1520, 437, 456, 307, 456, 370, 321, 1062, 915, 746, 1880, 370, 321, 393, 764, 309, 293, 1564, 51064], "temperature": 0.0, "avg_logprob": -0.1479943240130389, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.002527018543332815}, {"id": 238, "seek": 116748, "start": 1181.48, "end": 1187.04, "text": " we have everything considered we have a final proposal we raise an issue or a pull request", "tokens": [51064, 321, 362, 1203, 4888, 321, 362, 257, 2572, 11494, 321, 5300, 364, 2734, 420, 257, 2235, 5308, 51342], "temperature": 0.0, "avg_logprob": -0.1479943240130389, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.002527018543332815}, {"id": 239, "seek": 116748, "start": 1187.04, "end": 1193.48, "text": " directly and we start the discussion within the community we yeah particularly focusing on", "tokens": [51342, 3838, 293, 321, 722, 264, 5017, 1951, 264, 1768, 321, 1338, 4098, 8416, 322, 51664], "temperature": 0.0, "avg_logprob": -0.1479943240130389, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.002527018543332815}, {"id": 240, "seek": 119348, "start": 1193.84, "end": 1200.92, "text": " measuring the breaking changes because you can imagine that we want to avoid bringing frustration", "tokens": [50382, 13389, 264, 7697, 2962, 570, 291, 393, 3811, 300, 321, 528, 281, 5042, 5062, 20491, 50736], "temperature": 0.0, "avg_logprob": -0.15107881286997854, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.007528047077357769}, {"id": 241, "seek": 119348, "start": 1200.92, "end": 1208.68, "text": " to our users on both sides so yeah that's really unique really important thing to consider and", "tokens": [50736, 281, 527, 5022, 322, 1293, 4881, 370, 1338, 300, 311, 534, 3845, 534, 1021, 551, 281, 1949, 293, 51124], "temperature": 0.0, "avg_logprob": -0.15107881286997854, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.007528047077357769}, {"id": 242, "seek": 119348, "start": 1208.68, "end": 1214.52, "text": " we go through the review process and then once we have a conclusion we merge and then of course", "tokens": [51124, 321, 352, 807, 264, 3131, 1399, 293, 550, 1564, 321, 362, 257, 10063, 321, 22183, 293, 550, 295, 1164, 51416], "temperature": 0.0, "avg_logprob": -0.15107881286997854, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.007528047077357769}, {"id": 243, "seek": 119348, "start": 1214.52, "end": 1220.68, "text": " we need to handle the breaking changes because they are there most of the times and yeah the summary", "tokens": [51416, 321, 643, 281, 4813, 264, 7697, 2962, 570, 436, 366, 456, 881, 295, 264, 1413, 293, 1338, 264, 12691, 51724], "temperature": 0.0, "avg_logprob": -0.15107881286997854, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.007528047077357769}, {"id": 244, "seek": 122068, "start": 1220.72, "end": 1225.92, "text": " for today is that the merger is happening feel free to join us contributors are more than welcome", "tokens": [50366, 337, 965, 307, 300, 264, 48002, 307, 2737, 841, 1737, 281, 3917, 505, 45627, 366, 544, 813, 2928, 50626], "temperature": 0.0, "avg_logprob": -0.1420962046651007, "compression_ratio": 1.8984375, "no_speech_prob": 0.013292198069393635}, {"id": 245, "seek": 122068, "start": 1225.92, "end": 1230.8400000000001, "text": " everything happens in the app stream so if you are interested please join and you will see that", "tokens": [50626, 1203, 2314, 294, 264, 724, 4309, 370, 498, 291, 366, 3102, 1767, 3917, 293, 291, 486, 536, 300, 50872], "temperature": 0.0, "avg_logprob": -0.1420962046651007, "compression_ratio": 1.8984375, "no_speech_prob": 0.013292198069393635}, {"id": 246, "seek": 122068, "start": 1230.8400000000001, "end": 1238.0, "text": " you will find that you will have real impact from day one there and the goal of everyone is to make", "tokens": [50872, 291, 486, 915, 300, 291, 486, 362, 957, 2712, 490, 786, 472, 456, 293, 264, 3387, 295, 1518, 307, 281, 652, 51230], "temperature": 0.0, "avg_logprob": -0.1420962046651007, "compression_ratio": 1.8984375, "no_speech_prob": 0.013292198069393635}, {"id": 247, "seek": 122068, "start": 1238.0, "end": 1242.96, "text": " the semantic convention of open telemetry the one unique straight one unique and straightforward", "tokens": [51230, 264, 47982, 10286, 295, 1269, 4304, 5537, 627, 264, 472, 3845, 2997, 472, 3845, 293, 15325, 51478], "temperature": 0.0, "avg_logprob": -0.1420962046651007, "compression_ratio": 1.8984375, "no_speech_prob": 0.013292198069393635}, {"id": 248, "seek": 122068, "start": 1242.96, "end": 1250.5600000000002, "text": " standard for observability and security that will be there for the future so yeah with that you", "tokens": [51478, 3832, 337, 9951, 2310, 293, 3825, 300, 486, 312, 456, 337, 264, 2027, 370, 1338, 365, 300, 291, 51858], "temperature": 0.0, "avg_logprob": -0.1420962046651007, "compression_ratio": 1.8984375, "no_speech_prob": 0.013292198069393635}, {"id": 249, "seek": 125056, "start": 1250.6, "end": 1257.8799999999999, "text": " can find us on csf slack channels or by using our github handles and some project meetings on", "tokens": [50366, 393, 915, 505, 322, 28277, 69, 29767, 9235, 420, 538, 1228, 527, 290, 355, 836, 18722, 293, 512, 1716, 8410, 322, 50730], "temperature": 0.0, "avg_logprob": -0.2566825866699219, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0024602168705314398}, {"id": 250, "seek": 125056, "start": 1257.8799999999999, "end": 1265.52, "text": " Mondays we have the semantic events working group meeting same our next day Tuesdays we", "tokens": [50730, 7492, 3772, 321, 362, 264, 47982, 3931, 1364, 1594, 3440, 912, 527, 958, 786, 10017, 82, 321, 51112], "temperature": 0.0, "avg_logprob": -0.2566825866699219, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0024602168705314398}, {"id": 251, "seek": 125056, "start": 1265.52, "end": 1271.28, "text": " have the specification sig meeting and on Thursdays we have the system metrics working group 530", "tokens": [51112, 362, 264, 31256, 4556, 3440, 293, 322, 10383, 82, 321, 362, 264, 1185, 16367, 1364, 1594, 1025, 3446, 51400], "temperature": 0.0, "avg_logprob": -0.2566825866699219, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0024602168705314398}, {"id": 252, "seek": 125056, "start": 1271.28, "end": 1278.1599999999999, "text": " 30 central time and yeah without any questions I think we're out of time", "tokens": [51400, 2217, 5777, 565, 293, 1338, 1553, 604, 1651, 286, 519, 321, 434, 484, 295, 565, 51744], "temperature": 0.0, "avg_logprob": -0.2566825866699219, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.0024602168705314398}, {"id": 253, "seek": 128056, "start": 1280.56, "end": 1289.9199999999998, "text": " do we have any questions", "tokens": [50364, 360, 321, 362, 604, 1651, 50832], "temperature": 0.0, "avg_logprob": -0.7376798523796929, "compression_ratio": 0.75, "no_speech_prob": 0.039886873215436935}, {"id": 254, "seek": 128992, "start": 1289.92, "end": 1313.3600000000001, "text": " hi thank you for the talk this this was really interesting and clarified some things for me", "tokens": [50364, 4879, 1309, 291, 337, 264, 751, 341, 341, 390, 534, 1880, 293, 47605, 512, 721, 337, 385, 51536], "temperature": 0.0, "avg_logprob": -0.18740162395295643, "compression_ratio": 1.1666666666666667, "no_speech_prob": 0.04058361425995827}, {"id": 255, "seek": 131336, "start": 1313.52, "end": 1324.52, "text": " I have one question about what's how what are the benefits of these semantic conventions in terms", "tokens": [50372, 286, 362, 472, 1168, 466, 437, 311, 577, 437, 366, 264, 5311, 295, 613, 47982, 33520, 294, 2115, 50922], "temperature": 0.0, "avg_logprob": -0.23551978762187656, "compression_ratio": 1.6863905325443787, "no_speech_prob": 0.07038966566324234}, {"id": 256, "seek": 131336, "start": 1324.52, "end": 1331.36, "text": " of like front-end tooling that that we are using because I know that you know there's this idea in", "tokens": [50922, 295, 411, 1868, 12, 521, 46593, 300, 300, 321, 366, 1228, 570, 286, 458, 300, 291, 458, 456, 311, 341, 1558, 294, 51264], "temperature": 0.0, "avg_logprob": -0.23551978762187656, "compression_ratio": 1.6863905325443787, "no_speech_prob": 0.07038966566324234}, {"id": 257, "seek": 131336, "start": 1331.36, "end": 1335.84, "text": " open telemetry project that you have semantic conventions and you have common attributes", "tokens": [51264, 1269, 4304, 5537, 627, 1716, 300, 291, 362, 47982, 33520, 293, 291, 362, 2689, 17212, 51488], "temperature": 0.0, "avg_logprob": -0.23551978762187656, "compression_ratio": 1.6863905325443787, "no_speech_prob": 0.07038966566324234}, {"id": 258, "seek": 133584, "start": 1335.84, "end": 1344.76, "text": " for different signals and then we collect all this data in all these different signals in some", "tokens": [50364, 337, 819, 12354, 293, 550, 321, 2500, 439, 341, 1412, 294, 439, 613, 819, 12354, 294, 512, 50810], "temperature": 0.0, "avg_logprob": -0.1903686069306873, "compression_ratio": 1.7309941520467835, "no_speech_prob": 0.06199905648827553}, {"id": 259, "seek": 133584, "start": 1344.76, "end": 1352.9599999999998, "text": " observability tools and I imagine in like front-end we could automatically correlate different signals", "tokens": [50810, 9951, 2310, 3873, 293, 286, 3811, 294, 411, 1868, 12, 521, 321, 727, 6772, 48742, 819, 12354, 51220], "temperature": 0.0, "avg_logprob": -0.1903686069306873, "compression_ratio": 1.7309941520467835, "no_speech_prob": 0.06199905648827553}, {"id": 260, "seek": 133584, "start": 1352.9599999999998, "end": 1363.08, "text": " if we have this like common attributes I'm not up to date with the current state of this this area", "tokens": [51220, 498, 321, 362, 341, 411, 2689, 17212, 286, 478, 406, 493, 281, 4002, 365, 264, 2190, 1785, 295, 341, 341, 1859, 51726], "temperature": 0.0, "avg_logprob": -0.1903686069306873, "compression_ratio": 1.7309941520467835, "no_speech_prob": 0.06199905648827553}, {"id": 261, "seek": 136308, "start": 1363.12, "end": 1369.9199999999998, "text": " so yeah this is my question what are the main benefits of following this semantic conventions", "tokens": [50366, 370, 1338, 341, 307, 452, 1168, 437, 366, 264, 2135, 5311, 295, 3480, 341, 47982, 33520, 50706], "temperature": 0.0, "avg_logprob": -0.15160676657435407, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.01946992427110672}, {"id": 262, "seek": 136308, "start": 1369.9199999999998, "end": 1377.84, "text": " yeah I would say there are two actually one is I mean open telemetry is an open source standard", "tokens": [50706, 1338, 286, 576, 584, 456, 366, 732, 767, 472, 307, 286, 914, 1269, 4304, 5537, 627, 307, 364, 1269, 4009, 3832, 51102], "temperature": 0.0, "avg_logprob": -0.15160676657435407, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.01946992427110672}, {"id": 263, "seek": 136308, "start": 1377.84, "end": 1382.52, "text": " right and there are many vendors adopting this so we need common semantics of what the data", "tokens": [51102, 558, 293, 456, 366, 867, 22056, 32328, 341, 370, 321, 643, 2689, 4361, 45298, 295, 437, 264, 1412, 51336], "temperature": 0.0, "avg_logprob": -0.15160676657435407, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.01946992427110672}, {"id": 264, "seek": 136308, "start": 1382.52, "end": 1388.1599999999999, "text": " represents to build features higher level features on top this is the first thing and the other one", "tokens": [51336, 8855, 281, 1322, 4122, 2946, 1496, 4122, 322, 1192, 341, 307, 264, 700, 551, 293, 264, 661, 472, 51618], "temperature": 0.0, "avg_logprob": -0.15160676657435407, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.01946992427110672}, {"id": 265, "seek": 138816, "start": 1388.2, "end": 1395.72, "text": " is correlation as you already mentioned cross like different signals to also have correlation", "tokens": [50366, 307, 20009, 382, 291, 1217, 2835, 3278, 411, 819, 12354, 281, 611, 362, 20009, 50742], "temperature": 0.0, "avg_logprob": -0.1412359786360231, "compression_ratio": 1.8284313725490196, "no_speech_prob": 0.04696275293827057}, {"id": 266, "seek": 138816, "start": 1395.72, "end": 1400.76, "text": " cross or through the resource attributes for example so you can drill down basically on", "tokens": [50742, 3278, 420, 807, 264, 7684, 17212, 337, 1365, 370, 291, 393, 11392, 760, 1936, 322, 50994], "temperature": 0.0, "avg_logprob": -0.1412359786360231, "compression_ratio": 1.8284313725490196, "no_speech_prob": 0.04696275293827057}, {"id": 267, "seek": 138816, "start": 1400.76, "end": 1409.68, "text": " different signals into the same resource and yeah I would say these two things and also cross", "tokens": [50994, 819, 12354, 666, 264, 912, 7684, 293, 1338, 286, 576, 584, 613, 732, 721, 293, 611, 3278, 51440], "temperature": 0.0, "avg_logprob": -0.1412359786360231, "compression_ratio": 1.8284313725490196, "no_speech_prob": 0.04696275293827057}, {"id": 268, "seek": 138816, "start": 1409.68, "end": 1415.52, "text": " signal correlation not only through resources but things like trace ID to have them you know both", "tokens": [51440, 6358, 20009, 406, 787, 807, 3593, 457, 721, 411, 13508, 7348, 281, 362, 552, 291, 458, 1293, 51732], "temperature": 0.0, "avg_logprob": -0.1412359786360231, "compression_ratio": 1.8284313725490196, "no_speech_prob": 0.04696275293827057}, {"id": 269, "seek": 141552, "start": 1415.6399999999999, "end": 1422.4, "text": " on locks and traces and later maybe in profiling data this kind of things okay thank you so are", "tokens": [50370, 322, 20703, 293, 26076, 293, 1780, 1310, 294, 1740, 4883, 1412, 341, 733, 295, 721, 1392, 1309, 291, 370, 366, 50708], "temperature": 0.0, "avg_logprob": -0.1867219700532801, "compression_ratio": 1.7522935779816513, "no_speech_prob": 0.0071678063832223415}, {"id": 270, "seek": 141552, "start": 1422.4, "end": 1431.36, "text": " you doing something like that in elastic like in front-end at the moment is there any work going", "tokens": [50708, 291, 884, 746, 411, 300, 294, 17115, 411, 294, 1868, 12, 521, 412, 264, 1623, 307, 456, 604, 589, 516, 51156], "temperature": 0.0, "avg_logprob": -0.1867219700532801, "compression_ratio": 1.7522935779816513, "no_speech_prob": 0.0071678063832223415}, {"id": 271, "seek": 141552, "start": 1431.36, "end": 1436.8, "text": " on in this area like correlation of different signals yeah of course like I think that's that's", "tokens": [51156, 322, 294, 341, 1859, 411, 20009, 295, 819, 12354, 1338, 295, 1164, 411, 286, 519, 300, 311, 300, 311, 51428], "temperature": 0.0, "avg_logprob": -0.1867219700532801, "compression_ratio": 1.7522935779816513, "no_speech_prob": 0.0071678063832223415}, {"id": 272, "seek": 141552, "start": 1436.8, "end": 1440.48, "text": " the goal for for every observability vendor to bring all all these different signals together", "tokens": [51428, 264, 3387, 337, 337, 633, 9951, 2310, 24321, 281, 1565, 439, 439, 613, 819, 12354, 1214, 51612], "temperature": 0.0, "avg_logprob": -0.1867219700532801, "compression_ratio": 1.7522935779816513, "no_speech_prob": 0.0071678063832223415}, {"id": 273, "seek": 144048, "start": 1440.48, "end": 1455.52, "text": " yeah okay great thank you very much any other questions going once", "tokens": [50364, 1338, 1392, 869, 1309, 291, 588, 709, 604, 661, 1651, 516, 1564, 51116], "temperature": 0.0, "avg_logprob": -0.5718594426694124, "compression_ratio": 1.1794871794871795, "no_speech_prob": 0.00715922424569726}, {"id": 274, "seek": 144048, "start": 1461.6, "end": 1463.4, "text": " okay cool then bingo plus", "tokens": [51420, 1392, 1627, 550, 272, 18459, 1804, 51510], "temperature": 0.0, "avg_logprob": -0.5718594426694124, "compression_ratio": 1.1794871794871795, "no_speech_prob": 0.00715922424569726}, {"id": 275, "seek": 147048, "start": 1470.48, "end": 1471.48, "text": " okay", "tokens": [50400, 1392, 50414], "temperature": 1.0, "avg_logprob": -2.349188804626465, "compression_ratio": 0.3333333333333333, "no_speech_prob": 0.5822422504425049}], "language": "en"}