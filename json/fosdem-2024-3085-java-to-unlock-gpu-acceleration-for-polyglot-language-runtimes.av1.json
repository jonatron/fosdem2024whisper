{"text": " Okay, can you hear me? Excellent. Thank you. So it's a pleasure to be here. I'm on goal this amazing speakers today. So I'm Thanos. I'm a search fellow at the University of Manchester. I'm part of the Tornado VM team. And today I will talk about polyglot language implementations, which enable programming languages like Ruby, Python, and to run on top of the JVM, along with Java, of course. And I will try to make a step forward and show you how they can harness GPU acceleration from the JVM. I'll start a little bit with the polyglot programming, which has been here for many years, but in a sense it has been reignited by the advent of the Truffle framework from Graal VM. And in a sense it enables multiple programming languages to run on top of the JVM and interoperate. So that means that one Java class file can invoke a Python function and the Python program can invoke a Java method. Well, this is very interesting. It comes with many advantages. But what about GPU programming? Well, GPUs from Java. Well, this is not a thing yet. That's why we have been motivated at the University of Manchester and we have done all this research in the past eight years and we have created Tornado VM. Here is a link to the resources of Tornado VM with all the presentations that explain the programming model. Because my goal today is not to go very deep, to dive into the Tornado VM very deep, but to present the interoperability with the other programming languages and how they can use GPU acceleration from the JVM. So Tornado VM is an open source plug-in to existing JDK distributions. It is compatible with JDK 21, as you will see later. And it has some very cool features. So it has a platform agnostic API. So developers, they don't need to know GPU programming, FPGA programming. It comes with an optimizing compiler. So we extend GRAL with new phases that they can take Java methods and compile them to GPU code. We have a feature of dynamic reconfiguration at runtime, which means that the method execution can be migrated from a GPU back to the JVM and then go to the FPGA if it is appropriate. And with the latest release 1.0, we have enabled support for off-heap data types. So data can be allocated off-heap with a foreign function and memory API. And this is the API that Mauricio described earlier today. So feel free to follow Tornado VM in Twitter to engage with the website and of course to fork and try our examples which are open sourcing GitHub. So I spoke a little bit about off-heap data types, so I'll give an introduction, an example, because I'm not going to dive very into the API. So here we see two snapshots of code. On the left side, we see a main method that contains the allocation of float array by using primitive types, but is allocated as an object, in a sense, on-heap. So to migrate from such an allocation to the new allocation API that's exposed by the Tornado API, we have created the float array object that inside it can allocate memory by using the memory segment of the foreign function API. And it will allocate this memory off-heap. So this memory segment could be used directly from the GPU without the need to worry about GC collections and this stuff. And the cool part is that even if you don't use GPU programming, even if you don't want to execute on GPUs, you can still use this API to allocate memory off-heap. And here is a link that explains more. I hope it's visual from your side. If not, you will find my slides online in the Fosdome webpage. So the motivation for today is that Graal VM enables interoperability between programming languages like Ruby, JavaScript, and other programming languages. And Tornado VM enables hardware acceleration for Java. So what if we can combine them together and harness GPU acceleration from all these programming languages that are running on top of Trafl? Let's have a dive into the tech flow. So in this slide, I present a software stack from Graal VM for Trafl. So on the top, we see the Trafl framework and many implementations of polyglot runtimes like Graalpy, Graal.js, Trafl Ruby. And others because Trafl enables also programming language implementers to create their own programming languages by using the Java API. So I have grouped Python, Ruby, JavaScript, and Node.js in this side of the slide. And then beneath them, there is the Graal VM Zit compiler, so an optimizing compiler from Graal. So Java is also running on top of the JVM, of course. And all these languages, they start in the interpreted mode, and once they reach a hot state, then the optimizing compiler kicks in. And the cool part with such a polyglot implementation that enables polyglot programming is that there is, for the compiler enthusiasts, there is one Graal IR. So the nodes, at runtime, they are rewritten. That means that it can adjust. So if we kick in a Python function, then the node can be rewritten, and the Graal compiler will take a shape and will emit at the assembly code that will run on the CPU. So this solution offers the interoperability and offers the execution among different CPU instruction set architectures. But what if we have this heterogeneous hardware, like GPUs, FPGAs, which are available in some systems and servers? Well, then we'll have Tornado VM that enables Java methods to be compiled for GPUs, FPGAs, etc. Tornado VM has its own JIT compiler, which is an extension, a superset, I would say, of Graal, the Graal compiler, that it is enhanced with new phases in the compiler to automatically specialize the code from a method for GPU acceleration and FPG acceleration. So at the backbone of the compiler, we have three backends at the moment. We have OpenCL backend, CUDA, and SPV. And such a solution would enable many things. So if you want to learn more about the APIs, you can scan this QR code. And the code that is implemented with Tornado VM, it can harness besides the off-hip data types, it can also harness the execution with a Tornado VM profiler. If you want to learn more about the characteristics of your application, you can see how many data will be copying in the GPU memory, how expensive is the IEO maybe, because this could be very critical for the performance of the system. And you can customize even how many, how the data transfers will be performed. Because, for example, if you have a method that consumes redoneally data, then maybe you need to copy the data once, instead of copying the data every time you execute the kernel. Okay, so let's jump to the deployment. As I said, Tornado VM is compatible with different JDK distributions, so it's not a JVM, it is a plugin for JDK distributions. So it can be seen as a library, in a sense, because it offers an API in Java. And it is compatible with all these distributions. And on the other side, we have the compiler backends that makes it compatible with different heterogeneous hardware accelerators. We can emit vectorized code for multi-core CPU execution through OpenCL. We can run with different GPUs and FPGAs. In this particular talk, I will focus on GraVM, because we want to leverage polyglot, and NVIDIA GPUs, because I have created Docker images that they run on the NVIDIA GPUs. Now, regarding the GraVM deployment, I will focus in this slide in GraL Python, which is one implementation of polyglot runtime. This is shipped in two different standalone versions, releases. So we have the native standalone, which comes with the native image. And then we have the JVM standalone that enables the execution of Python programs on top of the JVM, and it has also the JVM compiler. The version that we tested is the 23.1, because tornado VM is compatible with this version of GraL. And here you can see that we have downloaded the community, and that's JVM. So we have the JVM standalone version downloaded. Well, we need the JVM standalone, because we want to run with tornado VM, and tornado VM will extend the GraL VM compiler. So this is the reason. The problem is that we tried it, and the JVM distribution is shipped with the JVM standalone, with a compiler built that it is built with libgral. So this comes with not many compiler modules, and that breaks the consistency for tornado VM. When we tried it. And this is because they wanted the image, the footprint to be lower, which makes sense, but it broke the compatibility with tornado VM. The good part on this story is that GraL is very active. The GraL community is very active in Slack workspace, so we managed to figure out what was the problem. On the bad side is that the solution was to build a GraL Pi and GraL VM from source, which was quite painful. And in order to avoid this pain for anyone who wants to try this work, we decided to build a Docker image that has inside GraL Pi, tornado VM, and we have also added the NVIDIA driver. So if you have a Linux machine or any machine that has an NVIDIA GPU, and you have also the NVIDIA container toolkit in this machine, then you will be able to run this image. The Docker file, the image is open source in GitHub. And on the other side, you can see the QR code that has the acceleration library. So the code that we have implemented in the examples module of tornado VM for the computation part that we will upload on the GPU, like K-means, matrix multiplication, and those are the examples. But there are also other compute examples that we have in the GitHub. And you can also pull the Docker image from Docker Hub. So we will jump into the examples. So as you see here, we have the Python and Java with tornado VM. So we have the Python program that imports Java, and then it loads the class from the compute examples class of the tornado VM repository. And then we have in this Java class that we have loaded, we have two methods that can be accessed by the Python program. The first one is the set inputs that set the actual data points and the number of packets that will be used for K-means. And the second one is the run with GPU. So this will invoke the actual GPU compilation for GPUs and the GPU execution. And on the other side, we have the Java tornado VM, where we use Java and the tornado VM API to create these parallel implementations of K-means. In this slide, you see, well, the steps, how to clone the repository that contains this Python program. And we see also the Python program, the K-means.py. So we see here beneath that we have the invocation of the actual method functions, Java methods, sorry. And here is the link for the Java implementation of K-means. And now if we jump into the Java part, which contains the computation that will be offloaded on the GPU. No, before we jump to the computation, we have the set inputs and I wanted to make a connection to reflect on the off-heap data types. So with these two, with a new vector float, this is an API type that is exposed by tornado VM and can allocate data vector types off-heap. And then we'll have the create matrix of clusters that does perform some initialization of the objects and also allocate some other data, like the clusters, which are going to be allocated off-heap as well. And now we are ready to move into the actual computation part. So on the left side, you see the run with Java implementation of this method. And on the right side, you see the accelerated one with the tornado VM API. So as we see here, the actual computation has been in this method, has been performed by this method. So they assign clusters. And the corresponding one on the right side, that is the tornado VM implementation, is this one. So in this one, I would like to focus on two parts. So you can see the task graph implementation. Task graph is an object exposed by the tornado VM API. In a sense, task graph enables you to define what code will go to the GPU. So what's going to be the actual computation and what data should be used on the GPU. So the input data and the output data. So in a sense, the task graph enables programmers to define what is going to go to the GPU for execution. And the second API, once we have done this, as you can see here, we can define also the data transfer mode, how often we want data input, input data or output data to be copied back and forth from the GPU. And once we have defined that, we can move to the second part, which is the execution plan. So the execution plan is another object that enables programmers to define how the execution will take place. So it could be, for example, with the profiler enabled, without the profiler enabled, with a custom grid size, which is defined by the programmer. And once we have defined how the execution will be done, will be performed, we are able to execute the actual task graph. So with execution.execute, it is this part that enables the actual execution of the code and the GIT compilation. So the second time that we will execute the assigned clusters, well, this is going to be the second time that we invoke the actual execute of the execution plan. And the second time that we will invoke the execution plan, the execution of the execution plan, this is going to be the time that the code will not be GIT because it is already GIT. So the code, the OpenCL code or the CUDA code will be all retrieved from the code cache of Tornado VM. So now we can move to the actual example to run. I have recorded a video that enables the execution of K-Means and MathExfoom.liblication because on my MacBook, I don't have an NVIDIA GPU. So we will fork the actual repository with examples. And now that we have forked, we will go inside, we check out the FOSDEM branch. And this is the Python code that we saw earlier. So it has these three. First, we load the class, and then we are able to invoke the Java code from Python. And here we will run, first, the Java implementation and then the GPU accelerated implementation. We can also pull the Docker image that we have created. And here in the repository, we have a launcher script that enables to run. So at first, we will try the Tornado devices to query how many NVIDIA GPUs exist in the system. And here it is the 2000 GPU that exists in my machine at home. And once we have done this, we will run with Truffle, the Python program. So Tornado Truffle, the Truffle flag and Python, will be able to run the actual Python program. And we will see here that at first, it will bring Hello World from Python. And then we run the Java implementation, which is a sequential, that I'm with Java. And then they run with GPU method. And as we see here, they take the first one, one second, and the second one, 140 milliseconds. So here we will try the same example, but with the thread info, which will enable the printing of the actual threads that have been used on the GPU. So as we see here, we have the number of data points that we passed with the set input. It has been the number of the global thread size that is uploaded on the GPU. And now we move to the second example, which is the matrix multiplication with Tornado VM. So in this example, we run five times the matrix multiplication. So we see here the execution time of matrix multiplication on the GPU. So the first time it was half second, and then it has moved to three milliseconds. This is because the first execution, it involves also the GIT compilation, which is expensive. Then the second time, third time, the execution time has been saturated because it is the actual launching of the code. Okay, I have showed you example of Python with Gralpy, but this is not the only one. We have also the key images for the other programming languages for JavaScript, Ruby, and you can find more details in those links where we have a blog post. And we explain also the polyglot programming from Tornado VM. So now we will try to find the other examples so now I will jump to the summary of my talk. So as key takeaways, I would like to emphasize that GralVM and Traffl enable Java interoperability with other programming languages that run on top of the JVM. Tornado VM afflows Java methods on GPUs, FPGAs, and multicore CPUs, so you can create parallel implementations. And that Tornado VM offers a Java API, so programmers, they don't need to know GPU programming. It is a Java API, a Java way to express parallelism. And we have also new off-hip data types. So finally, yes, it is possible to create high-performing implementations of code for data science libraries in Java, and reuse them by other programming languages. This is a slide that summarizes everyone who has contributed as a research staff for students at the University of Manchester, and these images are from our campus. And this is a surprise that it was taken and it was not raining. So I would like to invite you to join our community, follow us in GitHub, join us in the Tornado VM Slack space if you have questions, or if you want to interact with a team for discussions, and also to try our examples in GitHub. And in my last slide, I would like to acknowledge all these research funds that have supported their work at Tornado VM, like Elegant and Crip, Tango, Iro and InCode. So with that, I conclude my talk, and I think we have time for one or two questions. Okay, I've got the mic here, but first, I lived in Manchester for five years, and it doesn't always rain. Just mostly. Just mostly. Thanks for a great talk. Like one of the first pictures you had showed Tornado VM in parallel to the GrowlJIT using the JVMCI. So do you interact directly with JVMCI for generating code? Correct, yes. So the JVMCI enables other JIT compilers to be hooked in the JVM, and that's how we run, because we extend. So do you work with the standard JVMCI in upstream or open JDK, or you need the lab JDK with the latest JVMCI changes? Because the GrowlJIT compiler, as far as I know, requires the lab JDK with latest changes. We work with the standard JVMCI, yes. Thank you. Thank you. So when you write the kernel code in Java, then is it usually high-level code that you write, or do you try to write optimized code in Java? Like usually when you write, let's say, Qtacode, then you try to write a very specialized, use warp intrinsics and that kind of stuff. Is that something that is like in scope for turn out of VM, or not so much? No, that's a great question. Well, to answer this question, we do both. So we have two APIs. One is created for Java programmers. We will have, let's say, a computation that has four loops. So this is something that you can paralyze if you don't have data dependency. So we expose an annotation in this case, similar to OpenMP. So you can do add parallel in the four loop in order to give a hint to the compiler that this can run in parallel and will create parallel implementations in OpenCL or CUDA. And the second part is that if you are familiar with OpenCL and CUDA and you want to have access to low-level intrinsics, like, for example, use barriers or local memory, allocate local memory, then we'll have a second API, which is called kernel API. And with that, you can pretty much access every interesting that exists in OpenCL and CUDA programming from Java. So personally, I have used the second API to port existing OpenCL kernels in Java with Tonedo.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.0, "text": " Okay, can you hear me?", "tokens": [50364, 1033, 11, 393, 291, 1568, 385, 30, 50764], "temperature": 0.0, "avg_logprob": -0.3513695109974254, "compression_ratio": 1.4, "no_speech_prob": 0.1627117395401001}, {"id": 1, "seek": 0, "start": 8.0, "end": 10.64, "text": " Excellent. Thank you.", "tokens": [50764, 16723, 13, 1044, 291, 13, 50896], "temperature": 0.0, "avg_logprob": -0.3513695109974254, "compression_ratio": 1.4, "no_speech_prob": 0.1627117395401001}, {"id": 2, "seek": 0, "start": 10.64, "end": 12.48, "text": " So it's a pleasure to be here.", "tokens": [50896, 407, 309, 311, 257, 6834, 281, 312, 510, 13, 50988], "temperature": 0.0, "avg_logprob": -0.3513695109974254, "compression_ratio": 1.4, "no_speech_prob": 0.1627117395401001}, {"id": 3, "seek": 0, "start": 12.48, "end": 15.200000000000001, "text": " I'm on goal this amazing speakers today.", "tokens": [50988, 286, 478, 322, 3387, 341, 2243, 9518, 965, 13, 51124], "temperature": 0.0, "avg_logprob": -0.3513695109974254, "compression_ratio": 1.4, "no_speech_prob": 0.1627117395401001}, {"id": 4, "seek": 0, "start": 15.200000000000001, "end": 16.48, "text": " So I'm Thanos.", "tokens": [51124, 407, 286, 478, 35993, 13, 51188], "temperature": 0.0, "avg_logprob": -0.3513695109974254, "compression_ratio": 1.4, "no_speech_prob": 0.1627117395401001}, {"id": 5, "seek": 0, "start": 16.48, "end": 18.8, "text": " I'm a search fellow at the University of Manchester.", "tokens": [51188, 286, 478, 257, 3164, 7177, 412, 264, 3535, 295, 27180, 13, 51304], "temperature": 0.0, "avg_logprob": -0.3513695109974254, "compression_ratio": 1.4, "no_speech_prob": 0.1627117395401001}, {"id": 6, "seek": 0, "start": 18.8, "end": 20.96, "text": " I'm part of the Tornado VM team.", "tokens": [51304, 286, 478, 644, 295, 264, 314, 1865, 1573, 18038, 1469, 13, 51412], "temperature": 0.0, "avg_logprob": -0.3513695109974254, "compression_ratio": 1.4, "no_speech_prob": 0.1627117395401001}, {"id": 7, "seek": 0, "start": 20.96, "end": 26.240000000000002, "text": " And today I will talk about polyglot language implementations,", "tokens": [51412, 400, 965, 286, 486, 751, 466, 6754, 7191, 310, 2856, 4445, 763, 11, 51676], "temperature": 0.0, "avg_logprob": -0.3513695109974254, "compression_ratio": 1.4, "no_speech_prob": 0.1627117395401001}, {"id": 8, "seek": 2624, "start": 26.88, "end": 31.2, "text": " which enable programming languages like Ruby, Python,", "tokens": [50396, 597, 9528, 9410, 8650, 411, 19907, 11, 15329, 11, 50612], "temperature": 0.0, "avg_logprob": -0.135420289370093, "compression_ratio": 1.5147679324894514, "no_speech_prob": 0.026644738391041756}, {"id": 9, "seek": 2624, "start": 31.2, "end": 34.96, "text": " and to run on top of the JVM, along with Java, of course.", "tokens": [50612, 293, 281, 1190, 322, 1192, 295, 264, 508, 53, 44, 11, 2051, 365, 10745, 11, 295, 1164, 13, 50800], "temperature": 0.0, "avg_logprob": -0.135420289370093, "compression_ratio": 1.5147679324894514, "no_speech_prob": 0.026644738391041756}, {"id": 10, "seek": 2624, "start": 34.96, "end": 37.76, "text": " And I will try to make a step forward", "tokens": [50800, 400, 286, 486, 853, 281, 652, 257, 1823, 2128, 50940], "temperature": 0.0, "avg_logprob": -0.135420289370093, "compression_ratio": 1.5147679324894514, "no_speech_prob": 0.026644738391041756}, {"id": 11, "seek": 2624, "start": 37.76, "end": 42.239999999999995, "text": " and show you how they can harness GPU acceleration from the JVM.", "tokens": [50940, 293, 855, 291, 577, 436, 393, 19700, 18407, 17162, 490, 264, 508, 53, 44, 13, 51164], "temperature": 0.0, "avg_logprob": -0.135420289370093, "compression_ratio": 1.5147679324894514, "no_speech_prob": 0.026644738391041756}, {"id": 12, "seek": 2624, "start": 44.56, "end": 47.04, "text": " I'll start a little bit with the polyglot programming,", "tokens": [51280, 286, 603, 722, 257, 707, 857, 365, 264, 6754, 7191, 310, 9410, 11, 51404], "temperature": 0.0, "avg_logprob": -0.135420289370093, "compression_ratio": 1.5147679324894514, "no_speech_prob": 0.026644738391041756}, {"id": 13, "seek": 2624, "start": 47.04, "end": 49.44, "text": " which has been here for many years,", "tokens": [51404, 597, 575, 668, 510, 337, 867, 924, 11, 51524], "temperature": 0.0, "avg_logprob": -0.135420289370093, "compression_ratio": 1.5147679324894514, "no_speech_prob": 0.026644738391041756}, {"id": 14, "seek": 2624, "start": 49.44, "end": 53.36, "text": " but in a sense it has been reignited by the advent of", "tokens": [51524, 457, 294, 257, 2020, 309, 575, 668, 20350, 1226, 538, 264, 7045, 295, 51720], "temperature": 0.0, "avg_logprob": -0.135420289370093, "compression_ratio": 1.5147679324894514, "no_speech_prob": 0.026644738391041756}, {"id": 15, "seek": 5336, "start": 54.32, "end": 56.8, "text": " the Truffle framework from Graal VM.", "tokens": [50412, 264, 21388, 602, 306, 8388, 490, 8985, 304, 18038, 13, 50536], "temperature": 0.0, "avg_logprob": -0.14708616052355086, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.007284770254045725}, {"id": 16, "seek": 5336, "start": 57.519999999999996, "end": 60.72, "text": " And in a sense it enables multiple programming languages", "tokens": [50572, 400, 294, 257, 2020, 309, 17077, 3866, 9410, 8650, 50732], "temperature": 0.0, "avg_logprob": -0.14708616052355086, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.007284770254045725}, {"id": 17, "seek": 5336, "start": 60.72, "end": 63.68, "text": " to run on top of the JVM and interoperate.", "tokens": [50732, 281, 1190, 322, 1192, 295, 264, 508, 53, 44, 293, 728, 7192, 473, 13, 50880], "temperature": 0.0, "avg_logprob": -0.14708616052355086, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.007284770254045725}, {"id": 18, "seek": 5336, "start": 63.68, "end": 68.8, "text": " So that means that one Java class file can invoke a Python function", "tokens": [50880, 407, 300, 1355, 300, 472, 10745, 1508, 3991, 393, 41117, 257, 15329, 2445, 51136], "temperature": 0.0, "avg_logprob": -0.14708616052355086, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.007284770254045725}, {"id": 19, "seek": 5336, "start": 68.8, "end": 71.92, "text": " and the Python program can invoke a Java method.", "tokens": [51136, 293, 264, 15329, 1461, 393, 41117, 257, 10745, 3170, 13, 51292], "temperature": 0.0, "avg_logprob": -0.14708616052355086, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.007284770254045725}, {"id": 20, "seek": 5336, "start": 73.52, "end": 74.72, "text": " Well, this is very interesting.", "tokens": [51372, 1042, 11, 341, 307, 588, 1880, 13, 51432], "temperature": 0.0, "avg_logprob": -0.14708616052355086, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.007284770254045725}, {"id": 21, "seek": 5336, "start": 74.72, "end": 76.48, "text": " It comes with many advantages.", "tokens": [51432, 467, 1487, 365, 867, 14906, 13, 51520], "temperature": 0.0, "avg_logprob": -0.14708616052355086, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.007284770254045725}, {"id": 22, "seek": 5336, "start": 77.68, "end": 79.6, "text": " But what about GPU programming?", "tokens": [51580, 583, 437, 466, 18407, 9410, 30, 51676], "temperature": 0.0, "avg_logprob": -0.14708616052355086, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.007284770254045725}, {"id": 23, "seek": 5336, "start": 79.6, "end": 81.28, "text": " Well, GPUs from Java.", "tokens": [51676, 1042, 11, 18407, 82, 490, 10745, 13, 51760], "temperature": 0.0, "avg_logprob": -0.14708616052355086, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.007284770254045725}, {"id": 24, "seek": 5336, "start": 81.28, "end": 82.8, "text": " Well, this is not a thing yet.", "tokens": [51760, 1042, 11, 341, 307, 406, 257, 551, 1939, 13, 51836], "temperature": 0.0, "avg_logprob": -0.14708616052355086, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.007284770254045725}, {"id": 25, "seek": 8336, "start": 84.08, "end": 86.8, "text": " That's why we have been motivated at the University of Manchester", "tokens": [50400, 663, 311, 983, 321, 362, 668, 14515, 412, 264, 3535, 295, 27180, 50536], "temperature": 0.0, "avg_logprob": -0.09405571852273088, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00362385343760252}, {"id": 26, "seek": 8336, "start": 86.8, "end": 90.56, "text": " and we have done all this research in the past eight years", "tokens": [50536, 293, 321, 362, 1096, 439, 341, 2132, 294, 264, 1791, 3180, 924, 50724], "temperature": 0.0, "avg_logprob": -0.09405571852273088, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00362385343760252}, {"id": 27, "seek": 8336, "start": 90.56, "end": 92.96, "text": " and we have created Tornado VM.", "tokens": [50724, 293, 321, 362, 2942, 314, 1865, 1573, 18038, 13, 50844], "temperature": 0.0, "avg_logprob": -0.09405571852273088, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00362385343760252}, {"id": 28, "seek": 8336, "start": 93.76, "end": 96.24, "text": " Here is a link to the resources of Tornado VM", "tokens": [50884, 1692, 307, 257, 2113, 281, 264, 3593, 295, 314, 1865, 1573, 18038, 51008], "temperature": 0.0, "avg_logprob": -0.09405571852273088, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00362385343760252}, {"id": 29, "seek": 8336, "start": 96.24, "end": 99.92, "text": " with all the presentations that explain the programming model.", "tokens": [51008, 365, 439, 264, 18964, 300, 2903, 264, 9410, 2316, 13, 51192], "temperature": 0.0, "avg_logprob": -0.09405571852273088, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00362385343760252}, {"id": 30, "seek": 8336, "start": 99.92, "end": 102.4, "text": " Because my goal today is not to go very deep,", "tokens": [51192, 1436, 452, 3387, 965, 307, 406, 281, 352, 588, 2452, 11, 51316], "temperature": 0.0, "avg_logprob": -0.09405571852273088, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00362385343760252}, {"id": 31, "seek": 8336, "start": 102.4, "end": 104.96000000000001, "text": " to dive into the Tornado VM very deep,", "tokens": [51316, 281, 9192, 666, 264, 314, 1865, 1573, 18038, 588, 2452, 11, 51444], "temperature": 0.0, "avg_logprob": -0.09405571852273088, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00362385343760252}, {"id": 32, "seek": 8336, "start": 104.96000000000001, "end": 106.96000000000001, "text": " but to present the interoperability", "tokens": [51444, 457, 281, 1974, 264, 728, 7192, 2310, 51544], "temperature": 0.0, "avg_logprob": -0.09405571852273088, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00362385343760252}, {"id": 33, "seek": 8336, "start": 106.96000000000001, "end": 108.4, "text": " with the other programming languages", "tokens": [51544, 365, 264, 661, 9410, 8650, 51616], "temperature": 0.0, "avg_logprob": -0.09405571852273088, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00362385343760252}, {"id": 34, "seek": 8336, "start": 108.4, "end": 112.56, "text": " and how they can use GPU acceleration from the JVM.", "tokens": [51616, 293, 577, 436, 393, 764, 18407, 17162, 490, 264, 508, 53, 44, 13, 51824], "temperature": 0.0, "avg_logprob": -0.09405571852273088, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00362385343760252}, {"id": 35, "seek": 11336, "start": 114.16, "end": 116.96, "text": " So Tornado VM is an open source", "tokens": [50404, 407, 314, 1865, 1573, 18038, 307, 364, 1269, 4009, 50544], "temperature": 0.0, "avg_logprob": -0.11913118268003559, "compression_ratio": 1.4345991561181435, "no_speech_prob": 0.00114033545833081}, {"id": 36, "seek": 11336, "start": 116.96, "end": 120.56, "text": " plug-in to existing JDK distributions.", "tokens": [50544, 5452, 12, 259, 281, 6741, 37082, 42, 37870, 13, 50724], "temperature": 0.0, "avg_logprob": -0.11913118268003559, "compression_ratio": 1.4345991561181435, "no_speech_prob": 0.00114033545833081}, {"id": 37, "seek": 11336, "start": 120.56, "end": 123.92, "text": " It is compatible with JDK 21, as you will see later.", "tokens": [50724, 467, 307, 18218, 365, 37082, 42, 5080, 11, 382, 291, 486, 536, 1780, 13, 50892], "temperature": 0.0, "avg_logprob": -0.11913118268003559, "compression_ratio": 1.4345991561181435, "no_speech_prob": 0.00114033545833081}, {"id": 38, "seek": 11336, "start": 124.56, "end": 127.52, "text": " And it has some very cool features.", "tokens": [50924, 400, 309, 575, 512, 588, 1627, 4122, 13, 51072], "temperature": 0.0, "avg_logprob": -0.11913118268003559, "compression_ratio": 1.4345991561181435, "no_speech_prob": 0.00114033545833081}, {"id": 39, "seek": 11336, "start": 127.52, "end": 129.84, "text": " So it has a platform agnostic API.", "tokens": [51072, 407, 309, 575, 257, 3663, 623, 77, 19634, 9362, 13, 51188], "temperature": 0.0, "avg_logprob": -0.11913118268003559, "compression_ratio": 1.4345991561181435, "no_speech_prob": 0.00114033545833081}, {"id": 40, "seek": 11336, "start": 129.84, "end": 134.0, "text": " So developers, they don't need to know GPU programming,", "tokens": [51188, 407, 8849, 11, 436, 500, 380, 643, 281, 458, 18407, 9410, 11, 51396], "temperature": 0.0, "avg_logprob": -0.11913118268003559, "compression_ratio": 1.4345991561181435, "no_speech_prob": 0.00114033545833081}, {"id": 41, "seek": 11336, "start": 134.0, "end": 135.12, "text": " FPGA programming.", "tokens": [51396, 36655, 12570, 9410, 13, 51452], "temperature": 0.0, "avg_logprob": -0.11913118268003559, "compression_ratio": 1.4345991561181435, "no_speech_prob": 0.00114033545833081}, {"id": 42, "seek": 11336, "start": 136.96, "end": 139.76, "text": " It comes with an optimizing compiler.", "tokens": [51544, 467, 1487, 365, 364, 40425, 31958, 13, 51684], "temperature": 0.0, "avg_logprob": -0.11913118268003559, "compression_ratio": 1.4345991561181435, "no_speech_prob": 0.00114033545833081}, {"id": 43, "seek": 11336, "start": 139.76, "end": 142.4, "text": " So we extend GRAL with new phases", "tokens": [51684, 407, 321, 10101, 10903, 3427, 365, 777, 18764, 51816], "temperature": 0.0, "avg_logprob": -0.11913118268003559, "compression_ratio": 1.4345991561181435, "no_speech_prob": 0.00114033545833081}, {"id": 44, "seek": 14240, "start": 142.48000000000002, "end": 144.64000000000001, "text": " that they can take Java methods", "tokens": [50368, 300, 436, 393, 747, 10745, 7150, 50476], "temperature": 0.0, "avg_logprob": -0.10753031994434113, "compression_ratio": 1.4663677130044843, "no_speech_prob": 0.002787552773952484}, {"id": 45, "seek": 14240, "start": 144.64000000000001, "end": 146.64000000000001, "text": " and compile them to GPU code.", "tokens": [50476, 293, 31413, 552, 281, 18407, 3089, 13, 50576], "temperature": 0.0, "avg_logprob": -0.10753031994434113, "compression_ratio": 1.4663677130044843, "no_speech_prob": 0.002787552773952484}, {"id": 46, "seek": 14240, "start": 148.96, "end": 151.92000000000002, "text": " We have a feature of dynamic reconfiguration at runtime,", "tokens": [50692, 492, 362, 257, 4111, 295, 8546, 9993, 20646, 8167, 412, 34474, 11, 50840], "temperature": 0.0, "avg_logprob": -0.10753031994434113, "compression_ratio": 1.4663677130044843, "no_speech_prob": 0.002787552773952484}, {"id": 47, "seek": 14240, "start": 151.92000000000002, "end": 156.16, "text": " which means that the method execution can be migrated", "tokens": [50840, 597, 1355, 300, 264, 3170, 15058, 393, 312, 48329, 51052], "temperature": 0.0, "avg_logprob": -0.10753031994434113, "compression_ratio": 1.4663677130044843, "no_speech_prob": 0.002787552773952484}, {"id": 48, "seek": 14240, "start": 156.16, "end": 158.32, "text": " from a GPU back to the JVM", "tokens": [51052, 490, 257, 18407, 646, 281, 264, 508, 53, 44, 51160], "temperature": 0.0, "avg_logprob": -0.10753031994434113, "compression_ratio": 1.4663677130044843, "no_speech_prob": 0.002787552773952484}, {"id": 49, "seek": 14240, "start": 158.32, "end": 162.4, "text": " and then go to the FPGA if it is appropriate.", "tokens": [51160, 293, 550, 352, 281, 264, 36655, 12570, 498, 309, 307, 6854, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10753031994434113, "compression_ratio": 1.4663677130044843, "no_speech_prob": 0.002787552773952484}, {"id": 50, "seek": 14240, "start": 163.44, "end": 167.04000000000002, "text": " And with the latest release 1.0,", "tokens": [51416, 400, 365, 264, 6792, 4374, 502, 13, 15, 11, 51596], "temperature": 0.0, "avg_logprob": -0.10753031994434113, "compression_ratio": 1.4663677130044843, "no_speech_prob": 0.002787552773952484}, {"id": 51, "seek": 14240, "start": 167.04000000000002, "end": 170.24, "text": " we have enabled support for off-heap data types.", "tokens": [51596, 321, 362, 15172, 1406, 337, 766, 12, 675, 569, 1412, 3467, 13, 51756], "temperature": 0.0, "avg_logprob": -0.10753031994434113, "compression_ratio": 1.4663677130044843, "no_speech_prob": 0.002787552773952484}, {"id": 52, "seek": 17024, "start": 170.24, "end": 173.52, "text": " So data can be allocated off-heap", "tokens": [50364, 407, 1412, 393, 312, 29772, 766, 12, 675, 569, 50528], "temperature": 0.0, "avg_logprob": -0.11667777496634178, "compression_ratio": 1.5183673469387755, "no_speech_prob": 0.0007638348615728319}, {"id": 53, "seek": 17024, "start": 173.52, "end": 176.16, "text": " with a foreign function and memory API.", "tokens": [50528, 365, 257, 5329, 2445, 293, 4675, 9362, 13, 50660], "temperature": 0.0, "avg_logprob": -0.11667777496634178, "compression_ratio": 1.5183673469387755, "no_speech_prob": 0.0007638348615728319}, {"id": 54, "seek": 17024, "start": 176.8, "end": 181.68, "text": " And this is the API that Mauricio described earlier today.", "tokens": [50692, 400, 341, 307, 264, 9362, 300, 26133, 18322, 7619, 3071, 965, 13, 50936], "temperature": 0.0, "avg_logprob": -0.11667777496634178, "compression_ratio": 1.5183673469387755, "no_speech_prob": 0.0007638348615728319}, {"id": 55, "seek": 17024, "start": 181.68, "end": 185.28, "text": " So feel free to follow Tornado VM in Twitter", "tokens": [50936, 407, 841, 1737, 281, 1524, 314, 1865, 1573, 18038, 294, 5794, 51116], "temperature": 0.0, "avg_logprob": -0.11667777496634178, "compression_ratio": 1.5183673469387755, "no_speech_prob": 0.0007638348615728319}, {"id": 56, "seek": 17024, "start": 185.28, "end": 189.28, "text": " to engage with the website and of course to fork", "tokens": [51116, 281, 4683, 365, 264, 3144, 293, 295, 1164, 281, 17716, 51316], "temperature": 0.0, "avg_logprob": -0.11667777496634178, "compression_ratio": 1.5183673469387755, "no_speech_prob": 0.0007638348615728319}, {"id": 57, "seek": 17024, "start": 189.28, "end": 192.4, "text": " and try our examples which are open sourcing GitHub.", "tokens": [51316, 293, 853, 527, 5110, 597, 366, 1269, 11006, 2175, 23331, 13, 51472], "temperature": 0.0, "avg_logprob": -0.11667777496634178, "compression_ratio": 1.5183673469387755, "no_speech_prob": 0.0007638348615728319}, {"id": 58, "seek": 17024, "start": 194.4, "end": 196.8, "text": " So I spoke a little bit about off-heap data types,", "tokens": [51572, 407, 286, 7179, 257, 707, 857, 466, 766, 12, 675, 569, 1412, 3467, 11, 51692], "temperature": 0.0, "avg_logprob": -0.11667777496634178, "compression_ratio": 1.5183673469387755, "no_speech_prob": 0.0007638348615728319}, {"id": 59, "seek": 17024, "start": 196.8, "end": 199.36, "text": " so I'll give an introduction, an example,", "tokens": [51692, 370, 286, 603, 976, 364, 9339, 11, 364, 1365, 11, 51820], "temperature": 0.0, "avg_logprob": -0.11667777496634178, "compression_ratio": 1.5183673469387755, "no_speech_prob": 0.0007638348615728319}, {"id": 60, "seek": 19936, "start": 199.36, "end": 203.44000000000003, "text": " because I'm not going to dive very into the API.", "tokens": [50364, 570, 286, 478, 406, 516, 281, 9192, 588, 666, 264, 9362, 13, 50568], "temperature": 0.0, "avg_logprob": -0.09960906002499642, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0016991047887131572}, {"id": 61, "seek": 19936, "start": 203.44000000000003, "end": 205.76000000000002, "text": " So here we see two snapshots of code.", "tokens": [50568, 407, 510, 321, 536, 732, 19206, 27495, 295, 3089, 13, 50684], "temperature": 0.0, "avg_logprob": -0.09960906002499642, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0016991047887131572}, {"id": 62, "seek": 19936, "start": 205.76000000000002, "end": 209.20000000000002, "text": " On the left side, we see a main method", "tokens": [50684, 1282, 264, 1411, 1252, 11, 321, 536, 257, 2135, 3170, 50856], "temperature": 0.0, "avg_logprob": -0.09960906002499642, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0016991047887131572}, {"id": 63, "seek": 19936, "start": 209.20000000000002, "end": 212.0, "text": " that contains the allocation of float array", "tokens": [50856, 300, 8306, 264, 27599, 295, 15706, 10225, 50996], "temperature": 0.0, "avg_logprob": -0.09960906002499642, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0016991047887131572}, {"id": 64, "seek": 19936, "start": 212.8, "end": 214.16000000000003, "text": " by using primitive types,", "tokens": [51036, 538, 1228, 28540, 3467, 11, 51104], "temperature": 0.0, "avg_logprob": -0.09960906002499642, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0016991047887131572}, {"id": 65, "seek": 19936, "start": 214.96, "end": 218.8, "text": " but is allocated as an object, in a sense, on-heap.", "tokens": [51144, 457, 307, 29772, 382, 364, 2657, 11, 294, 257, 2020, 11, 322, 12, 675, 569, 13, 51336], "temperature": 0.0, "avg_logprob": -0.09960906002499642, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0016991047887131572}, {"id": 66, "seek": 19936, "start": 218.8, "end": 221.52, "text": " So to migrate from such an allocation", "tokens": [51336, 407, 281, 31821, 490, 1270, 364, 27599, 51472], "temperature": 0.0, "avg_logprob": -0.09960906002499642, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0016991047887131572}, {"id": 67, "seek": 19936, "start": 222.24, "end": 226.32000000000002, "text": " to the new allocation API that's exposed by the Tornado API,", "tokens": [51508, 281, 264, 777, 27599, 9362, 300, 311, 9495, 538, 264, 314, 1865, 1573, 9362, 11, 51712], "temperature": 0.0, "avg_logprob": -0.09960906002499642, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0016991047887131572}, {"id": 68, "seek": 19936, "start": 226.32000000000002, "end": 228.72000000000003, "text": " we have created the float array object", "tokens": [51712, 321, 362, 2942, 264, 15706, 10225, 2657, 51832], "temperature": 0.0, "avg_logprob": -0.09960906002499642, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0016991047887131572}, {"id": 69, "seek": 22872, "start": 228.72, "end": 231.6, "text": " that inside it can allocate memory", "tokens": [50364, 300, 1854, 309, 393, 35713, 4675, 50508], "temperature": 0.0, "avg_logprob": -0.09592128893651, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0010525243123993278}, {"id": 70, "seek": 22872, "start": 231.6, "end": 234.96, "text": " by using the memory segment of the foreign function API.", "tokens": [50508, 538, 1228, 264, 4675, 9469, 295, 264, 5329, 2445, 9362, 13, 50676], "temperature": 0.0, "avg_logprob": -0.09592128893651, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0010525243123993278}, {"id": 71, "seek": 22872, "start": 235.84, "end": 238.32, "text": " And it will allocate this memory off-heap.", "tokens": [50720, 400, 309, 486, 35713, 341, 4675, 766, 12, 675, 569, 13, 50844], "temperature": 0.0, "avg_logprob": -0.09592128893651, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0010525243123993278}, {"id": 72, "seek": 22872, "start": 238.32, "end": 243.84, "text": " So this memory segment could be used directly from the GPU", "tokens": [50844, 407, 341, 4675, 9469, 727, 312, 1143, 3838, 490, 264, 18407, 51120], "temperature": 0.0, "avg_logprob": -0.09592128893651, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0010525243123993278}, {"id": 73, "seek": 22872, "start": 244.88, "end": 248.0, "text": " without the need to worry about GC collections and this stuff.", "tokens": [51172, 1553, 264, 643, 281, 3292, 466, 29435, 16641, 293, 341, 1507, 13, 51328], "temperature": 0.0, "avg_logprob": -0.09592128893651, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0010525243123993278}, {"id": 74, "seek": 22872, "start": 248.56, "end": 251.76, "text": " And the cool part is that even if you don't use GPU programming,", "tokens": [51356, 400, 264, 1627, 644, 307, 300, 754, 498, 291, 500, 380, 764, 18407, 9410, 11, 51516], "temperature": 0.0, "avg_logprob": -0.09592128893651, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0010525243123993278}, {"id": 75, "seek": 22872, "start": 251.76, "end": 254.24, "text": " even if you don't want to execute on GPUs,", "tokens": [51516, 754, 498, 291, 500, 380, 528, 281, 14483, 322, 18407, 82, 11, 51640], "temperature": 0.0, "avg_logprob": -0.09592128893651, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0010525243123993278}, {"id": 76, "seek": 22872, "start": 254.24, "end": 258.32, "text": " you can still use this API to allocate memory off-heap.", "tokens": [51640, 291, 393, 920, 764, 341, 9362, 281, 35713, 4675, 766, 12, 675, 569, 13, 51844], "temperature": 0.0, "avg_logprob": -0.09592128893651, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0010525243123993278}, {"id": 77, "seek": 25872, "start": 259.12, "end": 263.68, "text": " And here is a link that explains more.", "tokens": [50384, 400, 510, 307, 257, 2113, 300, 13948, 544, 13, 50612], "temperature": 0.0, "avg_logprob": -0.1852154035246774, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.0014334499137476087}, {"id": 78, "seek": 25872, "start": 264.72, "end": 267.84000000000003, "text": " I hope it's visual from your side.", "tokens": [50664, 286, 1454, 309, 311, 5056, 490, 428, 1252, 13, 50820], "temperature": 0.0, "avg_logprob": -0.1852154035246774, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.0014334499137476087}, {"id": 79, "seek": 25872, "start": 267.84000000000003, "end": 272.32000000000005, "text": " If not, you will find my slides online in the Fosdome webpage.", "tokens": [50820, 759, 406, 11, 291, 486, 915, 452, 9788, 2950, 294, 264, 479, 329, 67, 423, 37852, 13, 51044], "temperature": 0.0, "avg_logprob": -0.1852154035246774, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.0014334499137476087}, {"id": 80, "seek": 25872, "start": 273.04, "end": 274.96000000000004, "text": " So the motivation for today is that", "tokens": [51080, 407, 264, 12335, 337, 965, 307, 300, 51176], "temperature": 0.0, "avg_logprob": -0.1852154035246774, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.0014334499137476087}, {"id": 81, "seek": 25872, "start": 275.52000000000004, "end": 279.28000000000003, "text": " Graal VM enables interoperability between programming languages", "tokens": [51204, 8985, 304, 18038, 17077, 728, 7192, 2310, 1296, 9410, 8650, 51392], "temperature": 0.0, "avg_logprob": -0.1852154035246774, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.0014334499137476087}, {"id": 82, "seek": 25872, "start": 279.28000000000003, "end": 282.56, "text": " like Ruby, JavaScript, and other programming languages.", "tokens": [51392, 411, 19907, 11, 15778, 11, 293, 661, 9410, 8650, 13, 51556], "temperature": 0.0, "avg_logprob": -0.1852154035246774, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.0014334499137476087}, {"id": 83, "seek": 25872, "start": 283.68, "end": 287.04, "text": " And Tornado VM enables hardware acceleration for Java.", "tokens": [51612, 400, 314, 1865, 1573, 18038, 17077, 8837, 17162, 337, 10745, 13, 51780], "temperature": 0.0, "avg_logprob": -0.1852154035246774, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.0014334499137476087}, {"id": 84, "seek": 28704, "start": 287.6, "end": 290.32, "text": " So what if we can combine them together", "tokens": [50392, 407, 437, 498, 321, 393, 10432, 552, 1214, 50528], "temperature": 0.0, "avg_logprob": -0.14851627874811854, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0064224982634186745}, {"id": 85, "seek": 28704, "start": 290.32, "end": 294.24, "text": " and harness GPU acceleration from all these programming languages", "tokens": [50528, 293, 19700, 18407, 17162, 490, 439, 613, 9410, 8650, 50724], "temperature": 0.0, "avg_logprob": -0.14851627874811854, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0064224982634186745}, {"id": 86, "seek": 28704, "start": 294.24, "end": 295.84000000000003, "text": " that are running on top of Trafl?", "tokens": [50724, 300, 366, 2614, 322, 1192, 295, 5403, 3423, 30, 50804], "temperature": 0.0, "avg_logprob": -0.14851627874811854, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0064224982634186745}, {"id": 87, "seek": 28704, "start": 298.0, "end": 300.16, "text": " Let's have a dive into the tech flow.", "tokens": [50912, 961, 311, 362, 257, 9192, 666, 264, 7553, 3095, 13, 51020], "temperature": 0.0, "avg_logprob": -0.14851627874811854, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0064224982634186745}, {"id": 88, "seek": 28704, "start": 300.16, "end": 304.48, "text": " So in this slide, I present a software stack", "tokens": [51020, 407, 294, 341, 4137, 11, 286, 1974, 257, 4722, 8630, 51236], "temperature": 0.0, "avg_logprob": -0.14851627874811854, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0064224982634186745}, {"id": 89, "seek": 28704, "start": 304.48, "end": 306.72, "text": " from Graal VM for Trafl.", "tokens": [51236, 490, 8985, 304, 18038, 337, 5403, 3423, 13, 51348], "temperature": 0.0, "avg_logprob": -0.14851627874811854, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0064224982634186745}, {"id": 90, "seek": 28704, "start": 306.72, "end": 310.16, "text": " So on the top, we see the Trafl framework", "tokens": [51348, 407, 322, 264, 1192, 11, 321, 536, 264, 5403, 3423, 8388, 51520], "temperature": 0.0, "avg_logprob": -0.14851627874811854, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0064224982634186745}, {"id": 91, "seek": 28704, "start": 310.16, "end": 313.36, "text": " and many implementations of polyglot runtimes", "tokens": [51520, 293, 867, 4445, 763, 295, 6754, 7191, 310, 49435, 1532, 51680], "temperature": 0.0, "avg_logprob": -0.14851627874811854, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0064224982634186745}, {"id": 92, "seek": 28704, "start": 313.36, "end": 316.88, "text": " like Graalpy, Graal.js, Trafl Ruby.", "tokens": [51680, 411, 8985, 304, 8200, 11, 8985, 304, 13, 25530, 11, 5403, 3423, 19907, 13, 51856], "temperature": 0.0, "avg_logprob": -0.14851627874811854, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0064224982634186745}, {"id": 93, "seek": 31688, "start": 316.96, "end": 320.24, "text": " And others because Trafl enables also programming language", "tokens": [50368, 400, 2357, 570, 5403, 3423, 17077, 611, 9410, 2856, 50532], "temperature": 0.0, "avg_logprob": -0.12325661335516414, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.0023716483265161514}, {"id": 94, "seek": 31688, "start": 320.24, "end": 323.2, "text": " implementers to create their own programming languages", "tokens": [50532, 4445, 433, 281, 1884, 641, 1065, 9410, 8650, 50680], "temperature": 0.0, "avg_logprob": -0.12325661335516414, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.0023716483265161514}, {"id": 95, "seek": 31688, "start": 324.48, "end": 326.24, "text": " by using the Java API.", "tokens": [50744, 538, 1228, 264, 10745, 9362, 13, 50832], "temperature": 0.0, "avg_logprob": -0.12325661335516414, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.0023716483265161514}, {"id": 96, "seek": 31688, "start": 326.24, "end": 330.08, "text": " So I have grouped Python, Ruby, JavaScript, and Node.js", "tokens": [50832, 407, 286, 362, 41877, 15329, 11, 19907, 11, 15778, 11, 293, 38640, 13, 25530, 51024], "temperature": 0.0, "avg_logprob": -0.12325661335516414, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.0023716483265161514}, {"id": 97, "seek": 31688, "start": 330.71999999999997, "end": 332.24, "text": " in this side of the slide.", "tokens": [51056, 294, 341, 1252, 295, 264, 4137, 13, 51132], "temperature": 0.0, "avg_logprob": -0.12325661335516414, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.0023716483265161514}, {"id": 98, "seek": 31688, "start": 333.44, "end": 337.36, "text": " And then beneath them, there is the Graal VM Zit compiler,", "tokens": [51192, 400, 550, 17149, 552, 11, 456, 307, 264, 8985, 304, 18038, 1176, 270, 31958, 11, 51388], "temperature": 0.0, "avg_logprob": -0.12325661335516414, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.0023716483265161514}, {"id": 99, "seek": 31688, "start": 337.36, "end": 339.28, "text": " so an optimizing compiler from Graal.", "tokens": [51388, 370, 364, 40425, 31958, 490, 8985, 304, 13, 51484], "temperature": 0.0, "avg_logprob": -0.12325661335516414, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.0023716483265161514}, {"id": 100, "seek": 31688, "start": 340.32, "end": 344.24, "text": " So Java is also running on top of the JVM, of course.", "tokens": [51536, 407, 10745, 307, 611, 2614, 322, 1192, 295, 264, 508, 53, 44, 11, 295, 1164, 13, 51732], "temperature": 0.0, "avg_logprob": -0.12325661335516414, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.0023716483265161514}, {"id": 101, "seek": 31688, "start": 344.24, "end": 346.32, "text": " And all these languages, they start", "tokens": [51732, 400, 439, 613, 8650, 11, 436, 722, 51836], "temperature": 0.0, "avg_logprob": -0.12325661335516414, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.0023716483265161514}, {"id": 102, "seek": 34688, "start": 346.88, "end": 349.76, "text": " in the interpreted mode, and once they reach a hot state,", "tokens": [50364, 294, 264, 26749, 4391, 11, 293, 1564, 436, 2524, 257, 2368, 1785, 11, 50508], "temperature": 0.0, "avg_logprob": -0.09788336473352768, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.007003685459494591}, {"id": 103, "seek": 34688, "start": 349.76, "end": 352.48, "text": " then the optimizing compiler kicks in.", "tokens": [50508, 550, 264, 40425, 31958, 21293, 294, 13, 50644], "temperature": 0.0, "avg_logprob": -0.09788336473352768, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.007003685459494591}, {"id": 104, "seek": 34688, "start": 353.36, "end": 357.52, "text": " And the cool part with such a polyglot implementation", "tokens": [50688, 400, 264, 1627, 644, 365, 1270, 257, 6754, 7191, 310, 11420, 50896], "temperature": 0.0, "avg_logprob": -0.09788336473352768, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.007003685459494591}, {"id": 105, "seek": 34688, "start": 357.52, "end": 359.36, "text": " that enables polyglot programming", "tokens": [50896, 300, 17077, 6754, 7191, 310, 9410, 50988], "temperature": 0.0, "avg_logprob": -0.09788336473352768, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.007003685459494591}, {"id": 106, "seek": 34688, "start": 359.36, "end": 363.36, "text": " is that there is, for the compiler enthusiasts,", "tokens": [50988, 307, 300, 456, 307, 11, 337, 264, 31958, 45873, 11, 51188], "temperature": 0.0, "avg_logprob": -0.09788336473352768, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.007003685459494591}, {"id": 107, "seek": 34688, "start": 363.36, "end": 365.68, "text": " there is one Graal IR.", "tokens": [51188, 456, 307, 472, 8985, 304, 16486, 13, 51304], "temperature": 0.0, "avg_logprob": -0.09788336473352768, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.007003685459494591}, {"id": 108, "seek": 34688, "start": 366.4, "end": 369.68, "text": " So the nodes, at runtime, they are rewritten.", "tokens": [51340, 407, 264, 13891, 11, 412, 34474, 11, 436, 366, 319, 26859, 13, 51504], "temperature": 0.0, "avg_logprob": -0.09788336473352768, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.007003685459494591}, {"id": 109, "seek": 34688, "start": 369.68, "end": 371.92, "text": " That means that it can adjust.", "tokens": [51504, 663, 1355, 300, 309, 393, 4369, 13, 51616], "temperature": 0.0, "avg_logprob": -0.09788336473352768, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.007003685459494591}, {"id": 110, "seek": 34688, "start": 371.92, "end": 374.96, "text": " So if we kick in a Python function,", "tokens": [51616, 407, 498, 321, 4437, 294, 257, 15329, 2445, 11, 51768], "temperature": 0.0, "avg_logprob": -0.09788336473352768, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.007003685459494591}, {"id": 111, "seek": 37496, "start": 374.96, "end": 377.12, "text": " then the node can be rewritten,", "tokens": [50364, 550, 264, 9984, 393, 312, 319, 26859, 11, 50472], "temperature": 0.0, "avg_logprob": -0.10244564134247448, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.0009206021204590797}, {"id": 112, "seek": 37496, "start": 377.12, "end": 380.79999999999995, "text": " and the Graal compiler will take a shape", "tokens": [50472, 293, 264, 8985, 304, 31958, 486, 747, 257, 3909, 50656], "temperature": 0.0, "avg_logprob": -0.10244564134247448, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.0009206021204590797}, {"id": 113, "seek": 37496, "start": 380.79999999999995, "end": 384.23999999999995, "text": " and will emit at the assembly code that will run on the CPU.", "tokens": [50656, 293, 486, 32084, 412, 264, 12103, 3089, 300, 486, 1190, 322, 264, 13199, 13, 50828], "temperature": 0.0, "avg_logprob": -0.10244564134247448, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.0009206021204590797}, {"id": 114, "seek": 37496, "start": 385.44, "end": 389.2, "text": " So this solution offers the interoperability", "tokens": [50888, 407, 341, 3827, 7736, 264, 728, 7192, 2310, 51076], "temperature": 0.0, "avg_logprob": -0.10244564134247448, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.0009206021204590797}, {"id": 115, "seek": 37496, "start": 389.2, "end": 392.4, "text": " and offers the execution among different CPU", "tokens": [51076, 293, 7736, 264, 15058, 3654, 819, 13199, 51236], "temperature": 0.0, "avg_logprob": -0.10244564134247448, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.0009206021204590797}, {"id": 116, "seek": 37496, "start": 392.4, "end": 393.84, "text": " instruction set architectures.", "tokens": [51236, 10951, 992, 6331, 1303, 13, 51308], "temperature": 0.0, "avg_logprob": -0.10244564134247448, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.0009206021204590797}, {"id": 117, "seek": 37496, "start": 394.56, "end": 398.71999999999997, "text": " But what if we have this heterogeneous hardware,", "tokens": [51344, 583, 437, 498, 321, 362, 341, 20789, 31112, 8837, 11, 51552], "temperature": 0.0, "avg_logprob": -0.10244564134247448, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.0009206021204590797}, {"id": 118, "seek": 37496, "start": 398.71999999999997, "end": 401.67999999999995, "text": " like GPUs, FPGAs, which are available", "tokens": [51552, 411, 18407, 82, 11, 36655, 38, 10884, 11, 597, 366, 2435, 51700], "temperature": 0.0, "avg_logprob": -0.10244564134247448, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.0009206021204590797}, {"id": 119, "seek": 37496, "start": 401.67999999999995, "end": 403.12, "text": " in some systems and servers?", "tokens": [51700, 294, 512, 3652, 293, 15909, 30, 51772], "temperature": 0.0, "avg_logprob": -0.10244564134247448, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.0009206021204590797}, {"id": 120, "seek": 40312, "start": 403.76, "end": 405.84000000000003, "text": " Well, then we'll have Tornado VM", "tokens": [50396, 1042, 11, 550, 321, 603, 362, 314, 1865, 1573, 18038, 50500], "temperature": 0.0, "avg_logprob": -0.11286185769473805, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0013614848721772432}, {"id": 121, "seek": 40312, "start": 405.84000000000003, "end": 411.2, "text": " that enables Java methods to be compiled for GPUs, FPGAs, etc.", "tokens": [50500, 300, 17077, 10745, 7150, 281, 312, 36548, 337, 18407, 82, 11, 36655, 38, 10884, 11, 5183, 13, 50768], "temperature": 0.0, "avg_logprob": -0.11286185769473805, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0013614848721772432}, {"id": 122, "seek": 40312, "start": 411.76, "end": 414.48, "text": " Tornado VM has its own JIT compiler,", "tokens": [50796, 314, 1865, 1573, 18038, 575, 1080, 1065, 508, 3927, 31958, 11, 50932], "temperature": 0.0, "avg_logprob": -0.11286185769473805, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0013614848721772432}, {"id": 123, "seek": 40312, "start": 414.48, "end": 417.68, "text": " which is an extension, a superset, I would say, of Graal,", "tokens": [50932, 597, 307, 364, 10320, 11, 257, 37906, 302, 11, 286, 576, 584, 11, 295, 8985, 304, 11, 51092], "temperature": 0.0, "avg_logprob": -0.11286185769473805, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0013614848721772432}, {"id": 124, "seek": 40312, "start": 417.68, "end": 422.72, "text": " the Graal compiler, that it is enhanced with new phases", "tokens": [51092, 264, 8985, 304, 31958, 11, 300, 309, 307, 21191, 365, 777, 18764, 51344], "temperature": 0.0, "avg_logprob": -0.11286185769473805, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0013614848721772432}, {"id": 125, "seek": 40312, "start": 422.72, "end": 425.68, "text": " in the compiler to automatically specialize", "tokens": [51344, 294, 264, 31958, 281, 6772, 37938, 51492], "temperature": 0.0, "avg_logprob": -0.11286185769473805, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0013614848721772432}, {"id": 126, "seek": 40312, "start": 425.68, "end": 430.72, "text": " the code from a method for GPU acceleration and FPG acceleration.", "tokens": [51492, 264, 3089, 490, 257, 3170, 337, 18407, 17162, 293, 36655, 38, 17162, 13, 51744], "temperature": 0.0, "avg_logprob": -0.11286185769473805, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0013614848721772432}, {"id": 127, "seek": 43072, "start": 430.72, "end": 432.32000000000005, "text": " So at the backbone of the compiler,", "tokens": [50364, 407, 412, 264, 34889, 295, 264, 31958, 11, 50444], "temperature": 0.0, "avg_logprob": -0.13466277266993668, "compression_ratio": 1.4863636363636363, "no_speech_prob": 0.0033931348007172346}, {"id": 128, "seek": 43072, "start": 432.32000000000005, "end": 434.24, "text": " we have three backends at the moment.", "tokens": [50444, 321, 362, 1045, 646, 2581, 412, 264, 1623, 13, 50540], "temperature": 0.0, "avg_logprob": -0.13466277266993668, "compression_ratio": 1.4863636363636363, "no_speech_prob": 0.0033931348007172346}, {"id": 129, "seek": 43072, "start": 434.24, "end": 437.44000000000005, "text": " We have OpenCL backend, CUDA, and SPV.", "tokens": [50540, 492, 362, 7238, 22458, 38087, 11, 29777, 7509, 11, 293, 8420, 53, 13, 50700], "temperature": 0.0, "avg_logprob": -0.13466277266993668, "compression_ratio": 1.4863636363636363, "no_speech_prob": 0.0033931348007172346}, {"id": 130, "seek": 43072, "start": 438.96000000000004, "end": 442.56, "text": " And such a solution would enable many things.", "tokens": [50776, 400, 1270, 257, 3827, 576, 9528, 867, 721, 13, 50956], "temperature": 0.0, "avg_logprob": -0.13466277266993668, "compression_ratio": 1.4863636363636363, "no_speech_prob": 0.0033931348007172346}, {"id": 131, "seek": 43072, "start": 442.56, "end": 445.92, "text": " So if you want to learn more about the APIs,", "tokens": [50956, 407, 498, 291, 528, 281, 1466, 544, 466, 264, 21445, 11, 51124], "temperature": 0.0, "avg_logprob": -0.13466277266993668, "compression_ratio": 1.4863636363636363, "no_speech_prob": 0.0033931348007172346}, {"id": 132, "seek": 43072, "start": 445.92, "end": 448.64000000000004, "text": " you can scan this QR code.", "tokens": [51124, 291, 393, 11049, 341, 32784, 3089, 13, 51260], "temperature": 0.0, "avg_logprob": -0.13466277266993668, "compression_ratio": 1.4863636363636363, "no_speech_prob": 0.0033931348007172346}, {"id": 133, "seek": 43072, "start": 449.36, "end": 453.84000000000003, "text": " And the code that is implemented with Tornado VM,", "tokens": [51296, 400, 264, 3089, 300, 307, 12270, 365, 314, 1865, 1573, 18038, 11, 51520], "temperature": 0.0, "avg_logprob": -0.13466277266993668, "compression_ratio": 1.4863636363636363, "no_speech_prob": 0.0033931348007172346}, {"id": 134, "seek": 43072, "start": 453.84000000000003, "end": 458.32000000000005, "text": " it can harness besides the off-hip data types,", "tokens": [51520, 309, 393, 19700, 11868, 264, 766, 12, 71, 647, 1412, 3467, 11, 51744], "temperature": 0.0, "avg_logprob": -0.13466277266993668, "compression_ratio": 1.4863636363636363, "no_speech_prob": 0.0033931348007172346}, {"id": 135, "seek": 45832, "start": 458.32, "end": 461.92, "text": " it can also harness the execution with a Tornado VM profiler.", "tokens": [50364, 309, 393, 611, 19700, 264, 15058, 365, 257, 314, 1865, 1573, 18038, 1740, 5441, 13, 50544], "temperature": 0.0, "avg_logprob": -0.13377540906270344, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.0015740477247163653}, {"id": 136, "seek": 45832, "start": 461.92, "end": 465.44, "text": " If you want to learn more about the characteristics", "tokens": [50544, 759, 291, 528, 281, 1466, 544, 466, 264, 10891, 50720], "temperature": 0.0, "avg_logprob": -0.13377540906270344, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.0015740477247163653}, {"id": 137, "seek": 45832, "start": 465.44, "end": 467.92, "text": " of your application, you can see how many data", "tokens": [50720, 295, 428, 3861, 11, 291, 393, 536, 577, 867, 1412, 50844], "temperature": 0.0, "avg_logprob": -0.13377540906270344, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.0015740477247163653}, {"id": 138, "seek": 45832, "start": 467.92, "end": 469.84, "text": " will be copying in the GPU memory,", "tokens": [50844, 486, 312, 27976, 294, 264, 18407, 4675, 11, 50940], "temperature": 0.0, "avg_logprob": -0.13377540906270344, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.0015740477247163653}, {"id": 139, "seek": 45832, "start": 470.71999999999997, "end": 472.8, "text": " how expensive is the IEO maybe,", "tokens": [50984, 577, 5124, 307, 264, 286, 6004, 1310, 11, 51088], "temperature": 0.0, "avg_logprob": -0.13377540906270344, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.0015740477247163653}, {"id": 140, "seek": 45832, "start": 472.8, "end": 475.12, "text": " because this could be very critical", "tokens": [51088, 570, 341, 727, 312, 588, 4924, 51204], "temperature": 0.0, "avg_logprob": -0.13377540906270344, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.0015740477247163653}, {"id": 141, "seek": 45832, "start": 475.12, "end": 476.56, "text": " for the performance of the system.", "tokens": [51204, 337, 264, 3389, 295, 264, 1185, 13, 51276], "temperature": 0.0, "avg_logprob": -0.13377540906270344, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.0015740477247163653}, {"id": 142, "seek": 45832, "start": 477.52, "end": 479.84, "text": " And you can customize even how many,", "tokens": [51324, 400, 291, 393, 19734, 754, 577, 867, 11, 51440], "temperature": 0.0, "avg_logprob": -0.13377540906270344, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.0015740477247163653}, {"id": 143, "seek": 45832, "start": 479.84, "end": 482.4, "text": " how the data transfers will be performed.", "tokens": [51440, 577, 264, 1412, 29137, 486, 312, 10332, 13, 51568], "temperature": 0.0, "avg_logprob": -0.13377540906270344, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.0015740477247163653}, {"id": 144, "seek": 45832, "start": 482.4, "end": 485.44, "text": " Because, for example, if you have a method", "tokens": [51568, 1436, 11, 337, 1365, 11, 498, 291, 362, 257, 3170, 51720], "temperature": 0.0, "avg_logprob": -0.13377540906270344, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.0015740477247163653}, {"id": 145, "seek": 45832, "start": 485.44, "end": 487.52, "text": " that consumes redoneally data,", "tokens": [51720, 300, 48823, 2182, 546, 379, 1412, 11, 51824], "temperature": 0.0, "avg_logprob": -0.13377540906270344, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.0015740477247163653}, {"id": 146, "seek": 48752, "start": 487.52, "end": 490.15999999999997, "text": " then maybe you need to copy the data once,", "tokens": [50364, 550, 1310, 291, 643, 281, 5055, 264, 1412, 1564, 11, 50496], "temperature": 0.0, "avg_logprob": -0.1267702102661133, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.0006161652854643762}, {"id": 147, "seek": 48752, "start": 490.15999999999997, "end": 493.2, "text": " instead of copying the data every time you execute the kernel.", "tokens": [50496, 2602, 295, 27976, 264, 1412, 633, 565, 291, 14483, 264, 28256, 13, 50648], "temperature": 0.0, "avg_logprob": -0.1267702102661133, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.0006161652854643762}, {"id": 148, "seek": 48752, "start": 495.68, "end": 498.08, "text": " Okay, so let's jump to the deployment.", "tokens": [50772, 1033, 11, 370, 718, 311, 3012, 281, 264, 19317, 13, 50892], "temperature": 0.0, "avg_logprob": -0.1267702102661133, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.0006161652854643762}, {"id": 149, "seek": 48752, "start": 499.76, "end": 501.68, "text": " As I said, Tornado VM is compatible", "tokens": [50976, 1018, 286, 848, 11, 314, 1865, 1573, 18038, 307, 18218, 51072], "temperature": 0.0, "avg_logprob": -0.1267702102661133, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.0006161652854643762}, {"id": 150, "seek": 48752, "start": 501.68, "end": 504.0, "text": " with different JDK distributions,", "tokens": [51072, 365, 819, 37082, 42, 37870, 11, 51188], "temperature": 0.0, "avg_logprob": -0.1267702102661133, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.0006161652854643762}, {"id": 151, "seek": 48752, "start": 504.0, "end": 509.59999999999997, "text": " so it's not a JVM, it is a plugin for JDK distributions.", "tokens": [51188, 370, 309, 311, 406, 257, 508, 53, 44, 11, 309, 307, 257, 23407, 337, 37082, 42, 37870, 13, 51468], "temperature": 0.0, "avg_logprob": -0.1267702102661133, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.0006161652854643762}, {"id": 152, "seek": 48752, "start": 510.15999999999997, "end": 512.72, "text": " So it can be seen as a library, in a sense,", "tokens": [51496, 407, 309, 393, 312, 1612, 382, 257, 6405, 11, 294, 257, 2020, 11, 51624], "temperature": 0.0, "avg_logprob": -0.1267702102661133, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.0006161652854643762}, {"id": 153, "seek": 48752, "start": 512.72, "end": 514.8, "text": " because it offers an API in Java.", "tokens": [51624, 570, 309, 7736, 364, 9362, 294, 10745, 13, 51728], "temperature": 0.0, "avg_logprob": -0.1267702102661133, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.0006161652854643762}, {"id": 154, "seek": 51480, "start": 515.52, "end": 518.88, "text": " And it is compatible with all these distributions.", "tokens": [50400, 400, 309, 307, 18218, 365, 439, 613, 37870, 13, 50568], "temperature": 0.0, "avg_logprob": -0.1397162594841522, "compression_ratio": 1.5179282868525896, "no_speech_prob": 0.00144135148730129}, {"id": 155, "seek": 51480, "start": 519.76, "end": 522.88, "text": " And on the other side, we have the compiler backends", "tokens": [50612, 400, 322, 264, 661, 1252, 11, 321, 362, 264, 31958, 646, 2581, 50768], "temperature": 0.0, "avg_logprob": -0.1397162594841522, "compression_ratio": 1.5179282868525896, "no_speech_prob": 0.00144135148730129}, {"id": 156, "seek": 51480, "start": 522.88, "end": 524.64, "text": " that makes it compatible with", "tokens": [50768, 300, 1669, 309, 18218, 365, 50856], "temperature": 0.0, "avg_logprob": -0.1397162594841522, "compression_ratio": 1.5179282868525896, "no_speech_prob": 0.00144135148730129}, {"id": 157, "seek": 51480, "start": 525.3599999999999, "end": 527.5999999999999, "text": " different heterogeneous hardware accelerators.", "tokens": [50892, 819, 20789, 31112, 8837, 10172, 3391, 13, 51004], "temperature": 0.0, "avg_logprob": -0.1397162594841522, "compression_ratio": 1.5179282868525896, "no_speech_prob": 0.00144135148730129}, {"id": 158, "seek": 51480, "start": 528.16, "end": 532.16, "text": " We can emit vectorized code for multi-core CPU execution", "tokens": [51032, 492, 393, 32084, 8062, 1602, 3089, 337, 4825, 12, 12352, 13199, 15058, 51232], "temperature": 0.0, "avg_logprob": -0.1397162594841522, "compression_ratio": 1.5179282868525896, "no_speech_prob": 0.00144135148730129}, {"id": 159, "seek": 51480, "start": 532.16, "end": 533.12, "text": " through OpenCL.", "tokens": [51232, 807, 7238, 22458, 13, 51280], "temperature": 0.0, "avg_logprob": -0.1397162594841522, "compression_ratio": 1.5179282868525896, "no_speech_prob": 0.00144135148730129}, {"id": 160, "seek": 51480, "start": 533.92, "end": 537.1999999999999, "text": " We can run with different GPUs and FPGAs.", "tokens": [51320, 492, 393, 1190, 365, 819, 18407, 82, 293, 36655, 38, 10884, 13, 51484], "temperature": 0.0, "avg_logprob": -0.1397162594841522, "compression_ratio": 1.5179282868525896, "no_speech_prob": 0.00144135148730129}, {"id": 161, "seek": 51480, "start": 537.92, "end": 540.7199999999999, "text": " In this particular talk, I will focus on GraVM,", "tokens": [51520, 682, 341, 1729, 751, 11, 286, 486, 1879, 322, 8985, 53, 44, 11, 51660], "temperature": 0.0, "avg_logprob": -0.1397162594841522, "compression_ratio": 1.5179282868525896, "no_speech_prob": 0.00144135148730129}, {"id": 162, "seek": 51480, "start": 540.7199999999999, "end": 542.4799999999999, "text": " because we want to leverage polyglot,", "tokens": [51660, 570, 321, 528, 281, 13982, 6754, 7191, 310, 11, 51748], "temperature": 0.0, "avg_logprob": -0.1397162594841522, "compression_ratio": 1.5179282868525896, "no_speech_prob": 0.00144135148730129}, {"id": 163, "seek": 54248, "start": 543.12, "end": 546.16, "text": " and NVIDIA GPUs, because I have created Docker images", "tokens": [50396, 293, 426, 3958, 6914, 18407, 82, 11, 570, 286, 362, 2942, 33772, 5267, 50548], "temperature": 0.0, "avg_logprob": -0.1170067246427241, "compression_ratio": 1.530701754385965, "no_speech_prob": 0.0019479587208479643}, {"id": 164, "seek": 54248, "start": 546.16, "end": 548.48, "text": " that they run on the NVIDIA GPUs.", "tokens": [50548, 300, 436, 1190, 322, 264, 426, 3958, 6914, 18407, 82, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1170067246427241, "compression_ratio": 1.530701754385965, "no_speech_prob": 0.0019479587208479643}, {"id": 165, "seek": 54248, "start": 550.72, "end": 553.52, "text": " Now, regarding the GraVM deployment,", "tokens": [50776, 823, 11, 8595, 264, 8985, 53, 44, 19317, 11, 50916], "temperature": 0.0, "avg_logprob": -0.1170067246427241, "compression_ratio": 1.530701754385965, "no_speech_prob": 0.0019479587208479643}, {"id": 166, "seek": 54248, "start": 554.72, "end": 556.72, "text": " I will focus in this slide in GraL Python,", "tokens": [50976, 286, 486, 1879, 294, 341, 4137, 294, 8985, 43, 15329, 11, 51076], "temperature": 0.0, "avg_logprob": -0.1170067246427241, "compression_ratio": 1.530701754385965, "no_speech_prob": 0.0019479587208479643}, {"id": 167, "seek": 54248, "start": 556.72, "end": 559.9200000000001, "text": " which is one implementation of polyglot runtime.", "tokens": [51076, 597, 307, 472, 11420, 295, 6754, 7191, 310, 34474, 13, 51236], "temperature": 0.0, "avg_logprob": -0.1170067246427241, "compression_ratio": 1.530701754385965, "no_speech_prob": 0.0019479587208479643}, {"id": 168, "seek": 54248, "start": 561.04, "end": 566.0, "text": " This is shipped in two different standalone versions, releases.", "tokens": [51292, 639, 307, 25312, 294, 732, 819, 37454, 9606, 11, 16952, 13, 51540], "temperature": 0.0, "avg_logprob": -0.1170067246427241, "compression_ratio": 1.530701754385965, "no_speech_prob": 0.0019479587208479643}, {"id": 169, "seek": 54248, "start": 566.0, "end": 567.76, "text": " So we have the native standalone,", "tokens": [51540, 407, 321, 362, 264, 8470, 37454, 11, 51628], "temperature": 0.0, "avg_logprob": -0.1170067246427241, "compression_ratio": 1.530701754385965, "no_speech_prob": 0.0019479587208479643}, {"id": 170, "seek": 54248, "start": 567.76, "end": 571.52, "text": " which comes with the native image.", "tokens": [51628, 597, 1487, 365, 264, 8470, 3256, 13, 51816], "temperature": 0.0, "avg_logprob": -0.1170067246427241, "compression_ratio": 1.530701754385965, "no_speech_prob": 0.0019479587208479643}, {"id": 171, "seek": 57152, "start": 571.52, "end": 574.4, "text": " And then we have the JVM standalone that enables", "tokens": [50364, 400, 550, 321, 362, 264, 508, 53, 44, 37454, 300, 17077, 50508], "temperature": 0.0, "avg_logprob": -0.13214701697939918, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.006606766022741795}, {"id": 172, "seek": 57152, "start": 574.4, "end": 578.16, "text": " the execution of Python programs on top of the JVM,", "tokens": [50508, 264, 15058, 295, 15329, 4268, 322, 1192, 295, 264, 508, 53, 44, 11, 50696], "temperature": 0.0, "avg_logprob": -0.13214701697939918, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.006606766022741795}, {"id": 173, "seek": 57152, "start": 578.16, "end": 580.56, "text": " and it has also the JVM compiler.", "tokens": [50696, 293, 309, 575, 611, 264, 508, 53, 44, 31958, 13, 50816], "temperature": 0.0, "avg_logprob": -0.13214701697939918, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.006606766022741795}, {"id": 174, "seek": 57152, "start": 581.6, "end": 585.1999999999999, "text": " The version that we tested is the 23.1,", "tokens": [50868, 440, 3037, 300, 321, 8246, 307, 264, 6673, 13, 16, 11, 51048], "temperature": 0.0, "avg_logprob": -0.13214701697939918, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.006606766022741795}, {"id": 175, "seek": 57152, "start": 585.1999999999999, "end": 588.4, "text": " because tornado VM is compatible with this version of GraL.", "tokens": [51048, 570, 27935, 18038, 307, 18218, 365, 341, 3037, 295, 8985, 43, 13, 51208], "temperature": 0.0, "avg_logprob": -0.13214701697939918, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.006606766022741795}, {"id": 176, "seek": 57152, "start": 589.1999999999999, "end": 592.64, "text": " And here you can see that we have downloaded the community,", "tokens": [51248, 400, 510, 291, 393, 536, 300, 321, 362, 21748, 264, 1768, 11, 51420], "temperature": 0.0, "avg_logprob": -0.13214701697939918, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.006606766022741795}, {"id": 177, "seek": 57152, "start": 592.64, "end": 594.3199999999999, "text": " and that's JVM.", "tokens": [51420, 293, 300, 311, 508, 53, 44, 13, 51504], "temperature": 0.0, "avg_logprob": -0.13214701697939918, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.006606766022741795}, {"id": 178, "seek": 57152, "start": 594.3199999999999, "end": 598.64, "text": " So we have the JVM standalone version downloaded.", "tokens": [51504, 407, 321, 362, 264, 508, 53, 44, 37454, 3037, 21748, 13, 51720], "temperature": 0.0, "avg_logprob": -0.13214701697939918, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.006606766022741795}, {"id": 179, "seek": 59864, "start": 599.52, "end": 602.72, "text": " Well, we need the JVM standalone,", "tokens": [50408, 1042, 11, 321, 643, 264, 508, 53, 44, 37454, 11, 50568], "temperature": 0.0, "avg_logprob": -0.15643532492897727, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0007804874912835658}, {"id": 180, "seek": 59864, "start": 602.72, "end": 604.88, "text": " because we want to run with tornado VM,", "tokens": [50568, 570, 321, 528, 281, 1190, 365, 27935, 18038, 11, 50676], "temperature": 0.0, "avg_logprob": -0.15643532492897727, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0007804874912835658}, {"id": 181, "seek": 59864, "start": 604.88, "end": 608.08, "text": " and tornado VM will extend the GraL VM compiler.", "tokens": [50676, 293, 27935, 18038, 486, 10101, 264, 8985, 43, 18038, 31958, 13, 50836], "temperature": 0.0, "avg_logprob": -0.15643532492897727, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0007804874912835658}, {"id": 182, "seek": 59864, "start": 608.08, "end": 609.4399999999999, "text": " So this is the reason.", "tokens": [50836, 407, 341, 307, 264, 1778, 13, 50904], "temperature": 0.0, "avg_logprob": -0.15643532492897727, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0007804874912835658}, {"id": 183, "seek": 59864, "start": 610.16, "end": 612.08, "text": " The problem is that we tried it,", "tokens": [50940, 440, 1154, 307, 300, 321, 3031, 309, 11, 51036], "temperature": 0.0, "avg_logprob": -0.15643532492897727, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0007804874912835658}, {"id": 184, "seek": 59864, "start": 612.08, "end": 616.08, "text": " and the JVM distribution is shipped with the JVM standalone,", "tokens": [51036, 293, 264, 508, 53, 44, 7316, 307, 25312, 365, 264, 508, 53, 44, 37454, 11, 51236], "temperature": 0.0, "avg_logprob": -0.15643532492897727, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0007804874912835658}, {"id": 185, "seek": 59864, "start": 616.08, "end": 619.92, "text": " with a compiler built that it is built with libgral.", "tokens": [51236, 365, 257, 31958, 3094, 300, 309, 307, 3094, 365, 22854, 861, 304, 13, 51428], "temperature": 0.0, "avg_logprob": -0.15643532492897727, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0007804874912835658}, {"id": 186, "seek": 59864, "start": 620.48, "end": 625.6, "text": " So this comes with not many compiler modules,", "tokens": [51456, 407, 341, 1487, 365, 406, 867, 31958, 16679, 11, 51712], "temperature": 0.0, "avg_logprob": -0.15643532492897727, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0007804874912835658}, {"id": 187, "seek": 59864, "start": 625.6, "end": 628.4, "text": " and that breaks the consistency for tornado VM.", "tokens": [51712, 293, 300, 9857, 264, 14416, 337, 27935, 18038, 13, 51852], "temperature": 0.0, "avg_logprob": -0.15643532492897727, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0007804874912835658}, {"id": 188, "seek": 62840, "start": 628.4, "end": 629.1999999999999, "text": " When we tried it.", "tokens": [50364, 1133, 321, 3031, 309, 13, 50404], "temperature": 0.0, "avg_logprob": -0.1309843790733208, "compression_ratio": 1.5977443609022557, "no_speech_prob": 0.006752557586878538}, {"id": 189, "seek": 62840, "start": 630.16, "end": 632.3199999999999, "text": " And this is because they wanted the image,", "tokens": [50452, 400, 341, 307, 570, 436, 1415, 264, 3256, 11, 50560], "temperature": 0.0, "avg_logprob": -0.1309843790733208, "compression_ratio": 1.5977443609022557, "no_speech_prob": 0.006752557586878538}, {"id": 190, "seek": 62840, "start": 632.3199999999999, "end": 634.64, "text": " the footprint to be lower, which makes sense,", "tokens": [50560, 264, 24222, 281, 312, 3126, 11, 597, 1669, 2020, 11, 50676], "temperature": 0.0, "avg_logprob": -0.1309843790733208, "compression_ratio": 1.5977443609022557, "no_speech_prob": 0.006752557586878538}, {"id": 191, "seek": 62840, "start": 634.64, "end": 636.88, "text": " but it broke the compatibility with tornado VM.", "tokens": [50676, 457, 309, 6902, 264, 34237, 365, 27935, 18038, 13, 50788], "temperature": 0.0, "avg_logprob": -0.1309843790733208, "compression_ratio": 1.5977443609022557, "no_speech_prob": 0.006752557586878538}, {"id": 192, "seek": 62840, "start": 637.68, "end": 641.1999999999999, "text": " The good part on this story is that GraL is very active.", "tokens": [50828, 440, 665, 644, 322, 341, 1657, 307, 300, 8985, 43, 307, 588, 4967, 13, 51004], "temperature": 0.0, "avg_logprob": -0.1309843790733208, "compression_ratio": 1.5977443609022557, "no_speech_prob": 0.006752557586878538}, {"id": 193, "seek": 62840, "start": 641.1999999999999, "end": 644.0, "text": " The GraL community is very active in Slack workspace,", "tokens": [51004, 440, 8985, 43, 1768, 307, 588, 4967, 294, 37211, 32706, 11, 51144], "temperature": 0.0, "avg_logprob": -0.1309843790733208, "compression_ratio": 1.5977443609022557, "no_speech_prob": 0.006752557586878538}, {"id": 194, "seek": 62840, "start": 644.0, "end": 647.84, "text": " so we managed to figure out what was the problem.", "tokens": [51144, 370, 321, 6453, 281, 2573, 484, 437, 390, 264, 1154, 13, 51336], "temperature": 0.0, "avg_logprob": -0.1309843790733208, "compression_ratio": 1.5977443609022557, "no_speech_prob": 0.006752557586878538}, {"id": 195, "seek": 62840, "start": 649.36, "end": 652.4, "text": " On the bad side is that the solution was to build", "tokens": [51412, 1282, 264, 1578, 1252, 307, 300, 264, 3827, 390, 281, 1322, 51564], "temperature": 0.0, "avg_logprob": -0.1309843790733208, "compression_ratio": 1.5977443609022557, "no_speech_prob": 0.006752557586878538}, {"id": 196, "seek": 62840, "start": 652.4, "end": 654.56, "text": " a GraL Pi and GraL VM from source,", "tokens": [51564, 257, 8985, 43, 17741, 293, 8985, 43, 18038, 490, 4009, 11, 51672], "temperature": 0.0, "avg_logprob": -0.1309843790733208, "compression_ratio": 1.5977443609022557, "no_speech_prob": 0.006752557586878538}, {"id": 197, "seek": 62840, "start": 654.56, "end": 656.4, "text": " which was quite painful.", "tokens": [51672, 597, 390, 1596, 11697, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1309843790733208, "compression_ratio": 1.5977443609022557, "no_speech_prob": 0.006752557586878538}, {"id": 198, "seek": 65640, "start": 657.04, "end": 660.3199999999999, "text": " And in order to avoid this pain for anyone", "tokens": [50396, 400, 294, 1668, 281, 5042, 341, 1822, 337, 2878, 50560], "temperature": 0.0, "avg_logprob": -0.0885435680173478, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.004598804749548435}, {"id": 199, "seek": 65640, "start": 660.3199999999999, "end": 661.84, "text": " who wants to try this work,", "tokens": [50560, 567, 2738, 281, 853, 341, 589, 11, 50636], "temperature": 0.0, "avg_logprob": -0.0885435680173478, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.004598804749548435}, {"id": 200, "seek": 65640, "start": 661.84, "end": 664.24, "text": " we decided to build a Docker image", "tokens": [50636, 321, 3047, 281, 1322, 257, 33772, 3256, 50756], "temperature": 0.0, "avg_logprob": -0.0885435680173478, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.004598804749548435}, {"id": 201, "seek": 65640, "start": 664.24, "end": 668.64, "text": " that has inside GraL Pi, tornado VM,", "tokens": [50756, 300, 575, 1854, 8985, 43, 17741, 11, 27935, 18038, 11, 50976], "temperature": 0.0, "avg_logprob": -0.0885435680173478, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.004598804749548435}, {"id": 202, "seek": 65640, "start": 668.64, "end": 671.04, "text": " and we have also added the NVIDIA driver.", "tokens": [50976, 293, 321, 362, 611, 3869, 264, 426, 3958, 6914, 6787, 13, 51096], "temperature": 0.0, "avg_logprob": -0.0885435680173478, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.004598804749548435}, {"id": 203, "seek": 65640, "start": 671.04, "end": 674.8, "text": " So if you have a Linux machine or any machine", "tokens": [51096, 407, 498, 291, 362, 257, 18734, 3479, 420, 604, 3479, 51284], "temperature": 0.0, "avg_logprob": -0.0885435680173478, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.004598804749548435}, {"id": 204, "seek": 65640, "start": 674.8, "end": 676.64, "text": " that has an NVIDIA GPU,", "tokens": [51284, 300, 575, 364, 426, 3958, 6914, 18407, 11, 51376], "temperature": 0.0, "avg_logprob": -0.0885435680173478, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.004598804749548435}, {"id": 205, "seek": 65640, "start": 676.64, "end": 680.3199999999999, "text": " and you have also the NVIDIA container toolkit in this machine,", "tokens": [51376, 293, 291, 362, 611, 264, 426, 3958, 6914, 10129, 40167, 294, 341, 3479, 11, 51560], "temperature": 0.0, "avg_logprob": -0.0885435680173478, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.004598804749548435}, {"id": 206, "seek": 65640, "start": 680.3199999999999, "end": 683.04, "text": " then you will be able to run this image.", "tokens": [51560, 550, 291, 486, 312, 1075, 281, 1190, 341, 3256, 13, 51696], "temperature": 0.0, "avg_logprob": -0.0885435680173478, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.004598804749548435}, {"id": 207, "seek": 68304, "start": 683.5999999999999, "end": 687.52, "text": " The Docker file, the image is open source in GitHub.", "tokens": [50392, 440, 33772, 3991, 11, 264, 3256, 307, 1269, 4009, 294, 23331, 13, 50588], "temperature": 0.0, "avg_logprob": -0.1265458250945469, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.0011739411856979132}, {"id": 208, "seek": 68304, "start": 688.56, "end": 691.36, "text": " And on the other side,", "tokens": [50640, 400, 322, 264, 661, 1252, 11, 50780], "temperature": 0.0, "avg_logprob": -0.1265458250945469, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.0011739411856979132}, {"id": 209, "seek": 68304, "start": 691.36, "end": 694.16, "text": " you can see the QR code that has the acceleration library.", "tokens": [50780, 291, 393, 536, 264, 32784, 3089, 300, 575, 264, 17162, 6405, 13, 50920], "temperature": 0.0, "avg_logprob": -0.1265458250945469, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.0011739411856979132}, {"id": 210, "seek": 68304, "start": 694.16, "end": 696.8, "text": " So the code that we have implemented", "tokens": [50920, 407, 264, 3089, 300, 321, 362, 12270, 51052], "temperature": 0.0, "avg_logprob": -0.1265458250945469, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.0011739411856979132}, {"id": 211, "seek": 68304, "start": 696.8, "end": 699.76, "text": " in the examples module of tornado VM", "tokens": [51052, 294, 264, 5110, 10088, 295, 27935, 18038, 51200], "temperature": 0.0, "avg_logprob": -0.1265458250945469, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.0011739411856979132}, {"id": 212, "seek": 68304, "start": 701.1999999999999, "end": 704.0, "text": " for the computation part that we will upload on the GPU,", "tokens": [51272, 337, 264, 24903, 644, 300, 321, 486, 6580, 322, 264, 18407, 11, 51412], "temperature": 0.0, "avg_logprob": -0.1265458250945469, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.0011739411856979132}, {"id": 213, "seek": 68304, "start": 704.0, "end": 707.52, "text": " like K-means, matrix multiplication,", "tokens": [51412, 411, 591, 12, 1398, 599, 11, 8141, 27290, 11, 51588], "temperature": 0.0, "avg_logprob": -0.1265458250945469, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.0011739411856979132}, {"id": 214, "seek": 68304, "start": 707.52, "end": 708.7199999999999, "text": " and those are the examples.", "tokens": [51588, 293, 729, 366, 264, 5110, 13, 51648], "temperature": 0.0, "avg_logprob": -0.1265458250945469, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.0011739411856979132}, {"id": 215, "seek": 68304, "start": 708.7199999999999, "end": 710.7199999999999, "text": " But there are also other compute examples", "tokens": [51648, 583, 456, 366, 611, 661, 14722, 5110, 51748], "temperature": 0.0, "avg_logprob": -0.1265458250945469, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.0011739411856979132}, {"id": 216, "seek": 68304, "start": 710.7199999999999, "end": 712.24, "text": " that we have in the GitHub.", "tokens": [51748, 300, 321, 362, 294, 264, 23331, 13, 51824], "temperature": 0.0, "avg_logprob": -0.1265458250945469, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.0011739411856979132}, {"id": 217, "seek": 71304, "start": 713.52, "end": 716.16, "text": " And you can also pull the Docker image from Docker Hub.", "tokens": [50388, 400, 291, 393, 611, 2235, 264, 33772, 3256, 490, 33772, 18986, 13, 50520], "temperature": 0.0, "avg_logprob": -0.09586548503441146, "compression_ratio": 1.6243093922651934, "no_speech_prob": 0.00314525468274951}, {"id": 218, "seek": 71304, "start": 718.8, "end": 721.36, "text": " So we will jump into the examples.", "tokens": [50652, 407, 321, 486, 3012, 666, 264, 5110, 13, 50780], "temperature": 0.0, "avg_logprob": -0.09586548503441146, "compression_ratio": 1.6243093922651934, "no_speech_prob": 0.00314525468274951}, {"id": 219, "seek": 71304, "start": 723.28, "end": 724.56, "text": " So as you see here,", "tokens": [50876, 407, 382, 291, 536, 510, 11, 50940], "temperature": 0.0, "avg_logprob": -0.09586548503441146, "compression_ratio": 1.6243093922651934, "no_speech_prob": 0.00314525468274951}, {"id": 220, "seek": 71304, "start": 724.56, "end": 728.8, "text": " we have the Python and Java with tornado VM.", "tokens": [50940, 321, 362, 264, 15329, 293, 10745, 365, 27935, 18038, 13, 51152], "temperature": 0.0, "avg_logprob": -0.09586548503441146, "compression_ratio": 1.6243093922651934, "no_speech_prob": 0.00314525468274951}, {"id": 221, "seek": 71304, "start": 728.8, "end": 732.88, "text": " So we have the Python program that imports Java,", "tokens": [51152, 407, 321, 362, 264, 15329, 1461, 300, 41596, 10745, 11, 51356], "temperature": 0.0, "avg_logprob": -0.09586548503441146, "compression_ratio": 1.6243093922651934, "no_speech_prob": 0.00314525468274951}, {"id": 222, "seek": 71304, "start": 732.88, "end": 737.4399999999999, "text": " and then it loads the class from the compute examples class", "tokens": [51356, 293, 550, 309, 12668, 264, 1508, 490, 264, 14722, 5110, 1508, 51584], "temperature": 0.0, "avg_logprob": -0.09586548503441146, "compression_ratio": 1.6243093922651934, "no_speech_prob": 0.00314525468274951}, {"id": 223, "seek": 71304, "start": 737.4399999999999, "end": 740.3199999999999, "text": " of the tornado VM repository.", "tokens": [51584, 295, 264, 27935, 18038, 25841, 13, 51728], "temperature": 0.0, "avg_logprob": -0.09586548503441146, "compression_ratio": 1.6243093922651934, "no_speech_prob": 0.00314525468274951}, {"id": 224, "seek": 74032, "start": 741.0400000000001, "end": 744.08, "text": " And then we have in this Java class that we have loaded,", "tokens": [50400, 400, 550, 321, 362, 294, 341, 10745, 1508, 300, 321, 362, 13210, 11, 50552], "temperature": 0.0, "avg_logprob": -0.10372671091331626, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0031999857164919376}, {"id": 225, "seek": 74032, "start": 744.08, "end": 748.0, "text": " we have two methods that can be accessed by the Python program.", "tokens": [50552, 321, 362, 732, 7150, 300, 393, 312, 34211, 538, 264, 15329, 1461, 13, 50748], "temperature": 0.0, "avg_logprob": -0.10372671091331626, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0031999857164919376}, {"id": 226, "seek": 74032, "start": 748.5600000000001, "end": 750.24, "text": " The first one is the set inputs", "tokens": [50776, 440, 700, 472, 307, 264, 992, 15743, 50860], "temperature": 0.0, "avg_logprob": -0.10372671091331626, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0031999857164919376}, {"id": 227, "seek": 74032, "start": 750.24, "end": 752.96, "text": " that set the actual data points", "tokens": [50860, 300, 992, 264, 3539, 1412, 2793, 50996], "temperature": 0.0, "avg_logprob": -0.10372671091331626, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0031999857164919376}, {"id": 228, "seek": 74032, "start": 752.96, "end": 756.24, "text": " and the number of packets that will be used for K-means.", "tokens": [50996, 293, 264, 1230, 295, 30364, 300, 486, 312, 1143, 337, 591, 12, 1398, 599, 13, 51160], "temperature": 0.0, "avg_logprob": -0.10372671091331626, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0031999857164919376}, {"id": 229, "seek": 74032, "start": 756.24, "end": 758.88, "text": " And the second one is the run with GPU.", "tokens": [51160, 400, 264, 1150, 472, 307, 264, 1190, 365, 18407, 13, 51292], "temperature": 0.0, "avg_logprob": -0.10372671091331626, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0031999857164919376}, {"id": 230, "seek": 74032, "start": 758.88, "end": 763.5200000000001, "text": " So this will invoke the actual GPU compilation", "tokens": [51292, 407, 341, 486, 41117, 264, 3539, 18407, 40261, 51524], "temperature": 0.0, "avg_logprob": -0.10372671091331626, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0031999857164919376}, {"id": 231, "seek": 74032, "start": 763.5200000000001, "end": 765.44, "text": " for GPUs and the GPU execution.", "tokens": [51524, 337, 18407, 82, 293, 264, 18407, 15058, 13, 51620], "temperature": 0.0, "avg_logprob": -0.10372671091331626, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0031999857164919376}, {"id": 232, "seek": 74032, "start": 766.96, "end": 768.08, "text": " And on the other side,", "tokens": [51696, 400, 322, 264, 661, 1252, 11, 51752], "temperature": 0.0, "avg_logprob": -0.10372671091331626, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0031999857164919376}, {"id": 233, "seek": 76808, "start": 768.1600000000001, "end": 770.1600000000001, "text": " we have the Java tornado VM,", "tokens": [50368, 321, 362, 264, 10745, 27935, 18038, 11, 50468], "temperature": 0.0, "avg_logprob": -0.13257852012728466, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.0006052011740393937}, {"id": 234, "seek": 76808, "start": 770.1600000000001, "end": 773.5200000000001, "text": " where we use Java and the tornado VM API", "tokens": [50468, 689, 321, 764, 10745, 293, 264, 27935, 18038, 9362, 50636], "temperature": 0.0, "avg_logprob": -0.13257852012728466, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.0006052011740393937}, {"id": 235, "seek": 76808, "start": 773.5200000000001, "end": 777.0400000000001, "text": " to create these parallel implementations of K-means.", "tokens": [50636, 281, 1884, 613, 8952, 4445, 763, 295, 591, 12, 1398, 599, 13, 50812], "temperature": 0.0, "avg_logprob": -0.13257852012728466, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.0006052011740393937}, {"id": 236, "seek": 76808, "start": 780.88, "end": 784.4000000000001, "text": " In this slide, you see, well, the steps,", "tokens": [51004, 682, 341, 4137, 11, 291, 536, 11, 731, 11, 264, 4439, 11, 51180], "temperature": 0.0, "avg_logprob": -0.13257852012728466, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.0006052011740393937}, {"id": 237, "seek": 76808, "start": 784.4000000000001, "end": 789.2, "text": " how to clone the repository that contains this Python program.", "tokens": [51180, 577, 281, 26506, 264, 25841, 300, 8306, 341, 15329, 1461, 13, 51420], "temperature": 0.0, "avg_logprob": -0.13257852012728466, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.0006052011740393937}, {"id": 238, "seek": 76808, "start": 789.2, "end": 792.6400000000001, "text": " And we see also the Python program, the K-means.py.", "tokens": [51420, 400, 321, 536, 611, 264, 15329, 1461, 11, 264, 591, 12, 1398, 599, 13, 8200, 13, 51592], "temperature": 0.0, "avg_logprob": -0.13257852012728466, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.0006052011740393937}, {"id": 239, "seek": 79264, "start": 793.36, "end": 798.88, "text": " So we see here beneath that we have the invocation", "tokens": [50400, 407, 321, 536, 510, 17149, 300, 321, 362, 264, 1048, 27943, 50676], "temperature": 0.0, "avg_logprob": -0.1412311318802507, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.001617751782760024}, {"id": 240, "seek": 79264, "start": 798.88, "end": 804.48, "text": " of the actual method functions, Java methods, sorry.", "tokens": [50676, 295, 264, 3539, 3170, 6828, 11, 10745, 7150, 11, 2597, 13, 50956], "temperature": 0.0, "avg_logprob": -0.1412311318802507, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.001617751782760024}, {"id": 241, "seek": 79264, "start": 806.16, "end": 809.76, "text": " And here is the link for the Java implementation of K-means.", "tokens": [51040, 400, 510, 307, 264, 2113, 337, 264, 10745, 11420, 295, 591, 12, 1398, 599, 13, 51220], "temperature": 0.0, "avg_logprob": -0.1412311318802507, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.001617751782760024}, {"id": 242, "seek": 79264, "start": 811.36, "end": 813.52, "text": " And now if we jump into the Java part,", "tokens": [51300, 400, 586, 498, 321, 3012, 666, 264, 10745, 644, 11, 51408], "temperature": 0.0, "avg_logprob": -0.1412311318802507, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.001617751782760024}, {"id": 243, "seek": 79264, "start": 813.52, "end": 817.04, "text": " which contains the computation that will be offloaded on the GPU.", "tokens": [51408, 597, 8306, 264, 24903, 300, 486, 312, 766, 2907, 292, 322, 264, 18407, 13, 51584], "temperature": 0.0, "avg_logprob": -0.1412311318802507, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.001617751782760024}, {"id": 244, "seek": 81704, "start": 817.04, "end": 823.28, "text": " No, before we jump to the computation,", "tokens": [50364, 883, 11, 949, 321, 3012, 281, 264, 24903, 11, 50676], "temperature": 0.0, "avg_logprob": -0.1289786873282967, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.0005645251367241144}, {"id": 245, "seek": 81704, "start": 823.28, "end": 826.56, "text": " we have the set inputs and I wanted to make a connection", "tokens": [50676, 321, 362, 264, 992, 15743, 293, 286, 1415, 281, 652, 257, 4984, 50840], "temperature": 0.0, "avg_logprob": -0.1289786873282967, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.0005645251367241144}, {"id": 246, "seek": 81704, "start": 826.56, "end": 829.04, "text": " to reflect on the off-heap data types.", "tokens": [50840, 281, 5031, 322, 264, 766, 12, 675, 569, 1412, 3467, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1289786873282967, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.0005645251367241144}, {"id": 247, "seek": 81704, "start": 829.04, "end": 832.3199999999999, "text": " So with these two, with a new vector float,", "tokens": [50964, 407, 365, 613, 732, 11, 365, 257, 777, 8062, 15706, 11, 51128], "temperature": 0.0, "avg_logprob": -0.1289786873282967, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.0005645251367241144}, {"id": 248, "seek": 81704, "start": 832.3199999999999, "end": 836.8, "text": " this is an API type that is exposed by tornado VM", "tokens": [51128, 341, 307, 364, 9362, 2010, 300, 307, 9495, 538, 27935, 18038, 51352], "temperature": 0.0, "avg_logprob": -0.1289786873282967, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.0005645251367241144}, {"id": 249, "seek": 81704, "start": 836.8, "end": 840.48, "text": " and can allocate data vector types off-heap.", "tokens": [51352, 293, 393, 35713, 1412, 8062, 3467, 766, 12, 675, 569, 13, 51536], "temperature": 0.0, "avg_logprob": -0.1289786873282967, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.0005645251367241144}, {"id": 250, "seek": 81704, "start": 841.52, "end": 843.76, "text": " And then we'll have the create matrix of clusters", "tokens": [51588, 400, 550, 321, 603, 362, 264, 1884, 8141, 295, 23313, 51700], "temperature": 0.0, "avg_logprob": -0.1289786873282967, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.0005645251367241144}, {"id": 251, "seek": 84376, "start": 843.76, "end": 847.52, "text": " that does perform some initialization of the objects", "tokens": [50364, 300, 775, 2042, 512, 5883, 2144, 295, 264, 6565, 50552], "temperature": 0.0, "avg_logprob": -0.10157067435128349, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0007514822063967586}, {"id": 252, "seek": 84376, "start": 847.52, "end": 851.68, "text": " and also allocate some other data, like the clusters,", "tokens": [50552, 293, 611, 35713, 512, 661, 1412, 11, 411, 264, 23313, 11, 50760], "temperature": 0.0, "avg_logprob": -0.10157067435128349, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0007514822063967586}, {"id": 253, "seek": 84376, "start": 851.68, "end": 855.04, "text": " which are going to be allocated off-heap as well.", "tokens": [50760, 597, 366, 516, 281, 312, 29772, 766, 12, 675, 569, 382, 731, 13, 50928], "temperature": 0.0, "avg_logprob": -0.10157067435128349, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0007514822063967586}, {"id": 254, "seek": 84376, "start": 856.3199999999999, "end": 859.68, "text": " And now we are ready to move into the actual computation part.", "tokens": [50992, 400, 586, 321, 366, 1919, 281, 1286, 666, 264, 3539, 24903, 644, 13, 51160], "temperature": 0.0, "avg_logprob": -0.10157067435128349, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0007514822063967586}, {"id": 255, "seek": 84376, "start": 859.68, "end": 864.0, "text": " So on the left side, you see the run with Java implementation", "tokens": [51160, 407, 322, 264, 1411, 1252, 11, 291, 536, 264, 1190, 365, 10745, 11420, 51376], "temperature": 0.0, "avg_logprob": -0.10157067435128349, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0007514822063967586}, {"id": 256, "seek": 84376, "start": 864.0, "end": 864.8, "text": " of this method.", "tokens": [51376, 295, 341, 3170, 13, 51416], "temperature": 0.0, "avg_logprob": -0.10157067435128349, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0007514822063967586}, {"id": 257, "seek": 84376, "start": 865.4399999999999, "end": 868.24, "text": " And on the right side, you see the accelerated one", "tokens": [51448, 400, 322, 264, 558, 1252, 11, 291, 536, 264, 29763, 472, 51588], "temperature": 0.0, "avg_logprob": -0.10157067435128349, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0007514822063967586}, {"id": 258, "seek": 84376, "start": 868.24, "end": 871.04, "text": " with the tornado VM API.", "tokens": [51588, 365, 264, 27935, 18038, 9362, 13, 51728], "temperature": 0.0, "avg_logprob": -0.10157067435128349, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0007514822063967586}, {"id": 259, "seek": 87104, "start": 871.36, "end": 875.04, "text": " So as we see here, the actual computation has been", "tokens": [50380, 407, 382, 321, 536, 510, 11, 264, 3539, 24903, 575, 668, 50564], "temperature": 0.0, "avg_logprob": -0.26052175521850585, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0007876704912632704}, {"id": 260, "seek": 87104, "start": 875.04, "end": 877.92, "text": " in this method, has been performed by this method.", "tokens": [50564, 294, 341, 3170, 11, 575, 668, 10332, 538, 341, 3170, 13, 50708], "temperature": 0.0, "avg_logprob": -0.26052175521850585, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0007876704912632704}, {"id": 261, "seek": 87104, "start": 877.92, "end": 879.36, "text": " So they assign clusters.", "tokens": [50708, 407, 436, 6269, 23313, 13, 50780], "temperature": 0.0, "avg_logprob": -0.26052175521850585, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0007876704912632704}, {"id": 262, "seek": 87104, "start": 880.48, "end": 882.8, "text": " And the corresponding one on the right side,", "tokens": [50836, 400, 264, 11760, 472, 322, 264, 558, 1252, 11, 50952], "temperature": 0.0, "avg_logprob": -0.26052175521850585, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0007876704912632704}, {"id": 263, "seek": 87104, "start": 882.8, "end": 888.24, "text": " that is the tornado VM implementation, is this one.", "tokens": [50952, 300, 307, 264, 27935, 18038, 11420, 11, 307, 341, 472, 13, 51224], "temperature": 0.0, "avg_logprob": -0.26052175521850585, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0007876704912632704}, {"id": 264, "seek": 87104, "start": 888.24, "end": 893.04, "text": " So in this one, I would like to focus on two parts.", "tokens": [51224, 407, 294, 341, 472, 11, 286, 576, 411, 281, 1879, 322, 732, 3166, 13, 51464], "temperature": 0.0, "avg_logprob": -0.26052175521850585, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0007876704912632704}, {"id": 265, "seek": 87104, "start": 893.04, "end": 895.8399999999999, "text": " So you can see the task graph implementation.", "tokens": [51464, 407, 291, 393, 536, 264, 5633, 4295, 11420, 13, 51604], "temperature": 0.0, "avg_logprob": -0.26052175521850585, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0007876704912632704}, {"id": 266, "seek": 87104, "start": 895.8399999999999, "end": 899.4399999999999, "text": " Task graph is an object exposed by the tornado VM API.", "tokens": [51604, 30428, 4295, 307, 364, 2657, 9495, 538, 264, 27935, 18038, 9362, 13, 51784], "temperature": 0.0, "avg_logprob": -0.26052175521850585, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0007876704912632704}, {"id": 267, "seek": 89944, "start": 900.4000000000001, "end": 903.6800000000001, "text": " In a sense, task graph enables you to define", "tokens": [50412, 682, 257, 2020, 11, 5633, 4295, 17077, 291, 281, 6964, 50576], "temperature": 0.0, "avg_logprob": -0.22713913236345565, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.0013609807938337326}, {"id": 268, "seek": 89944, "start": 903.6800000000001, "end": 906.1600000000001, "text": " what code will go to the GPU.", "tokens": [50576, 437, 3089, 486, 352, 281, 264, 18407, 13, 50700], "temperature": 0.0, "avg_logprob": -0.22713913236345565, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.0013609807938337326}, {"id": 269, "seek": 89944, "start": 906.1600000000001, "end": 908.48, "text": " So what's going to be the actual computation", "tokens": [50700, 407, 437, 311, 516, 281, 312, 264, 3539, 24903, 50816], "temperature": 0.0, "avg_logprob": -0.22713913236345565, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.0013609807938337326}, {"id": 270, "seek": 89944, "start": 909.12, "end": 912.8000000000001, "text": " and what data should be used on the GPU.", "tokens": [50848, 293, 437, 1412, 820, 312, 1143, 322, 264, 18407, 13, 51032], "temperature": 0.0, "avg_logprob": -0.22713913236345565, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.0013609807938337326}, {"id": 271, "seek": 89944, "start": 912.8000000000001, "end": 915.2, "text": " So the input data and the output data.", "tokens": [51032, 407, 264, 4846, 1412, 293, 264, 5598, 1412, 13, 51152], "temperature": 0.0, "avg_logprob": -0.22713913236345565, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.0013609807938337326}, {"id": 272, "seek": 89944, "start": 915.2, "end": 918.1600000000001, "text": " So in a sense, the task graph enables programmers", "tokens": [51152, 407, 294, 257, 2020, 11, 264, 5633, 4295, 17077, 41504, 51300], "temperature": 0.0, "avg_logprob": -0.22713913236345565, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.0013609807938337326}, {"id": 273, "seek": 89944, "start": 918.1600000000001, "end": 922.24, "text": " to define what is going to go to the GPU for execution.", "tokens": [51300, 281, 6964, 437, 307, 516, 281, 352, 281, 264, 18407, 337, 15058, 13, 51504], "temperature": 0.0, "avg_logprob": -0.22713913236345565, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.0013609807938337326}, {"id": 274, "seek": 89944, "start": 923.44, "end": 926.0, "text": " And the second API, once we have done this,", "tokens": [51564, 400, 264, 1150, 9362, 11, 1564, 321, 362, 1096, 341, 11, 51692], "temperature": 0.0, "avg_logprob": -0.22713913236345565, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.0013609807938337326}, {"id": 275, "seek": 89944, "start": 926.0, "end": 928.96, "text": " as you can see here, we can define also the data", "tokens": [51692, 382, 291, 393, 536, 510, 11, 321, 393, 6964, 611, 264, 1412, 51840], "temperature": 0.0, "avg_logprob": -0.22713913236345565, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.0013609807938337326}, {"id": 276, "seek": 92896, "start": 929.2, "end": 932.24, "text": " transfer mode, how often we want data input,", "tokens": [50376, 5003, 4391, 11, 577, 2049, 321, 528, 1412, 4846, 11, 50528], "temperature": 0.0, "avg_logprob": -0.10191087253758165, "compression_ratio": 1.8366533864541832, "no_speech_prob": 0.0014110232004895806}, {"id": 277, "seek": 92896, "start": 932.24, "end": 935.76, "text": " input data or output data to be copied back and forth", "tokens": [50528, 4846, 1412, 420, 5598, 1412, 281, 312, 25365, 646, 293, 5220, 50704], "temperature": 0.0, "avg_logprob": -0.10191087253758165, "compression_ratio": 1.8366533864541832, "no_speech_prob": 0.0014110232004895806}, {"id": 278, "seek": 92896, "start": 935.76, "end": 937.0400000000001, "text": " from the GPU.", "tokens": [50704, 490, 264, 18407, 13, 50768], "temperature": 0.0, "avg_logprob": -0.10191087253758165, "compression_ratio": 1.8366533864541832, "no_speech_prob": 0.0014110232004895806}, {"id": 279, "seek": 92896, "start": 937.0400000000001, "end": 939.9200000000001, "text": " And once we have defined that, we can move to the second part,", "tokens": [50768, 400, 1564, 321, 362, 7642, 300, 11, 321, 393, 1286, 281, 264, 1150, 644, 11, 50912], "temperature": 0.0, "avg_logprob": -0.10191087253758165, "compression_ratio": 1.8366533864541832, "no_speech_prob": 0.0014110232004895806}, {"id": 280, "seek": 92896, "start": 939.9200000000001, "end": 941.76, "text": " which is the execution plan.", "tokens": [50912, 597, 307, 264, 15058, 1393, 13, 51004], "temperature": 0.0, "avg_logprob": -0.10191087253758165, "compression_ratio": 1.8366533864541832, "no_speech_prob": 0.0014110232004895806}, {"id": 281, "seek": 92896, "start": 941.76, "end": 944.1600000000001, "text": " So the execution plan is another object", "tokens": [51004, 407, 264, 15058, 1393, 307, 1071, 2657, 51124], "temperature": 0.0, "avg_logprob": -0.10191087253758165, "compression_ratio": 1.8366533864541832, "no_speech_prob": 0.0014110232004895806}, {"id": 282, "seek": 92896, "start": 944.1600000000001, "end": 946.1600000000001, "text": " that enables programmers to define", "tokens": [51124, 300, 17077, 41504, 281, 6964, 51224], "temperature": 0.0, "avg_logprob": -0.10191087253758165, "compression_ratio": 1.8366533864541832, "no_speech_prob": 0.0014110232004895806}, {"id": 283, "seek": 92896, "start": 946.88, "end": 949.12, "text": " how the execution will take place.", "tokens": [51260, 577, 264, 15058, 486, 747, 1081, 13, 51372], "temperature": 0.0, "avg_logprob": -0.10191087253758165, "compression_ratio": 1.8366533864541832, "no_speech_prob": 0.0014110232004895806}, {"id": 284, "seek": 92896, "start": 949.12, "end": 952.08, "text": " So it could be, for example, with the profiler enabled,", "tokens": [51372, 407, 309, 727, 312, 11, 337, 1365, 11, 365, 264, 1740, 5441, 15172, 11, 51520], "temperature": 0.0, "avg_logprob": -0.10191087253758165, "compression_ratio": 1.8366533864541832, "no_speech_prob": 0.0014110232004895806}, {"id": 285, "seek": 92896, "start": 952.08, "end": 955.52, "text": " without the profiler enabled, with a custom grid size,", "tokens": [51520, 1553, 264, 1740, 5441, 15172, 11, 365, 257, 2375, 10748, 2744, 11, 51692], "temperature": 0.0, "avg_logprob": -0.10191087253758165, "compression_ratio": 1.8366533864541832, "no_speech_prob": 0.0014110232004895806}, {"id": 286, "seek": 92896, "start": 956.5600000000001, "end": 958.5600000000001, "text": " which is defined by the programmer.", "tokens": [51744, 597, 307, 7642, 538, 264, 32116, 13, 51844], "temperature": 0.0, "avg_logprob": -0.10191087253758165, "compression_ratio": 1.8366533864541832, "no_speech_prob": 0.0014110232004895806}, {"id": 287, "seek": 95856, "start": 958.64, "end": 963.5999999999999, "text": " And once we have defined how the execution will be done,", "tokens": [50368, 400, 1564, 321, 362, 7642, 577, 264, 15058, 486, 312, 1096, 11, 50616], "temperature": 0.0, "avg_logprob": -0.1096963988410102, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0009281745878979564}, {"id": 288, "seek": 95856, "start": 963.5999999999999, "end": 969.04, "text": " will be performed, we are able to execute the actual task graph.", "tokens": [50616, 486, 312, 10332, 11, 321, 366, 1075, 281, 14483, 264, 3539, 5633, 4295, 13, 50888], "temperature": 0.0, "avg_logprob": -0.1096963988410102, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0009281745878979564}, {"id": 289, "seek": 95856, "start": 969.04, "end": 972.16, "text": " So with execution.execute, it is this part", "tokens": [50888, 407, 365, 15058, 13, 3121, 3045, 1169, 11, 309, 307, 341, 644, 51044], "temperature": 0.0, "avg_logprob": -0.1096963988410102, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0009281745878979564}, {"id": 290, "seek": 95856, "start": 972.7199999999999, "end": 976.3199999999999, "text": " that enables the actual execution of the code", "tokens": [51072, 300, 17077, 264, 3539, 15058, 295, 264, 3089, 51252], "temperature": 0.0, "avg_logprob": -0.1096963988410102, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0009281745878979564}, {"id": 291, "seek": 95856, "start": 976.3199999999999, "end": 977.4399999999999, "text": " and the GIT compilation.", "tokens": [51252, 293, 264, 460, 3927, 40261, 13, 51308], "temperature": 0.0, "avg_logprob": -0.1096963988410102, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0009281745878979564}, {"id": 292, "seek": 95856, "start": 978.4799999999999, "end": 984.3199999999999, "text": " So the second time that we will execute the assigned clusters,", "tokens": [51360, 407, 264, 1150, 565, 300, 321, 486, 14483, 264, 13279, 23313, 11, 51652], "temperature": 0.0, "avg_logprob": -0.1096963988410102, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0009281745878979564}, {"id": 293, "seek": 95856, "start": 984.3199999999999, "end": 986.0, "text": " well, this is going to be the second time", "tokens": [51652, 731, 11, 341, 307, 516, 281, 312, 264, 1150, 565, 51736], "temperature": 0.0, "avg_logprob": -0.1096963988410102, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0009281745878979564}, {"id": 294, "seek": 98600, "start": 986.0, "end": 989.6, "text": " that we invoke the actual execute of the execution plan.", "tokens": [50364, 300, 321, 41117, 264, 3539, 14483, 295, 264, 15058, 1393, 13, 50544], "temperature": 0.0, "avg_logprob": -0.12498890269886363, "compression_ratio": 1.8798076923076923, "no_speech_prob": 0.0007174095371738076}, {"id": 295, "seek": 98600, "start": 990.72, "end": 993.84, "text": " And the second time that we will invoke the execution plan,", "tokens": [50600, 400, 264, 1150, 565, 300, 321, 486, 41117, 264, 15058, 1393, 11, 50756], "temperature": 0.0, "avg_logprob": -0.12498890269886363, "compression_ratio": 1.8798076923076923, "no_speech_prob": 0.0007174095371738076}, {"id": 296, "seek": 98600, "start": 993.84, "end": 995.84, "text": " the execution of the execution plan,", "tokens": [50756, 264, 15058, 295, 264, 15058, 1393, 11, 50856], "temperature": 0.0, "avg_logprob": -0.12498890269886363, "compression_ratio": 1.8798076923076923, "no_speech_prob": 0.0007174095371738076}, {"id": 297, "seek": 98600, "start": 995.84, "end": 998.88, "text": " this is going to be the time that the code will not be GIT", "tokens": [50856, 341, 307, 516, 281, 312, 264, 565, 300, 264, 3089, 486, 406, 312, 460, 3927, 51008], "temperature": 0.0, "avg_logprob": -0.12498890269886363, "compression_ratio": 1.8798076923076923, "no_speech_prob": 0.0007174095371738076}, {"id": 298, "seek": 98600, "start": 998.88, "end": 1000.32, "text": " because it is already GIT.", "tokens": [51008, 570, 309, 307, 1217, 460, 3927, 13, 51080], "temperature": 0.0, "avg_logprob": -0.12498890269886363, "compression_ratio": 1.8798076923076923, "no_speech_prob": 0.0007174095371738076}, {"id": 299, "seek": 98600, "start": 1000.32, "end": 1003.28, "text": " So the code, the OpenCL code or the CUDA code", "tokens": [51080, 407, 264, 3089, 11, 264, 7238, 22458, 3089, 420, 264, 29777, 7509, 3089, 51228], "temperature": 0.0, "avg_logprob": -0.12498890269886363, "compression_ratio": 1.8798076923076923, "no_speech_prob": 0.0007174095371738076}, {"id": 300, "seek": 98600, "start": 1003.92, "end": 1008.16, "text": " will be all retrieved from the code cache of Tornado VM.", "tokens": [51260, 486, 312, 439, 19817, 937, 490, 264, 3089, 19459, 295, 314, 1865, 1573, 18038, 13, 51472], "temperature": 0.0, "avg_logprob": -0.12498890269886363, "compression_ratio": 1.8798076923076923, "no_speech_prob": 0.0007174095371738076}, {"id": 301, "seek": 98600, "start": 1010.72, "end": 1014.16, "text": " So now we can move to the actual example to run.", "tokens": [51600, 407, 586, 321, 393, 1286, 281, 264, 3539, 1365, 281, 1190, 13, 51772], "temperature": 0.0, "avg_logprob": -0.12498890269886363, "compression_ratio": 1.8798076923076923, "no_speech_prob": 0.0007174095371738076}, {"id": 302, "seek": 101416, "start": 1014.9599999999999, "end": 1018.88, "text": " I have recorded a video that enables the execution", "tokens": [50404, 286, 362, 8287, 257, 960, 300, 17077, 264, 15058, 50600], "temperature": 0.0, "avg_logprob": -0.294531452412508, "compression_ratio": 1.4524886877828054, "no_speech_prob": 0.0021530676167458296}, {"id": 303, "seek": 101416, "start": 1018.88, "end": 1020.9599999999999, "text": " of K-Means and MathExfoom.liblication", "tokens": [50600, 295, 591, 12, 12671, 599, 293, 15776, 11149, 16931, 298, 13, 75, 897, 1050, 399, 50704], "temperature": 0.0, "avg_logprob": -0.294531452412508, "compression_ratio": 1.4524886877828054, "no_speech_prob": 0.0021530676167458296}, {"id": 304, "seek": 101416, "start": 1020.9599999999999, "end": 1023.4399999999999, "text": " because on my MacBook, I don't have an NVIDIA GPU.", "tokens": [50704, 570, 322, 452, 31737, 11, 286, 500, 380, 362, 364, 426, 3958, 6914, 18407, 13, 50828], "temperature": 0.0, "avg_logprob": -0.294531452412508, "compression_ratio": 1.4524886877828054, "no_speech_prob": 0.0021530676167458296}, {"id": 305, "seek": 101416, "start": 1024.08, "end": 1028.8799999999999, "text": " So we will fork the actual repository with examples.", "tokens": [50860, 407, 321, 486, 17716, 264, 3539, 25841, 365, 5110, 13, 51100], "temperature": 0.0, "avg_logprob": -0.294531452412508, "compression_ratio": 1.4524886877828054, "no_speech_prob": 0.0021530676167458296}, {"id": 306, "seek": 101416, "start": 1031.44, "end": 1034.32, "text": " And now that we have forked, we will go inside,", "tokens": [51228, 400, 586, 300, 321, 362, 17716, 292, 11, 321, 486, 352, 1854, 11, 51372], "temperature": 0.0, "avg_logprob": -0.294531452412508, "compression_ratio": 1.4524886877828054, "no_speech_prob": 0.0021530676167458296}, {"id": 307, "seek": 101416, "start": 1035.04, "end": 1036.72, "text": " we check out the FOSDEM branch.", "tokens": [51408, 321, 1520, 484, 264, 479, 4367, 35, 6683, 9819, 13, 51492], "temperature": 0.0, "avg_logprob": -0.294531452412508, "compression_ratio": 1.4524886877828054, "no_speech_prob": 0.0021530676167458296}, {"id": 308, "seek": 101416, "start": 1041.52, "end": 1043.68, "text": " And this is the Python code that we saw earlier.", "tokens": [51732, 400, 341, 307, 264, 15329, 3089, 300, 321, 1866, 3071, 13, 51840], "temperature": 0.0, "avg_logprob": -0.294531452412508, "compression_ratio": 1.4524886877828054, "no_speech_prob": 0.0021530676167458296}, {"id": 309, "seek": 104416, "start": 1044.16, "end": 1045.28, "text": " So it has these three.", "tokens": [50364, 407, 309, 575, 613, 1045, 13, 50420], "temperature": 0.0, "avg_logprob": -0.15654784308539496, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.0027280536014586687}, {"id": 310, "seek": 104416, "start": 1045.92, "end": 1049.0400000000002, "text": " First, we load the class, and then we are able to invoke", "tokens": [50452, 2386, 11, 321, 3677, 264, 1508, 11, 293, 550, 321, 366, 1075, 281, 41117, 50608], "temperature": 0.0, "avg_logprob": -0.15654784308539496, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.0027280536014586687}, {"id": 311, "seek": 104416, "start": 1049.0400000000002, "end": 1050.72, "text": " the Java code from Python.", "tokens": [50608, 264, 10745, 3089, 490, 15329, 13, 50692], "temperature": 0.0, "avg_logprob": -0.15654784308539496, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.0027280536014586687}, {"id": 312, "seek": 104416, "start": 1052.5600000000002, "end": 1056.0, "text": " And here we will run, first, the Java implementation", "tokens": [50784, 400, 510, 321, 486, 1190, 11, 700, 11, 264, 10745, 11420, 50956], "temperature": 0.0, "avg_logprob": -0.15654784308539496, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.0027280536014586687}, {"id": 313, "seek": 104416, "start": 1056.0, "end": 1059.44, "text": " and then the GPU accelerated implementation.", "tokens": [50956, 293, 550, 264, 18407, 29763, 11420, 13, 51128], "temperature": 0.0, "avg_logprob": -0.15654784308539496, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.0027280536014586687}, {"id": 314, "seek": 104416, "start": 1060.5600000000002, "end": 1064.0, "text": " We can also pull the Docker image that we have created.", "tokens": [51184, 492, 393, 611, 2235, 264, 33772, 3256, 300, 321, 362, 2942, 13, 51356], "temperature": 0.0, "avg_logprob": -0.15654784308539496, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.0027280536014586687}, {"id": 315, "seek": 104416, "start": 1068.48, "end": 1071.44, "text": " And here in the repository, we have a launcher script", "tokens": [51580, 400, 510, 294, 264, 25841, 11, 321, 362, 257, 36805, 5755, 51728], "temperature": 0.0, "avg_logprob": -0.15654784308539496, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.0027280536014586687}, {"id": 316, "seek": 104416, "start": 1071.44, "end": 1073.0400000000002, "text": " that enables to run.", "tokens": [51728, 300, 17077, 281, 1190, 13, 51808], "temperature": 0.0, "avg_logprob": -0.15654784308539496, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.0027280536014586687}, {"id": 317, "seek": 107304, "start": 1073.04, "end": 1076.48, "text": " So at first, we will try the Tornado devices", "tokens": [50364, 407, 412, 700, 11, 321, 486, 853, 264, 314, 1865, 1573, 5759, 50536], "temperature": 0.0, "avg_logprob": -0.13111235113704905, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0022349380888044834}, {"id": 318, "seek": 107304, "start": 1076.48, "end": 1080.24, "text": " to query how many NVIDIA GPUs exist in the system.", "tokens": [50536, 281, 14581, 577, 867, 426, 3958, 6914, 18407, 82, 2514, 294, 264, 1185, 13, 50724], "temperature": 0.0, "avg_logprob": -0.13111235113704905, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0022349380888044834}, {"id": 319, "seek": 107304, "start": 1081.84, "end": 1087.04, "text": " And here it is the 2000 GPU that exists in my machine at home.", "tokens": [50804, 400, 510, 309, 307, 264, 8132, 18407, 300, 8198, 294, 452, 3479, 412, 1280, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13111235113704905, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0022349380888044834}, {"id": 320, "seek": 107304, "start": 1088.8799999999999, "end": 1091.44, "text": " And once we have done this, we will run with Truffle,", "tokens": [51156, 400, 1564, 321, 362, 1096, 341, 11, 321, 486, 1190, 365, 21388, 602, 306, 11, 51284], "temperature": 0.0, "avg_logprob": -0.13111235113704905, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0022349380888044834}, {"id": 321, "seek": 107304, "start": 1091.44, "end": 1092.3999999999999, "text": " the Python program.", "tokens": [51284, 264, 15329, 1461, 13, 51332], "temperature": 0.0, "avg_logprob": -0.13111235113704905, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0022349380888044834}, {"id": 322, "seek": 107304, "start": 1092.3999999999999, "end": 1096.32, "text": " So Tornado Truffle, the Truffle flag and Python,", "tokens": [51332, 407, 314, 1865, 1573, 21388, 602, 306, 11, 264, 21388, 602, 306, 7166, 293, 15329, 11, 51528], "temperature": 0.0, "avg_logprob": -0.13111235113704905, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0022349380888044834}, {"id": 323, "seek": 107304, "start": 1096.32, "end": 1099.12, "text": " will be able to run the actual Python program.", "tokens": [51528, 486, 312, 1075, 281, 1190, 264, 3539, 15329, 1461, 13, 51668], "temperature": 0.0, "avg_logprob": -0.13111235113704905, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0022349380888044834}, {"id": 324, "seek": 109912, "start": 1099.76, "end": 1103.36, "text": " And we will see here that at first,", "tokens": [50396, 400, 321, 486, 536, 510, 300, 412, 700, 11, 50576], "temperature": 0.0, "avg_logprob": -0.17225864085745304, "compression_ratio": 1.5980392156862746, "no_speech_prob": 0.0016140870284289122}, {"id": 325, "seek": 109912, "start": 1104.08, "end": 1106.2399999999998, "text": " it will bring Hello World from Python.", "tokens": [50612, 309, 486, 1565, 2425, 3937, 490, 15329, 13, 50720], "temperature": 0.0, "avg_logprob": -0.17225864085745304, "compression_ratio": 1.5980392156862746, "no_speech_prob": 0.0016140870284289122}, {"id": 326, "seek": 109912, "start": 1106.2399999999998, "end": 1108.8, "text": " And then we run the Java implementation,", "tokens": [50720, 400, 550, 321, 1190, 264, 10745, 11420, 11, 50848], "temperature": 0.0, "avg_logprob": -0.17225864085745304, "compression_ratio": 1.5980392156862746, "no_speech_prob": 0.0016140870284289122}, {"id": 327, "seek": 109912, "start": 1108.8, "end": 1111.6799999999998, "text": " which is a sequential, that I'm with Java.", "tokens": [50848, 597, 307, 257, 42881, 11, 300, 286, 478, 365, 10745, 13, 50992], "temperature": 0.0, "avg_logprob": -0.17225864085745304, "compression_ratio": 1.5980392156862746, "no_speech_prob": 0.0016140870284289122}, {"id": 328, "seek": 109912, "start": 1111.6799999999998, "end": 1113.84, "text": " And then they run with GPU method.", "tokens": [50992, 400, 550, 436, 1190, 365, 18407, 3170, 13, 51100], "temperature": 0.0, "avg_logprob": -0.17225864085745304, "compression_ratio": 1.5980392156862746, "no_speech_prob": 0.0016140870284289122}, {"id": 329, "seek": 109912, "start": 1114.3999999999999, "end": 1117.6, "text": " And as we see here, they take the first one, one second,", "tokens": [51128, 400, 382, 321, 536, 510, 11, 436, 747, 264, 700, 472, 11, 472, 1150, 11, 51288], "temperature": 0.0, "avg_logprob": -0.17225864085745304, "compression_ratio": 1.5980392156862746, "no_speech_prob": 0.0016140870284289122}, {"id": 330, "seek": 109912, "start": 1117.6, "end": 1120.7199999999998, "text": " and the second one, 140 milliseconds.", "tokens": [51288, 293, 264, 1150, 472, 11, 21548, 34184, 13, 51444], "temperature": 0.0, "avg_logprob": -0.17225864085745304, "compression_ratio": 1.5980392156862746, "no_speech_prob": 0.0016140870284289122}, {"id": 331, "seek": 109912, "start": 1125.1999999999998, "end": 1128.3999999999999, "text": " So here we will try the same example,", "tokens": [51668, 407, 510, 321, 486, 853, 264, 912, 1365, 11, 51828], "temperature": 0.0, "avg_logprob": -0.17225864085745304, "compression_ratio": 1.5980392156862746, "no_speech_prob": 0.0016140870284289122}, {"id": 332, "seek": 112840, "start": 1128.4, "end": 1132.88, "text": " but with the thread info, which will enable the printing", "tokens": [50364, 457, 365, 264, 7207, 13614, 11, 597, 486, 9528, 264, 14699, 50588], "temperature": 0.0, "avg_logprob": -0.08977340829783473, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0010731284273788333}, {"id": 333, "seek": 112840, "start": 1132.88, "end": 1136.0800000000002, "text": " of the actual threads that have been used on the GPU.", "tokens": [50588, 295, 264, 3539, 19314, 300, 362, 668, 1143, 322, 264, 18407, 13, 50748], "temperature": 0.0, "avg_logprob": -0.08977340829783473, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0010731284273788333}, {"id": 334, "seek": 112840, "start": 1136.0800000000002, "end": 1139.2, "text": " So as we see here, we have the number of data points", "tokens": [50748, 407, 382, 321, 536, 510, 11, 321, 362, 264, 1230, 295, 1412, 2793, 50904], "temperature": 0.0, "avg_logprob": -0.08977340829783473, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0010731284273788333}, {"id": 335, "seek": 112840, "start": 1139.2, "end": 1141.2, "text": " that we passed with the set input.", "tokens": [50904, 300, 321, 4678, 365, 264, 992, 4846, 13, 51004], "temperature": 0.0, "avg_logprob": -0.08977340829783473, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0010731284273788333}, {"id": 336, "seek": 112840, "start": 1141.2, "end": 1144.3200000000002, "text": " It has been the number of the global thread size", "tokens": [51004, 467, 575, 668, 264, 1230, 295, 264, 4338, 7207, 2744, 51160], "temperature": 0.0, "avg_logprob": -0.08977340829783473, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0010731284273788333}, {"id": 337, "seek": 112840, "start": 1144.3200000000002, "end": 1145.8400000000001, "text": " that is uploaded on the GPU.", "tokens": [51160, 300, 307, 17135, 322, 264, 18407, 13, 51236], "temperature": 0.0, "avg_logprob": -0.08977340829783473, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0010731284273788333}, {"id": 338, "seek": 112840, "start": 1146.5600000000002, "end": 1148.48, "text": " And now we move to the second example,", "tokens": [51272, 400, 586, 321, 1286, 281, 264, 1150, 1365, 11, 51368], "temperature": 0.0, "avg_logprob": -0.08977340829783473, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0010731284273788333}, {"id": 339, "seek": 112840, "start": 1148.48, "end": 1152.0, "text": " which is the matrix multiplication with Tornado VM.", "tokens": [51368, 597, 307, 264, 8141, 27290, 365, 314, 1865, 1573, 18038, 13, 51544], "temperature": 0.0, "avg_logprob": -0.08977340829783473, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0010731284273788333}, {"id": 340, "seek": 112840, "start": 1152.0, "end": 1155.1200000000001, "text": " So in this example, we run five times", "tokens": [51544, 407, 294, 341, 1365, 11, 321, 1190, 1732, 1413, 51700], "temperature": 0.0, "avg_logprob": -0.08977340829783473, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0010731284273788333}, {"id": 341, "seek": 112840, "start": 1155.1200000000001, "end": 1156.8000000000002, "text": " the matrix multiplication.", "tokens": [51700, 264, 8141, 27290, 13, 51784], "temperature": 0.0, "avg_logprob": -0.08977340829783473, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0010731284273788333}, {"id": 342, "seek": 115680, "start": 1156.8, "end": 1159.04, "text": " So we see here the execution time", "tokens": [50364, 407, 321, 536, 510, 264, 15058, 565, 50476], "temperature": 0.0, "avg_logprob": -0.11208970923172801, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0007052086293697357}, {"id": 343, "seek": 115680, "start": 1159.04, "end": 1161.76, "text": " of matrix multiplication on the GPU.", "tokens": [50476, 295, 8141, 27290, 322, 264, 18407, 13, 50612], "temperature": 0.0, "avg_logprob": -0.11208970923172801, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0007052086293697357}, {"id": 344, "seek": 115680, "start": 1161.76, "end": 1164.56, "text": " So the first time it was half second,", "tokens": [50612, 407, 264, 700, 565, 309, 390, 1922, 1150, 11, 50752], "temperature": 0.0, "avg_logprob": -0.11208970923172801, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0007052086293697357}, {"id": 345, "seek": 115680, "start": 1164.56, "end": 1167.84, "text": " and then it has moved to three milliseconds.", "tokens": [50752, 293, 550, 309, 575, 4259, 281, 1045, 34184, 13, 50916], "temperature": 0.0, "avg_logprob": -0.11208970923172801, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0007052086293697357}, {"id": 346, "seek": 115680, "start": 1167.84, "end": 1170.1599999999999, "text": " This is because the first execution,", "tokens": [50916, 639, 307, 570, 264, 700, 15058, 11, 51032], "temperature": 0.0, "avg_logprob": -0.11208970923172801, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0007052086293697357}, {"id": 347, "seek": 115680, "start": 1170.1599999999999, "end": 1173.2, "text": " it involves also the GIT compilation, which is expensive.", "tokens": [51032, 309, 11626, 611, 264, 460, 3927, 40261, 11, 597, 307, 5124, 13, 51184], "temperature": 0.0, "avg_logprob": -0.11208970923172801, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0007052086293697357}, {"id": 348, "seek": 115680, "start": 1173.76, "end": 1175.84, "text": " Then the second time, third time,", "tokens": [51212, 1396, 264, 1150, 565, 11, 2636, 565, 11, 51316], "temperature": 0.0, "avg_logprob": -0.11208970923172801, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0007052086293697357}, {"id": 349, "seek": 115680, "start": 1175.84, "end": 1178.3999999999999, "text": " the execution time has been saturated", "tokens": [51316, 264, 15058, 565, 575, 668, 25408, 51444], "temperature": 0.0, "avg_logprob": -0.11208970923172801, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0007052086293697357}, {"id": 350, "seek": 115680, "start": 1178.3999999999999, "end": 1181.04, "text": " because it is the actual launching of the code.", "tokens": [51444, 570, 309, 307, 264, 3539, 18354, 295, 264, 3089, 13, 51576], "temperature": 0.0, "avg_logprob": -0.11208970923172801, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0007052086293697357}, {"id": 351, "seek": 118104, "start": 1182.0, "end": 1185.84, "text": " Okay, I have showed you example of Python with Gralpy,", "tokens": [50412, 1033, 11, 286, 362, 4712, 291, 1365, 295, 15329, 365, 2606, 304, 8200, 11, 50604], "temperature": 0.0, "avg_logprob": -0.531256666087141, "compression_ratio": 1.5625, "no_speech_prob": 0.0044283680617809296}, {"id": 352, "seek": 118104, "start": 1185.84, "end": 1187.92, "text": " but this is not the only one.", "tokens": [50604, 457, 341, 307, 406, 264, 787, 472, 13, 50708], "temperature": 0.0, "avg_logprob": -0.531256666087141, "compression_ratio": 1.5625, "no_speech_prob": 0.0044283680617809296}, {"id": 353, "seek": 118104, "start": 1187.92, "end": 1191.84, "text": " We have also the key images for the other programming languages", "tokens": [50708, 492, 362, 611, 264, 2141, 5267, 337, 264, 661, 9410, 8650, 50904], "temperature": 0.0, "avg_logprob": -0.531256666087141, "compression_ratio": 1.5625, "no_speech_prob": 0.0044283680617809296}, {"id": 354, "seek": 118104, "start": 1191.84, "end": 1193.52, "text": " for JavaScript, Ruby,", "tokens": [50904, 337, 15778, 11, 19907, 11, 50988], "temperature": 0.0, "avg_logprob": -0.531256666087141, "compression_ratio": 1.5625, "no_speech_prob": 0.0044283680617809296}, {"id": 355, "seek": 118104, "start": 1193.52, "end": 1196.08, "text": " and you can find more details in those links", "tokens": [50988, 293, 291, 393, 915, 544, 4365, 294, 729, 6123, 51116], "temperature": 0.0, "avg_logprob": -0.531256666087141, "compression_ratio": 1.5625, "no_speech_prob": 0.0044283680617809296}, {"id": 356, "seek": 118104, "start": 1196.08, "end": 1198.32, "text": " where we have a blog post.", "tokens": [51116, 689, 321, 362, 257, 6968, 2183, 13, 51228], "temperature": 0.0, "avg_logprob": -0.531256666087141, "compression_ratio": 1.5625, "no_speech_prob": 0.0044283680617809296}, {"id": 357, "seek": 118104, "start": 1198.32, "end": 1202.08, "text": " And we explain also the polyglot programming", "tokens": [51228, 400, 321, 2903, 611, 264, 6754, 7191, 310, 9410, 51416], "temperature": 0.0, "avg_logprob": -0.531256666087141, "compression_ratio": 1.5625, "no_speech_prob": 0.0044283680617809296}, {"id": 358, "seek": 118104, "start": 1202.08, "end": 1204.08, "text": " from Tornado VM.", "tokens": [51416, 490, 314, 1865, 1573, 18038, 13, 51516], "temperature": 0.0, "avg_logprob": -0.531256666087141, "compression_ratio": 1.5625, "no_speech_prob": 0.0044283680617809296}, {"id": 359, "seek": 118104, "start": 1206.56, "end": 1210.48, "text": " So now we will try to find the other examples", "tokens": [51640, 407, 586, 321, 486, 853, 281, 915, 264, 661, 5110, 51836], "temperature": 0.0, "avg_logprob": -0.531256666087141, "compression_ratio": 1.5625, "no_speech_prob": 0.0044283680617809296}, {"id": 360, "seek": 121048, "start": 1210.48, "end": 1212.96, "text": " so now I will jump to the summary of my talk.", "tokens": [50364, 370, 586, 286, 486, 3012, 281, 264, 12691, 295, 452, 751, 13, 50488], "temperature": 0.0, "avg_logprob": -0.2082388458251953, "compression_ratio": 1.541044776119403, "no_speech_prob": 0.0009953849948942661}, {"id": 361, "seek": 121048, "start": 1214.56, "end": 1215.84, "text": " So as key takeaways,", "tokens": [50568, 407, 382, 2141, 45584, 11, 50632], "temperature": 0.0, "avg_logprob": -0.2082388458251953, "compression_ratio": 1.541044776119403, "no_speech_prob": 0.0009953849948942661}, {"id": 362, "seek": 121048, "start": 1215.84, "end": 1218.88, "text": " I would like to emphasize that GralVM and Traffl", "tokens": [50632, 286, 576, 411, 281, 16078, 300, 2606, 304, 53, 44, 293, 5403, 602, 75, 50784], "temperature": 0.0, "avg_logprob": -0.2082388458251953, "compression_ratio": 1.541044776119403, "no_speech_prob": 0.0009953849948942661}, {"id": 363, "seek": 121048, "start": 1218.88, "end": 1222.08, "text": " enable Java interoperability with other programming languages", "tokens": [50784, 9528, 10745, 728, 7192, 2310, 365, 661, 9410, 8650, 50944], "temperature": 0.0, "avg_logprob": -0.2082388458251953, "compression_ratio": 1.541044776119403, "no_speech_prob": 0.0009953849948942661}, {"id": 364, "seek": 121048, "start": 1222.08, "end": 1223.68, "text": " that run on top of the JVM.", "tokens": [50944, 300, 1190, 322, 1192, 295, 264, 508, 53, 44, 13, 51024], "temperature": 0.0, "avg_logprob": -0.2082388458251953, "compression_ratio": 1.541044776119403, "no_speech_prob": 0.0009953849948942661}, {"id": 365, "seek": 121048, "start": 1224.4, "end": 1229.04, "text": " Tornado VM afflows Java methods on GPUs, FPGAs,", "tokens": [51060, 314, 1865, 1573, 18038, 2096, 75, 1509, 10745, 7150, 322, 18407, 82, 11, 36655, 38, 10884, 11, 51292], "temperature": 0.0, "avg_logprob": -0.2082388458251953, "compression_ratio": 1.541044776119403, "no_speech_prob": 0.0009953849948942661}, {"id": 366, "seek": 121048, "start": 1229.04, "end": 1230.16, "text": " and multicore CPUs,", "tokens": [51292, 293, 30608, 418, 13199, 82, 11, 51348], "temperature": 0.0, "avg_logprob": -0.2082388458251953, "compression_ratio": 1.541044776119403, "no_speech_prob": 0.0009953849948942661}, {"id": 367, "seek": 121048, "start": 1230.16, "end": 1233.3600000000001, "text": " so you can create parallel implementations.", "tokens": [51348, 370, 291, 393, 1884, 8952, 4445, 763, 13, 51508], "temperature": 0.0, "avg_logprob": -0.2082388458251953, "compression_ratio": 1.541044776119403, "no_speech_prob": 0.0009953849948942661}, {"id": 368, "seek": 121048, "start": 1234.32, "end": 1237.44, "text": " And that Tornado VM offers a Java API,", "tokens": [51556, 400, 300, 314, 1865, 1573, 18038, 7736, 257, 10745, 9362, 11, 51712], "temperature": 0.0, "avg_logprob": -0.2082388458251953, "compression_ratio": 1.541044776119403, "no_speech_prob": 0.0009953849948942661}, {"id": 369, "seek": 121048, "start": 1237.44, "end": 1240.32, "text": " so programmers, they don't need to know GPU programming.", "tokens": [51712, 370, 41504, 11, 436, 500, 380, 643, 281, 458, 18407, 9410, 13, 51856], "temperature": 0.0, "avg_logprob": -0.2082388458251953, "compression_ratio": 1.541044776119403, "no_speech_prob": 0.0009953849948942661}, {"id": 370, "seek": 124048, "start": 1240.48, "end": 1244.0, "text": " It is a Java API, a Java way to express parallelism.", "tokens": [50364, 467, 307, 257, 10745, 9362, 11, 257, 10745, 636, 281, 5109, 8952, 1434, 13, 50540], "temperature": 0.0, "avg_logprob": -0.15293778543886932, "compression_ratio": 1.480349344978166, "no_speech_prob": 0.001401796005666256}, {"id": 371, "seek": 124048, "start": 1244.56, "end": 1247.68, "text": " And we have also new off-hip data types.", "tokens": [50568, 400, 321, 362, 611, 777, 766, 12, 71, 647, 1412, 3467, 13, 50724], "temperature": 0.0, "avg_logprob": -0.15293778543886932, "compression_ratio": 1.480349344978166, "no_speech_prob": 0.001401796005666256}, {"id": 372, "seek": 124048, "start": 1249.04, "end": 1251.44, "text": " So finally, yes, it is possible to create", "tokens": [50792, 407, 2721, 11, 2086, 11, 309, 307, 1944, 281, 1884, 50912], "temperature": 0.0, "avg_logprob": -0.15293778543886932, "compression_ratio": 1.480349344978166, "no_speech_prob": 0.001401796005666256}, {"id": 373, "seek": 124048, "start": 1251.44, "end": 1254.72, "text": " high-performing implementations of code", "tokens": [50912, 1090, 12, 26765, 278, 4445, 763, 295, 3089, 51076], "temperature": 0.0, "avg_logprob": -0.15293778543886932, "compression_ratio": 1.480349344978166, "no_speech_prob": 0.001401796005666256}, {"id": 374, "seek": 124048, "start": 1254.72, "end": 1256.88, "text": " for data science libraries in Java,", "tokens": [51076, 337, 1412, 3497, 15148, 294, 10745, 11, 51184], "temperature": 0.0, "avg_logprob": -0.15293778543886932, "compression_ratio": 1.480349344978166, "no_speech_prob": 0.001401796005666256}, {"id": 375, "seek": 124048, "start": 1256.88, "end": 1259.84, "text": " and reuse them by other programming languages.", "tokens": [51184, 293, 26225, 552, 538, 661, 9410, 8650, 13, 51332], "temperature": 0.0, "avg_logprob": -0.15293778543886932, "compression_ratio": 1.480349344978166, "no_speech_prob": 0.001401796005666256}, {"id": 376, "seek": 124048, "start": 1263.1200000000001, "end": 1266.32, "text": " This is a slide that summarizes everyone", "tokens": [51496, 639, 307, 257, 4137, 300, 14611, 5660, 1518, 51656], "temperature": 0.0, "avg_logprob": -0.15293778543886932, "compression_ratio": 1.480349344978166, "no_speech_prob": 0.001401796005666256}, {"id": 377, "seek": 124048, "start": 1266.32, "end": 1268.16, "text": " who has contributed as a research staff", "tokens": [51656, 567, 575, 18434, 382, 257, 2132, 3525, 51748], "temperature": 0.0, "avg_logprob": -0.15293778543886932, "compression_ratio": 1.480349344978166, "no_speech_prob": 0.001401796005666256}, {"id": 378, "seek": 126816, "start": 1268.16, "end": 1270.88, "text": " for students at the University of Manchester,", "tokens": [50364, 337, 1731, 412, 264, 3535, 295, 27180, 11, 50500], "temperature": 0.0, "avg_logprob": -0.1331184277167687, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00844188779592514}, {"id": 379, "seek": 126816, "start": 1270.88, "end": 1272.88, "text": " and these images are from our campus.", "tokens": [50500, 293, 613, 5267, 366, 490, 527, 4828, 13, 50600], "temperature": 0.0, "avg_logprob": -0.1331184277167687, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00844188779592514}, {"id": 380, "seek": 126816, "start": 1274.0800000000002, "end": 1277.68, "text": " And this is a surprise that it was taken and it was not raining.", "tokens": [50660, 400, 341, 307, 257, 6365, 300, 309, 390, 2726, 293, 309, 390, 406, 18441, 13, 50840], "temperature": 0.0, "avg_logprob": -0.1331184277167687, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00844188779592514}, {"id": 381, "seek": 126816, "start": 1280.16, "end": 1282.5600000000002, "text": " So I would like to invite you to join our community,", "tokens": [50964, 407, 286, 576, 411, 281, 7980, 291, 281, 3917, 527, 1768, 11, 51084], "temperature": 0.0, "avg_logprob": -0.1331184277167687, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00844188779592514}, {"id": 382, "seek": 126816, "start": 1282.5600000000002, "end": 1284.3200000000002, "text": " follow us in GitHub,", "tokens": [51084, 1524, 505, 294, 23331, 11, 51172], "temperature": 0.0, "avg_logprob": -0.1331184277167687, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00844188779592514}, {"id": 383, "seek": 126816, "start": 1284.3200000000002, "end": 1286.96, "text": " join us in the Tornado VM Slack space", "tokens": [51172, 3917, 505, 294, 264, 314, 1865, 1573, 18038, 37211, 1901, 51304], "temperature": 0.0, "avg_logprob": -0.1331184277167687, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00844188779592514}, {"id": 384, "seek": 126816, "start": 1286.96, "end": 1288.0800000000002, "text": " if you have questions,", "tokens": [51304, 498, 291, 362, 1651, 11, 51360], "temperature": 0.0, "avg_logprob": -0.1331184277167687, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00844188779592514}, {"id": 385, "seek": 126816, "start": 1288.0800000000002, "end": 1291.68, "text": " or if you want to interact with a team for discussions,", "tokens": [51360, 420, 498, 291, 528, 281, 4648, 365, 257, 1469, 337, 11088, 11, 51540], "temperature": 0.0, "avg_logprob": -0.1331184277167687, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00844188779592514}, {"id": 386, "seek": 126816, "start": 1292.3200000000002, "end": 1294.4, "text": " and also to try our examples in GitHub.", "tokens": [51572, 293, 611, 281, 853, 527, 5110, 294, 23331, 13, 51676], "temperature": 0.0, "avg_logprob": -0.1331184277167687, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00844188779592514}, {"id": 387, "seek": 129440, "start": 1294.72, "end": 1296.24, "text": " And in my last slide,", "tokens": [50380, 400, 294, 452, 1036, 4137, 11, 50456], "temperature": 0.0, "avg_logprob": -0.34370816874707866, "compression_ratio": 1.4859437751004017, "no_speech_prob": 0.019486678764224052}, {"id": 388, "seek": 129440, "start": 1296.24, "end": 1299.3600000000001, "text": " I would like to acknowledge all these research funds", "tokens": [50456, 286, 576, 411, 281, 10692, 439, 613, 2132, 8271, 50612], "temperature": 0.0, "avg_logprob": -0.34370816874707866, "compression_ratio": 1.4859437751004017, "no_speech_prob": 0.019486678764224052}, {"id": 389, "seek": 129440, "start": 1299.3600000000001, "end": 1303.1200000000001, "text": " that have supported their work at Tornado VM,", "tokens": [50612, 300, 362, 8104, 641, 589, 412, 314, 1865, 1573, 18038, 11, 50800], "temperature": 0.0, "avg_logprob": -0.34370816874707866, "compression_ratio": 1.4859437751004017, "no_speech_prob": 0.019486678764224052}, {"id": 390, "seek": 129440, "start": 1303.1200000000001, "end": 1306.5600000000002, "text": " like Elegant and Crip, Tango, Iro and InCode.", "tokens": [50800, 411, 462, 6363, 394, 293, 383, 8400, 11, 314, 17150, 11, 286, 340, 293, 682, 34, 1429, 13, 50972], "temperature": 0.0, "avg_logprob": -0.34370816874707866, "compression_ratio": 1.4859437751004017, "no_speech_prob": 0.019486678764224052}, {"id": 391, "seek": 129440, "start": 1307.2, "end": 1309.6000000000001, "text": " So with that, I conclude my talk,", "tokens": [51004, 407, 365, 300, 11, 286, 16886, 452, 751, 11, 51124], "temperature": 0.0, "avg_logprob": -0.34370816874707866, "compression_ratio": 1.4859437751004017, "no_speech_prob": 0.019486678764224052}, {"id": 392, "seek": 129440, "start": 1309.6000000000001, "end": 1312.8000000000002, "text": " and I think we have time for one or two questions.", "tokens": [51124, 293, 286, 519, 321, 362, 565, 337, 472, 420, 732, 1651, 13, 51284], "temperature": 0.0, "avg_logprob": -0.34370816874707866, "compression_ratio": 1.4859437751004017, "no_speech_prob": 0.019486678764224052}, {"id": 393, "seek": 129440, "start": 1312.8000000000002, "end": 1314.0, "text": " Okay, I've got the mic here,", "tokens": [51284, 1033, 11, 286, 600, 658, 264, 3123, 510, 11, 51344], "temperature": 0.0, "avg_logprob": -0.34370816874707866, "compression_ratio": 1.4859437751004017, "no_speech_prob": 0.019486678764224052}, {"id": 394, "seek": 129440, "start": 1314.0, "end": 1315.8400000000001, "text": " but first, I lived in Manchester for five years,", "tokens": [51344, 457, 700, 11, 286, 5152, 294, 27180, 337, 1732, 924, 11, 51436], "temperature": 0.0, "avg_logprob": -0.34370816874707866, "compression_ratio": 1.4859437751004017, "no_speech_prob": 0.019486678764224052}, {"id": 395, "seek": 129440, "start": 1315.8400000000001, "end": 1317.3600000000001, "text": " and it doesn't always rain.", "tokens": [51436, 293, 309, 1177, 380, 1009, 4830, 13, 51512], "temperature": 0.0, "avg_logprob": -0.34370816874707866, "compression_ratio": 1.4859437751004017, "no_speech_prob": 0.019486678764224052}, {"id": 396, "seek": 129440, "start": 1317.3600000000001, "end": 1318.5600000000002, "text": " Just mostly.", "tokens": [51512, 1449, 5240, 13, 51572], "temperature": 0.0, "avg_logprob": -0.34370816874707866, "compression_ratio": 1.4859437751004017, "no_speech_prob": 0.019486678764224052}, {"id": 397, "seek": 131856, "start": 1318.6399999999999, "end": 1319.52, "text": " Just mostly.", "tokens": [50368, 1449, 5240, 13, 50412], "temperature": 0.0, "avg_logprob": -0.22595174761785977, "compression_ratio": 1.3229813664596273, "no_speech_prob": 0.024221904575824738}, {"id": 398, "seek": 131856, "start": 1328.08, "end": 1329.12, "text": " Thanks for a great talk.", "tokens": [50840, 2561, 337, 257, 869, 751, 13, 50892], "temperature": 0.0, "avg_logprob": -0.22595174761785977, "compression_ratio": 1.3229813664596273, "no_speech_prob": 0.024221904575824738}, {"id": 399, "seek": 131856, "start": 1329.6799999999998, "end": 1333.12, "text": " Like one of the first pictures you had showed Tornado VM", "tokens": [50920, 1743, 472, 295, 264, 700, 5242, 291, 632, 4712, 314, 1865, 1573, 18038, 51092], "temperature": 0.0, "avg_logprob": -0.22595174761785977, "compression_ratio": 1.3229813664596273, "no_speech_prob": 0.024221904575824738}, {"id": 400, "seek": 131856, "start": 1333.84, "end": 1338.3999999999999, "text": " in parallel to the GrowlJIT using the JVMCI.", "tokens": [51128, 294, 8952, 281, 264, 18476, 75, 41, 3927, 1228, 264, 508, 53, 44, 25240, 13, 51356], "temperature": 0.0, "avg_logprob": -0.22595174761785977, "compression_ratio": 1.3229813664596273, "no_speech_prob": 0.024221904575824738}, {"id": 401, "seek": 131856, "start": 1338.3999999999999, "end": 1344.08, "text": " So do you interact directly with JVMCI for generating code?", "tokens": [51356, 407, 360, 291, 4648, 3838, 365, 508, 53, 44, 25240, 337, 17746, 3089, 30, 51640], "temperature": 0.0, "avg_logprob": -0.22595174761785977, "compression_ratio": 1.3229813664596273, "no_speech_prob": 0.024221904575824738}, {"id": 402, "seek": 131856, "start": 1344.08, "end": 1345.04, "text": " Correct, yes.", "tokens": [51640, 12753, 11, 2086, 13, 51688], "temperature": 0.0, "avg_logprob": -0.22595174761785977, "compression_ratio": 1.3229813664596273, "no_speech_prob": 0.024221904575824738}, {"id": 403, "seek": 134504, "start": 1345.12, "end": 1349.2, "text": " So the JVMCI enables other JIT compilers", "tokens": [50368, 407, 264, 508, 53, 44, 25240, 17077, 661, 508, 3927, 715, 388, 433, 50572], "temperature": 0.0, "avg_logprob": -0.12437880423761183, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.012837461195886135}, {"id": 404, "seek": 134504, "start": 1349.2, "end": 1350.6399999999999, "text": " to be hooked in the JVM,", "tokens": [50572, 281, 312, 20410, 294, 264, 508, 53, 44, 11, 50644], "temperature": 0.0, "avg_logprob": -0.12437880423761183, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.012837461195886135}, {"id": 405, "seek": 134504, "start": 1350.6399999999999, "end": 1353.6, "text": " and that's how we run, because we extend.", "tokens": [50644, 293, 300, 311, 577, 321, 1190, 11, 570, 321, 10101, 13, 50792], "temperature": 0.0, "avg_logprob": -0.12437880423761183, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.012837461195886135}, {"id": 406, "seek": 134504, "start": 1353.6, "end": 1358.56, "text": " So do you work with the standard JVMCI in upstream or open JDK,", "tokens": [50792, 407, 360, 291, 589, 365, 264, 3832, 508, 53, 44, 25240, 294, 33915, 420, 1269, 37082, 42, 11, 51040], "temperature": 0.0, "avg_logprob": -0.12437880423761183, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.012837461195886135}, {"id": 407, "seek": 134504, "start": 1358.56, "end": 1361.92, "text": " or you need the lab JDK with the latest JVMCI changes?", "tokens": [51040, 420, 291, 643, 264, 2715, 37082, 42, 365, 264, 6792, 508, 53, 44, 25240, 2962, 30, 51208], "temperature": 0.0, "avg_logprob": -0.12437880423761183, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.012837461195886135}, {"id": 408, "seek": 134504, "start": 1361.92, "end": 1363.28, "text": " Because the GrowlJIT compiler,", "tokens": [51208, 1436, 264, 18476, 75, 41, 3927, 31958, 11, 51276], "temperature": 0.0, "avg_logprob": -0.12437880423761183, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.012837461195886135}, {"id": 409, "seek": 134504, "start": 1363.28, "end": 1366.3999999999999, "text": " as far as I know, requires the lab JDK with latest changes.", "tokens": [51276, 382, 1400, 382, 286, 458, 11, 7029, 264, 2715, 37082, 42, 365, 6792, 2962, 13, 51432], "temperature": 0.0, "avg_logprob": -0.12437880423761183, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.012837461195886135}, {"id": 410, "seek": 134504, "start": 1367.04, "end": 1369.52, "text": " We work with the standard JVMCI, yes.", "tokens": [51464, 492, 589, 365, 264, 3832, 508, 53, 44, 25240, 11, 2086, 13, 51588], "temperature": 0.0, "avg_logprob": -0.12437880423761183, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.012837461195886135}, {"id": 411, "seek": 134504, "start": 1370.08, "end": 1370.48, "text": " Thank you.", "tokens": [51616, 1044, 291, 13, 51636], "temperature": 0.0, "avg_logprob": -0.12437880423761183, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.012837461195886135}, {"id": 412, "seek": 137504, "start": 1375.28, "end": 1375.92, "text": " Thank you.", "tokens": [50376, 1044, 291, 13, 50408], "temperature": 0.2, "avg_logprob": -0.2679378820020099, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.00976062286645174}, {"id": 413, "seek": 137504, "start": 1386.24, "end": 1389.44, "text": " So when you write the kernel code in Java,", "tokens": [50924, 407, 562, 291, 2464, 264, 28256, 3089, 294, 10745, 11, 51084], "temperature": 0.2, "avg_logprob": -0.2679378820020099, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.00976062286645174}, {"id": 414, "seek": 137504, "start": 1389.44, "end": 1393.76, "text": " then is it usually high-level code that you write,", "tokens": [51084, 550, 307, 309, 2673, 1090, 12, 12418, 3089, 300, 291, 2464, 11, 51300], "temperature": 0.2, "avg_logprob": -0.2679378820020099, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.00976062286645174}, {"id": 415, "seek": 137504, "start": 1393.76, "end": 1396.3999999999999, "text": " or do you try to write optimized code in Java?", "tokens": [51300, 420, 360, 291, 853, 281, 2464, 26941, 3089, 294, 10745, 30, 51432], "temperature": 0.2, "avg_logprob": -0.2679378820020099, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.00976062286645174}, {"id": 416, "seek": 137504, "start": 1396.3999999999999, "end": 1399.12, "text": " Like usually when you write, let's say, Qtacode,", "tokens": [51432, 1743, 2673, 562, 291, 2464, 11, 718, 311, 584, 11, 1249, 83, 326, 1429, 11, 51568], "temperature": 0.2, "avg_logprob": -0.2679378820020099, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.00976062286645174}, {"id": 417, "seek": 137504, "start": 1399.12, "end": 1402.1599999999999, "text": " then you try to write a very specialized,", "tokens": [51568, 550, 291, 853, 281, 2464, 257, 588, 19813, 11, 51720], "temperature": 0.2, "avg_logprob": -0.2679378820020099, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.00976062286645174}, {"id": 418, "seek": 137504, "start": 1402.1599999999999, "end": 1404.32, "text": " use warp intrinsics and that kind of stuff.", "tokens": [51720, 764, 36030, 28621, 1167, 293, 300, 733, 295, 1507, 13, 51828], "temperature": 0.2, "avg_logprob": -0.2679378820020099, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.00976062286645174}, {"id": 419, "seek": 140432, "start": 1404.3999999999999, "end": 1406.8799999999999, "text": " Is that something that is like in scope for turn out of VM,", "tokens": [50368, 1119, 300, 746, 300, 307, 411, 294, 11923, 337, 1261, 484, 295, 18038, 11, 50492], "temperature": 0.0, "avg_logprob": -0.14882549569626485, "compression_ratio": 1.580392156862745, "no_speech_prob": 0.008228792808949947}, {"id": 420, "seek": 140432, "start": 1406.8799999999999, "end": 1407.84, "text": " or not so much?", "tokens": [50492, 420, 406, 370, 709, 30, 50540], "temperature": 0.0, "avg_logprob": -0.14882549569626485, "compression_ratio": 1.580392156862745, "no_speech_prob": 0.008228792808949947}, {"id": 421, "seek": 140432, "start": 1407.84, "end": 1409.4399999999998, "text": " No, that's a great question.", "tokens": [50540, 883, 11, 300, 311, 257, 869, 1168, 13, 50620], "temperature": 0.0, "avg_logprob": -0.14882549569626485, "compression_ratio": 1.580392156862745, "no_speech_prob": 0.008228792808949947}, {"id": 422, "seek": 140432, "start": 1409.4399999999998, "end": 1412.08, "text": " Well, to answer this question, we do both.", "tokens": [50620, 1042, 11, 281, 1867, 341, 1168, 11, 321, 360, 1293, 13, 50752], "temperature": 0.0, "avg_logprob": -0.14882549569626485, "compression_ratio": 1.580392156862745, "no_speech_prob": 0.008228792808949947}, {"id": 423, "seek": 140432, "start": 1412.08, "end": 1415.36, "text": " So we have two APIs.", "tokens": [50752, 407, 321, 362, 732, 21445, 13, 50916], "temperature": 0.0, "avg_logprob": -0.14882549569626485, "compression_ratio": 1.580392156862745, "no_speech_prob": 0.008228792808949947}, {"id": 424, "seek": 140432, "start": 1415.36, "end": 1418.96, "text": " One is created for Java programmers.", "tokens": [50916, 1485, 307, 2942, 337, 10745, 41504, 13, 51096], "temperature": 0.0, "avg_logprob": -0.14882549569626485, "compression_ratio": 1.580392156862745, "no_speech_prob": 0.008228792808949947}, {"id": 425, "seek": 140432, "start": 1418.96, "end": 1423.6, "text": " We will have, let's say, a computation that has four loops.", "tokens": [51096, 492, 486, 362, 11, 718, 311, 584, 11, 257, 24903, 300, 575, 1451, 16121, 13, 51328], "temperature": 0.0, "avg_logprob": -0.14882549569626485, "compression_ratio": 1.580392156862745, "no_speech_prob": 0.008228792808949947}, {"id": 426, "seek": 140432, "start": 1424.1599999999999, "end": 1426.8, "text": " So this is something that you can paralyze", "tokens": [51356, 407, 341, 307, 746, 300, 291, 393, 32645, 1381, 51488], "temperature": 0.0, "avg_logprob": -0.14882549569626485, "compression_ratio": 1.580392156862745, "no_speech_prob": 0.008228792808949947}, {"id": 427, "seek": 140432, "start": 1426.8, "end": 1428.8, "text": " if you don't have data dependency.", "tokens": [51488, 498, 291, 500, 380, 362, 1412, 33621, 13, 51588], "temperature": 0.0, "avg_logprob": -0.14882549569626485, "compression_ratio": 1.580392156862745, "no_speech_prob": 0.008228792808949947}, {"id": 428, "seek": 140432, "start": 1428.8, "end": 1431.9199999999998, "text": " So we expose an annotation in this case,", "tokens": [51588, 407, 321, 19219, 364, 48654, 294, 341, 1389, 11, 51744], "temperature": 0.0, "avg_logprob": -0.14882549569626485, "compression_ratio": 1.580392156862745, "no_speech_prob": 0.008228792808949947}, {"id": 429, "seek": 140432, "start": 1431.9199999999998, "end": 1433.28, "text": " similar to OpenMP.", "tokens": [51744, 2531, 281, 7238, 12224, 13, 51812], "temperature": 0.0, "avg_logprob": -0.14882549569626485, "compression_ratio": 1.580392156862745, "no_speech_prob": 0.008228792808949947}, {"id": 430, "seek": 143328, "start": 1433.28, "end": 1436.6399999999999, "text": " So you can do add parallel in the four loop", "tokens": [50364, 407, 291, 393, 360, 909, 8952, 294, 264, 1451, 6367, 50532], "temperature": 0.0, "avg_logprob": -0.12961599930472997, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.008646353147923946}, {"id": 431, "seek": 143328, "start": 1436.6399999999999, "end": 1438.72, "text": " in order to give a hint to the compiler", "tokens": [50532, 294, 1668, 281, 976, 257, 12075, 281, 264, 31958, 50636], "temperature": 0.0, "avg_logprob": -0.12961599930472997, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.008646353147923946}, {"id": 432, "seek": 143328, "start": 1438.72, "end": 1440.72, "text": " that this can run in parallel", "tokens": [50636, 300, 341, 393, 1190, 294, 8952, 50736], "temperature": 0.0, "avg_logprob": -0.12961599930472997, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.008646353147923946}, {"id": 433, "seek": 143328, "start": 1440.72, "end": 1444.16, "text": " and will create parallel implementations in OpenCL or CUDA.", "tokens": [50736, 293, 486, 1884, 8952, 4445, 763, 294, 7238, 22458, 420, 29777, 7509, 13, 50908], "temperature": 0.0, "avg_logprob": -0.12961599930472997, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.008646353147923946}, {"id": 434, "seek": 143328, "start": 1445.2, "end": 1450.0, "text": " And the second part is that if you are familiar with OpenCL", "tokens": [50960, 400, 264, 1150, 644, 307, 300, 498, 291, 366, 4963, 365, 7238, 22458, 51200], "temperature": 0.0, "avg_logprob": -0.12961599930472997, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.008646353147923946}, {"id": 435, "seek": 143328, "start": 1450.0, "end": 1454.3999999999999, "text": " and CUDA and you want to have access to low-level intrinsics,", "tokens": [51200, 293, 29777, 7509, 293, 291, 528, 281, 362, 2105, 281, 2295, 12, 12418, 28621, 1167, 11, 51420], "temperature": 0.0, "avg_logprob": -0.12961599930472997, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.008646353147923946}, {"id": 436, "seek": 143328, "start": 1454.3999999999999, "end": 1458.08, "text": " like, for example, use barriers or local memory,", "tokens": [51420, 411, 11, 337, 1365, 11, 764, 13565, 420, 2654, 4675, 11, 51604], "temperature": 0.0, "avg_logprob": -0.12961599930472997, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.008646353147923946}, {"id": 437, "seek": 143328, "start": 1458.08, "end": 1459.6, "text": " allocate local memory,", "tokens": [51604, 35713, 2654, 4675, 11, 51680], "temperature": 0.0, "avg_logprob": -0.12961599930472997, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.008646353147923946}, {"id": 438, "seek": 143328, "start": 1459.6, "end": 1463.12, "text": " then we'll have a second API, which is called kernel API.", "tokens": [51680, 550, 321, 603, 362, 257, 1150, 9362, 11, 597, 307, 1219, 28256, 9362, 13, 51856], "temperature": 0.0, "avg_logprob": -0.12961599930472997, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.008646353147923946}, {"id": 439, "seek": 146328, "start": 1464.24, "end": 1468.24, "text": " And with that, you can pretty much access every interesting", "tokens": [50412, 400, 365, 300, 11, 291, 393, 1238, 709, 2105, 633, 1880, 50612], "temperature": 0.0, "avg_logprob": -0.23431507457386364, "compression_ratio": 1.3419354838709678, "no_speech_prob": 0.009497351944446564}, {"id": 440, "seek": 146328, "start": 1468.24, "end": 1471.92, "text": " that exists in OpenCL and CUDA programming from Java.", "tokens": [50612, 300, 8198, 294, 7238, 22458, 293, 29777, 7509, 9410, 490, 10745, 13, 50796], "temperature": 0.0, "avg_logprob": -0.23431507457386364, "compression_ratio": 1.3419354838709678, "no_speech_prob": 0.009497351944446564}, {"id": 441, "seek": 146328, "start": 1471.92, "end": 1474.8799999999999, "text": " So personally, I have used the second API", "tokens": [50796, 407, 5665, 11, 286, 362, 1143, 264, 1150, 9362, 50944], "temperature": 0.0, "avg_logprob": -0.23431507457386364, "compression_ratio": 1.3419354838709678, "no_speech_prob": 0.009497351944446564}, {"id": 442, "seek": 146328, "start": 1474.8799999999999, "end": 1479.28, "text": " to port existing OpenCL kernels in Java with Tonedo.", "tokens": [50944, 281, 2436, 6741, 7238, 22458, 23434, 1625, 294, 10745, 365, 314, 546, 2595, 13, 51164], "temperature": 0.0, "avg_logprob": -0.23431507457386364, "compression_ratio": 1.3419354838709678, "no_speech_prob": 0.009497351944446564}], "language": "en"}