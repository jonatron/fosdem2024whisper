{"text": " Welcome the next speaker, Adrian. He's going to talk about tower. Hello everyone. Thanks to the organizers for having me. I've been working in Rust for about three years now, mostly contributing to QuickWidth. QuickWidth is an open source distributed search engine for log and traces. We'll be presenting tomorrow in the monitoring and observability room. And if you want to follow along, you'll find a slide on this website. There will be quite a bit of code during the presentation, so that may be easier. So as you know today, we're talking about tower. It's a crate for building modular and networking clients and servers. If you're using widely used traits in the Rust ecosystem such as AXM, WAP, Tonic, you're using tower under the hood. Maybe you don't know it. And everything's based on the service trait that I'm going to describe at length during this talk. But before I do that, I would like us to take a step back and think about why we need tower. So I want you to think that you are a web developer. You're working with any web framework in an imaginary dynamic language. And you ask to write a simple handler to get a user from a database. So you probably would come up with something like this, a function that you would call getUser that would take your request in, add some logging, get the user from the database, save the response, log again, return the response you've done, and you're pretty happy. Pretty quickly, you would realize that maybe there's another way to do it. Maybe you could decouple the fetching the user part that's strictly about the database and all that stuff from the logging. Like the pattern wise, I'm going to log something before the request, and I'm going to log something after the response. It doesn't need to know about everything that's user related. So maybe you would write two functions. The first one would be with logging that would take two parameters, the request still, but also a handler that would be a generic function that also that returns a, accepts a request. And then you would write your getUser simply. It'll be a bit more simple, very focused on what it has to do. And then you would compose your with logging function with your getUser function, and you would achieve what you had achieved before. But now you have that other function with logging that you can use for like all your end-to-end lists. And isn't that the sense of programming? We want to write generic and reasonable functions that are really easy to compose. For the purpose of this talk, we're specifically interested in decorators. So the decorator pattern is basically when a function or a class wraps another function. It applies new behaviors before or after calling the inner function. It doesn't know about that inner function. It's totally opaque, and it doesn't matter, we don't care. It doesn't modify the behavior of the inner function. And you've heard, you've used decorators sometimes with different names, very often millwares in the context of client and servers, proxies. This is a term that's coming up a lot. And then what you can do with those is you have your handler at the bottom. You can also call it a leaf, and you stack millwares with different behaviors on top of it. And you apply different behaviors depending on what you need. And then you can build a nice library of millwares that you can share, that you can use from different libraries, and so on and so forth. In that example, we were using a dynamic language. It was really, really easy to do. Duc typing gave us great flexibility. We don't really care about the types of inputs and outputs. When we code, we think about it, do they match implicitly? If they do, we think it's going to work. We ship it to production. We cross our fingers. But implicitly, everything should match and then everything should work fine. So this is what we're trying to do with TauR in the Rust ecosystem. But this time, we still want to compose functions, but we want to do it in a very type safe manner. And we want to be still very flexible. And this is when the TauR service trade comes into play. It is the common interface that allows us implementing components in a protocol, agnostic, and composable way. So let me describe the trade. It has one generic parameter, request. It's called request by convention, but it can be any type and anything. An associated type, response, same thing. Cold response by convention, but it could really be anything. Error type also could be anything. And finally, a future type. The future type is constrained by the future trade. So it has to be a future. And you'll notice that the output of the future is fixed for you. It has to be a result of the response and the error that you define. So the only choice is choosing a future type. The output is always chosen when you say, I want this kind of response, this kind of error. And then in the trade, you have two methods, the pull ready function and the call. Call takes itself mutably, accepts the request, return the future. The future you define. So your mental model should be much easier than this trade. It's just simply be, this is just a generic async function. This is the same thing. The trade gives us that flexibility that we wanted and gives us the type safety that we wanted. But really, this is the mental model of what is a service trade. With all of twist, you have that additional pull ready function that needs to be called once before calling call. And it provides a way to provide back pressure to the caller. So the pull ready implementation for a service without any external dependencies for the hello service that we're going to define is very simple. Pull ready return is always ready and returns okay. If you're implementing something else, if it's not ready, you return pending. And later on, the task will be pulled again. Hopefully, it'll be ready. If you're writing a service that has external dependencies, for instance, you depend on a database, you need to acquire that database connection. So all the services that have external dependency need to load up upfront to receive the need. And so they can say when they're ready or not to the caller. So in this example, if we haven't acquired yet connection, we will pull our pull of connection to acquire a connection. We will save it mutably in our internal state. And then later in call, we actually consume the connection using take. So that's why the tariff trade is mutable because it's not necessarily state less. It can be stateful. So you can manage your resources. And you see with this example why you need to call pull ready always at least once before first before call. And for the services that I'm you know, where's so they basically wrap an inner service, you usually implementation is really straightforward. You delegate to the inner service. So you call pull pull ready on the inner service and you're done. So if you're not doing something too fancy in your service, usually is going to boil down to those three use cases. So it sounds it sounds simple on paper. Why does it get complex? I think people are a bit afraid of like towers tower and towers services in general, could they use a lot of generics. So it's a bit of it's not always easy to write. It's even hard to read and it's even harder to write. It's using all the the rest construct heavily. So you have to be comfortable with lifetime lifetime, sensing market rates, what is a thing is going to expose you to that. So if you've been using rust as a better C++ trying to avoid those concepts, writing towers services are going to be a bit challenging. But but once you get more comfortable, it'd be very rewarding because like because you're going to be exposed to those concepts. At some point, muscle memory is going to help you and you're going to feel more and more comfortable and you're going to get a bit of rust programmer. And in some cases, if you start writing your own futures for your services, then you need to know about future pulling and pinning and it gets very complex. So the only way to get better understanding services and writing them is just to start simple with like a hello services and build upon until working like more complex services. So this is exactly what we're going to do during the rest of the talk. So let's implement hello service together. Remember the mental model is just this hello function. This is what we want to do, but we want to do it the other way. So the input parameter is a hello request. The hello response will be returned and we try to implement that simple function. So we start defining a health struct and we start implementing service for it. The input parameter is a request. The response type is a response. The error is going to be a little bit specific here. It's going to be infallible because we're never going to fail with just printing stuff. And now we have to choose our future type and we have various options. And the first thing we can do is go with a box future. We can define our own type alias or we can reuse the type alias from the futures scrape, for instance. And why would we do that? When you get started with writing your own service, I would recommend starting with that because it's pretty easy, it's very readable. The cons are you pay a small fee for allocation in dynamic dispatch. It's fine if you're not, if it's a client or server that doesn't have an insane amount of QPS, it's totally fine. Sometimes people are afraid of allocation in dynamic dispatch. If you're working on a client or server that doesn't need working in IO, we're talking milliseconds. That allocation, dynamic dispatch is going to cost you microseconds, maybe even nanoseconds. So you should not worry too much and if you start worrying, you should measure before going for the box stuff. I would want to say, it's writing your own future is way more fun. So sometimes it's a little for my own future, just my own personal fun. So box futures are good chose for applications, less for libraries. When you're writing libraries, you want to decide the users of your libraries whether or not they want to incur some overhead. So it's better if you don't use box futures in your libraries so that people can opt out of the overhead. So in this example, we upsold box futures. We need to notice that this box futures as a lifetime that we'll choose and it has to be sent as well. That will become important later. So we choose the static lifetime. The service, the tower service rate is not generic of a lifetime. So we have to go for static. Then we write priority. It's always ready. So it's pretty straightforward. And then we write our future. So we declare an async move block in which we build a message and build a response. And then we box that future on the hip. We pin it with box pin and we should be done. This is how you would unitize that service. You instantiate it. You call ready. We're using the extension trait that makes it a bit easier to work with. So you call ready and then you call call and it works. So it's obviously a bit more complicated than just writing that hello function that I showed at the beginning. But it's all doable. When it comes to choosing a future, sometimes you have the choice to choose one from a third party crate. Futures has some ready to go futures. You can use towers as well. So sometimes those futures are going to fit your use case. You don't have to go for the box. You don't have to go for implementing your own. You can just reuse one. It's convenient. So in this example, we can use the ready trait from the futures crate. So this time, you do see my pointer crate. We change the return type here. It's ready and this time we return ready. So we got rid of the allocation and the dynamic dispatch. And it's actually more readable. So we built a simple hello service. We're going to build a middle one on top of it. That's our logging service. We want our logging service to work with our hello service. But ideally, we like it to work with any kind of service that implements the trait. So we're going to make it generic. So logging has an input parameter called s that will be the inner service. And we start implementing service for logging. What we want to do is calling call on the inner service. And we can only do that if it's a service itself. So we need that now. And we say the inner service is also a service generic over r, our two generic types here. Then we implement already. We delegate to the inner service pretty straightforward. And then we implement call, starting with using a box feature. So we built the inner future calling call on inner. And then we create our own future using an instinct move block. We do the logging. We evaluate the future here. So here on this line, we build a future that's not evaluated. It's actually evaluated here. Then we do the logging. And we respond to response. We box in the outer future. And now we go back to our terminal. We run cargo test. And it should work. Except it doesn't. We've omitted a little technicality. And that technicality is coming from what I said before. The box future must be sent. So the future that you return must be sent. So the inner future must be sent as well. So we need to tell the compiler that constrate. So not only we have constrate the service, s, we told the compiler it is a service, we also tell the compiler the future that an inner service is going to return is also sent in static. And all this time it works. So this is how you would instantiate your logging service. It's wrapping the hello service. And then you call it the exact same way. It's obviously not going to show up during the unit test. But now you have logging on top of your service. And that logging service is usable for some other services. Tower HTTP has a bunch of services that are ready for metrics, tracing. It works a lot like that. So we're going to do this again this time rolling on our own future. Because it's fun. And because when you're going to recode from AXM to NIC, you're always going to encounter those handwritten features. So it's good to get used to them. Unfortunately, writing a future from scratch is actually non-trivial because of the whole pinning thing. And this is not a talk about writing futures and me explaining to you pinning because I don't totally understand it myself. But I can be very practical and tell you how to do it. And I'll show it to you right now. So now the logging is going to happen in the future. So before we were wrapping a service with a service, and the idea is the same. We're going to wrap the inner future with our logging future so we can add behavior to the future. So you can add behavior to your service, but you can also wrap the future and do stuff after you're done pulling or before you were about to pull the future. And this is what can be tricky with our terrorist services is sometimes, for instance, you take rate limiting. There's a bit of logic in Pall Ready. There's a bit of logic in Call and there's a bit of logic in the future. So it's hard to understand where's the logic that's really to the business, what you're trying to achieve versus what just really to trying to write a service. So that's also part of the complexity of our Dillon and Vittorio services. But back to our logging future, it's going to wrap another future. So it's going to be generic over F. We need to use the create pin project to pin the inner future. So that's why you have the pin attribute there. And now we, that's how our service, the logging service now is going to look. We replaced the future type with our logging future and the use which relies on the inner future. So s call and call in future. And then in call, now when we are called, we do the first logging statement. Then we build the future and when the future will be pulled already, then this is when we'll actually add the last logging statement. So this is how you implement future for logging future. Same idea. We need to add the constraint and tell the compiler the inner future is also a future. It's output is the output of the inner future. We need to project. I don't actually know where that the term comes from. But you need to project self. It's usually a convention is the convention is to call this. And this gives you the same object. But when you use the inner futures, they're actually pullable. Because pull is not defined on the future type. It's defined on a pin future. So that's why you need to project which pins the future, which allows you to pull it. So those are the technicalities that you have to deal with when you're writing your handwritten function. But once you've done that, it's exactly like writing the normal call, I want to say. So we pull our future. And if it's ready, we add the logging statement. If it's not ready, it means it's pending. It's going to be pulled again later. And also we know that when a future is pulled and it's ready, it will be no longer pulled. So we know that this statement will appear only once. So let's build on top of that. Let's build a timeout service now. So same thing. We want to add a timeout to any service. It's also generic over s. But the timeout service is interesting because the logging service is pretty simplistic. It doesn't mutate or touch anything. The request not in the response. It's like it did not go through it. The timeout service is a bit interesting because you have to signal the timeout somehow in your written type. And the way you're going to signal the error is potentially using an enum. And in that enum, you will have two variants. The first one would be the timeout. If timeout appears, that would be the error. But if the inner service returns its error itself, you have to wrap it in inner. And then the caller will know if the error comes from the timeout with the inner error. That looks like a good idea to do it this way. And you can totally do it this way. The problem is if you adopt this pattern for timeouts, authentication, rate limiting, the nesting of all those errors in the inner is going to become pretty complicated. And really hard to compose. And it is easy to deal with. So what libraries usually favor is boxing. So if you use the tower services that modify the error type, they will return the tower box error. And it's much easier to compose. The downside is at some point, you have to donk at the error to know exactly what happened. So we're going to use this error type this time to implement our service on future. So it's very similar than before. The difference this time is the error needs to be boxable. So with other compiler, I can only be a timeout service for services where the inner error is boxable. The polarity is a little bit annoying because it's still delegating to the inner polarity function. But you must not forget to convert the result that is an s error to a box error. So that's why the result returned by the inner service must be, its error must be mapped to the box error. So that's why we need this. Now we implement calls. So we take a look at what we do as for our future. So our timeout future is very similar to our logging future. It just wraps two inner futures. One will be used for the inner future and the other ones will be for the sleep future. So the sleep future, you can reuse the future from Tokyo, for instance, if you're using the Tokyo runtime. So call becomes pretty simple. All you have to do is build your inner future and your sleep future and create your timeout future, which we're going to implement now. So the core of the logic this time is leaves in the code of the future itself. So some libraries sometimes split the service implementation in one module and they put the future in another module so you open the service and you're like, what does it do? Because everything happens in the future. So you need to understand that sometimes a lot of the work is actually done in the future. And this depends for timeout. So implementing future for timeout future, we, as before, we project self into this. We pull the first future, the inner future, and if it's ready, great. It's now the timeout, we return pull ready with the result. We don't forget to map the error, the potential error into its box. If the inner future is not ready, maybe it's taking too long. So we need to verify whether a timeout occurred or not. So we pulled timeout future this time. So if it's pending, nothing to do, we'll be pulled again, we'll go this work again. If it's ready, it means this is an actual timeout. So we need to return the timeout error and we return elapsed error, but boxed. Hence the hint too. So now we're going to see how you can stack services together. I could obviously stack my timeout on top of my LO and then add the logging on top of it, but I can also do it with services that are already ready in tower. So I import them directly. So I import the concurrency limit, the timeout, and I compose my service by wrapping everything on top of each other. So instantiate service, I add a concurrency limit on top of it, and then my timeout, and then the logging. So that involves a bit of body play, but it's not too hard to do. What's really tricky and the compiler is not going to help you with that is the order with which you wrap your services actually matters. You really want your logging to be on top. If your logging is like logging how long it took, you don't want logging to be in the middle. You really want it to be on top to capture the whole life cycle of the request. And it's probably the same thing. You want timeout to be applied above everything that's doing race limiting. It would be better to, if you had like a notification service, it would be better to put it pretty high up because if you have services that are below it, maybe they can do whatever they want. So this is what's tricky when stacking layers and the compiler is not going to help you there. So you got to be careful and you can hopefully rely on a good reviewer for like a double check. I wanted to leave a lot of time for questions. This talk is called a deep dive into TARRA. I realized that in 30 minutes it's actually not that easy to cover everything that we could cover when we talk about TARRA. So let's just call it a shallow deep dive into TARRA. If you want a real deep dive, there are really good resources out there. Two years ago, not that much, but there's really great content, blog posts on YouTube that I'm going to talk about a little bit that really can help you get really, really comfortable with TARRA. So there's something that I don't talk about today called a layer. A layer, TARRA people are pretty obsessed with composability. So a layer is a way to compose a stack of services. A service builder is a way to conveniently build small stacks of services. A service builder helps you get rid of the boilerplate to write this, for instance. So if you want to keep studying TARRA, I would start with layer and service builder. Then I would recommend reading, inventing the service rate. That's a blog post by David Pedersen, who's a TARRA contributor. It's a really good introductory blog post. Maybe you can start with that. It takes about 15 minutes. I find that the AXM documentation page about MinoWares is really, really good. So I think that's a really good resource. And then in the spirit of building and reading and writing late services that are more and more complex, I think the next step for us is looking at the rate limit service. The concurrency limit is also a bit more complex, but really interesting. And then you can look at the channel from Tonic. It gets pretty complex as well. And then if you really want to take it to the next level, think the pool in TARRA. So here we're only showing that a service that wraps just one service. But now you could wrap multiple service. And that's what the pool adds. And the pool is a service. It wraps multiple services. And it's using pool ready to track the load of each inner service to handle back pressure. So it's a really interesting use case of using pool ready in a very fancy way. So pool is also very interesting. Good videos on YouTube. Also David Pedersen has a TARRA stream about tower. He goes through the same thing. There's a hello and logging and time out. And you see him dealing like the old delivery things. Because obviously on presentation, I get everything right on the first try. But it's not exactly how that happens when you either yourself. And if you want to get more comfortable with a single weight in rest and futures in general, John has a great talk on YouTube as well. And you'll find those resources on the slides that are available at this thing. Back in the first few slides, you mentioned the pool ready function. And you were initializing a database connection. I didn't get it. If the database connection is actually a future or not, because the pool ready function doesn't return a future. Is you talking the database one? Yeah, that's where you initialize the connection. You know, you depend on a connection. Yeah, it has to be a future. I mean, no, it doesn't return a future. But because that one, yeah, this one. Yeah, I'll pass quickly because it's a bit complex. So already doesn't return a future. But it behaves like one because you pass it the context, the context as a waker so you can rebuild future really easily with access to the context. And that's I'm happy you asked you asking this question because if you look at come back here. Talk about the concurrency element. The concurrency element uses a semaphore. So a naive way of implementing pool ready with a semaphore is would be to call try a choir. And if you get the permit right away at school, pool ready is ready and just save the permit and then you get rid of it in call once you're done. But a better way to do is to start pulling the future. You don't call pool ready. You call ready. And if it's not ready yet, it's okay. You keep you still maintain that future that you started that started acquiring a permit. You save it in your internal state. And then the next time you keep pulling that future instead of each time doing try, acquire, try, acquire. So it's really unique to trick that you can see in the concurrency element service. Okay, cool. Thank you. When is tower gonna go version one because now it's on zero and hyper just ditched it because it's not one yet. So I'm not a tower contributor. So I don't know exactly. I think they're waiting for the the Khmer people to give us the full story of what is sync is gonna is gonna become. You'll see that with the last release of rust now you can return an implement implement future future, but they still like the weather it said or not to to deal with though. So eventually, I think they want they're waiting for see what we can do in rust. And then then what once the old async work is stabilized, I think they will come up, they will come up with a new version of the trade. Like with those new those new constructs that are being released and will be released in the next version of rust. We can greatly simplify the service trade. So they will wait for this feature to land, revise the trade and then release 1.0. And the second question is there a way to make this tower layers, let's say optional so you can enable disable them from outside the code from the compiler somehow. Yeah, I mean you you have you could use there would be different ways you could use like feature flags, you could use environment viable like they services have a stake. So and you the they mutable so you can do you can do what you can do you can do a lot you could have you should I to make bullying somewhere in there and you can enable disable it. Thank you. Yeah. I'm on the time of future slide. What time of future this one. Yes, there's a poll function and if none of them is ready, it will just return and how would it know when to call it again. Like you pulled once and none of them is ready yet. It's the runtime is going to take care of it. With it's well it depends on the runtime you're using it's it knows it's not ready and then I mean then we're talking about how you implement the runtime basically. It maintains like a queue of features and it regularly pulls them using some a waker which is it's using that context in the waker when it's ready it's going to call the waker that's going to wake the the runtime so and so forth. But you as an implementer of the future you're not in charge of like telling when you're going to you should be pulled again. You're just in charge of saying I'm ready or I'm not. That whole machinery is left to whoever implements our AC runtime. Okay thank you. Thanks for your talk. I had a question because you mentioned that there was some overlap between implementing your own future and the tower service trade itself proposing like a poll function and a poll rate function. Is there any reason apart from fun to implement a future trip yourself like is there any benefit from dealing with the internals of creating a future versus implementing your whole logic inside of the service trade itself and just using the basic tower or basic futures future types. You mean a box of doc futures? Yeah exactly. Your question is like why would I bother writing my own future when I can use box? Yeah when I have this poll function inside of a service trade itself where I can bake my own logic and not have to deal with you know projecting my pins and so on and so forth. I'm not sure I totally understand your question so my answer might not satisfy you. I think you're right you want a future if you want you're writing a library and you don't want you don't want your users to incur the overhead of like the box future. You write it if you have like some constraint that might be related to performance that's going to push you to write a hand rolled future that doesn't allocate it doesn't have dynamic dispatch. Yeah that would be my answer. I think it also depends on your team if like if you're the only guy in the team that's going to understand like oh I pin the I project the future and are you spinning if just you maybe maybe you don't do it. It depends on those constraints. Okay thanks. Awesome so if there's no more questions let's thank our speaker. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " Welcome the next speaker, Adrian. He's going to talk about tower.", "tokens": [50364, 4027, 264, 958, 8145, 11, 31746, 13, 634, 311, 516, 281, 751, 466, 10567, 13, 50714], "temperature": 0.0, "avg_logprob": -0.5348564624786377, "compression_ratio": 1.246031746031746, "no_speech_prob": 0.29398617148399353}, {"id": 1, "seek": 0, "start": 19.400000000000002, "end": 25.48, "text": " Hello everyone. Thanks to the organizers for having me. I've been working in Rust for about", "tokens": [51334, 2425, 1518, 13, 2561, 281, 264, 35071, 337, 1419, 385, 13, 286, 600, 668, 1364, 294, 34952, 337, 466, 51638], "temperature": 0.0, "avg_logprob": -0.5348564624786377, "compression_ratio": 1.246031746031746, "no_speech_prob": 0.29398617148399353}, {"id": 2, "seek": 2548, "start": 25.48, "end": 30.48, "text": " three years now, mostly contributing to QuickWidth. QuickWidth is an open source distributed search", "tokens": [50364, 1045, 924, 586, 11, 5240, 19270, 281, 12101, 54, 327, 392, 13, 12101, 54, 327, 392, 307, 364, 1269, 4009, 12631, 3164, 50614], "temperature": 0.0, "avg_logprob": -0.22785676043966543, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.2607927918434143}, {"id": 3, "seek": 2548, "start": 30.48, "end": 36.28, "text": " engine for log and traces. We'll be presenting tomorrow in the monitoring and observability", "tokens": [50614, 2848, 337, 3565, 293, 26076, 13, 492, 603, 312, 15578, 4153, 294, 264, 11028, 293, 9951, 2310, 50904], "temperature": 0.0, "avg_logprob": -0.22785676043966543, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.2607927918434143}, {"id": 4, "seek": 2548, "start": 36.28, "end": 43.28, "text": " room. And if you want to follow along, you'll find a slide on this website. There will be", "tokens": [50904, 1808, 13, 400, 498, 291, 528, 281, 1524, 2051, 11, 291, 603, 915, 257, 4137, 322, 341, 3144, 13, 821, 486, 312, 51254], "temperature": 0.0, "avg_logprob": -0.22785676043966543, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.2607927918434143}, {"id": 5, "seek": 2548, "start": 44.120000000000005, "end": 50.400000000000006, "text": " quite a bit of code during the presentation, so that may be easier. So as you know today,", "tokens": [51296, 1596, 257, 857, 295, 3089, 1830, 264, 5860, 11, 370, 300, 815, 312, 3571, 13, 407, 382, 291, 458, 965, 11, 51610], "temperature": 0.0, "avg_logprob": -0.22785676043966543, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.2607927918434143}, {"id": 6, "seek": 5040, "start": 50.44, "end": 55.68, "text": " we're talking about tower. It's a crate for building modular and networking clients and", "tokens": [50366, 321, 434, 1417, 466, 10567, 13, 467, 311, 257, 42426, 337, 2390, 31111, 293, 17985, 6982, 293, 50628], "temperature": 0.0, "avg_logprob": -0.2533644581889058, "compression_ratio": 1.5150214592274678, "no_speech_prob": 0.12403788417577744}, {"id": 7, "seek": 5040, "start": 55.68, "end": 62.68, "text": " servers. If you're using widely used traits in the Rust ecosystem such as AXM, WAP, Tonic,", "tokens": [50628, 15909, 13, 759, 291, 434, 1228, 13371, 1143, 19526, 294, 264, 34952, 11311, 1270, 382, 316, 55, 44, 11, 343, 4715, 11, 11385, 299, 11, 50978], "temperature": 0.0, "avg_logprob": -0.2533644581889058, "compression_ratio": 1.5150214592274678, "no_speech_prob": 0.12403788417577744}, {"id": 8, "seek": 5040, "start": 66.48, "end": 72.24, "text": " you're using tower under the hood. Maybe you don't know it. And everything's based on", "tokens": [51168, 291, 434, 1228, 10567, 833, 264, 13376, 13, 2704, 291, 500, 380, 458, 309, 13, 400, 1203, 311, 2361, 322, 51456], "temperature": 0.0, "avg_logprob": -0.2533644581889058, "compression_ratio": 1.5150214592274678, "no_speech_prob": 0.12403788417577744}, {"id": 9, "seek": 5040, "start": 72.24, "end": 78.24, "text": " the service trait that I'm going to describe at length during this talk. But before I do", "tokens": [51456, 264, 2643, 22538, 300, 286, 478, 516, 281, 6786, 412, 4641, 1830, 341, 751, 13, 583, 949, 286, 360, 51756], "temperature": 0.0, "avg_logprob": -0.2533644581889058, "compression_ratio": 1.5150214592274678, "no_speech_prob": 0.12403788417577744}, {"id": 10, "seek": 7824, "start": 78.32, "end": 84.03999999999999, "text": " that, I would like us to take a step back and think about why we need tower. So I want", "tokens": [50368, 300, 11, 286, 576, 411, 505, 281, 747, 257, 1823, 646, 293, 519, 466, 983, 321, 643, 10567, 13, 407, 286, 528, 50654], "temperature": 0.0, "avg_logprob": -0.2247884767549532, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.004898711107671261}, {"id": 11, "seek": 7824, "start": 84.03999999999999, "end": 90.56, "text": " you to think that you are a web developer. You're working with any web framework in an", "tokens": [50654, 291, 281, 519, 300, 291, 366, 257, 3670, 10754, 13, 509, 434, 1364, 365, 604, 3670, 8388, 294, 364, 50980], "temperature": 0.0, "avg_logprob": -0.2247884767549532, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.004898711107671261}, {"id": 12, "seek": 7824, "start": 90.56, "end": 96.19999999999999, "text": " imaginary dynamic language. And you ask to write a simple handler to get a user from a", "tokens": [50980, 26164, 8546, 2856, 13, 400, 291, 1029, 281, 2464, 257, 2199, 41967, 281, 483, 257, 4195, 490, 257, 51262], "temperature": 0.0, "avg_logprob": -0.2247884767549532, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.004898711107671261}, {"id": 13, "seek": 7824, "start": 96.19999999999999, "end": 101.44, "text": " database. So you probably would come up with something like this, a function that you would", "tokens": [51262, 8149, 13, 407, 291, 1391, 576, 808, 493, 365, 746, 411, 341, 11, 257, 2445, 300, 291, 576, 51524], "temperature": 0.0, "avg_logprob": -0.2247884767549532, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.004898711107671261}, {"id": 14, "seek": 7824, "start": 101.44, "end": 107.94, "text": " call getUser that would take your request in, add some logging, get the user from the database,", "tokens": [51524, 818, 483, 52, 12484, 300, 576, 747, 428, 5308, 294, 11, 909, 512, 27991, 11, 483, 264, 4195, 490, 264, 8149, 11, 51849], "temperature": 0.0, "avg_logprob": -0.2247884767549532, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.004898711107671261}, {"id": 15, "seek": 10794, "start": 108.89999999999999, "end": 115.3, "text": " save the response, log again, return the response you've done, and you're pretty happy. Pretty", "tokens": [50412, 3155, 264, 4134, 11, 3565, 797, 11, 2736, 264, 4134, 291, 600, 1096, 11, 293, 291, 434, 1238, 2055, 13, 10693, 50732], "temperature": 0.0, "avg_logprob": -0.1823545282537287, "compression_ratio": 1.7984189723320159, "no_speech_prob": 0.00037986025563441217}, {"id": 16, "seek": 10794, "start": 115.3, "end": 121.3, "text": " quickly, you would realize that maybe there's another way to do it. Maybe you could decouple", "tokens": [50732, 2661, 11, 291, 576, 4325, 300, 1310, 456, 311, 1071, 636, 281, 360, 309, 13, 2704, 291, 727, 979, 263, 781, 51032], "temperature": 0.0, "avg_logprob": -0.1823545282537287, "compression_ratio": 1.7984189723320159, "no_speech_prob": 0.00037986025563441217}, {"id": 17, "seek": 10794, "start": 121.3, "end": 126.3, "text": " the fetching the user part that's strictly about the database and all that stuff from", "tokens": [51032, 264, 23673, 278, 264, 4195, 644, 300, 311, 20792, 466, 264, 8149, 293, 439, 300, 1507, 490, 51282], "temperature": 0.0, "avg_logprob": -0.1823545282537287, "compression_ratio": 1.7984189723320159, "no_speech_prob": 0.00037986025563441217}, {"id": 18, "seek": 10794, "start": 126.3, "end": 132.06, "text": " the logging. Like the pattern wise, I'm going to log something before the request, and I'm", "tokens": [51282, 264, 27991, 13, 1743, 264, 5102, 10829, 11, 286, 478, 516, 281, 3565, 746, 949, 264, 5308, 11, 293, 286, 478, 51570], "temperature": 0.0, "avg_logprob": -0.1823545282537287, "compression_ratio": 1.7984189723320159, "no_speech_prob": 0.00037986025563441217}, {"id": 19, "seek": 10794, "start": 132.06, "end": 135.34, "text": " going to log something after the response. It doesn't need to know about everything that's", "tokens": [51570, 516, 281, 3565, 746, 934, 264, 4134, 13, 467, 1177, 380, 643, 281, 458, 466, 1203, 300, 311, 51734], "temperature": 0.0, "avg_logprob": -0.1823545282537287, "compression_ratio": 1.7984189723320159, "no_speech_prob": 0.00037986025563441217}, {"id": 20, "seek": 13534, "start": 135.38, "end": 140.3, "text": " user related. So maybe you would write two functions. The first one would be with logging that", "tokens": [50366, 4195, 4077, 13, 407, 1310, 291, 576, 2464, 732, 6828, 13, 440, 700, 472, 576, 312, 365, 27991, 300, 50612], "temperature": 0.0, "avg_logprob": -0.1942316519247519, "compression_ratio": 1.889795918367347, "no_speech_prob": 0.003535002702847123}, {"id": 21, "seek": 13534, "start": 140.3, "end": 145.38, "text": " would take two parameters, the request still, but also a handler that would be a generic", "tokens": [50612, 576, 747, 732, 9834, 11, 264, 5308, 920, 11, 457, 611, 257, 41967, 300, 576, 312, 257, 19577, 50866], "temperature": 0.0, "avg_logprob": -0.1942316519247519, "compression_ratio": 1.889795918367347, "no_speech_prob": 0.003535002702847123}, {"id": 22, "seek": 13534, "start": 145.38, "end": 153.58, "text": " function that also that returns a, accepts a request. And then you would write your getUser", "tokens": [50866, 2445, 300, 611, 300, 11247, 257, 11, 33538, 257, 5308, 13, 400, 550, 291, 576, 2464, 428, 483, 52, 12484, 51276], "temperature": 0.0, "avg_logprob": -0.1942316519247519, "compression_ratio": 1.889795918367347, "no_speech_prob": 0.003535002702847123}, {"id": 23, "seek": 13534, "start": 153.58, "end": 159.18, "text": " simply. It'll be a bit more simple, very focused on what it has to do. And then you would compose", "tokens": [51276, 2935, 13, 467, 603, 312, 257, 857, 544, 2199, 11, 588, 5178, 322, 437, 309, 575, 281, 360, 13, 400, 550, 291, 576, 35925, 51556], "temperature": 0.0, "avg_logprob": -0.1942316519247519, "compression_ratio": 1.889795918367347, "no_speech_prob": 0.003535002702847123}, {"id": 24, "seek": 13534, "start": 159.18, "end": 163.46, "text": " your with logging function with your getUser function, and you would achieve what you had", "tokens": [51556, 428, 365, 27991, 2445, 365, 428, 483, 52, 12484, 2445, 11, 293, 291, 576, 4584, 437, 291, 632, 51770], "temperature": 0.0, "avg_logprob": -0.1942316519247519, "compression_ratio": 1.889795918367347, "no_speech_prob": 0.003535002702847123}, {"id": 25, "seek": 16346, "start": 163.5, "end": 167.34, "text": " achieved before. But now you have that other function with logging that you can use for", "tokens": [50366, 11042, 949, 13, 583, 586, 291, 362, 300, 661, 2445, 365, 27991, 300, 291, 393, 764, 337, 50558], "temperature": 0.0, "avg_logprob": -0.17515812149967055, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.00045101376599632204}, {"id": 26, "seek": 16346, "start": 167.34, "end": 173.34, "text": " like all your end-to-end lists. And isn't that the sense of programming? We want to write", "tokens": [50558, 411, 439, 428, 917, 12, 1353, 12, 521, 14511, 13, 400, 1943, 380, 300, 264, 2020, 295, 9410, 30, 492, 528, 281, 2464, 50858], "temperature": 0.0, "avg_logprob": -0.17515812149967055, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.00045101376599632204}, {"id": 27, "seek": 16346, "start": 173.34, "end": 180.18, "text": " generic and reasonable functions that are really easy to compose. For the purpose of", "tokens": [50858, 19577, 293, 10585, 6828, 300, 366, 534, 1858, 281, 35925, 13, 1171, 264, 4334, 295, 51200], "temperature": 0.0, "avg_logprob": -0.17515812149967055, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.00045101376599632204}, {"id": 28, "seek": 16346, "start": 180.18, "end": 186.74, "text": " this talk, we're specifically interested in decorators. So the decorator pattern is basically", "tokens": [51200, 341, 751, 11, 321, 434, 4682, 3102, 294, 7919, 3391, 13, 407, 264, 7919, 1639, 5102, 307, 1936, 51528], "temperature": 0.0, "avg_logprob": -0.17515812149967055, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.00045101376599632204}, {"id": 29, "seek": 18674, "start": 186.86, "end": 194.42000000000002, "text": " when a function or a class wraps another function. It applies new behaviors before or after calling", "tokens": [50370, 562, 257, 2445, 420, 257, 1508, 25831, 1071, 2445, 13, 467, 13165, 777, 15501, 949, 420, 934, 5141, 50748], "temperature": 0.0, "avg_logprob": -0.22949197557237414, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.016906719654798508}, {"id": 30, "seek": 18674, "start": 194.42000000000002, "end": 198.62, "text": " the inner function. It doesn't know about that inner function. It's totally opaque, and it", "tokens": [50748, 264, 7284, 2445, 13, 467, 1177, 380, 458, 466, 300, 7284, 2445, 13, 467, 311, 3879, 42687, 11, 293, 309, 50958], "temperature": 0.0, "avg_logprob": -0.22949197557237414, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.016906719654798508}, {"id": 31, "seek": 18674, "start": 198.62, "end": 205.54000000000002, "text": " doesn't matter, we don't care. It doesn't modify the behavior of the inner function. And you've", "tokens": [50958, 1177, 380, 1871, 11, 321, 500, 380, 1127, 13, 467, 1177, 380, 16927, 264, 5223, 295, 264, 7284, 2445, 13, 400, 291, 600, 51304], "temperature": 0.0, "avg_logprob": -0.22949197557237414, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.016906719654798508}, {"id": 32, "seek": 18674, "start": 205.54000000000002, "end": 211.10000000000002, "text": " heard, you've used decorators sometimes with different names, very often millwares in the", "tokens": [51304, 2198, 11, 291, 600, 1143, 7919, 3391, 2171, 365, 819, 5288, 11, 588, 2049, 1728, 4151, 495, 294, 264, 51582], "temperature": 0.0, "avg_logprob": -0.22949197557237414, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.016906719654798508}, {"id": 33, "seek": 21110, "start": 211.9, "end": 220.06, "text": " context of client and servers, proxies. This is a term that's coming up a lot. And then what you", "tokens": [50404, 4319, 295, 6423, 293, 15909, 11, 447, 87, 530, 13, 639, 307, 257, 1433, 300, 311, 1348, 493, 257, 688, 13, 400, 550, 437, 291, 50812], "temperature": 0.0, "avg_logprob": -0.16041467647359828, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.0022163428366184235}, {"id": 34, "seek": 21110, "start": 220.06, "end": 227.01999999999998, "text": " can do with those is you have your handler at the bottom. You can also call it a leaf, and you", "tokens": [50812, 393, 360, 365, 729, 307, 291, 362, 428, 41967, 412, 264, 2767, 13, 509, 393, 611, 818, 309, 257, 10871, 11, 293, 291, 51160], "temperature": 0.0, "avg_logprob": -0.16041467647359828, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.0022163428366184235}, {"id": 35, "seek": 21110, "start": 227.01999999999998, "end": 232.1, "text": " stack millwares with different behaviors on top of it. And you apply different behaviors depending", "tokens": [51160, 8630, 1728, 4151, 495, 365, 819, 15501, 322, 1192, 295, 309, 13, 400, 291, 3079, 819, 15501, 5413, 51414], "temperature": 0.0, "avg_logprob": -0.16041467647359828, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.0022163428366184235}, {"id": 36, "seek": 21110, "start": 232.1, "end": 236.98, "text": " on what you need. And then you can build a nice library of millwares that you can share, that", "tokens": [51414, 322, 437, 291, 643, 13, 400, 550, 291, 393, 1322, 257, 1481, 6405, 295, 1728, 4151, 495, 300, 291, 393, 2073, 11, 300, 51658], "temperature": 0.0, "avg_logprob": -0.16041467647359828, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.0022163428366184235}, {"id": 37, "seek": 23698, "start": 237.29999999999998, "end": 245.7, "text": " you can use from different libraries, and so on and so forth. In that example, we were using a", "tokens": [50380, 291, 393, 764, 490, 819, 15148, 11, 293, 370, 322, 293, 370, 5220, 13, 682, 300, 1365, 11, 321, 645, 1228, 257, 50800], "temperature": 0.0, "avg_logprob": -0.19043384212078435, "compression_ratio": 1.5958333333333334, "no_speech_prob": 0.0006065721390768886}, {"id": 38, "seek": 23698, "start": 245.7, "end": 252.89999999999998, "text": " dynamic language. It was really, really easy to do. Duc typing gave us great flexibility. We don't", "tokens": [50800, 8546, 2856, 13, 467, 390, 534, 11, 534, 1858, 281, 360, 13, 413, 1311, 18444, 2729, 505, 869, 12635, 13, 492, 500, 380, 51160], "temperature": 0.0, "avg_logprob": -0.19043384212078435, "compression_ratio": 1.5958333333333334, "no_speech_prob": 0.0006065721390768886}, {"id": 39, "seek": 23698, "start": 252.89999999999998, "end": 257.65999999999997, "text": " really care about the types of inputs and outputs. When we code, we think about it, do they match", "tokens": [51160, 534, 1127, 466, 264, 3467, 295, 15743, 293, 23930, 13, 1133, 321, 3089, 11, 321, 519, 466, 309, 11, 360, 436, 2995, 51398], "temperature": 0.0, "avg_logprob": -0.19043384212078435, "compression_ratio": 1.5958333333333334, "no_speech_prob": 0.0006065721390768886}, {"id": 40, "seek": 23698, "start": 257.65999999999997, "end": 261.78, "text": " implicitly? If they do, we think it's going to work. We ship it to production. We cross our", "tokens": [51398, 26947, 356, 30, 759, 436, 360, 11, 321, 519, 309, 311, 516, 281, 589, 13, 492, 5374, 309, 281, 4265, 13, 492, 3278, 527, 51604], "temperature": 0.0, "avg_logprob": -0.19043384212078435, "compression_ratio": 1.5958333333333334, "no_speech_prob": 0.0006065721390768886}, {"id": 41, "seek": 26178, "start": 261.82, "end": 273.14, "text": " fingers. But implicitly, everything should match and then everything should work fine. So this is", "tokens": [50366, 7350, 13, 583, 26947, 356, 11, 1203, 820, 2995, 293, 550, 1203, 820, 589, 2489, 13, 407, 341, 307, 50932], "temperature": 0.0, "avg_logprob": -0.23895881652832032, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.0025486627127975225}, {"id": 42, "seek": 26178, "start": 273.14, "end": 280.02, "text": " what we're trying to do with TauR in the Rust ecosystem. But this time, we still want to compose", "tokens": [50932, 437, 321, 434, 1382, 281, 360, 365, 314, 1459, 49, 294, 264, 34952, 11311, 13, 583, 341, 565, 11, 321, 920, 528, 281, 35925, 51276], "temperature": 0.0, "avg_logprob": -0.23895881652832032, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.0025486627127975225}, {"id": 43, "seek": 26178, "start": 280.02, "end": 285.5, "text": " functions, but we want to do it in a very type safe manner. And we want to be still very flexible.", "tokens": [51276, 6828, 11, 457, 321, 528, 281, 360, 309, 294, 257, 588, 2010, 3273, 9060, 13, 400, 321, 528, 281, 312, 920, 588, 11358, 13, 51550], "temperature": 0.0, "avg_logprob": -0.23895881652832032, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.0025486627127975225}, {"id": 44, "seek": 28550, "start": 286.34, "end": 293.46, "text": " And this is when the TauR service trade comes into play. It is the common interface that allows", "tokens": [50406, 400, 341, 307, 562, 264, 314, 1459, 49, 2643, 4923, 1487, 666, 862, 13, 467, 307, 264, 2689, 9226, 300, 4045, 50762], "temperature": 0.0, "avg_logprob": -0.19929414252712302, "compression_ratio": 1.5, "no_speech_prob": 0.0012243329547345638}, {"id": 45, "seek": 28550, "start": 293.46, "end": 300.74, "text": " us implementing components in a protocol, agnostic, and composable way. So let me describe the trade.", "tokens": [50762, 505, 18114, 6677, 294, 257, 10336, 11, 623, 77, 19634, 11, 293, 10199, 712, 636, 13, 407, 718, 385, 6786, 264, 4923, 13, 51126], "temperature": 0.0, "avg_logprob": -0.19929414252712302, "compression_ratio": 1.5, "no_speech_prob": 0.0012243329547345638}, {"id": 46, "seek": 28550, "start": 300.74, "end": 308.9, "text": " It has one generic parameter, request. It's called request by convention, but it can be any type", "tokens": [51126, 467, 575, 472, 19577, 13075, 11, 5308, 13, 467, 311, 1219, 5308, 538, 10286, 11, 457, 309, 393, 312, 604, 2010, 51534], "temperature": 0.0, "avg_logprob": -0.19929414252712302, "compression_ratio": 1.5, "no_speech_prob": 0.0012243329547345638}, {"id": 47, "seek": 30890, "start": 309.02, "end": 315.85999999999996, "text": " and anything. An associated type, response, same thing. Cold response by convention, but it could", "tokens": [50370, 293, 1340, 13, 1107, 6615, 2010, 11, 4134, 11, 912, 551, 13, 16918, 4134, 538, 10286, 11, 457, 309, 727, 50712], "temperature": 0.0, "avg_logprob": -0.17196734746297201, "compression_ratio": 1.8509615384615385, "no_speech_prob": 0.017411578446626663}, {"id": 48, "seek": 30890, "start": 315.85999999999996, "end": 324.62, "text": " really be anything. Error type also could be anything. And finally, a future type. The future", "tokens": [50712, 534, 312, 1340, 13, 3300, 2874, 2010, 611, 727, 312, 1340, 13, 400, 2721, 11, 257, 2027, 2010, 13, 440, 2027, 51150], "temperature": 0.0, "avg_logprob": -0.17196734746297201, "compression_ratio": 1.8509615384615385, "no_speech_prob": 0.017411578446626663}, {"id": 49, "seek": 30890, "start": 324.62, "end": 331.7, "text": " type is constrained by the future trade. So it has to be a future. And you'll notice that the output", "tokens": [51150, 2010, 307, 38901, 538, 264, 2027, 4923, 13, 407, 309, 575, 281, 312, 257, 2027, 13, 400, 291, 603, 3449, 300, 264, 5598, 51504], "temperature": 0.0, "avg_logprob": -0.17196734746297201, "compression_ratio": 1.8509615384615385, "no_speech_prob": 0.017411578446626663}, {"id": 50, "seek": 30890, "start": 331.7, "end": 337.78, "text": " of the future is fixed for you. It has to be a result of the response and the error that you", "tokens": [51504, 295, 264, 2027, 307, 6806, 337, 291, 13, 467, 575, 281, 312, 257, 1874, 295, 264, 4134, 293, 264, 6713, 300, 291, 51808], "temperature": 0.0, "avg_logprob": -0.17196734746297201, "compression_ratio": 1.8509615384615385, "no_speech_prob": 0.017411578446626663}, {"id": 51, "seek": 33778, "start": 337.78, "end": 345.21999999999997, "text": " define. So the only choice is choosing a future type. The output is always chosen when you say,", "tokens": [50364, 6964, 13, 407, 264, 787, 3922, 307, 10875, 257, 2027, 2010, 13, 440, 5598, 307, 1009, 8614, 562, 291, 584, 11, 50736], "temperature": 0.0, "avg_logprob": -0.17970078332083567, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0010970920557156205}, {"id": 52, "seek": 33778, "start": 345.21999999999997, "end": 350.53999999999996, "text": " I want this kind of response, this kind of error. And then in the trade, you have two methods,", "tokens": [50736, 286, 528, 341, 733, 295, 4134, 11, 341, 733, 295, 6713, 13, 400, 550, 294, 264, 4923, 11, 291, 362, 732, 7150, 11, 51002], "temperature": 0.0, "avg_logprob": -0.17970078332083567, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0010970920557156205}, {"id": 53, "seek": 33778, "start": 350.53999999999996, "end": 361.21999999999997, "text": " the pull ready function and the call. Call takes itself mutably, accepts the request,", "tokens": [51002, 264, 2235, 1919, 2445, 293, 264, 818, 13, 7807, 2516, 2564, 5839, 1188, 11, 33538, 264, 5308, 11, 51536], "temperature": 0.0, "avg_logprob": -0.17970078332083567, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0010970920557156205}, {"id": 54, "seek": 36122, "start": 362.18, "end": 369.86, "text": " return the future. The future you define. So your mental model should be much easier than this", "tokens": [50412, 2736, 264, 2027, 13, 440, 2027, 291, 6964, 13, 407, 428, 4973, 2316, 820, 312, 709, 3571, 813, 341, 50796], "temperature": 0.0, "avg_logprob": -0.21652327192590592, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.003648281330242753}, {"id": 55, "seek": 36122, "start": 369.86, "end": 375.66, "text": " trade. It's just simply be, this is just a generic async function. This is the same thing. The", "tokens": [50796, 4923, 13, 467, 311, 445, 2935, 312, 11, 341, 307, 445, 257, 19577, 382, 34015, 2445, 13, 639, 307, 264, 912, 551, 13, 440, 51086], "temperature": 0.0, "avg_logprob": -0.21652327192590592, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.003648281330242753}, {"id": 56, "seek": 36122, "start": 375.66, "end": 381.86, "text": " trade gives us that flexibility that we wanted and gives us the type safety that we wanted. But", "tokens": [51086, 4923, 2709, 505, 300, 12635, 300, 321, 1415, 293, 2709, 505, 264, 2010, 4514, 300, 321, 1415, 13, 583, 51396], "temperature": 0.0, "avg_logprob": -0.21652327192590592, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.003648281330242753}, {"id": 57, "seek": 36122, "start": 381.86, "end": 389.70000000000005, "text": " really, this is the mental model of what is a service trade. With all of twist, you have that", "tokens": [51396, 534, 11, 341, 307, 264, 4973, 2316, 295, 437, 307, 257, 2643, 4923, 13, 2022, 439, 295, 8203, 11, 291, 362, 300, 51788], "temperature": 0.0, "avg_logprob": -0.21652327192590592, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.003648281330242753}, {"id": 58, "seek": 38970, "start": 389.74, "end": 398.09999999999997, "text": " additional pull ready function that needs to be called once before calling call. And it provides", "tokens": [50366, 4497, 2235, 1919, 2445, 300, 2203, 281, 312, 1219, 1564, 949, 5141, 818, 13, 400, 309, 6417, 50784], "temperature": 0.0, "avg_logprob": -0.19264144584780835, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.0006984620704315603}, {"id": 59, "seek": 38970, "start": 398.09999999999997, "end": 407.41999999999996, "text": " a way to provide back pressure to the caller. So the pull ready implementation for a service", "tokens": [50784, 257, 636, 281, 2893, 646, 3321, 281, 264, 48324, 13, 407, 264, 2235, 1919, 11420, 337, 257, 2643, 51250], "temperature": 0.0, "avg_logprob": -0.19264144584780835, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.0006984620704315603}, {"id": 60, "seek": 38970, "start": 407.41999999999996, "end": 414.21999999999997, "text": " without any external dependencies for the hello service that we're going to define is very simple.", "tokens": [51250, 1553, 604, 8320, 36606, 337, 264, 7751, 2643, 300, 321, 434, 516, 281, 6964, 307, 588, 2199, 13, 51590], "temperature": 0.0, "avg_logprob": -0.19264144584780835, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.0006984620704315603}, {"id": 61, "seek": 41422, "start": 414.42, "end": 419.5, "text": " Pull ready return is always ready and returns okay. If you're implementing something else,", "tokens": [50374, 15074, 1919, 2736, 307, 1009, 1919, 293, 11247, 1392, 13, 759, 291, 434, 18114, 746, 1646, 11, 50628], "temperature": 0.0, "avg_logprob": -0.19599751992659134, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.004067625850439072}, {"id": 62, "seek": 41422, "start": 419.5, "end": 425.34000000000003, "text": " if it's not ready, you return pending. And later on, the task will be pulled again. Hopefully,", "tokens": [50628, 498, 309, 311, 406, 1919, 11, 291, 2736, 32110, 13, 400, 1780, 322, 11, 264, 5633, 486, 312, 7373, 797, 13, 10429, 11, 50920], "temperature": 0.0, "avg_logprob": -0.19599751992659134, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.004067625850439072}, {"id": 63, "seek": 41422, "start": 425.34000000000003, "end": 433.18, "text": " it'll be ready. If you're writing a service that has external dependencies, for instance,", "tokens": [50920, 309, 603, 312, 1919, 13, 759, 291, 434, 3579, 257, 2643, 300, 575, 8320, 36606, 11, 337, 5197, 11, 51312], "temperature": 0.0, "avg_logprob": -0.19599751992659134, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.004067625850439072}, {"id": 64, "seek": 41422, "start": 433.18, "end": 438.82000000000005, "text": " you depend on a database, you need to acquire that database connection. So all the services that", "tokens": [51312, 291, 5672, 322, 257, 8149, 11, 291, 643, 281, 20001, 300, 8149, 4984, 13, 407, 439, 264, 3328, 300, 51594], "temperature": 0.0, "avg_logprob": -0.19599751992659134, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.004067625850439072}, {"id": 65, "seek": 43882, "start": 438.86, "end": 444.7, "text": " have external dependency need to load up upfront to receive the need. And so they can say when", "tokens": [50366, 362, 8320, 33621, 643, 281, 3677, 493, 30264, 281, 4774, 264, 643, 13, 400, 370, 436, 393, 584, 562, 50658], "temperature": 0.0, "avg_logprob": -0.2135043412112118, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.004263032227754593}, {"id": 66, "seek": 43882, "start": 444.7, "end": 451.18, "text": " they're ready or not to the caller. So in this example, if we haven't acquired yet connection,", "tokens": [50658, 436, 434, 1919, 420, 406, 281, 264, 48324, 13, 407, 294, 341, 1365, 11, 498, 321, 2378, 380, 17554, 1939, 4984, 11, 50982], "temperature": 0.0, "avg_logprob": -0.2135043412112118, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.004263032227754593}, {"id": 67, "seek": 43882, "start": 451.18, "end": 459.98, "text": " we will pull our pull of connection to acquire a connection. We will save it mutably in our", "tokens": [50982, 321, 486, 2235, 527, 2235, 295, 4984, 281, 20001, 257, 4984, 13, 492, 486, 3155, 309, 5839, 1188, 294, 527, 51422], "temperature": 0.0, "avg_logprob": -0.2135043412112118, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.004263032227754593}, {"id": 68, "seek": 43882, "start": 459.98, "end": 468.65999999999997, "text": " internal state. And then later in call, we actually consume the connection using take. So", "tokens": [51422, 6920, 1785, 13, 400, 550, 1780, 294, 818, 11, 321, 767, 14732, 264, 4984, 1228, 747, 13, 407, 51856], "temperature": 0.0, "avg_logprob": -0.2135043412112118, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.004263032227754593}, {"id": 69, "seek": 46866, "start": 468.70000000000005, "end": 473.5, "text": " that's why the tariff trade is mutable because it's not necessarily state less. It can be", "tokens": [50366, 300, 311, 983, 264, 3112, 3661, 4923, 307, 5839, 712, 570, 309, 311, 406, 4725, 1785, 1570, 13, 467, 393, 312, 50606], "temperature": 0.0, "avg_logprob": -0.2843477725982666, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0007434870931319892}, {"id": 70, "seek": 46866, "start": 473.5, "end": 479.46000000000004, "text": " stateful. So you can manage your resources. And you see with this example why you need to call", "tokens": [50606, 1785, 906, 13, 407, 291, 393, 3067, 428, 3593, 13, 400, 291, 536, 365, 341, 1365, 983, 291, 643, 281, 818, 50904], "temperature": 0.0, "avg_logprob": -0.2843477725982666, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0007434870931319892}, {"id": 71, "seek": 46866, "start": 479.46000000000004, "end": 489.26000000000005, "text": " pull ready always at least once before first before call. And for the services that I'm", "tokens": [50904, 2235, 1919, 1009, 412, 1935, 1564, 949, 700, 949, 818, 13, 400, 337, 264, 3328, 300, 286, 478, 51394], "temperature": 0.0, "avg_logprob": -0.2843477725982666, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0007434870931319892}, {"id": 72, "seek": 46866, "start": 489.26000000000005, "end": 495.70000000000005, "text": " you know, where's so they basically wrap an inner service, you usually implementation is really", "tokens": [51394, 291, 458, 11, 689, 311, 370, 436, 1936, 7019, 364, 7284, 2643, 11, 291, 2673, 11420, 307, 534, 51716], "temperature": 0.0, "avg_logprob": -0.2843477725982666, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0007434870931319892}, {"id": 73, "seek": 49570, "start": 495.74, "end": 500.5, "text": " straightforward. You delegate to the inner service. So you call pull pull ready on the inner", "tokens": [50366, 15325, 13, 509, 40999, 281, 264, 7284, 2643, 13, 407, 291, 818, 2235, 2235, 1919, 322, 264, 7284, 50604], "temperature": 0.0, "avg_logprob": -0.2446085961310418, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.004899259191006422}, {"id": 74, "seek": 49570, "start": 500.5, "end": 506.14, "text": " service and you're done. So if you're not doing something too fancy in your service, usually", "tokens": [50604, 2643, 293, 291, 434, 1096, 13, 407, 498, 291, 434, 406, 884, 746, 886, 10247, 294, 428, 2643, 11, 2673, 50886], "temperature": 0.0, "avg_logprob": -0.2446085961310418, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.004899259191006422}, {"id": 75, "seek": 49570, "start": 506.14, "end": 517.18, "text": " is going to boil down to those three use cases. So it sounds it sounds simple on paper. Why does", "tokens": [50886, 307, 516, 281, 13329, 760, 281, 729, 1045, 764, 3331, 13, 407, 309, 3263, 309, 3263, 2199, 322, 3035, 13, 1545, 775, 51438], "temperature": 0.0, "avg_logprob": -0.2446085961310418, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.004899259191006422}, {"id": 76, "seek": 49570, "start": 517.18, "end": 522.1, "text": " it get complex? I think people are a bit afraid of like towers tower and towers services in general,", "tokens": [51438, 309, 483, 3997, 30, 286, 519, 561, 366, 257, 857, 4638, 295, 411, 25045, 10567, 293, 25045, 3328, 294, 2674, 11, 51684], "temperature": 0.0, "avg_logprob": -0.2446085961310418, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.004899259191006422}, {"id": 77, "seek": 52210, "start": 522.46, "end": 527.58, "text": " could they use a lot of generics. So it's a bit of it's not always easy to write. It's even hard", "tokens": [50382, 727, 436, 764, 257, 688, 295, 1337, 1167, 13, 407, 309, 311, 257, 857, 295, 309, 311, 406, 1009, 1858, 281, 2464, 13, 467, 311, 754, 1152, 50638], "temperature": 0.0, "avg_logprob": -0.2634606080896714, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.014934348873794079}, {"id": 78, "seek": 52210, "start": 527.9, "end": 535.02, "text": " to read and it's even harder to write. It's using all the the rest construct heavily. So you have", "tokens": [50654, 281, 1401, 293, 309, 311, 754, 6081, 281, 2464, 13, 467, 311, 1228, 439, 264, 264, 1472, 7690, 10950, 13, 407, 291, 362, 51010], "temperature": 0.0, "avg_logprob": -0.2634606080896714, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.014934348873794079}, {"id": 79, "seek": 52210, "start": 535.02, "end": 541.82, "text": " to be comfortable with lifetime lifetime, sensing market rates, what is a thing is going to expose", "tokens": [51010, 281, 312, 4619, 365, 11364, 11364, 11, 30654, 2142, 6846, 11, 437, 307, 257, 551, 307, 516, 281, 19219, 51350], "temperature": 0.0, "avg_logprob": -0.2634606080896714, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.014934348873794079}, {"id": 80, "seek": 52210, "start": 541.82, "end": 548.38, "text": " you to that. So if you've been using rust as a better C++ trying to avoid those concepts, writing", "tokens": [51350, 291, 281, 300, 13, 407, 498, 291, 600, 668, 1228, 15259, 382, 257, 1101, 383, 25472, 1382, 281, 5042, 729, 10392, 11, 3579, 51678], "temperature": 0.0, "avg_logprob": -0.2634606080896714, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.014934348873794079}, {"id": 81, "seek": 54838, "start": 548.42, "end": 553.9399999999999, "text": " towers services are going to be a bit challenging. But but once you get more comfortable, it'd be", "tokens": [50366, 25045, 3328, 366, 516, 281, 312, 257, 857, 7595, 13, 583, 457, 1564, 291, 483, 544, 4619, 11, 309, 1116, 312, 50642], "temperature": 0.0, "avg_logprob": -0.2342473080283717, "compression_ratio": 1.8587786259541985, "no_speech_prob": 0.0027541397139430046}, {"id": 82, "seek": 54838, "start": 553.9399999999999, "end": 557.82, "text": " very rewarding because like because you're going to be exposed to those concepts. At some point,", "tokens": [50642, 588, 20063, 570, 411, 570, 291, 434, 516, 281, 312, 9495, 281, 729, 10392, 13, 1711, 512, 935, 11, 50836], "temperature": 0.0, "avg_logprob": -0.2342473080283717, "compression_ratio": 1.8587786259541985, "no_speech_prob": 0.0027541397139430046}, {"id": 83, "seek": 54838, "start": 557.82, "end": 563.26, "text": " muscle memory is going to help you and you're going to feel more and more comfortable and you're", "tokens": [50836, 8679, 4675, 307, 516, 281, 854, 291, 293, 291, 434, 516, 281, 841, 544, 293, 544, 4619, 293, 291, 434, 51108], "temperature": 0.0, "avg_logprob": -0.2342473080283717, "compression_ratio": 1.8587786259541985, "no_speech_prob": 0.0027541397139430046}, {"id": 84, "seek": 54838, "start": 563.26, "end": 569.54, "text": " going to get a bit of rust programmer. And in some cases, if you start writing your own futures", "tokens": [51108, 516, 281, 483, 257, 857, 295, 15259, 32116, 13, 400, 294, 512, 3331, 11, 498, 291, 722, 3579, 428, 1065, 26071, 51422], "temperature": 0.0, "avg_logprob": -0.2342473080283717, "compression_ratio": 1.8587786259541985, "no_speech_prob": 0.0027541397139430046}, {"id": 85, "seek": 54838, "start": 569.54, "end": 574.5, "text": " for your services, then you need to know about future pulling and pinning and it gets very complex.", "tokens": [51422, 337, 428, 3328, 11, 550, 291, 643, 281, 458, 466, 2027, 8407, 293, 5447, 773, 293, 309, 2170, 588, 3997, 13, 51670], "temperature": 0.0, "avg_logprob": -0.2342473080283717, "compression_ratio": 1.8587786259541985, "no_speech_prob": 0.0027541397139430046}, {"id": 86, "seek": 57450, "start": 575.5, "end": 581.86, "text": " So the only way to get better understanding services and writing them is just to start simple", "tokens": [50414, 407, 264, 787, 636, 281, 483, 1101, 3701, 3328, 293, 3579, 552, 307, 445, 281, 722, 2199, 50732], "temperature": 0.0, "avg_logprob": -0.18204159456140856, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0003350531042087823}, {"id": 87, "seek": 57450, "start": 581.86, "end": 589.1, "text": " with like a hello services and build upon until working like more complex services. So this is", "tokens": [50732, 365, 411, 257, 7751, 3328, 293, 1322, 3564, 1826, 1364, 411, 544, 3997, 3328, 13, 407, 341, 307, 51094], "temperature": 0.0, "avg_logprob": -0.18204159456140856, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0003350531042087823}, {"id": 88, "seek": 57450, "start": 589.1, "end": 595.06, "text": " exactly what we're going to do during the rest of the talk. So let's implement hello service", "tokens": [51094, 2293, 437, 321, 434, 516, 281, 360, 1830, 264, 1472, 295, 264, 751, 13, 407, 718, 311, 4445, 7751, 2643, 51392], "temperature": 0.0, "avg_logprob": -0.18204159456140856, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0003350531042087823}, {"id": 89, "seek": 57450, "start": 595.06, "end": 599.66, "text": " together. Remember the mental model is just this hello function. This is what we want to do,", "tokens": [51392, 1214, 13, 5459, 264, 4973, 2316, 307, 445, 341, 7751, 2445, 13, 639, 307, 437, 321, 528, 281, 360, 11, 51622], "temperature": 0.0, "avg_logprob": -0.18204159456140856, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0003350531042087823}, {"id": 90, "seek": 59966, "start": 599.6999999999999, "end": 606.02, "text": " but we want to do it the other way. So the input parameter is a hello request. The hello response", "tokens": [50366, 457, 321, 528, 281, 360, 309, 264, 661, 636, 13, 407, 264, 4846, 13075, 307, 257, 7751, 5308, 13, 440, 7751, 4134, 50682], "temperature": 0.0, "avg_logprob": -0.200535519917806, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0013242517597973347}, {"id": 91, "seek": 59966, "start": 606.02, "end": 615.02, "text": " will be returned and we try to implement that simple function. So we start defining a health", "tokens": [50682, 486, 312, 8752, 293, 321, 853, 281, 4445, 300, 2199, 2445, 13, 407, 321, 722, 17827, 257, 1585, 51132], "temperature": 0.0, "avg_logprob": -0.200535519917806, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0013242517597973347}, {"id": 92, "seek": 59966, "start": 615.02, "end": 621.98, "text": " struct and we start implementing service for it. The input parameter is a request. The response", "tokens": [51132, 6594, 293, 321, 722, 18114, 2643, 337, 309, 13, 440, 4846, 13075, 307, 257, 5308, 13, 440, 4134, 51480], "temperature": 0.0, "avg_logprob": -0.200535519917806, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0013242517597973347}, {"id": 93, "seek": 59966, "start": 621.98, "end": 625.66, "text": " type is a response. The error is going to be a little bit specific here. It's going to be", "tokens": [51480, 2010, 307, 257, 4134, 13, 440, 6713, 307, 516, 281, 312, 257, 707, 857, 2685, 510, 13, 467, 311, 516, 281, 312, 51664], "temperature": 0.0, "avg_logprob": -0.200535519917806, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0013242517597973347}, {"id": 94, "seek": 62566, "start": 625.66, "end": 631.5799999999999, "text": " infallible because we're never going to fail with just printing stuff. And now we have to choose", "tokens": [50364, 1536, 336, 964, 570, 321, 434, 1128, 516, 281, 3061, 365, 445, 14699, 1507, 13, 400, 586, 321, 362, 281, 2826, 50660], "temperature": 0.0, "avg_logprob": -0.18514879288211947, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.00648579141125083}, {"id": 95, "seek": 62566, "start": 631.5799999999999, "end": 638.02, "text": " our future type and we have various options. And the first thing we can do is go with a box", "tokens": [50660, 527, 2027, 2010, 293, 321, 362, 3683, 3956, 13, 400, 264, 700, 551, 321, 393, 360, 307, 352, 365, 257, 2424, 50982], "temperature": 0.0, "avg_logprob": -0.18514879288211947, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.00648579141125083}, {"id": 96, "seek": 62566, "start": 638.02, "end": 644.4599999999999, "text": " future. We can define our own type alias or we can reuse the type alias from the futures scrape,", "tokens": [50982, 2027, 13, 492, 393, 6964, 527, 1065, 2010, 419, 4609, 420, 321, 393, 26225, 264, 2010, 419, 4609, 490, 264, 26071, 32827, 11, 51304], "temperature": 0.0, "avg_logprob": -0.18514879288211947, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.00648579141125083}, {"id": 97, "seek": 62566, "start": 644.4599999999999, "end": 652.14, "text": " for instance. And why would we do that? When you get started with writing your own", "tokens": [51304, 337, 5197, 13, 400, 983, 576, 321, 360, 300, 30, 1133, 291, 483, 1409, 365, 3579, 428, 1065, 51688], "temperature": 0.0, "avg_logprob": -0.18514879288211947, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.00648579141125083}, {"id": 98, "seek": 65214, "start": 652.22, "end": 656.34, "text": " service, I would recommend starting with that because it's pretty easy, it's very readable.", "tokens": [50368, 2643, 11, 286, 576, 2748, 2891, 365, 300, 570, 309, 311, 1238, 1858, 11, 309, 311, 588, 49857, 13, 50574], "temperature": 0.0, "avg_logprob": -0.26734999392894987, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.029255732893943787}, {"id": 99, "seek": 65214, "start": 656.34, "end": 665.62, "text": " The cons are you pay a small fee for allocation in dynamic dispatch. It's fine if you're not,", "tokens": [50574, 440, 1014, 366, 291, 1689, 257, 1359, 12054, 337, 27599, 294, 8546, 36729, 13, 467, 311, 2489, 498, 291, 434, 406, 11, 51038], "temperature": 0.0, "avg_logprob": -0.26734999392894987, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.029255732893943787}, {"id": 100, "seek": 65214, "start": 665.62, "end": 672.58, "text": " if it's a client or server that doesn't have an insane amount of QPS, it's totally fine. Sometimes", "tokens": [51038, 498, 309, 311, 257, 6423, 420, 7154, 300, 1177, 380, 362, 364, 10838, 2372, 295, 1249, 6273, 11, 309, 311, 3879, 2489, 13, 4803, 51386], "temperature": 0.0, "avg_logprob": -0.26734999392894987, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.029255732893943787}, {"id": 101, "seek": 65214, "start": 672.58, "end": 678.22, "text": " people are afraid of allocation in dynamic dispatch. If you're working on a client or server that", "tokens": [51386, 561, 366, 4638, 295, 27599, 294, 8546, 36729, 13, 759, 291, 434, 1364, 322, 257, 6423, 420, 7154, 300, 51668], "temperature": 0.0, "avg_logprob": -0.26734999392894987, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.029255732893943787}, {"id": 102, "seek": 67822, "start": 678.3000000000001, "end": 682.78, "text": " doesn't need working in IO, we're talking milliseconds. That allocation, dynamic dispatch is going to", "tokens": [50368, 1177, 380, 643, 1364, 294, 39839, 11, 321, 434, 1417, 34184, 13, 663, 27599, 11, 8546, 36729, 307, 516, 281, 50592], "temperature": 0.0, "avg_logprob": -0.3339683131167763, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0079253064468503}, {"id": 103, "seek": 67822, "start": 682.78, "end": 688.46, "text": " cost you microseconds, maybe even nanoseconds. So you should not worry too much and if you", "tokens": [50592, 2063, 291, 3123, 37841, 28750, 11, 1310, 754, 14067, 541, 28750, 13, 407, 291, 820, 406, 3292, 886, 709, 293, 498, 291, 50876], "temperature": 0.0, "avg_logprob": -0.3339683131167763, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0079253064468503}, {"id": 104, "seek": 67822, "start": 688.46, "end": 692.9, "text": " start worrying, you should measure before going for the box stuff. I would want to say,", "tokens": [50876, 722, 18788, 11, 291, 820, 3481, 949, 516, 337, 264, 2424, 1507, 13, 286, 576, 528, 281, 584, 11, 51098], "temperature": 0.0, "avg_logprob": -0.3339683131167763, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0079253064468503}, {"id": 105, "seek": 67822, "start": 692.9, "end": 697.82, "text": " it's writing your own future is way more fun. So sometimes it's a little for my own future,", "tokens": [51098, 309, 311, 3579, 428, 1065, 2027, 307, 636, 544, 1019, 13, 407, 2171, 309, 311, 257, 707, 337, 452, 1065, 2027, 11, 51344], "temperature": 0.0, "avg_logprob": -0.3339683131167763, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0079253064468503}, {"id": 106, "seek": 67822, "start": 697.82, "end": 704.14, "text": " just my own personal fun. So box futures are good chose for applications, less for libraries.", "tokens": [51344, 445, 452, 1065, 2973, 1019, 13, 407, 2424, 26071, 366, 665, 5111, 337, 5821, 11, 1570, 337, 15148, 13, 51660], "temperature": 0.0, "avg_logprob": -0.3339683131167763, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0079253064468503}, {"id": 107, "seek": 70414, "start": 704.54, "end": 708.62, "text": " When you're writing libraries, you want to decide the users of your libraries whether or not they", "tokens": [50384, 1133, 291, 434, 3579, 15148, 11, 291, 528, 281, 4536, 264, 5022, 295, 428, 15148, 1968, 420, 406, 436, 50588], "temperature": 0.0, "avg_logprob": -0.21391532818476358, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.0029795097652822733}, {"id": 108, "seek": 70414, "start": 708.62, "end": 714.62, "text": " want to incur some overhead. So it's better if you don't use box futures in your libraries so that", "tokens": [50588, 528, 281, 35774, 512, 19922, 13, 407, 309, 311, 1101, 498, 291, 500, 380, 764, 2424, 26071, 294, 428, 15148, 370, 300, 50888], "temperature": 0.0, "avg_logprob": -0.21391532818476358, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.0029795097652822733}, {"id": 109, "seek": 70414, "start": 714.62, "end": 722.9399999999999, "text": " people can opt out of the overhead. So in this example, we upsold box futures. We need to notice", "tokens": [50888, 561, 393, 2427, 484, 295, 264, 19922, 13, 407, 294, 341, 1365, 11, 321, 493, 45537, 2424, 26071, 13, 492, 643, 281, 3449, 51304], "temperature": 0.0, "avg_logprob": -0.21391532818476358, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.0029795097652822733}, {"id": 110, "seek": 70414, "start": 722.9399999999999, "end": 729.06, "text": " that this box futures as a lifetime that we'll choose and it has to be sent as well. That will", "tokens": [51304, 300, 341, 2424, 26071, 382, 257, 11364, 300, 321, 603, 2826, 293, 309, 575, 281, 312, 2279, 382, 731, 13, 663, 486, 51610], "temperature": 0.0, "avg_logprob": -0.21391532818476358, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.0029795097652822733}, {"id": 111, "seek": 72906, "start": 729.38, "end": 737.78, "text": " become important later. So we choose the static lifetime. The service, the tower service rate is", "tokens": [50380, 1813, 1021, 1780, 13, 407, 321, 2826, 264, 13437, 11364, 13, 440, 2643, 11, 264, 10567, 2643, 3314, 307, 50800], "temperature": 0.0, "avg_logprob": -0.18978028656334006, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.0010158759541809559}, {"id": 112, "seek": 72906, "start": 737.78, "end": 743.54, "text": " not generic of a lifetime. So we have to go for static. Then we write priority. It's always", "tokens": [50800, 406, 19577, 295, 257, 11364, 13, 407, 321, 362, 281, 352, 337, 13437, 13, 1396, 321, 2464, 9365, 13, 467, 311, 1009, 51088], "temperature": 0.0, "avg_logprob": -0.18978028656334006, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.0010158759541809559}, {"id": 113, "seek": 72906, "start": 743.54, "end": 751.9399999999999, "text": " ready. So it's pretty straightforward. And then we write our future. So we declare an async move", "tokens": [51088, 1919, 13, 407, 309, 311, 1238, 15325, 13, 400, 550, 321, 2464, 527, 2027, 13, 407, 321, 19710, 364, 382, 34015, 1286, 51508], "temperature": 0.0, "avg_logprob": -0.18978028656334006, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.0010158759541809559}, {"id": 114, "seek": 72906, "start": 751.9399999999999, "end": 758.3399999999999, "text": " block in which we build a message and build a response. And then we box that future on the", "tokens": [51508, 3461, 294, 597, 321, 1322, 257, 3636, 293, 1322, 257, 4134, 13, 400, 550, 321, 2424, 300, 2027, 322, 264, 51828], "temperature": 0.0, "avg_logprob": -0.18978028656334006, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.0010158759541809559}, {"id": 115, "seek": 75834, "start": 758.34, "end": 766.02, "text": " hip. We pin it with box pin and we should be done. This is how you would unitize that service.", "tokens": [50364, 8103, 13, 492, 5447, 309, 365, 2424, 5447, 293, 321, 820, 312, 1096, 13, 639, 307, 577, 291, 576, 4985, 1125, 300, 2643, 13, 50748], "temperature": 0.0, "avg_logprob": -0.15489847635485463, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.0006261443486437201}, {"id": 116, "seek": 75834, "start": 766.02, "end": 773.46, "text": " You instantiate it. You call ready. We're using the extension trait that makes it a bit easier to", "tokens": [50748, 509, 9836, 13024, 309, 13, 509, 818, 1919, 13, 492, 434, 1228, 264, 10320, 22538, 300, 1669, 309, 257, 857, 3571, 281, 51120], "temperature": 0.0, "avg_logprob": -0.15489847635485463, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.0006261443486437201}, {"id": 117, "seek": 75834, "start": 773.46, "end": 780.26, "text": " work with. So you call ready and then you call call and it works. So it's obviously a bit more", "tokens": [51120, 589, 365, 13, 407, 291, 818, 1919, 293, 550, 291, 818, 818, 293, 309, 1985, 13, 407, 309, 311, 2745, 257, 857, 544, 51460], "temperature": 0.0, "avg_logprob": -0.15489847635485463, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.0006261443486437201}, {"id": 118, "seek": 75834, "start": 780.26, "end": 786.1, "text": " complicated than just writing that hello function that I showed at the beginning. But it's all", "tokens": [51460, 6179, 813, 445, 3579, 300, 7751, 2445, 300, 286, 4712, 412, 264, 2863, 13, 583, 309, 311, 439, 51752], "temperature": 0.0, "avg_logprob": -0.15489847635485463, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.0006261443486437201}, {"id": 119, "seek": 78610, "start": 786.1800000000001, "end": 793.38, "text": " doable. When it comes to choosing a future, sometimes you have the choice to choose one from a", "tokens": [50368, 41183, 13, 1133, 309, 1487, 281, 10875, 257, 2027, 11, 2171, 291, 362, 264, 3922, 281, 2826, 472, 490, 257, 50728], "temperature": 0.0, "avg_logprob": -0.1783924289778167, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0034784129820764065}, {"id": 120, "seek": 78610, "start": 793.38, "end": 802.02, "text": " third party crate. Futures has some ready to go futures. You can use towers as well. So sometimes", "tokens": [50728, 2636, 3595, 42426, 13, 16569, 1303, 575, 512, 1919, 281, 352, 26071, 13, 509, 393, 764, 25045, 382, 731, 13, 407, 2171, 51160], "temperature": 0.0, "avg_logprob": -0.1783924289778167, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0034784129820764065}, {"id": 121, "seek": 78610, "start": 802.02, "end": 806.34, "text": " those futures are going to fit your use case. You don't have to go for the box. You don't have to go", "tokens": [51160, 729, 26071, 366, 516, 281, 3318, 428, 764, 1389, 13, 509, 500, 380, 362, 281, 352, 337, 264, 2424, 13, 509, 500, 380, 362, 281, 352, 51376], "temperature": 0.0, "avg_logprob": -0.1783924289778167, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0034784129820764065}, {"id": 122, "seek": 78610, "start": 806.34, "end": 811.5400000000001, "text": " for implementing your own. You can just reuse one. It's convenient. So in this example, we can use the", "tokens": [51376, 337, 18114, 428, 1065, 13, 509, 393, 445, 26225, 472, 13, 467, 311, 10851, 13, 407, 294, 341, 1365, 11, 321, 393, 764, 264, 51636], "temperature": 0.0, "avg_logprob": -0.1783924289778167, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0034784129820764065}, {"id": 123, "seek": 81154, "start": 812.0999999999999, "end": 817.86, "text": " ready trait from the futures crate. So this time, you do see my pointer crate.", "tokens": [50392, 1919, 22538, 490, 264, 26071, 42426, 13, 407, 341, 565, 11, 291, 360, 536, 452, 23918, 42426, 13, 50680], "temperature": 0.0, "avg_logprob": -0.14102228226200228, "compression_ratio": 1.5379746835443038, "no_speech_prob": 0.0015971355605870485}, {"id": 124, "seek": 81154, "start": 819.54, "end": 826.0999999999999, "text": " We change the return type here. It's ready and this time we return ready.", "tokens": [50764, 492, 1319, 264, 2736, 2010, 510, 13, 467, 311, 1919, 293, 341, 565, 321, 2736, 1919, 13, 51092], "temperature": 0.0, "avg_logprob": -0.14102228226200228, "compression_ratio": 1.5379746835443038, "no_speech_prob": 0.0015971355605870485}, {"id": 125, "seek": 81154, "start": 827.3, "end": 833.06, "text": " So we got rid of the allocation and the dynamic dispatch. And it's actually more readable.", "tokens": [51152, 407, 321, 658, 3973, 295, 264, 27599, 293, 264, 8546, 36729, 13, 400, 309, 311, 767, 544, 49857, 13, 51440], "temperature": 0.0, "avg_logprob": -0.14102228226200228, "compression_ratio": 1.5379746835443038, "no_speech_prob": 0.0015971355605870485}, {"id": 126, "seek": 83306, "start": 833.78, "end": 840.0999999999999, "text": " So we built a simple hello service. We're going to build a middle one on top of it. That's our", "tokens": [50400, 407, 321, 3094, 257, 2199, 7751, 2643, 13, 492, 434, 516, 281, 1322, 257, 2808, 472, 322, 1192, 295, 309, 13, 663, 311, 527, 50716], "temperature": 0.0, "avg_logprob": -0.22348192850748697, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0008687487570568919}, {"id": 127, "seek": 83306, "start": 840.0999999999999, "end": 847.54, "text": " logging service. We want our logging service to work with our hello service. But ideally, we like it", "tokens": [50716, 27991, 2643, 13, 492, 528, 527, 27991, 2643, 281, 589, 365, 527, 7751, 2643, 13, 583, 22915, 11, 321, 411, 309, 51088], "temperature": 0.0, "avg_logprob": -0.22348192850748697, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0008687487570568919}, {"id": 128, "seek": 83306, "start": 847.54, "end": 853.14, "text": " to work with any kind of service that implements the trait. So we're going to make it generic.", "tokens": [51088, 281, 589, 365, 604, 733, 295, 2643, 300, 704, 17988, 264, 22538, 13, 407, 321, 434, 516, 281, 652, 309, 19577, 13, 51368], "temperature": 0.0, "avg_logprob": -0.22348192850748697, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0008687487570568919}, {"id": 129, "seek": 85314, "start": 853.3, "end": 862.66, "text": " So logging has an input parameter called s that will be the inner service.", "tokens": [50372, 407, 27991, 575, 364, 4846, 13075, 1219, 262, 300, 486, 312, 264, 7284, 2643, 13, 50840], "temperature": 0.0, "avg_logprob": -0.14779035394841974, "compression_ratio": 1.5827338129496402, "no_speech_prob": 0.0005972021608613431}, {"id": 130, "seek": 85314, "start": 864.98, "end": 867.06, "text": " And we start implementing service for logging.", "tokens": [50956, 400, 321, 722, 18114, 2643, 337, 27991, 13, 51060], "temperature": 0.0, "avg_logprob": -0.14779035394841974, "compression_ratio": 1.5827338129496402, "no_speech_prob": 0.0005972021608613431}, {"id": 131, "seek": 85314, "start": 870.98, "end": 876.9, "text": " What we want to do is calling call on the inner service. And we can only do that if it's a service", "tokens": [51256, 708, 321, 528, 281, 360, 307, 5141, 818, 322, 264, 7284, 2643, 13, 400, 321, 393, 787, 360, 300, 498, 309, 311, 257, 2643, 51552], "temperature": 0.0, "avg_logprob": -0.14779035394841974, "compression_ratio": 1.5827338129496402, "no_speech_prob": 0.0005972021608613431}, {"id": 132, "seek": 87690, "start": 876.9, "end": 884.66, "text": " itself. So we need that now. And we say the inner service is also a service", "tokens": [50364, 2564, 13, 407, 321, 643, 300, 586, 13, 400, 321, 584, 264, 7284, 2643, 307, 611, 257, 2643, 50752], "temperature": 0.0, "avg_logprob": -0.1812460218157087, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.0007095469627529383}, {"id": 133, "seek": 87690, "start": 884.66, "end": 888.66, "text": " generic over r, our two generic types here.", "tokens": [50752, 19577, 670, 367, 11, 527, 732, 19577, 3467, 510, 13, 50952], "temperature": 0.0, "avg_logprob": -0.1812460218157087, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.0007095469627529383}, {"id": 134, "seek": 87690, "start": 891.4599999999999, "end": 896.5, "text": " Then we implement already. We delegate to the inner service pretty straightforward.", "tokens": [51092, 1396, 321, 4445, 1217, 13, 492, 40999, 281, 264, 7284, 2643, 1238, 15325, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1812460218157087, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.0007095469627529383}, {"id": 135, "seek": 87690, "start": 899.14, "end": 906.74, "text": " And then we implement call, starting with using a box feature. So we built the inner future", "tokens": [51476, 400, 550, 321, 4445, 818, 11, 2891, 365, 1228, 257, 2424, 4111, 13, 407, 321, 3094, 264, 7284, 2027, 51856], "temperature": 0.0, "avg_logprob": -0.1812460218157087, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.0007095469627529383}, {"id": 136, "seek": 90690, "start": 907.38, "end": 917.6999999999999, "text": " calling call on inner. And then we create our own future using an instinct move block. We do the", "tokens": [50388, 5141, 818, 322, 7284, 13, 400, 550, 321, 1884, 527, 1065, 2027, 1228, 364, 16556, 1286, 3461, 13, 492, 360, 264, 50904], "temperature": 0.0, "avg_logprob": -0.23958460489908853, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0011686311336234212}, {"id": 137, "seek": 90690, "start": 917.6999999999999, "end": 925.3, "text": " logging. We evaluate the future here. So here on this line, we build a future that's not evaluated.", "tokens": [50904, 27991, 13, 492, 13059, 264, 2027, 510, 13, 407, 510, 322, 341, 1622, 11, 321, 1322, 257, 2027, 300, 311, 406, 25509, 13, 51284], "temperature": 0.0, "avg_logprob": -0.23958460489908853, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0011686311336234212}, {"id": 138, "seek": 90690, "start": 925.3, "end": 932.5799999999999, "text": " It's actually evaluated here. Then we do the logging. And we respond to response. We box", "tokens": [51284, 467, 311, 767, 25509, 510, 13, 1396, 321, 360, 264, 27991, 13, 400, 321, 4196, 281, 4134, 13, 492, 2424, 51648], "temperature": 0.0, "avg_logprob": -0.23958460489908853, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0011686311336234212}, {"id": 139, "seek": 93258, "start": 933.5400000000001, "end": 941.62, "text": " in the outer future. And now we go back to our terminal. We run cargo test. And it should work.", "tokens": [50412, 294, 264, 10847, 2027, 13, 400, 586, 321, 352, 646, 281, 527, 14709, 13, 492, 1190, 19449, 1500, 13, 400, 309, 820, 589, 13, 50816], "temperature": 0.0, "avg_logprob": -0.19783616677308694, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.0018371344776824117}, {"id": 140, "seek": 93258, "start": 942.6600000000001, "end": 952.1800000000001, "text": " Except it doesn't. We've omitted a little technicality. And that technicality is coming from what I", "tokens": [50868, 16192, 309, 1177, 380, 13, 492, 600, 3406, 3944, 257, 707, 6191, 507, 13, 400, 300, 6191, 507, 307, 1348, 490, 437, 286, 51344], "temperature": 0.0, "avg_logprob": -0.19783616677308694, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.0018371344776824117}, {"id": 141, "seek": 93258, "start": 952.1800000000001, "end": 958.74, "text": " said before. The box future must be sent. So the future that you return must be sent. So the inner", "tokens": [51344, 848, 949, 13, 440, 2424, 2027, 1633, 312, 2279, 13, 407, 264, 2027, 300, 291, 2736, 1633, 312, 2279, 13, 407, 264, 7284, 51672], "temperature": 0.0, "avg_logprob": -0.19783616677308694, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.0018371344776824117}, {"id": 142, "seek": 95874, "start": 958.74, "end": 967.46, "text": " future must be sent as well. So we need to tell the compiler that constrate. So not only we have", "tokens": [50364, 2027, 1633, 312, 2279, 382, 731, 13, 407, 321, 643, 281, 980, 264, 31958, 300, 1817, 4404, 13, 407, 406, 787, 321, 362, 50800], "temperature": 0.0, "avg_logprob": -0.201364377339681, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.007803130894899368}, {"id": 143, "seek": 95874, "start": 967.46, "end": 974.74, "text": " constrate the service, s, we told the compiler it is a service, we also tell the compiler the", "tokens": [50800, 1817, 4404, 264, 2643, 11, 262, 11, 321, 1907, 264, 31958, 309, 307, 257, 2643, 11, 321, 611, 980, 264, 31958, 264, 51164], "temperature": 0.0, "avg_logprob": -0.201364377339681, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.007803130894899368}, {"id": 144, "seek": 95874, "start": 974.74, "end": 981.0600000000001, "text": " future that an inner service is going to return is also sent in static. And all this time it works.", "tokens": [51164, 2027, 300, 364, 7284, 2643, 307, 516, 281, 2736, 307, 611, 2279, 294, 13437, 13, 400, 439, 341, 565, 309, 1985, 13, 51480], "temperature": 0.0, "avg_logprob": -0.201364377339681, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.007803130894899368}, {"id": 145, "seek": 98106, "start": 981.38, "end": 989.06, "text": " So this is how you would instantiate your logging service. It's wrapping the", "tokens": [50380, 407, 341, 307, 577, 291, 576, 9836, 13024, 428, 27991, 2643, 13, 467, 311, 21993, 264, 50764], "temperature": 0.0, "avg_logprob": -0.1662009183098288, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.027948103845119476}, {"id": 146, "seek": 98106, "start": 989.8599999999999, "end": 994.0999999999999, "text": " hello service. And then you call it the exact same way. It's obviously not going to show up during", "tokens": [50804, 7751, 2643, 13, 400, 550, 291, 818, 309, 264, 1900, 912, 636, 13, 467, 311, 2745, 406, 516, 281, 855, 493, 1830, 51016], "temperature": 0.0, "avg_logprob": -0.1662009183098288, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.027948103845119476}, {"id": 147, "seek": 98106, "start": 994.0999999999999, "end": 998.18, "text": " the unit test. But now you have logging on top of your service. And that logging service", "tokens": [51016, 264, 4985, 1500, 13, 583, 586, 291, 362, 27991, 322, 1192, 295, 428, 2643, 13, 400, 300, 27991, 2643, 51220], "temperature": 0.0, "avg_logprob": -0.1662009183098288, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.027948103845119476}, {"id": 148, "seek": 98106, "start": 998.8199999999999, "end": 1006.42, "text": " is usable for some other services. Tower HTTP has a bunch of services that are ready for", "tokens": [51252, 307, 29975, 337, 512, 661, 3328, 13, 17877, 33283, 575, 257, 3840, 295, 3328, 300, 366, 1919, 337, 51632], "temperature": 0.0, "avg_logprob": -0.1662009183098288, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.027948103845119476}, {"id": 149, "seek": 100642, "start": 1007.38, "end": 1009.9399999999999, "text": " metrics, tracing. It works a lot like that.", "tokens": [50412, 16367, 11, 25262, 13, 467, 1985, 257, 688, 411, 300, 13, 50540], "temperature": 0.0, "avg_logprob": -0.23567121028900145, "compression_ratio": 1.5, "no_speech_prob": 0.0019252628553658724}, {"id": 150, "seek": 100642, "start": 1013.62, "end": 1018.26, "text": " So we're going to do this again this time rolling on our own future. Because it's fun.", "tokens": [50724, 407, 321, 434, 516, 281, 360, 341, 797, 341, 565, 9439, 322, 527, 1065, 2027, 13, 1436, 309, 311, 1019, 13, 50956], "temperature": 0.0, "avg_logprob": -0.23567121028900145, "compression_ratio": 1.5, "no_speech_prob": 0.0019252628553658724}, {"id": 151, "seek": 100642, "start": 1018.26, "end": 1026.58, "text": " And because when you're going to recode from AXM to NIC, you're always going to encounter those", "tokens": [50956, 400, 570, 562, 291, 434, 516, 281, 850, 1429, 490, 316, 55, 44, 281, 426, 2532, 11, 291, 434, 1009, 516, 281, 8593, 729, 51372], "temperature": 0.0, "avg_logprob": -0.23567121028900145, "compression_ratio": 1.5, "no_speech_prob": 0.0019252628553658724}, {"id": 152, "seek": 100642, "start": 1029.3, "end": 1032.74, "text": " handwritten features. So it's good to get used to them.", "tokens": [51508, 1011, 26859, 4122, 13, 407, 309, 311, 665, 281, 483, 1143, 281, 552, 13, 51680], "temperature": 0.0, "avg_logprob": -0.23567121028900145, "compression_ratio": 1.5, "no_speech_prob": 0.0019252628553658724}, {"id": 153, "seek": 103274, "start": 1033.14, "end": 1042.26, "text": " Unfortunately, writing a future from scratch is actually non-trivial because of the whole", "tokens": [50384, 8590, 11, 3579, 257, 2027, 490, 8459, 307, 767, 2107, 12, 83, 470, 22640, 570, 295, 264, 1379, 50840], "temperature": 0.0, "avg_logprob": -0.2627543850221496, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.0011503644054755569}, {"id": 154, "seek": 103274, "start": 1042.26, "end": 1048.18, "text": " pinning thing. And this is not a talk about writing futures and me explaining to you pinning", "tokens": [50840, 5447, 773, 551, 13, 400, 341, 307, 406, 257, 751, 466, 3579, 26071, 293, 385, 13468, 281, 291, 5447, 773, 51136], "temperature": 0.0, "avg_logprob": -0.2627543850221496, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.0011503644054755569}, {"id": 155, "seek": 103274, "start": 1048.18, "end": 1054.5, "text": " because I don't totally understand it myself. But I can be very practical and tell you how to do it.", "tokens": [51136, 570, 286, 500, 380, 3879, 1223, 309, 2059, 13, 583, 286, 393, 312, 588, 8496, 293, 980, 291, 577, 281, 360, 309, 13, 51452], "temperature": 0.0, "avg_logprob": -0.2627543850221496, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.0011503644054755569}, {"id": 156, "seek": 105450, "start": 1054.82, "end": 1067.22, "text": " And I'll show it to you right now. So now the logging is going to happen in the future. So before", "tokens": [50380, 400, 286, 603, 855, 309, 281, 291, 558, 586, 13, 407, 586, 264, 27991, 307, 516, 281, 1051, 294, 264, 2027, 13, 407, 949, 51000], "temperature": 0.0, "avg_logprob": -0.20452022552490234, "compression_ratio": 1.7305389221556886, "no_speech_prob": 0.00831089448183775}, {"id": 157, "seek": 105450, "start": 1067.22, "end": 1075.62, "text": " we were wrapping a service with a service, and the idea is the same. We're going to wrap the", "tokens": [51000, 321, 645, 21993, 257, 2643, 365, 257, 2643, 11, 293, 264, 1558, 307, 264, 912, 13, 492, 434, 516, 281, 7019, 264, 51420], "temperature": 0.0, "avg_logprob": -0.20452022552490234, "compression_ratio": 1.7305389221556886, "no_speech_prob": 0.00831089448183775}, {"id": 158, "seek": 105450, "start": 1075.62, "end": 1082.74, "text": " inner future with our logging future so we can add behavior to the future. So you can add behavior", "tokens": [51420, 7284, 2027, 365, 527, 27991, 2027, 370, 321, 393, 909, 5223, 281, 264, 2027, 13, 407, 291, 393, 909, 5223, 51776], "temperature": 0.0, "avg_logprob": -0.20452022552490234, "compression_ratio": 1.7305389221556886, "no_speech_prob": 0.00831089448183775}, {"id": 159, "seek": 108274, "start": 1083.3, "end": 1088.66, "text": " to your service, but you can also wrap the future and do stuff after you're done pulling or before", "tokens": [50392, 281, 428, 2643, 11, 457, 291, 393, 611, 7019, 264, 2027, 293, 360, 1507, 934, 291, 434, 1096, 8407, 420, 949, 50660], "temperature": 0.0, "avg_logprob": -0.244249375398494, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.004322266671806574}, {"id": 160, "seek": 108274, "start": 1088.66, "end": 1094.66, "text": " you were about to pull the future. And this is what can be tricky with our terrorist services is", "tokens": [50660, 291, 645, 466, 281, 2235, 264, 2027, 13, 400, 341, 307, 437, 393, 312, 12414, 365, 527, 20342, 3328, 307, 50960], "temperature": 0.0, "avg_logprob": -0.244249375398494, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.004322266671806574}, {"id": 161, "seek": 108274, "start": 1094.66, "end": 1100.42, "text": " sometimes, for instance, you take rate limiting. There's a bit of logic in Pall Ready. There's a", "tokens": [50960, 2171, 11, 337, 5197, 11, 291, 747, 3314, 22083, 13, 821, 311, 257, 857, 295, 9952, 294, 430, 336, 9944, 13, 821, 311, 257, 51248], "temperature": 0.0, "avg_logprob": -0.244249375398494, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.004322266671806574}, {"id": 162, "seek": 108274, "start": 1100.42, "end": 1105.46, "text": " bit of logic in Call and there's a bit of logic in the future. So it's hard to understand where's", "tokens": [51248, 857, 295, 9952, 294, 7807, 293, 456, 311, 257, 857, 295, 9952, 294, 264, 2027, 13, 407, 309, 311, 1152, 281, 1223, 689, 311, 51500], "temperature": 0.0, "avg_logprob": -0.244249375398494, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.004322266671806574}, {"id": 163, "seek": 108274, "start": 1105.46, "end": 1110.66, "text": " the logic that's really to the business, what you're trying to achieve versus what just really to", "tokens": [51500, 264, 9952, 300, 311, 534, 281, 264, 1606, 11, 437, 291, 434, 1382, 281, 4584, 5717, 437, 445, 534, 281, 51760], "temperature": 0.0, "avg_logprob": -0.244249375398494, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.004322266671806574}, {"id": 164, "seek": 111066, "start": 1110.66, "end": 1116.9, "text": " trying to write a service. So that's also part of the complexity of our Dillon and Vittorio services.", "tokens": [50364, 1382, 281, 2464, 257, 2643, 13, 407, 300, 311, 611, 644, 295, 264, 14024, 295, 527, 413, 30961, 293, 691, 593, 284, 1004, 3328, 13, 50676], "temperature": 0.0, "avg_logprob": -0.23863669286800337, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.0007666716701351106}, {"id": 165, "seek": 111066, "start": 1118.1000000000001, "end": 1125.14, "text": " But back to our logging future, it's going to wrap another future. So it's going to be generic over", "tokens": [50736, 583, 646, 281, 527, 27991, 2027, 11, 309, 311, 516, 281, 7019, 1071, 2027, 13, 407, 309, 311, 516, 281, 312, 19577, 670, 51088], "temperature": 0.0, "avg_logprob": -0.23863669286800337, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.0007666716701351106}, {"id": 166, "seek": 111066, "start": 1125.14, "end": 1132.8200000000002, "text": " F. We need to use the create pin project to pin the inner future. So that's why you have the", "tokens": [51088, 479, 13, 492, 643, 281, 764, 264, 1884, 5447, 1716, 281, 5447, 264, 7284, 2027, 13, 407, 300, 311, 983, 291, 362, 264, 51472], "temperature": 0.0, "avg_logprob": -0.23863669286800337, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.0007666716701351106}, {"id": 167, "seek": 113282, "start": 1132.82, "end": 1144.26, "text": " pin attribute there. And now we, that's how our service, the logging service now is going to look.", "tokens": [50364, 5447, 19667, 456, 13, 400, 586, 321, 11, 300, 311, 577, 527, 2643, 11, 264, 27991, 2643, 586, 307, 516, 281, 574, 13, 50936], "temperature": 0.0, "avg_logprob": -0.24023311614990234, "compression_ratio": 1.5, "no_speech_prob": 0.01636507175862789}, {"id": 168, "seek": 113282, "start": 1144.26, "end": 1153.22, "text": " We replaced the future type with our logging future and the use which relies on the inner future. So s", "tokens": [50936, 492, 10772, 264, 2027, 2010, 365, 527, 27991, 2027, 293, 264, 764, 597, 30910, 322, 264, 7284, 2027, 13, 407, 262, 51384], "temperature": 0.0, "avg_logprob": -0.24023311614990234, "compression_ratio": 1.5, "no_speech_prob": 0.01636507175862789}, {"id": 169, "seek": 115322, "start": 1154.18, "end": 1163.14, "text": " call and call in future. And then in call, now when we are called, we do the first logging statement.", "tokens": [50412, 818, 293, 818, 294, 2027, 13, 400, 550, 294, 818, 11, 586, 562, 321, 366, 1219, 11, 321, 360, 264, 700, 27991, 5629, 13, 50860], "temperature": 0.0, "avg_logprob": -0.27467741285051617, "compression_ratio": 1.7530120481927711, "no_speech_prob": 0.025939276441931725}, {"id": 170, "seek": 115322, "start": 1163.14, "end": 1168.5, "text": " Then we build the future and when the future will be pulled already, then this is when we'll actually", "tokens": [50860, 1396, 321, 1322, 264, 2027, 293, 562, 264, 2027, 486, 312, 7373, 1217, 11, 550, 341, 307, 562, 321, 603, 767, 51128], "temperature": 0.0, "avg_logprob": -0.27467741285051617, "compression_ratio": 1.7530120481927711, "no_speech_prob": 0.025939276441931725}, {"id": 171, "seek": 115322, "start": 1170.42, "end": 1177.3, "text": " add the last logging statement. So this is how you implement future for logging future.", "tokens": [51224, 909, 264, 1036, 27991, 5629, 13, 407, 341, 307, 577, 291, 4445, 2027, 337, 27991, 2027, 13, 51568], "temperature": 0.0, "avg_logprob": -0.27467741285051617, "compression_ratio": 1.7530120481927711, "no_speech_prob": 0.025939276441931725}, {"id": 172, "seek": 117730, "start": 1177.54, "end": 1185.06, "text": " Same idea. We need to add the constraint and tell the compiler the inner future is also a future.", "tokens": [50376, 10635, 1558, 13, 492, 643, 281, 909, 264, 25534, 293, 980, 264, 31958, 264, 7284, 2027, 307, 611, 257, 2027, 13, 50752], "temperature": 0.0, "avg_logprob": -0.23091130786471897, "compression_ratio": 1.748502994011976, "no_speech_prob": 0.008382292464375496}, {"id": 173, "seek": 117730, "start": 1185.78, "end": 1194.26, "text": " It's output is the output of the inner future. We need to project. I don't actually know where", "tokens": [50788, 467, 311, 5598, 307, 264, 5598, 295, 264, 7284, 2027, 13, 492, 643, 281, 1716, 13, 286, 500, 380, 767, 458, 689, 51212], "temperature": 0.0, "avg_logprob": -0.23091130786471897, "compression_ratio": 1.748502994011976, "no_speech_prob": 0.008382292464375496}, {"id": 174, "seek": 117730, "start": 1194.26, "end": 1200.82, "text": " that the term comes from. But you need to project self. It's usually a convention is the convention", "tokens": [51212, 300, 264, 1433, 1487, 490, 13, 583, 291, 643, 281, 1716, 2698, 13, 467, 311, 2673, 257, 10286, 307, 264, 10286, 51540], "temperature": 0.0, "avg_logprob": -0.23091130786471897, "compression_ratio": 1.748502994011976, "no_speech_prob": 0.008382292464375496}, {"id": 175, "seek": 120082, "start": 1200.82, "end": 1209.54, "text": " is to call this. And this gives you the same object. But when you use the inner futures,", "tokens": [50364, 307, 281, 818, 341, 13, 400, 341, 2709, 291, 264, 912, 2657, 13, 583, 562, 291, 764, 264, 7284, 26071, 11, 50800], "temperature": 0.0, "avg_logprob": -0.15675165881849315, "compression_ratio": 1.622093023255814, "no_speech_prob": 0.010322416201233864}, {"id": 176, "seek": 120082, "start": 1209.54, "end": 1215.22, "text": " they're actually pullable. Because pull is not defined on the future type. It's defined", "tokens": [50800, 436, 434, 767, 2235, 712, 13, 1436, 2235, 307, 406, 7642, 322, 264, 2027, 2010, 13, 467, 311, 7642, 51084], "temperature": 0.0, "avg_logprob": -0.15675165881849315, "compression_ratio": 1.622093023255814, "no_speech_prob": 0.010322416201233864}, {"id": 177, "seek": 120082, "start": 1216.5, "end": 1222.8999999999999, "text": " on a pin future. So that's why you need to project which pins the future, which allows you to pull it.", "tokens": [51148, 322, 257, 5447, 2027, 13, 407, 300, 311, 983, 291, 643, 281, 1716, 597, 16392, 264, 2027, 11, 597, 4045, 291, 281, 2235, 309, 13, 51468], "temperature": 0.0, "avg_logprob": -0.15675165881849315, "compression_ratio": 1.622093023255814, "no_speech_prob": 0.010322416201233864}, {"id": 178, "seek": 122290, "start": 1222.9, "end": 1230.26, "text": " So those are the technicalities that you have to deal with when you're writing your", "tokens": [50364, 407, 729, 366, 264, 6191, 1088, 300, 291, 362, 281, 2028, 365, 562, 291, 434, 3579, 428, 50732], "temperature": 0.0, "avg_logprob": -0.22043452391753326, "compression_ratio": 1.510752688172043, "no_speech_prob": 0.0011693123960867524}, {"id": 179, "seek": 122290, "start": 1231.94, "end": 1240.42, "text": " handwritten function. But once you've done that, it's exactly like writing the normal call, I want", "tokens": [50816, 1011, 26859, 2445, 13, 583, 1564, 291, 600, 1096, 300, 11, 309, 311, 2293, 411, 3579, 264, 2710, 818, 11, 286, 528, 51240], "temperature": 0.0, "avg_logprob": -0.22043452391753326, "compression_ratio": 1.510752688172043, "no_speech_prob": 0.0011693123960867524}, {"id": 180, "seek": 122290, "start": 1240.42, "end": 1248.66, "text": " to say. So we pull our future. And if it's ready, we add the logging statement. If it's not ready,", "tokens": [51240, 281, 584, 13, 407, 321, 2235, 527, 2027, 13, 400, 498, 309, 311, 1919, 11, 321, 909, 264, 27991, 5629, 13, 759, 309, 311, 406, 1919, 11, 51652], "temperature": 0.0, "avg_logprob": -0.22043452391753326, "compression_ratio": 1.510752688172043, "no_speech_prob": 0.0011693123960867524}, {"id": 181, "seek": 124866, "start": 1248.66, "end": 1252.5, "text": " it means it's pending. It's going to be pulled again later. And also we know that", "tokens": [50364, 309, 1355, 309, 311, 32110, 13, 467, 311, 516, 281, 312, 7373, 797, 1780, 13, 400, 611, 321, 458, 300, 50556], "temperature": 0.0, "avg_logprob": -0.17367086605149873, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0003981353365816176}, {"id": 182, "seek": 124866, "start": 1253.22, "end": 1259.0600000000002, "text": " when a future is pulled and it's ready, it will be no longer pulled. So we know that this", "tokens": [50592, 562, 257, 2027, 307, 7373, 293, 309, 311, 1919, 11, 309, 486, 312, 572, 2854, 7373, 13, 407, 321, 458, 300, 341, 50884], "temperature": 0.0, "avg_logprob": -0.17367086605149873, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0003981353365816176}, {"id": 183, "seek": 124866, "start": 1259.0600000000002, "end": 1267.94, "text": " statement will appear only once. So let's build on top of that. Let's build a timeout service now.", "tokens": [50884, 5629, 486, 4204, 787, 1564, 13, 407, 718, 311, 1322, 322, 1192, 295, 300, 13, 961, 311, 1322, 257, 565, 346, 2643, 586, 13, 51328], "temperature": 0.0, "avg_logprob": -0.17367086605149873, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0003981353365816176}, {"id": 184, "seek": 124866, "start": 1270.18, "end": 1274.74, "text": " So same thing. We want to add a timeout to any service. It's also generic over s.", "tokens": [51440, 407, 912, 551, 13, 492, 528, 281, 909, 257, 565, 346, 281, 604, 2643, 13, 467, 311, 611, 19577, 670, 262, 13, 51668], "temperature": 0.0, "avg_logprob": -0.17367086605149873, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0003981353365816176}, {"id": 185, "seek": 127474, "start": 1274.98, "end": 1282.82, "text": " But the timeout service is interesting because the logging service is pretty simplistic. It doesn't", "tokens": [50376, 583, 264, 565, 346, 2643, 307, 1880, 570, 264, 27991, 2643, 307, 1238, 44199, 13, 467, 1177, 380, 50768], "temperature": 0.0, "avg_logprob": -0.19606381370907738, "compression_ratio": 1.76, "no_speech_prob": 0.001500055892392993}, {"id": 186, "seek": 127474, "start": 1282.82, "end": 1288.42, "text": " mutate or touch anything. The request not in the response. It's like it did not go through it.", "tokens": [50768, 5839, 473, 420, 2557, 1340, 13, 440, 5308, 406, 294, 264, 4134, 13, 467, 311, 411, 309, 630, 406, 352, 807, 309, 13, 51048], "temperature": 0.0, "avg_logprob": -0.19606381370907738, "compression_ratio": 1.76, "no_speech_prob": 0.001500055892392993}, {"id": 187, "seek": 127474, "start": 1289.46, "end": 1294.9, "text": " The timeout service is a bit interesting because you have to signal the timeout", "tokens": [51100, 440, 565, 346, 2643, 307, 257, 857, 1880, 570, 291, 362, 281, 6358, 264, 565, 346, 51372], "temperature": 0.0, "avg_logprob": -0.19606381370907738, "compression_ratio": 1.76, "no_speech_prob": 0.001500055892392993}, {"id": 188, "seek": 127474, "start": 1295.46, "end": 1301.06, "text": " somehow in your written type. And the way you're going to signal the error is", "tokens": [51400, 6063, 294, 428, 3720, 2010, 13, 400, 264, 636, 291, 434, 516, 281, 6358, 264, 6713, 307, 51680], "temperature": 0.0, "avg_logprob": -0.19606381370907738, "compression_ratio": 1.76, "no_speech_prob": 0.001500055892392993}, {"id": 189, "seek": 130106, "start": 1302.02, "end": 1307.22, "text": " potentially using an enum. And in that enum, you will have two variants. The first one would be", "tokens": [50412, 7263, 1228, 364, 465, 449, 13, 400, 294, 300, 465, 449, 11, 291, 486, 362, 732, 21669, 13, 440, 700, 472, 576, 312, 50672], "temperature": 0.0, "avg_logprob": -0.1620486068725586, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.0004877728642895818}, {"id": 190, "seek": 130106, "start": 1307.22, "end": 1312.74, "text": " the timeout. If timeout appears, that would be the error. But if the inner service returns", "tokens": [50672, 264, 565, 346, 13, 759, 565, 346, 7038, 11, 300, 576, 312, 264, 6713, 13, 583, 498, 264, 7284, 2643, 11247, 50948], "temperature": 0.0, "avg_logprob": -0.1620486068725586, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.0004877728642895818}, {"id": 191, "seek": 130106, "start": 1313.86, "end": 1318.82, "text": " its error itself, you have to wrap it in inner. And then the caller will know if the error comes", "tokens": [51004, 1080, 6713, 2564, 11, 291, 362, 281, 7019, 309, 294, 7284, 13, 400, 550, 264, 48324, 486, 458, 498, 264, 6713, 1487, 51252], "temperature": 0.0, "avg_logprob": -0.1620486068725586, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.0004877728642895818}, {"id": 192, "seek": 130106, "start": 1318.82, "end": 1324.74, "text": " from the timeout with the inner error. That looks like a good idea to do it this way. And you can", "tokens": [51252, 490, 264, 565, 346, 365, 264, 7284, 6713, 13, 663, 1542, 411, 257, 665, 1558, 281, 360, 309, 341, 636, 13, 400, 291, 393, 51548], "temperature": 0.0, "avg_logprob": -0.1620486068725586, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.0004877728642895818}, {"id": 193, "seek": 132474, "start": 1324.74, "end": 1331.14, "text": " totally do it this way. The problem is if you adopt this pattern for timeouts, authentication,", "tokens": [50364, 3879, 360, 309, 341, 636, 13, 440, 1154, 307, 498, 291, 6878, 341, 5102, 337, 565, 7711, 11, 26643, 11, 50684], "temperature": 0.0, "avg_logprob": -0.22804712568010602, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.005382453091442585}, {"id": 194, "seek": 132474, "start": 1332.1, "end": 1338.26, "text": " rate limiting, the nesting of all those errors in the inner is going to become pretty complicated.", "tokens": [50732, 3314, 22083, 11, 264, 297, 8714, 295, 439, 729, 13603, 294, 264, 7284, 307, 516, 281, 1813, 1238, 6179, 13, 51040], "temperature": 0.0, "avg_logprob": -0.22804712568010602, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.005382453091442585}, {"id": 195, "seek": 132474, "start": 1339.14, "end": 1348.34, "text": " And really hard to compose. And it is easy to deal with. So what libraries usually favor is boxing.", "tokens": [51084, 400, 534, 1152, 281, 35925, 13, 400, 309, 307, 1858, 281, 2028, 365, 13, 407, 437, 15148, 2673, 2294, 307, 24424, 13, 51544], "temperature": 0.0, "avg_logprob": -0.22804712568010602, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.005382453091442585}, {"id": 196, "seek": 134834, "start": 1349.1399999999999, "end": 1354.74, "text": " So if you use the tower services that modify the error type, they will return", "tokens": [50404, 407, 498, 291, 764, 264, 10567, 3328, 300, 16927, 264, 6713, 2010, 11, 436, 486, 2736, 50684], "temperature": 0.0, "avg_logprob": -0.1561683803409725, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.0019553527235984802}, {"id": 197, "seek": 134834, "start": 1355.6999999999998, "end": 1362.1799999999998, "text": " the tower box error. And it's much easier to compose. The downside is at some point,", "tokens": [50732, 264, 10567, 2424, 6713, 13, 400, 309, 311, 709, 3571, 281, 35925, 13, 440, 25060, 307, 412, 512, 935, 11, 51056], "temperature": 0.0, "avg_logprob": -0.1561683803409725, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.0019553527235984802}, {"id": 198, "seek": 134834, "start": 1362.1799999999998, "end": 1364.4199999999998, "text": " you have to donk at the error to know exactly what happened.", "tokens": [51056, 291, 362, 281, 500, 74, 412, 264, 6713, 281, 458, 2293, 437, 2011, 13, 51168], "temperature": 0.0, "avg_logprob": -0.1561683803409725, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.0019553527235984802}, {"id": 199, "seek": 134834, "start": 1367.6999999999998, "end": 1372.58, "text": " So we're going to use this error type this time to implement our service on future.", "tokens": [51332, 407, 321, 434, 516, 281, 764, 341, 6713, 2010, 341, 565, 281, 4445, 527, 2643, 322, 2027, 13, 51576], "temperature": 0.0, "avg_logprob": -0.1561683803409725, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.0019553527235984802}, {"id": 200, "seek": 137834, "start": 1378.74, "end": 1386.6599999999999, "text": " So it's very similar than before. The difference this time is the error needs to be boxable.", "tokens": [50384, 407, 309, 311, 588, 2531, 813, 949, 13, 440, 2649, 341, 565, 307, 264, 6713, 2203, 281, 312, 2424, 712, 13, 50780], "temperature": 0.0, "avg_logprob": -0.16381518981036017, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.0005191348027437925}, {"id": 201, "seek": 137834, "start": 1386.6599999999999, "end": 1390.8999999999999, "text": " So with other compiler, I can only be a timeout service for services where the", "tokens": [50780, 407, 365, 661, 31958, 11, 286, 393, 787, 312, 257, 565, 346, 2643, 337, 3328, 689, 264, 50992], "temperature": 0.0, "avg_logprob": -0.16381518981036017, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.0005191348027437925}, {"id": 202, "seek": 137834, "start": 1392.58, "end": 1401.3799999999999, "text": " inner error is boxable. The polarity is a little bit annoying because it's still delegating to the", "tokens": [51076, 7284, 6713, 307, 2424, 712, 13, 440, 12367, 507, 307, 257, 707, 857, 11304, 570, 309, 311, 920, 15824, 990, 281, 264, 51516], "temperature": 0.0, "avg_logprob": -0.16381518981036017, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.0005191348027437925}, {"id": 203, "seek": 140138, "start": 1401.94, "end": 1414.5800000000002, "text": " inner polarity function. But you must not forget to convert the result that is an s error to a box", "tokens": [50392, 7284, 12367, 507, 2445, 13, 583, 291, 1633, 406, 2870, 281, 7620, 264, 1874, 300, 307, 364, 262, 6713, 281, 257, 2424, 51024], "temperature": 0.0, "avg_logprob": -0.14196171258625231, "compression_ratio": 1.676300578034682, "no_speech_prob": 0.002510161604732275}, {"id": 204, "seek": 140138, "start": 1414.5800000000002, "end": 1422.74, "text": " error. So that's why the result returned by the inner service must be, its error must be mapped", "tokens": [51024, 6713, 13, 407, 300, 311, 983, 264, 1874, 8752, 538, 264, 7284, 2643, 1633, 312, 11, 1080, 6713, 1633, 312, 33318, 51432], "temperature": 0.0, "avg_logprob": -0.14196171258625231, "compression_ratio": 1.676300578034682, "no_speech_prob": 0.002510161604732275}, {"id": 205, "seek": 140138, "start": 1422.74, "end": 1430.9, "text": " to the box error. So that's why we need this. Now we implement calls. So we take a look at what", "tokens": [51432, 281, 264, 2424, 6713, 13, 407, 300, 311, 983, 321, 643, 341, 13, 823, 321, 4445, 5498, 13, 407, 321, 747, 257, 574, 412, 437, 51840], "temperature": 0.0, "avg_logprob": -0.14196171258625231, "compression_ratio": 1.676300578034682, "no_speech_prob": 0.002510161604732275}, {"id": 206, "seek": 143090, "start": 1431.46, "end": 1439.6200000000001, "text": " we do as for our future. So our timeout future is very similar to our logging future. It just wraps", "tokens": [50392, 321, 360, 382, 337, 527, 2027, 13, 407, 527, 565, 346, 2027, 307, 588, 2531, 281, 527, 27991, 2027, 13, 467, 445, 25831, 50800], "temperature": 0.0, "avg_logprob": -0.12417648934029243, "compression_ratio": 1.7544910179640718, "no_speech_prob": 0.0015958338044583797}, {"id": 207, "seek": 143090, "start": 1439.6200000000001, "end": 1447.94, "text": " two inner futures. One will be used for the inner future and the other ones will be for the sleep", "tokens": [50800, 732, 7284, 26071, 13, 1485, 486, 312, 1143, 337, 264, 7284, 2027, 293, 264, 661, 2306, 486, 312, 337, 264, 2817, 51216], "temperature": 0.0, "avg_logprob": -0.12417648934029243, "compression_ratio": 1.7544910179640718, "no_speech_prob": 0.0015958338044583797}, {"id": 208, "seek": 143090, "start": 1448.8200000000002, "end": 1455.7800000000002, "text": " future. So the sleep future, you can reuse the future from Tokyo, for instance, if you're using", "tokens": [51260, 2027, 13, 407, 264, 2817, 2027, 11, 291, 393, 26225, 264, 2027, 490, 15147, 11, 337, 5197, 11, 498, 291, 434, 1228, 51608], "temperature": 0.0, "avg_logprob": -0.12417648934029243, "compression_ratio": 1.7544910179640718, "no_speech_prob": 0.0015958338044583797}, {"id": 209, "seek": 145578, "start": 1455.78, "end": 1462.42, "text": " the Tokyo runtime. So call becomes pretty simple. All you have to do is build your inner future and", "tokens": [50364, 264, 15147, 34474, 13, 407, 818, 3643, 1238, 2199, 13, 1057, 291, 362, 281, 360, 307, 1322, 428, 7284, 2027, 293, 50696], "temperature": 0.0, "avg_logprob": -0.1158668363795561, "compression_ratio": 1.6114285714285714, "no_speech_prob": 0.002510381629690528}, {"id": 210, "seek": 145578, "start": 1462.42, "end": 1470.98, "text": " your sleep future and create your timeout future, which we're going to implement now.", "tokens": [50696, 428, 2817, 2027, 293, 1884, 428, 565, 346, 2027, 11, 597, 321, 434, 516, 281, 4445, 586, 13, 51124], "temperature": 0.0, "avg_logprob": -0.1158668363795561, "compression_ratio": 1.6114285714285714, "no_speech_prob": 0.002510381629690528}, {"id": 211, "seek": 145578, "start": 1472.82, "end": 1481.94, "text": " So the core of the logic this time is leaves in the code of the future itself. So some libraries", "tokens": [51216, 407, 264, 4965, 295, 264, 9952, 341, 565, 307, 5510, 294, 264, 3089, 295, 264, 2027, 2564, 13, 407, 512, 15148, 51672], "temperature": 0.0, "avg_logprob": -0.1158668363795561, "compression_ratio": 1.6114285714285714, "no_speech_prob": 0.002510381629690528}, {"id": 212, "seek": 148194, "start": 1482.02, "end": 1491.94, "text": " sometimes split the service implementation in one module and they put the future in another", "tokens": [50368, 2171, 7472, 264, 2643, 11420, 294, 472, 10088, 293, 436, 829, 264, 2027, 294, 1071, 50864], "temperature": 0.0, "avg_logprob": -0.19822102624016838, "compression_ratio": 1.6614583333333333, "no_speech_prob": 0.004603465553373098}, {"id": 213, "seek": 148194, "start": 1491.94, "end": 1496.02, "text": " module so you open the service and you're like, what does it do? Because everything happens in the", "tokens": [50864, 10088, 370, 291, 1269, 264, 2643, 293, 291, 434, 411, 11, 437, 775, 309, 360, 30, 1436, 1203, 2314, 294, 264, 51068], "temperature": 0.0, "avg_logprob": -0.19822102624016838, "compression_ratio": 1.6614583333333333, "no_speech_prob": 0.004603465553373098}, {"id": 214, "seek": 148194, "start": 1496.02, "end": 1504.42, "text": " future. So you need to understand that sometimes a lot of the work is actually done in the future.", "tokens": [51068, 2027, 13, 407, 291, 643, 281, 1223, 300, 2171, 257, 688, 295, 264, 589, 307, 767, 1096, 294, 264, 2027, 13, 51488], "temperature": 0.0, "avg_logprob": -0.19822102624016838, "compression_ratio": 1.6614583333333333, "no_speech_prob": 0.004603465553373098}, {"id": 215, "seek": 148194, "start": 1505.6200000000001, "end": 1506.74, "text": " And this depends for timeout.", "tokens": [51548, 400, 341, 5946, 337, 565, 346, 13, 51604], "temperature": 0.0, "avg_logprob": -0.19822102624016838, "compression_ratio": 1.6614583333333333, "no_speech_prob": 0.004603465553373098}, {"id": 216, "seek": 150674, "start": 1506.74, "end": 1512.02, "text": " So implementing future for timeout future,", "tokens": [50364, 407, 18114, 2027, 337, 565, 346, 2027, 11, 50628], "temperature": 0.0, "avg_logprob": -0.2128494353521438, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0006767368176952004}, {"id": 217, "seek": 150674, "start": 1515.7, "end": 1524.26, "text": " we, as before, we project self into this. We pull the first future, the inner future,", "tokens": [50812, 321, 11, 382, 949, 11, 321, 1716, 2698, 666, 341, 13, 492, 2235, 264, 700, 2027, 11, 264, 7284, 2027, 11, 51240], "temperature": 0.0, "avg_logprob": -0.2128494353521438, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0006767368176952004}, {"id": 218, "seek": 150674, "start": 1524.9, "end": 1532.02, "text": " and if it's ready, great. It's now the timeout, we return pull ready with the result. We don't", "tokens": [51272, 293, 498, 309, 311, 1919, 11, 869, 13, 467, 311, 586, 264, 565, 346, 11, 321, 2736, 2235, 1919, 365, 264, 1874, 13, 492, 500, 380, 51628], "temperature": 0.0, "avg_logprob": -0.2128494353521438, "compression_ratio": 1.5486111111111112, "no_speech_prob": 0.0006767368176952004}, {"id": 219, "seek": 153202, "start": 1532.1, "end": 1538.74, "text": " forget to map the error, the potential error into its box. If the inner future is not ready,", "tokens": [50368, 2870, 281, 4471, 264, 6713, 11, 264, 3995, 6713, 666, 1080, 2424, 13, 759, 264, 7284, 2027, 307, 406, 1919, 11, 50700], "temperature": 0.0, "avg_logprob": -0.13393312181745257, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0007205765577964485}, {"id": 220, "seek": 153202, "start": 1538.74, "end": 1544.02, "text": " maybe it's taking too long. So we need to verify whether a timeout occurred or not. So we pulled", "tokens": [50700, 1310, 309, 311, 1940, 886, 938, 13, 407, 321, 643, 281, 16888, 1968, 257, 565, 346, 11068, 420, 406, 13, 407, 321, 7373, 50964], "temperature": 0.0, "avg_logprob": -0.13393312181745257, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0007205765577964485}, {"id": 221, "seek": 153202, "start": 1544.66, "end": 1551.3799999999999, "text": " timeout future this time. So if it's pending, nothing to do, we'll be pulled again, we'll go", "tokens": [50996, 565, 346, 2027, 341, 565, 13, 407, 498, 309, 311, 32110, 11, 1825, 281, 360, 11, 321, 603, 312, 7373, 797, 11, 321, 603, 352, 51332], "temperature": 0.0, "avg_logprob": -0.13393312181745257, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0007205765577964485}, {"id": 222, "seek": 153202, "start": 1551.3799999999999, "end": 1559.62, "text": " this work again. If it's ready, it means this is an actual timeout. So we need to return the timeout", "tokens": [51332, 341, 589, 797, 13, 759, 309, 311, 1919, 11, 309, 1355, 341, 307, 364, 3539, 565, 346, 13, 407, 321, 643, 281, 2736, 264, 565, 346, 51744], "temperature": 0.0, "avg_logprob": -0.13393312181745257, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0007205765577964485}, {"id": 223, "seek": 155962, "start": 1559.62, "end": 1569.3799999999999, "text": " error and we return elapsed error, but boxed. Hence the hint too.", "tokens": [50364, 6713, 293, 321, 2736, 806, 2382, 292, 6713, 11, 457, 2424, 292, 13, 22229, 264, 12075, 886, 13, 50852], "temperature": 0.0, "avg_logprob": -0.24672794342041016, "compression_ratio": 1.323943661971831, "no_speech_prob": 0.0009844595333561301}, {"id": 224, "seek": 155962, "start": 1573.78, "end": 1577.2199999999998, "text": " So now we're going to see how you can stack services together.", "tokens": [51072, 407, 586, 321, 434, 516, 281, 536, 577, 291, 393, 8630, 3328, 1214, 13, 51244], "temperature": 0.0, "avg_logprob": -0.24672794342041016, "compression_ratio": 1.323943661971831, "no_speech_prob": 0.0009844595333561301}, {"id": 225, "seek": 155962, "start": 1581.2199999999998, "end": 1587.06, "text": " I could obviously stack my timeout on top of my LO and then", "tokens": [51444, 286, 727, 2745, 8630, 452, 565, 346, 322, 1192, 295, 452, 15731, 293, 550, 51736], "temperature": 0.0, "avg_logprob": -0.24672794342041016, "compression_ratio": 1.323943661971831, "no_speech_prob": 0.0009844595333561301}, {"id": 226, "seek": 158706, "start": 1588.02, "end": 1594.1799999999998, "text": " add the logging on top of it, but I can also do it with services that are already ready in", "tokens": [50412, 909, 264, 27991, 322, 1192, 295, 309, 11, 457, 286, 393, 611, 360, 309, 365, 3328, 300, 366, 1217, 1919, 294, 50720], "temperature": 0.0, "avg_logprob": -0.1886090195697287, "compression_ratio": 1.8465608465608465, "no_speech_prob": 0.0029284523334354162}, {"id": 227, "seek": 158706, "start": 1594.1799999999998, "end": 1599.78, "text": " tower. So I import them directly. So I import the concurrency limit, the timeout,", "tokens": [50720, 10567, 13, 407, 286, 974, 552, 3838, 13, 407, 286, 974, 264, 23702, 10457, 4948, 11, 264, 565, 346, 11, 51000], "temperature": 0.0, "avg_logprob": -0.1886090195697287, "compression_ratio": 1.8465608465608465, "no_speech_prob": 0.0029284523334354162}, {"id": 228, "seek": 158706, "start": 1601.06, "end": 1607.94, "text": " and I compose my service by wrapping everything on top of each other. So instantiate service,", "tokens": [51064, 293, 286, 35925, 452, 2643, 538, 21993, 1203, 322, 1192, 295, 1184, 661, 13, 407, 9836, 13024, 2643, 11, 51408], "temperature": 0.0, "avg_logprob": -0.1886090195697287, "compression_ratio": 1.8465608465608465, "no_speech_prob": 0.0029284523334354162}, {"id": 229, "seek": 158706, "start": 1608.5, "end": 1612.8999999999999, "text": " I add a concurrency limit on top of it, and then my timeout, and then the logging.", "tokens": [51436, 286, 909, 257, 23702, 10457, 4948, 322, 1192, 295, 309, 11, 293, 550, 452, 565, 346, 11, 293, 550, 264, 27991, 13, 51656], "temperature": 0.0, "avg_logprob": -0.1886090195697287, "compression_ratio": 1.8465608465608465, "no_speech_prob": 0.0029284523334354162}, {"id": 230, "seek": 161290, "start": 1612.9, "end": 1620.8200000000002, "text": " So that involves a bit of body play, but it's not too hard to do. What's really tricky and the", "tokens": [50364, 407, 300, 11626, 257, 857, 295, 1772, 862, 11, 457, 309, 311, 406, 886, 1152, 281, 360, 13, 708, 311, 534, 12414, 293, 264, 50760], "temperature": 0.0, "avg_logprob": -0.12442469987712923, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0007094044703990221}, {"id": 231, "seek": 161290, "start": 1620.8200000000002, "end": 1626.5, "text": " compiler is not going to help you with that is the order with which you wrap your services actually", "tokens": [50760, 31958, 307, 406, 516, 281, 854, 291, 365, 300, 307, 264, 1668, 365, 597, 291, 7019, 428, 3328, 767, 51044], "temperature": 0.0, "avg_logprob": -0.12442469987712923, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0007094044703990221}, {"id": 232, "seek": 161290, "start": 1626.5, "end": 1632.8200000000002, "text": " matters. You really want your logging to be on top. If your logging is like logging how long it", "tokens": [51044, 7001, 13, 509, 534, 528, 428, 27991, 281, 312, 322, 1192, 13, 759, 428, 27991, 307, 411, 27991, 577, 938, 309, 51360], "temperature": 0.0, "avg_logprob": -0.12442469987712923, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0007094044703990221}, {"id": 233, "seek": 161290, "start": 1632.8200000000002, "end": 1637.0600000000002, "text": " took, you don't want logging to be in the middle. You really want it to be on top to capture the", "tokens": [51360, 1890, 11, 291, 500, 380, 528, 27991, 281, 312, 294, 264, 2808, 13, 509, 534, 528, 309, 281, 312, 322, 1192, 281, 7983, 264, 51572], "temperature": 0.0, "avg_logprob": -0.12442469987712923, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0007094044703990221}, {"id": 234, "seek": 161290, "start": 1637.0600000000002, "end": 1641.46, "text": " whole life cycle of the request. And it's probably the same thing. You want timeout to be applied", "tokens": [51572, 1379, 993, 6586, 295, 264, 5308, 13, 400, 309, 311, 1391, 264, 912, 551, 13, 509, 528, 565, 346, 281, 312, 6456, 51792], "temperature": 0.0, "avg_logprob": -0.12442469987712923, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0007094044703990221}, {"id": 235, "seek": 164146, "start": 1642.3400000000001, "end": 1649.46, "text": " above everything that's doing race limiting. It would be better to, if you had like a notification", "tokens": [50408, 3673, 1203, 300, 311, 884, 4569, 22083, 13, 467, 576, 312, 1101, 281, 11, 498, 291, 632, 411, 257, 11554, 50764], "temperature": 0.0, "avg_logprob": -0.13393801385229762, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0024328643921762705}, {"id": 236, "seek": 164146, "start": 1649.46, "end": 1656.18, "text": " service, it would be better to put it pretty high up because if you have services that are below it,", "tokens": [50764, 2643, 11, 309, 576, 312, 1101, 281, 829, 309, 1238, 1090, 493, 570, 498, 291, 362, 3328, 300, 366, 2507, 309, 11, 51100], "temperature": 0.0, "avg_logprob": -0.13393801385229762, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0024328643921762705}, {"id": 237, "seek": 164146, "start": 1656.18, "end": 1662.58, "text": " maybe they can do whatever they want. So this is what's tricky when stacking layers and the", "tokens": [51100, 1310, 436, 393, 360, 2035, 436, 528, 13, 407, 341, 307, 437, 311, 12414, 562, 41376, 7914, 293, 264, 51420], "temperature": 0.0, "avg_logprob": -0.13393801385229762, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0024328643921762705}, {"id": 238, "seek": 164146, "start": 1662.58, "end": 1671.3, "text": " compiler is not going to help you there. So you got to be careful and you can hopefully rely", "tokens": [51420, 31958, 307, 406, 516, 281, 854, 291, 456, 13, 407, 291, 658, 281, 312, 5026, 293, 291, 393, 4696, 10687, 51856], "temperature": 0.0, "avg_logprob": -0.13393801385229762, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0024328643921762705}, {"id": 239, "seek": 167130, "start": 1671.3, "end": 1679.7, "text": " on a good reviewer for like a double check. I wanted to leave a lot of time for questions.", "tokens": [50364, 322, 257, 665, 3131, 260, 337, 411, 257, 3834, 1520, 13, 286, 1415, 281, 1856, 257, 688, 295, 565, 337, 1651, 13, 50784], "temperature": 0.0, "avg_logprob": -0.12572452000209264, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0007203487330116332}, {"id": 240, "seek": 167130, "start": 1681.62, "end": 1686.8999999999999, "text": " This talk is called a deep dive into TARRA. I realized that in 30 minutes it's actually", "tokens": [50880, 639, 751, 307, 1219, 257, 2452, 9192, 666, 314, 1899, 3750, 13, 286, 5334, 300, 294, 2217, 2077, 309, 311, 767, 51144], "temperature": 0.0, "avg_logprob": -0.12572452000209264, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0007203487330116332}, {"id": 241, "seek": 167130, "start": 1687.78, "end": 1692.74, "text": " not that easy to cover everything that we could cover when we talk about TARRA. So let's just", "tokens": [51188, 406, 300, 1858, 281, 2060, 1203, 300, 321, 727, 2060, 562, 321, 751, 466, 314, 1899, 3750, 13, 407, 718, 311, 445, 51436], "temperature": 0.0, "avg_logprob": -0.12572452000209264, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0007203487330116332}, {"id": 242, "seek": 167130, "start": 1692.74, "end": 1699.86, "text": " call it a shallow deep dive into TARRA. If you want a real deep dive, there are really good", "tokens": [51436, 818, 309, 257, 20488, 2452, 9192, 666, 314, 1899, 3750, 13, 759, 291, 528, 257, 957, 2452, 9192, 11, 456, 366, 534, 665, 51792], "temperature": 0.0, "avg_logprob": -0.12572452000209264, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0007203487330116332}, {"id": 243, "seek": 169986, "start": 1699.86, "end": 1705.06, "text": " resources out there. Two years ago, not that much, but there's really great content,", "tokens": [50364, 3593, 484, 456, 13, 4453, 924, 2057, 11, 406, 300, 709, 11, 457, 456, 311, 534, 869, 2701, 11, 50624], "temperature": 0.0, "avg_logprob": -0.1429686194971988, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.00400554621592164}, {"id": 244, "seek": 169986, "start": 1705.06, "end": 1710.82, "text": " blog posts on YouTube that I'm going to talk about a little bit that really can help you get really,", "tokens": [50624, 6968, 12300, 322, 3088, 300, 286, 478, 516, 281, 751, 466, 257, 707, 857, 300, 534, 393, 854, 291, 483, 534, 11, 50912], "temperature": 0.0, "avg_logprob": -0.1429686194971988, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.00400554621592164}, {"id": 245, "seek": 169986, "start": 1710.82, "end": 1718.4199999999998, "text": " really comfortable with TARRA. So there's something that I don't talk about today called a layer.", "tokens": [50912, 534, 4619, 365, 314, 1899, 3750, 13, 407, 456, 311, 746, 300, 286, 500, 380, 751, 466, 965, 1219, 257, 4583, 13, 51292], "temperature": 0.0, "avg_logprob": -0.1429686194971988, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.00400554621592164}, {"id": 246, "seek": 169986, "start": 1719.3, "end": 1725.1399999999999, "text": " A layer, TARRA people are pretty obsessed with composability. So a layer is a way to compose", "tokens": [51336, 316, 4583, 11, 314, 1899, 3750, 561, 366, 1238, 16923, 365, 10199, 2310, 13, 407, 257, 4583, 307, 257, 636, 281, 35925, 51628], "temperature": 0.0, "avg_logprob": -0.1429686194971988, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.00400554621592164}, {"id": 247, "seek": 172514, "start": 1725.14, "end": 1731.22, "text": " a stack of services. A service builder is a way to conveniently build", "tokens": [50364, 257, 8630, 295, 3328, 13, 316, 2643, 27377, 307, 257, 636, 281, 44375, 1322, 50668], "temperature": 0.0, "avg_logprob": -0.1484145376417372, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.001986126881092787}, {"id": 248, "seek": 172514, "start": 1732.5800000000002, "end": 1737.94, "text": " small stacks of services. A service builder helps you get rid of the boilerplate to write this,", "tokens": [50736, 1359, 30792, 295, 3328, 13, 316, 2643, 27377, 3665, 291, 483, 3973, 295, 264, 39228, 37008, 281, 2464, 341, 11, 51004], "temperature": 0.0, "avg_logprob": -0.1484145376417372, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.001986126881092787}, {"id": 249, "seek": 172514, "start": 1737.94, "end": 1746.26, "text": " for instance. So if you want to keep studying TARRA, I would start with layer and service builder.", "tokens": [51004, 337, 5197, 13, 407, 498, 291, 528, 281, 1066, 7601, 314, 1899, 3750, 11, 286, 576, 722, 365, 4583, 293, 2643, 27377, 13, 51420], "temperature": 0.0, "avg_logprob": -0.1484145376417372, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.001986126881092787}, {"id": 250, "seek": 172514, "start": 1747.14, "end": 1753.38, "text": " Then I would recommend reading, inventing the service rate. That's a blog post by David Pedersen,", "tokens": [51464, 1396, 286, 576, 2748, 3760, 11, 7962, 278, 264, 2643, 3314, 13, 663, 311, 257, 6968, 2183, 538, 4389, 16689, 433, 268, 11, 51776], "temperature": 0.0, "avg_logprob": -0.1484145376417372, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.001986126881092787}, {"id": 251, "seek": 175338, "start": 1753.38, "end": 1758.5800000000002, "text": " who's a TARRA contributor. It's a really good introductory blog post. Maybe you can start with", "tokens": [50364, 567, 311, 257, 314, 1899, 3750, 42859, 13, 467, 311, 257, 534, 665, 39048, 6968, 2183, 13, 2704, 291, 393, 722, 365, 50624], "temperature": 0.0, "avg_logprob": -0.1713444621292586, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0013027433305978775}, {"id": 252, "seek": 175338, "start": 1758.5800000000002, "end": 1766.5, "text": " that. It takes about 15 minutes. I find that the AXM documentation page about MinoWares is really,", "tokens": [50624, 300, 13, 467, 2516, 466, 2119, 2077, 13, 286, 915, 300, 264, 316, 55, 44, 14333, 3028, 466, 2829, 78, 54, 8643, 307, 534, 11, 51020], "temperature": 0.0, "avg_logprob": -0.1713444621292586, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0013027433305978775}, {"id": 253, "seek": 175338, "start": 1766.5, "end": 1776.5800000000002, "text": " really good. So I think that's a really good resource. And then in the spirit of building and", "tokens": [51020, 534, 665, 13, 407, 286, 519, 300, 311, 257, 534, 665, 7684, 13, 400, 550, 294, 264, 3797, 295, 2390, 293, 51524], "temperature": 0.0, "avg_logprob": -0.1713444621292586, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0013027433305978775}, {"id": 254, "seek": 175338, "start": 1776.5800000000002, "end": 1782.2600000000002, "text": " reading and writing late services that are more and more complex, I think the next step for us", "tokens": [51524, 3760, 293, 3579, 3469, 3328, 300, 366, 544, 293, 544, 3997, 11, 286, 519, 264, 958, 1823, 337, 505, 51808], "temperature": 0.0, "avg_logprob": -0.1713444621292586, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0013027433305978775}, {"id": 255, "seek": 178226, "start": 1782.26, "end": 1788.18, "text": " is looking at the rate limit service. The concurrency limit is also a bit more complex,", "tokens": [50364, 307, 1237, 412, 264, 3314, 4948, 2643, 13, 440, 23702, 10457, 4948, 307, 611, 257, 857, 544, 3997, 11, 50660], "temperature": 0.0, "avg_logprob": -0.10535417719090238, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.009691436775028706}, {"id": 256, "seek": 178226, "start": 1788.18, "end": 1795.46, "text": " but really interesting. And then you can look at the channel from Tonic. It gets pretty complex", "tokens": [50660, 457, 534, 1880, 13, 400, 550, 291, 393, 574, 412, 264, 2269, 490, 11385, 299, 13, 467, 2170, 1238, 3997, 51024], "temperature": 0.0, "avg_logprob": -0.10535417719090238, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.009691436775028706}, {"id": 257, "seek": 178226, "start": 1795.46, "end": 1801.3, "text": " as well. And then if you really want to take it to the next level, think the pool in TARRA. So", "tokens": [51024, 382, 731, 13, 400, 550, 498, 291, 534, 528, 281, 747, 309, 281, 264, 958, 1496, 11, 519, 264, 7005, 294, 314, 1899, 3750, 13, 407, 51316], "temperature": 0.0, "avg_logprob": -0.10535417719090238, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.009691436775028706}, {"id": 258, "seek": 178226, "start": 1802.5, "end": 1808.02, "text": " here we're only showing that a service that wraps just one service. But now you could wrap", "tokens": [51376, 510, 321, 434, 787, 4099, 300, 257, 2643, 300, 25831, 445, 472, 2643, 13, 583, 586, 291, 727, 7019, 51652], "temperature": 0.0, "avg_logprob": -0.10535417719090238, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.009691436775028706}, {"id": 259, "seek": 180802, "start": 1808.02, "end": 1814.5, "text": " multiple service. And that's what the pool adds. And the pool is a service. It wraps multiple", "tokens": [50364, 3866, 2643, 13, 400, 300, 311, 437, 264, 7005, 10860, 13, 400, 264, 7005, 307, 257, 2643, 13, 467, 25831, 3866, 50688], "temperature": 0.0, "avg_logprob": -0.17926035429302015, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.004259598907083273}, {"id": 260, "seek": 180802, "start": 1814.5, "end": 1821.46, "text": " services. And it's using pool ready to track the load of each inner service to handle back pressure.", "tokens": [50688, 3328, 13, 400, 309, 311, 1228, 7005, 1919, 281, 2837, 264, 3677, 295, 1184, 7284, 2643, 281, 4813, 646, 3321, 13, 51036], "temperature": 0.0, "avg_logprob": -0.17926035429302015, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.004259598907083273}, {"id": 261, "seek": 180802, "start": 1821.46, "end": 1828.5, "text": " So it's a really interesting use case of using pool ready in a very fancy way. So pool is also", "tokens": [51036, 407, 309, 311, 257, 534, 1880, 764, 1389, 295, 1228, 7005, 1919, 294, 257, 588, 10247, 636, 13, 407, 7005, 307, 611, 51388], "temperature": 0.0, "avg_logprob": -0.17926035429302015, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.004259598907083273}, {"id": 262, "seek": 180802, "start": 1828.5, "end": 1837.3, "text": " very interesting. Good videos on YouTube. Also David Pedersen has a TARRA stream about tower.", "tokens": [51388, 588, 1880, 13, 2205, 2145, 322, 3088, 13, 2743, 4389, 16689, 433, 268, 575, 257, 314, 1899, 3750, 4309, 466, 10567, 13, 51828], "temperature": 0.0, "avg_logprob": -0.17926035429302015, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.004259598907083273}, {"id": 263, "seek": 183802, "start": 1838.02, "end": 1844.74, "text": " He goes through the same thing. There's a hello and logging and time out. And you see him dealing", "tokens": [50364, 634, 1709, 807, 264, 912, 551, 13, 821, 311, 257, 7751, 293, 27991, 293, 565, 484, 13, 400, 291, 536, 796, 6260, 50700], "temperature": 0.0, "avg_logprob": -0.23427739350692087, "compression_ratio": 1.584, "no_speech_prob": 0.003219898324459791}, {"id": 264, "seek": 183802, "start": 1844.74, "end": 1849.78, "text": " like the old delivery things. Because obviously on presentation, I get everything right on the first", "tokens": [50700, 411, 264, 1331, 8982, 721, 13, 1436, 2745, 322, 5860, 11, 286, 483, 1203, 558, 322, 264, 700, 50952], "temperature": 0.0, "avg_logprob": -0.23427739350692087, "compression_ratio": 1.584, "no_speech_prob": 0.003219898324459791}, {"id": 265, "seek": 183802, "start": 1849.78, "end": 1855.3, "text": " try. But it's not exactly how that happens when you either yourself. And if you want to get more", "tokens": [50952, 853, 13, 583, 309, 311, 406, 2293, 577, 300, 2314, 562, 291, 2139, 1803, 13, 400, 498, 291, 528, 281, 483, 544, 51228], "temperature": 0.0, "avg_logprob": -0.23427739350692087, "compression_ratio": 1.584, "no_speech_prob": 0.003219898324459791}, {"id": 266, "seek": 183802, "start": 1855.3, "end": 1861.3, "text": " comfortable with a single weight in rest and futures in general, John has a great talk on YouTube as", "tokens": [51228, 4619, 365, 257, 2167, 3364, 294, 1472, 293, 26071, 294, 2674, 11, 2619, 575, 257, 869, 751, 322, 3088, 382, 51528], "temperature": 0.0, "avg_logprob": -0.23427739350692087, "compression_ratio": 1.584, "no_speech_prob": 0.003219898324459791}, {"id": 267, "seek": 186130, "start": 1861.3, "end": 1866.8999999999999, "text": " well. And you'll find those resources on the slides that are available at this thing.", "tokens": [50364, 731, 13, 400, 291, 603, 915, 729, 3593, 322, 264, 9788, 300, 366, 2435, 412, 341, 551, 13, 50644], "temperature": 0.0, "avg_logprob": -0.2805898530142648, "compression_ratio": 1.103896103896104, "no_speech_prob": 0.012668637558817863}, {"id": 268, "seek": 189130, "start": 1891.62, "end": 1898.4199999999998, "text": " Back in the first few slides, you mentioned the pool ready function. And you were initializing a", "tokens": [50380, 5833, 294, 264, 700, 1326, 9788, 11, 291, 2835, 264, 7005, 1919, 2445, 13, 400, 291, 645, 5883, 3319, 257, 50720], "temperature": 0.0, "avg_logprob": -0.20636448559460338, "compression_ratio": 1.9773755656108598, "no_speech_prob": 0.028839686885476112}, {"id": 269, "seek": 189130, "start": 1898.4199999999998, "end": 1903.7, "text": " database connection. I didn't get it. If the database connection is actually a future or not,", "tokens": [50720, 8149, 4984, 13, 286, 994, 380, 483, 309, 13, 759, 264, 8149, 4984, 307, 767, 257, 2027, 420, 406, 11, 50984], "temperature": 0.0, "avg_logprob": -0.20636448559460338, "compression_ratio": 1.9773755656108598, "no_speech_prob": 0.028839686885476112}, {"id": 270, "seek": 189130, "start": 1903.7, "end": 1905.62, "text": " because the pool ready function doesn't return a future.", "tokens": [50984, 570, 264, 7005, 1919, 2445, 1177, 380, 2736, 257, 2027, 13, 51080], "temperature": 0.0, "avg_logprob": -0.20636448559460338, "compression_ratio": 1.9773755656108598, "no_speech_prob": 0.028839686885476112}, {"id": 271, "seek": 189130, "start": 1908.6599999999999, "end": 1912.8999999999999, "text": " Is you talking the database one? Yeah, that's where you initialize the connection. You know,", "tokens": [51232, 1119, 291, 1417, 264, 8149, 472, 30, 865, 11, 300, 311, 689, 291, 5883, 1125, 264, 4984, 13, 509, 458, 11, 51444], "temperature": 0.0, "avg_logprob": -0.20636448559460338, "compression_ratio": 1.9773755656108598, "no_speech_prob": 0.028839686885476112}, {"id": 272, "seek": 189130, "start": 1912.8999999999999, "end": 1918.82, "text": " you depend on a connection. Yeah, it has to be a future. I mean, no, it doesn't return a future.", "tokens": [51444, 291, 5672, 322, 257, 4984, 13, 865, 11, 309, 575, 281, 312, 257, 2027, 13, 286, 914, 11, 572, 11, 309, 1177, 380, 2736, 257, 2027, 13, 51740], "temperature": 0.0, "avg_logprob": -0.20636448559460338, "compression_ratio": 1.9773755656108598, "no_speech_prob": 0.028839686885476112}, {"id": 273, "seek": 191882, "start": 1918.82, "end": 1925.7, "text": " But because that one, yeah, this one. Yeah, I'll pass quickly because it's a bit complex.", "tokens": [50364, 583, 570, 300, 472, 11, 1338, 11, 341, 472, 13, 865, 11, 286, 603, 1320, 2661, 570, 309, 311, 257, 857, 3997, 13, 50708], "temperature": 0.0, "avg_logprob": -0.24862957000732422, "compression_ratio": 1.5872093023255813, "no_speech_prob": 0.0028422228060662746}, {"id": 274, "seek": 191882, "start": 1927.06, "end": 1936.1799999999998, "text": " So already doesn't return a future. But it behaves like one because you pass it the context,", "tokens": [50776, 407, 1217, 1177, 380, 2736, 257, 2027, 13, 583, 309, 36896, 411, 472, 570, 291, 1320, 309, 264, 4319, 11, 51232], "temperature": 0.0, "avg_logprob": -0.24862957000732422, "compression_ratio": 1.5872093023255813, "no_speech_prob": 0.0028422228060662746}, {"id": 275, "seek": 191882, "start": 1936.1799999999998, "end": 1943.9399999999998, "text": " the context as a waker so you can rebuild future really easily with access to the context.", "tokens": [51232, 264, 4319, 382, 257, 261, 4003, 370, 291, 393, 16877, 2027, 534, 3612, 365, 2105, 281, 264, 4319, 13, 51620], "temperature": 0.0, "avg_logprob": -0.24862957000732422, "compression_ratio": 1.5872093023255813, "no_speech_prob": 0.0028422228060662746}, {"id": 276, "seek": 194394, "start": 1944.26, "end": 1950.3400000000001, "text": " And that's I'm happy you asked you asking this question because if you look at", "tokens": [50380, 400, 300, 311, 286, 478, 2055, 291, 2351, 291, 3365, 341, 1168, 570, 498, 291, 574, 412, 50684], "temperature": 0.0, "avg_logprob": -0.30113891526764514, "compression_ratio": 1.4393939393939394, "no_speech_prob": 0.005378010682761669}, {"id": 277, "seek": 194394, "start": 1952.02, "end": 1952.8200000000002, "text": " come back here.", "tokens": [50768, 808, 646, 510, 13, 50808], "temperature": 0.0, "avg_logprob": -0.30113891526764514, "compression_ratio": 1.4393939393939394, "no_speech_prob": 0.005378010682761669}, {"id": 278, "seek": 194394, "start": 1958.3400000000001, "end": 1967.78, "text": " Talk about the concurrency element. The concurrency element uses a semaphore. So a naive way of", "tokens": [51084, 8780, 466, 264, 23702, 10457, 4478, 13, 440, 23702, 10457, 4478, 4960, 257, 4361, 13957, 418, 13, 407, 257, 29052, 636, 295, 51556], "temperature": 0.0, "avg_logprob": -0.30113891526764514, "compression_ratio": 1.4393939393939394, "no_speech_prob": 0.005378010682761669}, {"id": 279, "seek": 196778, "start": 1968.74, "end": 1977.78, "text": " implementing pool ready with a semaphore is would be to call try a choir. And if you get the permit", "tokens": [50412, 18114, 7005, 1919, 365, 257, 4361, 13957, 418, 307, 576, 312, 281, 818, 853, 257, 31244, 13, 400, 498, 291, 483, 264, 13423, 50864], "temperature": 0.0, "avg_logprob": -0.24839980978714793, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.028414582833647728}, {"id": 280, "seek": 196778, "start": 1977.78, "end": 1984.82, "text": " right away at school, pool ready is ready and just save the permit and then you get rid of it", "tokens": [50864, 558, 1314, 412, 1395, 11, 7005, 1919, 307, 1919, 293, 445, 3155, 264, 13423, 293, 550, 291, 483, 3973, 295, 309, 51216], "temperature": 0.0, "avg_logprob": -0.24839980978714793, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.028414582833647728}, {"id": 281, "seek": 196778, "start": 1984.82, "end": 1992.34, "text": " in call once you're done. But a better way to do is to start pulling the future. You don't call", "tokens": [51216, 294, 818, 1564, 291, 434, 1096, 13, 583, 257, 1101, 636, 281, 360, 307, 281, 722, 8407, 264, 2027, 13, 509, 500, 380, 818, 51592], "temperature": 0.0, "avg_logprob": -0.24839980978714793, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.028414582833647728}, {"id": 282, "seek": 199234, "start": 1992.34, "end": 1998.1799999999998, "text": " pool ready. You call ready. And if it's not ready yet, it's okay. You keep you still maintain", "tokens": [50364, 7005, 1919, 13, 509, 818, 1919, 13, 400, 498, 309, 311, 406, 1919, 1939, 11, 309, 311, 1392, 13, 509, 1066, 291, 920, 6909, 50656], "temperature": 0.0, "avg_logprob": -0.21052903110541185, "compression_ratio": 1.7685589519650655, "no_speech_prob": 0.013377117924392223}, {"id": 283, "seek": 199234, "start": 1998.1799999999998, "end": 2006.5, "text": " that future that you started that started acquiring a permit. You save it in your internal state.", "tokens": [50656, 300, 2027, 300, 291, 1409, 300, 1409, 37374, 257, 13423, 13, 509, 3155, 309, 294, 428, 6920, 1785, 13, 51072], "temperature": 0.0, "avg_logprob": -0.21052903110541185, "compression_ratio": 1.7685589519650655, "no_speech_prob": 0.013377117924392223}, {"id": 284, "seek": 199234, "start": 2006.5, "end": 2010.74, "text": " And then the next time you keep pulling that future instead of each time doing try, acquire,", "tokens": [51072, 400, 550, 264, 958, 565, 291, 1066, 8407, 300, 2027, 2602, 295, 1184, 565, 884, 853, 11, 20001, 11, 51284], "temperature": 0.0, "avg_logprob": -0.21052903110541185, "compression_ratio": 1.7685589519650655, "no_speech_prob": 0.013377117924392223}, {"id": 285, "seek": 199234, "start": 2010.74, "end": 2016.02, "text": " try, acquire. So it's really unique to trick that you can see in the concurrency element service.", "tokens": [51284, 853, 11, 20001, 13, 407, 309, 311, 534, 3845, 281, 4282, 300, 291, 393, 536, 294, 264, 23702, 10457, 4478, 2643, 13, 51548], "temperature": 0.0, "avg_logprob": -0.21052903110541185, "compression_ratio": 1.7685589519650655, "no_speech_prob": 0.013377117924392223}, {"id": 286, "seek": 199234, "start": 2017.3799999999999, "end": 2018.4199999999998, "text": " Okay, cool. Thank you.", "tokens": [51616, 1033, 11, 1627, 13, 1044, 291, 13, 51668], "temperature": 0.0, "avg_logprob": -0.21052903110541185, "compression_ratio": 1.7685589519650655, "no_speech_prob": 0.013377117924392223}, {"id": 287, "seek": 201842, "start": 2019.0600000000002, "end": 2028.5800000000002, "text": " When is tower gonna go version one because now it's on zero and hyper just ditched it because", "tokens": [50396, 1133, 307, 10567, 799, 352, 3037, 472, 570, 586, 309, 311, 322, 4018, 293, 9848, 445, 25325, 292, 309, 570, 50872], "temperature": 0.0, "avg_logprob": -0.25445275557668584, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.009565284475684166}, {"id": 288, "seek": 201842, "start": 2028.5800000000002, "end": 2035.94, "text": " it's not one yet. So I'm not a tower contributor. So I don't know exactly. I think they're waiting", "tokens": [50872, 309, 311, 406, 472, 1939, 13, 407, 286, 478, 406, 257, 10567, 42859, 13, 407, 286, 500, 380, 458, 2293, 13, 286, 519, 436, 434, 3806, 51240], "temperature": 0.0, "avg_logprob": -0.25445275557668584, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.009565284475684166}, {"id": 289, "seek": 201842, "start": 2035.94, "end": 2043.14, "text": " for the the Khmer people to give us the full story of what is sync is gonna is gonna become.", "tokens": [51240, 337, 264, 264, 11681, 936, 561, 281, 976, 505, 264, 1577, 1657, 295, 437, 307, 20271, 307, 799, 307, 799, 1813, 13, 51600], "temperature": 0.0, "avg_logprob": -0.25445275557668584, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.009565284475684166}, {"id": 290, "seek": 204314, "start": 2043.14, "end": 2046.8200000000002, "text": " You'll see that with the last release of rust now you can return", "tokens": [50364, 509, 603, 536, 300, 365, 264, 1036, 4374, 295, 15259, 586, 291, 393, 2736, 50548], "temperature": 0.0, "avg_logprob": -0.2848944389957121, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.033550016582012177}, {"id": 291, "seek": 204314, "start": 2048.34, "end": 2053.62, "text": " an implement implement future future, but they still like the weather it said or not to to deal", "tokens": [50624, 364, 4445, 4445, 2027, 2027, 11, 457, 436, 920, 411, 264, 5503, 309, 848, 420, 406, 281, 281, 2028, 50888], "temperature": 0.0, "avg_logprob": -0.2848944389957121, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.033550016582012177}, {"id": 292, "seek": 204314, "start": 2053.62, "end": 2061.78, "text": " with though. So eventually, I think they want they're waiting for see what we can do in rust.", "tokens": [50888, 365, 1673, 13, 407, 4728, 11, 286, 519, 436, 528, 436, 434, 3806, 337, 536, 437, 321, 393, 360, 294, 15259, 13, 51296], "temperature": 0.0, "avg_logprob": -0.2848944389957121, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.033550016582012177}, {"id": 293, "seek": 204314, "start": 2061.78, "end": 2068.1800000000003, "text": " And then then what once the old async work is stabilized, I think they will come up, they will", "tokens": [51296, 400, 550, 550, 437, 1564, 264, 1331, 382, 34015, 589, 307, 48384, 11, 286, 519, 436, 486, 808, 493, 11, 436, 486, 51616], "temperature": 0.0, "avg_logprob": -0.2848944389957121, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.033550016582012177}, {"id": 294, "seek": 206818, "start": 2068.18, "end": 2073.7, "text": " come up with a new version of the trade. Like with those new those new constructs that are being", "tokens": [50364, 808, 493, 365, 257, 777, 3037, 295, 264, 4923, 13, 1743, 365, 729, 777, 729, 777, 7690, 82, 300, 366, 885, 50640], "temperature": 0.0, "avg_logprob": -0.19229113537332285, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.017682749778032303}, {"id": 295, "seek": 206818, "start": 2073.7, "end": 2078.5, "text": " released and will be released in the next version of rust. We can greatly simplify the service", "tokens": [50640, 4736, 293, 486, 312, 4736, 294, 264, 958, 3037, 295, 15259, 13, 492, 393, 14147, 20460, 264, 2643, 50880], "temperature": 0.0, "avg_logprob": -0.19229113537332285, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.017682749778032303}, {"id": 296, "seek": 206818, "start": 2078.5, "end": 2085.06, "text": " trade. So they will wait for this feature to land, revise the trade and then release 1.0.", "tokens": [50880, 4923, 13, 407, 436, 486, 1699, 337, 341, 4111, 281, 2117, 11, 44252, 264, 4923, 293, 550, 4374, 502, 13, 15, 13, 51208], "temperature": 0.0, "avg_logprob": -0.19229113537332285, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.017682749778032303}, {"id": 297, "seek": 206818, "start": 2086.66, "end": 2093.14, "text": " And the second question is there a way to make this tower layers, let's say optional so you can", "tokens": [51288, 400, 264, 1150, 1168, 307, 456, 257, 636, 281, 652, 341, 10567, 7914, 11, 718, 311, 584, 17312, 370, 291, 393, 51612], "temperature": 0.0, "avg_logprob": -0.19229113537332285, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.017682749778032303}, {"id": 298, "seek": 209314, "start": 2093.14, "end": 2100.1, "text": " enable disable them from outside the code from the compiler somehow. Yeah, I mean you you have", "tokens": [50364, 9528, 28362, 552, 490, 2380, 264, 3089, 490, 264, 31958, 6063, 13, 865, 11, 286, 914, 291, 291, 362, 50712], "temperature": 0.0, "avg_logprob": -0.23372812968928638, "compression_ratio": 1.8833333333333333, "no_speech_prob": 0.02921467460691929}, {"id": 299, "seek": 209314, "start": 2101.22, "end": 2105.3799999999997, "text": " you could use there would be different ways you could use like feature flags,", "tokens": [50768, 291, 727, 764, 456, 576, 312, 819, 2098, 291, 727, 764, 411, 4111, 23265, 11, 50976], "temperature": 0.0, "avg_logprob": -0.23372812968928638, "compression_ratio": 1.8833333333333333, "no_speech_prob": 0.02921467460691929}, {"id": 300, "seek": 209314, "start": 2105.3799999999997, "end": 2111.54, "text": " you could use environment viable like they services have a stake. So and you the", "tokens": [50976, 291, 727, 764, 2823, 22024, 411, 436, 3328, 362, 257, 10407, 13, 407, 293, 291, 264, 51284], "temperature": 0.0, "avg_logprob": -0.23372812968928638, "compression_ratio": 1.8833333333333333, "no_speech_prob": 0.02921467460691929}, {"id": 301, "seek": 209314, "start": 2112.5, "end": 2116.3399999999997, "text": " they mutable so you can do you can do what you can do you can do a lot you could have", "tokens": [51332, 436, 5839, 712, 370, 291, 393, 360, 291, 393, 360, 437, 291, 393, 360, 291, 393, 360, 257, 688, 291, 727, 362, 51524], "temperature": 0.0, "avg_logprob": -0.23372812968928638, "compression_ratio": 1.8833333333333333, "no_speech_prob": 0.02921467460691929}, {"id": 302, "seek": 211634, "start": 2117.3, "end": 2122.26, "text": " you should I to make bullying somewhere in there and you can enable disable it.", "tokens": [50412, 291, 820, 286, 281, 652, 25633, 4079, 294, 456, 293, 291, 393, 9528, 28362, 309, 13, 50660], "temperature": 0.0, "avg_logprob": -0.7525051879882813, "compression_ratio": 1.0975609756097562, "no_speech_prob": 0.02782534435391426}, {"id": 303, "seek": 211634, "start": 2124.98, "end": 2125.46, "text": " Thank you.", "tokens": [50796, 1044, 291, 13, 50820], "temperature": 0.0, "avg_logprob": -0.7525051879882813, "compression_ratio": 1.0975609756097562, "no_speech_prob": 0.02782534435391426}, {"id": 304, "seek": 212546, "start": 2125.46, "end": 2131.46, "text": " Yeah.", "tokens": [50364, 865, 13, 50664], "temperature": 0.0, "avg_logprob": -0.4647673812567019, "compression_ratio": 1.3781512605042017, "no_speech_prob": 0.010097825899720192}, {"id": 305, "seek": 212546, "start": 2141.54, "end": 2142.98, "text": " I'm on the time of future slide.", "tokens": [51168, 286, 478, 322, 264, 565, 295, 2027, 4137, 13, 51240], "temperature": 0.0, "avg_logprob": -0.4647673812567019, "compression_ratio": 1.3781512605042017, "no_speech_prob": 0.010097825899720192}, {"id": 306, "seek": 212546, "start": 2144.18, "end": 2148.7400000000002, "text": " What time of future this one.", "tokens": [51300, 708, 565, 295, 2027, 341, 472, 13, 51528], "temperature": 0.0, "avg_logprob": -0.4647673812567019, "compression_ratio": 1.3781512605042017, "no_speech_prob": 0.010097825899720192}, {"id": 307, "seek": 212546, "start": 2149.3, "end": 2154.34, "text": " Yes, there's a poll function and if none of them is ready, it will just return and how would it", "tokens": [51556, 1079, 11, 456, 311, 257, 6418, 2445, 293, 498, 6022, 295, 552, 307, 1919, 11, 309, 486, 445, 2736, 293, 577, 576, 309, 51808], "temperature": 0.0, "avg_logprob": -0.4647673812567019, "compression_ratio": 1.3781512605042017, "no_speech_prob": 0.010097825899720192}, {"id": 308, "seek": 215434, "start": 2154.34, "end": 2155.54, "text": " know when to call it again.", "tokens": [50364, 458, 562, 281, 818, 309, 797, 13, 50424], "temperature": 0.0, "avg_logprob": -0.24545397077287948, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.004816221073269844}, {"id": 309, "seek": 215434, "start": 2159.54, "end": 2161.54, "text": " Like you pulled once and none of them is ready yet.", "tokens": [50624, 1743, 291, 7373, 1564, 293, 6022, 295, 552, 307, 1919, 1939, 13, 50724], "temperature": 0.0, "avg_logprob": -0.24545397077287948, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.004816221073269844}, {"id": 310, "seek": 215434, "start": 2162.1800000000003, "end": 2164.1800000000003, "text": " It's the runtime is going to take care of it.", "tokens": [50756, 467, 311, 264, 34474, 307, 516, 281, 747, 1127, 295, 309, 13, 50856], "temperature": 0.0, "avg_logprob": -0.24545397077287948, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.004816221073269844}, {"id": 311, "seek": 215434, "start": 2166.9, "end": 2169.46, "text": " With it's well it depends on the runtime you're using it's", "tokens": [50992, 2022, 309, 311, 731, 309, 5946, 322, 264, 34474, 291, 434, 1228, 309, 311, 51120], "temperature": 0.0, "avg_logprob": -0.24545397077287948, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.004816221073269844}, {"id": 312, "seek": 215434, "start": 2171.06, "end": 2175.7000000000003, "text": " it knows it's not ready and then I mean then we're talking about how you implement the runtime", "tokens": [51200, 309, 3255, 309, 311, 406, 1919, 293, 550, 286, 914, 550, 321, 434, 1417, 466, 577, 291, 4445, 264, 34474, 51432], "temperature": 0.0, "avg_logprob": -0.24545397077287948, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.004816221073269844}, {"id": 313, "seek": 217570, "start": 2175.7, "end": 2182.66, "text": " basically. It maintains like a queue of features and it regularly pulls them using some", "tokens": [50364, 1936, 13, 467, 33385, 411, 257, 18639, 295, 4122, 293, 309, 11672, 16982, 552, 1228, 512, 50712], "temperature": 0.0, "avg_logprob": -0.22833634961035945, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.15164214372634888}, {"id": 314, "seek": 217570, "start": 2183.7, "end": 2189.46, "text": " a waker which is it's using that context in the waker when it's ready it's going to call the waker", "tokens": [50764, 257, 261, 4003, 597, 307, 309, 311, 1228, 300, 4319, 294, 264, 261, 4003, 562, 309, 311, 1919, 309, 311, 516, 281, 818, 264, 261, 4003, 51052], "temperature": 0.0, "avg_logprob": -0.22833634961035945, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.15164214372634888}, {"id": 315, "seek": 217570, "start": 2189.46, "end": 2192.2599999999998, "text": " that's going to wake the the runtime so and so forth.", "tokens": [51052, 300, 311, 516, 281, 6634, 264, 264, 34474, 370, 293, 370, 5220, 13, 51192], "temperature": 0.0, "avg_logprob": -0.22833634961035945, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.15164214372634888}, {"id": 316, "seek": 217570, "start": 2192.98, "end": 2197.2999999999997, "text": " But you as an implementer of the future you're not in charge of like telling when you're going to", "tokens": [51228, 583, 291, 382, 364, 4445, 260, 295, 264, 2027, 291, 434, 406, 294, 4602, 295, 411, 3585, 562, 291, 434, 516, 281, 51444], "temperature": 0.0, "avg_logprob": -0.22833634961035945, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.15164214372634888}, {"id": 317, "seek": 217570, "start": 2197.2999999999997, "end": 2200.18, "text": " you should be pulled again. You're just in charge of saying I'm ready or I'm not.", "tokens": [51444, 291, 820, 312, 7373, 797, 13, 509, 434, 445, 294, 4602, 295, 1566, 286, 478, 1919, 420, 286, 478, 406, 13, 51588], "temperature": 0.0, "avg_logprob": -0.22833634961035945, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.15164214372634888}, {"id": 318, "seek": 217570, "start": 2200.8199999999997, "end": 2205.22, "text": " That whole machinery is left to whoever implements our AC runtime.", "tokens": [51620, 663, 1379, 27302, 307, 1411, 281, 11387, 704, 17988, 527, 8157, 34474, 13, 51840], "temperature": 0.0, "avg_logprob": -0.22833634961035945, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.15164214372634888}, {"id": 319, "seek": 220570, "start": 2205.9399999999996, "end": 2206.5, "text": " Okay thank you.", "tokens": [50376, 1033, 1309, 291, 13, 50404], "temperature": 0.0, "avg_logprob": -0.16978369653224945, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.004387773107737303}, {"id": 320, "seek": 220570, "start": 2214.1, "end": 2218.4199999999996, "text": " Thanks for your talk. I had a question because you mentioned that there was some overlap between", "tokens": [50784, 2561, 337, 428, 751, 13, 286, 632, 257, 1168, 570, 291, 2835, 300, 456, 390, 512, 19959, 1296, 51000], "temperature": 0.0, "avg_logprob": -0.16978369653224945, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.004387773107737303}, {"id": 321, "seek": 220570, "start": 2218.98, "end": 2224.02, "text": " implementing your own future and the tower service trade itself proposing like a poll", "tokens": [51028, 18114, 428, 1065, 2027, 293, 264, 10567, 2643, 4923, 2564, 29939, 411, 257, 6418, 51280], "temperature": 0.0, "avg_logprob": -0.16978369653224945, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.004387773107737303}, {"id": 322, "seek": 220570, "start": 2224.02, "end": 2230.8999999999996, "text": " function and a poll rate function. Is there any reason apart from fun to implement a future", "tokens": [51280, 2445, 293, 257, 6418, 3314, 2445, 13, 1119, 456, 604, 1778, 4936, 490, 1019, 281, 4445, 257, 2027, 51624], "temperature": 0.0, "avg_logprob": -0.16978369653224945, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.004387773107737303}, {"id": 323, "seek": 223090, "start": 2230.9, "end": 2238.58, "text": " trip yourself like is there any benefit from dealing with the internals of creating a future", "tokens": [50364, 4931, 1803, 411, 307, 456, 604, 5121, 490, 6260, 365, 264, 2154, 1124, 295, 4084, 257, 2027, 50748], "temperature": 0.0, "avg_logprob": -0.19744507270523265, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.015863636508584023}, {"id": 324, "seek": 223090, "start": 2238.58, "end": 2244.58, "text": " versus implementing your whole logic inside of the service trade itself and just using the", "tokens": [50748, 5717, 18114, 428, 1379, 9952, 1854, 295, 264, 2643, 4923, 2564, 293, 445, 1228, 264, 51048], "temperature": 0.0, "avg_logprob": -0.19744507270523265, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.015863636508584023}, {"id": 325, "seek": 223090, "start": 2245.78, "end": 2252.6600000000003, "text": " basic tower or basic futures future types. You mean a box of doc futures? Yeah exactly.", "tokens": [51108, 3875, 10567, 420, 3875, 26071, 2027, 3467, 13, 509, 914, 257, 2424, 295, 3211, 26071, 30, 865, 2293, 13, 51452], "temperature": 0.0, "avg_logprob": -0.19744507270523265, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.015863636508584023}, {"id": 326, "seek": 223090, "start": 2252.6600000000003, "end": 2256.1800000000003, "text": " Your question is like why would I bother writing my own future when I can use box?", "tokens": [51452, 2260, 1168, 307, 411, 983, 576, 286, 8677, 3579, 452, 1065, 2027, 562, 286, 393, 764, 2424, 30, 51628], "temperature": 0.0, "avg_logprob": -0.19744507270523265, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.015863636508584023}, {"id": 327, "seek": 225618, "start": 2256.8199999999997, "end": 2261.2999999999997, "text": " Yeah when I have this poll function inside of a service trade itself where I can", "tokens": [50396, 865, 562, 286, 362, 341, 6418, 2445, 1854, 295, 257, 2643, 4923, 2564, 689, 286, 393, 50620], "temperature": 0.0, "avg_logprob": -0.15428571079088294, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.002973322756588459}, {"id": 328, "seek": 225618, "start": 2261.2999999999997, "end": 2266.02, "text": " bake my own logic and not have to deal with you know projecting my pins and so on and so forth.", "tokens": [50620, 16562, 452, 1065, 9952, 293, 406, 362, 281, 2028, 365, 291, 458, 43001, 452, 16392, 293, 370, 322, 293, 370, 5220, 13, 50856], "temperature": 0.0, "avg_logprob": -0.15428571079088294, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.002973322756588459}, {"id": 329, "seek": 225618, "start": 2268.8999999999996, "end": 2274.98, "text": " I'm not sure I totally understand your question so my answer might not satisfy you. I think you're", "tokens": [51000, 286, 478, 406, 988, 286, 3879, 1223, 428, 1168, 370, 452, 1867, 1062, 406, 19319, 291, 13, 286, 519, 291, 434, 51304], "temperature": 0.0, "avg_logprob": -0.15428571079088294, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.002973322756588459}, {"id": 330, "seek": 225618, "start": 2274.98, "end": 2280.4199999999996, "text": " right you want a future if you want you're writing a library and you don't want you don't want your", "tokens": [51304, 558, 291, 528, 257, 2027, 498, 291, 528, 291, 434, 3579, 257, 6405, 293, 291, 500, 380, 528, 291, 500, 380, 528, 428, 51576], "temperature": 0.0, "avg_logprob": -0.15428571079088294, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.002973322756588459}, {"id": 331, "seek": 228042, "start": 2280.42, "end": 2288.1, "text": " users to incur the overhead of like the box future. You write it if you have like some constraint", "tokens": [50364, 5022, 281, 35774, 264, 19922, 295, 411, 264, 2424, 2027, 13, 509, 2464, 309, 498, 291, 362, 411, 512, 25534, 50748], "temperature": 0.0, "avg_logprob": -0.1967136756233547, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.006485783029347658}, {"id": 332, "seek": 228042, "start": 2288.1, "end": 2295.7000000000003, "text": " that might be related to performance that's going to push you to write a hand rolled future that", "tokens": [50748, 300, 1062, 312, 4077, 281, 3389, 300, 311, 516, 281, 2944, 291, 281, 2464, 257, 1011, 14306, 2027, 300, 51128], "temperature": 0.0, "avg_logprob": -0.1967136756233547, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.006485783029347658}, {"id": 333, "seek": 228042, "start": 2295.7000000000003, "end": 2305.14, "text": " doesn't allocate it doesn't have dynamic dispatch. Yeah that would be my answer. I think it also", "tokens": [51128, 1177, 380, 35713, 309, 1177, 380, 362, 8546, 36729, 13, 865, 300, 576, 312, 452, 1867, 13, 286, 519, 309, 611, 51600], "temperature": 0.0, "avg_logprob": -0.1967136756233547, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.006485783029347658}, {"id": 334, "seek": 228042, "start": 2305.14, "end": 2309.86, "text": " depends on your team if like if you're the only guy in the team that's going to understand like oh", "tokens": [51600, 5946, 322, 428, 1469, 498, 411, 498, 291, 434, 264, 787, 2146, 294, 264, 1469, 300, 311, 516, 281, 1223, 411, 1954, 51836], "temperature": 0.0, "avg_logprob": -0.1967136756233547, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.006485783029347658}, {"id": 335, "seek": 230986, "start": 2309.94, "end": 2314.42, "text": " I pin the I project the future and are you spinning if just you maybe maybe you don't do it.", "tokens": [50368, 286, 5447, 264, 286, 1716, 264, 2027, 293, 366, 291, 15640, 498, 445, 291, 1310, 1310, 291, 500, 380, 360, 309, 13, 50592], "temperature": 0.0, "avg_logprob": -0.3239118055863814, "compression_ratio": 1.4172185430463575, "no_speech_prob": 0.0025770370848476887}, {"id": 336, "seek": 230986, "start": 2315.86, "end": 2318.26, "text": " It depends on those constraints. Okay thanks.", "tokens": [50664, 467, 5946, 322, 729, 18491, 13, 1033, 3231, 13, 50784], "temperature": 0.0, "avg_logprob": -0.3239118055863814, "compression_ratio": 1.4172185430463575, "no_speech_prob": 0.0025770370848476887}, {"id": 337, "seek": 230986, "start": 2326.1800000000003, "end": 2330.9, "text": " Awesome so if there's no more questions let's thank our speaker. Thank you.", "tokens": [51180, 10391, 370, 498, 456, 311, 572, 544, 1651, 718, 311, 1309, 527, 8145, 13, 1044, 291, 13, 51416], "temperature": 0.0, "avg_logprob": -0.3239118055863814, "compression_ratio": 1.4172185430463575, "no_speech_prob": 0.0025770370848476887}], "language": "en"}