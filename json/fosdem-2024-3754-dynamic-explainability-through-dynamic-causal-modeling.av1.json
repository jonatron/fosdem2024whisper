{"text": " Hi everyone. I like being early so we're going to start a little bit early. First of all, who's enjoying the room? Yes? Yeah? Yeah, make some noise. Come on. There we go. There we go. It's all Will's work by the way, so thank him after this fact. All right, I have the pleasure of introducing Will again today. He started our event office this morning and he's continuing here helping us to do it today. He recently took up bouldering and fell two meters his first time of doing it and still keeps going on. I have to admit that is impressive. Take it away Will. Thank you. Yes, so I'm quickly plugging this talk because one of our speakers dropped out. So if I'm going anywhere nearer over time, just let me know. Just cut me off. It's fine. And what I want to talk about is a little bit of work I did while I was in neuroscience researcher. I think we had one of those earlier. Anyone else neuroscience? Nope, not a single one. Never mind. Well, it doesn't matter. It was there was some interesting work I did while I was a neuroscientist that looked at how to reverse engineer the causality between different brain regions. So the idea is we go out, we take FMRI of somebody, we take EEG, we get a signal of how their brain activity evolves over time. And what we want to understand is how these different brain regions are interacting with each other to produce this activity, which is a useful thing to do in research. It's useful in medical applications as well. And what we're actually doing here at its core is something I think it's really interesting and it's really powerful. We're simultaneously estimating a causal system that has engineered our behavior and we're parameterizing it at the same time with the values that made that happen. And that's actually a really difficult and challenging problem to do. And it's a bit of a shame that the technology sort of confined to academia at the moment. So this project I did, what I'll talk about today, is the work I did to make this more free and open source and not that the old code was bad, but it was research code to make it better and more commercially viable. So let's talk a little bit. We'll back off a little bit now and talk a little bit about what this technology does in a bit more detail and we'll talk about then what I did with it. So what we do with this technology, it's a Bayesian technology for anyone who knows what that means, but that means that we start with an estimate of everything we do know about the problem or at least an estimate of all the things that we don't know. We take this, we take our data and then with a dynamic causal modeling framework, we infer simultaneously a system that describes this behavior and also, as I said, the values of the parameters that parameterize the system. And we do this because it's Bayesian in an uncertainty aware way. So every parameter, everything we estimate comes with a, it comes with a probability distribution. It comes with an error estimate of how wrong we could be about this. So let's walk through a simple example. So the way this systems inference works is we infer what is called a dynamical system. That is what physicists use to describe how, I mean, basically everything works. And this consists of two things. It consists of a state that is in our planet's example, that could be the position mass of velocity of each planet. And it consists of a rule for how that state evolves over time, which would in this case be the laws of physics. Things like this, I was going off the slides a bit, but GM1M2 forces that two bodies with mass are applying to each other. And this is a dynamical system and it's what we're trying to infer from our data. And some of our viewers may notice the one I'm describing here, in fact, even on the slides is a differential equation. And for those guys, I would point out that dynamical systems are known for basically, basically not having any good solutions, which is partially a neutral framework. And in the dynamical, dynamic course of modelling world, we would be doing something like taking the position mass and velocity of planet A and saying, okay, I know there's probably some other planetary bodies affecting this, but I don't know how many, how many planetary bodies are affecting this planet that I can observe? And what is the position mass and velocities of each of these? And how uncertain am I about that? That's an interesting thing to be able to do. A real world application of this was in the COVID-19 pandemic. Here we have two very, very noisy things, hence the need for uncertainty that we understand about the pandemic. That is the number of positive COVID tests and the number of deaths. And we have a broad understanding of how the pandemic works. It's or at least broad understanding of ways we can model it, something like a CER model, which is what we opted for here. Well, I didn't originate this work, but I did make it work at the end. A CER model, which is susceptible, exposed, infected, recovered. And so we can take these two things. We can take the observed deaths and positive COVID tests and our understanding, and we can project backwards to a dynamical system that explains this. And there's three reasons this is a sort of particularly useful thing to be able to do. The first one is because we're understanding our data from partially, at least, or as much as we want to, we can tip in the things that we understand about the model beforehand. We can include things like hospital admissions and ventilator usage. And because we can include these parameters in our model, that means we can estimate them and it means we can look at them afterwards and have good guesses at them. And then we can do things like we did here, where we look at the number of deaths from COVID-19 over time. And we can estimate these parameters and fairly accurately, if you can see these graphs, because we knew they were there, they've been included, we can estimate them. That's a powerful thing to be able to do. Similarly, because we've included them out in model, and because of the way these things work, because it is a dynamical system with different parts, and we now have it all interact with each other, we can do things like look at counterfactuals. So we can say, what if I only have 200 hospital beds this week? What happens then? What happens? How bad do things become? And again, that's a really, really nice thing to be able to do. Finally, because this is a Bayesian method, because it is uncertainty aware, we don't just get single point estimates out of this method. We get probability distributions. So we can say things instead of like, I need 200 hospital beds this week. We can say, the model says, I'm 95% sure that I need between 200 and 400, or I'm 90% sure I need at least 300. And that's much, much, much more valuable than if you're planning things. That's what you need to know. You don't need to guess at what's going to happen. You need to know what might happen and how likely that is. In terms of the project we did about this, my work about this, it was really about making it more open source. And I say more because the original code base was open source, but it was written in MATLAB. And MATLAB is not really free and open source. And I will give credit to the original researchers. They did modify things so that it could mostly be run in Octave. But the unfortunate fact of the matter is, Octave is a wonderful thing. It really is. I can't give enough credit to developers, but it is not a replacement for MATLAB. Complicated MATLAB code, particularly MATLAB code, but dependencies will just break and then it's annoying to fix. And it's at least 10 times slower. Depending on what you're doing, it could be much, much, much slower than that. So having your code in MATLAB, even if it's open sourced, it's not really open sourced. So we rewrote the project in C++, which I know some people will disagree is an improvement, but it certainly goes a lot quicker. And yeah, as we did this, we have tried really hard to make this code more generic as well, because the original code was research code. It was designed for just two problems, neuroscience stuff and a little bit of COVID stuff. Now the code works generally. The other thing that we have been trying to do with this is to try and make it more open in other ways. Again, a problem with this is its research work. And so although there are papers describing how all of this works, I'm not going to lie, I literally did a PhD in this, they're incomprehensible. It's really, really, really difficult to get to the bottom of the papers, even if you really understand the subject material. If you don't understand the subject material, it's just awful. And the thing is, when you've gone through it, and you do understand all of this, it's not that hard. It's interesting and useful and creative, but it's difficult, it is not. And it doesn't need to be, so that's the other part of the work we have been doing with this. To try and make it free and open source by making it open about exactly what's happening and why. Anyway, that is the end of my talk. That is my GitHub repo. If anybody wants to look at it, do we have any questions? William, you've written it in C++. A lot of AI people expect to use Python. What's your solution for them? I have been writing a Python front-end for it. Is the solution. It's a little bit of work in progress, but you can draw from this in Python if you want to. Any other questions? You're going to make me run? No? All right, everyone give another round of applause for Will.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.4, "text": " Hi everyone. I like being early so we're going to start a little bit early. First of all,", "tokens": [50364, 2421, 1518, 13, 286, 411, 885, 2440, 370, 321, 434, 516, 281, 722, 257, 707, 857, 2440, 13, 2386, 295, 439, 11, 50984], "temperature": 0.0, "avg_logprob": -0.30732145188729976, "compression_ratio": 1.4585635359116023, "no_speech_prob": 0.36517593264579773}, {"id": 1, "seek": 0, "start": 12.4, "end": 19.44, "text": " who's enjoying the room? Yes? Yeah? Yeah, make some noise. Come on. There we go. There", "tokens": [50984, 567, 311, 9929, 264, 1808, 30, 1079, 30, 865, 30, 865, 11, 652, 512, 5658, 13, 2492, 322, 13, 821, 321, 352, 13, 821, 51336], "temperature": 0.0, "avg_logprob": -0.30732145188729976, "compression_ratio": 1.4585635359116023, "no_speech_prob": 0.36517593264579773}, {"id": 2, "seek": 0, "start": 19.44, "end": 26.52, "text": " we go. It's all Will's work by the way, so thank him after this fact. All right, I have", "tokens": [51336, 321, 352, 13, 467, 311, 439, 3099, 311, 589, 538, 264, 636, 11, 370, 1309, 796, 934, 341, 1186, 13, 1057, 558, 11, 286, 362, 51690], "temperature": 0.0, "avg_logprob": -0.30732145188729976, "compression_ratio": 1.4585635359116023, "no_speech_prob": 0.36517593264579773}, {"id": 3, "seek": 2652, "start": 26.52, "end": 32.48, "text": " the pleasure of introducing Will again today. He started our event office this morning and", "tokens": [50364, 264, 6834, 295, 15424, 3099, 797, 965, 13, 634, 1409, 527, 2280, 3398, 341, 2446, 293, 50662], "temperature": 0.0, "avg_logprob": -0.1624872737460666, "compression_ratio": 1.5625, "no_speech_prob": 0.08844183385372162}, {"id": 4, "seek": 2652, "start": 32.48, "end": 38.56, "text": " he's continuing here helping us to do it today. He recently took up bouldering and fell two meters", "tokens": [50662, 415, 311, 9289, 510, 4315, 505, 281, 360, 309, 965, 13, 634, 3938, 1890, 493, 272, 429, 1794, 293, 5696, 732, 8146, 50966], "temperature": 0.0, "avg_logprob": -0.1624872737460666, "compression_ratio": 1.5625, "no_speech_prob": 0.08844183385372162}, {"id": 5, "seek": 2652, "start": 38.56, "end": 44.519999999999996, "text": " his first time of doing it and still keeps going on. I have to admit that is impressive. Take it", "tokens": [50966, 702, 700, 565, 295, 884, 309, 293, 920, 5965, 516, 322, 13, 286, 362, 281, 9796, 300, 307, 8992, 13, 3664, 309, 51264], "temperature": 0.0, "avg_logprob": -0.1624872737460666, "compression_ratio": 1.5625, "no_speech_prob": 0.08844183385372162}, {"id": 6, "seek": 2652, "start": 44.519999999999996, "end": 55.0, "text": " away Will. Thank you. Yes, so I'm quickly plugging this talk because one of our speakers", "tokens": [51264, 1314, 3099, 13, 1044, 291, 13, 1079, 11, 370, 286, 478, 2661, 42975, 341, 751, 570, 472, 295, 527, 9518, 51788], "temperature": 0.0, "avg_logprob": -0.1624872737460666, "compression_ratio": 1.5625, "no_speech_prob": 0.08844183385372162}, {"id": 7, "seek": 5500, "start": 55.0, "end": 60.24, "text": " dropped out. So if I'm going anywhere nearer over time, just let me know. Just cut me off. It's", "tokens": [50364, 8119, 484, 13, 407, 498, 286, 478, 516, 4992, 2651, 260, 670, 565, 11, 445, 718, 385, 458, 13, 1449, 1723, 385, 766, 13, 467, 311, 50626], "temperature": 0.0, "avg_logprob": -0.19110975913631106, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.10387004911899567}, {"id": 8, "seek": 5500, "start": 60.24, "end": 67.48, "text": " fine. And what I want to talk about is a little bit of work I did while I was in neuroscience", "tokens": [50626, 2489, 13, 400, 437, 286, 528, 281, 751, 466, 307, 257, 707, 857, 295, 589, 286, 630, 1339, 286, 390, 294, 42762, 50988], "temperature": 0.0, "avg_logprob": -0.19110975913631106, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.10387004911899567}, {"id": 9, "seek": 5500, "start": 67.48, "end": 74.2, "text": " researcher. I think we had one of those earlier. Anyone else neuroscience? Nope, not a single one.", "tokens": [50988, 21751, 13, 286, 519, 321, 632, 472, 295, 729, 3071, 13, 14643, 1646, 42762, 30, 12172, 11, 406, 257, 2167, 472, 13, 51324], "temperature": 0.0, "avg_logprob": -0.19110975913631106, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.10387004911899567}, {"id": 10, "seek": 5500, "start": 74.2, "end": 80.32, "text": " Never mind. Well, it doesn't matter. It was there was some interesting work I did while I was a", "tokens": [51324, 7344, 1575, 13, 1042, 11, 309, 1177, 380, 1871, 13, 467, 390, 456, 390, 512, 1880, 589, 286, 630, 1339, 286, 390, 257, 51630], "temperature": 0.0, "avg_logprob": -0.19110975913631106, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.10387004911899567}, {"id": 11, "seek": 8032, "start": 80.36, "end": 86.44, "text": " neuroscientist that looked at how to reverse engineer the causality between different brain", "tokens": [50366, 28813, 5412, 468, 300, 2956, 412, 577, 281, 9943, 11403, 264, 3302, 1860, 1296, 819, 3567, 50670], "temperature": 0.0, "avg_logprob": -0.1501520655371926, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.15372279286384583}, {"id": 12, "seek": 8032, "start": 86.44, "end": 94.16, "text": " regions. So the idea is we go out, we take FMRI of somebody, we take EEG, we get a signal of how", "tokens": [50670, 10682, 13, 407, 264, 1558, 307, 321, 352, 484, 11, 321, 747, 29614, 5577, 295, 2618, 11, 321, 747, 33685, 38, 11, 321, 483, 257, 6358, 295, 577, 51056], "temperature": 0.0, "avg_logprob": -0.1501520655371926, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.15372279286384583}, {"id": 13, "seek": 8032, "start": 94.16, "end": 100.35999999999999, "text": " their brain activity evolves over time. And what we want to understand is how these different", "tokens": [51056, 641, 3567, 5191, 43737, 670, 565, 13, 400, 437, 321, 528, 281, 1223, 307, 577, 613, 819, 51366], "temperature": 0.0, "avg_logprob": -0.1501520655371926, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.15372279286384583}, {"id": 14, "seek": 8032, "start": 100.35999999999999, "end": 105.6, "text": " brain regions are interacting with each other to produce this activity, which is a useful thing", "tokens": [51366, 3567, 10682, 366, 18017, 365, 1184, 661, 281, 5258, 341, 5191, 11, 597, 307, 257, 4420, 551, 51628], "temperature": 0.0, "avg_logprob": -0.1501520655371926, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.15372279286384583}, {"id": 15, "seek": 10560, "start": 105.72, "end": 113.16, "text": " to do in research. It's useful in medical applications as well. And what we're actually doing", "tokens": [50370, 281, 360, 294, 2132, 13, 467, 311, 4420, 294, 4625, 5821, 382, 731, 13, 400, 437, 321, 434, 767, 884, 50742], "temperature": 0.0, "avg_logprob": -0.22274238949730282, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.048716962337493896}, {"id": 16, "seek": 10560, "start": 113.16, "end": 118.08, "text": " here at its core is something I think it's really interesting and it's really powerful. We're", "tokens": [50742, 510, 412, 1080, 4965, 307, 746, 286, 519, 309, 311, 534, 1880, 293, 309, 311, 534, 4005, 13, 492, 434, 50988], "temperature": 0.0, "avg_logprob": -0.22274238949730282, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.048716962337493896}, {"id": 17, "seek": 10560, "start": 118.08, "end": 125.75999999999999, "text": " simultaneously estimating a causal system that has engineered our behavior and we're", "tokens": [50988, 16561, 8017, 990, 257, 38755, 1185, 300, 575, 38648, 527, 5223, 293, 321, 434, 51372], "temperature": 0.0, "avg_logprob": -0.22274238949730282, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.048716962337493896}, {"id": 18, "seek": 10560, "start": 125.75999999999999, "end": 129.2, "text": " parameterizing it at the same time with the values that made that happen. And that's actually a", "tokens": [51372, 13075, 3319, 309, 412, 264, 912, 565, 365, 264, 4190, 300, 1027, 300, 1051, 13, 400, 300, 311, 767, 257, 51544], "temperature": 0.0, "avg_logprob": -0.22274238949730282, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.048716962337493896}, {"id": 19, "seek": 10560, "start": 129.2, "end": 133.44, "text": " really difficult and challenging problem to do. And it's a bit of a shame that the technology", "tokens": [51544, 534, 2252, 293, 7595, 1154, 281, 360, 13, 400, 309, 311, 257, 857, 295, 257, 10069, 300, 264, 2899, 51756], "temperature": 0.0, "avg_logprob": -0.22274238949730282, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.048716962337493896}, {"id": 20, "seek": 13344, "start": 133.52, "end": 138.72, "text": " sort of confined to academia at the moment. So this project I did, what I'll talk about today,", "tokens": [50368, 1333, 295, 31745, 281, 28937, 412, 264, 1623, 13, 407, 341, 1716, 286, 630, 11, 437, 286, 603, 751, 466, 965, 11, 50628], "temperature": 0.0, "avg_logprob": -0.1493300994237264, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.015201255679130554}, {"id": 21, "seek": 13344, "start": 138.72, "end": 145.28, "text": " is the work I did to make this more free and open source and not that the old code was bad,", "tokens": [50628, 307, 264, 589, 286, 630, 281, 652, 341, 544, 1737, 293, 1269, 4009, 293, 406, 300, 264, 1331, 3089, 390, 1578, 11, 50956], "temperature": 0.0, "avg_logprob": -0.1493300994237264, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.015201255679130554}, {"id": 22, "seek": 13344, "start": 145.28, "end": 151.72, "text": " but it was research code to make it better and more commercially viable. So let's talk a little", "tokens": [50956, 457, 309, 390, 2132, 3089, 281, 652, 309, 1101, 293, 544, 41751, 22024, 13, 407, 718, 311, 751, 257, 707, 51278], "temperature": 0.0, "avg_logprob": -0.1493300994237264, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.015201255679130554}, {"id": 23, "seek": 13344, "start": 151.72, "end": 156.84, "text": " bit. We'll back off a little bit now and talk a little bit about what this technology does in a", "tokens": [51278, 857, 13, 492, 603, 646, 766, 257, 707, 857, 586, 293, 751, 257, 707, 857, 466, 437, 341, 2899, 775, 294, 257, 51534], "temperature": 0.0, "avg_logprob": -0.1493300994237264, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.015201255679130554}, {"id": 24, "seek": 15684, "start": 156.92000000000002, "end": 165.4, "text": " bit more detail and we'll talk about then what I did with it. So what we do with this technology,", "tokens": [50368, 857, 544, 2607, 293, 321, 603, 751, 466, 550, 437, 286, 630, 365, 309, 13, 407, 437, 321, 360, 365, 341, 2899, 11, 50792], "temperature": 0.0, "avg_logprob": -0.18894439935684204, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.01613990031182766}, {"id": 25, "seek": 15684, "start": 165.4, "end": 170.56, "text": " it's a Bayesian technology for anyone who knows what that means, but that means that we start with", "tokens": [50792, 309, 311, 257, 7840, 42434, 2899, 337, 2878, 567, 3255, 437, 300, 1355, 11, 457, 300, 1355, 300, 321, 722, 365, 51050], "temperature": 0.0, "avg_logprob": -0.18894439935684204, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.01613990031182766}, {"id": 26, "seek": 15684, "start": 170.56, "end": 176.4, "text": " an estimate of everything we do know about the problem or at least an estimate of all the things", "tokens": [51050, 364, 12539, 295, 1203, 321, 360, 458, 466, 264, 1154, 420, 412, 1935, 364, 12539, 295, 439, 264, 721, 51342], "temperature": 0.0, "avg_logprob": -0.18894439935684204, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.01613990031182766}, {"id": 27, "seek": 15684, "start": 176.4, "end": 182.84, "text": " that we don't know. We take this, we take our data and then with a dynamic causal modeling framework,", "tokens": [51342, 300, 321, 500, 380, 458, 13, 492, 747, 341, 11, 321, 747, 527, 1412, 293, 550, 365, 257, 8546, 38755, 15983, 8388, 11, 51664], "temperature": 0.0, "avg_logprob": -0.18894439935684204, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.01613990031182766}, {"id": 28, "seek": 18284, "start": 183.24, "end": 190.04, "text": " we infer simultaneously a system that describes this behavior and also, as I said, the values of", "tokens": [50384, 321, 13596, 16561, 257, 1185, 300, 15626, 341, 5223, 293, 611, 11, 382, 286, 848, 11, 264, 4190, 295, 50724], "temperature": 0.0, "avg_logprob": -0.12575329555554338, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.022031573578715324}, {"id": 29, "seek": 18284, "start": 190.04, "end": 195.4, "text": " the parameters that parameterize the system. And we do this because it's Bayesian in an uncertainty", "tokens": [50724, 264, 9834, 300, 13075, 1125, 264, 1185, 13, 400, 321, 360, 341, 570, 309, 311, 7840, 42434, 294, 364, 15697, 50992], "temperature": 0.0, "avg_logprob": -0.12575329555554338, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.022031573578715324}, {"id": 30, "seek": 18284, "start": 195.4, "end": 204.12, "text": " aware way. So every parameter, everything we estimate comes with a, it comes with a probability", "tokens": [50992, 3650, 636, 13, 407, 633, 13075, 11, 1203, 321, 12539, 1487, 365, 257, 11, 309, 1487, 365, 257, 8482, 51428], "temperature": 0.0, "avg_logprob": -0.12575329555554338, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.022031573578715324}, {"id": 31, "seek": 18284, "start": 204.12, "end": 212.04, "text": " distribution. It comes with an error estimate of how wrong we could be about this. So let's walk", "tokens": [51428, 7316, 13, 467, 1487, 365, 364, 6713, 12539, 295, 577, 2085, 321, 727, 312, 466, 341, 13, 407, 718, 311, 1792, 51824], "temperature": 0.0, "avg_logprob": -0.12575329555554338, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.022031573578715324}, {"id": 32, "seek": 21204, "start": 212.12, "end": 221.32, "text": " through a simple example. So the way this systems inference works is we infer what is called a", "tokens": [50368, 807, 257, 2199, 1365, 13, 407, 264, 636, 341, 3652, 38253, 1985, 307, 321, 13596, 437, 307, 1219, 257, 50828], "temperature": 0.0, "avg_logprob": -0.21927617943805197, "compression_ratio": 1.7324561403508771, "no_speech_prob": 0.009975223802030087}, {"id": 33, "seek": 21204, "start": 221.32, "end": 227.76, "text": " dynamical system. That is what physicists use to describe how, I mean, basically everything works.", "tokens": [50828, 5999, 804, 1185, 13, 663, 307, 437, 48716, 764, 281, 6786, 577, 11, 286, 914, 11, 1936, 1203, 1985, 13, 51150], "temperature": 0.0, "avg_logprob": -0.21927617943805197, "compression_ratio": 1.7324561403508771, "no_speech_prob": 0.009975223802030087}, {"id": 34, "seek": 21204, "start": 227.76, "end": 235.2, "text": " And this consists of two things. It consists of a state that is in our planet's example, that could", "tokens": [51150, 400, 341, 14689, 295, 732, 721, 13, 467, 14689, 295, 257, 1785, 300, 307, 294, 527, 5054, 311, 1365, 11, 300, 727, 51522], "temperature": 0.0, "avg_logprob": -0.21927617943805197, "compression_ratio": 1.7324561403508771, "no_speech_prob": 0.009975223802030087}, {"id": 35, "seek": 21204, "start": 235.2, "end": 240.79999999999998, "text": " be the position mass of velocity of each planet. And it consists of a rule for how that state evolves", "tokens": [51522, 312, 264, 2535, 2758, 295, 9269, 295, 1184, 5054, 13, 400, 309, 14689, 295, 257, 4978, 337, 577, 300, 1785, 43737, 51802], "temperature": 0.0, "avg_logprob": -0.21927617943805197, "compression_ratio": 1.7324561403508771, "no_speech_prob": 0.009975223802030087}, {"id": 36, "seek": 24080, "start": 240.88000000000002, "end": 245.92000000000002, "text": " over time, which would in this case be the laws of physics. Things like this, I was going off the", "tokens": [50368, 670, 565, 11, 597, 576, 294, 341, 1389, 312, 264, 6064, 295, 10649, 13, 9514, 411, 341, 11, 286, 390, 516, 766, 264, 50620], "temperature": 0.0, "avg_logprob": -0.1998306723201976, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.03412925451993942}, {"id": 37, "seek": 24080, "start": 245.92000000000002, "end": 254.08, "text": " slides a bit, but GM1M2 forces that two bodies with mass are applying to each other. And this is a", "tokens": [50620, 9788, 257, 857, 11, 457, 16609, 16, 44, 17, 5874, 300, 732, 7510, 365, 2758, 366, 9275, 281, 1184, 661, 13, 400, 341, 307, 257, 51028], "temperature": 0.0, "avg_logprob": -0.1998306723201976, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.03412925451993942}, {"id": 38, "seek": 24080, "start": 254.08, "end": 260.08000000000004, "text": " dynamical system and it's what we're trying to infer from our data. And some of our viewers may", "tokens": [51028, 5999, 804, 1185, 293, 309, 311, 437, 321, 434, 1382, 281, 13596, 490, 527, 1412, 13, 400, 512, 295, 527, 8499, 815, 51328], "temperature": 0.0, "avg_logprob": -0.1998306723201976, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.03412925451993942}, {"id": 39, "seek": 24080, "start": 260.08000000000004, "end": 265.72, "text": " notice the one I'm describing here, in fact, even on the slides is a differential equation. And for", "tokens": [51328, 3449, 264, 472, 286, 478, 16141, 510, 11, 294, 1186, 11, 754, 322, 264, 9788, 307, 257, 15756, 5367, 13, 400, 337, 51610], "temperature": 0.0, "avg_logprob": -0.1998306723201976, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.03412925451993942}, {"id": 40, "seek": 26572, "start": 265.72, "end": 273.64000000000004, "text": " those guys, I would point out that dynamical systems are known for basically, basically not", "tokens": [50364, 729, 1074, 11, 286, 576, 935, 484, 300, 5999, 804, 3652, 366, 2570, 337, 1936, 11, 1936, 406, 50760], "temperature": 0.0, "avg_logprob": -0.21580159222638165, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.017537826672196388}, {"id": 41, "seek": 26572, "start": 273.64000000000004, "end": 282.68, "text": " having any good solutions, which is partially a neutral framework. And in the dynamical, dynamic", "tokens": [50760, 1419, 604, 665, 6547, 11, 597, 307, 18886, 257, 10598, 8388, 13, 400, 294, 264, 5999, 804, 11, 8546, 51212], "temperature": 0.0, "avg_logprob": -0.21580159222638165, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.017537826672196388}, {"id": 42, "seek": 26572, "start": 282.68, "end": 286.84000000000003, "text": " course of modelling world, we would be doing something like taking the position mass and", "tokens": [51212, 1164, 295, 42253, 1002, 11, 321, 576, 312, 884, 746, 411, 1940, 264, 2535, 2758, 293, 51420], "temperature": 0.0, "avg_logprob": -0.21580159222638165, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.017537826672196388}, {"id": 43, "seek": 26572, "start": 286.84000000000003, "end": 292.84000000000003, "text": " velocity of planet A and saying, okay, I know there's probably some other planetary bodies", "tokens": [51420, 9269, 295, 5054, 316, 293, 1566, 11, 1392, 11, 286, 458, 456, 311, 1391, 512, 661, 35788, 7510, 51720], "temperature": 0.0, "avg_logprob": -0.21580159222638165, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.017537826672196388}, {"id": 44, "seek": 29284, "start": 292.84, "end": 297.08, "text": " affecting this, but I don't know how many, how many planetary bodies are affecting this planet", "tokens": [50364, 17476, 341, 11, 457, 286, 500, 380, 458, 577, 867, 11, 577, 867, 35788, 7510, 366, 17476, 341, 5054, 50576], "temperature": 0.0, "avg_logprob": -0.14210231747247476, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.026316776871681213}, {"id": 45, "seek": 29284, "start": 297.08, "end": 301.32, "text": " that I can observe? And what is the position mass and velocities of each of these? And how", "tokens": [50576, 300, 286, 393, 11441, 30, 400, 437, 307, 264, 2535, 2758, 293, 7806, 1088, 295, 1184, 295, 613, 30, 400, 577, 50788], "temperature": 0.0, "avg_logprob": -0.14210231747247476, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.026316776871681213}, {"id": 46, "seek": 29284, "start": 301.32, "end": 308.2, "text": " uncertain am I about that? That's an interesting thing to be able to do. A real world application", "tokens": [50788, 11308, 669, 286, 466, 300, 30, 663, 311, 364, 1880, 551, 281, 312, 1075, 281, 360, 13, 316, 957, 1002, 3861, 51132], "temperature": 0.0, "avg_logprob": -0.14210231747247476, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.026316776871681213}, {"id": 47, "seek": 29284, "start": 308.2, "end": 314.12, "text": " of this was in the COVID-19 pandemic. Here we have two very, very noisy things, hence the need", "tokens": [51132, 295, 341, 390, 294, 264, 4566, 12, 3405, 5388, 13, 1692, 321, 362, 732, 588, 11, 588, 24518, 721, 11, 16678, 264, 643, 51428], "temperature": 0.0, "avg_logprob": -0.14210231747247476, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.026316776871681213}, {"id": 48, "seek": 29284, "start": 314.12, "end": 318.84, "text": " for uncertainty that we understand about the pandemic. That is the number of positive COVID tests", "tokens": [51428, 337, 15697, 300, 321, 1223, 466, 264, 5388, 13, 663, 307, 264, 1230, 295, 3353, 4566, 6921, 51664], "temperature": 0.0, "avg_logprob": -0.14210231747247476, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.026316776871681213}, {"id": 49, "seek": 31884, "start": 318.84, "end": 325.64, "text": " and the number of deaths. And we have a broad understanding of how the pandemic works. It's", "tokens": [50364, 293, 264, 1230, 295, 13027, 13, 400, 321, 362, 257, 4152, 3701, 295, 577, 264, 5388, 1985, 13, 467, 311, 50704], "temperature": 0.0, "avg_logprob": -0.15636461575826008, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.01700655184686184}, {"id": 50, "seek": 31884, "start": 325.64, "end": 329.88, "text": " or at least broad understanding of ways we can model it, something like a CER model, which is", "tokens": [50704, 420, 412, 1935, 4152, 3701, 295, 2098, 321, 393, 2316, 309, 11, 746, 411, 257, 383, 1598, 2316, 11, 597, 307, 50916], "temperature": 0.0, "avg_logprob": -0.15636461575826008, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.01700655184686184}, {"id": 51, "seek": 31884, "start": 329.88, "end": 334.03999999999996, "text": " what we opted for here. Well, I didn't originate this work, but I did make it work at the end.", "tokens": [50916, 437, 321, 40768, 337, 510, 13, 1042, 11, 286, 994, 380, 4957, 473, 341, 589, 11, 457, 286, 630, 652, 309, 589, 412, 264, 917, 13, 51124], "temperature": 0.0, "avg_logprob": -0.15636461575826008, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.01700655184686184}, {"id": 52, "seek": 31884, "start": 335.32, "end": 341.71999999999997, "text": " A CER model, which is susceptible, exposed, infected, recovered. And so we can take these two", "tokens": [51188, 316, 383, 1598, 2316, 11, 597, 307, 31249, 11, 9495, 11, 15414, 11, 19542, 13, 400, 370, 321, 393, 747, 613, 732, 51508], "temperature": 0.0, "avg_logprob": -0.15636461575826008, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.01700655184686184}, {"id": 53, "seek": 31884, "start": 341.71999999999997, "end": 345.88, "text": " things. We can take the observed deaths and positive COVID tests and our understanding, and we can", "tokens": [51508, 721, 13, 492, 393, 747, 264, 13095, 13027, 293, 3353, 4566, 6921, 293, 527, 3701, 11, 293, 321, 393, 51716], "temperature": 0.0, "avg_logprob": -0.15636461575826008, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.01700655184686184}, {"id": 54, "seek": 34588, "start": 345.88, "end": 353.08, "text": " project backwards to a dynamical system that explains this. And there's three reasons this is", "tokens": [50364, 1716, 12204, 281, 257, 5999, 804, 1185, 300, 13948, 341, 13, 400, 456, 311, 1045, 4112, 341, 307, 50724], "temperature": 0.0, "avg_logprob": -0.07144153227499865, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.009589836932718754}, {"id": 55, "seek": 34588, "start": 353.08, "end": 360.36, "text": " a sort of particularly useful thing to be able to do. The first one is because we're understanding", "tokens": [50724, 257, 1333, 295, 4098, 4420, 551, 281, 312, 1075, 281, 360, 13, 440, 700, 472, 307, 570, 321, 434, 3701, 51088], "temperature": 0.0, "avg_logprob": -0.07144153227499865, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.009589836932718754}, {"id": 56, "seek": 34588, "start": 360.36, "end": 365.8, "text": " our data from partially, at least, or as much as we want to, we can tip in the things that we", "tokens": [51088, 527, 1412, 490, 18886, 11, 412, 1935, 11, 420, 382, 709, 382, 321, 528, 281, 11, 321, 393, 4125, 294, 264, 721, 300, 321, 51360], "temperature": 0.0, "avg_logprob": -0.07144153227499865, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.009589836932718754}, {"id": 57, "seek": 34588, "start": 365.8, "end": 370.84, "text": " understand about the model beforehand. We can include things like hospital admissions and ventilator", "tokens": [51360, 1223, 466, 264, 2316, 22893, 13, 492, 393, 4090, 721, 411, 4530, 29856, 293, 27498, 1639, 51612], "temperature": 0.0, "avg_logprob": -0.07144153227499865, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.009589836932718754}, {"id": 58, "seek": 34588, "start": 370.84, "end": 375.48, "text": " usage. And because we can include these parameters in our model, that means we can estimate them", "tokens": [51612, 14924, 13, 400, 570, 321, 393, 4090, 613, 9834, 294, 527, 2316, 11, 300, 1355, 321, 393, 12539, 552, 51844], "temperature": 0.0, "avg_logprob": -0.07144153227499865, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.009589836932718754}, {"id": 59, "seek": 37548, "start": 375.48, "end": 380.04, "text": " and it means we can look at them afterwards and have good guesses at them. And then we can do things", "tokens": [50364, 293, 309, 1355, 321, 393, 574, 412, 552, 10543, 293, 362, 665, 42703, 412, 552, 13, 400, 550, 321, 393, 360, 721, 50592], "temperature": 0.0, "avg_logprob": -0.11422877752480387, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.0019482591887935996}, {"id": 60, "seek": 37548, "start": 380.04, "end": 387.08000000000004, "text": " like we did here, where we look at the number of deaths from COVID-19 over time. And we can", "tokens": [50592, 411, 321, 630, 510, 11, 689, 321, 574, 412, 264, 1230, 295, 13027, 490, 4566, 12, 3405, 670, 565, 13, 400, 321, 393, 50944], "temperature": 0.0, "avg_logprob": -0.11422877752480387, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.0019482591887935996}, {"id": 61, "seek": 37548, "start": 387.08000000000004, "end": 393.72, "text": " estimate these parameters and fairly accurately, if you can see these graphs, because we knew they", "tokens": [50944, 12539, 613, 9834, 293, 6457, 20095, 11, 498, 291, 393, 536, 613, 24877, 11, 570, 321, 2586, 436, 51276], "temperature": 0.0, "avg_logprob": -0.11422877752480387, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.0019482591887935996}, {"id": 62, "seek": 37548, "start": 393.72, "end": 397.08000000000004, "text": " were there, they've been included, we can estimate them. That's a powerful thing to be able to do.", "tokens": [51276, 645, 456, 11, 436, 600, 668, 5556, 11, 321, 393, 12539, 552, 13, 663, 311, 257, 4005, 551, 281, 312, 1075, 281, 360, 13, 51444], "temperature": 0.0, "avg_logprob": -0.11422877752480387, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.0019482591887935996}, {"id": 63, "seek": 37548, "start": 398.36, "end": 402.28000000000003, "text": " Similarly, because we've included them out in model, and because of the way these things work,", "tokens": [51508, 13157, 11, 570, 321, 600, 5556, 552, 484, 294, 2316, 11, 293, 570, 295, 264, 636, 613, 721, 589, 11, 51704], "temperature": 0.0, "avg_logprob": -0.11422877752480387, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.0019482591887935996}, {"id": 64, "seek": 40228, "start": 402.28, "end": 407.79999999999995, "text": " because it is a dynamical system with different parts, and we now have it all interact with each", "tokens": [50364, 570, 309, 307, 257, 5999, 804, 1185, 365, 819, 3166, 11, 293, 321, 586, 362, 309, 439, 4648, 365, 1184, 50640], "temperature": 0.0, "avg_logprob": -0.06728553295135498, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.010075622238218784}, {"id": 65, "seek": 40228, "start": 407.79999999999995, "end": 414.84, "text": " other, we can do things like look at counterfactuals. So we can say, what if I only have 200 hospital", "tokens": [50640, 661, 11, 321, 393, 360, 721, 411, 574, 412, 5682, 44919, 901, 82, 13, 407, 321, 393, 584, 11, 437, 498, 286, 787, 362, 2331, 4530, 50992], "temperature": 0.0, "avg_logprob": -0.06728553295135498, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.010075622238218784}, {"id": 66, "seek": 40228, "start": 414.84, "end": 419.88, "text": " beds this week? What happens then? What happens? How bad do things become? And again, that's a", "tokens": [50992, 18068, 341, 1243, 30, 708, 2314, 550, 30, 708, 2314, 30, 1012, 1578, 360, 721, 1813, 30, 400, 797, 11, 300, 311, 257, 51244], "temperature": 0.0, "avg_logprob": -0.06728553295135498, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.010075622238218784}, {"id": 67, "seek": 40228, "start": 419.88, "end": 429.32, "text": " really, really nice thing to be able to do. Finally, because this is a Bayesian method,", "tokens": [51244, 534, 11, 534, 1481, 551, 281, 312, 1075, 281, 360, 13, 6288, 11, 570, 341, 307, 257, 7840, 42434, 3170, 11, 51716], "temperature": 0.0, "avg_logprob": -0.06728553295135498, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.010075622238218784}, {"id": 68, "seek": 42932, "start": 429.32, "end": 434.2, "text": " because it is uncertainty aware, we don't just get single point estimates out of this method.", "tokens": [50364, 570, 309, 307, 15697, 3650, 11, 321, 500, 380, 445, 483, 2167, 935, 20561, 484, 295, 341, 3170, 13, 50608], "temperature": 0.0, "avg_logprob": -0.0896599143743515, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.10411403328180313}, {"id": 69, "seek": 42932, "start": 434.2, "end": 440.2, "text": " We get probability distributions. So we can say things instead of like, I need 200 hospital beds", "tokens": [50608, 492, 483, 8482, 37870, 13, 407, 321, 393, 584, 721, 2602, 295, 411, 11, 286, 643, 2331, 4530, 18068, 50908], "temperature": 0.0, "avg_logprob": -0.0896599143743515, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.10411403328180313}, {"id": 70, "seek": 42932, "start": 440.2, "end": 447.24, "text": " this week. We can say, the model says, I'm 95% sure that I need between 200 and 400, or I'm 90%", "tokens": [50908, 341, 1243, 13, 492, 393, 584, 11, 264, 2316, 1619, 11, 286, 478, 13420, 4, 988, 300, 286, 643, 1296, 2331, 293, 8423, 11, 420, 286, 478, 4289, 4, 51260], "temperature": 0.0, "avg_logprob": -0.0896599143743515, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.10411403328180313}, {"id": 71, "seek": 42932, "start": 447.24, "end": 452.12, "text": " sure I need at least 300. And that's much, much, much more valuable than if you're planning things.", "tokens": [51260, 988, 286, 643, 412, 1935, 6641, 13, 400, 300, 311, 709, 11, 709, 11, 709, 544, 8263, 813, 498, 291, 434, 5038, 721, 13, 51504], "temperature": 0.0, "avg_logprob": -0.0896599143743515, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.10411403328180313}, {"id": 72, "seek": 42932, "start": 452.12, "end": 455.64, "text": " That's what you need to know. You don't need to guess at what's going to happen. You need to know", "tokens": [51504, 663, 311, 437, 291, 643, 281, 458, 13, 509, 500, 380, 643, 281, 2041, 412, 437, 311, 516, 281, 1051, 13, 509, 643, 281, 458, 51680], "temperature": 0.0, "avg_logprob": -0.0896599143743515, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.10411403328180313}, {"id": 73, "seek": 45564, "start": 456.59999999999997, "end": 465.15999999999997, "text": " what might happen and how likely that is. In terms of the project we did about this,", "tokens": [50412, 437, 1062, 1051, 293, 577, 3700, 300, 307, 13, 682, 2115, 295, 264, 1716, 321, 630, 466, 341, 11, 50840], "temperature": 0.0, "avg_logprob": -0.15688172630641772, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.014345007948577404}, {"id": 74, "seek": 45564, "start": 465.88, "end": 471.88, "text": " my work about this, it was really about making it more open source. And I say more because the", "tokens": [50876, 452, 589, 466, 341, 11, 309, 390, 534, 466, 1455, 309, 544, 1269, 4009, 13, 400, 286, 584, 544, 570, 264, 51176], "temperature": 0.0, "avg_logprob": -0.15688172630641772, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.014345007948577404}, {"id": 75, "seek": 45564, "start": 471.88, "end": 477.24, "text": " original code base was open source, but it was written in MATLAB. And MATLAB is not really free", "tokens": [51176, 3380, 3089, 3096, 390, 1269, 4009, 11, 457, 309, 390, 3720, 294, 5904, 11435, 33, 13, 400, 5904, 11435, 33, 307, 406, 534, 1737, 51444], "temperature": 0.0, "avg_logprob": -0.15688172630641772, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.014345007948577404}, {"id": 76, "seek": 45564, "start": 477.24, "end": 483.8, "text": " and open source. And I will give credit to the original researchers. They did modify things", "tokens": [51444, 293, 1269, 4009, 13, 400, 286, 486, 976, 5397, 281, 264, 3380, 10309, 13, 814, 630, 16927, 721, 51772], "temperature": 0.0, "avg_logprob": -0.15688172630641772, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.014345007948577404}, {"id": 77, "seek": 48380, "start": 484.6, "end": 488.92, "text": " so that it could mostly be run in Octave. But the unfortunate fact of the matter is,", "tokens": [50404, 370, 300, 309, 727, 5240, 312, 1190, 294, 6788, 946, 13, 583, 264, 17843, 1186, 295, 264, 1871, 307, 11, 50620], "temperature": 0.0, "avg_logprob": -0.13210382236270454, "compression_ratio": 1.6013513513513513, "no_speech_prob": 0.022418798878788948}, {"id": 78, "seek": 48380, "start": 490.44, "end": 495.0, "text": " Octave is a wonderful thing. It really is. I can't give enough credit to developers, but it is not", "tokens": [50696, 6788, 946, 307, 257, 3715, 551, 13, 467, 534, 307, 13, 286, 393, 380, 976, 1547, 5397, 281, 8849, 11, 457, 309, 307, 406, 50924], "temperature": 0.0, "avg_logprob": -0.13210382236270454, "compression_ratio": 1.6013513513513513, "no_speech_prob": 0.022418798878788948}, {"id": 79, "seek": 48380, "start": 495.0, "end": 499.56, "text": " a replacement for MATLAB. Complicated MATLAB code, particularly MATLAB code, but dependencies", "tokens": [50924, 257, 14419, 337, 5904, 11435, 33, 13, 33736, 3587, 5904, 11435, 33, 3089, 11, 4098, 5904, 11435, 33, 3089, 11, 457, 36606, 51152], "temperature": 0.0, "avg_logprob": -0.13210382236270454, "compression_ratio": 1.6013513513513513, "no_speech_prob": 0.022418798878788948}, {"id": 80, "seek": 48380, "start": 499.56, "end": 504.44, "text": " will just break and then it's annoying to fix. And it's at least 10 times slower. Depending on what", "tokens": [51152, 486, 445, 1821, 293, 550, 309, 311, 11304, 281, 3191, 13, 400, 309, 311, 412, 1935, 1266, 1413, 14009, 13, 22539, 322, 437, 51396], "temperature": 0.0, "avg_logprob": -0.13210382236270454, "compression_ratio": 1.6013513513513513, "no_speech_prob": 0.022418798878788948}, {"id": 81, "seek": 48380, "start": 504.44, "end": 508.84000000000003, "text": " you're doing, it could be much, much, much slower than that. So having your code in MATLAB, even", "tokens": [51396, 291, 434, 884, 11, 309, 727, 312, 709, 11, 709, 11, 709, 14009, 813, 300, 13, 407, 1419, 428, 3089, 294, 5904, 11435, 33, 11, 754, 51616], "temperature": 0.0, "avg_logprob": -0.13210382236270454, "compression_ratio": 1.6013513513513513, "no_speech_prob": 0.022418798878788948}, {"id": 82, "seek": 50884, "start": 508.84, "end": 514.6, "text": " if it's open sourced, it's not really open sourced. So we rewrote the project in C++,", "tokens": [50364, 498, 309, 311, 1269, 11006, 1232, 11, 309, 311, 406, 534, 1269, 11006, 1232, 13, 407, 321, 319, 7449, 1370, 264, 1716, 294, 383, 25472, 11, 50652], "temperature": 0.0, "avg_logprob": -0.11213007918349258, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.057748712599277496}, {"id": 83, "seek": 50884, "start": 515.24, "end": 519.48, "text": " which I know some people will disagree is an improvement, but it certainly goes a lot quicker.", "tokens": [50684, 597, 286, 458, 512, 561, 486, 14091, 307, 364, 10444, 11, 457, 309, 3297, 1709, 257, 688, 16255, 13, 50896], "temperature": 0.0, "avg_logprob": -0.11213007918349258, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.057748712599277496}, {"id": 84, "seek": 50884, "start": 521.0, "end": 527.48, "text": " And yeah, as we did this, we have tried really hard to make this code more generic as well,", "tokens": [50972, 400, 1338, 11, 382, 321, 630, 341, 11, 321, 362, 3031, 534, 1152, 281, 652, 341, 3089, 544, 19577, 382, 731, 11, 51296], "temperature": 0.0, "avg_logprob": -0.11213007918349258, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.057748712599277496}, {"id": 85, "seek": 50884, "start": 527.48, "end": 531.3199999999999, "text": " because the original code was research code. It was designed for just two problems,", "tokens": [51296, 570, 264, 3380, 3089, 390, 2132, 3089, 13, 467, 390, 4761, 337, 445, 732, 2740, 11, 51488], "temperature": 0.0, "avg_logprob": -0.11213007918349258, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.057748712599277496}, {"id": 86, "seek": 50884, "start": 531.88, "end": 536.68, "text": " neuroscience stuff and a little bit of COVID stuff. Now the code works generally.", "tokens": [51516, 42762, 1507, 293, 257, 707, 857, 295, 4566, 1507, 13, 823, 264, 3089, 1985, 5101, 13, 51756], "temperature": 0.0, "avg_logprob": -0.11213007918349258, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.057748712599277496}, {"id": 87, "seek": 53668, "start": 537.0, "end": 542.5999999999999, "text": " The other thing that we have been trying to do with this is to try and make it more open in other", "tokens": [50380, 440, 661, 551, 300, 321, 362, 668, 1382, 281, 360, 365, 341, 307, 281, 853, 293, 652, 309, 544, 1269, 294, 661, 50660], "temperature": 0.0, "avg_logprob": -0.20094217347704674, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.008690888062119484}, {"id": 88, "seek": 53668, "start": 542.5999999999999, "end": 548.68, "text": " ways. Again, a problem with this is its research work. And so although there are papers describing", "tokens": [50660, 2098, 13, 3764, 11, 257, 1154, 365, 341, 307, 1080, 2132, 589, 13, 400, 370, 4878, 456, 366, 10577, 16141, 50964], "temperature": 0.0, "avg_logprob": -0.20094217347704674, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.008690888062119484}, {"id": 89, "seek": 53668, "start": 548.68, "end": 553.64, "text": " how all of this works, I'm not going to lie, I literally did a PhD in this, they're incomprehensible.", "tokens": [50964, 577, 439, 295, 341, 1985, 11, 286, 478, 406, 516, 281, 4544, 11, 286, 3736, 630, 257, 14476, 294, 341, 11, 436, 434, 14036, 40128, 30633, 13, 51212], "temperature": 0.0, "avg_logprob": -0.20094217347704674, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.008690888062119484}, {"id": 90, "seek": 53668, "start": 554.5999999999999, "end": 558.68, "text": " It's really, really, really difficult to get to the bottom of the papers, even if you really", "tokens": [51260, 467, 311, 534, 11, 534, 11, 534, 2252, 281, 483, 281, 264, 2767, 295, 264, 10577, 11, 754, 498, 291, 534, 51464], "temperature": 0.0, "avg_logprob": -0.20094217347704674, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.008690888062119484}, {"id": 91, "seek": 53668, "start": 558.68, "end": 562.68, "text": " understand the subject material. If you don't understand the subject material, it's just awful.", "tokens": [51464, 1223, 264, 3983, 2527, 13, 759, 291, 500, 380, 1223, 264, 3983, 2527, 11, 309, 311, 445, 11232, 13, 51664], "temperature": 0.0, "avg_logprob": -0.20094217347704674, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.008690888062119484}, {"id": 92, "seek": 56268, "start": 563.64, "end": 567.8, "text": " And the thing is, when you've gone through it, and you do understand all of this, it's not that hard.", "tokens": [50412, 400, 264, 551, 307, 11, 562, 291, 600, 2780, 807, 309, 11, 293, 291, 360, 1223, 439, 295, 341, 11, 309, 311, 406, 300, 1152, 13, 50620], "temperature": 0.0, "avg_logprob": -0.23406612629793128, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.07000505924224854}, {"id": 93, "seek": 56268, "start": 569.0799999999999, "end": 575.4, "text": " It's interesting and useful and creative, but it's difficult, it is not. And it doesn't need to", "tokens": [50684, 467, 311, 1880, 293, 4420, 293, 5880, 11, 457, 309, 311, 2252, 11, 309, 307, 406, 13, 400, 309, 1177, 380, 643, 281, 51000], "temperature": 0.0, "avg_logprob": -0.23406612629793128, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.07000505924224854}, {"id": 94, "seek": 56268, "start": 575.4, "end": 579.4, "text": " be, so that's the other part of the work we have been doing with this. To try and make it", "tokens": [51000, 312, 11, 370, 300, 311, 264, 661, 644, 295, 264, 589, 321, 362, 668, 884, 365, 341, 13, 1407, 853, 293, 652, 309, 51200], "temperature": 0.0, "avg_logprob": -0.23406612629793128, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.07000505924224854}, {"id": 95, "seek": 56268, "start": 579.4, "end": 584.28, "text": " free and open source by making it open about exactly what's happening and why.", "tokens": [51200, 1737, 293, 1269, 4009, 538, 1455, 309, 1269, 466, 2293, 437, 311, 2737, 293, 983, 13, 51444], "temperature": 0.0, "avg_logprob": -0.23406612629793128, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.07000505924224854}, {"id": 96, "seek": 58428, "start": 584.92, "end": 593.0, "text": " Anyway, that is the end of my talk. That is my GitHub repo. If anybody wants to look at it,", "tokens": [50396, 5684, 11, 300, 307, 264, 917, 295, 452, 751, 13, 663, 307, 452, 23331, 49040, 13, 759, 4472, 2738, 281, 574, 412, 309, 11, 50800], "temperature": 0.0, "avg_logprob": -0.18801000041346397, "compression_ratio": 1.3048780487804879, "no_speech_prob": 0.046461403369903564}, {"id": 97, "seek": 58428, "start": 594.04, "end": 595.0, "text": " do we have any questions?", "tokens": [50852, 360, 321, 362, 604, 1651, 30, 50900], "temperature": 0.0, "avg_logprob": -0.18801000041346397, "compression_ratio": 1.3048780487804879, "no_speech_prob": 0.046461403369903564}, {"id": 98, "seek": 58428, "start": 604.6, "end": 612.04, "text": " William, you've written it in C++. A lot of AI people expect to use Python. What's your solution", "tokens": [51380, 6740, 11, 291, 600, 3720, 309, 294, 383, 25472, 13, 316, 688, 295, 7318, 561, 2066, 281, 764, 15329, 13, 708, 311, 428, 3827, 51752], "temperature": 0.0, "avg_logprob": -0.18801000041346397, "compression_ratio": 1.3048780487804879, "no_speech_prob": 0.046461403369903564}, {"id": 99, "seek": 61204, "start": 612.12, "end": 619.24, "text": " for them? I have been writing a Python front-end for it. Is the solution.", "tokens": [50368, 337, 552, 30, 286, 362, 668, 3579, 257, 15329, 1868, 12, 521, 337, 309, 13, 1119, 264, 3827, 13, 50724], "temperature": 0.0, "avg_logprob": -0.2916439719822096, "compression_ratio": 1.3942857142857144, "no_speech_prob": 0.031346481293439865}, {"id": 100, "seek": 61204, "start": 619.88, "end": 623.24, "text": " It's a little bit of work in progress, but you can draw from this in Python if you want to.", "tokens": [50756, 467, 311, 257, 707, 857, 295, 589, 294, 4205, 11, 457, 291, 393, 2642, 490, 341, 294, 15329, 498, 291, 528, 281, 13, 50924], "temperature": 0.0, "avg_logprob": -0.2916439719822096, "compression_ratio": 1.3942857142857144, "no_speech_prob": 0.031346481293439865}, {"id": 101, "seek": 61204, "start": 625.9599999999999, "end": 631.24, "text": " Any other questions? You're going to make me run? No? All right, everyone give", "tokens": [51060, 2639, 661, 1651, 30, 509, 434, 516, 281, 652, 385, 1190, 30, 883, 30, 1057, 558, 11, 1518, 976, 51324], "temperature": 0.0, "avg_logprob": -0.2916439719822096, "compression_ratio": 1.3942857142857144, "no_speech_prob": 0.031346481293439865}, {"id": 102, "seek": 63124, "start": 631.24, "end": 638.04, "text": " another round of applause for Will.", "tokens": [50368, 1071, 3098, 295, 9969, 337, 3099, 13, 50704], "temperature": 0.0, "avg_logprob": -0.48795008659362793, "compression_ratio": 0.813953488372093, "no_speech_prob": 0.0520520955324173}], "language": "en"}