{"text": " Okay, so thanks video team for fixing this. So that was very helpful and now few minutes late, but we just, you know, do the same talk a bit later, same 20 minutes slot plus five minutes Q&A is quick wit Fran\u00e7ois. Welcome. Should I say it is it sufficient? Yes, I should be okay. Hi everyone. I'm very happy to be here. Thanks for having me in this room. We have been working like on observability since three years at quick wit and I would like to present here what we have done during the three years of outcome of it at least. First I will introduce myself a bit. So I'm Fran\u00e7ois. I'm working on the core engine of quick wit, which is a search engine. I also the co-founder of quick wit and I also work a lot. I'm working a lot on the graph and data spring that I will show you in this presentation. So for this, for the agenda, I would start like with taking a step back, a short one and then we'll talk about like the problem of cardinality that we can have with metrics and then I will show you like very briefly the engine of quick wit and how it works. And finally, I will show you a demo of quick wit working in Grafana for application monitoring. So let's start by taking a step back. So I'm showing this graph. It's not mine. This diagram is from Ben Siegelman. This is a Googler who works on the distributed tracing dapper, software dapper when he was at Google and you co-founded also LightStep, which is a company doing observability and monitoring stuff. I kind of like this diagram because it summarizes all the complexities, the intricacies between monitoring and observability and the different signals that we can get from our applications or our servers. So at the bottom, you have the three signals or the three pillars. It depends on how you call them. Traces, metrics and logs. And generally, you store them in different databases, metrics, you store them in time series databases because you want something optimized for it and it can be very optimized for this kind of data. And for trace and logs, it depends. You can store them in a search engine or you can store them in dedicated storage. I'm sure you know tempo and loki, loki for logs and tempo for traces. And on top of it, you try to build your monitoring software with alerts on metrics or could be on logs or even traces if you can. So let me talk a bit about how the problem of cardinality here. At the bottom, you can see that metrics goes always into the TSDB, but you can also put some traces information into your TSDB. But in this case, you need to be very careful about what you did because in traces, you can have a lot of labels and you can be very, very accurate about what's happening. So that's why I just want to stop there a minute. When you want to monitor a distributed system and we all generally have that somewhere in our job, they can fail for various number of reasons, even for a tremendous amount of reasons. And so you may want to label everything like if you have a software that is deployed with different versions, you want to have this version label, same for the host, same for the customer ID, if you are a SaaS, for example, you can have thousands of them. And you want also to monitor your services, your endpoints. In summary, your cardinality will explode and this will be a problem for your time series database. It's a problem, it can be either a performance problem or a money problem because if you look at data dock pricing, for example, you will pay $5 for 100 custom metrics. So if you want custom metrics because that's it, if you want something very specific to your business, in this case, it will cost you a lot. So generally, you don't want to have all those labels on your metrics. You want to control and just keep a low number of them. That's not the same for traces. In general, traces, you want to keep everything so that you can dig into, like, really understand the full trace, each unit of work in your software, you will be able to understand for each customer, for one customer particularly, for one request ID or for one user ID, you will know what happened in your system. So generally, you keep everything in your traces. And that's what I will talk about today and that's what QuickWit is for. So QuickWit is an engine that is storing logs and traces and particular traces. It does not handle metrics. And it is a bit different than other search engines in the sense that we decoupled a compute storage. So we have the same approach as low key and tempo on this, that chip storage is great. If you use an object storage, it's also very reliable. So you don't lose your data and you have all the benefits of decoupling your write pass, your read pass. It's really great when you want to scale to a lot of data and that's the case in observability. And last point is that we worked on the search part a lot so that it can stay sub-second even if all your data is on object storage. And I will explain how it works very briefly. So the engine architecture is quite simple. It's globally the same for this kind of decoupled compute and storage architecture. You will find the same for a tempo, for example. So at the middle, you have your object storage where you store your data. This is the source of truth. On the left side, this is the write pass. And on the right side, this is the read pass. So on the right pass, you have your incoming, gizand documents, could be traces, could be whatever. And you have your indexes that is running. And each 30 seconds typically or each 15 seconds, it will build what we call a split in QuickWit. This is a file where we put all the data structures that are used at search time. So it's several well-optimized data structures to be searchable on object storage. So we create them, the indexer creates them, and then upload it to the object storage. So you will have a bunch of splits that are put on the object storage each 30 seconds, for example. And each time you put the split, you also put one row in a meta store. It could be a PostgreSQL database or it could be just a gizand file stored on the object storage. So we will add just the metadata of the split in it. And once it is inside the meta store, like the metadata of the split that was uploaded on object storage, on the object storage, then the searcher is able to search it. So you have this nice decoupling where then if you want more searchers, you just have to increase your number of searchers. You can even shut all of them down. That's not a problem. So that's for the high-level view. To understand why QuickWit is fat on object storage, I have to show you also how to do how a split is made. This is an interesting part because it shows you also the different data structures that we are using. And it will help understand how we can achieve fast search later on. So you have basically three data structures in the split and one thing that we call a hard cache. The first data structure is the doc store. It's a row-oriented storage. So if you have a document ID, we will give you the whole jizz and document. The second data structure is the inverting index. So in this case, if you are looking for a user ID or a quest ID or a keyword, it's optimized in the sense that if you give a user ID, we will retrieve immediately the list of document ID that contains this user ID. So it is very fast. And then you just have to retrieve a document from the list of whose document ID is. The third data structure is the column now store. So here it's for doing aggregations. If you want to do analytics on your logs or traces, we will use this column now store. You can have a lot of columns. You can have spare columns. That's optimized for that. And the last part is what we call it's a split footer that we keep in general in the memory of a searcher because it's very, very small. I put it 0.07% of the size of a split. So it's very, very small. For that, it's cool because you can always keep it in your cache on your searcher. And in this hot cache, you will find all small pointers to the other data structures so that when you make a search request, you will need only to make one or two requests to your object storage to find the response. So that's why I said when we optimized QuikWit for object storage, it's because of that because we optimized those pointers. We put that in one footer that we can keep in cache. So just the next part now, I will explain you a bit how spans are stored in QuikWit. Okay. I have only eight minutes left, so I need to speed up it. In QuikWit, you can model things as you want. You can put any documents. But for span generally, you want to stick to the open telemetry data model. So what we did for this demo is that we used the data model based on the open telemetry data model. Well, you have a bunch of fields that are always there. And you have also some dynamic fields like resource attributes and span attributes. All those are very dynamic. So I put some random examples here where you can generate random keys and random fields. And here are the nice things that QuikWit is also shameless so that we can store every inverting index and the columnar storage. We can store all those dynamic fields without declaring which fields you have or which you don't have. It will index everything in the inverting index and in the columnar storage. So it's nice when you don't know in advance all your attributes that you have on your spans. So it's time for the demo. So for the demo, I prepared a demo for application monitoring. So my first problem was generating spans, traces that are understandable for this kind of goal. So I discovered recently a tool called, which is an extension of K6, which is like a project from Grafana for testing, for load testing. And there is a nice extension to generate traces. I will show you a bit or it works. And then I deployed a QuikWit cluster on Kubernetes and I did a Grafana instance to show you the results. So a word on the XK6 extensions. It's a nice extension just to, you can declare some spans. Like here I put some services like shop backend, ethical service. And you can declare whatever you want. So it's a template and then you can set the cardinality. You can set if you want some random attributes. So it's pretty nice to stress test and see if your engine can handle high cardinality fields, can handle like many random attributes. So it's pretty cool. So let's do the demo because I prepared it's live. Can you see it? Maybe I can zoom it a bit. So here I'm a heating or Kubernetes cluster and the index in which I'm setting traces. So we have approximately here now 341 million spans. So if I do a refresh, you will see this number moving. So now we have 355 millions of spans. So great. You can see that the uncompressed size of the document that are ingested in QuikWit have around 200 and almost 300 gigabytes of size. And that it's less, we compressed data a lot in QuikWit. So here it's around you can divide by seven on this example, the size of the data ingested in QuikWit. So here the size of Publish Plits is the size that is taken on the JStorage by QuikWit. So what can we do? So we are sending a lot of traces just to confirm that it's live. So here it's just a dashboard based on QuikWit Prometheus Matrix. So it's live. I launched it I think six hours earlier today. So I'm just sending traces at 11 megabytes per second. That's not huge, but it's already pretty decent because it represents around one terabyte per day. And you can see that it is running with only one indexer. And we have one indexer that is not really doing a lot of things. It is using between one CPU and two CPU here. The spikes are due to the compaction that we are doing because QuikWit generates a lot of splits. And so we need to merge them. We need to merge them so that we reduce the number of threads that we will do on the object storage. So great, it's working. It's live. And I can show you that we can search traces with it. So let's have a look. So I'm running a search query on all the documents. So you have a lot of them here. You can see that per minute here you have one million spans. So that's a lot. And we can focus on maybe I want to remove QuikWit traces. It should work. It's service name, I think. Okay, great. So here, as we are sending QuikWit traces in QuikWit so that we can monitor our own cluster with it, so now I'm selecting only the traces that I'm generated for this demo. So for example, we can look for query articles. So here, for example, what I can do is focus on one span ID. So as we have an inverting index, you can look for very accurate attributes. So here I took a span ID, but I guess we can do this on another. Let me check. I need to check article two card. So now I want to focus on this one because I added on this span particularly a high cardinality field for the demo. So you can see here that you have this random attribute with this random value that is here. So I can focus on it. And very fast I will get the results. You can see that this attribute value happens only very rarely. And it's with a search engine, it's even faster when you have this kind of query. So that's nice to search through spans, but generally you want to dig into one trace particularly. So you can use a Yeager plugin that is pointing at QuickWit cluster and it will return all the spans for the given trace. So it's easy to go from one span to the whole traces. But usually you want more from your data. And by that I mean you want to monitor your services. Looking at one particular span is very nice, but when you know what you are looking for, but when you don't know, it's better to build this kind of monitoring dashboard. So here, okay, I will try to go a bit earlier in the past. Yeah, good. So I have set different panels. The first one is like a data histogram and I'm counting HTTP requests. So for calling HTTP requests, I'm using span attributes. I can show you that. So here I'm saying I want all HTTP requests. So I'm using like the open telemetry semantic for that. And I'm saying, okay, I don't want all other spans. I want only those ones who have at least one HTTP target. And here in the second panel, I'm doing like a group by by status code. So I'm using again like a dynamic attribute that is the status code. And I'm doing a group by on it and building this data histogram. This one is a bit the same. So here instead of doing a group by on a status code, I'm doing a group by on target. So if you want to monitor your endpoints, that's useful. This one I can show you a bit more because here it's not the count. We are computing the sum of the duration of each span. So I'm doing group by on each target and I'm summing all the span duration. So you have a good feeling about the time taken by each of your target, your each endpoint. But that's not enough, right? You want something that is more useful. It's nice to see this, but it's not enough to take a decision like is there a problem or not. The last one is the P95 latency panel is more for that. So you can see that you have a nice, smooth latency, a P95 latency on all your services, except for this one. This is normal because I sent some traces with different latencies. So we will dig into that. So we know that maybe there is a problem here. The panel average latency here is just the average. I think you understand that. And this one is also interesting because I'm sending spans from two virtual data centers. So one is CDG and one is a BIHRU. And so you can see that most of the spans are coming from CDG here. So for example, if we want to dig into why there is a problem on the P95 latency, and I will stop after that. I will just show you that you can add your query here and say, okay, I want to look at all the spans that are over one second and one second, 0.1 seconds. And so you can see that only the endpoint article, so card is problematic. You can see that maybe there is a problem in the data center because you have suddenly more requests here. And of course, if you go up to two seconds, then you see that here the color has changed, but it's coming from this data center. And if you want to dig into one particular traces, you can open like and have a look at it. So that's it. I think I will stop there because time is up. If you have questions, come and see me. I will be happy to answer them. And also I have some stickers and hoodies, so don't hesitate. I'm happy to give you some.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.56, "text": " Okay, so thanks video team for fixing this.", "tokens": [50364, 1033, 11, 370, 3231, 960, 1469, 337, 19442, 341, 13, 50892], "temperature": 0.0, "avg_logprob": -0.3846312009371244, "compression_ratio": 1.3433734939759037, "no_speech_prob": 0.5996912121772766}, {"id": 1, "seek": 0, "start": 10.56, "end": 15.780000000000001, "text": " So that was very helpful and now few minutes late, but we just, you know, do the same talk", "tokens": [50892, 407, 300, 390, 588, 4961, 293, 586, 1326, 2077, 3469, 11, 457, 321, 445, 11, 291, 458, 11, 360, 264, 912, 751, 51153], "temperature": 0.0, "avg_logprob": -0.3846312009371244, "compression_ratio": 1.3433734939759037, "no_speech_prob": 0.5996912121772766}, {"id": 2, "seek": 0, "start": 15.780000000000001, "end": 21.84, "text": " a bit later, same 20 minutes slot plus five minutes Q&A is quick wit Fran\u00e7ois.", "tokens": [51153, 257, 857, 1780, 11, 912, 945, 2077, 14747, 1804, 1732, 2077, 1249, 5, 32, 307, 1702, 32161, 1526, 12368, 7376, 13, 51456], "temperature": 0.0, "avg_logprob": -0.3846312009371244, "compression_ratio": 1.3433734939759037, "no_speech_prob": 0.5996912121772766}, {"id": 3, "seek": 0, "start": 21.84, "end": 22.84, "text": " Welcome.", "tokens": [51456, 4027, 13, 51506], "temperature": 0.0, "avg_logprob": -0.3846312009371244, "compression_ratio": 1.3433734939759037, "no_speech_prob": 0.5996912121772766}, {"id": 4, "seek": 2284, "start": 22.84, "end": 28.84, "text": " Should I say it is it sufficient?", "tokens": [50364, 6454, 286, 584, 309, 307, 309, 11563, 30, 50664], "temperature": 0.0, "avg_logprob": -0.40462449539539425, "compression_ratio": 1.5148514851485149, "no_speech_prob": 0.14276885986328125}, {"id": 5, "seek": 2284, "start": 28.84, "end": 31.68, "text": " Yes, I should be okay.", "tokens": [50664, 1079, 11, 286, 820, 312, 1392, 13, 50806], "temperature": 0.0, "avg_logprob": -0.40462449539539425, "compression_ratio": 1.5148514851485149, "no_speech_prob": 0.14276885986328125}, {"id": 6, "seek": 2284, "start": 31.68, "end": 33.68, "text": " Hi everyone.", "tokens": [50806, 2421, 1518, 13, 50906], "temperature": 0.0, "avg_logprob": -0.40462449539539425, "compression_ratio": 1.5148514851485149, "no_speech_prob": 0.14276885986328125}, {"id": 7, "seek": 2284, "start": 33.68, "end": 35.4, "text": " I'm very happy to be here.", "tokens": [50906, 286, 478, 588, 2055, 281, 312, 510, 13, 50992], "temperature": 0.0, "avg_logprob": -0.40462449539539425, "compression_ratio": 1.5148514851485149, "no_speech_prob": 0.14276885986328125}, {"id": 8, "seek": 2284, "start": 35.4, "end": 38.4, "text": " Thanks for having me in this room.", "tokens": [50992, 2561, 337, 1419, 385, 294, 341, 1808, 13, 51142], "temperature": 0.0, "avg_logprob": -0.40462449539539425, "compression_ratio": 1.5148514851485149, "no_speech_prob": 0.14276885986328125}, {"id": 9, "seek": 2284, "start": 38.4, "end": 43.28, "text": " We have been working like on observability since three years at quick wit and I would", "tokens": [51142, 492, 362, 668, 1364, 411, 322, 9951, 2310, 1670, 1045, 924, 412, 1702, 32161, 293, 286, 576, 51386], "temperature": 0.0, "avg_logprob": -0.40462449539539425, "compression_ratio": 1.5148514851485149, "no_speech_prob": 0.14276885986328125}, {"id": 10, "seek": 2284, "start": 43.28, "end": 50.44, "text": " like to present here what we have done during the three years of outcome of it at least.", "tokens": [51386, 411, 281, 1974, 510, 437, 321, 362, 1096, 1830, 264, 1045, 924, 295, 9700, 295, 309, 412, 1935, 13, 51744], "temperature": 0.0, "avg_logprob": -0.40462449539539425, "compression_ratio": 1.5148514851485149, "no_speech_prob": 0.14276885986328125}, {"id": 11, "seek": 5044, "start": 50.44, "end": 52.919999999999995, "text": " First I will introduce myself a bit.", "tokens": [50364, 2386, 286, 486, 5366, 2059, 257, 857, 13, 50488], "temperature": 0.0, "avg_logprob": -0.25920250415802004, "compression_ratio": 1.56, "no_speech_prob": 0.04497920349240303}, {"id": 12, "seek": 5044, "start": 52.919999999999995, "end": 53.919999999999995, "text": " So I'm Fran\u00e7ois.", "tokens": [50488, 407, 286, 478, 1526, 12368, 7376, 13, 50538], "temperature": 0.0, "avg_logprob": -0.25920250415802004, "compression_ratio": 1.56, "no_speech_prob": 0.04497920349240303}, {"id": 13, "seek": 5044, "start": 53.919999999999995, "end": 60.04, "text": " I'm working on the core engine of quick wit, which is a search engine.", "tokens": [50538, 286, 478, 1364, 322, 264, 4965, 2848, 295, 1702, 32161, 11, 597, 307, 257, 3164, 2848, 13, 50844], "temperature": 0.0, "avg_logprob": -0.25920250415802004, "compression_ratio": 1.56, "no_speech_prob": 0.04497920349240303}, {"id": 14, "seek": 5044, "start": 60.04, "end": 64.8, "text": " I also the co-founder of quick wit and I also work a lot.", "tokens": [50844, 286, 611, 264, 598, 12, 33348, 295, 1702, 32161, 293, 286, 611, 589, 257, 688, 13, 51082], "temperature": 0.0, "avg_logprob": -0.25920250415802004, "compression_ratio": 1.56, "no_speech_prob": 0.04497920349240303}, {"id": 15, "seek": 5044, "start": 64.8, "end": 72.4, "text": " I'm working a lot on the graph and data spring that I will show you in this presentation.", "tokens": [51082, 286, 478, 1364, 257, 688, 322, 264, 4295, 293, 1412, 5587, 300, 286, 486, 855, 291, 294, 341, 5860, 13, 51462], "temperature": 0.0, "avg_logprob": -0.25920250415802004, "compression_ratio": 1.56, "no_speech_prob": 0.04497920349240303}, {"id": 16, "seek": 7240, "start": 72.4, "end": 82.52000000000001, "text": " So for this, for the agenda, I would start like with taking a step back, a short one", "tokens": [50364, 407, 337, 341, 11, 337, 264, 9829, 11, 286, 576, 722, 411, 365, 1940, 257, 1823, 646, 11, 257, 2099, 472, 50870], "temperature": 0.0, "avg_logprob": -0.23880909813774956, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.34270039200782776}, {"id": 17, "seek": 7240, "start": 82.52000000000001, "end": 87.24000000000001, "text": " and then we'll talk about like the problem of cardinality that we can have with metrics", "tokens": [50870, 293, 550, 321, 603, 751, 466, 411, 264, 1154, 295, 2920, 259, 1860, 300, 321, 393, 362, 365, 16367, 51106], "temperature": 0.0, "avg_logprob": -0.23880909813774956, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.34270039200782776}, {"id": 18, "seek": 7240, "start": 87.24000000000001, "end": 95.68, "text": " and then I will show you like very briefly the engine of quick wit and how it works.", "tokens": [51106, 293, 550, 286, 486, 855, 291, 411, 588, 10515, 264, 2848, 295, 1702, 32161, 293, 577, 309, 1985, 13, 51528], "temperature": 0.0, "avg_logprob": -0.23880909813774956, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.34270039200782776}, {"id": 19, "seek": 7240, "start": 95.68, "end": 101.64000000000001, "text": " And finally, I will show you a demo of quick wit working in Grafana for application monitoring.", "tokens": [51528, 400, 2721, 11, 286, 486, 855, 291, 257, 10723, 295, 1702, 32161, 1364, 294, 8985, 69, 2095, 337, 3861, 11028, 13, 51826], "temperature": 0.0, "avg_logprob": -0.23880909813774956, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.34270039200782776}, {"id": 20, "seek": 10164, "start": 102.64, "end": 107.24, "text": " So let's start by taking a step back.", "tokens": [50414, 407, 718, 311, 722, 538, 1940, 257, 1823, 646, 13, 50644], "temperature": 0.0, "avg_logprob": -0.2625496039229832, "compression_ratio": 1.471698113207547, "no_speech_prob": 0.04820817708969116}, {"id": 21, "seek": 10164, "start": 107.24, "end": 108.68, "text": " So I'm showing this graph.", "tokens": [50644, 407, 286, 478, 4099, 341, 4295, 13, 50716], "temperature": 0.0, "avg_logprob": -0.2625496039229832, "compression_ratio": 1.471698113207547, "no_speech_prob": 0.04820817708969116}, {"id": 22, "seek": 10164, "start": 108.68, "end": 109.68, "text": " It's not mine.", "tokens": [50716, 467, 311, 406, 3892, 13, 50766], "temperature": 0.0, "avg_logprob": -0.2625496039229832, "compression_ratio": 1.471698113207547, "no_speech_prob": 0.04820817708969116}, {"id": 23, "seek": 10164, "start": 109.68, "end": 111.64, "text": " This diagram is from Ben Siegelman.", "tokens": [50766, 639, 10686, 307, 490, 3964, 3559, 10345, 1601, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2625496039229832, "compression_ratio": 1.471698113207547, "no_speech_prob": 0.04820817708969116}, {"id": 24, "seek": 10164, "start": 111.64, "end": 119.16, "text": " This is a Googler who works on the distributed tracing dapper, software dapper when he was", "tokens": [50864, 639, 307, 257, 45005, 1918, 567, 1985, 322, 264, 12631, 25262, 1120, 3717, 11, 4722, 1120, 3717, 562, 415, 390, 51240], "temperature": 0.0, "avg_logprob": -0.2625496039229832, "compression_ratio": 1.471698113207547, "no_speech_prob": 0.04820817708969116}, {"id": 25, "seek": 10164, "start": 119.16, "end": 125.12, "text": " at Google and you co-founded also LightStep, which is a company doing observability and", "tokens": [51240, 412, 3329, 293, 291, 598, 12, 49547, 611, 8279, 23624, 11, 597, 307, 257, 2237, 884, 9951, 2310, 293, 51538], "temperature": 0.0, "avg_logprob": -0.2625496039229832, "compression_ratio": 1.471698113207547, "no_speech_prob": 0.04820817708969116}, {"id": 26, "seek": 10164, "start": 125.12, "end": 126.88, "text": " monitoring stuff.", "tokens": [51538, 11028, 1507, 13, 51626], "temperature": 0.0, "avg_logprob": -0.2625496039229832, "compression_ratio": 1.471698113207547, "no_speech_prob": 0.04820817708969116}, {"id": 27, "seek": 12688, "start": 127.0, "end": 135.12, "text": " I kind of like this diagram because it summarizes all the complexities, the intricacies between", "tokens": [50370, 286, 733, 295, 411, 341, 10686, 570, 309, 14611, 5660, 439, 264, 48705, 11, 264, 30242, 20330, 1296, 50776], "temperature": 0.0, "avg_logprob": -0.2534038402416088, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.024138154461979866}, {"id": 28, "seek": 12688, "start": 135.12, "end": 141.76, "text": " monitoring and observability and the different signals that we can get from our applications", "tokens": [50776, 11028, 293, 9951, 2310, 293, 264, 819, 12354, 300, 321, 393, 483, 490, 527, 5821, 51108], "temperature": 0.0, "avg_logprob": -0.2534038402416088, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.024138154461979866}, {"id": 29, "seek": 12688, "start": 141.76, "end": 143.76, "text": " or our servers.", "tokens": [51108, 420, 527, 15909, 13, 51208], "temperature": 0.0, "avg_logprob": -0.2534038402416088, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.024138154461979866}, {"id": 30, "seek": 12688, "start": 143.76, "end": 148.44, "text": " So at the bottom, you have the three signals or the three pillars.", "tokens": [51208, 407, 412, 264, 2767, 11, 291, 362, 264, 1045, 12354, 420, 264, 1045, 26729, 13, 51442], "temperature": 0.0, "avg_logprob": -0.2534038402416088, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.024138154461979866}, {"id": 31, "seek": 12688, "start": 148.44, "end": 150.92, "text": " It depends on how you call them.", "tokens": [51442, 467, 5946, 322, 577, 291, 818, 552, 13, 51566], "temperature": 0.0, "avg_logprob": -0.2534038402416088, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.024138154461979866}, {"id": 32, "seek": 12688, "start": 150.92, "end": 152.64, "text": " Traces, metrics and logs.", "tokens": [51566, 1765, 2116, 11, 16367, 293, 20820, 13, 51652], "temperature": 0.0, "avg_logprob": -0.2534038402416088, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.024138154461979866}, {"id": 33, "seek": 15264, "start": 152.64, "end": 157.44, "text": " And generally, you store them in different databases, metrics, you store them in time", "tokens": [50364, 400, 5101, 11, 291, 3531, 552, 294, 819, 22380, 11, 16367, 11, 291, 3531, 552, 294, 565, 50604], "temperature": 0.0, "avg_logprob": -0.16143777198398235, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.009105932898819447}, {"id": 34, "seek": 15264, "start": 157.44, "end": 163.35999999999999, "text": " series databases because you want something optimized for it and it can be very optimized", "tokens": [50604, 2638, 22380, 570, 291, 528, 746, 26941, 337, 309, 293, 309, 393, 312, 588, 26941, 50900], "temperature": 0.0, "avg_logprob": -0.16143777198398235, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.009105932898819447}, {"id": 35, "seek": 15264, "start": 163.35999999999999, "end": 166.67999999999998, "text": " for this kind of data.", "tokens": [50900, 337, 341, 733, 295, 1412, 13, 51066], "temperature": 0.0, "avg_logprob": -0.16143777198398235, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.009105932898819447}, {"id": 36, "seek": 15264, "start": 166.67999999999998, "end": 169.32, "text": " And for trace and logs, it depends.", "tokens": [51066, 400, 337, 13508, 293, 20820, 11, 309, 5946, 13, 51198], "temperature": 0.0, "avg_logprob": -0.16143777198398235, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.009105932898819447}, {"id": 37, "seek": 15264, "start": 169.32, "end": 174.2, "text": " You can store them in a search engine or you can store them in dedicated storage.", "tokens": [51198, 509, 393, 3531, 552, 294, 257, 3164, 2848, 420, 291, 393, 3531, 552, 294, 8374, 6725, 13, 51442], "temperature": 0.0, "avg_logprob": -0.16143777198398235, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.009105932898819447}, {"id": 38, "seek": 15264, "start": 174.2, "end": 181.23999999999998, "text": " I'm sure you know tempo and loki, loki for logs and tempo for traces.", "tokens": [51442, 286, 478, 988, 291, 458, 8972, 293, 450, 2984, 11, 450, 2984, 337, 20820, 293, 8972, 337, 26076, 13, 51794], "temperature": 0.0, "avg_logprob": -0.16143777198398235, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.009105932898819447}, {"id": 39, "seek": 18124, "start": 181.24, "end": 188.72, "text": " And on top of it, you try to build your monitoring software with alerts on metrics or could be", "tokens": [50364, 400, 322, 1192, 295, 309, 11, 291, 853, 281, 1322, 428, 11028, 4722, 365, 28061, 322, 16367, 420, 727, 312, 50738], "temperature": 0.0, "avg_logprob": -0.1683692681161981, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.0067924936302006245}, {"id": 40, "seek": 18124, "start": 188.72, "end": 193.48000000000002, "text": " on logs or even traces if you can.", "tokens": [50738, 322, 20820, 420, 754, 26076, 498, 291, 393, 13, 50976], "temperature": 0.0, "avg_logprob": -0.1683692681161981, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.0067924936302006245}, {"id": 41, "seek": 18124, "start": 193.48000000000002, "end": 201.0, "text": " So let me talk a bit about how the problem of cardinality here.", "tokens": [50976, 407, 718, 385, 751, 257, 857, 466, 577, 264, 1154, 295, 2920, 259, 1860, 510, 13, 51352], "temperature": 0.0, "avg_logprob": -0.1683692681161981, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.0067924936302006245}, {"id": 42, "seek": 18124, "start": 201.0, "end": 209.28, "text": " At the bottom, you can see that metrics goes always into the TSDB, but you can also put", "tokens": [51352, 1711, 264, 2767, 11, 291, 393, 536, 300, 16367, 1709, 1009, 666, 264, 37645, 27735, 11, 457, 291, 393, 611, 829, 51766], "temperature": 0.0, "avg_logprob": -0.1683692681161981, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.0067924936302006245}, {"id": 43, "seek": 20928, "start": 209.28, "end": 212.44, "text": " some traces information into your TSDB.", "tokens": [50364, 512, 26076, 1589, 666, 428, 37645, 27735, 13, 50522], "temperature": 0.0, "avg_logprob": -0.16818659955805, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.011086749844253063}, {"id": 44, "seek": 20928, "start": 212.44, "end": 218.2, "text": " But in this case, you need to be very careful about what you did because in traces, you", "tokens": [50522, 583, 294, 341, 1389, 11, 291, 643, 281, 312, 588, 5026, 466, 437, 291, 630, 570, 294, 26076, 11, 291, 50810], "temperature": 0.0, "avg_logprob": -0.16818659955805, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.011086749844253063}, {"id": 45, "seek": 20928, "start": 218.2, "end": 224.34, "text": " can have a lot of labels and you can be very, very accurate about what's happening.", "tokens": [50810, 393, 362, 257, 688, 295, 16949, 293, 291, 393, 312, 588, 11, 588, 8559, 466, 437, 311, 2737, 13, 51117], "temperature": 0.0, "avg_logprob": -0.16818659955805, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.011086749844253063}, {"id": 46, "seek": 20928, "start": 224.34, "end": 230.88, "text": " So that's why I just want to stop there a minute.", "tokens": [51117, 407, 300, 311, 983, 286, 445, 528, 281, 1590, 456, 257, 3456, 13, 51444], "temperature": 0.0, "avg_logprob": -0.16818659955805, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.011086749844253063}, {"id": 47, "seek": 20928, "start": 230.88, "end": 237.76, "text": " When you want to monitor a distributed system and we all generally have that somewhere in", "tokens": [51444, 1133, 291, 528, 281, 6002, 257, 12631, 1185, 293, 321, 439, 5101, 362, 300, 4079, 294, 51788], "temperature": 0.0, "avg_logprob": -0.16818659955805, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.011086749844253063}, {"id": 48, "seek": 23776, "start": 237.76, "end": 245.56, "text": " our job, they can fail for various number of reasons, even for a tremendous amount of", "tokens": [50364, 527, 1691, 11, 436, 393, 3061, 337, 3683, 1230, 295, 4112, 11, 754, 337, 257, 10048, 2372, 295, 50754], "temperature": 0.0, "avg_logprob": -0.22582944956692783, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.03653889521956444}, {"id": 49, "seek": 23776, "start": 245.56, "end": 246.88, "text": " reasons.", "tokens": [50754, 4112, 13, 50820], "temperature": 0.0, "avg_logprob": -0.22582944956692783, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.03653889521956444}, {"id": 50, "seek": 23776, "start": 246.88, "end": 252.84, "text": " And so you may want to label everything like if you have a software that is deployed with", "tokens": [50820, 400, 370, 291, 815, 528, 281, 7645, 1203, 411, 498, 291, 362, 257, 4722, 300, 307, 17826, 365, 51118], "temperature": 0.0, "avg_logprob": -0.22582944956692783, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.03653889521956444}, {"id": 51, "seek": 23776, "start": 252.84, "end": 259.44, "text": " different versions, you want to have this version label, same for the host, same for", "tokens": [51118, 819, 9606, 11, 291, 528, 281, 362, 341, 3037, 7645, 11, 912, 337, 264, 3975, 11, 912, 337, 51448], "temperature": 0.0, "avg_logprob": -0.22582944956692783, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.03653889521956444}, {"id": 52, "seek": 23776, "start": 259.44, "end": 265.76, "text": " the customer ID, if you are a SaaS, for example, you can have thousands of them.", "tokens": [51448, 264, 5474, 7348, 11, 498, 291, 366, 257, 49733, 11, 337, 1365, 11, 291, 393, 362, 5383, 295, 552, 13, 51764], "temperature": 0.0, "avg_logprob": -0.22582944956692783, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.03653889521956444}, {"id": 53, "seek": 26576, "start": 265.76, "end": 270.4, "text": " And you want also to monitor your services, your endpoints.", "tokens": [50364, 400, 291, 528, 611, 281, 6002, 428, 3328, 11, 428, 917, 20552, 13, 50596], "temperature": 0.0, "avg_logprob": -0.19023886181059338, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0723022073507309}, {"id": 54, "seek": 26576, "start": 270.4, "end": 276.32, "text": " In summary, your cardinality will explode and this will be a problem for your time series", "tokens": [50596, 682, 12691, 11, 428, 2920, 259, 1860, 486, 21411, 293, 341, 486, 312, 257, 1154, 337, 428, 565, 2638, 50892], "temperature": 0.0, "avg_logprob": -0.19023886181059338, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0723022073507309}, {"id": 55, "seek": 26576, "start": 276.32, "end": 279.36, "text": " database.", "tokens": [50892, 8149, 13, 51044], "temperature": 0.0, "avg_logprob": -0.19023886181059338, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0723022073507309}, {"id": 56, "seek": 26576, "start": 279.36, "end": 284.56, "text": " It's a problem, it can be either a performance problem or a money problem because if you look", "tokens": [51044, 467, 311, 257, 1154, 11, 309, 393, 312, 2139, 257, 3389, 1154, 420, 257, 1460, 1154, 570, 498, 291, 574, 51304], "temperature": 0.0, "avg_logprob": -0.19023886181059338, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0723022073507309}, {"id": 57, "seek": 26576, "start": 284.56, "end": 291.92, "text": " at data dock pricing, for example, you will pay $5 for 100 custom metrics.", "tokens": [51304, 412, 1412, 20929, 17621, 11, 337, 1365, 11, 291, 486, 1689, 1848, 20, 337, 2319, 2375, 16367, 13, 51672], "temperature": 0.0, "avg_logprob": -0.19023886181059338, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0723022073507309}, {"id": 58, "seek": 29192, "start": 291.92, "end": 296.48, "text": " So if you want custom metrics because that's it, if you want something very specific to", "tokens": [50364, 407, 498, 291, 528, 2375, 16367, 570, 300, 311, 309, 11, 498, 291, 528, 746, 588, 2685, 281, 50592], "temperature": 0.0, "avg_logprob": -0.17483245409451997, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.23266172409057617}, {"id": 59, "seek": 29192, "start": 296.48, "end": 300.44, "text": " your business, in this case, it will cost you a lot.", "tokens": [50592, 428, 1606, 11, 294, 341, 1389, 11, 309, 486, 2063, 291, 257, 688, 13, 50790], "temperature": 0.0, "avg_logprob": -0.17483245409451997, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.23266172409057617}, {"id": 60, "seek": 29192, "start": 300.44, "end": 305.08000000000004, "text": " So generally, you don't want to have all those labels on your metrics.", "tokens": [50790, 407, 5101, 11, 291, 500, 380, 528, 281, 362, 439, 729, 16949, 322, 428, 16367, 13, 51022], "temperature": 0.0, "avg_logprob": -0.17483245409451997, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.23266172409057617}, {"id": 61, "seek": 29192, "start": 305.08000000000004, "end": 310.96000000000004, "text": " You want to control and just keep a low number of them.", "tokens": [51022, 509, 528, 281, 1969, 293, 445, 1066, 257, 2295, 1230, 295, 552, 13, 51316], "temperature": 0.0, "avg_logprob": -0.17483245409451997, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.23266172409057617}, {"id": 62, "seek": 29192, "start": 310.96000000000004, "end": 314.08000000000004, "text": " That's not the same for traces.", "tokens": [51316, 663, 311, 406, 264, 912, 337, 26076, 13, 51472], "temperature": 0.0, "avg_logprob": -0.17483245409451997, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.23266172409057617}, {"id": 63, "seek": 29192, "start": 314.08000000000004, "end": 318.88, "text": " In general, traces, you want to keep everything so that you can dig into, like, really understand", "tokens": [51472, 682, 2674, 11, 26076, 11, 291, 528, 281, 1066, 1203, 370, 300, 291, 393, 2528, 666, 11, 411, 11, 534, 1223, 51712], "temperature": 0.0, "avg_logprob": -0.17483245409451997, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.23266172409057617}, {"id": 64, "seek": 31888, "start": 318.88, "end": 325.04, "text": " the full trace, each unit of work in your software, you will be able to understand for", "tokens": [50364, 264, 1577, 13508, 11, 1184, 4985, 295, 589, 294, 428, 4722, 11, 291, 486, 312, 1075, 281, 1223, 337, 50672], "temperature": 0.0, "avg_logprob": -0.18002333586243377, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.017139118164777756}, {"id": 65, "seek": 31888, "start": 325.04, "end": 330.88, "text": " each customer, for one customer particularly, for one request ID or for one user ID, you", "tokens": [50672, 1184, 5474, 11, 337, 472, 5474, 4098, 11, 337, 472, 5308, 7348, 420, 337, 472, 4195, 7348, 11, 291, 50964], "temperature": 0.0, "avg_logprob": -0.18002333586243377, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.017139118164777756}, {"id": 66, "seek": 31888, "start": 330.88, "end": 334.2, "text": " will know what happened in your system.", "tokens": [50964, 486, 458, 437, 2011, 294, 428, 1185, 13, 51130], "temperature": 0.0, "avg_logprob": -0.18002333586243377, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.017139118164777756}, {"id": 67, "seek": 31888, "start": 334.2, "end": 340.04, "text": " So generally, you keep everything in your traces.", "tokens": [51130, 407, 5101, 11, 291, 1066, 1203, 294, 428, 26076, 13, 51422], "temperature": 0.0, "avg_logprob": -0.18002333586243377, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.017139118164777756}, {"id": 68, "seek": 31888, "start": 340.04, "end": 344.36, "text": " And that's what I will talk about today and that's what QuickWit is for.", "tokens": [51422, 400, 300, 311, 437, 286, 486, 751, 466, 965, 293, 300, 311, 437, 12101, 54, 270, 307, 337, 13, 51638], "temperature": 0.0, "avg_logprob": -0.18002333586243377, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.017139118164777756}, {"id": 69, "seek": 34436, "start": 344.36, "end": 350.24, "text": " So QuickWit is an engine that is storing logs and traces and particular traces.", "tokens": [50364, 407, 12101, 54, 270, 307, 364, 2848, 300, 307, 26085, 20820, 293, 26076, 293, 1729, 26076, 13, 50658], "temperature": 0.0, "avg_logprob": -0.23628572795702063, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.10973668843507767}, {"id": 70, "seek": 34436, "start": 350.24, "end": 353.72, "text": " It does not handle metrics.", "tokens": [50658, 467, 775, 406, 4813, 16367, 13, 50832], "temperature": 0.0, "avg_logprob": -0.23628572795702063, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.10973668843507767}, {"id": 71, "seek": 34436, "start": 353.72, "end": 361.32, "text": " And it is a bit different than other search engines in the sense that we decoupled a compute", "tokens": [50832, 400, 309, 307, 257, 857, 819, 813, 661, 3164, 12982, 294, 264, 2020, 300, 321, 979, 263, 15551, 257, 14722, 51212], "temperature": 0.0, "avg_logprob": -0.23628572795702063, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.10973668843507767}, {"id": 72, "seek": 34436, "start": 361.32, "end": 362.32, "text": " storage.", "tokens": [51212, 6725, 13, 51262], "temperature": 0.0, "avg_logprob": -0.23628572795702063, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.10973668843507767}, {"id": 73, "seek": 34436, "start": 362.32, "end": 369.08000000000004, "text": " So we have the same approach as low key and tempo on this, that chip storage is great.", "tokens": [51262, 407, 321, 362, 264, 912, 3109, 382, 2295, 2141, 293, 8972, 322, 341, 11, 300, 11409, 6725, 307, 869, 13, 51600], "temperature": 0.0, "avg_logprob": -0.23628572795702063, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.10973668843507767}, {"id": 74, "seek": 34436, "start": 369.08000000000004, "end": 372.56, "text": " If you use an object storage, it's also very reliable.", "tokens": [51600, 759, 291, 764, 364, 2657, 6725, 11, 309, 311, 611, 588, 12924, 13, 51774], "temperature": 0.0, "avg_logprob": -0.23628572795702063, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.10973668843507767}, {"id": 75, "seek": 37256, "start": 372.56, "end": 378.72, "text": " So you don't lose your data and you have all the benefits of decoupling your write", "tokens": [50364, 407, 291, 500, 380, 3624, 428, 1412, 293, 291, 362, 439, 264, 5311, 295, 979, 263, 11970, 428, 2464, 50672], "temperature": 0.0, "avg_logprob": -0.1899769751580207, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.04032690450549126}, {"id": 76, "seek": 37256, "start": 378.72, "end": 381.32, "text": " pass, your read pass.", "tokens": [50672, 1320, 11, 428, 1401, 1320, 13, 50802], "temperature": 0.0, "avg_logprob": -0.1899769751580207, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.04032690450549126}, {"id": 77, "seek": 37256, "start": 381.32, "end": 387.28000000000003, "text": " It's really great when you want to scale to a lot of data and that's the case in observability.", "tokens": [50802, 467, 311, 534, 869, 562, 291, 528, 281, 4373, 281, 257, 688, 295, 1412, 293, 300, 311, 264, 1389, 294, 9951, 2310, 13, 51100], "temperature": 0.0, "avg_logprob": -0.1899769751580207, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.04032690450549126}, {"id": 78, "seek": 37256, "start": 387.28000000000003, "end": 394.92, "text": " And last point is that we worked on the search part a lot so that it can stay sub-second", "tokens": [51100, 400, 1036, 935, 307, 300, 321, 2732, 322, 264, 3164, 644, 257, 688, 370, 300, 309, 393, 1754, 1422, 12, 27375, 51482], "temperature": 0.0, "avg_logprob": -0.1899769751580207, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.04032690450549126}, {"id": 79, "seek": 37256, "start": 394.92, "end": 398.76, "text": " even if all your data is on object storage.", "tokens": [51482, 754, 498, 439, 428, 1412, 307, 322, 2657, 6725, 13, 51674], "temperature": 0.0, "avg_logprob": -0.1899769751580207, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.04032690450549126}, {"id": 80, "seek": 39876, "start": 398.76, "end": 404.32, "text": " And I will explain how it works very briefly.", "tokens": [50364, 400, 286, 486, 2903, 577, 309, 1985, 588, 10515, 13, 50642], "temperature": 0.0, "avg_logprob": -0.16438689686003186, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.04610607400536537}, {"id": 81, "seek": 39876, "start": 404.32, "end": 409.52, "text": " So the engine architecture is quite simple.", "tokens": [50642, 407, 264, 2848, 9482, 307, 1596, 2199, 13, 50902], "temperature": 0.0, "avg_logprob": -0.16438689686003186, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.04610607400536537}, {"id": 82, "seek": 39876, "start": 409.52, "end": 415.59999999999997, "text": " It's globally the same for this kind of decoupled compute and storage architecture.", "tokens": [50902, 467, 311, 18958, 264, 912, 337, 341, 733, 295, 979, 263, 15551, 14722, 293, 6725, 9482, 13, 51206], "temperature": 0.0, "avg_logprob": -0.16438689686003186, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.04610607400536537}, {"id": 83, "seek": 39876, "start": 415.59999999999997, "end": 418.2, "text": " You will find the same for a tempo, for example.", "tokens": [51206, 509, 486, 915, 264, 912, 337, 257, 8972, 11, 337, 1365, 13, 51336], "temperature": 0.0, "avg_logprob": -0.16438689686003186, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.04610607400536537}, {"id": 84, "seek": 39876, "start": 418.2, "end": 422.44, "text": " So at the middle, you have your object storage where you store your data.", "tokens": [51336, 407, 412, 264, 2808, 11, 291, 362, 428, 2657, 6725, 689, 291, 3531, 428, 1412, 13, 51548], "temperature": 0.0, "avg_logprob": -0.16438689686003186, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.04610607400536537}, {"id": 85, "seek": 39876, "start": 422.44, "end": 424.8, "text": " This is the source of truth.", "tokens": [51548, 639, 307, 264, 4009, 295, 3494, 13, 51666], "temperature": 0.0, "avg_logprob": -0.16438689686003186, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.04610607400536537}, {"id": 86, "seek": 42480, "start": 424.8, "end": 428.40000000000003, "text": " On the left side, this is the write pass.", "tokens": [50364, 1282, 264, 1411, 1252, 11, 341, 307, 264, 2464, 1320, 13, 50544], "temperature": 0.0, "avg_logprob": -0.27974027395248413, "compression_ratio": 1.682051282051282, "no_speech_prob": 0.04283292591571808}, {"id": 87, "seek": 42480, "start": 428.40000000000003, "end": 431.32, "text": " And on the right side, this is the read pass.", "tokens": [50544, 400, 322, 264, 558, 1252, 11, 341, 307, 264, 1401, 1320, 13, 50690], "temperature": 0.0, "avg_logprob": -0.27974027395248413, "compression_ratio": 1.682051282051282, "no_speech_prob": 0.04283292591571808}, {"id": 88, "seek": 42480, "start": 431.32, "end": 437.2, "text": " So on the right pass, you have your incoming, gizand documents, could be traces, could be", "tokens": [50690, 407, 322, 264, 558, 1320, 11, 291, 362, 428, 22341, 11, 290, 590, 474, 8512, 11, 727, 312, 26076, 11, 727, 312, 50984], "temperature": 0.0, "avg_logprob": -0.27974027395248413, "compression_ratio": 1.682051282051282, "no_speech_prob": 0.04283292591571808}, {"id": 89, "seek": 42480, "start": 437.2, "end": 438.36, "text": " whatever.", "tokens": [50984, 2035, 13, 51042], "temperature": 0.0, "avg_logprob": -0.27974027395248413, "compression_ratio": 1.682051282051282, "no_speech_prob": 0.04283292591571808}, {"id": 90, "seek": 42480, "start": 438.36, "end": 440.96000000000004, "text": " And you have your indexes that is running.", "tokens": [51042, 400, 291, 362, 428, 8186, 279, 300, 307, 2614, 13, 51172], "temperature": 0.0, "avg_logprob": -0.27974027395248413, "compression_ratio": 1.682051282051282, "no_speech_prob": 0.04283292591571808}, {"id": 91, "seek": 42480, "start": 440.96000000000004, "end": 450.28000000000003, "text": " And each 30 seconds typically or each 15 seconds, it will build what we call a split", "tokens": [51172, 400, 1184, 2217, 3949, 5850, 420, 1184, 2119, 3949, 11, 309, 486, 1322, 437, 321, 818, 257, 7472, 51638], "temperature": 0.0, "avg_logprob": -0.27974027395248413, "compression_ratio": 1.682051282051282, "no_speech_prob": 0.04283292591571808}, {"id": 92, "seek": 42480, "start": 450.28000000000003, "end": 451.28000000000003, "text": " in QuickWit.", "tokens": [51638, 294, 12101, 54, 270, 13, 51688], "temperature": 0.0, "avg_logprob": -0.27974027395248413, "compression_ratio": 1.682051282051282, "no_speech_prob": 0.04283292591571808}, {"id": 93, "seek": 45128, "start": 451.28, "end": 457.23999999999995, "text": " This is a file where we put all the data structures that are used at search time.", "tokens": [50364, 639, 307, 257, 3991, 689, 321, 829, 439, 264, 1412, 9227, 300, 366, 1143, 412, 3164, 565, 13, 50662], "temperature": 0.0, "avg_logprob": -0.11248716417249742, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.006214401219040155}, {"id": 94, "seek": 45128, "start": 457.23999999999995, "end": 467.2, "text": " So it's several well-optimized data structures to be searchable on object storage.", "tokens": [50662, 407, 309, 311, 2940, 731, 12, 5747, 332, 1602, 1412, 9227, 281, 312, 3164, 712, 322, 2657, 6725, 13, 51160], "temperature": 0.0, "avg_logprob": -0.11248716417249742, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.006214401219040155}, {"id": 95, "seek": 45128, "start": 467.2, "end": 474.08, "text": " So we create them, the indexer creates them, and then upload it to the object storage.", "tokens": [51160, 407, 321, 1884, 552, 11, 264, 8186, 260, 7829, 552, 11, 293, 550, 6580, 309, 281, 264, 2657, 6725, 13, 51504], "temperature": 0.0, "avg_logprob": -0.11248716417249742, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.006214401219040155}, {"id": 96, "seek": 45128, "start": 474.08, "end": 479.64, "text": " So you will have a bunch of splits that are put on the object storage each 30 seconds,", "tokens": [51504, 407, 291, 486, 362, 257, 3840, 295, 37741, 300, 366, 829, 322, 264, 2657, 6725, 1184, 2217, 3949, 11, 51782], "temperature": 0.0, "avg_logprob": -0.11248716417249742, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.006214401219040155}, {"id": 97, "seek": 45128, "start": 479.64, "end": 481.2, "text": " for example.", "tokens": [51782, 337, 1365, 13, 51860], "temperature": 0.0, "avg_logprob": -0.11248716417249742, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.006214401219040155}, {"id": 98, "seek": 48120, "start": 481.2, "end": 487.92, "text": " And each time you put the split, you also put one row in a meta store.", "tokens": [50364, 400, 1184, 565, 291, 829, 264, 7472, 11, 291, 611, 829, 472, 5386, 294, 257, 19616, 3531, 13, 50700], "temperature": 0.0, "avg_logprob": -0.18908045151654412, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.018691183999180794}, {"id": 99, "seek": 48120, "start": 487.92, "end": 494.92, "text": " It could be a PostgreSQL database or it could be just a gizand file stored on the object", "tokens": [50700, 467, 727, 312, 257, 10223, 33248, 39934, 8149, 420, 309, 727, 312, 445, 257, 290, 590, 474, 3991, 12187, 322, 264, 2657, 51050], "temperature": 0.0, "avg_logprob": -0.18908045151654412, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.018691183999180794}, {"id": 100, "seek": 48120, "start": 494.92, "end": 495.92, "text": " storage.", "tokens": [51050, 6725, 13, 51100], "temperature": 0.0, "avg_logprob": -0.18908045151654412, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.018691183999180794}, {"id": 101, "seek": 48120, "start": 495.92, "end": 501.68, "text": " So we will add just the metadata of the split in it.", "tokens": [51100, 407, 321, 486, 909, 445, 264, 26603, 295, 264, 7472, 294, 309, 13, 51388], "temperature": 0.0, "avg_logprob": -0.18908045151654412, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.018691183999180794}, {"id": 102, "seek": 48120, "start": 501.68, "end": 508.56, "text": " And once it is inside the meta store, like the metadata of the split that was uploaded", "tokens": [51388, 400, 1564, 309, 307, 1854, 264, 19616, 3531, 11, 411, 264, 26603, 295, 264, 7472, 300, 390, 17135, 51732], "temperature": 0.0, "avg_logprob": -0.18908045151654412, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.018691183999180794}, {"id": 103, "seek": 50856, "start": 508.56, "end": 514.72, "text": " on object storage, on the object storage, then the searcher is able to search it.", "tokens": [50364, 322, 2657, 6725, 11, 322, 264, 2657, 6725, 11, 550, 264, 3164, 260, 307, 1075, 281, 3164, 309, 13, 50672], "temperature": 0.0, "avg_logprob": -0.18942612464274836, "compression_ratio": 1.71875, "no_speech_prob": 0.059259094297885895}, {"id": 104, "seek": 50856, "start": 514.72, "end": 519.04, "text": " So you have this nice decoupling where then if you want more searchers, you just have", "tokens": [50672, 407, 291, 362, 341, 1481, 979, 263, 11970, 689, 550, 498, 291, 528, 544, 3164, 433, 11, 291, 445, 362, 50888], "temperature": 0.0, "avg_logprob": -0.18942612464274836, "compression_ratio": 1.71875, "no_speech_prob": 0.059259094297885895}, {"id": 105, "seek": 50856, "start": 519.04, "end": 521.48, "text": " to increase your number of searchers.", "tokens": [50888, 281, 3488, 428, 1230, 295, 3164, 433, 13, 51010], "temperature": 0.0, "avg_logprob": -0.18942612464274836, "compression_ratio": 1.71875, "no_speech_prob": 0.059259094297885895}, {"id": 106, "seek": 50856, "start": 521.48, "end": 524.8, "text": " You can even shut all of them down.", "tokens": [51010, 509, 393, 754, 5309, 439, 295, 552, 760, 13, 51176], "temperature": 0.0, "avg_logprob": -0.18942612464274836, "compression_ratio": 1.71875, "no_speech_prob": 0.059259094297885895}, {"id": 107, "seek": 50856, "start": 524.8, "end": 528.36, "text": " That's not a problem.", "tokens": [51176, 663, 311, 406, 257, 1154, 13, 51354], "temperature": 0.0, "avg_logprob": -0.18942612464274836, "compression_ratio": 1.71875, "no_speech_prob": 0.059259094297885895}, {"id": 108, "seek": 50856, "start": 528.36, "end": 531.44, "text": " So that's for the high-level view.", "tokens": [51354, 407, 300, 311, 337, 264, 1090, 12, 12418, 1910, 13, 51508], "temperature": 0.0, "avg_logprob": -0.18942612464274836, "compression_ratio": 1.71875, "no_speech_prob": 0.059259094297885895}, {"id": 109, "seek": 50856, "start": 531.44, "end": 538.52, "text": " To understand why QuickWit is fat on object storage, I have to show you also how to do", "tokens": [51508, 1407, 1223, 983, 12101, 54, 270, 307, 4046, 322, 2657, 6725, 11, 286, 362, 281, 855, 291, 611, 577, 281, 360, 51862], "temperature": 0.0, "avg_logprob": -0.18942612464274836, "compression_ratio": 1.71875, "no_speech_prob": 0.059259094297885895}, {"id": 110, "seek": 53852, "start": 538.52, "end": 541.4, "text": " how a split is made.", "tokens": [50364, 577, 257, 7472, 307, 1027, 13, 50508], "temperature": 0.0, "avg_logprob": -0.2199689175220246, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.020161619409918785}, {"id": 111, "seek": 53852, "start": 541.4, "end": 546.0799999999999, "text": " This is an interesting part because it shows you also the different data structures that", "tokens": [50508, 639, 307, 364, 1880, 644, 570, 309, 3110, 291, 611, 264, 819, 1412, 9227, 300, 50742], "temperature": 0.0, "avg_logprob": -0.2199689175220246, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.020161619409918785}, {"id": 112, "seek": 53852, "start": 546.0799999999999, "end": 547.56, "text": " we are using.", "tokens": [50742, 321, 366, 1228, 13, 50816], "temperature": 0.0, "avg_logprob": -0.2199689175220246, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.020161619409918785}, {"id": 113, "seek": 53852, "start": 547.56, "end": 553.1999999999999, "text": " And it will help understand how we can achieve fast search later on.", "tokens": [50816, 400, 309, 486, 854, 1223, 577, 321, 393, 4584, 2370, 3164, 1780, 322, 13, 51098], "temperature": 0.0, "avg_logprob": -0.2199689175220246, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.020161619409918785}, {"id": 114, "seek": 53852, "start": 553.1999999999999, "end": 559.4399999999999, "text": " So you have basically three data structures in the split and one thing that we call a", "tokens": [51098, 407, 291, 362, 1936, 1045, 1412, 9227, 294, 264, 7472, 293, 472, 551, 300, 321, 818, 257, 51410], "temperature": 0.0, "avg_logprob": -0.2199689175220246, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.020161619409918785}, {"id": 115, "seek": 53852, "start": 559.4399999999999, "end": 560.4399999999999, "text": " hard cache.", "tokens": [51410, 1152, 19459, 13, 51460], "temperature": 0.0, "avg_logprob": -0.2199689175220246, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.020161619409918785}, {"id": 116, "seek": 53852, "start": 560.4399999999999, "end": 563.52, "text": " The first data structure is the doc store.", "tokens": [51460, 440, 700, 1412, 3877, 307, 264, 3211, 3531, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2199689175220246, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.020161619409918785}, {"id": 117, "seek": 53852, "start": 563.52, "end": 565.56, "text": " It's a row-oriented storage.", "tokens": [51614, 467, 311, 257, 5386, 12, 27414, 6725, 13, 51716], "temperature": 0.0, "avg_logprob": -0.2199689175220246, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.020161619409918785}, {"id": 118, "seek": 56556, "start": 565.56, "end": 570.9599999999999, "text": " So if you have a document ID, we will give you the whole jizz and document.", "tokens": [50364, 407, 498, 291, 362, 257, 4166, 7348, 11, 321, 486, 976, 291, 264, 1379, 361, 8072, 293, 4166, 13, 50634], "temperature": 0.0, "avg_logprob": -0.1799036172720102, "compression_ratio": 1.691542288557214, "no_speech_prob": 0.0052167936228215694}, {"id": 119, "seek": 56556, "start": 570.9599999999999, "end": 574.3199999999999, "text": " The second data structure is the inverting index.", "tokens": [50634, 440, 1150, 1412, 3877, 307, 264, 28653, 783, 8186, 13, 50802], "temperature": 0.0, "avg_logprob": -0.1799036172720102, "compression_ratio": 1.691542288557214, "no_speech_prob": 0.0052167936228215694}, {"id": 120, "seek": 56556, "start": 574.3199999999999, "end": 582.3199999999999, "text": " So in this case, if you are looking for a user ID or a quest ID or a keyword, it's optimized", "tokens": [50802, 407, 294, 341, 1389, 11, 498, 291, 366, 1237, 337, 257, 4195, 7348, 420, 257, 866, 7348, 420, 257, 20428, 11, 309, 311, 26941, 51202], "temperature": 0.0, "avg_logprob": -0.1799036172720102, "compression_ratio": 1.691542288557214, "no_speech_prob": 0.0052167936228215694}, {"id": 121, "seek": 56556, "start": 582.3199999999999, "end": 589.04, "text": " in the sense that if you give a user ID, we will retrieve immediately the list of document", "tokens": [51202, 294, 264, 2020, 300, 498, 291, 976, 257, 4195, 7348, 11, 321, 486, 30254, 4258, 264, 1329, 295, 4166, 51538], "temperature": 0.0, "avg_logprob": -0.1799036172720102, "compression_ratio": 1.691542288557214, "no_speech_prob": 0.0052167936228215694}, {"id": 122, "seek": 56556, "start": 589.04, "end": 592.76, "text": " ID that contains this user ID.", "tokens": [51538, 7348, 300, 8306, 341, 4195, 7348, 13, 51724], "temperature": 0.0, "avg_logprob": -0.1799036172720102, "compression_ratio": 1.691542288557214, "no_speech_prob": 0.0052167936228215694}, {"id": 123, "seek": 59276, "start": 592.76, "end": 596.16, "text": " So it is very fast.", "tokens": [50364, 407, 309, 307, 588, 2370, 13, 50534], "temperature": 0.0, "avg_logprob": -0.16637850320467384, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.0036575915291905403}, {"id": 124, "seek": 59276, "start": 596.16, "end": 603.8, "text": " And then you just have to retrieve a document from the list of whose document ID is.", "tokens": [50534, 400, 550, 291, 445, 362, 281, 30254, 257, 4166, 490, 264, 1329, 295, 6104, 4166, 7348, 307, 13, 50916], "temperature": 0.0, "avg_logprob": -0.16637850320467384, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.0036575915291905403}, {"id": 125, "seek": 59276, "start": 603.8, "end": 607.16, "text": " The third data structure is the column now store.", "tokens": [50916, 440, 2636, 1412, 3877, 307, 264, 7738, 586, 3531, 13, 51084], "temperature": 0.0, "avg_logprob": -0.16637850320467384, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.0036575915291905403}, {"id": 126, "seek": 59276, "start": 607.16, "end": 610.48, "text": " So here it's for doing aggregations.", "tokens": [51084, 407, 510, 309, 311, 337, 884, 16743, 763, 13, 51250], "temperature": 0.0, "avg_logprob": -0.16637850320467384, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.0036575915291905403}, {"id": 127, "seek": 59276, "start": 610.48, "end": 618.28, "text": " If you want to do analytics on your logs or traces, we will use this column now store.", "tokens": [51250, 759, 291, 528, 281, 360, 15370, 322, 428, 20820, 420, 26076, 11, 321, 486, 764, 341, 7738, 586, 3531, 13, 51640], "temperature": 0.0, "avg_logprob": -0.16637850320467384, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.0036575915291905403}, {"id": 128, "seek": 59276, "start": 618.28, "end": 619.92, "text": " You can have a lot of columns.", "tokens": [51640, 509, 393, 362, 257, 688, 295, 13766, 13, 51722], "temperature": 0.0, "avg_logprob": -0.16637850320467384, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.0036575915291905403}, {"id": 129, "seek": 59276, "start": 619.92, "end": 621.12, "text": " You can have spare columns.", "tokens": [51722, 509, 393, 362, 13798, 13766, 13, 51782], "temperature": 0.0, "avg_logprob": -0.16637850320467384, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.0036575915291905403}, {"id": 130, "seek": 62112, "start": 621.12, "end": 624.16, "text": " That's optimized for that.", "tokens": [50364, 663, 311, 26941, 337, 300, 13, 50516], "temperature": 0.0, "avg_logprob": -0.18305039405822754, "compression_ratio": 1.6631016042780749, "no_speech_prob": 0.2606547474861145}, {"id": 131, "seek": 62112, "start": 624.16, "end": 633.24, "text": " And the last part is what we call it's a split footer that we keep in general in the memory", "tokens": [50516, 400, 264, 1036, 644, 307, 437, 321, 818, 309, 311, 257, 7472, 2671, 260, 300, 321, 1066, 294, 2674, 294, 264, 4675, 50970], "temperature": 0.0, "avg_logprob": -0.18305039405822754, "compression_ratio": 1.6631016042780749, "no_speech_prob": 0.2606547474861145}, {"id": 132, "seek": 62112, "start": 633.24, "end": 636.0, "text": " of a searcher because it's very, very small.", "tokens": [50970, 295, 257, 3164, 260, 570, 309, 311, 588, 11, 588, 1359, 13, 51108], "temperature": 0.0, "avg_logprob": -0.18305039405822754, "compression_ratio": 1.6631016042780749, "no_speech_prob": 0.2606547474861145}, {"id": 133, "seek": 62112, "start": 636.0, "end": 642.44, "text": " I put it 0.07% of the size of a split.", "tokens": [51108, 286, 829, 309, 1958, 13, 16231, 4, 295, 264, 2744, 295, 257, 7472, 13, 51430], "temperature": 0.0, "avg_logprob": -0.18305039405822754, "compression_ratio": 1.6631016042780749, "no_speech_prob": 0.2606547474861145}, {"id": 134, "seek": 62112, "start": 642.44, "end": 644.08, "text": " So it's very, very small.", "tokens": [51430, 407, 309, 311, 588, 11, 588, 1359, 13, 51512], "temperature": 0.0, "avg_logprob": -0.18305039405822754, "compression_ratio": 1.6631016042780749, "no_speech_prob": 0.2606547474861145}, {"id": 135, "seek": 62112, "start": 644.08, "end": 649.96, "text": " For that, it's cool because you can always keep it in your cache on your searcher.", "tokens": [51512, 1171, 300, 11, 309, 311, 1627, 570, 291, 393, 1009, 1066, 309, 294, 428, 19459, 322, 428, 3164, 260, 13, 51806], "temperature": 0.0, "avg_logprob": -0.18305039405822754, "compression_ratio": 1.6631016042780749, "no_speech_prob": 0.2606547474861145}, {"id": 136, "seek": 64996, "start": 649.96, "end": 658.88, "text": " And in this hot cache, you will find all small pointers to the other data structures so that", "tokens": [50364, 400, 294, 341, 2368, 19459, 11, 291, 486, 915, 439, 1359, 44548, 281, 264, 661, 1412, 9227, 370, 300, 50810], "temperature": 0.0, "avg_logprob": -0.21842173408059512, "compression_ratio": 1.688118811881188, "no_speech_prob": 0.1438775211572647}, {"id": 137, "seek": 64996, "start": 658.88, "end": 664.4000000000001, "text": " when you make a search request, you will need only to make one or two requests to your object", "tokens": [50810, 562, 291, 652, 257, 3164, 5308, 11, 291, 486, 643, 787, 281, 652, 472, 420, 732, 12475, 281, 428, 2657, 51086], "temperature": 0.0, "avg_logprob": -0.21842173408059512, "compression_ratio": 1.688118811881188, "no_speech_prob": 0.1438775211572647}, {"id": 138, "seek": 64996, "start": 664.4000000000001, "end": 667.36, "text": " storage to find the response.", "tokens": [51086, 6725, 281, 915, 264, 4134, 13, 51234], "temperature": 0.0, "avg_logprob": -0.21842173408059512, "compression_ratio": 1.688118811881188, "no_speech_prob": 0.1438775211572647}, {"id": 139, "seek": 64996, "start": 667.36, "end": 675.48, "text": " So that's why I said when we optimized QuikWit for object storage, it's because of that because", "tokens": [51234, 407, 300, 311, 983, 286, 848, 562, 321, 26941, 2326, 1035, 54, 270, 337, 2657, 6725, 11, 309, 311, 570, 295, 300, 570, 51640], "temperature": 0.0, "avg_logprob": -0.21842173408059512, "compression_ratio": 1.688118811881188, "no_speech_prob": 0.1438775211572647}, {"id": 140, "seek": 64996, "start": 675.48, "end": 678.64, "text": " we optimized those pointers.", "tokens": [51640, 321, 26941, 729, 44548, 13, 51798], "temperature": 0.0, "avg_logprob": -0.21842173408059512, "compression_ratio": 1.688118811881188, "no_speech_prob": 0.1438775211572647}, {"id": 141, "seek": 67864, "start": 678.64, "end": 687.4399999999999, "text": " We put that in one footer that we can keep in cache.", "tokens": [50364, 492, 829, 300, 294, 472, 2671, 260, 300, 321, 393, 1066, 294, 19459, 13, 50804], "temperature": 0.0, "avg_logprob": -0.2309981107711792, "compression_ratio": 1.4277456647398843, "no_speech_prob": 0.09189968556165695}, {"id": 142, "seek": 67864, "start": 687.4399999999999, "end": 696.92, "text": " So just the next part now, I will explain you a bit how spans are stored in QuikWit.", "tokens": [50804, 407, 445, 264, 958, 644, 586, 11, 286, 486, 2903, 291, 257, 857, 577, 44086, 366, 12187, 294, 2326, 1035, 54, 270, 13, 51278], "temperature": 0.0, "avg_logprob": -0.2309981107711792, "compression_ratio": 1.4277456647398843, "no_speech_prob": 0.09189968556165695}, {"id": 143, "seek": 67864, "start": 696.92, "end": 698.42, "text": " Okay.", "tokens": [51278, 1033, 13, 51353], "temperature": 0.0, "avg_logprob": -0.2309981107711792, "compression_ratio": 1.4277456647398843, "no_speech_prob": 0.09189968556165695}, {"id": 144, "seek": 67864, "start": 698.42, "end": 704.36, "text": " I have only eight minutes left, so I need to speed up it.", "tokens": [51353, 286, 362, 787, 3180, 2077, 1411, 11, 370, 286, 643, 281, 3073, 493, 309, 13, 51650], "temperature": 0.0, "avg_logprob": -0.2309981107711792, "compression_ratio": 1.4277456647398843, "no_speech_prob": 0.09189968556165695}, {"id": 145, "seek": 67864, "start": 704.36, "end": 707.4399999999999, "text": " In QuikWit, you can model things as you want.", "tokens": [51650, 682, 2326, 1035, 54, 270, 11, 291, 393, 2316, 721, 382, 291, 528, 13, 51804], "temperature": 0.0, "avg_logprob": -0.2309981107711792, "compression_ratio": 1.4277456647398843, "no_speech_prob": 0.09189968556165695}, {"id": 146, "seek": 70744, "start": 707.44, "end": 709.36, "text": " You can put any documents.", "tokens": [50364, 509, 393, 829, 604, 8512, 13, 50460], "temperature": 0.0, "avg_logprob": -0.18927803818060426, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.20351973176002502}, {"id": 147, "seek": 70744, "start": 709.36, "end": 715.72, "text": " But for span generally, you want to stick to the open telemetry data model.", "tokens": [50460, 583, 337, 16174, 1337, 379, 11, 291, 528, 281, 2897, 281, 264, 1269, 4304, 5537, 627, 1412, 2316, 13, 50778], "temperature": 0.0, "avg_logprob": -0.18927803818060426, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.20351973176002502}, {"id": 148, "seek": 70744, "start": 715.72, "end": 722.2, "text": " So what we did for this demo is that we used the data model based on the open telemetry", "tokens": [50778, 407, 437, 321, 630, 337, 341, 10723, 307, 300, 321, 1143, 264, 1412, 2316, 2361, 322, 264, 1269, 4304, 5537, 627, 51102], "temperature": 0.0, "avg_logprob": -0.18927803818060426, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.20351973176002502}, {"id": 149, "seek": 70744, "start": 722.2, "end": 723.2, "text": " data model.", "tokens": [51102, 1412, 2316, 13, 51152], "temperature": 0.0, "avg_logprob": -0.18927803818060426, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.20351973176002502}, {"id": 150, "seek": 70744, "start": 723.2, "end": 729.32, "text": " Well, you have a bunch of fields that are always there.", "tokens": [51152, 1042, 11, 291, 362, 257, 3840, 295, 7909, 300, 366, 1009, 456, 13, 51458], "temperature": 0.0, "avg_logprob": -0.18927803818060426, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.20351973176002502}, {"id": 151, "seek": 70744, "start": 729.32, "end": 733.2800000000001, "text": " And you have also some dynamic fields like resource attributes and span attributes.", "tokens": [51458, 400, 291, 362, 611, 512, 8546, 7909, 411, 7684, 17212, 293, 16174, 17212, 13, 51656], "temperature": 0.0, "avg_logprob": -0.18927803818060426, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.20351973176002502}, {"id": 152, "seek": 70744, "start": 733.2800000000001, "end": 735.08, "text": " All those are very dynamic.", "tokens": [51656, 1057, 729, 366, 588, 8546, 13, 51746], "temperature": 0.0, "avg_logprob": -0.18927803818060426, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.20351973176002502}, {"id": 153, "seek": 73508, "start": 735.08, "end": 741.48, "text": " So I put some random examples here where you can generate random keys and random fields.", "tokens": [50364, 407, 286, 829, 512, 4974, 5110, 510, 689, 291, 393, 8460, 4974, 9317, 293, 4974, 7909, 13, 50684], "temperature": 0.0, "avg_logprob": -0.1815263032913208, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.02096090465784073}, {"id": 154, "seek": 73508, "start": 741.48, "end": 749.12, "text": " And here are the nice things that QuikWit is also shameless so that we can store every", "tokens": [50684, 400, 510, 366, 264, 1481, 721, 300, 2326, 1035, 54, 270, 307, 611, 40164, 370, 300, 321, 393, 3531, 633, 51066], "temperature": 0.0, "avg_logprob": -0.1815263032913208, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.02096090465784073}, {"id": 155, "seek": 73508, "start": 749.12, "end": 751.5200000000001, "text": " inverting index and the columnar storage.", "tokens": [51066, 28653, 783, 8186, 293, 264, 7738, 289, 6725, 13, 51186], "temperature": 0.0, "avg_logprob": -0.1815263032913208, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.02096090465784073}, {"id": 156, "seek": 73508, "start": 751.5200000000001, "end": 756.8000000000001, "text": " We can store all those dynamic fields without declaring which fields you have or which you", "tokens": [51186, 492, 393, 3531, 439, 729, 8546, 7909, 1553, 40374, 597, 7909, 291, 362, 420, 597, 291, 51450], "temperature": 0.0, "avg_logprob": -0.1815263032913208, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.02096090465784073}, {"id": 157, "seek": 73508, "start": 756.8000000000001, "end": 758.2, "text": " don't have.", "tokens": [51450, 500, 380, 362, 13, 51520], "temperature": 0.0, "avg_logprob": -0.1815263032913208, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.02096090465784073}, {"id": 158, "seek": 73508, "start": 758.2, "end": 764.2, "text": " It will index everything in the inverting index and in the columnar storage.", "tokens": [51520, 467, 486, 8186, 1203, 294, 264, 28653, 783, 8186, 293, 294, 264, 7738, 289, 6725, 13, 51820], "temperature": 0.0, "avg_logprob": -0.1815263032913208, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.02096090465784073}, {"id": 159, "seek": 76420, "start": 764.2, "end": 770.72, "text": " So it's nice when you don't know in advance all your attributes that you have on your", "tokens": [50364, 407, 309, 311, 1481, 562, 291, 500, 380, 458, 294, 7295, 439, 428, 17212, 300, 291, 362, 322, 428, 50690], "temperature": 0.0, "avg_logprob": -0.1607092883851793, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.032004352658987045}, {"id": 160, "seek": 76420, "start": 770.72, "end": 773.48, "text": " spans.", "tokens": [50690, 44086, 13, 50828], "temperature": 0.0, "avg_logprob": -0.1607092883851793, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.032004352658987045}, {"id": 161, "seek": 76420, "start": 773.48, "end": 776.5200000000001, "text": " So it's time for the demo.", "tokens": [50828, 407, 309, 311, 565, 337, 264, 10723, 13, 50980], "temperature": 0.0, "avg_logprob": -0.1607092883851793, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.032004352658987045}, {"id": 162, "seek": 76420, "start": 776.5200000000001, "end": 780.6, "text": " So for the demo, I prepared a demo for application monitoring.", "tokens": [50980, 407, 337, 264, 10723, 11, 286, 4927, 257, 10723, 337, 3861, 11028, 13, 51184], "temperature": 0.0, "avg_logprob": -0.1607092883851793, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.032004352658987045}, {"id": 163, "seek": 76420, "start": 780.6, "end": 789.6400000000001, "text": " So my first problem was generating spans, traces that are understandable for this kind of goal.", "tokens": [51184, 407, 452, 700, 1154, 390, 17746, 44086, 11, 26076, 300, 366, 25648, 337, 341, 733, 295, 3387, 13, 51636], "temperature": 0.0, "avg_logprob": -0.1607092883851793, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.032004352658987045}, {"id": 164, "seek": 78964, "start": 789.64, "end": 797.72, "text": " So I discovered recently a tool called, which is an extension of K6, which is like a project", "tokens": [50364, 407, 286, 6941, 3938, 257, 2290, 1219, 11, 597, 307, 364, 10320, 295, 591, 21, 11, 597, 307, 411, 257, 1716, 50768], "temperature": 0.0, "avg_logprob": -0.21425527075062628, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.06652086228132248}, {"id": 165, "seek": 78964, "start": 797.72, "end": 804.08, "text": " from Grafana for testing, for load testing.", "tokens": [50768, 490, 8985, 69, 2095, 337, 4997, 11, 337, 3677, 4997, 13, 51086], "temperature": 0.0, "avg_logprob": -0.21425527075062628, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.06652086228132248}, {"id": 166, "seek": 78964, "start": 804.08, "end": 806.64, "text": " And there is a nice extension to generate traces.", "tokens": [51086, 400, 456, 307, 257, 1481, 10320, 281, 8460, 26076, 13, 51214], "temperature": 0.0, "avg_logprob": -0.21425527075062628, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.06652086228132248}, {"id": 167, "seek": 78964, "start": 806.64, "end": 808.92, "text": " I will show you a bit or it works.", "tokens": [51214, 286, 486, 855, 291, 257, 857, 420, 309, 1985, 13, 51328], "temperature": 0.0, "avg_logprob": -0.21425527075062628, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.06652086228132248}, {"id": 168, "seek": 78964, "start": 808.92, "end": 815.96, "text": " And then I deployed a QuikWit cluster on Kubernetes and I did a Grafana instance to show you the", "tokens": [51328, 400, 550, 286, 17826, 257, 2326, 1035, 54, 270, 13630, 322, 23145, 293, 286, 630, 257, 8985, 69, 2095, 5197, 281, 855, 291, 264, 51680], "temperature": 0.0, "avg_logprob": -0.21425527075062628, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.06652086228132248}, {"id": 169, "seek": 78964, "start": 815.96, "end": 818.3199999999999, "text": " results.", "tokens": [51680, 3542, 13, 51798], "temperature": 0.0, "avg_logprob": -0.21425527075062628, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.06652086228132248}, {"id": 170, "seek": 81832, "start": 818.32, "end": 823.08, "text": " So a word on the XK6 extensions.", "tokens": [50364, 407, 257, 1349, 322, 264, 1783, 42, 21, 25129, 13, 50602], "temperature": 0.0, "avg_logprob": -0.22611370994931176, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.028261370956897736}, {"id": 171, "seek": 81832, "start": 823.08, "end": 827.0, "text": " It's a nice extension just to, you can declare some spans.", "tokens": [50602, 467, 311, 257, 1481, 10320, 445, 281, 11, 291, 393, 19710, 512, 44086, 13, 50798], "temperature": 0.0, "avg_logprob": -0.22611370994931176, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.028261370956897736}, {"id": 172, "seek": 81832, "start": 827.0, "end": 831.7600000000001, "text": " Like here I put some services like shop backend, ethical service.", "tokens": [50798, 1743, 510, 286, 829, 512, 3328, 411, 3945, 38087, 11, 18890, 2643, 13, 51036], "temperature": 0.0, "avg_logprob": -0.22611370994931176, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.028261370956897736}, {"id": 173, "seek": 81832, "start": 831.7600000000001, "end": 834.08, "text": " And you can declare whatever you want.", "tokens": [51036, 400, 291, 393, 19710, 2035, 291, 528, 13, 51152], "temperature": 0.0, "avg_logprob": -0.22611370994931176, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.028261370956897736}, {"id": 174, "seek": 81832, "start": 834.08, "end": 839.7600000000001, "text": " So it's a template and then you can set the cardinality.", "tokens": [51152, 407, 309, 311, 257, 12379, 293, 550, 291, 393, 992, 264, 2920, 259, 1860, 13, 51436], "temperature": 0.0, "avg_logprob": -0.22611370994931176, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.028261370956897736}, {"id": 175, "seek": 81832, "start": 839.7600000000001, "end": 842.32, "text": " You can set if you want some random attributes.", "tokens": [51436, 509, 393, 992, 498, 291, 528, 512, 4974, 17212, 13, 51564], "temperature": 0.0, "avg_logprob": -0.22611370994931176, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.028261370956897736}, {"id": 176, "seek": 81832, "start": 842.32, "end": 847.7600000000001, "text": " So it's pretty nice to stress test and see if your engine can handle high cardinality", "tokens": [51564, 407, 309, 311, 1238, 1481, 281, 4244, 1500, 293, 536, 498, 428, 2848, 393, 4813, 1090, 2920, 259, 1860, 51836], "temperature": 0.0, "avg_logprob": -0.22611370994931176, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.028261370956897736}, {"id": 177, "seek": 84776, "start": 847.76, "end": 851.56, "text": " fields, can handle like many random attributes.", "tokens": [50364, 7909, 11, 393, 4813, 411, 867, 4974, 17212, 13, 50554], "temperature": 0.0, "avg_logprob": -0.20369921503840266, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.013040335848927498}, {"id": 178, "seek": 84776, "start": 851.56, "end": 856.88, "text": " So it's pretty cool.", "tokens": [50554, 407, 309, 311, 1238, 1627, 13, 50820], "temperature": 0.0, "avg_logprob": -0.20369921503840266, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.013040335848927498}, {"id": 179, "seek": 84776, "start": 856.88, "end": 862.04, "text": " So let's do the demo because I prepared it's live.", "tokens": [50820, 407, 718, 311, 360, 264, 10723, 570, 286, 4927, 309, 311, 1621, 13, 51078], "temperature": 0.0, "avg_logprob": -0.20369921503840266, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.013040335848927498}, {"id": 180, "seek": 84776, "start": 862.04, "end": 863.04, "text": " Can you see it?", "tokens": [51078, 1664, 291, 536, 309, 30, 51128], "temperature": 0.0, "avg_logprob": -0.20369921503840266, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.013040335848927498}, {"id": 181, "seek": 84776, "start": 863.04, "end": 865.0, "text": " Maybe I can zoom it a bit.", "tokens": [51128, 2704, 286, 393, 8863, 309, 257, 857, 13, 51226], "temperature": 0.0, "avg_logprob": -0.20369921503840266, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.013040335848927498}, {"id": 182, "seek": 84776, "start": 865.0, "end": 872.64, "text": " So here I'm a heating or Kubernetes cluster and the index in which I'm setting traces.", "tokens": [51226, 407, 510, 286, 478, 257, 15082, 420, 23145, 13630, 293, 264, 8186, 294, 597, 286, 478, 3287, 26076, 13, 51608], "temperature": 0.0, "avg_logprob": -0.20369921503840266, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.013040335848927498}, {"id": 183, "seek": 87264, "start": 872.64, "end": 878.72, "text": " So we have approximately here now 341 million spans.", "tokens": [50364, 407, 321, 362, 10447, 510, 586, 12790, 16, 2459, 44086, 13, 50668], "temperature": 0.0, "avg_logprob": -0.16643363899654812, "compression_ratio": 1.4437869822485208, "no_speech_prob": 0.05293373018503189}, {"id": 184, "seek": 87264, "start": 878.72, "end": 882.08, "text": " So if I do a refresh, you will see this number moving.", "tokens": [50668, 407, 498, 286, 360, 257, 15134, 11, 291, 486, 536, 341, 1230, 2684, 13, 50836], "temperature": 0.0, "avg_logprob": -0.16643363899654812, "compression_ratio": 1.4437869822485208, "no_speech_prob": 0.05293373018503189}, {"id": 185, "seek": 87264, "start": 882.08, "end": 886.76, "text": " So now we have 355 millions of spans.", "tokens": [50836, 407, 586, 321, 362, 6976, 20, 6803, 295, 44086, 13, 51070], "temperature": 0.0, "avg_logprob": -0.16643363899654812, "compression_ratio": 1.4437869822485208, "no_speech_prob": 0.05293373018503189}, {"id": 186, "seek": 87264, "start": 886.76, "end": 890.4399999999999, "text": " So great.", "tokens": [51070, 407, 869, 13, 51254], "temperature": 0.0, "avg_logprob": -0.16643363899654812, "compression_ratio": 1.4437869822485208, "no_speech_prob": 0.05293373018503189}, {"id": 187, "seek": 87264, "start": 890.4399999999999, "end": 896.4, "text": " You can see that the uncompressed size of the document that are ingested in QuikWit have", "tokens": [51254, 509, 393, 536, 300, 264, 8585, 79, 3805, 2744, 295, 264, 4166, 300, 366, 3957, 21885, 294, 2326, 1035, 54, 270, 362, 51552], "temperature": 0.0, "avg_logprob": -0.16643363899654812, "compression_ratio": 1.4437869822485208, "no_speech_prob": 0.05293373018503189}, {"id": 188, "seek": 89640, "start": 896.4, "end": 901.68, "text": " around 200 and almost 300 gigabytes of size.", "tokens": [50364, 926, 2331, 293, 1920, 6641, 42741, 295, 2744, 13, 50628], "temperature": 0.0, "avg_logprob": -0.2631743923648373, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.08081237971782684}, {"id": 189, "seek": 89640, "start": 901.68, "end": 907.48, "text": " And that it's less, we compressed data a lot in QuikWit.", "tokens": [50628, 400, 300, 309, 311, 1570, 11, 321, 30353, 1412, 257, 688, 294, 2326, 1035, 54, 270, 13, 50918], "temperature": 0.0, "avg_logprob": -0.2631743923648373, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.08081237971782684}, {"id": 190, "seek": 89640, "start": 907.48, "end": 915.56, "text": " So here it's around you can divide by seven on this example, the size of the data ingested", "tokens": [50918, 407, 510, 309, 311, 926, 291, 393, 9845, 538, 3407, 322, 341, 1365, 11, 264, 2744, 295, 264, 1412, 3957, 21885, 51322], "temperature": 0.0, "avg_logprob": -0.2631743923648373, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.08081237971782684}, {"id": 191, "seek": 89640, "start": 915.56, "end": 916.84, "text": " in QuikWit.", "tokens": [51322, 294, 2326, 1035, 54, 270, 13, 51386], "temperature": 0.0, "avg_logprob": -0.2631743923648373, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.08081237971782684}, {"id": 192, "seek": 89640, "start": 916.84, "end": 924.4399999999999, "text": " So here the size of Publish Plits is the size that is taken on the JStorage by QuikWit.", "tokens": [51386, 407, 510, 264, 2744, 295, 21808, 1933, 2149, 1208, 307, 264, 2744, 300, 307, 2726, 322, 264, 508, 4520, 29226, 538, 2326, 1035, 54, 270, 13, 51766], "temperature": 0.0, "avg_logprob": -0.2631743923648373, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.08081237971782684}, {"id": 193, "seek": 92444, "start": 924.44, "end": 926.6400000000001, "text": " So what can we do?", "tokens": [50364, 407, 437, 393, 321, 360, 30, 50474], "temperature": 0.0, "avg_logprob": -0.16057231379490272, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.09057260304689407}, {"id": 194, "seek": 92444, "start": 926.6400000000001, "end": 932.5200000000001, "text": " So we are sending a lot of traces just to confirm that it's live.", "tokens": [50474, 407, 321, 366, 7750, 257, 688, 295, 26076, 445, 281, 9064, 300, 309, 311, 1621, 13, 50768], "temperature": 0.0, "avg_logprob": -0.16057231379490272, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.09057260304689407}, {"id": 195, "seek": 92444, "start": 932.5200000000001, "end": 937.2800000000001, "text": " So here it's just a dashboard based on QuikWit Prometheus Matrix.", "tokens": [50768, 407, 510, 309, 311, 445, 257, 18342, 2361, 322, 2326, 1035, 54, 270, 2114, 649, 42209, 36274, 13, 51006], "temperature": 0.0, "avg_logprob": -0.16057231379490272, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.09057260304689407}, {"id": 196, "seek": 92444, "start": 937.2800000000001, "end": 938.2800000000001, "text": " So it's live.", "tokens": [51006, 407, 309, 311, 1621, 13, 51056], "temperature": 0.0, "avg_logprob": -0.16057231379490272, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.09057260304689407}, {"id": 197, "seek": 92444, "start": 938.2800000000001, "end": 943.8000000000001, "text": " I launched it I think six hours earlier today.", "tokens": [51056, 286, 8730, 309, 286, 519, 2309, 2496, 3071, 965, 13, 51332], "temperature": 0.0, "avg_logprob": -0.16057231379490272, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.09057260304689407}, {"id": 198, "seek": 92444, "start": 943.8000000000001, "end": 948.2800000000001, "text": " So I'm just sending traces at 11 megabytes per second.", "tokens": [51332, 407, 286, 478, 445, 7750, 26076, 412, 2975, 10816, 24538, 680, 1150, 13, 51556], "temperature": 0.0, "avg_logprob": -0.16057231379490272, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.09057260304689407}, {"id": 199, "seek": 92444, "start": 948.2800000000001, "end": 953.8800000000001, "text": " That's not huge, but it's already pretty decent because it represents around one terabyte", "tokens": [51556, 663, 311, 406, 2603, 11, 457, 309, 311, 1217, 1238, 8681, 570, 309, 8855, 926, 472, 1796, 34529, 51836], "temperature": 0.0, "avg_logprob": -0.16057231379490272, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.09057260304689407}, {"id": 200, "seek": 95388, "start": 953.88, "end": 956.4, "text": " per day.", "tokens": [50364, 680, 786, 13, 50490], "temperature": 0.0, "avg_logprob": -0.1661149643279694, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.07219813019037247}, {"id": 201, "seek": 95388, "start": 956.4, "end": 960.8, "text": " And you can see that it is running with only one indexer.", "tokens": [50490, 400, 291, 393, 536, 300, 309, 307, 2614, 365, 787, 472, 8186, 260, 13, 50710], "temperature": 0.0, "avg_logprob": -0.1661149643279694, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.07219813019037247}, {"id": 202, "seek": 95388, "start": 960.8, "end": 968.24, "text": " And we have one indexer that is not really doing a lot of things.", "tokens": [50710, 400, 321, 362, 472, 8186, 260, 300, 307, 406, 534, 884, 257, 688, 295, 721, 13, 51082], "temperature": 0.0, "avg_logprob": -0.1661149643279694, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.07219813019037247}, {"id": 203, "seek": 95388, "start": 968.24, "end": 972.84, "text": " It is using between one CPU and two CPU here.", "tokens": [51082, 467, 307, 1228, 1296, 472, 13199, 293, 732, 13199, 510, 13, 51312], "temperature": 0.0, "avg_logprob": -0.1661149643279694, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.07219813019037247}, {"id": 204, "seek": 95388, "start": 972.84, "end": 978.92, "text": " The spikes are due to the compaction that we are doing because QuikWit generates a lot", "tokens": [51312, 440, 28997, 366, 3462, 281, 264, 715, 2894, 300, 321, 366, 884, 570, 2326, 1035, 54, 270, 23815, 257, 688, 51616], "temperature": 0.0, "avg_logprob": -0.1661149643279694, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.07219813019037247}, {"id": 205, "seek": 95388, "start": 978.92, "end": 980.04, "text": " of splits.", "tokens": [51616, 295, 37741, 13, 51672], "temperature": 0.0, "avg_logprob": -0.1661149643279694, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.07219813019037247}, {"id": 206, "seek": 95388, "start": 980.04, "end": 983.6, "text": " And so we need to merge them.", "tokens": [51672, 400, 370, 321, 643, 281, 22183, 552, 13, 51850], "temperature": 0.0, "avg_logprob": -0.1661149643279694, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.07219813019037247}, {"id": 207, "seek": 98360, "start": 983.6, "end": 988.48, "text": " We need to merge them so that we reduce the number of threads that we will do on the object", "tokens": [50364, 492, 643, 281, 22183, 552, 370, 300, 321, 5407, 264, 1230, 295, 19314, 300, 321, 486, 360, 322, 264, 2657, 50608], "temperature": 0.0, "avg_logprob": -0.15844374117643936, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.006686340551823378}, {"id": 208, "seek": 98360, "start": 988.48, "end": 992.0, "text": " storage.", "tokens": [50608, 6725, 13, 50784], "temperature": 0.0, "avg_logprob": -0.15844374117643936, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.006686340551823378}, {"id": 209, "seek": 98360, "start": 992.0, "end": 993.52, "text": " So great, it's working.", "tokens": [50784, 407, 869, 11, 309, 311, 1364, 13, 50860], "temperature": 0.0, "avg_logprob": -0.15844374117643936, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.006686340551823378}, {"id": 210, "seek": 98360, "start": 993.52, "end": 994.52, "text": " It's live.", "tokens": [50860, 467, 311, 1621, 13, 50910], "temperature": 0.0, "avg_logprob": -0.15844374117643936, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.006686340551823378}, {"id": 211, "seek": 98360, "start": 994.52, "end": 999.36, "text": " And I can show you that we can search traces with it.", "tokens": [50910, 400, 286, 393, 855, 291, 300, 321, 393, 3164, 26076, 365, 309, 13, 51152], "temperature": 0.0, "avg_logprob": -0.15844374117643936, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.006686340551823378}, {"id": 212, "seek": 98360, "start": 999.36, "end": 1001.36, "text": " So let's have a look.", "tokens": [51152, 407, 718, 311, 362, 257, 574, 13, 51252], "temperature": 0.0, "avg_logprob": -0.15844374117643936, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.006686340551823378}, {"id": 213, "seek": 98360, "start": 1001.36, "end": 1005.88, "text": " So I'm running a search query on all the documents.", "tokens": [51252, 407, 286, 478, 2614, 257, 3164, 14581, 322, 439, 264, 8512, 13, 51478], "temperature": 0.0, "avg_logprob": -0.15844374117643936, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.006686340551823378}, {"id": 214, "seek": 98360, "start": 1005.88, "end": 1007.6, "text": " So you have a lot of them here.", "tokens": [51478, 407, 291, 362, 257, 688, 295, 552, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15844374117643936, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.006686340551823378}, {"id": 215, "seek": 100760, "start": 1007.6, "end": 1013.88, "text": " You can see that per minute here you have one million spans.", "tokens": [50364, 509, 393, 536, 300, 680, 3456, 510, 291, 362, 472, 2459, 44086, 13, 50678], "temperature": 0.0, "avg_logprob": -0.17386058885224012, "compression_ratio": 1.2459016393442623, "no_speech_prob": 0.024076437577605247}, {"id": 216, "seek": 100760, "start": 1013.88, "end": 1015.6, "text": " So that's a lot.", "tokens": [50678, 407, 300, 311, 257, 688, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17386058885224012, "compression_ratio": 1.2459016393442623, "no_speech_prob": 0.024076437577605247}, {"id": 217, "seek": 100760, "start": 1015.6, "end": 1032.96, "text": " And we can focus on maybe I want to remove QuikWit traces.", "tokens": [50764, 400, 321, 393, 1879, 322, 1310, 286, 528, 281, 4159, 2326, 1035, 54, 270, 26076, 13, 51632], "temperature": 0.0, "avg_logprob": -0.17386058885224012, "compression_ratio": 1.2459016393442623, "no_speech_prob": 0.024076437577605247}, {"id": 218, "seek": 100760, "start": 1032.96, "end": 1033.96, "text": " It should work.", "tokens": [51632, 467, 820, 589, 13, 51682], "temperature": 0.0, "avg_logprob": -0.17386058885224012, "compression_ratio": 1.2459016393442623, "no_speech_prob": 0.024076437577605247}, {"id": 219, "seek": 103396, "start": 1033.96, "end": 1040.28, "text": " It's service name, I think.", "tokens": [50364, 467, 311, 2643, 1315, 11, 286, 519, 13, 50680], "temperature": 0.0, "avg_logprob": -0.2851568535913395, "compression_ratio": 1.452513966480447, "no_speech_prob": 0.12539370357990265}, {"id": 220, "seek": 103396, "start": 1040.28, "end": 1041.44, "text": " Okay, great.", "tokens": [50680, 1033, 11, 869, 13, 50738], "temperature": 0.0, "avg_logprob": -0.2851568535913395, "compression_ratio": 1.452513966480447, "no_speech_prob": 0.12539370357990265}, {"id": 221, "seek": 103396, "start": 1041.44, "end": 1047.48, "text": " So here, as we are sending QuikWit traces in QuikWit so that we can monitor our own cluster", "tokens": [50738, 407, 510, 11, 382, 321, 366, 7750, 2326, 1035, 54, 270, 26076, 294, 2326, 1035, 54, 270, 370, 300, 321, 393, 6002, 527, 1065, 13630, 51040], "temperature": 0.0, "avg_logprob": -0.2851568535913395, "compression_ratio": 1.452513966480447, "no_speech_prob": 0.12539370357990265}, {"id": 222, "seek": 103396, "start": 1047.48, "end": 1056.24, "text": " with it, so now I'm selecting only the traces that I'm generated for this demo.", "tokens": [51040, 365, 309, 11, 370, 586, 286, 478, 18182, 787, 264, 26076, 300, 286, 478, 10833, 337, 341, 10723, 13, 51478], "temperature": 0.0, "avg_logprob": -0.2851568535913395, "compression_ratio": 1.452513966480447, "no_speech_prob": 0.12539370357990265}, {"id": 223, "seek": 103396, "start": 1056.24, "end": 1061.72, "text": " So for example, we can look for query articles.", "tokens": [51478, 407, 337, 1365, 11, 321, 393, 574, 337, 14581, 11290, 13, 51752], "temperature": 0.0, "avg_logprob": -0.2851568535913395, "compression_ratio": 1.452513966480447, "no_speech_prob": 0.12539370357990265}, {"id": 224, "seek": 106172, "start": 1061.72, "end": 1073.3600000000001, "text": " So here, for example, what I can do is focus on one span ID.", "tokens": [50364, 407, 510, 11, 337, 1365, 11, 437, 286, 393, 360, 307, 1879, 322, 472, 16174, 7348, 13, 50946], "temperature": 0.0, "avg_logprob": -0.1750718593597412, "compression_ratio": 1.4225352112676057, "no_speech_prob": 0.02457946352660656}, {"id": 225, "seek": 106172, "start": 1073.3600000000001, "end": 1081.3600000000001, "text": " So as we have an inverting index, you can look for very accurate attributes.", "tokens": [50946, 407, 382, 321, 362, 364, 28653, 783, 8186, 11, 291, 393, 574, 337, 588, 8559, 17212, 13, 51346], "temperature": 0.0, "avg_logprob": -0.1750718593597412, "compression_ratio": 1.4225352112676057, "no_speech_prob": 0.02457946352660656}, {"id": 226, "seek": 106172, "start": 1081.3600000000001, "end": 1091.04, "text": " So here I took a span ID, but I guess we can do this on another.", "tokens": [51346, 407, 510, 286, 1890, 257, 16174, 7348, 11, 457, 286, 2041, 321, 393, 360, 341, 322, 1071, 13, 51830], "temperature": 0.0, "avg_logprob": -0.1750718593597412, "compression_ratio": 1.4225352112676057, "no_speech_prob": 0.02457946352660656}, {"id": 227, "seek": 109104, "start": 1091.04, "end": 1093.12, "text": " Let me check.", "tokens": [50364, 961, 385, 1520, 13, 50468], "temperature": 0.0, "avg_logprob": -0.3303842743237813, "compression_ratio": 1.3360655737704918, "no_speech_prob": 0.02547396533191204}, {"id": 228, "seek": 109104, "start": 1093.12, "end": 1106.08, "text": " I need to check article two card.", "tokens": [50468, 286, 643, 281, 1520, 7222, 732, 2920, 13, 51116], "temperature": 0.0, "avg_logprob": -0.3303842743237813, "compression_ratio": 1.3360655737704918, "no_speech_prob": 0.02547396533191204}, {"id": 229, "seek": 109104, "start": 1106.08, "end": 1113.8799999999999, "text": " So now I want to focus on this one because I added on this span particularly a high cardinality", "tokens": [51116, 407, 586, 286, 528, 281, 1879, 322, 341, 472, 570, 286, 3869, 322, 341, 16174, 4098, 257, 1090, 2920, 259, 1860, 51506], "temperature": 0.0, "avg_logprob": -0.3303842743237813, "compression_ratio": 1.3360655737704918, "no_speech_prob": 0.02547396533191204}, {"id": 230, "seek": 109104, "start": 1113.8799999999999, "end": 1116.1599999999999, "text": " field for the demo.", "tokens": [51506, 2519, 337, 264, 10723, 13, 51620], "temperature": 0.0, "avg_logprob": -0.3303842743237813, "compression_ratio": 1.3360655737704918, "no_speech_prob": 0.02547396533191204}, {"id": 231, "seek": 111616, "start": 1116.16, "end": 1124.44, "text": " So you can see here that you have this random attribute with this random value that is here.", "tokens": [50364, 407, 291, 393, 536, 510, 300, 291, 362, 341, 4974, 19667, 365, 341, 4974, 2158, 300, 307, 510, 13, 50778], "temperature": 0.0, "avg_logprob": -0.15279604513433914, "compression_ratio": 1.6836158192090396, "no_speech_prob": 0.10442335903644562}, {"id": 232, "seek": 111616, "start": 1124.44, "end": 1126.8000000000002, "text": " So I can focus on it.", "tokens": [50778, 407, 286, 393, 1879, 322, 309, 13, 50896], "temperature": 0.0, "avg_logprob": -0.15279604513433914, "compression_ratio": 1.6836158192090396, "no_speech_prob": 0.10442335903644562}, {"id": 233, "seek": 111616, "start": 1126.8000000000002, "end": 1129.8400000000001, "text": " And very fast I will get the results.", "tokens": [50896, 400, 588, 2370, 286, 486, 483, 264, 3542, 13, 51048], "temperature": 0.0, "avg_logprob": -0.15279604513433914, "compression_ratio": 1.6836158192090396, "no_speech_prob": 0.10442335903644562}, {"id": 234, "seek": 111616, "start": 1129.8400000000001, "end": 1136.3600000000001, "text": " You can see that this attribute value happens only very rarely.", "tokens": [51048, 509, 393, 536, 300, 341, 19667, 2158, 2314, 787, 588, 13752, 13, 51374], "temperature": 0.0, "avg_logprob": -0.15279604513433914, "compression_ratio": 1.6836158192090396, "no_speech_prob": 0.10442335903644562}, {"id": 235, "seek": 111616, "start": 1136.3600000000001, "end": 1142.8400000000001, "text": " And it's with a search engine, it's even faster when you have this kind of query.", "tokens": [51374, 400, 309, 311, 365, 257, 3164, 2848, 11, 309, 311, 754, 4663, 562, 291, 362, 341, 733, 295, 14581, 13, 51698], "temperature": 0.0, "avg_logprob": -0.15279604513433914, "compression_ratio": 1.6836158192090396, "no_speech_prob": 0.10442335903644562}, {"id": 236, "seek": 114284, "start": 1142.84, "end": 1152.4399999999998, "text": " So that's nice to search through spans, but generally you want to dig into one trace particularly.", "tokens": [50364, 407, 300, 311, 1481, 281, 3164, 807, 44086, 11, 457, 5101, 291, 528, 281, 2528, 666, 472, 13508, 4098, 13, 50844], "temperature": 0.0, "avg_logprob": -0.2853506462914603, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.026868760585784912}, {"id": 237, "seek": 114284, "start": 1152.4399999999998, "end": 1161.48, "text": " So you can use a Yeager plugin that is pointing at QuickWit cluster and it will return all", "tokens": [50844, 407, 291, 393, 764, 257, 835, 3557, 23407, 300, 307, 12166, 412, 12101, 54, 270, 13630, 293, 309, 486, 2736, 439, 51296], "temperature": 0.0, "avg_logprob": -0.2853506462914603, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.026868760585784912}, {"id": 238, "seek": 114284, "start": 1161.48, "end": 1164.04, "text": " the spans for the given trace.", "tokens": [51296, 264, 44086, 337, 264, 2212, 13508, 13, 51424], "temperature": 0.0, "avg_logprob": -0.2853506462914603, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.026868760585784912}, {"id": 239, "seek": 116404, "start": 1164.04, "end": 1172.92, "text": " So it's easy to go from one span to the whole traces.", "tokens": [50364, 407, 309, 311, 1858, 281, 352, 490, 472, 16174, 281, 264, 1379, 26076, 13, 50808], "temperature": 0.0, "avg_logprob": -0.11431614557902019, "compression_ratio": 1.6030150753768844, "no_speech_prob": 0.34347474575042725}, {"id": 240, "seek": 116404, "start": 1172.92, "end": 1177.28, "text": " But usually you want more from your data.", "tokens": [50808, 583, 2673, 291, 528, 544, 490, 428, 1412, 13, 51026], "temperature": 0.0, "avg_logprob": -0.11431614557902019, "compression_ratio": 1.6030150753768844, "no_speech_prob": 0.34347474575042725}, {"id": 241, "seek": 116404, "start": 1177.28, "end": 1181.32, "text": " And by that I mean you want to monitor your services.", "tokens": [51026, 400, 538, 300, 286, 914, 291, 528, 281, 6002, 428, 3328, 13, 51228], "temperature": 0.0, "avg_logprob": -0.11431614557902019, "compression_ratio": 1.6030150753768844, "no_speech_prob": 0.34347474575042725}, {"id": 242, "seek": 116404, "start": 1181.32, "end": 1186.72, "text": " Looking at one particular span is very nice, but when you know what you are looking for,", "tokens": [51228, 11053, 412, 472, 1729, 16174, 307, 588, 1481, 11, 457, 562, 291, 458, 437, 291, 366, 1237, 337, 11, 51498], "temperature": 0.0, "avg_logprob": -0.11431614557902019, "compression_ratio": 1.6030150753768844, "no_speech_prob": 0.34347474575042725}, {"id": 243, "seek": 116404, "start": 1186.72, "end": 1193.44, "text": " but when you don't know, it's better to build this kind of monitoring dashboard.", "tokens": [51498, 457, 562, 291, 500, 380, 458, 11, 309, 311, 1101, 281, 1322, 341, 733, 295, 11028, 18342, 13, 51834], "temperature": 0.0, "avg_logprob": -0.11431614557902019, "compression_ratio": 1.6030150753768844, "no_speech_prob": 0.34347474575042725}, {"id": 244, "seek": 119344, "start": 1193.44, "end": 1201.3600000000001, "text": " So here, okay, I will try to go a bit earlier in the past.", "tokens": [50364, 407, 510, 11, 1392, 11, 286, 486, 853, 281, 352, 257, 857, 3071, 294, 264, 1791, 13, 50760], "temperature": 0.0, "avg_logprob": -0.26305201212565105, "compression_ratio": 1.4425287356321839, "no_speech_prob": 0.13265541195869446}, {"id": 245, "seek": 119344, "start": 1201.3600000000001, "end": 1203.1200000000001, "text": " Yeah, good.", "tokens": [50760, 865, 11, 665, 13, 50848], "temperature": 0.0, "avg_logprob": -0.26305201212565105, "compression_ratio": 1.4425287356321839, "no_speech_prob": 0.13265541195869446}, {"id": 246, "seek": 119344, "start": 1203.1200000000001, "end": 1208.04, "text": " So I have set different panels.", "tokens": [50848, 407, 286, 362, 992, 819, 13419, 13, 51094], "temperature": 0.0, "avg_logprob": -0.26305201212565105, "compression_ratio": 1.4425287356321839, "no_speech_prob": 0.13265541195869446}, {"id": 247, "seek": 119344, "start": 1208.04, "end": 1213.92, "text": " The first one is like a data histogram and I'm counting HTTP requests.", "tokens": [51094, 440, 700, 472, 307, 411, 257, 1412, 49816, 293, 286, 478, 13251, 33283, 12475, 13, 51388], "temperature": 0.0, "avg_logprob": -0.26305201212565105, "compression_ratio": 1.4425287356321839, "no_speech_prob": 0.13265541195869446}, {"id": 248, "seek": 119344, "start": 1213.92, "end": 1219.0800000000002, "text": " So for calling HTTP requests, I'm using span attributes.", "tokens": [51388, 407, 337, 5141, 33283, 12475, 11, 286, 478, 1228, 16174, 17212, 13, 51646], "temperature": 0.0, "avg_logprob": -0.26305201212565105, "compression_ratio": 1.4425287356321839, "no_speech_prob": 0.13265541195869446}, {"id": 249, "seek": 119344, "start": 1219.0800000000002, "end": 1222.24, "text": " I can show you that.", "tokens": [51646, 286, 393, 855, 291, 300, 13, 51804], "temperature": 0.0, "avg_logprob": -0.26305201212565105, "compression_ratio": 1.4425287356321839, "no_speech_prob": 0.13265541195869446}, {"id": 250, "seek": 122224, "start": 1222.24, "end": 1226.1200000000001, "text": " So here I'm saying I want all HTTP requests.", "tokens": [50364, 407, 510, 286, 478, 1566, 286, 528, 439, 33283, 12475, 13, 50558], "temperature": 0.0, "avg_logprob": -0.1982658570071301, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.005373431835323572}, {"id": 251, "seek": 122224, "start": 1226.1200000000001, "end": 1229.36, "text": " So I'm using like the open telemetry semantic for that.", "tokens": [50558, 407, 286, 478, 1228, 411, 264, 1269, 4304, 5537, 627, 47982, 337, 300, 13, 50720], "temperature": 0.0, "avg_logprob": -0.1982658570071301, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.005373431835323572}, {"id": 252, "seek": 122224, "start": 1229.36, "end": 1233.04, "text": " And I'm saying, okay, I don't want all other spans.", "tokens": [50720, 400, 286, 478, 1566, 11, 1392, 11, 286, 500, 380, 528, 439, 661, 44086, 13, 50904], "temperature": 0.0, "avg_logprob": -0.1982658570071301, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.005373431835323572}, {"id": 253, "seek": 122224, "start": 1233.04, "end": 1239.64, "text": " I want only those ones who have at least one HTTP target.", "tokens": [50904, 286, 528, 787, 729, 2306, 567, 362, 412, 1935, 472, 33283, 3779, 13, 51234], "temperature": 0.0, "avg_logprob": -0.1982658570071301, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.005373431835323572}, {"id": 254, "seek": 122224, "start": 1239.64, "end": 1245.68, "text": " And here in the second panel, I'm doing like a group by by status code.", "tokens": [51234, 400, 510, 294, 264, 1150, 4831, 11, 286, 478, 884, 411, 257, 1594, 538, 538, 6558, 3089, 13, 51536], "temperature": 0.0, "avg_logprob": -0.1982658570071301, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.005373431835323572}, {"id": 255, "seek": 124568, "start": 1245.68, "end": 1253.72, "text": " So I'm using again like a dynamic attribute that is the status code.", "tokens": [50364, 407, 286, 478, 1228, 797, 411, 257, 8546, 19667, 300, 307, 264, 6558, 3089, 13, 50766], "temperature": 0.0, "avg_logprob": -0.16731837136404856, "compression_ratio": 1.6689189189189189, "no_speech_prob": 0.04196688160300255}, {"id": 256, "seek": 124568, "start": 1253.72, "end": 1262.4, "text": " And I'm doing a group by on it and building this data histogram.", "tokens": [50766, 400, 286, 478, 884, 257, 1594, 538, 322, 309, 293, 2390, 341, 1412, 49816, 13, 51200], "temperature": 0.0, "avg_logprob": -0.16731837136404856, "compression_ratio": 1.6689189189189189, "no_speech_prob": 0.04196688160300255}, {"id": 257, "seek": 124568, "start": 1262.4, "end": 1265.16, "text": " This one is a bit the same.", "tokens": [51200, 639, 472, 307, 257, 857, 264, 912, 13, 51338], "temperature": 0.0, "avg_logprob": -0.16731837136404856, "compression_ratio": 1.6689189189189189, "no_speech_prob": 0.04196688160300255}, {"id": 258, "seek": 124568, "start": 1265.16, "end": 1271.16, "text": " So here instead of doing a group by on a status code, I'm doing a group by on target.", "tokens": [51338, 407, 510, 2602, 295, 884, 257, 1594, 538, 322, 257, 6558, 3089, 11, 286, 478, 884, 257, 1594, 538, 322, 3779, 13, 51638], "temperature": 0.0, "avg_logprob": -0.16731837136404856, "compression_ratio": 1.6689189189189189, "no_speech_prob": 0.04196688160300255}, {"id": 259, "seek": 127116, "start": 1271.16, "end": 1276.44, "text": " So if you want to monitor your endpoints, that's useful.", "tokens": [50364, 407, 498, 291, 528, 281, 6002, 428, 917, 20552, 11, 300, 311, 4420, 13, 50628], "temperature": 0.0, "avg_logprob": -0.1926250245836046, "compression_ratio": 1.6108374384236452, "no_speech_prob": 0.08961108326911926}, {"id": 260, "seek": 127116, "start": 1276.44, "end": 1283.3200000000002, "text": " This one I can show you a bit more because here it's not the count.", "tokens": [50628, 639, 472, 286, 393, 855, 291, 257, 857, 544, 570, 510, 309, 311, 406, 264, 1207, 13, 50972], "temperature": 0.0, "avg_logprob": -0.1926250245836046, "compression_ratio": 1.6108374384236452, "no_speech_prob": 0.08961108326911926}, {"id": 261, "seek": 127116, "start": 1283.3200000000002, "end": 1289.0, "text": " We are computing the sum of the duration of each span.", "tokens": [50972, 492, 366, 15866, 264, 2408, 295, 264, 16365, 295, 1184, 16174, 13, 51256], "temperature": 0.0, "avg_logprob": -0.1926250245836046, "compression_ratio": 1.6108374384236452, "no_speech_prob": 0.08961108326911926}, {"id": 262, "seek": 127116, "start": 1289.0, "end": 1294.0, "text": " So I'm doing group by on each target and I'm summing all the span duration.", "tokens": [51256, 407, 286, 478, 884, 1594, 538, 322, 1184, 3779, 293, 286, 478, 2408, 2810, 439, 264, 16174, 16365, 13, 51506], "temperature": 0.0, "avg_logprob": -0.1926250245836046, "compression_ratio": 1.6108374384236452, "no_speech_prob": 0.08961108326911926}, {"id": 263, "seek": 127116, "start": 1294.0, "end": 1299.0800000000002, "text": " So you have a good feeling about the time taken by each of your target,", "tokens": [51506, 407, 291, 362, 257, 665, 2633, 466, 264, 565, 2726, 538, 1184, 295, 428, 3779, 11, 51760], "temperature": 0.0, "avg_logprob": -0.1926250245836046, "compression_ratio": 1.6108374384236452, "no_speech_prob": 0.08961108326911926}, {"id": 264, "seek": 129908, "start": 1299.08, "end": 1302.36, "text": " your each endpoint.", "tokens": [50364, 428, 1184, 35795, 13, 50528], "temperature": 0.0, "avg_logprob": -0.2943196689381319, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.010134330950677395}, {"id": 265, "seek": 129908, "start": 1302.36, "end": 1303.72, "text": " But that's not enough, right?", "tokens": [50528, 583, 300, 311, 406, 1547, 11, 558, 30, 50596], "temperature": 0.0, "avg_logprob": -0.2943196689381319, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.010134330950677395}, {"id": 266, "seek": 129908, "start": 1303.72, "end": 1305.8, "text": " You want something that is more useful.", "tokens": [50596, 509, 528, 746, 300, 307, 544, 4420, 13, 50700], "temperature": 0.0, "avg_logprob": -0.2943196689381319, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.010134330950677395}, {"id": 267, "seek": 129908, "start": 1305.8, "end": 1310.08, "text": " It's nice to see this, but it's not enough to take a decision like is there a problem or not.", "tokens": [50700, 467, 311, 1481, 281, 536, 341, 11, 457, 309, 311, 406, 1547, 281, 747, 257, 3537, 411, 307, 456, 257, 1154, 420, 406, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2943196689381319, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.010134330950677395}, {"id": 268, "seek": 129908, "start": 1311.3999999999999, "end": 1319.0, "text": " The last one is the P95 latency panel is more for that.", "tokens": [50980, 440, 1036, 472, 307, 264, 430, 15718, 27043, 4831, 307, 544, 337, 300, 13, 51360], "temperature": 0.0, "avg_logprob": -0.2943196689381319, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.010134330950677395}, {"id": 269, "seek": 129908, "start": 1319.0, "end": 1324.4399999999998, "text": " So you can see that you have a nice, smooth latency,", "tokens": [51360, 407, 291, 393, 536, 300, 291, 362, 257, 1481, 11, 5508, 27043, 11, 51632], "temperature": 0.0, "avg_logprob": -0.2943196689381319, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.010134330950677395}, {"id": 270, "seek": 132444, "start": 1324.44, "end": 1328.76, "text": " a P95 latency on all your services, except for this one.", "tokens": [50364, 257, 430, 15718, 27043, 322, 439, 428, 3328, 11, 3993, 337, 341, 472, 13, 50580], "temperature": 0.0, "avg_logprob": -0.36959645285535214, "compression_ratio": 1.5, "no_speech_prob": 0.015320700593292713}, {"id": 271, "seek": 132444, "start": 1330.6000000000001, "end": 1335.6000000000001, "text": " This is normal because I sent some traces with different latencies.", "tokens": [50672, 639, 307, 2710, 570, 286, 2279, 512, 26076, 365, 819, 4465, 6464, 13, 50922], "temperature": 0.0, "avg_logprob": -0.36959645285535214, "compression_ratio": 1.5, "no_speech_prob": 0.015320700593292713}, {"id": 272, "seek": 132444, "start": 1339.4, "end": 1340.68, "text": " So we will dig into that.", "tokens": [51112, 407, 321, 486, 2528, 666, 300, 13, 51176], "temperature": 0.0, "avg_logprob": -0.36959645285535214, "compression_ratio": 1.5, "no_speech_prob": 0.015320700593292713}, {"id": 273, "seek": 132444, "start": 1342.04, "end": 1345.64, "text": " So we know that maybe there is a problem here.", "tokens": [51244, 407, 321, 458, 300, 1310, 456, 307, 257, 1154, 510, 13, 51424], "temperature": 0.0, "avg_logprob": -0.36959645285535214, "compression_ratio": 1.5, "no_speech_prob": 0.015320700593292713}, {"id": 274, "seek": 132444, "start": 1347.4, "end": 1352.2, "text": " The panel average latency here is just the average.", "tokens": [51512, 440, 4831, 4274, 27043, 510, 307, 445, 264, 4274, 13, 51752], "temperature": 0.0, "avg_logprob": -0.36959645285535214, "compression_ratio": 1.5, "no_speech_prob": 0.015320700593292713}, {"id": 275, "seek": 135220, "start": 1352.2, "end": 1354.44, "text": " I think you understand that.", "tokens": [50364, 286, 519, 291, 1223, 300, 13, 50476], "temperature": 0.0, "avg_logprob": -0.2803124189376831, "compression_ratio": 1.5207373271889402, "no_speech_prob": 0.005635029170662165}, {"id": 276, "seek": 135220, "start": 1354.44, "end": 1359.96, "text": " And this one is also interesting because I'm sending spans from two virtual data centers.", "tokens": [50476, 400, 341, 472, 307, 611, 1880, 570, 286, 478, 7750, 44086, 490, 732, 6374, 1412, 10898, 13, 50752], "temperature": 0.0, "avg_logprob": -0.2803124189376831, "compression_ratio": 1.5207373271889402, "no_speech_prob": 0.005635029170662165}, {"id": 277, "seek": 135220, "start": 1359.96, "end": 1364.96, "text": " So one is CDG and one is a BIHRU.", "tokens": [50752, 407, 472, 307, 6743, 38, 293, 472, 307, 257, 23524, 39, 49, 52, 13, 51002], "temperature": 0.0, "avg_logprob": -0.2803124189376831, "compression_ratio": 1.5207373271889402, "no_speech_prob": 0.005635029170662165}, {"id": 278, "seek": 135220, "start": 1364.96, "end": 1369.0, "text": " And so you can see that most of the spans are coming from CDG here.", "tokens": [51002, 400, 370, 291, 393, 536, 300, 881, 295, 264, 44086, 366, 1348, 490, 6743, 38, 510, 13, 51204], "temperature": 0.0, "avg_logprob": -0.2803124189376831, "compression_ratio": 1.5207373271889402, "no_speech_prob": 0.005635029170662165}, {"id": 279, "seek": 135220, "start": 1370.32, "end": 1377.4, "text": " So for example, if we want to dig into why there is a problem on the P95 latency,", "tokens": [51270, 407, 337, 1365, 11, 498, 321, 528, 281, 2528, 666, 983, 456, 307, 257, 1154, 322, 264, 430, 15718, 27043, 11, 51624], "temperature": 0.0, "avg_logprob": -0.2803124189376831, "compression_ratio": 1.5207373271889402, "no_speech_prob": 0.005635029170662165}, {"id": 280, "seek": 135220, "start": 1378.6000000000001, "end": 1379.8, "text": " and I will stop after that.", "tokens": [51684, 293, 286, 486, 1590, 934, 300, 13, 51744], "temperature": 0.0, "avg_logprob": -0.2803124189376831, "compression_ratio": 1.5207373271889402, "no_speech_prob": 0.005635029170662165}, {"id": 281, "seek": 137980, "start": 1380.36, "end": 1387.0, "text": " I will just show you that you can add your query here and say,", "tokens": [50392, 286, 486, 445, 855, 291, 300, 291, 393, 909, 428, 14581, 510, 293, 584, 11, 50724], "temperature": 0.0, "avg_logprob": -0.345909784006518, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.011166999116539955}, {"id": 282, "seek": 137980, "start": 1387.0, "end": 1393.8, "text": " okay, I want to look at all the spans that are over one second and one second, 0.1 seconds.", "tokens": [50724, 1392, 11, 286, 528, 281, 574, 412, 439, 264, 44086, 300, 366, 670, 472, 1150, 293, 472, 1150, 11, 1958, 13, 16, 3949, 13, 51064], "temperature": 0.0, "avg_logprob": -0.345909784006518, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.011166999116539955}, {"id": 283, "seek": 137980, "start": 1396.2, "end": 1401.3999999999999, "text": " And so you can see that only the endpoint article, so card is problematic.", "tokens": [51184, 400, 370, 291, 393, 536, 300, 787, 264, 35795, 7222, 11, 370, 2920, 307, 19011, 13, 51444], "temperature": 0.0, "avg_logprob": -0.345909784006518, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.011166999116539955}, {"id": 284, "seek": 137980, "start": 1401.3999999999999, "end": 1407.3999999999999, "text": " You can see that maybe there is a problem in the data center because you have suddenly more requests here.", "tokens": [51444, 509, 393, 536, 300, 1310, 456, 307, 257, 1154, 294, 264, 1412, 3056, 570, 291, 362, 5800, 544, 12475, 510, 13, 51744], "temperature": 0.0, "avg_logprob": -0.345909784006518, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.011166999116539955}, {"id": 285, "seek": 140740, "start": 1407.8000000000002, "end": 1415.8000000000002, "text": " And of course, if you go up to two seconds, then you see that here the color has changed,", "tokens": [50384, 400, 295, 1164, 11, 498, 291, 352, 493, 281, 732, 3949, 11, 550, 291, 536, 300, 510, 264, 2017, 575, 3105, 11, 50784], "temperature": 0.0, "avg_logprob": -0.2509173169548129, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.009996033273637295}, {"id": 286, "seek": 140740, "start": 1415.8000000000002, "end": 1418.6000000000001, "text": " but it's coming from this data center.", "tokens": [50784, 457, 309, 311, 1348, 490, 341, 1412, 3056, 13, 50924], "temperature": 0.0, "avg_logprob": -0.2509173169548129, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.009996033273637295}, {"id": 287, "seek": 140740, "start": 1419.4, "end": 1427.8000000000002, "text": " And if you want to dig into one particular traces, you can open like and have a look at it.", "tokens": [50964, 400, 498, 291, 528, 281, 2528, 666, 472, 1729, 26076, 11, 291, 393, 1269, 411, 293, 362, 257, 574, 412, 309, 13, 51384], "temperature": 0.0, "avg_logprob": -0.2509173169548129, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.009996033273637295}, {"id": 288, "seek": 140740, "start": 1429.0, "end": 1430.6000000000001, "text": " So that's it.", "tokens": [51444, 407, 300, 311, 309, 13, 51524], "temperature": 0.0, "avg_logprob": -0.2509173169548129, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.009996033273637295}, {"id": 289, "seek": 140740, "start": 1432.2, "end": 1435.0, "text": " I think I will stop there because time is up.", "tokens": [51604, 286, 519, 286, 486, 1590, 456, 570, 565, 307, 493, 13, 51744], "temperature": 0.0, "avg_logprob": -0.2509173169548129, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.009996033273637295}, {"id": 290, "seek": 143500, "start": 1435.4, "end": 1438.6, "text": " If you have questions, come and see me.", "tokens": [50384, 759, 291, 362, 1651, 11, 808, 293, 536, 385, 13, 50544], "temperature": 0.0, "avg_logprob": -0.25474334716796876, "compression_ratio": 1.3416666666666666, "no_speech_prob": 0.015970300883054733}, {"id": 291, "seek": 143500, "start": 1438.6, "end": 1441.0, "text": " I will be happy to answer them.", "tokens": [50544, 286, 486, 312, 2055, 281, 1867, 552, 13, 50664], "temperature": 0.0, "avg_logprob": -0.25474334716796876, "compression_ratio": 1.3416666666666666, "no_speech_prob": 0.015970300883054733}, {"id": 292, "seek": 143500, "start": 1441.0, "end": 1445.0, "text": " And also I have some stickers and hoodies, so don't hesitate.", "tokens": [50664, 400, 611, 286, 362, 512, 21019, 293, 1106, 6087, 11, 370, 500, 380, 20842, 13, 50864], "temperature": 0.0, "avg_logprob": -0.25474334716796876, "compression_ratio": 1.3416666666666666, "no_speech_prob": 0.015970300883054733}, {"id": 293, "seek": 143500, "start": 1445.0, "end": 1447.0, "text": " I'm happy to give you some.", "tokens": [50864, 286, 478, 2055, 281, 976, 291, 512, 13, 50964], "temperature": 0.0, "avg_logprob": -0.25474334716796876, "compression_ratio": 1.3416666666666666, "no_speech_prob": 0.015970300883054733}], "language": "en"}