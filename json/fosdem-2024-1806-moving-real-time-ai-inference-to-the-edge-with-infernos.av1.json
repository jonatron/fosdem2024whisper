{"text": " Hi everybody, thanks for the wait, sorry, a bit of technical challenges, but now we can, this is the last talk related to AI, to AI staff, and I think it's also a nice continuation from Saou's first presentation, so we'll have Maxime coming from all the way from Vancouver, I think probably the farthest participant here today, so please go ahead, thank you. Okay, thank you, thank you Lorenzo, thanks everyone for attending my presentation. So let's get started, today I'm going to talk about a little bit some of our newest work on the eyesight of things, we've been a little bit about myself, I'm born and raised in Ukraine, I have masters in physics and radio physics from Key State University, I live in Vancouver for about 20 years now, father of three, and I've been involved in CIPA in various forms and projects and whatnot since about 2003, so basically since that first day I give a little bit of background of what me and open source, today I discovered free BSD in my university at some like second year, got very curious, started exploring and eventually submitted patches and all that, got accepted as developer, and then found CIPA Express router which is kind of later became came earlier in OpenCIP, added some modules, we use it extensively, I also created Pproxy, which people who use CIPA probably know about, and yeah, I get busy with various open source projects to this day. Speaking of machine learning, I started in about 2010, read this really nice book, it's actually free book, it's at 99 sort of age, but surprisingly got pretty good basic introduction into neural networks and all that, I already got a little bit curious about that, and then as time went on, it became more available, I trained a little toy model just trying to figure out if I can detect DTMF from 729 encode frames essentially, because 729 is just a bunch of floating point of coefficient, so you can feed it to the model and get sort of DTMF detection that way, it was just kind of weekend type of fun project. I also got installed a little AI powered ADAS called Coma 2, and it's open source project also, the user AI model to drive your car, you can really like install it yourself, you can hack it, and it's pretty nice, a good community, I participated in some events there. Also played with DeepFake lab, which kind of DeepFake framework, which allows you to work faces, also involves some training, so it was curious, and also lately I've been in San Diego, we've been doing like building a little from scratch, we've been building model which will drive a little robot across the office, also was quite fun, lately I've been also playing with Mu Zero support, first for the game of Digger, which I maintain, Linux port, it was pretty fun to sell highly interested, this is just the device that drives my car, and it runs open source AI model inside that little chip. So anyway, back to the main point of this talk, we're basically looking at that from our perspective, which is trying to build a system which is, we basically have a lot of customers who route calls with us, so we try to build a model that can be scaled out for provider level, not a model but software which can run those models. So basically the idea behind this project is that we try to figure out how to scale those models on reasonably priced hardware, so as the problem with the area right now, we didn't kind of vacuum tube error, as I call it, machine learning, because we have very expensive hardware for, you know, to run something mixed trial, you need, you know, like 64 gigabytes of GPU, and that is very easy, you need several of them or you need a very expensive one, but eventually all this stuff will get more affordable, so we're trying to kind of work towards that goal. So right now there are like two major frameworks which people use, which torsion and tether flow, and they are also pretty heavy as well, it's like hundreds of megabytes, if not gigabytes, to get stuff running, but it looks like at least from my perspective, in a few years we'll see some changes, already, see people working on alternative frameworks that are expected to be more lightweight and more flexible in terms of environment, because Python is not very easy to scale and integrate into something like CAPIs, although doable, but anyway. So we started earlier this year with the project, and the original idea was to just, for starters, we tried to implement text-to-speech, we already took our SIP stack, which is conveniently Python based, so that was pretty easy, pretty much like 20 lines of code to get SIP endpoint implemented RTP generation thread and tried to run those models. Essentially my, yeah, so basically we started with this guy, and it's like four gigabytes, cart, Nvidia 1050, and I was able to run like one channel essentially on it, of text-to-speech, and then obviously the next thing was like how to scale it, oh no, hold on, yeah, a little bit into how this works, essentially. So text-to-speech is, at least with transformers, you basically take your text, you send it to your model, and then it runs multiple iterations, so you basically have all your stuff through the model, and then it spills out something like male spectrum, which you put through Valkoder, and get the audio out. And then the first problem we run out into is that it basically required this run for the whole duration of the audio, so essentially on my small GPU it would take quite a lot of time to actually produce audio. So I had to modify it's a sph-t5 model, it's like not the latest, but one of the pretty good ones from Microsoft a few years ago, they released it, so I had it to modify it. I mean I re-wrote the model itself, not the model itself, but the pipeline, so instead of processing the whole audio in one go, I made it so it spits out those smaller chunks. The unfortunate problem came with that is it started like clicking, because Valkoder probably was not trained for this mode, so I tried to retrain the Valkoder, it did not go well, very well, so it did not produce a good result, so I had to build a little kind of post Valkoder scene, which smooths out essentially and fixes those clicks. And now it sounds pretty well, I will maybe play some examples when I get finished with the stuff. And then how I tried to scale it, so I got kind of normal size GPU, I would say, so it's like 16 gigabytes card, and I expected to get like maybe 10 times, 20 times performance, just looking at the spec, but to my surprise I only got like two times more performance, so I can only like with this model, I can only run two real-time threads of TTS on the bigger card. So I started looking into why this is happening and how to improve, because theoretically it has much more performance. Turns out that in order to get good performance out of those models you need to use batch inference, so instead of generating like each prompt, each audio in each one session you batch your prompts that you need to voice out and submit it into the model, and then it generates all those streams at the same kind of cost. Because my main problem with using GPUs is that they are pretty widely computationally, but they are pretty expensive to send, so it's like you're operating through very slow network with a very fast device, so you need to load a lot of, well not a lot, but several jobs to it. And so I considered like several ways how to batch it, so my first idea was that maybe I can just vary the size of the batch, so I run it in continuously, but then as sessions come and go I can add or remove them to this running batch. Unfortunately this does not work with the sequence to sequence model, because internally it kind of clocks itself, so you cannot really add another session of it, so they should be running all of them at the same time, so essentially you need to do something like this, so you batch a bunch of sentences and that you need to generate, pump it up and wait for them, all of them to finish, and then at the time you can collect a new request, then you batch them up and repeat again, and obviously if you have like pretty powerful GPU you can probably run a few of them, or if you have several GPUs you can improve latency by running on multiple of them. Yeah, so that part kind of works. The next thing I'm right now working on kind of in the other way, so we need to have something like Whisper to do the other way around, and that one already supports batching, so basically should be pretty scalable there as well, so right now I have on that $300 card I can do 50 sessions of text-to-speech in real time at the same time, so it's pretty good result because this is all running locally, so I don't use any anything pretty much and I can run it on a reasonably small device. And yeah, I guess the last thing that I played recently was there is a framework called Ray. It's basically when you can build a little cluster of machines with different kind of, well maybe the same GPU, maybe different hardware, and distribute your training or inference work over them, so that's just me running and not 20 probably games of digger. It's basically, all of this is a model doing inference, just looking at the screen doing inference saying where this should go basically, and kind of training itself to win at some point of time. So yeah, Ray, yeah this is like a CCH training, kind of improving a little bit, maybe, but anyway, a good interesting part of it, I figured out how to use that Ray, so it's kind of useful open source framework that you can kind of scale, use to scale up your AI project, so I'll probably use some of it to distribute. Yeah, there are some links and I guess I have probably a few minutes for questions. Yeah, yeah, yeah we can do video technically because it's all about, you know, as soon as we have like the whole mechanism set up, we can do video as well, so yeah. Right now I'm using PyTorch, but oh okay, the question is what kind of model can they run? Right now I'm running with this existing code, I'm using PyTorch, but I also played with TinyGrad, so I might use some of it as well because as I said, it's kind of very lightweight, so the whole goal of the guy who wrote it is to keep like usable framework in like 5,000 lines of Python code, so it's kind of very interesting from that perspective, but yeah, it's not really limited, it could use anything, right? I think another question, no? I have one. If you have another question, please give a round of applause.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.72, "text": " Hi everybody, thanks for the wait, sorry, a bit of technical challenges, but now we can,", "tokens": [50364, 2421, 2201, 11, 3231, 337, 264, 1699, 11, 2597, 11, 257, 857, 295, 6191, 4759, 11, 457, 586, 321, 393, 11, 50900], "temperature": 0.0, "avg_logprob": -0.3201608657836914, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.22841544449329376}, {"id": 1, "seek": 0, "start": 10.72, "end": 16.240000000000002, "text": " this is the last talk related to AI, to AI staff, and I think it's also a nice continuation", "tokens": [50900, 341, 307, 264, 1036, 751, 4077, 281, 7318, 11, 281, 7318, 3525, 11, 293, 286, 519, 309, 311, 611, 257, 1481, 29357, 51176], "temperature": 0.0, "avg_logprob": -0.3201608657836914, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.22841544449329376}, {"id": 2, "seek": 0, "start": 16.240000000000002, "end": 21.96, "text": " from Saou's first presentation, so we'll have Maxime coming from all the way from Vancouver,", "tokens": [51176, 490, 6299, 263, 311, 700, 5860, 11, 370, 321, 603, 362, 7402, 1312, 1348, 490, 439, 264, 636, 490, 26563, 11, 51462], "temperature": 0.0, "avg_logprob": -0.3201608657836914, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.22841544449329376}, {"id": 3, "seek": 2196, "start": 21.96, "end": 28.560000000000002, "text": " I think probably the farthest participant here today, so please go ahead, thank you.", "tokens": [50364, 286, 519, 1391, 264, 1400, 36356, 24950, 510, 965, 11, 370, 1767, 352, 2286, 11, 1309, 291, 13, 50694], "temperature": 0.0, "avg_logprob": -0.251881350003756, "compression_ratio": 1.48, "no_speech_prob": 0.11875308305025101}, {"id": 4, "seek": 2196, "start": 28.560000000000002, "end": 36.28, "text": " Okay, thank you, thank you Lorenzo, thanks everyone for attending my presentation.", "tokens": [50694, 1033, 11, 1309, 291, 11, 1309, 291, 37162, 4765, 11, 3231, 1518, 337, 15862, 452, 5860, 13, 51080], "temperature": 0.0, "avg_logprob": -0.251881350003756, "compression_ratio": 1.48, "no_speech_prob": 0.11875308305025101}, {"id": 5, "seek": 2196, "start": 36.28, "end": 45.88, "text": " So let's get started, today I'm going to talk about a little bit some of our newest work on", "tokens": [51080, 407, 718, 311, 483, 1409, 11, 965, 286, 478, 516, 281, 751, 466, 257, 707, 857, 512, 295, 527, 17569, 589, 322, 51560], "temperature": 0.0, "avg_logprob": -0.251881350003756, "compression_ratio": 1.48, "no_speech_prob": 0.11875308305025101}, {"id": 6, "seek": 4588, "start": 46.84, "end": 52.56, "text": " the eyesight of things, we've been a little bit about myself, I'm born and raised in Ukraine,", "tokens": [50412, 264, 49887, 295, 721, 11, 321, 600, 668, 257, 707, 857, 466, 2059, 11, 286, 478, 4232, 293, 6005, 294, 14081, 11, 50698], "temperature": 0.0, "avg_logprob": -0.3012505920839981, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.13283595442771912}, {"id": 7, "seek": 4588, "start": 52.56, "end": 59.28, "text": " I have masters in physics and radio physics from Key State University, I live in Vancouver for about", "tokens": [50698, 286, 362, 19294, 294, 10649, 293, 6477, 10649, 490, 12759, 4533, 3535, 11, 286, 1621, 294, 26563, 337, 466, 51034], "temperature": 0.0, "avg_logprob": -0.3012505920839981, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.13283595442771912}, {"id": 8, "seek": 4588, "start": 59.28, "end": 71.28, "text": " 20 years now, father of three, and I've been involved in CIPA in various forms and projects", "tokens": [51034, 945, 924, 586, 11, 3086, 295, 1045, 11, 293, 286, 600, 668, 3288, 294, 383, 9139, 32, 294, 3683, 6422, 293, 4455, 51634], "temperature": 0.0, "avg_logprob": -0.3012505920839981, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.13283595442771912}, {"id": 9, "seek": 7128, "start": 71.28, "end": 79.28, "text": " and whatnot since about 2003, so basically since that first day I give a little bit of", "tokens": [50364, 293, 25882, 1670, 466, 16416, 11, 370, 1936, 1670, 300, 700, 786, 286, 976, 257, 707, 857, 295, 50764], "temperature": 0.0, "avg_logprob": -0.29862626932435116, "compression_ratio": 1.424731182795699, "no_speech_prob": 0.046517226845026016}, {"id": 10, "seek": 7128, "start": 79.28, "end": 88.44, "text": " background of what me and open source, today I discovered free BSD in my university at some", "tokens": [50764, 3678, 295, 437, 385, 293, 1269, 4009, 11, 965, 286, 6941, 1737, 363, 23969, 294, 452, 5454, 412, 512, 51222], "temperature": 0.0, "avg_logprob": -0.29862626932435116, "compression_ratio": 1.424731182795699, "no_speech_prob": 0.046517226845026016}, {"id": 11, "seek": 7128, "start": 88.44, "end": 97.16, "text": " like second year, got very curious, started exploring and eventually submitted patches", "tokens": [51222, 411, 1150, 1064, 11, 658, 588, 6369, 11, 1409, 12736, 293, 4728, 14405, 26531, 51658], "temperature": 0.0, "avg_logprob": -0.29862626932435116, "compression_ratio": 1.424731182795699, "no_speech_prob": 0.046517226845026016}, {"id": 12, "seek": 9716, "start": 97.16, "end": 105.28, "text": " and all that, got accepted as developer, and then found CIPA Express router which is kind of", "tokens": [50364, 293, 439, 300, 11, 658, 9035, 382, 10754, 11, 293, 550, 1352, 383, 9139, 32, 20212, 22492, 597, 307, 733, 295, 50770], "temperature": 0.0, "avg_logprob": -0.4401928186416626, "compression_ratio": 1.3146853146853146, "no_speech_prob": 0.024989085271954536}, {"id": 13, "seek": 9716, "start": 105.28, "end": 115.19999999999999, "text": " later became came earlier in OpenCIP, added some modules, we use it extensively, I also created", "tokens": [50770, 1780, 3062, 1361, 3071, 294, 7238, 34, 9139, 11, 3869, 512, 16679, 11, 321, 764, 309, 32636, 11, 286, 611, 2942, 51266], "temperature": 0.0, "avg_logprob": -0.4401928186416626, "compression_ratio": 1.3146853146853146, "no_speech_prob": 0.024989085271954536}, {"id": 14, "seek": 11520, "start": 115.24000000000001, "end": 125.2, "text": " Pproxy, which people who use CIPA probably know about, and yeah, I get busy with various", "tokens": [50366, 430, 4318, 12876, 11, 597, 561, 567, 764, 383, 9139, 32, 1391, 458, 466, 11, 293, 1338, 11, 286, 483, 5856, 365, 3683, 50864], "temperature": 0.0, "avg_logprob": -0.33167607733543886, "compression_ratio": 1.2377622377622377, "no_speech_prob": 0.039090029895305634}, {"id": 15, "seek": 11520, "start": 125.2, "end": 134.72, "text": " open source projects to this day. Speaking of machine learning, I started in about 2010,", "tokens": [50864, 1269, 4009, 4455, 281, 341, 786, 13, 13069, 295, 3479, 2539, 11, 286, 1409, 294, 466, 9657, 11, 51340], "temperature": 0.0, "avg_logprob": -0.33167607733543886, "compression_ratio": 1.2377622377622377, "no_speech_prob": 0.039090029895305634}, {"id": 16, "seek": 13472, "start": 135.24, "end": 147.92, "text": " read this really nice book, it's actually free book, it's at 99 sort of age, but surprisingly got", "tokens": [50390, 1401, 341, 534, 1481, 1446, 11, 309, 311, 767, 1737, 1446, 11, 309, 311, 412, 11803, 1333, 295, 3205, 11, 457, 17600, 658, 51024], "temperature": 0.0, "avg_logprob": -0.2958882580632749, "compression_ratio": 1.4179104477611941, "no_speech_prob": 0.094180628657341}, {"id": 17, "seek": 13472, "start": 147.92, "end": 155.16, "text": " pretty good basic introduction into neural networks and all that, I already got a little bit", "tokens": [51024, 1238, 665, 3875, 9339, 666, 18161, 9590, 293, 439, 300, 11, 286, 1217, 658, 257, 707, 857, 51386], "temperature": 0.0, "avg_logprob": -0.2958882580632749, "compression_ratio": 1.4179104477611941, "no_speech_prob": 0.094180628657341}, {"id": 18, "seek": 15516, "start": 155.2, "end": 165.88, "text": " curious about that, and then as time went on, it became more available, I trained a little toy", "tokens": [50366, 6369, 466, 300, 11, 293, 550, 382, 565, 1437, 322, 11, 309, 3062, 544, 2435, 11, 286, 8895, 257, 707, 12058, 50900], "temperature": 0.0, "avg_logprob": -0.2480322265625, "compression_ratio": 1.465, "no_speech_prob": 0.031318556517362595}, {"id": 19, "seek": 15516, "start": 165.88, "end": 173.76, "text": " model just trying to figure out if I can detect DTMF from 729 encode frames essentially, because", "tokens": [50900, 2316, 445, 1382, 281, 2573, 484, 498, 286, 393, 5531, 413, 42023, 37, 490, 1614, 11871, 2058, 1429, 12083, 4476, 11, 570, 51294], "temperature": 0.0, "avg_logprob": -0.2480322265625, "compression_ratio": 1.465, "no_speech_prob": 0.031318556517362595}, {"id": 20, "seek": 15516, "start": 173.76, "end": 183.64, "text": " 729 is just a bunch of floating point of coefficient, so you can feed it to the model and get sort of", "tokens": [51294, 1614, 11871, 307, 445, 257, 3840, 295, 12607, 935, 295, 17619, 11, 370, 291, 393, 3154, 309, 281, 264, 2316, 293, 483, 1333, 295, 51788], "temperature": 0.0, "avg_logprob": -0.2480322265625, "compression_ratio": 1.465, "no_speech_prob": 0.031318556517362595}, {"id": 21, "seek": 18364, "start": 184.0, "end": 195.95999999999998, "text": " DTMF detection that way, it was just kind of weekend type of fun project. I also got installed", "tokens": [50382, 413, 42023, 37, 17784, 300, 636, 11, 309, 390, 445, 733, 295, 6711, 2010, 295, 1019, 1716, 13, 286, 611, 658, 8899, 50980], "temperature": 0.0, "avg_logprob": -0.3210786305941068, "compression_ratio": 1.2837837837837838, "no_speech_prob": 0.01936754211783409}, {"id": 22, "seek": 18364, "start": 195.95999999999998, "end": 207.39999999999998, "text": " a little AI powered ADAS called Coma 2, and it's open source project also, the user AI model to", "tokens": [50980, 257, 707, 7318, 17786, 9135, 3160, 1219, 2432, 64, 568, 11, 293, 309, 311, 1269, 4009, 1716, 611, 11, 264, 4195, 7318, 2316, 281, 51552], "temperature": 0.0, "avg_logprob": -0.3210786305941068, "compression_ratio": 1.2837837837837838, "no_speech_prob": 0.01936754211783409}, {"id": 23, "seek": 20740, "start": 207.48000000000002, "end": 214.96, "text": " drive your car, you can really like install it yourself, you can hack it, and it's pretty nice,", "tokens": [50368, 3332, 428, 1032, 11, 291, 393, 534, 411, 3625, 309, 1803, 11, 291, 393, 10339, 309, 11, 293, 309, 311, 1238, 1481, 11, 50742], "temperature": 0.0, "avg_logprob": -0.32735743204752604, "compression_ratio": 1.5206185567010309, "no_speech_prob": 0.19149255752563477}, {"id": 24, "seek": 20740, "start": 214.96, "end": 223.28, "text": " a good community, I participated in some events there. Also played with DeepFake lab, which kind of", "tokens": [50742, 257, 665, 1768, 11, 286, 17978, 294, 512, 3931, 456, 13, 2743, 3737, 365, 14895, 37, 619, 2715, 11, 597, 733, 295, 51158], "temperature": 0.0, "avg_logprob": -0.32735743204752604, "compression_ratio": 1.5206185567010309, "no_speech_prob": 0.19149255752563477}, {"id": 25, "seek": 20740, "start": 223.28, "end": 231.16, "text": " DeepFake framework, which allows you to work faces, also involves some training, so it was curious,", "tokens": [51158, 14895, 37, 619, 8388, 11, 597, 4045, 291, 281, 589, 8475, 11, 611, 11626, 512, 3097, 11, 370, 309, 390, 6369, 11, 51552], "temperature": 0.0, "avg_logprob": -0.32735743204752604, "compression_ratio": 1.5206185567010309, "no_speech_prob": 0.19149255752563477}, {"id": 26, "seek": 23116, "start": 231.96, "end": 240.72, "text": " and also lately I've been in San Diego, we've been doing like building a little from scratch,", "tokens": [50404, 293, 611, 12881, 286, 600, 668, 294, 5271, 16377, 11, 321, 600, 668, 884, 411, 2390, 257, 707, 490, 8459, 11, 50842], "temperature": 0.0, "avg_logprob": -0.35171590911017525, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.06518189609050751}, {"id": 27, "seek": 23116, "start": 240.72, "end": 248.4, "text": " we've been building model which will drive a little robot across the office, also was quite fun,", "tokens": [50842, 321, 600, 668, 2390, 2316, 597, 486, 3332, 257, 707, 7881, 2108, 264, 3398, 11, 611, 390, 1596, 1019, 11, 51226], "temperature": 0.0, "avg_logprob": -0.35171590911017525, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.06518189609050751}, {"id": 28, "seek": 23116, "start": 248.4, "end": 257.2, "text": " lately I've been also playing with Mu Zero support, first for the game of Digger, which I maintain,", "tokens": [51226, 12881, 286, 600, 668, 611, 2433, 365, 15601, 17182, 1406, 11, 700, 337, 264, 1216, 295, 413, 6812, 11, 597, 286, 6909, 11, 51666], "temperature": 0.0, "avg_logprob": -0.35171590911017525, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.06518189609050751}, {"id": 29, "seek": 25720, "start": 258.2, "end": 266.84, "text": " Linux port, it was pretty fun to sell highly interested, this is just the device that drives", "tokens": [50414, 18734, 2436, 11, 309, 390, 1238, 1019, 281, 3607, 5405, 3102, 11, 341, 307, 445, 264, 4302, 300, 11754, 50846], "temperature": 0.0, "avg_logprob": -0.28228255680629183, "compression_ratio": 1.4422110552763818, "no_speech_prob": 0.014507907442748547}, {"id": 30, "seek": 25720, "start": 266.84, "end": 275.64, "text": " my car, and it runs open source AI model inside that little chip. So anyway, back to the main point", "tokens": [50846, 452, 1032, 11, 293, 309, 6676, 1269, 4009, 7318, 2316, 1854, 300, 707, 11409, 13, 407, 4033, 11, 646, 281, 264, 2135, 935, 51286], "temperature": 0.0, "avg_logprob": -0.28228255680629183, "compression_ratio": 1.4422110552763818, "no_speech_prob": 0.014507907442748547}, {"id": 31, "seek": 25720, "start": 275.64, "end": 284.0, "text": " of this talk, we're basically looking at that from our perspective, which is trying to build a", "tokens": [51286, 295, 341, 751, 11, 321, 434, 1936, 1237, 412, 300, 490, 527, 4585, 11, 597, 307, 1382, 281, 1322, 257, 51704], "temperature": 0.0, "avg_logprob": -0.28228255680629183, "compression_ratio": 1.4422110552763818, "no_speech_prob": 0.014507907442748547}, {"id": 32, "seek": 28400, "start": 284.0, "end": 292.64, "text": " system which is, we basically have a lot of customers who route calls with us, so we try to", "tokens": [50364, 1185, 597, 307, 11, 321, 1936, 362, 257, 688, 295, 4581, 567, 7955, 5498, 365, 505, 11, 370, 321, 853, 281, 50796], "temperature": 0.0, "avg_logprob": -0.16931137140246405, "compression_ratio": 1.6193181818181819, "no_speech_prob": 0.015334752388298512}, {"id": 33, "seek": 28400, "start": 292.64, "end": 299.32, "text": " build a model that can be scaled out for provider level, not a model but software which can run", "tokens": [50796, 1322, 257, 2316, 300, 393, 312, 36039, 484, 337, 12398, 1496, 11, 406, 257, 2316, 457, 4722, 597, 393, 1190, 51130], "temperature": 0.0, "avg_logprob": -0.16931137140246405, "compression_ratio": 1.6193181818181819, "no_speech_prob": 0.015334752388298512}, {"id": 34, "seek": 28400, "start": 299.32, "end": 307.36, "text": " those models. So basically the idea behind this project is that we try to figure out how to scale", "tokens": [51130, 729, 5245, 13, 407, 1936, 264, 1558, 2261, 341, 1716, 307, 300, 321, 853, 281, 2573, 484, 577, 281, 4373, 51532], "temperature": 0.0, "avg_logprob": -0.16931137140246405, "compression_ratio": 1.6193181818181819, "no_speech_prob": 0.015334752388298512}, {"id": 35, "seek": 30736, "start": 307.40000000000003, "end": 317.32, "text": " those models on reasonably priced hardware, so as the problem with the area right now,", "tokens": [50366, 729, 5245, 322, 23551, 30349, 8837, 11, 370, 382, 264, 1154, 365, 264, 1859, 558, 586, 11, 50862], "temperature": 0.0, "avg_logprob": -0.3320897973102072, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.024924958124756813}, {"id": 36, "seek": 30736, "start": 317.32, "end": 324.04, "text": " we didn't kind of vacuum tube error, as I call it, machine learning, because we have very expensive", "tokens": [50862, 321, 994, 380, 733, 295, 14224, 9917, 6713, 11, 382, 286, 818, 309, 11, 3479, 2539, 11, 570, 321, 362, 588, 5124, 51198], "temperature": 0.0, "avg_logprob": -0.3320897973102072, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.024924958124756813}, {"id": 37, "seek": 30736, "start": 324.04, "end": 335.0, "text": " hardware for, you know, to run something mixed trial, you need, you know, like 64 gigabytes of", "tokens": [51198, 8837, 337, 11, 291, 458, 11, 281, 1190, 746, 7467, 7308, 11, 291, 643, 11, 291, 458, 11, 411, 12145, 42741, 295, 51746], "temperature": 0.0, "avg_logprob": -0.3320897973102072, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.024924958124756813}, {"id": 38, "seek": 33500, "start": 335.04, "end": 342.16, "text": " GPU, and that is very easy, you need several of them or you need a very expensive one, but", "tokens": [50366, 18407, 11, 293, 300, 307, 588, 1858, 11, 291, 643, 2940, 295, 552, 420, 291, 643, 257, 588, 5124, 472, 11, 457, 50722], "temperature": 0.0, "avg_logprob": -0.26836347579956055, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.01111327949911356}, {"id": 39, "seek": 33500, "start": 342.16, "end": 351.68, "text": " eventually all this stuff will get more affordable, so we're trying to kind of work towards that", "tokens": [50722, 4728, 439, 341, 1507, 486, 483, 544, 12028, 11, 370, 321, 434, 1382, 281, 733, 295, 589, 3030, 300, 51198], "temperature": 0.0, "avg_logprob": -0.26836347579956055, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.01111327949911356}, {"id": 40, "seek": 33500, "start": 351.68, "end": 361.68, "text": " goal. So right now there are like two major frameworks which people use, which torsion", "tokens": [51198, 3387, 13, 407, 558, 586, 456, 366, 411, 732, 2563, 29834, 597, 561, 764, 11, 597, 3930, 82, 313, 51698], "temperature": 0.0, "avg_logprob": -0.26836347579956055, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.01111327949911356}, {"id": 41, "seek": 36168, "start": 361.68, "end": 370.36, "text": " and tether flow, and they are also pretty heavy as well, it's like hundreds of megabytes,", "tokens": [50364, 293, 256, 1666, 3095, 11, 293, 436, 366, 611, 1238, 4676, 382, 731, 11, 309, 311, 411, 6779, 295, 10816, 24538, 11, 50798], "temperature": 0.0, "avg_logprob": -0.32847788796496036, "compression_ratio": 1.5340909090909092, "no_speech_prob": 0.01724150963127613}, {"id": 42, "seek": 36168, "start": 370.36, "end": 377.56, "text": " if not gigabytes, to get stuff running, but it looks like at least from my perspective,", "tokens": [50798, 498, 406, 42741, 11, 281, 483, 1507, 2614, 11, 457, 309, 1542, 411, 412, 1935, 490, 452, 4585, 11, 51158], "temperature": 0.0, "avg_logprob": -0.32847788796496036, "compression_ratio": 1.5340909090909092, "no_speech_prob": 0.01724150963127613}, {"id": 43, "seek": 36168, "start": 377.56, "end": 384.88, "text": " in a few years we'll see some changes, already, see people working on alternative frameworks", "tokens": [51158, 294, 257, 1326, 924, 321, 603, 536, 512, 2962, 11, 1217, 11, 536, 561, 1364, 322, 8535, 29834, 51524], "temperature": 0.0, "avg_logprob": -0.32847788796496036, "compression_ratio": 1.5340909090909092, "no_speech_prob": 0.01724150963127613}, {"id": 44, "seek": 38488, "start": 385.56, "end": 394.0, "text": " that are expected to be more lightweight and more flexible in terms of environment,", "tokens": [50398, 300, 366, 5176, 281, 312, 544, 22052, 293, 544, 11358, 294, 2115, 295, 2823, 11, 50820], "temperature": 0.0, "avg_logprob": -0.2212207200097256, "compression_ratio": 1.4361702127659575, "no_speech_prob": 0.017494596540927887}, {"id": 45, "seek": 38488, "start": 394.0, "end": 404.71999999999997, "text": " because Python is not very easy to scale and integrate into something like CAPIs, although", "tokens": [50820, 570, 15329, 307, 406, 588, 1858, 281, 4373, 293, 13365, 666, 746, 411, 383, 4715, 6802, 11, 4878, 51356], "temperature": 0.0, "avg_logprob": -0.2212207200097256, "compression_ratio": 1.4361702127659575, "no_speech_prob": 0.017494596540927887}, {"id": 46, "seek": 38488, "start": 404.71999999999997, "end": 412.88, "text": " doable, but anyway. So we started earlier this year with the project, and the original idea was", "tokens": [51356, 41183, 11, 457, 4033, 13, 407, 321, 1409, 3071, 341, 1064, 365, 264, 1716, 11, 293, 264, 3380, 1558, 390, 51764], "temperature": 0.0, "avg_logprob": -0.2212207200097256, "compression_ratio": 1.4361702127659575, "no_speech_prob": 0.017494596540927887}, {"id": 47, "seek": 41288, "start": 412.88, "end": 420.0, "text": " to just, for starters, we tried to implement text-to-speech, we already took our SIP stack,", "tokens": [50364, 281, 445, 11, 337, 35131, 11, 321, 3031, 281, 4445, 2487, 12, 1353, 12, 7053, 5023, 11, 321, 1217, 1890, 527, 318, 9139, 8630, 11, 50720], "temperature": 0.0, "avg_logprob": -0.25185816628592356, "compression_ratio": 1.4512820512820512, "no_speech_prob": 0.014420535415410995}, {"id": 48, "seek": 41288, "start": 420.0, "end": 428.68, "text": " which is conveniently Python based, so that was pretty easy, pretty much like 20 lines of code", "tokens": [50720, 597, 307, 44375, 15329, 2361, 11, 370, 300, 390, 1238, 1858, 11, 1238, 709, 411, 945, 3876, 295, 3089, 51154], "temperature": 0.0, "avg_logprob": -0.25185816628592356, "compression_ratio": 1.4512820512820512, "no_speech_prob": 0.014420535415410995}, {"id": 49, "seek": 41288, "start": 428.68, "end": 438.56, "text": " to get SIP endpoint implemented RTP generation thread and tried to run those models. Essentially", "tokens": [51154, 281, 483, 318, 9139, 35795, 12270, 497, 16804, 5125, 7207, 293, 3031, 281, 1190, 729, 5245, 13, 23596, 51648], "temperature": 0.0, "avg_logprob": -0.25185816628592356, "compression_ratio": 1.4512820512820512, "no_speech_prob": 0.014420535415410995}, {"id": 50, "seek": 43856, "start": 438.56, "end": 452.16, "text": " my, yeah, so basically we started with this guy, and it's like four gigabytes, cart, Nvidia 1050,", "tokens": [50364, 452, 11, 1338, 11, 370, 1936, 321, 1409, 365, 341, 2146, 11, 293, 309, 311, 411, 1451, 42741, 11, 5467, 11, 46284, 1266, 2803, 11, 51044], "temperature": 0.0, "avg_logprob": -0.3708492279052734, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.018523691222071648}, {"id": 51, "seek": 43856, "start": 452.16, "end": 466.48, "text": " and I was able to run like one channel essentially on it, of text-to-speech, and then obviously", "tokens": [51044, 293, 286, 390, 1075, 281, 1190, 411, 472, 2269, 4476, 322, 309, 11, 295, 2487, 12, 1353, 12, 7053, 5023, 11, 293, 550, 2745, 51760], "temperature": 0.0, "avg_logprob": -0.3708492279052734, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.018523691222071648}, {"id": 52, "seek": 46648, "start": 466.48, "end": 473.76, "text": " the next thing was like how to scale it, oh no, hold on, yeah, a little bit into how this works,", "tokens": [50364, 264, 958, 551, 390, 411, 577, 281, 4373, 309, 11, 1954, 572, 11, 1797, 322, 11, 1338, 11, 257, 707, 857, 666, 577, 341, 1985, 11, 50728], "temperature": 0.0, "avg_logprob": -0.24258579351963142, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.005231891758739948}, {"id": 53, "seek": 46648, "start": 473.76, "end": 482.56, "text": " essentially. So text-to-speech is, at least with transformers, you basically take your text,", "tokens": [50728, 4476, 13, 407, 2487, 12, 1353, 12, 7053, 5023, 307, 11, 412, 1935, 365, 4088, 433, 11, 291, 1936, 747, 428, 2487, 11, 51168], "temperature": 0.0, "avg_logprob": -0.24258579351963142, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.005231891758739948}, {"id": 54, "seek": 46648, "start": 482.56, "end": 491.44, "text": " you send it to your model, and then it runs multiple iterations, so you basically have all your stuff", "tokens": [51168, 291, 2845, 309, 281, 428, 2316, 11, 293, 550, 309, 6676, 3866, 36540, 11, 370, 291, 1936, 362, 439, 428, 1507, 51612], "temperature": 0.0, "avg_logprob": -0.24258579351963142, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.005231891758739948}, {"id": 55, "seek": 49144, "start": 492.4, "end": 499.36, "text": " through the model, and then it spills out something like male spectrum, which you put through", "tokens": [50412, 807, 264, 2316, 11, 293, 550, 309, 637, 2565, 484, 746, 411, 7133, 11143, 11, 597, 291, 829, 807, 50760], "temperature": 0.0, "avg_logprob": -0.35543092091878253, "compression_ratio": 1.4074074074074074, "no_speech_prob": 0.004790619481354952}, {"id": 56, "seek": 49144, "start": 499.36, "end": 511.36, "text": " Valkoder, and get the audio out. And then the first problem we run out into is that it basically", "tokens": [50760, 691, 667, 19866, 11, 293, 483, 264, 6278, 484, 13, 400, 550, 264, 700, 1154, 321, 1190, 484, 666, 307, 300, 309, 1936, 51360], "temperature": 0.0, "avg_logprob": -0.35543092091878253, "compression_ratio": 1.4074074074074074, "no_speech_prob": 0.004790619481354952}, {"id": 57, "seek": 51136, "start": 512.24, "end": 524.72, "text": " required this run for the whole duration of the audio, so essentially on my small GPU it would take", "tokens": [50408, 4739, 341, 1190, 337, 264, 1379, 16365, 295, 264, 6278, 11, 370, 4476, 322, 452, 1359, 18407, 309, 576, 747, 51032], "temperature": 0.0, "avg_logprob": -0.27088627448448765, "compression_ratio": 1.47, "no_speech_prob": 0.04023301601409912}, {"id": 58, "seek": 51136, "start": 524.72, "end": 534.32, "text": " quite a lot of time to actually produce audio. So I had to modify it's a sph-t5 model, it's like", "tokens": [51032, 1596, 257, 688, 295, 565, 281, 767, 5258, 6278, 13, 407, 286, 632, 281, 16927, 309, 311, 257, 637, 71, 12, 83, 20, 2316, 11, 309, 311, 411, 51512], "temperature": 0.0, "avg_logprob": -0.27088627448448765, "compression_ratio": 1.47, "no_speech_prob": 0.04023301601409912}, {"id": 59, "seek": 51136, "start": 534.32, "end": 540.72, "text": " not the latest, but one of the pretty good ones from Microsoft a few years ago, they released it,", "tokens": [51512, 406, 264, 6792, 11, 457, 472, 295, 264, 1238, 665, 2306, 490, 8116, 257, 1326, 924, 2057, 11, 436, 4736, 309, 11, 51832], "temperature": 0.0, "avg_logprob": -0.27088627448448765, "compression_ratio": 1.47, "no_speech_prob": 0.04023301601409912}, {"id": 60, "seek": 54072, "start": 541.44, "end": 548.48, "text": " so I had it to modify it. I mean I re-wrote the model itself, not the model itself, but the pipeline,", "tokens": [50400, 370, 286, 632, 309, 281, 16927, 309, 13, 286, 914, 286, 319, 12, 7449, 1370, 264, 2316, 2564, 11, 406, 264, 2316, 2564, 11, 457, 264, 15517, 11, 50752], "temperature": 0.0, "avg_logprob": -0.1946148621408563, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.0029053541366010904}, {"id": 61, "seek": 54072, "start": 548.48, "end": 558.88, "text": " so instead of processing the whole audio in one go, I made it so it spits out those smaller chunks.", "tokens": [50752, 370, 2602, 295, 9007, 264, 1379, 6278, 294, 472, 352, 11, 286, 1027, 309, 370, 309, 637, 1208, 484, 729, 4356, 24004, 13, 51272], "temperature": 0.0, "avg_logprob": -0.1946148621408563, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.0029053541366010904}, {"id": 62, "seek": 54072, "start": 559.52, "end": 565.36, "text": " The unfortunate problem came with that is it started like clicking, because Valkoder probably", "tokens": [51304, 440, 17843, 1154, 1361, 365, 300, 307, 309, 1409, 411, 9697, 11, 570, 691, 667, 19866, 1391, 51596], "temperature": 0.0, "avg_logprob": -0.1946148621408563, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.0029053541366010904}, {"id": 63, "seek": 56536, "start": 565.36, "end": 571.28, "text": " was not trained for this mode, so I tried to retrain the Valkoder, it did not go well, very well,", "tokens": [50364, 390, 406, 8895, 337, 341, 4391, 11, 370, 286, 3031, 281, 1533, 7146, 264, 691, 667, 19866, 11, 309, 630, 406, 352, 731, 11, 588, 731, 11, 50660], "temperature": 0.0, "avg_logprob": -0.18972314596176149, "compression_ratio": 1.5585106382978724, "no_speech_prob": 0.013661894015967846}, {"id": 64, "seek": 56536, "start": 571.28, "end": 578.88, "text": " so it did not produce a good result, so I had to build a little kind of post Valkoder scene, which", "tokens": [50660, 370, 309, 630, 406, 5258, 257, 665, 1874, 11, 370, 286, 632, 281, 1322, 257, 707, 733, 295, 2183, 691, 667, 19866, 4145, 11, 597, 51040], "temperature": 0.0, "avg_logprob": -0.18972314596176149, "compression_ratio": 1.5585106382978724, "no_speech_prob": 0.013661894015967846}, {"id": 65, "seek": 56536, "start": 580.5600000000001, "end": 588.16, "text": " smooths out essentially and fixes those clicks. And now it sounds pretty well, I will maybe play", "tokens": [51124, 5508, 82, 484, 4476, 293, 32539, 729, 18521, 13, 400, 586, 309, 3263, 1238, 731, 11, 286, 486, 1310, 862, 51504], "temperature": 0.0, "avg_logprob": -0.18972314596176149, "compression_ratio": 1.5585106382978724, "no_speech_prob": 0.013661894015967846}, {"id": 66, "seek": 58816, "start": 588.16, "end": 600.3199999999999, "text": " some examples when I get finished with the stuff. And then how I tried to scale it, so I got", "tokens": [50364, 512, 5110, 562, 286, 483, 4335, 365, 264, 1507, 13, 400, 550, 577, 286, 3031, 281, 4373, 309, 11, 370, 286, 658, 50972], "temperature": 0.0, "avg_logprob": -0.1990372758162649, "compression_ratio": 1.4257425742574257, "no_speech_prob": 0.028148403391242027}, {"id": 67, "seek": 58816, "start": 601.36, "end": 608.64, "text": " kind of normal size GPU, I would say, so it's like 16 gigabytes card, and I expected to get like", "tokens": [51024, 733, 295, 2710, 2744, 18407, 11, 286, 576, 584, 11, 370, 309, 311, 411, 3165, 42741, 2920, 11, 293, 286, 5176, 281, 483, 411, 51388], "temperature": 0.0, "avg_logprob": -0.1990372758162649, "compression_ratio": 1.4257425742574257, "no_speech_prob": 0.028148403391242027}, {"id": 68, "seek": 58816, "start": 608.64, "end": 615.76, "text": " maybe 10 times, 20 times performance, just looking at the spec, but to my surprise I only got like", "tokens": [51388, 1310, 1266, 1413, 11, 945, 1413, 3389, 11, 445, 1237, 412, 264, 1608, 11, 457, 281, 452, 6365, 286, 787, 658, 411, 51744], "temperature": 0.0, "avg_logprob": -0.1990372758162649, "compression_ratio": 1.4257425742574257, "no_speech_prob": 0.028148403391242027}, {"id": 69, "seek": 61576, "start": 615.76, "end": 624.3199999999999, "text": " two times more performance, so I can only like with this model, I can only run two real-time threads", "tokens": [50364, 732, 1413, 544, 3389, 11, 370, 286, 393, 787, 411, 365, 341, 2316, 11, 286, 393, 787, 1190, 732, 957, 12, 3766, 19314, 50792], "temperature": 0.0, "avg_logprob": -0.14089867728097097, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.011216207407414913}, {"id": 70, "seek": 61576, "start": 624.3199999999999, "end": 632.0, "text": " of TTS on the bigger card. So I started looking into why this is happening and how to improve,", "tokens": [50792, 295, 314, 7327, 322, 264, 3801, 2920, 13, 407, 286, 1409, 1237, 666, 983, 341, 307, 2737, 293, 577, 281, 3470, 11, 51176], "temperature": 0.0, "avg_logprob": -0.14089867728097097, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.011216207407414913}, {"id": 71, "seek": 61576, "start": 632.0, "end": 641.4399999999999, "text": " because theoretically it has much more performance. Turns out that in order to get good performance", "tokens": [51176, 570, 29400, 309, 575, 709, 544, 3389, 13, 29524, 484, 300, 294, 1668, 281, 483, 665, 3389, 51648], "temperature": 0.0, "avg_logprob": -0.14089867728097097, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.011216207407414913}, {"id": 72, "seek": 64144, "start": 641.44, "end": 650.5600000000001, "text": " out of those models you need to use batch inference, so instead of generating like each", "tokens": [50364, 484, 295, 729, 5245, 291, 643, 281, 764, 15245, 38253, 11, 370, 2602, 295, 17746, 411, 1184, 50820], "temperature": 0.0, "avg_logprob": -0.15489062666893005, "compression_ratio": 1.68944099378882, "no_speech_prob": 0.026401961222290993}, {"id": 73, "seek": 64144, "start": 650.5600000000001, "end": 658.4000000000001, "text": " prompt, each audio in each one session you batch your prompts that you need to voice out and", "tokens": [50820, 12391, 11, 1184, 6278, 294, 1184, 472, 5481, 291, 15245, 428, 41095, 300, 291, 643, 281, 3177, 484, 293, 51212], "temperature": 0.0, "avg_logprob": -0.15489062666893005, "compression_ratio": 1.68944099378882, "no_speech_prob": 0.026401961222290993}, {"id": 74, "seek": 64144, "start": 659.6800000000001, "end": 668.32, "text": " submit it into the model, and then it generates all those streams at the same kind of cost.", "tokens": [51276, 10315, 309, 666, 264, 2316, 11, 293, 550, 309, 23815, 439, 729, 15842, 412, 264, 912, 733, 295, 2063, 13, 51708], "temperature": 0.0, "avg_logprob": -0.15489062666893005, "compression_ratio": 1.68944099378882, "no_speech_prob": 0.026401961222290993}, {"id": 75, "seek": 66832, "start": 668.88, "end": 678.8000000000001, "text": " Because my main problem with using GPUs is that they are pretty widely computationally,", "tokens": [50392, 1436, 452, 2135, 1154, 365, 1228, 18407, 82, 307, 300, 436, 366, 1238, 13371, 24903, 379, 11, 50888], "temperature": 0.0, "avg_logprob": -0.18218213490077428, "compression_ratio": 1.5054347826086956, "no_speech_prob": 0.009780860505998135}, {"id": 76, "seek": 66832, "start": 678.8000000000001, "end": 684.6400000000001, "text": " but they are pretty expensive to send, so it's like you're operating through very slow network", "tokens": [50888, 457, 436, 366, 1238, 5124, 281, 2845, 11, 370, 309, 311, 411, 291, 434, 7447, 807, 588, 2964, 3209, 51180], "temperature": 0.0, "avg_logprob": -0.18218213490077428, "compression_ratio": 1.5054347826086956, "no_speech_prob": 0.009780860505998135}, {"id": 77, "seek": 66832, "start": 686.6400000000001, "end": 696.08, "text": " with a very fast device, so you need to load a lot of, well not a lot, but several jobs to it.", "tokens": [51280, 365, 257, 588, 2370, 4302, 11, 370, 291, 643, 281, 3677, 257, 688, 295, 11, 731, 406, 257, 688, 11, 457, 2940, 4782, 281, 309, 13, 51752], "temperature": 0.0, "avg_logprob": -0.18218213490077428, "compression_ratio": 1.5054347826086956, "no_speech_prob": 0.009780860505998135}, {"id": 78, "seek": 69608, "start": 696.8000000000001, "end": 710.0, "text": " And so I considered like several ways how to batch it, so my first idea was that maybe I can just", "tokens": [50400, 400, 370, 286, 4888, 411, 2940, 2098, 577, 281, 15245, 309, 11, 370, 452, 700, 1558, 390, 300, 1310, 286, 393, 445, 51060], "temperature": 0.0, "avg_logprob": -0.14304655620029993, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.004315313417464495}, {"id": 79, "seek": 69608, "start": 711.5200000000001, "end": 717.44, "text": " vary the size of the batch, so I run it in continuously, but then as sessions come and go", "tokens": [51136, 10559, 264, 2744, 295, 264, 15245, 11, 370, 286, 1190, 309, 294, 15684, 11, 457, 550, 382, 11081, 808, 293, 352, 51432], "temperature": 0.0, "avg_logprob": -0.14304655620029993, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.004315313417464495}, {"id": 80, "seek": 69608, "start": 717.44, "end": 724.8000000000001, "text": " I can add or remove them to this running batch. Unfortunately this does not work with the sequence", "tokens": [51432, 286, 393, 909, 420, 4159, 552, 281, 341, 2614, 15245, 13, 8590, 341, 775, 406, 589, 365, 264, 8310, 51800], "temperature": 0.0, "avg_logprob": -0.14304655620029993, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.004315313417464495}, {"id": 81, "seek": 72480, "start": 724.88, "end": 731.12, "text": " to sequence model, because internally it kind of clocks itself, so you cannot really add another", "tokens": [50368, 281, 8310, 2316, 11, 570, 19501, 309, 733, 295, 41528, 2564, 11, 370, 291, 2644, 534, 909, 1071, 50680], "temperature": 0.0, "avg_logprob": -0.12870484656030004, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.009387561120092869}, {"id": 82, "seek": 72480, "start": 731.12, "end": 737.8399999999999, "text": " session of it, so they should be running all of them at the same time, so essentially you need", "tokens": [50680, 5481, 295, 309, 11, 370, 436, 820, 312, 2614, 439, 295, 552, 412, 264, 912, 565, 11, 370, 4476, 291, 643, 51016], "temperature": 0.0, "avg_logprob": -0.12870484656030004, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.009387561120092869}, {"id": 83, "seek": 72480, "start": 737.8399999999999, "end": 744.16, "text": " to do something like this, so you batch a bunch of sentences and that you need to generate,", "tokens": [51016, 281, 360, 746, 411, 341, 11, 370, 291, 15245, 257, 3840, 295, 16579, 293, 300, 291, 643, 281, 8460, 11, 51332], "temperature": 0.0, "avg_logprob": -0.12870484656030004, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.009387561120092869}, {"id": 84, "seek": 72480, "start": 745.4399999999999, "end": 752.88, "text": " pump it up and wait for them, all of them to finish, and then at the time you can collect", "tokens": [51396, 5889, 309, 493, 293, 1699, 337, 552, 11, 439, 295, 552, 281, 2413, 11, 293, 550, 412, 264, 565, 291, 393, 2500, 51768], "temperature": 0.0, "avg_logprob": -0.12870484656030004, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.009387561120092869}, {"id": 85, "seek": 75288, "start": 753.4399999999999, "end": 762.88, "text": " a new request, then you batch them up and repeat again, and obviously if you have like", "tokens": [50392, 257, 777, 5308, 11, 550, 291, 15245, 552, 493, 293, 7149, 797, 11, 293, 2745, 498, 291, 362, 411, 50864], "temperature": 0.0, "avg_logprob": -0.13560191371984648, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.0064662606455385685}, {"id": 86, "seek": 75288, "start": 763.68, "end": 769.6, "text": " pretty powerful GPU you can probably run a few of them, or if you have several GPUs you can", "tokens": [50904, 1238, 4005, 18407, 291, 393, 1391, 1190, 257, 1326, 295, 552, 11, 420, 498, 291, 362, 2940, 18407, 82, 291, 393, 51200], "temperature": 0.0, "avg_logprob": -0.13560191371984648, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.0064662606455385685}, {"id": 87, "seek": 75288, "start": 770.72, "end": 778.24, "text": " improve latency by running on multiple of them.", "tokens": [51256, 3470, 27043, 538, 2614, 322, 3866, 295, 552, 13, 51632], "temperature": 0.0, "avg_logprob": -0.13560191371984648, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.0064662606455385685}, {"id": 88, "seek": 77824, "start": 779.2, "end": 787.36, "text": " Yeah, so that part kind of works. The next thing I'm right now working on", "tokens": [50412, 865, 11, 370, 300, 644, 733, 295, 1985, 13, 440, 958, 551, 286, 478, 558, 586, 1364, 322, 50820], "temperature": 0.0, "avg_logprob": -0.18919638733365643, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.00422177417203784}, {"id": 89, "seek": 77824, "start": 788.64, "end": 796.72, "text": " kind of in the other way, so we need to have something like Whisper to do the other way around,", "tokens": [50884, 733, 295, 294, 264, 661, 636, 11, 370, 321, 643, 281, 362, 746, 411, 41132, 610, 281, 360, 264, 661, 636, 926, 11, 51288], "temperature": 0.0, "avg_logprob": -0.18919638733365643, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.00422177417203784}, {"id": 90, "seek": 77824, "start": 798.4, "end": 807.52, "text": " and that one already supports batching, so basically should be pretty scalable there as well,", "tokens": [51372, 293, 300, 472, 1217, 9346, 15245, 278, 11, 370, 1936, 820, 312, 1238, 38481, 456, 382, 731, 11, 51828], "temperature": 0.0, "avg_logprob": -0.18919638733365643, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.00422177417203784}, {"id": 91, "seek": 80752, "start": 807.52, "end": 814.8, "text": " so right now I have on that $300 card I can do 50 sessions of text-to-speech", "tokens": [50364, 370, 558, 586, 286, 362, 322, 300, 1848, 12566, 2920, 286, 393, 360, 2625, 11081, 295, 2487, 12, 1353, 12, 7053, 5023, 50728], "temperature": 0.0, "avg_logprob": -0.17479141665176606, "compression_ratio": 1.4576271186440677, "no_speech_prob": 0.002659926889464259}, {"id": 92, "seek": 80752, "start": 815.92, "end": 822.0, "text": " in real time at the same time, so it's pretty good result because this is all running locally,", "tokens": [50784, 294, 957, 565, 412, 264, 912, 565, 11, 370, 309, 311, 1238, 665, 1874, 570, 341, 307, 439, 2614, 16143, 11, 51088], "temperature": 0.0, "avg_logprob": -0.17479141665176606, "compression_ratio": 1.4576271186440677, "no_speech_prob": 0.002659926889464259}, {"id": 93, "seek": 80752, "start": 822.72, "end": 830.48, "text": " so I don't use any anything pretty much and I can run it on a reasonably small device.", "tokens": [51124, 370, 286, 500, 380, 764, 604, 1340, 1238, 709, 293, 286, 393, 1190, 309, 322, 257, 23551, 1359, 4302, 13, 51512], "temperature": 0.0, "avg_logprob": -0.17479141665176606, "compression_ratio": 1.4576271186440677, "no_speech_prob": 0.002659926889464259}, {"id": 94, "seek": 83048, "start": 831.2, "end": 840.64, "text": " And yeah, I guess the last thing that I played recently was", "tokens": [50400, 400, 1338, 11, 286, 2041, 264, 1036, 551, 300, 286, 3737, 3938, 390, 50872], "temperature": 0.0, "avg_logprob": -0.20286729986017402, "compression_ratio": 1.3928571428571428, "no_speech_prob": 0.004504499025642872}, {"id": 95, "seek": 83048, "start": 843.9200000000001, "end": 851.04, "text": " there is a framework called Ray. It's basically when you can build a little cluster of", "tokens": [51036, 456, 307, 257, 8388, 1219, 10883, 13, 467, 311, 1936, 562, 291, 393, 1322, 257, 707, 13630, 295, 51392], "temperature": 0.0, "avg_logprob": -0.20286729986017402, "compression_ratio": 1.3928571428571428, "no_speech_prob": 0.004504499025642872}, {"id": 96, "seek": 83048, "start": 852.64, "end": 859.76, "text": " machines with different kind of, well maybe the same GPU, maybe different hardware, and", "tokens": [51472, 8379, 365, 819, 733, 295, 11, 731, 1310, 264, 912, 18407, 11, 1310, 819, 8837, 11, 293, 51828], "temperature": 0.0, "avg_logprob": -0.20286729986017402, "compression_ratio": 1.3928571428571428, "no_speech_prob": 0.004504499025642872}, {"id": 97, "seek": 86048, "start": 860.96, "end": 869.76, "text": " distribute your training or inference work over them, so that's just me running", "tokens": [50388, 20594, 428, 3097, 420, 38253, 589, 670, 552, 11, 370, 300, 311, 445, 385, 2614, 50828], "temperature": 0.0, "avg_logprob": -0.2848036491264731, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.0026036014314740896}, {"id": 98, "seek": 86048, "start": 871.2, "end": 880.4, "text": " and not 20 probably games of digger. It's basically, all of this is a model doing inference,", "tokens": [50900, 293, 406, 945, 1391, 2813, 295, 2528, 1321, 13, 467, 311, 1936, 11, 439, 295, 341, 307, 257, 2316, 884, 38253, 11, 51360], "temperature": 0.0, "avg_logprob": -0.2848036491264731, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.0026036014314740896}, {"id": 99, "seek": 86048, "start": 882.32, "end": 888.48, "text": " just looking at the screen doing inference saying where this should go basically, and", "tokens": [51456, 445, 1237, 412, 264, 2568, 884, 38253, 1566, 689, 341, 820, 352, 1936, 11, 293, 51764], "temperature": 0.0, "avg_logprob": -0.2848036491264731, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.0026036014314740896}, {"id": 100, "seek": 89048, "start": 891.12, "end": 900.48, "text": " kind of training itself to win at some point of time. So yeah, Ray,", "tokens": [50396, 733, 295, 3097, 2564, 281, 1942, 412, 512, 935, 295, 565, 13, 407, 1338, 11, 10883, 11, 50864], "temperature": 0.0, "avg_logprob": -0.25969956902896657, "compression_ratio": 1.478527607361963, "no_speech_prob": 0.00372298713773489}, {"id": 101, "seek": 89048, "start": 902.0, "end": 911.44, "text": " yeah this is like a CCH training, kind of improving a little bit, maybe, but anyway,", "tokens": [50940, 1338, 341, 307, 411, 257, 12630, 39, 3097, 11, 733, 295, 11470, 257, 707, 857, 11, 1310, 11, 457, 4033, 11, 51412], "temperature": 0.0, "avg_logprob": -0.25969956902896657, "compression_ratio": 1.478527607361963, "no_speech_prob": 0.00372298713773489}, {"id": 102, "seek": 89048, "start": 911.44, "end": 918.88, "text": " a good interesting part of it, I figured out how to use that Ray, so it's kind of useful", "tokens": [51412, 257, 665, 1880, 644, 295, 309, 11, 286, 8932, 484, 577, 281, 764, 300, 10883, 11, 370, 309, 311, 733, 295, 4420, 51784], "temperature": 0.0, "avg_logprob": -0.25969956902896657, "compression_ratio": 1.478527607361963, "no_speech_prob": 0.00372298713773489}, {"id": 103, "seek": 91888, "start": 919.84, "end": 927.84, "text": " open source framework that you can kind of scale, use to scale up your AI project, so I'll probably", "tokens": [50412, 1269, 4009, 8388, 300, 291, 393, 733, 295, 4373, 11, 764, 281, 4373, 493, 428, 7318, 1716, 11, 370, 286, 603, 1391, 50812], "temperature": 0.0, "avg_logprob": -0.2060883903503418, "compression_ratio": 1.375, "no_speech_prob": 0.009581844322383404}, {"id": 104, "seek": 91888, "start": 928.56, "end": 937.92, "text": " use some of it to distribute. Yeah, there are some links and I guess I have probably a few minutes", "tokens": [50848, 764, 512, 295, 309, 281, 20594, 13, 865, 11, 456, 366, 512, 6123, 293, 286, 2041, 286, 362, 1391, 257, 1326, 2077, 51316], "temperature": 0.0, "avg_logprob": -0.2060883903503418, "compression_ratio": 1.375, "no_speech_prob": 0.009581844322383404}, {"id": 105, "seek": 93792, "start": 937.92, "end": 956.3199999999999, "text": " for questions. Yeah, yeah, yeah we can do video technically because it's all about,", "tokens": [50364, 337, 1651, 13, 865, 11, 1338, 11, 1338, 321, 393, 360, 960, 12120, 570, 309, 311, 439, 466, 11, 51284], "temperature": 0.0, "avg_logprob": -0.26709335408312207, "compression_ratio": 1.4049586776859504, "no_speech_prob": 0.11055666208267212}, {"id": 106, "seek": 93792, "start": 960.24, "end": 966.48, "text": " you know, as soon as we have like the whole mechanism set up, we can do video as well,", "tokens": [51480, 291, 458, 11, 382, 2321, 382, 321, 362, 411, 264, 1379, 7513, 992, 493, 11, 321, 393, 360, 960, 382, 731, 11, 51792], "temperature": 0.0, "avg_logprob": -0.26709335408312207, "compression_ratio": 1.4049586776859504, "no_speech_prob": 0.11055666208267212}, {"id": 107, "seek": 96648, "start": 967.2, "end": 975.04, "text": " so yeah.", "tokens": [50400, 370, 1338, 13, 50792], "temperature": 0.0, "avg_logprob": -0.36163590171120386, "compression_ratio": 1.053191489361702, "no_speech_prob": 0.06702078878879547}, {"id": 108, "seek": 96648, "start": 988.64, "end": 996.16, "text": " Right now I'm using PyTorch, but oh okay, the question is what kind of model can they run?", "tokens": [51472, 1779, 586, 286, 478, 1228, 9953, 51, 284, 339, 11, 457, 1954, 1392, 11, 264, 1168, 307, 437, 733, 295, 2316, 393, 436, 1190, 30, 51848], "temperature": 0.0, "avg_logprob": -0.36163590171120386, "compression_ratio": 1.053191489361702, "no_speech_prob": 0.06702078878879547}, {"id": 109, "seek": 99648, "start": 997.2, "end": 1004.64, "text": " Right now I'm running with this existing code, I'm using PyTorch, but I also played with", "tokens": [50400, 1779, 586, 286, 478, 2614, 365, 341, 6741, 3089, 11, 286, 478, 1228, 9953, 51, 284, 339, 11, 457, 286, 611, 3737, 365, 50772], "temperature": 0.0, "avg_logprob": -0.17446505717742136, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.0071937087923288345}, {"id": 110, "seek": 99648, "start": 1005.76, "end": 1012.96, "text": " TinyGrad, so I might use some of it as well because as I said, it's kind of very lightweight,", "tokens": [50828, 39992, 38, 6206, 11, 370, 286, 1062, 764, 512, 295, 309, 382, 731, 570, 382, 286, 848, 11, 309, 311, 733, 295, 588, 22052, 11, 51188], "temperature": 0.0, "avg_logprob": -0.17446505717742136, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.0071937087923288345}, {"id": 111, "seek": 99648, "start": 1012.96, "end": 1023.84, "text": " so the whole goal of the guy who wrote it is to keep like usable framework in like 5,000 lines", "tokens": [51188, 370, 264, 1379, 3387, 295, 264, 2146, 567, 4114, 309, 307, 281, 1066, 411, 29975, 8388, 294, 411, 1025, 11, 1360, 3876, 51732], "temperature": 0.0, "avg_logprob": -0.17446505717742136, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.0071937087923288345}, {"id": 112, "seek": 102384, "start": 1023.9200000000001, "end": 1033.76, "text": " of Python code, so it's kind of very interesting from that perspective, but yeah, it's not really", "tokens": [50368, 295, 15329, 3089, 11, 370, 309, 311, 733, 295, 588, 1880, 490, 300, 4585, 11, 457, 1338, 11, 309, 311, 406, 534, 50860], "temperature": 0.0, "avg_logprob": -0.31460863893682306, "compression_ratio": 1.296875, "no_speech_prob": 0.01761583983898163}, {"id": 113, "seek": 102384, "start": 1033.76, "end": 1045.52, "text": " limited, it could use anything, right? I think another question, no?", "tokens": [50860, 5567, 11, 309, 727, 764, 1340, 11, 558, 30, 286, 519, 1071, 1168, 11, 572, 30, 51448], "temperature": 0.0, "avg_logprob": -0.31460863893682306, "compression_ratio": 1.296875, "no_speech_prob": 0.01761583983898163}, {"id": 114, "seek": 104552, "start": 1045.92, "end": 1048.08, "text": " I have one.", "tokens": [50384, 286, 362, 472, 13, 50492], "temperature": 1.0, "avg_logprob": -1.6365089416503906, "compression_ratio": 1.0277777777777777, "no_speech_prob": 0.14560289680957794}, {"id": 115, "seek": 104552, "start": 1051.44, "end": 1059.2, "text": " If you have another question, please give a round of applause.", "tokens": [50660, 759, 291, 362, 1071, 1168, 11, 1767, 976, 257, 3098, 295, 9969, 13, 51048], "temperature": 1.0, "avg_logprob": -1.6365089416503906, "compression_ratio": 1.0277777777777777, "no_speech_prob": 0.14560289680957794}], "language": "en"}