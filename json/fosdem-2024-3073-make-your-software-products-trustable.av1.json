{"text": " Hello everyone. Thanks for coming. My name is Dejan. Unfortunately Marco couldn't be here today. He got a call but yeah. What I want to talk about today is so we saw a lot of sessions today about producing gas bombs and producing the data a little very little. I think only Philip sessions was about managing actually the produce data right. So the challenge we try to tackle with the justification project is how to get all these data that are currently being produced by the more and more organizations like S bombs but also X files and more and more advisory data that we get and get them into some kind of manageable system because without that information is just a bunch of mostly JSON files spread all over the place right. So what we want to what we try to do is to provide a system that will get all this data put it in into a system that can be searchable and queryable and actually get us get us actionable information. So making software development more proactive in managing security but also making it much more easier to respond to the security issues. And yeah as I said these got us to start working on a justification project which basically set these goals for itself. So being able to ingest and store all kind of S bombs and VEX documents are open source but also proprietary company products right. Discover for those ingested S bombs and VEX is learn about all the new vulnerabilities and advisories related to the packages inside of the S bombs and being able to explore and search those information but also create an API that can be integratable in other systems and provide us to share this information with the rest of the developer toolchain like IDEs and CI CD tools. So ideally we would want to mark all the vulnerable dependencies directly in the developer's IDE and also for example fail the builds that tries to build a software that contains some of the dependencies that are known to be vulnerable. When we started to do this sometime last year this time last year we also figure out that there is another open source initiative that revolves around the similar ideas and it's called GUAC. It was mentioned in the previous session as well and I will cover it a little bit more here. So GUAC stands for Graph for Understanding Artifact Composition and the idea is to being able to ingest all different kinds of artifact documents like S bombs and VEX files and advisory data from all kinds of sources and basically create a graph ontology of that. So at first we started just experimenting with the GraphQL database but today ontology is based on the GraphQL API and can be implemented by the multiple persistent backends. That's on the left side right on the right side of the graph we also want to be able to query all these data. So GUAC should be able to provide us with all the answers about what are the dependencies in my S bomb, how these dependencies correlate with each other, so what's dependent on the what, so it's easy to find all the graph tree of dependency in your project but also being able to attach to this particular dependency all the vulnerability and the advisories and VEX data that we can find in additional systems. This is the basic architecture. Let me just see how much time I have here. But I basically explained it in the previous graph. So we can collect documents from different sources, we can certify them against different sources like OSV or DApps Dev, get it all through the GraphQL API ontology into a database. Two currently supported databases today is POSGRES, relational database that we use basically and works just fine and there's an Orango DB back end which is a pure GraphQL back end right and then on the other side provide the GraphQL API to be able to query that and provide a bunch of CLIs that it can be able to extract the data from the system. So in the classification project we try to provide a little bit more functionality on top of that. First of all we want to be able to actually not just ingest all the data about different relations in the database but we also want to provide a central place to store all your documents for the organization. So it provides an S3 compatible storage for storing and ingesting all the company's data into a single place so it can be an S3 bucket in the AWS but also for local deployments it can be some kind of a Minio instance for that. It has what we call walkers for different kind of CSEF repositories so that we can automatically ingest Asgum and Vex files and then provide what we can see on top and on the bottom. So what we call a single pane of glass like a nice UI to be able to search all this data that we have but also the Exort API as I said for integrating the system to the rest of the developer tool chain. So there's a nice VS Code plugin that can work basically with justification today and automatically from the project get all the dependencies and flag vulnerabilities if it's found in the system. So I thought to do a little demo so let's see how it's going to work. So Neil it will be easier. So here we can see the UI with some pre-loaded data and we can see that we have basically what we call six products here which are actually six S-bombs that are already already ingested in the system and a large number of CVs that have been collected from multiple sources and we can see that we identified around 2000 packages for these S-bombs and most importantly from the Vex files ingested here we identified 29 advisories for these. So if we go to a certain product we can see a couple of information obtained from the S-bombs so we can see the basic metadata that we have. Usually we can see all the packages and how they relate to each other. I think this S-bombs is pretty flat in structure so there's no much dependency going on there but the most important thing is that we can see different kinds of advisories that are against and also immediately see which actual packages are being affected by these advisories. We can go back and forth through this system so we can go to the actual package see that it's actually affected by this vulnerability. We can also go from the package and find the S-bombs that it belongs to, the S-bombs or the product but also what we can provide is that nice search capability as we said like maybe at some point you don't remember exact vulnerability we're looking for so you can basically just do a full text search or maybe yeah and find that there's a packages related to that but also find the exact vulnerabilities that we talked about a little bit earlier. So this is just like a basic demo right? I have a little bit more time just to explain so what were the challenges for us and I think we heard in a lot of sessions all about these challenges so it's mostly still early adopters everywhere, tools are immature including the project I'm working on so we definitely don't consider it mature but also there's a lot of inconsistency in the data wherever you look right so we heard today about all the multiple computing formats in S-bombs space and all the work that people are doing to bring that more closer and together over time which I think is awesome. We also heard a nice discussion about all the different kind of identifiers and you can see so if you work only with one source of data then it's easier but then if you try to correlate this S-bomb with this Vex file and this S-bomb is using PURELs and these are the CPEs it's becoming impossible to correlate data and build the graph basically properly. Also what we found is that even all these things are standards there's a lot of unwritten rules in all the organizations about how they are presenting their data so the documents will pass but what you have as an information from the document really depends so I think yeah it's good that you're all here and there's a lot of things to do right because it's early early days. For the project itself we'll try to additionally simplify architecture and the deployment model we're all about microservices and Kubernetes for now which is okay but I think we could reach much more people with simplifying how much resources and where they can deploy a project like this and go into supporting more standards. So you saw here just basic searches and basic correlation I think once we have much more data in the system we can get much more vision from all this data in and provide that as that's the value of the project in my opinion right and continue working on the future integrations because in my mind if you do continue doing this right I think at some point in a couple years all these infrastructures should be invisible to developers right so it should be part of your developer toolchain automatically working in VS code in all the Git for pipelines and everything right so we are just beginning that's it so justification side doesn't have too much data saying about immature projects but there's a dev box sandbox that you can try there's a code there and we always on the metric channel so if you're interested please reach out and yeah. I'm going to ask the question are you using the SPX libraries for helping with the ingestion? No no we're using yeah sorry yeah the question is are we using existing SPX libraries yes we are yeah so there's one in Golan using in in guac but there is also in Rust one using the classification itself because they are good yeah. So why is the reason that you decided to start a project from the ground instead of help at least four or five open source projects big ones that already do exactly what they do but not yet on the level but mostly 90 percent that we are doing today. Why you not helping that one instead of creating one? So yeah why we are starting a new project instead of instead of helping others so first of all we joined the guac project which is also another new project but yeah I can't answer that I mean a lot of people were involved in that kind of decision but we are trying to be as much I mean it's all open source we are contributing to other projects so it's not a closed source product basically yeah. So one of your early slides said this can be used to sort of share S-bomb data can you talk a little about that feature how you this can be used to sort of send S-bomb data around to other projects? So it's not about yeah sorry about it so about sharing the S-bomb data it's not about sharing the data but providing the API so the external systems can query things so basically the VSCode plugin would get all the URLs from the current project and being able to query this and get actionable item back so there's no any distributed sharing of the data just integration API. Okay please thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.4, "text": " Hello everyone. Thanks for coming. My name is Dejan. Unfortunately Marco couldn't be", "tokens": [50364, 2425, 1518, 13, 2561, 337, 1348, 13, 1222, 1315, 307, 1346, 14763, 13, 8590, 26535, 2809, 380, 312, 50984], "temperature": 0.0, "avg_logprob": -0.28889154287484975, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.6576403975486755}, {"id": 1, "seek": 0, "start": 12.4, "end": 19.92, "text": " here today. He got a call but yeah. What I want to talk about today is so we saw a lot", "tokens": [50984, 510, 965, 13, 634, 658, 257, 818, 457, 1338, 13, 708, 286, 528, 281, 751, 466, 965, 307, 370, 321, 1866, 257, 688, 51360], "temperature": 0.0, "avg_logprob": -0.28889154287484975, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.6576403975486755}, {"id": 2, "seek": 0, "start": 19.92, "end": 25.92, "text": " of sessions today about producing gas bombs and producing the data a little very little.", "tokens": [51360, 295, 11081, 965, 466, 10501, 4211, 19043, 293, 10501, 264, 1412, 257, 707, 588, 707, 13, 51660], "temperature": 0.0, "avg_logprob": -0.28889154287484975, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.6576403975486755}, {"id": 3, "seek": 2592, "start": 25.92, "end": 31.76, "text": " I think only Philip sessions was about managing actually the produce data right. So the challenge", "tokens": [50364, 286, 519, 787, 21144, 11081, 390, 466, 11642, 767, 264, 5258, 1412, 558, 13, 407, 264, 3430, 50656], "temperature": 0.0, "avg_logprob": -0.2211366347324701, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.027353735640645027}, {"id": 4, "seek": 2592, "start": 31.76, "end": 39.36, "text": " we try to tackle with the justification project is how to get all these data that are currently", "tokens": [50656, 321, 853, 281, 14896, 365, 264, 31591, 1716, 307, 577, 281, 483, 439, 613, 1412, 300, 366, 4362, 51036], "temperature": 0.0, "avg_logprob": -0.2211366347324701, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.027353735640645027}, {"id": 5, "seek": 2592, "start": 39.36, "end": 44.96, "text": " being produced by the more and more organizations like S bombs but also X files and more and more", "tokens": [51036, 885, 7126, 538, 264, 544, 293, 544, 6150, 411, 318, 19043, 457, 611, 1783, 7098, 293, 544, 293, 544, 51316], "temperature": 0.0, "avg_logprob": -0.2211366347324701, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.027353735640645027}, {"id": 6, "seek": 2592, "start": 44.96, "end": 52.24, "text": " advisory data that we get and get them into some kind of manageable system because without that", "tokens": [51316, 26289, 1412, 300, 321, 483, 293, 483, 552, 666, 512, 733, 295, 38798, 1185, 570, 1553, 300, 51680], "temperature": 0.0, "avg_logprob": -0.2211366347324701, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.027353735640645027}, {"id": 7, "seek": 5224, "start": 53.2, "end": 60.24, "text": " information is just a bunch of mostly JSON files spread all over the place right. So what we want", "tokens": [50412, 1589, 307, 445, 257, 3840, 295, 5240, 31828, 7098, 3974, 439, 670, 264, 1081, 558, 13, 407, 437, 321, 528, 50764], "temperature": 0.0, "avg_logprob": -0.21496447154453824, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.022042281925678253}, {"id": 8, "seek": 5224, "start": 60.24, "end": 67.04, "text": " to what we try to do is to provide a system that will get all this data put it in into a system", "tokens": [50764, 281, 437, 321, 853, 281, 360, 307, 281, 2893, 257, 1185, 300, 486, 483, 439, 341, 1412, 829, 309, 294, 666, 257, 1185, 51104], "temperature": 0.0, "avg_logprob": -0.21496447154453824, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.022042281925678253}, {"id": 9, "seek": 5224, "start": 67.04, "end": 76.32000000000001, "text": " that can be searchable and queryable and actually get us get us actionable information. So making", "tokens": [51104, 300, 393, 312, 3164, 712, 293, 14581, 712, 293, 767, 483, 505, 483, 505, 45098, 1589, 13, 407, 1455, 51568], "temperature": 0.0, "avg_logprob": -0.21496447154453824, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.022042281925678253}, {"id": 10, "seek": 7632, "start": 76.39999999999999, "end": 83.52, "text": " software development more proactive in managing security but also making it much more easier to", "tokens": [50368, 4722, 3250, 544, 28028, 294, 11642, 3825, 457, 611, 1455, 309, 709, 544, 3571, 281, 50724], "temperature": 0.0, "avg_logprob": -0.2622188309491691, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.031178535893559456}, {"id": 11, "seek": 7632, "start": 83.52, "end": 93.75999999999999, "text": " respond to the security issues. And yeah as I said these got us to start working on a", "tokens": [50724, 4196, 281, 264, 3825, 2663, 13, 400, 1338, 382, 286, 848, 613, 658, 505, 281, 722, 1364, 322, 257, 51236], "temperature": 0.0, "avg_logprob": -0.2622188309491691, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.031178535893559456}, {"id": 12, "seek": 7632, "start": 93.75999999999999, "end": 100.0, "text": " justification project which basically set these goals for itself. So being able to ingest and", "tokens": [51236, 31591, 1716, 597, 1936, 992, 613, 5493, 337, 2564, 13, 407, 885, 1075, 281, 3957, 377, 293, 51548], "temperature": 0.0, "avg_logprob": -0.2622188309491691, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.031178535893559456}, {"id": 13, "seek": 10000, "start": 100.96, "end": 109.04, "text": " store all kind of S bombs and VEX documents are open source but also proprietary company products", "tokens": [50412, 3531, 439, 733, 295, 318, 19043, 293, 691, 39814, 8512, 366, 1269, 4009, 457, 611, 38992, 2237, 3383, 50816], "temperature": 0.0, "avg_logprob": -0.2008906453847885, "compression_ratio": 1.5621621621621622, "no_speech_prob": 0.04943421855568886}, {"id": 14, "seek": 10000, "start": 109.04, "end": 117.76, "text": " right. Discover for those ingested S bombs and VEX is learn about all the new vulnerabilities", "tokens": [50816, 558, 13, 40386, 337, 729, 3957, 21885, 318, 19043, 293, 691, 39814, 307, 1466, 466, 439, 264, 777, 37633, 51252], "temperature": 0.0, "avg_logprob": -0.2008906453847885, "compression_ratio": 1.5621621621621622, "no_speech_prob": 0.04943421855568886}, {"id": 15, "seek": 10000, "start": 117.76, "end": 126.4, "text": " and advisories related to the packages inside of the S bombs and being able to explore and search", "tokens": [51252, 293, 10280, 2083, 4077, 281, 264, 17401, 1854, 295, 264, 318, 19043, 293, 885, 1075, 281, 6839, 293, 3164, 51684], "temperature": 0.0, "avg_logprob": -0.2008906453847885, "compression_ratio": 1.5621621621621622, "no_speech_prob": 0.04943421855568886}, {"id": 16, "seek": 12640, "start": 126.96000000000001, "end": 131.68, "text": " those information but also create an API that can be", "tokens": [50392, 729, 1589, 457, 611, 1884, 364, 9362, 300, 393, 312, 50628], "temperature": 0.0, "avg_logprob": -0.16458567551204137, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.0310650784522295}, {"id": 17, "seek": 12640, "start": 133.28, "end": 139.44, "text": " integratable in other systems and provide us to share this information with the rest of the", "tokens": [50708, 3572, 31415, 294, 661, 3652, 293, 2893, 505, 281, 2073, 341, 1589, 365, 264, 1472, 295, 264, 51016], "temperature": 0.0, "avg_logprob": -0.16458567551204137, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.0310650784522295}, {"id": 18, "seek": 12640, "start": 139.44, "end": 149.68, "text": " developer toolchain like IDEs and CI CD tools. So ideally we would want to mark all the vulnerable", "tokens": [51016, 10754, 2290, 11509, 411, 7348, 20442, 293, 37777, 6743, 3873, 13, 407, 22915, 321, 576, 528, 281, 1491, 439, 264, 10955, 51528], "temperature": 0.0, "avg_logprob": -0.16458567551204137, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.0310650784522295}, {"id": 19, "seek": 14968, "start": 150.4, "end": 157.20000000000002, "text": " dependencies directly in the developer's IDE and also for example fail the builds that", "tokens": [50400, 36606, 3838, 294, 264, 10754, 311, 40930, 293, 611, 337, 1365, 3061, 264, 15182, 300, 50740], "temperature": 0.0, "avg_logprob": -0.15793011552196437, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.01075735129415989}, {"id": 20, "seek": 14968, "start": 158.32, "end": 164.96, "text": " tries to build a software that contains some of the dependencies that are known to be vulnerable.", "tokens": [50796, 9898, 281, 1322, 257, 4722, 300, 8306, 512, 295, 264, 36606, 300, 366, 2570, 281, 312, 10955, 13, 51128], "temperature": 0.0, "avg_logprob": -0.15793011552196437, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.01075735129415989}, {"id": 21, "seek": 14968, "start": 166.48000000000002, "end": 174.08, "text": " When we started to do this sometime last year this time last year we also figure out that", "tokens": [51204, 1133, 321, 1409, 281, 360, 341, 15053, 1036, 1064, 341, 565, 1036, 1064, 321, 611, 2573, 484, 300, 51584], "temperature": 0.0, "avg_logprob": -0.15793011552196437, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.01075735129415989}, {"id": 22, "seek": 17408, "start": 174.08, "end": 183.12, "text": " there is another open source initiative that revolves around the similar ideas and it's called", "tokens": [50364, 456, 307, 1071, 1269, 4009, 11552, 300, 47934, 926, 264, 2531, 3487, 293, 309, 311, 1219, 50816], "temperature": 0.0, "avg_logprob": -0.14406246297499714, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.02988453581929207}, {"id": 23, "seek": 17408, "start": 183.12, "end": 190.32000000000002, "text": " GUAC. It was mentioned in the previous session as well and I will cover it a little bit more here.", "tokens": [50816, 17917, 4378, 13, 467, 390, 2835, 294, 264, 3894, 5481, 382, 731, 293, 286, 486, 2060, 309, 257, 707, 857, 544, 510, 13, 51176], "temperature": 0.0, "avg_logprob": -0.14406246297499714, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.02988453581929207}, {"id": 24, "seek": 17408, "start": 191.12, "end": 198.48000000000002, "text": " So GUAC stands for Graph for Understanding Artifact Composition and the idea is to being able to", "tokens": [51216, 407, 17917, 4378, 7382, 337, 21884, 337, 36858, 5735, 351, 578, 6620, 5830, 293, 264, 1558, 307, 281, 885, 1075, 281, 51584], "temperature": 0.0, "avg_logprob": -0.14406246297499714, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.02988453581929207}, {"id": 25, "seek": 19848, "start": 198.48, "end": 207.35999999999999, "text": " ingest all different kinds of artifact documents like S bombs and VEX files and advisory data from", "tokens": [50364, 3957, 377, 439, 819, 3685, 295, 34806, 8512, 411, 318, 19043, 293, 691, 39814, 7098, 293, 26289, 1412, 490, 50808], "temperature": 0.0, "avg_logprob": -0.14094640247857393, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.029478110373020172}, {"id": 26, "seek": 19848, "start": 207.35999999999999, "end": 215.6, "text": " all kinds of sources and basically create a graph ontology of that. So at first we started just", "tokens": [50808, 439, 3685, 295, 7139, 293, 1936, 1884, 257, 4295, 6592, 1793, 295, 300, 13, 407, 412, 700, 321, 1409, 445, 51220], "temperature": 0.0, "avg_logprob": -0.14094640247857393, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.029478110373020172}, {"id": 27, "seek": 19848, "start": 215.6, "end": 222.39999999999998, "text": " experimenting with the GraphQL database but today ontology is based on the GraphQL API and can be", "tokens": [51220, 29070, 365, 264, 21884, 13695, 8149, 457, 965, 6592, 1793, 307, 2361, 322, 264, 21884, 13695, 9362, 293, 393, 312, 51560], "temperature": 0.0, "avg_logprob": -0.14094640247857393, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.029478110373020172}, {"id": 28, "seek": 22240, "start": 222.4, "end": 228.48000000000002, "text": " implemented by the multiple persistent backends. That's on the left side right on the right side", "tokens": [50364, 12270, 538, 264, 3866, 24315, 646, 2581, 13, 663, 311, 322, 264, 1411, 1252, 558, 322, 264, 558, 1252, 50668], "temperature": 0.0, "avg_logprob": -0.13242625186317844, "compression_ratio": 1.7577092511013215, "no_speech_prob": 0.02955452725291252}, {"id": 29, "seek": 22240, "start": 228.48000000000002, "end": 237.36, "text": " of the graph we also want to be able to query all these data. So GUAC should be able to provide us", "tokens": [50668, 295, 264, 4295, 321, 611, 528, 281, 312, 1075, 281, 14581, 439, 613, 1412, 13, 407, 17917, 4378, 820, 312, 1075, 281, 2893, 505, 51112], "temperature": 0.0, "avg_logprob": -0.13242625186317844, "compression_ratio": 1.7577092511013215, "no_speech_prob": 0.02955452725291252}, {"id": 30, "seek": 22240, "start": 237.36, "end": 245.04000000000002, "text": " with all the answers about what are the dependencies in my S bomb, how these dependencies correlate", "tokens": [51112, 365, 439, 264, 6338, 466, 437, 366, 264, 36606, 294, 452, 318, 7851, 11, 577, 613, 36606, 48742, 51496], "temperature": 0.0, "avg_logprob": -0.13242625186317844, "compression_ratio": 1.7577092511013215, "no_speech_prob": 0.02955452725291252}, {"id": 31, "seek": 22240, "start": 245.04000000000002, "end": 251.36, "text": " with each other, so what's dependent on the what, so it's easy to find all the graph tree of dependency", "tokens": [51496, 365, 1184, 661, 11, 370, 437, 311, 12334, 322, 264, 437, 11, 370, 309, 311, 1858, 281, 915, 439, 264, 4295, 4230, 295, 33621, 51812], "temperature": 0.0, "avg_logprob": -0.13242625186317844, "compression_ratio": 1.7577092511013215, "no_speech_prob": 0.02955452725291252}, {"id": 32, "seek": 25136, "start": 251.44000000000003, "end": 260.0, "text": " in your project but also being able to attach to this particular dependency all the vulnerability", "tokens": [50368, 294, 428, 1716, 457, 611, 885, 1075, 281, 5085, 281, 341, 1729, 33621, 439, 264, 24210, 50796], "temperature": 0.0, "avg_logprob": -0.18883363405863443, "compression_ratio": 1.4207650273224044, "no_speech_prob": 0.004273802973330021}, {"id": 33, "seek": 25136, "start": 260.0, "end": 266.56, "text": " and the advisories and VEX data that we can find in additional systems.", "tokens": [50796, 293, 264, 10280, 2083, 293, 691, 39814, 1412, 300, 321, 393, 915, 294, 4497, 3652, 13, 51124], "temperature": 0.0, "avg_logprob": -0.18883363405863443, "compression_ratio": 1.4207650273224044, "no_speech_prob": 0.004273802973330021}, {"id": 34, "seek": 25136, "start": 268.72, "end": 275.12, "text": " This is the basic architecture. Let me just see how much time I have here. But I basically", "tokens": [51232, 639, 307, 264, 3875, 9482, 13, 961, 385, 445, 536, 577, 709, 565, 286, 362, 510, 13, 583, 286, 1936, 51552], "temperature": 0.0, "avg_logprob": -0.18883363405863443, "compression_ratio": 1.4207650273224044, "no_speech_prob": 0.004273802973330021}, {"id": 35, "seek": 27512, "start": 275.92, "end": 283.12, "text": " explained it in the previous graph. So we can collect documents from different sources,", "tokens": [50404, 8825, 309, 294, 264, 3894, 4295, 13, 407, 321, 393, 2500, 8512, 490, 819, 7139, 11, 50764], "temperature": 0.0, "avg_logprob": -0.2605766647163479, "compression_ratio": 1.5020576131687242, "no_speech_prob": 0.040054548531770706}, {"id": 36, "seek": 27512, "start": 283.12, "end": 289.04, "text": " we can certify them against different sources like OSV or DApps Dev, get it all through the", "tokens": [50764, 321, 393, 5351, 2505, 552, 1970, 819, 7139, 411, 12731, 53, 420, 413, 9132, 82, 9096, 11, 483, 309, 439, 807, 264, 51060], "temperature": 0.0, "avg_logprob": -0.2605766647163479, "compression_ratio": 1.5020576131687242, "no_speech_prob": 0.040054548531770706}, {"id": 37, "seek": 27512, "start": 290.16, "end": 296.64, "text": " GraphQL API ontology into a database. Two currently supported databases today is POSGRES,", "tokens": [51116, 21884, 13695, 9362, 6592, 1793, 666, 257, 8149, 13, 4453, 4362, 8104, 22380, 965, 307, 430, 4367, 23971, 2358, 11, 51440], "temperature": 0.0, "avg_logprob": -0.2605766647163479, "compression_ratio": 1.5020576131687242, "no_speech_prob": 0.040054548531770706}, {"id": 38, "seek": 27512, "start": 296.64, "end": 302.08, "text": " relational database that we use basically and works just fine and there's an Orango DB back end", "tokens": [51440, 38444, 8149, 300, 321, 764, 1936, 293, 1985, 445, 2489, 293, 456, 311, 364, 1610, 17150, 26754, 646, 917, 51712], "temperature": 0.0, "avg_logprob": -0.2605766647163479, "compression_ratio": 1.5020576131687242, "no_speech_prob": 0.040054548531770706}, {"id": 39, "seek": 30208, "start": 302.08, "end": 308.47999999999996, "text": " which is a pure GraphQL back end right and then on the other side provide the GraphQL API to", "tokens": [50364, 597, 307, 257, 6075, 21884, 13695, 646, 917, 558, 293, 550, 322, 264, 661, 1252, 2893, 264, 21884, 13695, 9362, 281, 50684], "temperature": 0.0, "avg_logprob": -0.13380223682948522, "compression_ratio": 1.553763440860215, "no_speech_prob": 0.0048888372257351875}, {"id": 40, "seek": 30208, "start": 309.59999999999997, "end": 316.47999999999996, "text": " be able to query that and provide a bunch of CLIs that it can be able to extract the data from the", "tokens": [50740, 312, 1075, 281, 14581, 300, 293, 2893, 257, 3840, 295, 12855, 6802, 300, 309, 393, 312, 1075, 281, 8947, 264, 1412, 490, 264, 51084], "temperature": 0.0, "avg_logprob": -0.13380223682948522, "compression_ratio": 1.553763440860215, "no_speech_prob": 0.0048888372257351875}, {"id": 41, "seek": 30208, "start": 316.47999999999996, "end": 327.36, "text": " system. So in the classification project we try to provide a little bit more functionality on top", "tokens": [51084, 1185, 13, 407, 294, 264, 21538, 1716, 321, 853, 281, 2893, 257, 707, 857, 544, 14980, 322, 1192, 51628], "temperature": 0.0, "avg_logprob": -0.13380223682948522, "compression_ratio": 1.553763440860215, "no_speech_prob": 0.0048888372257351875}, {"id": 42, "seek": 32736, "start": 327.36, "end": 336.56, "text": " of that. First of all we want to be able to actually not just ingest all the data about", "tokens": [50364, 295, 300, 13, 2386, 295, 439, 321, 528, 281, 312, 1075, 281, 767, 406, 445, 3957, 377, 439, 264, 1412, 466, 50824], "temperature": 0.0, "avg_logprob": -0.08678790747401226, "compression_ratio": 1.674757281553398, "no_speech_prob": 0.018060123547911644}, {"id": 43, "seek": 32736, "start": 336.56, "end": 342.08000000000004, "text": " different relations in the database but we also want to provide a central place to store all", "tokens": [50824, 819, 2299, 294, 264, 8149, 457, 321, 611, 528, 281, 2893, 257, 5777, 1081, 281, 3531, 439, 51100], "temperature": 0.0, "avg_logprob": -0.08678790747401226, "compression_ratio": 1.674757281553398, "no_speech_prob": 0.018060123547911644}, {"id": 44, "seek": 32736, "start": 342.08000000000004, "end": 348.96000000000004, "text": " your documents for the organization. So it provides an S3 compatible storage for storing", "tokens": [51100, 428, 8512, 337, 264, 4475, 13, 407, 309, 6417, 364, 318, 18, 18218, 6725, 337, 26085, 51444], "temperature": 0.0, "avg_logprob": -0.08678790747401226, "compression_ratio": 1.674757281553398, "no_speech_prob": 0.018060123547911644}, {"id": 45, "seek": 32736, "start": 348.96000000000004, "end": 356.08000000000004, "text": " and ingesting all the company's data into a single place so it can be an S3", "tokens": [51444, 293, 3957, 8714, 439, 264, 2237, 311, 1412, 666, 257, 2167, 1081, 370, 309, 393, 312, 364, 318, 18, 51800], "temperature": 0.0, "avg_logprob": -0.08678790747401226, "compression_ratio": 1.674757281553398, "no_speech_prob": 0.018060123547911644}, {"id": 46, "seek": 35608, "start": 356.8, "end": 362.88, "text": " bucket in the AWS but also for local deployments it can be some kind of a", "tokens": [50400, 13058, 294, 264, 17650, 457, 611, 337, 2654, 7274, 1117, 309, 393, 312, 512, 733, 295, 257, 50704], "temperature": 0.0, "avg_logprob": -0.20272325066959157, "compression_ratio": 1.4406779661016949, "no_speech_prob": 0.028437625616788864}, {"id": 47, "seek": 35608, "start": 362.88, "end": 371.59999999999997, "text": " Minio instance for that. It has what we call walkers for different kind of CSEF repositories", "tokens": [50704, 2829, 1004, 5197, 337, 300, 13, 467, 575, 437, 321, 818, 1792, 433, 337, 819, 733, 295, 383, 5879, 37, 22283, 2083, 51140], "temperature": 0.0, "avg_logprob": -0.20272325066959157, "compression_ratio": 1.4406779661016949, "no_speech_prob": 0.028437625616788864}, {"id": 48, "seek": 35608, "start": 371.59999999999997, "end": 382.0, "text": " so that we can automatically ingest Asgum and Vex files and then provide what we can see", "tokens": [51140, 370, 300, 321, 393, 6772, 3957, 377, 1018, 70, 449, 293, 691, 3121, 7098, 293, 550, 2893, 437, 321, 393, 536, 51660], "temperature": 0.0, "avg_logprob": -0.20272325066959157, "compression_ratio": 1.4406779661016949, "no_speech_prob": 0.028437625616788864}, {"id": 49, "seek": 38200, "start": 382.0, "end": 389.04, "text": " on top and on the bottom. So what we call a single pane of glass like a nice UI to be able to", "tokens": [50364, 322, 1192, 293, 322, 264, 2767, 13, 407, 437, 321, 818, 257, 2167, 32605, 295, 4276, 411, 257, 1481, 15682, 281, 312, 1075, 281, 50716], "temperature": 0.0, "avg_logprob": -0.21252484517554715, "compression_ratio": 1.484375, "no_speech_prob": 0.0135775376111269}, {"id": 50, "seek": 38200, "start": 389.04, "end": 397.04, "text": " search all this data that we have but also the Exort API as I said for integrating the system", "tokens": [50716, 3164, 439, 341, 1412, 300, 321, 362, 457, 611, 264, 2111, 477, 9362, 382, 286, 848, 337, 26889, 264, 1185, 51116], "temperature": 0.0, "avg_logprob": -0.21252484517554715, "compression_ratio": 1.484375, "no_speech_prob": 0.0135775376111269}, {"id": 51, "seek": 38200, "start": 397.04, "end": 406.0, "text": " to the rest of the developer tool chain. So there's a nice VS Code plugin that can work basically", "tokens": [51116, 281, 264, 1472, 295, 264, 10754, 2290, 5021, 13, 407, 456, 311, 257, 1481, 25091, 15549, 23407, 300, 393, 589, 1936, 51564], "temperature": 0.0, "avg_logprob": -0.21252484517554715, "compression_ratio": 1.484375, "no_speech_prob": 0.0135775376111269}, {"id": 52, "seek": 40600, "start": 406.48, "end": 413.28, "text": " with justification today and automatically from the project get all the dependencies and flag", "tokens": [50388, 365, 31591, 965, 293, 6772, 490, 264, 1716, 483, 439, 264, 36606, 293, 7166, 50728], "temperature": 0.0, "avg_logprob": -0.14270121710641043, "compression_ratio": 1.4947916666666667, "no_speech_prob": 0.007775104139000177}, {"id": 53, "seek": 40600, "start": 414.72, "end": 424.0, "text": " vulnerabilities if it's found in the system. So I thought to do a little demo so let's see how", "tokens": [50800, 37633, 498, 309, 311, 1352, 294, 264, 1185, 13, 407, 286, 1194, 281, 360, 257, 707, 10723, 370, 718, 311, 536, 577, 51264], "temperature": 0.0, "avg_logprob": -0.14270121710641043, "compression_ratio": 1.4947916666666667, "no_speech_prob": 0.007775104139000177}, {"id": 54, "seek": 40600, "start": 424.0, "end": 434.48, "text": " it's going to work. So Neil it will be easier. So here we can see the UI with some pre-loaded data", "tokens": [51264, 309, 311, 516, 281, 589, 13, 407, 18615, 309, 486, 312, 3571, 13, 407, 510, 321, 393, 536, 264, 15682, 365, 512, 659, 12, 2907, 292, 1412, 51788], "temperature": 0.0, "avg_logprob": -0.14270121710641043, "compression_ratio": 1.4947916666666667, "no_speech_prob": 0.007775104139000177}, {"id": 55, "seek": 43600, "start": 436.4, "end": 441.44, "text": " and we can see that we have basically what we call six products here which are actually", "tokens": [50384, 293, 321, 393, 536, 300, 321, 362, 1936, 437, 321, 818, 2309, 3383, 510, 597, 366, 767, 50636], "temperature": 0.0, "avg_logprob": -0.17296797037124634, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.01996275782585144}, {"id": 56, "seek": 43600, "start": 441.44, "end": 448.4, "text": " six S-bombs that are already already ingested in the system and a large number of CVs that", "tokens": [50636, 2309, 318, 12, 65, 298, 929, 300, 366, 1217, 1217, 3957, 21885, 294, 264, 1185, 293, 257, 2416, 1230, 295, 22995, 82, 300, 50984], "temperature": 0.0, "avg_logprob": -0.17296797037124634, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.01996275782585144}, {"id": 57, "seek": 43600, "start": 449.76, "end": 457.36, "text": " have been collected from multiple sources and we can see that we identified around 2000 packages", "tokens": [51052, 362, 668, 11087, 490, 3866, 7139, 293, 321, 393, 536, 300, 321, 9234, 926, 8132, 17401, 51432], "temperature": 0.0, "avg_logprob": -0.17296797037124634, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.01996275782585144}, {"id": 58, "seek": 45736, "start": 457.84000000000003, "end": 467.12, "text": " for these S-bombs and most importantly from the Vex files ingested here we identified 29", "tokens": [50388, 337, 613, 318, 12, 65, 298, 929, 293, 881, 8906, 490, 264, 691, 3121, 7098, 3957, 21885, 510, 321, 9234, 9413, 50852], "temperature": 0.0, "avg_logprob": -0.13928323249294333, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.04233011230826378}, {"id": 59, "seek": 45736, "start": 468.08000000000004, "end": 477.28000000000003, "text": " advisories for these. So if we go to a certain product we can see a couple of information", "tokens": [50900, 10280, 2083, 337, 613, 13, 407, 498, 321, 352, 281, 257, 1629, 1674, 321, 393, 536, 257, 1916, 295, 1589, 51360], "temperature": 0.0, "avg_logprob": -0.13928323249294333, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.04233011230826378}, {"id": 60, "seek": 45736, "start": 478.88, "end": 486.72, "text": " obtained from the S-bombs so we can see the basic metadata that we have. Usually we can see all", "tokens": [51440, 14879, 490, 264, 318, 12, 65, 298, 929, 370, 321, 393, 536, 264, 3875, 26603, 300, 321, 362, 13, 11419, 321, 393, 536, 439, 51832], "temperature": 0.0, "avg_logprob": -0.13928323249294333, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.04233011230826378}, {"id": 61, "seek": 48672, "start": 486.72, "end": 493.04, "text": " the packages and how they relate to each other. I think this S-bombs is pretty flat in structure so", "tokens": [50364, 264, 17401, 293, 577, 436, 10961, 281, 1184, 661, 13, 286, 519, 341, 318, 12, 65, 298, 929, 307, 1238, 4962, 294, 3877, 370, 50680], "temperature": 0.0, "avg_logprob": -0.08928173925818467, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.009325890801846981}, {"id": 62, "seek": 48672, "start": 493.04, "end": 500.48, "text": " there's no much dependency going on there but the most important thing is that we can see", "tokens": [50680, 456, 311, 572, 709, 33621, 516, 322, 456, 457, 264, 881, 1021, 551, 307, 300, 321, 393, 536, 51052], "temperature": 0.0, "avg_logprob": -0.08928173925818467, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.009325890801846981}, {"id": 63, "seek": 48672, "start": 501.04, "end": 507.84000000000003, "text": " different kinds of advisories that are against and also immediately see which", "tokens": [51080, 819, 3685, 295, 10280, 2083, 300, 366, 1970, 293, 611, 4258, 536, 597, 51420], "temperature": 0.0, "avg_logprob": -0.08928173925818467, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.009325890801846981}, {"id": 64, "seek": 48672, "start": 509.28000000000003, "end": 516.08, "text": " actual packages are being affected by these advisories. We can go back and forth through", "tokens": [51492, 3539, 17401, 366, 885, 8028, 538, 613, 10280, 2083, 13, 492, 393, 352, 646, 293, 5220, 807, 51832], "temperature": 0.0, "avg_logprob": -0.08928173925818467, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.009325890801846981}, {"id": 65, "seek": 51608, "start": 516.08, "end": 523.9200000000001, "text": " this system so we can go to the actual package see that it's actually affected by this vulnerability.", "tokens": [50364, 341, 1185, 370, 321, 393, 352, 281, 264, 3539, 7372, 536, 300, 309, 311, 767, 8028, 538, 341, 24210, 13, 50756], "temperature": 0.0, "avg_logprob": -0.11711247343766062, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.011754008941352367}, {"id": 66, "seek": 51608, "start": 523.9200000000001, "end": 530.8000000000001, "text": " We can also go from the package and find the S-bombs that it belongs to, the S-bombs or the product", "tokens": [50756, 492, 393, 611, 352, 490, 264, 7372, 293, 915, 264, 318, 12, 65, 298, 929, 300, 309, 12953, 281, 11, 264, 318, 12, 65, 298, 929, 420, 264, 1674, 51100], "temperature": 0.0, "avg_logprob": -0.11711247343766062, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.011754008941352367}, {"id": 67, "seek": 51608, "start": 531.36, "end": 538.88, "text": " but also what we can provide is that nice search capability as we said like maybe at some point", "tokens": [51128, 457, 611, 437, 321, 393, 2893, 307, 300, 1481, 3164, 13759, 382, 321, 848, 411, 1310, 412, 512, 935, 51504], "temperature": 0.0, "avg_logprob": -0.11711247343766062, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.011754008941352367}, {"id": 68, "seek": 53888, "start": 539.28, "end": 544.48, "text": " you don't remember exact vulnerability we're looking for so you can basically", "tokens": [50384, 291, 500, 380, 1604, 1900, 24210, 321, 434, 1237, 337, 370, 291, 393, 1936, 50644], "temperature": 0.0, "avg_logprob": -0.17322273254394532, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.01578194834291935}, {"id": 69, "seek": 53888, "start": 547.76, "end": 558.48, "text": " just do a full text search or maybe yeah and find that there's a packages related to that but also", "tokens": [50808, 445, 360, 257, 1577, 2487, 3164, 420, 1310, 1338, 293, 915, 300, 456, 311, 257, 17401, 4077, 281, 300, 457, 611, 51344], "temperature": 0.0, "avg_logprob": -0.17322273254394532, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.01578194834291935}, {"id": 70, "seek": 53888, "start": 558.48, "end": 567.36, "text": " find the exact vulnerabilities that we talked about a little bit earlier. So this is just", "tokens": [51344, 915, 264, 1900, 37633, 300, 321, 2825, 466, 257, 707, 857, 3071, 13, 407, 341, 307, 445, 51788], "temperature": 0.0, "avg_logprob": -0.17322273254394532, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.01578194834291935}, {"id": 71, "seek": 56736, "start": 567.36, "end": 575.76, "text": " like a basic demo right? I have a little bit more time just to explain so what were the challenges", "tokens": [50364, 411, 257, 3875, 10723, 558, 30, 286, 362, 257, 707, 857, 544, 565, 445, 281, 2903, 370, 437, 645, 264, 4759, 50784], "temperature": 0.0, "avg_logprob": -0.13791241365320542, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.012261183932423592}, {"id": 72, "seek": 56736, "start": 575.76, "end": 583.2, "text": " for us and I think we heard in a lot of sessions all about these challenges so it's mostly", "tokens": [50784, 337, 505, 293, 286, 519, 321, 2198, 294, 257, 688, 295, 11081, 439, 466, 613, 4759, 370, 309, 311, 5240, 51156], "temperature": 0.0, "avg_logprob": -0.13791241365320542, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.012261183932423592}, {"id": 73, "seek": 56736, "start": 583.2, "end": 590.4, "text": " still early adopters everywhere, tools are immature including the project I'm working on so we definitely", "tokens": [51156, 920, 2440, 22486, 1559, 5315, 11, 3873, 366, 49539, 3009, 264, 1716, 286, 478, 1364, 322, 370, 321, 2138, 51516], "temperature": 0.0, "avg_logprob": -0.13791241365320542, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.012261183932423592}, {"id": 74, "seek": 59040, "start": 591.36, "end": 598.0, "text": " don't consider it mature but also there's a lot of inconsistency in the data wherever you look right", "tokens": [50412, 500, 380, 1949, 309, 14442, 457, 611, 456, 311, 257, 688, 295, 22039, 468, 3020, 294, 264, 1412, 8660, 291, 574, 558, 50744], "temperature": 0.0, "avg_logprob": -0.13669430641900926, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.026384996250271797}, {"id": 75, "seek": 59040, "start": 598.0, "end": 604.24, "text": " so we heard today about all the multiple computing formats in S-bombs space and all the work that", "tokens": [50744, 370, 321, 2198, 965, 466, 439, 264, 3866, 15866, 25879, 294, 318, 12, 65, 298, 929, 1901, 293, 439, 264, 589, 300, 51056], "temperature": 0.0, "avg_logprob": -0.13669430641900926, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.026384996250271797}, {"id": 76, "seek": 59040, "start": 604.24, "end": 609.36, "text": " people are doing to bring that more closer and together over time which I think is awesome.", "tokens": [51056, 561, 366, 884, 281, 1565, 300, 544, 4966, 293, 1214, 670, 565, 597, 286, 519, 307, 3476, 13, 51312], "temperature": 0.0, "avg_logprob": -0.13669430641900926, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.026384996250271797}, {"id": 77, "seek": 59040, "start": 610.4, "end": 613.52, "text": " We also heard a nice discussion about all the different kind of", "tokens": [51364, 492, 611, 2198, 257, 1481, 5017, 466, 439, 264, 819, 733, 295, 51520], "temperature": 0.0, "avg_logprob": -0.13669430641900926, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.026384996250271797}, {"id": 78, "seek": 61352, "start": 614.24, "end": 620.96, "text": " identifiers and you can see so if you work only with one source of data then it's easier but then", "tokens": [50400, 2473, 23463, 293, 291, 393, 536, 370, 498, 291, 589, 787, 365, 472, 4009, 295, 1412, 550, 309, 311, 3571, 457, 550, 50736], "temperature": 0.0, "avg_logprob": -0.12473168999257714, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.08023088425397873}, {"id": 79, "seek": 61352, "start": 620.96, "end": 627.68, "text": " if you try to correlate this S-bomb with this Vex file and this S-bomb is using PURELs and these", "tokens": [50736, 498, 291, 853, 281, 48742, 341, 318, 12, 65, 3548, 365, 341, 691, 3121, 3991, 293, 341, 318, 12, 65, 3548, 307, 1228, 430, 7932, 3158, 82, 293, 613, 51072], "temperature": 0.0, "avg_logprob": -0.12473168999257714, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.08023088425397873}, {"id": 80, "seek": 61352, "start": 627.68, "end": 635.1999999999999, "text": " are the CPEs it's becoming impossible to correlate data and build the graph basically", "tokens": [51072, 366, 264, 383, 5208, 82, 309, 311, 5617, 6243, 281, 48742, 1412, 293, 1322, 264, 4295, 1936, 51448], "temperature": 0.0, "avg_logprob": -0.12473168999257714, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.08023088425397873}, {"id": 81, "seek": 61352, "start": 636.64, "end": 642.96, "text": " properly. Also what we found is that even all these things are standards there's a lot of unwritten", "tokens": [51520, 6108, 13, 2743, 437, 321, 1352, 307, 300, 754, 439, 613, 721, 366, 7787, 456, 311, 257, 688, 295, 517, 26859, 51836], "temperature": 0.0, "avg_logprob": -0.12473168999257714, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.08023088425397873}, {"id": 82, "seek": 64296, "start": 642.96, "end": 650.0, "text": " rules in all the organizations about how they are presenting their data so the documents will pass", "tokens": [50364, 4474, 294, 439, 264, 6150, 466, 577, 436, 366, 15578, 641, 1412, 370, 264, 8512, 486, 1320, 50716], "temperature": 0.0, "avg_logprob": -0.1709527525790902, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.018055832013487816}, {"id": 83, "seek": 64296, "start": 650.0, "end": 657.2, "text": " but what you have as an information from the document really depends so I think yeah it's good", "tokens": [50716, 457, 437, 291, 362, 382, 364, 1589, 490, 264, 4166, 534, 5946, 370, 286, 519, 1338, 309, 311, 665, 51076], "temperature": 0.0, "avg_logprob": -0.1709527525790902, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.018055832013487816}, {"id": 84, "seek": 64296, "start": 657.2, "end": 664.1600000000001, "text": " that you're all here and there's a lot of things to do right because it's early early days. For the", "tokens": [51076, 300, 291, 434, 439, 510, 293, 456, 311, 257, 688, 295, 721, 281, 360, 558, 570, 309, 311, 2440, 2440, 1708, 13, 1171, 264, 51424], "temperature": 0.0, "avg_logprob": -0.1709527525790902, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.018055832013487816}, {"id": 85, "seek": 64296, "start": 664.1600000000001, "end": 669.76, "text": " project itself we'll try to additionally simplify architecture and the deployment model we're all", "tokens": [51424, 1716, 2564, 321, 603, 853, 281, 43181, 20460, 9482, 293, 264, 19317, 2316, 321, 434, 439, 51704], "temperature": 0.0, "avg_logprob": -0.1709527525790902, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.018055832013487816}, {"id": 86, "seek": 66976, "start": 669.76, "end": 677.04, "text": " about microservices and Kubernetes for now which is okay but I think we could reach much more", "tokens": [50364, 466, 15547, 47480, 293, 23145, 337, 586, 597, 307, 1392, 457, 286, 519, 321, 727, 2524, 709, 544, 50728], "temperature": 0.0, "avg_logprob": -0.10897917561716848, "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.02421775460243225}, {"id": 87, "seek": 66976, "start": 677.04, "end": 683.36, "text": " people with simplifying how much resources and where they can deploy a project like this", "tokens": [50728, 561, 365, 6883, 5489, 577, 709, 3593, 293, 689, 436, 393, 7274, 257, 1716, 411, 341, 51044], "temperature": 0.0, "avg_logprob": -0.10897917561716848, "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.02421775460243225}, {"id": 88, "seek": 66976, "start": 684.24, "end": 693.28, "text": " and go into supporting more standards. So you saw here just basic searches and basic", "tokens": [51088, 293, 352, 666, 7231, 544, 7787, 13, 407, 291, 1866, 510, 445, 3875, 26701, 293, 3875, 51540], "temperature": 0.0, "avg_logprob": -0.10897917561716848, "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.02421775460243225}, {"id": 89, "seek": 66976, "start": 693.28, "end": 698.88, "text": " correlation I think once we have much more data in the system we can get much more", "tokens": [51540, 20009, 286, 519, 1564, 321, 362, 709, 544, 1412, 294, 264, 1185, 321, 393, 483, 709, 544, 51820], "temperature": 0.0, "avg_logprob": -0.10897917561716848, "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.02421775460243225}, {"id": 90, "seek": 69888, "start": 698.88, "end": 707.12, "text": " vision from all this data in and provide that as that's the value of the project in my opinion", "tokens": [50364, 5201, 490, 439, 341, 1412, 294, 293, 2893, 300, 382, 300, 311, 264, 2158, 295, 264, 1716, 294, 452, 4800, 50776], "temperature": 0.0, "avg_logprob": -0.14448935811112568, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.009435356594622135}, {"id": 91, "seek": 69888, "start": 707.12, "end": 714.0, "text": " right and continue working on the future integrations because in my mind if you do continue doing this", "tokens": [50776, 558, 293, 2354, 1364, 322, 264, 2027, 3572, 763, 570, 294, 452, 1575, 498, 291, 360, 2354, 884, 341, 51120], "temperature": 0.0, "avg_logprob": -0.14448935811112568, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.009435356594622135}, {"id": 92, "seek": 69888, "start": 714.0, "end": 719.68, "text": " right I think at some point in a couple years all these infrastructures should be invisible to", "tokens": [51120, 558, 286, 519, 412, 512, 935, 294, 257, 1916, 924, 439, 613, 6534, 44513, 820, 312, 14603, 281, 51404], "temperature": 0.0, "avg_logprob": -0.14448935811112568, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.009435356594622135}, {"id": 93, "seek": 69888, "start": 719.68, "end": 724.72, "text": " developers right so it should be part of your developer toolchain automatically working in", "tokens": [51404, 8849, 558, 370, 309, 820, 312, 644, 295, 428, 10754, 2290, 11509, 6772, 1364, 294, 51656], "temperature": 0.0, "avg_logprob": -0.14448935811112568, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.009435356594622135}, {"id": 94, "seek": 72472, "start": 724.72, "end": 731.6800000000001, "text": " VS code in all the Git for pipelines and everything right so we are just beginning", "tokens": [50364, 25091, 3089, 294, 439, 264, 16939, 337, 40168, 293, 1203, 558, 370, 321, 366, 445, 2863, 50712], "temperature": 0.0, "avg_logprob": -0.22796667627541414, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.005511270370334387}, {"id": 95, "seek": 72472, "start": 732.5600000000001, "end": 738.72, "text": " that's it so justification side doesn't have too much data saying about immature projects but", "tokens": [50756, 300, 311, 309, 370, 31591, 1252, 1177, 380, 362, 886, 709, 1412, 1566, 466, 49539, 4455, 457, 51064], "temperature": 0.0, "avg_logprob": -0.22796667627541414, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.005511270370334387}, {"id": 96, "seek": 72472, "start": 738.72, "end": 744.64, "text": " there's a dev box sandbox that you can try there's a code there and we always on the", "tokens": [51064, 456, 311, 257, 1905, 2424, 42115, 300, 291, 393, 853, 456, 311, 257, 3089, 456, 293, 321, 1009, 322, 264, 51360], "temperature": 0.0, "avg_logprob": -0.22796667627541414, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.005511270370334387}, {"id": 97, "seek": 72472, "start": 744.64, "end": 751.2, "text": " metric channel so if you're interested please reach out and yeah. I'm going to ask the question", "tokens": [51360, 20678, 2269, 370, 498, 291, 434, 3102, 1767, 2524, 484, 293, 1338, 13, 286, 478, 516, 281, 1029, 264, 1168, 51688], "temperature": 0.0, "avg_logprob": -0.22796667627541414, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.005511270370334387}, {"id": 98, "seek": 75120, "start": 752.1600000000001, "end": 755.44, "text": " are you using the SPX libraries for helping with the ingestion?", "tokens": [50412, 366, 291, 1228, 264, 8420, 55, 15148, 337, 4315, 365, 264, 3957, 31342, 30, 50576], "temperature": 0.0, "avg_logprob": -0.31606411933898926, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.012873868457973003}, {"id": 99, "seek": 75120, "start": 757.0400000000001, "end": 764.32, "text": " No no we're using yeah sorry yeah the question is are we using existing SPX libraries yes we are", "tokens": [50656, 883, 572, 321, 434, 1228, 1338, 2597, 1338, 264, 1168, 307, 366, 321, 1228, 6741, 8420, 55, 15148, 2086, 321, 366, 51020], "temperature": 0.0, "avg_logprob": -0.31606411933898926, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.012873868457973003}, {"id": 100, "seek": 75120, "start": 764.32, "end": 771.0400000000001, "text": " yeah so there's one in Golan using in in guac but there is also in Rust one using the classification", "tokens": [51020, 1338, 370, 456, 311, 472, 294, 460, 23754, 1228, 294, 294, 695, 326, 457, 456, 307, 611, 294, 34952, 472, 1228, 264, 21538, 51356], "temperature": 0.0, "avg_logprob": -0.31606411933898926, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.012873868457973003}, {"id": 101, "seek": 75120, "start": 771.0400000000001, "end": 776.8000000000001, "text": " itself because they are good yeah. So why is the reason that you decided to start a project from", "tokens": [51356, 2564, 570, 436, 366, 665, 1338, 13, 407, 983, 307, 264, 1778, 300, 291, 3047, 281, 722, 257, 1716, 490, 51644], "temperature": 0.0, "avg_logprob": -0.31606411933898926, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.012873868457973003}, {"id": 102, "seek": 77680, "start": 776.8, "end": 781.52, "text": " the ground instead of help at least four or five open source projects big ones that already do", "tokens": [50364, 264, 2727, 2602, 295, 854, 412, 1935, 1451, 420, 1732, 1269, 4009, 4455, 955, 2306, 300, 1217, 360, 50600], "temperature": 0.0, "avg_logprob": -0.13625876108805338, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.061063095927238464}, {"id": 103, "seek": 77680, "start": 781.52, "end": 786.4, "text": " exactly what they do but not yet on the level but mostly 90 percent that we are doing today.", "tokens": [50600, 2293, 437, 436, 360, 457, 406, 1939, 322, 264, 1496, 457, 5240, 4289, 3043, 300, 321, 366, 884, 965, 13, 50844], "temperature": 0.0, "avg_logprob": -0.13625876108805338, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.061063095927238464}, {"id": 104, "seek": 77680, "start": 786.4, "end": 793.52, "text": " Why you not helping that one instead of creating one? So yeah why we are starting a new project", "tokens": [50844, 1545, 291, 406, 4315, 300, 472, 2602, 295, 4084, 472, 30, 407, 1338, 983, 321, 366, 2891, 257, 777, 1716, 51200], "temperature": 0.0, "avg_logprob": -0.13625876108805338, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.061063095927238464}, {"id": 105, "seek": 77680, "start": 793.52, "end": 799.92, "text": " instead of instead of helping others so first of all we joined the guac project which is also", "tokens": [51200, 2602, 295, 2602, 295, 4315, 2357, 370, 700, 295, 439, 321, 6869, 264, 695, 326, 1716, 597, 307, 611, 51520], "temperature": 0.0, "avg_logprob": -0.13625876108805338, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.061063095927238464}, {"id": 106, "seek": 79992, "start": 800.0, "end": 807.12, "text": " another new project but yeah I can't answer that I mean a lot of people were involved in that kind", "tokens": [50368, 1071, 777, 1716, 457, 1338, 286, 393, 380, 1867, 300, 286, 914, 257, 688, 295, 561, 645, 3288, 294, 300, 733, 50724], "temperature": 0.0, "avg_logprob": -0.16070556640625, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.06830593198537827}, {"id": 107, "seek": 79992, "start": 807.12, "end": 813.12, "text": " of decision but we are trying to be as much I mean it's all open source we are contributing to other", "tokens": [50724, 295, 3537, 457, 321, 366, 1382, 281, 312, 382, 709, 286, 914, 309, 311, 439, 1269, 4009, 321, 366, 19270, 281, 661, 51024], "temperature": 0.0, "avg_logprob": -0.16070556640625, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.06830593198537827}, {"id": 108, "seek": 79992, "start": 813.12, "end": 822.0, "text": " projects so it's not a closed source product basically yeah. So one of your early slides said", "tokens": [51024, 4455, 370, 309, 311, 406, 257, 5395, 4009, 1674, 1936, 1338, 13, 407, 472, 295, 428, 2440, 9788, 848, 51468], "temperature": 0.0, "avg_logprob": -0.16070556640625, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.06830593198537827}, {"id": 109, "seek": 79992, "start": 822.0, "end": 827.1999999999999, "text": " this can be used to sort of share S-bomb data can you talk a little about that feature how", "tokens": [51468, 341, 393, 312, 1143, 281, 1333, 295, 2073, 318, 12, 65, 3548, 1412, 393, 291, 751, 257, 707, 466, 300, 4111, 577, 51728], "temperature": 0.0, "avg_logprob": -0.16070556640625, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.06830593198537827}, {"id": 110, "seek": 82720, "start": 828.0, "end": 831.5200000000001, "text": " you this can be used to sort of send S-bomb data around to other projects?", "tokens": [50404, 291, 341, 393, 312, 1143, 281, 1333, 295, 2845, 318, 12, 65, 3548, 1412, 926, 281, 661, 4455, 30, 50580], "temperature": 0.0, "avg_logprob": -0.1733654858021254, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.009329807944595814}, {"id": 111, "seek": 82720, "start": 832.48, "end": 840.48, "text": " So it's not about yeah sorry about it so about sharing the S-bomb data it's not about sharing", "tokens": [50628, 407, 309, 311, 406, 466, 1338, 2597, 466, 309, 370, 466, 5414, 264, 318, 12, 65, 3548, 1412, 309, 311, 406, 466, 5414, 51028], "temperature": 0.0, "avg_logprob": -0.1733654858021254, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.009329807944595814}, {"id": 112, "seek": 82720, "start": 840.48, "end": 847.6800000000001, "text": " the data but providing the API so the external systems can query things so basically the VSCode", "tokens": [51028, 264, 1412, 457, 6530, 264, 9362, 370, 264, 8320, 3652, 393, 14581, 721, 370, 1936, 264, 691, 20839, 1429, 51388], "temperature": 0.0, "avg_logprob": -0.1733654858021254, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.009329807944595814}, {"id": 113, "seek": 82720, "start": 847.6800000000001, "end": 855.2, "text": " plugin would get all the URLs from the current project and being able to query this and get", "tokens": [51388, 23407, 576, 483, 439, 264, 43267, 490, 264, 2190, 1716, 293, 885, 1075, 281, 14581, 341, 293, 483, 51764], "temperature": 0.0, "avg_logprob": -0.1733654858021254, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.009329807944595814}, {"id": 114, "seek": 85520, "start": 855.2, "end": 862.32, "text": " actionable item back so there's no any distributed sharing of the data just integration API.", "tokens": [50364, 45098, 3174, 646, 370, 456, 311, 572, 604, 12631, 5414, 295, 264, 1412, 445, 10980, 9362, 13, 50720], "temperature": 0.0, "avg_logprob": -0.3885854085286458, "compression_ratio": 1.1775700934579438, "no_speech_prob": 0.011350760236382484}, {"id": 115, "seek": 85520, "start": 864.08, "end": 871.12, "text": " Okay please thank you. Thank you.", "tokens": [50808, 1033, 1767, 1309, 291, 13, 1044, 291, 13, 51160], "temperature": 0.0, "avg_logprob": -0.3885854085286458, "compression_ratio": 1.1775700934579438, "no_speech_prob": 0.011350760236382484}], "language": "en"}