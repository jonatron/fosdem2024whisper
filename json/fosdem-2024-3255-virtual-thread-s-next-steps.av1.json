{"text": " Okay, so apologies for those who are expecting Mark Rhino to be speaking today. He couldn't make it, so I'm the stand-in for today. So you've got a stand-in speaker and a stand-in topic. So the topic is, we're actually going to talk about virtual threads, which is part of Project Loom. Charlie mentioned a bit of it in his previous talk, and we're going to talk about what we're working on in this area. So Project Loom, I'm not going to go through all of the project. There's a lot of material out there that you can actually search, and most of it is actually pretty good. At a high level, what Project Loom in OpenJDK is about is really about an upgrade to Java's concurrency model. It's about enabling much more higher-scale server applications with very easy to write code. So I'm not going to go through all of that, as I said, but the main thing I'm going to talk about is virtual threads, which is Charlie called them lightweight threads in the previous slide. It's all about having much more lighter weight type of threads for execution. So think about replacing tasks with thread, thread per task models, and thread per connection, that kind of thing. There's other things that we're working on, structuring currency and some other features, they're topics for other talks. So we've been working on this feature for a long time. This is one of these features that requires jacking up the house, replacing the foundations, putting the house back down without actually breaking anything. I think actually we've actually been mostly successful on that. It went through a couple of preview releases within 19 and 20. We made it a permanent feature in 21, being well received by the ecosystem in general. If you actually look at all of the frameworks and libraries out there, they've actually got something working with virtual threads in a very short period of time. That's one of the nice things about having things in preview for a couple of releases is it allows these frameworks to try things out and actually find issues. The big thing about it and why there's such an interest in it is because it allows applications and developers to move away from the world of scalability that they had before, where they had to go to async and reactive, which actually is incompatible with a lot of the things in the Java platform, particularly around things like debugging and just being able to get your right mental model of actually what code is executing. Overall, we're in a pretty good shape. Performance is actually in pretty good shape. Reliability is pretty good. There's a couple of rough areas around performance that we need to work on. We talk about that another time. What I do want to talk about is the other 90%. This is one of the things about these big features, is you get the first 90% done and then you've got to figure out how to get the other 90% done. We do have some quality of implementation issues. One of the compromises that was made in order to actually get this feature in is that we didn't do a nice job on Java monitors. I want to talk a bit about that today because that is by far the top pain point, the second top pain point, and the third top pain point that people actually trying out virtual threads for the last couple of years is running into. I'm guessing that anyone that actually has tried virtual threads, I've read some of the articles. People always have these gotchas type of sections in their blogs and articles and it's all about pinning. I want to talk a bit about that today. I also want to talk about a few things that we're doing around expanding the set of libraries that actually work well with virtual threads. There's other projects that we're actually working on, as I said, that's for another day. In order to actually understand some of the slides that I'm going through and the material that I'm going to talk about is, you have to have some little bit of understanding as to how virtual threads are actually implemented. There's an underlying concept in the hotspot VM which is support for delimited continuations. It's not something that's exposed generally, it's just something that's there for the underlying construct that virtual machines is actually built on. What happens is that a virtual thread essentially wraps one of these continuations so that when a thread needs to block because of a lock or an IO operation or something like that, that translates to the continuation yielding and then when it actually continues again, it's like the thread continues execution again. In order to be able to actually do a threading library in Java, you actually need to combine it with some kind of a scheduler. The scheduler that we're actually combining for now is the fork join thread pool that's been in Java for quite some time and that's actually a work stealing scheduler. We're using it in very, very different ways than it's used for things like parallel streams and we're using more kind of in FIFO mode but you get the obvious kind of default parallelism which is based on the number of cores although it is a little bit dynamic and I'll get into that in a few minutes. So mental model to think about anyway is that you've got this sort of magic with continuations, we're combining with a scheduler and that scheduler is managing a set of threads. The mental model to think about is walking around with a little kid on your shoulders. The platform thread or the carrier thread is carrying the little child around which is the virtual thread on the shoulders. When the child wants to stop and do something, you take them off, drop them down, some other adult comes along, the other guardian picks them up and moves them on their shoulders. So that's essentially sort of what to think about with virtual threads. Okay, so in order to talk through some of these slides, I'm going to use the kind of the same sort of layout in all of these slides and so what I'm going to have is, is there's going to be a little bit of code example on the left and then I'm going to show some stacks on the right side. They're color coded and I'll show you these just to give you an idea what's actually going to happen. The thread that is going to be the subject of this talk, we're actually just going to give it an ID, it's number 22. We've doubled down on thread IDs, quite a bit in the platform the last couple of years. So what we're going to do here is we're going to have a bit of code that's executed by virtual thread number 22. In this case, for the first example, we're actually going to use the same before. Just think about a semaphore as something that has a number of permits. You actually acquire a permit and, and when you're, and run some critical code and then when you're done with the permits, you actually return it back to the semaphore with a release. So typically the typical idea that you actually use is, is, is acquire and then release at the end and use it to try resources concept. Okay, so very, very simple. So let's see what actually happens when a virtual thread goes and executes this code. So the arrow here, just think that this is, this thing of the red arrow is kind of like a program point, program counter. That's kind of where we are. On the right side now we see our first sort of stack traces here. There's actually two threads in the picture here. What you actually see at the top, and by the way, the stacks are in this, in, in, in, in all of these slides grow from the top to top down. So the, the, the, the orange brownie frames that you see at the top there, they're actually the fork join pool thread that's, that's actually carrying the virtual thread. And the greens, the green frames that you see there are the virtual thread. And so I've just merged them there together because that's actually what you have on the native thread underneath the, the covers. So when we get to do, to doing a semaphore acquire, what we see is, is, is, is in green here. If you look down right at the bottom, the, the, the, the frame there is actually semaphore.acquire. So we're about to call this guy and we'll actually see what actually happens when we call semaphore acquire. Okay, so semaphore acquire gets to this point here. Now every, every good movie needs a villain and I need a villain here. So in this case, let us assume that the, the villain is Andrew and he actually has the permit for this summer for. So what's going to happen is this virtual thread has to go and park because it can't acquire a semaphore. So what we see here is, is we're going down through the Javiutil concurrent code to try to acquire the semaphore. There's no permits available. So the thread has to park. What actually happens is, it bottoms out at the bottom trying to do a park which will actually yield the continuation. And there, this is when the, this is, this is where the magic occurs is, is the, the, the, the, the, the thread is now parking and, and magically its frames get removed from the native thread stack. And the worker thread or the fork joint thread is able to go and do other work. So that's actually what happens sort of at a high level with, with, with, with virtual threads. Now Andrew is finished with his, the, the, the semaphore and is doing a release. So he's returning back the permit back to the, to the semaphore. So now we're actually going to look at what happens here now when the, when the, the, the virtual thread is actually going to continue. So remember a virtual thread is waiting to acquire the semaphore. Andrew is doing the release and that goes and triggers the, the, the, the, the, goes through the Javu-Tilkin current code and it'll bottom out then doing an unpark of the, of the virtual thread that's actually waiting, which will do the continue. What, and, and what it's actually really going to do is actually schedule the task that's associated with the virtual thread back to the scheduler so that it actually can continue again. Very, very, very kind of straightforward. It's just submitting the, the, the virtual thread back to the, to, to the scheduler so it can continue. So back to our slide again. So what happens is, is, is, is, we'll assume the scheduler has now started executing this code again. It'll return back from, from, from, from the park that we were on earlier on and magically we, we, the frames start popping off and we go into our tri block and we are done. So that's sort of thing, how, how, how things actually work with parking and unparking and how they actually integrate with the scheduler. Now we get into the sort of the, the problem areas and where we have the pain points with, with virtual threads today. So I'm going to go through two, two, two, two scenarios. One of them is, is parking while holding a monitor. I mentioned about all these, these blogs that have these gotchas at the end. This is essentially what they're actually trying to show you in, in, in, in these blogs. So we've taken the same example but we've actually put it into a synchronized block. So the synchronized block here is, is, so that's kind of, think of that as a monitor enter, here's a monitor exit. Same thing if this was a, a synchronized method. The code that we had in the previous, the previous section is, is, is exactly the same. So what happens is, is we're going to do the, going to do the, the choir here. Andrew R. Villan again is actually holding the, the, the, the permit for the semaphore. So this virtual thread has to, has, has to go and park. So what happens this time is we're going to try to park but we're actually holding a monitor. So, so this yield down the bottom fails. Why does it fail? Well, we, we get into why it actually fails in, in, in a second. But something, we're not able to actually go and release the, the, the, the, the, the, the, the, the, the, the, the, the carrier thread to do other work here. This is actually why we have, you get performance issues and, and why we say that monitors lead to a quality implementation issue is because of this, this, this issue here. Now what actually happens in this particular case is, is that instead of actually failing, it actually falls back to actually to park on the carrier and the, the, the, the, the semaphore in this case works exactly the same as, as, as, as this would if, if, if you were able to unmount, we're just not able to unmount. We can't let the, the, the carrier go away and, and, and, and do other work. And right, why, why do we have this problem? And so we have this problem because of the way that monitors are implemented in, in the, in, in the Java VM. There's different, there's different kind of locking modes. A lot of this is sort of beyond where, where I typically work. But in, in, in Roman's talk earlier on, he actually, actually shows some of this where, where, the fast locking type is essentially, is essentially putting a pointer into the object back into a lock record that's actually in the, in the stack. Oh, if we, that, then we can't actually start removing frames that, when, when, when we, when we unmount. There's also these inflated cases where, where, you're actually building up a waiter list of who's actually waiting for the monitor. And what, what goes on to that waste list is, it's actually, it's actually the, the VM's internal Java thread, which is, is essentially the, the carrier thread in this case. So these are the reasons why, at least in this particular locking mode, that you cannot release the, the, the carrier at this point. And so there's, there's, there's, there's magic in the implementation to actually to track monitor usage to prevent this happening. There's another locking mode, which, which is, which is the, the, the, the newer one, the lightweight one where there's a, a, a per, per, per thread little stat lock. And that's got issues as well, because that's actually associated with the carrier thread, not with the virtual thread. So what we do about this, so there is a, there, there's a sort of larger, longer term effort that has, has kind of been underway. I think I saw Robin Ian here at one point. He actually started this work to actually completely re, re, reexamine and do a new implementation of Java monitors. Do it in Java, rather than actually in the VM. Because a lot of legacy code and a lot of, there's, there's a lot of history there. Now that is a longer term effort. There's a lot of unknowns, there's a lot of exploration. Both, we needed a plan B and, and, and plan B involved a hero and the hero in this case is, is, is, is, is, is Patricio in, in, in the hot spot team decided to go and have a go at actually trying to do a plan B, which is change the existing object, object monitor implementation to work with virtual threads. So, so what he actually did was, was, he's, he's, he's come up with a, well, there's, there's, there's several steps in this and this is, this is, this is by the way, this work is all actually in the, in the, in the Loom repo. So what he actually does is, is for the, for the StackLock cases, is he just, just, just inflates and then for the inflated state, he's actually for the moment has, has the VM actually doing a, a, a, a, a, a StackWalk to actually replace the owner so that it's actually not the, the, the, the Java thread, it gets, it, it, it, so the, the VM's view of the thread is actually the, the, the Java thread. And so that's a little bit expensive, but it actually does work. And, and here's a solution for the lightweight, locking mode as well, where the, the, the lock stack actually moves at, at, at, the, the, the mount and the unmount time. There's other work that's actually going on in parallel, some of the work they're calling Fillmore is actually doing about changing the, the, the, the, the lock ownership to be the thread ID. Once that work actually comes in, that means that you actually can eliminate the StackWalk, you eliminate all the GC overhead, you eliminate all of the actual overhead there of, of, of this, which is actually quite nice. So a lot of pieces from a lot of people are coming, coming, coming together, which, which is nice. So this is working in the, in the Loom repo for the moment. So we'll go back to our, our, our, our slides again and what would happen with, with that example, if we actually run it with the, some of the bills that we have from, from the Loom repo today. So when we do, we actually do our acquire, we bottom out again at the, at the, at the yield as, as before, but this time it actually succeeds. The, we release the carriage, go off and do other work, all very positive. So that is good. So that's one of the pain points with pinning and it'll be wonderful actually to, to, to, to, to, to get that in. Second scenario then is, is the contended monitor case. And that one is, is, is, is you have an example like this. In this case, this is, I've got rid of the, the, the, the, the, the, the, the semaphore from this example. And, but I'm actually going to block here. So we assume again now that Andrew is actually holding a lock this time, rather than the semaphore. And, and, and here we have our virtual thread number 22 is going to attempt to do a monitor enter at, at this point. And what happens today is, is it actually just, it actually blocks when at that monitor, at that, at that, essentially at that, at that, that monitor enter. So what's going on here is, is, is, is a contended monitor is, is, is actually a call into the runtime. It's essentially parking in the runtime. And, and, this is something that has to be, we have to remove that and essentially pop all those frames in order to be able to actually, to, to, to, to deal with this. So the way things are actually working at the moment is, is, is what, what, what Patricia has come up with is, is, is that essentially allows the VM to do a yield while it's from, from, from in the runtime. So normally we actually would do these yield from, from, from the, from the Java site. So it copies the frames off the, off, off, off the stack into the heap, just like we would with, with, with, with, with a, with a normal and, and, and, we would do it a normal yield and, and freeze. And what it does is, is it actually puts the, the virtual thread onto the wait list for the, for, for, for the, for the lock. And at a high level, it's as if we're actually doing a yield at that point. There's a bit of, there's a bit of magic that goes on with where it actually has to, has to return back as, as if it hold the, hold the lock and then you've got to run the, the, the stub that actually goes and actually does some fix up some, and there's, there's VM magic that actually happens. But essentially you're turning back to Java in a, kind of a blocking mode and then we can actually fix up the state and, and, and move it to its block mode so that the, the thread is actually blocked. So what I've put here is, is just to give you, just to kind of, you can visualize how this works. We do our monitor enter, which this could be a, a, a synchronized method as well. And it's as if we're actually calling into yield at that point. So that's actually very, very nice. That actually can work. So when Andrew releases the, the lock, then we, we just continue on and we're, and when we do that, what happens is, is Andrew releases the lock. We've now got no, no, wait for virtual thread 22. How do we actually get, how do we actually integrate back into the scheduler to get that to, to, to run again? And the way it actually works is it actually just moves the, the thread into, into, into a, into a list and, and unblocker thread will, will actually unblock it. This, so for those that have, understand how, how reference processing work in the GC, this is essentially kind of like another reference handler. It's the, it's, it's, it's, it's, it's, it's essentially queuing up, queuing up objects that are, get handled by, by, by Java thread. So the unblocker then just, it, it, it just snapshots the, however that, that, that list and, and then it just wakes up those threads, which puts it back into the, to the scheduler's queue. So that's all very nice. So when our example here is, is that Andrew has released the lock and we, we, we, we, we, we queue up that virtual thread to actually continue and it gets scheduled and it's a continuous execution inside the synchronized block. Very, very nice. So okay, high level question then, does this actually solve all of our pinning issues? And the answer is no. There's always work to do. We've got, we've got, there's, there's issues with native frames. You can't, you can't actually yield with the native frame on the continuation stack. That's not a problem. I think that, that we, we, we, we will ever, ever, ever address this, a lot of unsolvable issues there. There are other things that go along with monitors. There's object weight and it's important that we actually make progress on, on, on that one. There's ideas on how to, how to improve that one. And then there's the other more difficult one, which is class initializers. And, because class initializers is, is, will require more surgery in the VM in order to be able to address it. So these are things that are not addressed in what's in the Loom repo now, but are things that have to be addressed over the next while in order to eliminate these, these, these problems. Okay, moving on. I'm going to talk about IO now because there's a whole other set of this that, that, that, that, that goes along with, with, with, with IO. So let's talk a bit about SOCAs first because networking is, is, is actually, is, is, is actually straightforward. So here we have our virtual thread is actually going to try, attempt to make, establish a TCP connection. This case is Fosn port 443. So it's the SSL port. Okay, what happens there? So same, same, same diagram as before. We've got our carrier at the top and then we've got our green, green frames for the virtual thread. We're doing a socket, we're in the socket constructor, which actually initiates the connect. What does the connect actually do when you're on a virtual thread? It's actually going to initiate the connection, arm the file descriptor and then do a yield and to, to, so, so that the, so that the, the, the, the carrier can be released to actually go and do other work. So this is what our stack would actually look like. It goes down through the, the, the, the IO code and, and IO code and, and does, does, does, does our, does our yield. Carrier gets released to go and do other work. We're all good. What we have then in the, in the background is, is, is the way things work for the moment is there is this thing called a polar thread. The polar thread interacts with whatever the IO mechanism is on, on the, on the platform. There's implementations for, for, for, for E-Pole and, and KQ. There's one that integrates with the, the Windows WinSock driver. And what it does is it's just listening for events. When there's events from the operating system to tell you that the, that these are already events, then it just unblocks the corresponding virtual thread by, well, what it does is it actually just, it, it, it unparks it and just queues its task to the schedule and things that work. That little diagram over there, did you see that it was spinning? I'm going to reuse that in a couple of slides just to make, essentially all that's doing is listening for events on parking throughout, listening for events on parking throughout, listening for events and so on. That's all it does. And so, but back to this, back, back, back, back to our example here is, is we're, we're trying to establish a connection. The connection is now established and because we've done a wake up, we pop all the frames, we're, we're, we're, we're gone past the socket and constructor now and we're, we're, we're all good. So this is kind of the way things work today in, in, in JDK 21. You've got this, you've got this thread that's picking up IO events. It's queuing up and the corresponding virtual threads to, to, to, to the schedule, in this case I should, I did, depict it as a, as, as a box full of carrier threads. That's actually not all that efficient because a lot, there's a number of issues with this one. You actually start scaling things, scaling things up and particularly is, is, is that you've got a, you've got a number of carrier threads that correspond to the number cores and then you've got these other polar threads that are actually trying to compete for CPU cycles. You've also got the issue where, where you're picking up IO events on one thread but it's actually going to end up being processed on a different thread. So this is the, the, the, the, there's room for, for, for efficiency on, on this. So one of the things we actually have done in, in, in, in, in JDK 22 is we magically move these polar threads at least on some platforms from being platform threads to being virtual threads. Now the implications for that is they actually integrate then with the schedule. You're only picking up, you're only picking up IO events and queuing up virtual threads to on-park when there's actually cycles to actually to go and do that. In addition, because of the way this polar thread is actually written is it will actually, most of the time, continue the IO operation on the same thread that picked up the event. So you avoid having to actually go and dealing with events to, going, going, going, going between threads. When you actually scale it up, this is actually kind of what it actually goes and looks like is there's actually in, in polar threads that are virtual threads are actually, and, and then there's this, there's this other gutter guy in the background which is when it has to wake up polar threads, when there's nothing else for them to, to, to go, to go and do. And this is actually quite nice. There's, there's a nice paper written by them. There's a, there's, there's, there's a team in the University of Waterloo that actually work in, in, in the sort of similar area on their own library of, of for lightweight threads in C++. And they've got an initial paper which deals with all the IO strategies. And this is kind of one of the IO strategies that, that they, they're, they're also using by default. So, so that's Martin, and Karinson's team in, in Waterloo. It's the paper is, is, have your cake and eat it, which is a great title for, for, for, for a paper. So, so this is actually turns out, and some, and some benchmarks turns out to be actually quite, very, very profitable. And because, and so this, this, this is just a, some random benchmark that's actually sending, sending a 1K request and getting a 16K response. It's, it's on the loop back. There's, there's a client and, and, and, and, and a server thread for each one of these. So there's a lot of parking actually going on. There's a lot of IO between them. But the nice thing is, is we actually see improvements, significant improvements on, on, on, on, on, on, on arranging systems, which is actually quite good. Okay. Mo, moving on a lot, I want to talk about a bit of file IO, because this is where we actually put on our sad face. Because it's, it's not as, not as, not as good a story. So the example I'm actually using on this one is, is, is, is, is, I'm, I'm opening a file. So this is actually, this is code executing on a virtual thread. There's, I'm, I'm doing a file open here and I'm doing a file read. So two, two, two file operations. Down on the right, I've got just, I've just showing the, the, the box with the, let's assume this is a four core system. This four carrier thread is actually sitting there. So what actually happens today, and it's, it's, it's a bit lame, but I'm just explaining with the way it actually works today, is when you attempt to do a file IO operation that may actually consume the thread, it temporarily increases the parallelism, which will trigger an additional worker thread to be available to do other work. So if you've ever seen fork joint pool manage blocker, this is essentially the same thing which your actual compensation that actually can happen in managed blocking operations. So you, you do your file open, your thread is actually not, unavailable to do other work. And then when, when, when the file IO operation completes, you decrement the parallelism again, and what happens is, is then is that the, the, the number of worker threads that are available reverts back to where it actually was. These additional extra worker threads that might get created for these, they might hang around for a little while, but they will eventually, they, they, they, they, they, they will eventually terminate once the system is acquiescent. So same thing, what happens when we had to, to do a file IO, or sorry, we do a read, same kind of thing, increase the parallelism, and we do our read, and the once, once the read is actually complete, we will decrement the parallelism again. So this is all kind of lame, and you say, well, why haven't we done any better on this? So this is one of the things that we actually have been playing around with for, for quite some time. There are asynchronous IO interfaces on different operating systems. I'll just talk about what, for IO, you ring today. I'm just talking about in the context of file, we've also looked at it in the context of, of, of, of Socus as well. So in the Linux operating system, there is, there's, there's a completely different type of, of interface to the operating system, which is, which is supports asynchronous IO operations, is essentially, essentially allows the trap into the kernel. And so you've actually, you, you, you can actually queue up submissions in, in, in, in memory, and any events that are associated with those when they're complete are actually queued up in a ring. That's also, so it's successful from both the, the kernel and, and, and user space. So there's, there's a lot of issues and a lot of complications trying to interface this to the, to, to, to the JDK to be able to support a lot of the, the, the libraries. So what we have been doing, and this is in the, in the open JDK sandboxes, we have an, we have a low level API, which sits on top of the work that Maurizio was talking about in the previous slide, a previous presentation with an FFM. And that will provide the lower level access to the, the, the submission and the, the, the, the, the completion rings. And it, it, it, it, it, it hides a lot of that, that, that, that kind of detail. Now, in order to be able to work through some virtual threads, we actually have to do, we have to do a bit more replacing of foundations around the place. So there's a lot of prototype re-implementation of a lot of the Java, Java I.O. classes. And to be able to work with this as well. And that actually allows the, the problem of file I.O. to be actually reduced down very significantly. None of this, none of the completed bits for this are in the Loom repo yet, but we will, we will get there eventually. There's a bunch of design choices that, that have to be worked out when you're actually interfacing with something like I.O. U-ring because as to which threads can actually access the, the, it's, I.O. U-ring is not kind of designed for, for multiple threads to be, to be accessing a ring at the same time. So what we will end up doing is, is, is, is, is, is, is having essentially multiple I.O. ring instances and one per, per carrier essentially. And that actually fixes the completion, the, the submission side. The completion side is a little bit more complicated and there's a number of design choices around that. So the main message here is, is that there's a lot going on in this area as well because all of that areas of the libraries have to, have, have to, have to play cleanly with them, with virtual threads as well. Okay, so I'm not going to go through all of the other things that are going on, but I'll just talk, just, just, just, just mention a few of them. So Professor Doug Lee who's the, the, the sort of the, the, the world expert on concurrency is doing quite a bit of work at the, exploration into fork joint pool at the moment. That is for scenarios where you have a smaller number of cores, because a lot of, a lot of container and, and, and the cloudy type systems you're running with two or you're running with four cores. And we've, we've observed in many scenarios where you get underutilization. And a lot of of that relates to just the time it actually takes to work for, well worker threads are parking and then they have to be unparked in order to actually to actually to do work. So he's exploring a number of things on that and we're trying to come up with good benchmarks to be able to actually to measure these kind of things but this should, if this turns out to be profitable then it'll help some of these scenarios where we appear not to be having full utilization on smaller systems. JVMTI makes me scream. It's a very very invasive API and it's very much challenged by features in the Java platform where you move them out of the VM into Java. So having a native tool interface where a lot of the runtime is actually in Java rather than the VM is a challenge. We have a very good story for JVMTI and virtual threads and debugging and for many other types of tool agents but it's not working well for profiler type tools that want to use JVMTI and because there's a there's it's having to coordinate with a lot of code that's actually executing in Java. So there's a lot of work going on there to try to solve some of the problems and it's one where there's been some progress made but it's one that's going to take more time. And there's other efforts that I'm not going to talk about today which is about scope values which is essentially allows us to communicate something to a remote callee without having parameters in the call frames. Andrew Haley is actually leading that effort in Project Loom. Then we have the other big area which is structured concurrency which is all about being able to coordinate multiple threads that are decomposed running some operation is be able to actually to deal with them as a single unit of work. So there's a lot of interesting things going on there that API is currently in preview and we will have to do another preview of this for the next release. So these are other kind of efforts that are actually going on in this project at the moment. So that's kind of it. I think I've actually made it in with few minutes to spare and this is sort of links to the current JEPs that we have, the repository. When I was talking about the work on the monitors and some of the other changes around fork joint pool is they're accumulating in the Loom repo now. And yeah, okay. You'll have to hand out microphones. Okay. Hello. Hello. Hello. Test. Yes. Hello. I have to take questions here first. I think. I had a question on, can you hear me at all? Okay. So you mentioned on the, from network IO, you said you had a good solution, right? And then you said for file IO things are much trickier. But what? I'm sorry. I'm sorry. Thank you. Yeah, I'm sorry. Would you mind repeating it? Yes. So I was saying, so you mentioned network IO and you said, yeah, we have a pretty good solution here, but for file IO things are much trickier. But what's the fundamental issue that save a solution from network IO cannot be used for file IO too? Okay. So the question is, is, is why is the solution for, why can't the solution for network IO be used for file IO? And that is because there isn't the equipment sort of readiness API that you get for file, for non-blocking IO. So the reverse works, but not, what we're able to do at the moment is we're able to actually to map onto multiple different 20 years of scalable IO mechanisms for networking IO. There isn't really the equivalent for file IO. Okay. I see. Thank you. So simple question. When, when you solve these problems and get a good implementation and it's all in the next version of Java and you've, you've solved all of these problems, what do you think is going to be the impact? What's, what, what, what are you aiming for? Okay. So, so, so the, the, the ultimate goal is this for Java developers to be able to write code that, that reads exactly how it actually executes. So we need, we want to avoid having complicated, hard to, hard to read, hard to debug code that you get to, that the people are actually forced to write today with a synchronous IO are, are, are reactive. That's sort of the, the ultimate goal with this. So get the scalability with, with, with very obvious easy to read code. And at the same time as harmonious with all the other parts of the platform, such as debugging and profiling and so on. Because one of the things you actually lose today when you start going down the async type route is you, you lose so much of the, of the, of the tooling, you lose your debugging, you lose your profiling. We want to bring all of that back so that everything just works. What we have today in 21 is actually, is actually, is actually pretty good. You have a large range of applications can actually be developed and, and scale very, very well. But there's, there's, there's quite a lot of other amount of code that we want to get working with, well with virtual threads too. First, thank you so much. I think like suffering of millions will end with this holy light. My question is, when you look at the mitigation techniques that are recommended against the shortcoming of monitors, one of, like the primary thing is you're just recommending people to replace those with locks. Just use a lock. Okay. That's a, okay. That's a good observation. So in JEP 444, which is the, the, the Java enhancement proposal that introduced virtual threads as a permanent feature is, it, it, it suggests that if you're running into issues with them, with, with pinning with object monitors, you can just replace them with Java till concurrent locks. And that was very much kind of short term, short term advice in order to actually take to avoid the quality of implementation issue. But we never, we never said we'd never fix this problem. It was, it's, it's, it was, it was, it was a tactical decision to, to make the feature permanent without addressing the, the monitors issue. No, I very well understand the solution. It was just felt to me like it sounds like something machine should be doing. Like why doesn't the VM replace it with a lock on behalf of me? Right. So this, so, so, so what you're asking is, is why don't the VM magically replace it? There's a lot of issues with, with, with, with, with doing something like that. So. Okay. Thank you. Okay. Oh, there's one other. So with all the work going into addressing issues with monitors and pinning will constructs living inside Java, you till concurrent also benefit from that work or is that isolated from each other? Okay. So Java till concurrent does not base itself on, on, on, on, does, does not base itself on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on,", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.34, "text": " Okay, so apologies for those who are expecting Mark Rhino to be speaking today. He couldn't", "tokens": [50364, 1033, 11, 370, 34929, 337, 729, 567, 366, 9650, 3934, 16111, 2982, 281, 312, 4124, 965, 13, 634, 2809, 380, 50881], "temperature": 0.0, "avg_logprob": -0.1856581835242791, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.5442888736724854}, {"id": 1, "seek": 0, "start": 10.34, "end": 14.98, "text": " make it, so I'm the stand-in for today. So you've got a stand-in speaker and a stand-in", "tokens": [50881, 652, 309, 11, 370, 286, 478, 264, 1463, 12, 259, 337, 965, 13, 407, 291, 600, 658, 257, 1463, 12, 259, 8145, 293, 257, 1463, 12, 259, 51113], "temperature": 0.0, "avg_logprob": -0.1856581835242791, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.5442888736724854}, {"id": 2, "seek": 0, "start": 14.98, "end": 18.900000000000002, "text": " topic. So the topic is, we're actually going to talk about virtual threads, which is part", "tokens": [51113, 4829, 13, 407, 264, 4829, 307, 11, 321, 434, 767, 516, 281, 751, 466, 6374, 19314, 11, 597, 307, 644, 51309], "temperature": 0.0, "avg_logprob": -0.1856581835242791, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.5442888736724854}, {"id": 3, "seek": 0, "start": 18.900000000000002, "end": 23.900000000000002, "text": " of Project Loom. Charlie mentioned a bit of it in his previous talk, and we're going to", "tokens": [51309, 295, 9849, 6130, 298, 13, 13754, 2835, 257, 857, 295, 309, 294, 702, 3894, 751, 11, 293, 321, 434, 516, 281, 51559], "temperature": 0.0, "avg_logprob": -0.1856581835242791, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.5442888736724854}, {"id": 4, "seek": 0, "start": 23.900000000000002, "end": 29.6, "text": " talk about what we're working on in this area. So Project Loom, I'm not going to go through", "tokens": [51559, 751, 466, 437, 321, 434, 1364, 322, 294, 341, 1859, 13, 407, 9849, 6130, 298, 11, 286, 478, 406, 516, 281, 352, 807, 51844], "temperature": 0.0, "avg_logprob": -0.1856581835242791, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.5442888736724854}, {"id": 5, "seek": 2960, "start": 29.6, "end": 36.72, "text": " all of the project. There's a lot of material out there that you can actually search, and", "tokens": [50364, 439, 295, 264, 1716, 13, 821, 311, 257, 688, 295, 2527, 484, 456, 300, 291, 393, 767, 3164, 11, 293, 50720], "temperature": 0.0, "avg_logprob": -0.17691898345947266, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.02009841613471508}, {"id": 6, "seek": 2960, "start": 36.72, "end": 41.36, "text": " most of it is actually pretty good. At a high level, what Project Loom in OpenJDK is about", "tokens": [50720, 881, 295, 309, 307, 767, 1238, 665, 13, 1711, 257, 1090, 1496, 11, 437, 9849, 6130, 298, 294, 7238, 41, 35, 42, 307, 466, 50952], "temperature": 0.0, "avg_logprob": -0.17691898345947266, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.02009841613471508}, {"id": 7, "seek": 2960, "start": 41.36, "end": 48.16, "text": " is really about an upgrade to Java's concurrency model. It's about enabling much more higher-scale", "tokens": [50952, 307, 534, 466, 364, 11484, 281, 10745, 311, 23702, 10457, 2316, 13, 467, 311, 466, 23148, 709, 544, 2946, 12, 20033, 51292], "temperature": 0.0, "avg_logprob": -0.17691898345947266, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.02009841613471508}, {"id": 8, "seek": 2960, "start": 48.16, "end": 54.36, "text": " server applications with very easy to write code. So I'm not going to go through all of", "tokens": [51292, 7154, 5821, 365, 588, 1858, 281, 2464, 3089, 13, 407, 286, 478, 406, 516, 281, 352, 807, 439, 295, 51602], "temperature": 0.0, "avg_logprob": -0.17691898345947266, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.02009841613471508}, {"id": 9, "seek": 2960, "start": 54.36, "end": 58.32, "text": " that, as I said, but the main thing I'm going to talk about is virtual threads, which is", "tokens": [51602, 300, 11, 382, 286, 848, 11, 457, 264, 2135, 551, 286, 478, 516, 281, 751, 466, 307, 6374, 19314, 11, 597, 307, 51800], "temperature": 0.0, "avg_logprob": -0.17691898345947266, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.02009841613471508}, {"id": 10, "seek": 5832, "start": 58.68, "end": 63.68, "text": " Charlie called them lightweight threads in the previous slide. It's all about having much", "tokens": [50382, 13754, 1219, 552, 22052, 19314, 294, 264, 3894, 4137, 13, 467, 311, 439, 466, 1419, 709, 50632], "temperature": 0.0, "avg_logprob": -0.18956347612234262, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.015888657420873642}, {"id": 11, "seek": 5832, "start": 63.68, "end": 70.68, "text": " more lighter weight type of threads for execution. So think about replacing tasks with thread,", "tokens": [50632, 544, 11546, 3364, 2010, 295, 19314, 337, 15058, 13, 407, 519, 466, 19139, 9608, 365, 7207, 11, 50982], "temperature": 0.0, "avg_logprob": -0.18956347612234262, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.015888657420873642}, {"id": 12, "seek": 5832, "start": 70.68, "end": 76.48, "text": " thread per task models, and thread per connection, that kind of thing. There's other things that", "tokens": [50982, 7207, 680, 5633, 5245, 11, 293, 7207, 680, 4984, 11, 300, 733, 295, 551, 13, 821, 311, 661, 721, 300, 51272], "temperature": 0.0, "avg_logprob": -0.18956347612234262, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.015888657420873642}, {"id": 13, "seek": 5832, "start": 76.48, "end": 80.92, "text": " we're working on, structuring currency and some other features, they're topics for other", "tokens": [51272, 321, 434, 1364, 322, 11, 6594, 1345, 13346, 293, 512, 661, 4122, 11, 436, 434, 8378, 337, 661, 51494], "temperature": 0.0, "avg_logprob": -0.18956347612234262, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.015888657420873642}, {"id": 14, "seek": 5832, "start": 80.92, "end": 86.72, "text": " talks. So we've been working on this feature for a long time. This is one of these features", "tokens": [51494, 6686, 13, 407, 321, 600, 668, 1364, 322, 341, 4111, 337, 257, 938, 565, 13, 639, 307, 472, 295, 613, 4122, 51784], "temperature": 0.0, "avg_logprob": -0.18956347612234262, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.015888657420873642}, {"id": 15, "seek": 8672, "start": 86.76, "end": 90.67999999999999, "text": " that requires jacking up the house, replacing the foundations, putting the house back down", "tokens": [50366, 300, 7029, 7109, 278, 493, 264, 1782, 11, 19139, 264, 22467, 11, 3372, 264, 1782, 646, 760, 50562], "temperature": 0.0, "avg_logprob": -0.11716516017913818, "compression_ratio": 1.7015384615384614, "no_speech_prob": 0.01788371428847313}, {"id": 16, "seek": 8672, "start": 90.67999999999999, "end": 95.88, "text": " without actually breaking anything. I think actually we've actually been mostly successful", "tokens": [50562, 1553, 767, 7697, 1340, 13, 286, 519, 767, 321, 600, 767, 668, 5240, 4406, 50822], "temperature": 0.0, "avg_logprob": -0.11716516017913818, "compression_ratio": 1.7015384615384614, "no_speech_prob": 0.01788371428847313}, {"id": 17, "seek": 8672, "start": 95.88, "end": 100.56, "text": " on that. It went through a couple of preview releases within 19 and 20. We made it a permanent", "tokens": [50822, 322, 300, 13, 467, 1437, 807, 257, 1916, 295, 14281, 16952, 1951, 1294, 293, 945, 13, 492, 1027, 309, 257, 10996, 51056], "temperature": 0.0, "avg_logprob": -0.11716516017913818, "compression_ratio": 1.7015384615384614, "no_speech_prob": 0.01788371428847313}, {"id": 18, "seek": 8672, "start": 100.56, "end": 106.24, "text": " feature in 21, being well received by the ecosystem in general. If you actually look at all of", "tokens": [51056, 4111, 294, 5080, 11, 885, 731, 4613, 538, 264, 11311, 294, 2674, 13, 759, 291, 767, 574, 412, 439, 295, 51340], "temperature": 0.0, "avg_logprob": -0.11716516017913818, "compression_ratio": 1.7015384615384614, "no_speech_prob": 0.01788371428847313}, {"id": 19, "seek": 8672, "start": 106.24, "end": 110.6, "text": " the frameworks and libraries out there, they've actually got something working with virtual", "tokens": [51340, 264, 29834, 293, 15148, 484, 456, 11, 436, 600, 767, 658, 746, 1364, 365, 6374, 51558], "temperature": 0.0, "avg_logprob": -0.11716516017913818, "compression_ratio": 1.7015384615384614, "no_speech_prob": 0.01788371428847313}, {"id": 20, "seek": 8672, "start": 110.6, "end": 114.48, "text": " threads in a very short period of time. That's one of the nice things about having things", "tokens": [51558, 19314, 294, 257, 588, 2099, 2896, 295, 565, 13, 663, 311, 472, 295, 264, 1481, 721, 466, 1419, 721, 51752], "temperature": 0.0, "avg_logprob": -0.11716516017913818, "compression_ratio": 1.7015384615384614, "no_speech_prob": 0.01788371428847313}, {"id": 21, "seek": 11448, "start": 114.52000000000001, "end": 119.24000000000001, "text": " in preview for a couple of releases is it allows these frameworks to try things out and actually", "tokens": [50366, 294, 14281, 337, 257, 1916, 295, 16952, 307, 309, 4045, 613, 29834, 281, 853, 721, 484, 293, 767, 50602], "temperature": 0.0, "avg_logprob": -0.20642485845656622, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0039182244800031185}, {"id": 22, "seek": 11448, "start": 119.24000000000001, "end": 125.56, "text": " find issues. The big thing about it and why there's such an interest in it is because it allows", "tokens": [50602, 915, 2663, 13, 440, 955, 551, 466, 309, 293, 983, 456, 311, 1270, 364, 1179, 294, 309, 307, 570, 309, 4045, 50918], "temperature": 0.0, "avg_logprob": -0.20642485845656622, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0039182244800031185}, {"id": 23, "seek": 11448, "start": 126.56, "end": 133.04, "text": " applications and developers to move away from the world of scalability that they had before,", "tokens": [50968, 5821, 293, 8849, 281, 1286, 1314, 490, 264, 1002, 295, 15664, 2310, 300, 436, 632, 949, 11, 51292], "temperature": 0.0, "avg_logprob": -0.20642485845656622, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0039182244800031185}, {"id": 24, "seek": 11448, "start": 133.04, "end": 138.24, "text": " where they had to go to async and reactive, which actually is incompatible with a lot of the things", "tokens": [51292, 689, 436, 632, 281, 352, 281, 382, 34015, 293, 28897, 11, 597, 767, 307, 40393, 267, 964, 365, 257, 688, 295, 264, 721, 51552], "temperature": 0.0, "avg_logprob": -0.20642485845656622, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0039182244800031185}, {"id": 25, "seek": 11448, "start": 138.24, "end": 144.04000000000002, "text": " in the Java platform, particularly around things like debugging and just being able to", "tokens": [51552, 294, 264, 10745, 3663, 11, 4098, 926, 721, 411, 45592, 293, 445, 885, 1075, 281, 51842], "temperature": 0.0, "avg_logprob": -0.20642485845656622, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0039182244800031185}, {"id": 26, "seek": 14448, "start": 144.67999999999998, "end": 149.2, "text": " get your right mental model of actually what code is executing. Overall, we're in a pretty good", "tokens": [50374, 483, 428, 558, 4973, 2316, 295, 767, 437, 3089, 307, 32368, 13, 18420, 11, 321, 434, 294, 257, 1238, 665, 50600], "temperature": 0.0, "avg_logprob": -0.15949652989705404, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.0015759319067001343}, {"id": 27, "seek": 14448, "start": 149.2, "end": 155.12, "text": " shape. Performance is actually in pretty good shape. Reliability is pretty good. There's a couple", "tokens": [50600, 3909, 13, 25047, 307, 767, 294, 1238, 665, 3909, 13, 8738, 72, 2310, 307, 1238, 665, 13, 821, 311, 257, 1916, 50896], "temperature": 0.0, "avg_logprob": -0.15949652989705404, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.0015759319067001343}, {"id": 28, "seek": 14448, "start": 155.12, "end": 160.64, "text": " of rough areas around performance that we need to work on. We talk about that another time.", "tokens": [50896, 295, 5903, 3179, 926, 3389, 300, 321, 643, 281, 589, 322, 13, 492, 751, 466, 300, 1071, 565, 13, 51172], "temperature": 0.0, "avg_logprob": -0.15949652989705404, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.0015759319067001343}, {"id": 29, "seek": 14448, "start": 160.64, "end": 167.48, "text": " What I do want to talk about is the other 90%. This is one of the things about these big features,", "tokens": [51172, 708, 286, 360, 528, 281, 751, 466, 307, 264, 661, 4289, 6856, 639, 307, 472, 295, 264, 721, 466, 613, 955, 4122, 11, 51514], "temperature": 0.0, "avg_logprob": -0.15949652989705404, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.0015759319067001343}, {"id": 30, "seek": 14448, "start": 167.48, "end": 172.2, "text": " is you get the first 90% done and then you've got to figure out how to get the other 90% done.", "tokens": [51514, 307, 291, 483, 264, 700, 4289, 4, 1096, 293, 550, 291, 600, 658, 281, 2573, 484, 577, 281, 483, 264, 661, 4289, 4, 1096, 13, 51750], "temperature": 0.0, "avg_logprob": -0.15949652989705404, "compression_ratio": 1.7545787545787546, "no_speech_prob": 0.0015759319067001343}, {"id": 31, "seek": 17220, "start": 172.39999999999998, "end": 179.39999999999998, "text": " We do have some quality of implementation issues. One of the compromises that was made in order", "tokens": [50374, 492, 360, 362, 512, 3125, 295, 11420, 2663, 13, 1485, 295, 264, 11482, 3598, 300, 390, 1027, 294, 1668, 50724], "temperature": 0.0, "avg_logprob": -0.1631374677022298, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.003211437491700053}, {"id": 32, "seek": 17220, "start": 179.39999999999998, "end": 188.28, "text": " to actually get this feature in is that we didn't do a nice job on Java monitors. I want to talk", "tokens": [50724, 281, 767, 483, 341, 4111, 294, 307, 300, 321, 994, 380, 360, 257, 1481, 1691, 322, 10745, 26518, 13, 286, 528, 281, 751, 51168], "temperature": 0.0, "avg_logprob": -0.1631374677022298, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.003211437491700053}, {"id": 33, "seek": 17220, "start": 188.28, "end": 194.67999999999998, "text": " a bit about that today because that is by far the top pain point, the second top pain point,", "tokens": [51168, 257, 857, 466, 300, 965, 570, 300, 307, 538, 1400, 264, 1192, 1822, 935, 11, 264, 1150, 1192, 1822, 935, 11, 51488], "temperature": 0.0, "avg_logprob": -0.1631374677022298, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.003211437491700053}, {"id": 34, "seek": 17220, "start": 194.67999999999998, "end": 200.44, "text": " and the third top pain point that people actually trying out virtual threads for the last couple", "tokens": [51488, 293, 264, 2636, 1192, 1822, 935, 300, 561, 767, 1382, 484, 6374, 19314, 337, 264, 1036, 1916, 51776], "temperature": 0.0, "avg_logprob": -0.1631374677022298, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.003211437491700053}, {"id": 35, "seek": 20044, "start": 200.48, "end": 205.72, "text": " of years is running into. I'm guessing that anyone that actually has tried virtual threads,", "tokens": [50366, 295, 924, 307, 2614, 666, 13, 286, 478, 17939, 300, 2878, 300, 767, 575, 3031, 6374, 19314, 11, 50628], "temperature": 0.0, "avg_logprob": -0.20765345437186106, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.014887974597513676}, {"id": 36, "seek": 20044, "start": 205.72, "end": 210.0, "text": " I've read some of the articles. People always have these gotchas type of sections in their blogs", "tokens": [50628, 286, 600, 1401, 512, 295, 264, 11290, 13, 3432, 1009, 362, 613, 658, 41299, 2010, 295, 10863, 294, 641, 31038, 50842], "temperature": 0.0, "avg_logprob": -0.20765345437186106, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.014887974597513676}, {"id": 37, "seek": 20044, "start": 210.0, "end": 214.12, "text": " and articles and it's all about pinning. I want to talk a bit about that today. I also want to talk", "tokens": [50842, 293, 11290, 293, 309, 311, 439, 466, 5447, 773, 13, 286, 528, 281, 751, 257, 857, 466, 300, 965, 13, 286, 611, 528, 281, 751, 51048], "temperature": 0.0, "avg_logprob": -0.20765345437186106, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.014887974597513676}, {"id": 38, "seek": 20044, "start": 214.12, "end": 219.88, "text": " about a few things that we're doing around expanding the set of libraries that actually work", "tokens": [51048, 466, 257, 1326, 721, 300, 321, 434, 884, 926, 14702, 264, 992, 295, 15148, 300, 767, 589, 51336], "temperature": 0.0, "avg_logprob": -0.20765345437186106, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.014887974597513676}, {"id": 39, "seek": 20044, "start": 219.88, "end": 224.4, "text": " well with virtual threads. There's other projects that we're actually working on, as I said,", "tokens": [51336, 731, 365, 6374, 19314, 13, 821, 311, 661, 4455, 300, 321, 434, 767, 1364, 322, 11, 382, 286, 848, 11, 51562], "temperature": 0.0, "avg_logprob": -0.20765345437186106, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.014887974597513676}, {"id": 40, "seek": 22440, "start": 224.6, "end": 232.24, "text": " that's for another day. In order to actually understand some of the slides that I'm going", "tokens": [50374, 300, 311, 337, 1071, 786, 13, 682, 1668, 281, 767, 1223, 512, 295, 264, 9788, 300, 286, 478, 516, 50756], "temperature": 0.0, "avg_logprob": -0.18423131306966145, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.037621963769197464}, {"id": 41, "seek": 22440, "start": 232.24, "end": 235.52, "text": " through and the material that I'm going to talk about is, you have to have some little bit of", "tokens": [50756, 807, 293, 264, 2527, 300, 286, 478, 516, 281, 751, 466, 307, 11, 291, 362, 281, 362, 512, 707, 857, 295, 50920], "temperature": 0.0, "avg_logprob": -0.18423131306966145, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.037621963769197464}, {"id": 42, "seek": 22440, "start": 235.52, "end": 241.28, "text": " understanding as to how virtual threads are actually implemented. There's an underlying concept in", "tokens": [50920, 3701, 382, 281, 577, 6374, 19314, 366, 767, 12270, 13, 821, 311, 364, 14217, 3410, 294, 51208], "temperature": 0.0, "avg_logprob": -0.18423131306966145, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.037621963769197464}, {"id": 43, "seek": 22440, "start": 241.28, "end": 247.64000000000001, "text": " the hotspot VM which is support for delimited continuations. It's not something that's exposed", "tokens": [51208, 264, 36121, 17698, 18038, 597, 307, 1406, 337, 1103, 332, 1226, 2993, 763, 13, 467, 311, 406, 746, 300, 311, 9495, 51526], "temperature": 0.0, "avg_logprob": -0.18423131306966145, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.037621963769197464}, {"id": 44, "seek": 22440, "start": 247.64000000000001, "end": 252.44, "text": " generally, it's just something that's there for the underlying construct that virtual machines", "tokens": [51526, 5101, 11, 309, 311, 445, 746, 300, 311, 456, 337, 264, 14217, 7690, 300, 6374, 8379, 51766], "temperature": 0.0, "avg_logprob": -0.18423131306966145, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.037621963769197464}, {"id": 45, "seek": 25244, "start": 252.48, "end": 259.52, "text": " is actually built on. What happens is that a virtual thread essentially wraps one of these", "tokens": [50366, 307, 767, 3094, 322, 13, 708, 2314, 307, 300, 257, 6374, 7207, 4476, 25831, 472, 295, 613, 50718], "temperature": 0.0, "avg_logprob": -0.1741806783793885, "compression_ratio": 1.78743961352657, "no_speech_prob": 0.005047732964158058}, {"id": 46, "seek": 25244, "start": 259.52, "end": 266.76, "text": " continuations so that when a thread needs to block because of a lock or an IO operation or", "tokens": [50718, 2993, 763, 370, 300, 562, 257, 7207, 2203, 281, 3461, 570, 295, 257, 4017, 420, 364, 39839, 6916, 420, 51080], "temperature": 0.0, "avg_logprob": -0.1741806783793885, "compression_ratio": 1.78743961352657, "no_speech_prob": 0.005047732964158058}, {"id": 47, "seek": 25244, "start": 266.76, "end": 272.92, "text": " something like that, that translates to the continuation yielding and then when it actually", "tokens": [51080, 746, 411, 300, 11, 300, 28468, 281, 264, 29357, 11257, 278, 293, 550, 562, 309, 767, 51388], "temperature": 0.0, "avg_logprob": -0.1741806783793885, "compression_ratio": 1.78743961352657, "no_speech_prob": 0.005047732964158058}, {"id": 48, "seek": 25244, "start": 272.92, "end": 280.15999999999997, "text": " continues again, it's like the thread continues execution again. In order to be able to actually", "tokens": [51388, 6515, 797, 11, 309, 311, 411, 264, 7207, 6515, 15058, 797, 13, 682, 1668, 281, 312, 1075, 281, 767, 51750], "temperature": 0.0, "avg_logprob": -0.1741806783793885, "compression_ratio": 1.78743961352657, "no_speech_prob": 0.005047732964158058}, {"id": 49, "seek": 28016, "start": 280.20000000000005, "end": 283.52000000000004, "text": " do a threading library in Java, you actually need to combine it with some kind of a scheduler.", "tokens": [50366, 360, 257, 7207, 278, 6405, 294, 10745, 11, 291, 767, 643, 281, 10432, 309, 365, 512, 733, 295, 257, 12000, 260, 13, 50532], "temperature": 0.0, "avg_logprob": -0.17132227644961104, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.01019132137298584}, {"id": 50, "seek": 28016, "start": 283.52000000000004, "end": 291.96000000000004, "text": " The scheduler that we're actually combining for now is the fork join thread pool that's been", "tokens": [50532, 440, 12000, 260, 300, 321, 434, 767, 21928, 337, 586, 307, 264, 17716, 3917, 7207, 7005, 300, 311, 668, 50954], "temperature": 0.0, "avg_logprob": -0.17132227644961104, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.01019132137298584}, {"id": 51, "seek": 28016, "start": 291.96000000000004, "end": 296.36, "text": " in Java for quite some time and that's actually a work stealing scheduler. We're using it in very,", "tokens": [50954, 294, 10745, 337, 1596, 512, 565, 293, 300, 311, 767, 257, 589, 19757, 12000, 260, 13, 492, 434, 1228, 309, 294, 588, 11, 51174], "temperature": 0.0, "avg_logprob": -0.17132227644961104, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.01019132137298584}, {"id": 52, "seek": 28016, "start": 296.36, "end": 300.68, "text": " very different ways than it's used for things like parallel streams and we're using more kind of", "tokens": [51174, 588, 819, 2098, 813, 309, 311, 1143, 337, 721, 411, 8952, 15842, 293, 321, 434, 1228, 544, 733, 295, 51390], "temperature": 0.0, "avg_logprob": -0.17132227644961104, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.01019132137298584}, {"id": 53, "seek": 28016, "start": 300.68, "end": 305.68, "text": " in FIFO mode but you get the obvious kind of default parallelism which is based on the number of", "tokens": [51390, 294, 479, 12775, 46, 4391, 457, 291, 483, 264, 6322, 733, 295, 7576, 8952, 1434, 597, 307, 2361, 322, 264, 1230, 295, 51640], "temperature": 0.0, "avg_logprob": -0.17132227644961104, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.01019132137298584}, {"id": 54, "seek": 30568, "start": 305.72, "end": 310.40000000000003, "text": " cores although it is a little bit dynamic and I'll get into that in a few minutes.", "tokens": [50366, 24826, 4878, 309, 307, 257, 707, 857, 8546, 293, 286, 603, 483, 666, 300, 294, 257, 1326, 2077, 13, 50600], "temperature": 0.0, "avg_logprob": -0.1603880281801577, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.01833161897957325}, {"id": 55, "seek": 30568, "start": 310.40000000000003, "end": 317.92, "text": " So mental model to think about anyway is that you've got this sort of magic with continuations,", "tokens": [50600, 407, 4973, 2316, 281, 519, 466, 4033, 307, 300, 291, 600, 658, 341, 1333, 295, 5585, 365, 2993, 763, 11, 50976], "temperature": 0.0, "avg_logprob": -0.1603880281801577, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.01833161897957325}, {"id": 56, "seek": 30568, "start": 317.92, "end": 323.6, "text": " we're combining with a scheduler and that scheduler is managing a set of threads. The mental model", "tokens": [50976, 321, 434, 21928, 365, 257, 12000, 260, 293, 300, 12000, 260, 307, 11642, 257, 992, 295, 19314, 13, 440, 4973, 2316, 51260], "temperature": 0.0, "avg_logprob": -0.1603880281801577, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.01833161897957325}, {"id": 57, "seek": 30568, "start": 323.6, "end": 328.68, "text": " to think about is walking around with a little kid on your shoulders. The platform thread or the", "tokens": [51260, 281, 519, 466, 307, 4494, 926, 365, 257, 707, 1636, 322, 428, 10245, 13, 440, 3663, 7207, 420, 264, 51514], "temperature": 0.0, "avg_logprob": -0.1603880281801577, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.01833161897957325}, {"id": 58, "seek": 30568, "start": 328.68, "end": 333.24, "text": " carrier thread is carrying the little child around which is the virtual thread on the shoulders.", "tokens": [51514, 17574, 7207, 307, 9792, 264, 707, 1440, 926, 597, 307, 264, 6374, 7207, 322, 264, 10245, 13, 51742], "temperature": 0.0, "avg_logprob": -0.1603880281801577, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.01833161897957325}, {"id": 59, "seek": 33324, "start": 334.24, "end": 340.12, "text": " When the child wants to stop and do something, you take them off, drop them down, some other", "tokens": [50414, 1133, 264, 1440, 2738, 281, 1590, 293, 360, 746, 11, 291, 747, 552, 766, 11, 3270, 552, 760, 11, 512, 661, 50708], "temperature": 0.0, "avg_logprob": -0.20008769035339355, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.00800099316984415}, {"id": 60, "seek": 33324, "start": 340.12, "end": 345.08, "text": " adult comes along, the other guardian picks them up and moves them on their shoulders. So that's", "tokens": [50708, 5075, 1487, 2051, 11, 264, 661, 30355, 16137, 552, 493, 293, 6067, 552, 322, 641, 10245, 13, 407, 300, 311, 50956], "temperature": 0.0, "avg_logprob": -0.20008769035339355, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.00800099316984415}, {"id": 61, "seek": 33324, "start": 345.08, "end": 352.68, "text": " essentially sort of what to think about with virtual threads. Okay, so in order to talk through", "tokens": [50956, 4476, 1333, 295, 437, 281, 519, 466, 365, 6374, 19314, 13, 1033, 11, 370, 294, 1668, 281, 751, 807, 51336], "temperature": 0.0, "avg_logprob": -0.20008769035339355, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.00800099316984415}, {"id": 62, "seek": 33324, "start": 352.68, "end": 356.40000000000003, "text": " some of these slides, I'm going to use the kind of the same sort of layout in all of these slides", "tokens": [51336, 512, 295, 613, 9788, 11, 286, 478, 516, 281, 764, 264, 733, 295, 264, 912, 1333, 295, 13333, 294, 439, 295, 613, 9788, 51522], "temperature": 0.0, "avg_logprob": -0.20008769035339355, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.00800099316984415}, {"id": 63, "seek": 33324, "start": 356.40000000000003, "end": 361.56, "text": " and so what I'm going to have is, is there's going to be a little bit of code example on the left", "tokens": [51522, 293, 370, 437, 286, 478, 516, 281, 362, 307, 11, 307, 456, 311, 516, 281, 312, 257, 707, 857, 295, 3089, 1365, 322, 264, 1411, 51780], "temperature": 0.0, "avg_logprob": -0.20008769035339355, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.00800099316984415}, {"id": 64, "seek": 36156, "start": 361.8, "end": 368.36, "text": " and then I'm going to show some stacks on the right side. They're color coded and I'll show you", "tokens": [50376, 293, 550, 286, 478, 516, 281, 855, 512, 30792, 322, 264, 558, 1252, 13, 814, 434, 2017, 34874, 293, 286, 603, 855, 291, 50704], "temperature": 0.0, "avg_logprob": -0.18471240234375, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.011354774236679077}, {"id": 65, "seek": 36156, "start": 368.36, "end": 374.24, "text": " these just to give you an idea what's actually going to happen. The thread that is going to be", "tokens": [50704, 613, 445, 281, 976, 291, 364, 1558, 437, 311, 767, 516, 281, 1051, 13, 440, 7207, 300, 307, 516, 281, 312, 50998], "temperature": 0.0, "avg_logprob": -0.18471240234375, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.011354774236679077}, {"id": 66, "seek": 36156, "start": 374.24, "end": 378.88, "text": " the subject of this talk, we're actually just going to give it an ID, it's number 22. We've", "tokens": [50998, 264, 3983, 295, 341, 751, 11, 321, 434, 767, 445, 516, 281, 976, 309, 364, 7348, 11, 309, 311, 1230, 5853, 13, 492, 600, 51230], "temperature": 0.0, "avg_logprob": -0.18471240234375, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.011354774236679077}, {"id": 67, "seek": 36156, "start": 378.88, "end": 384.8, "text": " doubled down on thread IDs, quite a bit in the platform the last couple of years. So what we're", "tokens": [51230, 24405, 760, 322, 7207, 48212, 11, 1596, 257, 857, 294, 264, 3663, 264, 1036, 1916, 295, 924, 13, 407, 437, 321, 434, 51526], "temperature": 0.0, "avg_logprob": -0.18471240234375, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.011354774236679077}, {"id": 68, "seek": 36156, "start": 384.8, "end": 389.2, "text": " going to do here is we're going to have a bit of code that's executed by virtual thread number 22.", "tokens": [51526, 516, 281, 360, 510, 307, 321, 434, 516, 281, 362, 257, 857, 295, 3089, 300, 311, 17577, 538, 6374, 7207, 1230, 5853, 13, 51746], "temperature": 0.0, "avg_logprob": -0.18471240234375, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.011354774236679077}, {"id": 69, "seek": 38920, "start": 389.4, "end": 394.36, "text": " In this case, for the first example, we're actually going to use the same before. Just think about", "tokens": [50374, 682, 341, 1389, 11, 337, 264, 700, 1365, 11, 321, 434, 767, 516, 281, 764, 264, 912, 949, 13, 1449, 519, 466, 50622], "temperature": 0.0, "avg_logprob": -0.20703915383318344, "compression_ratio": 1.858085808580858, "no_speech_prob": 0.014820672571659088}, {"id": 70, "seek": 38920, "start": 394.36, "end": 399.15999999999997, "text": " a semaphore as something that has a number of permits. You actually acquire a permit and,", "tokens": [50622, 257, 4361, 13957, 418, 382, 746, 300, 575, 257, 1230, 295, 30990, 13, 509, 767, 20001, 257, 13423, 293, 11, 50862], "temperature": 0.0, "avg_logprob": -0.20703915383318344, "compression_ratio": 1.858085808580858, "no_speech_prob": 0.014820672571659088}, {"id": 71, "seek": 38920, "start": 399.15999999999997, "end": 403.71999999999997, "text": " and when you're, and run some critical code and then when you're done with the permits,", "tokens": [50862, 293, 562, 291, 434, 11, 293, 1190, 512, 4924, 3089, 293, 550, 562, 291, 434, 1096, 365, 264, 30990, 11, 51090], "temperature": 0.0, "avg_logprob": -0.20703915383318344, "compression_ratio": 1.858085808580858, "no_speech_prob": 0.014820672571659088}, {"id": 72, "seek": 38920, "start": 403.71999999999997, "end": 407.24, "text": " you actually return it back to the semaphore with a release. So typically the typical idea", "tokens": [51090, 291, 767, 2736, 309, 646, 281, 264, 4361, 13957, 418, 365, 257, 4374, 13, 407, 5850, 264, 7476, 1558, 51266], "temperature": 0.0, "avg_logprob": -0.20703915383318344, "compression_ratio": 1.858085808580858, "no_speech_prob": 0.014820672571659088}, {"id": 73, "seek": 38920, "start": 407.24, "end": 412.12, "text": " that you actually use is, is, is acquire and then release at the end and use it to try resources", "tokens": [51266, 300, 291, 767, 764, 307, 11, 307, 11, 307, 20001, 293, 550, 4374, 412, 264, 917, 293, 764, 309, 281, 853, 3593, 51510], "temperature": 0.0, "avg_logprob": -0.20703915383318344, "compression_ratio": 1.858085808580858, "no_speech_prob": 0.014820672571659088}, {"id": 74, "seek": 38920, "start": 412.12, "end": 417.2, "text": " concept. Okay, so very, very simple. So let's see what actually happens when a virtual thread goes", "tokens": [51510, 3410, 13, 1033, 11, 370, 588, 11, 588, 2199, 13, 407, 718, 311, 536, 437, 767, 2314, 562, 257, 6374, 7207, 1709, 51764], "temperature": 0.0, "avg_logprob": -0.20703915383318344, "compression_ratio": 1.858085808580858, "no_speech_prob": 0.014820672571659088}, {"id": 75, "seek": 41720, "start": 417.24, "end": 423.24, "text": " and executes this code. So the arrow here, just think that this is, this thing of the red arrow", "tokens": [50366, 293, 4454, 1819, 341, 3089, 13, 407, 264, 11610, 510, 11, 445, 519, 300, 341, 307, 11, 341, 551, 295, 264, 2182, 11610, 50666], "temperature": 0.0, "avg_logprob": -0.16485254388106496, "compression_ratio": 1.8631178707224334, "no_speech_prob": 0.011640410870313644}, {"id": 76, "seek": 41720, "start": 423.24, "end": 428.44, "text": " is kind of like a program point, program counter. That's kind of where we are. On the right side", "tokens": [50666, 307, 733, 295, 411, 257, 1461, 935, 11, 1461, 5682, 13, 663, 311, 733, 295, 689, 321, 366, 13, 1282, 264, 558, 1252, 50926], "temperature": 0.0, "avg_logprob": -0.16485254388106496, "compression_ratio": 1.8631178707224334, "no_speech_prob": 0.011640410870313644}, {"id": 77, "seek": 41720, "start": 428.44, "end": 432.4, "text": " now we see our first sort of stack traces here. There's actually two threads in the picture here.", "tokens": [50926, 586, 321, 536, 527, 700, 1333, 295, 8630, 26076, 510, 13, 821, 311, 767, 732, 19314, 294, 264, 3036, 510, 13, 51124], "temperature": 0.0, "avg_logprob": -0.16485254388106496, "compression_ratio": 1.8631178707224334, "no_speech_prob": 0.011640410870313644}, {"id": 78, "seek": 41720, "start": 432.4, "end": 437.2, "text": " What you actually see at the top, and by the way, the stacks are in this, in, in, in, in all of these", "tokens": [51124, 708, 291, 767, 536, 412, 264, 1192, 11, 293, 538, 264, 636, 11, 264, 30792, 366, 294, 341, 11, 294, 11, 294, 11, 294, 11, 294, 439, 295, 613, 51364], "temperature": 0.0, "avg_logprob": -0.16485254388106496, "compression_ratio": 1.8631178707224334, "no_speech_prob": 0.011640410870313644}, {"id": 79, "seek": 41720, "start": 437.2, "end": 444.12, "text": " slides grow from the top to top down. So the, the, the, the orange brownie frames that you see at", "tokens": [51364, 9788, 1852, 490, 264, 1192, 281, 1192, 760, 13, 407, 264, 11, 264, 11, 264, 11, 264, 7671, 6292, 414, 12083, 300, 291, 536, 412, 51710], "temperature": 0.0, "avg_logprob": -0.16485254388106496, "compression_ratio": 1.8631178707224334, "no_speech_prob": 0.011640410870313644}, {"id": 80, "seek": 44412, "start": 444.16, "end": 449.04, "text": " the top there, they're actually the fork join pool thread that's, that's actually carrying the virtual", "tokens": [50366, 264, 1192, 456, 11, 436, 434, 767, 264, 17716, 3917, 7005, 7207, 300, 311, 11, 300, 311, 767, 9792, 264, 6374, 50610], "temperature": 0.0, "avg_logprob": -0.1756404571533203, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.012142214924097061}, {"id": 81, "seek": 44412, "start": 449.04, "end": 455.28000000000003, "text": " thread. And the greens, the green frames that you see there are the virtual thread. And so I've", "tokens": [50610, 7207, 13, 400, 264, 22897, 11, 264, 3092, 12083, 300, 291, 536, 456, 366, 264, 6374, 7207, 13, 400, 370, 286, 600, 50922], "temperature": 0.0, "avg_logprob": -0.1756404571533203, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.012142214924097061}, {"id": 82, "seek": 44412, "start": 455.28000000000003, "end": 458.88, "text": " just merged them there together because that's actually what you have on the native thread", "tokens": [50922, 445, 36427, 552, 456, 1214, 570, 300, 311, 767, 437, 291, 362, 322, 264, 8470, 7207, 51102], "temperature": 0.0, "avg_logprob": -0.1756404571533203, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.012142214924097061}, {"id": 83, "seek": 44412, "start": 458.88, "end": 465.6, "text": " underneath the, the covers. So when we get to do, to doing a semaphore acquire, what we see is, is,", "tokens": [51102, 7223, 264, 11, 264, 10538, 13, 407, 562, 321, 483, 281, 360, 11, 281, 884, 257, 4361, 13957, 418, 20001, 11, 437, 321, 536, 307, 11, 307, 11, 51438], "temperature": 0.0, "avg_logprob": -0.1756404571533203, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.012142214924097061}, {"id": 84, "seek": 44412, "start": 465.6, "end": 470.88, "text": " is, is in green here. If you look down right at the bottom, the, the, the, the frame there is", "tokens": [51438, 307, 11, 307, 294, 3092, 510, 13, 759, 291, 574, 760, 558, 412, 264, 2767, 11, 264, 11, 264, 11, 264, 11, 264, 3920, 456, 307, 51702], "temperature": 0.0, "avg_logprob": -0.1756404571533203, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.012142214924097061}, {"id": 85, "seek": 47088, "start": 470.92, "end": 475.2, "text": " actually semaphore.acquire. So we're about to call this guy and we'll actually see what actually", "tokens": [50366, 767, 4361, 13957, 418, 13, 326, 358, 621, 13, 407, 321, 434, 466, 281, 818, 341, 2146, 293, 321, 603, 767, 536, 437, 767, 50580], "temperature": 0.0, "avg_logprob": -0.13185906788659474, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.03685558959841728}, {"id": 86, "seek": 47088, "start": 475.2, "end": 482.56, "text": " happens when we call semaphore acquire. Okay, so semaphore acquire gets to this point here. Now", "tokens": [50580, 2314, 562, 321, 818, 4361, 13957, 418, 20001, 13, 1033, 11, 370, 4361, 13957, 418, 20001, 2170, 281, 341, 935, 510, 13, 823, 50948], "temperature": 0.0, "avg_logprob": -0.13185906788659474, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.03685558959841728}, {"id": 87, "seek": 47088, "start": 482.56, "end": 488.88, "text": " every, every good movie needs a villain and I need a villain here. So in this case, let us assume", "tokens": [50948, 633, 11, 633, 665, 3169, 2203, 257, 17906, 293, 286, 643, 257, 17906, 510, 13, 407, 294, 341, 1389, 11, 718, 505, 6552, 51264], "temperature": 0.0, "avg_logprob": -0.13185906788659474, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.03685558959841728}, {"id": 88, "seek": 47088, "start": 488.88, "end": 493.4, "text": " that the, the villain is Andrew and he actually has the permit for this summer for. So what's", "tokens": [51264, 300, 264, 11, 264, 17906, 307, 10110, 293, 415, 767, 575, 264, 13423, 337, 341, 4266, 337, 13, 407, 437, 311, 51490], "temperature": 0.0, "avg_logprob": -0.13185906788659474, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.03685558959841728}, {"id": 89, "seek": 47088, "start": 493.4, "end": 499.2, "text": " going to happen is this virtual thread has to go and park because it can't acquire a semaphore. So", "tokens": [51490, 516, 281, 1051, 307, 341, 6374, 7207, 575, 281, 352, 293, 3884, 570, 309, 393, 380, 20001, 257, 4361, 13957, 418, 13, 407, 51780], "temperature": 0.0, "avg_logprob": -0.13185906788659474, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.03685558959841728}, {"id": 90, "seek": 49920, "start": 499.24, "end": 503.76, "text": " what we see here is, is we're going down through the Javiutil concurrent code to try to acquire", "tokens": [50366, 437, 321, 536, 510, 307, 11, 307, 321, 434, 516, 760, 807, 264, 508, 18442, 325, 388, 37702, 3089, 281, 853, 281, 20001, 50592], "temperature": 0.0, "avg_logprob": -0.189382500844459, "compression_ratio": 1.8745874587458746, "no_speech_prob": 0.0011155948741361499}, {"id": 91, "seek": 49920, "start": 503.76, "end": 508.52, "text": " the semaphore. There's no permits available. So the thread has to park. What actually happens is,", "tokens": [50592, 264, 4361, 13957, 418, 13, 821, 311, 572, 30990, 2435, 13, 407, 264, 7207, 575, 281, 3884, 13, 708, 767, 2314, 307, 11, 50830], "temperature": 0.0, "avg_logprob": -0.189382500844459, "compression_ratio": 1.8745874587458746, "no_speech_prob": 0.0011155948741361499}, {"id": 92, "seek": 49920, "start": 508.52, "end": 513.36, "text": " it bottoms out at the bottom trying to do a park which will actually yield the continuation. And", "tokens": [50830, 309, 43413, 484, 412, 264, 2767, 1382, 281, 360, 257, 3884, 597, 486, 767, 11257, 264, 29357, 13, 400, 51072], "temperature": 0.0, "avg_logprob": -0.189382500844459, "compression_ratio": 1.8745874587458746, "no_speech_prob": 0.0011155948741361499}, {"id": 93, "seek": 49920, "start": 513.36, "end": 519.4399999999999, "text": " there, this is when the, this is, this is where the magic occurs is, is the, the, the, the, the,", "tokens": [51072, 456, 11, 341, 307, 562, 264, 11, 341, 307, 11, 341, 307, 689, 264, 5585, 11843, 307, 11, 307, 264, 11, 264, 11, 264, 11, 264, 11, 264, 11, 51376], "temperature": 0.0, "avg_logprob": -0.189382500844459, "compression_ratio": 1.8745874587458746, "no_speech_prob": 0.0011155948741361499}, {"id": 94, "seek": 49920, "start": 519.4399999999999, "end": 524.48, "text": " the thread is now parking and, and magically its frames get removed from the native thread stack.", "tokens": [51376, 264, 7207, 307, 586, 9893, 293, 11, 293, 39763, 1080, 12083, 483, 7261, 490, 264, 8470, 7207, 8630, 13, 51628], "temperature": 0.0, "avg_logprob": -0.189382500844459, "compression_ratio": 1.8745874587458746, "no_speech_prob": 0.0011155948741361499}, {"id": 95, "seek": 49920, "start": 524.48, "end": 529.16, "text": " And the worker thread or the fork joint thread is able to go and do other work. So", "tokens": [51628, 400, 264, 11346, 7207, 420, 264, 17716, 7225, 7207, 307, 1075, 281, 352, 293, 360, 661, 589, 13, 407, 51862], "temperature": 0.0, "avg_logprob": -0.189382500844459, "compression_ratio": 1.8745874587458746, "no_speech_prob": 0.0011155948741361499}, {"id": 96, "seek": 52916, "start": 529.1999999999999, "end": 534.48, "text": " that's actually what happens sort of at a high level with, with, with, with virtual threads.", "tokens": [50366, 300, 311, 767, 437, 2314, 1333, 295, 412, 257, 1090, 1496, 365, 11, 365, 11, 365, 11, 365, 6374, 19314, 13, 50630], "temperature": 0.0, "avg_logprob": -0.14836218377121357, "compression_ratio": 2.0535714285714284, "no_speech_prob": 0.0005381997907534242}, {"id": 97, "seek": 52916, "start": 534.48, "end": 542.0799999999999, "text": " Now Andrew is finished with his, the, the, the semaphore and is doing a release. So", "tokens": [50630, 823, 10110, 307, 4335, 365, 702, 11, 264, 11, 264, 11, 264, 4361, 13957, 418, 293, 307, 884, 257, 4374, 13, 407, 51010], "temperature": 0.0, "avg_logprob": -0.14836218377121357, "compression_ratio": 2.0535714285714284, "no_speech_prob": 0.0005381997907534242}, {"id": 98, "seek": 52916, "start": 542.0799999999999, "end": 547.28, "text": " he's returning back the permit back to the, to the semaphore. So now we're actually going to", "tokens": [51010, 415, 311, 12678, 646, 264, 13423, 646, 281, 264, 11, 281, 264, 4361, 13957, 418, 13, 407, 586, 321, 434, 767, 516, 281, 51270], "temperature": 0.0, "avg_logprob": -0.14836218377121357, "compression_ratio": 2.0535714285714284, "no_speech_prob": 0.0005381997907534242}, {"id": 99, "seek": 52916, "start": 547.28, "end": 551.48, "text": " look at what happens here now when the, when the, the, the virtual thread is actually going to", "tokens": [51270, 574, 412, 437, 2314, 510, 586, 562, 264, 11, 562, 264, 11, 264, 11, 264, 6374, 7207, 307, 767, 516, 281, 51480], "temperature": 0.0, "avg_logprob": -0.14836218377121357, "compression_ratio": 2.0535714285714284, "no_speech_prob": 0.0005381997907534242}, {"id": 100, "seek": 52916, "start": 551.48, "end": 556.0799999999999, "text": " continue. So remember a virtual thread is waiting to acquire the semaphore. Andrew is doing the", "tokens": [51480, 2354, 13, 407, 1604, 257, 6374, 7207, 307, 3806, 281, 20001, 264, 4361, 13957, 418, 13, 10110, 307, 884, 264, 51710], "temperature": 0.0, "avg_logprob": -0.14836218377121357, "compression_ratio": 2.0535714285714284, "no_speech_prob": 0.0005381997907534242}, {"id": 101, "seek": 55608, "start": 556.12, "end": 561.88, "text": " release and that goes and triggers the, the, the, the, the, goes through the Javu-Tilkin", "tokens": [50366, 4374, 293, 300, 1709, 293, 22827, 264, 11, 264, 11, 264, 11, 264, 11, 264, 11, 1709, 807, 264, 508, 706, 84, 12, 51, 388, 5843, 50654], "temperature": 0.0, "avg_logprob": -0.2046507199605306, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.02447742410004139}, {"id": 102, "seek": 55608, "start": 561.88, "end": 566.24, "text": " current code and it'll bottom out then doing an unpark of the, of the virtual thread that's", "tokens": [50654, 2190, 3089, 293, 309, 603, 2767, 484, 550, 884, 364, 517, 31239, 295, 264, 11, 295, 264, 6374, 7207, 300, 311, 50872], "temperature": 0.0, "avg_logprob": -0.2046507199605306, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.02447742410004139}, {"id": 103, "seek": 55608, "start": 566.24, "end": 570.76, "text": " actually waiting, which will do the continue. What, and, and what it's actually really going to do", "tokens": [50872, 767, 3806, 11, 597, 486, 360, 264, 2354, 13, 708, 11, 293, 11, 293, 437, 309, 311, 767, 534, 516, 281, 360, 51098], "temperature": 0.0, "avg_logprob": -0.2046507199605306, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.02447742410004139}, {"id": 104, "seek": 55608, "start": 570.76, "end": 574.9200000000001, "text": " is actually schedule the task that's associated with the virtual thread back to the scheduler", "tokens": [51098, 307, 767, 7567, 264, 5633, 300, 311, 6615, 365, 264, 6374, 7207, 646, 281, 264, 12000, 260, 51306], "temperature": 0.0, "avg_logprob": -0.2046507199605306, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.02447742410004139}, {"id": 105, "seek": 55608, "start": 574.9200000000001, "end": 579.8000000000001, "text": " so that it actually can continue again. Very, very, very kind of straightforward. It's just", "tokens": [51306, 370, 300, 309, 767, 393, 2354, 797, 13, 4372, 11, 588, 11, 588, 733, 295, 15325, 13, 467, 311, 445, 51550], "temperature": 0.0, "avg_logprob": -0.2046507199605306, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.02447742410004139}, {"id": 106, "seek": 55608, "start": 579.8000000000001, "end": 585.6400000000001, "text": " submitting the, the, the virtual thread back to the, to, to the scheduler so it can continue. So", "tokens": [51550, 31836, 264, 11, 264, 11, 264, 6374, 7207, 646, 281, 264, 11, 281, 11, 281, 264, 12000, 260, 370, 309, 393, 2354, 13, 407, 51842], "temperature": 0.0, "avg_logprob": -0.2046507199605306, "compression_ratio": 2.0661764705882355, "no_speech_prob": 0.02447742410004139}, {"id": 107, "seek": 58564, "start": 585.68, "end": 592.0, "text": " back to our slide again. So what happens is, is, is, is, we'll assume the scheduler has now", "tokens": [50366, 646, 281, 527, 4137, 797, 13, 407, 437, 2314, 307, 11, 307, 11, 307, 11, 307, 11, 321, 603, 6552, 264, 12000, 260, 575, 586, 50682], "temperature": 0.0, "avg_logprob": -0.15417658996582032, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.0016571047017350793}, {"id": 108, "seek": 58564, "start": 592.0, "end": 596.72, "text": " started executing this code again. It'll return back from, from, from, from the park that we were", "tokens": [50682, 1409, 32368, 341, 3089, 797, 13, 467, 603, 2736, 646, 490, 11, 490, 11, 490, 11, 490, 264, 3884, 300, 321, 645, 50918], "temperature": 0.0, "avg_logprob": -0.15417658996582032, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.0016571047017350793}, {"id": 109, "seek": 58564, "start": 596.72, "end": 602.08, "text": " on earlier on and magically we, we, the frames start popping off and we go into our tri block", "tokens": [50918, 322, 3071, 322, 293, 39763, 321, 11, 321, 11, 264, 12083, 722, 18374, 766, 293, 321, 352, 666, 527, 1376, 3461, 51186], "temperature": 0.0, "avg_logprob": -0.15417658996582032, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.0016571047017350793}, {"id": 110, "seek": 58564, "start": 602.08, "end": 607.72, "text": " and we are done. So that's sort of thing, how, how, how things actually work with parking and", "tokens": [51186, 293, 321, 366, 1096, 13, 407, 300, 311, 1333, 295, 551, 11, 577, 11, 577, 11, 577, 721, 767, 589, 365, 9893, 293, 51468], "temperature": 0.0, "avg_logprob": -0.15417658996582032, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.0016571047017350793}, {"id": 111, "seek": 58564, "start": 607.72, "end": 612.16, "text": " unparking and how they actually integrate with the scheduler. Now we get into the sort of the,", "tokens": [51468, 517, 31239, 278, 293, 577, 436, 767, 13365, 365, 264, 12000, 260, 13, 823, 321, 483, 666, 264, 1333, 295, 264, 11, 51690], "temperature": 0.0, "avg_logprob": -0.15417658996582032, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.0016571047017350793}, {"id": 112, "seek": 61216, "start": 612.36, "end": 619.0799999999999, "text": " the problem areas and where we have the pain points with, with virtual threads today. So I'm", "tokens": [50374, 264, 1154, 3179, 293, 689, 321, 362, 264, 1822, 2793, 365, 11, 365, 6374, 19314, 965, 13, 407, 286, 478, 50710], "temperature": 0.0, "avg_logprob": -0.14748956061698296, "compression_ratio": 1.8419354838709678, "no_speech_prob": 0.01660853438079357}, {"id": 113, "seek": 61216, "start": 619.0799999999999, "end": 623.4, "text": " going to go through two, two, two, two scenarios. One of them is, is parking while holding a", "tokens": [50710, 516, 281, 352, 807, 732, 11, 732, 11, 732, 11, 732, 15077, 13, 1485, 295, 552, 307, 11, 307, 9893, 1339, 5061, 257, 50926], "temperature": 0.0, "avg_logprob": -0.14748956061698296, "compression_ratio": 1.8419354838709678, "no_speech_prob": 0.01660853438079357}, {"id": 114, "seek": 61216, "start": 623.4, "end": 627.24, "text": " monitor. I mentioned about all these, these blogs that have these gotchas at the end. This is", "tokens": [50926, 6002, 13, 286, 2835, 466, 439, 613, 11, 613, 31038, 300, 362, 613, 658, 41299, 412, 264, 917, 13, 639, 307, 51118], "temperature": 0.0, "avg_logprob": -0.14748956061698296, "compression_ratio": 1.8419354838709678, "no_speech_prob": 0.01660853438079357}, {"id": 115, "seek": 61216, "start": 627.24, "end": 631.68, "text": " essentially what they're actually trying to show you in, in, in, in these blogs. So we've taken", "tokens": [51118, 4476, 437, 436, 434, 767, 1382, 281, 855, 291, 294, 11, 294, 11, 294, 11, 294, 613, 31038, 13, 407, 321, 600, 2726, 51340], "temperature": 0.0, "avg_logprob": -0.14748956061698296, "compression_ratio": 1.8419354838709678, "no_speech_prob": 0.01660853438079357}, {"id": 116, "seek": 61216, "start": 631.68, "end": 635.52, "text": " the same example but we've actually put it into a synchronized block. So the synchronized block", "tokens": [51340, 264, 912, 1365, 457, 321, 600, 767, 829, 309, 666, 257, 19331, 1602, 3461, 13, 407, 264, 19331, 1602, 3461, 51532], "temperature": 0.0, "avg_logprob": -0.14748956061698296, "compression_ratio": 1.8419354838709678, "no_speech_prob": 0.01660853438079357}, {"id": 117, "seek": 61216, "start": 635.52, "end": 639.9599999999999, "text": " here is, is, so that's kind of, think of that as a monitor enter, here's a monitor exit. Same thing", "tokens": [51532, 510, 307, 11, 307, 11, 370, 300, 311, 733, 295, 11, 519, 295, 300, 382, 257, 6002, 3242, 11, 510, 311, 257, 6002, 11043, 13, 10635, 551, 51754], "temperature": 0.0, "avg_logprob": -0.14748956061698296, "compression_ratio": 1.8419354838709678, "no_speech_prob": 0.01660853438079357}, {"id": 118, "seek": 63996, "start": 640.0, "end": 645.84, "text": " if this was a, a synchronized method. The code that we had in the previous, the previous section", "tokens": [50366, 498, 341, 390, 257, 11, 257, 19331, 1602, 3170, 13, 440, 3089, 300, 321, 632, 294, 264, 3894, 11, 264, 3894, 3541, 50658], "temperature": 0.0, "avg_logprob": -0.15059165954589843, "compression_ratio": 1.792626728110599, "no_speech_prob": 0.00683917012065649}, {"id": 119, "seek": 63996, "start": 645.84, "end": 652.8000000000001, "text": " is, is, is exactly the same. So what happens is, is we're going to do the, going to do the, the", "tokens": [50658, 307, 11, 307, 11, 307, 2293, 264, 912, 13, 407, 437, 2314, 307, 11, 307, 321, 434, 516, 281, 360, 264, 11, 516, 281, 360, 264, 11, 264, 51006], "temperature": 0.0, "avg_logprob": -0.15059165954589843, "compression_ratio": 1.792626728110599, "no_speech_prob": 0.00683917012065649}, {"id": 120, "seek": 63996, "start": 652.8000000000001, "end": 657.88, "text": " choir here. Andrew R. Villan again is actually holding the, the, the, the permit for the semaphore.", "tokens": [51006, 31244, 510, 13, 10110, 497, 13, 14244, 282, 797, 307, 767, 5061, 264, 11, 264, 11, 264, 11, 264, 13423, 337, 264, 4361, 13957, 418, 13, 51260], "temperature": 0.0, "avg_logprob": -0.15059165954589843, "compression_ratio": 1.792626728110599, "no_speech_prob": 0.00683917012065649}, {"id": 121, "seek": 63996, "start": 657.88, "end": 664.88, "text": " So this virtual thread has to, has, has to go and park. So what happens this time is we're going", "tokens": [51260, 407, 341, 6374, 7207, 575, 281, 11, 575, 11, 575, 281, 352, 293, 3884, 13, 407, 437, 2314, 341, 565, 307, 321, 434, 516, 51610], "temperature": 0.0, "avg_logprob": -0.15059165954589843, "compression_ratio": 1.792626728110599, "no_speech_prob": 0.00683917012065649}, {"id": 122, "seek": 66488, "start": 664.92, "end": 672.0, "text": " to try to park but we're actually holding a monitor. So, so this yield down the bottom fails. Why", "tokens": [50366, 281, 853, 281, 3884, 457, 321, 434, 767, 5061, 257, 6002, 13, 407, 11, 370, 341, 11257, 760, 264, 2767, 18199, 13, 1545, 50720], "temperature": 0.0, "avg_logprob": -0.1988184893572772, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0632849782705307}, {"id": 123, "seek": 66488, "start": 672.0, "end": 678.12, "text": " does it fail? Well, we, we get into why it actually fails in, in, in a second. But something, we're", "tokens": [50720, 775, 309, 3061, 30, 1042, 11, 321, 11, 321, 483, 666, 983, 309, 767, 18199, 294, 11, 294, 11, 294, 257, 1150, 13, 583, 746, 11, 321, 434, 51026], "temperature": 0.0, "avg_logprob": -0.1988184893572772, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0632849782705307}, {"id": 124, "seek": 66488, "start": 678.12, "end": 682.0, "text": " not able to actually go and release the, the, the, the, the, the, the, the, the, the, the, the, the", "tokens": [51026, 406, 1075, 281, 767, 352, 293, 4374, 264, 11, 264, 11, 264, 11, 264, 11, 264, 11, 264, 11, 264, 11, 264, 11, 264, 11, 264, 11, 264, 11, 264, 11, 264, 51220], "temperature": 0.0, "avg_logprob": -0.1988184893572772, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0632849782705307}, {"id": 125, "seek": 66488, "start": 682.0, "end": 687.32, "text": " carrier thread to do other work here. This is actually why we have, you get performance issues", "tokens": [51220, 17574, 7207, 281, 360, 661, 589, 510, 13, 639, 307, 767, 983, 321, 362, 11, 291, 483, 3389, 2663, 51486], "temperature": 0.0, "avg_logprob": -0.1988184893572772, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0632849782705307}, {"id": 126, "seek": 66488, "start": 687.32, "end": 693.0, "text": " and, and why we say that monitors lead to a quality implementation issue is because of this, this,", "tokens": [51486, 293, 11, 293, 983, 321, 584, 300, 26518, 1477, 281, 257, 3125, 11420, 2734, 307, 570, 295, 341, 11, 341, 11, 51770], "temperature": 0.0, "avg_logprob": -0.1988184893572772, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0632849782705307}, {"id": 127, "seek": 69300, "start": 693.12, "end": 698.28, "text": " this issue here. Now what actually happens in this particular case is, is that instead of", "tokens": [50370, 341, 2734, 510, 13, 823, 437, 767, 2314, 294, 341, 1729, 1389, 307, 11, 307, 300, 2602, 295, 50628], "temperature": 0.0, "avg_logprob": -0.1646276968496817, "compression_ratio": 1.967479674796748, "no_speech_prob": 0.005499456077814102}, {"id": 128, "seek": 69300, "start": 698.28, "end": 703.72, "text": " actually failing, it actually falls back to actually to park on the carrier and the, the, the, the,", "tokens": [50628, 767, 18223, 11, 309, 767, 8804, 646, 281, 767, 281, 3884, 322, 264, 17574, 293, 264, 11, 264, 11, 264, 11, 264, 11, 50900], "temperature": 0.0, "avg_logprob": -0.1646276968496817, "compression_ratio": 1.967479674796748, "no_speech_prob": 0.005499456077814102}, {"id": 129, "seek": 69300, "start": 703.72, "end": 709.0, "text": " the semaphore in this case works exactly the same as, as, as, as this would if, if, if you were able", "tokens": [50900, 264, 4361, 13957, 418, 294, 341, 1389, 1985, 2293, 264, 912, 382, 11, 382, 11, 382, 11, 382, 341, 576, 498, 11, 498, 11, 498, 291, 645, 1075, 51164], "temperature": 0.0, "avg_logprob": -0.1646276968496817, "compression_ratio": 1.967479674796748, "no_speech_prob": 0.005499456077814102}, {"id": 130, "seek": 69300, "start": 709.0, "end": 715.32, "text": " to unmount, we're just not able to unmount. We can't let the, the, the carrier go away and, and, and,", "tokens": [51164, 281, 19334, 792, 11, 321, 434, 445, 406, 1075, 281, 19334, 792, 13, 492, 393, 380, 718, 264, 11, 264, 11, 264, 17574, 352, 1314, 293, 11, 293, 11, 293, 11, 51480], "temperature": 0.0, "avg_logprob": -0.1646276968496817, "compression_ratio": 1.967479674796748, "no_speech_prob": 0.005499456077814102}, {"id": 131, "seek": 69300, "start": 715.32, "end": 722.0, "text": " and do other work. And right, why, why do we have this problem? And so we have this problem", "tokens": [51480, 293, 360, 661, 589, 13, 400, 558, 11, 983, 11, 983, 360, 321, 362, 341, 1154, 30, 400, 370, 321, 362, 341, 1154, 51814], "temperature": 0.0, "avg_logprob": -0.1646276968496817, "compression_ratio": 1.967479674796748, "no_speech_prob": 0.005499456077814102}, {"id": 132, "seek": 72200, "start": 722.04, "end": 729.76, "text": " because of the way that monitors are implemented in, in the, in, in the Java VM. There's different,", "tokens": [50366, 570, 295, 264, 636, 300, 26518, 366, 12270, 294, 11, 294, 264, 11, 294, 11, 294, 264, 10745, 18038, 13, 821, 311, 819, 11, 50752], "temperature": 0.0, "avg_logprob": -0.2303115575000493, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.0067176204174757}, {"id": 133, "seek": 72200, "start": 729.76, "end": 734.36, "text": " there's different kind of locking modes. A lot of this is sort of beyond where, where I typically", "tokens": [50752, 456, 311, 819, 733, 295, 23954, 14068, 13, 316, 688, 295, 341, 307, 1333, 295, 4399, 689, 11, 689, 286, 5850, 50982], "temperature": 0.0, "avg_logprob": -0.2303115575000493, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.0067176204174757}, {"id": 134, "seek": 72200, "start": 734.36, "end": 740.04, "text": " work. But in, in, in Roman's talk earlier on, he actually, actually shows some of this where, where,", "tokens": [50982, 589, 13, 583, 294, 11, 294, 11, 294, 8566, 311, 751, 3071, 322, 11, 415, 767, 11, 767, 3110, 512, 295, 341, 689, 11, 689, 11, 51266], "temperature": 0.0, "avg_logprob": -0.2303115575000493, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.0067176204174757}, {"id": 135, "seek": 72200, "start": 740.04, "end": 748.08, "text": " the fast locking type is essentially, is essentially putting a pointer into the object back into a", "tokens": [51266, 264, 2370, 23954, 2010, 307, 4476, 11, 307, 4476, 3372, 257, 23918, 666, 264, 2657, 646, 666, 257, 51668], "temperature": 0.0, "avg_logprob": -0.2303115575000493, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.0067176204174757}, {"id": 136, "seek": 74808, "start": 748.1600000000001, "end": 752.72, "text": " lock record that's actually in the, in the stack. Oh, if we, that, then we can't actually start removing", "tokens": [50368, 4017, 2136, 300, 311, 767, 294, 264, 11, 294, 264, 8630, 13, 876, 11, 498, 321, 11, 300, 11, 550, 321, 393, 380, 767, 722, 12720, 50596], "temperature": 0.0, "avg_logprob": -0.19113154844804245, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.053665343672037125}, {"id": 137, "seek": 74808, "start": 752.72, "end": 757.9200000000001, "text": " frames that, when, when, when we, when we unmount. There's also these inflated cases where, where,", "tokens": [50596, 12083, 300, 11, 562, 11, 562, 11, 562, 321, 11, 562, 321, 19334, 792, 13, 821, 311, 611, 613, 9922, 770, 3331, 689, 11, 689, 11, 50856], "temperature": 0.0, "avg_logprob": -0.19113154844804245, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.053665343672037125}, {"id": 138, "seek": 74808, "start": 757.9200000000001, "end": 763.5200000000001, "text": " you're actually building up a waiter list of who's actually waiting for the monitor. And what,", "tokens": [50856, 291, 434, 767, 2390, 493, 257, 45389, 1329, 295, 567, 311, 767, 3806, 337, 264, 6002, 13, 400, 437, 11, 51136], "temperature": 0.0, "avg_logprob": -0.19113154844804245, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.053665343672037125}, {"id": 139, "seek": 74808, "start": 763.5200000000001, "end": 768.64, "text": " what goes on to that waste list is, it's actually, it's actually the, the VM's internal Java thread,", "tokens": [51136, 437, 1709, 322, 281, 300, 5964, 1329, 307, 11, 309, 311, 767, 11, 309, 311, 767, 264, 11, 264, 18038, 311, 6920, 10745, 7207, 11, 51392], "temperature": 0.0, "avg_logprob": -0.19113154844804245, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.053665343672037125}, {"id": 140, "seek": 74808, "start": 768.64, "end": 775.0400000000001, "text": " which is, is essentially the, the carrier thread in this case. So these are the reasons why, at least", "tokens": [51392, 597, 307, 11, 307, 4476, 264, 11, 264, 17574, 7207, 294, 341, 1389, 13, 407, 613, 366, 264, 4112, 983, 11, 412, 1935, 51712], "temperature": 0.0, "avg_logprob": -0.19113154844804245, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.053665343672037125}, {"id": 141, "seek": 77504, "start": 775.12, "end": 781.1999999999999, "text": " in this particular locking mode, that you cannot release the, the, the carrier at this point. And so", "tokens": [50368, 294, 341, 1729, 23954, 4391, 11, 300, 291, 2644, 4374, 264, 11, 264, 11, 264, 17574, 412, 341, 935, 13, 400, 370, 50672], "temperature": 0.0, "avg_logprob": -0.1524891815185547, "compression_ratio": 1.9682539682539681, "no_speech_prob": 0.006685581058263779}, {"id": 142, "seek": 77504, "start": 781.1999999999999, "end": 785.1999999999999, "text": " there's, there's, there's, there's magic in the implementation to actually to track monitor usage", "tokens": [50672, 456, 311, 11, 456, 311, 11, 456, 311, 11, 456, 311, 5585, 294, 264, 11420, 281, 767, 281, 2837, 6002, 14924, 50872], "temperature": 0.0, "avg_logprob": -0.1524891815185547, "compression_ratio": 1.9682539682539681, "no_speech_prob": 0.006685581058263779}, {"id": 143, "seek": 77504, "start": 785.1999999999999, "end": 789.68, "text": " to prevent this happening. There's another locking mode, which, which is, which is the, the, the, the", "tokens": [50872, 281, 4871, 341, 2737, 13, 821, 311, 1071, 23954, 4391, 11, 597, 11, 597, 307, 11, 597, 307, 264, 11, 264, 11, 264, 11, 264, 51096], "temperature": 0.0, "avg_logprob": -0.1524891815185547, "compression_ratio": 1.9682539682539681, "no_speech_prob": 0.006685581058263779}, {"id": 144, "seek": 77504, "start": 789.68, "end": 795.92, "text": " newer one, the lightweight one where there's a, a, a per, per, per thread little stat lock. And that's", "tokens": [51096, 17628, 472, 11, 264, 22052, 472, 689, 456, 311, 257, 11, 257, 11, 257, 680, 11, 680, 11, 680, 7207, 707, 2219, 4017, 13, 400, 300, 311, 51408], "temperature": 0.0, "avg_logprob": -0.1524891815185547, "compression_ratio": 1.9682539682539681, "no_speech_prob": 0.006685581058263779}, {"id": 145, "seek": 77504, "start": 795.92, "end": 799.28, "text": " got issues as well, because that's actually associated with the carrier thread, not with the", "tokens": [51408, 658, 2663, 382, 731, 11, 570, 300, 311, 767, 6615, 365, 264, 17574, 7207, 11, 406, 365, 264, 51576], "temperature": 0.0, "avg_logprob": -0.1524891815185547, "compression_ratio": 1.9682539682539681, "no_speech_prob": 0.006685581058263779}, {"id": 146, "seek": 79928, "start": 799.28, "end": 804.8, "text": " virtual thread. So what we do about this, so there is a, there, there's a sort of larger,", "tokens": [50364, 6374, 7207, 13, 407, 437, 321, 360, 466, 341, 11, 370, 456, 307, 257, 11, 456, 11, 456, 311, 257, 1333, 295, 4833, 11, 50640], "temperature": 0.0, "avg_logprob": -0.1608111614317406, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.033830396831035614}, {"id": 147, "seek": 79928, "start": 804.8, "end": 809.12, "text": " longer term effort that has, has kind of been underway. I think I saw Robin Ian here at one", "tokens": [50640, 2854, 1433, 4630, 300, 575, 11, 575, 733, 295, 668, 27534, 13, 286, 519, 286, 1866, 16533, 19595, 510, 412, 472, 50856], "temperature": 0.0, "avg_logprob": -0.1608111614317406, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.033830396831035614}, {"id": 148, "seek": 79928, "start": 809.12, "end": 815.12, "text": " point. He actually started this work to actually completely re, re, reexamine and do a new implementation", "tokens": [50856, 935, 13, 634, 767, 1409, 341, 589, 281, 767, 2584, 319, 11, 319, 11, 319, 3121, 18929, 293, 360, 257, 777, 11420, 51156], "temperature": 0.0, "avg_logprob": -0.1608111614317406, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.033830396831035614}, {"id": 149, "seek": 79928, "start": 815.12, "end": 819.52, "text": " of Java monitors. Do it in Java, rather than actually in the VM. Because a lot of legacy code", "tokens": [51156, 295, 10745, 26518, 13, 1144, 309, 294, 10745, 11, 2831, 813, 767, 294, 264, 18038, 13, 1436, 257, 688, 295, 11711, 3089, 51376], "temperature": 0.0, "avg_logprob": -0.1608111614317406, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.033830396831035614}, {"id": 150, "seek": 79928, "start": 819.52, "end": 824.0, "text": " and a lot of, there's, there's a lot of history there. Now that is a longer term effort. There's", "tokens": [51376, 293, 257, 688, 295, 11, 456, 311, 11, 456, 311, 257, 688, 295, 2503, 456, 13, 823, 300, 307, 257, 2854, 1433, 4630, 13, 821, 311, 51600], "temperature": 0.0, "avg_logprob": -0.1608111614317406, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.033830396831035614}, {"id": 151, "seek": 82400, "start": 824.0, "end": 831.76, "text": " a lot of unknowns, there's a lot of exploration. Both, we needed a plan B and, and, and plan B", "tokens": [50364, 257, 688, 295, 46048, 11, 456, 311, 257, 688, 295, 16197, 13, 6767, 11, 321, 2978, 257, 1393, 363, 293, 11, 293, 11, 293, 1393, 363, 50752], "temperature": 0.4, "avg_logprob": -0.20492819689829414, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.06137213483452797}, {"id": 152, "seek": 82400, "start": 831.76, "end": 837.68, "text": " involved a hero and the hero in this case is, is, is, is, is, is Patricio in, in, in the hot spot team", "tokens": [50752, 3288, 257, 5316, 293, 264, 5316, 294, 341, 1389, 307, 11, 307, 11, 307, 11, 307, 11, 307, 11, 307, 4379, 1341, 1004, 294, 11, 294, 11, 294, 264, 2368, 4008, 1469, 51048], "temperature": 0.4, "avg_logprob": -0.20492819689829414, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.06137213483452797}, {"id": 153, "seek": 82400, "start": 837.68, "end": 841.76, "text": " decided to go and have a go at actually trying to do a plan B, which is change the existing", "tokens": [51048, 3047, 281, 352, 293, 362, 257, 352, 412, 767, 1382, 281, 360, 257, 1393, 363, 11, 597, 307, 1319, 264, 6741, 51252], "temperature": 0.4, "avg_logprob": -0.20492819689829414, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.06137213483452797}, {"id": 154, "seek": 82400, "start": 841.76, "end": 848.4, "text": " object, object monitor implementation to work with virtual threads. So, so what he actually did was,", "tokens": [51252, 2657, 11, 2657, 6002, 11420, 281, 589, 365, 6374, 19314, 13, 407, 11, 370, 437, 415, 767, 630, 390, 11, 51584], "temperature": 0.4, "avg_logprob": -0.20492819689829414, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.06137213483452797}, {"id": 155, "seek": 84840, "start": 848.4, "end": 855.28, "text": " was, he's, he's, he's come up with a, well, there's, there's, there's several steps in this and this is,", "tokens": [50364, 390, 11, 415, 311, 11, 415, 311, 11, 415, 311, 808, 493, 365, 257, 11, 731, 11, 456, 311, 11, 456, 311, 11, 456, 311, 2940, 4439, 294, 341, 293, 341, 307, 11, 50708], "temperature": 0.4, "avg_logprob": -0.2095137842443605, "compression_ratio": 2.150442477876106, "no_speech_prob": 0.0380704328417778}, {"id": 156, "seek": 84840, "start": 855.28, "end": 858.56, "text": " this is, this is by the way, this work is all actually in the, in the, in the Loom repo.", "tokens": [50708, 341, 307, 11, 341, 307, 538, 264, 636, 11, 341, 589, 307, 439, 767, 294, 264, 11, 294, 264, 11, 294, 264, 6130, 298, 49040, 13, 50872], "temperature": 0.4, "avg_logprob": -0.2095137842443605, "compression_ratio": 2.150442477876106, "no_speech_prob": 0.0380704328417778}, {"id": 157, "seek": 84840, "start": 859.4399999999999, "end": 864.56, "text": " So what he actually does is, is for the, for the StackLock cases, is he just, just, just inflates", "tokens": [50916, 407, 437, 415, 767, 775, 307, 11, 307, 337, 264, 11, 337, 264, 37649, 43, 1560, 3331, 11, 307, 415, 445, 11, 445, 11, 445, 9922, 1024, 51172], "temperature": 0.4, "avg_logprob": -0.2095137842443605, "compression_ratio": 2.150442477876106, "no_speech_prob": 0.0380704328417778}, {"id": 158, "seek": 84840, "start": 864.56, "end": 869.68, "text": " and then for the inflated state, he's actually for the moment has, has the VM actually doing", "tokens": [51172, 293, 550, 337, 264, 9922, 770, 1785, 11, 415, 311, 767, 337, 264, 1623, 575, 11, 575, 264, 18038, 767, 884, 51428], "temperature": 0.4, "avg_logprob": -0.2095137842443605, "compression_ratio": 2.150442477876106, "no_speech_prob": 0.0380704328417778}, {"id": 159, "seek": 84840, "start": 869.68, "end": 875.04, "text": " a, a, a, a, a, a StackWalk to actually replace the owner so that it's actually not the, the, the, the", "tokens": [51428, 257, 11, 257, 11, 257, 11, 257, 11, 257, 11, 257, 37649, 54, 667, 281, 767, 7406, 264, 7289, 370, 300, 309, 311, 767, 406, 264, 11, 264, 11, 264, 11, 264, 51696], "temperature": 0.4, "avg_logprob": -0.2095137842443605, "compression_ratio": 2.150442477876106, "no_speech_prob": 0.0380704328417778}, {"id": 160, "seek": 87504, "start": 875.12, "end": 881.12, "text": " Java thread, it gets, it, it, it, so the, the VM's view of the thread is actually the, the, the Java", "tokens": [50368, 10745, 7207, 11, 309, 2170, 11, 309, 11, 309, 11, 309, 11, 370, 264, 11, 264, 18038, 311, 1910, 295, 264, 7207, 307, 767, 264, 11, 264, 11, 264, 10745, 50668], "temperature": 0.0, "avg_logprob": -0.2396714983195284, "compression_ratio": 1.9488188976377954, "no_speech_prob": 0.0060469768941402435}, {"id": 161, "seek": 87504, "start": 881.12, "end": 887.36, "text": " thread. And so that's a little bit expensive, but it actually does work. And, and here's a solution", "tokens": [50668, 7207, 13, 400, 370, 300, 311, 257, 707, 857, 5124, 11, 457, 309, 767, 775, 589, 13, 400, 11, 293, 510, 311, 257, 3827, 50980], "temperature": 0.0, "avg_logprob": -0.2396714983195284, "compression_ratio": 1.9488188976377954, "no_speech_prob": 0.0060469768941402435}, {"id": 162, "seek": 87504, "start": 887.36, "end": 894.3199999999999, "text": " for the lightweight, locking mode as well, where the, the, the lock stack actually moves at, at, at,", "tokens": [50980, 337, 264, 22052, 11, 23954, 4391, 382, 731, 11, 689, 264, 11, 264, 11, 264, 4017, 8630, 767, 6067, 412, 11, 412, 11, 412, 11, 51328], "temperature": 0.0, "avg_logprob": -0.2396714983195284, "compression_ratio": 1.9488188976377954, "no_speech_prob": 0.0060469768941402435}, {"id": 163, "seek": 87504, "start": 894.3199999999999, "end": 898.88, "text": " the, the, the mount and the unmount time. There's other work that's actually going on in parallel,", "tokens": [51328, 264, 11, 264, 11, 264, 3746, 293, 264, 19334, 792, 565, 13, 821, 311, 661, 589, 300, 311, 767, 516, 322, 294, 8952, 11, 51556], "temperature": 0.0, "avg_logprob": -0.2396714983195284, "compression_ratio": 1.9488188976377954, "no_speech_prob": 0.0060469768941402435}, {"id": 164, "seek": 87504, "start": 898.88, "end": 903.52, "text": " some of the work they're calling Fillmore is actually doing about changing the, the, the, the,", "tokens": [51556, 512, 295, 264, 589, 436, 434, 5141, 25315, 3138, 307, 767, 884, 466, 4473, 264, 11, 264, 11, 264, 11, 264, 11, 51788], "temperature": 0.0, "avg_logprob": -0.2396714983195284, "compression_ratio": 1.9488188976377954, "no_speech_prob": 0.0060469768941402435}, {"id": 165, "seek": 90352, "start": 903.6, "end": 909.1999999999999, "text": " the lock ownership to be the thread ID. Once that work actually comes in, that means that you", "tokens": [50368, 264, 4017, 15279, 281, 312, 264, 7207, 7348, 13, 3443, 300, 589, 767, 1487, 294, 11, 300, 1355, 300, 291, 50648], "temperature": 0.0, "avg_logprob": -0.10686661067761873, "compression_ratio": 1.9372937293729373, "no_speech_prob": 0.004371301271021366}, {"id": 166, "seek": 90352, "start": 909.1999999999999, "end": 912.96, "text": " actually can eliminate the StackWalk, you eliminate all the GC overhead, you eliminate all of the", "tokens": [50648, 767, 393, 13819, 264, 37649, 54, 667, 11, 291, 13819, 439, 264, 29435, 19922, 11, 291, 13819, 439, 295, 264, 50836], "temperature": 0.0, "avg_logprob": -0.10686661067761873, "compression_ratio": 1.9372937293729373, "no_speech_prob": 0.004371301271021366}, {"id": 167, "seek": 90352, "start": 912.96, "end": 917.92, "text": " actual overhead there of, of, of this, which is actually quite nice. So a lot of pieces from a", "tokens": [50836, 3539, 19922, 456, 295, 11, 295, 11, 295, 341, 11, 597, 307, 767, 1596, 1481, 13, 407, 257, 688, 295, 3755, 490, 257, 51084], "temperature": 0.0, "avg_logprob": -0.10686661067761873, "compression_ratio": 1.9372937293729373, "no_speech_prob": 0.004371301271021366}, {"id": 168, "seek": 90352, "start": 917.92, "end": 922.24, "text": " lot of people are coming, coming, coming together, which, which is nice. So this is working in the,", "tokens": [51084, 688, 295, 561, 366, 1348, 11, 1348, 11, 1348, 1214, 11, 597, 11, 597, 307, 1481, 13, 407, 341, 307, 1364, 294, 264, 11, 51300], "temperature": 0.0, "avg_logprob": -0.10686661067761873, "compression_ratio": 1.9372937293729373, "no_speech_prob": 0.004371301271021366}, {"id": 169, "seek": 90352, "start": 922.24, "end": 927.04, "text": " in the Loom repo for the moment. So we'll go back to our, our, our, our slides again and what would", "tokens": [51300, 294, 264, 6130, 298, 49040, 337, 264, 1623, 13, 407, 321, 603, 352, 646, 281, 527, 11, 527, 11, 527, 11, 527, 9788, 797, 293, 437, 576, 51540], "temperature": 0.0, "avg_logprob": -0.10686661067761873, "compression_ratio": 1.9372937293729373, "no_speech_prob": 0.004371301271021366}, {"id": 170, "seek": 90352, "start": 927.04, "end": 931.92, "text": " happen with, with that example, if we actually run it with the, some of the bills that we have from,", "tokens": [51540, 1051, 365, 11, 365, 300, 1365, 11, 498, 321, 767, 1190, 309, 365, 264, 11, 512, 295, 264, 12433, 300, 321, 362, 490, 11, 51784], "temperature": 0.0, "avg_logprob": -0.10686661067761873, "compression_ratio": 1.9372937293729373, "no_speech_prob": 0.004371301271021366}, {"id": 171, "seek": 93192, "start": 931.92, "end": 936.56, "text": " from the Loom repo today. So when we do, we actually do our acquire, we bottom out again at the,", "tokens": [50364, 490, 264, 6130, 298, 49040, 965, 13, 407, 562, 321, 360, 11, 321, 767, 360, 527, 20001, 11, 321, 2767, 484, 797, 412, 264, 11, 50596], "temperature": 0.0, "avg_logprob": -0.15329520614058884, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0060695745050907135}, {"id": 172, "seek": 93192, "start": 936.56, "end": 943.36, "text": " at the, at the yield as, as before, but this time it actually succeeds. The, we release the", "tokens": [50596, 412, 264, 11, 412, 264, 11257, 382, 11, 382, 949, 11, 457, 341, 565, 309, 767, 49263, 13, 440, 11, 321, 4374, 264, 50936], "temperature": 0.0, "avg_logprob": -0.15329520614058884, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0060695745050907135}, {"id": 173, "seek": 93192, "start": 943.36, "end": 947.5999999999999, "text": " carriage, go off and do other work, all very positive. So that is good. So that's one of the", "tokens": [50936, 31811, 11, 352, 766, 293, 360, 661, 589, 11, 439, 588, 3353, 13, 407, 300, 307, 665, 13, 407, 300, 311, 472, 295, 264, 51148], "temperature": 0.0, "avg_logprob": -0.15329520614058884, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0060695745050907135}, {"id": 174, "seek": 93192, "start": 947.5999999999999, "end": 952.16, "text": " pain points with pinning and it'll be wonderful actually to, to, to, to, to, to get that in.", "tokens": [51148, 1822, 2793, 365, 5447, 773, 293, 309, 603, 312, 3715, 767, 281, 11, 281, 11, 281, 11, 281, 11, 281, 11, 281, 483, 300, 294, 13, 51376], "temperature": 0.0, "avg_logprob": -0.15329520614058884, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0060695745050907135}, {"id": 175, "seek": 93192, "start": 953.4399999999999, "end": 958.88, "text": " Second scenario then is, is the contended monitor case. And that one is, is, is, is you have an", "tokens": [51440, 5736, 9005, 550, 307, 11, 307, 264, 660, 3502, 6002, 1389, 13, 400, 300, 472, 307, 11, 307, 11, 307, 11, 307, 291, 362, 364, 51712], "temperature": 0.0, "avg_logprob": -0.15329520614058884, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0060695745050907135}, {"id": 176, "seek": 95888, "start": 958.88, "end": 965.36, "text": " example like this. In this case, this is, I've got rid of the, the, the, the, the, the, the, the", "tokens": [50364, 1365, 411, 341, 13, 682, 341, 1389, 11, 341, 307, 11, 286, 600, 658, 3973, 295, 264, 11, 264, 11, 264, 11, 264, 11, 264, 11, 264, 11, 264, 11, 264, 50688], "temperature": 0.0, "avg_logprob": -0.1369549131741489, "compression_ratio": 1.9236641221374047, "no_speech_prob": 0.008811671286821365}, {"id": 177, "seek": 95888, "start": 965.36, "end": 970.4, "text": " semaphore from this example. And, but I'm actually going to block here. So we assume again now that", "tokens": [50688, 4361, 13957, 418, 490, 341, 1365, 13, 400, 11, 457, 286, 478, 767, 516, 281, 3461, 510, 13, 407, 321, 6552, 797, 586, 300, 50940], "temperature": 0.0, "avg_logprob": -0.1369549131741489, "compression_ratio": 1.9236641221374047, "no_speech_prob": 0.008811671286821365}, {"id": 178, "seek": 95888, "start": 970.4, "end": 975.12, "text": " Andrew is actually holding a lock this time, rather than the semaphore. And, and, and here we have our", "tokens": [50940, 10110, 307, 767, 5061, 257, 4017, 341, 565, 11, 2831, 813, 264, 4361, 13957, 418, 13, 400, 11, 293, 11, 293, 510, 321, 362, 527, 51176], "temperature": 0.0, "avg_logprob": -0.1369549131741489, "compression_ratio": 1.9236641221374047, "no_speech_prob": 0.008811671286821365}, {"id": 179, "seek": 95888, "start": 975.12, "end": 980.16, "text": " virtual thread number 22 is going to attempt to do a monitor enter at, at this point. And what happens", "tokens": [51176, 6374, 7207, 1230, 5853, 307, 516, 281, 5217, 281, 360, 257, 6002, 3242, 412, 11, 412, 341, 935, 13, 400, 437, 2314, 51428], "temperature": 0.0, "avg_logprob": -0.1369549131741489, "compression_ratio": 1.9236641221374047, "no_speech_prob": 0.008811671286821365}, {"id": 180, "seek": 95888, "start": 980.16, "end": 986.0, "text": " today is, is it actually just, it actually blocks when at that monitor, at that, at that, essentially", "tokens": [51428, 965, 307, 11, 307, 309, 767, 445, 11, 309, 767, 8474, 562, 412, 300, 6002, 11, 412, 300, 11, 412, 300, 11, 4476, 51720], "temperature": 0.0, "avg_logprob": -0.1369549131741489, "compression_ratio": 1.9236641221374047, "no_speech_prob": 0.008811671286821365}, {"id": 181, "seek": 98600, "start": 986.0, "end": 992.8, "text": " at that, at that, that monitor enter. So what's going on here is, is, is, is a contended monitor", "tokens": [50364, 412, 300, 11, 412, 300, 11, 300, 6002, 3242, 13, 407, 437, 311, 516, 322, 510, 307, 11, 307, 11, 307, 11, 307, 257, 660, 3502, 6002, 50704], "temperature": 0.0, "avg_logprob": -0.1256481996223108, "compression_ratio": 2.0081632653061225, "no_speech_prob": 0.003030091058462858}, {"id": 182, "seek": 98600, "start": 992.8, "end": 998.16, "text": " is, is, is actually a call into the runtime. It's essentially parking in the runtime. And, and,", "tokens": [50704, 307, 11, 307, 11, 307, 767, 257, 818, 666, 264, 34474, 13, 467, 311, 4476, 9893, 294, 264, 34474, 13, 400, 11, 293, 11, 50972], "temperature": 0.0, "avg_logprob": -0.1256481996223108, "compression_ratio": 2.0081632653061225, "no_speech_prob": 0.003030091058462858}, {"id": 183, "seek": 98600, "start": 998.16, "end": 1003.76, "text": " this is something that has to be, we have to remove that and essentially pop all those frames in", "tokens": [50972, 341, 307, 746, 300, 575, 281, 312, 11, 321, 362, 281, 4159, 300, 293, 4476, 1665, 439, 729, 12083, 294, 51252], "temperature": 0.0, "avg_logprob": -0.1256481996223108, "compression_ratio": 2.0081632653061225, "no_speech_prob": 0.003030091058462858}, {"id": 184, "seek": 98600, "start": 1003.76, "end": 1008.48, "text": " order to be able to actually, to, to, to, to deal with this. So the way things are actually working", "tokens": [51252, 1668, 281, 312, 1075, 281, 767, 11, 281, 11, 281, 11, 281, 11, 281, 2028, 365, 341, 13, 407, 264, 636, 721, 366, 767, 1364, 51488], "temperature": 0.0, "avg_logprob": -0.1256481996223108, "compression_ratio": 2.0081632653061225, "no_speech_prob": 0.003030091058462858}, {"id": 185, "seek": 98600, "start": 1008.48, "end": 1014.56, "text": " at the moment is, is, is what, what, what Patricia has come up with is, is, is that essentially allows", "tokens": [51488, 412, 264, 1623, 307, 11, 307, 11, 307, 437, 11, 437, 11, 437, 34307, 575, 808, 493, 365, 307, 11, 307, 11, 307, 300, 4476, 4045, 51792], "temperature": 0.0, "avg_logprob": -0.1256481996223108, "compression_ratio": 2.0081632653061225, "no_speech_prob": 0.003030091058462858}, {"id": 186, "seek": 101456, "start": 1014.56, "end": 1020.2399999999999, "text": " the VM to do a yield while it's from, from, from in the runtime. So normally we actually would do", "tokens": [50364, 264, 18038, 281, 360, 257, 11257, 1339, 309, 311, 490, 11, 490, 11, 490, 294, 264, 34474, 13, 407, 5646, 321, 767, 576, 360, 50648], "temperature": 0.0, "avg_logprob": -0.20812792570694633, "compression_ratio": 1.9603960396039604, "no_speech_prob": 0.0023512952029705048}, {"id": 187, "seek": 101456, "start": 1020.2399999999999, "end": 1025.6799999999998, "text": " these yield from, from, from the, from the Java site. So it copies the frames off the, off, off,", "tokens": [50648, 613, 11257, 490, 11, 490, 11, 490, 264, 11, 490, 264, 10745, 3621, 13, 407, 309, 14341, 264, 12083, 766, 264, 11, 766, 11, 766, 11, 50920], "temperature": 0.0, "avg_logprob": -0.20812792570694633, "compression_ratio": 1.9603960396039604, "no_speech_prob": 0.0023512952029705048}, {"id": 188, "seek": 101456, "start": 1025.6799999999998, "end": 1029.84, "text": " off the stack into the heap, just like we would with, with, with, with, with a, with a normal and,", "tokens": [50920, 766, 264, 8630, 666, 264, 33591, 11, 445, 411, 321, 576, 365, 11, 365, 11, 365, 11, 365, 11, 365, 257, 11, 365, 257, 2710, 293, 11, 51128], "temperature": 0.0, "avg_logprob": -0.20812792570694633, "compression_ratio": 1.9603960396039604, "no_speech_prob": 0.0023512952029705048}, {"id": 189, "seek": 101456, "start": 1030.56, "end": 1038.32, "text": " and, and, we would do it a normal yield and, and freeze. And what it does is, is it actually puts the,", "tokens": [51164, 293, 11, 293, 11, 321, 576, 360, 309, 257, 2710, 11257, 293, 11, 293, 15959, 13, 400, 437, 309, 775, 307, 11, 307, 309, 767, 8137, 264, 11, 51552], "temperature": 0.0, "avg_logprob": -0.20812792570694633, "compression_ratio": 1.9603960396039604, "no_speech_prob": 0.0023512952029705048}, {"id": 190, "seek": 103832, "start": 1038.72, "end": 1045.2, "text": " the virtual thread onto the wait list for the, for, for, for the, for the lock. And at a high level,", "tokens": [50384, 264, 6374, 7207, 3911, 264, 1699, 1329, 337, 264, 11, 337, 11, 337, 11, 337, 264, 11, 337, 264, 4017, 13, 400, 412, 257, 1090, 1496, 11, 50708], "temperature": 0.0, "avg_logprob": -0.15456194077094976, "compression_ratio": 1.9157088122605364, "no_speech_prob": 0.025419412180781364}, {"id": 191, "seek": 103832, "start": 1045.2, "end": 1051.28, "text": " it's as if we're actually doing a yield at that point. There's a bit of, there's a bit of magic", "tokens": [50708, 309, 311, 382, 498, 321, 434, 767, 884, 257, 11257, 412, 300, 935, 13, 821, 311, 257, 857, 295, 11, 456, 311, 257, 857, 295, 5585, 51012], "temperature": 0.0, "avg_logprob": -0.15456194077094976, "compression_ratio": 1.9157088122605364, "no_speech_prob": 0.025419412180781364}, {"id": 192, "seek": 103832, "start": 1051.28, "end": 1056.1599999999999, "text": " that goes on with where it actually has to, has to return back as, as if it hold the, hold the lock", "tokens": [51012, 300, 1709, 322, 365, 689, 309, 767, 575, 281, 11, 575, 281, 2736, 646, 382, 11, 382, 498, 309, 1797, 264, 11, 1797, 264, 4017, 51256], "temperature": 0.0, "avg_logprob": -0.15456194077094976, "compression_ratio": 1.9157088122605364, "no_speech_prob": 0.025419412180781364}, {"id": 193, "seek": 103832, "start": 1056.1599999999999, "end": 1060.08, "text": " and then you've got to run the, the, the stub that actually goes and actually does some fix up some,", "tokens": [51256, 293, 550, 291, 600, 658, 281, 1190, 264, 11, 264, 11, 264, 20266, 300, 767, 1709, 293, 767, 775, 512, 3191, 493, 512, 11, 51452], "temperature": 0.0, "avg_logprob": -0.15456194077094976, "compression_ratio": 1.9157088122605364, "no_speech_prob": 0.025419412180781364}, {"id": 194, "seek": 103832, "start": 1060.08, "end": 1065.2, "text": " and there's, there's VM magic that actually happens. But essentially you're turning back to Java in a,", "tokens": [51452, 293, 456, 311, 11, 456, 311, 18038, 5585, 300, 767, 2314, 13, 583, 4476, 291, 434, 6246, 646, 281, 10745, 294, 257, 11, 51708], "temperature": 0.0, "avg_logprob": -0.15456194077094976, "compression_ratio": 1.9157088122605364, "no_speech_prob": 0.025419412180781364}, {"id": 195, "seek": 106520, "start": 1065.2, "end": 1070.48, "text": " kind of a blocking mode and then we can actually fix up the state and, and, and move it to its", "tokens": [50364, 733, 295, 257, 17776, 4391, 293, 550, 321, 393, 767, 3191, 493, 264, 1785, 293, 11, 293, 11, 293, 1286, 309, 281, 1080, 50628], "temperature": 0.0, "avg_logprob": -0.13608781681504362, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0035852850414812565}, {"id": 196, "seek": 106520, "start": 1070.48, "end": 1076.0800000000002, "text": " block mode so that the, the thread is actually blocked. So what I've put here is, is just to give", "tokens": [50628, 3461, 4391, 370, 300, 264, 11, 264, 7207, 307, 767, 15470, 13, 407, 437, 286, 600, 829, 510, 307, 11, 307, 445, 281, 976, 50908], "temperature": 0.0, "avg_logprob": -0.13608781681504362, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0035852850414812565}, {"id": 197, "seek": 106520, "start": 1076.0800000000002, "end": 1080.48, "text": " you, just to kind of, you can visualize how this works. We do our monitor enter, which this could", "tokens": [50908, 291, 11, 445, 281, 733, 295, 11, 291, 393, 23273, 577, 341, 1985, 13, 492, 360, 527, 6002, 3242, 11, 597, 341, 727, 51128], "temperature": 0.0, "avg_logprob": -0.13608781681504362, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0035852850414812565}, {"id": 198, "seek": 106520, "start": 1080.48, "end": 1086.0800000000002, "text": " be a, a, a synchronized method as well. And it's as if we're actually calling into yield at that", "tokens": [51128, 312, 257, 11, 257, 11, 257, 19331, 1602, 3170, 382, 731, 13, 400, 309, 311, 382, 498, 321, 434, 767, 5141, 666, 11257, 412, 300, 51408], "temperature": 0.0, "avg_logprob": -0.13608781681504362, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0035852850414812565}, {"id": 199, "seek": 106520, "start": 1086.0800000000002, "end": 1092.56, "text": " point. So that's actually very, very nice. That actually can work. So when Andrew releases the,", "tokens": [51408, 935, 13, 407, 300, 311, 767, 588, 11, 588, 1481, 13, 663, 767, 393, 589, 13, 407, 562, 10110, 16952, 264, 11, 51732], "temperature": 0.0, "avg_logprob": -0.13608781681504362, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0035852850414812565}, {"id": 200, "seek": 109256, "start": 1092.72, "end": 1099.6799999999998, "text": " the lock, then we, we just continue on and we're, and when we do that, what happens is, is Andrew", "tokens": [50372, 264, 4017, 11, 550, 321, 11, 321, 445, 2354, 322, 293, 321, 434, 11, 293, 562, 321, 360, 300, 11, 437, 2314, 307, 11, 307, 10110, 50720], "temperature": 0.0, "avg_logprob": -0.1877139555204899, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.009636761620640755}, {"id": 201, "seek": 109256, "start": 1099.6799999999998, "end": 1105.12, "text": " releases the lock. We've now got no, no, wait for virtual thread 22. How do we actually get,", "tokens": [50720, 16952, 264, 4017, 13, 492, 600, 586, 658, 572, 11, 572, 11, 1699, 337, 6374, 7207, 5853, 13, 1012, 360, 321, 767, 483, 11, 50992], "temperature": 0.0, "avg_logprob": -0.1877139555204899, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.009636761620640755}, {"id": 202, "seek": 109256, "start": 1105.12, "end": 1110.8, "text": " how do we actually integrate back into the scheduler to get that to, to, to run again? And the way it", "tokens": [50992, 577, 360, 321, 767, 13365, 646, 666, 264, 12000, 260, 281, 483, 300, 281, 11, 281, 11, 281, 1190, 797, 30, 400, 264, 636, 309, 51276], "temperature": 0.0, "avg_logprob": -0.1877139555204899, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.009636761620640755}, {"id": 203, "seek": 109256, "start": 1110.8, "end": 1116.96, "text": " actually works is it actually just moves the, the thread into, into, into a, into a list and, and", "tokens": [51276, 767, 1985, 307, 309, 767, 445, 6067, 264, 11, 264, 7207, 666, 11, 666, 11, 666, 257, 11, 666, 257, 1329, 293, 11, 293, 51584], "temperature": 0.0, "avg_logprob": -0.1877139555204899, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.009636761620640755}, {"id": 204, "seek": 111696, "start": 1116.96, "end": 1123.92, "text": " unblocker thread will, will actually unblock it. This, so for those that have, understand how, how", "tokens": [50364, 517, 28830, 260, 7207, 486, 11, 486, 767, 517, 28830, 309, 13, 639, 11, 370, 337, 729, 300, 362, 11, 1223, 577, 11, 577, 50712], "temperature": 0.0, "avg_logprob": -0.15359134811291592, "compression_ratio": 1.896153846153846, "no_speech_prob": 0.02552894689142704}, {"id": 205, "seek": 111696, "start": 1123.92, "end": 1128.32, "text": " reference processing work in the GC, this is essentially kind of like another reference handler.", "tokens": [50712, 6408, 9007, 589, 294, 264, 29435, 11, 341, 307, 4476, 733, 295, 411, 1071, 6408, 41967, 13, 50932], "temperature": 0.0, "avg_logprob": -0.15359134811291592, "compression_ratio": 1.896153846153846, "no_speech_prob": 0.02552894689142704}, {"id": 206, "seek": 111696, "start": 1128.32, "end": 1133.04, "text": " It's the, it's, it's, it's, it's, it's, it's essentially queuing up, queuing up objects that", "tokens": [50932, 467, 311, 264, 11, 309, 311, 11, 309, 311, 11, 309, 311, 11, 309, 311, 11, 309, 311, 11, 309, 311, 4476, 631, 9635, 493, 11, 631, 9635, 493, 6565, 300, 51168], "temperature": 0.0, "avg_logprob": -0.15359134811291592, "compression_ratio": 1.896153846153846, "no_speech_prob": 0.02552894689142704}, {"id": 207, "seek": 111696, "start": 1133.04, "end": 1139.04, "text": " are, get handled by, by, by Java thread. So the unblocker then just, it, it, it just snapshots the,", "tokens": [51168, 366, 11, 483, 18033, 538, 11, 538, 11, 538, 10745, 7207, 13, 407, 264, 517, 28830, 260, 550, 445, 11, 309, 11, 309, 11, 309, 445, 19206, 27495, 264, 11, 51468], "temperature": 0.0, "avg_logprob": -0.15359134811291592, "compression_ratio": 1.896153846153846, "no_speech_prob": 0.02552894689142704}, {"id": 208, "seek": 111696, "start": 1139.04, "end": 1144.8, "text": " however that, that, that list and, and then it just wakes up those threads, which puts it back into the,", "tokens": [51468, 4461, 300, 11, 300, 11, 300, 1329, 293, 11, 293, 550, 309, 445, 29610, 493, 729, 19314, 11, 597, 8137, 309, 646, 666, 264, 11, 51756], "temperature": 0.0, "avg_logprob": -0.15359134811291592, "compression_ratio": 1.896153846153846, "no_speech_prob": 0.02552894689142704}, {"id": 209, "seek": 114480, "start": 1145.44, "end": 1151.52, "text": " to the scheduler's queue. So that's all very nice. So when our example here is, is that Andrew has", "tokens": [50396, 281, 264, 12000, 260, 311, 18639, 13, 407, 300, 311, 439, 588, 1481, 13, 407, 562, 527, 1365, 510, 307, 11, 307, 300, 10110, 575, 50700], "temperature": 0.0, "avg_logprob": -0.15506088733673096, "compression_ratio": 1.7985074626865671, "no_speech_prob": 0.002647265326231718}, {"id": 210, "seek": 114480, "start": 1151.52, "end": 1158.3999999999999, "text": " released the lock and we, we, we, we, we, we queue up that virtual thread to actually continue", "tokens": [50700, 4736, 264, 4017, 293, 321, 11, 321, 11, 321, 11, 321, 11, 321, 11, 321, 18639, 493, 300, 6374, 7207, 281, 767, 2354, 51044], "temperature": 0.0, "avg_logprob": -0.15506088733673096, "compression_ratio": 1.7985074626865671, "no_speech_prob": 0.002647265326231718}, {"id": 211, "seek": 114480, "start": 1158.3999999999999, "end": 1163.6, "text": " and it gets scheduled and it's a continuous execution inside the synchronized block. Very,", "tokens": [51044, 293, 309, 2170, 15678, 293, 309, 311, 257, 10957, 15058, 1854, 264, 19331, 1602, 3461, 13, 4372, 11, 51304], "temperature": 0.0, "avg_logprob": -0.15506088733673096, "compression_ratio": 1.7985074626865671, "no_speech_prob": 0.002647265326231718}, {"id": 212, "seek": 114480, "start": 1163.6, "end": 1168.96, "text": " very nice. So okay, high level question then, does this actually solve all of our pinning issues?", "tokens": [51304, 588, 1481, 13, 407, 1392, 11, 1090, 1496, 1168, 550, 11, 775, 341, 767, 5039, 439, 295, 527, 5447, 773, 2663, 30, 51572], "temperature": 0.0, "avg_logprob": -0.15506088733673096, "compression_ratio": 1.7985074626865671, "no_speech_prob": 0.002647265326231718}, {"id": 213, "seek": 114480, "start": 1168.96, "end": 1173.6, "text": " And the answer is no. There's always work to do. We've got, we've got, there's, there's issues with", "tokens": [51572, 400, 264, 1867, 307, 572, 13, 821, 311, 1009, 589, 281, 360, 13, 492, 600, 658, 11, 321, 600, 658, 11, 456, 311, 11, 456, 311, 2663, 365, 51804], "temperature": 0.0, "avg_logprob": -0.15506088733673096, "compression_ratio": 1.7985074626865671, "no_speech_prob": 0.002647265326231718}, {"id": 214, "seek": 117360, "start": 1173.6, "end": 1177.84, "text": " native frames. You can't, you can't actually yield with the native frame on the continuation stack.", "tokens": [50364, 8470, 12083, 13, 509, 393, 380, 11, 291, 393, 380, 767, 11257, 365, 264, 8470, 3920, 322, 264, 29357, 8630, 13, 50576], "temperature": 0.0, "avg_logprob": -0.15325881898865218, "compression_ratio": 1.8168498168498168, "no_speech_prob": 0.007460603956133127}, {"id": 215, "seek": 117360, "start": 1177.84, "end": 1183.36, "text": " That's not a problem. I think that, that we, we, we, we will ever, ever, ever address this, a lot of", "tokens": [50576, 663, 311, 406, 257, 1154, 13, 286, 519, 300, 11, 300, 321, 11, 321, 11, 321, 11, 321, 486, 1562, 11, 1562, 11, 1562, 2985, 341, 11, 257, 688, 295, 50852], "temperature": 0.0, "avg_logprob": -0.15325881898865218, "compression_ratio": 1.8168498168498168, "no_speech_prob": 0.007460603956133127}, {"id": 216, "seek": 117360, "start": 1183.36, "end": 1188.08, "text": " unsolvable issues there. There are other things that go along with monitors. There's object weight", "tokens": [50852, 2693, 401, 17915, 2663, 456, 13, 821, 366, 661, 721, 300, 352, 2051, 365, 26518, 13, 821, 311, 2657, 3364, 51088], "temperature": 0.0, "avg_logprob": -0.15325881898865218, "compression_ratio": 1.8168498168498168, "no_speech_prob": 0.007460603956133127}, {"id": 217, "seek": 117360, "start": 1188.08, "end": 1192.1599999999999, "text": " and it's important that we actually make progress on, on, on that one. There's ideas on how to, how to", "tokens": [51088, 293, 309, 311, 1021, 300, 321, 767, 652, 4205, 322, 11, 322, 11, 322, 300, 472, 13, 821, 311, 3487, 322, 577, 281, 11, 577, 281, 51292], "temperature": 0.0, "avg_logprob": -0.15325881898865218, "compression_ratio": 1.8168498168498168, "no_speech_prob": 0.007460603956133127}, {"id": 218, "seek": 117360, "start": 1193.36, "end": 1199.28, "text": " improve that one. And then there's the other more difficult one, which is class initializers.", "tokens": [51352, 3470, 300, 472, 13, 400, 550, 456, 311, 264, 661, 544, 2252, 472, 11, 597, 307, 1508, 5883, 22525, 13, 51648], "temperature": 0.0, "avg_logprob": -0.15325881898865218, "compression_ratio": 1.8168498168498168, "no_speech_prob": 0.007460603956133127}, {"id": 219, "seek": 119928, "start": 1199.84, "end": 1205.76, "text": " And, because class initializers is, is, will require more surgery in the VM in order to be able to", "tokens": [50392, 400, 11, 570, 1508, 5883, 22525, 307, 11, 307, 11, 486, 3651, 544, 7930, 294, 264, 18038, 294, 1668, 281, 312, 1075, 281, 50688], "temperature": 0.0, "avg_logprob": -0.16312452463003305, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.006532988976687193}, {"id": 220, "seek": 119928, "start": 1205.76, "end": 1209.76, "text": " address it. So these are things that are not addressed in what's in the Loom repo now,", "tokens": [50688, 2985, 309, 13, 407, 613, 366, 721, 300, 366, 406, 13847, 294, 437, 311, 294, 264, 6130, 298, 49040, 586, 11, 50888], "temperature": 0.0, "avg_logprob": -0.16312452463003305, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.006532988976687193}, {"id": 221, "seek": 119928, "start": 1209.76, "end": 1214.24, "text": " but are things that have to be addressed over the next while in order to eliminate these, these,", "tokens": [50888, 457, 366, 721, 300, 362, 281, 312, 13847, 670, 264, 958, 1339, 294, 1668, 281, 13819, 613, 11, 613, 11, 51112], "temperature": 0.0, "avg_logprob": -0.16312452463003305, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.006532988976687193}, {"id": 222, "seek": 119928, "start": 1214.24, "end": 1220.8799999999999, "text": " these problems. Okay, moving on. I'm going to talk about IO now because there's a whole other set", "tokens": [51112, 613, 2740, 13, 1033, 11, 2684, 322, 13, 286, 478, 516, 281, 751, 466, 286, 46, 586, 570, 456, 311, 257, 1379, 661, 992, 51444], "temperature": 0.0, "avg_logprob": -0.16312452463003305, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.006532988976687193}, {"id": 223, "seek": 119928, "start": 1220.8799999999999, "end": 1227.2, "text": " of this that, that, that, that, that goes along with, with, with, with IO. So let's talk a bit about", "tokens": [51444, 295, 341, 300, 11, 300, 11, 300, 11, 300, 11, 300, 1709, 2051, 365, 11, 365, 11, 365, 11, 365, 286, 46, 13, 407, 718, 311, 751, 257, 857, 466, 51760], "temperature": 0.0, "avg_logprob": -0.16312452463003305, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.006532988976687193}, {"id": 224, "seek": 122720, "start": 1228.16, "end": 1232.8, "text": " SOCAs first because networking is, is, is actually, is, is, is actually straightforward.", "tokens": [50412, 10621, 34, 10884, 700, 570, 17985, 307, 11, 307, 11, 307, 767, 11, 307, 11, 307, 11, 307, 767, 15325, 13, 50644], "temperature": 0.0, "avg_logprob": -0.23222086164686415, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0127189876511693}, {"id": 225, "seek": 122720, "start": 1232.8, "end": 1237.28, "text": " So here we have our virtual thread is actually going to try, attempt to make, establish a TCP", "tokens": [50644, 407, 510, 321, 362, 527, 6374, 7207, 307, 767, 516, 281, 853, 11, 5217, 281, 652, 11, 8327, 257, 48965, 50868], "temperature": 0.0, "avg_logprob": -0.23222086164686415, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0127189876511693}, {"id": 226, "seek": 122720, "start": 1237.28, "end": 1244.16, "text": " connection. This case is Fosn port 443. So it's the SSL port. Okay, what happens there? So same,", "tokens": [50868, 4984, 13, 639, 1389, 307, 479, 329, 77, 2436, 16408, 18, 13, 407, 309, 311, 264, 12238, 43, 2436, 13, 1033, 11, 437, 2314, 456, 30, 407, 912, 11, 51212], "temperature": 0.0, "avg_logprob": -0.23222086164686415, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0127189876511693}, {"id": 227, "seek": 122720, "start": 1245.28, "end": 1249.52, "text": " same, same diagram as before. We've got our carrier at the top and then we've got our green, green", "tokens": [51268, 912, 11, 912, 10686, 382, 949, 13, 492, 600, 658, 527, 17574, 412, 264, 1192, 293, 550, 321, 600, 658, 527, 3092, 11, 3092, 51480], "temperature": 0.0, "avg_logprob": -0.23222086164686415, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0127189876511693}, {"id": 228, "seek": 122720, "start": 1249.52, "end": 1254.48, "text": " frames for the virtual thread. We're doing a socket, we're in the socket constructor, which", "tokens": [51480, 12083, 337, 264, 6374, 7207, 13, 492, 434, 884, 257, 19741, 11, 321, 434, 294, 264, 19741, 47479, 11, 597, 51728], "temperature": 0.0, "avg_logprob": -0.23222086164686415, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0127189876511693}, {"id": 229, "seek": 125448, "start": 1254.48, "end": 1259.6, "text": " actually initiates the connect. What does the connect actually do when you're on a virtual thread?", "tokens": [50364, 767, 6265, 1024, 264, 1745, 13, 708, 775, 264, 1745, 767, 360, 562, 291, 434, 322, 257, 6374, 7207, 30, 50620], "temperature": 0.0, "avg_logprob": -0.11656931334850835, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.0026085362769663334}, {"id": 230, "seek": 125448, "start": 1259.6, "end": 1267.2, "text": " It's actually going to initiate the connection, arm the file descriptor and then do a yield and to,", "tokens": [50620, 467, 311, 767, 516, 281, 31574, 264, 4984, 11, 3726, 264, 3991, 31280, 284, 293, 550, 360, 257, 11257, 293, 281, 11, 51000], "temperature": 0.0, "avg_logprob": -0.11656931334850835, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.0026085362769663334}, {"id": 231, "seek": 125448, "start": 1267.2, "end": 1274.0, "text": " to, so, so that the, so that the, the, the, the carrier can be released to actually go and do", "tokens": [51000, 281, 11, 370, 11, 370, 300, 264, 11, 370, 300, 264, 11, 264, 11, 264, 11, 264, 17574, 393, 312, 4736, 281, 767, 352, 293, 360, 51340], "temperature": 0.0, "avg_logprob": -0.11656931334850835, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.0026085362769663334}, {"id": 232, "seek": 125448, "start": 1274.0, "end": 1277.84, "text": " other work. So this is what our stack would actually look like. It goes down through the, the, the,", "tokens": [51340, 661, 589, 13, 407, 341, 307, 437, 527, 8630, 576, 767, 574, 411, 13, 467, 1709, 760, 807, 264, 11, 264, 11, 264, 11, 51532], "temperature": 0.0, "avg_logprob": -0.11656931334850835, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.0026085362769663334}, {"id": 233, "seek": 127784, "start": 1277.9199999999998, "end": 1283.6, "text": " the IO code and, and IO code and, and does, does, does, does our, does our yield.", "tokens": [50368, 264, 39839, 3089, 293, 11, 293, 39839, 3089, 293, 11, 293, 775, 11, 775, 11, 775, 11, 775, 527, 11, 775, 527, 11257, 13, 50652], "temperature": 0.0, "avg_logprob": -0.1546652634938558, "compression_ratio": 1.8162393162393162, "no_speech_prob": 0.020200535655021667}, {"id": 234, "seek": 127784, "start": 1284.8, "end": 1287.6, "text": " Carrier gets released to go and do other work. We're all good.", "tokens": [50712, 2741, 7326, 2170, 4736, 281, 352, 293, 360, 661, 589, 13, 492, 434, 439, 665, 13, 50852], "temperature": 0.0, "avg_logprob": -0.1546652634938558, "compression_ratio": 1.8162393162393162, "no_speech_prob": 0.020200535655021667}, {"id": 235, "seek": 127784, "start": 1288.8799999999999, "end": 1293.12, "text": " What we have then in the, in the background is, is, is the way things work for the moment", "tokens": [50916, 708, 321, 362, 550, 294, 264, 11, 294, 264, 3678, 307, 11, 307, 11, 307, 264, 636, 721, 589, 337, 264, 1623, 51128], "temperature": 0.0, "avg_logprob": -0.1546652634938558, "compression_ratio": 1.8162393162393162, "no_speech_prob": 0.020200535655021667}, {"id": 236, "seek": 127784, "start": 1293.12, "end": 1297.04, "text": " is there is this thing called a polar thread. The polar thread interacts with whatever the", "tokens": [51128, 307, 456, 307, 341, 551, 1219, 257, 12367, 7207, 13, 440, 12367, 7207, 43582, 365, 2035, 264, 51324], "temperature": 0.0, "avg_logprob": -0.1546652634938558, "compression_ratio": 1.8162393162393162, "no_speech_prob": 0.020200535655021667}, {"id": 237, "seek": 127784, "start": 1297.04, "end": 1302.8, "text": " IO mechanism is on, on the, on the platform. There's implementations for, for, for, for E-Pole and,", "tokens": [51324, 39839, 7513, 307, 322, 11, 322, 264, 11, 322, 264, 3663, 13, 821, 311, 4445, 763, 337, 11, 337, 11, 337, 11, 337, 462, 12, 47, 4812, 293, 11, 51612], "temperature": 0.0, "avg_logprob": -0.1546652634938558, "compression_ratio": 1.8162393162393162, "no_speech_prob": 0.020200535655021667}, {"id": 238, "seek": 130280, "start": 1302.8, "end": 1308.08, "text": " and KQ. There's one that integrates with the, the Windows WinSock driver. And what it does is", "tokens": [50364, 293, 591, 48, 13, 821, 311, 472, 300, 3572, 1024, 365, 264, 11, 264, 8591, 10427, 50, 1560, 6787, 13, 400, 437, 309, 775, 307, 50628], "temperature": 0.0, "avg_logprob": -0.22947784949993266, "compression_ratio": 1.774294670846395, "no_speech_prob": 0.10635566711425781}, {"id": 239, "seek": 130280, "start": 1308.08, "end": 1312.24, "text": " it's just listening for events. When there's events from the operating system to tell you that the,", "tokens": [50628, 309, 311, 445, 4764, 337, 3931, 13, 1133, 456, 311, 3931, 490, 264, 7447, 1185, 281, 980, 291, 300, 264, 11, 50836], "temperature": 0.0, "avg_logprob": -0.22947784949993266, "compression_ratio": 1.774294670846395, "no_speech_prob": 0.10635566711425781}, {"id": 240, "seek": 130280, "start": 1312.24, "end": 1318.0, "text": " that these are already events, then it just unblocks the corresponding virtual thread", "tokens": [50836, 300, 613, 366, 1217, 3931, 11, 550, 309, 445, 517, 15962, 2761, 264, 11760, 6374, 7207, 51124], "temperature": 0.0, "avg_logprob": -0.22947784949993266, "compression_ratio": 1.774294670846395, "no_speech_prob": 0.10635566711425781}, {"id": 241, "seek": 130280, "start": 1318.72, "end": 1322.8799999999999, "text": " by, well, what it does is it actually just, it, it, it unparks it and just queues its task to the", "tokens": [51160, 538, 11, 731, 11, 437, 309, 775, 307, 309, 767, 445, 11, 309, 11, 309, 11, 309, 20994, 20851, 309, 293, 445, 631, 1247, 1080, 5633, 281, 264, 51368], "temperature": 0.0, "avg_logprob": -0.22947784949993266, "compression_ratio": 1.774294670846395, "no_speech_prob": 0.10635566711425781}, {"id": 242, "seek": 130280, "start": 1322.8799999999999, "end": 1326.8799999999999, "text": " schedule and things that work. That little diagram over there, did you see that it was spinning?", "tokens": [51368, 7567, 293, 721, 300, 589, 13, 663, 707, 10686, 670, 456, 11, 630, 291, 536, 300, 309, 390, 15640, 30, 51568], "temperature": 0.0, "avg_logprob": -0.22947784949993266, "compression_ratio": 1.774294670846395, "no_speech_prob": 0.10635566711425781}, {"id": 243, "seek": 130280, "start": 1327.6, "end": 1331.12, "text": " I'm going to reuse that in a couple of slides just to make, essentially all that's doing is", "tokens": [51604, 286, 478, 516, 281, 26225, 300, 294, 257, 1916, 295, 9788, 445, 281, 652, 11, 4476, 439, 300, 311, 884, 307, 51780], "temperature": 0.0, "avg_logprob": -0.22947784949993266, "compression_ratio": 1.774294670846395, "no_speech_prob": 0.10635566711425781}, {"id": 244, "seek": 133112, "start": 1331.12, "end": 1333.6799999999998, "text": " listening for events on parking throughout, listening for events on parking throughout,", "tokens": [50364, 4764, 337, 3931, 322, 9893, 3710, 11, 4764, 337, 3931, 322, 9893, 3710, 11, 50492], "temperature": 0.0, "avg_logprob": -0.20402545660314425, "compression_ratio": 2.019455252918288, "no_speech_prob": 0.004503385629504919}, {"id": 245, "seek": 133112, "start": 1333.6799999999998, "end": 1335.04, "text": " listening for events and so on. That's all it does.", "tokens": [50492, 4764, 337, 3931, 293, 370, 322, 13, 663, 311, 439, 309, 775, 13, 50560], "temperature": 0.0, "avg_logprob": -0.20402545660314425, "compression_ratio": 2.019455252918288, "no_speech_prob": 0.004503385629504919}, {"id": 246, "seek": 133112, "start": 1337.04, "end": 1343.12, "text": " And so, but back to this, back, back, back, back to our example here is, is we're, we're trying to", "tokens": [50660, 400, 370, 11, 457, 646, 281, 341, 11, 646, 11, 646, 11, 646, 11, 646, 281, 527, 1365, 510, 307, 11, 307, 321, 434, 11, 321, 434, 1382, 281, 50964], "temperature": 0.0, "avg_logprob": -0.20402545660314425, "compression_ratio": 2.019455252918288, "no_speech_prob": 0.004503385629504919}, {"id": 247, "seek": 133112, "start": 1343.12, "end": 1348.8799999999999, "text": " establish a connection. The connection is now established and because we've done a wake up,", "tokens": [50964, 8327, 257, 4984, 13, 440, 4984, 307, 586, 7545, 293, 570, 321, 600, 1096, 257, 6634, 493, 11, 51252], "temperature": 0.0, "avg_logprob": -0.20402545660314425, "compression_ratio": 2.019455252918288, "no_speech_prob": 0.004503385629504919}, {"id": 248, "seek": 133112, "start": 1348.8799999999999, "end": 1353.36, "text": " we pop all the frames, we're, we're, we're, we're gone past the socket and constructor now and", "tokens": [51252, 321, 1665, 439, 264, 12083, 11, 321, 434, 11, 321, 434, 11, 321, 434, 11, 321, 434, 2780, 1791, 264, 19741, 293, 47479, 586, 293, 51476], "temperature": 0.0, "avg_logprob": -0.20402545660314425, "compression_ratio": 2.019455252918288, "no_speech_prob": 0.004503385629504919}, {"id": 249, "seek": 133112, "start": 1354.3999999999999, "end": 1360.7199999999998, "text": " we're, we're, we're all good. So this is kind of the way things work today in, in, in JDK 21.", "tokens": [51528, 321, 434, 11, 321, 434, 11, 321, 434, 439, 665, 13, 407, 341, 307, 733, 295, 264, 636, 721, 589, 965, 294, 11, 294, 11, 294, 37082, 42, 5080, 13, 51844], "temperature": 0.0, "avg_logprob": -0.20402545660314425, "compression_ratio": 2.019455252918288, "no_speech_prob": 0.004503385629504919}, {"id": 250, "seek": 136112, "start": 1361.12, "end": 1365.4399999999998, "text": " You've got this, you've got this thread that's picking up IO events. It's queuing up and", "tokens": [50364, 509, 600, 658, 341, 11, 291, 600, 658, 341, 7207, 300, 311, 8867, 493, 39839, 3931, 13, 467, 311, 631, 9635, 493, 293, 50580], "temperature": 0.0, "avg_logprob": -0.2011659048437103, "compression_ratio": 1.8629032258064515, "no_speech_prob": 0.0035110556054860353}, {"id": 251, "seek": 136112, "start": 1367.4399999999998, "end": 1372.8799999999999, "text": " the corresponding virtual threads to, to, to, to the schedule, in this case I should, I did,", "tokens": [50680, 264, 11760, 6374, 19314, 281, 11, 281, 11, 281, 11, 281, 264, 7567, 11, 294, 341, 1389, 286, 820, 11, 286, 630, 11, 50952], "temperature": 0.0, "avg_logprob": -0.2011659048437103, "compression_ratio": 1.8629032258064515, "no_speech_prob": 0.0035110556054860353}, {"id": 252, "seek": 136112, "start": 1372.8799999999999, "end": 1378.9599999999998, "text": " depict it as a, as, as a box full of carrier threads. That's actually not all that efficient", "tokens": [50952, 31553, 309, 382, 257, 11, 382, 11, 382, 257, 2424, 1577, 295, 17574, 19314, 13, 663, 311, 767, 406, 439, 300, 7148, 51256], "temperature": 0.0, "avg_logprob": -0.2011659048437103, "compression_ratio": 1.8629032258064515, "no_speech_prob": 0.0035110556054860353}, {"id": 253, "seek": 136112, "start": 1378.9599999999998, "end": 1382.7199999999998, "text": " because a lot, there's a number of issues with this one. You actually start scaling things,", "tokens": [51256, 570, 257, 688, 11, 456, 311, 257, 1230, 295, 2663, 365, 341, 472, 13, 509, 767, 722, 21589, 721, 11, 51444], "temperature": 0.0, "avg_logprob": -0.2011659048437103, "compression_ratio": 1.8629032258064515, "no_speech_prob": 0.0035110556054860353}, {"id": 254, "seek": 136112, "start": 1382.7199999999998, "end": 1387.76, "text": " scaling things up and particularly is, is, is that you've got a, you've got a number of carrier", "tokens": [51444, 21589, 721, 493, 293, 4098, 307, 11, 307, 11, 307, 300, 291, 600, 658, 257, 11, 291, 600, 658, 257, 1230, 295, 17574, 51696], "temperature": 0.0, "avg_logprob": -0.2011659048437103, "compression_ratio": 1.8629032258064515, "no_speech_prob": 0.0035110556054860353}, {"id": 255, "seek": 138776, "start": 1387.76, "end": 1391.12, "text": " threads that correspond to the number cores and then you've got these other polar threads that", "tokens": [50364, 19314, 300, 6805, 281, 264, 1230, 24826, 293, 550, 291, 600, 658, 613, 661, 12367, 19314, 300, 50532], "temperature": 0.0, "avg_logprob": -0.09058826089762956, "compression_ratio": 1.8745874587458746, "no_speech_prob": 0.012389312498271465}, {"id": 256, "seek": 138776, "start": 1391.12, "end": 1395.6, "text": " are actually trying to compete for CPU cycles. You've also got the issue where, where you're", "tokens": [50532, 366, 767, 1382, 281, 11831, 337, 13199, 17796, 13, 509, 600, 611, 658, 264, 2734, 689, 11, 689, 291, 434, 50756], "temperature": 0.0, "avg_logprob": -0.09058826089762956, "compression_ratio": 1.8745874587458746, "no_speech_prob": 0.012389312498271465}, {"id": 257, "seek": 138776, "start": 1395.6, "end": 1399.36, "text": " picking up IO events on one thread but it's actually going to end up being processed on a", "tokens": [50756, 8867, 493, 39839, 3931, 322, 472, 7207, 457, 309, 311, 767, 516, 281, 917, 493, 885, 18846, 322, 257, 50944], "temperature": 0.0, "avg_logprob": -0.09058826089762956, "compression_ratio": 1.8745874587458746, "no_speech_prob": 0.012389312498271465}, {"id": 258, "seek": 138776, "start": 1399.36, "end": 1404.64, "text": " different thread. So this is the, the, the, the, there's room for, for, for efficiency on, on this.", "tokens": [50944, 819, 7207, 13, 407, 341, 307, 264, 11, 264, 11, 264, 11, 264, 11, 456, 311, 1808, 337, 11, 337, 11, 337, 10493, 322, 11, 322, 341, 13, 51208], "temperature": 0.0, "avg_logprob": -0.09058826089762956, "compression_ratio": 1.8745874587458746, "no_speech_prob": 0.012389312498271465}, {"id": 259, "seek": 138776, "start": 1405.52, "end": 1411.12, "text": " So one of the things we actually have done in, in, in, in, in JDK 22 is we magically move these", "tokens": [51252, 407, 472, 295, 264, 721, 321, 767, 362, 1096, 294, 11, 294, 11, 294, 11, 294, 11, 294, 37082, 42, 5853, 307, 321, 39763, 1286, 613, 51532], "temperature": 0.0, "avg_logprob": -0.09058826089762956, "compression_ratio": 1.8745874587458746, "no_speech_prob": 0.012389312498271465}, {"id": 260, "seek": 138776, "start": 1411.12, "end": 1417.04, "text": " polar threads at least on some platforms from being platform threads to being virtual threads.", "tokens": [51532, 12367, 19314, 412, 1935, 322, 512, 9473, 490, 885, 3663, 19314, 281, 885, 6374, 19314, 13, 51828], "temperature": 0.0, "avg_logprob": -0.09058826089762956, "compression_ratio": 1.8745874587458746, "no_speech_prob": 0.012389312498271465}, {"id": 261, "seek": 141776, "start": 1417.76, "end": 1422.0, "text": " Now the implications for that is they actually integrate then with the schedule. You're only", "tokens": [50364, 823, 264, 16602, 337, 300, 307, 436, 767, 13365, 550, 365, 264, 7567, 13, 509, 434, 787, 50576], "temperature": 0.0, "avg_logprob": -0.10187093788218275, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0011333645088598132}, {"id": 262, "seek": 141776, "start": 1422.0, "end": 1426.48, "text": " picking up, you're only picking up IO events and queuing up virtual threads to on-park", "tokens": [50576, 8867, 493, 11, 291, 434, 787, 8867, 493, 39839, 3931, 293, 631, 9635, 493, 6374, 19314, 281, 322, 12, 31239, 50800], "temperature": 0.0, "avg_logprob": -0.10187093788218275, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0011333645088598132}, {"id": 263, "seek": 141776, "start": 1426.48, "end": 1431.44, "text": " when there's actually cycles to actually to go and do that. In addition, because of the way this", "tokens": [50800, 562, 456, 311, 767, 17796, 281, 767, 281, 352, 293, 360, 300, 13, 682, 4500, 11, 570, 295, 264, 636, 341, 51048], "temperature": 0.0, "avg_logprob": -0.10187093788218275, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0011333645088598132}, {"id": 264, "seek": 141776, "start": 1431.44, "end": 1437.04, "text": " polar thread is actually written is it will actually, most of the time, continue the IO", "tokens": [51048, 12367, 7207, 307, 767, 3720, 307, 309, 486, 767, 11, 881, 295, 264, 565, 11, 2354, 264, 39839, 51328], "temperature": 0.0, "avg_logprob": -0.10187093788218275, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0011333645088598132}, {"id": 265, "seek": 141776, "start": 1437.04, "end": 1441.76, "text": " operation on the same thread that picked up the event. So you avoid having to actually go and", "tokens": [51328, 6916, 322, 264, 912, 7207, 300, 6183, 493, 264, 2280, 13, 407, 291, 5042, 1419, 281, 767, 352, 293, 51564], "temperature": 0.0, "avg_logprob": -0.10187093788218275, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0011333645088598132}, {"id": 266, "seek": 144176, "start": 1442.48, "end": 1447.92, "text": " dealing with events to, going, going, going, going between threads. When you actually scale it up,", "tokens": [50400, 6260, 365, 3931, 281, 11, 516, 11, 516, 11, 516, 11, 516, 1296, 19314, 13, 1133, 291, 767, 4373, 309, 493, 11, 50672], "temperature": 0.0, "avg_logprob": -0.13440556428870376, "compression_ratio": 2.0977443609022557, "no_speech_prob": 0.14144423604011536}, {"id": 267, "seek": 144176, "start": 1447.92, "end": 1451.04, "text": " this is actually kind of what it actually goes and looks like is there's actually in,", "tokens": [50672, 341, 307, 767, 733, 295, 437, 309, 767, 1709, 293, 1542, 411, 307, 456, 311, 767, 294, 11, 50828], "temperature": 0.0, "avg_logprob": -0.13440556428870376, "compression_ratio": 2.0977443609022557, "no_speech_prob": 0.14144423604011536}, {"id": 268, "seek": 144176, "start": 1451.76, "end": 1455.76, "text": " in polar threads that are virtual threads are actually, and, and then there's this,", "tokens": [50864, 294, 12367, 19314, 300, 366, 6374, 19314, 366, 767, 11, 293, 11, 293, 550, 456, 311, 341, 11, 51064], "temperature": 0.0, "avg_logprob": -0.13440556428870376, "compression_ratio": 2.0977443609022557, "no_speech_prob": 0.14144423604011536}, {"id": 269, "seek": 144176, "start": 1455.76, "end": 1460.72, "text": " there's this other gutter guy in the background which is when it has to wake up polar threads,", "tokens": [51064, 456, 311, 341, 661, 5228, 391, 2146, 294, 264, 3678, 597, 307, 562, 309, 575, 281, 6634, 493, 12367, 19314, 11, 51312], "temperature": 0.0, "avg_logprob": -0.13440556428870376, "compression_ratio": 2.0977443609022557, "no_speech_prob": 0.14144423604011536}, {"id": 270, "seek": 144176, "start": 1460.72, "end": 1465.2, "text": " when there's nothing else for them to, to, to go, to go and do. And this is actually quite nice.", "tokens": [51312, 562, 456, 311, 1825, 1646, 337, 552, 281, 11, 281, 11, 281, 352, 11, 281, 352, 293, 360, 13, 400, 341, 307, 767, 1596, 1481, 13, 51536], "temperature": 0.0, "avg_logprob": -0.13440556428870376, "compression_ratio": 2.0977443609022557, "no_speech_prob": 0.14144423604011536}, {"id": 271, "seek": 144176, "start": 1465.2, "end": 1470.8, "text": " There's, there's a nice paper written by them. There's a, there's, there's, there's a team in the", "tokens": [51536, 821, 311, 11, 456, 311, 257, 1481, 3035, 3720, 538, 552, 13, 821, 311, 257, 11, 456, 311, 11, 456, 311, 11, 456, 311, 257, 1469, 294, 264, 51816], "temperature": 0.0, "avg_logprob": -0.13440556428870376, "compression_ratio": 2.0977443609022557, "no_speech_prob": 0.14144423604011536}, {"id": 272, "seek": 147080, "start": 1470.8, "end": 1476.24, "text": " University of Waterloo that actually work in, in, in the sort of similar area on their own library", "tokens": [50364, 3535, 295, 8772, 38511, 300, 767, 589, 294, 11, 294, 11, 294, 264, 1333, 295, 2531, 1859, 322, 641, 1065, 6405, 50636], "temperature": 0.0, "avg_logprob": -0.22986150510383374, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.005135767627507448}, {"id": 273, "seek": 147080, "start": 1476.24, "end": 1481.84, "text": " of, of for lightweight threads in C++. And they've got an initial paper which deals with all the IO", "tokens": [50636, 295, 11, 295, 337, 22052, 19314, 294, 383, 25472, 13, 400, 436, 600, 658, 364, 5883, 3035, 597, 11215, 365, 439, 264, 39839, 50916], "temperature": 0.0, "avg_logprob": -0.22986150510383374, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.005135767627507448}, {"id": 274, "seek": 147080, "start": 1481.84, "end": 1485.76, "text": " strategies. And this is kind of one of the IO strategies that, that they, they're, they're also", "tokens": [50916, 9029, 13, 400, 341, 307, 733, 295, 472, 295, 264, 39839, 9029, 300, 11, 300, 436, 11, 436, 434, 11, 436, 434, 611, 51112], "temperature": 0.0, "avg_logprob": -0.22986150510383374, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.005135767627507448}, {"id": 275, "seek": 147080, "start": 1485.76, "end": 1491.28, "text": " using by default. So, so that's Martin, and Karinson's team in, in Waterloo. It's the paper is, is,", "tokens": [51112, 1228, 538, 7576, 13, 407, 11, 370, 300, 311, 9184, 11, 293, 8009, 259, 3015, 311, 1469, 294, 11, 294, 8772, 38511, 13, 467, 311, 264, 3035, 307, 11, 307, 11, 51388], "temperature": 0.0, "avg_logprob": -0.22986150510383374, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.005135767627507448}, {"id": 276, "seek": 147080, "start": 1491.28, "end": 1494.96, "text": " have your cake and eat it, which is a great title for, for, for, for a paper.", "tokens": [51388, 362, 428, 5908, 293, 1862, 309, 11, 597, 307, 257, 869, 4876, 337, 11, 337, 11, 337, 11, 337, 257, 3035, 13, 51572], "temperature": 0.0, "avg_logprob": -0.22986150510383374, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.005135767627507448}, {"id": 277, "seek": 149496, "start": 1495.2, "end": 1502.56, "text": " So, so this is actually turns out, and some, and some benchmarks turns out to be actually quite,", "tokens": [50376, 407, 11, 370, 341, 307, 767, 4523, 484, 11, 293, 512, 11, 293, 512, 43751, 4523, 484, 281, 312, 767, 1596, 11, 50744], "temperature": 0.0, "avg_logprob": -0.23339026564851814, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.017906678840517998}, {"id": 278, "seek": 149496, "start": 1504.0, "end": 1510.24, "text": " very, very profitable. And because, and so this, this, this is just a, some random benchmark that's", "tokens": [50816, 588, 11, 588, 21608, 13, 400, 570, 11, 293, 370, 341, 11, 341, 11, 341, 307, 445, 257, 11, 512, 4974, 18927, 300, 311, 51128], "temperature": 0.0, "avg_logprob": -0.23339026564851814, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.017906678840517998}, {"id": 279, "seek": 149496, "start": 1510.24, "end": 1517.28, "text": " actually sending, sending a 1K request and getting a 16K response. It's, it's on the loop back.", "tokens": [51128, 767, 7750, 11, 7750, 257, 502, 42, 5308, 293, 1242, 257, 3165, 42, 4134, 13, 467, 311, 11, 309, 311, 322, 264, 6367, 646, 13, 51480], "temperature": 0.0, "avg_logprob": -0.23339026564851814, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.017906678840517998}, {"id": 280, "seek": 149496, "start": 1517.28, "end": 1522.24, "text": " There's, there's a client and, and, and, and, and a server thread for each one of these. So there's", "tokens": [51480, 821, 311, 11, 456, 311, 257, 6423, 293, 11, 293, 11, 293, 11, 293, 11, 293, 257, 7154, 7207, 337, 1184, 472, 295, 613, 13, 407, 456, 311, 51728], "temperature": 0.0, "avg_logprob": -0.23339026564851814, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.017906678840517998}, {"id": 281, "seek": 152224, "start": 1522.24, "end": 1526.48, "text": " a lot of parking actually going on. There's a lot of IO between them. But the nice thing is, is we", "tokens": [50364, 257, 688, 295, 9893, 767, 516, 322, 13, 821, 311, 257, 688, 295, 39839, 1296, 552, 13, 583, 264, 1481, 551, 307, 11, 307, 321, 50576], "temperature": 0.0, "avg_logprob": -0.13548344617698624, "compression_ratio": 1.9830508474576272, "no_speech_prob": 0.0061466577462852}, {"id": 282, "seek": 152224, "start": 1526.48, "end": 1530.72, "text": " actually see improvements, significant improvements on, on, on, on, on, on, on arranging systems,", "tokens": [50576, 767, 536, 13797, 11, 4776, 13797, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 5539, 9741, 3652, 11, 50788], "temperature": 0.0, "avg_logprob": -0.13548344617698624, "compression_ratio": 1.9830508474576272, "no_speech_prob": 0.0061466577462852}, {"id": 283, "seek": 152224, "start": 1530.72, "end": 1536.88, "text": " which is actually quite good. Okay. Mo, moving on a lot, I want to talk about a bit of file IO,", "tokens": [50788, 597, 307, 767, 1596, 665, 13, 1033, 13, 3335, 11, 2684, 322, 257, 688, 11, 286, 528, 281, 751, 466, 257, 857, 295, 3991, 39839, 11, 51096], "temperature": 0.0, "avg_logprob": -0.13548344617698624, "compression_ratio": 1.9830508474576272, "no_speech_prob": 0.0061466577462852}, {"id": 284, "seek": 152224, "start": 1536.88, "end": 1541.44, "text": " because this is where we actually put on our sad face. Because it's, it's not as, not as, not as", "tokens": [51096, 570, 341, 307, 689, 321, 767, 829, 322, 527, 4227, 1851, 13, 1436, 309, 311, 11, 309, 311, 406, 382, 11, 406, 382, 11, 406, 382, 51324], "temperature": 0.0, "avg_logprob": -0.13548344617698624, "compression_ratio": 1.9830508474576272, "no_speech_prob": 0.0061466577462852}, {"id": 285, "seek": 152224, "start": 1541.44, "end": 1545.92, "text": " good a story. So the example I'm actually using on this one is, is, is, is, is, I'm, I'm opening a", "tokens": [51324, 665, 257, 1657, 13, 407, 264, 1365, 286, 478, 767, 1228, 322, 341, 472, 307, 11, 307, 11, 307, 11, 307, 11, 307, 11, 286, 478, 11, 286, 478, 5193, 257, 51548], "temperature": 0.0, "avg_logprob": -0.13548344617698624, "compression_ratio": 1.9830508474576272, "no_speech_prob": 0.0061466577462852}, {"id": 286, "seek": 152224, "start": 1545.92, "end": 1550.48, "text": " file. So this is actually, this is code executing on a virtual thread. There's, I'm, I'm doing a", "tokens": [51548, 3991, 13, 407, 341, 307, 767, 11, 341, 307, 3089, 32368, 322, 257, 6374, 7207, 13, 821, 311, 11, 286, 478, 11, 286, 478, 884, 257, 51776], "temperature": 0.0, "avg_logprob": -0.13548344617698624, "compression_ratio": 1.9830508474576272, "no_speech_prob": 0.0061466577462852}, {"id": 287, "seek": 155048, "start": 1550.48, "end": 1555.52, "text": " file open here and I'm doing a file read. So two, two, two file operations. Down on the right,", "tokens": [50364, 3991, 1269, 510, 293, 286, 478, 884, 257, 3991, 1401, 13, 407, 732, 11, 732, 11, 732, 3991, 7705, 13, 9506, 322, 264, 558, 11, 50616], "temperature": 0.0, "avg_logprob": -0.11898795318603515, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.0075329262763261795}, {"id": 288, "seek": 155048, "start": 1555.52, "end": 1559.92, "text": " I've got just, I've just showing the, the, the box with the, let's assume this is a four core", "tokens": [50616, 286, 600, 658, 445, 11, 286, 600, 445, 4099, 264, 11, 264, 11, 264, 2424, 365, 264, 11, 718, 311, 6552, 341, 307, 257, 1451, 4965, 50836], "temperature": 0.0, "avg_logprob": -0.11898795318603515, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.0075329262763261795}, {"id": 289, "seek": 155048, "start": 1559.92, "end": 1564.0, "text": " system. This four carrier thread is actually sitting there. So what actually happens today,", "tokens": [50836, 1185, 13, 639, 1451, 17574, 7207, 307, 767, 3798, 456, 13, 407, 437, 767, 2314, 965, 11, 51040], "temperature": 0.0, "avg_logprob": -0.11898795318603515, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.0075329262763261795}, {"id": 290, "seek": 155048, "start": 1564.0, "end": 1567.76, "text": " and it's, it's, it's a bit lame, but I'm just explaining with the way it actually works today,", "tokens": [51040, 293, 309, 311, 11, 309, 311, 11, 309, 311, 257, 857, 27635, 11, 457, 286, 478, 445, 13468, 365, 264, 636, 309, 767, 1985, 965, 11, 51228], "temperature": 0.0, "avg_logprob": -0.11898795318603515, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.0075329262763261795}, {"id": 291, "seek": 155048, "start": 1568.4, "end": 1575.68, "text": " is when you attempt to do a file IO operation that may actually consume the thread, it temporarily", "tokens": [51260, 307, 562, 291, 5217, 281, 360, 257, 3991, 39839, 6916, 300, 815, 767, 14732, 264, 7207, 11, 309, 23750, 51624], "temperature": 0.0, "avg_logprob": -0.11898795318603515, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.0075329262763261795}, {"id": 292, "seek": 157568, "start": 1575.68, "end": 1582.64, "text": " increases the parallelism, which will trigger an additional worker thread to be available to do", "tokens": [50364, 8637, 264, 8952, 1434, 11, 597, 486, 7875, 364, 4497, 11346, 7207, 281, 312, 2435, 281, 360, 50712], "temperature": 0.0, "avg_logprob": -0.14000631679188122, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.022654613479971886}, {"id": 293, "seek": 157568, "start": 1582.64, "end": 1588.64, "text": " other work. So if you've ever seen fork joint pool manage blocker, this is essentially the same thing", "tokens": [50712, 661, 589, 13, 407, 498, 291, 600, 1562, 1612, 17716, 7225, 7005, 3067, 3461, 260, 11, 341, 307, 4476, 264, 912, 551, 51012], "temperature": 0.0, "avg_logprob": -0.14000631679188122, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.022654613479971886}, {"id": 294, "seek": 157568, "start": 1588.64, "end": 1593.44, "text": " which your actual compensation that actually can happen in managed blocking operations. So you,", "tokens": [51012, 597, 428, 3539, 19644, 300, 767, 393, 1051, 294, 6453, 17776, 7705, 13, 407, 291, 11, 51252], "temperature": 0.0, "avg_logprob": -0.14000631679188122, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.022654613479971886}, {"id": 295, "seek": 157568, "start": 1593.44, "end": 1598.16, "text": " you do your file open, your thread is actually not, unavailable to do other work. And then when,", "tokens": [51252, 291, 360, 428, 3991, 1269, 11, 428, 7207, 307, 767, 406, 11, 36541, 32699, 281, 360, 661, 589, 13, 400, 550, 562, 11, 51488], "temperature": 0.0, "avg_logprob": -0.14000631679188122, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.022654613479971886}, {"id": 296, "seek": 157568, "start": 1598.16, "end": 1604.48, "text": " when, when the file IO operation completes, you decrement the parallelism again, and what happens", "tokens": [51488, 562, 11, 562, 264, 3991, 39839, 6916, 36362, 11, 291, 6853, 518, 264, 8952, 1434, 797, 11, 293, 437, 2314, 51804], "temperature": 0.0, "avg_logprob": -0.14000631679188122, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.022654613479971886}, {"id": 297, "seek": 160448, "start": 1604.48, "end": 1610.48, "text": " is, is then is that the, the, the number of worker threads that are available reverts back to where", "tokens": [50364, 307, 11, 307, 550, 307, 300, 264, 11, 264, 11, 264, 1230, 295, 11346, 19314, 300, 366, 2435, 18438, 1373, 646, 281, 689, 50664], "temperature": 0.0, "avg_logprob": -0.1520078240371332, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.0008488727617077529}, {"id": 298, "seek": 160448, "start": 1610.48, "end": 1616.0, "text": " it actually was. These additional extra worker threads that might get created for these, they", "tokens": [50664, 309, 767, 390, 13, 1981, 4497, 2857, 11346, 19314, 300, 1062, 483, 2942, 337, 613, 11, 436, 50940], "temperature": 0.0, "avg_logprob": -0.1520078240371332, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.0008488727617077529}, {"id": 299, "seek": 160448, "start": 1616.0, "end": 1619.76, "text": " might hang around for a little while, but they will eventually, they, they, they, they, they, they", "tokens": [50940, 1062, 3967, 926, 337, 257, 707, 1339, 11, 457, 436, 486, 4728, 11, 436, 11, 436, 11, 436, 11, 436, 11, 436, 11, 436, 51128], "temperature": 0.0, "avg_logprob": -0.1520078240371332, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.0008488727617077529}, {"id": 300, "seek": 160448, "start": 1619.76, "end": 1625.04, "text": " will eventually terminate once the system is acquiescent. So same thing, what happens when we", "tokens": [51128, 486, 4728, 10761, 473, 1564, 264, 1185, 307, 6667, 530, 2207, 13, 407, 912, 551, 11, 437, 2314, 562, 321, 51392], "temperature": 0.0, "avg_logprob": -0.1520078240371332, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.0008488727617077529}, {"id": 301, "seek": 160448, "start": 1625.04, "end": 1630.72, "text": " had to, to do a file IO, or sorry, we do a read, same kind of thing, increase the parallelism,", "tokens": [51392, 632, 281, 11, 281, 360, 257, 3991, 39839, 11, 420, 2597, 11, 321, 360, 257, 1401, 11, 912, 733, 295, 551, 11, 3488, 264, 8952, 1434, 11, 51676], "temperature": 0.0, "avg_logprob": -0.1520078240371332, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.0008488727617077529}, {"id": 302, "seek": 163072, "start": 1630.72, "end": 1637.84, "text": " and we do our read, and the once, once the read is actually complete, we will decrement the", "tokens": [50364, 293, 321, 360, 527, 1401, 11, 293, 264, 1564, 11, 1564, 264, 1401, 307, 767, 3566, 11, 321, 486, 6853, 518, 264, 50720], "temperature": 0.0, "avg_logprob": -0.13261094614237295, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.0008636231068521738}, {"id": 303, "seek": 163072, "start": 1637.84, "end": 1642.96, "text": " parallelism again. So this is all kind of lame, and you say, well, why haven't we done any better", "tokens": [50720, 8952, 1434, 797, 13, 407, 341, 307, 439, 733, 295, 27635, 11, 293, 291, 584, 11, 731, 11, 983, 2378, 380, 321, 1096, 604, 1101, 50976], "temperature": 0.0, "avg_logprob": -0.13261094614237295, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.0008636231068521738}, {"id": 304, "seek": 163072, "start": 1642.96, "end": 1646.0, "text": " on this? So this is one of the things that we actually have been playing around with for, for", "tokens": [50976, 322, 341, 30, 407, 341, 307, 472, 295, 264, 721, 300, 321, 767, 362, 668, 2433, 926, 365, 337, 11, 337, 51128], "temperature": 0.0, "avg_logprob": -0.13261094614237295, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.0008636231068521738}, {"id": 305, "seek": 163072, "start": 1646.0, "end": 1650.48, "text": " quite some time. There are asynchronous IO interfaces on different operating systems.", "tokens": [51128, 1596, 512, 565, 13, 821, 366, 49174, 39839, 28416, 322, 819, 7447, 3652, 13, 51352], "temperature": 0.0, "avg_logprob": -0.13261094614237295, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.0008636231068521738}, {"id": 306, "seek": 163072, "start": 1650.48, "end": 1656.16, "text": " I'll just talk about what, for IO, you ring today. I'm just talking about in the context of file,", "tokens": [51352, 286, 603, 445, 751, 466, 437, 11, 337, 39839, 11, 291, 4875, 965, 13, 286, 478, 445, 1417, 466, 294, 264, 4319, 295, 3991, 11, 51636], "temperature": 0.0, "avg_logprob": -0.13261094614237295, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.0008636231068521738}, {"id": 307, "seek": 165616, "start": 1656.16, "end": 1662.16, "text": " we've also looked at it in the context of, of, of, of Socus as well. So in the Linux operating", "tokens": [50364, 321, 600, 611, 2956, 412, 309, 294, 264, 4319, 295, 11, 295, 11, 295, 11, 295, 407, 1149, 382, 731, 13, 407, 294, 264, 18734, 7447, 50664], "temperature": 0.0, "avg_logprob": -0.18999645835474918, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.012462967075407505}, {"id": 308, "seek": 165616, "start": 1662.16, "end": 1669.0400000000002, "text": " system, there is, there's, there's a completely different type of, of interface to the operating", "tokens": [50664, 1185, 11, 456, 307, 11, 456, 311, 11, 456, 311, 257, 2584, 819, 2010, 295, 11, 295, 9226, 281, 264, 7447, 51008], "temperature": 0.0, "avg_logprob": -0.18999645835474918, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.012462967075407505}, {"id": 309, "seek": 165616, "start": 1669.0400000000002, "end": 1674.4, "text": " system, which is, which is supports asynchronous IO operations, is essentially, essentially", "tokens": [51008, 1185, 11, 597, 307, 11, 597, 307, 9346, 49174, 39839, 7705, 11, 307, 4476, 11, 4476, 51276], "temperature": 0.0, "avg_logprob": -0.18999645835474918, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.012462967075407505}, {"id": 310, "seek": 165616, "start": 1674.4, "end": 1680.5600000000002, "text": " allows the trap into the kernel. And so you've actually, you, you, you can actually queue up", "tokens": [51276, 4045, 264, 11487, 666, 264, 28256, 13, 400, 370, 291, 600, 767, 11, 291, 11, 291, 11, 291, 393, 767, 18639, 493, 51584], "temperature": 0.0, "avg_logprob": -0.18999645835474918, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.012462967075407505}, {"id": 311, "seek": 168056, "start": 1681.28, "end": 1685.76, "text": " submissions in, in, in, in memory, and any events that are associated with those when", "tokens": [50400, 40429, 294, 11, 294, 11, 294, 11, 294, 4675, 11, 293, 604, 3931, 300, 366, 6615, 365, 729, 562, 50624], "temperature": 0.0, "avg_logprob": -0.15460326583297165, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.0604495033621788}, {"id": 312, "seek": 168056, "start": 1685.76, "end": 1689.76, "text": " they're complete are actually queued up in a ring. That's also, so it's successful from both the,", "tokens": [50624, 436, 434, 3566, 366, 767, 631, 5827, 493, 294, 257, 4875, 13, 663, 311, 611, 11, 370, 309, 311, 4406, 490, 1293, 264, 11, 50824], "temperature": 0.0, "avg_logprob": -0.15460326583297165, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.0604495033621788}, {"id": 313, "seek": 168056, "start": 1689.76, "end": 1697.44, "text": " the kernel and, and, and user space. So there's, there's a lot of issues and a lot of complications", "tokens": [50824, 264, 28256, 293, 11, 293, 11, 293, 4195, 1901, 13, 407, 456, 311, 11, 456, 311, 257, 688, 295, 2663, 293, 257, 688, 295, 26566, 51208], "temperature": 0.0, "avg_logprob": -0.15460326583297165, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.0604495033621788}, {"id": 314, "seek": 168056, "start": 1697.44, "end": 1703.6799999999998, "text": " trying to interface this to the, to, to, to the JDK to be able to support a lot of the, the, the", "tokens": [51208, 1382, 281, 9226, 341, 281, 264, 11, 281, 11, 281, 11, 281, 264, 37082, 42, 281, 312, 1075, 281, 1406, 257, 688, 295, 264, 11, 264, 11, 264, 51520], "temperature": 0.0, "avg_logprob": -0.15460326583297165, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.0604495033621788}, {"id": 315, "seek": 168056, "start": 1703.6799999999998, "end": 1709.04, "text": " libraries. So what we have been doing, and this is in the, in the open JDK sandboxes, we have an,", "tokens": [51520, 15148, 13, 407, 437, 321, 362, 668, 884, 11, 293, 341, 307, 294, 264, 11, 294, 264, 1269, 37082, 42, 42115, 279, 11, 321, 362, 364, 11, 51788], "temperature": 0.0, "avg_logprob": -0.15460326583297165, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.0604495033621788}, {"id": 316, "seek": 170904, "start": 1709.12, "end": 1714.6399999999999, "text": " we have a low level API, which sits on top of the work that Maurizio was talking about in the", "tokens": [50368, 321, 362, 257, 2295, 1496, 9362, 11, 597, 12696, 322, 1192, 295, 264, 589, 300, 26133, 590, 1004, 390, 1417, 466, 294, 264, 50644], "temperature": 0.0, "avg_logprob": -0.14698944091796876, "compression_ratio": 1.751131221719457, "no_speech_prob": 0.011484213173389435}, {"id": 317, "seek": 170904, "start": 1714.6399999999999, "end": 1722.24, "text": " previous slide, a previous presentation with an FFM. And that will provide the lower level access", "tokens": [50644, 3894, 4137, 11, 257, 3894, 5860, 365, 364, 479, 37, 44, 13, 400, 300, 486, 2893, 264, 3126, 1496, 2105, 51024], "temperature": 0.0, "avg_logprob": -0.14698944091796876, "compression_ratio": 1.751131221719457, "no_speech_prob": 0.011484213173389435}, {"id": 318, "seek": 170904, "start": 1722.24, "end": 1728.08, "text": " to the, the, the submission and the, the, the, the, the completion rings. And it, it, it, it, it,", "tokens": [51024, 281, 264, 11, 264, 11, 264, 23689, 293, 264, 11, 264, 11, 264, 11, 264, 11, 264, 19372, 11136, 13, 400, 309, 11, 309, 11, 309, 11, 309, 11, 309, 11, 51316], "temperature": 0.0, "avg_logprob": -0.14698944091796876, "compression_ratio": 1.751131221719457, "no_speech_prob": 0.011484213173389435}, {"id": 319, "seek": 170904, "start": 1728.08, "end": 1733.2, "text": " it hides a lot of that, that, that, that kind of detail. Now, in order to be able to work through", "tokens": [51316, 309, 35953, 257, 688, 295, 300, 11, 300, 11, 300, 11, 300, 733, 295, 2607, 13, 823, 11, 294, 1668, 281, 312, 1075, 281, 589, 807, 51572], "temperature": 0.0, "avg_logprob": -0.14698944091796876, "compression_ratio": 1.751131221719457, "no_speech_prob": 0.011484213173389435}, {"id": 320, "seek": 173320, "start": 1733.28, "end": 1739.28, "text": " some virtual threads, we actually have to do, we have to do a bit more replacing of foundations", "tokens": [50368, 512, 6374, 19314, 11, 321, 767, 362, 281, 360, 11, 321, 362, 281, 360, 257, 857, 544, 19139, 295, 22467, 50668], "temperature": 0.0, "avg_logprob": -0.12238583602304534, "compression_ratio": 1.75, "no_speech_prob": 0.17034105956554413}, {"id": 321, "seek": 173320, "start": 1739.28, "end": 1742.8, "text": " around the place. So there's a lot of prototype re-implementation of a lot of the Java, Java", "tokens": [50668, 926, 264, 1081, 13, 407, 456, 311, 257, 688, 295, 19475, 319, 12, 332, 781, 19631, 295, 257, 688, 295, 264, 10745, 11, 10745, 50844], "temperature": 0.0, "avg_logprob": -0.12238583602304534, "compression_ratio": 1.75, "no_speech_prob": 0.17034105956554413}, {"id": 322, "seek": 173320, "start": 1742.8, "end": 1747.8400000000001, "text": " I.O. classes. And to be able to work with this as well. And that actually allows the, the problem", "tokens": [50844, 286, 13, 46, 13, 5359, 13, 400, 281, 312, 1075, 281, 589, 365, 341, 382, 731, 13, 400, 300, 767, 4045, 264, 11, 264, 1154, 51096], "temperature": 0.0, "avg_logprob": -0.12238583602304534, "compression_ratio": 1.75, "no_speech_prob": 0.17034105956554413}, {"id": 323, "seek": 173320, "start": 1747.8400000000001, "end": 1753.52, "text": " of file I.O. to be actually reduced down very significantly. None of this, none of the completed", "tokens": [51096, 295, 3991, 286, 13, 46, 13, 281, 312, 767, 9212, 760, 588, 10591, 13, 14492, 295, 341, 11, 6022, 295, 264, 7365, 51380], "temperature": 0.0, "avg_logprob": -0.12238583602304534, "compression_ratio": 1.75, "no_speech_prob": 0.17034105956554413}, {"id": 324, "seek": 173320, "start": 1753.52, "end": 1757.68, "text": " bits for this are in the Loom repo yet, but we will, we will get there eventually. There's a", "tokens": [51380, 9239, 337, 341, 366, 294, 264, 6130, 298, 49040, 1939, 11, 457, 321, 486, 11, 321, 486, 483, 456, 4728, 13, 821, 311, 257, 51588], "temperature": 0.0, "avg_logprob": -0.12238583602304534, "compression_ratio": 1.75, "no_speech_prob": 0.17034105956554413}, {"id": 325, "seek": 175768, "start": 1757.76, "end": 1763.2, "text": " bunch of design choices that, that have to be worked out when you're actually interfacing with", "tokens": [50368, 3840, 295, 1715, 7994, 300, 11, 300, 362, 281, 312, 2732, 484, 562, 291, 434, 767, 14510, 5615, 365, 50640], "temperature": 0.0, "avg_logprob": -0.12480031981948735, "compression_ratio": 1.8136882129277567, "no_speech_prob": 0.08942599594593048}, {"id": 326, "seek": 175768, "start": 1763.2, "end": 1769.04, "text": " something like I.O. U-ring because as to which threads can actually access the, the, it's, I.O.", "tokens": [50640, 746, 411, 286, 13, 46, 13, 624, 12, 2937, 570, 382, 281, 597, 19314, 393, 767, 2105, 264, 11, 264, 11, 309, 311, 11, 286, 13, 46, 13, 50932], "temperature": 0.0, "avg_logprob": -0.12480031981948735, "compression_ratio": 1.8136882129277567, "no_speech_prob": 0.08942599594593048}, {"id": 327, "seek": 175768, "start": 1769.04, "end": 1774.0, "text": " U-ring is not kind of designed for, for multiple threads to be, to be accessing a ring at the", "tokens": [50932, 624, 12, 2937, 307, 406, 733, 295, 4761, 337, 11, 337, 3866, 19314, 281, 312, 11, 281, 312, 26440, 257, 4875, 412, 264, 51180], "temperature": 0.0, "avg_logprob": -0.12480031981948735, "compression_ratio": 1.8136882129277567, "no_speech_prob": 0.08942599594593048}, {"id": 328, "seek": 175768, "start": 1774.0, "end": 1780.5600000000002, "text": " same time. So what we will end up doing is, is, is, is, is, is, is having essentially multiple", "tokens": [51180, 912, 565, 13, 407, 437, 321, 486, 917, 493, 884, 307, 11, 307, 11, 307, 11, 307, 11, 307, 11, 307, 11, 307, 1419, 4476, 3866, 51508], "temperature": 0.0, "avg_logprob": -0.12480031981948735, "compression_ratio": 1.8136882129277567, "no_speech_prob": 0.08942599594593048}, {"id": 329, "seek": 175768, "start": 1780.5600000000002, "end": 1785.92, "text": " I.O. ring instances and one per, per carrier essentially. And that actually fixes the completion,", "tokens": [51508, 286, 13, 46, 13, 4875, 14519, 293, 472, 680, 11, 680, 17574, 4476, 13, 400, 300, 767, 32539, 264, 19372, 11, 51776], "temperature": 0.0, "avg_logprob": -0.12480031981948735, "compression_ratio": 1.8136882129277567, "no_speech_prob": 0.08942599594593048}, {"id": 330, "seek": 178592, "start": 1785.92, "end": 1790.16, "text": " the, the submission side. The completion side is a little bit more complicated and there's a", "tokens": [50364, 264, 11, 264, 23689, 1252, 13, 440, 19372, 1252, 307, 257, 707, 857, 544, 6179, 293, 456, 311, 257, 50576], "temperature": 0.0, "avg_logprob": -0.11606740576075757, "compression_ratio": 1.8270676691729324, "no_speech_prob": 0.0016333286184817553}, {"id": 331, "seek": 178592, "start": 1790.16, "end": 1794.88, "text": " number of design choices around that. So the main message here is, is that there's a lot going on", "tokens": [50576, 1230, 295, 1715, 7994, 926, 300, 13, 407, 264, 2135, 3636, 510, 307, 11, 307, 300, 456, 311, 257, 688, 516, 322, 50812], "temperature": 0.0, "avg_logprob": -0.11606740576075757, "compression_ratio": 1.8270676691729324, "no_speech_prob": 0.0016333286184817553}, {"id": 332, "seek": 178592, "start": 1794.88, "end": 1800.5600000000002, "text": " in this area as well because all of that areas of the libraries have to, have, have to, have to play", "tokens": [50812, 294, 341, 1859, 382, 731, 570, 439, 295, 300, 3179, 295, 264, 15148, 362, 281, 11, 362, 11, 362, 281, 11, 362, 281, 862, 51096], "temperature": 0.0, "avg_logprob": -0.11606740576075757, "compression_ratio": 1.8270676691729324, "no_speech_prob": 0.0016333286184817553}, {"id": 333, "seek": 178592, "start": 1800.5600000000002, "end": 1805.6000000000001, "text": " cleanly with them, with virtual threads as well. Okay, so I'm not going to go through all of the", "tokens": [51096, 2541, 356, 365, 552, 11, 365, 6374, 19314, 382, 731, 13, 1033, 11, 370, 286, 478, 406, 516, 281, 352, 807, 439, 295, 264, 51348], "temperature": 0.0, "avg_logprob": -0.11606740576075757, "compression_ratio": 1.8270676691729324, "no_speech_prob": 0.0016333286184817553}, {"id": 334, "seek": 178592, "start": 1805.6000000000001, "end": 1809.44, "text": " other things that are going on, but I'll just talk, just, just, just, just mention a few of them.", "tokens": [51348, 661, 721, 300, 366, 516, 322, 11, 457, 286, 603, 445, 751, 11, 445, 11, 445, 11, 445, 11, 445, 2152, 257, 1326, 295, 552, 13, 51540], "temperature": 0.0, "avg_logprob": -0.11606740576075757, "compression_ratio": 1.8270676691729324, "no_speech_prob": 0.0016333286184817553}, {"id": 335, "seek": 180944, "start": 1809.44, "end": 1817.1200000000001, "text": " So Professor Doug Lee who's the, the, the sort of the, the, the world expert on concurrency is", "tokens": [50364, 407, 8419, 12742, 6957, 567, 311, 264, 11, 264, 11, 264, 1333, 295, 264, 11, 264, 11, 264, 1002, 5844, 322, 23702, 10457, 307, 50748], "temperature": 0.6, "avg_logprob": -0.26129998279218913, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.006668788846582174}, {"id": 336, "seek": 180944, "start": 1817.1200000000001, "end": 1822.88, "text": " doing quite a bit of work at the, exploration into fork joint pool at the moment. That is", "tokens": [50748, 884, 1596, 257, 857, 295, 589, 412, 264, 11, 16197, 666, 17716, 7225, 7005, 412, 264, 1623, 13, 663, 307, 51036], "temperature": 0.6, "avg_logprob": -0.26129998279218913, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.006668788846582174}, {"id": 337, "seek": 180944, "start": 1822.88, "end": 1828.88, "text": " for scenarios where you have a smaller number of cores, because a lot of, a lot of container", "tokens": [51036, 337, 15077, 689, 291, 362, 257, 4356, 1230, 295, 24826, 11, 570, 257, 688, 295, 11, 257, 688, 295, 10129, 51336], "temperature": 0.6, "avg_logprob": -0.26129998279218913, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.006668788846582174}, {"id": 338, "seek": 180944, "start": 1828.88, "end": 1833.3600000000001, "text": " and, and, and the cloudy type systems you're running with two or you're running with four cores.", "tokens": [51336, 293, 11, 293, 11, 293, 264, 33060, 2010, 3652, 291, 434, 2614, 365, 732, 420, 291, 434, 2614, 365, 1451, 24826, 13, 51560], "temperature": 0.6, "avg_logprob": -0.26129998279218913, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.006668788846582174}, {"id": 339, "seek": 180944, "start": 1833.3600000000001, "end": 1839.2, "text": " And we've, we've observed in many scenarios where you get underutilization. And a lot of", "tokens": [51560, 400, 321, 600, 11, 321, 600, 13095, 294, 867, 15077, 689, 291, 483, 833, 20835, 2144, 13, 400, 257, 688, 295, 51852], "temperature": 0.6, "avg_logprob": -0.26129998279218913, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.006668788846582174}, {"id": 340, "seek": 183920, "start": 1839.2, "end": 1846.02, "text": " of that relates to just the time it actually takes to work for, well worker", "tokens": [50364, 295, 300, 16155, 281, 445, 264, 565, 309, 767, 2516, 281, 589, 337, 11, 731, 11346, 50705], "temperature": 0.0, "avg_logprob": -0.265906467616001, "compression_ratio": 1.7732793522267207, "no_speech_prob": 0.3976685404777527}, {"id": 341, "seek": 183920, "start": 1846.02, "end": 1849.02, "text": " threads are parking and then they have to be unparked in order to actually to", "tokens": [50705, 19314, 366, 9893, 293, 550, 436, 362, 281, 312, 517, 31239, 292, 294, 1668, 281, 767, 281, 50855], "temperature": 0.0, "avg_logprob": -0.265906467616001, "compression_ratio": 1.7732793522267207, "no_speech_prob": 0.3976685404777527}, {"id": 342, "seek": 183920, "start": 1849.02, "end": 1853.42, "text": " actually to do work. So he's exploring a number of things on that and", "tokens": [50855, 767, 281, 360, 589, 13, 407, 415, 311, 12736, 257, 1230, 295, 721, 322, 300, 293, 51075], "temperature": 0.0, "avg_logprob": -0.265906467616001, "compression_ratio": 1.7732793522267207, "no_speech_prob": 0.3976685404777527}, {"id": 343, "seek": 183920, "start": 1853.42, "end": 1855.8600000000001, "text": " we're trying to come up with good benchmarks to be able to actually to", "tokens": [51075, 321, 434, 1382, 281, 808, 493, 365, 665, 43751, 281, 312, 1075, 281, 767, 281, 51197], "temperature": 0.0, "avg_logprob": -0.265906467616001, "compression_ratio": 1.7732793522267207, "no_speech_prob": 0.3976685404777527}, {"id": 344, "seek": 183920, "start": 1855.8600000000001, "end": 1861.22, "text": " measure these kind of things but this should, if this turns out to be", "tokens": [51197, 3481, 613, 733, 295, 721, 457, 341, 820, 11, 498, 341, 4523, 484, 281, 312, 51465], "temperature": 0.0, "avg_logprob": -0.265906467616001, "compression_ratio": 1.7732793522267207, "no_speech_prob": 0.3976685404777527}, {"id": 345, "seek": 183920, "start": 1861.22, "end": 1866.06, "text": " profitable then it'll help some of these scenarios where we appear not to", "tokens": [51465, 21608, 550, 309, 603, 854, 512, 295, 613, 15077, 689, 321, 4204, 406, 281, 51707], "temperature": 0.0, "avg_logprob": -0.265906467616001, "compression_ratio": 1.7732793522267207, "no_speech_prob": 0.3976685404777527}, {"id": 346, "seek": 186606, "start": 1866.1, "end": 1872.78, "text": " be having full utilization on smaller systems. JVMTI makes me scream.", "tokens": [50366, 312, 1419, 1577, 37074, 322, 4356, 3652, 13, 508, 53, 44, 5422, 1669, 385, 7291, 13, 50700], "temperature": 0.0, "avg_logprob": -0.24182228303291428, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.018213016912341118}, {"id": 347, "seek": 186606, "start": 1872.78, "end": 1882.54, "text": " It's a very very invasive API and it's very much challenged by", "tokens": [50700, 467, 311, 257, 588, 588, 30894, 9362, 293, 309, 311, 588, 709, 17737, 538, 51188], "temperature": 0.0, "avg_logprob": -0.24182228303291428, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.018213016912341118}, {"id": 348, "seek": 186606, "start": 1882.54, "end": 1887.58, "text": " features in the Java platform where you move them out of the VM into Java.", "tokens": [51188, 4122, 294, 264, 10745, 3663, 689, 291, 1286, 552, 484, 295, 264, 18038, 666, 10745, 13, 51440], "temperature": 0.0, "avg_logprob": -0.24182228303291428, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.018213016912341118}, {"id": 349, "seek": 186606, "start": 1887.58, "end": 1891.06, "text": " So having a native tool interface where a lot of the runtime is actually in", "tokens": [51440, 407, 1419, 257, 8470, 2290, 9226, 689, 257, 688, 295, 264, 34474, 307, 767, 294, 51614], "temperature": 0.0, "avg_logprob": -0.24182228303291428, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.018213016912341118}, {"id": 350, "seek": 189106, "start": 1891.3799999999999, "end": 1898.3, "text": " Java rather than the VM is a challenge. We have a very good story for JVMTI and", "tokens": [50380, 10745, 2831, 813, 264, 18038, 307, 257, 3430, 13, 492, 362, 257, 588, 665, 1657, 337, 508, 53, 44, 5422, 293, 50726], "temperature": 0.0, "avg_logprob": -0.1883431047496229, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.013039047829806805}, {"id": 351, "seek": 189106, "start": 1898.3, "end": 1903.86, "text": " virtual threads and debugging and for many other types of tool agents but it's", "tokens": [50726, 6374, 19314, 293, 45592, 293, 337, 867, 661, 3467, 295, 2290, 12554, 457, 309, 311, 51004], "temperature": 0.0, "avg_logprob": -0.1883431047496229, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.013039047829806805}, {"id": 352, "seek": 189106, "start": 1903.86, "end": 1910.1, "text": " not working well for profiler type tools that want to use JVMTI and because", "tokens": [51004, 406, 1364, 731, 337, 1740, 5441, 2010, 3873, 300, 528, 281, 764, 508, 53, 44, 5422, 293, 570, 51316], "temperature": 0.0, "avg_logprob": -0.1883431047496229, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.013039047829806805}, {"id": 353, "seek": 189106, "start": 1910.1, "end": 1913.74, "text": " there's a there's it's having to coordinate with a lot of code that's", "tokens": [51316, 456, 311, 257, 456, 311, 309, 311, 1419, 281, 15670, 365, 257, 688, 295, 3089, 300, 311, 51498], "temperature": 0.0, "avg_logprob": -0.1883431047496229, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.013039047829806805}, {"id": 354, "seek": 189106, "start": 1913.74, "end": 1916.8999999999999, "text": " actually executing in Java. So there's a lot of work going on there to try to", "tokens": [51498, 767, 32368, 294, 10745, 13, 407, 456, 311, 257, 688, 295, 589, 516, 322, 456, 281, 853, 281, 51656], "temperature": 0.0, "avg_logprob": -0.1883431047496229, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.013039047829806805}, {"id": 355, "seek": 191690, "start": 1916.94, "end": 1923.38, "text": " solve some of the problems and it's one where there's been some progress made", "tokens": [50366, 5039, 512, 295, 264, 2740, 293, 309, 311, 472, 689, 456, 311, 668, 512, 4205, 1027, 50688], "temperature": 0.0, "avg_logprob": -0.19788163503011066, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.02591957524418831}, {"id": 356, "seek": 191690, "start": 1923.38, "end": 1927.5800000000002, "text": " but it's one that's going to take more time. And there's other efforts that I'm", "tokens": [50688, 457, 309, 311, 472, 300, 311, 516, 281, 747, 544, 565, 13, 400, 456, 311, 661, 6484, 300, 286, 478, 50898], "temperature": 0.0, "avg_logprob": -0.19788163503011066, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.02591957524418831}, {"id": 357, "seek": 191690, "start": 1927.5800000000002, "end": 1932.8200000000002, "text": " not going to talk about today which is about scope values which is essentially", "tokens": [50898, 406, 516, 281, 751, 466, 965, 597, 307, 466, 11923, 4190, 597, 307, 4476, 51160], "temperature": 0.0, "avg_logprob": -0.19788163503011066, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.02591957524418831}, {"id": 358, "seek": 191690, "start": 1932.8200000000002, "end": 1939.9, "text": " allows us to communicate something to a remote callee without having parameters", "tokens": [51160, 4045, 505, 281, 7890, 746, 281, 257, 8607, 818, 1653, 1553, 1419, 9834, 51514], "temperature": 0.0, "avg_logprob": -0.19788163503011066, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.02591957524418831}, {"id": 359, "seek": 191690, "start": 1939.9, "end": 1943.24, "text": " in the call frames. Andrew Haley is actually leading that effort in Project", "tokens": [51514, 294, 264, 818, 12083, 13, 10110, 389, 29172, 307, 767, 5775, 300, 4630, 294, 9849, 51681], "temperature": 0.0, "avg_logprob": -0.19788163503011066, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.02591957524418831}, {"id": 360, "seek": 194324, "start": 1943.28, "end": 1948.1200000000001, "text": " Loom. Then we have the other big area which is structured concurrency which is", "tokens": [50366, 6130, 298, 13, 1396, 321, 362, 264, 661, 955, 1859, 597, 307, 18519, 23702, 10457, 597, 307, 50608], "temperature": 0.0, "avg_logprob": -0.2662361893698434, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.0845237523317337}, {"id": 361, "seek": 194324, "start": 1948.1200000000001, "end": 1956.08, "text": " all about being able to coordinate multiple threads that are decomposed", "tokens": [50608, 439, 466, 885, 1075, 281, 15670, 3866, 19314, 300, 366, 22867, 1744, 51006], "temperature": 0.0, "avg_logprob": -0.2662361893698434, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.0845237523317337}, {"id": 362, "seek": 194324, "start": 1956.08, "end": 1961.16, "text": " running some operation is be able to actually to deal with them as a single", "tokens": [51006, 2614, 512, 6916, 307, 312, 1075, 281, 767, 281, 2028, 365, 552, 382, 257, 2167, 51260], "temperature": 0.0, "avg_logprob": -0.2662361893698434, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.0845237523317337}, {"id": 363, "seek": 194324, "start": 1961.16, "end": 1963.92, "text": " unit of work. So there's a lot of interesting things going on there that", "tokens": [51260, 4985, 295, 589, 13, 407, 456, 311, 257, 688, 295, 1880, 721, 516, 322, 456, 300, 51398], "temperature": 0.0, "avg_logprob": -0.2662361893698434, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.0845237523317337}, {"id": 364, "seek": 194324, "start": 1963.92, "end": 1969.28, "text": " API is currently in preview and we will have to do another preview of this for", "tokens": [51398, 9362, 307, 4362, 294, 14281, 293, 321, 486, 362, 281, 360, 1071, 14281, 295, 341, 337, 51666], "temperature": 0.0, "avg_logprob": -0.2662361893698434, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.0845237523317337}, {"id": 365, "seek": 194324, "start": 1969.28, "end": 1973.0, "text": " the next release. So these are other kind of efforts that are actually going on in", "tokens": [51666, 264, 958, 4374, 13, 407, 613, 366, 661, 733, 295, 6484, 300, 366, 767, 516, 322, 294, 51852], "temperature": 0.0, "avg_logprob": -0.2662361893698434, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.0845237523317337}, {"id": 366, "seek": 197300, "start": 1973.08, "end": 1977.28, "text": " this project at the moment. So that's kind of it. I think I've actually made it", "tokens": [50368, 341, 1716, 412, 264, 1623, 13, 407, 300, 311, 733, 295, 309, 13, 286, 519, 286, 600, 767, 1027, 309, 50578], "temperature": 0.0, "avg_logprob": -0.28122996258479294, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.0027485378086566925}, {"id": 367, "seek": 197300, "start": 1977.28, "end": 1981.52, "text": " in with few minutes to spare and this is sort of links to the current JEPs that", "tokens": [50578, 294, 365, 1326, 2077, 281, 13798, 293, 341, 307, 1333, 295, 6123, 281, 264, 2190, 508, 8929, 82, 300, 50790], "temperature": 0.0, "avg_logprob": -0.28122996258479294, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.0027485378086566925}, {"id": 368, "seek": 197300, "start": 1981.52, "end": 1985.16, "text": " we have, the repository. When I was talking about the work on the monitors and", "tokens": [50790, 321, 362, 11, 264, 25841, 13, 1133, 286, 390, 1417, 466, 264, 589, 322, 264, 26518, 293, 50972], "temperature": 0.0, "avg_logprob": -0.28122996258479294, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.0027485378086566925}, {"id": 369, "seek": 197300, "start": 1985.16, "end": 1988.88, "text": " some of the other changes around fork joint pool is they're accumulating in", "tokens": [50972, 512, 295, 264, 661, 2962, 926, 17716, 7225, 7005, 307, 436, 434, 12989, 12162, 294, 51158], "temperature": 0.0, "avg_logprob": -0.28122996258479294, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.0027485378086566925}, {"id": 370, "seek": 197300, "start": 1988.88, "end": 1993.6, "text": " the Loom repo now. And yeah, okay.", "tokens": [51158, 264, 6130, 298, 49040, 586, 13, 400, 1338, 11, 1392, 13, 51394], "temperature": 0.0, "avg_logprob": -0.28122996258479294, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.0027485378086566925}, {"id": 371, "seek": 200300, "start": 2003.0, "end": 2010.0, "text": " You'll have to hand out microphones.", "tokens": [50364, 509, 603, 362, 281, 1011, 484, 30495, 13, 50714], "temperature": 0.0, "avg_logprob": -0.9605751037597656, "compression_ratio": 0.8909090909090909, "no_speech_prob": 0.0849328488111496}, {"id": 372, "seek": 200300, "start": 2010.0, "end": 2013.0, "text": " Okay.", "tokens": [50714, 1033, 13, 50864], "temperature": 0.0, "avg_logprob": -0.9605751037597656, "compression_ratio": 0.8909090909090909, "no_speech_prob": 0.0849328488111496}, {"id": 373, "seek": 200300, "start": 2013.0, "end": 2016.0, "text": " Hello.", "tokens": [50864, 2425, 13, 51014], "temperature": 0.0, "avg_logprob": -0.9605751037597656, "compression_ratio": 0.8909090909090909, "no_speech_prob": 0.0849328488111496}, {"id": 374, "seek": 201600, "start": 2016.0, "end": 2026.0, "text": " Hello.", "tokens": [50364, 2425, 13, 50864], "temperature": 0.0, "avg_logprob": -0.618874979019165, "compression_ratio": 1.1071428571428572, "no_speech_prob": 0.03026576153934002}, {"id": 375, "seek": 201600, "start": 2026.0, "end": 2032.0, "text": " Hello.", "tokens": [50864, 2425, 13, 51164], "temperature": 0.0, "avg_logprob": -0.618874979019165, "compression_ratio": 1.1071428571428572, "no_speech_prob": 0.03026576153934002}, {"id": 376, "seek": 201600, "start": 2032.0, "end": 2038.0, "text": " Test. Yes.", "tokens": [51164, 9279, 13, 1079, 13, 51464], "temperature": 0.0, "avg_logprob": -0.618874979019165, "compression_ratio": 1.1071428571428572, "no_speech_prob": 0.03026576153934002}, {"id": 377, "seek": 201600, "start": 2038.0, "end": 2045.0, "text": " Hello.", "tokens": [51464, 2425, 13, 51814], "temperature": 0.0, "avg_logprob": -0.618874979019165, "compression_ratio": 1.1071428571428572, "no_speech_prob": 0.03026576153934002}, {"id": 378, "seek": 204500, "start": 2045.0, "end": 2059.0, "text": " I have to take questions here first. I think.", "tokens": [50364, 286, 362, 281, 747, 1651, 510, 700, 13, 286, 519, 13, 51064], "temperature": 0.0, "avg_logprob": -0.24065671247594497, "compression_ratio": 1.103448275862069, "no_speech_prob": 0.011239989660680294}, {"id": 379, "seek": 204500, "start": 2059.0, "end": 2064.0, "text": " I had a question on, can you hear me at all?", "tokens": [51064, 286, 632, 257, 1168, 322, 11, 393, 291, 1568, 385, 412, 439, 30, 51314], "temperature": 0.0, "avg_logprob": -0.24065671247594497, "compression_ratio": 1.103448275862069, "no_speech_prob": 0.011239989660680294}, {"id": 380, "seek": 204500, "start": 2064.0, "end": 2069.0, "text": " Okay.", "tokens": [51314, 1033, 13, 51564], "temperature": 0.0, "avg_logprob": -0.24065671247594497, "compression_ratio": 1.103448275862069, "no_speech_prob": 0.011239989660680294}, {"id": 381, "seek": 206900, "start": 2069.0, "end": 2075.0, "text": " So you mentioned on the, from network IO, you said you had a good solution, right?", "tokens": [50364, 407, 291, 2835, 322, 264, 11, 490, 3209, 39839, 11, 291, 848, 291, 632, 257, 665, 3827, 11, 558, 30, 50664], "temperature": 0.0, "avg_logprob": -0.3276548190992706, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.08050530403852463}, {"id": 382, "seek": 206900, "start": 2075.0, "end": 2080.0, "text": " And then you said for file IO things are much trickier.", "tokens": [50664, 400, 550, 291, 848, 337, 3991, 39839, 721, 366, 709, 4282, 811, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3276548190992706, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.08050530403852463}, {"id": 383, "seek": 206900, "start": 2080.0, "end": 2082.0, "text": " But what?", "tokens": [50914, 583, 437, 30, 51014], "temperature": 0.0, "avg_logprob": -0.3276548190992706, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.08050530403852463}, {"id": 384, "seek": 206900, "start": 2082.0, "end": 2085.0, "text": " I'm sorry.", "tokens": [51014, 286, 478, 2597, 13, 51164], "temperature": 0.0, "avg_logprob": -0.3276548190992706, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.08050530403852463}, {"id": 385, "seek": 206900, "start": 2085.0, "end": 2090.0, "text": " I'm sorry.", "tokens": [51164, 286, 478, 2597, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3276548190992706, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.08050530403852463}, {"id": 386, "seek": 206900, "start": 2090.0, "end": 2091.0, "text": " Thank you.", "tokens": [51414, 1044, 291, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3276548190992706, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.08050530403852463}, {"id": 387, "seek": 206900, "start": 2091.0, "end": 2093.0, "text": " Yeah, I'm sorry. Would you mind repeating it?", "tokens": [51464, 865, 11, 286, 478, 2597, 13, 6068, 291, 1575, 18617, 309, 30, 51564], "temperature": 0.0, "avg_logprob": -0.3276548190992706, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.08050530403852463}, {"id": 388, "seek": 206900, "start": 2093.0, "end": 2098.0, "text": " Yes. So I was saying, so you mentioned network IO and you said, yeah, we have a", "tokens": [51564, 1079, 13, 407, 286, 390, 1566, 11, 370, 291, 2835, 3209, 39839, 293, 291, 848, 11, 1338, 11, 321, 362, 257, 51814], "temperature": 0.0, "avg_logprob": -0.3276548190992706, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.08050530403852463}, {"id": 389, "seek": 209800, "start": 2098.0, "end": 2103.0, "text": " pretty good solution here, but for file IO things are much trickier.", "tokens": [50364, 1238, 665, 3827, 510, 11, 457, 337, 3991, 39839, 721, 366, 709, 4282, 811, 13, 50614], "temperature": 0.0, "avg_logprob": -0.16773461395839476, "compression_ratio": 1.75, "no_speech_prob": 0.0648106187582016}, {"id": 390, "seek": 209800, "start": 2103.0, "end": 2107.0, "text": " But what's the fundamental issue that save a solution from network IO cannot be", "tokens": [50614, 583, 437, 311, 264, 8088, 2734, 300, 3155, 257, 3827, 490, 3209, 39839, 2644, 312, 50814], "temperature": 0.0, "avg_logprob": -0.16773461395839476, "compression_ratio": 1.75, "no_speech_prob": 0.0648106187582016}, {"id": 391, "seek": 209800, "start": 2107.0, "end": 2110.0, "text": " used for file IO too?", "tokens": [50814, 1143, 337, 3991, 39839, 886, 30, 50964], "temperature": 0.0, "avg_logprob": -0.16773461395839476, "compression_ratio": 1.75, "no_speech_prob": 0.0648106187582016}, {"id": 392, "seek": 209800, "start": 2110.0, "end": 2115.0, "text": " Okay. So the question is, is, is why is the solution for, why can't the solution", "tokens": [50964, 1033, 13, 407, 264, 1168, 307, 11, 307, 11, 307, 983, 307, 264, 3827, 337, 11, 983, 393, 380, 264, 3827, 51214], "temperature": 0.0, "avg_logprob": -0.16773461395839476, "compression_ratio": 1.75, "no_speech_prob": 0.0648106187582016}, {"id": 393, "seek": 209800, "start": 2115.0, "end": 2120.0, "text": " for network IO be used for file IO? And that is because there isn't the", "tokens": [51214, 337, 3209, 39839, 312, 1143, 337, 3991, 39839, 30, 400, 300, 307, 570, 456, 1943, 380, 264, 51464], "temperature": 0.0, "avg_logprob": -0.16773461395839476, "compression_ratio": 1.75, "no_speech_prob": 0.0648106187582016}, {"id": 394, "seek": 209800, "start": 2120.0, "end": 2127.0, "text": " equipment sort of readiness API that you get for file, for non-blocking IO.", "tokens": [51464, 5927, 1333, 295, 34954, 9362, 300, 291, 483, 337, 3991, 11, 337, 2107, 12, 28830, 278, 39839, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16773461395839476, "compression_ratio": 1.75, "no_speech_prob": 0.0648106187582016}, {"id": 395, "seek": 212700, "start": 2127.0, "end": 2133.0, "text": " So the reverse works, but not, what we're able to do at the moment is we're", "tokens": [50364, 407, 264, 9943, 1985, 11, 457, 406, 11, 437, 321, 434, 1075, 281, 360, 412, 264, 1623, 307, 321, 434, 50664], "temperature": 0.0, "avg_logprob": -0.1618515939423532, "compression_ratio": 1.4502923976608186, "no_speech_prob": 0.00723009929060936}, {"id": 396, "seek": 212700, "start": 2133.0, "end": 2141.0, "text": " able to actually to map onto multiple different 20 years of scalable IO", "tokens": [50664, 1075, 281, 767, 281, 4471, 3911, 3866, 819, 945, 924, 295, 38481, 39839, 51064], "temperature": 0.0, "avg_logprob": -0.1618515939423532, "compression_ratio": 1.4502923976608186, "no_speech_prob": 0.00723009929060936}, {"id": 397, "seek": 212700, "start": 2141.0, "end": 2146.0, "text": " mechanisms for networking IO. There isn't really the equivalent for file IO.", "tokens": [51064, 15902, 337, 17985, 39839, 13, 821, 1943, 380, 534, 264, 10344, 337, 3991, 39839, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1618515939423532, "compression_ratio": 1.4502923976608186, "no_speech_prob": 0.00723009929060936}, {"id": 398, "seek": 212700, "start": 2146.0, "end": 2152.0, "text": " Okay. I see. Thank you.", "tokens": [51314, 1033, 13, 286, 536, 13, 1044, 291, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1618515939423532, "compression_ratio": 1.4502923976608186, "no_speech_prob": 0.00723009929060936}, {"id": 399, "seek": 215200, "start": 2152.0, "end": 2157.0, "text": " So simple question. When, when you solve these problems and get a good implementation", "tokens": [50364, 407, 2199, 1168, 13, 1133, 11, 562, 291, 5039, 613, 2740, 293, 483, 257, 665, 11420, 50614], "temperature": 0.0, "avg_logprob": -0.1080153329031808, "compression_ratio": 1.6967213114754098, "no_speech_prob": 0.05130048096179962}, {"id": 400, "seek": 215200, "start": 2157.0, "end": 2161.0, "text": " and it's all in the next version of Java and you've, you've solved all of these", "tokens": [50614, 293, 309, 311, 439, 294, 264, 958, 3037, 295, 10745, 293, 291, 600, 11, 291, 600, 13041, 439, 295, 613, 50814], "temperature": 0.0, "avg_logprob": -0.1080153329031808, "compression_ratio": 1.6967213114754098, "no_speech_prob": 0.05130048096179962}, {"id": 401, "seek": 215200, "start": 2161.0, "end": 2166.0, "text": " problems, what do you think is going to be the impact? What's, what, what, what are you", "tokens": [50814, 2740, 11, 437, 360, 291, 519, 307, 516, 281, 312, 264, 2712, 30, 708, 311, 11, 437, 11, 437, 11, 437, 366, 291, 51064], "temperature": 0.0, "avg_logprob": -0.1080153329031808, "compression_ratio": 1.6967213114754098, "no_speech_prob": 0.05130048096179962}, {"id": 402, "seek": 215200, "start": 2166.0, "end": 2168.0, "text": " aiming for?", "tokens": [51064, 20253, 337, 30, 51164], "temperature": 0.0, "avg_logprob": -0.1080153329031808, "compression_ratio": 1.6967213114754098, "no_speech_prob": 0.05130048096179962}, {"id": 403, "seek": 215200, "start": 2168.0, "end": 2175.0, "text": " Okay. So, so, so the, the, the ultimate goal is this for Java developers to be", "tokens": [51164, 1033, 13, 407, 11, 370, 11, 370, 264, 11, 264, 11, 264, 9705, 3387, 307, 341, 337, 10745, 8849, 281, 312, 51514], "temperature": 0.0, "avg_logprob": -0.1080153329031808, "compression_ratio": 1.6967213114754098, "no_speech_prob": 0.05130048096179962}, {"id": 404, "seek": 215200, "start": 2175.0, "end": 2181.0, "text": " able to write code that, that reads exactly how it actually executes.", "tokens": [51514, 1075, 281, 2464, 3089, 300, 11, 300, 15700, 2293, 577, 309, 767, 4454, 1819, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1080153329031808, "compression_ratio": 1.6967213114754098, "no_speech_prob": 0.05130048096179962}, {"id": 405, "seek": 218100, "start": 2181.0, "end": 2187.0, "text": " So we need, we want to avoid having complicated, hard to, hard to read, hard to", "tokens": [50364, 407, 321, 643, 11, 321, 528, 281, 5042, 1419, 6179, 11, 1152, 281, 11, 1152, 281, 1401, 11, 1152, 281, 50664], "temperature": 0.0, "avg_logprob": -0.10441287102237824, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.009648335166275501}, {"id": 406, "seek": 218100, "start": 2187.0, "end": 2191.0, "text": " debug code that you get to, that the people are actually forced to write today", "tokens": [50664, 24083, 3089, 300, 291, 483, 281, 11, 300, 264, 561, 366, 767, 7579, 281, 2464, 965, 50864], "temperature": 0.0, "avg_logprob": -0.10441287102237824, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.009648335166275501}, {"id": 407, "seek": 218100, "start": 2191.0, "end": 2197.0, "text": " with a synchronous IO are, are, are reactive. That's sort of the, the ultimate", "tokens": [50864, 365, 257, 44743, 39839, 366, 11, 366, 11, 366, 28897, 13, 663, 311, 1333, 295, 264, 11, 264, 9705, 51164], "temperature": 0.0, "avg_logprob": -0.10441287102237824, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.009648335166275501}, {"id": 408, "seek": 218100, "start": 2197.0, "end": 2203.0, "text": " goal with this. So get the scalability with, with, with very obvious easy to read", "tokens": [51164, 3387, 365, 341, 13, 407, 483, 264, 15664, 2310, 365, 11, 365, 11, 365, 588, 6322, 1858, 281, 1401, 51464], "temperature": 0.0, "avg_logprob": -0.10441287102237824, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.009648335166275501}, {"id": 409, "seek": 218100, "start": 2203.0, "end": 2206.0, "text": " code. And at the same time as harmonious with all the other parts of the platform,", "tokens": [51464, 3089, 13, 400, 412, 264, 912, 565, 382, 14750, 851, 365, 439, 264, 661, 3166, 295, 264, 3663, 11, 51614], "temperature": 0.0, "avg_logprob": -0.10441287102237824, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.009648335166275501}, {"id": 410, "seek": 218100, "start": 2206.0, "end": 2210.0, "text": " such as debugging and profiling and so on. Because one of the things you actually", "tokens": [51614, 1270, 382, 45592, 293, 1740, 4883, 293, 370, 322, 13, 1436, 472, 295, 264, 721, 291, 767, 51814], "temperature": 0.0, "avg_logprob": -0.10441287102237824, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.009648335166275501}, {"id": 411, "seek": 221000, "start": 2210.0, "end": 2215.0, "text": " lose today when you start going down the async type route is you, you lose so much", "tokens": [50364, 3624, 965, 562, 291, 722, 516, 760, 264, 382, 34015, 2010, 7955, 307, 291, 11, 291, 3624, 370, 709, 50614], "temperature": 0.0, "avg_logprob": -0.15685944259166718, "compression_ratio": 1.878787878787879, "no_speech_prob": 0.013896041549742222}, {"id": 412, "seek": 221000, "start": 2215.0, "end": 2220.0, "text": " of the, of the, of the tooling, you lose your debugging, you lose your profiling.", "tokens": [50614, 295, 264, 11, 295, 264, 11, 295, 264, 46593, 11, 291, 3624, 428, 45592, 11, 291, 3624, 428, 1740, 4883, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15685944259166718, "compression_ratio": 1.878787878787879, "no_speech_prob": 0.013896041549742222}, {"id": 413, "seek": 221000, "start": 2220.0, "end": 2225.0, "text": " We want to bring all of that back so that everything just works. What we have today", "tokens": [50864, 492, 528, 281, 1565, 439, 295, 300, 646, 370, 300, 1203, 445, 1985, 13, 708, 321, 362, 965, 51114], "temperature": 0.0, "avg_logprob": -0.15685944259166718, "compression_ratio": 1.878787878787879, "no_speech_prob": 0.013896041549742222}, {"id": 414, "seek": 221000, "start": 2225.0, "end": 2231.0, "text": " in 21 is actually, is actually, is actually pretty good. You have a large range of", "tokens": [51114, 294, 5080, 307, 767, 11, 307, 767, 11, 307, 767, 1238, 665, 13, 509, 362, 257, 2416, 3613, 295, 51414], "temperature": 0.0, "avg_logprob": -0.15685944259166718, "compression_ratio": 1.878787878787879, "no_speech_prob": 0.013896041549742222}, {"id": 415, "seek": 221000, "start": 2231.0, "end": 2235.0, "text": " applications can actually be developed and, and scale very, very well. But there's,", "tokens": [51414, 5821, 393, 767, 312, 4743, 293, 11, 293, 4373, 588, 11, 588, 731, 13, 583, 456, 311, 11, 51614], "temperature": 0.0, "avg_logprob": -0.15685944259166718, "compression_ratio": 1.878787878787879, "no_speech_prob": 0.013896041549742222}, {"id": 416, "seek": 221000, "start": 2235.0, "end": 2238.0, "text": " there's, there's quite a lot of other amount of code that we want to get working", "tokens": [51614, 456, 311, 11, 456, 311, 1596, 257, 688, 295, 661, 2372, 295, 3089, 300, 321, 528, 281, 483, 1364, 51764], "temperature": 0.0, "avg_logprob": -0.15685944259166718, "compression_ratio": 1.878787878787879, "no_speech_prob": 0.013896041549742222}, {"id": 417, "seek": 223800, "start": 2238.0, "end": 2242.0, "text": " with, well with virtual threads too.", "tokens": [50364, 365, 11, 731, 365, 6374, 19314, 886, 13, 50564], "temperature": 0.0, "avg_logprob": -0.16032208818377872, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.015826484188437462}, {"id": 418, "seek": 223800, "start": 2247.0, "end": 2252.0, "text": " First, thank you so much. I think like suffering of millions will end with this", "tokens": [50814, 2386, 11, 1309, 291, 370, 709, 13, 286, 519, 411, 7755, 295, 6803, 486, 917, 365, 341, 51064], "temperature": 0.0, "avg_logprob": -0.16032208818377872, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.015826484188437462}, {"id": 419, "seek": 223800, "start": 2252.0, "end": 2258.0, "text": " holy light. My question is, when you look at the mitigation techniques that are", "tokens": [51064, 10622, 1442, 13, 1222, 1168, 307, 11, 562, 291, 574, 412, 264, 32649, 7512, 300, 366, 51364], "temperature": 0.0, "avg_logprob": -0.16032208818377872, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.015826484188437462}, {"id": 420, "seek": 223800, "start": 2258.0, "end": 2264.0, "text": " recommended against the shortcoming of monitors, one of, like the primary thing", "tokens": [51364, 9628, 1970, 264, 2099, 6590, 295, 26518, 11, 472, 295, 11, 411, 264, 6194, 551, 51664], "temperature": 0.0, "avg_logprob": -0.16032208818377872, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.015826484188437462}, {"id": 421, "seek": 226400, "start": 2264.0, "end": 2269.0, "text": " is you're just recommending people to replace those with locks. Just use a lock.", "tokens": [50364, 307, 291, 434, 445, 30559, 561, 281, 7406, 729, 365, 20703, 13, 1449, 764, 257, 4017, 13, 50614], "temperature": 0.0, "avg_logprob": -0.22738053865521868, "compression_ratio": 1.6420233463035019, "no_speech_prob": 0.008356768637895584}, {"id": 422, "seek": 226400, "start": 2269.0, "end": 2275.0, "text": " Okay. That's a, okay. That's a good observation. So in JEP 444, which is the, the, the", "tokens": [50614, 1033, 13, 663, 311, 257, 11, 1392, 13, 663, 311, 257, 665, 14816, 13, 407, 294, 508, 8929, 1017, 13912, 11, 597, 307, 264, 11, 264, 11, 264, 50914], "temperature": 0.0, "avg_logprob": -0.22738053865521868, "compression_ratio": 1.6420233463035019, "no_speech_prob": 0.008356768637895584}, {"id": 423, "seek": 226400, "start": 2275.0, "end": 2279.0, "text": " Java enhancement proposal that introduced virtual threads as a permanent feature is,", "tokens": [50914, 10745, 40776, 11494, 300, 7268, 6374, 19314, 382, 257, 10996, 4111, 307, 11, 51114], "temperature": 0.0, "avg_logprob": -0.22738053865521868, "compression_ratio": 1.6420233463035019, "no_speech_prob": 0.008356768637895584}, {"id": 424, "seek": 226400, "start": 2279.0, "end": 2285.0, "text": " it, it, it suggests that if you're running into issues with them, with, with pinning", "tokens": [51114, 309, 11, 309, 11, 309, 13409, 300, 498, 291, 434, 2614, 666, 2663, 365, 552, 11, 365, 11, 365, 5447, 773, 51414], "temperature": 0.0, "avg_logprob": -0.22738053865521868, "compression_ratio": 1.6420233463035019, "no_speech_prob": 0.008356768637895584}, {"id": 425, "seek": 226400, "start": 2285.0, "end": 2290.0, "text": " with object monitors, you can just replace them with Java till concurrent locks. And", "tokens": [51414, 365, 2657, 26518, 11, 291, 393, 445, 7406, 552, 365, 10745, 4288, 37702, 20703, 13, 400, 51664], "temperature": 0.0, "avg_logprob": -0.22738053865521868, "compression_ratio": 1.6420233463035019, "no_speech_prob": 0.008356768637895584}, {"id": 426, "seek": 229000, "start": 2290.0, "end": 2294.0, "text": " that was very much kind of short term, short term advice in order to actually take", "tokens": [50364, 300, 390, 588, 709, 733, 295, 2099, 1433, 11, 2099, 1433, 5192, 294, 1668, 281, 767, 747, 50564], "temperature": 0.0, "avg_logprob": -0.15211554027739024, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.006306593306362629}, {"id": 427, "seek": 229000, "start": 2294.0, "end": 2299.0, "text": " to avoid the quality of implementation issue. But we never, we never said we'd never", "tokens": [50564, 281, 5042, 264, 3125, 295, 11420, 2734, 13, 583, 321, 1128, 11, 321, 1128, 848, 321, 1116, 1128, 50814], "temperature": 0.0, "avg_logprob": -0.15211554027739024, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.006306593306362629}, {"id": 428, "seek": 229000, "start": 2299.0, "end": 2308.0, "text": " fix this problem. It was, it's, it's, it was, it was, it was a tactical decision to,", "tokens": [50814, 3191, 341, 1154, 13, 467, 390, 11, 309, 311, 11, 309, 311, 11, 309, 390, 11, 309, 390, 11, 309, 390, 257, 26323, 3537, 281, 11, 51264], "temperature": 0.0, "avg_logprob": -0.15211554027739024, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.006306593306362629}, {"id": 429, "seek": 229000, "start": 2308.0, "end": 2313.0, "text": " to make the feature permanent without addressing the, the monitors issue.", "tokens": [51264, 281, 652, 264, 4111, 10996, 1553, 14329, 264, 11, 264, 26518, 2734, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15211554027739024, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.006306593306362629}, {"id": 430, "seek": 229000, "start": 2313.0, "end": 2319.0, "text": " No, I very well understand the solution. It was just felt to me like it sounds like", "tokens": [51514, 883, 11, 286, 588, 731, 1223, 264, 3827, 13, 467, 390, 445, 2762, 281, 385, 411, 309, 3263, 411, 51814], "temperature": 0.0, "avg_logprob": -0.15211554027739024, "compression_ratio": 1.715481171548117, "no_speech_prob": 0.006306593306362629}, {"id": 431, "seek": 231900, "start": 2319.0, "end": 2324.0, "text": " something machine should be doing. Like why doesn't the VM replace it with a lock on", "tokens": [50364, 746, 3479, 820, 312, 884, 13, 1743, 983, 1177, 380, 264, 18038, 7406, 309, 365, 257, 4017, 322, 50614], "temperature": 0.0, "avg_logprob": -0.20894766868428982, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.023439418524503708}, {"id": 432, "seek": 231900, "start": 2324.0, "end": 2329.0, "text": " behalf of me? Right. So this, so, so, so what you're asking is, is why don't the VM", "tokens": [50614, 9490, 295, 385, 30, 1779, 13, 407, 341, 11, 370, 11, 370, 11, 370, 437, 291, 434, 3365, 307, 11, 307, 983, 500, 380, 264, 18038, 50864], "temperature": 0.0, "avg_logprob": -0.20894766868428982, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.023439418524503708}, {"id": 433, "seek": 231900, "start": 2329.0, "end": 2333.0, "text": " magically replace it? There's a lot of issues with, with, with, with, with doing something", "tokens": [50864, 39763, 7406, 309, 30, 821, 311, 257, 688, 295, 2663, 365, 11, 365, 11, 365, 11, 365, 11, 365, 884, 746, 51064], "temperature": 0.0, "avg_logprob": -0.20894766868428982, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.023439418524503708}, {"id": 434, "seek": 231900, "start": 2333.0, "end": 2346.0, "text": " like that. So. Okay. Thank you. Okay. Oh, there's one other.", "tokens": [51064, 411, 300, 13, 407, 13, 1033, 13, 1044, 291, 13, 1033, 13, 876, 11, 456, 311, 472, 661, 13, 51714], "temperature": 0.0, "avg_logprob": -0.20894766868428982, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.023439418524503708}, {"id": 435, "seek": 234600, "start": 2346.0, "end": 2356.0, "text": " So with all the work going into addressing issues with monitors and pinning will constructs", "tokens": [50364, 407, 365, 439, 264, 589, 516, 666, 14329, 2663, 365, 26518, 293, 5447, 773, 486, 7690, 82, 50864], "temperature": 0.0, "avg_logprob": -0.26874234126164365, "compression_ratio": 1.6107784431137724, "no_speech_prob": 0.0873601883649826}, {"id": 436, "seek": 234600, "start": 2356.0, "end": 2362.0, "text": " living inside Java, you till concurrent also benefit from that work or is that isolated", "tokens": [50864, 2647, 1854, 10745, 11, 291, 4288, 37702, 611, 5121, 490, 300, 589, 420, 307, 300, 14621, 51164], "temperature": 0.0, "avg_logprob": -0.26874234126164365, "compression_ratio": 1.6107784431137724, "no_speech_prob": 0.0873601883649826}, {"id": 437, "seek": 234600, "start": 2362.0, "end": 2368.0, "text": " from each other? Okay. So Java till concurrent does not base itself on, on, on, on, does,", "tokens": [51164, 490, 1184, 661, 30, 1033, 13, 407, 10745, 4288, 37702, 775, 406, 3096, 2564, 322, 11, 322, 11, 322, 11, 322, 11, 775, 11, 51464], "temperature": 0.0, "avg_logprob": -0.26874234126164365, "compression_ratio": 1.6107784431137724, "no_speech_prob": 0.0873601883649826}, {"id": 438, "seek": 236800, "start": 2368.0, "end": 2375.0, "text": " does not base itself on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on,", "tokens": [50364, 775, 406, 3096, 2564, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 50714], "temperature": 0.0, "avg_logprob": -0.15171248118082684, "compression_ratio": 10.057142857142857, "no_speech_prob": 0.9929704070091248}, {"id": 439, "seek": 236800, "start": 2375.0, "end": 2381.0, "text": " on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on,", "tokens": [50714, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 51014], "temperature": 0.0, "avg_logprob": -0.15171248118082684, "compression_ratio": 10.057142857142857, "no_speech_prob": 0.9929704070091248}, {"id": 440, "seek": 236800, "start": 2381.0, "end": 2387.0, "text": " on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on,", "tokens": [51014, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 51314], "temperature": 0.0, "avg_logprob": -0.15171248118082684, "compression_ratio": 10.057142857142857, "no_speech_prob": 0.9929704070091248}, {"id": 441, "seek": 236800, "start": 2387.0, "end": 2393.0, "text": " on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, on,", "tokens": [51314, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 322, 11, 51614], "temperature": 0.0, "avg_logprob": -0.15171248118082684, "compression_ratio": 10.057142857142857, "no_speech_prob": 0.9929704070091248}], "language": "en"}