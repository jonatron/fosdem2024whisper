{"text": " Okay, I... Yeah, it switched off to like... By itself. I didn't touch it. So yeah, when you debug, for example, your code, when you're trying to find out why you have a strange error or something like that, you can use our long tracing. And it's very powerful, as we said before. And for example, you can use tools like DBG or Recon that are using error tracing underneath. And the first step is to choose which functions you want to trace, actually, because you don't trace everything. Although you can trace what you want, you cannot trace everything at once. So you choose like, I want this function to be traced for this bunch of functions. And then, when you call these functions, you get your traces being printed out. So you get the information of this function is called, these are the arguments, return values, things like that. You can get it to console, you can get it to a file, and you can also send it to a network, and that's what I have been doing for many, many years. I was just setting up, for example, I said many years, yeah, for 15 years, I think, with Erlang. So I was just setting up a special node that was like collecting traces for all the other nodes. So you can also send them to the network. And, well, afterwards, you either read the traces that you collected, or you can also search them, grab them, parse them, do some other operations on them if you want. But these are just text logs, let's say, mostly. And the problem is that very often you have to repeat the whole process. That's because you've traced one function, but you found out that maybe the problem is in another function, maybe in a completely different module, and so on and so on. So you do it, so you repeat, repeat, and that might be kind of a problem. So this doesn't scale well. And what I mean by that is if you try to trace a lot of functions, well, I found out that at least for me, when I get like 100 to 1000 traces, it becomes difficult to read, like for a human to read that amount of information. Okay, but you can search, for example. And this also has a limit. So, of course, this is just a rough estimate, let's say, but for me, usually, when I have like 10 to 100,000 traces, then it becomes difficult, because even my system can slow down, IO can slow down, and actually it's quite surprising, but sending traces to a file or to a network, it's actually quite expensive. And it can slow down the system quite a lot. And it's a heavy operation, so sometimes I had traces accumulating for three minutes after I finished traces or something like that, and the messages were still in the queue, still being processed. Yeah, so this doesn't scale that well. Okay, so let's sum up. Choosing the function to trace is kind of a guesswork. Not always, of course, sometimes we know precisely, but most often I don't know. I know kind of what I'm looking for, but not exactly, and that's the problem, but I need to somehow know the function exactly here, to choose it to be traced. So, possibly many iterations of the process. This is, for me, this is like ad hoc logging. This is very much like logging, but I don't need to have a log statement in my code. I just choose dynamically, right now, I want to do these functions to be logged. And what if the trace behaviors have tests that fail every 20 runs, for example, do I need to repeat this 20 times? So what? That's the problem, right? And answer to some of those issues is Erlang Doctor, at least for me, and for the people who've used that. So what's the difference? So you set up the tracing for entire application, not always. Sometimes it's not possible. Sometimes you have to trace individual modules, but usually you can start with just one entire application. You capture traces, store them in the ETS table, and you clear the traces afterwards. And you can repeat this process instead of repeating everything, because you've collected enough stuff to query and to find out about different functions, for example. To query, oh, this was this function called, maybe another one, and so on and so on. You can do this. And of course, rarely you have to repeat this, but for me it's only when I, for example, trace the wrong application, because the problem is not in my code, it's in a library that I've used. Then I need to trace another Erlang application, right? But it doesn't occur that often. This scales much better. What are the limits? So on my laptop, for example, querying becomes slow at about 10 million traces collected, which is quite a lot, but it's like tracing a function, a system under heavy load, for example. And of course it depends on the size of individual traces, because you can have big arguments being passed or something like that. Yeah. System memory becomes the limit at 4 million at about 50 million traces, but sometimes it's 10 million, sometimes it's 100 million, it depends. But basically when you have a few million traces, it's probably too much. So there is a limit, of course. So to sum up, very few iterations of the whole process, usually one. This is for me like ad hoc instrumentation instead of ad hoc logging, because you're gathering structured information in the ETS table. I will show you details in a moment. And use cases, for me there are many use cases. For example, debugging, system exploration. I often use it to just learn about the system. I just run the system, do it with the usual stuff when I'm tracing the whole application, and I'm just querying what the system did, actually, with the traces. And you can also do some lightweight profiling without the need to set up the profiler for a particular function. Yeah. So let's go to the Erlang doctor itself. How to get it from GitHub for Erlang or for Elixir? For Elixir it's called Xdoctor, which looks like a former doctor, but it's just a bit funny. Yeah, so here is a package of Xdox for both of them. And yeah. So how to run it? Three options. The first one that I'm using sometimes when doing firefighting, this is when you don't have it in your shell, but you want it right now, like in a system that's misbehaving or something. In both tools there are snippets that just download it from the Internet and compile it and run it, which works in this particular case. It's probably the best option if you just want it right now. And yeah, all you need is to have the access to the Internet, which is usually the case. The second option, which I'm using always in development, is just that you set it up in your .erlang or .iex.exe file. So that it's always available whenever you start any Erlang or Elixir shell, be it in your project or wherever. And third option, packaging. You can always include it in your application, in your software, if you think it's that useful. Okay, so let's move on. Let's start. Examples are in Erlang, but they are also available for Elixir in the docs. You can find them. The first thing to do is to start. It needs a GenServer, so it just starts at GenServer. And a few other examples how you can start them. You can choose a different ETS table. You can just have multiple ones if you want, switch between them. You can limit the size of a table. Very useful, like in a production environment, if you need to do some tracing, you just set it to like 1,000 or something. Just the table will never grow bigger, so you will never consume all memory. And yeah, there is also a start link. Okay, so let's set up tracing. I'm just tracing an example module. It's a test suite, but it contains functions that we can trace. It's good. So yeah, I'm just starting that tracer. And I can also trace a specific function, like provide a module function argument, whole application, multiple applications. And add a bit more. You can trace messages. You can trace specific processes and so on. There are a few more options. Capturing the traces. Okay, so let's call a function from the trace module. I'm calling just a sleepy factorial of 3. It's a function that calculates a factorial and just sleeps for 1 millisecond between each step, right? It will have some time difference. That's it. Yeah, very simple. And yeah, I'm just... Okay, now we can stop tracing. It's a good habit because you don't accumulate traces when you don't want them anymore. And now what can you do? Because we've accumulated traces, what can you do with them? So let's read the record definition. By the way, I'm using records because they are very performant. And even maps were giving me five times worse performance for some operations. So yeah, I'm using records. Yeah, so let's get all the traces. So I got all the traces and I don't want to talk about everything. Let's talk about the arguments. So these are the arguments and these are the return values, okay? For calls, for returns. And I will just introduce the other fields as we go. Arguments are in brackets. So now trace selection. You can do a select. It's a fancy way of doing this ETS select with ETS from 2MS. And let's get all function calls. And for each argument, let's just get this argument. So I'm getting a list of arguments. And of course, this is a recursive way of calculating factorial. So it's 3, 2, 1, 0. And there is also select slash 2. And this one takes like any term and looks for that term. So here it found, for example, it has an argument. Here it found it as a return value. But there is more. It can be hidden inside any lists, maps, tuples. So it will look recursively inside your data structures to find out anything you're looking for. So for example, you can look for an error message, even if it's called unknown error, which occurred to me once. And I just found this unknown error. I just put unknown error here and I just found the function that that causes, right, instantly. Okay, there is also filter. It's similar to select. But here you can pass any function. It's a bit slower, but it has more features simply. So you can, for example, assign a result to a search, to a variable, and then you can search in that list again. Oops, sorry. Then you can search in that list again. So you can narrow down your search. You got like two traces. Now you search in that two traces, but only for calls and you get only one. So another way to query it. And the tracebacks are very important for me because I want to, like, know the source where this originated, this particular, for example, function call. So here I'm just looking for any return value of one. And the sleepy factor of one actually matches it and it returned one. So this is returned, the traceback of this one. The call itself is first. It's right here. Sleepy factor of one and the rest is just the traceback. And the sleepy factor of zero returned also one, but it skipped because of some skipping logic. Yeah, it's details, but it helps you, like, limit the output that you get. Actually, you can disable that and you can get, like, all the traces with output all. Then you get, then you have no skipping of traceback that include another tracebacks. And you can limit the number of much traces. You can reverse call order. You can search in a different table, in a list, for example. And you can get only the first traceback if you want, very useful, just shortcut, let's say. And you can also get it for a particular record or just an index of a trace because there has just this auto-incremented indexes. Yeah, and similar to a traceback, you have ranges and ranges look inside. So traceback is like what's the source and ranges give you, like, all the traces starting with a function call until it's returned. Everything in between, from one process. And, yeah, so for example, here we are really looking for any traces that are just for function calls that have one as an argument and you get a range of traces from the call until the return. A range options, you can get, like, limit call depth is quite interesting and very useful because by having one you just get a call and return, which is very useful. And searching in a list of traces is also possible, getting only the first range if there are many, also possible. And getting the range for a specific trace. So quite a lot of options. I've just, you know, added, I've been adding and adding over a few years of development of these two. So they're all quite useful. Utilities, two simple utilities they wanted to talk about. One is to just look up a trace. Nothing fancy here, ETS lookup, does it, right? But then you can execute the trace, which is quite useful for me. So if this was a function call, I can just execute it right now, again. This is just, for example, let's say I fix the bug and then instead of writing some long code, I can just execute a trace and see if the result is the same or different. Or I can trace again, right? I can start the traces and trace again. Okay, now a bit of profiling. So I find this lightweight profiling very useful because it doesn't put as much stress on the system as F-Prof, for example, the Erlang profiler. And it's like instantly available. I don't have to prepare for it in any way. So call start, it's statistics aggregated by this function. So I'm aggregating everything under the total atom. So I'm getting like four calls and this accumulated time and this own time. These are equal because I'm just accumulating everything. But if I aggregated by function argument, you can see that there was this one call with each of the arguments. And this call took the longest time, but actually it's accumulated time because its own time was the shortest section, right? You can also do filtering here. So you can say when N is smaller than 3 and we just skipped one of them. So you can do that and you can sort them. Yes, you can sort them and print them as a table. We had some just nice utilities to have. And the last feature I wanted to talk about is function call 3 statistics. I called it like that because let's say we have a function that calculates the Fibonacci sequence in a suboptimal way, you probably all know that it's suboptimal. It branches a lot and let's clean up the traces. Trace again, call fib of 4 which returns 3, which is the correct value and stop tracing. So we now have different traces in the table and let's do it. Let's just call this function with default arguments. So it says that there is a call 3, I mean by that function calls, returns, everything inside repeated twice because there is this number 2 and it took 10 microseconds that there is no sleep in this example. So it took 10 microseconds in total. And this is how the function call 3 looks like. So you can see that, yeah, indeed, it repeated twice. So this can help you find out redundant code. Yeah. Okay, so this function also has some options but I don't have time to talk about them. You can just customize them a bit. And table manipulation, you can get the current table, dump it to a file and load it on a different Erlang node. And then you can continue analysis on a different Erlang node. And that's all I wanted to talk about. And that's me on a mountain bike. Thank you.", "segments": [{"id": 0, "seek": 12000, "start": 120.0, "end": 130.0, "text": " Okay, I...", "tokens": [50364, 1033, 11, 286, 485, 50864], "temperature": 0.0, "avg_logprob": -0.33104893932603807, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.5397909283638}, {"id": 1, "seek": 12000, "start": 130.0, "end": 133.0, "text": " Yeah, it switched off to like...", "tokens": [50864, 865, 11, 309, 16858, 766, 281, 411, 485, 51014], "temperature": 0.0, "avg_logprob": -0.33104893932603807, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.5397909283638}, {"id": 2, "seek": 12000, "start": 133.0, "end": 136.0, "text": " By itself. I didn't touch it.", "tokens": [51014, 3146, 2564, 13, 286, 994, 380, 2557, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.33104893932603807, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.5397909283638}, {"id": 3, "seek": 12000, "start": 136.0, "end": 141.0, "text": " So yeah, when you debug, for example, your code,", "tokens": [51164, 407, 1338, 11, 562, 291, 24083, 11, 337, 1365, 11, 428, 3089, 11, 51414], "temperature": 0.0, "avg_logprob": -0.33104893932603807, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.5397909283638}, {"id": 4, "seek": 12000, "start": 141.0, "end": 144.0, "text": " when you're trying to find out why you have a strange error", "tokens": [51414, 562, 291, 434, 1382, 281, 915, 484, 983, 291, 362, 257, 5861, 6713, 51564], "temperature": 0.0, "avg_logprob": -0.33104893932603807, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.5397909283638}, {"id": 5, "seek": 12000, "start": 144.0, "end": 148.0, "text": " or something like that, you can use our long tracing.", "tokens": [51564, 420, 746, 411, 300, 11, 291, 393, 764, 527, 938, 25262, 13, 51764], "temperature": 0.0, "avg_logprob": -0.33104893932603807, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.5397909283638}, {"id": 6, "seek": 14800, "start": 148.0, "end": 151.0, "text": " And it's very powerful, as we said before.", "tokens": [50364, 400, 309, 311, 588, 4005, 11, 382, 321, 848, 949, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09291874885559082, "compression_ratio": 1.6875, "no_speech_prob": 0.08575589209794998}, {"id": 7, "seek": 14800, "start": 151.0, "end": 154.0, "text": " And for example, you can use tools like DBG or Recon", "tokens": [50514, 400, 337, 1365, 11, 291, 393, 764, 3873, 411, 26754, 38, 420, 1300, 1671, 50664], "temperature": 0.0, "avg_logprob": -0.09291874885559082, "compression_ratio": 1.6875, "no_speech_prob": 0.08575589209794998}, {"id": 8, "seek": 14800, "start": 154.0, "end": 157.0, "text": " that are using error tracing underneath.", "tokens": [50664, 300, 366, 1228, 6713, 25262, 7223, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09291874885559082, "compression_ratio": 1.6875, "no_speech_prob": 0.08575589209794998}, {"id": 9, "seek": 14800, "start": 157.0, "end": 161.0, "text": " And the first step is to choose which functions", "tokens": [50814, 400, 264, 700, 1823, 307, 281, 2826, 597, 6828, 51014], "temperature": 0.0, "avg_logprob": -0.09291874885559082, "compression_ratio": 1.6875, "no_speech_prob": 0.08575589209794998}, {"id": 10, "seek": 14800, "start": 161.0, "end": 166.0, "text": " you want to trace, actually, because you don't trace everything.", "tokens": [51014, 291, 528, 281, 13508, 11, 767, 11, 570, 291, 500, 380, 13508, 1203, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09291874885559082, "compression_ratio": 1.6875, "no_speech_prob": 0.08575589209794998}, {"id": 11, "seek": 14800, "start": 166.0, "end": 169.0, "text": " Although you can trace what you want,", "tokens": [51264, 5780, 291, 393, 13508, 437, 291, 528, 11, 51414], "temperature": 0.0, "avg_logprob": -0.09291874885559082, "compression_ratio": 1.6875, "no_speech_prob": 0.08575589209794998}, {"id": 12, "seek": 14800, "start": 169.0, "end": 172.0, "text": " you cannot trace everything at once.", "tokens": [51414, 291, 2644, 13508, 1203, 412, 1564, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09291874885559082, "compression_ratio": 1.6875, "no_speech_prob": 0.08575589209794998}, {"id": 13, "seek": 14800, "start": 172.0, "end": 175.0, "text": " So you choose like, I want this function to be traced", "tokens": [51564, 407, 291, 2826, 411, 11, 286, 528, 341, 2445, 281, 312, 38141, 51714], "temperature": 0.0, "avg_logprob": -0.09291874885559082, "compression_ratio": 1.6875, "no_speech_prob": 0.08575589209794998}, {"id": 14, "seek": 17500, "start": 175.0, "end": 177.0, "text": " for this bunch of functions.", "tokens": [50364, 337, 341, 3840, 295, 6828, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1163167953491211, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.04582342132925987}, {"id": 15, "seek": 17500, "start": 177.0, "end": 181.0, "text": " And then, when you call these functions,", "tokens": [50464, 400, 550, 11, 562, 291, 818, 613, 6828, 11, 50664], "temperature": 0.0, "avg_logprob": -0.1163167953491211, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.04582342132925987}, {"id": 16, "seek": 17500, "start": 181.0, "end": 183.0, "text": " you get your traces being printed out.", "tokens": [50664, 291, 483, 428, 26076, 885, 13567, 484, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1163167953491211, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.04582342132925987}, {"id": 17, "seek": 17500, "start": 183.0, "end": 186.0, "text": " So you get the information of this function is called,", "tokens": [50764, 407, 291, 483, 264, 1589, 295, 341, 2445, 307, 1219, 11, 50914], "temperature": 0.0, "avg_logprob": -0.1163167953491211, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.04582342132925987}, {"id": 18, "seek": 17500, "start": 186.0, "end": 190.0, "text": " these are the arguments, return values, things like that.", "tokens": [50914, 613, 366, 264, 12869, 11, 2736, 4190, 11, 721, 411, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1163167953491211, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.04582342132925987}, {"id": 19, "seek": 17500, "start": 190.0, "end": 194.0, "text": " You can get it to console, you can get it to a file,", "tokens": [51114, 509, 393, 483, 309, 281, 11076, 11, 291, 393, 483, 309, 281, 257, 3991, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1163167953491211, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.04582342132925987}, {"id": 20, "seek": 17500, "start": 194.0, "end": 196.0, "text": " and you can also send it to a network,", "tokens": [51314, 293, 291, 393, 611, 2845, 309, 281, 257, 3209, 11, 51414], "temperature": 0.0, "avg_logprob": -0.1163167953491211, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.04582342132925987}, {"id": 21, "seek": 17500, "start": 196.0, "end": 199.0, "text": " and that's what I have been doing for many, many years.", "tokens": [51414, 293, 300, 311, 437, 286, 362, 668, 884, 337, 867, 11, 867, 924, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1163167953491211, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.04582342132925987}, {"id": 22, "seek": 17500, "start": 199.0, "end": 202.0, "text": " I was just setting up, for example, I said many years, yeah,", "tokens": [51564, 286, 390, 445, 3287, 493, 11, 337, 1365, 11, 286, 848, 867, 924, 11, 1338, 11, 51714], "temperature": 0.0, "avg_logprob": -0.1163167953491211, "compression_ratio": 1.7695473251028806, "no_speech_prob": 0.04582342132925987}, {"id": 23, "seek": 20200, "start": 202.0, "end": 205.0, "text": " for 15 years, I think, with Erlang.", "tokens": [50364, 337, 2119, 924, 11, 286, 519, 11, 365, 3300, 25241, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10795622738924894, "compression_ratio": 1.6567796610169492, "no_speech_prob": 0.044152043759822845}, {"id": 24, "seek": 20200, "start": 205.0, "end": 208.0, "text": " So I was just setting up a special node", "tokens": [50514, 407, 286, 390, 445, 3287, 493, 257, 2121, 9984, 50664], "temperature": 0.0, "avg_logprob": -0.10795622738924894, "compression_ratio": 1.6567796610169492, "no_speech_prob": 0.044152043759822845}, {"id": 25, "seek": 20200, "start": 208.0, "end": 211.0, "text": " that was like collecting traces for all the other nodes.", "tokens": [50664, 300, 390, 411, 12510, 26076, 337, 439, 264, 661, 13891, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10795622738924894, "compression_ratio": 1.6567796610169492, "no_speech_prob": 0.044152043759822845}, {"id": 26, "seek": 20200, "start": 211.0, "end": 213.0, "text": " So you can also send them to the network.", "tokens": [50814, 407, 291, 393, 611, 2845, 552, 281, 264, 3209, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10795622738924894, "compression_ratio": 1.6567796610169492, "no_speech_prob": 0.044152043759822845}, {"id": 27, "seek": 20200, "start": 213.0, "end": 219.0, "text": " And, well, afterwards, you either read the traces that you collected,", "tokens": [50914, 400, 11, 731, 11, 10543, 11, 291, 2139, 1401, 264, 26076, 300, 291, 11087, 11, 51214], "temperature": 0.0, "avg_logprob": -0.10795622738924894, "compression_ratio": 1.6567796610169492, "no_speech_prob": 0.044152043759822845}, {"id": 28, "seek": 20200, "start": 219.0, "end": 223.0, "text": " or you can also search them, grab them, parse them,", "tokens": [51214, 420, 291, 393, 611, 3164, 552, 11, 4444, 552, 11, 48377, 552, 11, 51414], "temperature": 0.0, "avg_logprob": -0.10795622738924894, "compression_ratio": 1.6567796610169492, "no_speech_prob": 0.044152043759822845}, {"id": 29, "seek": 20200, "start": 223.0, "end": 226.0, "text": " do some other operations on them if you want.", "tokens": [51414, 360, 512, 661, 7705, 322, 552, 498, 291, 528, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10795622738924894, "compression_ratio": 1.6567796610169492, "no_speech_prob": 0.044152043759822845}, {"id": 30, "seek": 20200, "start": 226.0, "end": 229.0, "text": " But these are just text logs, let's say, mostly.", "tokens": [51564, 583, 613, 366, 445, 2487, 20820, 11, 718, 311, 584, 11, 5240, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10795622738924894, "compression_ratio": 1.6567796610169492, "no_speech_prob": 0.044152043759822845}, {"id": 31, "seek": 22900, "start": 229.0, "end": 233.0, "text": " And the problem is that very often you have to repeat the whole process.", "tokens": [50364, 400, 264, 1154, 307, 300, 588, 2049, 291, 362, 281, 7149, 264, 1379, 1399, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08850542027899559, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.07706771045923233}, {"id": 32, "seek": 22900, "start": 233.0, "end": 236.0, "text": " That's because you've traced one function,", "tokens": [50564, 663, 311, 570, 291, 600, 38141, 472, 2445, 11, 50714], "temperature": 0.0, "avg_logprob": -0.08850542027899559, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.07706771045923233}, {"id": 33, "seek": 22900, "start": 236.0, "end": 241.0, "text": " but you found out that maybe the problem is in another function,", "tokens": [50714, 457, 291, 1352, 484, 300, 1310, 264, 1154, 307, 294, 1071, 2445, 11, 50964], "temperature": 0.0, "avg_logprob": -0.08850542027899559, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.07706771045923233}, {"id": 34, "seek": 22900, "start": 241.0, "end": 244.0, "text": " maybe in a completely different module, and so on and so on.", "tokens": [50964, 1310, 294, 257, 2584, 819, 10088, 11, 293, 370, 322, 293, 370, 322, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08850542027899559, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.07706771045923233}, {"id": 35, "seek": 22900, "start": 244.0, "end": 246.0, "text": " So you do it, so you repeat, repeat,", "tokens": [51114, 407, 291, 360, 309, 11, 370, 291, 7149, 11, 7149, 11, 51214], "temperature": 0.0, "avg_logprob": -0.08850542027899559, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.07706771045923233}, {"id": 36, "seek": 22900, "start": 246.0, "end": 250.0, "text": " and that might be kind of a problem.", "tokens": [51214, 293, 300, 1062, 312, 733, 295, 257, 1154, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08850542027899559, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.07706771045923233}, {"id": 37, "seek": 22900, "start": 250.0, "end": 254.0, "text": " So this doesn't scale well.", "tokens": [51414, 407, 341, 1177, 380, 4373, 731, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08850542027899559, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.07706771045923233}, {"id": 38, "seek": 25400, "start": 254.0, "end": 260.0, "text": " And what I mean by that is if you try to trace a lot of functions,", "tokens": [50364, 400, 437, 286, 914, 538, 300, 307, 498, 291, 853, 281, 13508, 257, 688, 295, 6828, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10200170868808783, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.13691580295562744}, {"id": 39, "seek": 25400, "start": 260.0, "end": 264.0, "text": " well, I found out that at least for me,", "tokens": [50664, 731, 11, 286, 1352, 484, 300, 412, 1935, 337, 385, 11, 50864], "temperature": 0.0, "avg_logprob": -0.10200170868808783, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.13691580295562744}, {"id": 40, "seek": 25400, "start": 264.0, "end": 269.0, "text": " when I get like 100 to 1000 traces, it becomes difficult to read,", "tokens": [50864, 562, 286, 483, 411, 2319, 281, 9714, 26076, 11, 309, 3643, 2252, 281, 1401, 11, 51114], "temperature": 0.0, "avg_logprob": -0.10200170868808783, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.13691580295562744}, {"id": 41, "seek": 25400, "start": 269.0, "end": 273.0, "text": " like for a human to read that amount of information.", "tokens": [51114, 411, 337, 257, 1952, 281, 1401, 300, 2372, 295, 1589, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10200170868808783, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.13691580295562744}, {"id": 42, "seek": 25400, "start": 273.0, "end": 275.0, "text": " Okay, but you can search, for example.", "tokens": [51314, 1033, 11, 457, 291, 393, 3164, 11, 337, 1365, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10200170868808783, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.13691580295562744}, {"id": 43, "seek": 25400, "start": 275.0, "end": 278.0, "text": " And this also has a limit.", "tokens": [51414, 400, 341, 611, 575, 257, 4948, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10200170868808783, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.13691580295562744}, {"id": 44, "seek": 25400, "start": 278.0, "end": 282.0, "text": " So, of course, this is just a rough estimate, let's say,", "tokens": [51564, 407, 11, 295, 1164, 11, 341, 307, 445, 257, 5903, 12539, 11, 718, 311, 584, 11, 51764], "temperature": 0.0, "avg_logprob": -0.10200170868808783, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.13691580295562744}, {"id": 45, "seek": 28200, "start": 282.0, "end": 288.0, "text": " but for me, usually, when I have like 10 to 100,000 traces,", "tokens": [50364, 457, 337, 385, 11, 2673, 11, 562, 286, 362, 411, 1266, 281, 2319, 11, 1360, 26076, 11, 50664], "temperature": 0.0, "avg_logprob": -0.16011526229533743, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.04770157113671303}, {"id": 46, "seek": 28200, "start": 288.0, "end": 292.0, "text": " then it becomes difficult, because even my system can slow down,", "tokens": [50664, 550, 309, 3643, 2252, 11, 570, 754, 452, 1185, 393, 2964, 760, 11, 50864], "temperature": 0.0, "avg_logprob": -0.16011526229533743, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.04770157113671303}, {"id": 47, "seek": 28200, "start": 292.0, "end": 296.0, "text": " IO can slow down, and actually it's quite surprising,", "tokens": [50864, 286, 46, 393, 2964, 760, 11, 293, 767, 309, 311, 1596, 8830, 11, 51064], "temperature": 0.0, "avg_logprob": -0.16011526229533743, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.04770157113671303}, {"id": 48, "seek": 28200, "start": 296.0, "end": 301.0, "text": " but sending traces to a file or to a network,", "tokens": [51064, 457, 7750, 26076, 281, 257, 3991, 420, 281, 257, 3209, 11, 51314], "temperature": 0.0, "avg_logprob": -0.16011526229533743, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.04770157113671303}, {"id": 49, "seek": 28200, "start": 301.0, "end": 304.0, "text": " it's actually quite expensive.", "tokens": [51314, 309, 311, 767, 1596, 5124, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16011526229533743, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.04770157113671303}, {"id": 50, "seek": 28200, "start": 304.0, "end": 307.0, "text": " And it can slow down the system quite a lot.", "tokens": [51464, 400, 309, 393, 2964, 760, 264, 1185, 1596, 257, 688, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16011526229533743, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.04770157113671303}, {"id": 51, "seek": 28200, "start": 307.0, "end": 309.0, "text": " And it's a heavy operation,", "tokens": [51614, 400, 309, 311, 257, 4676, 6916, 11, 51714], "temperature": 0.0, "avg_logprob": -0.16011526229533743, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.04770157113671303}, {"id": 52, "seek": 30900, "start": 309.0, "end": 312.0, "text": " so sometimes I had traces accumulating for three minutes", "tokens": [50364, 370, 2171, 286, 632, 26076, 12989, 12162, 337, 1045, 2077, 50514], "temperature": 0.0, "avg_logprob": -0.08399098494957233, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.03573470190167427}, {"id": 53, "seek": 30900, "start": 312.0, "end": 315.0, "text": " after I finished traces or something like that,", "tokens": [50514, 934, 286, 4335, 26076, 420, 746, 411, 300, 11, 50664], "temperature": 0.0, "avg_logprob": -0.08399098494957233, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.03573470190167427}, {"id": 54, "seek": 30900, "start": 315.0, "end": 318.0, "text": " and the messages were still in the queue, still being processed.", "tokens": [50664, 293, 264, 7897, 645, 920, 294, 264, 18639, 11, 920, 885, 18846, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08399098494957233, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.03573470190167427}, {"id": 55, "seek": 30900, "start": 318.0, "end": 321.0, "text": " Yeah, so this doesn't scale that well.", "tokens": [50814, 865, 11, 370, 341, 1177, 380, 4373, 300, 731, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08399098494957233, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.03573470190167427}, {"id": 56, "seek": 30900, "start": 321.0, "end": 323.0, "text": " Okay, so let's sum up.", "tokens": [50964, 1033, 11, 370, 718, 311, 2408, 493, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08399098494957233, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.03573470190167427}, {"id": 57, "seek": 30900, "start": 323.0, "end": 326.0, "text": " Choosing the function to trace is kind of a guesswork.", "tokens": [51064, 12366, 6110, 264, 2445, 281, 13508, 307, 733, 295, 257, 2041, 1902, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08399098494957233, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.03573470190167427}, {"id": 58, "seek": 30900, "start": 326.0, "end": 330.0, "text": " Not always, of course, sometimes we know precisely,", "tokens": [51214, 1726, 1009, 11, 295, 1164, 11, 2171, 321, 458, 13402, 11, 51414], "temperature": 0.0, "avg_logprob": -0.08399098494957233, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.03573470190167427}, {"id": 59, "seek": 30900, "start": 330.0, "end": 332.0, "text": " but most often I don't know.", "tokens": [51414, 457, 881, 2049, 286, 500, 380, 458, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08399098494957233, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.03573470190167427}, {"id": 60, "seek": 30900, "start": 332.0, "end": 335.0, "text": " I know kind of what I'm looking for, but not exactly,", "tokens": [51514, 286, 458, 733, 295, 437, 286, 478, 1237, 337, 11, 457, 406, 2293, 11, 51664], "temperature": 0.0, "avg_logprob": -0.08399098494957233, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.03573470190167427}, {"id": 61, "seek": 33500, "start": 335.0, "end": 339.0, "text": " and that's the problem, but I need to somehow know the function exactly here,", "tokens": [50364, 293, 300, 311, 264, 1154, 11, 457, 286, 643, 281, 6063, 458, 264, 2445, 2293, 510, 11, 50564], "temperature": 0.0, "avg_logprob": -0.15182046890258788, "compression_ratio": 1.6806083650190113, "no_speech_prob": 0.01688551716506481}, {"id": 62, "seek": 33500, "start": 339.0, "end": 342.0, "text": " to choose it to be traced.", "tokens": [50564, 281, 2826, 309, 281, 312, 38141, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15182046890258788, "compression_ratio": 1.6806083650190113, "no_speech_prob": 0.01688551716506481}, {"id": 63, "seek": 33500, "start": 342.0, "end": 346.0, "text": " So, possibly many iterations of the process.", "tokens": [50714, 407, 11, 6264, 867, 36540, 295, 264, 1399, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15182046890258788, "compression_ratio": 1.6806083650190113, "no_speech_prob": 0.01688551716506481}, {"id": 64, "seek": 33500, "start": 346.0, "end": 348.0, "text": " This is, for me, this is like ad hoc logging.", "tokens": [50914, 639, 307, 11, 337, 385, 11, 341, 307, 411, 614, 16708, 27991, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15182046890258788, "compression_ratio": 1.6806083650190113, "no_speech_prob": 0.01688551716506481}, {"id": 65, "seek": 33500, "start": 348.0, "end": 350.0, "text": " This is very much like logging,", "tokens": [51014, 639, 307, 588, 709, 411, 27991, 11, 51114], "temperature": 0.0, "avg_logprob": -0.15182046890258788, "compression_ratio": 1.6806083650190113, "no_speech_prob": 0.01688551716506481}, {"id": 66, "seek": 33500, "start": 350.0, "end": 353.0, "text": " but I don't need to have a log statement in my code.", "tokens": [51114, 457, 286, 500, 380, 643, 281, 362, 257, 3565, 5629, 294, 452, 3089, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15182046890258788, "compression_ratio": 1.6806083650190113, "no_speech_prob": 0.01688551716506481}, {"id": 67, "seek": 33500, "start": 353.0, "end": 357.0, "text": " I just choose dynamically, right now, I want to do these functions to be logged.", "tokens": [51264, 286, 445, 2826, 43492, 11, 558, 586, 11, 286, 528, 281, 360, 613, 6828, 281, 312, 27231, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15182046890258788, "compression_ratio": 1.6806083650190113, "no_speech_prob": 0.01688551716506481}, {"id": 68, "seek": 33500, "start": 357.0, "end": 363.0, "text": " And what if the trace behaviors have tests that fail every 20 runs, for example,", "tokens": [51464, 400, 437, 498, 264, 13508, 15501, 362, 6921, 300, 3061, 633, 945, 6676, 11, 337, 1365, 11, 51764], "temperature": 0.0, "avg_logprob": -0.15182046890258788, "compression_ratio": 1.6806083650190113, "no_speech_prob": 0.01688551716506481}, {"id": 69, "seek": 36300, "start": 363.0, "end": 365.0, "text": " do I need to repeat this 20 times?", "tokens": [50364, 360, 286, 643, 281, 7149, 341, 945, 1413, 30, 50464], "temperature": 0.0, "avg_logprob": -0.1482125728502186, "compression_ratio": 1.608, "no_speech_prob": 0.08938875794410706}, {"id": 70, "seek": 36300, "start": 365.0, "end": 368.0, "text": " So what? That's the problem, right?", "tokens": [50464, 407, 437, 30, 663, 311, 264, 1154, 11, 558, 30, 50614], "temperature": 0.0, "avg_logprob": -0.1482125728502186, "compression_ratio": 1.608, "no_speech_prob": 0.08938875794410706}, {"id": 71, "seek": 36300, "start": 368.0, "end": 373.0, "text": " And answer to some of those issues is Erlang Doctor,", "tokens": [50614, 400, 1867, 281, 512, 295, 729, 2663, 307, 3300, 25241, 10143, 11, 50864], "temperature": 0.0, "avg_logprob": -0.1482125728502186, "compression_ratio": 1.608, "no_speech_prob": 0.08938875794410706}, {"id": 72, "seek": 36300, "start": 373.0, "end": 376.0, "text": " at least for me, and for the people who've used that.", "tokens": [50864, 412, 1935, 337, 385, 11, 293, 337, 264, 561, 567, 600, 1143, 300, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1482125728502186, "compression_ratio": 1.608, "no_speech_prob": 0.08938875794410706}, {"id": 73, "seek": 36300, "start": 376.0, "end": 379.0, "text": " So what's the difference?", "tokens": [51014, 407, 437, 311, 264, 2649, 30, 51164], "temperature": 0.0, "avg_logprob": -0.1482125728502186, "compression_ratio": 1.608, "no_speech_prob": 0.08938875794410706}, {"id": 74, "seek": 36300, "start": 379.0, "end": 384.0, "text": " So you set up the tracing for entire application, not always.", "tokens": [51164, 407, 291, 992, 493, 264, 25262, 337, 2302, 3861, 11, 406, 1009, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1482125728502186, "compression_ratio": 1.608, "no_speech_prob": 0.08938875794410706}, {"id": 75, "seek": 36300, "start": 384.0, "end": 386.0, "text": " Sometimes it's not possible.", "tokens": [51414, 4803, 309, 311, 406, 1944, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1482125728502186, "compression_ratio": 1.608, "no_speech_prob": 0.08938875794410706}, {"id": 76, "seek": 36300, "start": 386.0, "end": 388.0, "text": " Sometimes you have to trace individual modules,", "tokens": [51514, 4803, 291, 362, 281, 13508, 2609, 16679, 11, 51614], "temperature": 0.0, "avg_logprob": -0.1482125728502186, "compression_ratio": 1.608, "no_speech_prob": 0.08938875794410706}, {"id": 77, "seek": 36300, "start": 388.0, "end": 392.0, "text": " but usually you can start with just one entire application.", "tokens": [51614, 457, 2673, 291, 393, 722, 365, 445, 472, 2302, 3861, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1482125728502186, "compression_ratio": 1.608, "no_speech_prob": 0.08938875794410706}, {"id": 78, "seek": 39200, "start": 393.0, "end": 397.0, "text": " You capture traces, store them in the ETS table,", "tokens": [50414, 509, 7983, 26076, 11, 3531, 552, 294, 264, 462, 7327, 3199, 11, 50614], "temperature": 0.0, "avg_logprob": -0.12492013209074446, "compression_ratio": 1.675, "no_speech_prob": 0.06855708360671997}, {"id": 79, "seek": 39200, "start": 397.0, "end": 399.0, "text": " and you clear the traces afterwards.", "tokens": [50614, 293, 291, 1850, 264, 26076, 10543, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12492013209074446, "compression_ratio": 1.675, "no_speech_prob": 0.06855708360671997}, {"id": 80, "seek": 39200, "start": 399.0, "end": 402.0, "text": " And you can repeat this process instead of repeating everything,", "tokens": [50714, 400, 291, 393, 7149, 341, 1399, 2602, 295, 18617, 1203, 11, 50864], "temperature": 0.0, "avg_logprob": -0.12492013209074446, "compression_ratio": 1.675, "no_speech_prob": 0.06855708360671997}, {"id": 81, "seek": 39200, "start": 402.0, "end": 409.0, "text": " because you've collected enough stuff to query and to find out about different functions, for example.", "tokens": [50864, 570, 291, 600, 11087, 1547, 1507, 281, 14581, 293, 281, 915, 484, 466, 819, 6828, 11, 337, 1365, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12492013209074446, "compression_ratio": 1.675, "no_speech_prob": 0.06855708360671997}, {"id": 82, "seek": 39200, "start": 409.0, "end": 413.0, "text": " To query, oh, this was this function called, maybe another one, and so on and so on.", "tokens": [51214, 1407, 14581, 11, 1954, 11, 341, 390, 341, 2445, 1219, 11, 1310, 1071, 472, 11, 293, 370, 322, 293, 370, 322, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12492013209074446, "compression_ratio": 1.675, "no_speech_prob": 0.06855708360671997}, {"id": 83, "seek": 39200, "start": 413.0, "end": 415.0, "text": " You can do this.", "tokens": [51414, 509, 393, 360, 341, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12492013209074446, "compression_ratio": 1.675, "no_speech_prob": 0.06855708360671997}, {"id": 84, "seek": 39200, "start": 415.0, "end": 419.0, "text": " And of course, rarely you have to repeat this,", "tokens": [51514, 400, 295, 1164, 11, 13752, 291, 362, 281, 7149, 341, 11, 51714], "temperature": 0.0, "avg_logprob": -0.12492013209074446, "compression_ratio": 1.675, "no_speech_prob": 0.06855708360671997}, {"id": 85, "seek": 41900, "start": 419.0, "end": 422.0, "text": " but for me it's only when I, for example, trace the wrong application,", "tokens": [50364, 457, 337, 385, 309, 311, 787, 562, 286, 11, 337, 1365, 11, 13508, 264, 2085, 3861, 11, 50514], "temperature": 0.0, "avg_logprob": -0.08688709327766488, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.02863597311079502}, {"id": 86, "seek": 41900, "start": 422.0, "end": 426.0, "text": " because the problem is not in my code, it's in a library that I've used.", "tokens": [50514, 570, 264, 1154, 307, 406, 294, 452, 3089, 11, 309, 311, 294, 257, 6405, 300, 286, 600, 1143, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08688709327766488, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.02863597311079502}, {"id": 87, "seek": 41900, "start": 426.0, "end": 429.0, "text": " Then I need to trace another Erlang application, right?", "tokens": [50714, 1396, 286, 643, 281, 13508, 1071, 3300, 25241, 3861, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.08688709327766488, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.02863597311079502}, {"id": 88, "seek": 41900, "start": 429.0, "end": 434.0, "text": " But it doesn't occur that often.", "tokens": [50864, 583, 309, 1177, 380, 5160, 300, 2049, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08688709327766488, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.02863597311079502}, {"id": 89, "seek": 41900, "start": 434.0, "end": 436.0, "text": " This scales much better.", "tokens": [51114, 639, 17408, 709, 1101, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08688709327766488, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.02863597311079502}, {"id": 90, "seek": 41900, "start": 436.0, "end": 438.0, "text": " What are the limits?", "tokens": [51214, 708, 366, 264, 10406, 30, 51314], "temperature": 0.0, "avg_logprob": -0.08688709327766488, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.02863597311079502}, {"id": 91, "seek": 41900, "start": 438.0, "end": 440.0, "text": " So on my laptop, for example,", "tokens": [51314, 407, 322, 452, 10732, 11, 337, 1365, 11, 51414], "temperature": 0.0, "avg_logprob": -0.08688709327766488, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.02863597311079502}, {"id": 92, "seek": 41900, "start": 440.0, "end": 443.0, "text": " querying becomes slow at about 10 million traces collected,", "tokens": [51414, 7083, 1840, 3643, 2964, 412, 466, 1266, 2459, 26076, 11087, 11, 51564], "temperature": 0.0, "avg_logprob": -0.08688709327766488, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.02863597311079502}, {"id": 93, "seek": 41900, "start": 443.0, "end": 445.0, "text": " which is quite a lot,", "tokens": [51564, 597, 307, 1596, 257, 688, 11, 51664], "temperature": 0.0, "avg_logprob": -0.08688709327766488, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.02863597311079502}, {"id": 94, "seek": 44500, "start": 445.0, "end": 450.0, "text": " but it's like tracing a function, a system under heavy load, for example.", "tokens": [50364, 457, 309, 311, 411, 25262, 257, 2445, 11, 257, 1185, 833, 4676, 3677, 11, 337, 1365, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17101827442136586, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.04384268447756767}, {"id": 95, "seek": 44500, "start": 450.0, "end": 453.0, "text": " And of course it depends on the size of individual traces,", "tokens": [50614, 400, 295, 1164, 309, 5946, 322, 264, 2744, 295, 2609, 26076, 11, 50764], "temperature": 0.0, "avg_logprob": -0.17101827442136586, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.04384268447756767}, {"id": 96, "seek": 44500, "start": 453.0, "end": 458.0, "text": " because you can have big arguments being passed or something like that.", "tokens": [50764, 570, 291, 393, 362, 955, 12869, 885, 4678, 420, 746, 411, 300, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17101827442136586, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.04384268447756767}, {"id": 97, "seek": 44500, "start": 458.0, "end": 459.0, "text": " Yeah.", "tokens": [51014, 865, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17101827442136586, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.04384268447756767}, {"id": 98, "seek": 44500, "start": 459.0, "end": 464.0, "text": " System memory becomes the limit at 4 million at about 50 million traces,", "tokens": [51064, 8910, 4675, 3643, 264, 4948, 412, 1017, 2459, 412, 466, 2625, 2459, 26076, 11, 51314], "temperature": 0.0, "avg_logprob": -0.17101827442136586, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.04384268447756767}, {"id": 99, "seek": 44500, "start": 464.0, "end": 467.0, "text": " but sometimes it's 10 million, sometimes it's 100 million, it depends.", "tokens": [51314, 457, 2171, 309, 311, 1266, 2459, 11, 2171, 309, 311, 2319, 2459, 11, 309, 5946, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17101827442136586, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.04384268447756767}, {"id": 100, "seek": 44500, "start": 467.0, "end": 471.0, "text": " But basically when you have a few million traces, it's probably too much.", "tokens": [51464, 583, 1936, 562, 291, 362, 257, 1326, 2459, 26076, 11, 309, 311, 1391, 886, 709, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17101827442136586, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.04384268447756767}, {"id": 101, "seek": 44500, "start": 471.0, "end": 473.0, "text": " So there is a limit, of course.", "tokens": [51664, 407, 456, 307, 257, 4948, 11, 295, 1164, 13, 51764], "temperature": 0.0, "avg_logprob": -0.17101827442136586, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.04384268447756767}, {"id": 102, "seek": 47300, "start": 473.0, "end": 480.0, "text": " So to sum up, very few iterations of the whole process, usually one.", "tokens": [50364, 407, 281, 2408, 493, 11, 588, 1326, 36540, 295, 264, 1379, 1399, 11, 2673, 472, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13515538880319306, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.036879077553749084}, {"id": 103, "seek": 47300, "start": 480.0, "end": 484.0, "text": " This is for me like ad hoc instrumentation instead of ad hoc logging,", "tokens": [50714, 639, 307, 337, 385, 411, 614, 16708, 7198, 399, 2602, 295, 614, 16708, 27991, 11, 50914], "temperature": 0.0, "avg_logprob": -0.13515538880319306, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.036879077553749084}, {"id": 104, "seek": 47300, "start": 484.0, "end": 489.0, "text": " because you're gathering structured information in the ETS table.", "tokens": [50914, 570, 291, 434, 13519, 18519, 1589, 294, 264, 462, 7327, 3199, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13515538880319306, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.036879077553749084}, {"id": 105, "seek": 47300, "start": 489.0, "end": 491.0, "text": " I will show you details in a moment.", "tokens": [51164, 286, 486, 855, 291, 4365, 294, 257, 1623, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13515538880319306, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.036879077553749084}, {"id": 106, "seek": 47300, "start": 491.0, "end": 495.0, "text": " And use cases, for me there are many use cases.", "tokens": [51264, 400, 764, 3331, 11, 337, 385, 456, 366, 867, 764, 3331, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13515538880319306, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.036879077553749084}, {"id": 107, "seek": 47300, "start": 495.0, "end": 498.0, "text": " For example, debugging, system exploration.", "tokens": [51464, 1171, 1365, 11, 45592, 11, 1185, 16197, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13515538880319306, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.036879077553749084}, {"id": 108, "seek": 47300, "start": 498.0, "end": 500.0, "text": " I often use it to just learn about the system.", "tokens": [51614, 286, 2049, 764, 309, 281, 445, 1466, 466, 264, 1185, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13515538880319306, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.036879077553749084}, {"id": 109, "seek": 50000, "start": 500.0, "end": 505.0, "text": " I just run the system, do it with the usual stuff when I'm tracing the whole application,", "tokens": [50364, 286, 445, 1190, 264, 1185, 11, 360, 309, 365, 264, 7713, 1507, 562, 286, 478, 25262, 264, 1379, 3861, 11, 50614], "temperature": 0.0, "avg_logprob": -0.180328836628035, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.09524612128734589}, {"id": 110, "seek": 50000, "start": 505.0, "end": 510.0, "text": " and I'm just querying what the system did, actually, with the traces.", "tokens": [50614, 293, 286, 478, 445, 7083, 1840, 437, 264, 1185, 630, 11, 767, 11, 365, 264, 26076, 13, 50864], "temperature": 0.0, "avg_logprob": -0.180328836628035, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.09524612128734589}, {"id": 111, "seek": 50000, "start": 510.0, "end": 519.0, "text": " And you can also do some lightweight profiling without the need to set up the profiler for a particular function.", "tokens": [50864, 400, 291, 393, 611, 360, 512, 22052, 1740, 4883, 1553, 264, 643, 281, 992, 493, 264, 1740, 5441, 337, 257, 1729, 2445, 13, 51314], "temperature": 0.0, "avg_logprob": -0.180328836628035, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.09524612128734589}, {"id": 112, "seek": 50000, "start": 519.0, "end": 520.0, "text": " Yeah.", "tokens": [51314, 865, 13, 51364], "temperature": 0.0, "avg_logprob": -0.180328836628035, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.09524612128734589}, {"id": 113, "seek": 50000, "start": 520.0, "end": 523.0, "text": " So let's go to the Erlang doctor itself.", "tokens": [51364, 407, 718, 311, 352, 281, 264, 3300, 25241, 4631, 2564, 13, 51514], "temperature": 0.0, "avg_logprob": -0.180328836628035, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.09524612128734589}, {"id": 114, "seek": 50000, "start": 523.0, "end": 528.0, "text": " How to get it from GitHub for Erlang or for Elixir?", "tokens": [51514, 1012, 281, 483, 309, 490, 23331, 337, 3300, 25241, 420, 337, 2699, 970, 347, 30, 51764], "temperature": 0.0, "avg_logprob": -0.180328836628035, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.09524612128734589}, {"id": 115, "seek": 52800, "start": 528.0, "end": 537.0, "text": " For Elixir it's called Xdoctor, which looks like a former doctor, but it's just a bit funny.", "tokens": [50364, 1171, 2699, 970, 347, 309, 311, 1219, 1783, 2595, 1672, 11, 597, 1542, 411, 257, 5819, 4631, 11, 457, 309, 311, 445, 257, 857, 4074, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2245303730905792, "compression_ratio": 1.3695652173913044, "no_speech_prob": 0.08298265188932419}, {"id": 116, "seek": 52800, "start": 537.0, "end": 542.0, "text": " Yeah, so here is a package of Xdox for both of them.", "tokens": [50814, 865, 11, 370, 510, 307, 257, 7372, 295, 1783, 2595, 87, 337, 1293, 295, 552, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2245303730905792, "compression_ratio": 1.3695652173913044, "no_speech_prob": 0.08298265188932419}, {"id": 117, "seek": 52800, "start": 542.0, "end": 547.0, "text": " And yeah.", "tokens": [51064, 400, 1338, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2245303730905792, "compression_ratio": 1.3695652173913044, "no_speech_prob": 0.08298265188932419}, {"id": 118, "seek": 52800, "start": 547.0, "end": 548.0, "text": " So how to run it?", "tokens": [51314, 407, 577, 281, 1190, 309, 30, 51364], "temperature": 0.0, "avg_logprob": -0.2245303730905792, "compression_ratio": 1.3695652173913044, "no_speech_prob": 0.08298265188932419}, {"id": 119, "seek": 52800, "start": 548.0, "end": 549.0, "text": " Three options.", "tokens": [51364, 6244, 3956, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2245303730905792, "compression_ratio": 1.3695652173913044, "no_speech_prob": 0.08298265188932419}, {"id": 120, "seek": 52800, "start": 549.0, "end": 553.0, "text": " The first one that I'm using sometimes when doing firefighting,", "tokens": [51414, 440, 700, 472, 300, 286, 478, 1228, 2171, 562, 884, 25256, 278, 11, 51614], "temperature": 0.0, "avg_logprob": -0.2245303730905792, "compression_ratio": 1.3695652173913044, "no_speech_prob": 0.08298265188932419}, {"id": 121, "seek": 55300, "start": 553.0, "end": 562.0, "text": " this is when you don't have it in your shell, but you want it right now, like in a system that's misbehaving or something.", "tokens": [50364, 341, 307, 562, 291, 500, 380, 362, 309, 294, 428, 8720, 11, 457, 291, 528, 309, 558, 586, 11, 411, 294, 257, 1185, 300, 311, 3346, 29437, 6152, 420, 746, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10969397533370788, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.47834455966949463}, {"id": 122, "seek": 55300, "start": 562.0, "end": 568.0, "text": " In both tools there are snippets that just download it from the Internet and compile it and run it,", "tokens": [50814, 682, 1293, 3873, 456, 366, 35623, 1385, 300, 445, 5484, 309, 490, 264, 7703, 293, 31413, 309, 293, 1190, 309, 11, 51114], "temperature": 0.0, "avg_logprob": -0.10969397533370788, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.47834455966949463}, {"id": 123, "seek": 55300, "start": 568.0, "end": 573.0, "text": " which works in this particular case.", "tokens": [51114, 597, 1985, 294, 341, 1729, 1389, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10969397533370788, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.47834455966949463}, {"id": 124, "seek": 55300, "start": 573.0, "end": 577.0, "text": " It's probably the best option if you just want it right now.", "tokens": [51364, 467, 311, 1391, 264, 1151, 3614, 498, 291, 445, 528, 309, 558, 586, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10969397533370788, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.47834455966949463}, {"id": 125, "seek": 57700, "start": 577.0, "end": 587.0, "text": " And yeah, all you need is to have the access to the Internet, which is usually the case.", "tokens": [50364, 400, 1338, 11, 439, 291, 643, 307, 281, 362, 264, 2105, 281, 264, 7703, 11, 597, 307, 2673, 264, 1389, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12418752366846258, "compression_ratio": 1.5637254901960784, "no_speech_prob": 0.0758352056145668}, {"id": 126, "seek": 57700, "start": 587.0, "end": 598.0, "text": " The second option, which I'm using always in development, is just that you set it up in your .erlang or .iex.exe file.", "tokens": [50864, 440, 1150, 3614, 11, 597, 286, 478, 1228, 1009, 294, 3250, 11, 307, 445, 300, 291, 992, 309, 493, 294, 428, 2411, 260, 25241, 420, 2411, 414, 87, 13, 3121, 68, 3991, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12418752366846258, "compression_ratio": 1.5637254901960784, "no_speech_prob": 0.0758352056145668}, {"id": 127, "seek": 57700, "start": 598.0, "end": 605.0, "text": " So that it's always available whenever you start any Erlang or Elixir shell, be it in your project or wherever.", "tokens": [51414, 407, 300, 309, 311, 1009, 2435, 5699, 291, 722, 604, 3300, 25241, 420, 2699, 970, 347, 8720, 11, 312, 309, 294, 428, 1716, 420, 8660, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12418752366846258, "compression_ratio": 1.5637254901960784, "no_speech_prob": 0.0758352056145668}, {"id": 128, "seek": 60500, "start": 605.0, "end": 607.0, "text": " And third option, packaging.", "tokens": [50364, 400, 2636, 3614, 11, 16836, 13, 50464], "temperature": 0.0, "avg_logprob": -0.12390169107689047, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.04030609130859375}, {"id": 129, "seek": 60500, "start": 607.0, "end": 616.0, "text": " You can always include it in your application, in your software, if you think it's that useful.", "tokens": [50464, 509, 393, 1009, 4090, 309, 294, 428, 3861, 11, 294, 428, 4722, 11, 498, 291, 519, 309, 311, 300, 4420, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12390169107689047, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.04030609130859375}, {"id": 130, "seek": 60500, "start": 616.0, "end": 618.0, "text": " Okay, so let's move on.", "tokens": [50914, 1033, 11, 370, 718, 311, 1286, 322, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12390169107689047, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.04030609130859375}, {"id": 131, "seek": 60500, "start": 618.0, "end": 619.0, "text": " Let's start.", "tokens": [51014, 961, 311, 722, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12390169107689047, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.04030609130859375}, {"id": 132, "seek": 60500, "start": 619.0, "end": 626.0, "text": " Examples are in Erlang, but they are also available for Elixir in the docs.", "tokens": [51064, 48591, 366, 294, 3300, 25241, 11, 457, 436, 366, 611, 2435, 337, 2699, 970, 347, 294, 264, 45623, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12390169107689047, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.04030609130859375}, {"id": 133, "seek": 60500, "start": 626.0, "end": 628.0, "text": " You can find them.", "tokens": [51414, 509, 393, 915, 552, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12390169107689047, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.04030609130859375}, {"id": 134, "seek": 60500, "start": 628.0, "end": 630.0, "text": " The first thing to do is to start.", "tokens": [51514, 440, 700, 551, 281, 360, 307, 281, 722, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12390169107689047, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.04030609130859375}, {"id": 135, "seek": 60500, "start": 630.0, "end": 634.0, "text": " It needs a GenServer, so it just starts at GenServer.", "tokens": [51614, 467, 2203, 257, 3632, 31859, 331, 11, 370, 309, 445, 3719, 412, 3632, 31859, 331, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12390169107689047, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.04030609130859375}, {"id": 136, "seek": 63400, "start": 634.0, "end": 637.0, "text": " And a few other examples how you can start them.", "tokens": [50364, 400, 257, 1326, 661, 5110, 577, 291, 393, 722, 552, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11259375957020543, "compression_ratio": 1.628352490421456, "no_speech_prob": 0.03749614208936691}, {"id": 137, "seek": 63400, "start": 637.0, "end": 639.0, "text": " You can choose a different ETS table.", "tokens": [50514, 509, 393, 2826, 257, 819, 462, 7327, 3199, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11259375957020543, "compression_ratio": 1.628352490421456, "no_speech_prob": 0.03749614208936691}, {"id": 138, "seek": 63400, "start": 639.0, "end": 643.0, "text": " You can just have multiple ones if you want, switch between them.", "tokens": [50614, 509, 393, 445, 362, 3866, 2306, 498, 291, 528, 11, 3679, 1296, 552, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11259375957020543, "compression_ratio": 1.628352490421456, "no_speech_prob": 0.03749614208936691}, {"id": 139, "seek": 63400, "start": 643.0, "end": 645.0, "text": " You can limit the size of a table.", "tokens": [50814, 509, 393, 4948, 264, 2744, 295, 257, 3199, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11259375957020543, "compression_ratio": 1.628352490421456, "no_speech_prob": 0.03749614208936691}, {"id": 140, "seek": 63400, "start": 645.0, "end": 652.0, "text": " Very useful, like in a production environment, if you need to do some tracing, you just set it to like 1,000 or something.", "tokens": [50914, 4372, 4420, 11, 411, 294, 257, 4265, 2823, 11, 498, 291, 643, 281, 360, 512, 25262, 11, 291, 445, 992, 309, 281, 411, 502, 11, 1360, 420, 746, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11259375957020543, "compression_ratio": 1.628352490421456, "no_speech_prob": 0.03749614208936691}, {"id": 141, "seek": 63400, "start": 652.0, "end": 657.0, "text": " Just the table will never grow bigger, so you will never consume all memory.", "tokens": [51264, 1449, 264, 3199, 486, 1128, 1852, 3801, 11, 370, 291, 486, 1128, 14732, 439, 4675, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11259375957020543, "compression_ratio": 1.628352490421456, "no_speech_prob": 0.03749614208936691}, {"id": 142, "seek": 63400, "start": 657.0, "end": 661.0, "text": " And yeah, there is also a start link.", "tokens": [51514, 400, 1338, 11, 456, 307, 611, 257, 722, 2113, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11259375957020543, "compression_ratio": 1.628352490421456, "no_speech_prob": 0.03749614208936691}, {"id": 143, "seek": 66100, "start": 661.0, "end": 664.0, "text": " Okay, so let's set up tracing.", "tokens": [50364, 1033, 11, 370, 718, 311, 992, 493, 25262, 13, 50514], "temperature": 0.0, "avg_logprob": -0.12386374875723598, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.039123084396123886}, {"id": 144, "seek": 66100, "start": 664.0, "end": 666.0, "text": " I'm just tracing an example module.", "tokens": [50514, 286, 478, 445, 25262, 364, 1365, 10088, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12386374875723598, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.039123084396123886}, {"id": 145, "seek": 66100, "start": 666.0, "end": 670.0, "text": " It's a test suite, but it contains functions that we can trace.", "tokens": [50614, 467, 311, 257, 1500, 14205, 11, 457, 309, 8306, 6828, 300, 321, 393, 13508, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12386374875723598, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.039123084396123886}, {"id": 146, "seek": 66100, "start": 670.0, "end": 672.0, "text": " It's good.", "tokens": [50814, 467, 311, 665, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12386374875723598, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.039123084396123886}, {"id": 147, "seek": 66100, "start": 672.0, "end": 675.0, "text": " So yeah, I'm just starting that tracer.", "tokens": [50914, 407, 1338, 11, 286, 478, 445, 2891, 300, 504, 12858, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12386374875723598, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.039123084396123886}, {"id": 148, "seek": 66100, "start": 675.0, "end": 686.0, "text": " And I can also trace a specific function, like provide a module function argument, whole application, multiple applications.", "tokens": [51064, 400, 286, 393, 611, 13508, 257, 2685, 2445, 11, 411, 2893, 257, 10088, 2445, 6770, 11, 1379, 3861, 11, 3866, 5821, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12386374875723598, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.039123084396123886}, {"id": 149, "seek": 68600, "start": 686.0, "end": 688.0, "text": " And add a bit more.", "tokens": [50364, 400, 909, 257, 857, 544, 13, 50464], "temperature": 0.0, "avg_logprob": -0.14241168030306825, "compression_ratio": 1.616, "no_speech_prob": 0.030015746131539345}, {"id": 150, "seek": 68600, "start": 688.0, "end": 689.0, "text": " You can trace messages.", "tokens": [50464, 509, 393, 13508, 7897, 13, 50514], "temperature": 0.0, "avg_logprob": -0.14241168030306825, "compression_ratio": 1.616, "no_speech_prob": 0.030015746131539345}, {"id": 151, "seek": 68600, "start": 689.0, "end": 691.0, "text": " You can trace specific processes and so on.", "tokens": [50514, 509, 393, 13508, 2685, 7555, 293, 370, 322, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14241168030306825, "compression_ratio": 1.616, "no_speech_prob": 0.030015746131539345}, {"id": 152, "seek": 68600, "start": 691.0, "end": 694.0, "text": " There are a few more options.", "tokens": [50614, 821, 366, 257, 1326, 544, 3956, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14241168030306825, "compression_ratio": 1.616, "no_speech_prob": 0.030015746131539345}, {"id": 153, "seek": 68600, "start": 694.0, "end": 696.0, "text": " Capturing the traces.", "tokens": [50764, 9480, 1345, 264, 26076, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14241168030306825, "compression_ratio": 1.616, "no_speech_prob": 0.030015746131539345}, {"id": 154, "seek": 68600, "start": 696.0, "end": 699.0, "text": " Okay, so let's call a function from the trace module.", "tokens": [50864, 1033, 11, 370, 718, 311, 818, 257, 2445, 490, 264, 13508, 10088, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14241168030306825, "compression_ratio": 1.616, "no_speech_prob": 0.030015746131539345}, {"id": 155, "seek": 68600, "start": 699.0, "end": 702.0, "text": " I'm calling just a sleepy factorial of 3.", "tokens": [51014, 286, 478, 5141, 445, 257, 24908, 36916, 295, 805, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14241168030306825, "compression_ratio": 1.616, "no_speech_prob": 0.030015746131539345}, {"id": 156, "seek": 68600, "start": 702.0, "end": 707.0, "text": " It's a function that calculates a factorial and just sleeps for 1 millisecond between each step, right?", "tokens": [51164, 467, 311, 257, 2445, 300, 4322, 1024, 257, 36916, 293, 445, 37991, 337, 502, 27940, 18882, 1296, 1184, 1823, 11, 558, 30, 51414], "temperature": 0.0, "avg_logprob": -0.14241168030306825, "compression_ratio": 1.616, "no_speech_prob": 0.030015746131539345}, {"id": 157, "seek": 68600, "start": 707.0, "end": 709.0, "text": " It will have some time difference.", "tokens": [51414, 467, 486, 362, 512, 565, 2649, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14241168030306825, "compression_ratio": 1.616, "no_speech_prob": 0.030015746131539345}, {"id": 158, "seek": 68600, "start": 709.0, "end": 711.0, "text": " That's it.", "tokens": [51514, 663, 311, 309, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14241168030306825, "compression_ratio": 1.616, "no_speech_prob": 0.030015746131539345}, {"id": 159, "seek": 68600, "start": 711.0, "end": 713.0, "text": " Yeah, very simple.", "tokens": [51614, 865, 11, 588, 2199, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14241168030306825, "compression_ratio": 1.616, "no_speech_prob": 0.030015746131539345}, {"id": 160, "seek": 71300, "start": 713.0, "end": 717.0, "text": " And yeah, I'm just...", "tokens": [50364, 400, 1338, 11, 286, 478, 445, 485, 50564], "temperature": 0.0, "avg_logprob": -0.12396614525907783, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.07210636883974075}, {"id": 161, "seek": 71300, "start": 717.0, "end": 720.0, "text": " Okay, now we can stop tracing.", "tokens": [50564, 1033, 11, 586, 321, 393, 1590, 25262, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12396614525907783, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.07210636883974075}, {"id": 162, "seek": 71300, "start": 720.0, "end": 726.0, "text": " It's a good habit because you don't accumulate traces when you don't want them anymore.", "tokens": [50714, 467, 311, 257, 665, 7164, 570, 291, 500, 380, 33384, 26076, 562, 291, 500, 380, 528, 552, 3602, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12396614525907783, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.07210636883974075}, {"id": 163, "seek": 71300, "start": 726.0, "end": 729.0, "text": " And now what can you do?", "tokens": [51014, 400, 586, 437, 393, 291, 360, 30, 51164], "temperature": 0.0, "avg_logprob": -0.12396614525907783, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.07210636883974075}, {"id": 164, "seek": 71300, "start": 729.0, "end": 732.0, "text": " Because we've accumulated traces, what can you do with them?", "tokens": [51164, 1436, 321, 600, 31346, 26076, 11, 437, 393, 291, 360, 365, 552, 30, 51314], "temperature": 0.0, "avg_logprob": -0.12396614525907783, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.07210636883974075}, {"id": 165, "seek": 71300, "start": 732.0, "end": 735.0, "text": " So let's read the record definition.", "tokens": [51314, 407, 718, 311, 1401, 264, 2136, 7123, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12396614525907783, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.07210636883974075}, {"id": 166, "seek": 71300, "start": 735.0, "end": 741.0, "text": " By the way, I'm using records because they are very performant.", "tokens": [51464, 3146, 264, 636, 11, 286, 478, 1228, 7724, 570, 436, 366, 588, 2042, 394, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12396614525907783, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.07210636883974075}, {"id": 167, "seek": 74100, "start": 741.0, "end": 747.0, "text": " And even maps were giving me five times worse performance for some operations.", "tokens": [50364, 400, 754, 11317, 645, 2902, 385, 1732, 1413, 5324, 3389, 337, 512, 7705, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10134684244791667, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.04818175360560417}, {"id": 168, "seek": 74100, "start": 747.0, "end": 750.0, "text": " So yeah, I'm using records.", "tokens": [50664, 407, 1338, 11, 286, 478, 1228, 7724, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10134684244791667, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.04818175360560417}, {"id": 169, "seek": 74100, "start": 750.0, "end": 753.0, "text": " Yeah, so let's get all the traces.", "tokens": [50814, 865, 11, 370, 718, 311, 483, 439, 264, 26076, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10134684244791667, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.04818175360560417}, {"id": 170, "seek": 74100, "start": 753.0, "end": 756.0, "text": " So I got all the traces and I don't want to talk about everything.", "tokens": [50964, 407, 286, 658, 439, 264, 26076, 293, 286, 500, 380, 528, 281, 751, 466, 1203, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10134684244791667, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.04818175360560417}, {"id": 171, "seek": 74100, "start": 756.0, "end": 758.0, "text": " Let's talk about the arguments.", "tokens": [51114, 961, 311, 751, 466, 264, 12869, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10134684244791667, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.04818175360560417}, {"id": 172, "seek": 74100, "start": 758.0, "end": 761.0, "text": " So these are the arguments and these are the return values, okay?", "tokens": [51214, 407, 613, 366, 264, 12869, 293, 613, 366, 264, 2736, 4190, 11, 1392, 30, 51364], "temperature": 0.0, "avg_logprob": -0.10134684244791667, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.04818175360560417}, {"id": 173, "seek": 74100, "start": 761.0, "end": 764.0, "text": " For calls, for returns.", "tokens": [51364, 1171, 5498, 11, 337, 11247, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10134684244791667, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.04818175360560417}, {"id": 174, "seek": 74100, "start": 764.0, "end": 768.0, "text": " And I will just introduce the other fields as we go.", "tokens": [51514, 400, 286, 486, 445, 5366, 264, 661, 7909, 382, 321, 352, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10134684244791667, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.04818175360560417}, {"id": 175, "seek": 76800, "start": 769.0, "end": 771.0, "text": " Arguments are in brackets.", "tokens": [50414, 40081, 4697, 366, 294, 26179, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1542194086477297, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.06588385999202728}, {"id": 176, "seek": 76800, "start": 771.0, "end": 775.0, "text": " So now trace selection.", "tokens": [50514, 407, 586, 13508, 9450, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1542194086477297, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.06588385999202728}, {"id": 177, "seek": 76800, "start": 775.0, "end": 777.0, "text": " You can do a select.", "tokens": [50714, 509, 393, 360, 257, 3048, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1542194086477297, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.06588385999202728}, {"id": 178, "seek": 76800, "start": 777.0, "end": 782.0, "text": " It's a fancy way of doing this ETS select with ETS from 2MS.", "tokens": [50814, 467, 311, 257, 10247, 636, 295, 884, 341, 462, 7327, 3048, 365, 462, 7327, 490, 568, 10288, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1542194086477297, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.06588385999202728}, {"id": 179, "seek": 76800, "start": 782.0, "end": 785.0, "text": " And let's get all function calls.", "tokens": [51064, 400, 718, 311, 483, 439, 2445, 5498, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1542194086477297, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.06588385999202728}, {"id": 180, "seek": 76800, "start": 785.0, "end": 788.0, "text": " And for each argument, let's just get this argument.", "tokens": [51214, 400, 337, 1184, 6770, 11, 718, 311, 445, 483, 341, 6770, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1542194086477297, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.06588385999202728}, {"id": 181, "seek": 76800, "start": 788.0, "end": 790.0, "text": " So I'm getting a list of arguments.", "tokens": [51364, 407, 286, 478, 1242, 257, 1329, 295, 12869, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1542194086477297, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.06588385999202728}, {"id": 182, "seek": 76800, "start": 790.0, "end": 794.0, "text": " And of course, this is a recursive way of calculating factorial.", "tokens": [51464, 400, 295, 1164, 11, 341, 307, 257, 20560, 488, 636, 295, 28258, 36916, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1542194086477297, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.06588385999202728}, {"id": 183, "seek": 76800, "start": 794.0, "end": 796.0, "text": " So it's 3, 2, 1, 0.", "tokens": [51664, 407, 309, 311, 805, 11, 568, 11, 502, 11, 1958, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1542194086477297, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.06588385999202728}, {"id": 184, "seek": 79600, "start": 796.0, "end": 800.0, "text": " And there is also select slash 2.", "tokens": [50364, 400, 456, 307, 611, 3048, 17330, 568, 13, 50564], "temperature": 0.0, "avg_logprob": -0.126999964316686, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.03514764830470085}, {"id": 185, "seek": 79600, "start": 800.0, "end": 804.0, "text": " And this one takes like any term and looks for that term.", "tokens": [50564, 400, 341, 472, 2516, 411, 604, 1433, 293, 1542, 337, 300, 1433, 13, 50764], "temperature": 0.0, "avg_logprob": -0.126999964316686, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.03514764830470085}, {"id": 186, "seek": 79600, "start": 804.0, "end": 808.0, "text": " So here it found, for example, it has an argument.", "tokens": [50764, 407, 510, 309, 1352, 11, 337, 1365, 11, 309, 575, 364, 6770, 13, 50964], "temperature": 0.0, "avg_logprob": -0.126999964316686, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.03514764830470085}, {"id": 187, "seek": 79600, "start": 808.0, "end": 810.0, "text": " Here it found it as a return value.", "tokens": [50964, 1692, 309, 1352, 309, 382, 257, 2736, 2158, 13, 51064], "temperature": 0.0, "avg_logprob": -0.126999964316686, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.03514764830470085}, {"id": 188, "seek": 79600, "start": 810.0, "end": 812.0, "text": " But there is more.", "tokens": [51064, 583, 456, 307, 544, 13, 51164], "temperature": 0.0, "avg_logprob": -0.126999964316686, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.03514764830470085}, {"id": 189, "seek": 79600, "start": 812.0, "end": 815.0, "text": " It can be hidden inside any lists, maps, tuples.", "tokens": [51164, 467, 393, 312, 7633, 1854, 604, 14511, 11, 11317, 11, 2604, 2622, 13, 51314], "temperature": 0.0, "avg_logprob": -0.126999964316686, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.03514764830470085}, {"id": 190, "seek": 79600, "start": 815.0, "end": 822.0, "text": " So it will look recursively inside your data structures to find out anything you're looking for.", "tokens": [51314, 407, 309, 486, 574, 20560, 3413, 1854, 428, 1412, 9227, 281, 915, 484, 1340, 291, 434, 1237, 337, 13, 51664], "temperature": 0.0, "avg_logprob": -0.126999964316686, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.03514764830470085}, {"id": 191, "seek": 82200, "start": 822.0, "end": 825.0, "text": " So for example, you can look for an error message,", "tokens": [50364, 407, 337, 1365, 11, 291, 393, 574, 337, 364, 6713, 3636, 11, 50514], "temperature": 0.0, "avg_logprob": -0.14053763083691867, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.17553076148033142}, {"id": 192, "seek": 82200, "start": 825.0, "end": 831.0, "text": " even if it's called unknown error, which occurred to me once.", "tokens": [50514, 754, 498, 309, 311, 1219, 9841, 6713, 11, 597, 11068, 281, 385, 1564, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14053763083691867, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.17553076148033142}, {"id": 193, "seek": 82200, "start": 831.0, "end": 833.0, "text": " And I just found this unknown error.", "tokens": [50814, 400, 286, 445, 1352, 341, 9841, 6713, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14053763083691867, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.17553076148033142}, {"id": 194, "seek": 82200, "start": 833.0, "end": 841.0, "text": " I just put unknown error here and I just found the function that that causes, right, instantly.", "tokens": [50914, 286, 445, 829, 9841, 6713, 510, 293, 286, 445, 1352, 264, 2445, 300, 300, 7700, 11, 558, 11, 13518, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14053763083691867, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.17553076148033142}, {"id": 195, "seek": 82200, "start": 841.0, "end": 842.0, "text": " Okay, there is also filter.", "tokens": [51314, 1033, 11, 456, 307, 611, 6608, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14053763083691867, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.17553076148033142}, {"id": 196, "seek": 82200, "start": 842.0, "end": 844.0, "text": " It's similar to select.", "tokens": [51364, 467, 311, 2531, 281, 3048, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14053763083691867, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.17553076148033142}, {"id": 197, "seek": 82200, "start": 844.0, "end": 846.0, "text": " But here you can pass any function.", "tokens": [51464, 583, 510, 291, 393, 1320, 604, 2445, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14053763083691867, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.17553076148033142}, {"id": 198, "seek": 82200, "start": 846.0, "end": 849.0, "text": " It's a bit slower, but it has more features simply.", "tokens": [51564, 467, 311, 257, 857, 14009, 11, 457, 309, 575, 544, 4122, 2935, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14053763083691867, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.17553076148033142}, {"id": 199, "seek": 84900, "start": 849.0, "end": 857.0, "text": " So you can, for example, assign a result to a search, to a variable,", "tokens": [50364, 407, 291, 393, 11, 337, 1365, 11, 6269, 257, 1874, 281, 257, 3164, 11, 281, 257, 7006, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1249006816319057, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.3430160582065582}, {"id": 200, "seek": 84900, "start": 857.0, "end": 859.0, "text": " and then you can search in that list again.", "tokens": [50764, 293, 550, 291, 393, 3164, 294, 300, 1329, 797, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1249006816319057, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.3430160582065582}, {"id": 201, "seek": 84900, "start": 859.0, "end": 860.0, "text": " Oops, sorry.", "tokens": [50864, 21726, 11, 2597, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1249006816319057, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.3430160582065582}, {"id": 202, "seek": 84900, "start": 860.0, "end": 862.0, "text": " Then you can search in that list again.", "tokens": [50914, 1396, 291, 393, 3164, 294, 300, 1329, 797, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1249006816319057, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.3430160582065582}, {"id": 203, "seek": 84900, "start": 862.0, "end": 864.0, "text": " So you can narrow down your search.", "tokens": [51014, 407, 291, 393, 9432, 760, 428, 3164, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1249006816319057, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.3430160582065582}, {"id": 204, "seek": 84900, "start": 864.0, "end": 866.0, "text": " You got like two traces.", "tokens": [51114, 509, 658, 411, 732, 26076, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1249006816319057, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.3430160582065582}, {"id": 205, "seek": 84900, "start": 866.0, "end": 871.0, "text": " Now you search in that two traces, but only for calls and you get only one.", "tokens": [51214, 823, 291, 3164, 294, 300, 732, 26076, 11, 457, 787, 337, 5498, 293, 291, 483, 787, 472, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1249006816319057, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.3430160582065582}, {"id": 206, "seek": 84900, "start": 871.0, "end": 875.0, "text": " So another way to query it.", "tokens": [51464, 407, 1071, 636, 281, 14581, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1249006816319057, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.3430160582065582}, {"id": 207, "seek": 87500, "start": 875.0, "end": 882.0, "text": " And the tracebacks are very important for me because I want to, like, know the source where this originated,", "tokens": [50364, 400, 264, 13508, 17758, 366, 588, 1021, 337, 385, 570, 286, 528, 281, 11, 411, 11, 458, 264, 4009, 689, 341, 31129, 11, 50714], "temperature": 0.0, "avg_logprob": -0.14197538901066434, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.2820427417755127}, {"id": 208, "seek": 87500, "start": 882.0, "end": 885.0, "text": " this particular, for example, function call.", "tokens": [50714, 341, 1729, 11, 337, 1365, 11, 2445, 818, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14197538901066434, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.2820427417755127}, {"id": 209, "seek": 87500, "start": 885.0, "end": 893.0, "text": " So here I'm just looking for any return value of one.", "tokens": [50864, 407, 510, 286, 478, 445, 1237, 337, 604, 2736, 2158, 295, 472, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14197538901066434, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.2820427417755127}, {"id": 210, "seek": 87500, "start": 893.0, "end": 901.0, "text": " And the sleepy factor of one actually matches it and it returned one.", "tokens": [51264, 400, 264, 24908, 5952, 295, 472, 767, 10676, 309, 293, 309, 8752, 472, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14197538901066434, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.2820427417755127}, {"id": 211, "seek": 90100, "start": 901.0, "end": 905.0, "text": " So this is returned, the traceback of this one.", "tokens": [50364, 407, 341, 307, 8752, 11, 264, 13508, 3207, 295, 341, 472, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1341003312004937, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.2970442473888397}, {"id": 212, "seek": 90100, "start": 905.0, "end": 907.0, "text": " The call itself is first.", "tokens": [50564, 440, 818, 2564, 307, 700, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1341003312004937, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.2970442473888397}, {"id": 213, "seek": 90100, "start": 907.0, "end": 908.0, "text": " It's right here.", "tokens": [50664, 467, 311, 558, 510, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1341003312004937, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.2970442473888397}, {"id": 214, "seek": 90100, "start": 908.0, "end": 912.0, "text": " Sleepy factor of one and the rest is just the traceback.", "tokens": [50714, 19383, 88, 5952, 295, 472, 293, 264, 1472, 307, 445, 264, 13508, 3207, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1341003312004937, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.2970442473888397}, {"id": 215, "seek": 90100, "start": 912.0, "end": 921.0, "text": " And the sleepy factor of zero returned also one, but it skipped because of some skipping logic.", "tokens": [50914, 400, 264, 24908, 5952, 295, 4018, 8752, 611, 472, 11, 457, 309, 30193, 570, 295, 512, 31533, 9952, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1341003312004937, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.2970442473888397}, {"id": 216, "seek": 90100, "start": 921.0, "end": 926.0, "text": " Yeah, it's details, but it helps you, like, limit the output that you get.", "tokens": [51364, 865, 11, 309, 311, 4365, 11, 457, 309, 3665, 291, 11, 411, 11, 4948, 264, 5598, 300, 291, 483, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1341003312004937, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.2970442473888397}, {"id": 217, "seek": 92600, "start": 926.0, "end": 938.0, "text": " Actually, you can disable that and you can get, like, all the traces with output all.", "tokens": [50364, 5135, 11, 291, 393, 28362, 300, 293, 291, 393, 483, 11, 411, 11, 439, 264, 26076, 365, 5598, 439, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15324461460113525, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.3158593475818634}, {"id": 218, "seek": 92600, "start": 938.0, "end": 945.0, "text": " Then you get, then you have no skipping of traceback that include another tracebacks.", "tokens": [50964, 1396, 291, 483, 11, 550, 291, 362, 572, 31533, 295, 13508, 3207, 300, 4090, 1071, 13508, 17758, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15324461460113525, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.3158593475818634}, {"id": 219, "seek": 92600, "start": 945.0, "end": 948.0, "text": " And you can limit the number of much traces.", "tokens": [51314, 400, 291, 393, 4948, 264, 1230, 295, 709, 26076, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15324461460113525, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.3158593475818634}, {"id": 220, "seek": 92600, "start": 948.0, "end": 951.0, "text": " You can reverse call order.", "tokens": [51464, 509, 393, 9943, 818, 1668, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15324461460113525, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.3158593475818634}, {"id": 221, "seek": 95100, "start": 951.0, "end": 956.0, "text": " You can search in a different table, in a list, for example.", "tokens": [50364, 509, 393, 3164, 294, 257, 819, 3199, 11, 294, 257, 1329, 11, 337, 1365, 13, 50614], "temperature": 0.0, "avg_logprob": -0.157946787382427, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.1438618153333664}, {"id": 222, "seek": 95100, "start": 956.0, "end": 964.0, "text": " And you can get only the first traceback if you want, very useful, just shortcut, let's say.", "tokens": [50614, 400, 291, 393, 483, 787, 264, 700, 13508, 3207, 498, 291, 528, 11, 588, 4420, 11, 445, 24822, 11, 718, 311, 584, 13, 51014], "temperature": 0.0, "avg_logprob": -0.157946787382427, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.1438618153333664}, {"id": 223, "seek": 95100, "start": 964.0, "end": 977.0, "text": " And you can also get it for a particular record or just an index of a trace because there has just this auto-incremented indexes.", "tokens": [51014, 400, 291, 393, 611, 483, 309, 337, 257, 1729, 2136, 420, 445, 364, 8186, 295, 257, 13508, 570, 456, 575, 445, 341, 8399, 12, 4647, 265, 14684, 8186, 279, 13, 51664], "temperature": 0.0, "avg_logprob": -0.157946787382427, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.1438618153333664}, {"id": 224, "seek": 97700, "start": 978.0, "end": 984.0, "text": " Yeah, and similar to a traceback, you have ranges and ranges look inside.", "tokens": [50414, 865, 11, 293, 2531, 281, 257, 13508, 3207, 11, 291, 362, 22526, 293, 22526, 574, 1854, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13930841114210046, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.065176822245121}, {"id": 225, "seek": 97700, "start": 984.0, "end": 993.0, "text": " So traceback is like what's the source and ranges give you, like, all the traces starting with a function call until it's returned.", "tokens": [50714, 407, 13508, 3207, 307, 411, 437, 311, 264, 4009, 293, 22526, 976, 291, 11, 411, 11, 439, 264, 26076, 2891, 365, 257, 2445, 818, 1826, 309, 311, 8752, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13930841114210046, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.065176822245121}, {"id": 226, "seek": 97700, "start": 993.0, "end": 995.0, "text": " Everything in between, from one process.", "tokens": [51164, 5471, 294, 1296, 11, 490, 472, 1399, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13930841114210046, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.065176822245121}, {"id": 227, "seek": 97700, "start": 995.0, "end": 1006.0, "text": " And, yeah, so for example, here we are really looking for any traces that are just for function calls that have one as an argument", "tokens": [51264, 400, 11, 1338, 11, 370, 337, 1365, 11, 510, 321, 366, 534, 1237, 337, 604, 26076, 300, 366, 445, 337, 2445, 5498, 300, 362, 472, 382, 364, 6770, 51814], "temperature": 0.0, "avg_logprob": -0.13930841114210046, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.065176822245121}, {"id": 228, "seek": 100600, "start": 1006.0, "end": 1011.0, "text": " and you get a range of traces from the call until the return.", "tokens": [50364, 293, 291, 483, 257, 3613, 295, 26076, 490, 264, 818, 1826, 264, 2736, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13441706839061918, "compression_ratio": 1.71, "no_speech_prob": 0.09920982271432877}, {"id": 229, "seek": 100600, "start": 1011.0, "end": 1019.0, "text": " A range options, you can get, like, limit call depth is quite interesting and very useful", "tokens": [50614, 316, 3613, 3956, 11, 291, 393, 483, 11, 411, 11, 4948, 818, 7161, 307, 1596, 1880, 293, 588, 4420, 51014], "temperature": 0.0, "avg_logprob": -0.13441706839061918, "compression_ratio": 1.71, "no_speech_prob": 0.09920982271432877}, {"id": 230, "seek": 100600, "start": 1019.0, "end": 1024.0, "text": " because by having one you just get a call and return, which is very useful.", "tokens": [51014, 570, 538, 1419, 472, 291, 445, 483, 257, 818, 293, 2736, 11, 597, 307, 588, 4420, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13441706839061918, "compression_ratio": 1.71, "no_speech_prob": 0.09920982271432877}, {"id": 231, "seek": 100600, "start": 1024.0, "end": 1035.0, "text": " And searching in a list of traces is also possible, getting only the first range if there are many, also possible.", "tokens": [51264, 400, 10808, 294, 257, 1329, 295, 26076, 307, 611, 1944, 11, 1242, 787, 264, 700, 3613, 498, 456, 366, 867, 11, 611, 1944, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13441706839061918, "compression_ratio": 1.71, "no_speech_prob": 0.09920982271432877}, {"id": 232, "seek": 103500, "start": 1035.0, "end": 1038.0, "text": " And getting the range for a specific trace.", "tokens": [50364, 400, 1242, 264, 3613, 337, 257, 2685, 13508, 13, 50514], "temperature": 0.0, "avg_logprob": -0.12049374087103482, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.15667007863521576}, {"id": 233, "seek": 103500, "start": 1038.0, "end": 1040.0, "text": " So quite a lot of options.", "tokens": [50514, 407, 1596, 257, 688, 295, 3956, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12049374087103482, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.15667007863521576}, {"id": 234, "seek": 103500, "start": 1040.0, "end": 1046.0, "text": " I've just, you know, added, I've been adding and adding over a few years of development of these two.", "tokens": [50614, 286, 600, 445, 11, 291, 458, 11, 3869, 11, 286, 600, 668, 5127, 293, 5127, 670, 257, 1326, 924, 295, 3250, 295, 613, 732, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12049374087103482, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.15667007863521576}, {"id": 235, "seek": 103500, "start": 1046.0, "end": 1049.0, "text": " So they're all quite useful.", "tokens": [50914, 407, 436, 434, 439, 1596, 4420, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12049374087103482, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.15667007863521576}, {"id": 236, "seek": 103500, "start": 1049.0, "end": 1053.0, "text": " Utilities, two simple utilities they wanted to talk about.", "tokens": [51064, 12555, 3064, 11, 732, 2199, 30482, 436, 1415, 281, 751, 466, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12049374087103482, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.15667007863521576}, {"id": 237, "seek": 103500, "start": 1053.0, "end": 1055.0, "text": " One is to just look up a trace.", "tokens": [51264, 1485, 307, 281, 445, 574, 493, 257, 13508, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12049374087103482, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.15667007863521576}, {"id": 238, "seek": 103500, "start": 1055.0, "end": 1059.0, "text": " Nothing fancy here, ETS lookup, does it, right?", "tokens": [51364, 6693, 10247, 510, 11, 462, 7327, 574, 1010, 11, 775, 309, 11, 558, 30, 51564], "temperature": 0.0, "avg_logprob": -0.12049374087103482, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.15667007863521576}, {"id": 239, "seek": 103500, "start": 1059.0, "end": 1063.0, "text": " But then you can execute the trace, which is quite useful for me.", "tokens": [51564, 583, 550, 291, 393, 14483, 264, 13508, 11, 597, 307, 1596, 4420, 337, 385, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12049374087103482, "compression_ratio": 1.573643410852713, "no_speech_prob": 0.15667007863521576}, {"id": 240, "seek": 106300, "start": 1063.0, "end": 1067.0, "text": " So if this was a function call, I can just execute it right now, again.", "tokens": [50364, 407, 498, 341, 390, 257, 2445, 818, 11, 286, 393, 445, 14483, 309, 558, 586, 11, 797, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1080017479098573, "compression_ratio": 1.6267942583732058, "no_speech_prob": 0.03734096884727478}, {"id": 241, "seek": 106300, "start": 1067.0, "end": 1073.0, "text": " This is just, for example, let's say I fix the bug and then instead of writing some long code,", "tokens": [50564, 639, 307, 445, 11, 337, 1365, 11, 718, 311, 584, 286, 3191, 264, 7426, 293, 550, 2602, 295, 3579, 512, 938, 3089, 11, 50864], "temperature": 0.0, "avg_logprob": -0.1080017479098573, "compression_ratio": 1.6267942583732058, "no_speech_prob": 0.03734096884727478}, {"id": 242, "seek": 106300, "start": 1073.0, "end": 1078.0, "text": " I can just execute a trace and see if the result is the same or different.", "tokens": [50864, 286, 393, 445, 14483, 257, 13508, 293, 536, 498, 264, 1874, 307, 264, 912, 420, 819, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1080017479098573, "compression_ratio": 1.6267942583732058, "no_speech_prob": 0.03734096884727478}, {"id": 243, "seek": 106300, "start": 1078.0, "end": 1081.0, "text": " Or I can trace again, right?", "tokens": [51114, 1610, 286, 393, 13508, 797, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.1080017479098573, "compression_ratio": 1.6267942583732058, "no_speech_prob": 0.03734096884727478}, {"id": 244, "seek": 106300, "start": 1081.0, "end": 1084.0, "text": " I can start the traces and trace again.", "tokens": [51264, 286, 393, 722, 264, 26076, 293, 13508, 797, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1080017479098573, "compression_ratio": 1.6267942583732058, "no_speech_prob": 0.03734096884727478}, {"id": 245, "seek": 106300, "start": 1084.0, "end": 1086.0, "text": " Okay, now a bit of profiling.", "tokens": [51414, 1033, 11, 586, 257, 857, 295, 1740, 4883, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1080017479098573, "compression_ratio": 1.6267942583732058, "no_speech_prob": 0.03734096884727478}, {"id": 246, "seek": 108600, "start": 1086.0, "end": 1093.0, "text": " So I find this lightweight profiling very useful because it doesn't put as much stress on the system", "tokens": [50364, 407, 286, 915, 341, 22052, 1740, 4883, 588, 4420, 570, 309, 1177, 380, 829, 382, 709, 4244, 322, 264, 1185, 50714], "temperature": 0.0, "avg_logprob": -0.1535121254298998, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.3368007242679596}, {"id": 247, "seek": 108600, "start": 1093.0, "end": 1096.0, "text": " as F-Prof, for example, the Erlang profiler.", "tokens": [50714, 382, 479, 12, 12681, 69, 11, 337, 1365, 11, 264, 3300, 25241, 1740, 5441, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1535121254298998, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.3368007242679596}, {"id": 248, "seek": 108600, "start": 1096.0, "end": 1099.0, "text": " And it's like instantly available.", "tokens": [50864, 400, 309, 311, 411, 13518, 2435, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1535121254298998, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.3368007242679596}, {"id": 249, "seek": 108600, "start": 1099.0, "end": 1102.0, "text": " I don't have to prepare for it in any way.", "tokens": [51014, 286, 500, 380, 362, 281, 5940, 337, 309, 294, 604, 636, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1535121254298998, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.3368007242679596}, {"id": 250, "seek": 108600, "start": 1102.0, "end": 1108.0, "text": " So call start, it's statistics aggregated by this function.", "tokens": [51164, 407, 818, 722, 11, 309, 311, 12523, 16743, 770, 538, 341, 2445, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1535121254298998, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.3368007242679596}, {"id": 251, "seek": 108600, "start": 1108.0, "end": 1112.0, "text": " So I'm aggregating everything under the total atom.", "tokens": [51464, 407, 286, 478, 16743, 990, 1203, 833, 264, 3217, 12018, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1535121254298998, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.3368007242679596}, {"id": 252, "seek": 111200, "start": 1112.0, "end": 1116.0, "text": " So I'm getting like four calls and this accumulated time and this own time.", "tokens": [50364, 407, 286, 478, 1242, 411, 1451, 5498, 293, 341, 31346, 565, 293, 341, 1065, 565, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09868282632729442, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.08512821793556213}, {"id": 253, "seek": 111200, "start": 1116.0, "end": 1121.0, "text": " These are equal because I'm just accumulating everything.", "tokens": [50564, 1981, 366, 2681, 570, 286, 478, 445, 12989, 12162, 1203, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09868282632729442, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.08512821793556213}, {"id": 254, "seek": 111200, "start": 1121.0, "end": 1128.0, "text": " But if I aggregated by function argument, you can see that there was this one call with each of the arguments.", "tokens": [50814, 583, 498, 286, 16743, 770, 538, 2445, 6770, 11, 291, 393, 536, 300, 456, 390, 341, 472, 818, 365, 1184, 295, 264, 12869, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09868282632729442, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.08512821793556213}, {"id": 255, "seek": 111200, "start": 1128.0, "end": 1132.0, "text": " And this call took the longest time, but actually it's accumulated time", "tokens": [51164, 400, 341, 818, 1890, 264, 15438, 565, 11, 457, 767, 309, 311, 31346, 565, 51364], "temperature": 0.0, "avg_logprob": -0.09868282632729442, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.08512821793556213}, {"id": 256, "seek": 111200, "start": 1132.0, "end": 1136.0, "text": " because its own time was the shortest section, right?", "tokens": [51364, 570, 1080, 1065, 565, 390, 264, 31875, 3541, 11, 558, 30, 51564], "temperature": 0.0, "avg_logprob": -0.09868282632729442, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.08512821793556213}, {"id": 257, "seek": 111200, "start": 1136.0, "end": 1139.0, "text": " You can also do filtering here.", "tokens": [51564, 509, 393, 611, 360, 30822, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09868282632729442, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.08512821793556213}, {"id": 258, "seek": 113900, "start": 1139.0, "end": 1144.0, "text": " So you can say when N is smaller than 3 and we just skipped one of them.", "tokens": [50364, 407, 291, 393, 584, 562, 426, 307, 4356, 813, 805, 293, 321, 445, 30193, 472, 295, 552, 13, 50614], "temperature": 0.0, "avg_logprob": -0.16213506307357398, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.10520869493484497}, {"id": 259, "seek": 113900, "start": 1144.0, "end": 1150.0, "text": " So you can do that and you can sort them.", "tokens": [50614, 407, 291, 393, 360, 300, 293, 291, 393, 1333, 552, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16213506307357398, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.10520869493484497}, {"id": 260, "seek": 113900, "start": 1150.0, "end": 1154.0, "text": " Yes, you can sort them and print them as a table.", "tokens": [50914, 1079, 11, 291, 393, 1333, 552, 293, 4482, 552, 382, 257, 3199, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16213506307357398, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.10520869493484497}, {"id": 261, "seek": 113900, "start": 1154.0, "end": 1158.0, "text": " We had some just nice utilities to have.", "tokens": [51114, 492, 632, 512, 445, 1481, 30482, 281, 362, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16213506307357398, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.10520869493484497}, {"id": 262, "seek": 113900, "start": 1158.0, "end": 1163.0, "text": " And the last feature I wanted to talk about is function call 3 statistics.", "tokens": [51314, 400, 264, 1036, 4111, 286, 1415, 281, 751, 466, 307, 2445, 818, 805, 12523, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16213506307357398, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.10520869493484497}, {"id": 263, "seek": 116300, "start": 1163.0, "end": 1171.0, "text": " I called it like that because let's say we have a function that calculates the Fibonacci sequence", "tokens": [50364, 286, 1219, 309, 411, 300, 570, 718, 311, 584, 321, 362, 257, 2445, 300, 4322, 1024, 264, 479, 897, 266, 43870, 8310, 50764], "temperature": 0.0, "avg_logprob": -0.19499121516583914, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.11117834597826004}, {"id": 264, "seek": 116300, "start": 1171.0, "end": 1176.0, "text": " in a suboptimal way, you probably all know that it's suboptimal.", "tokens": [50764, 294, 257, 1422, 5747, 10650, 636, 11, 291, 1391, 439, 458, 300, 309, 311, 1422, 5747, 10650, 13, 51014], "temperature": 0.0, "avg_logprob": -0.19499121516583914, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.11117834597826004}, {"id": 265, "seek": 116300, "start": 1176.0, "end": 1182.0, "text": " It branches a lot and let's clean up the traces.", "tokens": [51014, 467, 14770, 257, 688, 293, 718, 311, 2541, 493, 264, 26076, 13, 51314], "temperature": 0.0, "avg_logprob": -0.19499121516583914, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.11117834597826004}, {"id": 266, "seek": 116300, "start": 1182.0, "end": 1189.0, "text": " Trace again, call fib of 4 which returns 3, which is the correct value and stop tracing.", "tokens": [51314, 1765, 617, 797, 11, 818, 13116, 295, 1017, 597, 11247, 805, 11, 597, 307, 264, 3006, 2158, 293, 1590, 25262, 13, 51664], "temperature": 0.0, "avg_logprob": -0.19499121516583914, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.11117834597826004}, {"id": 267, "seek": 118900, "start": 1189.0, "end": 1194.0, "text": " So we now have different traces in the table and let's do it.", "tokens": [50364, 407, 321, 586, 362, 819, 26076, 294, 264, 3199, 293, 718, 311, 360, 309, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1302720878435218, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.06771627813577652}, {"id": 268, "seek": 118900, "start": 1194.0, "end": 1197.0, "text": " Let's just call this function with default arguments.", "tokens": [50614, 961, 311, 445, 818, 341, 2445, 365, 7576, 12869, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1302720878435218, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.06771627813577652}, {"id": 269, "seek": 118900, "start": 1197.0, "end": 1206.0, "text": " So it says that there is a call 3, I mean by that function calls, returns, everything inside repeated twice", "tokens": [50764, 407, 309, 1619, 300, 456, 307, 257, 818, 805, 11, 286, 914, 538, 300, 2445, 5498, 11, 11247, 11, 1203, 1854, 10477, 6091, 51214], "temperature": 0.0, "avg_logprob": -0.1302720878435218, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.06771627813577652}, {"id": 270, "seek": 118900, "start": 1206.0, "end": 1212.0, "text": " because there is this number 2 and it took 10 microseconds that there is no sleep in this example.", "tokens": [51214, 570, 456, 307, 341, 1230, 568, 293, 309, 1890, 1266, 3123, 37841, 28750, 300, 456, 307, 572, 2817, 294, 341, 1365, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1302720878435218, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.06771627813577652}, {"id": 271, "seek": 118900, "start": 1212.0, "end": 1215.0, "text": " So it took 10 microseconds in total.", "tokens": [51514, 407, 309, 1890, 1266, 3123, 37841, 28750, 294, 3217, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1302720878435218, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.06771627813577652}, {"id": 272, "seek": 121500, "start": 1215.0, "end": 1218.0, "text": " And this is how the function call 3 looks like.", "tokens": [50364, 400, 341, 307, 577, 264, 2445, 818, 805, 1542, 411, 13, 50514], "temperature": 0.0, "avg_logprob": -0.12129651720278731, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.0490262545645237}, {"id": 273, "seek": 121500, "start": 1218.0, "end": 1221.0, "text": " So you can see that, yeah, indeed, it repeated twice.", "tokens": [50514, 407, 291, 393, 536, 300, 11, 1338, 11, 6451, 11, 309, 10477, 6091, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12129651720278731, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.0490262545645237}, {"id": 274, "seek": 121500, "start": 1221.0, "end": 1226.0, "text": " So this can help you find out redundant code.", "tokens": [50664, 407, 341, 393, 854, 291, 915, 484, 40997, 3089, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12129651720278731, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.0490262545645237}, {"id": 275, "seek": 121500, "start": 1226.0, "end": 1227.0, "text": " Yeah.", "tokens": [50914, 865, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12129651720278731, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.0490262545645237}, {"id": 276, "seek": 121500, "start": 1227.0, "end": 1232.0, "text": " Okay, so this function also has some options but I don't have time to talk about them.", "tokens": [50964, 1033, 11, 370, 341, 2445, 611, 575, 512, 3956, 457, 286, 500, 380, 362, 565, 281, 751, 466, 552, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12129651720278731, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.0490262545645237}, {"id": 277, "seek": 121500, "start": 1232.0, "end": 1235.0, "text": " You can just customize them a bit.", "tokens": [51214, 509, 393, 445, 19734, 552, 257, 857, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12129651720278731, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.0490262545645237}, {"id": 278, "seek": 121500, "start": 1235.0, "end": 1244.0, "text": " And table manipulation, you can get the current table, dump it to a file and load it on a different Erlang node.", "tokens": [51364, 400, 3199, 26475, 11, 291, 393, 483, 264, 2190, 3199, 11, 11430, 309, 281, 257, 3991, 293, 3677, 309, 322, 257, 819, 3300, 25241, 9984, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12129651720278731, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.0490262545645237}, {"id": 279, "seek": 124400, "start": 1244.0, "end": 1247.0, "text": " And then you can continue analysis on a different Erlang node.", "tokens": [50364, 400, 550, 291, 393, 2354, 5215, 322, 257, 819, 3300, 25241, 9984, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09306300821758452, "compression_ratio": 1.2920353982300885, "no_speech_prob": 0.2195618450641632}, {"id": 280, "seek": 124400, "start": 1247.0, "end": 1250.0, "text": " And that's all I wanted to talk about.", "tokens": [50514, 400, 300, 311, 439, 286, 1415, 281, 751, 466, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09306300821758452, "compression_ratio": 1.2920353982300885, "no_speech_prob": 0.2195618450641632}, {"id": 281, "seek": 124400, "start": 1250.0, "end": 1252.0, "text": " And that's me on a mountain bike. Thank you.", "tokens": [50664, 400, 300, 311, 385, 322, 257, 6937, 5656, 13, 1044, 291, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09306300821758452, "compression_ratio": 1.2920353982300885, "no_speech_prob": 0.2195618450641632}], "language": "en"}