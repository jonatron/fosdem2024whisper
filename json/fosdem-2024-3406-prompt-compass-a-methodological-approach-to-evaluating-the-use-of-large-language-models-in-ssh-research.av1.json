{"text": " Good morning everybody. I am Eric Borra. I am an assistant professor in journalism and new media at the University of Amsterdam. I have a background in artificial intelligence and I have been a tool maker with the Digital Medicine Initiative for about two decades now. And one of the reasons why I make tools is to understand new technologies. And today I will talk about large language models and particularly their use in social science and humanities research. So who of you has used chatGPT? I think most of the Western world actually has used chatGPT in the last year. ChatGPT is based on a large language model where you ask a question and get an answer. But social science and humanities researchers have also found that you can just send it instructions. And you can instruct it to do all kinds of things such as sentiment analysis. So you prompt. It's the way of interacting with large language models. You specify a prompt and you say classify this tweet. You input a tweet and it will give you a nice classification of that tweet. You can also extract entities and actors. You can include topics and teams. So you have the prompt I just showed. You enter a New York Times article and it will extract all these things for you. It will extract country names, organization names, people names, specific teams and topics. And it's actually pretty good at this. At least chatGPT is. And I'll discuss other models in a second. And it's not just named entity extraction or simple classification. It also, like, somehow extracts teams and it can abstract from the text. And researchers have been using this, for example, to extract narratives from posts. So here's a very complex prompt, which you can see in the slides after the talk. But there they use this prompt to go through many, many Reddit posts, hundreds of thousands, to find out whether there were any conspiracy theories in there. And they devised a prompt to draw out these narratives. And they actually found that LLMs worked really well in extracting conspiracy theories as well. So researchers have been using this and they have been looking at all the tasks that are typically done in social science and humanities and are starting to test whether LLMs can help in doing these tasks. And there has been a lot of research in the last year, especially 2023. This is just a really small snippet of this research. But this research also comes with problems, which I'll touch upon in a bit. But their use is also understandable, the use of large language models in social science and humanities research, because they seem to ease and speed up previously difficult and laborious tasks, such as classification, extraction summarization, and so forth. And they're actually employed as junior research assistants. Now, while this may seem useful somehow, a lot of people seem to be using chat GPT. Actually, all the papers I've just shown, they're based on chat GPT. And chat GPT comes with problems, because it's a platform service. And platform services, as I guess all of you know, are volatile black boxes. You don't know what's going on in the back end when they update their model, when they align something differently or sensor or whatever, whether it's getting dumber or not. You basically don't know. Chat GPT is also very expensive. If you're using the API, you pay for a request. This is one research project by Miguel Escobar of Arela, who calculated that to process the one and a half million news items he had in his corpus. He'd need $150,000. It's just too expensive. There are, of course, also privacy concerns with chat GPT and other platform models, whereby with chat GPT we know that whatever you input into chat GPT is also used as training data for the LLM. Users have also found personal and private information resurfacing from other users, etc. So if you think in terms of open science, replicable research and ethical research with privacy concerns, you basically cannot use these models, even though you can go to privacy.openai.com and state that you don't want your inputs to be used as training data. Well, so how to deal with this? Can we use LLMs in social science and humanities research? Fortunately, the answer is yes. Chat GPT is not the only model available. You probably heard of Google Bart or Gemini. You may have heard of Clodes, which are other platform models, but there are also a lot of publicly available models. All the yellow ones highlighted here, and this is only, I think, until the second quarter of 2023. Since then, a lot of new models have appeared, most notably Mistral, for example, the French model, or Mistral, the 8x7B model of Mistral, which are really good and are catching up on the performance of chat GPT. Publicly available, however, doesn't mean that it's open, that it's open source, that it's free, because there is this whole infrastructure and apparatus to train models, to fine tune models, to use models in your own work. And you see all the orange and red here. Most of these models aren't open, or have different licenses, etc. So it's not, yeah, you can't just use another one. You need to think about these things. Two other considerations before I go to the actual tool. If you use the same prompt in different models, you'll get different results. And this is actually the same prompt in a series of image models, but you can visually see how results may differ. This is something to take into account. And last but not least, there are a few technical parameters in using LLMs. And one way to control differences or variability in the output of LLMs is through the so-called temperature parameter. If you set the temperature to zero, you'll always get the same result. The most probable or most likely outcome was if you increase the temperature, there is a chance of less likely outputs to be included in the results. But again, all these papers, none of them mention temperature, whilst it's a very important parameter. Last but not least, this is work I've been doing with Maichieu, small syntactic differences in a query that's semantically the same may lead to different results as well. So you need to test your prompt for robustness or consistency. So summarizing, open AI or platforms like chat GPT are volatile black boxes that cost a lot of money. There are issues of privacy and security. There are different models which have different licenses, which have different results. LLMs are not deterministic and small changes in prompts may lead to different outputs. So we need research interfaces where we can control for such things. We want to be able to do open science with LLMs. So how do we take into account the volatility of platforms, the robustness of research and its replicability and explainability? And this is where I started tinkering with a tool I called prompt compass, or actually I had chat GPT call it prompt compass. And it's a research interface. It's not a chat interface. It allows you to take into account all these considerations that I've put up. So you can choose various local models. It has default parameters for replicability. It contains a library of research prompts, allows for batch processing user input, and allows you to evaluate prompt model combinations. Do I still have some time to demo this? Cool. Let's do that. So prompt compass is available on GitHub. We also run it at one of our servers. The design doesn't really shine on this beamer, but anyway. Here you can select various models which are loaded from hugging face. You can easily add a new model and select one of these. You can find out more by clicking on the model card and then see what the model was made for, how it was trained and so forth. All these models are loaded from hugging face, which is like the GitHub for language models, but we can also choose GPT for where you and then or any other model of open AI or platforms and then enter your API key and go over that. You can go into the settings which are default sets to replicability. There's a little explanation of it. There are a lot of prompts extracted from the literature and from actual research. And you can input your own prompts like this. You can or you can adapt existing prompts. And then you can provide user input either line by line or upload a CSV. And then if you click submit, the selected model will be loaded. And each of the lines will be run through the model with the indicated prompt. So in this time we chose sentiment analysis, which says you're in advanced classifying AI, you're tasked with classifying the sentiment of a text, which can be either positive, negative or neutral. And this is where we'll input or loop over our inputs. So in this time the user is happy, it's classified as positive. When user is just a user, it's classified as neutral and the other user is a liar, it's classified as negative. And this tool is not the end all go all tool for working with LLMs, but it is a way to test models, to test parameters, to test prompts, to test the robustness of prompts and to get all this into easily digestible outputs CSV. So far for the demo. The technology is used, it's really simple. I'm not like a hardcore coder, but I'm like more of a tie some stuff together coder. Streamlit is a Python interface for making easily making web applications of machine learning tasks. Lang chain is a very bloated way to easily connect LLMs and to work with LLMs and prompts and Huckingface is the place where all these LLMs are stored. We run this on a 24 gigabyte GPU, which is a bit expensive, but it's not very expensive. Like each research group should be able to get one. And I mean, yeah, so to get back to my rent against platforms, making LLMs locally accessible makes them stable and replicable. But we cannot run the biggest models unless we have access to bigger infrastructure, which we sometimes have. But this is really meant for researchers that want easy access to local models. I made a video tutorial, tutorial which you may want to watch. And maybe there's still room for questions. Just three minutes. Atlas TI is a rather big and well-known software package for qualitative coding, right? I'm not sure why they chose to only use Chatchi PT. But yeah, I mean, we've had experience with local LLMs that you can actually also do similar things with extraction and coding. So I would definitely be in favor of actually using local LLMs. On the other hand, if you have proper validation procedures such as intercoder reliability and F1 scores, etc., you can get a long way with Chatchi PT because human coders are also fallible and may also be different today than they were a few weeks ago. So it's not that it's not possible or not usable at all, but you should be prudent, I think. You said it's mostly for testing the models, but how big of an input file do you think? We've run this on more than 100,000 lines of CSV, I think even more. So in the digital methods winter school and past summer school, we actually run a lot of prompts through it. And it seemed to work. Sorry. It was asked how big of CSV files it could handle, and I answered more than 100,000. So it's actually also used in production for relatively small-scale qualitative research, but it's not limited to things you could do manually anyway. So let's switch.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.64, "text": " Good morning everybody. I am Eric Borra. I am an assistant professor in journalism and", "tokens": [50364, 2205, 2446, 2201, 13, 286, 669, 9336, 13739, 424, 13, 286, 669, 364, 10994, 8304, 294, 23191, 293, 50846], "temperature": 0.0, "avg_logprob": -0.23181976416172126, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.2089592069387436}, {"id": 1, "seek": 0, "start": 9.64, "end": 14.120000000000001, "text": " new media at the University of Amsterdam. I have a background in artificial intelligence", "tokens": [50846, 777, 3021, 412, 264, 3535, 295, 28291, 13, 286, 362, 257, 3678, 294, 11677, 7599, 51070], "temperature": 0.0, "avg_logprob": -0.23181976416172126, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.2089592069387436}, {"id": 2, "seek": 0, "start": 14.120000000000001, "end": 20.44, "text": " and I have been a tool maker with the Digital Medicine Initiative for about two decades now.", "tokens": [51070, 293, 286, 362, 668, 257, 2290, 17127, 365, 264, 15522, 20338, 26166, 337, 466, 732, 7878, 586, 13, 51386], "temperature": 0.0, "avg_logprob": -0.23181976416172126, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.2089592069387436}, {"id": 3, "seek": 0, "start": 20.44, "end": 25.72, "text": " And one of the reasons why I make tools is to understand new technologies. And today", "tokens": [51386, 400, 472, 295, 264, 4112, 983, 286, 652, 3873, 307, 281, 1223, 777, 7943, 13, 400, 965, 51650], "temperature": 0.0, "avg_logprob": -0.23181976416172126, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.2089592069387436}, {"id": 4, "seek": 2572, "start": 25.72, "end": 31.28, "text": " I will talk about large language models and particularly their use in social science and", "tokens": [50364, 286, 486, 751, 466, 2416, 2856, 5245, 293, 4098, 641, 764, 294, 2093, 3497, 293, 50642], "temperature": 0.0, "avg_logprob": -0.15508217758007264, "compression_ratio": 1.7268518518518519, "no_speech_prob": 0.11528433859348297}, {"id": 5, "seek": 2572, "start": 31.28, "end": 39.12, "text": " humanities research. So who of you has used chatGPT? I think most of the Western world actually", "tokens": [50642, 36140, 2132, 13, 407, 567, 295, 291, 575, 1143, 5081, 38, 47, 51, 30, 286, 519, 881, 295, 264, 8724, 1002, 767, 51034], "temperature": 0.0, "avg_logprob": -0.15508217758007264, "compression_ratio": 1.7268518518518519, "no_speech_prob": 0.11528433859348297}, {"id": 6, "seek": 2572, "start": 39.12, "end": 45.92, "text": " has used chatGPT in the last year. ChatGPT is based on a large language model where you ask a", "tokens": [51034, 575, 1143, 5081, 38, 47, 51, 294, 264, 1036, 1064, 13, 27503, 38, 47, 51, 307, 2361, 322, 257, 2416, 2856, 2316, 689, 291, 1029, 257, 51374], "temperature": 0.0, "avg_logprob": -0.15508217758007264, "compression_ratio": 1.7268518518518519, "no_speech_prob": 0.11528433859348297}, {"id": 7, "seek": 2572, "start": 45.92, "end": 51.599999999999994, "text": " question and get an answer. But social science and humanities researchers have also found that", "tokens": [51374, 1168, 293, 483, 364, 1867, 13, 583, 2093, 3497, 293, 36140, 10309, 362, 611, 1352, 300, 51658], "temperature": 0.0, "avg_logprob": -0.15508217758007264, "compression_ratio": 1.7268518518518519, "no_speech_prob": 0.11528433859348297}, {"id": 8, "seek": 5160, "start": 51.6, "end": 58.4, "text": " you can just send it instructions. And you can instruct it to do all kinds of things such as", "tokens": [50364, 291, 393, 445, 2845, 309, 9415, 13, 400, 291, 393, 7232, 309, 281, 360, 439, 3685, 295, 721, 1270, 382, 50704], "temperature": 0.0, "avg_logprob": -0.16395788842981512, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.03695625066757202}, {"id": 9, "seek": 5160, "start": 58.4, "end": 64.0, "text": " sentiment analysis. So you prompt. It's the way of interacting with large language models. You", "tokens": [50704, 16149, 5215, 13, 407, 291, 12391, 13, 467, 311, 264, 636, 295, 18017, 365, 2416, 2856, 5245, 13, 509, 50984], "temperature": 0.0, "avg_logprob": -0.16395788842981512, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.03695625066757202}, {"id": 10, "seek": 5160, "start": 64.0, "end": 69.36, "text": " specify a prompt and you say classify this tweet. You input a tweet and it will give you a nice", "tokens": [50984, 16500, 257, 12391, 293, 291, 584, 33872, 341, 15258, 13, 509, 4846, 257, 15258, 293, 309, 486, 976, 291, 257, 1481, 51252], "temperature": 0.0, "avg_logprob": -0.16395788842981512, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.03695625066757202}, {"id": 11, "seek": 5160, "start": 69.36, "end": 76.12, "text": " classification of that tweet. You can also extract entities and actors. You can include topics and", "tokens": [51252, 21538, 295, 300, 15258, 13, 509, 393, 611, 8947, 16667, 293, 10037, 13, 509, 393, 4090, 8378, 293, 51590], "temperature": 0.0, "avg_logprob": -0.16395788842981512, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.03695625066757202}, {"id": 12, "seek": 7612, "start": 76.2, "end": 82.76, "text": " teams. So you have the prompt I just showed. You enter a New York Times article and it will", "tokens": [50368, 5491, 13, 407, 291, 362, 264, 12391, 286, 445, 4712, 13, 509, 3242, 257, 1873, 3609, 11366, 7222, 293, 309, 486, 50696], "temperature": 0.0, "avg_logprob": -0.12188413295340031, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.009695261716842651}, {"id": 13, "seek": 7612, "start": 82.76, "end": 86.76, "text": " extract all these things for you. It will extract country names, organization names, people names,", "tokens": [50696, 8947, 439, 613, 721, 337, 291, 13, 467, 486, 8947, 1941, 5288, 11, 4475, 5288, 11, 561, 5288, 11, 50896], "temperature": 0.0, "avg_logprob": -0.12188413295340031, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.009695261716842651}, {"id": 14, "seek": 7612, "start": 86.76, "end": 94.52000000000001, "text": " specific teams and topics. And it's actually pretty good at this. At least chatGPT is. And I'll", "tokens": [50896, 2685, 5491, 293, 8378, 13, 400, 309, 311, 767, 1238, 665, 412, 341, 13, 1711, 1935, 5081, 38, 47, 51, 307, 13, 400, 286, 603, 51284], "temperature": 0.0, "avg_logprob": -0.12188413295340031, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.009695261716842651}, {"id": 15, "seek": 7612, "start": 94.52000000000001, "end": 101.56, "text": " discuss other models in a second. And it's not just named entity extraction or simple classification.", "tokens": [51284, 2248, 661, 5245, 294, 257, 1150, 13, 400, 309, 311, 406, 445, 4926, 13977, 30197, 420, 2199, 21538, 13, 51636], "temperature": 0.0, "avg_logprob": -0.12188413295340031, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.009695261716842651}, {"id": 16, "seek": 10156, "start": 102.2, "end": 112.52000000000001, "text": " It also, like, somehow extracts teams and it can abstract from the text. And researchers have been", "tokens": [50396, 467, 611, 11, 411, 11, 6063, 8947, 82, 5491, 293, 309, 393, 12649, 490, 264, 2487, 13, 400, 10309, 362, 668, 50912], "temperature": 0.0, "avg_logprob": -0.18311480953268808, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.0026473351754248142}, {"id": 17, "seek": 10156, "start": 112.52000000000001, "end": 118.92, "text": " using this, for example, to extract narratives from posts. So here's a very complex prompt, which you", "tokens": [50912, 1228, 341, 11, 337, 1365, 11, 281, 8947, 28016, 490, 12300, 13, 407, 510, 311, 257, 588, 3997, 12391, 11, 597, 291, 51232], "temperature": 0.0, "avg_logprob": -0.18311480953268808, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.0026473351754248142}, {"id": 18, "seek": 10156, "start": 118.92, "end": 126.12, "text": " can see in the slides after the talk. But there they use this prompt to go through many, many Reddit", "tokens": [51232, 393, 536, 294, 264, 9788, 934, 264, 751, 13, 583, 456, 436, 764, 341, 12391, 281, 352, 807, 867, 11, 867, 32210, 51592], "temperature": 0.0, "avg_logprob": -0.18311480953268808, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.0026473351754248142}, {"id": 19, "seek": 12612, "start": 126.12, "end": 134.04, "text": " posts, hundreds of thousands, to find out whether there were any conspiracy theories in there. And", "tokens": [50364, 12300, 11, 6779, 295, 5383, 11, 281, 915, 484, 1968, 456, 645, 604, 20439, 13667, 294, 456, 13, 400, 50760], "temperature": 0.0, "avg_logprob": -0.10570577212742396, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.01763751544058323}, {"id": 20, "seek": 12612, "start": 134.04, "end": 142.44, "text": " they devised a prompt to draw out these narratives. And they actually found that LLMs worked really", "tokens": [50760, 436, 1905, 2640, 257, 12391, 281, 2642, 484, 613, 28016, 13, 400, 436, 767, 1352, 300, 441, 43, 26386, 2732, 534, 51180], "temperature": 0.0, "avg_logprob": -0.10570577212742396, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.01763751544058323}, {"id": 21, "seek": 12612, "start": 142.44, "end": 148.52, "text": " well in extracting conspiracy theories as well. So researchers have been using this and they have", "tokens": [51180, 731, 294, 49844, 20439, 13667, 382, 731, 13, 407, 10309, 362, 668, 1228, 341, 293, 436, 362, 51484], "temperature": 0.0, "avg_logprob": -0.10570577212742396, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.01763751544058323}, {"id": 22, "seek": 12612, "start": 148.52, "end": 153.88, "text": " been looking at all the tasks that are typically done in social science and humanities and are", "tokens": [51484, 668, 1237, 412, 439, 264, 9608, 300, 366, 5850, 1096, 294, 2093, 3497, 293, 36140, 293, 366, 51752], "temperature": 0.0, "avg_logprob": -0.10570577212742396, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.01763751544058323}, {"id": 23, "seek": 15388, "start": 153.88, "end": 163.4, "text": " starting to test whether LLMs can help in doing these tasks. And there has been a lot of research", "tokens": [50364, 2891, 281, 1500, 1968, 441, 43, 26386, 393, 854, 294, 884, 613, 9608, 13, 400, 456, 575, 668, 257, 688, 295, 2132, 50840], "temperature": 0.0, "avg_logprob": -0.06363777319590251, "compression_ratio": 1.4947368421052631, "no_speech_prob": 0.005021640565246344}, {"id": 24, "seek": 15388, "start": 163.4, "end": 168.84, "text": " in the last year, especially 2023. This is just a really small snippet of this research.", "tokens": [50840, 294, 264, 1036, 1064, 11, 2318, 44377, 13, 639, 307, 445, 257, 534, 1359, 35623, 302, 295, 341, 2132, 13, 51112], "temperature": 0.0, "avg_logprob": -0.06363777319590251, "compression_ratio": 1.4947368421052631, "no_speech_prob": 0.005021640565246344}, {"id": 25, "seek": 15388, "start": 171.16, "end": 179.56, "text": " But this research also comes with problems, which I'll touch upon in a bit. But their use is also", "tokens": [51228, 583, 341, 2132, 611, 1487, 365, 2740, 11, 597, 286, 603, 2557, 3564, 294, 257, 857, 13, 583, 641, 764, 307, 611, 51648], "temperature": 0.0, "avg_logprob": -0.06363777319590251, "compression_ratio": 1.4947368421052631, "no_speech_prob": 0.005021640565246344}, {"id": 26, "seek": 17956, "start": 179.56, "end": 184.28, "text": " understandable, the use of large language models in social science and humanities research, because", "tokens": [50364, 25648, 11, 264, 764, 295, 2416, 2856, 5245, 294, 2093, 3497, 293, 36140, 2132, 11, 570, 50600], "temperature": 0.0, "avg_logprob": -0.12145919799804687, "compression_ratio": 1.582995951417004, "no_speech_prob": 0.0023465093690901995}, {"id": 27, "seek": 17956, "start": 185.72, "end": 191.24, "text": " they seem to ease and speed up previously difficult and laborious tasks, such as classification,", "tokens": [50672, 436, 1643, 281, 12708, 293, 3073, 493, 8046, 2252, 293, 5938, 851, 9608, 11, 1270, 382, 21538, 11, 50948], "temperature": 0.0, "avg_logprob": -0.12145919799804687, "compression_ratio": 1.582995951417004, "no_speech_prob": 0.0023465093690901995}, {"id": 28, "seek": 17956, "start": 191.24, "end": 197.32, "text": " extraction summarization, and so forth. And they're actually employed as junior research assistants.", "tokens": [50948, 30197, 14611, 2144, 11, 293, 370, 5220, 13, 400, 436, 434, 767, 20115, 382, 16195, 2132, 34949, 13, 51252], "temperature": 0.0, "avg_logprob": -0.12145919799804687, "compression_ratio": 1.582995951417004, "no_speech_prob": 0.0023465093690901995}, {"id": 29, "seek": 17956, "start": 199.4, "end": 206.84, "text": " Now, while this may seem useful somehow, a lot of people seem to be using chat GPT. Actually,", "tokens": [51356, 823, 11, 1339, 341, 815, 1643, 4420, 6063, 11, 257, 688, 295, 561, 1643, 281, 312, 1228, 5081, 26039, 51, 13, 5135, 11, 51728], "temperature": 0.0, "avg_logprob": -0.12145919799804687, "compression_ratio": 1.582995951417004, "no_speech_prob": 0.0023465093690901995}, {"id": 30, "seek": 20684, "start": 206.84, "end": 212.44, "text": " all the papers I've just shown, they're based on chat GPT. And chat GPT comes with problems,", "tokens": [50364, 439, 264, 10577, 286, 600, 445, 4898, 11, 436, 434, 2361, 322, 5081, 26039, 51, 13, 400, 5081, 26039, 51, 1487, 365, 2740, 11, 50644], "temperature": 0.0, "avg_logprob": -0.09698331970529459, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.006974363699555397}, {"id": 31, "seek": 20684, "start": 212.44, "end": 218.12, "text": " because it's a platform service. And platform services, as I guess all of you know, are volatile", "tokens": [50644, 570, 309, 311, 257, 3663, 2643, 13, 400, 3663, 3328, 11, 382, 286, 2041, 439, 295, 291, 458, 11, 366, 34377, 50928], "temperature": 0.0, "avg_logprob": -0.09698331970529459, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.006974363699555397}, {"id": 32, "seek": 20684, "start": 218.12, "end": 223.48000000000002, "text": " black boxes. You don't know what's going on in the back end when they update their model, when they", "tokens": [50928, 2211, 9002, 13, 509, 500, 380, 458, 437, 311, 516, 322, 294, 264, 646, 917, 562, 436, 5623, 641, 2316, 11, 562, 436, 51196], "temperature": 0.0, "avg_logprob": -0.09698331970529459, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.006974363699555397}, {"id": 33, "seek": 20684, "start": 224.44, "end": 230.2, "text": " align something differently or sensor or whatever, whether it's getting dumber or not. You basically", "tokens": [51244, 7975, 746, 7614, 420, 10200, 420, 2035, 11, 1968, 309, 311, 1242, 274, 4182, 420, 406, 13, 509, 1936, 51532], "temperature": 0.0, "avg_logprob": -0.09698331970529459, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.006974363699555397}, {"id": 34, "seek": 23020, "start": 230.2, "end": 237.23999999999998, "text": " don't know. Chat GPT is also very expensive. If you're using the API, you pay for a request.", "tokens": [50364, 500, 380, 458, 13, 27503, 26039, 51, 307, 611, 588, 5124, 13, 759, 291, 434, 1228, 264, 9362, 11, 291, 1689, 337, 257, 5308, 13, 50716], "temperature": 0.0, "avg_logprob": -0.14223450790216893, "compression_ratio": 1.3380281690140845, "no_speech_prob": 0.011105403304100037}, {"id": 35, "seek": 23020, "start": 238.6, "end": 245.72, "text": " This is one research project by Miguel Escobar of Arela, who calculated that to process the one and", "tokens": [50784, 639, 307, 472, 2132, 1716, 538, 29150, 30379, 996, 289, 295, 2014, 875, 11, 567, 15598, 300, 281, 1399, 264, 472, 293, 51140], "temperature": 0.0, "avg_logprob": -0.14223450790216893, "compression_ratio": 1.3380281690140845, "no_speech_prob": 0.011105403304100037}, {"id": 36, "seek": 23020, "start": 245.72, "end": 255.88, "text": " a half million news items he had in his corpus. He'd need $150,000. It's just too expensive.", "tokens": [51140, 257, 1922, 2459, 2583, 4754, 415, 632, 294, 702, 1181, 31624, 13, 634, 1116, 643, 1848, 20120, 11, 1360, 13, 467, 311, 445, 886, 5124, 13, 51648], "temperature": 0.0, "avg_logprob": -0.14223450790216893, "compression_ratio": 1.3380281690140845, "no_speech_prob": 0.011105403304100037}, {"id": 37, "seek": 25588, "start": 256.6, "end": 263.0, "text": " There are, of course, also privacy concerns with chat GPT and other platform models, whereby", "tokens": [50400, 821, 366, 11, 295, 1164, 11, 611, 11427, 7389, 365, 5081, 26039, 51, 293, 661, 3663, 5245, 11, 36998, 50720], "temperature": 0.0, "avg_logprob": -0.09653639447861824, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.002387857064604759}, {"id": 38, "seek": 25588, "start": 263.0, "end": 269.32, "text": " with chat GPT we know that whatever you input into chat GPT is also used as training data for the", "tokens": [50720, 365, 5081, 26039, 51, 321, 458, 300, 2035, 291, 4846, 666, 5081, 26039, 51, 307, 611, 1143, 382, 3097, 1412, 337, 264, 51036], "temperature": 0.0, "avg_logprob": -0.09653639447861824, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.002387857064604759}, {"id": 39, "seek": 25588, "start": 269.32, "end": 277.8, "text": " LLM. Users have also found personal and private information resurfacing from other users, etc.", "tokens": [51036, 441, 43, 44, 13, 47092, 362, 611, 1352, 2973, 293, 4551, 1589, 16042, 44046, 490, 661, 5022, 11, 5183, 13, 51460], "temperature": 0.0, "avg_logprob": -0.09653639447861824, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.002387857064604759}, {"id": 40, "seek": 27780, "start": 278.52000000000004, "end": 285.32, "text": " So if you think in terms of open science, replicable research and ethical", "tokens": [50400, 407, 498, 291, 519, 294, 2115, 295, 1269, 3497, 11, 3248, 43023, 2132, 293, 18890, 50740], "temperature": 0.0, "avg_logprob": -0.11180112528246502, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.007173758465796709}, {"id": 41, "seek": 27780, "start": 286.2, "end": 291.8, "text": " research with privacy concerns, you basically cannot use these models, even though you can go to", "tokens": [50784, 2132, 365, 11427, 7389, 11, 291, 1936, 2644, 764, 613, 5245, 11, 754, 1673, 291, 393, 352, 281, 51064], "temperature": 0.0, "avg_logprob": -0.11180112528246502, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.007173758465796709}, {"id": 42, "seek": 27780, "start": 291.8, "end": 298.12, "text": " privacy.openai.com and state that you don't want your inputs to be used as training data.", "tokens": [51064, 11427, 13, 15752, 1301, 13, 1112, 293, 1785, 300, 291, 500, 380, 528, 428, 15743, 281, 312, 1143, 382, 3097, 1412, 13, 51380], "temperature": 0.0, "avg_logprob": -0.11180112528246502, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.007173758465796709}, {"id": 43, "seek": 27780, "start": 299.88, "end": 307.24, "text": " Well, so how to deal with this? Can we use LLMs in social science and humanities research?", "tokens": [51468, 1042, 11, 370, 577, 281, 2028, 365, 341, 30, 1664, 321, 764, 441, 43, 26386, 294, 2093, 3497, 293, 36140, 2132, 30, 51836], "temperature": 0.0, "avg_logprob": -0.11180112528246502, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.007173758465796709}, {"id": 44, "seek": 30780, "start": 308.52000000000004, "end": 315.56, "text": " Fortunately, the answer is yes. Chat GPT is not the only model available. You probably heard of", "tokens": [50400, 20652, 11, 264, 1867, 307, 2086, 13, 27503, 26039, 51, 307, 406, 264, 787, 2316, 2435, 13, 509, 1391, 2198, 295, 50752], "temperature": 0.0, "avg_logprob": -0.13665036653217516, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.0011132367653772235}, {"id": 45, "seek": 30780, "start": 315.56, "end": 321.96000000000004, "text": " Google Bart or Gemini. You may have heard of Clodes, which are other platform models, but there are", "tokens": [50752, 3329, 22338, 420, 22894, 3812, 13, 509, 815, 362, 2198, 295, 2033, 4789, 11, 597, 366, 661, 3663, 5245, 11, 457, 456, 366, 51072], "temperature": 0.0, "avg_logprob": -0.13665036653217516, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.0011132367653772235}, {"id": 46, "seek": 30780, "start": 321.96000000000004, "end": 327.0, "text": " also a lot of publicly available models. All the yellow ones highlighted here, and this is only,", "tokens": [51072, 611, 257, 688, 295, 14843, 2435, 5245, 13, 1057, 264, 5566, 2306, 17173, 510, 11, 293, 341, 307, 787, 11, 51324], "temperature": 0.0, "avg_logprob": -0.13665036653217516, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.0011132367653772235}, {"id": 47, "seek": 30780, "start": 328.28000000000003, "end": 333.88, "text": " I think, until the second quarter of 2023. Since then, a lot of new models have appeared,", "tokens": [51388, 286, 519, 11, 1826, 264, 1150, 6555, 295, 44377, 13, 4162, 550, 11, 257, 688, 295, 777, 5245, 362, 8516, 11, 51668], "temperature": 0.0, "avg_logprob": -0.13665036653217516, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.0011132367653772235}, {"id": 48, "seek": 33388, "start": 333.88, "end": 342.44, "text": " most notably Mistral, for example, the French model, or Mistral, the 8x7B model of Mistral,", "tokens": [50364, 881, 31357, 20166, 2155, 11, 337, 1365, 11, 264, 5522, 2316, 11, 420, 20166, 2155, 11, 264, 1649, 87, 22, 33, 2316, 295, 20166, 2155, 11, 50792], "temperature": 0.0, "avg_logprob": -0.11618144989013672, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.0017528568860143423}, {"id": 49, "seek": 33388, "start": 342.44, "end": 348.92, "text": " which are really good and are catching up on the performance of chat GPT. Publicly available,", "tokens": [50792, 597, 366, 534, 665, 293, 366, 16124, 493, 322, 264, 3389, 295, 5081, 26039, 51, 13, 9489, 356, 2435, 11, 51116], "temperature": 0.0, "avg_logprob": -0.11618144989013672, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.0017528568860143423}, {"id": 50, "seek": 33388, "start": 348.92, "end": 355.0, "text": " however, doesn't mean that it's open, that it's open source, that it's free, because there is this", "tokens": [51116, 4461, 11, 1177, 380, 914, 300, 309, 311, 1269, 11, 300, 309, 311, 1269, 4009, 11, 300, 309, 311, 1737, 11, 570, 456, 307, 341, 51420], "temperature": 0.0, "avg_logprob": -0.11618144989013672, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.0017528568860143423}, {"id": 51, "seek": 33388, "start": 355.0, "end": 361.96, "text": " whole infrastructure and apparatus to train models, to fine tune models, to use models in your own", "tokens": [51420, 1379, 6896, 293, 38573, 281, 3847, 5245, 11, 281, 2489, 10864, 5245, 11, 281, 764, 5245, 294, 428, 1065, 51768], "temperature": 0.0, "avg_logprob": -0.11618144989013672, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.0017528568860143423}, {"id": 52, "seek": 36196, "start": 361.96, "end": 369.56, "text": " work. And you see all the orange and red here. Most of these models aren't open,", "tokens": [50364, 589, 13, 400, 291, 536, 439, 264, 7671, 293, 2182, 510, 13, 4534, 295, 613, 5245, 3212, 380, 1269, 11, 50744], "temperature": 0.0, "avg_logprob": -0.11217965020073785, "compression_ratio": 1.4516129032258065, "no_speech_prob": 0.003959974739700556}, {"id": 53, "seek": 36196, "start": 372.76, "end": 379.64, "text": " or have different licenses, etc. So it's not, yeah, you can't just use another one. You need to think", "tokens": [50904, 420, 362, 819, 32821, 11, 5183, 13, 407, 309, 311, 406, 11, 1338, 11, 291, 393, 380, 445, 764, 1071, 472, 13, 509, 643, 281, 519, 51248], "temperature": 0.0, "avg_logprob": -0.11217965020073785, "compression_ratio": 1.4516129032258065, "no_speech_prob": 0.003959974739700556}, {"id": 54, "seek": 36196, "start": 379.64, "end": 386.28, "text": " about these things. Two other considerations before I go to the actual tool. If you use", "tokens": [51248, 466, 613, 721, 13, 4453, 661, 24070, 949, 286, 352, 281, 264, 3539, 2290, 13, 759, 291, 764, 51580], "temperature": 0.0, "avg_logprob": -0.11217965020073785, "compression_ratio": 1.4516129032258065, "no_speech_prob": 0.003959974739700556}, {"id": 55, "seek": 38628, "start": 387.23999999999995, "end": 392.84, "text": " the same prompt in different models, you'll get different results. And this is actually", "tokens": [50412, 264, 912, 12391, 294, 819, 5245, 11, 291, 603, 483, 819, 3542, 13, 400, 341, 307, 767, 50692], "temperature": 0.0, "avg_logprob": -0.08991307020187378, "compression_ratio": 1.6130952380952381, "no_speech_prob": 0.0030306014232337475}, {"id": 56, "seek": 38628, "start": 393.96, "end": 401.4, "text": " the same prompt in a series of image models, but you can visually see how results may differ.", "tokens": [50748, 264, 912, 12391, 294, 257, 2638, 295, 3256, 5245, 11, 457, 291, 393, 19622, 536, 577, 3542, 815, 743, 13, 51120], "temperature": 0.0, "avg_logprob": -0.08991307020187378, "compression_ratio": 1.6130952380952381, "no_speech_prob": 0.0030306014232337475}, {"id": 57, "seek": 38628, "start": 402.84, "end": 410.59999999999997, "text": " This is something to take into account. And last but not least, there are a few technical", "tokens": [51192, 639, 307, 746, 281, 747, 666, 2696, 13, 400, 1036, 457, 406, 1935, 11, 456, 366, 257, 1326, 6191, 51580], "temperature": 0.0, "avg_logprob": -0.08991307020187378, "compression_ratio": 1.6130952380952381, "no_speech_prob": 0.0030306014232337475}, {"id": 58, "seek": 41060, "start": 410.68, "end": 418.68, "text": " parameters in using LLMs. And one way to control differences or variability in the output of LLMs", "tokens": [50368, 9834, 294, 1228, 441, 43, 26386, 13, 400, 472, 636, 281, 1969, 7300, 420, 35709, 294, 264, 5598, 295, 441, 43, 26386, 50768], "temperature": 0.0, "avg_logprob": -0.08930362449897515, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.013398938812315464}, {"id": 59, "seek": 41060, "start": 418.68, "end": 423.8, "text": " is through the so-called temperature parameter. If you set the temperature to zero, you'll always", "tokens": [50768, 307, 807, 264, 370, 12, 11880, 4292, 13075, 13, 759, 291, 992, 264, 4292, 281, 4018, 11, 291, 603, 1009, 51024], "temperature": 0.0, "avg_logprob": -0.08930362449897515, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.013398938812315464}, {"id": 60, "seek": 41060, "start": 423.8, "end": 428.84000000000003, "text": " get the same result. The most probable or most likely outcome was if you increase the temperature,", "tokens": [51024, 483, 264, 912, 1874, 13, 440, 881, 21759, 420, 881, 3700, 9700, 390, 498, 291, 3488, 264, 4292, 11, 51276], "temperature": 0.0, "avg_logprob": -0.08930362449897515, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.013398938812315464}, {"id": 61, "seek": 41060, "start": 429.96000000000004, "end": 439.0, "text": " there is a chance of less likely outputs to be included in the results. But again, all these", "tokens": [51332, 456, 307, 257, 2931, 295, 1570, 3700, 23930, 281, 312, 5556, 294, 264, 3542, 13, 583, 797, 11, 439, 613, 51784], "temperature": 0.0, "avg_logprob": -0.08930362449897515, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.013398938812315464}, {"id": 62, "seek": 43900, "start": 439.0, "end": 446.12, "text": " papers, none of them mention temperature, whilst it's a very important parameter. Last but not", "tokens": [50364, 10577, 11, 6022, 295, 552, 2152, 4292, 11, 18534, 309, 311, 257, 588, 1021, 13075, 13, 5264, 457, 406, 50720], "temperature": 0.0, "avg_logprob": -0.16069404975227689, "compression_ratio": 1.4980544747081712, "no_speech_prob": 0.0018766700522974133}, {"id": 63, "seek": 43900, "start": 446.12, "end": 452.28, "text": " least, this is work I've been doing with Maichieu, small syntactic differences in a query that's", "tokens": [50720, 1935, 11, 341, 307, 589, 286, 600, 668, 884, 365, 4042, 480, 19347, 11, 1359, 23980, 19892, 7300, 294, 257, 14581, 300, 311, 51028], "temperature": 0.0, "avg_logprob": -0.16069404975227689, "compression_ratio": 1.4980544747081712, "no_speech_prob": 0.0018766700522974133}, {"id": 64, "seek": 43900, "start": 452.28, "end": 457.72, "text": " semantically the same may lead to different results as well. So you need to test your prompt for", "tokens": [51028, 4361, 49505, 264, 912, 815, 1477, 281, 819, 3542, 382, 731, 13, 407, 291, 643, 281, 1500, 428, 12391, 337, 51300], "temperature": 0.0, "avg_logprob": -0.16069404975227689, "compression_ratio": 1.4980544747081712, "no_speech_prob": 0.0018766700522974133}, {"id": 65, "seek": 43900, "start": 457.72, "end": 465.72, "text": " robustness or consistency. So summarizing, open AI or platforms like chat GPT are volatile black", "tokens": [51300, 13956, 1287, 420, 14416, 13, 407, 14611, 3319, 11, 1269, 7318, 420, 9473, 411, 5081, 26039, 51, 366, 34377, 2211, 51700], "temperature": 0.0, "avg_logprob": -0.16069404975227689, "compression_ratio": 1.4980544747081712, "no_speech_prob": 0.0018766700522974133}, {"id": 66, "seek": 46572, "start": 465.72, "end": 470.36, "text": " boxes that cost a lot of money. There are issues of privacy and security. There are different models", "tokens": [50364, 9002, 300, 2063, 257, 688, 295, 1460, 13, 821, 366, 2663, 295, 11427, 293, 3825, 13, 821, 366, 819, 5245, 50596], "temperature": 0.0, "avg_logprob": -0.06658735950436212, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.008760982193052769}, {"id": 67, "seek": 46572, "start": 470.36, "end": 476.12, "text": " which have different licenses, which have different results. LLMs are not deterministic and small", "tokens": [50596, 597, 362, 819, 32821, 11, 597, 362, 819, 3542, 13, 441, 43, 26386, 366, 406, 15957, 3142, 293, 1359, 50884], "temperature": 0.0, "avg_logprob": -0.06658735950436212, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.008760982193052769}, {"id": 68, "seek": 46572, "start": 476.12, "end": 481.48, "text": " changes in prompts may lead to different outputs. So we need research interfaces where we can", "tokens": [50884, 2962, 294, 41095, 815, 1477, 281, 819, 23930, 13, 407, 321, 643, 2132, 28416, 689, 321, 393, 51152], "temperature": 0.0, "avg_logprob": -0.06658735950436212, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.008760982193052769}, {"id": 69, "seek": 46572, "start": 481.48, "end": 487.64000000000004, "text": " control for such things. We want to be able to do open science with LLMs. So how do we take into", "tokens": [51152, 1969, 337, 1270, 721, 13, 492, 528, 281, 312, 1075, 281, 360, 1269, 3497, 365, 441, 43, 26386, 13, 407, 577, 360, 321, 747, 666, 51460], "temperature": 0.0, "avg_logprob": -0.06658735950436212, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.008760982193052769}, {"id": 70, "seek": 46572, "start": 487.64000000000004, "end": 494.36, "text": " account the volatility of platforms, the robustness of research and its replicability and explainability?", "tokens": [51460, 2696, 264, 25877, 295, 9473, 11, 264, 13956, 1287, 295, 2132, 293, 1080, 3248, 299, 2310, 293, 2903, 2310, 30, 51796], "temperature": 0.0, "avg_logprob": -0.06658735950436212, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.008760982193052769}, {"id": 71, "seek": 49436, "start": 495.32, "end": 501.56, "text": " And this is where I started tinkering with a tool I called prompt compass, or actually I had chat", "tokens": [50412, 400, 341, 307, 689, 286, 1409, 256, 475, 1794, 365, 257, 2290, 286, 1219, 12391, 10707, 11, 420, 767, 286, 632, 5081, 50724], "temperature": 0.0, "avg_logprob": -0.12009662262936856, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.000650343019515276}, {"id": 72, "seek": 49436, "start": 501.56, "end": 509.96000000000004, "text": " GPT call it prompt compass. And it's a research interface. It's not a chat interface. It allows", "tokens": [50724, 26039, 51, 818, 309, 12391, 10707, 13, 400, 309, 311, 257, 2132, 9226, 13, 467, 311, 406, 257, 5081, 9226, 13, 467, 4045, 51144], "temperature": 0.0, "avg_logprob": -0.12009662262936856, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.000650343019515276}, {"id": 73, "seek": 49436, "start": 509.96000000000004, "end": 516.6800000000001, "text": " you to take into account all these considerations that I've put up. So you can choose various local", "tokens": [51144, 291, 281, 747, 666, 2696, 439, 613, 24070, 300, 286, 600, 829, 493, 13, 407, 291, 393, 2826, 3683, 2654, 51480], "temperature": 0.0, "avg_logprob": -0.12009662262936856, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.000650343019515276}, {"id": 74, "seek": 49436, "start": 516.6800000000001, "end": 522.28, "text": " models. It has default parameters for replicability. It contains a library of research prompts,", "tokens": [51480, 5245, 13, 467, 575, 7576, 9834, 337, 3248, 299, 2310, 13, 467, 8306, 257, 6405, 295, 2132, 41095, 11, 51760], "temperature": 0.0, "avg_logprob": -0.12009662262936856, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.000650343019515276}, {"id": 75, "seek": 52228, "start": 522.36, "end": 527.88, "text": " allows for batch processing user input, and allows you to evaluate prompt model combinations.", "tokens": [50368, 4045, 337, 15245, 9007, 4195, 4846, 11, 293, 4045, 291, 281, 13059, 12391, 2316, 21267, 13, 50644], "temperature": 0.0, "avg_logprob": -0.12208647046770368, "compression_ratio": 1.41, "no_speech_prob": 0.000732128566596657}, {"id": 76, "seek": 52228, "start": 527.88, "end": 537.0799999999999, "text": " Do I still have some time to demo this? Cool. Let's do that. So prompt compass is available", "tokens": [50644, 1144, 286, 920, 362, 512, 565, 281, 10723, 341, 30, 8561, 13, 961, 311, 360, 300, 13, 407, 12391, 10707, 307, 2435, 51104], "temperature": 0.0, "avg_logprob": -0.12208647046770368, "compression_ratio": 1.41, "no_speech_prob": 0.000732128566596657}, {"id": 77, "seek": 52228, "start": 537.9599999999999, "end": 546.36, "text": " on GitHub. We also run it at one of our servers. The design doesn't really shine on this beamer,", "tokens": [51148, 322, 23331, 13, 492, 611, 1190, 309, 412, 472, 295, 527, 15909, 13, 440, 1715, 1177, 380, 534, 12207, 322, 341, 312, 13530, 11, 51568], "temperature": 0.0, "avg_logprob": -0.12208647046770368, "compression_ratio": 1.41, "no_speech_prob": 0.000732128566596657}, {"id": 78, "seek": 54636, "start": 546.36, "end": 553.72, "text": " but anyway. Here you can select various models which are loaded from hugging face. You can easily", "tokens": [50364, 457, 4033, 13, 1692, 291, 393, 3048, 3683, 5245, 597, 366, 13210, 490, 41706, 1851, 13, 509, 393, 3612, 50732], "temperature": 0.0, "avg_logprob": -0.08713132994515556, "compression_ratio": 1.6342857142857143, "no_speech_prob": 0.005488418508321047}, {"id": 79, "seek": 54636, "start": 554.36, "end": 563.08, "text": " add a new model and select one of these. You can find out more by clicking on the model card", "tokens": [50764, 909, 257, 777, 2316, 293, 3048, 472, 295, 613, 13, 509, 393, 915, 484, 544, 538, 9697, 322, 264, 2316, 2920, 51200], "temperature": 0.0, "avg_logprob": -0.08713132994515556, "compression_ratio": 1.6342857142857143, "no_speech_prob": 0.005488418508321047}, {"id": 80, "seek": 54636, "start": 563.08, "end": 568.28, "text": " and then see what the model was made for, how it was trained and so forth. All these models are", "tokens": [51200, 293, 550, 536, 437, 264, 2316, 390, 1027, 337, 11, 577, 309, 390, 8895, 293, 370, 5220, 13, 1057, 613, 5245, 366, 51460], "temperature": 0.0, "avg_logprob": -0.08713132994515556, "compression_ratio": 1.6342857142857143, "no_speech_prob": 0.005488418508321047}, {"id": 81, "seek": 56828, "start": 568.28, "end": 577.0, "text": " loaded from hugging face, which is like the GitHub for language models, but we can also", "tokens": [50364, 13210, 490, 41706, 1851, 11, 597, 307, 411, 264, 23331, 337, 2856, 5245, 11, 457, 321, 393, 611, 50800], "temperature": 0.0, "avg_logprob": -0.16088523864746093, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.006508911028504372}, {"id": 82, "seek": 56828, "start": 578.12, "end": 584.6, "text": " choose GPT for where you and then or any other model of open AI or platforms and then", "tokens": [50856, 2826, 26039, 51, 337, 689, 291, 293, 550, 420, 604, 661, 2316, 295, 1269, 7318, 420, 9473, 293, 550, 51180], "temperature": 0.0, "avg_logprob": -0.16088523864746093, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.006508911028504372}, {"id": 83, "seek": 56828, "start": 585.16, "end": 591.9599999999999, "text": " enter your API key and go over that. You can go into the settings which are default sets to", "tokens": [51208, 3242, 428, 9362, 2141, 293, 352, 670, 300, 13, 509, 393, 352, 666, 264, 6257, 597, 366, 7576, 6352, 281, 51548], "temperature": 0.0, "avg_logprob": -0.16088523864746093, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.006508911028504372}, {"id": 84, "seek": 59196, "start": 592.76, "end": 599.96, "text": " replicability. There's a little explanation of it. There are a lot of prompts extracted from the", "tokens": [50404, 3248, 299, 2310, 13, 821, 311, 257, 707, 10835, 295, 309, 13, 821, 366, 257, 688, 295, 41095, 34086, 490, 264, 50764], "temperature": 0.0, "avg_logprob": -0.15333670207432337, "compression_ratio": 1.6298342541436464, "no_speech_prob": 0.0012255957117304206}, {"id": 85, "seek": 59196, "start": 599.96, "end": 610.6800000000001, "text": " literature and from actual research. And you can input your own prompts like this. You can or you", "tokens": [50764, 10394, 293, 490, 3539, 2132, 13, 400, 291, 393, 4846, 428, 1065, 41095, 411, 341, 13, 509, 393, 420, 291, 51300], "temperature": 0.0, "avg_logprob": -0.15333670207432337, "compression_ratio": 1.6298342541436464, "no_speech_prob": 0.0012255957117304206}, {"id": 86, "seek": 59196, "start": 610.6800000000001, "end": 620.6, "text": " can adapt existing prompts. And then you can provide user input either line by line or upload a CSV.", "tokens": [51300, 393, 6231, 6741, 41095, 13, 400, 550, 291, 393, 2893, 4195, 4846, 2139, 1622, 538, 1622, 420, 6580, 257, 48814, 13, 51796], "temperature": 0.0, "avg_logprob": -0.15333670207432337, "compression_ratio": 1.6298342541436464, "no_speech_prob": 0.0012255957117304206}, {"id": 87, "seek": 62060, "start": 621.4, "end": 629.64, "text": " And then if you click submit, the selected model will be loaded. And each of the lines will be run", "tokens": [50404, 400, 550, 498, 291, 2052, 10315, 11, 264, 8209, 2316, 486, 312, 13210, 13, 400, 1184, 295, 264, 3876, 486, 312, 1190, 50816], "temperature": 0.0, "avg_logprob": -0.1699792332119412, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.0008106340537779033}, {"id": 88, "seek": 62060, "start": 629.64, "end": 636.9200000000001, "text": " through the model with the indicated prompt. So in this time we chose sentiment analysis,", "tokens": [50816, 807, 264, 2316, 365, 264, 16176, 12391, 13, 407, 294, 341, 565, 321, 5111, 16149, 5215, 11, 51180], "temperature": 0.0, "avg_logprob": -0.1699792332119412, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.0008106340537779033}, {"id": 89, "seek": 62060, "start": 637.8000000000001, "end": 641.32, "text": " which says you're in advanced classifying AI, you're tasked with classifying the sentiment of a", "tokens": [51224, 597, 1619, 291, 434, 294, 7339, 1508, 5489, 7318, 11, 291, 434, 38621, 365, 1508, 5489, 264, 16149, 295, 257, 51400], "temperature": 0.0, "avg_logprob": -0.1699792332119412, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.0008106340537779033}, {"id": 90, "seek": 62060, "start": 641.32, "end": 647.88, "text": " text, which can be either positive, negative or neutral. And this is where we'll input or loop", "tokens": [51400, 2487, 11, 597, 393, 312, 2139, 3353, 11, 3671, 420, 10598, 13, 400, 341, 307, 689, 321, 603, 4846, 420, 6367, 51728], "temperature": 0.0, "avg_logprob": -0.1699792332119412, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.0008106340537779033}, {"id": 91, "seek": 64788, "start": 647.88, "end": 653.48, "text": " over our inputs. So in this time the user is happy, it's classified as positive. When user is just", "tokens": [50364, 670, 527, 15743, 13, 407, 294, 341, 565, 264, 4195, 307, 2055, 11, 309, 311, 20627, 382, 3353, 13, 1133, 4195, 307, 445, 50644], "temperature": 0.0, "avg_logprob": -0.14096783724698153, "compression_ratio": 1.8217391304347825, "no_speech_prob": 0.001003938028588891}, {"id": 92, "seek": 64788, "start": 653.48, "end": 658.4399999999999, "text": " a user, it's classified as neutral and the other user is a liar, it's classified as negative.", "tokens": [50644, 257, 4195, 11, 309, 311, 20627, 382, 10598, 293, 264, 661, 4195, 307, 257, 27323, 11, 309, 311, 20627, 382, 3671, 13, 50892], "temperature": 0.0, "avg_logprob": -0.14096783724698153, "compression_ratio": 1.8217391304347825, "no_speech_prob": 0.001003938028588891}, {"id": 93, "seek": 64788, "start": 658.4399999999999, "end": 663.4, "text": " And this tool is not the end all go all tool for working with LLMs, but it is a way to test", "tokens": [50892, 400, 341, 2290, 307, 406, 264, 917, 439, 352, 439, 2290, 337, 1364, 365, 441, 43, 26386, 11, 457, 309, 307, 257, 636, 281, 1500, 51140], "temperature": 0.0, "avg_logprob": -0.14096783724698153, "compression_ratio": 1.8217391304347825, "no_speech_prob": 0.001003938028588891}, {"id": 94, "seek": 64788, "start": 663.96, "end": 669.32, "text": " models, to test parameters, to test prompts, to test the robustness of prompts and to", "tokens": [51168, 5245, 11, 281, 1500, 9834, 11, 281, 1500, 41095, 11, 281, 1500, 264, 13956, 1287, 295, 41095, 293, 281, 51436], "temperature": 0.0, "avg_logprob": -0.14096783724698153, "compression_ratio": 1.8217391304347825, "no_speech_prob": 0.001003938028588891}, {"id": 95, "seek": 64788, "start": 670.92, "end": 674.68, "text": " get all this into easily digestible outputs CSV.", "tokens": [51516, 483, 439, 341, 666, 3612, 13884, 964, 23930, 48814, 13, 51704], "temperature": 0.0, "avg_logprob": -0.14096783724698153, "compression_ratio": 1.8217391304347825, "no_speech_prob": 0.001003938028588891}, {"id": 96, "seek": 67788, "start": 678.4399999999999, "end": 685.56, "text": " So far for the demo. The technology is used, it's really simple. I'm not like a hardcore coder,", "tokens": [50392, 407, 1400, 337, 264, 10723, 13, 440, 2899, 307, 1143, 11, 309, 311, 534, 2199, 13, 286, 478, 406, 411, 257, 28196, 17656, 260, 11, 50748], "temperature": 0.0, "avg_logprob": -0.14986323327133336, "compression_ratio": 1.5625, "no_speech_prob": 0.0020423433743417263}, {"id": 97, "seek": 67788, "start": 685.56, "end": 693.08, "text": " but I'm like more of a tie some stuff together coder. Streamlit is a Python interface for", "tokens": [50748, 457, 286, 478, 411, 544, 295, 257, 7582, 512, 1507, 1214, 17656, 260, 13, 24904, 23062, 307, 257, 15329, 9226, 337, 51124], "temperature": 0.0, "avg_logprob": -0.14986323327133336, "compression_ratio": 1.5625, "no_speech_prob": 0.0020423433743417263}, {"id": 98, "seek": 67788, "start": 693.8, "end": 700.84, "text": " making easily making web applications of machine learning tasks. Lang chain is a very bloated", "tokens": [51160, 1455, 3612, 1455, 3670, 5821, 295, 3479, 2539, 9608, 13, 13313, 5021, 307, 257, 588, 1749, 770, 51512], "temperature": 0.0, "avg_logprob": -0.14986323327133336, "compression_ratio": 1.5625, "no_speech_prob": 0.0020423433743417263}, {"id": 99, "seek": 67788, "start": 700.84, "end": 707.24, "text": " way to easily connect LLMs and to work with LLMs and prompts and Huckingface is the place where", "tokens": [51512, 636, 281, 3612, 1745, 441, 43, 26386, 293, 281, 589, 365, 441, 43, 26386, 293, 41095, 293, 389, 33260, 2868, 307, 264, 1081, 689, 51832], "temperature": 0.0, "avg_logprob": -0.14986323327133336, "compression_ratio": 1.5625, "no_speech_prob": 0.0020423433743417263}, {"id": 100, "seek": 70724, "start": 707.32, "end": 715.64, "text": " all these LLMs are stored. We run this on a 24 gigabyte GPU, which is a bit expensive,", "tokens": [50368, 439, 613, 441, 43, 26386, 366, 12187, 13, 492, 1190, 341, 322, 257, 4022, 8741, 34529, 18407, 11, 597, 307, 257, 857, 5124, 11, 50784], "temperature": 0.0, "avg_logprob": -0.15591054492526585, "compression_ratio": 1.4678111587982832, "no_speech_prob": 0.0024180314503610134}, {"id": 101, "seek": 70724, "start": 715.64, "end": 719.08, "text": " but it's not very expensive. Like each research group should be able to get one.", "tokens": [50784, 457, 309, 311, 406, 588, 5124, 13, 1743, 1184, 2132, 1594, 820, 312, 1075, 281, 483, 472, 13, 50956], "temperature": 0.0, "avg_logprob": -0.15591054492526585, "compression_ratio": 1.4678111587982832, "no_speech_prob": 0.0024180314503610134}, {"id": 102, "seek": 70724, "start": 721.0, "end": 727.96, "text": " And I mean, yeah, so to get back to my rent against platforms, making LLMs", "tokens": [51052, 400, 286, 914, 11, 1338, 11, 370, 281, 483, 646, 281, 452, 6214, 1970, 9473, 11, 1455, 441, 43, 26386, 51400], "temperature": 0.0, "avg_logprob": -0.15591054492526585, "compression_ratio": 1.4678111587982832, "no_speech_prob": 0.0024180314503610134}, {"id": 103, "seek": 70724, "start": 728.6, "end": 736.2, "text": " locally accessible makes them stable and replicable. But we cannot run the biggest models unless we", "tokens": [51432, 16143, 9515, 1669, 552, 8351, 293, 3248, 43023, 13, 583, 321, 2644, 1190, 264, 3880, 5245, 5969, 321, 51812], "temperature": 0.0, "avg_logprob": -0.15591054492526585, "compression_ratio": 1.4678111587982832, "no_speech_prob": 0.0024180314503610134}, {"id": 104, "seek": 73620, "start": 737.0, "end": 742.2, "text": " have access to bigger infrastructure, which we sometimes have. But this is really meant for", "tokens": [50404, 362, 2105, 281, 3801, 6896, 11, 597, 321, 2171, 362, 13, 583, 341, 307, 534, 4140, 337, 50664], "temperature": 0.0, "avg_logprob": -0.20461437158417284, "compression_ratio": 1.456140350877193, "no_speech_prob": 0.005491626914590597}, {"id": 105, "seek": 73620, "start": 742.9200000000001, "end": 750.2800000000001, "text": " researchers that want easy access to local models. I made a video tutorial,", "tokens": [50700, 10309, 300, 528, 1858, 2105, 281, 2654, 5245, 13, 286, 1027, 257, 960, 7073, 11, 51068], "temperature": 0.0, "avg_logprob": -0.20461437158417284, "compression_ratio": 1.456140350877193, "no_speech_prob": 0.005491626914590597}, {"id": 106, "seek": 73620, "start": 750.2800000000001, "end": 755.08, "text": " tutorial which you may want to watch. And maybe there's still room for questions.", "tokens": [51068, 7073, 597, 291, 815, 528, 281, 1159, 13, 400, 1310, 456, 311, 920, 1808, 337, 1651, 13, 51308], "temperature": 0.0, "avg_logprob": -0.20461437158417284, "compression_ratio": 1.456140350877193, "no_speech_prob": 0.005491626914590597}, {"id": 107, "seek": 75508, "start": 755.48, "end": 757.8000000000001, "text": " Just three minutes.", "tokens": [50384, 1449, 1045, 2077, 13, 50500], "temperature": 0.0, "avg_logprob": -0.4961331332171405, "compression_ratio": 1.1030927835051547, "no_speech_prob": 0.048295680433511734}, {"id": 108, "seek": 75508, "start": 777.32, "end": 784.36, "text": " Atlas TI is a rather big and well-known software package for qualitative coding, right?", "tokens": [51476, 32485, 28819, 307, 257, 2831, 955, 293, 731, 12, 6861, 4722, 7372, 337, 31312, 17720, 11, 558, 30, 51828], "temperature": 0.0, "avg_logprob": -0.4961331332171405, "compression_ratio": 1.1030927835051547, "no_speech_prob": 0.048295680433511734}, {"id": 109, "seek": 78508, "start": 785.72, "end": 794.6, "text": " I'm not sure why they chose to only use Chatchi PT. But yeah, I mean, we've had experience with", "tokens": [50396, 286, 478, 406, 988, 983, 436, 5111, 281, 787, 764, 761, 852, 72, 35460, 13, 583, 1338, 11, 286, 914, 11, 321, 600, 632, 1752, 365, 50840], "temperature": 0.0, "avg_logprob": -0.22155148050059442, "compression_ratio": 1.4915966386554622, "no_speech_prob": 0.0009642305667512119}, {"id": 110, "seek": 78508, "start": 794.6, "end": 799.5600000000001, "text": " local LLMs that you can actually also do similar things with extraction and coding.", "tokens": [50840, 2654, 441, 43, 26386, 300, 291, 393, 767, 611, 360, 2531, 721, 365, 30197, 293, 17720, 13, 51088], "temperature": 0.0, "avg_logprob": -0.22155148050059442, "compression_ratio": 1.4915966386554622, "no_speech_prob": 0.0009642305667512119}, {"id": 111, "seek": 78508, "start": 801.1600000000001, "end": 807.8000000000001, "text": " So I would definitely be in favor of actually using local LLMs. On the other hand, if you have", "tokens": [51168, 407, 286, 576, 2138, 312, 294, 2294, 295, 767, 1228, 2654, 441, 43, 26386, 13, 1282, 264, 661, 1011, 11, 498, 291, 362, 51500], "temperature": 0.0, "avg_logprob": -0.22155148050059442, "compression_ratio": 1.4915966386554622, "no_speech_prob": 0.0009642305667512119}, {"id": 112, "seek": 78508, "start": 807.8000000000001, "end": 814.6800000000001, "text": " proper validation procedures such as intercoder reliability and F1 scores, etc.,", "tokens": [51500, 2296, 24071, 13846, 1270, 382, 728, 66, 19866, 24550, 293, 479, 16, 13444, 11, 5183, 7933, 51844], "temperature": 0.0, "avg_logprob": -0.22155148050059442, "compression_ratio": 1.4915966386554622, "no_speech_prob": 0.0009642305667512119}, {"id": 113, "seek": 81468, "start": 814.68, "end": 820.04, "text": " you can get a long way with Chatchi PT because human coders are also fallible and may also", "tokens": [50364, 291, 393, 483, 257, 938, 636, 365, 761, 852, 72, 35460, 570, 1952, 17656, 433, 366, 611, 2100, 964, 293, 815, 611, 50632], "temperature": 0.0, "avg_logprob": -0.11979962840224757, "compression_ratio": 1.4181818181818182, "no_speech_prob": 0.002798312809318304}, {"id": 114, "seek": 81468, "start": 820.04, "end": 826.5999999999999, "text": " be different today than they were a few weeks ago. So it's not that it's not possible or not", "tokens": [50632, 312, 819, 965, 813, 436, 645, 257, 1326, 3259, 2057, 13, 407, 309, 311, 406, 300, 309, 311, 406, 1944, 420, 406, 50960], "temperature": 0.0, "avg_logprob": -0.11979962840224757, "compression_ratio": 1.4181818181818182, "no_speech_prob": 0.002798312809318304}, {"id": 115, "seek": 81468, "start": 826.5999999999999, "end": 831.4, "text": " usable at all, but you should be prudent, I think.", "tokens": [50960, 29975, 412, 439, 11, 457, 291, 820, 312, 582, 24064, 11, 286, 519, 13, 51200], "temperature": 0.0, "avg_logprob": -0.11979962840224757, "compression_ratio": 1.4181818181818182, "no_speech_prob": 0.002798312809318304}, {"id": 116, "seek": 83140, "start": 831.88, "end": 842.12, "text": " You said it's mostly for testing the models, but how big of an input file do you think?", "tokens": [50388, 509, 848, 309, 311, 5240, 337, 4997, 264, 5245, 11, 457, 577, 955, 295, 364, 4846, 3991, 360, 291, 519, 30, 50900], "temperature": 0.0, "avg_logprob": -0.26952664511544366, "compression_ratio": 1.394736842105263, "no_speech_prob": 0.018812596797943115}, {"id": 117, "seek": 83140, "start": 843.4, "end": 850.52, "text": " We've run this on more than 100,000 lines of CSV, I think even more. So in the digital methods", "tokens": [50964, 492, 600, 1190, 341, 322, 544, 813, 2319, 11, 1360, 3876, 295, 48814, 11, 286, 519, 754, 544, 13, 407, 294, 264, 4562, 7150, 51320], "temperature": 0.0, "avg_logprob": -0.26952664511544366, "compression_ratio": 1.394736842105263, "no_speech_prob": 0.018812596797943115}, {"id": 118, "seek": 83140, "start": 850.52, "end": 855.96, "text": " winter school and past summer school, we actually run a lot of prompts through it.", "tokens": [51320, 6355, 1395, 293, 1791, 4266, 1395, 11, 321, 767, 1190, 257, 688, 295, 41095, 807, 309, 13, 51592], "temperature": 0.0, "avg_logprob": -0.26952664511544366, "compression_ratio": 1.394736842105263, "no_speech_prob": 0.018812596797943115}, {"id": 119, "seek": 85596, "start": 856.76, "end": 866.6800000000001, "text": " And it seemed to work. Sorry. It was asked how big of CSV files it could handle,", "tokens": [50404, 400, 309, 6576, 281, 589, 13, 4919, 13, 467, 390, 2351, 577, 955, 295, 48814, 7098, 309, 727, 4813, 11, 50900], "temperature": 0.0, "avg_logprob": -0.183334747950236, "compression_ratio": 1.4387755102040816, "no_speech_prob": 0.0021707029081881046}, {"id": 120, "seek": 85596, "start": 867.8000000000001, "end": 873.0, "text": " and I answered more than 100,000. So it's actually also used in production for", "tokens": [50956, 293, 286, 10103, 544, 813, 2319, 11, 1360, 13, 407, 309, 311, 767, 611, 1143, 294, 4265, 337, 51216], "temperature": 0.0, "avg_logprob": -0.183334747950236, "compression_ratio": 1.4387755102040816, "no_speech_prob": 0.0021707029081881046}, {"id": 121, "seek": 85596, "start": 873.64, "end": 878.6800000000001, "text": " relatively small-scale qualitative research, but it's not limited to things you could do manually", "tokens": [51248, 7226, 1359, 12, 20033, 31312, 2132, 11, 457, 309, 311, 406, 5567, 281, 721, 291, 727, 360, 16945, 51500], "temperature": 0.0, "avg_logprob": -0.183334747950236, "compression_ratio": 1.4387755102040816, "no_speech_prob": 0.0021707029081881046}, {"id": 122, "seek": 85596, "start": 878.6800000000001, "end": 881.96, "text": " anyway. So let's switch.", "tokens": [51500, 4033, 13, 407, 718, 311, 3679, 13, 51664], "temperature": 0.0, "avg_logprob": -0.183334747950236, "compression_ratio": 1.4387755102040816, "no_speech_prob": 0.0021707029081881046}], "language": "en"}