{"text": " Thank you for being here. I'm going to talk about progressive delivery and hopefully by the end of this talk you're going to know how to easily do canary deployments on Kubernetes. Who is using Kubernetes today? Raise your hand, please. Everybody. I'm not asking if everybody knows what Kubernetes is because you're in the wrong place. I'm a principal scientist on the Adobe Experience Manager Cloud Service. This is a content management system. I'm a long time open source contributor to Maven, Jenkins, Puppet, a few other things. I'm also part of the Google Developer Experts Program. But probably most of you know me because of what I did with Jenkins on Kubernetes. Some people will love it. Some people will hate me. We'll talk about that later. Actually, I just, before this talk, 15 minutes before, I realized, oh, this is 10 years ago. Time flies when people didn't know what Kubernetes was. So, what is progressive delivery? This also came, this was August 2018. This is when the term was coined at the LaunchDarly blog. And also was picked up by Red Monk. And I said, this is a great name for these things that everybody knows about. But the name kind of sums up very well what we're trying to do. So, I said, I'm going to steal this. That's the gist of it. So, it includes deployment strategies that avoid this. I'm going to push this to all my nodes, all my containers, all my files, whatever it is that you're running. I'm going to push this new version to all of them. And if it breaks, it breaks for everybody, we want to avoid that. So, you, with progressive delivery, you have new versions that do not replace the system versions. And you have both old and new versions running in parallel for an amount of time. But the interesting part is that this is happening in production. And you can evaluate both old version, new version during a period of time that you figure out what was the best time for you. And before saying that this is a successful thing that I need to roll to everybody in all my customers. So, continuous delivery is hard. I used to say, like, progressive delivery makes it continuous delivery easier to adopt. Because it reduces a lot of the risk associated with continuous delivery. Yeah, it's great that you commit something to main and it gets pushed to everybody. But what if that's breaking in production? Then you have these methods behind progressive delivery that will prevent you from breaking things. And give you these guardrails that will protect your users. The key points, avoiding downtime, limit the blast radius. You deploy something, it only affects a subset of your users, not all of them. And also shorten the time from your idea to production. So, from the time you create a commit until you push it to production, you can use these techniques. So, you can shorten really as much as possible that time. And it's not affecting your life customers, it could affect your maybe internal customers, employees, something like that. So, you can confidently push things to production. The name is great, but all the techniques already existed for a long time. We have rolling updates on Kubernetes. This is the standard way when you change something on your deployments. You just get a new pod with a new version. When that pod comes up, the old pods start going away. And you can configure that easily on Kubernetes. You can configure how many pods you want to come up, if you want them to come up a little by a little. If you want all of them to come up at once, and they will start rolling. So, Kubernetes has been around. Blue-green deployments, same thing. It's been around forever. Well, defined for some definition of ever. And you have green, what you consider the old version, which is green, the new version, which is blue, or the other one, we're on, I don't know. And you have both running at the same time. You evaluate or you start sending traffic to the new version. And if something happens, you just have to flip the switch to put back to the old version. So, this is a variation. The difference is, in a couple of days, you don't need to have all the machines running at the same time, all the containers. With blue-green, you need to have room for both versions running at the same time. Canary deployment is one of the most interesting ones, where you send a percentage of the traffic or a percentage of your users to the new version, and a percentage, a small percentage to the new version, and you keep growing. I mean, you could just do a small percentage, or you could keep growing that canary percentage. A lot of companies do this. First, this gets deployed to internal employees only, then some countries, some like New Zealand, or a percentage of users, depending on some characteristics of them. And they keep growing this canary pool over time until you reach 100%. Feature Flags, another interesting one, where it allows you to push things to production behind Feature Flags, so you can test them in production. And also, disable them after you deploy them. You push something, you realize, either it breaks for a lot of users, or it breaks for a percentage of users, you can switch that feature off using some tools, or using something as simple as environment variables. But yeah, there's tools that allow you to manage Feature Flags, so you don't have to deal with environment variables, things like that. Monitoring is the new testing, so you know the goal is to know when the users are experiencing issues in production, and the other characteristic, I think, is they react to the issues automatically. So if you deploy something that is bad, how you can automatically roll it back before some human has to go and figure out what happened. So did you know that 90% of the outages could be solved? There's a study that said 90% of the outages could be solved with progressive delivery. Did you know that? No? Because I just made that up. And one thing they need is, yeah, some requirement is having a good amount of metrics, or you need to know what's happening in your production system before you can react, knowing what users are seeing the new version, what users are breaking with the new version, what's happening here. So you need to have this visibility. And I always love to plug Devos Barat, which disappeared from the Twitter server, to make your resumant to prepogator or in-law server in automatic way, that's what DevOps is. Raise your hand if you have broken a lot of servers by doing this automatically. So yeah, what I love to say is if you're breaking something automatically, is that you haven't automated enough. I think that's the... When you get there, it's like, okay, maybe I should step back a little bit. Until you get there, you keep automating things. Now, more to the practical side, how can I do this in Kubernetes? Introduction, who's familiar with Ingress? Ingress in Kubernetes, okay, yes. Then, 10 years ago, this was not like this. So on Kubernetes, you have the load balancer, and you can have services, and from the load balancer, Kubernetes will send you to one service or another. So your load balancer would send traffic to one service or another. But this was kind of the old way. The new way is you have Ingress controllers on Kubernetes, where the Ingress controller is running on Kubernetes. I typically domain names, but you could also do headers. For each of these traffic, it's easy. You can do headers, you can do all sorts of things. And that Ingress is sending the traffic to whatever service you're running. So you can have one service A, one service B, and with their pods. And the Ingress is the one that's, okay, you configure this domain to go to the service, you configure this domain to go to this other server. And there's a lot of Ingress controllers out there. If you run on a cloud provider, you're going to have the AWS, the GC, whatever. And then you can have your own NGINX, ambassador, STO, traffic. That's a lot of it. And ARGO rollouts, anybody using ARGO? Wow, okay. What are you doing here? I mean, we already know this. So provides advanced deployment capabilities. All the things that I mentioned, blue, green, canary, category analysis, experimentation, there are variations over the same thing. ARGO rollouts provides that to you and makes it very easy to do it. And the good thing is you don't need to use ARGO CD, for it to use ARGO rollouts. You don't actually need to use anything else to use ARGO rollouts. You can run ARGO rollouts just with Kubernetes, nothing else. You don't need external dependencies. And, yeah, it allows you to do this very easily. I'm a bit on the architecture of ARGO rollouts. So we have the controller that is watching a new object called a rollout. So ARGO rollouts has this object that can replace or complement your existing deployments. And I think I'll go down there in a bit. So this rollout manages the replica sets. So these replica sets, typically, you would have your deployments with the replica sets, and now they become part of the rollout. And it has the concept of analysis run that will check metrics or any other external source that this analysis will decide is this rollout successful or not. And based on that, it's going to cancel the rollout or keep it going. So you get the traffic coming from the ingress into your services, and you can tell ARGO, OK, send me, send traffic to this new canary replica set or send it to the old one. The percentage base one, for that, you need a service match. So if you need to do something fancy like, oh, I want to send 1% of the traffic or I want some traffic that matches this header, then you need something like a service match or the integration with the ARGO rollouts integration with the ingress controller. But if you use bare Kubernetes without integration between rollouts, you can still do it. Basically, it will use the number of pods in the replica sets. So if you have 10 pods, you can tell ARGO, OK, one new pod is going to go to the new version, and now you have a 10%, 90% sort of split, more or less. You cannot do things fancy that require support from the ingress controller or a service match, but you can still do things. The rollout object, you have two ways of defining the rollout. One is you replace the deployment with the rollout and add extra fields, or you create a rollout that points to a deployment. I don't like a lot the way of replacing the deployment because then people that are not aware of the rollouts objects, they may go and see, oh, there's no deployments. What's going on here? So it requires you changing things. And for us, it requires also you have to change rambles, you have to change commands that people need to secure documentation and all that. So I don't know why the decision was made that way, but it's not something that I'm too happy about it. Of course, you require all the Jamel tools to write these things. And let's go to the demo now. So I have here... So I'm running the Argo Rollouts demo. This is hitting the backend and it's returning one color or another, depending on what is running on the backend. So right now I have the blue one. Let me see how can I do this easier. So what I'm going to do is to change, update my deployment to use a new image that is going to be green. And ring. I lost the terminal. Okay, so it updated the image and let me fit here in big To show what this is doing. Okay, I think I pushed twice and now I see two Rollouts happening at the same time. Otherwise it's not working. Here it is. Okay, so I have the green. The one that shows green is the one that was running and it's stable. So I think I have five pots running and I push a new change, which is the canary. And this should be using the green. Okay, there it is. So like 20% of the traffic is getting green. Right? And how I define this rollout, this one is at the bottom is just the standard deployment configuration. So what image do I want? What ports do I want to expose and so on. But at the top I have the strategy configuration from Marco rollouts. So I can say point to this analysis template. This is what defines what is successful, what is not successful. And I'll show you that in a bit. And it's, I have several steps. So set weight 20 and then do a pause, set weight 40, pause for 10 seconds, set weight 60. So this is percentage. Pause for 10 seconds, set weight 80. So this is my definition of a rollout. 20% wait for me to manually do something. I only do that for demos in real life. That's a bit harder to do as you could still do it. But this is my definition of what the rollout is. So right now it's waiting because I set a pause and it's waiting for me to give you a key. I look at it, it says it looks okay. So I can do the promote. And this is going to continue through the rest of the steps. So hopefully we'll see this in like 60 seconds. It should continue the progression until everybody receives a green. A green color when they call the API. So this shows you just by creating a rollout object with this small section defining what your rollout is, you can do this. There's nothing else you need. Well, you need to install Argo. And what else can you do? Oh, yes, you can also have a preview version. So you can have another ingress pointing to your preview version. So you can even if I said I want zero traffic to go to the new version. All the existing traffic I wanted to go to the old version, but I want to see the new version in a new place. I can do that too. So that's very useful for preview environments sort of thing. So if I go back, okay. So while this continues running, this is running on Google Kubernetes and sending autopilot clusters, but you can run it in any Kubernetes. And the autopilot is pretty cool because you only pay for what you use. So if you scale things to zero, then you don't pay anything. What does it says here? Okay. So now green is the stable one. It says here, stable here. What if I want to do... I was talking about how does this protect me, right? What if I want to do a rollout that is broken? So... Let's see. This works. Right. Okay. So now I push an image that is bad. So I'm changing the deployment. Of course, you would do this with the GitOps. You would never push the production, but YOLO. And so I'm pushing the red image, but this red image is returning in 500 errors. And now Argo realized, oh, this is giving errors based on my analysis template that I'll show you. And this is in the graded status. And it went down and the scale it down, and my canary was set as failed. And you see that only a few percentage of traffic got the red dots, and then it was automatically rolled back. So I think this is the power of doing progressive delivery. Of course, this is very easy if your application is exploding. It's very easy to see. It's like, what if people ask me, oh, can we do this if a button doesn't work? Can we do this? Well, it depends what button. If it's the button that adds, imagine you're in Amazon, you break the button that adds things to the cart, and you get a metric that says nobody's adding things to the cart, maybe you're like, oh, something is really bad. Right. So let me show you the analysis template. Is this one? Yeah. In my case, my analysis template is a very complicated call that fails if this fails, if this doesn't return a 200. But again, you can integrate this with whatever you want, metrics. Argo rollouts also gives you a nice dashboard. If you are not into the command line, you can come here. And here. So where I can see the status of my rollout, what is strategy. As I said, Argo rollout supports multiple strategies on some of the more complex drivers. I can see my steps that I showed you before in the Jamel, 20, 40, 50, 80. And I can say what was the last image that I pushed, and I could click here and do the clickity clock instead of doing Jamel. Okay. So, yeah, what I mentioned before was if you're using service mesh, like Istio, then it integrates with a bunch of service mesh ingress providers. So you could go and say, I want 1% of traffic because Istio supports doing those things instead of saying more, because when you are using only pods, you don't have anyone here. Pod is going to receive the traffic or not. So it's more of an approximation. But with Istio and other advanced things, you can do more complex. We hook it up with Prometheus, also the support for multiple things to get metrics from. And, yeah, hopefully you'll learn how to do a progressive delivery canary deployment very easily. Just you need to do some Jamel here and there. Let me see. On here, this one. So you can have the other labels to the existing version, to the stable version as labels to the new version. So you can do other things with services on Kubernetes. You can pass what analysis you want to run and you set what steps to run. And everything else is just the template. And if in the deployment template. If you don't want to put the deployment template in the rollout object, you just point here, there's another option that says points to existing deployment. The only problem with that is that rollouts is not when you're migrating, rollouts is not going to scale down the deployment. A colleague of mine, she submitted a PR to Argus, which is going to be in the next version. So it will automatically, if you have like thousands of deployments, when you spin out a rollout with a deployment, I pointed to a deployment, when the rollout is successful, it's going to scale down the deployment. So that's how it will actually exist. Okay, so, yeah, and what's that thing? I lost my... Did I close it? Yeah. Okay, so, just a quick summary, you saw everything? And I hope that this helped you and you can try it and do it at home if you like it. And I have time for asking me two questions. Two questions. No questions. One question. I was wondering if you've been testing using the gateway API and some fingers in the waiting? So the question is if I tested using the gateway API instead of fingers, no, I have not been using the API yet, but I'm guessing that if there's no support already, there will be. Because... We did not. Yeah. Hello. So my question is that for, in case of buggy rollout, the particular traffic which is forward to the buggy instances, is it possible to automatically replicate it and send it to the stable versions after the fail? To ensure that even the traffic which hits the buggy rollout instances is served later by stable versions? So if it's... It's possible to run it back automatically, but also... Yeah, the individual traffic, individual request. So you don't want any user to see the spot? Yes. The other thing you could do, if you use a service mess, probably is send a clone the traffic and send a clone to the new version, but the actual traffic is going to the old version. And you could see if the new version is breaking or not. But also that's tricky because you need to make sure that it's not changing your state. If you are getting gets, it's fine if you are changing status. That's my point. Don't do the duplication in advance because it will go to the parallel execution, but do it only when the first execution failed because it's go to the canary instance. Yeah, I think you can do that. Send traffic to the new version, but it's a copy of the traffic that is not seen by any user. And then at some point you could say, okay, this is good. I'm promoting this. I think it's doable. Yeah, thank you. Okay. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.0, "text": " Thank you for being here.", "tokens": [50364, 1044, 291, 337, 885, 510, 13, 51014], "temperature": 0.0, "avg_logprob": -0.3581525845961137, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.21021759510040283}, {"id": 1, "seek": 0, "start": 13.0, "end": 25.0, "text": " I'm going to talk about progressive delivery and hopefully by the end of this talk you're going to know how to easily do canary deployments on Kubernetes.", "tokens": [51014, 286, 478, 516, 281, 751, 466, 16131, 8982, 293, 4696, 538, 264, 917, 295, 341, 751, 291, 434, 516, 281, 458, 577, 281, 3612, 360, 393, 822, 7274, 1117, 322, 23145, 13, 51614], "temperature": 0.0, "avg_logprob": -0.3581525845961137, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.21021759510040283}, {"id": 2, "seek": 2500, "start": 25.0, "end": 30.0, "text": " Who is using Kubernetes today?", "tokens": [50364, 2102, 307, 1228, 23145, 965, 30, 50614], "temperature": 0.0, "avg_logprob": -0.2548890882922757, "compression_ratio": 1.4193548387096775, "no_speech_prob": 0.2324710637331009}, {"id": 3, "seek": 2500, "start": 30.0, "end": 33.0, "text": " Raise your hand, please. Everybody.", "tokens": [50614, 30062, 428, 1011, 11, 1767, 13, 7646, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2548890882922757, "compression_ratio": 1.4193548387096775, "no_speech_prob": 0.2324710637331009}, {"id": 4, "seek": 2500, "start": 33.0, "end": 38.0, "text": " I'm not asking if everybody knows what Kubernetes is because you're in the wrong place.", "tokens": [50764, 286, 478, 406, 3365, 498, 2201, 3255, 437, 23145, 307, 570, 291, 434, 294, 264, 2085, 1081, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2548890882922757, "compression_ratio": 1.4193548387096775, "no_speech_prob": 0.2324710637331009}, {"id": 5, "seek": 2500, "start": 38.0, "end": 45.0, "text": " I'm a principal scientist on the Adobe Experience Manager Cloud Service. This is a content management system.", "tokens": [51014, 286, 478, 257, 9716, 12662, 322, 264, 24862, 28503, 13821, 8061, 9561, 13, 639, 307, 257, 2701, 4592, 1185, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2548890882922757, "compression_ratio": 1.4193548387096775, "no_speech_prob": 0.2324710637331009}, {"id": 6, "seek": 4500, "start": 45.0, "end": 53.0, "text": " I'm a long time open source contributor to Maven, Jenkins, Puppet, a few other things.", "tokens": [50364, 286, 478, 257, 938, 565, 1269, 4009, 42859, 281, 4042, 553, 11, 41273, 11, 430, 10504, 302, 11, 257, 1326, 661, 721, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16745659157081885, "compression_ratio": 1.4454976303317535, "no_speech_prob": 0.4197949469089508}, {"id": 7, "seek": 4500, "start": 53.0, "end": 58.0, "text": " I'm also part of the Google Developer Experts Program.", "tokens": [50764, 286, 478, 611, 644, 295, 264, 3329, 44915, 12522, 1373, 8338, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16745659157081885, "compression_ratio": 1.4454976303317535, "no_speech_prob": 0.4197949469089508}, {"id": 8, "seek": 4500, "start": 58.0, "end": 65.0, "text": " But probably most of you know me because of what I did with Jenkins on Kubernetes.", "tokens": [51014, 583, 1391, 881, 295, 291, 458, 385, 570, 295, 437, 286, 630, 365, 41273, 322, 23145, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16745659157081885, "compression_ratio": 1.4454976303317535, "no_speech_prob": 0.4197949469089508}, {"id": 9, "seek": 4500, "start": 65.0, "end": 72.0, "text": " Some people will love it. Some people will hate me. We'll talk about that later.", "tokens": [51364, 2188, 561, 486, 959, 309, 13, 2188, 561, 486, 4700, 385, 13, 492, 603, 751, 466, 300, 1780, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16745659157081885, "compression_ratio": 1.4454976303317535, "no_speech_prob": 0.4197949469089508}, {"id": 10, "seek": 7200, "start": 72.0, "end": 80.0, "text": " Actually, I just, before this talk, 15 minutes before, I realized, oh, this is 10 years ago.", "tokens": [50364, 5135, 11, 286, 445, 11, 949, 341, 751, 11, 2119, 2077, 949, 11, 286, 5334, 11, 1954, 11, 341, 307, 1266, 924, 2057, 13, 50764], "temperature": 0.0, "avg_logprob": -0.22189626938257462, "compression_ratio": 1.39, "no_speech_prob": 0.11601610481739044}, {"id": 11, "seek": 7200, "start": 80.0, "end": 86.0, "text": " Time flies when people didn't know what Kubernetes was.", "tokens": [50764, 6161, 17414, 562, 561, 994, 380, 458, 437, 23145, 390, 13, 51064], "temperature": 0.0, "avg_logprob": -0.22189626938257462, "compression_ratio": 1.39, "no_speech_prob": 0.11601610481739044}, {"id": 12, "seek": 7200, "start": 86.0, "end": 90.0, "text": " So, what is progressive delivery?", "tokens": [51064, 407, 11, 437, 307, 16131, 8982, 30, 51264], "temperature": 0.0, "avg_logprob": -0.22189626938257462, "compression_ratio": 1.39, "no_speech_prob": 0.11601610481739044}, {"id": 13, "seek": 7200, "start": 90.0, "end": 95.0, "text": " This also came, this was August 2018.", "tokens": [51264, 639, 611, 1361, 11, 341, 390, 6897, 6096, 13, 51514], "temperature": 0.0, "avg_logprob": -0.22189626938257462, "compression_ratio": 1.39, "no_speech_prob": 0.11601610481739044}, {"id": 14, "seek": 7200, "start": 95.0, "end": 100.0, "text": " This is when the term was coined at the LaunchDarly blog.", "tokens": [51514, 639, 307, 562, 264, 1433, 390, 45222, 412, 264, 28119, 35, 289, 356, 6968, 13, 51764], "temperature": 0.0, "avg_logprob": -0.22189626938257462, "compression_ratio": 1.39, "no_speech_prob": 0.11601610481739044}, {"id": 15, "seek": 10000, "start": 100.0, "end": 104.0, "text": " And also was picked up by Red Monk.", "tokens": [50364, 400, 611, 390, 6183, 493, 538, 4477, 4713, 74, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1673517896417986, "compression_ratio": 1.6570247933884297, "no_speech_prob": 0.0694277286529541}, {"id": 16, "seek": 10000, "start": 104.0, "end": 109.0, "text": " And I said, this is a great name for these things that everybody knows about.", "tokens": [50564, 400, 286, 848, 11, 341, 307, 257, 869, 1315, 337, 613, 721, 300, 2201, 3255, 466, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1673517896417986, "compression_ratio": 1.6570247933884297, "no_speech_prob": 0.0694277286529541}, {"id": 17, "seek": 10000, "start": 109.0, "end": 113.0, "text": " But the name kind of sums up very well what we're trying to do.", "tokens": [50814, 583, 264, 1315, 733, 295, 34499, 493, 588, 731, 437, 321, 434, 1382, 281, 360, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1673517896417986, "compression_ratio": 1.6570247933884297, "no_speech_prob": 0.0694277286529541}, {"id": 18, "seek": 10000, "start": 113.0, "end": 118.0, "text": " So, I said, I'm going to steal this. That's the gist of it.", "tokens": [51014, 407, 11, 286, 848, 11, 286, 478, 516, 281, 11009, 341, 13, 663, 311, 264, 290, 468, 295, 309, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1673517896417986, "compression_ratio": 1.6570247933884297, "no_speech_prob": 0.0694277286529541}, {"id": 19, "seek": 10000, "start": 118.0, "end": 122.0, "text": " So, it includes deployment strategies that avoid this.", "tokens": [51264, 407, 11, 309, 5974, 19317, 9029, 300, 5042, 341, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1673517896417986, "compression_ratio": 1.6570247933884297, "no_speech_prob": 0.0694277286529541}, {"id": 20, "seek": 10000, "start": 122.0, "end": 129.0, "text": " I'm going to push this to all my nodes, all my containers, all my files, whatever it is that you're running.", "tokens": [51464, 286, 478, 516, 281, 2944, 341, 281, 439, 452, 13891, 11, 439, 452, 17089, 11, 439, 452, 7098, 11, 2035, 309, 307, 300, 291, 434, 2614, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1673517896417986, "compression_ratio": 1.6570247933884297, "no_speech_prob": 0.0694277286529541}, {"id": 21, "seek": 12900, "start": 129.0, "end": 132.0, "text": " I'm going to push this new version to all of them.", "tokens": [50364, 286, 478, 516, 281, 2944, 341, 777, 3037, 281, 439, 295, 552, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1087073008219401, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.03046625293791294}, {"id": 22, "seek": 12900, "start": 132.0, "end": 137.0, "text": " And if it breaks, it breaks for everybody, we want to avoid that.", "tokens": [50514, 400, 498, 309, 9857, 11, 309, 9857, 337, 2201, 11, 321, 528, 281, 5042, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1087073008219401, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.03046625293791294}, {"id": 23, "seek": 12900, "start": 137.0, "end": 144.0, "text": " So, you, with progressive delivery, you have new versions that do not replace the system versions.", "tokens": [50764, 407, 11, 291, 11, 365, 16131, 8982, 11, 291, 362, 777, 9606, 300, 360, 406, 7406, 264, 1185, 9606, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1087073008219401, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.03046625293791294}, {"id": 24, "seek": 12900, "start": 144.0, "end": 150.0, "text": " And you have both old and new versions running in parallel for an amount of time.", "tokens": [51114, 400, 291, 362, 1293, 1331, 293, 777, 9606, 2614, 294, 8952, 337, 364, 2372, 295, 565, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1087073008219401, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.03046625293791294}, {"id": 25, "seek": 12900, "start": 150.0, "end": 155.0, "text": " But the interesting part is that this is happening in production.", "tokens": [51414, 583, 264, 1880, 644, 307, 300, 341, 307, 2737, 294, 4265, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1087073008219401, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.03046625293791294}, {"id": 26, "seek": 15500, "start": 155.0, "end": 163.0, "text": " And you can evaluate both old version, new version during a period of time that you figure out what was the best time for you.", "tokens": [50364, 400, 291, 393, 13059, 1293, 1331, 3037, 11, 777, 3037, 1830, 257, 2896, 295, 565, 300, 291, 2573, 484, 437, 390, 264, 1151, 565, 337, 291, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15271461888363486, "compression_ratio": 1.7206477732793521, "no_speech_prob": 0.014257529750466347}, {"id": 27, "seek": 15500, "start": 163.0, "end": 172.0, "text": " And before saying that this is a successful thing that I need to roll to everybody in all my customers.", "tokens": [50764, 400, 949, 1566, 300, 341, 307, 257, 4406, 551, 300, 286, 643, 281, 3373, 281, 2201, 294, 439, 452, 4581, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15271461888363486, "compression_ratio": 1.7206477732793521, "no_speech_prob": 0.014257529750466347}, {"id": 28, "seek": 15500, "start": 172.0, "end": 179.0, "text": " So, continuous delivery is hard. I used to say, like, progressive delivery makes it continuous delivery easier to adopt.", "tokens": [51214, 407, 11, 10957, 8982, 307, 1152, 13, 286, 1143, 281, 584, 11, 411, 11, 16131, 8982, 1669, 309, 10957, 8982, 3571, 281, 6878, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15271461888363486, "compression_ratio": 1.7206477732793521, "no_speech_prob": 0.014257529750466347}, {"id": 29, "seek": 15500, "start": 179.0, "end": 182.0, "text": " Because it reduces a lot of the risk associated with continuous delivery.", "tokens": [51564, 1436, 309, 18081, 257, 688, 295, 264, 3148, 6615, 365, 10957, 8982, 13, 51714], "temperature": 0.0, "avg_logprob": -0.15271461888363486, "compression_ratio": 1.7206477732793521, "no_speech_prob": 0.014257529750466347}, {"id": 30, "seek": 18200, "start": 182.0, "end": 187.0, "text": " Yeah, it's great that you commit something to main and it gets pushed to everybody.", "tokens": [50364, 865, 11, 309, 311, 869, 300, 291, 5599, 746, 281, 2135, 293, 309, 2170, 9152, 281, 2201, 13, 50614], "temperature": 0.0, "avg_logprob": -0.15150637924671173, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.08691387623548508}, {"id": 31, "seek": 18200, "start": 187.0, "end": 197.0, "text": " But what if that's breaking in production? Then you have these methods behind progressive delivery that will prevent you from breaking things.", "tokens": [50614, 583, 437, 498, 300, 311, 7697, 294, 4265, 30, 1396, 291, 362, 613, 7150, 2261, 16131, 8982, 300, 486, 4871, 291, 490, 7697, 721, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15150637924671173, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.08691387623548508}, {"id": 32, "seek": 18200, "start": 197.0, "end": 206.0, "text": " And give you these guardrails that will protect your users.", "tokens": [51114, 400, 976, 291, 613, 6290, 424, 4174, 300, 486, 2371, 428, 5022, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15150637924671173, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.08691387623548508}, {"id": 33, "seek": 20600, "start": 206.0, "end": 211.0, "text": " The key points, avoiding downtime, limit the blast radius.", "tokens": [50364, 440, 2141, 2793, 11, 20220, 49648, 11, 4948, 264, 12035, 15845, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12704084136269309, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.21320173144340515}, {"id": 34, "seek": 20600, "start": 211.0, "end": 216.0, "text": " You deploy something, it only affects a subset of your users, not all of them.", "tokens": [50614, 509, 7274, 746, 11, 309, 787, 11807, 257, 25993, 295, 428, 5022, 11, 406, 439, 295, 552, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12704084136269309, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.21320173144340515}, {"id": 35, "seek": 20600, "start": 216.0, "end": 220.0, "text": " And also shorten the time from your idea to production.", "tokens": [50864, 400, 611, 39632, 264, 565, 490, 428, 1558, 281, 4265, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12704084136269309, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.21320173144340515}, {"id": 36, "seek": 20600, "start": 220.0, "end": 227.0, "text": " So, from the time you create a commit until you push it to production, you can use these techniques.", "tokens": [51064, 407, 11, 490, 264, 565, 291, 1884, 257, 5599, 1826, 291, 2944, 309, 281, 4265, 11, 291, 393, 764, 613, 7512, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12704084136269309, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.21320173144340515}, {"id": 37, "seek": 20600, "start": 227.0, "end": 232.0, "text": " So, you can shorten really as much as possible that time.", "tokens": [51414, 407, 11, 291, 393, 39632, 534, 382, 709, 382, 1944, 300, 565, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12704084136269309, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.21320173144340515}, {"id": 38, "seek": 23200, "start": 232.0, "end": 239.0, "text": " And it's not affecting your life customers, it could affect your maybe internal customers, employees, something like that.", "tokens": [50364, 400, 309, 311, 406, 17476, 428, 993, 4581, 11, 309, 727, 3345, 428, 1310, 6920, 4581, 11, 6619, 11, 746, 411, 300, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10375102758407592, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.10920585691928864}, {"id": 39, "seek": 23200, "start": 239.0, "end": 243.0, "text": " So, you can confidently push things to production.", "tokens": [50714, 407, 11, 291, 393, 41956, 2944, 721, 281, 4265, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10375102758407592, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.10920585691928864}, {"id": 40, "seek": 23200, "start": 243.0, "end": 249.0, "text": " The name is great, but all the techniques already existed for a long time.", "tokens": [50914, 440, 1315, 307, 869, 11, 457, 439, 264, 7512, 1217, 13135, 337, 257, 938, 565, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10375102758407592, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.10920585691928864}, {"id": 41, "seek": 23200, "start": 249.0, "end": 258.0, "text": " We have rolling updates on Kubernetes. This is the standard way when you change something on your deployments.", "tokens": [51214, 492, 362, 9439, 9205, 322, 23145, 13, 639, 307, 264, 3832, 636, 562, 291, 1319, 746, 322, 428, 7274, 1117, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10375102758407592, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.10920585691928864}, {"id": 42, "seek": 25800, "start": 258.0, "end": 265.0, "text": " You just get a new pod with a new version. When that pod comes up, the old pods start going away.", "tokens": [50364, 509, 445, 483, 257, 777, 2497, 365, 257, 777, 3037, 13, 1133, 300, 2497, 1487, 493, 11, 264, 1331, 31925, 722, 516, 1314, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11067673217418582, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.4113282859325409}, {"id": 43, "seek": 25800, "start": 265.0, "end": 269.0, "text": " And you can configure that easily on Kubernetes.", "tokens": [50714, 400, 291, 393, 22162, 300, 3612, 322, 23145, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11067673217418582, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.4113282859325409}, {"id": 44, "seek": 25800, "start": 269.0, "end": 276.0, "text": " You can configure how many pods you want to come up, if you want them to come up a little by a little.", "tokens": [50914, 509, 393, 22162, 577, 867, 31925, 291, 528, 281, 808, 493, 11, 498, 291, 528, 552, 281, 808, 493, 257, 707, 538, 257, 707, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11067673217418582, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.4113282859325409}, {"id": 45, "seek": 25800, "start": 276.0, "end": 281.0, "text": " If you want all of them to come up at once, and they will start rolling.", "tokens": [51264, 759, 291, 528, 439, 295, 552, 281, 808, 493, 412, 1564, 11, 293, 436, 486, 722, 9439, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11067673217418582, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.4113282859325409}, {"id": 46, "seek": 28100, "start": 281.0, "end": 291.0, "text": " So, Kubernetes has been around.", "tokens": [50364, 407, 11, 23145, 575, 668, 926, 13, 50864], "temperature": 0.0, "avg_logprob": -0.29402117185954807, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.3757151961326599}, {"id": 47, "seek": 28100, "start": 291.0, "end": 296.0, "text": " Blue-green deployments, same thing. It's been around forever.", "tokens": [50864, 8510, 12, 27399, 7274, 1117, 11, 912, 551, 13, 467, 311, 668, 926, 5680, 13, 51114], "temperature": 0.0, "avg_logprob": -0.29402117185954807, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.3757151961326599}, {"id": 48, "seek": 28100, "start": 296.0, "end": 299.0, "text": " Well, defined for some definition of ever.", "tokens": [51114, 1042, 11, 7642, 337, 512, 7123, 295, 1562, 13, 51264], "temperature": 0.0, "avg_logprob": -0.29402117185954807, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.3757151961326599}, {"id": 49, "seek": 28100, "start": 299.0, "end": 308.0, "text": " And you have green, what you consider the old version, which is green, the new version, which is blue, or the other one, we're on, I don't know.", "tokens": [51264, 400, 291, 362, 3092, 11, 437, 291, 1949, 264, 1331, 3037, 11, 597, 307, 3092, 11, 264, 777, 3037, 11, 597, 307, 3344, 11, 420, 264, 661, 472, 11, 321, 434, 322, 11, 286, 500, 380, 458, 13, 51714], "temperature": 0.0, "avg_logprob": -0.29402117185954807, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.3757151961326599}, {"id": 50, "seek": 30800, "start": 308.0, "end": 312.0, "text": " And you have both running at the same time.", "tokens": [50364, 400, 291, 362, 1293, 2614, 412, 264, 912, 565, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13710128996107313, "compression_ratio": 1.6682692307692308, "no_speech_prob": 0.06626283377408981}, {"id": 51, "seek": 30800, "start": 312.0, "end": 318.0, "text": " You evaluate or you start sending traffic to the new version.", "tokens": [50564, 509, 13059, 420, 291, 722, 7750, 6419, 281, 264, 777, 3037, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13710128996107313, "compression_ratio": 1.6682692307692308, "no_speech_prob": 0.06626283377408981}, {"id": 52, "seek": 30800, "start": 318.0, "end": 323.0, "text": " And if something happens, you just have to flip the switch to put back to the old version.", "tokens": [50864, 400, 498, 746, 2314, 11, 291, 445, 362, 281, 7929, 264, 3679, 281, 829, 646, 281, 264, 1331, 3037, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13710128996107313, "compression_ratio": 1.6682692307692308, "no_speech_prob": 0.06626283377408981}, {"id": 53, "seek": 30800, "start": 323.0, "end": 327.0, "text": " So, this is a variation.", "tokens": [51114, 407, 11, 341, 307, 257, 12990, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13710128996107313, "compression_ratio": 1.6682692307692308, "no_speech_prob": 0.06626283377408981}, {"id": 54, "seek": 30800, "start": 327.0, "end": 333.0, "text": " The difference is, in a couple of days, you don't need to have all the machines running at the same time, all the containers.", "tokens": [51314, 440, 2649, 307, 11, 294, 257, 1916, 295, 1708, 11, 291, 500, 380, 643, 281, 362, 439, 264, 8379, 2614, 412, 264, 912, 565, 11, 439, 264, 17089, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13710128996107313, "compression_ratio": 1.6682692307692308, "no_speech_prob": 0.06626283377408981}, {"id": 55, "seek": 33300, "start": 333.0, "end": 343.0, "text": " With blue-green, you need to have room for both versions running at the same time.", "tokens": [50364, 2022, 3344, 12, 27399, 11, 291, 643, 281, 362, 1808, 337, 1293, 9606, 2614, 412, 264, 912, 565, 13, 50864], "temperature": 0.0, "avg_logprob": -0.21354604411769557, "compression_ratio": 1.7758620689655173, "no_speech_prob": 0.06474887579679489}, {"id": 56, "seek": 33300, "start": 343.0, "end": 354.0, "text": " Canary deployment is one of the most interesting ones, where you send a percentage of the traffic or a percentage of your users to the new version,", "tokens": [50864, 1664, 822, 19317, 307, 472, 295, 264, 881, 1880, 2306, 11, 689, 291, 2845, 257, 9668, 295, 264, 6419, 420, 257, 9668, 295, 428, 5022, 281, 264, 777, 3037, 11, 51414], "temperature": 0.0, "avg_logprob": -0.21354604411769557, "compression_ratio": 1.7758620689655173, "no_speech_prob": 0.06474887579679489}, {"id": 57, "seek": 33300, "start": 354.0, "end": 358.0, "text": " and a percentage, a small percentage to the new version, and you keep growing.", "tokens": [51414, 293, 257, 9668, 11, 257, 1359, 9668, 281, 264, 777, 3037, 11, 293, 291, 1066, 4194, 13, 51614], "temperature": 0.0, "avg_logprob": -0.21354604411769557, "compression_ratio": 1.7758620689655173, "no_speech_prob": 0.06474887579679489}, {"id": 58, "seek": 35800, "start": 358.0, "end": 366.0, "text": " I mean, you could just do a small percentage, or you could keep growing that canary percentage.", "tokens": [50364, 286, 914, 11, 291, 727, 445, 360, 257, 1359, 9668, 11, 420, 291, 727, 1066, 4194, 300, 393, 822, 9668, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12351678399478688, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.42986953258514404}, {"id": 59, "seek": 35800, "start": 366.0, "end": 368.0, "text": " A lot of companies do this.", "tokens": [50764, 316, 688, 295, 3431, 360, 341, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12351678399478688, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.42986953258514404}, {"id": 60, "seek": 35800, "start": 368.0, "end": 382.0, "text": " First, this gets deployed to internal employees only, then some countries, some like New Zealand, or a percentage of users, depending on some characteristics of them.", "tokens": [50864, 2386, 11, 341, 2170, 17826, 281, 6920, 6619, 787, 11, 550, 512, 3517, 11, 512, 411, 1873, 13883, 11, 420, 257, 9668, 295, 5022, 11, 5413, 322, 512, 10891, 295, 552, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12351678399478688, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.42986953258514404}, {"id": 61, "seek": 38200, "start": 382.0, "end": 391.0, "text": " And they keep growing this canary pool over time until you reach 100%.", "tokens": [50364, 400, 436, 1066, 4194, 341, 393, 822, 7005, 670, 565, 1826, 291, 2524, 2319, 6856, 50814], "temperature": 0.0, "avg_logprob": -0.14590848776010368, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.07326935231685638}, {"id": 62, "seek": 38200, "start": 391.0, "end": 401.0, "text": " Feature Flags, another interesting one, where it allows you to push things to production behind Feature Flags, so you can test them in production.", "tokens": [50814, 3697, 1503, 3235, 12109, 11, 1071, 1880, 472, 11, 689, 309, 4045, 291, 281, 2944, 721, 281, 4265, 2261, 3697, 1503, 3235, 12109, 11, 370, 291, 393, 1500, 552, 294, 4265, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14590848776010368, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.07326935231685638}, {"id": 63, "seek": 38200, "start": 401.0, "end": 406.0, "text": " And also, disable them after you deploy them.", "tokens": [51314, 400, 611, 11, 28362, 552, 934, 291, 7274, 552, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14590848776010368, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.07326935231685638}, {"id": 64, "seek": 40600, "start": 406.0, "end": 419.0, "text": " You push something, you realize, either it breaks for a lot of users, or it breaks for a percentage of users, you can switch that feature off using some tools,", "tokens": [50364, 509, 2944, 746, 11, 291, 4325, 11, 2139, 309, 9857, 337, 257, 688, 295, 5022, 11, 420, 309, 9857, 337, 257, 9668, 295, 5022, 11, 291, 393, 3679, 300, 4111, 766, 1228, 512, 3873, 11, 51014], "temperature": 0.0, "avg_logprob": -0.15424320496708513, "compression_ratio": 1.7412935323383085, "no_speech_prob": 0.13534148037433624}, {"id": 65, "seek": 40600, "start": 419.0, "end": 423.0, "text": " or using something as simple as environment variables.", "tokens": [51014, 420, 1228, 746, 382, 2199, 382, 2823, 9102, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15424320496708513, "compression_ratio": 1.7412935323383085, "no_speech_prob": 0.13534148037433624}, {"id": 66, "seek": 40600, "start": 423.0, "end": 435.0, "text": " But yeah, there's tools that allow you to manage Feature Flags, so you don't have to deal with environment variables, things like that.", "tokens": [51214, 583, 1338, 11, 456, 311, 3873, 300, 2089, 291, 281, 3067, 3697, 1503, 3235, 12109, 11, 370, 291, 500, 380, 362, 281, 2028, 365, 2823, 9102, 11, 721, 411, 300, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15424320496708513, "compression_ratio": 1.7412935323383085, "no_speech_prob": 0.13534148037433624}, {"id": 67, "seek": 43500, "start": 436.0, "end": 444.0, "text": " Monitoring is the new testing, so you know the goal is to know when the users are experiencing issues in production,", "tokens": [50414, 33799, 278, 307, 264, 777, 4997, 11, 370, 291, 458, 264, 3387, 307, 281, 458, 562, 264, 5022, 366, 11139, 2663, 294, 4265, 11, 50814], "temperature": 0.0, "avg_logprob": -0.11925402440522846, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.05109962821006775}, {"id": 68, "seek": 43500, "start": 444.0, "end": 449.0, "text": " and the other characteristic, I think, is they react to the issues automatically.", "tokens": [50814, 293, 264, 661, 16282, 11, 286, 519, 11, 307, 436, 4515, 281, 264, 2663, 6772, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11925402440522846, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.05109962821006775}, {"id": 69, "seek": 43500, "start": 449.0, "end": 461.0, "text": " So if you deploy something that is bad, how you can automatically roll it back before some human has to go and figure out what happened.", "tokens": [51064, 407, 498, 291, 7274, 746, 300, 307, 1578, 11, 577, 291, 393, 6772, 3373, 309, 646, 949, 512, 1952, 575, 281, 352, 293, 2573, 484, 437, 2011, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11925402440522846, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.05109962821006775}, {"id": 70, "seek": 46100, "start": 462.0, "end": 466.0, "text": " So did you know that 90% of the outages could be solved?", "tokens": [50414, 407, 630, 291, 458, 300, 4289, 4, 295, 264, 484, 1660, 727, 312, 13041, 30, 50614], "temperature": 0.0, "avg_logprob": -0.14347957571347555, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.011738136410713196}, {"id": 71, "seek": 46100, "start": 466.0, "end": 470.0, "text": " There's a study that said 90% of the outages could be solved with progressive delivery.", "tokens": [50614, 821, 311, 257, 2979, 300, 848, 4289, 4, 295, 264, 484, 1660, 727, 312, 13041, 365, 16131, 8982, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14347957571347555, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.011738136410713196}, {"id": 72, "seek": 46100, "start": 470.0, "end": 473.0, "text": " Did you know that? No? Because I just made that up.", "tokens": [50814, 2589, 291, 458, 300, 30, 883, 30, 1436, 286, 445, 1027, 300, 493, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14347957571347555, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.011738136410713196}, {"id": 73, "seek": 46100, "start": 477.0, "end": 482.0, "text": " And one thing they need is, yeah, some requirement is having a good amount of metrics,", "tokens": [51164, 400, 472, 551, 436, 643, 307, 11, 1338, 11, 512, 11695, 307, 1419, 257, 665, 2372, 295, 16367, 11, 51414], "temperature": 0.0, "avg_logprob": -0.14347957571347555, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.011738136410713196}, {"id": 74, "seek": 46100, "start": 482.0, "end": 486.0, "text": " or you need to know what's happening in your production system before you can react,", "tokens": [51414, 420, 291, 643, 281, 458, 437, 311, 2737, 294, 428, 4265, 1185, 949, 291, 393, 4515, 11, 51614], "temperature": 0.0, "avg_logprob": -0.14347957571347555, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.011738136410713196}, {"id": 75, "seek": 48600, "start": 486.0, "end": 493.0, "text": " knowing what users are seeing the new version, what users are breaking with the new version, what's happening here.", "tokens": [50364, 5276, 437, 5022, 366, 2577, 264, 777, 3037, 11, 437, 5022, 366, 7697, 365, 264, 777, 3037, 11, 437, 311, 2737, 510, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2731906399868502, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.059605199843645096}, {"id": 76, "seek": 48600, "start": 493.0, "end": 495.0, "text": " So you need to have this visibility.", "tokens": [50714, 407, 291, 643, 281, 362, 341, 19883, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2731906399868502, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.059605199843645096}, {"id": 77, "seek": 48600, "start": 497.0, "end": 503.0, "text": " And I always love to plug Devos Barat, which disappeared from the Twitter server,", "tokens": [50914, 400, 286, 1009, 959, 281, 5452, 9096, 329, 4156, 267, 11, 597, 13954, 490, 264, 5794, 7154, 11, 51214], "temperature": 0.0, "avg_logprob": -0.2731906399868502, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.059605199843645096}, {"id": 78, "seek": 48600, "start": 503.0, "end": 509.0, "text": " to make your resumant to prepogator or in-law server in automatic way, that's what DevOps is.", "tokens": [51214, 281, 652, 428, 725, 449, 394, 281, 2666, 664, 1639, 420, 294, 12, 5901, 7154, 294, 12509, 636, 11, 300, 311, 437, 43051, 307, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2731906399868502, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.059605199843645096}, {"id": 79, "seek": 48600, "start": 509.0, "end": 513.0, "text": " Raise your hand if you have broken a lot of servers by doing this automatically.", "tokens": [51514, 30062, 428, 1011, 498, 291, 362, 5463, 257, 688, 295, 15909, 538, 884, 341, 6772, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2731906399868502, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.059605199843645096}, {"id": 80, "seek": 51600, "start": 516.0, "end": 522.0, "text": " So yeah, what I love to say is if you're breaking something automatically,", "tokens": [50364, 407, 1338, 11, 437, 286, 959, 281, 584, 307, 498, 291, 434, 7697, 746, 6772, 11, 50664], "temperature": 0.0, "avg_logprob": -0.19397972442291594, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.009382609277963638}, {"id": 81, "seek": 51600, "start": 522.0, "end": 524.0, "text": " is that you haven't automated enough.", "tokens": [50664, 307, 300, 291, 2378, 380, 18473, 1547, 13, 50764], "temperature": 0.0, "avg_logprob": -0.19397972442291594, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.009382609277963638}, {"id": 82, "seek": 51600, "start": 524.0, "end": 527.0, "text": " I think that's the...", "tokens": [50764, 286, 519, 300, 311, 264, 485, 50914], "temperature": 0.0, "avg_logprob": -0.19397972442291594, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.009382609277963638}, {"id": 83, "seek": 51600, "start": 527.0, "end": 530.0, "text": " When you get there, it's like, okay, maybe I should step back a little bit.", "tokens": [50914, 1133, 291, 483, 456, 11, 309, 311, 411, 11, 1392, 11, 1310, 286, 820, 1823, 646, 257, 707, 857, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19397972442291594, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.009382609277963638}, {"id": 84, "seek": 51600, "start": 530.0, "end": 532.0, "text": " Until you get there, you keep automating things.", "tokens": [51064, 9088, 291, 483, 456, 11, 291, 1066, 3553, 990, 721, 13, 51164], "temperature": 0.0, "avg_logprob": -0.19397972442291594, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.009382609277963638}, {"id": 85, "seek": 51600, "start": 535.0, "end": 539.0, "text": " Now, more to the practical side, how can I do this in Kubernetes?", "tokens": [51314, 823, 11, 544, 281, 264, 8496, 1252, 11, 577, 393, 286, 360, 341, 294, 23145, 30, 51514], "temperature": 0.0, "avg_logprob": -0.19397972442291594, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.009382609277963638}, {"id": 86, "seek": 53900, "start": 540.0, "end": 546.0, "text": " Introduction, who's familiar with Ingress?", "tokens": [50414, 27193, 882, 11, 567, 311, 4963, 365, 682, 3091, 30, 50714], "temperature": 0.0, "avg_logprob": -0.16847381796888125, "compression_ratio": 1.7947368421052632, "no_speech_prob": 0.034896813333034515}, {"id": 87, "seek": 53900, "start": 546.0, "end": 549.0, "text": " Ingress in Kubernetes, okay, yes.", "tokens": [50714, 682, 3091, 294, 23145, 11, 1392, 11, 2086, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16847381796888125, "compression_ratio": 1.7947368421052632, "no_speech_prob": 0.034896813333034515}, {"id": 88, "seek": 53900, "start": 549.0, "end": 552.0, "text": " Then, 10 years ago, this was not like this.", "tokens": [50864, 1396, 11, 1266, 924, 2057, 11, 341, 390, 406, 411, 341, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16847381796888125, "compression_ratio": 1.7947368421052632, "no_speech_prob": 0.034896813333034515}, {"id": 89, "seek": 53900, "start": 554.0, "end": 558.0, "text": " So on Kubernetes, you have the load balancer, and you can have services,", "tokens": [51114, 407, 322, 23145, 11, 291, 362, 264, 3677, 3119, 28347, 11, 293, 291, 393, 362, 3328, 11, 51314], "temperature": 0.0, "avg_logprob": -0.16847381796888125, "compression_ratio": 1.7947368421052632, "no_speech_prob": 0.034896813333034515}, {"id": 90, "seek": 53900, "start": 558.0, "end": 563.0, "text": " and from the load balancer, Kubernetes will send you to one service or another.", "tokens": [51314, 293, 490, 264, 3677, 3119, 28347, 11, 23145, 486, 2845, 291, 281, 472, 2643, 420, 1071, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16847381796888125, "compression_ratio": 1.7947368421052632, "no_speech_prob": 0.034896813333034515}, {"id": 91, "seek": 53900, "start": 563.0, "end": 567.0, "text": " So your load balancer would send traffic to one service or another.", "tokens": [51564, 407, 428, 3677, 3119, 28347, 576, 2845, 6419, 281, 472, 2643, 420, 1071, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16847381796888125, "compression_ratio": 1.7947368421052632, "no_speech_prob": 0.034896813333034515}, {"id": 92, "seek": 56700, "start": 568.0, "end": 570.0, "text": " But this was kind of the old way.", "tokens": [50414, 583, 341, 390, 733, 295, 264, 1331, 636, 13, 50514], "temperature": 0.0, "avg_logprob": -0.17677505729124718, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.006977004464715719}, {"id": 93, "seek": 56700, "start": 570.0, "end": 574.0, "text": " The new way is you have Ingress controllers on Kubernetes,", "tokens": [50514, 440, 777, 636, 307, 291, 362, 682, 3091, 26903, 322, 23145, 11, 50714], "temperature": 0.0, "avg_logprob": -0.17677505729124718, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.006977004464715719}, {"id": 94, "seek": 56700, "start": 574.0, "end": 578.0, "text": " where the Ingress controller is running on Kubernetes.", "tokens": [50714, 689, 264, 682, 3091, 10561, 307, 2614, 322, 23145, 13, 50914], "temperature": 0.0, "avg_logprob": -0.17677505729124718, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.006977004464715719}, {"id": 95, "seek": 56700, "start": 580.0, "end": 585.0, "text": " I typically domain names, but you could also do headers.", "tokens": [51014, 286, 5850, 9274, 5288, 11, 457, 291, 727, 611, 360, 45101, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17677505729124718, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.006977004464715719}, {"id": 96, "seek": 56700, "start": 585.0, "end": 587.0, "text": " For each of these traffic, it's easy.", "tokens": [51264, 1171, 1184, 295, 613, 6419, 11, 309, 311, 1858, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17677505729124718, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.006977004464715719}, {"id": 97, "seek": 56700, "start": 587.0, "end": 590.0, "text": " You can do headers, you can do all sorts of things.", "tokens": [51364, 509, 393, 360, 45101, 11, 291, 393, 360, 439, 7527, 295, 721, 13, 51514], "temperature": 0.0, "avg_logprob": -0.17677505729124718, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.006977004464715719}, {"id": 98, "seek": 56700, "start": 591.0, "end": 595.0, "text": " And that Ingress is sending the traffic to whatever service you're running.", "tokens": [51564, 400, 300, 682, 3091, 307, 7750, 264, 6419, 281, 2035, 2643, 291, 434, 2614, 13, 51764], "temperature": 0.0, "avg_logprob": -0.17677505729124718, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.006977004464715719}, {"id": 99, "seek": 59500, "start": 595.0, "end": 599.0, "text": " So you can have one service A, one service B, and with their pods.", "tokens": [50364, 407, 291, 393, 362, 472, 2643, 316, 11, 472, 2643, 363, 11, 293, 365, 641, 31925, 13, 50564], "temperature": 0.0, "avg_logprob": -0.18150161874705348, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.001986890798434615}, {"id": 100, "seek": 59500, "start": 599.0, "end": 603.0, "text": " And the Ingress is the one that's, okay, you configure this domain to go to the service,", "tokens": [50564, 400, 264, 682, 3091, 307, 264, 472, 300, 311, 11, 1392, 11, 291, 22162, 341, 9274, 281, 352, 281, 264, 2643, 11, 50764], "temperature": 0.0, "avg_logprob": -0.18150161874705348, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.001986890798434615}, {"id": 101, "seek": 59500, "start": 603.0, "end": 605.0, "text": " you configure this domain to go to this other server.", "tokens": [50764, 291, 22162, 341, 9274, 281, 352, 281, 341, 661, 7154, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18150161874705348, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.001986890798434615}, {"id": 102, "seek": 59500, "start": 607.0, "end": 609.0, "text": " And there's a lot of Ingress controllers out there.", "tokens": [50964, 400, 456, 311, 257, 688, 295, 682, 3091, 26903, 484, 456, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18150161874705348, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.001986890798434615}, {"id": 103, "seek": 59500, "start": 611.0, "end": 616.0, "text": " If you run on a cloud provider, you're going to have the AWS, the GC, whatever.", "tokens": [51164, 759, 291, 1190, 322, 257, 4588, 12398, 11, 291, 434, 516, 281, 362, 264, 17650, 11, 264, 29435, 11, 2035, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18150161874705348, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.001986890798434615}, {"id": 104, "seek": 59500, "start": 616.0, "end": 623.0, "text": " And then you can have your own NGINX, ambassador, STO, traffic.", "tokens": [51414, 400, 550, 291, 393, 362, 428, 1065, 426, 38, 1464, 55, 11, 25445, 11, 4904, 46, 11, 6419, 13, 51764], "temperature": 0.0, "avg_logprob": -0.18150161874705348, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.001986890798434615}, {"id": 105, "seek": 62300, "start": 624.0, "end": 625.0, "text": " That's a lot of it.", "tokens": [50414, 663, 311, 257, 688, 295, 309, 13, 50464], "temperature": 0.0, "avg_logprob": -0.18770943747626412, "compression_ratio": 1.3099415204678362, "no_speech_prob": 0.00401815352961421}, {"id": 106, "seek": 62300, "start": 628.0, "end": 631.0, "text": " And ARGO rollouts, anybody using ARGO?", "tokens": [50614, 400, 8943, 11601, 3373, 7711, 11, 4472, 1228, 8943, 11601, 30, 50764], "temperature": 0.0, "avg_logprob": -0.18770943747626412, "compression_ratio": 1.3099415204678362, "no_speech_prob": 0.00401815352961421}, {"id": 107, "seek": 62300, "start": 632.0, "end": 633.0, "text": " Wow, okay.", "tokens": [50814, 3153, 11, 1392, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18770943747626412, "compression_ratio": 1.3099415204678362, "no_speech_prob": 0.00401815352961421}, {"id": 108, "seek": 62300, "start": 637.0, "end": 638.0, "text": " What are you doing here?", "tokens": [51064, 708, 366, 291, 884, 510, 30, 51114], "temperature": 0.0, "avg_logprob": -0.18770943747626412, "compression_ratio": 1.3099415204678362, "no_speech_prob": 0.00401815352961421}, {"id": 109, "seek": 62300, "start": 638.0, "end": 640.0, "text": " I mean, we already know this.", "tokens": [51114, 286, 914, 11, 321, 1217, 458, 341, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18770943747626412, "compression_ratio": 1.3099415204678362, "no_speech_prob": 0.00401815352961421}, {"id": 110, "seek": 62300, "start": 641.0, "end": 644.0, "text": " So provides advanced deployment capabilities.", "tokens": [51264, 407, 6417, 7339, 19317, 10862, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18770943747626412, "compression_ratio": 1.3099415204678362, "no_speech_prob": 0.00401815352961421}, {"id": 111, "seek": 62300, "start": 645.0, "end": 648.0, "text": " All the things that I mentioned, blue, green, canary,", "tokens": [51464, 1057, 264, 721, 300, 286, 2835, 11, 3344, 11, 3092, 11, 393, 822, 11, 51614], "temperature": 0.0, "avg_logprob": -0.18770943747626412, "compression_ratio": 1.3099415204678362, "no_speech_prob": 0.00401815352961421}, {"id": 112, "seek": 64800, "start": 648.0, "end": 652.0, "text": " category analysis, experimentation, there are variations over the same thing.", "tokens": [50364, 7719, 5215, 11, 37142, 11, 456, 366, 17840, 670, 264, 912, 551, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1539520263671875, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.02011430636048317}, {"id": 113, "seek": 64800, "start": 653.0, "end": 658.0, "text": " ARGO rollouts provides that to you and makes it very easy to do it.", "tokens": [50614, 8943, 11601, 3373, 7711, 6417, 300, 281, 291, 293, 1669, 309, 588, 1858, 281, 360, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1539520263671875, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.02011430636048317}, {"id": 114, "seek": 64800, "start": 659.0, "end": 663.0, "text": " And the good thing is you don't need to use ARGO CD, for it to use ARGO rollouts.", "tokens": [50914, 400, 264, 665, 551, 307, 291, 500, 380, 643, 281, 764, 8943, 11601, 6743, 11, 337, 309, 281, 764, 8943, 11601, 3373, 7711, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1539520263671875, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.02011430636048317}, {"id": 115, "seek": 64800, "start": 663.0, "end": 666.0, "text": " You don't actually need to use anything else to use ARGO rollouts.", "tokens": [51114, 509, 500, 380, 767, 643, 281, 764, 1340, 1646, 281, 764, 8943, 11601, 3373, 7711, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1539520263671875, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.02011430636048317}, {"id": 116, "seek": 64800, "start": 666.0, "end": 670.0, "text": " You can run ARGO rollouts just with Kubernetes, nothing else.", "tokens": [51264, 509, 393, 1190, 8943, 11601, 3373, 7711, 445, 365, 23145, 11, 1825, 1646, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1539520263671875, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.02011430636048317}, {"id": 117, "seek": 64800, "start": 670.0, "end": 673.0, "text": " You don't need external dependencies.", "tokens": [51464, 509, 500, 380, 643, 8320, 36606, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1539520263671875, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.02011430636048317}, {"id": 118, "seek": 67300, "start": 674.0, "end": 678.0, "text": " And, yeah, it allows you to do this very easily.", "tokens": [50414, 400, 11, 1338, 11, 309, 4045, 291, 281, 360, 341, 588, 3612, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12242087863740467, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.008340917527675629}, {"id": 119, "seek": 67300, "start": 680.0, "end": 682.0, "text": " I'm a bit on the architecture of ARGO rollouts.", "tokens": [50714, 286, 478, 257, 857, 322, 264, 9482, 295, 8943, 11601, 3373, 7711, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12242087863740467, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.008340917527675629}, {"id": 120, "seek": 67300, "start": 682.0, "end": 687.0, "text": " So we have the controller that is watching a new object called a rollout.", "tokens": [50814, 407, 321, 362, 264, 10561, 300, 307, 1976, 257, 777, 2657, 1219, 257, 3373, 346, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12242087863740467, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.008340917527675629}, {"id": 121, "seek": 67300, "start": 688.0, "end": 696.0, "text": " So ARGO rollouts has this object that can replace or complement your existing deployments.", "tokens": [51114, 407, 8943, 11601, 3373, 7711, 575, 341, 2657, 300, 393, 7406, 420, 17103, 428, 6741, 7274, 1117, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12242087863740467, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.008340917527675629}, {"id": 122, "seek": 67300, "start": 697.0, "end": 700.0, "text": " And I think I'll go down there in a bit.", "tokens": [51564, 400, 286, 519, 286, 603, 352, 760, 456, 294, 257, 857, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12242087863740467, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.008340917527675629}, {"id": 123, "seek": 70000, "start": 701.0, "end": 705.0, "text": " So this rollout manages the replica sets.", "tokens": [50414, 407, 341, 3373, 346, 22489, 264, 35456, 6352, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12866272070469, "compression_ratio": 1.7135416666666667, "no_speech_prob": 0.016032231971621513}, {"id": 124, "seek": 70000, "start": 706.0, "end": 711.0, "text": " So these replica sets, typically, you would have your deployments with the replica sets,", "tokens": [50664, 407, 613, 35456, 6352, 11, 5850, 11, 291, 576, 362, 428, 7274, 1117, 365, 264, 35456, 6352, 11, 50914], "temperature": 0.0, "avg_logprob": -0.12866272070469, "compression_ratio": 1.7135416666666667, "no_speech_prob": 0.016032231971621513}, {"id": 125, "seek": 70000, "start": 712.0, "end": 714.0, "text": " and now they become part of the rollout.", "tokens": [50964, 293, 586, 436, 1813, 644, 295, 264, 3373, 346, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12866272070469, "compression_ratio": 1.7135416666666667, "no_speech_prob": 0.016032231971621513}, {"id": 126, "seek": 70000, "start": 715.0, "end": 723.0, "text": " And it has the concept of analysis run that will check metrics or any other external source", "tokens": [51114, 400, 309, 575, 264, 3410, 295, 5215, 1190, 300, 486, 1520, 16367, 420, 604, 661, 8320, 4009, 51514], "temperature": 0.0, "avg_logprob": -0.12866272070469, "compression_ratio": 1.7135416666666667, "no_speech_prob": 0.016032231971621513}, {"id": 127, "seek": 70000, "start": 723.0, "end": 729.0, "text": " that this analysis will decide is this rollout successful or not.", "tokens": [51514, 300, 341, 5215, 486, 4536, 307, 341, 3373, 346, 4406, 420, 406, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12866272070469, "compression_ratio": 1.7135416666666667, "no_speech_prob": 0.016032231971621513}, {"id": 128, "seek": 73000, "start": 730.0, "end": 736.0, "text": " And based on that, it's going to cancel the rollout or keep it going.", "tokens": [50364, 400, 2361, 322, 300, 11, 309, 311, 516, 281, 10373, 264, 3373, 346, 420, 1066, 309, 516, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1141717625760484, "compression_ratio": 1.5431472081218274, "no_speech_prob": 0.00289890356361866}, {"id": 129, "seek": 73000, "start": 738.0, "end": 741.0, "text": " So you get the traffic coming from the ingress into your services,", "tokens": [50764, 407, 291, 483, 264, 6419, 1348, 490, 264, 3957, 735, 666, 428, 3328, 11, 50914], "temperature": 0.0, "avg_logprob": -0.1141717625760484, "compression_ratio": 1.5431472081218274, "no_speech_prob": 0.00289890356361866}, {"id": 130, "seek": 73000, "start": 741.0, "end": 750.0, "text": " and you can tell ARGO, OK, send me, send traffic to this new canary replica set", "tokens": [50914, 293, 291, 393, 980, 8943, 11601, 11, 2264, 11, 2845, 385, 11, 2845, 6419, 281, 341, 777, 393, 822, 35456, 992, 51364], "temperature": 0.0, "avg_logprob": -0.1141717625760484, "compression_ratio": 1.5431472081218274, "no_speech_prob": 0.00289890356361866}, {"id": 131, "seek": 73000, "start": 750.0, "end": 752.0, "text": " or send it to the old one.", "tokens": [51364, 420, 2845, 309, 281, 264, 1331, 472, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1141717625760484, "compression_ratio": 1.5431472081218274, "no_speech_prob": 0.00289890356361866}, {"id": 132, "seek": 73000, "start": 752.0, "end": 756.0, "text": " The percentage base one, for that, you need a service match.", "tokens": [51464, 440, 9668, 3096, 472, 11, 337, 300, 11, 291, 643, 257, 2643, 2995, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1141717625760484, "compression_ratio": 1.5431472081218274, "no_speech_prob": 0.00289890356361866}, {"id": 133, "seek": 75600, "start": 756.0, "end": 761.0, "text": " So if you need to do something fancy like, oh, I want to send 1% of the traffic", "tokens": [50364, 407, 498, 291, 643, 281, 360, 746, 10247, 411, 11, 1954, 11, 286, 528, 281, 2845, 502, 4, 295, 264, 6419, 50614], "temperature": 0.0, "avg_logprob": -0.08582464666927563, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.011938776820898056}, {"id": 134, "seek": 75600, "start": 761.0, "end": 764.0, "text": " or I want some traffic that matches this header,", "tokens": [50614, 420, 286, 528, 512, 6419, 300, 10676, 341, 23117, 11, 50764], "temperature": 0.0, "avg_logprob": -0.08582464666927563, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.011938776820898056}, {"id": 135, "seek": 75600, "start": 764.0, "end": 767.0, "text": " then you need something like a service match", "tokens": [50764, 550, 291, 643, 746, 411, 257, 2643, 2995, 50914], "temperature": 0.0, "avg_logprob": -0.08582464666927563, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.011938776820898056}, {"id": 136, "seek": 75600, "start": 767.0, "end": 772.0, "text": " or the integration with the ARGO rollouts integration with the ingress controller.", "tokens": [50914, 420, 264, 10980, 365, 264, 8943, 11601, 3373, 7711, 10980, 365, 264, 3957, 735, 10561, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08582464666927563, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.011938776820898056}, {"id": 137, "seek": 75600, "start": 773.0, "end": 782.0, "text": " But if you use bare Kubernetes without integration between rollouts, you can still do it.", "tokens": [51214, 583, 498, 291, 764, 6949, 23145, 1553, 10980, 1296, 3373, 7711, 11, 291, 393, 920, 360, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08582464666927563, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.011938776820898056}, {"id": 138, "seek": 78200, "start": 783.0, "end": 787.0, "text": " Basically, it will use the number of pods in the replica sets.", "tokens": [50414, 8537, 11, 309, 486, 764, 264, 1230, 295, 31925, 294, 264, 35456, 6352, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09307773991634971, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0063984026201069355}, {"id": 139, "seek": 78200, "start": 787.0, "end": 795.0, "text": " So if you have 10 pods, you can tell ARGO, OK, one new pod is going to go to the new version,", "tokens": [50614, 407, 498, 291, 362, 1266, 31925, 11, 291, 393, 980, 8943, 11601, 11, 2264, 11, 472, 777, 2497, 307, 516, 281, 352, 281, 264, 777, 3037, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09307773991634971, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0063984026201069355}, {"id": 140, "seek": 78200, "start": 795.0, "end": 800.0, "text": " and now you have a 10%, 90% sort of split, more or less.", "tokens": [51014, 293, 586, 291, 362, 257, 1266, 8923, 4289, 4, 1333, 295, 7472, 11, 544, 420, 1570, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09307773991634971, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0063984026201069355}, {"id": 141, "seek": 78200, "start": 800.0, "end": 805.0, "text": " You cannot do things fancy that require support from the ingress controller or a service match,", "tokens": [51264, 509, 2644, 360, 721, 10247, 300, 3651, 1406, 490, 264, 3957, 735, 10561, 420, 257, 2643, 2995, 11, 51514], "temperature": 0.0, "avg_logprob": -0.09307773991634971, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0063984026201069355}, {"id": 142, "seek": 78200, "start": 805.0, "end": 807.0, "text": " but you can still do things.", "tokens": [51514, 457, 291, 393, 920, 360, 721, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09307773991634971, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0063984026201069355}, {"id": 143, "seek": 80700, "start": 808.0, "end": 813.0, "text": " The rollout object, you have two ways of defining the rollout.", "tokens": [50414, 440, 3373, 346, 2657, 11, 291, 362, 732, 2098, 295, 17827, 264, 3373, 346, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09606032120554071, "compression_ratio": 1.771144278606965, "no_speech_prob": 0.02929946966469288}, {"id": 144, "seek": 80700, "start": 813.0, "end": 818.0, "text": " One is you replace the deployment with the rollout and add extra fields,", "tokens": [50664, 1485, 307, 291, 7406, 264, 19317, 365, 264, 3373, 346, 293, 909, 2857, 7909, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09606032120554071, "compression_ratio": 1.771144278606965, "no_speech_prob": 0.02929946966469288}, {"id": 145, "seek": 80700, "start": 818.0, "end": 821.0, "text": " or you create a rollout that points to a deployment.", "tokens": [50914, 420, 291, 1884, 257, 3373, 346, 300, 2793, 281, 257, 19317, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09606032120554071, "compression_ratio": 1.771144278606965, "no_speech_prob": 0.02929946966469288}, {"id": 146, "seek": 80700, "start": 822.0, "end": 826.0, "text": " I don't like a lot the way of replacing the deployment", "tokens": [51114, 286, 500, 380, 411, 257, 688, 264, 636, 295, 19139, 264, 19317, 51314], "temperature": 0.0, "avg_logprob": -0.09606032120554071, "compression_ratio": 1.771144278606965, "no_speech_prob": 0.02929946966469288}, {"id": 147, "seek": 80700, "start": 826.0, "end": 831.0, "text": " because then people that are not aware of the rollouts objects,", "tokens": [51314, 570, 550, 561, 300, 366, 406, 3650, 295, 264, 3373, 7711, 6565, 11, 51564], "temperature": 0.0, "avg_logprob": -0.09606032120554071, "compression_ratio": 1.771144278606965, "no_speech_prob": 0.02929946966469288}, {"id": 148, "seek": 80700, "start": 831.0, "end": 834.0, "text": " they may go and see, oh, there's no deployments.", "tokens": [51564, 436, 815, 352, 293, 536, 11, 1954, 11, 456, 311, 572, 7274, 1117, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09606032120554071, "compression_ratio": 1.771144278606965, "no_speech_prob": 0.02929946966469288}, {"id": 149, "seek": 83400, "start": 834.0, "end": 836.0, "text": " What's going on here?", "tokens": [50364, 708, 311, 516, 322, 510, 30, 50464], "temperature": 0.0, "avg_logprob": -0.17427974700927734, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.043662771582603455}, {"id": 150, "seek": 83400, "start": 836.0, "end": 840.0, "text": " So it requires you changing things.", "tokens": [50464, 407, 309, 7029, 291, 4473, 721, 13, 50664], "temperature": 0.0, "avg_logprob": -0.17427974700927734, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.043662771582603455}, {"id": 151, "seek": 83400, "start": 840.0, "end": 844.0, "text": " And for us, it requires also you have to change rambles,", "tokens": [50664, 400, 337, 505, 11, 309, 7029, 611, 291, 362, 281, 1319, 367, 2173, 904, 11, 50864], "temperature": 0.0, "avg_logprob": -0.17427974700927734, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.043662771582603455}, {"id": 152, "seek": 83400, "start": 844.0, "end": 848.0, "text": " you have to change commands that people need to secure documentation and all that.", "tokens": [50864, 291, 362, 281, 1319, 16901, 300, 561, 643, 281, 7144, 14333, 293, 439, 300, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17427974700927734, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.043662771582603455}, {"id": 153, "seek": 83400, "start": 848.0, "end": 851.0, "text": " So I don't know why the decision was made that way,", "tokens": [51064, 407, 286, 500, 380, 458, 983, 264, 3537, 390, 1027, 300, 636, 11, 51214], "temperature": 0.0, "avg_logprob": -0.17427974700927734, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.043662771582603455}, {"id": 154, "seek": 83400, "start": 851.0, "end": 856.0, "text": " but it's not something that I'm too happy about it.", "tokens": [51214, 457, 309, 311, 406, 746, 300, 286, 478, 886, 2055, 466, 309, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17427974700927734, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.043662771582603455}, {"id": 155, "seek": 83400, "start": 856.0, "end": 861.0, "text": " Of course, you require all the Jamel tools to write these things.", "tokens": [51464, 2720, 1164, 11, 291, 3651, 439, 264, 10372, 338, 3873, 281, 2464, 613, 721, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17427974700927734, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.043662771582603455}, {"id": 156, "seek": 86100, "start": 862.0, "end": 865.0, "text": " And let's go to the demo now.", "tokens": [50414, 400, 718, 311, 352, 281, 264, 10723, 586, 13, 50564], "temperature": 0.0, "avg_logprob": -0.28191595366506866, "compression_ratio": 1.075, "no_speech_prob": 0.030844075605273247}, {"id": 157, "seek": 86100, "start": 873.0, "end": 875.0, "text": " So I have here...", "tokens": [50964, 407, 286, 362, 510, 485, 51064], "temperature": 0.0, "avg_logprob": -0.28191595366506866, "compression_ratio": 1.075, "no_speech_prob": 0.030844075605273247}, {"id": 158, "seek": 86100, "start": 886.0, "end": 889.0, "text": " So I'm running the Argo Rollouts demo.", "tokens": [51614, 407, 286, 478, 2614, 264, 1587, 1571, 9926, 7711, 10723, 13, 51764], "temperature": 0.0, "avg_logprob": -0.28191595366506866, "compression_ratio": 1.075, "no_speech_prob": 0.030844075605273247}, {"id": 159, "seek": 88900, "start": 889.0, "end": 894.0, "text": " This is hitting the backend and it's returning one color or another,", "tokens": [50364, 639, 307, 8850, 264, 38087, 293, 309, 311, 12678, 472, 2017, 420, 1071, 11, 50614], "temperature": 0.0, "avg_logprob": -0.13301477314513407, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.003514548297971487}, {"id": 160, "seek": 88900, "start": 894.0, "end": 897.0, "text": " depending on what is running on the backend.", "tokens": [50614, 5413, 322, 437, 307, 2614, 322, 264, 38087, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13301477314513407, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.003514548297971487}, {"id": 161, "seek": 88900, "start": 897.0, "end": 900.0, "text": " So right now I have the blue one.", "tokens": [50764, 407, 558, 586, 286, 362, 264, 3344, 472, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13301477314513407, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.003514548297971487}, {"id": 162, "seek": 88900, "start": 904.0, "end": 907.0, "text": " Let me see how can I do this easier.", "tokens": [51114, 961, 385, 536, 577, 393, 286, 360, 341, 3571, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13301477314513407, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.003514548297971487}, {"id": 163, "seek": 88900, "start": 912.0, "end": 914.0, "text": " So what I'm going to do is to change,", "tokens": [51514, 407, 437, 286, 478, 516, 281, 360, 307, 281, 1319, 11, 51614], "temperature": 0.0, "avg_logprob": -0.13301477314513407, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.003514548297971487}, {"id": 164, "seek": 88900, "start": 914.0, "end": 918.0, "text": " update my deployment to use a new image that is going to be green.", "tokens": [51614, 5623, 452, 19317, 281, 764, 257, 777, 3256, 300, 307, 516, 281, 312, 3092, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13301477314513407, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.003514548297971487}, {"id": 165, "seek": 91900, "start": 919.0, "end": 924.0, "text": " And ring.", "tokens": [50364, 400, 4875, 13, 50614], "temperature": 0.0, "avg_logprob": -0.48642510634202224, "compression_ratio": 0.7894736842105263, "no_speech_prob": 0.030597398057579994}, {"id": 166, "seek": 91900, "start": 939.0, "end": 942.0, "text": " I lost the terminal.", "tokens": [51364, 286, 2731, 264, 14709, 13, 51514], "temperature": 0.0, "avg_logprob": -0.48642510634202224, "compression_ratio": 0.7894736842105263, "no_speech_prob": 0.030597398057579994}, {"id": 167, "seek": 94900, "start": 949.0, "end": 955.0, "text": " Okay, so it updated the image and let me fit here in big", "tokens": [50364, 1033, 11, 370, 309, 10588, 264, 3256, 293, 718, 385, 3318, 510, 294, 955, 50664], "temperature": 0.0, "avg_logprob": -0.2743818495008681, "compression_ratio": 1.05, "no_speech_prob": 0.010335427708923817}, {"id": 168, "seek": 94900, "start": 959.0, "end": 961.0, "text": " To show what this is doing.", "tokens": [50864, 1407, 855, 437, 341, 307, 884, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2743818495008681, "compression_ratio": 1.05, "no_speech_prob": 0.010335427708923817}, {"id": 169, "seek": 96100, "start": 961.0, "end": 977.0, "text": " Okay, I think I pushed twice and now I see two", "tokens": [50364, 1033, 11, 286, 519, 286, 9152, 6091, 293, 586, 286, 536, 732, 51164], "temperature": 0.0, "avg_logprob": -0.2573661804199219, "compression_ratio": 1.1603773584905661, "no_speech_prob": 0.016672883182764053}, {"id": 170, "seek": 96100, "start": 977.0, "end": 980.0, "text": " Rollouts happening at the same time.", "tokens": [51164, 9926, 7711, 2737, 412, 264, 912, 565, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2573661804199219, "compression_ratio": 1.1603773584905661, "no_speech_prob": 0.016672883182764053}, {"id": 171, "seek": 96100, "start": 980.0, "end": 982.0, "text": " Otherwise it's not working.", "tokens": [51314, 10328, 309, 311, 406, 1364, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2573661804199219, "compression_ratio": 1.1603773584905661, "no_speech_prob": 0.016672883182764053}, {"id": 172, "seek": 96100, "start": 982.0, "end": 984.0, "text": " Here it is.", "tokens": [51414, 1692, 309, 307, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2573661804199219, "compression_ratio": 1.1603773584905661, "no_speech_prob": 0.016672883182764053}, {"id": 173, "seek": 98400, "start": 984.0, "end": 993.0, "text": " Okay, so I have the green.", "tokens": [50364, 1033, 11, 370, 286, 362, 264, 3092, 13, 50814], "temperature": 0.0, "avg_logprob": -0.21283625089205227, "compression_ratio": 1.4965034965034965, "no_speech_prob": 0.016667120158672333}, {"id": 174, "seek": 98400, "start": 993.0, "end": 998.0, "text": " The one that shows green is the one that was running and it's stable.", "tokens": [50814, 440, 472, 300, 3110, 3092, 307, 264, 472, 300, 390, 2614, 293, 309, 311, 8351, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21283625089205227, "compression_ratio": 1.4965034965034965, "no_speech_prob": 0.016667120158672333}, {"id": 175, "seek": 98400, "start": 998.0, "end": 1003.0, "text": " So I think I have five pots running and I push a new change,", "tokens": [51064, 407, 286, 519, 286, 362, 1732, 22022, 2614, 293, 286, 2944, 257, 777, 1319, 11, 51314], "temperature": 0.0, "avg_logprob": -0.21283625089205227, "compression_ratio": 1.4965034965034965, "no_speech_prob": 0.016667120158672333}, {"id": 176, "seek": 98400, "start": 1003.0, "end": 1005.0, "text": " which is the canary.", "tokens": [51314, 597, 307, 264, 393, 822, 13, 51414], "temperature": 0.0, "avg_logprob": -0.21283625089205227, "compression_ratio": 1.4965034965034965, "no_speech_prob": 0.016667120158672333}, {"id": 177, "seek": 98400, "start": 1005.0, "end": 1011.0, "text": " And this should be using the green.", "tokens": [51414, 400, 341, 820, 312, 1228, 264, 3092, 13, 51714], "temperature": 0.0, "avg_logprob": -0.21283625089205227, "compression_ratio": 1.4965034965034965, "no_speech_prob": 0.016667120158672333}, {"id": 178, "seek": 101100, "start": 1011.0, "end": 1013.0, "text": " Okay, there it is.", "tokens": [50364, 1033, 11, 456, 309, 307, 13, 50464], "temperature": 0.0, "avg_logprob": -0.21368299528609874, "compression_ratio": 1.49746192893401, "no_speech_prob": 0.00830777082592249}, {"id": 179, "seek": 101100, "start": 1013.0, "end": 1018.0, "text": " So like 20% of the traffic is getting green.", "tokens": [50464, 407, 411, 945, 4, 295, 264, 6419, 307, 1242, 3092, 13, 50714], "temperature": 0.0, "avg_logprob": -0.21368299528609874, "compression_ratio": 1.49746192893401, "no_speech_prob": 0.00830777082592249}, {"id": 180, "seek": 101100, "start": 1018.0, "end": 1020.0, "text": " Right?", "tokens": [50714, 1779, 30, 50814], "temperature": 0.0, "avg_logprob": -0.21368299528609874, "compression_ratio": 1.49746192893401, "no_speech_prob": 0.00830777082592249}, {"id": 181, "seek": 101100, "start": 1020.0, "end": 1025.0, "text": " And how I define this rollout, this one is at the bottom is", "tokens": [50814, 400, 577, 286, 6964, 341, 3373, 346, 11, 341, 472, 307, 412, 264, 2767, 307, 51064], "temperature": 0.0, "avg_logprob": -0.21368299528609874, "compression_ratio": 1.49746192893401, "no_speech_prob": 0.00830777082592249}, {"id": 182, "seek": 101100, "start": 1025.0, "end": 1029.0, "text": " just the standard deployment configuration.", "tokens": [51064, 445, 264, 3832, 19317, 11694, 13, 51264], "temperature": 0.0, "avg_logprob": -0.21368299528609874, "compression_ratio": 1.49746192893401, "no_speech_prob": 0.00830777082592249}, {"id": 183, "seek": 101100, "start": 1029.0, "end": 1031.0, "text": " So what image do I want?", "tokens": [51264, 407, 437, 3256, 360, 286, 528, 30, 51364], "temperature": 0.0, "avg_logprob": -0.21368299528609874, "compression_ratio": 1.49746192893401, "no_speech_prob": 0.00830777082592249}, {"id": 184, "seek": 101100, "start": 1031.0, "end": 1034.0, "text": " What ports do I want to expose and so on.", "tokens": [51364, 708, 18160, 360, 286, 528, 281, 19219, 293, 370, 322, 13, 51514], "temperature": 0.0, "avg_logprob": -0.21368299528609874, "compression_ratio": 1.49746192893401, "no_speech_prob": 0.00830777082592249}, {"id": 185, "seek": 101100, "start": 1034.0, "end": 1040.0, "text": " But at the top I have the strategy configuration from", "tokens": [51514, 583, 412, 264, 1192, 286, 362, 264, 5206, 11694, 490, 51814], "temperature": 0.0, "avg_logprob": -0.21368299528609874, "compression_ratio": 1.49746192893401, "no_speech_prob": 0.00830777082592249}, {"id": 186, "seek": 104000, "start": 1040.0, "end": 1042.0, "text": " Marco rollouts.", "tokens": [50364, 26535, 3373, 7711, 13, 50464], "temperature": 0.0, "avg_logprob": -0.22552144009134042, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.037573717534542084}, {"id": 187, "seek": 104000, "start": 1042.0, "end": 1046.0, "text": " So I can say point to this analysis template.", "tokens": [50464, 407, 286, 393, 584, 935, 281, 341, 5215, 12379, 13, 50664], "temperature": 0.0, "avg_logprob": -0.22552144009134042, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.037573717534542084}, {"id": 188, "seek": 104000, "start": 1046.0, "end": 1050.0, "text": " This is what defines what is successful, what is not successful.", "tokens": [50664, 639, 307, 437, 23122, 437, 307, 4406, 11, 437, 307, 406, 4406, 13, 50864], "temperature": 0.0, "avg_logprob": -0.22552144009134042, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.037573717534542084}, {"id": 189, "seek": 104000, "start": 1050.0, "end": 1053.0, "text": " And I'll show you that in a bit.", "tokens": [50864, 400, 286, 603, 855, 291, 300, 294, 257, 857, 13, 51014], "temperature": 0.0, "avg_logprob": -0.22552144009134042, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.037573717534542084}, {"id": 190, "seek": 104000, "start": 1053.0, "end": 1057.0, "text": " And it's, I have several steps.", "tokens": [51014, 400, 309, 311, 11, 286, 362, 2940, 4439, 13, 51214], "temperature": 0.0, "avg_logprob": -0.22552144009134042, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.037573717534542084}, {"id": 191, "seek": 104000, "start": 1057.0, "end": 1063.0, "text": " So set weight 20 and then do a pause, set weight 40,", "tokens": [51214, 407, 992, 3364, 945, 293, 550, 360, 257, 10465, 11, 992, 3364, 3356, 11, 51514], "temperature": 0.0, "avg_logprob": -0.22552144009134042, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.037573717534542084}, {"id": 192, "seek": 104000, "start": 1063.0, "end": 1066.0, "text": " pause for 10 seconds, set weight 60.", "tokens": [51514, 10465, 337, 1266, 3949, 11, 992, 3364, 4060, 13, 51664], "temperature": 0.0, "avg_logprob": -0.22552144009134042, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.037573717534542084}, {"id": 193, "seek": 104000, "start": 1066.0, "end": 1068.0, "text": " So this is percentage.", "tokens": [51664, 407, 341, 307, 9668, 13, 51764], "temperature": 0.0, "avg_logprob": -0.22552144009134042, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.037573717534542084}, {"id": 194, "seek": 106800, "start": 1068.0, "end": 1072.0, "text": " Pause for 10 seconds, set weight 80.", "tokens": [50364, 31973, 337, 1266, 3949, 11, 992, 3364, 4688, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15818203196806066, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.008944625966250896}, {"id": 195, "seek": 106800, "start": 1072.0, "end": 1076.0, "text": " So this is my definition of a rollout.", "tokens": [50564, 407, 341, 307, 452, 7123, 295, 257, 3373, 346, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15818203196806066, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.008944625966250896}, {"id": 196, "seek": 106800, "start": 1076.0, "end": 1079.0, "text": " 20% wait for me to manually do something.", "tokens": [50764, 945, 4, 1699, 337, 385, 281, 16945, 360, 746, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15818203196806066, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.008944625966250896}, {"id": 197, "seek": 106800, "start": 1079.0, "end": 1081.0, "text": " I only do that for demos in real life.", "tokens": [50914, 286, 787, 360, 300, 337, 33788, 294, 957, 993, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15818203196806066, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.008944625966250896}, {"id": 198, "seek": 106800, "start": 1081.0, "end": 1085.0, "text": " That's a bit harder to do as you could still do it.", "tokens": [51014, 663, 311, 257, 857, 6081, 281, 360, 382, 291, 727, 920, 360, 309, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15818203196806066, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.008944625966250896}, {"id": 199, "seek": 106800, "start": 1085.0, "end": 1089.0, "text": " But this is my definition of what the rollout is.", "tokens": [51214, 583, 341, 307, 452, 7123, 295, 437, 264, 3373, 346, 307, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15818203196806066, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.008944625966250896}, {"id": 200, "seek": 106800, "start": 1089.0, "end": 1092.0, "text": " So right now it's waiting because I set a pause and it's waiting", "tokens": [51414, 407, 558, 586, 309, 311, 3806, 570, 286, 992, 257, 10465, 293, 309, 311, 3806, 51564], "temperature": 0.0, "avg_logprob": -0.15818203196806066, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.008944625966250896}, {"id": 201, "seek": 106800, "start": 1092.0, "end": 1094.0, "text": " for me to give you a key.", "tokens": [51564, 337, 385, 281, 976, 291, 257, 2141, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15818203196806066, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.008944625966250896}, {"id": 202, "seek": 106800, "start": 1094.0, "end": 1096.0, "text": " I look at it, it says it looks okay.", "tokens": [51664, 286, 574, 412, 309, 11, 309, 1619, 309, 1542, 1392, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15818203196806066, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.008944625966250896}, {"id": 203, "seek": 109600, "start": 1096.0, "end": 1100.0, "text": " So I can do the promote.", "tokens": [50364, 407, 286, 393, 360, 264, 9773, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13662671125852144, "compression_ratio": 1.3835616438356164, "no_speech_prob": 0.025327399373054504}, {"id": 204, "seek": 109600, "start": 1100.0, "end": 1109.0, "text": " And this is going to continue through the rest of the steps.", "tokens": [50564, 400, 341, 307, 516, 281, 2354, 807, 264, 1472, 295, 264, 4439, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13662671125852144, "compression_ratio": 1.3835616438356164, "no_speech_prob": 0.025327399373054504}, {"id": 205, "seek": 109600, "start": 1109.0, "end": 1113.0, "text": " So hopefully we'll see this in like 60 seconds.", "tokens": [51014, 407, 4696, 321, 603, 536, 341, 294, 411, 4060, 3949, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13662671125852144, "compression_ratio": 1.3835616438356164, "no_speech_prob": 0.025327399373054504}, {"id": 206, "seek": 109600, "start": 1113.0, "end": 1123.0, "text": " It should continue the progression until everybody receives a green.", "tokens": [51214, 467, 820, 2354, 264, 18733, 1826, 2201, 20717, 257, 3092, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13662671125852144, "compression_ratio": 1.3835616438356164, "no_speech_prob": 0.025327399373054504}, {"id": 207, "seek": 112300, "start": 1123.0, "end": 1127.0, "text": " A green color when they call the API.", "tokens": [50364, 316, 3092, 2017, 562, 436, 818, 264, 9362, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1642000721950157, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.05935369059443474}, {"id": 208, "seek": 112300, "start": 1127.0, "end": 1132.0, "text": " So this shows you just by creating a rollout object with this", "tokens": [50564, 407, 341, 3110, 291, 445, 538, 4084, 257, 3373, 346, 2657, 365, 341, 50814], "temperature": 0.0, "avg_logprob": -0.1642000721950157, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.05935369059443474}, {"id": 209, "seek": 112300, "start": 1132.0, "end": 1137.0, "text": " small section defining what your rollout is, you can do this.", "tokens": [50814, 1359, 3541, 17827, 437, 428, 3373, 346, 307, 11, 291, 393, 360, 341, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1642000721950157, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.05935369059443474}, {"id": 210, "seek": 112300, "start": 1137.0, "end": 1138.0, "text": " There's nothing else you need.", "tokens": [51064, 821, 311, 1825, 1646, 291, 643, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1642000721950157, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.05935369059443474}, {"id": 211, "seek": 112300, "start": 1138.0, "end": 1140.0, "text": " Well, you need to install Argo.", "tokens": [51114, 1042, 11, 291, 643, 281, 3625, 1587, 1571, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1642000721950157, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.05935369059443474}, {"id": 212, "seek": 112300, "start": 1140.0, "end": 1144.0, "text": " And what else can you do?", "tokens": [51214, 400, 437, 1646, 393, 291, 360, 30, 51414], "temperature": 0.0, "avg_logprob": -0.1642000721950157, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.05935369059443474}, {"id": 213, "seek": 112300, "start": 1144.0, "end": 1149.0, "text": " Oh, yes, you can also have a preview version.", "tokens": [51414, 876, 11, 2086, 11, 291, 393, 611, 362, 257, 14281, 3037, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1642000721950157, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.05935369059443474}, {"id": 214, "seek": 112300, "start": 1149.0, "end": 1152.0, "text": " So you can have another ingress pointing to your preview version.", "tokens": [51664, 407, 291, 393, 362, 1071, 3957, 735, 12166, 281, 428, 14281, 3037, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1642000721950157, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.05935369059443474}, {"id": 215, "seek": 115200, "start": 1152.0, "end": 1158.0, "text": " So you can even if I said I want zero traffic to go to the new version.", "tokens": [50364, 407, 291, 393, 754, 498, 286, 848, 286, 528, 4018, 6419, 281, 352, 281, 264, 777, 3037, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15299830144765425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0286201573908329}, {"id": 216, "seek": 115200, "start": 1158.0, "end": 1162.0, "text": " All the existing traffic I wanted to go to the old version,", "tokens": [50664, 1057, 264, 6741, 6419, 286, 1415, 281, 352, 281, 264, 1331, 3037, 11, 50864], "temperature": 0.0, "avg_logprob": -0.15299830144765425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0286201573908329}, {"id": 217, "seek": 115200, "start": 1162.0, "end": 1165.0, "text": " but I want to see the new version in a new place.", "tokens": [50864, 457, 286, 528, 281, 536, 264, 777, 3037, 294, 257, 777, 1081, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15299830144765425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0286201573908329}, {"id": 218, "seek": 115200, "start": 1165.0, "end": 1166.0, "text": " I can do that too.", "tokens": [51014, 286, 393, 360, 300, 886, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15299830144765425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0286201573908329}, {"id": 219, "seek": 115200, "start": 1166.0, "end": 1171.0, "text": " So that's very useful for preview environments sort of thing.", "tokens": [51064, 407, 300, 311, 588, 4420, 337, 14281, 12388, 1333, 295, 551, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15299830144765425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0286201573908329}, {"id": 220, "seek": 115200, "start": 1171.0, "end": 1174.0, "text": " So if I go back, okay.", "tokens": [51314, 407, 498, 286, 352, 646, 11, 1392, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15299830144765425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0286201573908329}, {"id": 221, "seek": 115200, "start": 1174.0, "end": 1179.0, "text": " So while this continues running, this is running on Google Kubernetes", "tokens": [51464, 407, 1339, 341, 6515, 2614, 11, 341, 307, 2614, 322, 3329, 23145, 51714], "temperature": 0.0, "avg_logprob": -0.15299830144765425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0286201573908329}, {"id": 222, "seek": 117900, "start": 1179.0, "end": 1185.0, "text": " and sending autopilot clusters, but you can run it in any Kubernetes.", "tokens": [50364, 293, 7750, 31090, 31516, 23313, 11, 457, 291, 393, 1190, 309, 294, 604, 23145, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16908734265495748, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.03221823647618294}, {"id": 223, "seek": 117900, "start": 1185.0, "end": 1192.0, "text": " And the autopilot is pretty cool because you only pay for what you use.", "tokens": [50664, 400, 264, 31090, 31516, 307, 1238, 1627, 570, 291, 787, 1689, 337, 437, 291, 764, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16908734265495748, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.03221823647618294}, {"id": 224, "seek": 117900, "start": 1192.0, "end": 1198.0, "text": " So if you scale things to zero, then you don't pay anything.", "tokens": [51014, 407, 498, 291, 4373, 721, 281, 4018, 11, 550, 291, 500, 380, 1689, 1340, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16908734265495748, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.03221823647618294}, {"id": 225, "seek": 117900, "start": 1198.0, "end": 1200.0, "text": " What does it says here?", "tokens": [51314, 708, 775, 309, 1619, 510, 30, 51414], "temperature": 0.0, "avg_logprob": -0.16908734265495748, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.03221823647618294}, {"id": 226, "seek": 117900, "start": 1200.0, "end": 1201.0, "text": " Okay.", "tokens": [51414, 1033, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16908734265495748, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.03221823647618294}, {"id": 227, "seek": 117900, "start": 1201.0, "end": 1203.0, "text": " So now green is the stable one.", "tokens": [51464, 407, 586, 3092, 307, 264, 8351, 472, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16908734265495748, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.03221823647618294}, {"id": 228, "seek": 117900, "start": 1203.0, "end": 1208.0, "text": " It says here, stable here.", "tokens": [51564, 467, 1619, 510, 11, 8351, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16908734265495748, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.03221823647618294}, {"id": 229, "seek": 120800, "start": 1208.0, "end": 1213.0, "text": " What if I want to do...", "tokens": [50364, 708, 498, 286, 528, 281, 360, 485, 50614], "temperature": 0.0, "avg_logprob": -0.23156158447265626, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.017933344468474388}, {"id": 230, "seek": 120800, "start": 1213.0, "end": 1217.0, "text": " I was talking about how does this protect me, right?", "tokens": [50614, 286, 390, 1417, 466, 577, 775, 341, 2371, 385, 11, 558, 30, 50814], "temperature": 0.0, "avg_logprob": -0.23156158447265626, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.017933344468474388}, {"id": 231, "seek": 120800, "start": 1217.0, "end": 1223.0, "text": " What if I want to do a rollout that is broken?", "tokens": [50814, 708, 498, 286, 528, 281, 360, 257, 3373, 346, 300, 307, 5463, 30, 51114], "temperature": 0.0, "avg_logprob": -0.23156158447265626, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.017933344468474388}, {"id": 232, "seek": 120800, "start": 1223.0, "end": 1225.0, "text": " So...", "tokens": [51114, 407, 485, 51214], "temperature": 0.0, "avg_logprob": -0.23156158447265626, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.017933344468474388}, {"id": 233, "seek": 120800, "start": 1225.0, "end": 1227.0, "text": " Let's see.", "tokens": [51214, 961, 311, 536, 13, 51314], "temperature": 0.0, "avg_logprob": -0.23156158447265626, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.017933344468474388}, {"id": 234, "seek": 120800, "start": 1227.0, "end": 1229.0, "text": " This works.", "tokens": [51314, 639, 1985, 13, 51414], "temperature": 0.0, "avg_logprob": -0.23156158447265626, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.017933344468474388}, {"id": 235, "seek": 120800, "start": 1229.0, "end": 1232.0, "text": " Right.", "tokens": [51414, 1779, 13, 51564], "temperature": 0.0, "avg_logprob": -0.23156158447265626, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.017933344468474388}, {"id": 236, "seek": 120800, "start": 1232.0, "end": 1234.0, "text": " Okay.", "tokens": [51564, 1033, 13, 51664], "temperature": 0.0, "avg_logprob": -0.23156158447265626, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.017933344468474388}, {"id": 237, "seek": 120800, "start": 1234.0, "end": 1237.0, "text": " So now I push an image that is bad.", "tokens": [51664, 407, 586, 286, 2944, 364, 3256, 300, 307, 1578, 13, 51814], "temperature": 0.0, "avg_logprob": -0.23156158447265626, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.017933344468474388}, {"id": 238, "seek": 123700, "start": 1237.0, "end": 1239.0, "text": " So I'm changing the deployment.", "tokens": [50364, 407, 286, 478, 4473, 264, 19317, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1456665893395742, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.018091432750225067}, {"id": 239, "seek": 123700, "start": 1239.0, "end": 1241.0, "text": " Of course, you would do this with the GitOps.", "tokens": [50464, 2720, 1164, 11, 291, 576, 360, 341, 365, 264, 16939, 36179, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1456665893395742, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.018091432750225067}, {"id": 240, "seek": 123700, "start": 1241.0, "end": 1244.0, "text": " You would never push the production, but YOLO.", "tokens": [50564, 509, 576, 1128, 2944, 264, 4265, 11, 457, 398, 5046, 46, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1456665893395742, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.018091432750225067}, {"id": 241, "seek": 123700, "start": 1244.0, "end": 1253.0, "text": " And so I'm pushing the red image, but this red image is returning in 500 errors.", "tokens": [50714, 400, 370, 286, 478, 7380, 264, 2182, 3256, 11, 457, 341, 2182, 3256, 307, 12678, 294, 5923, 13603, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1456665893395742, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.018091432750225067}, {"id": 242, "seek": 123700, "start": 1253.0, "end": 1263.0, "text": " And now Argo realized, oh, this is giving errors based on my analysis template that I'll show you.", "tokens": [51164, 400, 586, 1587, 1571, 5334, 11, 1954, 11, 341, 307, 2902, 13603, 2361, 322, 452, 5215, 12379, 300, 286, 603, 855, 291, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1456665893395742, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.018091432750225067}, {"id": 243, "seek": 123700, "start": 1263.0, "end": 1266.0, "text": " And this is in the graded status.", "tokens": [51664, 400, 341, 307, 294, 264, 2771, 292, 6558, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1456665893395742, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.018091432750225067}, {"id": 244, "seek": 126600, "start": 1266.0, "end": 1274.0, "text": " And it went down and the scale it down, and my canary was set as failed.", "tokens": [50364, 400, 309, 1437, 760, 293, 264, 4373, 309, 760, 11, 293, 452, 393, 822, 390, 992, 382, 7612, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09273391420190985, "compression_ratio": 1.5592417061611374, "no_speech_prob": 0.011335330083966255}, {"id": 245, "seek": 126600, "start": 1274.0, "end": 1279.0, "text": " And you see that only a few percentage of traffic got the red dots,", "tokens": [50764, 400, 291, 536, 300, 787, 257, 1326, 9668, 295, 6419, 658, 264, 2182, 15026, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09273391420190985, "compression_ratio": 1.5592417061611374, "no_speech_prob": 0.011335330083966255}, {"id": 246, "seek": 126600, "start": 1279.0, "end": 1282.0, "text": " and then it was automatically rolled back.", "tokens": [51014, 293, 550, 309, 390, 6772, 14306, 646, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09273391420190985, "compression_ratio": 1.5592417061611374, "no_speech_prob": 0.011335330083966255}, {"id": 247, "seek": 126600, "start": 1282.0, "end": 1287.0, "text": " So I think this is the power of doing progressive delivery.", "tokens": [51164, 407, 286, 519, 341, 307, 264, 1347, 295, 884, 16131, 8982, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09273391420190985, "compression_ratio": 1.5592417061611374, "no_speech_prob": 0.011335330083966255}, {"id": 248, "seek": 126600, "start": 1287.0, "end": 1292.0, "text": " Of course, this is very easy if your application is exploding.", "tokens": [51414, 2720, 1164, 11, 341, 307, 588, 1858, 498, 428, 3861, 307, 35175, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09273391420190985, "compression_ratio": 1.5592417061611374, "no_speech_prob": 0.011335330083966255}, {"id": 249, "seek": 126600, "start": 1292.0, "end": 1294.0, "text": " It's very easy to see.", "tokens": [51664, 467, 311, 588, 1858, 281, 536, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09273391420190985, "compression_ratio": 1.5592417061611374, "no_speech_prob": 0.011335330083966255}, {"id": 250, "seek": 129400, "start": 1294.0, "end": 1300.0, "text": " It's like, what if people ask me, oh, can we do this if a button doesn't work?", "tokens": [50364, 467, 311, 411, 11, 437, 498, 561, 1029, 385, 11, 1954, 11, 393, 321, 360, 341, 498, 257, 2960, 1177, 380, 589, 30, 50664], "temperature": 0.0, "avg_logprob": -0.14292953754293508, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.05965551733970642}, {"id": 251, "seek": 129400, "start": 1300.0, "end": 1301.0, "text": " Can we do this?", "tokens": [50664, 1664, 321, 360, 341, 30, 50714], "temperature": 0.0, "avg_logprob": -0.14292953754293508, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.05965551733970642}, {"id": 252, "seek": 129400, "start": 1301.0, "end": 1303.0, "text": " Well, it depends what button.", "tokens": [50714, 1042, 11, 309, 5946, 437, 2960, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14292953754293508, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.05965551733970642}, {"id": 253, "seek": 129400, "start": 1303.0, "end": 1310.0, "text": " If it's the button that adds, imagine you're in Amazon, you break the button that adds things to the cart,", "tokens": [50814, 759, 309, 311, 264, 2960, 300, 10860, 11, 3811, 291, 434, 294, 6795, 11, 291, 1821, 264, 2960, 300, 10860, 721, 281, 264, 5467, 11, 51164], "temperature": 0.0, "avg_logprob": -0.14292953754293508, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.05965551733970642}, {"id": 254, "seek": 129400, "start": 1310.0, "end": 1313.0, "text": " and you get a metric that says nobody's adding things to the cart,", "tokens": [51164, 293, 291, 483, 257, 20678, 300, 1619, 5079, 311, 5127, 721, 281, 264, 5467, 11, 51314], "temperature": 0.0, "avg_logprob": -0.14292953754293508, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.05965551733970642}, {"id": 255, "seek": 129400, "start": 1313.0, "end": 1315.0, "text": " maybe you're like, oh, something is really bad.", "tokens": [51314, 1310, 291, 434, 411, 11, 1954, 11, 746, 307, 534, 1578, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14292953754293508, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.05965551733970642}, {"id": 256, "seek": 129400, "start": 1315.0, "end": 1317.0, "text": " Right.", "tokens": [51414, 1779, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14292953754293508, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.05965551733970642}, {"id": 257, "seek": 129400, "start": 1317.0, "end": 1323.0, "text": " So let me show you the analysis template.", "tokens": [51514, 407, 718, 385, 855, 291, 264, 5215, 12379, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14292953754293508, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.05965551733970642}, {"id": 258, "seek": 132300, "start": 1323.0, "end": 1328.0, "text": " Is this one?", "tokens": [50364, 1119, 341, 472, 30, 50614], "temperature": 0.0, "avg_logprob": -0.14385157618029365, "compression_ratio": 1.3673469387755102, "no_speech_prob": 0.011672143824398518}, {"id": 259, "seek": 132300, "start": 1328.0, "end": 1329.0, "text": " Yeah.", "tokens": [50614, 865, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14385157618029365, "compression_ratio": 1.3673469387755102, "no_speech_prob": 0.011672143824398518}, {"id": 260, "seek": 132300, "start": 1329.0, "end": 1336.0, "text": " In my case, my analysis template is a very complicated call that fails if this fails,", "tokens": [50664, 682, 452, 1389, 11, 452, 5215, 12379, 307, 257, 588, 6179, 818, 300, 18199, 498, 341, 18199, 11, 51014], "temperature": 0.0, "avg_logprob": -0.14385157618029365, "compression_ratio": 1.3673469387755102, "no_speech_prob": 0.011672143824398518}, {"id": 261, "seek": 132300, "start": 1336.0, "end": 1338.0, "text": " if this doesn't return a 200.", "tokens": [51014, 498, 341, 1177, 380, 2736, 257, 2331, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14385157618029365, "compression_ratio": 1.3673469387755102, "no_speech_prob": 0.011672143824398518}, {"id": 262, "seek": 132300, "start": 1338.0, "end": 1344.0, "text": " But again, you can integrate this with whatever you want, metrics.", "tokens": [51114, 583, 797, 11, 291, 393, 13365, 341, 365, 2035, 291, 528, 11, 16367, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14385157618029365, "compression_ratio": 1.3673469387755102, "no_speech_prob": 0.011672143824398518}, {"id": 263, "seek": 134400, "start": 1344.0, "end": 1351.0, "text": " Argo rollouts also gives you a nice dashboard.", "tokens": [50364, 1587, 1571, 3373, 7711, 611, 2709, 291, 257, 1481, 18342, 13, 50714], "temperature": 0.0, "avg_logprob": -0.22767341613769532, "compression_ratio": 1.5739644970414202, "no_speech_prob": 0.1460241675376892}, {"id": 264, "seek": 134400, "start": 1351.0, "end": 1355.0, "text": " If you are not into the command line, you can come here.", "tokens": [50714, 759, 291, 366, 406, 666, 264, 5622, 1622, 11, 291, 393, 808, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.22767341613769532, "compression_ratio": 1.5739644970414202, "no_speech_prob": 0.1460241675376892}, {"id": 265, "seek": 134400, "start": 1355.0, "end": 1360.0, "text": " And here.", "tokens": [50914, 400, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.22767341613769532, "compression_ratio": 1.5739644970414202, "no_speech_prob": 0.1460241675376892}, {"id": 266, "seek": 134400, "start": 1360.0, "end": 1366.0, "text": " So where I can see the status of my rollout, what is strategy.", "tokens": [51164, 407, 689, 286, 393, 536, 264, 6558, 295, 452, 3373, 346, 11, 437, 307, 5206, 13, 51464], "temperature": 0.0, "avg_logprob": -0.22767341613769532, "compression_ratio": 1.5739644970414202, "no_speech_prob": 0.1460241675376892}, {"id": 267, "seek": 134400, "start": 1366.0, "end": 1372.0, "text": " As I said, Argo rollout supports multiple strategies on some of the more complex drivers.", "tokens": [51464, 1018, 286, 848, 11, 1587, 1571, 3373, 346, 9346, 3866, 9029, 322, 512, 295, 264, 544, 3997, 11590, 13, 51764], "temperature": 0.0, "avg_logprob": -0.22767341613769532, "compression_ratio": 1.5739644970414202, "no_speech_prob": 0.1460241675376892}, {"id": 268, "seek": 137200, "start": 1372.0, "end": 1380.0, "text": " I can see my steps that I showed you before in the Jamel, 20, 40, 50, 80.", "tokens": [50364, 286, 393, 536, 452, 4439, 300, 286, 4712, 291, 949, 294, 264, 10372, 338, 11, 945, 11, 3356, 11, 2625, 11, 4688, 13, 50764], "temperature": 0.0, "avg_logprob": -0.19506302817923124, "compression_ratio": 1.3724137931034484, "no_speech_prob": 0.12425345927476883}, {"id": 269, "seek": 137200, "start": 1380.0, "end": 1386.0, "text": " And I can say what was the last image that I pushed,", "tokens": [50764, 400, 286, 393, 584, 437, 390, 264, 1036, 3256, 300, 286, 9152, 11, 51064], "temperature": 0.0, "avg_logprob": -0.19506302817923124, "compression_ratio": 1.3724137931034484, "no_speech_prob": 0.12425345927476883}, {"id": 270, "seek": 137200, "start": 1386.0, "end": 1398.0, "text": " and I could click here and do the clickity clock instead of doing Jamel.", "tokens": [51064, 293, 286, 727, 2052, 510, 293, 360, 264, 2052, 507, 7830, 2602, 295, 884, 10372, 338, 13, 51664], "temperature": 0.0, "avg_logprob": -0.19506302817923124, "compression_ratio": 1.3724137931034484, "no_speech_prob": 0.12425345927476883}, {"id": 271, "seek": 139800, "start": 1398.0, "end": 1403.0, "text": " Okay.", "tokens": [50364, 1033, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2718074083328247, "compression_ratio": 1.5, "no_speech_prob": 0.03323392570018768}, {"id": 272, "seek": 139800, "start": 1403.0, "end": 1408.0, "text": " So, yeah, what I mentioned before was if you're using service mesh,", "tokens": [50614, 407, 11, 1338, 11, 437, 286, 2835, 949, 390, 498, 291, 434, 1228, 2643, 17407, 11, 50864], "temperature": 0.0, "avg_logprob": -0.2718074083328247, "compression_ratio": 1.5, "no_speech_prob": 0.03323392570018768}, {"id": 273, "seek": 139800, "start": 1408.0, "end": 1412.0, "text": " like Istio, then it integrates with a bunch of service mesh", "tokens": [50864, 411, 12810, 1004, 11, 550, 309, 3572, 1024, 365, 257, 3840, 295, 2643, 17407, 51064], "temperature": 0.0, "avg_logprob": -0.2718074083328247, "compression_ratio": 1.5, "no_speech_prob": 0.03323392570018768}, {"id": 274, "seek": 139800, "start": 1412.0, "end": 1413.0, "text": " ingress providers.", "tokens": [51064, 3957, 735, 11330, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2718074083328247, "compression_ratio": 1.5, "no_speech_prob": 0.03323392570018768}, {"id": 275, "seek": 139800, "start": 1413.0, "end": 1417.0, "text": " So you could go and say, I want 1% of traffic because Istio supports", "tokens": [51114, 407, 291, 727, 352, 293, 584, 11, 286, 528, 502, 4, 295, 6419, 570, 12810, 1004, 9346, 51314], "temperature": 0.0, "avg_logprob": -0.2718074083328247, "compression_ratio": 1.5, "no_speech_prob": 0.03323392570018768}, {"id": 276, "seek": 139800, "start": 1417.0, "end": 1422.0, "text": " doing those things instead of saying more, because when you are using", "tokens": [51314, 884, 729, 721, 2602, 295, 1566, 544, 11, 570, 562, 291, 366, 1228, 51564], "temperature": 0.0, "avg_logprob": -0.2718074083328247, "compression_ratio": 1.5, "no_speech_prob": 0.03323392570018768}, {"id": 277, "seek": 142200, "start": 1422.0, "end": 1425.0, "text": " only pods, you don't have anyone here.", "tokens": [50364, 787, 31925, 11, 291, 500, 380, 362, 2878, 510, 13, 50514], "temperature": 0.0, "avg_logprob": -0.18548094431559245, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.05585584416985512}, {"id": 278, "seek": 142200, "start": 1425.0, "end": 1428.0, "text": " Pod is going to receive the traffic or not.", "tokens": [50514, 12646, 307, 516, 281, 4774, 264, 6419, 420, 406, 13, 50664], "temperature": 0.0, "avg_logprob": -0.18548094431559245, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.05585584416985512}, {"id": 279, "seek": 142200, "start": 1428.0, "end": 1432.0, "text": " So it's more of an approximation.", "tokens": [50664, 407, 309, 311, 544, 295, 364, 28023, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18548094431559245, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.05585584416985512}, {"id": 280, "seek": 142200, "start": 1432.0, "end": 1437.0, "text": " But with Istio and other advanced things, you can do more complex.", "tokens": [50864, 583, 365, 12810, 1004, 293, 661, 7339, 721, 11, 291, 393, 360, 544, 3997, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18548094431559245, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.05585584416985512}, {"id": 281, "seek": 142200, "start": 1437.0, "end": 1446.0, "text": " We hook it up with Prometheus, also the support for multiple things to get metrics from.", "tokens": [51114, 492, 6328, 309, 493, 365, 2114, 649, 42209, 11, 611, 264, 1406, 337, 3866, 721, 281, 483, 16367, 490, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18548094431559245, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.05585584416985512}, {"id": 282, "seek": 144600, "start": 1446.0, "end": 1453.0, "text": " And, yeah, hopefully you'll learn how to do a progressive delivery", "tokens": [50364, 400, 11, 1338, 11, 4696, 291, 603, 1466, 577, 281, 360, 257, 16131, 8982, 50714], "temperature": 0.0, "avg_logprob": -0.17002210897557876, "compression_ratio": 1.4382716049382716, "no_speech_prob": 0.03891584649682045}, {"id": 283, "seek": 144600, "start": 1453.0, "end": 1456.0, "text": " canary deployment very easily.", "tokens": [50714, 393, 822, 19317, 588, 3612, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17002210897557876, "compression_ratio": 1.4382716049382716, "no_speech_prob": 0.03891584649682045}, {"id": 284, "seek": 144600, "start": 1456.0, "end": 1460.0, "text": " Just you need to do some Jamel here and there.", "tokens": [50864, 1449, 291, 643, 281, 360, 512, 10372, 338, 510, 293, 456, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17002210897557876, "compression_ratio": 1.4382716049382716, "no_speech_prob": 0.03891584649682045}, {"id": 285, "seek": 144600, "start": 1460.0, "end": 1464.0, "text": " Let me see.", "tokens": [51064, 961, 385, 536, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17002210897557876, "compression_ratio": 1.4382716049382716, "no_speech_prob": 0.03891584649682045}, {"id": 286, "seek": 144600, "start": 1464.0, "end": 1469.0, "text": " On here, this one.", "tokens": [51264, 1282, 510, 11, 341, 472, 13, 51514], "temperature": 0.0, "avg_logprob": -0.17002210897557876, "compression_ratio": 1.4382716049382716, "no_speech_prob": 0.03891584649682045}, {"id": 287, "seek": 144600, "start": 1469.0, "end": 1474.0, "text": " So you can have the other labels to the existing version,", "tokens": [51514, 407, 291, 393, 362, 264, 661, 16949, 281, 264, 6741, 3037, 11, 51764], "temperature": 0.0, "avg_logprob": -0.17002210897557876, "compression_ratio": 1.4382716049382716, "no_speech_prob": 0.03891584649682045}, {"id": 288, "seek": 147400, "start": 1474.0, "end": 1477.0, "text": " to the stable version as labels to the new version.", "tokens": [50364, 281, 264, 8351, 3037, 382, 16949, 281, 264, 777, 3037, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1479980057361079, "compression_ratio": 1.8122270742358078, "no_speech_prob": 0.05280371755361557}, {"id": 289, "seek": 147400, "start": 1477.0, "end": 1481.0, "text": " So you can do other things with services on Kubernetes.", "tokens": [50514, 407, 291, 393, 360, 661, 721, 365, 3328, 322, 23145, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1479980057361079, "compression_ratio": 1.8122270742358078, "no_speech_prob": 0.05280371755361557}, {"id": 290, "seek": 147400, "start": 1481.0, "end": 1490.0, "text": " You can pass what analysis you want to run and you set what steps to run.", "tokens": [50714, 509, 393, 1320, 437, 5215, 291, 528, 281, 1190, 293, 291, 992, 437, 4439, 281, 1190, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1479980057361079, "compression_ratio": 1.8122270742358078, "no_speech_prob": 0.05280371755361557}, {"id": 291, "seek": 147400, "start": 1490.0, "end": 1493.0, "text": " And everything else is just the template.", "tokens": [51164, 400, 1203, 1646, 307, 445, 264, 12379, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1479980057361079, "compression_ratio": 1.8122270742358078, "no_speech_prob": 0.05280371755361557}, {"id": 292, "seek": 147400, "start": 1493.0, "end": 1495.0, "text": " And if in the deployment template.", "tokens": [51314, 400, 498, 294, 264, 19317, 12379, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1479980057361079, "compression_ratio": 1.8122270742358078, "no_speech_prob": 0.05280371755361557}, {"id": 293, "seek": 147400, "start": 1495.0, "end": 1498.0, "text": " If you don't want to put the deployment template in the rollout object,", "tokens": [51414, 759, 291, 500, 380, 528, 281, 829, 264, 19317, 12379, 294, 264, 3373, 346, 2657, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1479980057361079, "compression_ratio": 1.8122270742358078, "no_speech_prob": 0.05280371755361557}, {"id": 294, "seek": 147400, "start": 1498.0, "end": 1503.0, "text": " you just point here, there's another option that says points to existing deployment.", "tokens": [51564, 291, 445, 935, 510, 11, 456, 311, 1071, 3614, 300, 1619, 2793, 281, 6741, 19317, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1479980057361079, "compression_ratio": 1.8122270742358078, "no_speech_prob": 0.05280371755361557}, {"id": 295, "seek": 150300, "start": 1503.0, "end": 1508.0, "text": " The only problem with that is that rollouts is not when you're migrating,", "tokens": [50364, 440, 787, 1154, 365, 300, 307, 300, 3373, 7711, 307, 406, 562, 291, 434, 6186, 8754, 11, 50614], "temperature": 0.0, "avg_logprob": -0.18068451716982084, "compression_ratio": 1.819327731092437, "no_speech_prob": 0.025123877450823784}, {"id": 296, "seek": 150300, "start": 1508.0, "end": 1511.0, "text": " rollouts is not going to scale down the deployment.", "tokens": [50614, 3373, 7711, 307, 406, 516, 281, 4373, 760, 264, 19317, 13, 50764], "temperature": 0.0, "avg_logprob": -0.18068451716982084, "compression_ratio": 1.819327731092437, "no_speech_prob": 0.025123877450823784}, {"id": 297, "seek": 150300, "start": 1511.0, "end": 1515.0, "text": " A colleague of mine, she submitted a PR to Argus,", "tokens": [50764, 316, 13532, 295, 3892, 11, 750, 14405, 257, 11568, 281, 1587, 21956, 11, 50964], "temperature": 0.0, "avg_logprob": -0.18068451716982084, "compression_ratio": 1.819327731092437, "no_speech_prob": 0.025123877450823784}, {"id": 298, "seek": 150300, "start": 1515.0, "end": 1517.0, "text": " which is going to be in the next version.", "tokens": [50964, 597, 307, 516, 281, 312, 294, 264, 958, 3037, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18068451716982084, "compression_ratio": 1.819327731092437, "no_speech_prob": 0.025123877450823784}, {"id": 299, "seek": 150300, "start": 1517.0, "end": 1522.0, "text": " So it will automatically, if you have like thousands of deployments,", "tokens": [51064, 407, 309, 486, 6772, 11, 498, 291, 362, 411, 5383, 295, 7274, 1117, 11, 51314], "temperature": 0.0, "avg_logprob": -0.18068451716982084, "compression_ratio": 1.819327731092437, "no_speech_prob": 0.025123877450823784}, {"id": 300, "seek": 150300, "start": 1522.0, "end": 1525.0, "text": " when you spin out a rollout with a deployment,", "tokens": [51314, 562, 291, 6060, 484, 257, 3373, 346, 365, 257, 19317, 11, 51464], "temperature": 0.0, "avg_logprob": -0.18068451716982084, "compression_ratio": 1.819327731092437, "no_speech_prob": 0.025123877450823784}, {"id": 301, "seek": 150300, "start": 1525.0, "end": 1528.0, "text": " I pointed to a deployment, when the rollout is successful,", "tokens": [51464, 286, 10932, 281, 257, 19317, 11, 562, 264, 3373, 346, 307, 4406, 11, 51614], "temperature": 0.0, "avg_logprob": -0.18068451716982084, "compression_ratio": 1.819327731092437, "no_speech_prob": 0.025123877450823784}, {"id": 302, "seek": 150300, "start": 1528.0, "end": 1530.0, "text": " it's going to scale down the deployment.", "tokens": [51614, 309, 311, 516, 281, 4373, 760, 264, 19317, 13, 51714], "temperature": 0.0, "avg_logprob": -0.18068451716982084, "compression_ratio": 1.819327731092437, "no_speech_prob": 0.025123877450823784}, {"id": 303, "seek": 153000, "start": 1530.0, "end": 1535.0, "text": " So that's how it will actually exist.", "tokens": [50364, 407, 300, 311, 577, 309, 486, 767, 2514, 13, 50614], "temperature": 0.0, "avg_logprob": -0.49846863746643066, "compression_ratio": 1.09375, "no_speech_prob": 0.1725824922323227}, {"id": 304, "seek": 153000, "start": 1535.0, "end": 1544.0, "text": " Okay, so, yeah, and what's that thing?", "tokens": [50614, 1033, 11, 370, 11, 1338, 11, 293, 437, 311, 300, 551, 30, 51064], "temperature": 0.0, "avg_logprob": -0.49846863746643066, "compression_ratio": 1.09375, "no_speech_prob": 0.1725824922323227}, {"id": 305, "seek": 153000, "start": 1544.0, "end": 1552.0, "text": " I lost my...", "tokens": [51064, 286, 2731, 452, 485, 51464], "temperature": 0.0, "avg_logprob": -0.49846863746643066, "compression_ratio": 1.09375, "no_speech_prob": 0.1725824922323227}, {"id": 306, "seek": 153000, "start": 1552.0, "end": 1555.0, "text": " Did I close it?", "tokens": [51464, 2589, 286, 1998, 309, 30, 51614], "temperature": 0.0, "avg_logprob": -0.49846863746643066, "compression_ratio": 1.09375, "no_speech_prob": 0.1725824922323227}, {"id": 307, "seek": 155500, "start": 1555.0, "end": 1568.0, "text": " Yeah.", "tokens": [50364, 865, 13, 51014], "temperature": 0.0, "avg_logprob": -0.25025982326931423, "compression_ratio": 1.2347826086956522, "no_speech_prob": 0.08419130742549896}, {"id": 308, "seek": 155500, "start": 1568.0, "end": 1573.0, "text": " Okay, so, just a quick summary, you saw everything?", "tokens": [51014, 1033, 11, 370, 11, 445, 257, 1702, 12691, 11, 291, 1866, 1203, 30, 51264], "temperature": 0.0, "avg_logprob": -0.25025982326931423, "compression_ratio": 1.2347826086956522, "no_speech_prob": 0.08419130742549896}, {"id": 309, "seek": 155500, "start": 1573.0, "end": 1582.0, "text": " And I hope that this helped you and you can try it and do it at home if you like it.", "tokens": [51264, 400, 286, 1454, 300, 341, 4254, 291, 293, 291, 393, 853, 309, 293, 360, 309, 412, 1280, 498, 291, 411, 309, 13, 51714], "temperature": 0.0, "avg_logprob": -0.25025982326931423, "compression_ratio": 1.2347826086956522, "no_speech_prob": 0.08419130742549896}, {"id": 310, "seek": 158200, "start": 1582.0, "end": 1586.0, "text": " And I have time for asking me two questions.", "tokens": [50364, 400, 286, 362, 565, 337, 3365, 385, 732, 1651, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2800886936676808, "compression_ratio": 1.7650602409638554, "no_speech_prob": 0.15534819662570953}, {"id": 311, "seek": 158200, "start": 1586.0, "end": 1590.0, "text": " Two questions.", "tokens": [50564, 4453, 1651, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2800886936676808, "compression_ratio": 1.7650602409638554, "no_speech_prob": 0.15534819662570953}, {"id": 312, "seek": 158200, "start": 1590.0, "end": 1592.0, "text": " No questions. One question.", "tokens": [50764, 883, 1651, 13, 1485, 1168, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2800886936676808, "compression_ratio": 1.7650602409638554, "no_speech_prob": 0.15534819662570953}, {"id": 313, "seek": 158200, "start": 1592.0, "end": 1599.0, "text": " I was wondering if you've been testing using the gateway API and some fingers in the waiting?", "tokens": [50864, 286, 390, 6359, 498, 291, 600, 668, 4997, 1228, 264, 28532, 9362, 293, 512, 7350, 294, 264, 3806, 30, 51214], "temperature": 0.0, "avg_logprob": -0.2800886936676808, "compression_ratio": 1.7650602409638554, "no_speech_prob": 0.15534819662570953}, {"id": 314, "seek": 158200, "start": 1599.0, "end": 1603.0, "text": " So the question is if I tested using the gateway API instead of fingers,", "tokens": [51214, 407, 264, 1168, 307, 498, 286, 8246, 1228, 264, 28532, 9362, 2602, 295, 7350, 11, 51414], "temperature": 0.0, "avg_logprob": -0.2800886936676808, "compression_ratio": 1.7650602409638554, "no_speech_prob": 0.15534819662570953}, {"id": 315, "seek": 158200, "start": 1603.0, "end": 1607.0, "text": " no, I have not been using the API yet,", "tokens": [51414, 572, 11, 286, 362, 406, 668, 1228, 264, 9362, 1939, 11, 51614], "temperature": 0.0, "avg_logprob": -0.2800886936676808, "compression_ratio": 1.7650602409638554, "no_speech_prob": 0.15534819662570953}, {"id": 316, "seek": 160700, "start": 1607.0, "end": 1612.0, "text": " but I'm guessing that if there's no support already, there will be.", "tokens": [50364, 457, 286, 478, 17939, 300, 498, 456, 311, 572, 1406, 1217, 11, 456, 486, 312, 13, 50614], "temperature": 0.0, "avg_logprob": -0.3306284490621315, "compression_ratio": 1.236220472440945, "no_speech_prob": 0.09199705719947815}, {"id": 317, "seek": 160700, "start": 1612.0, "end": 1618.0, "text": " Because...", "tokens": [50614, 1436, 485, 50914], "temperature": 0.0, "avg_logprob": -0.3306284490621315, "compression_ratio": 1.236220472440945, "no_speech_prob": 0.09199705719947815}, {"id": 318, "seek": 160700, "start": 1618.0, "end": 1620.0, "text": " We did not.", "tokens": [50914, 492, 630, 406, 13, 51014], "temperature": 0.0, "avg_logprob": -0.3306284490621315, "compression_ratio": 1.236220472440945, "no_speech_prob": 0.09199705719947815}, {"id": 319, "seek": 160700, "start": 1620.0, "end": 1626.0, "text": " Yeah.", "tokens": [51014, 865, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3306284490621315, "compression_ratio": 1.236220472440945, "no_speech_prob": 0.09199705719947815}, {"id": 320, "seek": 160700, "start": 1626.0, "end": 1634.0, "text": " Hello. So my question is that for, in case of buggy rollout,", "tokens": [51314, 2425, 13, 407, 452, 1168, 307, 300, 337, 11, 294, 1389, 295, 7426, 1480, 3373, 346, 11, 51714], "temperature": 0.0, "avg_logprob": -0.3306284490621315, "compression_ratio": 1.236220472440945, "no_speech_prob": 0.09199705719947815}, {"id": 321, "seek": 163400, "start": 1634.0, "end": 1640.0, "text": " the particular traffic which is forward to the buggy instances,", "tokens": [50364, 264, 1729, 6419, 597, 307, 2128, 281, 264, 7426, 1480, 14519, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11837466863485482, "compression_ratio": 1.564625850340136, "no_speech_prob": 0.15348191559314728}, {"id": 322, "seek": 163400, "start": 1640.0, "end": 1650.0, "text": " is it possible to automatically replicate it and send it to the stable versions after the fail?", "tokens": [50664, 307, 309, 1944, 281, 6772, 25356, 309, 293, 2845, 309, 281, 264, 8351, 9606, 934, 264, 3061, 30, 51164], "temperature": 0.0, "avg_logprob": -0.11837466863485482, "compression_ratio": 1.564625850340136, "no_speech_prob": 0.15348191559314728}, {"id": 323, "seek": 163400, "start": 1650.0, "end": 1658.0, "text": " To ensure that even the traffic which hits the buggy rollout instances", "tokens": [51164, 1407, 5586, 300, 754, 264, 6419, 597, 8664, 264, 7426, 1480, 3373, 346, 14519, 51564], "temperature": 0.0, "avg_logprob": -0.11837466863485482, "compression_ratio": 1.564625850340136, "no_speech_prob": 0.15348191559314728}, {"id": 324, "seek": 165800, "start": 1658.0, "end": 1664.0, "text": " is served later by stable versions?", "tokens": [50364, 307, 7584, 1780, 538, 8351, 9606, 30, 50664], "temperature": 0.0, "avg_logprob": -0.3081659098140529, "compression_ratio": 1.36, "no_speech_prob": 0.1909511685371399}, {"id": 325, "seek": 165800, "start": 1664.0, "end": 1666.0, "text": " So if it's...", "tokens": [50664, 407, 498, 309, 311, 485, 50764], "temperature": 0.0, "avg_logprob": -0.3081659098140529, "compression_ratio": 1.36, "no_speech_prob": 0.1909511685371399}, {"id": 326, "seek": 165800, "start": 1666.0, "end": 1670.0, "text": " It's possible to run it back automatically, but also...", "tokens": [50764, 467, 311, 1944, 281, 1190, 309, 646, 6772, 11, 457, 611, 485, 50964], "temperature": 0.0, "avg_logprob": -0.3081659098140529, "compression_ratio": 1.36, "no_speech_prob": 0.1909511685371399}, {"id": 327, "seek": 165800, "start": 1670.0, "end": 1674.0, "text": " Yeah, the individual traffic, individual request.", "tokens": [50964, 865, 11, 264, 2609, 6419, 11, 2609, 5308, 13, 51164], "temperature": 0.0, "avg_logprob": -0.3081659098140529, "compression_ratio": 1.36, "no_speech_prob": 0.1909511685371399}, {"id": 328, "seek": 165800, "start": 1674.0, "end": 1678.0, "text": " So you don't want any user to see the spot?", "tokens": [51164, 407, 291, 500, 380, 528, 604, 4195, 281, 536, 264, 4008, 30, 51364], "temperature": 0.0, "avg_logprob": -0.3081659098140529, "compression_ratio": 1.36, "no_speech_prob": 0.1909511685371399}, {"id": 329, "seek": 165800, "start": 1678.0, "end": 1680.0, "text": " Yes.", "tokens": [51364, 1079, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3081659098140529, "compression_ratio": 1.36, "no_speech_prob": 0.1909511685371399}, {"id": 330, "seek": 168000, "start": 1680.0, "end": 1688.0, "text": " The other thing you could do, if you use a service mess,", "tokens": [50364, 440, 661, 551, 291, 727, 360, 11, 498, 291, 764, 257, 2643, 2082, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1778268477495979, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.20470872521400452}, {"id": 331, "seek": 168000, "start": 1688.0, "end": 1696.0, "text": " probably is send a clone the traffic and send a clone to the new version,", "tokens": [50764, 1391, 307, 2845, 257, 26506, 264, 6419, 293, 2845, 257, 26506, 281, 264, 777, 3037, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1778268477495979, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.20470872521400452}, {"id": 332, "seek": 168000, "start": 1696.0, "end": 1699.0, "text": " but the actual traffic is going to the old version.", "tokens": [51164, 457, 264, 3539, 6419, 307, 516, 281, 264, 1331, 3037, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1778268477495979, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.20470872521400452}, {"id": 333, "seek": 168000, "start": 1699.0, "end": 1703.0, "text": " And you could see if the new version is breaking or not.", "tokens": [51314, 400, 291, 727, 536, 498, 264, 777, 3037, 307, 7697, 420, 406, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1778268477495979, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.20470872521400452}, {"id": 334, "seek": 168000, "start": 1703.0, "end": 1709.0, "text": " But also that's tricky because you need to make sure that it's not changing your state.", "tokens": [51514, 583, 611, 300, 311, 12414, 570, 291, 643, 281, 652, 988, 300, 309, 311, 406, 4473, 428, 1785, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1778268477495979, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.20470872521400452}, {"id": 335, "seek": 170900, "start": 1709.0, "end": 1714.0, "text": " If you are getting gets, it's fine if you are changing status.", "tokens": [50364, 759, 291, 366, 1242, 2170, 11, 309, 311, 2489, 498, 291, 366, 4473, 6558, 13, 50614], "temperature": 0.0, "avg_logprob": -0.16693061828613281, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.24321703612804413}, {"id": 336, "seek": 170900, "start": 1714.0, "end": 1721.0, "text": " That's my point. Don't do the duplication in advance because it will go to the parallel execution,", "tokens": [50614, 663, 311, 452, 935, 13, 1468, 380, 360, 264, 17154, 399, 294, 7295, 570, 309, 486, 352, 281, 264, 8952, 15058, 11, 50964], "temperature": 0.0, "avg_logprob": -0.16693061828613281, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.24321703612804413}, {"id": 337, "seek": 170900, "start": 1721.0, "end": 1728.0, "text": " but do it only when the first execution failed because it's go to the canary instance.", "tokens": [50964, 457, 360, 309, 787, 562, 264, 700, 15058, 7612, 570, 309, 311, 352, 281, 264, 393, 822, 5197, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16693061828613281, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.24321703612804413}, {"id": 338, "seek": 170900, "start": 1728.0, "end": 1730.0, "text": " Yeah, I think you can do that.", "tokens": [51314, 865, 11, 286, 519, 291, 393, 360, 300, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16693061828613281, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.24321703612804413}, {"id": 339, "seek": 170900, "start": 1730.0, "end": 1737.0, "text": " Send traffic to the new version, but it's a copy of the traffic that is not seen by any user.", "tokens": [51414, 17908, 6419, 281, 264, 777, 3037, 11, 457, 309, 311, 257, 5055, 295, 264, 6419, 300, 307, 406, 1612, 538, 604, 4195, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16693061828613281, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.24321703612804413}, {"id": 340, "seek": 173700, "start": 1737.0, "end": 1742.0, "text": " And then at some point you could say, okay, this is good. I'm promoting this.", "tokens": [50364, 400, 550, 412, 512, 935, 291, 727, 584, 11, 1392, 11, 341, 307, 665, 13, 286, 478, 16383, 341, 13, 50614], "temperature": 0.0, "avg_logprob": -0.18183999969845727, "compression_ratio": 1.174757281553398, "no_speech_prob": 0.1619356870651245}, {"id": 341, "seek": 173700, "start": 1742.0, "end": 1744.0, "text": " I think it's doable.", "tokens": [50614, 286, 519, 309, 311, 41183, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18183999969845727, "compression_ratio": 1.174757281553398, "no_speech_prob": 0.1619356870651245}, {"id": 342, "seek": 173700, "start": 1744.0, "end": 1746.0, "text": " Yeah, thank you.", "tokens": [50714, 865, 11, 1309, 291, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18183999969845727, "compression_ratio": 1.174757281553398, "no_speech_prob": 0.1619356870651245}, {"id": 343, "seek": 173700, "start": 1746.0, "end": 1748.0, "text": " Okay.", "tokens": [50814, 1033, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18183999969845727, "compression_ratio": 1.174757281553398, "no_speech_prob": 0.1619356870651245}, {"id": 344, "seek": 176700, "start": 1767.0, "end": 1769.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.588980476061503, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.988408625125885}, {"id": 345, "seek": 179700, "start": 1797.0, "end": 1800.0, "text": " Thank you.", "tokens": [50414, 1044, 291, 13, 50514], "temperature": 0.0, "avg_logprob": -0.4671509265899658, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9894458651542664}], "language": "en"}