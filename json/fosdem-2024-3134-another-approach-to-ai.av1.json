{"text": " Thank you all for joining us in a great, you don't. It's not working? Why is it not working? It is? Okay, good. Let's not correct. So thank you all for joining us and here to talk about next. AI. Exactly. Another approach to AI is just port fleet. Okay. Does this work well enough? You can hear it in the back and all that. Okay. I see thumbs up. That's wonderful. Yeah. Well, I had this. I'm your sport fleet. Direct communications co-founder at NextLoud. So, yeah, the thing we do at NextLoud is collaboration. That's of course what this room is all about. Now, there's this AI thing coming. And so I'm hoping to try and make this conversation a little bit interactive. I mean, there are other people here from Xweek and other projects who are working on, well, collaboration tools as well, open source collaboration tools. And, you know, this AI thing, I mean, there have been AI-ish tools being used for a long time, but a lot of them are also still quite new. So I'm kind of hoping that we can also have a bit of a conversation about it. Because, well, there are pros and cons. I mean, we'll get to all that stuff. Of course, the big thing here is, yeah, that we have, like, these big companies, right? They all want our data, and AI is for them another thing to use that data for. So, yeah, I mean, AI, I don't know how deep I want to go into what it is, because I think all of us know it a little bit. But we don't want to live in a world where there are five companies, you know, who run all our data, and that's kind of a little bit the case right now. I think that if Trump and his next presidency tells Microsoft to shut down their service in Europe, then basically you cannot get a new passport, you cannot, yeah, nobody can work at government here, for example, right? This is, I think, a bit of an issue. And, I mean, NextLoud is one of the projects that's working on solving that issue, essentially trying to give, well, companies, individuals, but also hopefully government, back the control over their data. We've built a collaboration platform. I'm guessing how many of you are not familiar with NextLoud? Yeah, okay, it's like six people. Google it. I will then not go into that, sorry, or duck-duck-go it, that would be better, obviously. So, as a company, we build an alternative for 365 in very quick, simple terms. And with alternative, we mean that as a government or a company, we think it's important that you have a choice. It's totally fine if you're happy that your data is at an American company and that U.S. buy agencies have access to it. If you're good with that, if that's not a threat to your business, that's fine. Government, I think it's by definition a threat, but that's their choice. But we think there should be, like, a choice. There should be an alternative. And an alternative is only an alternative, is it is, if it does what the other product does, obviously, and in a safe way and has enough ability to be used for a serious company. So, that's what we're building and we have built. That's why German and French and most European governments are in places already using Nexlout, be it cities, be it at a state level or federal level. So, as a company, we care a lot. Nexlout for us is a mission, it's a goal. It's like our way to try and make the world a tiny little bit better. And we want to work in an open, collaborative way. Therefore, we're very happy, of course, that it's used by thousands of governments and universities, et cetera, et cetera. And, of course, we're building this completely in the open. And again, that will be relevant because, of course, I think the future for AI will better be open, otherwise we are, well, just as crude as it is with collaboration platforms, honestly. And we have a wonderful community working with us and all this stuff, which is awesome. Also, as a company, we try to be open and transparent, not depending on venture capital, et cetera, but be self-owned. Anyhow, AI. So, AI is like, we've already introduced a ton of AI things over the years, like little things, and I will show some of them, but of course, with the latest LLMs and stuff, it's getting really complicated. I mean, there are tons of problems with it. I have a lot of potential, right? AI can help us make repetitive tasks easier, quicker, et cetera, but at the same time, Big Tech is basically loving it. They have all the data to be able to build the AI's. It costs tens or hundreds of millions right now to really train the proper LLMs, so they really have a bit of a monopoly here. And, yeah, the rest of us will have to just accept that they're using all our data to do it. And a lot of companies are already really realizing this is a problem for them, right? It's Citigroup and Goldman Sachs. They are actually not allowing their employees to use tools like chatGPT. I mean, if you're BMW and you're working on a new car and you're using an AI to generate some ideas or summarize some proposals, and you discover later that six months later Tesla, while designing their car, suddenly got some of your ideas coming into their AI planning, then there's a bit of an issue here. And, of course, this kind of stuff is happening. The company, like a while ago, Twitter and Zoom, they changed their terms of service to allow for training on user data. And, yeah, this is really an issue for business as well as, well, obviously, all our society. And then I'm not even talking about data biases in these models, carbon footprint. I mean, I think most of you are aware of all the issues with AI. So, honestly, I don't think the question is to AI or not, because there are too many benefits. The opportunities are really big, I think. I've been trying to make a bit of a list of that, but I was just changing it while standing here in line outside. So this is definitely not complete. So I'm just going to put it all on the screen and ask what's missing. I mean, I think there are some basics, you know, text to speech, speech to text, recognizing faces on photos and recognizing objects on it, et cetera. This is already, like Nexot has been shipping this for three years, four years already, these kind of things. It's just one model that you download and does this stuff, and translation and other one. It's fairly, I think it's, I mean, it's not simple. It's technically complicated stuff, but it works. And there are not huge risks. You don't need to send your data to Google anymore if you want text to speech, or if you want image recognition and being able to search for a dog and find all the pictures of your favorite pet. So this is already there, and it's not terribly complicated to use for a person. But of course, you now have all these new language models. I think there's really a big benefit, unlike dealing with information overload. You have tons of emails coming in. You have, I don't know, papers to read, et cetera. And these LLMs, they, I know they create a lot of fake content and hallucinate stuff. But the thing they're pretty reliable at is summarizing. And this is really quite important. I don't know how many emails you get, but I get a ton, and I would love to be able to summarize it or help select, you know, useful emails, et cetera. And this stuff is really possible, or, I don't know, meeting notes. And, yeah. So this is, I think, where these models can be super helpful. And you have, like, text generation, of course, they can do help out with this. You have also image analysis of various things. There have been some demos from Microsoft and Google already about a year ago, where they basically were showing that you have, like, a spreadsheet, and you select something in it, and then you type a question about it, and then it makes a graph that answers the question. This kind of stuff is also pretty magical. And there are tons of people in office all over the world that would benefit a lot from having this stuff. So I think, yeah, the benefits are really there. Another thing is, like, automation. Just talking about it with a colleague. This is, like, also a next step, you know, if you can say to the LLM, like, hey, send, make an appointment with another person, and then they try chat, and if that doesn't work, they try email. These kind of things would be really helpful, I think, in day-to-day work. So, yeah, I don't know. If there are other ideas or things that are missing, I'd love to hear it, actually, and make my list a little bit more complete, but we'll get to that, I think. So I just wanted to show a couple of examples, like we have this feature now, the Threads Summary, that makes a summary of your emails. Another example is, like, an Excel text. You can just select some text and say, hey, summarize it, create a headline. It's all quite simple to use. And image generation, of course, I mean, this is a horrible image, but, you know, you can make things that look good. And then you have the data analysis, and you have automation, all these other features we have ideas on. I'll share some things a bit later on. So I think we need to do AI in our collaboration platforms, like X-Wiki, you guys need to have a plan. I know only Office, but they integrated just chat GPT. I think we need a little bit more than just that, because, well, we're losing the on-prem capabilities, right? It's not competitive if you're just integrating chat GPT, then the data is sent to the U.S. anyway. So, yeah, that's not really a good solution. So the question is, how can we get this without the problems? And I think I'm in a room with open source people, so I think the answer for most of you is obvious, and this to me at least, just transparency and being open, yeah? And this is kind of the thing that we've been working on at NextLoud. We kind of made some rules for ourselves, so we have been doing AI as things, but when the whole text stuff from chat GPT came out, actually that was at the FOSDEM two years ago, we talked to people and each other, and we have some fairly smart people on board, also from the research community, and we tried to come up with, like, how can we handle this? Because we add more AI features, like we don't want to be left behind, and we need to be an alternative, as I said earlier, and you can only be an alternative if you offer similar features, otherwise who's going to use your product? But then how can you do that in an okayish way? So the idea we came up with was to at least create transparency, and of course, choice, I'll get to that next, but first the transparency. So we came up with the idea of creating a rating that has basically red, orange, yellow, green, and we would rate each of the integrations of AI features in NextCloud with this rating. So first, is it open source? Is the model available, and is the training data available? And so if a model has all three, it's green, if it has two of them, it's orange, if it has one of them, it's yellow, if it has none of them, it's red. So chat GPT integration, red. Completely on-prem model that is trained and has the training data available, for example, for speech to text, that can be green, and you have everything in between, of course. And the second thing is choice. So for us, it's really important that you, well, can choose, right? I mean, there are, again, legitimate users for something like chat GPT, and I mean, they're throwing so many billions at this problem that you can hardly argue that open source can really keep up to the latest stuff they're doing, and sometimes you just need it, fine? So in our user interface, we have these choices that you can have, like Opus, that's a translation exactly, so this would be a fully green one, and that's, well, we all know, chat GPT. So we try to make sure that for the various features that you can choose between these different models, on-prem, et cetera. So for us, of course, most of the work we put in on-prem and open source locally running AI features, because, well, that fits with our values as a company and, well, with our ethical AI rating, but the others are available. So at the moment, I made a list, but I'm sure there are many more you can use, like models like these in NextLoud. I have four of the various features. I'll show, well, actually, I'm showing examples right now. So this is just a bunch of the features we have. There is more, but suspicious login detection is something we developed like a really, really long time ago. It's basically a neural network that gets trained on your login data. It just runs completely local every time you log in. If you work nine to five from the Berlin office, let's say, and suddenly somebody at 3 a.m. logs in in your account from China, maybe there's something wrong, the model will detect that and give you a warning. Very simple, and we have had this for, I don't know, since 2020, so quite a while. And it's green, right? It runs fully local. There's nothing special about it, no data sent anywhere. We basically do a very similar thing with our mail app, where we basically train a neural network on subjects, sender, and email recipients, et cetera. And it creates a smart inbox trying to put important emails on top and the rest not, and again, no data sent anywhere, because it just runs on premise. I already mentioned phase recognition and stuff we did in 2022, I think, so this is all. But the problem with this is already, you need to download a multi-gigabyte file, which has, like, all the values needed for the neural network to recognize stuff. So we already had to re-architect a lot of the way Nexard works just to be able to download this big blob without creating all kinds of complexities for the users. And obviously, this problem gets bigger and bigger when you get to modern AIs. We even have music genre recognition using machine learning. It's yellow because it's trained on all the music on Spotify, which means the training data is actually copyrighted and therefore not open. And, yeah, we had, like, a pre-trained model to do call transcripts. We introduced that last year. That is nice. You have a call and the recording then gets text, speeds to text so that you get the text of the recording. Again, this model runs fully local, so that's cool. And speeds to text the other way around. Background blur, just a JavaScript thing that we upload in the browser, very simple translations. First, we made it with Deepol, which is not cool. So then we made one using the Opus corpus. You saw it earlier, and that is running fully local, so that's much better. So these are still mostly basic features, I think, today, and yet already pretty complicated. You need to keep an eye out where the data is being sent to, like with translation. But, of course, the big thing are LLMs, like the text operations. What we've been doing is to create, basically, NexLad Assistant. It uses the large language models, but open source on-prem ones that you can host yourself. It's like this little thing on the top. When you click on it, you get a dialogue. You can give a free prompt, or you can give a text to summarize and some other things. And it just runs this through one of the models that is supported by NexLad. And, again, you can put JetGPT here as model, or as back-end, but you can also then run your own LLM and connect that to NexLad, and then it can do all this stuff on-premise. So it's fairly simple. When it's running, then it'll get the results, and after a while you get the output of it. You can copy it into a document, et cetera. And, again, if you take a local model that is trained on public data, then it can be a fully green solution. So that's really cool. In places like NexLad Text, I already showed that, that you can select some text and then run this. Mail, I already showed this as well. In talk, like our video calling and JetSolution, you can translate a message, select it, and then choose translate, insert images and other stuff. We even made a little bot. This isn't the smartest bot. It's a very small model, but hey, it's fast. And you can ask it questions. Honestly, I wouldn't say such smart things. It's fairly shitty, I've noticed. But still, it works on your own server. That's kind of nice. So a lot is possible. One of the newer things we're working on is more of these services, because they're now companies like Amazon. They are running LLMs as a service. And other companies are doing this also purely in Europe, like you have ALEF, ALFA, and I think MIRROR or something. In France, there's also a company that is building local AI. So we're trying to support these, that you... You know, everybody can run these AIs, like you need a lot of heavy GPUs. It's a lot of compute. So you can use it as a service that at least it stays in Europe or at a company that you trust. Then I wouldn't recommend Amazon, per se, perhaps. For this, we also made it possible that you can put in some limits, otherwise users get a little creative and start to basically cost you a lot of money. And we worked on the interaction with this. I'll skip through this. A thing we're working on now is also to make all of this even smarter. A newer thing is the ability to take your documents that you have into account. So ContextChat is a feature of the Assistant that basically it has access to your documents, your emails, everything you have that gets indexed. Let me see. It's indexed into a vector database. So this runs as a separate service next to NextLoud. And then when you ask a question from the Assistant, it can actually answer using your documents, your company documentation, your emails, etc. So you can really do stuff like, you know, can you give me an idea of how we organize events, rather than in general, it can look at your documentation and then tell you, like, oh, you know, at your company, organize events this way. Or you can say, hey, can you give me a summary of the different requests that a colleague has emailed to me last week, and hopefully it'll give you all the to-dos that you got from that colleague in the last week. So this, yeah, has the context basically of what you are doing as a user at hand. It's, I think, really kind of, yeah, an important step forward to make this useful, because otherwise you're just getting the generic info that's in the LLM. As I said, they hallucinate stuff all the time. They're much better at, like, taking information and summarizing it, and that's, of course, what this does. I think it's much more reliable in that way, you know, vacation process, et cetera, et cetera. So that's a couple of things we've been doing lately on this, as well as in the context yet. So that's our approach to AI. I would really like to hear thoughts on that, and, like, I don't know what other projects are planning with this. One of these will be giving a talk after mine. But I know any feedback, questions, thoughts, fears, and anxieties? Okay, so is this working? Can anybody confirm in the back? Great, thank you very much. So any thoughts, questions? Let's start here. So you said your screenshot showed that... Yes, the screenshot showed that I need to double check the information that the assistant gave me. Notice that it doesn't give me the reference to the emails that it was quoting from. Is there a possibility to get that? Currently not, but thinking of the way this works, I mean, I have one developer here. They can interject, but I think that should actually be quite doable, because the way it works is it looks in this vector database and gives that information to the LLM to then summarize and give you the answer. And, well, in the vector database, I guess it knows where it came from, and therefore can then say what information was used to summarize that answer. So I would think this is possible, but I don't... I don't know. Yeah, I see a thumbs up. Excellent, okay. Any other questions, ideas? Okay. Or are we going to do this? Yes, for me, I am an e-aseptic. There's something... some examples. So it's good, something... when user is at the end, and he can correct what is said by the AI. So an example for translation, I have the word in Dutch, Acad\u00e9mie de Sie. It's not universitaire, the translation in French. It's personne issue des milieux acad\u00e9miques. And that's any... the translator or a consultant don't give this response. Yeah, but it's... So you have a control, they say, from the user, also from the citizen in general, so when the user has no power of the system. You can't check a human translator either, though, unless you know the language at that point you didn't need them in the first place. So, yeah, you have to use this stuff in a skeptical way, but then... yeah. Yes, and the other thing is about consuming energy. So it was an emission in the RTBF about consuming of energy, of shaggivity. It was hard. Yeah, so the amount of energy that these models use is big. That's, by the way, one of the reasons I think they should be open source, is that the researchers who do stuff companies aren't interested in can try to optimize them and make them run with less energy. Yeah. Hi. I think another good use case for all these... If we combine these features, that would mean that we could have a super accessible environment, because if someone is blind or nearly blind, people could use all this text to speech if someone has autism, ADHD, whatever. You could try to find a shorter version, an easier, understandable version of a text or whatever, and combining this would help, I think. That is awesome. I'm going to add that to my slides right now, but I am completely making the laptop slow now. That's a really good point. Accessibility is a really important benefit. Actually, hint to the developer, bring it up in the team. Maybe we can already work on that. Yeah, any more? Yeah, just a question on the REC approach that you were describing before. Do you have any figure that you can share to which extent you tried the retrieval of the vector? Sorry, I did not hear the question. So when you were describing the rag, the retrieval of the augmented... The green, the colors, yes. No, the rag. When you're retrieving the vectors from the vector DB. Right. So can you give us some figures on to which extent you tried that? Talk to somebody, not me, in one of these, who knows the technical part there, and I'm not even sure we have somebody right here at the moment with that. Sorry. Okay. And we're out of time. I'm afraid. So this is probably it then. Somebody wants their microphone back. Alright, thank you all. Thank you very much, Josh.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.0, "text": " Thank you all for joining us in a great, you don't.", "tokens": [50364, 1044, 291, 439, 337, 5549, 505, 294, 257, 869, 11, 291, 500, 380, 13, 50814], "temperature": 0.0, "avg_logprob": -0.36474327674278845, "compression_ratio": 1.4661654135338347, "no_speech_prob": 0.6001693606376648}, {"id": 1, "seek": 0, "start": 9.0, "end": 14.0, "text": " It's not working? Why is it not working?", "tokens": [50814, 467, 311, 406, 1364, 30, 1545, 307, 309, 406, 1364, 30, 51064], "temperature": 0.0, "avg_logprob": -0.36474327674278845, "compression_ratio": 1.4661654135338347, "no_speech_prob": 0.6001693606376648}, {"id": 2, "seek": 0, "start": 14.0, "end": 18.0, "text": " It is? Okay, good.", "tokens": [51064, 467, 307, 30, 1033, 11, 665, 13, 51264], "temperature": 0.0, "avg_logprob": -0.36474327674278845, "compression_ratio": 1.4661654135338347, "no_speech_prob": 0.6001693606376648}, {"id": 3, "seek": 0, "start": 18.0, "end": 24.0, "text": " Let's not correct. So thank you all for joining us and here to talk about next.", "tokens": [51264, 961, 311, 406, 3006, 13, 407, 1309, 291, 439, 337, 5549, 505, 293, 510, 281, 751, 466, 958, 13, 51564], "temperature": 0.0, "avg_logprob": -0.36474327674278845, "compression_ratio": 1.4661654135338347, "no_speech_prob": 0.6001693606376648}, {"id": 4, "seek": 0, "start": 24.0, "end": 26.0, "text": " AI.", "tokens": [51564, 7318, 13, 51664], "temperature": 0.0, "avg_logprob": -0.36474327674278845, "compression_ratio": 1.4661654135338347, "no_speech_prob": 0.6001693606376648}, {"id": 5, "seek": 2600, "start": 26.0, "end": 31.0, "text": " Exactly. Another approach to AI is just port fleet.", "tokens": [50364, 7587, 13, 3996, 3109, 281, 7318, 307, 445, 2436, 19396, 13, 50614], "temperature": 0.0, "avg_logprob": -0.21742531404656879, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.4610617756843567}, {"id": 6, "seek": 2600, "start": 31.0, "end": 35.0, "text": " Okay. Does this work well enough? You can hear it in the back and all that.", "tokens": [50614, 1033, 13, 4402, 341, 589, 731, 1547, 30, 509, 393, 1568, 309, 294, 264, 646, 293, 439, 300, 13, 50814], "temperature": 0.0, "avg_logprob": -0.21742531404656879, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.4610617756843567}, {"id": 7, "seek": 2600, "start": 35.0, "end": 38.0, "text": " Okay. I see thumbs up. That's wonderful.", "tokens": [50814, 1033, 13, 286, 536, 8838, 493, 13, 663, 311, 3715, 13, 50964], "temperature": 0.0, "avg_logprob": -0.21742531404656879, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.4610617756843567}, {"id": 8, "seek": 2600, "start": 38.0, "end": 40.0, "text": " Yeah. Well, I had this. I'm your sport fleet.", "tokens": [50964, 865, 13, 1042, 11, 286, 632, 341, 13, 286, 478, 428, 7282, 19396, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21742531404656879, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.4610617756843567}, {"id": 9, "seek": 2600, "start": 40.0, "end": 44.0, "text": " Direct communications co-founder at NextLoud.", "tokens": [51064, 18308, 15163, 598, 12, 33348, 412, 3087, 43, 1861, 13, 51264], "temperature": 0.0, "avg_logprob": -0.21742531404656879, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.4610617756843567}, {"id": 10, "seek": 2600, "start": 44.0, "end": 47.0, "text": " So, yeah, the thing we do at NextLoud is collaboration.", "tokens": [51264, 407, 11, 1338, 11, 264, 551, 321, 360, 412, 3087, 43, 1861, 307, 9363, 13, 51414], "temperature": 0.0, "avg_logprob": -0.21742531404656879, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.4610617756843567}, {"id": 11, "seek": 2600, "start": 47.0, "end": 50.0, "text": " That's of course what this room is all about.", "tokens": [51414, 663, 311, 295, 1164, 437, 341, 1808, 307, 439, 466, 13, 51564], "temperature": 0.0, "avg_logprob": -0.21742531404656879, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.4610617756843567}, {"id": 12, "seek": 2600, "start": 50.0, "end": 53.0, "text": " Now, there's this AI thing coming.", "tokens": [51564, 823, 11, 456, 311, 341, 7318, 551, 1348, 13, 51714], "temperature": 0.0, "avg_logprob": -0.21742531404656879, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.4610617756843567}, {"id": 13, "seek": 5300, "start": 53.0, "end": 59.0, "text": " And so I'm hoping to try and make this conversation a little bit interactive.", "tokens": [50364, 400, 370, 286, 478, 7159, 281, 853, 293, 652, 341, 3761, 257, 707, 857, 15141, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09034197554629073, "compression_ratio": 1.7431906614785992, "no_speech_prob": 0.10101097077131271}, {"id": 14, "seek": 5300, "start": 59.0, "end": 64.0, "text": " I mean, there are other people here from Xweek and other projects who are working on,", "tokens": [50664, 286, 914, 11, 456, 366, 661, 561, 510, 490, 1783, 23188, 293, 661, 4455, 567, 366, 1364, 322, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09034197554629073, "compression_ratio": 1.7431906614785992, "no_speech_prob": 0.10101097077131271}, {"id": 15, "seek": 5300, "start": 64.0, "end": 68.0, "text": " well, collaboration tools as well, open source collaboration tools.", "tokens": [50914, 731, 11, 9363, 3873, 382, 731, 11, 1269, 4009, 9363, 3873, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09034197554629073, "compression_ratio": 1.7431906614785992, "no_speech_prob": 0.10101097077131271}, {"id": 16, "seek": 5300, "start": 68.0, "end": 74.0, "text": " And, you know, this AI thing, I mean, there have been AI-ish tools being used for a long time,", "tokens": [51114, 400, 11, 291, 458, 11, 341, 7318, 551, 11, 286, 914, 11, 456, 362, 668, 7318, 12, 742, 3873, 885, 1143, 337, 257, 938, 565, 11, 51414], "temperature": 0.0, "avg_logprob": -0.09034197554629073, "compression_ratio": 1.7431906614785992, "no_speech_prob": 0.10101097077131271}, {"id": 17, "seek": 5300, "start": 74.0, "end": 76.0, "text": " but a lot of them are also still quite new.", "tokens": [51414, 457, 257, 688, 295, 552, 366, 611, 920, 1596, 777, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09034197554629073, "compression_ratio": 1.7431906614785992, "no_speech_prob": 0.10101097077131271}, {"id": 18, "seek": 5300, "start": 76.0, "end": 80.0, "text": " So I'm kind of hoping that we can also have a bit of a conversation about it.", "tokens": [51514, 407, 286, 478, 733, 295, 7159, 300, 321, 393, 611, 362, 257, 857, 295, 257, 3761, 466, 309, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09034197554629073, "compression_ratio": 1.7431906614785992, "no_speech_prob": 0.10101097077131271}, {"id": 19, "seek": 8000, "start": 81.0, "end": 85.0, "text": " Because, well, there are pros and cons. I mean, we'll get to all that stuff.", "tokens": [50414, 1436, 11, 731, 11, 456, 366, 6267, 293, 1014, 13, 286, 914, 11, 321, 603, 483, 281, 439, 300, 1507, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09606949058738914, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.022641649469733238}, {"id": 20, "seek": 8000, "start": 85.0, "end": 91.0, "text": " Of course, the big thing here is, yeah, that we have, like, these big companies, right?", "tokens": [50614, 2720, 1164, 11, 264, 955, 551, 510, 307, 11, 1338, 11, 300, 321, 362, 11, 411, 11, 613, 955, 3431, 11, 558, 30, 50914], "temperature": 0.0, "avg_logprob": -0.09606949058738914, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.022641649469733238}, {"id": 21, "seek": 8000, "start": 91.0, "end": 97.0, "text": " They all want our data, and AI is for them another thing to use that data for.", "tokens": [50914, 814, 439, 528, 527, 1412, 11, 293, 7318, 307, 337, 552, 1071, 551, 281, 764, 300, 1412, 337, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09606949058738914, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.022641649469733238}, {"id": 22, "seek": 8000, "start": 97.0, "end": 104.0, "text": " So, yeah, I mean, AI, I don't know how deep I want to go into what it is,", "tokens": [51214, 407, 11, 1338, 11, 286, 914, 11, 7318, 11, 286, 500, 380, 458, 577, 2452, 286, 528, 281, 352, 666, 437, 309, 307, 11, 51564], "temperature": 0.0, "avg_logprob": -0.09606949058738914, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.022641649469733238}, {"id": 23, "seek": 8000, "start": 104.0, "end": 107.0, "text": " because I think all of us know it a little bit.", "tokens": [51564, 570, 286, 519, 439, 295, 505, 458, 309, 257, 707, 857, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09606949058738914, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.022641649469733238}, {"id": 24, "seek": 10700, "start": 107.0, "end": 110.0, "text": " But we don't want to live in a world where there are five companies, you know,", "tokens": [50364, 583, 321, 500, 380, 528, 281, 1621, 294, 257, 1002, 689, 456, 366, 1732, 3431, 11, 291, 458, 11, 50514], "temperature": 0.0, "avg_logprob": -0.09659222089327299, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0627262219786644}, {"id": 25, "seek": 10700, "start": 110.0, "end": 114.0, "text": " who run all our data, and that's kind of a little bit the case right now.", "tokens": [50514, 567, 1190, 439, 527, 1412, 11, 293, 300, 311, 733, 295, 257, 707, 857, 264, 1389, 558, 586, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09659222089327299, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0627262219786644}, {"id": 26, "seek": 10700, "start": 114.0, "end": 120.0, "text": " I think that if Trump and his next presidency tells Microsoft to shut down their service in Europe,", "tokens": [50714, 286, 519, 300, 498, 3899, 293, 702, 958, 26702, 5112, 8116, 281, 5309, 760, 641, 2643, 294, 3315, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09659222089327299, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0627262219786644}, {"id": 27, "seek": 10700, "start": 120.0, "end": 127.0, "text": " then basically you cannot get a new passport, you cannot, yeah, nobody can work at government here, for example, right?", "tokens": [51014, 550, 1936, 291, 2644, 483, 257, 777, 24694, 11, 291, 2644, 11, 1338, 11, 5079, 393, 589, 412, 2463, 510, 11, 337, 1365, 11, 558, 30, 51364], "temperature": 0.0, "avg_logprob": -0.09659222089327299, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0627262219786644}, {"id": 28, "seek": 10700, "start": 127.0, "end": 129.0, "text": " This is, I think, a bit of an issue.", "tokens": [51364, 639, 307, 11, 286, 519, 11, 257, 857, 295, 364, 2734, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09659222089327299, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0627262219786644}, {"id": 29, "seek": 10700, "start": 129.0, "end": 134.0, "text": " And, I mean, NextLoud is one of the projects that's working on solving that issue,", "tokens": [51464, 400, 11, 286, 914, 11, 3087, 43, 1861, 307, 472, 295, 264, 4455, 300, 311, 1364, 322, 12606, 300, 2734, 11, 51714], "temperature": 0.0, "avg_logprob": -0.09659222089327299, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0627262219786644}, {"id": 30, "seek": 13400, "start": 134.0, "end": 140.0, "text": " essentially trying to give, well, companies, individuals, but also hopefully government,", "tokens": [50364, 4476, 1382, 281, 976, 11, 731, 11, 3431, 11, 5346, 11, 457, 611, 4696, 2463, 11, 50664], "temperature": 0.0, "avg_logprob": -0.15474237442016603, "compression_ratio": 1.4650205761316872, "no_speech_prob": 0.04052472114562988}, {"id": 31, "seek": 13400, "start": 140.0, "end": 142.0, "text": " back the control over their data.", "tokens": [50664, 646, 264, 1969, 670, 641, 1412, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15474237442016603, "compression_ratio": 1.4650205761316872, "no_speech_prob": 0.04052472114562988}, {"id": 32, "seek": 13400, "start": 142.0, "end": 144.0, "text": " We've built a collaboration platform.", "tokens": [50764, 492, 600, 3094, 257, 9363, 3663, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15474237442016603, "compression_ratio": 1.4650205761316872, "no_speech_prob": 0.04052472114562988}, {"id": 33, "seek": 13400, "start": 144.0, "end": 149.0, "text": " I'm guessing how many of you are not familiar with NextLoud?", "tokens": [50864, 286, 478, 17939, 577, 867, 295, 291, 366, 406, 4963, 365, 3087, 43, 1861, 30, 51114], "temperature": 0.0, "avg_logprob": -0.15474237442016603, "compression_ratio": 1.4650205761316872, "no_speech_prob": 0.04052472114562988}, {"id": 34, "seek": 13400, "start": 149.0, "end": 153.0, "text": " Yeah, okay, it's like six people.", "tokens": [51114, 865, 11, 1392, 11, 309, 311, 411, 2309, 561, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15474237442016603, "compression_ratio": 1.4650205761316872, "no_speech_prob": 0.04052472114562988}, {"id": 35, "seek": 13400, "start": 153.0, "end": 155.0, "text": " Google it.", "tokens": [51314, 3329, 309, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15474237442016603, "compression_ratio": 1.4650205761316872, "no_speech_prob": 0.04052472114562988}, {"id": 36, "seek": 13400, "start": 155.0, "end": 160.0, "text": " I will then not go into that, sorry, or duck-duck-go it, that would be better, obviously.", "tokens": [51414, 286, 486, 550, 406, 352, 666, 300, 11, 2597, 11, 420, 12482, 12, 67, 1134, 12, 1571, 309, 11, 300, 576, 312, 1101, 11, 2745, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15474237442016603, "compression_ratio": 1.4650205761316872, "no_speech_prob": 0.04052472114562988}, {"id": 37, "seek": 16000, "start": 160.0, "end": 166.0, "text": " So, as a company, we build an alternative for 365 in very quick, simple terms.", "tokens": [50364, 407, 11, 382, 257, 2237, 11, 321, 1322, 364, 8535, 337, 22046, 294, 588, 1702, 11, 2199, 2115, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09666345669673039, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.16642315685749054}, {"id": 38, "seek": 16000, "start": 166.0, "end": 174.0, "text": " And with alternative, we mean that as a government or a company, we think it's important that you have a choice.", "tokens": [50664, 400, 365, 8535, 11, 321, 914, 300, 382, 257, 2463, 420, 257, 2237, 11, 321, 519, 309, 311, 1021, 300, 291, 362, 257, 3922, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09666345669673039, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.16642315685749054}, {"id": 39, "seek": 16000, "start": 174.0, "end": 182.0, "text": " It's totally fine if you're happy that your data is at an American company and that U.S. buy agencies have access to it.", "tokens": [51064, 467, 311, 3879, 2489, 498, 291, 434, 2055, 300, 428, 1412, 307, 412, 364, 2665, 2237, 293, 300, 624, 13, 50, 13, 2256, 9504, 362, 2105, 281, 309, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09666345669673039, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.16642315685749054}, {"id": 40, "seek": 16000, "start": 182.0, "end": 187.0, "text": " If you're good with that, if that's not a threat to your business, that's fine.", "tokens": [51464, 759, 291, 434, 665, 365, 300, 11, 498, 300, 311, 406, 257, 4734, 281, 428, 1606, 11, 300, 311, 2489, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09666345669673039, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.16642315685749054}, {"id": 41, "seek": 18700, "start": 187.0, "end": 192.0, "text": " Government, I think it's by definition a threat, but that's their choice.", "tokens": [50364, 7321, 11, 286, 519, 309, 311, 538, 7123, 257, 4734, 11, 457, 300, 311, 641, 3922, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17268719392664292, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.37979656457901}, {"id": 42, "seek": 18700, "start": 192.0, "end": 195.0, "text": " But we think there should be, like, a choice.", "tokens": [50614, 583, 321, 519, 456, 820, 312, 11, 411, 11, 257, 3922, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17268719392664292, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.37979656457901}, {"id": 43, "seek": 18700, "start": 195.0, "end": 197.0, "text": " There should be an alternative.", "tokens": [50764, 821, 820, 312, 364, 8535, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17268719392664292, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.37979656457901}, {"id": 44, "seek": 18700, "start": 197.0, "end": 203.0, "text": " And an alternative is only an alternative, is it is, if it does what the other product does, obviously,", "tokens": [50864, 400, 364, 8535, 307, 787, 364, 8535, 11, 307, 309, 307, 11, 498, 309, 775, 437, 264, 661, 1674, 775, 11, 2745, 11, 51164], "temperature": 0.0, "avg_logprob": -0.17268719392664292, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.37979656457901}, {"id": 45, "seek": 18700, "start": 203.0, "end": 208.0, "text": " and in a safe way and has enough ability to be used for a serious company.", "tokens": [51164, 293, 294, 257, 3273, 636, 293, 575, 1547, 3485, 281, 312, 1143, 337, 257, 3156, 2237, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17268719392664292, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.37979656457901}, {"id": 46, "seek": 18700, "start": 208.0, "end": 211.0, "text": " So, that's what we're building and we have built.", "tokens": [51414, 407, 11, 300, 311, 437, 321, 434, 2390, 293, 321, 362, 3094, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17268719392664292, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.37979656457901}, {"id": 47, "seek": 21100, "start": 211.0, "end": 217.0, "text": " That's why German and French and most European governments are in places already using Nexlout,", "tokens": [50364, 663, 311, 983, 6521, 293, 5522, 293, 881, 6473, 11280, 366, 294, 3190, 1217, 1228, 1734, 87, 75, 346, 11, 50664], "temperature": 0.0, "avg_logprob": -0.12251061732226079, "compression_ratio": 1.5043478260869565, "no_speech_prob": 0.18127067387104034}, {"id": 48, "seek": 21100, "start": 217.0, "end": 222.0, "text": " be it cities, be it at a state level or federal level.", "tokens": [50664, 312, 309, 6486, 11, 312, 309, 412, 257, 1785, 1496, 420, 6019, 1496, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12251061732226079, "compression_ratio": 1.5043478260869565, "no_speech_prob": 0.18127067387104034}, {"id": 49, "seek": 21100, "start": 222.0, "end": 226.0, "text": " So, as a company, we care a lot. Nexlout for us is a mission, it's a goal.", "tokens": [50914, 407, 11, 382, 257, 2237, 11, 321, 1127, 257, 688, 13, 1734, 87, 75, 346, 337, 505, 307, 257, 4447, 11, 309, 311, 257, 3387, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12251061732226079, "compression_ratio": 1.5043478260869565, "no_speech_prob": 0.18127067387104034}, {"id": 50, "seek": 21100, "start": 226.0, "end": 231.0, "text": " It's like our way to try and make the world a tiny little bit better.", "tokens": [51114, 467, 311, 411, 527, 636, 281, 853, 293, 652, 264, 1002, 257, 5870, 707, 857, 1101, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12251061732226079, "compression_ratio": 1.5043478260869565, "no_speech_prob": 0.18127067387104034}, {"id": 51, "seek": 21100, "start": 231.0, "end": 236.0, "text": " And we want to work in an open, collaborative way.", "tokens": [51364, 400, 321, 528, 281, 589, 294, 364, 1269, 11, 16555, 636, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12251061732226079, "compression_ratio": 1.5043478260869565, "no_speech_prob": 0.18127067387104034}, {"id": 52, "seek": 23600, "start": 236.0, "end": 243.0, "text": " Therefore, we're very happy, of course, that it's used by thousands of governments and universities, et cetera, et cetera.", "tokens": [50364, 7504, 11, 321, 434, 588, 2055, 11, 295, 1164, 11, 300, 309, 311, 1143, 538, 5383, 295, 11280, 293, 11779, 11, 1030, 11458, 11, 1030, 11458, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09961964402879987, "compression_ratio": 1.6925925925925926, "no_speech_prob": 0.20919983088970184}, {"id": 53, "seek": 23600, "start": 243.0, "end": 245.0, "text": " And, of course, we're building this completely in the open.", "tokens": [50714, 400, 11, 295, 1164, 11, 321, 434, 2390, 341, 2584, 294, 264, 1269, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09961964402879987, "compression_ratio": 1.6925925925925926, "no_speech_prob": 0.20919983088970184}, {"id": 54, "seek": 23600, "start": 245.0, "end": 251.0, "text": " And again, that will be relevant because, of course, I think the future for AI will better be open,", "tokens": [50814, 400, 797, 11, 300, 486, 312, 7340, 570, 11, 295, 1164, 11, 286, 519, 264, 2027, 337, 7318, 486, 1101, 312, 1269, 11, 51114], "temperature": 0.0, "avg_logprob": -0.09961964402879987, "compression_ratio": 1.6925925925925926, "no_speech_prob": 0.20919983088970184}, {"id": 55, "seek": 23600, "start": 251.0, "end": 256.0, "text": " otherwise we are, well, just as crude as it is with collaboration platforms, honestly.", "tokens": [51114, 5911, 321, 366, 11, 731, 11, 445, 382, 30796, 382, 309, 307, 365, 9363, 9473, 11, 6095, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09961964402879987, "compression_ratio": 1.6925925925925926, "no_speech_prob": 0.20919983088970184}, {"id": 56, "seek": 23600, "start": 256.0, "end": 260.0, "text": " And we have a wonderful community working with us and all this stuff, which is awesome.", "tokens": [51364, 400, 321, 362, 257, 3715, 1768, 1364, 365, 505, 293, 439, 341, 1507, 11, 597, 307, 3476, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09961964402879987, "compression_ratio": 1.6925925925925926, "no_speech_prob": 0.20919983088970184}, {"id": 57, "seek": 26000, "start": 260.0, "end": 267.0, "text": " Also, as a company, we try to be open and transparent, not depending on venture capital, et cetera, but be self-owned.", "tokens": [50364, 2743, 11, 382, 257, 2237, 11, 321, 853, 281, 312, 1269, 293, 12737, 11, 406, 5413, 322, 18474, 4238, 11, 1030, 11458, 11, 457, 312, 2698, 12, 14683, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1002458322828061, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.06558137387037277}, {"id": 58, "seek": 26000, "start": 267.0, "end": 269.0, "text": " Anyhow, AI.", "tokens": [50714, 2639, 4286, 11, 7318, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1002458322828061, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.06558137387037277}, {"id": 59, "seek": 26000, "start": 269.0, "end": 276.0, "text": " So, AI is like, we've already introduced a ton of AI things over the years, like little things,", "tokens": [50814, 407, 11, 7318, 307, 411, 11, 321, 600, 1217, 7268, 257, 2952, 295, 7318, 721, 670, 264, 924, 11, 411, 707, 721, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1002458322828061, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.06558137387037277}, {"id": 60, "seek": 26000, "start": 276.0, "end": 282.0, "text": " and I will show some of them, but of course, with the latest LLMs and stuff, it's getting really complicated.", "tokens": [51164, 293, 286, 486, 855, 512, 295, 552, 11, 457, 295, 1164, 11, 365, 264, 6792, 441, 43, 26386, 293, 1507, 11, 309, 311, 1242, 534, 6179, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1002458322828061, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.06558137387037277}, {"id": 61, "seek": 26000, "start": 282.0, "end": 284.0, "text": " I mean, there are tons of problems with it.", "tokens": [51464, 286, 914, 11, 456, 366, 9131, 295, 2740, 365, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1002458322828061, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.06558137387037277}, {"id": 62, "seek": 28400, "start": 284.0, "end": 286.0, "text": " I have a lot of potential, right?", "tokens": [50364, 286, 362, 257, 688, 295, 3995, 11, 558, 30, 50464], "temperature": 0.0, "avg_logprob": -0.0833462326614945, "compression_ratio": 1.6710097719869708, "no_speech_prob": 0.04305720701813698}, {"id": 63, "seek": 28400, "start": 286.0, "end": 294.0, "text": " AI can help us make repetitive tasks easier, quicker, et cetera, but at the same time, Big Tech is basically loving it.", "tokens": [50464, 7318, 393, 854, 505, 652, 29404, 9608, 3571, 11, 16255, 11, 1030, 11458, 11, 457, 412, 264, 912, 565, 11, 5429, 13795, 307, 1936, 9344, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0833462326614945, "compression_ratio": 1.6710097719869708, "no_speech_prob": 0.04305720701813698}, {"id": 64, "seek": 28400, "start": 294.0, "end": 297.0, "text": " They have all the data to be able to build the AI's.", "tokens": [50864, 814, 362, 439, 264, 1412, 281, 312, 1075, 281, 1322, 264, 7318, 311, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0833462326614945, "compression_ratio": 1.6710097719869708, "no_speech_prob": 0.04305720701813698}, {"id": 65, "seek": 28400, "start": 297.0, "end": 304.0, "text": " It costs tens or hundreds of millions right now to really train the proper LLMs, so they really have a bit of a monopoly here.", "tokens": [51014, 467, 5497, 10688, 420, 6779, 295, 6803, 558, 586, 281, 534, 3847, 264, 2296, 441, 43, 26386, 11, 370, 436, 534, 362, 257, 857, 295, 257, 37061, 510, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0833462326614945, "compression_ratio": 1.6710097719869708, "no_speech_prob": 0.04305720701813698}, {"id": 66, "seek": 28400, "start": 304.0, "end": 309.0, "text": " And, yeah, the rest of us will have to just accept that they're using all our data to do it.", "tokens": [51364, 400, 11, 1338, 11, 264, 1472, 295, 505, 486, 362, 281, 445, 3241, 300, 436, 434, 1228, 439, 527, 1412, 281, 360, 309, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0833462326614945, "compression_ratio": 1.6710097719869708, "no_speech_prob": 0.04305720701813698}, {"id": 67, "seek": 28400, "start": 309.0, "end": 313.0, "text": " And a lot of companies are already really realizing this is a problem for them, right?", "tokens": [51614, 400, 257, 688, 295, 3431, 366, 1217, 534, 16734, 341, 307, 257, 1154, 337, 552, 11, 558, 30, 51814], "temperature": 0.0, "avg_logprob": -0.0833462326614945, "compression_ratio": 1.6710097719869708, "no_speech_prob": 0.04305720701813698}, {"id": 68, "seek": 31300, "start": 313.0, "end": 315.0, "text": " It's Citigroup and Goldman Sachs.", "tokens": [50364, 467, 311, 18435, 328, 81, 1250, 293, 45378, 25626, 82, 13, 50464], "temperature": 0.0, "avg_logprob": -0.10709067110745411, "compression_ratio": 1.5961538461538463, "no_speech_prob": 0.14612224698066711}, {"id": 69, "seek": 31300, "start": 315.0, "end": 319.0, "text": " They are actually not allowing their employees to use tools like chatGPT.", "tokens": [50464, 814, 366, 767, 406, 8293, 641, 6619, 281, 764, 3873, 411, 5081, 38, 47, 51, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10709067110745411, "compression_ratio": 1.5961538461538463, "no_speech_prob": 0.14612224698066711}, {"id": 70, "seek": 31300, "start": 319.0, "end": 329.0, "text": " I mean, if you're BMW and you're working on a new car and you're using an AI to generate some ideas or summarize some proposals,", "tokens": [50664, 286, 914, 11, 498, 291, 434, 21355, 293, 291, 434, 1364, 322, 257, 777, 1032, 293, 291, 434, 1228, 364, 7318, 281, 8460, 512, 3487, 420, 20858, 512, 20198, 11, 51164], "temperature": 0.0, "avg_logprob": -0.10709067110745411, "compression_ratio": 1.5961538461538463, "no_speech_prob": 0.14612224698066711}, {"id": 71, "seek": 31300, "start": 329.0, "end": 333.0, "text": " and you discover later that six months later Tesla, while designing their car,", "tokens": [51164, 293, 291, 4411, 1780, 300, 2309, 2493, 1780, 13666, 11, 1339, 14685, 641, 1032, 11, 51364], "temperature": 0.0, "avg_logprob": -0.10709067110745411, "compression_ratio": 1.5961538461538463, "no_speech_prob": 0.14612224698066711}, {"id": 72, "seek": 31300, "start": 333.0, "end": 340.0, "text": " suddenly got some of your ideas coming into their AI planning, then there's a bit of an issue here.", "tokens": [51364, 5800, 658, 512, 295, 428, 3487, 1348, 666, 641, 7318, 5038, 11, 550, 456, 311, 257, 857, 295, 364, 2734, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10709067110745411, "compression_ratio": 1.5961538461538463, "no_speech_prob": 0.14612224698066711}, {"id": 73, "seek": 34000, "start": 340.0, "end": 342.0, "text": " And, of course, this kind of stuff is happening.", "tokens": [50364, 400, 11, 295, 1164, 11, 341, 733, 295, 1507, 307, 2737, 13, 50464], "temperature": 0.0, "avg_logprob": -0.06966548545338283, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.07461690157651901}, {"id": 74, "seek": 34000, "start": 342.0, "end": 348.0, "text": " The company, like a while ago, Twitter and Zoom, they changed their terms of service to allow for training on user data.", "tokens": [50464, 440, 2237, 11, 411, 257, 1339, 2057, 11, 5794, 293, 13453, 11, 436, 3105, 641, 2115, 295, 2643, 281, 2089, 337, 3097, 322, 4195, 1412, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06966548545338283, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.07461690157651901}, {"id": 75, "seek": 34000, "start": 348.0, "end": 355.0, "text": " And, yeah, this is really an issue for business as well as, well, obviously, all our society.", "tokens": [50764, 400, 11, 1338, 11, 341, 307, 534, 364, 2734, 337, 1606, 382, 731, 382, 11, 731, 11, 2745, 11, 439, 527, 4086, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06966548545338283, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.07461690157651901}, {"id": 76, "seek": 34000, "start": 355.0, "end": 361.0, "text": " And then I'm not even talking about data biases in these models, carbon footprint.", "tokens": [51114, 400, 550, 286, 478, 406, 754, 1417, 466, 1412, 32152, 294, 613, 5245, 11, 5954, 24222, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06966548545338283, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.07461690157651901}, {"id": 77, "seek": 34000, "start": 361.0, "end": 365.0, "text": " I mean, I think most of you are aware of all the issues with AI.", "tokens": [51414, 286, 914, 11, 286, 519, 881, 295, 291, 366, 3650, 295, 439, 264, 2663, 365, 7318, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06966548545338283, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.07461690157651901}, {"id": 78, "seek": 36500, "start": 365.0, "end": 371.0, "text": " So, honestly, I don't think the question is to AI or not, because there are too many benefits.", "tokens": [50364, 407, 11, 6095, 11, 286, 500, 380, 519, 264, 1168, 307, 281, 7318, 420, 406, 11, 570, 456, 366, 886, 867, 5311, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09893831685811531, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.20033825933933258}, {"id": 79, "seek": 36500, "start": 371.0, "end": 374.0, "text": " The opportunities are really big, I think.", "tokens": [50664, 440, 4786, 366, 534, 955, 11, 286, 519, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09893831685811531, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.20033825933933258}, {"id": 80, "seek": 36500, "start": 374.0, "end": 381.0, "text": " I've been trying to make a bit of a list of that, but I was just changing it while standing here in line outside.", "tokens": [50814, 286, 600, 668, 1382, 281, 652, 257, 857, 295, 257, 1329, 295, 300, 11, 457, 286, 390, 445, 4473, 309, 1339, 4877, 510, 294, 1622, 2380, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09893831685811531, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.20033825933933258}, {"id": 81, "seek": 36500, "start": 381.0, "end": 383.0, "text": " So this is definitely not complete.", "tokens": [51164, 407, 341, 307, 2138, 406, 3566, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09893831685811531, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.20033825933933258}, {"id": 82, "seek": 36500, "start": 383.0, "end": 387.0, "text": " So I'm just going to put it all on the screen and ask what's missing.", "tokens": [51264, 407, 286, 478, 445, 516, 281, 829, 309, 439, 322, 264, 2568, 293, 1029, 437, 311, 5361, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09893831685811531, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.20033825933933258}, {"id": 83, "seek": 36500, "start": 387.0, "end": 391.0, "text": " I mean, I think there are some basics, you know, text to speech, speech to text,", "tokens": [51464, 286, 914, 11, 286, 519, 456, 366, 512, 14688, 11, 291, 458, 11, 2487, 281, 6218, 11, 6218, 281, 2487, 11, 51664], "temperature": 0.0, "avg_logprob": -0.09893831685811531, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.20033825933933258}, {"id": 84, "seek": 39100, "start": 391.0, "end": 397.0, "text": " recognizing faces on photos and recognizing objects on it, et cetera.", "tokens": [50364, 18538, 8475, 322, 5787, 293, 18538, 6565, 322, 309, 11, 1030, 11458, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15357796805245535, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.12825296819210052}, {"id": 85, "seek": 39100, "start": 397.0, "end": 403.0, "text": " This is already, like Nexot has been shipping this for three years, four years already, these kind of things.", "tokens": [50664, 639, 307, 1217, 11, 411, 1734, 87, 310, 575, 668, 14122, 341, 337, 1045, 924, 11, 1451, 924, 1217, 11, 613, 733, 295, 721, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15357796805245535, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.12825296819210052}, {"id": 86, "seek": 39100, "start": 403.0, "end": 408.0, "text": " It's just one model that you download and does this stuff, and translation and other one.", "tokens": [50964, 467, 311, 445, 472, 2316, 300, 291, 5484, 293, 775, 341, 1507, 11, 293, 12853, 293, 661, 472, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15357796805245535, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.12825296819210052}, {"id": 87, "seek": 39100, "start": 408.0, "end": 412.0, "text": " It's fairly, I think it's, I mean, it's not simple.", "tokens": [51214, 467, 311, 6457, 11, 286, 519, 309, 311, 11, 286, 914, 11, 309, 311, 406, 2199, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15357796805245535, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.12825296819210052}, {"id": 88, "seek": 39100, "start": 412.0, "end": 415.0, "text": " It's technically complicated stuff, but it works.", "tokens": [51414, 467, 311, 12120, 6179, 1507, 11, 457, 309, 1985, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15357796805245535, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.12825296819210052}, {"id": 89, "seek": 39100, "start": 415.0, "end": 416.0, "text": " And there are not huge risks.", "tokens": [51564, 400, 456, 366, 406, 2603, 10888, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15357796805245535, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.12825296819210052}, {"id": 90, "seek": 41600, "start": 416.0, "end": 420.0, "text": " You don't need to send your data to Google anymore if you want text to speech,", "tokens": [50364, 509, 500, 380, 643, 281, 2845, 428, 1412, 281, 3329, 3602, 498, 291, 528, 2487, 281, 6218, 11, 50564], "temperature": 0.0, "avg_logprob": -0.07703929321438659, "compression_ratio": 1.5984555984555984, "no_speech_prob": 0.2569199800491333}, {"id": 91, "seek": 41600, "start": 420.0, "end": 424.0, "text": " or if you want image recognition and being able to search for a dog", "tokens": [50564, 420, 498, 291, 528, 3256, 11150, 293, 885, 1075, 281, 3164, 337, 257, 3000, 50764], "temperature": 0.0, "avg_logprob": -0.07703929321438659, "compression_ratio": 1.5984555984555984, "no_speech_prob": 0.2569199800491333}, {"id": 92, "seek": 41600, "start": 424.0, "end": 427.0, "text": " and find all the pictures of your favorite pet.", "tokens": [50764, 293, 915, 439, 264, 5242, 295, 428, 2954, 3817, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07703929321438659, "compression_ratio": 1.5984555984555984, "no_speech_prob": 0.2569199800491333}, {"id": 93, "seek": 41600, "start": 427.0, "end": 434.0, "text": " So this is already there, and it's not terribly complicated to use for a person.", "tokens": [50914, 407, 341, 307, 1217, 456, 11, 293, 309, 311, 406, 22903, 6179, 281, 764, 337, 257, 954, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07703929321438659, "compression_ratio": 1.5984555984555984, "no_speech_prob": 0.2569199800491333}, {"id": 94, "seek": 41600, "start": 434.0, "end": 438.0, "text": " But of course, you now have all these new language models.", "tokens": [51264, 583, 295, 1164, 11, 291, 586, 362, 439, 613, 777, 2856, 5245, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07703929321438659, "compression_ratio": 1.5984555984555984, "no_speech_prob": 0.2569199800491333}, {"id": 95, "seek": 41600, "start": 438.0, "end": 443.0, "text": " I think there's really a big benefit, unlike dealing with information overload.", "tokens": [51464, 286, 519, 456, 311, 534, 257, 955, 5121, 11, 8343, 6260, 365, 1589, 28777, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07703929321438659, "compression_ratio": 1.5984555984555984, "no_speech_prob": 0.2569199800491333}, {"id": 96, "seek": 44300, "start": 443.0, "end": 445.0, "text": " You have tons of emails coming in.", "tokens": [50364, 509, 362, 9131, 295, 12524, 1348, 294, 13, 50464], "temperature": 0.0, "avg_logprob": -0.07009366629780203, "compression_ratio": 1.75, "no_speech_prob": 0.11384309083223343}, {"id": 97, "seek": 44300, "start": 445.0, "end": 448.0, "text": " You have, I don't know, papers to read, et cetera.", "tokens": [50464, 509, 362, 11, 286, 500, 380, 458, 11, 10577, 281, 1401, 11, 1030, 11458, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07009366629780203, "compression_ratio": 1.75, "no_speech_prob": 0.11384309083223343}, {"id": 98, "seek": 44300, "start": 448.0, "end": 453.0, "text": " And these LLMs, they, I know they create a lot of fake content and hallucinate stuff.", "tokens": [50614, 400, 613, 441, 43, 26386, 11, 436, 11, 286, 458, 436, 1884, 257, 688, 295, 7592, 2701, 293, 35212, 13923, 1507, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07009366629780203, "compression_ratio": 1.75, "no_speech_prob": 0.11384309083223343}, {"id": 99, "seek": 44300, "start": 453.0, "end": 456.0, "text": " But the thing they're pretty reliable at is summarizing.", "tokens": [50864, 583, 264, 551, 436, 434, 1238, 12924, 412, 307, 14611, 3319, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07009366629780203, "compression_ratio": 1.75, "no_speech_prob": 0.11384309083223343}, {"id": 100, "seek": 44300, "start": 456.0, "end": 458.0, "text": " And this is really quite important.", "tokens": [51014, 400, 341, 307, 534, 1596, 1021, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07009366629780203, "compression_ratio": 1.75, "no_speech_prob": 0.11384309083223343}, {"id": 101, "seek": 44300, "start": 458.0, "end": 460.0, "text": " I don't know how many emails you get, but I get a ton,", "tokens": [51114, 286, 500, 380, 458, 577, 867, 12524, 291, 483, 11, 457, 286, 483, 257, 2952, 11, 51214], "temperature": 0.0, "avg_logprob": -0.07009366629780203, "compression_ratio": 1.75, "no_speech_prob": 0.11384309083223343}, {"id": 102, "seek": 44300, "start": 460.0, "end": 467.0, "text": " and I would love to be able to summarize it or help select, you know, useful emails, et cetera.", "tokens": [51214, 293, 286, 576, 959, 281, 312, 1075, 281, 20858, 309, 420, 854, 3048, 11, 291, 458, 11, 4420, 12524, 11, 1030, 11458, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07009366629780203, "compression_ratio": 1.75, "no_speech_prob": 0.11384309083223343}, {"id": 103, "seek": 44300, "start": 467.0, "end": 471.0, "text": " And this stuff is really possible, or, I don't know, meeting notes.", "tokens": [51564, 400, 341, 1507, 307, 534, 1944, 11, 420, 11, 286, 500, 380, 458, 11, 3440, 5570, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07009366629780203, "compression_ratio": 1.75, "no_speech_prob": 0.11384309083223343}, {"id": 104, "seek": 47100, "start": 471.0, "end": 473.0, "text": " And, yeah.", "tokens": [50364, 400, 11, 1338, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11974202609453045, "compression_ratio": 1.72, "no_speech_prob": 0.03432565927505493}, {"id": 105, "seek": 47100, "start": 473.0, "end": 477.0, "text": " So this is, I think, where these models can be super helpful.", "tokens": [50464, 407, 341, 307, 11, 286, 519, 11, 689, 613, 5245, 393, 312, 1687, 4961, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11974202609453045, "compression_ratio": 1.72, "no_speech_prob": 0.03432565927505493}, {"id": 106, "seek": 47100, "start": 477.0, "end": 482.0, "text": " And you have, like, text generation, of course, they can do help out with this.", "tokens": [50664, 400, 291, 362, 11, 411, 11, 2487, 5125, 11, 295, 1164, 11, 436, 393, 360, 854, 484, 365, 341, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11974202609453045, "compression_ratio": 1.72, "no_speech_prob": 0.03432565927505493}, {"id": 107, "seek": 47100, "start": 482.0, "end": 486.0, "text": " You have also image analysis of various things.", "tokens": [50914, 509, 362, 611, 3256, 5215, 295, 3683, 721, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11974202609453045, "compression_ratio": 1.72, "no_speech_prob": 0.03432565927505493}, {"id": 108, "seek": 47100, "start": 486.0, "end": 490.0, "text": " There have been some demos from Microsoft and Google already about a year ago,", "tokens": [51114, 821, 362, 668, 512, 33788, 490, 8116, 293, 3329, 1217, 466, 257, 1064, 2057, 11, 51314], "temperature": 0.0, "avg_logprob": -0.11974202609453045, "compression_ratio": 1.72, "no_speech_prob": 0.03432565927505493}, {"id": 109, "seek": 47100, "start": 490.0, "end": 493.0, "text": " where they basically were showing that you have, like, a spreadsheet,", "tokens": [51314, 689, 436, 1936, 645, 4099, 300, 291, 362, 11, 411, 11, 257, 27733, 11, 51464], "temperature": 0.0, "avg_logprob": -0.11974202609453045, "compression_ratio": 1.72, "no_speech_prob": 0.03432565927505493}, {"id": 110, "seek": 47100, "start": 493.0, "end": 496.0, "text": " and you select something in it, and then you type a question about it,", "tokens": [51464, 293, 291, 3048, 746, 294, 309, 11, 293, 550, 291, 2010, 257, 1168, 466, 309, 11, 51614], "temperature": 0.0, "avg_logprob": -0.11974202609453045, "compression_ratio": 1.72, "no_speech_prob": 0.03432565927505493}, {"id": 111, "seek": 47100, "start": 496.0, "end": 498.0, "text": " and then it makes a graph that answers the question.", "tokens": [51614, 293, 550, 309, 1669, 257, 4295, 300, 6338, 264, 1168, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11974202609453045, "compression_ratio": 1.72, "no_speech_prob": 0.03432565927505493}, {"id": 112, "seek": 49800, "start": 498.0, "end": 501.0, "text": " This kind of stuff is also pretty magical.", "tokens": [50364, 639, 733, 295, 1507, 307, 611, 1238, 12066, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06700318200247628, "compression_ratio": 1.7459807073954985, "no_speech_prob": 0.04393727332353592}, {"id": 113, "seek": 49800, "start": 501.0, "end": 503.0, "text": " And there are tons of people in office all over the world", "tokens": [50514, 400, 456, 366, 9131, 295, 561, 294, 3398, 439, 670, 264, 1002, 50614], "temperature": 0.0, "avg_logprob": -0.06700318200247628, "compression_ratio": 1.7459807073954985, "no_speech_prob": 0.04393727332353592}, {"id": 114, "seek": 49800, "start": 503.0, "end": 506.0, "text": " that would benefit a lot from having this stuff.", "tokens": [50614, 300, 576, 5121, 257, 688, 490, 1419, 341, 1507, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06700318200247628, "compression_ratio": 1.7459807073954985, "no_speech_prob": 0.04393727332353592}, {"id": 115, "seek": 49800, "start": 506.0, "end": 509.0, "text": " So I think, yeah, the benefits are really there.", "tokens": [50764, 407, 286, 519, 11, 1338, 11, 264, 5311, 366, 534, 456, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06700318200247628, "compression_ratio": 1.7459807073954985, "no_speech_prob": 0.04393727332353592}, {"id": 116, "seek": 49800, "start": 509.0, "end": 511.0, "text": " Another thing is, like, automation.", "tokens": [50914, 3996, 551, 307, 11, 411, 11, 17769, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06700318200247628, "compression_ratio": 1.7459807073954985, "no_speech_prob": 0.04393727332353592}, {"id": 117, "seek": 49800, "start": 511.0, "end": 513.0, "text": " Just talking about it with a colleague.", "tokens": [51014, 1449, 1417, 466, 309, 365, 257, 13532, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06700318200247628, "compression_ratio": 1.7459807073954985, "no_speech_prob": 0.04393727332353592}, {"id": 118, "seek": 49800, "start": 513.0, "end": 517.0, "text": " This is, like, also a next step, you know, if you can say to the LLM,", "tokens": [51114, 639, 307, 11, 411, 11, 611, 257, 958, 1823, 11, 291, 458, 11, 498, 291, 393, 584, 281, 264, 441, 43, 44, 11, 51314], "temperature": 0.0, "avg_logprob": -0.06700318200247628, "compression_ratio": 1.7459807073954985, "no_speech_prob": 0.04393727332353592}, {"id": 119, "seek": 49800, "start": 517.0, "end": 520.0, "text": " like, hey, send, make an appointment with another person,", "tokens": [51314, 411, 11, 4177, 11, 2845, 11, 652, 364, 13653, 365, 1071, 954, 11, 51464], "temperature": 0.0, "avg_logprob": -0.06700318200247628, "compression_ratio": 1.7459807073954985, "no_speech_prob": 0.04393727332353592}, {"id": 120, "seek": 49800, "start": 520.0, "end": 523.0, "text": " and then they try chat, and if that doesn't work, they try email.", "tokens": [51464, 293, 550, 436, 853, 5081, 11, 293, 498, 300, 1177, 380, 589, 11, 436, 853, 3796, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06700318200247628, "compression_ratio": 1.7459807073954985, "no_speech_prob": 0.04393727332353592}, {"id": 121, "seek": 49800, "start": 523.0, "end": 527.0, "text": " These kind of things would be really helpful, I think, in day-to-day work.", "tokens": [51614, 1981, 733, 295, 721, 576, 312, 534, 4961, 11, 286, 519, 11, 294, 786, 12, 1353, 12, 810, 589, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06700318200247628, "compression_ratio": 1.7459807073954985, "no_speech_prob": 0.04393727332353592}, {"id": 122, "seek": 52700, "start": 528.0, "end": 530.0, "text": " So, yeah, I don't know.", "tokens": [50414, 407, 11, 1338, 11, 286, 500, 380, 458, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10654684837828292, "compression_ratio": 1.625, "no_speech_prob": 0.002328539965674281}, {"id": 123, "seek": 52700, "start": 530.0, "end": 532.0, "text": " If there are other ideas or things that are missing,", "tokens": [50514, 759, 456, 366, 661, 3487, 420, 721, 300, 366, 5361, 11, 50614], "temperature": 0.0, "avg_logprob": -0.10654684837828292, "compression_ratio": 1.625, "no_speech_prob": 0.002328539965674281}, {"id": 124, "seek": 52700, "start": 532.0, "end": 536.0, "text": " I'd love to hear it, actually, and make my list a little bit more complete,", "tokens": [50614, 286, 1116, 959, 281, 1568, 309, 11, 767, 11, 293, 652, 452, 1329, 257, 707, 857, 544, 3566, 11, 50814], "temperature": 0.0, "avg_logprob": -0.10654684837828292, "compression_ratio": 1.625, "no_speech_prob": 0.002328539965674281}, {"id": 125, "seek": 52700, "start": 536.0, "end": 539.0, "text": " but we'll get to that, I think.", "tokens": [50814, 457, 321, 603, 483, 281, 300, 11, 286, 519, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10654684837828292, "compression_ratio": 1.625, "no_speech_prob": 0.002328539965674281}, {"id": 126, "seek": 52700, "start": 539.0, "end": 541.0, "text": " So I just wanted to show a couple of examples,", "tokens": [50964, 407, 286, 445, 1415, 281, 855, 257, 1916, 295, 5110, 11, 51064], "temperature": 0.0, "avg_logprob": -0.10654684837828292, "compression_ratio": 1.625, "no_speech_prob": 0.002328539965674281}, {"id": 127, "seek": 52700, "start": 541.0, "end": 543.0, "text": " like we have this feature now, the Threads Summary,", "tokens": [51064, 411, 321, 362, 341, 4111, 586, 11, 264, 334, 2538, 82, 8626, 76, 822, 11, 51164], "temperature": 0.0, "avg_logprob": -0.10654684837828292, "compression_ratio": 1.625, "no_speech_prob": 0.002328539965674281}, {"id": 128, "seek": 52700, "start": 543.0, "end": 545.0, "text": " that makes a summary of your emails.", "tokens": [51164, 300, 1669, 257, 12691, 295, 428, 12524, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10654684837828292, "compression_ratio": 1.625, "no_speech_prob": 0.002328539965674281}, {"id": 129, "seek": 52700, "start": 545.0, "end": 548.0, "text": " Another example is, like, an Excel text.", "tokens": [51264, 3996, 1365, 307, 11, 411, 11, 364, 19060, 2487, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10654684837828292, "compression_ratio": 1.625, "no_speech_prob": 0.002328539965674281}, {"id": 130, "seek": 52700, "start": 548.0, "end": 551.0, "text": " You can just select some text and say, hey, summarize it, create a headline.", "tokens": [51414, 509, 393, 445, 3048, 512, 2487, 293, 584, 11, 4177, 11, 20858, 309, 11, 1884, 257, 28380, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10654684837828292, "compression_ratio": 1.625, "no_speech_prob": 0.002328539965674281}, {"id": 131, "seek": 52700, "start": 551.0, "end": 554.0, "text": " It's all quite simple to use.", "tokens": [51564, 467, 311, 439, 1596, 2199, 281, 764, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10654684837828292, "compression_ratio": 1.625, "no_speech_prob": 0.002328539965674281}, {"id": 132, "seek": 55400, "start": 554.0, "end": 558.0, "text": " And image generation, of course, I mean, this is a horrible image,", "tokens": [50364, 400, 3256, 5125, 11, 295, 1164, 11, 286, 914, 11, 341, 307, 257, 9263, 3256, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11998080490227032, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.014456327073276043}, {"id": 133, "seek": 55400, "start": 558.0, "end": 561.0, "text": " but, you know, you can make things that look good.", "tokens": [50564, 457, 11, 291, 458, 11, 291, 393, 652, 721, 300, 574, 665, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11998080490227032, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.014456327073276043}, {"id": 134, "seek": 55400, "start": 561.0, "end": 564.0, "text": " And then you have the data analysis, and you have automation,", "tokens": [50714, 400, 550, 291, 362, 264, 1412, 5215, 11, 293, 291, 362, 17769, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11998080490227032, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.014456327073276043}, {"id": 135, "seek": 55400, "start": 564.0, "end": 566.0, "text": " all these other features we have ideas on.", "tokens": [50864, 439, 613, 661, 4122, 321, 362, 3487, 322, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11998080490227032, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.014456327073276043}, {"id": 136, "seek": 55400, "start": 566.0, "end": 568.0, "text": " I'll share some things a bit later on.", "tokens": [50964, 286, 603, 2073, 512, 721, 257, 857, 1780, 322, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11998080490227032, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.014456327073276043}, {"id": 137, "seek": 55400, "start": 568.0, "end": 573.0, "text": " So I think we need to do AI in our collaboration platforms,", "tokens": [51064, 407, 286, 519, 321, 643, 281, 360, 7318, 294, 527, 9363, 9473, 11, 51314], "temperature": 0.0, "avg_logprob": -0.11998080490227032, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.014456327073276043}, {"id": 138, "seek": 55400, "start": 573.0, "end": 575.0, "text": " like X-Wiki, you guys need to have a plan.", "tokens": [51314, 411, 1783, 12, 54, 9850, 11, 291, 1074, 643, 281, 362, 257, 1393, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11998080490227032, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.014456327073276043}, {"id": 139, "seek": 55400, "start": 575.0, "end": 578.0, "text": " I know only Office, but they integrated just chat GPT.", "tokens": [51414, 286, 458, 787, 8935, 11, 457, 436, 10919, 445, 5081, 26039, 51, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11998080490227032, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.014456327073276043}, {"id": 140, "seek": 55400, "start": 578.0, "end": 581.0, "text": " I think we need a little bit more than just that,", "tokens": [51564, 286, 519, 321, 643, 257, 707, 857, 544, 813, 445, 300, 11, 51714], "temperature": 0.0, "avg_logprob": -0.11998080490227032, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.014456327073276043}, {"id": 141, "seek": 58100, "start": 581.0, "end": 585.0, "text": " because, well, we're losing the on-prem capabilities, right?", "tokens": [50364, 570, 11, 731, 11, 321, 434, 7027, 264, 322, 12, 29403, 10862, 11, 558, 30, 50564], "temperature": 0.0, "avg_logprob": -0.13226017761230469, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.013090063817799091}, {"id": 142, "seek": 58100, "start": 585.0, "end": 588.0, "text": " It's not competitive if you're just integrating chat GPT,", "tokens": [50564, 467, 311, 406, 10043, 498, 291, 434, 445, 26889, 5081, 26039, 51, 11, 50714], "temperature": 0.0, "avg_logprob": -0.13226017761230469, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.013090063817799091}, {"id": 143, "seek": 58100, "start": 588.0, "end": 591.0, "text": " then the data is sent to the U.S. anyway.", "tokens": [50714, 550, 264, 1412, 307, 2279, 281, 264, 624, 13, 50, 13, 4033, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13226017761230469, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.013090063817799091}, {"id": 144, "seek": 58100, "start": 591.0, "end": 594.0, "text": " So, yeah, that's not really a good solution.", "tokens": [50864, 407, 11, 1338, 11, 300, 311, 406, 534, 257, 665, 3827, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13226017761230469, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.013090063817799091}, {"id": 145, "seek": 58100, "start": 594.0, "end": 597.0, "text": " So the question is, how can we get this without the problems?", "tokens": [51014, 407, 264, 1168, 307, 11, 577, 393, 321, 483, 341, 1553, 264, 2740, 30, 51164], "temperature": 0.0, "avg_logprob": -0.13226017761230469, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.013090063817799091}, {"id": 146, "seek": 58100, "start": 597.0, "end": 600.0, "text": " And I think I'm in a room with open source people,", "tokens": [51164, 400, 286, 519, 286, 478, 294, 257, 1808, 365, 1269, 4009, 561, 11, 51314], "temperature": 0.0, "avg_logprob": -0.13226017761230469, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.013090063817799091}, {"id": 147, "seek": 58100, "start": 600.0, "end": 603.0, "text": " so I think the answer for most of you is obvious,", "tokens": [51314, 370, 286, 519, 264, 1867, 337, 881, 295, 291, 307, 6322, 11, 51464], "temperature": 0.0, "avg_logprob": -0.13226017761230469, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.013090063817799091}, {"id": 148, "seek": 58100, "start": 603.0, "end": 607.0, "text": " and this to me at least, just transparency and being open, yeah?", "tokens": [51464, 293, 341, 281, 385, 412, 1935, 11, 445, 17131, 293, 885, 1269, 11, 1338, 30, 51664], "temperature": 0.0, "avg_logprob": -0.13226017761230469, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.013090063817799091}, {"id": 149, "seek": 60700, "start": 608.0, "end": 611.0, "text": " And this is kind of the thing that we've been working on at NextLoud.", "tokens": [50414, 400, 341, 307, 733, 295, 264, 551, 300, 321, 600, 668, 1364, 322, 412, 3087, 43, 1861, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1303333610784812, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.023284735158085823}, {"id": 150, "seek": 60700, "start": 611.0, "end": 613.0, "text": " We kind of made some rules for ourselves,", "tokens": [50564, 492, 733, 295, 1027, 512, 4474, 337, 4175, 11, 50664], "temperature": 0.0, "avg_logprob": -0.1303333610784812, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.023284735158085823}, {"id": 151, "seek": 60700, "start": 613.0, "end": 615.0, "text": " so we have been doing AI as things,", "tokens": [50664, 370, 321, 362, 668, 884, 7318, 382, 721, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1303333610784812, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.023284735158085823}, {"id": 152, "seek": 60700, "start": 615.0, "end": 620.0, "text": " but when the whole text stuff from chat GPT came out,", "tokens": [50764, 457, 562, 264, 1379, 2487, 1507, 490, 5081, 26039, 51, 1361, 484, 11, 51014], "temperature": 0.0, "avg_logprob": -0.1303333610784812, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.023284735158085823}, {"id": 153, "seek": 60700, "start": 620.0, "end": 623.0, "text": " actually that was at the FOSDEM two years ago,", "tokens": [51014, 767, 300, 390, 412, 264, 479, 4367, 35, 6683, 732, 924, 2057, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1303333610784812, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.023284735158085823}, {"id": 154, "seek": 60700, "start": 623.0, "end": 626.0, "text": " we talked to people and each other,", "tokens": [51164, 321, 2825, 281, 561, 293, 1184, 661, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1303333610784812, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.023284735158085823}, {"id": 155, "seek": 60700, "start": 626.0, "end": 629.0, "text": " and we have some fairly smart people on board,", "tokens": [51314, 293, 321, 362, 512, 6457, 4069, 561, 322, 3150, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1303333610784812, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.023284735158085823}, {"id": 156, "seek": 60700, "start": 629.0, "end": 632.0, "text": " also from the research community,", "tokens": [51464, 611, 490, 264, 2132, 1768, 11, 51614], "temperature": 0.0, "avg_logprob": -0.1303333610784812, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.023284735158085823}, {"id": 157, "seek": 60700, "start": 632.0, "end": 636.0, "text": " and we tried to come up with, like, how can we handle this?", "tokens": [51614, 293, 321, 3031, 281, 808, 493, 365, 11, 411, 11, 577, 393, 321, 4813, 341, 30, 51814], "temperature": 0.0, "avg_logprob": -0.1303333610784812, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.023284735158085823}, {"id": 158, "seek": 63600, "start": 636.0, "end": 638.0, "text": " Because we add more AI features,", "tokens": [50364, 1436, 321, 909, 544, 7318, 4122, 11, 50464], "temperature": 0.0, "avg_logprob": -0.11303013354748279, "compression_ratio": 1.757679180887372, "no_speech_prob": 0.009250696748495102}, {"id": 159, "seek": 63600, "start": 638.0, "end": 640.0, "text": " like we don't want to be left behind,", "tokens": [50464, 411, 321, 500, 380, 528, 281, 312, 1411, 2261, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11303013354748279, "compression_ratio": 1.757679180887372, "no_speech_prob": 0.009250696748495102}, {"id": 160, "seek": 63600, "start": 640.0, "end": 643.0, "text": " and we need to be an alternative, as I said earlier,", "tokens": [50564, 293, 321, 643, 281, 312, 364, 8535, 11, 382, 286, 848, 3071, 11, 50714], "temperature": 0.0, "avg_logprob": -0.11303013354748279, "compression_ratio": 1.757679180887372, "no_speech_prob": 0.009250696748495102}, {"id": 161, "seek": 63600, "start": 643.0, "end": 646.0, "text": " and you can only be an alternative if you offer similar features,", "tokens": [50714, 293, 291, 393, 787, 312, 364, 8535, 498, 291, 2626, 2531, 4122, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11303013354748279, "compression_ratio": 1.757679180887372, "no_speech_prob": 0.009250696748495102}, {"id": 162, "seek": 63600, "start": 646.0, "end": 648.0, "text": " otherwise who's going to use your product?", "tokens": [50864, 5911, 567, 311, 516, 281, 764, 428, 1674, 30, 50964], "temperature": 0.0, "avg_logprob": -0.11303013354748279, "compression_ratio": 1.757679180887372, "no_speech_prob": 0.009250696748495102}, {"id": 163, "seek": 63600, "start": 648.0, "end": 651.0, "text": " But then how can you do that in an okayish way?", "tokens": [50964, 583, 550, 577, 393, 291, 360, 300, 294, 364, 1392, 742, 636, 30, 51114], "temperature": 0.0, "avg_logprob": -0.11303013354748279, "compression_ratio": 1.757679180887372, "no_speech_prob": 0.009250696748495102}, {"id": 164, "seek": 63600, "start": 651.0, "end": 655.0, "text": " So the idea we came up with was to at least create transparency,", "tokens": [51114, 407, 264, 1558, 321, 1361, 493, 365, 390, 281, 412, 1935, 1884, 17131, 11, 51314], "temperature": 0.0, "avg_logprob": -0.11303013354748279, "compression_ratio": 1.757679180887372, "no_speech_prob": 0.009250696748495102}, {"id": 165, "seek": 63600, "start": 655.0, "end": 657.0, "text": " and of course, choice, I'll get to that next,", "tokens": [51314, 293, 295, 1164, 11, 3922, 11, 286, 603, 483, 281, 300, 958, 11, 51414], "temperature": 0.0, "avg_logprob": -0.11303013354748279, "compression_ratio": 1.757679180887372, "no_speech_prob": 0.009250696748495102}, {"id": 166, "seek": 63600, "start": 657.0, "end": 659.0, "text": " but first the transparency.", "tokens": [51414, 457, 700, 264, 17131, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11303013354748279, "compression_ratio": 1.757679180887372, "no_speech_prob": 0.009250696748495102}, {"id": 167, "seek": 63600, "start": 659.0, "end": 661.0, "text": " So we came up with the idea of creating a rating", "tokens": [51514, 407, 321, 1361, 493, 365, 264, 1558, 295, 4084, 257, 10990, 51614], "temperature": 0.0, "avg_logprob": -0.11303013354748279, "compression_ratio": 1.757679180887372, "no_speech_prob": 0.009250696748495102}, {"id": 168, "seek": 63600, "start": 661.0, "end": 664.0, "text": " that has basically red, orange, yellow, green,", "tokens": [51614, 300, 575, 1936, 2182, 11, 7671, 11, 5566, 11, 3092, 11, 51764], "temperature": 0.0, "avg_logprob": -0.11303013354748279, "compression_ratio": 1.757679180887372, "no_speech_prob": 0.009250696748495102}, {"id": 169, "seek": 66400, "start": 664.0, "end": 668.0, "text": " and we would rate each of the integrations of AI features", "tokens": [50364, 293, 321, 576, 3314, 1184, 295, 264, 3572, 763, 295, 7318, 4122, 50564], "temperature": 0.0, "avg_logprob": -0.0817327654458643, "compression_ratio": 1.75, "no_speech_prob": 0.006466865073889494}, {"id": 170, "seek": 66400, "start": 668.0, "end": 671.0, "text": " in NextCloud with this rating.", "tokens": [50564, 294, 3087, 32787, 365, 341, 10990, 13, 50714], "temperature": 0.0, "avg_logprob": -0.0817327654458643, "compression_ratio": 1.75, "no_speech_prob": 0.006466865073889494}, {"id": 171, "seek": 66400, "start": 671.0, "end": 673.0, "text": " So first, is it open source?", "tokens": [50714, 407, 700, 11, 307, 309, 1269, 4009, 30, 50814], "temperature": 0.0, "avg_logprob": -0.0817327654458643, "compression_ratio": 1.75, "no_speech_prob": 0.006466865073889494}, {"id": 172, "seek": 66400, "start": 673.0, "end": 677.0, "text": " Is the model available, and is the training data available?", "tokens": [50814, 1119, 264, 2316, 2435, 11, 293, 307, 264, 3097, 1412, 2435, 30, 51014], "temperature": 0.0, "avg_logprob": -0.0817327654458643, "compression_ratio": 1.75, "no_speech_prob": 0.006466865073889494}, {"id": 173, "seek": 66400, "start": 677.0, "end": 680.0, "text": " And so if a model has all three, it's green,", "tokens": [51014, 400, 370, 498, 257, 2316, 575, 439, 1045, 11, 309, 311, 3092, 11, 51164], "temperature": 0.0, "avg_logprob": -0.0817327654458643, "compression_ratio": 1.75, "no_speech_prob": 0.006466865073889494}, {"id": 174, "seek": 66400, "start": 680.0, "end": 682.0, "text": " if it has two of them, it's orange,", "tokens": [51164, 498, 309, 575, 732, 295, 552, 11, 309, 311, 7671, 11, 51264], "temperature": 0.0, "avg_logprob": -0.0817327654458643, "compression_ratio": 1.75, "no_speech_prob": 0.006466865073889494}, {"id": 175, "seek": 66400, "start": 682.0, "end": 684.0, "text": " if it has one of them, it's yellow,", "tokens": [51264, 498, 309, 575, 472, 295, 552, 11, 309, 311, 5566, 11, 51364], "temperature": 0.0, "avg_logprob": -0.0817327654458643, "compression_ratio": 1.75, "no_speech_prob": 0.006466865073889494}, {"id": 176, "seek": 66400, "start": 684.0, "end": 686.0, "text": " if it has none of them, it's red.", "tokens": [51364, 498, 309, 575, 6022, 295, 552, 11, 309, 311, 2182, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0817327654458643, "compression_ratio": 1.75, "no_speech_prob": 0.006466865073889494}, {"id": 177, "seek": 66400, "start": 686.0, "end": 689.0, "text": " So chat GPT integration, red.", "tokens": [51464, 407, 5081, 26039, 51, 10980, 11, 2182, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0817327654458643, "compression_ratio": 1.75, "no_speech_prob": 0.006466865073889494}, {"id": 178, "seek": 66400, "start": 689.0, "end": 692.0, "text": " Completely on-prem model that is trained", "tokens": [51614, 39978, 322, 12, 29403, 2316, 300, 307, 8895, 51764], "temperature": 0.0, "avg_logprob": -0.0817327654458643, "compression_ratio": 1.75, "no_speech_prob": 0.006466865073889494}, {"id": 179, "seek": 69200, "start": 692.0, "end": 694.0, "text": " and has the training data available,", "tokens": [50364, 293, 575, 264, 3097, 1412, 2435, 11, 50464], "temperature": 0.0, "avg_logprob": -0.09337720479050728, "compression_ratio": 1.6932907348242812, "no_speech_prob": 0.006689050234854221}, {"id": 180, "seek": 69200, "start": 694.0, "end": 697.0, "text": " for example, for speech to text, that can be green,", "tokens": [50464, 337, 1365, 11, 337, 6218, 281, 2487, 11, 300, 393, 312, 3092, 11, 50614], "temperature": 0.0, "avg_logprob": -0.09337720479050728, "compression_ratio": 1.6932907348242812, "no_speech_prob": 0.006689050234854221}, {"id": 181, "seek": 69200, "start": 697.0, "end": 700.0, "text": " and you have everything in between, of course.", "tokens": [50614, 293, 291, 362, 1203, 294, 1296, 11, 295, 1164, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09337720479050728, "compression_ratio": 1.6932907348242812, "no_speech_prob": 0.006689050234854221}, {"id": 182, "seek": 69200, "start": 700.0, "end": 702.0, "text": " And the second thing is choice.", "tokens": [50764, 400, 264, 1150, 551, 307, 3922, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09337720479050728, "compression_ratio": 1.6932907348242812, "no_speech_prob": 0.006689050234854221}, {"id": 183, "seek": 69200, "start": 702.0, "end": 705.0, "text": " So for us, it's really important that you, well, can choose, right?", "tokens": [50864, 407, 337, 505, 11, 309, 311, 534, 1021, 300, 291, 11, 731, 11, 393, 2826, 11, 558, 30, 51014], "temperature": 0.0, "avg_logprob": -0.09337720479050728, "compression_ratio": 1.6932907348242812, "no_speech_prob": 0.006689050234854221}, {"id": 184, "seek": 69200, "start": 705.0, "end": 707.0, "text": " I mean, there are, again, legitimate users", "tokens": [51014, 286, 914, 11, 456, 366, 11, 797, 11, 17956, 5022, 51114], "temperature": 0.0, "avg_logprob": -0.09337720479050728, "compression_ratio": 1.6932907348242812, "no_speech_prob": 0.006689050234854221}, {"id": 185, "seek": 69200, "start": 707.0, "end": 709.0, "text": " for something like chat GPT, and I mean,", "tokens": [51114, 337, 746, 411, 5081, 26039, 51, 11, 293, 286, 914, 11, 51214], "temperature": 0.0, "avg_logprob": -0.09337720479050728, "compression_ratio": 1.6932907348242812, "no_speech_prob": 0.006689050234854221}, {"id": 186, "seek": 69200, "start": 709.0, "end": 711.0, "text": " they're throwing so many billions at this problem", "tokens": [51214, 436, 434, 10238, 370, 867, 17375, 412, 341, 1154, 51314], "temperature": 0.0, "avg_logprob": -0.09337720479050728, "compression_ratio": 1.6932907348242812, "no_speech_prob": 0.006689050234854221}, {"id": 187, "seek": 69200, "start": 711.0, "end": 713.0, "text": " that you can hardly argue that open source", "tokens": [51314, 300, 291, 393, 13572, 9695, 300, 1269, 4009, 51414], "temperature": 0.0, "avg_logprob": -0.09337720479050728, "compression_ratio": 1.6932907348242812, "no_speech_prob": 0.006689050234854221}, {"id": 188, "seek": 69200, "start": 713.0, "end": 716.0, "text": " can really keep up to the latest stuff they're doing,", "tokens": [51414, 393, 534, 1066, 493, 281, 264, 6792, 1507, 436, 434, 884, 11, 51564], "temperature": 0.0, "avg_logprob": -0.09337720479050728, "compression_ratio": 1.6932907348242812, "no_speech_prob": 0.006689050234854221}, {"id": 189, "seek": 69200, "start": 716.0, "end": 718.0, "text": " and sometimes you just need it, fine?", "tokens": [51564, 293, 2171, 291, 445, 643, 309, 11, 2489, 30, 51664], "temperature": 0.0, "avg_logprob": -0.09337720479050728, "compression_ratio": 1.6932907348242812, "no_speech_prob": 0.006689050234854221}, {"id": 190, "seek": 69200, "start": 718.0, "end": 720.0, "text": " So in our user interface,", "tokens": [51664, 407, 294, 527, 4195, 9226, 11, 51764], "temperature": 0.0, "avg_logprob": -0.09337720479050728, "compression_ratio": 1.6932907348242812, "no_speech_prob": 0.006689050234854221}, {"id": 191, "seek": 72000, "start": 720.0, "end": 724.0, "text": " we have these choices that you can have, like Opus,", "tokens": [50364, 321, 362, 613, 7994, 300, 291, 393, 362, 11, 411, 12011, 301, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11483561247587204, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.007625607308000326}, {"id": 192, "seek": 72000, "start": 724.0, "end": 726.0, "text": " that's a translation exactly,", "tokens": [50564, 300, 311, 257, 12853, 2293, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11483561247587204, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.007625607308000326}, {"id": 193, "seek": 72000, "start": 726.0, "end": 728.0, "text": " so this would be a fully green one,", "tokens": [50664, 370, 341, 576, 312, 257, 4498, 3092, 472, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11483561247587204, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.007625607308000326}, {"id": 194, "seek": 72000, "start": 728.0, "end": 731.0, "text": " and that's, well, we all know, chat GPT.", "tokens": [50764, 293, 300, 311, 11, 731, 11, 321, 439, 458, 11, 5081, 26039, 51, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11483561247587204, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.007625607308000326}, {"id": 195, "seek": 72000, "start": 731.0, "end": 733.0, "text": " So we try to make sure that for the various features", "tokens": [50914, 407, 321, 853, 281, 652, 988, 300, 337, 264, 3683, 4122, 51014], "temperature": 0.0, "avg_logprob": -0.11483561247587204, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.007625607308000326}, {"id": 196, "seek": 72000, "start": 733.0, "end": 735.0, "text": " that you can choose between these different models,", "tokens": [51014, 300, 291, 393, 2826, 1296, 613, 819, 5245, 11, 51114], "temperature": 0.0, "avg_logprob": -0.11483561247587204, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.007625607308000326}, {"id": 197, "seek": 72000, "start": 735.0, "end": 737.0, "text": " on-prem, et cetera.", "tokens": [51114, 322, 12, 29403, 11, 1030, 11458, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11483561247587204, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.007625607308000326}, {"id": 198, "seek": 72000, "start": 737.0, "end": 740.0, "text": " So for us, of course, most of the work we put in", "tokens": [51214, 407, 337, 505, 11, 295, 1164, 11, 881, 295, 264, 589, 321, 829, 294, 51364], "temperature": 0.0, "avg_logprob": -0.11483561247587204, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.007625607308000326}, {"id": 199, "seek": 72000, "start": 740.0, "end": 745.0, "text": " on-prem and open source locally running AI features,", "tokens": [51364, 322, 12, 29403, 293, 1269, 4009, 16143, 2614, 7318, 4122, 11, 51614], "temperature": 0.0, "avg_logprob": -0.11483561247587204, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.007625607308000326}, {"id": 200, "seek": 72000, "start": 745.0, "end": 748.0, "text": " because, well, that fits with our values as a company", "tokens": [51614, 570, 11, 731, 11, 300, 9001, 365, 527, 4190, 382, 257, 2237, 51764], "temperature": 0.0, "avg_logprob": -0.11483561247587204, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.007625607308000326}, {"id": 201, "seek": 74800, "start": 748.0, "end": 750.0, "text": " and, well, with our ethical AI rating,", "tokens": [50364, 293, 11, 731, 11, 365, 527, 18890, 7318, 10990, 11, 50464], "temperature": 0.0, "avg_logprob": -0.09212967865449145, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.009416686370968819}, {"id": 202, "seek": 74800, "start": 750.0, "end": 752.0, "text": " but the others are available.", "tokens": [50464, 457, 264, 2357, 366, 2435, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09212967865449145, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.009416686370968819}, {"id": 203, "seek": 74800, "start": 752.0, "end": 754.0, "text": " So at the moment, I made a list,", "tokens": [50564, 407, 412, 264, 1623, 11, 286, 1027, 257, 1329, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09212967865449145, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.009416686370968819}, {"id": 204, "seek": 74800, "start": 754.0, "end": 756.0, "text": " but I'm sure there are many more you can use,", "tokens": [50664, 457, 286, 478, 988, 456, 366, 867, 544, 291, 393, 764, 11, 50764], "temperature": 0.0, "avg_logprob": -0.09212967865449145, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.009416686370968819}, {"id": 205, "seek": 74800, "start": 756.0, "end": 758.0, "text": " like models like these in NextLoud.", "tokens": [50764, 411, 5245, 411, 613, 294, 3087, 43, 1861, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09212967865449145, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.009416686370968819}, {"id": 206, "seek": 74800, "start": 758.0, "end": 761.0, "text": " I have four of the various features.", "tokens": [50864, 286, 362, 1451, 295, 264, 3683, 4122, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09212967865449145, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.009416686370968819}, {"id": 207, "seek": 74800, "start": 761.0, "end": 765.0, "text": " I'll show, well, actually, I'm showing examples right now.", "tokens": [51014, 286, 603, 855, 11, 731, 11, 767, 11, 286, 478, 4099, 5110, 558, 586, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09212967865449145, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.009416686370968819}, {"id": 208, "seek": 74800, "start": 765.0, "end": 767.0, "text": " So this is just a bunch of the features we have.", "tokens": [51214, 407, 341, 307, 445, 257, 3840, 295, 264, 4122, 321, 362, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09212967865449145, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.009416686370968819}, {"id": 209, "seek": 74800, "start": 767.0, "end": 770.0, "text": " There is more, but suspicious login detection", "tokens": [51314, 821, 307, 544, 11, 457, 17931, 24276, 17784, 51464], "temperature": 0.0, "avg_logprob": -0.09212967865449145, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.009416686370968819}, {"id": 210, "seek": 74800, "start": 770.0, "end": 773.0, "text": " is something we developed like a really, really long time ago.", "tokens": [51464, 307, 746, 321, 4743, 411, 257, 534, 11, 534, 938, 565, 2057, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09212967865449145, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.009416686370968819}, {"id": 211, "seek": 74800, "start": 773.0, "end": 776.0, "text": " It's basically a neural network", "tokens": [51614, 467, 311, 1936, 257, 18161, 3209, 51764], "temperature": 0.0, "avg_logprob": -0.09212967865449145, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.009416686370968819}, {"id": 212, "seek": 77600, "start": 776.0, "end": 778.0, "text": " that gets trained on your login data.", "tokens": [50364, 300, 2170, 8895, 322, 428, 24276, 1412, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11335716247558594, "compression_ratio": 1.5471014492753623, "no_speech_prob": 0.01469175610691309}, {"id": 213, "seek": 77600, "start": 778.0, "end": 781.0, "text": " It just runs completely local every time you log in.", "tokens": [50464, 467, 445, 6676, 2584, 2654, 633, 565, 291, 3565, 294, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11335716247558594, "compression_ratio": 1.5471014492753623, "no_speech_prob": 0.01469175610691309}, {"id": 214, "seek": 77600, "start": 781.0, "end": 784.0, "text": " If you work nine to five from the Berlin office, let's say,", "tokens": [50614, 759, 291, 589, 4949, 281, 1732, 490, 264, 13848, 3398, 11, 718, 311, 584, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11335716247558594, "compression_ratio": 1.5471014492753623, "no_speech_prob": 0.01469175610691309}, {"id": 215, "seek": 77600, "start": 784.0, "end": 790.0, "text": " and suddenly somebody at 3 a.m. logs in in your account from China,", "tokens": [50764, 293, 5800, 2618, 412, 805, 257, 13, 76, 13, 20820, 294, 294, 428, 2696, 490, 3533, 11, 51064], "temperature": 0.0, "avg_logprob": -0.11335716247558594, "compression_ratio": 1.5471014492753623, "no_speech_prob": 0.01469175610691309}, {"id": 216, "seek": 77600, "start": 790.0, "end": 793.0, "text": " maybe there's something wrong, the model will detect that", "tokens": [51064, 1310, 456, 311, 746, 2085, 11, 264, 2316, 486, 5531, 300, 51214], "temperature": 0.0, "avg_logprob": -0.11335716247558594, "compression_ratio": 1.5471014492753623, "no_speech_prob": 0.01469175610691309}, {"id": 217, "seek": 77600, "start": 793.0, "end": 794.0, "text": " and give you a warning.", "tokens": [51214, 293, 976, 291, 257, 9164, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11335716247558594, "compression_ratio": 1.5471014492753623, "no_speech_prob": 0.01469175610691309}, {"id": 218, "seek": 77600, "start": 794.0, "end": 797.0, "text": " Very simple, and we have had this for, I don't know,", "tokens": [51264, 4372, 2199, 11, 293, 321, 362, 632, 341, 337, 11, 286, 500, 380, 458, 11, 51414], "temperature": 0.0, "avg_logprob": -0.11335716247558594, "compression_ratio": 1.5471014492753623, "no_speech_prob": 0.01469175610691309}, {"id": 219, "seek": 77600, "start": 797.0, "end": 800.0, "text": " since 2020, so quite a while.", "tokens": [51414, 1670, 4808, 11, 370, 1596, 257, 1339, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11335716247558594, "compression_ratio": 1.5471014492753623, "no_speech_prob": 0.01469175610691309}, {"id": 220, "seek": 77600, "start": 800.0, "end": 802.0, "text": " And it's green, right?", "tokens": [51564, 400, 309, 311, 3092, 11, 558, 30, 51664], "temperature": 0.0, "avg_logprob": -0.11335716247558594, "compression_ratio": 1.5471014492753623, "no_speech_prob": 0.01469175610691309}, {"id": 221, "seek": 77600, "start": 802.0, "end": 803.0, "text": " It runs fully local.", "tokens": [51664, 467, 6676, 4498, 2654, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11335716247558594, "compression_ratio": 1.5471014492753623, "no_speech_prob": 0.01469175610691309}, {"id": 222, "seek": 80300, "start": 803.0, "end": 806.0, "text": " There's nothing special about it, no data sent anywhere.", "tokens": [50364, 821, 311, 1825, 2121, 466, 309, 11, 572, 1412, 2279, 4992, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1386390746109129, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.012808797881007195}, {"id": 223, "seek": 80300, "start": 806.0, "end": 809.0, "text": " We basically do a very similar thing with our mail app,", "tokens": [50514, 492, 1936, 360, 257, 588, 2531, 551, 365, 527, 10071, 724, 11, 50664], "temperature": 0.0, "avg_logprob": -0.1386390746109129, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.012808797881007195}, {"id": 224, "seek": 80300, "start": 809.0, "end": 812.0, "text": " where we basically train a neural network on subjects,", "tokens": [50664, 689, 321, 1936, 3847, 257, 18161, 3209, 322, 13066, 11, 50814], "temperature": 0.0, "avg_logprob": -0.1386390746109129, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.012808797881007195}, {"id": 225, "seek": 80300, "start": 812.0, "end": 815.0, "text": " sender, and email recipients, et cetera.", "tokens": [50814, 2845, 260, 11, 293, 3796, 32440, 11, 1030, 11458, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1386390746109129, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.012808797881007195}, {"id": 226, "seek": 80300, "start": 815.0, "end": 819.0, "text": " And it creates a smart inbox trying to put important emails on top", "tokens": [50964, 400, 309, 7829, 257, 4069, 35067, 1382, 281, 829, 1021, 12524, 322, 1192, 51164], "temperature": 0.0, "avg_logprob": -0.1386390746109129, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.012808797881007195}, {"id": 227, "seek": 80300, "start": 819.0, "end": 822.0, "text": " and the rest not, and again, no data sent anywhere,", "tokens": [51164, 293, 264, 1472, 406, 11, 293, 797, 11, 572, 1412, 2279, 4992, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1386390746109129, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.012808797881007195}, {"id": 228, "seek": 80300, "start": 822.0, "end": 824.0, "text": " because it just runs on premise.", "tokens": [51314, 570, 309, 445, 6676, 322, 22045, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1386390746109129, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.012808797881007195}, {"id": 229, "seek": 80300, "start": 824.0, "end": 828.0, "text": " I already mentioned phase recognition and stuff we did in 2022,", "tokens": [51414, 286, 1217, 2835, 5574, 11150, 293, 1507, 321, 630, 294, 20229, 11, 51614], "temperature": 0.0, "avg_logprob": -0.1386390746109129, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.012808797881007195}, {"id": 230, "seek": 80300, "start": 828.0, "end": 830.0, "text": " I think, so this is all.", "tokens": [51614, 286, 519, 11, 370, 341, 307, 439, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1386390746109129, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.012808797881007195}, {"id": 231, "seek": 80300, "start": 830.0, "end": 832.0, "text": " But the problem with this is already,", "tokens": [51714, 583, 264, 1154, 365, 341, 307, 1217, 11, 51814], "temperature": 0.0, "avg_logprob": -0.1386390746109129, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.012808797881007195}, {"id": 232, "seek": 83200, "start": 832.0, "end": 834.0, "text": " you need to download a multi-gigabyte file,", "tokens": [50364, 291, 643, 281, 5484, 257, 4825, 12, 70, 328, 34529, 3991, 11, 50464], "temperature": 0.0, "avg_logprob": -0.09988880896753119, "compression_ratio": 1.6151315789473684, "no_speech_prob": 0.014599546790122986}, {"id": 233, "seek": 83200, "start": 834.0, "end": 837.0, "text": " which has, like, all the values needed for the neural network", "tokens": [50464, 597, 575, 11, 411, 11, 439, 264, 4190, 2978, 337, 264, 18161, 3209, 50614], "temperature": 0.0, "avg_logprob": -0.09988880896753119, "compression_ratio": 1.6151315789473684, "no_speech_prob": 0.014599546790122986}, {"id": 234, "seek": 83200, "start": 837.0, "end": 839.0, "text": " to recognize stuff.", "tokens": [50614, 281, 5521, 1507, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09988880896753119, "compression_ratio": 1.6151315789473684, "no_speech_prob": 0.014599546790122986}, {"id": 235, "seek": 83200, "start": 839.0, "end": 843.0, "text": " So we already had to re-architect a lot of the way Nexard works", "tokens": [50714, 407, 321, 1217, 632, 281, 319, 12, 1178, 5739, 257, 688, 295, 264, 636, 1734, 87, 515, 1985, 50914], "temperature": 0.0, "avg_logprob": -0.09988880896753119, "compression_ratio": 1.6151315789473684, "no_speech_prob": 0.014599546790122986}, {"id": 236, "seek": 83200, "start": 843.0, "end": 845.0, "text": " just to be able to download this big blob", "tokens": [50914, 445, 281, 312, 1075, 281, 5484, 341, 955, 46115, 51014], "temperature": 0.0, "avg_logprob": -0.09988880896753119, "compression_ratio": 1.6151315789473684, "no_speech_prob": 0.014599546790122986}, {"id": 237, "seek": 83200, "start": 845.0, "end": 848.0, "text": " without creating all kinds of complexities for the users.", "tokens": [51014, 1553, 4084, 439, 3685, 295, 48705, 337, 264, 5022, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09988880896753119, "compression_ratio": 1.6151315789473684, "no_speech_prob": 0.014599546790122986}, {"id": 238, "seek": 83200, "start": 848.0, "end": 851.0, "text": " And obviously, this problem gets bigger and bigger", "tokens": [51164, 400, 2745, 11, 341, 1154, 2170, 3801, 293, 3801, 51314], "temperature": 0.0, "avg_logprob": -0.09988880896753119, "compression_ratio": 1.6151315789473684, "no_speech_prob": 0.014599546790122986}, {"id": 239, "seek": 83200, "start": 851.0, "end": 853.0, "text": " when you get to modern AIs.", "tokens": [51314, 562, 291, 483, 281, 4363, 316, 6802, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09988880896753119, "compression_ratio": 1.6151315789473684, "no_speech_prob": 0.014599546790122986}, {"id": 240, "seek": 83200, "start": 853.0, "end": 858.0, "text": " We even have music genre recognition using machine learning.", "tokens": [51414, 492, 754, 362, 1318, 11022, 11150, 1228, 3479, 2539, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09988880896753119, "compression_ratio": 1.6151315789473684, "no_speech_prob": 0.014599546790122986}, {"id": 241, "seek": 83200, "start": 858.0, "end": 861.0, "text": " It's yellow because it's trained on all the music on Spotify,", "tokens": [51664, 467, 311, 5566, 570, 309, 311, 8895, 322, 439, 264, 1318, 322, 29036, 11, 51814], "temperature": 0.0, "avg_logprob": -0.09988880896753119, "compression_ratio": 1.6151315789473684, "no_speech_prob": 0.014599546790122986}, {"id": 242, "seek": 86100, "start": 861.0, "end": 864.0, "text": " which means the training data is actually copyrighted", "tokens": [50364, 597, 1355, 264, 3097, 1412, 307, 767, 17996, 292, 50514], "temperature": 0.0, "avg_logprob": -0.13161259643302475, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.012945813126862049}, {"id": 243, "seek": 86100, "start": 864.0, "end": 866.0, "text": " and therefore not open.", "tokens": [50514, 293, 4412, 406, 1269, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13161259643302475, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.012945813126862049}, {"id": 244, "seek": 86100, "start": 866.0, "end": 872.0, "text": " And, yeah, we had, like, a pre-trained model to do call transcripts.", "tokens": [50614, 400, 11, 1338, 11, 321, 632, 11, 411, 11, 257, 659, 12, 17227, 2001, 2316, 281, 360, 818, 24444, 82, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13161259643302475, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.012945813126862049}, {"id": 245, "seek": 86100, "start": 872.0, "end": 874.0, "text": " We introduced that last year.", "tokens": [50914, 492, 7268, 300, 1036, 1064, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13161259643302475, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.012945813126862049}, {"id": 246, "seek": 86100, "start": 874.0, "end": 877.0, "text": " That is nice. You have a call and the recording then gets text,", "tokens": [51014, 663, 307, 1481, 13, 509, 362, 257, 818, 293, 264, 6613, 550, 2170, 2487, 11, 51164], "temperature": 0.0, "avg_logprob": -0.13161259643302475, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.012945813126862049}, {"id": 247, "seek": 86100, "start": 877.0, "end": 881.0, "text": " speeds to text so that you get the text of the recording.", "tokens": [51164, 16411, 281, 2487, 370, 300, 291, 483, 264, 2487, 295, 264, 6613, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13161259643302475, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.012945813126862049}, {"id": 248, "seek": 86100, "start": 881.0, "end": 884.0, "text": " Again, this model runs fully local, so that's cool.", "tokens": [51364, 3764, 11, 341, 2316, 6676, 4498, 2654, 11, 370, 300, 311, 1627, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13161259643302475, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.012945813126862049}, {"id": 249, "seek": 86100, "start": 884.0, "end": 887.0, "text": " And speeds to text the other way around.", "tokens": [51514, 400, 16411, 281, 2487, 264, 661, 636, 926, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13161259643302475, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.012945813126862049}, {"id": 250, "seek": 86100, "start": 887.0, "end": 890.0, "text": " Background blur, just a JavaScript thing that we upload", "tokens": [51664, 36904, 14257, 11, 445, 257, 15778, 551, 300, 321, 6580, 51814], "temperature": 0.0, "avg_logprob": -0.13161259643302475, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.012945813126862049}, {"id": 251, "seek": 89000, "start": 890.0, "end": 893.0, "text": " in the browser, very simple translations.", "tokens": [50364, 294, 264, 11185, 11, 588, 2199, 37578, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10408039093017578, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.0035520007368177176}, {"id": 252, "seek": 89000, "start": 893.0, "end": 896.0, "text": " First, we made it with Deepol, which is not cool.", "tokens": [50514, 2386, 11, 321, 1027, 309, 365, 14895, 401, 11, 597, 307, 406, 1627, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10408039093017578, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.0035520007368177176}, {"id": 253, "seek": 89000, "start": 896.0, "end": 899.0, "text": " So then we made one using the Opus corpus.", "tokens": [50664, 407, 550, 321, 1027, 472, 1228, 264, 12011, 301, 1181, 31624, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10408039093017578, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.0035520007368177176}, {"id": 254, "seek": 89000, "start": 899.0, "end": 902.0, "text": " You saw it earlier, and that is running fully local,", "tokens": [50814, 509, 1866, 309, 3071, 11, 293, 300, 307, 2614, 4498, 2654, 11, 50964], "temperature": 0.0, "avg_logprob": -0.10408039093017578, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.0035520007368177176}, {"id": 255, "seek": 89000, "start": 902.0, "end": 904.0, "text": " so that's much better.", "tokens": [50964, 370, 300, 311, 709, 1101, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10408039093017578, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.0035520007368177176}, {"id": 256, "seek": 89000, "start": 904.0, "end": 908.0, "text": " So these are still mostly basic features, I think, today,", "tokens": [51064, 407, 613, 366, 920, 5240, 3875, 4122, 11, 286, 519, 11, 965, 11, 51264], "temperature": 0.0, "avg_logprob": -0.10408039093017578, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.0035520007368177176}, {"id": 257, "seek": 89000, "start": 908.0, "end": 910.0, "text": " and yet already pretty complicated.", "tokens": [51264, 293, 1939, 1217, 1238, 6179, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10408039093017578, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.0035520007368177176}, {"id": 258, "seek": 89000, "start": 910.0, "end": 913.0, "text": " You need to keep an eye out where the data is being sent to,", "tokens": [51364, 509, 643, 281, 1066, 364, 3313, 484, 689, 264, 1412, 307, 885, 2279, 281, 11, 51514], "temperature": 0.0, "avg_logprob": -0.10408039093017578, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.0035520007368177176}, {"id": 259, "seek": 89000, "start": 913.0, "end": 915.0, "text": " like with translation.", "tokens": [51514, 411, 365, 12853, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10408039093017578, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.0035520007368177176}, {"id": 260, "seek": 89000, "start": 915.0, "end": 917.0, "text": " But, of course, the big thing are LLMs,", "tokens": [51614, 583, 11, 295, 1164, 11, 264, 955, 551, 366, 441, 43, 26386, 11, 51714], "temperature": 0.0, "avg_logprob": -0.10408039093017578, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.0035520007368177176}, {"id": 261, "seek": 91700, "start": 917.0, "end": 920.0, "text": " like the text operations.", "tokens": [50364, 411, 264, 2487, 7705, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16040217447623933, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.038299161940813065}, {"id": 262, "seek": 91700, "start": 920.0, "end": 923.0, "text": " What we've been doing is to create, basically, NexLad Assistant.", "tokens": [50514, 708, 321, 600, 668, 884, 307, 281, 1884, 11, 1936, 11, 1734, 87, 43, 345, 14890, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16040217447623933, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.038299161940813065}, {"id": 263, "seek": 91700, "start": 923.0, "end": 928.0, "text": " It uses the large language models, but open source on-prem ones", "tokens": [50664, 467, 4960, 264, 2416, 2856, 5245, 11, 457, 1269, 4009, 322, 12, 29403, 2306, 50914], "temperature": 0.0, "avg_logprob": -0.16040217447623933, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.038299161940813065}, {"id": 264, "seek": 91700, "start": 928.0, "end": 930.0, "text": " that you can host yourself.", "tokens": [50914, 300, 291, 393, 3975, 1803, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16040217447623933, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.038299161940813065}, {"id": 265, "seek": 91700, "start": 930.0, "end": 932.0, "text": " It's like this little thing on the top.", "tokens": [51014, 467, 311, 411, 341, 707, 551, 322, 264, 1192, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16040217447623933, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.038299161940813065}, {"id": 266, "seek": 91700, "start": 932.0, "end": 934.0, "text": " When you click on it, you get a dialogue.", "tokens": [51114, 1133, 291, 2052, 322, 309, 11, 291, 483, 257, 10221, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16040217447623933, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.038299161940813065}, {"id": 267, "seek": 91700, "start": 934.0, "end": 937.0, "text": " You can give a free prompt, or you can give a text to summarize", "tokens": [51214, 509, 393, 976, 257, 1737, 12391, 11, 420, 291, 393, 976, 257, 2487, 281, 20858, 51364], "temperature": 0.0, "avg_logprob": -0.16040217447623933, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.038299161940813065}, {"id": 268, "seek": 91700, "start": 937.0, "end": 939.0, "text": " and some other things.", "tokens": [51364, 293, 512, 661, 721, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16040217447623933, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.038299161940813065}, {"id": 269, "seek": 91700, "start": 939.0, "end": 942.0, "text": " And it just runs this through one of the models that is supported by NexLad.", "tokens": [51464, 400, 309, 445, 6676, 341, 807, 472, 295, 264, 5245, 300, 307, 8104, 538, 1734, 87, 43, 345, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16040217447623933, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.038299161940813065}, {"id": 270, "seek": 91700, "start": 942.0, "end": 946.0, "text": " And, again, you can put JetGPT here as model,", "tokens": [51614, 400, 11, 797, 11, 291, 393, 829, 28730, 38, 47, 51, 510, 382, 2316, 11, 51814], "temperature": 0.0, "avg_logprob": -0.16040217447623933, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.038299161940813065}, {"id": 271, "seek": 94600, "start": 946.0, "end": 950.0, "text": " or as back-end, but you can also then run your own LLM", "tokens": [50364, 420, 382, 646, 12, 521, 11, 457, 291, 393, 611, 550, 1190, 428, 1065, 441, 43, 44, 50564], "temperature": 0.0, "avg_logprob": -0.10098436224551602, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.004502429626882076}, {"id": 272, "seek": 94600, "start": 950.0, "end": 954.0, "text": " and connect that to NexLad, and then it can do all this stuff on-premise.", "tokens": [50564, 293, 1745, 300, 281, 1734, 87, 43, 345, 11, 293, 550, 309, 393, 360, 439, 341, 1507, 322, 12, 29403, 908, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10098436224551602, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.004502429626882076}, {"id": 273, "seek": 94600, "start": 954.0, "end": 956.0, "text": " So it's fairly simple.", "tokens": [50764, 407, 309, 311, 6457, 2199, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10098436224551602, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.004502429626882076}, {"id": 274, "seek": 94600, "start": 956.0, "end": 959.0, "text": " When it's running, then it'll get the results,", "tokens": [50864, 1133, 309, 311, 2614, 11, 550, 309, 603, 483, 264, 3542, 11, 51014], "temperature": 0.0, "avg_logprob": -0.10098436224551602, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.004502429626882076}, {"id": 275, "seek": 94600, "start": 959.0, "end": 962.0, "text": " and after a while you get the output of it.", "tokens": [51014, 293, 934, 257, 1339, 291, 483, 264, 5598, 295, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10098436224551602, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.004502429626882076}, {"id": 276, "seek": 94600, "start": 962.0, "end": 964.0, "text": " You can copy it into a document, et cetera.", "tokens": [51164, 509, 393, 5055, 309, 666, 257, 4166, 11, 1030, 11458, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10098436224551602, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.004502429626882076}, {"id": 277, "seek": 94600, "start": 964.0, "end": 968.0, "text": " And, again, if you take a local model that is trained on public data,", "tokens": [51264, 400, 11, 797, 11, 498, 291, 747, 257, 2654, 2316, 300, 307, 8895, 322, 1908, 1412, 11, 51464], "temperature": 0.0, "avg_logprob": -0.10098436224551602, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.004502429626882076}, {"id": 278, "seek": 94600, "start": 968.0, "end": 971.0, "text": " then it can be a fully green solution.", "tokens": [51464, 550, 309, 393, 312, 257, 4498, 3092, 3827, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10098436224551602, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.004502429626882076}, {"id": 279, "seek": 94600, "start": 971.0, "end": 973.0, "text": " So that's really cool.", "tokens": [51614, 407, 300, 311, 534, 1627, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10098436224551602, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.004502429626882076}, {"id": 280, "seek": 97300, "start": 973.0, "end": 977.0, "text": " In places like NexLad Text, I already showed that,", "tokens": [50364, 682, 3190, 411, 1734, 87, 43, 345, 18643, 11, 286, 1217, 4712, 300, 11, 50564], "temperature": 0.0, "avg_logprob": -0.1274669364646629, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.1634245663881302}, {"id": 281, "seek": 97300, "start": 977.0, "end": 980.0, "text": " that you can select some text and then run this.", "tokens": [50564, 300, 291, 393, 3048, 512, 2487, 293, 550, 1190, 341, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1274669364646629, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.1634245663881302}, {"id": 282, "seek": 97300, "start": 980.0, "end": 982.0, "text": " Mail, I already showed this as well.", "tokens": [50714, 29164, 11, 286, 1217, 4712, 341, 382, 731, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1274669364646629, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.1634245663881302}, {"id": 283, "seek": 97300, "start": 982.0, "end": 986.0, "text": " In talk, like our video calling and JetSolution,", "tokens": [50814, 682, 751, 11, 411, 527, 960, 5141, 293, 28730, 50, 3386, 11, 51014], "temperature": 0.0, "avg_logprob": -0.1274669364646629, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.1634245663881302}, {"id": 284, "seek": 97300, "start": 986.0, "end": 990.0, "text": " you can translate a message, select it, and then choose translate,", "tokens": [51014, 291, 393, 13799, 257, 3636, 11, 3048, 309, 11, 293, 550, 2826, 13799, 11, 51214], "temperature": 0.0, "avg_logprob": -0.1274669364646629, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.1634245663881302}, {"id": 285, "seek": 97300, "start": 990.0, "end": 992.0, "text": " insert images and other stuff.", "tokens": [51214, 8969, 5267, 293, 661, 1507, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1274669364646629, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.1634245663881302}, {"id": 286, "seek": 97300, "start": 992.0, "end": 994.0, "text": " We even made a little bot.", "tokens": [51314, 492, 754, 1027, 257, 707, 10592, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1274669364646629, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.1634245663881302}, {"id": 287, "seek": 97300, "start": 994.0, "end": 996.0, "text": " This isn't the smartest bot.", "tokens": [51414, 639, 1943, 380, 264, 41491, 10592, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1274669364646629, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.1634245663881302}, {"id": 288, "seek": 97300, "start": 996.0, "end": 998.0, "text": " It's a very small model, but hey, it's fast.", "tokens": [51514, 467, 311, 257, 588, 1359, 2316, 11, 457, 4177, 11, 309, 311, 2370, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1274669364646629, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.1634245663881302}, {"id": 289, "seek": 97300, "start": 998.0, "end": 1000.0, "text": " And you can ask it questions.", "tokens": [51614, 400, 291, 393, 1029, 309, 1651, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1274669364646629, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.1634245663881302}, {"id": 290, "seek": 97300, "start": 1000.0, "end": 1002.0, "text": " Honestly, I wouldn't say such smart things.", "tokens": [51714, 12348, 11, 286, 2759, 380, 584, 1270, 4069, 721, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1274669364646629, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.1634245663881302}, {"id": 291, "seek": 100200, "start": 1002.0, "end": 1004.0, "text": " It's fairly shitty, I've noticed.", "tokens": [50364, 467, 311, 6457, 30748, 11, 286, 600, 5694, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1421263308925483, "compression_ratio": 1.575, "no_speech_prob": 0.03503665328025818}, {"id": 292, "seek": 100200, "start": 1004.0, "end": 1007.0, "text": " But still, it works on your own server.", "tokens": [50464, 583, 920, 11, 309, 1985, 322, 428, 1065, 7154, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1421263308925483, "compression_ratio": 1.575, "no_speech_prob": 0.03503665328025818}, {"id": 293, "seek": 100200, "start": 1007.0, "end": 1009.0, "text": " That's kind of nice.", "tokens": [50614, 663, 311, 733, 295, 1481, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1421263308925483, "compression_ratio": 1.575, "no_speech_prob": 0.03503665328025818}, {"id": 294, "seek": 100200, "start": 1009.0, "end": 1011.0, "text": " So a lot is possible.", "tokens": [50714, 407, 257, 688, 307, 1944, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1421263308925483, "compression_ratio": 1.575, "no_speech_prob": 0.03503665328025818}, {"id": 295, "seek": 100200, "start": 1011.0, "end": 1013.0, "text": " One of the newer things we're working on is more of these services,", "tokens": [50814, 1485, 295, 264, 17628, 721, 321, 434, 1364, 322, 307, 544, 295, 613, 3328, 11, 50914], "temperature": 0.0, "avg_logprob": -0.1421263308925483, "compression_ratio": 1.575, "no_speech_prob": 0.03503665328025818}, {"id": 296, "seek": 100200, "start": 1013.0, "end": 1015.0, "text": " because they're now companies like Amazon.", "tokens": [50914, 570, 436, 434, 586, 3431, 411, 6795, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1421263308925483, "compression_ratio": 1.575, "no_speech_prob": 0.03503665328025818}, {"id": 297, "seek": 100200, "start": 1015.0, "end": 1018.0, "text": " They are running LLMs as a service.", "tokens": [51014, 814, 366, 2614, 441, 43, 26386, 382, 257, 2643, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1421263308925483, "compression_ratio": 1.575, "no_speech_prob": 0.03503665328025818}, {"id": 298, "seek": 100200, "start": 1018.0, "end": 1021.0, "text": " And other companies are doing this also purely in Europe,", "tokens": [51164, 400, 661, 3431, 366, 884, 341, 611, 17491, 294, 3315, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1421263308925483, "compression_ratio": 1.575, "no_speech_prob": 0.03503665328025818}, {"id": 299, "seek": 100200, "start": 1021.0, "end": 1025.0, "text": " like you have ALEF, ALFA, and I think MIRROR or something.", "tokens": [51314, 411, 291, 362, 316, 2634, 37, 11, 7056, 19684, 11, 293, 286, 519, 376, 7740, 49, 2483, 420, 746, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1421263308925483, "compression_ratio": 1.575, "no_speech_prob": 0.03503665328025818}, {"id": 300, "seek": 100200, "start": 1025.0, "end": 1029.0, "text": " In France, there's also a company that is building local AI.", "tokens": [51514, 682, 6190, 11, 456, 311, 611, 257, 2237, 300, 307, 2390, 2654, 7318, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1421263308925483, "compression_ratio": 1.575, "no_speech_prob": 0.03503665328025818}, {"id": 301, "seek": 102900, "start": 1029.0, "end": 1031.0, "text": " So we're trying to support these, that you...", "tokens": [50364, 407, 321, 434, 1382, 281, 1406, 613, 11, 300, 291, 485, 50464], "temperature": 0.0, "avg_logprob": -0.12589941353633485, "compression_ratio": 1.6170886075949367, "no_speech_prob": 0.007348577491939068}, {"id": 302, "seek": 102900, "start": 1031.0, "end": 1033.0, "text": " You know, everybody can run these AIs,", "tokens": [50464, 509, 458, 11, 2201, 393, 1190, 613, 316, 6802, 11, 50564], "temperature": 0.0, "avg_logprob": -0.12589941353633485, "compression_ratio": 1.6170886075949367, "no_speech_prob": 0.007348577491939068}, {"id": 303, "seek": 102900, "start": 1033.0, "end": 1035.0, "text": " like you need a lot of heavy GPUs.", "tokens": [50564, 411, 291, 643, 257, 688, 295, 4676, 18407, 82, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12589941353633485, "compression_ratio": 1.6170886075949367, "no_speech_prob": 0.007348577491939068}, {"id": 304, "seek": 102900, "start": 1035.0, "end": 1037.0, "text": " It's a lot of compute.", "tokens": [50664, 467, 311, 257, 688, 295, 14722, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12589941353633485, "compression_ratio": 1.6170886075949367, "no_speech_prob": 0.007348577491939068}, {"id": 305, "seek": 102900, "start": 1037.0, "end": 1039.0, "text": " So you can use it as a service that at least it stays in Europe", "tokens": [50764, 407, 291, 393, 764, 309, 382, 257, 2643, 300, 412, 1935, 309, 10834, 294, 3315, 50864], "temperature": 0.0, "avg_logprob": -0.12589941353633485, "compression_ratio": 1.6170886075949367, "no_speech_prob": 0.007348577491939068}, {"id": 306, "seek": 102900, "start": 1039.0, "end": 1042.0, "text": " or at a company that you trust.", "tokens": [50864, 420, 412, 257, 2237, 300, 291, 3361, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12589941353633485, "compression_ratio": 1.6170886075949367, "no_speech_prob": 0.007348577491939068}, {"id": 307, "seek": 102900, "start": 1042.0, "end": 1045.0, "text": " Then I wouldn't recommend Amazon, per se, perhaps.", "tokens": [51014, 1396, 286, 2759, 380, 2748, 6795, 11, 680, 369, 11, 4317, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12589941353633485, "compression_ratio": 1.6170886075949367, "no_speech_prob": 0.007348577491939068}, {"id": 308, "seek": 102900, "start": 1045.0, "end": 1048.0, "text": " For this, we also made it possible that you can put in some limits,", "tokens": [51164, 1171, 341, 11, 321, 611, 1027, 309, 1944, 300, 291, 393, 829, 294, 512, 10406, 11, 51314], "temperature": 0.0, "avg_logprob": -0.12589941353633485, "compression_ratio": 1.6170886075949367, "no_speech_prob": 0.007348577491939068}, {"id": 309, "seek": 102900, "start": 1048.0, "end": 1054.0, "text": " otherwise users get a little creative and start to basically cost you a lot of money.", "tokens": [51314, 5911, 5022, 483, 257, 707, 5880, 293, 722, 281, 1936, 2063, 291, 257, 688, 295, 1460, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12589941353633485, "compression_ratio": 1.6170886075949367, "no_speech_prob": 0.007348577491939068}, {"id": 310, "seek": 102900, "start": 1054.0, "end": 1056.0, "text": " And we worked on the interaction with this.", "tokens": [51614, 400, 321, 2732, 322, 264, 9285, 365, 341, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12589941353633485, "compression_ratio": 1.6170886075949367, "no_speech_prob": 0.007348577491939068}, {"id": 311, "seek": 102900, "start": 1056.0, "end": 1058.0, "text": " I'll skip through this.", "tokens": [51714, 286, 603, 10023, 807, 341, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12589941353633485, "compression_ratio": 1.6170886075949367, "no_speech_prob": 0.007348577491939068}, {"id": 312, "seek": 105800, "start": 1058.0, "end": 1063.0, "text": " A thing we're working on now is also to make all of this even smarter.", "tokens": [50364, 316, 551, 321, 434, 1364, 322, 586, 307, 611, 281, 652, 439, 295, 341, 754, 20294, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10980311802455357, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.004918539430946112}, {"id": 313, "seek": 105800, "start": 1063.0, "end": 1070.0, "text": " A newer thing is the ability to take your documents that you have into account.", "tokens": [50614, 316, 17628, 551, 307, 264, 3485, 281, 747, 428, 8512, 300, 291, 362, 666, 2696, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10980311802455357, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.004918539430946112}, {"id": 314, "seek": 105800, "start": 1070.0, "end": 1075.0, "text": " So ContextChat is a feature of the Assistant that basically it has access to your documents,", "tokens": [50964, 407, 4839, 3828, 41683, 307, 257, 4111, 295, 264, 14890, 300, 1936, 309, 575, 2105, 281, 428, 8512, 11, 51214], "temperature": 0.0, "avg_logprob": -0.10980311802455357, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.004918539430946112}, {"id": 315, "seek": 105800, "start": 1075.0, "end": 1078.0, "text": " your emails, everything you have that gets indexed.", "tokens": [51214, 428, 12524, 11, 1203, 291, 362, 300, 2170, 8186, 292, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10980311802455357, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.004918539430946112}, {"id": 316, "seek": 105800, "start": 1078.0, "end": 1080.0, "text": " Let me see.", "tokens": [51364, 961, 385, 536, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10980311802455357, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.004918539430946112}, {"id": 317, "seek": 105800, "start": 1080.0, "end": 1082.0, "text": " It's indexed into a vector database.", "tokens": [51464, 467, 311, 8186, 292, 666, 257, 8062, 8149, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10980311802455357, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.004918539430946112}, {"id": 318, "seek": 105800, "start": 1082.0, "end": 1086.0, "text": " So this runs as a separate service next to NextLoud.", "tokens": [51564, 407, 341, 6676, 382, 257, 4994, 2643, 958, 281, 3087, 43, 1861, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10980311802455357, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.004918539430946112}, {"id": 319, "seek": 108600, "start": 1086.0, "end": 1090.0, "text": " And then when you ask a question from the Assistant,", "tokens": [50364, 400, 550, 562, 291, 1029, 257, 1168, 490, 264, 14890, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11286183446645737, "compression_ratio": 1.857677902621723, "no_speech_prob": 0.0088193379342556}, {"id": 320, "seek": 108600, "start": 1090.0, "end": 1095.0, "text": " it can actually answer using your documents, your company documentation, your emails, etc.", "tokens": [50564, 309, 393, 767, 1867, 1228, 428, 8512, 11, 428, 2237, 14333, 11, 428, 12524, 11, 5183, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11286183446645737, "compression_ratio": 1.857677902621723, "no_speech_prob": 0.0088193379342556}, {"id": 321, "seek": 108600, "start": 1095.0, "end": 1098.0, "text": " So you can really do stuff like, you know,", "tokens": [50814, 407, 291, 393, 534, 360, 1507, 411, 11, 291, 458, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11286183446645737, "compression_ratio": 1.857677902621723, "no_speech_prob": 0.0088193379342556}, {"id": 322, "seek": 108600, "start": 1098.0, "end": 1102.0, "text": " can you give me an idea of how we organize events, rather than in general,", "tokens": [50964, 393, 291, 976, 385, 364, 1558, 295, 577, 321, 13859, 3931, 11, 2831, 813, 294, 2674, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11286183446645737, "compression_ratio": 1.857677902621723, "no_speech_prob": 0.0088193379342556}, {"id": 323, "seek": 108600, "start": 1102.0, "end": 1105.0, "text": " it can look at your documentation and then tell you, like,", "tokens": [51164, 309, 393, 574, 412, 428, 14333, 293, 550, 980, 291, 11, 411, 11, 51314], "temperature": 0.0, "avg_logprob": -0.11286183446645737, "compression_ratio": 1.857677902621723, "no_speech_prob": 0.0088193379342556}, {"id": 324, "seek": 108600, "start": 1105.0, "end": 1108.0, "text": " oh, you know, at your company, organize events this way.", "tokens": [51314, 1954, 11, 291, 458, 11, 412, 428, 2237, 11, 13859, 3931, 341, 636, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11286183446645737, "compression_ratio": 1.857677902621723, "no_speech_prob": 0.0088193379342556}, {"id": 325, "seek": 108600, "start": 1108.0, "end": 1112.0, "text": " Or you can say, hey, can you give me a summary of the different requests", "tokens": [51464, 1610, 291, 393, 584, 11, 4177, 11, 393, 291, 976, 385, 257, 12691, 295, 264, 819, 12475, 51664], "temperature": 0.0, "avg_logprob": -0.11286183446645737, "compression_ratio": 1.857677902621723, "no_speech_prob": 0.0088193379342556}, {"id": 326, "seek": 108600, "start": 1112.0, "end": 1115.0, "text": " that a colleague has emailed to me last week,", "tokens": [51664, 300, 257, 13532, 575, 45460, 281, 385, 1036, 1243, 11, 51814], "temperature": 0.0, "avg_logprob": -0.11286183446645737, "compression_ratio": 1.857677902621723, "no_speech_prob": 0.0088193379342556}, {"id": 327, "seek": 111500, "start": 1115.0, "end": 1119.0, "text": " and hopefully it'll give you all the to-dos that you got from that colleague in the last week.", "tokens": [50364, 293, 4696, 309, 603, 976, 291, 439, 264, 281, 12, 33749, 300, 291, 658, 490, 300, 13532, 294, 264, 1036, 1243, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0893175566374366, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.018276594579219818}, {"id": 328, "seek": 111500, "start": 1119.0, "end": 1126.0, "text": " So this, yeah, has the context basically of what you are doing as a user at hand.", "tokens": [50564, 407, 341, 11, 1338, 11, 575, 264, 4319, 1936, 295, 437, 291, 366, 884, 382, 257, 4195, 412, 1011, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0893175566374366, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.018276594579219818}, {"id": 329, "seek": 111500, "start": 1126.0, "end": 1133.0, "text": " It's, I think, really kind of, yeah, an important step forward to make this useful,", "tokens": [50914, 467, 311, 11, 286, 519, 11, 534, 733, 295, 11, 1338, 11, 364, 1021, 1823, 2128, 281, 652, 341, 4420, 11, 51264], "temperature": 0.0, "avg_logprob": -0.0893175566374366, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.018276594579219818}, {"id": 330, "seek": 111500, "start": 1133.0, "end": 1136.0, "text": " because otherwise you're just getting the generic info that's in the LLM.", "tokens": [51264, 570, 5911, 291, 434, 445, 1242, 264, 19577, 13614, 300, 311, 294, 264, 441, 43, 44, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0893175566374366, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.018276594579219818}, {"id": 331, "seek": 111500, "start": 1136.0, "end": 1139.0, "text": " As I said, they hallucinate stuff all the time.", "tokens": [51414, 1018, 286, 848, 11, 436, 35212, 13923, 1507, 439, 264, 565, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0893175566374366, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.018276594579219818}, {"id": 332, "seek": 111500, "start": 1139.0, "end": 1142.0, "text": " They're much better at, like, taking information and summarizing it,", "tokens": [51564, 814, 434, 709, 1101, 412, 11, 411, 11, 1940, 1589, 293, 14611, 3319, 309, 11, 51714], "temperature": 0.0, "avg_logprob": -0.0893175566374366, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.018276594579219818}, {"id": 333, "seek": 111500, "start": 1142.0, "end": 1144.0, "text": " and that's, of course, what this does.", "tokens": [51714, 293, 300, 311, 11, 295, 1164, 11, 437, 341, 775, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0893175566374366, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.018276594579219818}, {"id": 334, "seek": 114400, "start": 1144.0, "end": 1150.0, "text": " I think it's much more reliable in that way, you know, vacation process, et cetera, et cetera.", "tokens": [50364, 286, 519, 309, 311, 709, 544, 12924, 294, 300, 636, 11, 291, 458, 11, 12830, 1399, 11, 1030, 11458, 11, 1030, 11458, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0753545397803897, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006477393209934235}, {"id": 335, "seek": 114400, "start": 1150.0, "end": 1156.0, "text": " So that's a couple of things we've been doing lately on this, as well as in the context yet.", "tokens": [50664, 407, 300, 311, 257, 1916, 295, 721, 321, 600, 668, 884, 12881, 322, 341, 11, 382, 731, 382, 294, 264, 4319, 1939, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0753545397803897, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006477393209934235}, {"id": 336, "seek": 114400, "start": 1156.0, "end": 1159.0, "text": " So that's our approach to AI.", "tokens": [50964, 407, 300, 311, 527, 3109, 281, 7318, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0753545397803897, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006477393209934235}, {"id": 337, "seek": 114400, "start": 1159.0, "end": 1163.0, "text": " I would really like to hear thoughts on that, and, like,", "tokens": [51114, 286, 576, 534, 411, 281, 1568, 4598, 322, 300, 11, 293, 11, 411, 11, 51314], "temperature": 0.0, "avg_logprob": -0.0753545397803897, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006477393209934235}, {"id": 338, "seek": 114400, "start": 1163.0, "end": 1166.0, "text": " I don't know what other projects are planning with this.", "tokens": [51314, 286, 500, 380, 458, 437, 661, 4455, 366, 5038, 365, 341, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0753545397803897, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006477393209934235}, {"id": 339, "seek": 114400, "start": 1166.0, "end": 1170.0, "text": " One of these will be giving a talk after mine.", "tokens": [51464, 1485, 295, 613, 486, 312, 2902, 257, 751, 934, 3892, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0753545397803897, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006477393209934235}, {"id": 340, "seek": 117000, "start": 1170.0, "end": 1177.0, "text": " But I know any feedback, questions, thoughts, fears, and anxieties?", "tokens": [50364, 583, 286, 458, 604, 5824, 11, 1651, 11, 4598, 11, 15649, 11, 293, 6739, 19084, 30, 50714], "temperature": 0.0, "avg_logprob": -0.1439169684609214, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.06922029703855515}, {"id": 341, "seek": 117000, "start": 1177.0, "end": 1179.0, "text": " Okay, so is this working?", "tokens": [50714, 1033, 11, 370, 307, 341, 1364, 30, 50814], "temperature": 0.0, "avg_logprob": -0.1439169684609214, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.06922029703855515}, {"id": 342, "seek": 117000, "start": 1179.0, "end": 1181.0, "text": " Can anybody confirm in the back?", "tokens": [50814, 1664, 4472, 9064, 294, 264, 646, 30, 50914], "temperature": 0.0, "avg_logprob": -0.1439169684609214, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.06922029703855515}, {"id": 343, "seek": 117000, "start": 1181.0, "end": 1184.0, "text": " Great, thank you very much.", "tokens": [50914, 3769, 11, 1309, 291, 588, 709, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1439169684609214, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.06922029703855515}, {"id": 344, "seek": 117000, "start": 1184.0, "end": 1188.0, "text": " So any thoughts, questions? Let's start here.", "tokens": [51064, 407, 604, 4598, 11, 1651, 30, 961, 311, 722, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1439169684609214, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.06922029703855515}, {"id": 345, "seek": 117000, "start": 1188.0, "end": 1192.0, "text": " So you said your screenshot showed that...", "tokens": [51264, 407, 291, 848, 428, 27712, 4712, 300, 485, 51464], "temperature": 0.0, "avg_logprob": -0.1439169684609214, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.06922029703855515}, {"id": 346, "seek": 117000, "start": 1192.0, "end": 1196.0, "text": " Yes, the screenshot showed that I need to double check the information that the assistant gave me.", "tokens": [51464, 1079, 11, 264, 27712, 4712, 300, 286, 643, 281, 3834, 1520, 264, 1589, 300, 264, 10994, 2729, 385, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1439169684609214, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.06922029703855515}, {"id": 347, "seek": 119600, "start": 1196.0, "end": 1201.0, "text": " Notice that it doesn't give me the reference to the emails that it was quoting from.", "tokens": [50364, 13428, 300, 309, 1177, 380, 976, 385, 264, 6408, 281, 264, 12524, 300, 309, 390, 41552, 490, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12266798202808087, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.043673280626535416}, {"id": 348, "seek": 119600, "start": 1201.0, "end": 1204.0, "text": " Is there a possibility to get that?", "tokens": [50614, 1119, 456, 257, 7959, 281, 483, 300, 30, 50764], "temperature": 0.0, "avg_logprob": -0.12266798202808087, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.043673280626535416}, {"id": 349, "seek": 119600, "start": 1204.0, "end": 1212.0, "text": " Currently not, but thinking of the way this works, I mean, I have one developer here.", "tokens": [50764, 19964, 406, 11, 457, 1953, 295, 264, 636, 341, 1985, 11, 286, 914, 11, 286, 362, 472, 10754, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12266798202808087, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.043673280626535416}, {"id": 350, "seek": 119600, "start": 1212.0, "end": 1217.0, "text": " They can interject, but I think that should actually be quite doable,", "tokens": [51164, 814, 393, 46787, 11, 457, 286, 519, 300, 820, 767, 312, 1596, 41183, 11, 51414], "temperature": 0.0, "avg_logprob": -0.12266798202808087, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.043673280626535416}, {"id": 351, "seek": 119600, "start": 1217.0, "end": 1220.0, "text": " because the way it works is it looks in this vector database", "tokens": [51414, 570, 264, 636, 309, 1985, 307, 309, 1542, 294, 341, 8062, 8149, 51564], "temperature": 0.0, "avg_logprob": -0.12266798202808087, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.043673280626535416}, {"id": 352, "seek": 119600, "start": 1220.0, "end": 1224.0, "text": " and gives that information to the LLM to then summarize and give you the answer.", "tokens": [51564, 293, 2709, 300, 1589, 281, 264, 441, 43, 44, 281, 550, 20858, 293, 976, 291, 264, 1867, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12266798202808087, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.043673280626535416}, {"id": 353, "seek": 122400, "start": 1224.0, "end": 1228.0, "text": " And, well, in the vector database, I guess it knows where it came from,", "tokens": [50364, 400, 11, 731, 11, 294, 264, 8062, 8149, 11, 286, 2041, 309, 3255, 689, 309, 1361, 490, 11, 50564], "temperature": 0.0, "avg_logprob": -0.17568005906774642, "compression_ratio": 1.452054794520548, "no_speech_prob": 0.047252871096134186}, {"id": 354, "seek": 122400, "start": 1228.0, "end": 1234.0, "text": " and therefore can then say what information was used to summarize that answer.", "tokens": [50564, 293, 4412, 393, 550, 584, 437, 1589, 390, 1143, 281, 20858, 300, 1867, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17568005906774642, "compression_ratio": 1.452054794520548, "no_speech_prob": 0.047252871096134186}, {"id": 355, "seek": 122400, "start": 1234.0, "end": 1238.0, "text": " So I would think this is possible, but I don't... I don't know.", "tokens": [50864, 407, 286, 576, 519, 341, 307, 1944, 11, 457, 286, 500, 380, 485, 286, 500, 380, 458, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17568005906774642, "compression_ratio": 1.452054794520548, "no_speech_prob": 0.047252871096134186}, {"id": 356, "seek": 122400, "start": 1238.0, "end": 1241.0, "text": " Yeah, I see a thumbs up. Excellent, okay.", "tokens": [51064, 865, 11, 286, 536, 257, 8838, 493, 13, 16723, 11, 1392, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17568005906774642, "compression_ratio": 1.452054794520548, "no_speech_prob": 0.047252871096134186}, {"id": 357, "seek": 122400, "start": 1241.0, "end": 1244.0, "text": " Any other questions, ideas?", "tokens": [51214, 2639, 661, 1651, 11, 3487, 30, 51364], "temperature": 0.0, "avg_logprob": -0.17568005906774642, "compression_ratio": 1.452054794520548, "no_speech_prob": 0.047252871096134186}, {"id": 358, "seek": 122400, "start": 1244.0, "end": 1246.0, "text": " Okay.", "tokens": [51364, 1033, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17568005906774642, "compression_ratio": 1.452054794520548, "no_speech_prob": 0.047252871096134186}, {"id": 359, "seek": 122400, "start": 1246.0, "end": 1249.0, "text": " Or are we going to do this?", "tokens": [51464, 1610, 366, 321, 516, 281, 360, 341, 30, 51614], "temperature": 0.0, "avg_logprob": -0.17568005906774642, "compression_ratio": 1.452054794520548, "no_speech_prob": 0.047252871096134186}, {"id": 360, "seek": 124900, "start": 1249.0, "end": 1253.0, "text": " Yes, for me, I am an e-aseptic.", "tokens": [50364, 1079, 11, 337, 385, 11, 286, 669, 364, 308, 12, 296, 5250, 299, 13, 50564], "temperature": 0.0, "avg_logprob": -0.3778453240027794, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.1891123354434967}, {"id": 361, "seek": 124900, "start": 1253.0, "end": 1256.0, "text": " There's something... some examples.", "tokens": [50564, 821, 311, 746, 485, 512, 5110, 13, 50714], "temperature": 0.0, "avg_logprob": -0.3778453240027794, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.1891123354434967}, {"id": 362, "seek": 124900, "start": 1256.0, "end": 1262.0, "text": " So it's good, something... when user is at the end,", "tokens": [50714, 407, 309, 311, 665, 11, 746, 485, 562, 4195, 307, 412, 264, 917, 11, 51014], "temperature": 0.0, "avg_logprob": -0.3778453240027794, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.1891123354434967}, {"id": 363, "seek": 124900, "start": 1262.0, "end": 1268.0, "text": " and he can correct what is said by the AI.", "tokens": [51014, 293, 415, 393, 3006, 437, 307, 848, 538, 264, 7318, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3778453240027794, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.1891123354434967}, {"id": 364, "seek": 124900, "start": 1268.0, "end": 1275.0, "text": " So an example for translation, I have the word in Dutch, Acad\u00e9mie de Sie.", "tokens": [51314, 407, 364, 1365, 337, 12853, 11, 286, 362, 264, 1349, 294, 15719, 11, 9740, 4011, 414, 368, 318, 414, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3778453240027794, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.1891123354434967}, {"id": 365, "seek": 127500, "start": 1275.0, "end": 1278.0, "text": " It's not universitaire, the translation in French.", "tokens": [50364, 467, 311, 406, 5950, 30914, 11, 264, 12853, 294, 5522, 13, 50514], "temperature": 0.0, "avg_logprob": -0.20678338380617517, "compression_ratio": 1.584, "no_speech_prob": 0.2097015231847763}, {"id": 366, "seek": 127500, "start": 1278.0, "end": 1282.0, "text": " It's personne issue des milieux acad\u00e9miques.", "tokens": [50514, 467, 311, 17219, 2734, 730, 1962, 35371, 5558, 4011, 4911, 13, 50714], "temperature": 0.0, "avg_logprob": -0.20678338380617517, "compression_ratio": 1.584, "no_speech_prob": 0.2097015231847763}, {"id": 367, "seek": 127500, "start": 1282.0, "end": 1286.0, "text": " And that's any... the translator or a consultant don't give this response.", "tokens": [50714, 400, 300, 311, 604, 485, 264, 35223, 420, 257, 24676, 500, 380, 976, 341, 4134, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20678338380617517, "compression_ratio": 1.584, "no_speech_prob": 0.2097015231847763}, {"id": 368, "seek": 127500, "start": 1286.0, "end": 1288.0, "text": " Yeah, but it's...", "tokens": [50914, 865, 11, 457, 309, 311, 485, 51014], "temperature": 0.0, "avg_logprob": -0.20678338380617517, "compression_ratio": 1.584, "no_speech_prob": 0.2097015231847763}, {"id": 369, "seek": 127500, "start": 1288.0, "end": 1295.0, "text": " So you have a control, they say, from the user, also from the citizen in general,", "tokens": [51014, 407, 291, 362, 257, 1969, 11, 436, 584, 11, 490, 264, 4195, 11, 611, 490, 264, 13326, 294, 2674, 11, 51364], "temperature": 0.0, "avg_logprob": -0.20678338380617517, "compression_ratio": 1.584, "no_speech_prob": 0.2097015231847763}, {"id": 370, "seek": 127500, "start": 1295.0, "end": 1299.0, "text": " so when the user has no power of the system.", "tokens": [51364, 370, 562, 264, 4195, 575, 572, 1347, 295, 264, 1185, 13, 51564], "temperature": 0.0, "avg_logprob": -0.20678338380617517, "compression_ratio": 1.584, "no_speech_prob": 0.2097015231847763}, {"id": 371, "seek": 127500, "start": 1299.0, "end": 1303.0, "text": " You can't check a human translator either, though, unless you know the language", "tokens": [51564, 509, 393, 380, 1520, 257, 1952, 35223, 2139, 11, 1673, 11, 5969, 291, 458, 264, 2856, 51764], "temperature": 0.0, "avg_logprob": -0.20678338380617517, "compression_ratio": 1.584, "no_speech_prob": 0.2097015231847763}, {"id": 372, "seek": 130300, "start": 1303.0, "end": 1306.0, "text": " at that point you didn't need them in the first place.", "tokens": [50364, 412, 300, 935, 291, 994, 380, 643, 552, 294, 264, 700, 1081, 13, 50514], "temperature": 0.0, "avg_logprob": -0.19570536532644497, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.26027849316596985}, {"id": 373, "seek": 130300, "start": 1306.0, "end": 1312.0, "text": " So, yeah, you have to use this stuff in a skeptical way, but then... yeah.", "tokens": [50514, 407, 11, 1338, 11, 291, 362, 281, 764, 341, 1507, 294, 257, 28601, 636, 11, 457, 550, 485, 1338, 13, 50814], "temperature": 0.0, "avg_logprob": -0.19570536532644497, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.26027849316596985}, {"id": 374, "seek": 130300, "start": 1312.0, "end": 1317.0, "text": " Yes, and the other thing is about consuming energy.", "tokens": [50814, 1079, 11, 293, 264, 661, 551, 307, 466, 19867, 2281, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19570536532644497, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.26027849316596985}, {"id": 375, "seek": 130300, "start": 1317.0, "end": 1323.0, "text": " So it was an emission in the RTBF about consuming of energy, of shaggivity.", "tokens": [51064, 407, 309, 390, 364, 29513, 294, 264, 21797, 33, 37, 466, 19867, 295, 2281, 11, 295, 402, 559, 70, 4253, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19570536532644497, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.26027849316596985}, {"id": 376, "seek": 130300, "start": 1323.0, "end": 1325.0, "text": " It was hard.", "tokens": [51364, 467, 390, 1152, 13, 51464], "temperature": 0.0, "avg_logprob": -0.19570536532644497, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.26027849316596985}, {"id": 377, "seek": 130300, "start": 1325.0, "end": 1328.0, "text": " Yeah, so the amount of energy that these models use is big.", "tokens": [51464, 865, 11, 370, 264, 2372, 295, 2281, 300, 613, 5245, 764, 307, 955, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19570536532644497, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.26027849316596985}, {"id": 378, "seek": 130300, "start": 1328.0, "end": 1331.0, "text": " That's, by the way, one of the reasons I think they should be open source,", "tokens": [51614, 663, 311, 11, 538, 264, 636, 11, 472, 295, 264, 4112, 286, 519, 436, 820, 312, 1269, 4009, 11, 51764], "temperature": 0.0, "avg_logprob": -0.19570536532644497, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.26027849316596985}, {"id": 379, "seek": 133100, "start": 1331.0, "end": 1335.0, "text": " is that the researchers who do stuff companies aren't interested in", "tokens": [50364, 307, 300, 264, 10309, 567, 360, 1507, 3431, 3212, 380, 3102, 294, 50564], "temperature": 0.0, "avg_logprob": -0.1868722915649414, "compression_ratio": 1.52803738317757, "no_speech_prob": 0.23193544149398804}, {"id": 380, "seek": 133100, "start": 1335.0, "end": 1339.0, "text": " can try to optimize them and make them run with less energy.", "tokens": [50564, 393, 853, 281, 19719, 552, 293, 652, 552, 1190, 365, 1570, 2281, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1868722915649414, "compression_ratio": 1.52803738317757, "no_speech_prob": 0.23193544149398804}, {"id": 381, "seek": 133100, "start": 1339.0, "end": 1341.0, "text": " Yeah.", "tokens": [50764, 865, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1868722915649414, "compression_ratio": 1.52803738317757, "no_speech_prob": 0.23193544149398804}, {"id": 382, "seek": 133100, "start": 1341.0, "end": 1345.0, "text": " Hi. I think another good use case for all these...", "tokens": [50864, 2421, 13, 286, 519, 1071, 665, 764, 1389, 337, 439, 613, 485, 51064], "temperature": 0.0, "avg_logprob": -0.1868722915649414, "compression_ratio": 1.52803738317757, "no_speech_prob": 0.23193544149398804}, {"id": 383, "seek": 133100, "start": 1345.0, "end": 1354.0, "text": " If we combine these features, that would mean that we could have a super accessible environment,", "tokens": [51064, 759, 321, 10432, 613, 4122, 11, 300, 576, 914, 300, 321, 727, 362, 257, 1687, 9515, 2823, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1868722915649414, "compression_ratio": 1.52803738317757, "no_speech_prob": 0.23193544149398804}, {"id": 384, "seek": 133100, "start": 1354.0, "end": 1358.0, "text": " because if someone is blind or nearly blind,", "tokens": [51514, 570, 498, 1580, 307, 6865, 420, 6217, 6865, 11, 51714], "temperature": 0.0, "avg_logprob": -0.1868722915649414, "compression_ratio": 1.52803738317757, "no_speech_prob": 0.23193544149398804}, {"id": 385, "seek": 135800, "start": 1358.0, "end": 1366.0, "text": " people could use all this text to speech if someone has autism, ADHD, whatever.", "tokens": [50364, 561, 727, 764, 439, 341, 2487, 281, 6218, 498, 1580, 575, 21471, 11, 38680, 11, 2035, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12315506026858375, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.10471522808074951}, {"id": 386, "seek": 135800, "start": 1366.0, "end": 1374.0, "text": " You could try to find a shorter version, an easier, understandable version of a text or whatever,", "tokens": [50764, 509, 727, 853, 281, 915, 257, 11639, 3037, 11, 364, 3571, 11, 25648, 3037, 295, 257, 2487, 420, 2035, 11, 51164], "temperature": 0.0, "avg_logprob": -0.12315506026858375, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.10471522808074951}, {"id": 387, "seek": 135800, "start": 1374.0, "end": 1378.0, "text": " and combining this would help, I think.", "tokens": [51164, 293, 21928, 341, 576, 854, 11, 286, 519, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12315506026858375, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.10471522808074951}, {"id": 388, "seek": 135800, "start": 1378.0, "end": 1381.0, "text": " That is awesome. I'm going to add that to my slides right now,", "tokens": [51364, 663, 307, 3476, 13, 286, 478, 516, 281, 909, 300, 281, 452, 9788, 558, 586, 11, 51514], "temperature": 0.0, "avg_logprob": -0.12315506026858375, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.10471522808074951}, {"id": 389, "seek": 135800, "start": 1381.0, "end": 1384.0, "text": " but I am completely making the laptop slow now.", "tokens": [51514, 457, 286, 669, 2584, 1455, 264, 10732, 2964, 586, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12315506026858375, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.10471522808074951}, {"id": 390, "seek": 138400, "start": 1384.0, "end": 1389.0, "text": " That's a really good point. Accessibility is a really important benefit.", "tokens": [50364, 663, 311, 257, 534, 665, 935, 13, 17166, 2841, 307, 257, 534, 1021, 5121, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1450213704790388, "compression_ratio": 1.4143646408839778, "no_speech_prob": 0.03604475408792496}, {"id": 391, "seek": 138400, "start": 1389.0, "end": 1393.0, "text": " Actually, hint to the developer, bring it up in the team.", "tokens": [50614, 5135, 11, 12075, 281, 264, 10754, 11, 1565, 309, 493, 294, 264, 1469, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1450213704790388, "compression_ratio": 1.4143646408839778, "no_speech_prob": 0.03604475408792496}, {"id": 392, "seek": 138400, "start": 1393.0, "end": 1395.0, "text": " Maybe we can already work on that.", "tokens": [50814, 2704, 321, 393, 1217, 589, 322, 300, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1450213704790388, "compression_ratio": 1.4143646408839778, "no_speech_prob": 0.03604475408792496}, {"id": 393, "seek": 138400, "start": 1402.0, "end": 1404.0, "text": " Yeah, any more?", "tokens": [51264, 865, 11, 604, 544, 30, 51364], "temperature": 0.0, "avg_logprob": -0.1450213704790388, "compression_ratio": 1.4143646408839778, "no_speech_prob": 0.03604475408792496}, {"id": 394, "seek": 138400, "start": 1404.0, "end": 1408.0, "text": " Yeah, just a question on the REC approach that you were describing before.", "tokens": [51364, 865, 11, 445, 257, 1168, 322, 264, 497, 8140, 3109, 300, 291, 645, 16141, 949, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1450213704790388, "compression_ratio": 1.4143646408839778, "no_speech_prob": 0.03604475408792496}, {"id": 395, "seek": 140800, "start": 1408.0, "end": 1415.0, "text": " Do you have any figure that you can share to which extent you tried the retrieval of the vector?", "tokens": [50364, 1144, 291, 362, 604, 2573, 300, 291, 393, 2073, 281, 597, 8396, 291, 3031, 264, 19817, 3337, 295, 264, 8062, 30, 50714], "temperature": 0.0, "avg_logprob": -0.24930850278983996, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.8060103058815002}, {"id": 396, "seek": 140800, "start": 1415.0, "end": 1417.0, "text": " Sorry, I did not hear the question.", "tokens": [50714, 4919, 11, 286, 630, 406, 1568, 264, 1168, 13, 50814], "temperature": 0.0, "avg_logprob": -0.24930850278983996, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.8060103058815002}, {"id": 397, "seek": 140800, "start": 1417.0, "end": 1421.0, "text": " So when you were describing the rag, the retrieval of the augmented...", "tokens": [50814, 407, 562, 291, 645, 16141, 264, 17539, 11, 264, 19817, 3337, 295, 264, 36155, 485, 51014], "temperature": 0.0, "avg_logprob": -0.24930850278983996, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.8060103058815002}, {"id": 398, "seek": 140800, "start": 1421.0, "end": 1423.0, "text": " The green, the colors, yes.", "tokens": [51014, 440, 3092, 11, 264, 4577, 11, 2086, 13, 51114], "temperature": 0.0, "avg_logprob": -0.24930850278983996, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.8060103058815002}, {"id": 399, "seek": 140800, "start": 1423.0, "end": 1427.0, "text": " No, the rag. When you're retrieving the vectors from the vector DB.", "tokens": [51114, 883, 11, 264, 17539, 13, 1133, 291, 434, 19817, 798, 264, 18875, 490, 264, 8062, 26754, 13, 51314], "temperature": 0.0, "avg_logprob": -0.24930850278983996, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.8060103058815002}, {"id": 400, "seek": 140800, "start": 1427.0, "end": 1428.0, "text": " Right.", "tokens": [51314, 1779, 13, 51364], "temperature": 0.0, "avg_logprob": -0.24930850278983996, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.8060103058815002}, {"id": 401, "seek": 140800, "start": 1428.0, "end": 1432.0, "text": " So can you give us some figures on to which extent you tried that?", "tokens": [51364, 407, 393, 291, 976, 505, 512, 9624, 322, 281, 597, 8396, 291, 3031, 300, 30, 51564], "temperature": 0.0, "avg_logprob": -0.24930850278983996, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.8060103058815002}, {"id": 402, "seek": 143200, "start": 1432.0, "end": 1437.0, "text": " Talk to somebody, not me, in one of these, who knows the technical part there,", "tokens": [50364, 8780, 281, 2618, 11, 406, 385, 11, 294, 472, 295, 613, 11, 567, 3255, 264, 6191, 644, 456, 11, 50614], "temperature": 0.0, "avg_logprob": -0.13828740949216095, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.5820713639259338}, {"id": 403, "seek": 143200, "start": 1437.0, "end": 1442.0, "text": " and I'm not even sure we have somebody right here at the moment with that. Sorry.", "tokens": [50614, 293, 286, 478, 406, 754, 988, 321, 362, 2618, 558, 510, 412, 264, 1623, 365, 300, 13, 4919, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13828740949216095, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.5820713639259338}, {"id": 404, "seek": 143200, "start": 1442.0, "end": 1443.0, "text": " Okay.", "tokens": [50864, 1033, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13828740949216095, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.5820713639259338}, {"id": 405, "seek": 143200, "start": 1443.0, "end": 1447.0, "text": " And we're out of time. I'm afraid. So this is probably it then.", "tokens": [50914, 400, 321, 434, 484, 295, 565, 13, 286, 478, 4638, 13, 407, 341, 307, 1391, 309, 550, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13828740949216095, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.5820713639259338}, {"id": 406, "seek": 143200, "start": 1447.0, "end": 1449.0, "text": " Somebody wants their microphone back.", "tokens": [51114, 13463, 2738, 641, 10952, 646, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13828740949216095, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.5820713639259338}, {"id": 407, "seek": 143200, "start": 1449.0, "end": 1451.0, "text": " Alright, thank you all.", "tokens": [51214, 2798, 11, 1309, 291, 439, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13828740949216095, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.5820713639259338}, {"id": 408, "seek": 143200, "start": 1451.0, "end": 1453.0, "text": " Thank you very much, Josh.", "tokens": [51314, 1044, 291, 588, 709, 11, 9785, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13828740949216095, "compression_ratio": 1.541062801932367, "no_speech_prob": 0.5820713639259338}], "language": "en"}