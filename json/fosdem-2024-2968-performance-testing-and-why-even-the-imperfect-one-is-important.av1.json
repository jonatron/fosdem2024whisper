{"text": " Hello everyone. You have two more minutes. My name is Andre. I work for... Okay. I've worked for Red Hat for several years now as quality engineering. And today's talk is really about performance testing, but it's not about the testing itself. It's more than why we should do it. You have six minutes in class. Okay. You're starting early, just so you know. Yeah. Okay. So it's more why we should do it and why there are benefits in it, even if you do it wrongly or imperfectly. So that's the main point of the talk for today. So first thing first, why should we do it? What are the benefits for us doing the performance testing, even if we don't have isolated environment and all this kind of stuff? Well, for me, the main benefit is that even if you don't have the environment you would want, you can still find the bottlenecks in your application or whatever you are testing. And you still can optimize it even if you don't have everything ready because the truth is that the performance testing is quite expensive. And for the good one, I don't think that there are the companies that will give you the resources that you need to do it perfectly. So that's for me the main option or the main reason why to do it. And for me, the second most important is that you will gain knowledge that you need or you will obtain the knowledge about the product itself because you will suddenly see things that you cannot see, even if you normally deploy things and everything. You will see the little thingies that are happening here and there. And the information you gain are quite nice to get. So that's probably the most, the points that you should look for if you are thinking about the performance testing. This is actually what it will gain from it. So this is only in my opinion, like you will see a lot of papers about the performance testing and all the things that you have to take care of. Like on the GitHub, I know about two or three papers that have like 40 pages about the performance testing and all the criteria that you have to fill. In my opinion, there are like two variants of the performance testing. And first is measurement and second is testing. For me, the measurement is really the thing that you are looking for numbers. And you need those numbers for, I would say, legal reasons or anything what you have to declare for your customer. You will say that, for example, for us, I work on the division. Like if we would want to say that this connector is actually able to do 30k per second, we would need some kind of a proof that we can do it. And getting this proof is like, it's very complicated and you have to do it in very specific ways. And even if you have everything, it's not quite, it's not always acceptable. But the second part or second variation is just testing. And for me, the testing, the testing is really just finding the bottlenecks in your product. And I think that's even more, like the testing is even more important because there you will find all the bottlenecks and you can optimize your application really. And you can see the flaws in your call because you cannot see these things when you run it regularly and you don't have everything around the application tuned up so you don't throttle your application to the maximum. These things usually happen when you go over the top or near the maximum, near the max. So, yeah, this is in my opinion two ways how to do the performance testing or two variants of the performance testing. What is not really optimal about the testing, which I was talking about, just finding the bottlenecks, not the number, you need massive monitoring and I will say more about it in the talk, but that's like main disadvantage of it that most of the time you will go around tracing, monitoring, metrics and you will find some stuff that will really give you hard times figuring it out because you are going for performance and you are speaking in milliseconds, yeah? But most of the things that are used for monitoring are not really prepared to handle you for one minute second. They think that it's okay to scrape, for example, metrics by 10 seconds and like this will give you massive headaches during the time. I think I have already somehow gathered those, but the goal of the performance testing is, as I said, find the bottlenecks, but they are much more to it because, for example, the load types, you know, even if someone of you had crossed the performance testing, the main point all of the guys are talking about is what kind of load we are going to do, how we are going to do it to make it reproducible, you know? Because that's like a problem because for some application you put constant loads to them. Let's say you are going 10k per second, some requests for the API for one hour. And like that could be fine, but we all saw that some websites or, for example, the systems that you are buying tickets for concerts, these kind of things need peak loads. You know, you are going low for 5k per second and suddenly you spin it up for 100k per second or something like that. So you really need something that will generate the load for you and do it reproducible. You need to have the same load so you can repeat the testing for a couple of times and be consistent in that because otherwise you will find all the other things except the flux in your cold. So the main problems during the performance that you will find. I have said that you don't need the isolated environment to do it. And that's true. We don't have isolated environment in our team when we do performance testing. But if you don't have entirely isolated environment, you need to know your environment pretty well. You have to know your latencies, you have to know all the hardware specs and these kind of things. You really need all of the information because if you see some very specific things happen during the test which are not common, you then can somehow put the puzzle together with all the information you get from the environment and you can somehow at least, I would say, decrease the number of stupid mistakes that you will have to gather around. The next thing that is very important is to have the monitoring which I have said already. You need all the metrics you can get because before that you don't even know what kind of the information would be valuable for you when you start doing it, but you will need all of it. If you can get rid and you will surely use it, if you don't gather those metrics and then you will figure out you need it, you cannot get them from the past. So really the thing is gather everything you can and it will be fine. And the last thing for this is you need to tune up all the systems that you are dependent on. For us, we are working on databases and we cannot really throttle up our product if the database isn't optimized to the hardware it's running because if the database is not throttling, we are not throttling. So you basically need to have everything on the high spec there to not bottleneck your application. So that's one of the main points because on some things it's quite problematic to tune it up. I quite like this quote because it's all about the metrics and if you have them, it's fine and it's nice. If you don't have them, it's massive problems. So I think that's really the quote that you should be looking for. So again, monitoring. I have already talked about the problem with the scraping. So let's say we have used Prometheus mostly and the maximum what you can get from Prometheus is one second scraping. So that's fine for information causes but not for the performance metrics because the things happen during the milliseconds. Maybe 10 milliseconds would be enough but one second is really you are losing a lot of information and later on I have the example of what you could see when the scrapers are not fast enough. And there's like massive problem because not every scraper is or I would say there are no scrapers that can do it really fast. So you probably have to implement it and we are working on that actually. So that's the main problem. And the second problem is that you will end up with having a lot of the systems in the field because you need hardware metrics. You need JMX metrics and I don't know what other. It really depends on your application. But for us, we needed hardware metrics, JMX metrics and some metrics from our test suite. And these three things actually all the different outputs. You know, we have used net data for the hardware metrics. It's like really nice tool open source fast everything nice. But you cannot import JMX metrics to net data. And you net data has also one problem that you cannot import like anything would happen in the past because it's strictly hard coded for now. So that's the problem, you know, and then you will say yes. Okay. So I cannot have JMX metrics to the net data. So I'll add Prometheus, you know, that's fine. Okay. So you have now net data and Prometheus. Sorry. Well, and then you will continue because you still have all at least in our expertise. We still need that someplace to store the metrics from our test suite and you can just import it everywhere. So then you happen that you will deploy the Postgres because you can use Postgres as backup for the Prometheus for the data storage. So now we have net data, Postgres and also Prometheus. And last but not least, you will add the Grafana because you need to visualize it, you know, and getting all those things in shape that you have like massive monitoring. You know, everything can go wrong. So if you can use the least amount of the tools that you can use, it's better because once you have too many, it's nightmare to somehow get that in shape for the whole time. Yeah. So with the performance testing, you are not really looking for the numbers. Numbers are not important in this case because you don't want to see the throughput is like this or like that. You need to see the trends in the graphs because there you can see if you are constantly slowing down or if you are going like optimizing your way. So really you have to look for the patterns in graphs and the trends in the graphs. I have the example for our testing that I will show you the patterns which we have found. But before that, just our system under test was the BZM. I don't know if you guys know the BZM, but it's effectively changed that I capture streaming, which means that we sit on the top of the database and we scan the transaction logs and sense all the events that happen there to the Kafka. We are effectively running in Kafka Connect runtime, which makes the performance testing even more juicy, I would say, because the runtime is not ours. So it's a little tricky. So that's our system under test. And this is the first example that I have put on. The image on the graph on the top is basically our process duration. And there are two things that you can see on the graph. We are effectively most of the time we are oscillating between some values around 200 or 170 to 220. And that's entirely fine. That's actually what we want to see if you are looking for some responses. You need to oscillate around some value like a sinus graph or something like that. But what is not actually nice is this on the star. Where we are constantly getting slower and slower and slower. And we have some peaks there, which have these peaks are don't have a reason to be there because the data are the same all the time. So this is most likely the flaw in the code. There is something happening, what shouldn't be happening. And it can be the database flashing out to the score. It can be basically anything, but you know that there might be a problem. You have all other metrics. You can have metrics from the databases that will show you that flashing was happening or anything. But this is what you have to look for. These, it will certainly be different for your application than ours. You will have to define what you are looking for on the star. But that's the main thing. And the more funny example is this. No, here. This is Jmx metrics from our, from Libyseum. And it shows you the size of the queue. Internal queue of the Bizm. And that basically means that from the database you are reading to internal queue. So once the queue is zero, you are not reading. You know, but we are still processing, right? So there should, there is some mistake. And this is actually the problem with the scrapers. Because if the scraper takes each second, the database is pretty fast and it can empty up the, it can empty the queue during that time. And if the scraper hits the right time, it will give you zero. You know, so from this until the end, the graph is all wrong. It's not true. And it's all because of the speed of the scraper. Because it basically hit the wrong time. So that is the thing that you have to be worried about because this will happen surely. And these are some other graphs. These are, I would say, more wild. I would say it's from the start of our playing with the performance. But the top one is also, it's pretty cool. It's not that constant as it was for the previous one. But it's still in some borders, you know, we are somehow oscillating, but there is not really clear way. But the queue size is okay now. You can see because you have some data there, but not zero. So there's an issue with the scrapers, as I said. And this is actually the thing that you will have to look for the patterns. Really important in graphs. You can see all the different ones and there are a lot of papers on the Internet that you can find. What to look for on your specific application. So, yeah, but not look to the numbers. Numbers don't say you anything. You can usually get the higher numbers if you change up the hardware that you are running on. But if you can optimize it on the some hardware that you have, you will surely get the big numbers on almost anywhere. So some tips and tricks for me. During the way that we have started playing with the performance, we have developed a lot of tools. First is the database manipulation tool, which is effectively giving you a Json API. And just with the Json API, you can create DML for almost any databases. We have now probably MySQL, Postgres, and Oracle there. So it's just you don't need to have a lot of different JDBC connectors in your code. We'll just deploy this and it will take care of it. We have also implemented some kind of the load generator that can generate you. Load for, I would say, constant load, P-codes and all this kind of stuff. We have also some automation and the other, like MySQL auto-tune, that's it. We are pretty proud of that because it can basically tune your MySQL to the whole VM that you are running it or physical machine. And you would say that it's easy, but it's not. You know it's hard when you look on the seven or eight page of the Google. You know, in this phase, you know, we are probably not on the good shape and this is one of the things. So please take a look if you are working on MySQL. We have some counting of the parameters for the database there. And it will save you a lot of time if you want to tune up your database. We have spent the time for you. And secondly, we have implemented the metadata to Prometheus Creper. So you can get rid of one, one struggle point in your monitoring environment. And we are also starting working on the fast Creper for the, for your monitoring, for our monitoring stack. But it's not done yet because it's quite more complicated than before. So, but yeah, please take a look. I will have the links on our GitHub and everything. It's all open source. So you can just also add some code there if you want. Yeah, so, so I have started quite early on than I should. So I have some time now then. But okay, we can just summarize everything now. And I hope for some discussion before you guys who done performance testing. So for me, don't be scared around the performance testing. He's not like some, some monster. People are mostly just like creating the monster from it. But if you don't need that for some legal options or anything, it's fine. You can play with it. It's funny. You will gain a lot of knowledge about your, about your product on that. And especially if you are QE, I mean, a lot of QE folks don't have the necessary knowledge about the product itself. And this helps a lot to get through everything because in the end, you will, you will just go through the code and look for, for the mistakes or something like that. So that really helps a lot. So, gather all the metrics you can. And well, we are also writing our blog and all the repositories will be on the other side before the two, before the two links. So I would be happy to hear from you, you know, like repository or organization before the two links. And yeah, that's probably it for, for my talk. As I said, I have started a lot earlier. So thank you very much for listening to me. Please do have some questions. Yeah. So my question would be, so what kind of experience do you have in your complex system? And then you see something happens there. And say, okay, here, here's the latest EP or something like that. Which experience do you have with, let's say, find the cause of the problem? Cause, so when something happens randomly, you will see it with wrap and say, okay, something happens there. And that's maybe annoying, especially when it happens, happens randomly. So what kind of strategies you are using then when you know, okay, there is something to find cause of the problem in the complex chain. Yeah, so this is, okay. So, so the question was actually, if there are some, some changes in the environment, some like latency things or everything, how we can deal with that and how we can find the causes of the problems. Yeah, so surely this is the main problem of the whole performance testing outside of the like isolating and, you know, well, you need the metrics from everything because then you somehow at least it will help you to, to get all the things in the right timeline and you can see the need and picks what could happen. But if it's like something that is really bad, you can find it usually because it will mostly, it will just disappear in all the logs because it can be something like if you have the smaller machines, I have to write once on the, some microchips, you know, it was funny thing that you fill up your TCP queue. You cannot find that anywhere in the world. So at that case, you will just repeat the testing, even if it's take long and you will see if same thing happens or not. I don't have any other like recommendation for that because this is like really main problem if you are doing it outside of the ideal environment. This surely will face this, but mostly it's not happening that often, I would say, because you can have observation for and tracing for a lot of things and most of the times you can like colorate those things together. So you exactly know what is wrong, especially for the network. You can get a lot of network traffic, like, how is the word? You can see all the traffic and what is going on, especially on one line. So then you can usually put those graphs together and you know at the time. If that is okay, answer for you. Yeah, yeah, yeah. Thanks for the talk. I was just wondering that how to use the traces, analyzing the traces, because I've seen that you mentioned metrics and traces. Sorry, can you speak more louder? Oh, yeah. Can you hear me now? Yes. Yeah, I was wondering how do you use the traces for performance testing, because when you collect the traces, how do you deal with the sampling of the traces? And if you miss something because the sampling is bad or you are not sampling everything, maybe you have to infer something from the metrics and the traces, I was wondering how do you deal with the traces? And if you use distributed tracing in a large project like collecting all this kind of stuff. I'm not sure. I understand the question. The question actually is, I've seen that you're using collecting the metrics and then you are analyzing the metrics. Yeah. And what about the traces? Yeah, so, well, the business does not have really that amount of traces that we could get from it. We have mostly like JMX metrics, you know, from the Java environment. So that's for us what we analyze. And I'm not sure how can I answer more for the questions. So I'm sorry. We can discuss it later. I'm coming to you. Okay, so my question is about the long running tests. Sometimes the performance validation is visible only after long run, for example, one week, couple of weeks and sometimes even more. So how do you address this in your process or how do you recommend to address this problem? Yes. So for this, actually colleagues of mine as part of our open source organization, they are also developing the long running cluster environment, something like that is like, because, you know, having a long running thing is complicated on itself because you have to manage it a lot, especially on OpenShift or Kubernetes or these kind of things is like little problematic before the upgrades and this kind of stuff. So we have not dealt with that yet, but we are planning that once we are okay, that we manage that we have everything prepared for like databases and everything, we want to get the up running on the long running clusters and like regularly doing the performance tests over, I would say a month or something like that or a week, it usually is enough, even especially when you put all the numbers to the low ranges for the retention for the memory and all this kind of stuff. It doesn't take too long to fill everything when you will start to see the retention and flashes and everything. So yeah, that's our plan, but we haven't done it yet. So yeah, but if you are interested in that, you should definitely look in the hub that we have in the repository because it could be useful. Do you have any tips for running performance tests in the cloud? Because for me that's quite the opposite of dedicated test runners. But when the software runs finally in the cloud you should probably also performance test it there. It's a problem. A big one. We have tried it and it is so inconsistent. The results are so all over the place. If you run, you have two same clusters, Kubernetes, OpenShifting doesn't matter actually, and you run the tests on the same clusters at the same time, clusters are in different zones on AWS, you will get entirely different results because all the load balancers and these kind of stuff, it really, you know, if you have only internal communication on the cluster, we found anything from the outside. It could be actually doable, I think, but if you have any communication during the test that is going outside the cluster and it has to go through the load balancers and these kind of stuff, I think that's not doable in any way because it's like you don't know what latency we will have for this kind of the request and travels. So I think that that would be like really problematic, but if you could mock up the external communication with just some internal endpoint, it should be quite okay-ish, I would say, but you will not get like really good results from that, I think, even if you try more and more. But I think there are some Kubernetes, some special Kubernetes builds that should be used for these kind of measurements, but I have never actually tried it, so I cannot recommend it when I try it, when I try it, but yeah, this is definitely a problem. Okay, so I have more questions? No, we have a few more minutes for questions. Come on. Otherwise, I'm going to ask you to, you know, to move your seats. Wait, wait. You said you would want to have a very small statement developed, so in the milliseconds ago. Yeah. So doesn't that create problems on their own, something like noisy neighbors and so on? Yes, it does. It does. Right, but... How big of a problem that is? Well, that's the thing. We are really thinking about writing some scraper that is fast enough for this, and yes, you will probably generate some problems during the way, especially if you would like to send the metrics directly to the Prometheus every millisecond. You will probably fill up the network line or the TCP stack or whatever, because it's really fast. So you will... It will strongly depend on the machine that you are running and if you have space there, if you have a lot of RAM, you could actually batch all the metrics and send it like one package after the test is done. But yes, that's actually what we are now fighting with and we are trying to figure out how we are going to aim for this. But mostly we are thinking that we will do somehow a configurable scraper that will either batch the request or send them or something like that. I cannot say you because we haven't tried it, actually, what problems it makes, especially with batching, because I have counted it up and metrics aren't small, actually. So it will take a lot of place in the memory. So we will have to try it and somehow figure it out. But before the fast scraper, it will give you really headaches because you will try to find something and fight something and you will spend 20 times debugging it and then you will find that the scraper hit it the wrong time every time. So we have to deal with this in some way. But it will be hard and problematic. I think we have time for one last question. No one? Tough crowd. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.0, "text": " Hello everyone.", "tokens": [50364, 2425, 1518, 13, 50614], "temperature": 0.0, "avg_logprob": -0.3273804355675066, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.6271665692329407}, {"id": 1, "seek": 0, "start": 5.0, "end": 8.0, "text": " You have two more minutes.", "tokens": [50614, 509, 362, 732, 544, 2077, 13, 50764], "temperature": 0.0, "avg_logprob": -0.3273804355675066, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.6271665692329407}, {"id": 2, "seek": 0, "start": 8.0, "end": 10.0, "text": " My name is Andre.", "tokens": [50764, 1222, 1315, 307, 20667, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3273804355675066, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.6271665692329407}, {"id": 3, "seek": 0, "start": 10.0, "end": 12.0, "text": " I work for...", "tokens": [50864, 286, 589, 337, 485, 50964], "temperature": 0.0, "avg_logprob": -0.3273804355675066, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.6271665692329407}, {"id": 4, "seek": 0, "start": 12.0, "end": 16.0, "text": " Okay.", "tokens": [50964, 1033, 13, 51164], "temperature": 0.0, "avg_logprob": -0.3273804355675066, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.6271665692329407}, {"id": 5, "seek": 0, "start": 16.0, "end": 20.0, "text": " I've worked for Red Hat for several years now as quality engineering.", "tokens": [51164, 286, 600, 2732, 337, 4477, 15867, 337, 2940, 924, 586, 382, 3125, 7043, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3273804355675066, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.6271665692329407}, {"id": 6, "seek": 0, "start": 20.0, "end": 24.0, "text": " And today's talk is really about performance testing,", "tokens": [51364, 400, 965, 311, 751, 307, 534, 466, 3389, 4997, 11, 51564], "temperature": 0.0, "avg_logprob": -0.3273804355675066, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.6271665692329407}, {"id": 7, "seek": 0, "start": 24.0, "end": 26.0, "text": " but it's not about the testing itself.", "tokens": [51564, 457, 309, 311, 406, 466, 264, 4997, 2564, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3273804355675066, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.6271665692329407}, {"id": 8, "seek": 2600, "start": 26.0, "end": 30.0, "text": " It's more than why we should do it.", "tokens": [50364, 467, 311, 544, 813, 983, 321, 820, 360, 309, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2053328893517935, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.2150198221206665}, {"id": 9, "seek": 2600, "start": 30.0, "end": 32.0, "text": " You have six minutes in class.", "tokens": [50564, 509, 362, 2309, 2077, 294, 1508, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2053328893517935, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.2150198221206665}, {"id": 10, "seek": 2600, "start": 32.0, "end": 33.0, "text": " Okay.", "tokens": [50664, 1033, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2053328893517935, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.2150198221206665}, {"id": 11, "seek": 2600, "start": 33.0, "end": 35.0, "text": " You're starting early, just so you know.", "tokens": [50714, 509, 434, 2891, 2440, 11, 445, 370, 291, 458, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2053328893517935, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.2150198221206665}, {"id": 12, "seek": 2600, "start": 35.0, "end": 37.0, "text": " Yeah.", "tokens": [50814, 865, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2053328893517935, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.2150198221206665}, {"id": 13, "seek": 2600, "start": 37.0, "end": 39.0, "text": " Okay.", "tokens": [50914, 1033, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2053328893517935, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.2150198221206665}, {"id": 14, "seek": 2600, "start": 39.0, "end": 46.0, "text": " So it's more why we should do it and why there are benefits in it,", "tokens": [51014, 407, 309, 311, 544, 983, 321, 820, 360, 309, 293, 983, 456, 366, 5311, 294, 309, 11, 51364], "temperature": 0.0, "avg_logprob": -0.2053328893517935, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.2150198221206665}, {"id": 15, "seek": 2600, "start": 46.0, "end": 49.0, "text": " even if you do it wrongly or imperfectly.", "tokens": [51364, 754, 498, 291, 360, 309, 2085, 356, 420, 26714, 356, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2053328893517935, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.2150198221206665}, {"id": 16, "seek": 2600, "start": 49.0, "end": 54.0, "text": " So that's the main point of the talk for today.", "tokens": [51514, 407, 300, 311, 264, 2135, 935, 295, 264, 751, 337, 965, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2053328893517935, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.2150198221206665}, {"id": 17, "seek": 5400, "start": 54.0, "end": 58.0, "text": " So first thing first, why should we do it?", "tokens": [50364, 407, 700, 551, 700, 11, 983, 820, 321, 360, 309, 30, 50564], "temperature": 0.0, "avg_logprob": -0.11236593092995129, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.1208537146449089}, {"id": 18, "seek": 5400, "start": 58.0, "end": 63.0, "text": " What are the benefits for us doing the performance testing,", "tokens": [50564, 708, 366, 264, 5311, 337, 505, 884, 264, 3389, 4997, 11, 50814], "temperature": 0.0, "avg_logprob": -0.11236593092995129, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.1208537146449089}, {"id": 19, "seek": 5400, "start": 63.0, "end": 68.0, "text": " even if we don't have isolated environment and all this kind of stuff?", "tokens": [50814, 754, 498, 321, 500, 380, 362, 14621, 2823, 293, 439, 341, 733, 295, 1507, 30, 51064], "temperature": 0.0, "avg_logprob": -0.11236593092995129, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.1208537146449089}, {"id": 20, "seek": 5400, "start": 68.0, "end": 74.0, "text": " Well, for me, the main benefit is that even if you don't have the environment you would want,", "tokens": [51064, 1042, 11, 337, 385, 11, 264, 2135, 5121, 307, 300, 754, 498, 291, 500, 380, 362, 264, 2823, 291, 576, 528, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11236593092995129, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.1208537146449089}, {"id": 21, "seek": 5400, "start": 74.0, "end": 79.0, "text": " you can still find the bottlenecks in your application or whatever you are testing.", "tokens": [51364, 291, 393, 920, 915, 264, 44641, 2761, 294, 428, 3861, 420, 2035, 291, 366, 4997, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11236593092995129, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.1208537146449089}, {"id": 22, "seek": 7900, "start": 79.0, "end": 84.0, "text": " And you still can optimize it even if you don't have everything ready", "tokens": [50364, 400, 291, 920, 393, 19719, 309, 754, 498, 291, 500, 380, 362, 1203, 1919, 50614], "temperature": 0.0, "avg_logprob": -0.08504650138673328, "compression_ratio": 1.6519607843137254, "no_speech_prob": 0.059154026210308075}, {"id": 23, "seek": 7900, "start": 84.0, "end": 89.0, "text": " because the truth is that the performance testing is quite expensive.", "tokens": [50614, 570, 264, 3494, 307, 300, 264, 3389, 4997, 307, 1596, 5124, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08504650138673328, "compression_ratio": 1.6519607843137254, "no_speech_prob": 0.059154026210308075}, {"id": 24, "seek": 7900, "start": 89.0, "end": 93.0, "text": " And for the good one, I don't think that there are the companies", "tokens": [50864, 400, 337, 264, 665, 472, 11, 286, 500, 380, 519, 300, 456, 366, 264, 3431, 51064], "temperature": 0.0, "avg_logprob": -0.08504650138673328, "compression_ratio": 1.6519607843137254, "no_speech_prob": 0.059154026210308075}, {"id": 25, "seek": 7900, "start": 93.0, "end": 97.0, "text": " that will give you the resources that you need to do it perfectly.", "tokens": [51064, 300, 486, 976, 291, 264, 3593, 300, 291, 643, 281, 360, 309, 6239, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08504650138673328, "compression_ratio": 1.6519607843137254, "no_speech_prob": 0.059154026210308075}, {"id": 26, "seek": 7900, "start": 97.0, "end": 103.0, "text": " So that's for me the main option or the main reason why to do it.", "tokens": [51264, 407, 300, 311, 337, 385, 264, 2135, 3614, 420, 264, 2135, 1778, 983, 281, 360, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08504650138673328, "compression_ratio": 1.6519607843137254, "no_speech_prob": 0.059154026210308075}, {"id": 27, "seek": 10300, "start": 103.0, "end": 110.0, "text": " And for me, the second most important is that you will gain knowledge that you need", "tokens": [50364, 400, 337, 385, 11, 264, 1150, 881, 1021, 307, 300, 291, 486, 6052, 3601, 300, 291, 643, 50714], "temperature": 0.0, "avg_logprob": -0.08583447545073754, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.11777538061141968}, {"id": 28, "seek": 10300, "start": 110.0, "end": 114.0, "text": " or you will obtain the knowledge about the product itself", "tokens": [50714, 420, 291, 486, 12701, 264, 3601, 466, 264, 1674, 2564, 50914], "temperature": 0.0, "avg_logprob": -0.08583447545073754, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.11777538061141968}, {"id": 29, "seek": 10300, "start": 114.0, "end": 119.0, "text": " because you will suddenly see things that you cannot see,", "tokens": [50914, 570, 291, 486, 5800, 536, 721, 300, 291, 2644, 536, 11, 51164], "temperature": 0.0, "avg_logprob": -0.08583447545073754, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.11777538061141968}, {"id": 30, "seek": 10300, "start": 119.0, "end": 122.0, "text": " even if you normally deploy things and everything.", "tokens": [51164, 754, 498, 291, 5646, 7274, 721, 293, 1203, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08583447545073754, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.11777538061141968}, {"id": 31, "seek": 10300, "start": 122.0, "end": 126.0, "text": " You will see the little thingies that are happening here and there.", "tokens": [51314, 509, 486, 536, 264, 707, 551, 530, 300, 366, 2737, 510, 293, 456, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08583447545073754, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.11777538061141968}, {"id": 32, "seek": 10300, "start": 126.0, "end": 132.0, "text": " And the information you gain are quite nice to get.", "tokens": [51514, 400, 264, 1589, 291, 6052, 366, 1596, 1481, 281, 483, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08583447545073754, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.11777538061141968}, {"id": 33, "seek": 13200, "start": 132.0, "end": 137.0, "text": " So that's probably the most, the points that you should look for", "tokens": [50364, 407, 300, 311, 1391, 264, 881, 11, 264, 2793, 300, 291, 820, 574, 337, 50614], "temperature": 0.0, "avg_logprob": -0.12621641778326653, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0959671139717102}, {"id": 34, "seek": 13200, "start": 137.0, "end": 140.0, "text": " if you are thinking about the performance testing.", "tokens": [50614, 498, 291, 366, 1953, 466, 264, 3389, 4997, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12621641778326653, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0959671139717102}, {"id": 35, "seek": 13200, "start": 140.0, "end": 145.0, "text": " This is actually what it will gain from it.", "tokens": [50764, 639, 307, 767, 437, 309, 486, 6052, 490, 309, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12621641778326653, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0959671139717102}, {"id": 36, "seek": 13200, "start": 145.0, "end": 152.0, "text": " So this is only in my opinion, like you will see a lot of papers about the performance testing", "tokens": [51014, 407, 341, 307, 787, 294, 452, 4800, 11, 411, 291, 486, 536, 257, 688, 295, 10577, 466, 264, 3389, 4997, 51364], "temperature": 0.0, "avg_logprob": -0.12621641778326653, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0959671139717102}, {"id": 37, "seek": 13200, "start": 152.0, "end": 156.0, "text": " and all the things that you have to take care of.", "tokens": [51364, 293, 439, 264, 721, 300, 291, 362, 281, 747, 1127, 295, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12621641778326653, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0959671139717102}, {"id": 38, "seek": 15600, "start": 157.0, "end": 162.0, "text": " Like on the GitHub, I know about two or three papers that have like 40 pages", "tokens": [50414, 1743, 322, 264, 23331, 11, 286, 458, 466, 732, 420, 1045, 10577, 300, 362, 411, 3356, 7183, 50664], "temperature": 0.0, "avg_logprob": -0.13828350857990543, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.1691160649061203}, {"id": 39, "seek": 15600, "start": 162.0, "end": 166.0, "text": " about the performance testing and all the criteria that you have to fill.", "tokens": [50664, 466, 264, 3389, 4997, 293, 439, 264, 11101, 300, 291, 362, 281, 2836, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13828350857990543, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.1691160649061203}, {"id": 40, "seek": 15600, "start": 166.0, "end": 173.0, "text": " In my opinion, there are like two variants of the performance testing.", "tokens": [50864, 682, 452, 4800, 11, 456, 366, 411, 732, 21669, 295, 264, 3389, 4997, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13828350857990543, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.1691160649061203}, {"id": 41, "seek": 15600, "start": 173.0, "end": 177.0, "text": " And first is measurement and second is testing.", "tokens": [51214, 400, 700, 307, 13160, 293, 1150, 307, 4997, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13828350857990543, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.1691160649061203}, {"id": 42, "seek": 15600, "start": 177.0, "end": 182.0, "text": " For me, the measurement is really the thing that you are looking for numbers.", "tokens": [51414, 1171, 385, 11, 264, 13160, 307, 534, 264, 551, 300, 291, 366, 1237, 337, 3547, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13828350857990543, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.1691160649061203}, {"id": 43, "seek": 18200, "start": 182.0, "end": 186.0, "text": " And you need those numbers for, I would say, legal reasons", "tokens": [50364, 400, 291, 643, 729, 3547, 337, 11, 286, 576, 584, 11, 5089, 4112, 50564], "temperature": 0.0, "avg_logprob": -0.1574467078022573, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.33005619049072266}, {"id": 44, "seek": 18200, "start": 186.0, "end": 190.0, "text": " or anything what you have to declare for your customer.", "tokens": [50564, 420, 1340, 437, 291, 362, 281, 19710, 337, 428, 5474, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1574467078022573, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.33005619049072266}, {"id": 45, "seek": 18200, "start": 190.0, "end": 195.0, "text": " You will say that, for example, for us, I work on the division.", "tokens": [50764, 509, 486, 584, 300, 11, 337, 1365, 11, 337, 505, 11, 286, 589, 322, 264, 10044, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1574467078022573, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.33005619049072266}, {"id": 46, "seek": 18200, "start": 195.0, "end": 201.0, "text": " Like if we would want to say that this connector is actually able to do 30k per second,", "tokens": [51014, 1743, 498, 321, 576, 528, 281, 584, 300, 341, 19127, 307, 767, 1075, 281, 360, 2217, 74, 680, 1150, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1574467078022573, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.33005619049072266}, {"id": 47, "seek": 18200, "start": 201.0, "end": 208.0, "text": " we would need some kind of a proof that we can do it.", "tokens": [51314, 321, 576, 643, 512, 733, 295, 257, 8177, 300, 321, 393, 360, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1574467078022573, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.33005619049072266}, {"id": 48, "seek": 20800, "start": 208.0, "end": 212.0, "text": " And getting this proof is like, it's very complicated", "tokens": [50364, 400, 1242, 341, 8177, 307, 411, 11, 309, 311, 588, 6179, 50564], "temperature": 0.0, "avg_logprob": -0.08346719052418168, "compression_ratio": 1.6581632653061225, "no_speech_prob": 0.10140864551067352}, {"id": 49, "seek": 20800, "start": 212.0, "end": 217.0, "text": " and you have to do it in very specific ways.", "tokens": [50564, 293, 291, 362, 281, 360, 309, 294, 588, 2685, 2098, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08346719052418168, "compression_ratio": 1.6581632653061225, "no_speech_prob": 0.10140864551067352}, {"id": 50, "seek": 20800, "start": 217.0, "end": 224.0, "text": " And even if you have everything, it's not quite, it's not always acceptable.", "tokens": [50814, 400, 754, 498, 291, 362, 1203, 11, 309, 311, 406, 1596, 11, 309, 311, 406, 1009, 15513, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08346719052418168, "compression_ratio": 1.6581632653061225, "no_speech_prob": 0.10140864551067352}, {"id": 51, "seek": 20800, "start": 224.0, "end": 228.0, "text": " But the second part or second variation is just testing.", "tokens": [51164, 583, 264, 1150, 644, 420, 1150, 12990, 307, 445, 4997, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08346719052418168, "compression_ratio": 1.6581632653061225, "no_speech_prob": 0.10140864551067352}, {"id": 52, "seek": 20800, "start": 228.0, "end": 233.0, "text": " And for me, the testing, the testing is really just finding the bottlenecks in your product.", "tokens": [51364, 400, 337, 385, 11, 264, 4997, 11, 264, 4997, 307, 534, 445, 5006, 264, 44641, 2761, 294, 428, 1674, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08346719052418168, "compression_ratio": 1.6581632653061225, "no_speech_prob": 0.10140864551067352}, {"id": 53, "seek": 23300, "start": 233.0, "end": 241.0, "text": " And I think that's even more, like the testing is even more important", "tokens": [50364, 400, 286, 519, 300, 311, 754, 544, 11, 411, 264, 4997, 307, 754, 544, 1021, 50764], "temperature": 0.0, "avg_logprob": -0.10632662440455237, "compression_ratio": 1.8142857142857143, "no_speech_prob": 0.187360942363739}, {"id": 54, "seek": 23300, "start": 241.0, "end": 248.0, "text": " because there you will find all the bottlenecks and you can optimize your application really.", "tokens": [50764, 570, 456, 291, 486, 915, 439, 264, 44641, 2761, 293, 291, 393, 19719, 428, 3861, 534, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10632662440455237, "compression_ratio": 1.8142857142857143, "no_speech_prob": 0.187360942363739}, {"id": 55, "seek": 23300, "start": 248.0, "end": 253.0, "text": " And you can see the flaws in your call because you cannot see these things", "tokens": [51114, 400, 291, 393, 536, 264, 27108, 294, 428, 818, 570, 291, 2644, 536, 613, 721, 51364], "temperature": 0.0, "avg_logprob": -0.10632662440455237, "compression_ratio": 1.8142857142857143, "no_speech_prob": 0.187360942363739}, {"id": 56, "seek": 23300, "start": 253.0, "end": 258.0, "text": " when you run it regularly and you don't have everything around the application tuned up", "tokens": [51364, 562, 291, 1190, 309, 11672, 293, 291, 500, 380, 362, 1203, 926, 264, 3861, 10870, 493, 51614], "temperature": 0.0, "avg_logprob": -0.10632662440455237, "compression_ratio": 1.8142857142857143, "no_speech_prob": 0.187360942363739}, {"id": 57, "seek": 23300, "start": 258.0, "end": 261.0, "text": " so you don't throttle your application to the maximum.", "tokens": [51614, 370, 291, 500, 380, 24235, 428, 3861, 281, 264, 6674, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10632662440455237, "compression_ratio": 1.8142857142857143, "no_speech_prob": 0.187360942363739}, {"id": 58, "seek": 26100, "start": 261.0, "end": 268.0, "text": " These things usually happen when you go over the top or near the maximum, near the max.", "tokens": [50364, 1981, 721, 2673, 1051, 562, 291, 352, 670, 264, 1192, 420, 2651, 264, 6674, 11, 2651, 264, 11469, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1202185125911937, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.09068579971790314}, {"id": 59, "seek": 26100, "start": 268.0, "end": 274.0, "text": " So, yeah, this is in my opinion two ways how to do the performance testing", "tokens": [50714, 407, 11, 1338, 11, 341, 307, 294, 452, 4800, 732, 2098, 577, 281, 360, 264, 3389, 4997, 51014], "temperature": 0.0, "avg_logprob": -0.1202185125911937, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.09068579971790314}, {"id": 60, "seek": 26100, "start": 274.0, "end": 278.0, "text": " or two variants of the performance testing.", "tokens": [51014, 420, 732, 21669, 295, 264, 3389, 4997, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1202185125911937, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.09068579971790314}, {"id": 61, "seek": 26100, "start": 278.0, "end": 284.0, "text": " What is not really optimal about the testing, which I was talking about,", "tokens": [51214, 708, 307, 406, 534, 16252, 466, 264, 4997, 11, 597, 286, 390, 1417, 466, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1202185125911937, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.09068579971790314}, {"id": 62, "seek": 26100, "start": 284.0, "end": 288.0, "text": " just finding the bottlenecks, not the number, you need massive monitoring", "tokens": [51514, 445, 5006, 264, 44641, 2761, 11, 406, 264, 1230, 11, 291, 643, 5994, 11028, 51714], "temperature": 0.0, "avg_logprob": -0.1202185125911937, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.09068579971790314}, {"id": 63, "seek": 28800, "start": 288.0, "end": 294.0, "text": " and I will say more about it in the talk, but that's like main disadvantage of it", "tokens": [50364, 293, 286, 486, 584, 544, 466, 309, 294, 264, 751, 11, 457, 300, 311, 411, 2135, 24292, 295, 309, 50664], "temperature": 0.0, "avg_logprob": -0.1431716841620368, "compression_ratio": 1.6321243523316062, "no_speech_prob": 0.21796663105487823}, {"id": 64, "seek": 28800, "start": 294.0, "end": 301.0, "text": " that most of the time you will go around tracing, monitoring, metrics", "tokens": [50664, 300, 881, 295, 264, 565, 291, 486, 352, 926, 25262, 11, 11028, 11, 16367, 51014], "temperature": 0.0, "avg_logprob": -0.1431716841620368, "compression_ratio": 1.6321243523316062, "no_speech_prob": 0.21796663105487823}, {"id": 65, "seek": 28800, "start": 301.0, "end": 308.0, "text": " and you will find some stuff that will really give you hard times figuring it out", "tokens": [51014, 293, 291, 486, 915, 512, 1507, 300, 486, 534, 976, 291, 1152, 1413, 15213, 309, 484, 51364], "temperature": 0.0, "avg_logprob": -0.1431716841620368, "compression_ratio": 1.6321243523316062, "no_speech_prob": 0.21796663105487823}, {"id": 66, "seek": 28800, "start": 308.0, "end": 314.0, "text": " because you are going for performance and you are speaking in milliseconds, yeah?", "tokens": [51364, 570, 291, 366, 516, 337, 3389, 293, 291, 366, 4124, 294, 34184, 11, 1338, 30, 51664], "temperature": 0.0, "avg_logprob": -0.1431716841620368, "compression_ratio": 1.6321243523316062, "no_speech_prob": 0.21796663105487823}, {"id": 67, "seek": 31400, "start": 314.0, "end": 323.0, "text": " But most of the things that are used for monitoring are not really prepared to handle you for one minute second.", "tokens": [50364, 583, 881, 295, 264, 721, 300, 366, 1143, 337, 11028, 366, 406, 534, 4927, 281, 4813, 291, 337, 472, 3456, 1150, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12969273787278396, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.05967848002910614}, {"id": 68, "seek": 31400, "start": 323.0, "end": 329.0, "text": " They think that it's okay to scrape, for example, metrics by 10 seconds", "tokens": [50814, 814, 519, 300, 309, 311, 1392, 281, 32827, 11, 337, 1365, 11, 16367, 538, 1266, 3949, 51114], "temperature": 0.0, "avg_logprob": -0.12969273787278396, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.05967848002910614}, {"id": 69, "seek": 31400, "start": 329.0, "end": 335.0, "text": " and like this will give you massive headaches during the time.", "tokens": [51114, 293, 411, 341, 486, 976, 291, 5994, 35046, 1830, 264, 565, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12969273787278396, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.05967848002910614}, {"id": 70, "seek": 31400, "start": 335.0, "end": 343.0, "text": " I think I have already somehow gathered those, but the goal of the performance testing is,", "tokens": [51414, 286, 519, 286, 362, 1217, 6063, 13032, 729, 11, 457, 264, 3387, 295, 264, 3389, 4997, 307, 11, 51814], "temperature": 0.0, "avg_logprob": -0.12969273787278396, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.05967848002910614}, {"id": 71, "seek": 34300, "start": 343.0, "end": 352.0, "text": " as I said, find the bottlenecks, but they are much more to it because, for example, the load types, you know,", "tokens": [50364, 382, 286, 848, 11, 915, 264, 44641, 2761, 11, 457, 436, 366, 709, 544, 281, 309, 570, 11, 337, 1365, 11, 264, 3677, 3467, 11, 291, 458, 11, 50814], "temperature": 0.0, "avg_logprob": -0.1562902813866025, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.08501892536878586}, {"id": 72, "seek": 34300, "start": 352.0, "end": 360.0, "text": " even if someone of you had crossed the performance testing, the main point all of the guys are talking about", "tokens": [50814, 754, 498, 1580, 295, 291, 632, 14622, 264, 3389, 4997, 11, 264, 2135, 935, 439, 295, 264, 1074, 366, 1417, 466, 51214], "temperature": 0.0, "avg_logprob": -0.1562902813866025, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.08501892536878586}, {"id": 73, "seek": 34300, "start": 360.0, "end": 366.0, "text": " is what kind of load we are going to do, how we are going to do it to make it reproducible, you know?", "tokens": [51214, 307, 437, 733, 295, 3677, 321, 366, 516, 281, 360, 11, 577, 321, 366, 516, 281, 360, 309, 281, 652, 309, 11408, 32128, 11, 291, 458, 30, 51514], "temperature": 0.0, "avg_logprob": -0.1562902813866025, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.08501892536878586}, {"id": 74, "seek": 36600, "start": 366.0, "end": 372.0, "text": " Because that's like a problem because for some application you put constant loads to them.", "tokens": [50364, 1436, 300, 311, 411, 257, 1154, 570, 337, 512, 3861, 291, 829, 5754, 12668, 281, 552, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15390918061539932, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.5464252829551697}, {"id": 75, "seek": 36600, "start": 372.0, "end": 379.0, "text": " Let's say you are going 10k per second, some requests for the API for one hour.", "tokens": [50664, 961, 311, 584, 291, 366, 516, 1266, 74, 680, 1150, 11, 512, 12475, 337, 264, 9362, 337, 472, 1773, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15390918061539932, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.5464252829551697}, {"id": 76, "seek": 36600, "start": 379.0, "end": 389.0, "text": " And like that could be fine, but we all saw that some websites or, for example, the systems that you are buying tickets for concerts,", "tokens": [51014, 400, 411, 300, 727, 312, 2489, 11, 457, 321, 439, 1866, 300, 512, 12891, 420, 11, 337, 1365, 11, 264, 3652, 300, 291, 366, 6382, 12628, 337, 24924, 11, 51514], "temperature": 0.0, "avg_logprob": -0.15390918061539932, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.5464252829551697}, {"id": 77, "seek": 38900, "start": 389.0, "end": 396.0, "text": " these kind of things need peak loads. You know, you are going low for 5k per second", "tokens": [50364, 613, 733, 295, 721, 643, 10651, 12668, 13, 509, 458, 11, 291, 366, 516, 2295, 337, 1025, 74, 680, 1150, 50714], "temperature": 0.0, "avg_logprob": -0.12977628010075268, "compression_ratio": 1.64321608040201, "no_speech_prob": 0.1047210618853569}, {"id": 78, "seek": 38900, "start": 396.0, "end": 402.0, "text": " and suddenly you spin it up for 100k per second or something like that.", "tokens": [50714, 293, 5800, 291, 6060, 309, 493, 337, 2319, 74, 680, 1150, 420, 746, 411, 300, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12977628010075268, "compression_ratio": 1.64321608040201, "no_speech_prob": 0.1047210618853569}, {"id": 79, "seek": 38900, "start": 402.0, "end": 410.0, "text": " So you really need something that will generate the load for you and do it reproducible.", "tokens": [51014, 407, 291, 534, 643, 746, 300, 486, 8460, 264, 3677, 337, 291, 293, 360, 309, 11408, 32128, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12977628010075268, "compression_ratio": 1.64321608040201, "no_speech_prob": 0.1047210618853569}, {"id": 80, "seek": 38900, "start": 410.0, "end": 417.0, "text": " You need to have the same load so you can repeat the testing for a couple of times", "tokens": [51414, 509, 643, 281, 362, 264, 912, 3677, 370, 291, 393, 7149, 264, 4997, 337, 257, 1916, 295, 1413, 51764], "temperature": 0.0, "avg_logprob": -0.12977628010075268, "compression_ratio": 1.64321608040201, "no_speech_prob": 0.1047210618853569}, {"id": 81, "seek": 41700, "start": 417.0, "end": 429.0, "text": " and be consistent in that because otherwise you will find all the other things except the flux in your cold.", "tokens": [50364, 293, 312, 8398, 294, 300, 570, 5911, 291, 486, 915, 439, 264, 661, 721, 3993, 264, 19298, 294, 428, 3554, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18344830244015425, "compression_ratio": 1.7447916666666667, "no_speech_prob": 0.11398957669734955}, {"id": 82, "seek": 41700, "start": 429.0, "end": 434.0, "text": " So the main problems during the performance that you will find.", "tokens": [50964, 407, 264, 2135, 2740, 1830, 264, 3389, 300, 291, 486, 915, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18344830244015425, "compression_ratio": 1.7447916666666667, "no_speech_prob": 0.11398957669734955}, {"id": 83, "seek": 41700, "start": 434.0, "end": 438.0, "text": " I have said that you don't need the isolated environment to do it.", "tokens": [51214, 286, 362, 848, 300, 291, 500, 380, 643, 264, 14621, 2823, 281, 360, 309, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18344830244015425, "compression_ratio": 1.7447916666666667, "no_speech_prob": 0.11398957669734955}, {"id": 84, "seek": 41700, "start": 438.0, "end": 445.0, "text": " And that's true. We don't have isolated environment in our team when we do performance testing.", "tokens": [51414, 400, 300, 311, 2074, 13, 492, 500, 380, 362, 14621, 2823, 294, 527, 1469, 562, 321, 360, 3389, 4997, 13, 51764], "temperature": 0.0, "avg_logprob": -0.18344830244015425, "compression_ratio": 1.7447916666666667, "no_speech_prob": 0.11398957669734955}, {"id": 85, "seek": 44500, "start": 445.0, "end": 451.0, "text": " But if you don't have entirely isolated environment, you need to know your environment pretty well.", "tokens": [50364, 583, 498, 291, 500, 380, 362, 7696, 14621, 2823, 11, 291, 643, 281, 458, 428, 2823, 1238, 731, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08788013458251953, "compression_ratio": 1.703125, "no_speech_prob": 0.0705011785030365}, {"id": 86, "seek": 44500, "start": 451.0, "end": 457.0, "text": " You have to know your latencies, you have to know all the hardware specs and these kind of things.", "tokens": [50664, 509, 362, 281, 458, 428, 4465, 6464, 11, 291, 362, 281, 458, 439, 264, 8837, 27911, 293, 613, 733, 295, 721, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08788013458251953, "compression_ratio": 1.703125, "no_speech_prob": 0.0705011785030365}, {"id": 87, "seek": 44500, "start": 457.0, "end": 468.0, "text": " You really need all of the information because if you see some very specific things happen during the test which are not common,", "tokens": [50964, 509, 534, 643, 439, 295, 264, 1589, 570, 498, 291, 536, 512, 588, 2685, 721, 1051, 1830, 264, 1500, 597, 366, 406, 2689, 11, 51514], "temperature": 0.0, "avg_logprob": -0.08788013458251953, "compression_ratio": 1.703125, "no_speech_prob": 0.0705011785030365}, {"id": 88, "seek": 46800, "start": 469.0, "end": 475.0, "text": " you then can somehow put the puzzle together with all the information you get from the environment", "tokens": [50414, 291, 550, 393, 6063, 829, 264, 12805, 1214, 365, 439, 264, 1589, 291, 483, 490, 264, 2823, 50714], "temperature": 0.0, "avg_logprob": -0.09207965345943675, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.1502426713705063}, {"id": 89, "seek": 46800, "start": 475.0, "end": 487.0, "text": " and you can somehow at least, I would say, decrease the number of stupid mistakes that you will have to gather around.", "tokens": [50714, 293, 291, 393, 6063, 412, 1935, 11, 286, 576, 584, 11, 11514, 264, 1230, 295, 6631, 8038, 300, 291, 486, 362, 281, 5448, 926, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09207965345943675, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.1502426713705063}, {"id": 90, "seek": 46800, "start": 487.0, "end": 496.0, "text": " The next thing that is very important is to have the monitoring which I have said already.", "tokens": [51314, 440, 958, 551, 300, 307, 588, 1021, 307, 281, 362, 264, 11028, 597, 286, 362, 848, 1217, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09207965345943675, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.1502426713705063}, {"id": 91, "seek": 49600, "start": 496.0, "end": 506.0, "text": " You need all the metrics you can get because before that you don't even know what kind of the information would be valuable for you when you start doing it,", "tokens": [50364, 509, 643, 439, 264, 16367, 291, 393, 483, 570, 949, 300, 291, 500, 380, 754, 458, 437, 733, 295, 264, 1589, 576, 312, 8263, 337, 291, 562, 291, 722, 884, 309, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11166986242517249, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.4031837582588196}, {"id": 92, "seek": 49600, "start": 506.0, "end": 508.0, "text": " but you will need all of it.", "tokens": [50864, 457, 291, 486, 643, 439, 295, 309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11166986242517249, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.4031837582588196}, {"id": 93, "seek": 49600, "start": 508.0, "end": 518.0, "text": " If you can get rid and you will surely use it, if you don't gather those metrics and then you will figure out you need it,", "tokens": [50964, 759, 291, 393, 483, 3973, 293, 291, 486, 11468, 764, 309, 11, 498, 291, 500, 380, 5448, 729, 16367, 293, 550, 291, 486, 2573, 484, 291, 643, 309, 11, 51464], "temperature": 0.0, "avg_logprob": -0.11166986242517249, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.4031837582588196}, {"id": 94, "seek": 51800, "start": 518.0, "end": 521.0, "text": " you cannot get them from the past.", "tokens": [50364, 291, 2644, 483, 552, 490, 264, 1791, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15116621808307926, "compression_ratio": 1.642512077294686, "no_speech_prob": 0.2678910195827484}, {"id": 95, "seek": 51800, "start": 521.0, "end": 527.0, "text": " So really the thing is gather everything you can and it will be fine.", "tokens": [50514, 407, 534, 264, 551, 307, 5448, 1203, 291, 393, 293, 309, 486, 312, 2489, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15116621808307926, "compression_ratio": 1.642512077294686, "no_speech_prob": 0.2678910195827484}, {"id": 96, "seek": 51800, "start": 527.0, "end": 536.0, "text": " And the last thing for this is you need to tune up all the systems that you are dependent on.", "tokens": [50814, 400, 264, 1036, 551, 337, 341, 307, 291, 643, 281, 10864, 493, 439, 264, 3652, 300, 291, 366, 12334, 322, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15116621808307926, "compression_ratio": 1.642512077294686, "no_speech_prob": 0.2678910195827484}, {"id": 97, "seek": 51800, "start": 536.0, "end": 546.0, "text": " For us, we are working on databases and we cannot really throttle up our product if the database isn't optimized to the hardware it's running", "tokens": [51264, 1171, 505, 11, 321, 366, 1364, 322, 22380, 293, 321, 2644, 534, 24235, 493, 527, 1674, 498, 264, 8149, 1943, 380, 26941, 281, 264, 8837, 309, 311, 2614, 51764], "temperature": 0.0, "avg_logprob": -0.15116621808307926, "compression_ratio": 1.642512077294686, "no_speech_prob": 0.2678910195827484}, {"id": 98, "seek": 54600, "start": 546.0, "end": 550.0, "text": " because if the database is not throttling, we are not throttling.", "tokens": [50364, 570, 498, 264, 8149, 307, 406, 739, 1521, 1688, 11, 321, 366, 406, 739, 1521, 1688, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10315999251145583, "compression_ratio": 1.5696969696969696, "no_speech_prob": 0.07816026359796524}, {"id": 99, "seek": 54600, "start": 550.0, "end": 560.0, "text": " So you basically need to have everything on the high spec there to not bottleneck your application.", "tokens": [50564, 407, 291, 1936, 643, 281, 362, 1203, 322, 264, 1090, 1608, 456, 281, 406, 44641, 547, 428, 3861, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10315999251145583, "compression_ratio": 1.5696969696969696, "no_speech_prob": 0.07816026359796524}, {"id": 100, "seek": 54600, "start": 560.0, "end": 569.0, "text": " So that's one of the main points because on some things it's quite problematic to tune it up.", "tokens": [51064, 407, 300, 311, 472, 295, 264, 2135, 2793, 570, 322, 512, 721, 309, 311, 1596, 19011, 281, 10864, 309, 493, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10315999251145583, "compression_ratio": 1.5696969696969696, "no_speech_prob": 0.07816026359796524}, {"id": 101, "seek": 56900, "start": 570.0, "end": 580.0, "text": " I quite like this quote because it's all about the metrics and if you have them, it's fine and it's nice.", "tokens": [50414, 286, 1596, 411, 341, 6513, 570, 309, 311, 439, 466, 264, 16367, 293, 498, 291, 362, 552, 11, 309, 311, 2489, 293, 309, 311, 1481, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15419869364043812, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.12915879487991333}, {"id": 102, "seek": 56900, "start": 580.0, "end": 582.0, "text": " If you don't have them, it's massive problems.", "tokens": [50914, 759, 291, 500, 380, 362, 552, 11, 309, 311, 5994, 2740, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15419869364043812, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.12915879487991333}, {"id": 103, "seek": 56900, "start": 582.0, "end": 587.0, "text": " So I think that's really the quote that you should be looking for.", "tokens": [51014, 407, 286, 519, 300, 311, 534, 264, 6513, 300, 291, 820, 312, 1237, 337, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15419869364043812, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.12915879487991333}, {"id": 104, "seek": 56900, "start": 590.0, "end": 592.0, "text": " So again, monitoring.", "tokens": [51414, 407, 797, 11, 11028, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15419869364043812, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.12915879487991333}, {"id": 105, "seek": 56900, "start": 592.0, "end": 597.0, "text": " I have already talked about the problem with the scraping.", "tokens": [51514, 286, 362, 1217, 2825, 466, 264, 1154, 365, 264, 43738, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15419869364043812, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.12915879487991333}, {"id": 106, "seek": 59700, "start": 597.0, "end": 608.0, "text": " So let's say we have used Prometheus mostly and the maximum what you can get from Prometheus is one second scraping.", "tokens": [50364, 407, 718, 311, 584, 321, 362, 1143, 2114, 649, 42209, 5240, 293, 264, 6674, 437, 291, 393, 483, 490, 2114, 649, 42209, 307, 472, 1150, 43738, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14668556495949073, "compression_ratio": 1.5253164556962024, "no_speech_prob": 0.023549145087599754}, {"id": 107, "seek": 59700, "start": 608.0, "end": 620.0, "text": " So that's fine for information causes but not for the performance metrics because the things happen during the milliseconds.", "tokens": [50914, 407, 300, 311, 2489, 337, 1589, 7700, 457, 406, 337, 264, 3389, 16367, 570, 264, 721, 1051, 1830, 264, 34184, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14668556495949073, "compression_ratio": 1.5253164556962024, "no_speech_prob": 0.023549145087599754}, {"id": 108, "seek": 62000, "start": 620.0, "end": 633.0, "text": " Maybe 10 milliseconds would be enough but one second is really you are losing a lot of information and later on I have the example of what you could see when the scrapers are not fast enough.", "tokens": [50364, 2704, 1266, 34184, 576, 312, 1547, 457, 472, 1150, 307, 534, 291, 366, 7027, 257, 688, 295, 1589, 293, 1780, 322, 286, 362, 264, 1365, 295, 437, 291, 727, 536, 562, 264, 23138, 433, 366, 406, 2370, 1547, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12855151494344075, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.38942956924438477}, {"id": 109, "seek": 62000, "start": 633.0, "end": 643.0, "text": " And there's like massive problem because not every scraper is or I would say there are no scrapers that can do it really fast.", "tokens": [51014, 400, 456, 311, 411, 5994, 1154, 570, 406, 633, 13943, 610, 307, 420, 286, 576, 584, 456, 366, 572, 23138, 433, 300, 393, 360, 309, 534, 2370, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12855151494344075, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.38942956924438477}, {"id": 110, "seek": 62000, "start": 643.0, "end": 647.0, "text": " So you probably have to implement it and we are working on that actually.", "tokens": [51514, 407, 291, 1391, 362, 281, 4445, 309, 293, 321, 366, 1364, 322, 300, 767, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12855151494344075, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.38942956924438477}, {"id": 111, "seek": 64700, "start": 647.0, "end": 650.0, "text": " So that's the main problem.", "tokens": [50364, 407, 300, 311, 264, 2135, 1154, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16882293525783496, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.03366725519299507}, {"id": 112, "seek": 64700, "start": 650.0, "end": 663.0, "text": " And the second problem is that you will end up with having a lot of the systems in the field because you need hardware metrics.", "tokens": [50514, 400, 264, 1150, 1154, 307, 300, 291, 486, 917, 493, 365, 1419, 257, 688, 295, 264, 3652, 294, 264, 2519, 570, 291, 643, 8837, 16367, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16882293525783496, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.03366725519299507}, {"id": 113, "seek": 64700, "start": 663.0, "end": 666.0, "text": " You need JMX metrics and I don't know what other.", "tokens": [51164, 509, 643, 508, 44, 55, 16367, 293, 286, 500, 380, 458, 437, 661, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16882293525783496, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.03366725519299507}, {"id": 114, "seek": 64700, "start": 666.0, "end": 668.0, "text": " It really depends on your application.", "tokens": [51314, 467, 534, 5946, 322, 428, 3861, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16882293525783496, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.03366725519299507}, {"id": 115, "seek": 64700, "start": 668.0, "end": 674.0, "text": " But for us, we needed hardware metrics, JMX metrics and some metrics from our test suite.", "tokens": [51414, 583, 337, 505, 11, 321, 2978, 8837, 16367, 11, 508, 44, 55, 16367, 293, 512, 16367, 490, 527, 1500, 14205, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16882293525783496, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.03366725519299507}, {"id": 116, "seek": 67400, "start": 674.0, "end": 679.0, "text": " And these three things actually all the different outputs.", "tokens": [50364, 400, 613, 1045, 721, 767, 439, 264, 819, 23930, 13, 50614], "temperature": 0.0, "avg_logprob": -0.18937105881540398, "compression_ratio": 1.4267515923566878, "no_speech_prob": 0.16732101142406464}, {"id": 117, "seek": 67400, "start": 679.0, "end": 684.0, "text": " You know, we have used net data for the hardware metrics.", "tokens": [50614, 509, 458, 11, 321, 362, 1143, 2533, 1412, 337, 264, 8837, 16367, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18937105881540398, "compression_ratio": 1.4267515923566878, "no_speech_prob": 0.16732101142406464}, {"id": 118, "seek": 67400, "start": 684.0, "end": 688.0, "text": " It's like really nice tool open source fast everything nice.", "tokens": [50864, 467, 311, 411, 534, 1481, 2290, 1269, 4009, 2370, 1203, 1481, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18937105881540398, "compression_ratio": 1.4267515923566878, "no_speech_prob": 0.16732101142406464}, {"id": 119, "seek": 67400, "start": 688.0, "end": 692.0, "text": " But you cannot import JMX metrics to net data.", "tokens": [51064, 583, 291, 2644, 974, 508, 44, 55, 16367, 281, 2533, 1412, 13, 51264], "temperature": 0.0, "avg_logprob": -0.18937105881540398, "compression_ratio": 1.4267515923566878, "no_speech_prob": 0.16732101142406464}, {"id": 120, "seek": 69200, "start": 693.0, "end": 705.0, "text": " And you net data has also one problem that you cannot import like anything would happen in the past because it's strictly hard coded for now.", "tokens": [50414, 400, 291, 2533, 1412, 575, 611, 472, 1154, 300, 291, 2644, 974, 411, 1340, 576, 1051, 294, 264, 1791, 570, 309, 311, 20792, 1152, 34874, 337, 586, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16685483621996502, "compression_ratio": 1.5463917525773196, "no_speech_prob": 0.3120667040348053}, {"id": 121, "seek": 69200, "start": 705.0, "end": 711.0, "text": " So that's the problem, you know, and then you will say yes.", "tokens": [51014, 407, 300, 311, 264, 1154, 11, 291, 458, 11, 293, 550, 291, 486, 584, 2086, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16685483621996502, "compression_ratio": 1.5463917525773196, "no_speech_prob": 0.3120667040348053}, {"id": 122, "seek": 69200, "start": 711.0, "end": 712.0, "text": " Okay.", "tokens": [51314, 1033, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16685483621996502, "compression_ratio": 1.5463917525773196, "no_speech_prob": 0.3120667040348053}, {"id": 123, "seek": 69200, "start": 712.0, "end": 715.0, "text": " So I cannot have JMX metrics to the net data.", "tokens": [51364, 407, 286, 2644, 362, 508, 44, 55, 16367, 281, 264, 2533, 1412, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16685483621996502, "compression_ratio": 1.5463917525773196, "no_speech_prob": 0.3120667040348053}, {"id": 124, "seek": 69200, "start": 715.0, "end": 719.0, "text": " So I'll add Prometheus, you know, that's fine.", "tokens": [51514, 407, 286, 603, 909, 2114, 649, 42209, 11, 291, 458, 11, 300, 311, 2489, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16685483621996502, "compression_ratio": 1.5463917525773196, "no_speech_prob": 0.3120667040348053}, {"id": 125, "seek": 71900, "start": 719.0, "end": 724.0, "text": " Okay. So you have now net data and Prometheus.", "tokens": [50364, 1033, 13, 407, 291, 362, 586, 2533, 1412, 293, 2114, 649, 42209, 13, 50614], "temperature": 0.0, "avg_logprob": -0.20259886980056763, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.06752585619688034}, {"id": 126, "seek": 71900, "start": 724.0, "end": 725.0, "text": " Sorry.", "tokens": [50614, 4919, 13, 50664], "temperature": 0.0, "avg_logprob": -0.20259886980056763, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.06752585619688034}, {"id": 127, "seek": 71900, "start": 732.0, "end": 740.0, "text": " Well, and then you will continue because you still have all at least in our expertise.", "tokens": [51014, 1042, 11, 293, 550, 291, 486, 2354, 570, 291, 920, 362, 439, 412, 1935, 294, 527, 11769, 13, 51414], "temperature": 0.0, "avg_logprob": -0.20259886980056763, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.06752585619688034}, {"id": 128, "seek": 71900, "start": 740.0, "end": 748.0, "text": " We still need that someplace to store the metrics from our test suite and you can just import it everywhere.", "tokens": [51414, 492, 920, 643, 300, 37126, 281, 3531, 264, 16367, 490, 527, 1500, 14205, 293, 291, 393, 445, 974, 309, 5315, 13, 51814], "temperature": 0.0, "avg_logprob": -0.20259886980056763, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.06752585619688034}, {"id": 129, "seek": 74800, "start": 748.0, "end": 758.0, "text": " So then you happen that you will deploy the Postgres because you can use Postgres as backup for the Prometheus for the data storage.", "tokens": [50364, 407, 550, 291, 1051, 300, 291, 486, 7274, 264, 10223, 45189, 570, 291, 393, 764, 10223, 45189, 382, 14807, 337, 264, 2114, 649, 42209, 337, 264, 1412, 6725, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1689774373943886, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.03384682908654213}, {"id": 130, "seek": 74800, "start": 758.0, "end": 765.0, "text": " So now we have net data, Postgres and also Prometheus.", "tokens": [50864, 407, 586, 321, 362, 2533, 1412, 11, 10223, 45189, 293, 611, 2114, 649, 42209, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1689774373943886, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.03384682908654213}, {"id": 131, "seek": 74800, "start": 765.0, "end": 777.0, "text": " And last but not least, you will add the Grafana because you need to visualize it, you know, and getting all those things in shape that you have like massive monitoring.", "tokens": [51214, 400, 1036, 457, 406, 1935, 11, 291, 486, 909, 264, 8985, 69, 2095, 570, 291, 643, 281, 23273, 309, 11, 291, 458, 11, 293, 1242, 439, 729, 721, 294, 3909, 300, 291, 362, 411, 5994, 11028, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1689774373943886, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.03384682908654213}, {"id": 132, "seek": 77700, "start": 777.0, "end": 780.0, "text": " You know, everything can go wrong.", "tokens": [50364, 509, 458, 11, 1203, 393, 352, 2085, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1384786605834961, "compression_ratio": 1.598901098901099, "no_speech_prob": 0.07538511604070663}, {"id": 133, "seek": 77700, "start": 780.0, "end": 798.0, "text": " So if you can use the least amount of the tools that you can use, it's better because once you have too many, it's nightmare to somehow get that in shape for the whole time.", "tokens": [50514, 407, 498, 291, 393, 764, 264, 1935, 2372, 295, 264, 3873, 300, 291, 393, 764, 11, 309, 311, 1101, 570, 1564, 291, 362, 886, 867, 11, 309, 311, 18724, 281, 6063, 483, 300, 294, 3909, 337, 264, 1379, 565, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1384786605834961, "compression_ratio": 1.598901098901099, "no_speech_prob": 0.07538511604070663}, {"id": 134, "seek": 77700, "start": 798.0, "end": 799.0, "text": " Yeah.", "tokens": [51414, 865, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1384786605834961, "compression_ratio": 1.598901098901099, "no_speech_prob": 0.07538511604070663}, {"id": 135, "seek": 77700, "start": 799.0, "end": 804.0, "text": " So with the performance testing, you are not really looking for the numbers.", "tokens": [51464, 407, 365, 264, 3389, 4997, 11, 291, 366, 406, 534, 1237, 337, 264, 3547, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1384786605834961, "compression_ratio": 1.598901098901099, "no_speech_prob": 0.07538511604070663}, {"id": 136, "seek": 80400, "start": 805.0, "end": 813.0, "text": " Numbers are not important in this case because you don't want to see the throughput is like this or like that.", "tokens": [50414, 22592, 1616, 366, 406, 1021, 294, 341, 1389, 570, 291, 500, 380, 528, 281, 536, 264, 44629, 307, 411, 341, 420, 411, 300, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07360092401504517, "compression_ratio": 1.7628865979381443, "no_speech_prob": 0.11147954314947128}, {"id": 137, "seek": 80400, "start": 813.0, "end": 824.0, "text": " You need to see the trends in the graphs because there you can see if you are constantly slowing down or if you are going like optimizing your way.", "tokens": [50814, 509, 643, 281, 536, 264, 13892, 294, 264, 24877, 570, 456, 291, 393, 536, 498, 291, 366, 6460, 26958, 760, 420, 498, 291, 366, 516, 411, 40425, 428, 636, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07360092401504517, "compression_ratio": 1.7628865979381443, "no_speech_prob": 0.11147954314947128}, {"id": 138, "seek": 80400, "start": 824.0, "end": 831.0, "text": " So really you have to look for the patterns in graphs and the trends in the graphs.", "tokens": [51364, 407, 534, 291, 362, 281, 574, 337, 264, 8294, 294, 24877, 293, 264, 13892, 294, 264, 24877, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07360092401504517, "compression_ratio": 1.7628865979381443, "no_speech_prob": 0.11147954314947128}, {"id": 139, "seek": 83100, "start": 831.0, "end": 838.0, "text": " I have the example for our testing that I will show you the patterns which we have found.", "tokens": [50364, 286, 362, 264, 1365, 337, 527, 4997, 300, 286, 486, 855, 291, 264, 8294, 597, 321, 362, 1352, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13997660291955827, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.18031269311904907}, {"id": 140, "seek": 83100, "start": 838.0, "end": 842.0, "text": " But before that, just our system under test was the BZM.", "tokens": [50714, 583, 949, 300, 11, 445, 527, 1185, 833, 1500, 390, 264, 363, 57, 44, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13997660291955827, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.18031269311904907}, {"id": 141, "seek": 83100, "start": 842.0, "end": 857.0, "text": " I don't know if you guys know the BZM, but it's effectively changed that I capture streaming, which means that we sit on the top of the database and we scan the transaction logs and sense all the events that happen there to the Kafka.", "tokens": [50914, 286, 500, 380, 458, 498, 291, 1074, 458, 264, 363, 57, 44, 11, 457, 309, 311, 8659, 3105, 300, 286, 7983, 11791, 11, 597, 1355, 300, 321, 1394, 322, 264, 1192, 295, 264, 8149, 293, 321, 11049, 264, 14425, 20820, 293, 2020, 439, 264, 3931, 300, 1051, 456, 281, 264, 47064, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13997660291955827, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.18031269311904907}, {"id": 142, "seek": 85700, "start": 857.0, "end": 866.0, "text": " We are effectively running in Kafka Connect runtime, which makes the performance testing even more juicy, I would say, because the runtime is not ours.", "tokens": [50364, 492, 366, 8659, 2614, 294, 47064, 11653, 34474, 11, 597, 1669, 264, 3389, 4997, 754, 544, 24696, 11, 286, 576, 584, 11, 570, 264, 34474, 307, 406, 11896, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11945871027504526, "compression_ratio": 1.5045871559633028, "no_speech_prob": 0.014895554631948471}, {"id": 143, "seek": 85700, "start": 866.0, "end": 869.0, "text": " So it's a little tricky.", "tokens": [50814, 407, 309, 311, 257, 707, 12414, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11945871027504526, "compression_ratio": 1.5045871559633028, "no_speech_prob": 0.014895554631948471}, {"id": 144, "seek": 85700, "start": 869.0, "end": 873.0, "text": " So that's our system under test.", "tokens": [50964, 407, 300, 311, 527, 1185, 833, 1500, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11945871027504526, "compression_ratio": 1.5045871559633028, "no_speech_prob": 0.014895554631948471}, {"id": 145, "seek": 85700, "start": 873.0, "end": 878.0, "text": " And this is the first example that I have put on.", "tokens": [51164, 400, 341, 307, 264, 700, 1365, 300, 286, 362, 829, 322, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11945871027504526, "compression_ratio": 1.5045871559633028, "no_speech_prob": 0.014895554631948471}, {"id": 146, "seek": 85700, "start": 878.0, "end": 884.0, "text": " The image on the graph on the top is basically our process duration.", "tokens": [51414, 440, 3256, 322, 264, 4295, 322, 264, 1192, 307, 1936, 527, 1399, 16365, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11945871027504526, "compression_ratio": 1.5045871559633028, "no_speech_prob": 0.014895554631948471}, {"id": 147, "seek": 88400, "start": 884.0, "end": 888.0, "text": " And there are two things that you can see on the graph.", "tokens": [50364, 400, 456, 366, 732, 721, 300, 291, 393, 536, 322, 264, 4295, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1216919349901604, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.05847694352269173}, {"id": 148, "seek": 88400, "start": 888.0, "end": 896.0, "text": " We are effectively most of the time we are oscillating between some values around 200 or 170 to 220.", "tokens": [50564, 492, 366, 8659, 881, 295, 264, 565, 321, 366, 18225, 990, 1296, 512, 4190, 926, 2331, 420, 27228, 281, 29387, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1216919349901604, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.05847694352269173}, {"id": 149, "seek": 88400, "start": 896.0, "end": 898.0, "text": " And that's entirely fine.", "tokens": [50964, 400, 300, 311, 7696, 2489, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1216919349901604, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.05847694352269173}, {"id": 150, "seek": 88400, "start": 898.0, "end": 902.0, "text": " That's actually what we want to see if you are looking for some responses.", "tokens": [51064, 663, 311, 767, 437, 321, 528, 281, 536, 498, 291, 366, 1237, 337, 512, 13019, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1216919349901604, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.05847694352269173}, {"id": 151, "seek": 88400, "start": 902.0, "end": 908.0, "text": " You need to oscillate around some value like a sinus graph or something like that.", "tokens": [51264, 509, 643, 281, 18225, 473, 926, 512, 2158, 411, 257, 41503, 4295, 420, 746, 411, 300, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1216919349901604, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.05847694352269173}, {"id": 152, "seek": 88400, "start": 908.0, "end": 913.0, "text": " But what is not actually nice is this on the star.", "tokens": [51564, 583, 437, 307, 406, 767, 1481, 307, 341, 322, 264, 3543, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1216919349901604, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.05847694352269173}, {"id": 153, "seek": 91300, "start": 914.0, "end": 919.0, "text": " Where we are constantly getting slower and slower and slower.", "tokens": [50414, 2305, 321, 366, 6460, 1242, 14009, 293, 14009, 293, 14009, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16268964494977678, "compression_ratio": 1.775, "no_speech_prob": 0.0638473704457283}, {"id": 154, "seek": 91300, "start": 919.0, "end": 929.0, "text": " And we have some peaks there, which have these peaks are don't have a reason to be there because the data are the same all the time.", "tokens": [50664, 400, 321, 362, 512, 26897, 456, 11, 597, 362, 613, 26897, 366, 500, 380, 362, 257, 1778, 281, 312, 456, 570, 264, 1412, 366, 264, 912, 439, 264, 565, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16268964494977678, "compression_ratio": 1.775, "no_speech_prob": 0.0638473704457283}, {"id": 155, "seek": 91300, "start": 929.0, "end": 931.0, "text": " So this is most likely the flaw in the code.", "tokens": [51164, 407, 341, 307, 881, 3700, 264, 13717, 294, 264, 3089, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16268964494977678, "compression_ratio": 1.775, "no_speech_prob": 0.0638473704457283}, {"id": 156, "seek": 91300, "start": 931.0, "end": 934.0, "text": " There is something happening, what shouldn't be happening.", "tokens": [51264, 821, 307, 746, 2737, 11, 437, 4659, 380, 312, 2737, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16268964494977678, "compression_ratio": 1.775, "no_speech_prob": 0.0638473704457283}, {"id": 157, "seek": 91300, "start": 934.0, "end": 938.0, "text": " And it can be the database flashing out to the score.", "tokens": [51414, 400, 309, 393, 312, 264, 8149, 31049, 484, 281, 264, 6175, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16268964494977678, "compression_ratio": 1.775, "no_speech_prob": 0.0638473704457283}, {"id": 158, "seek": 91300, "start": 938.0, "end": 942.0, "text": " It can be basically anything, but you know that there might be a problem.", "tokens": [51614, 467, 393, 312, 1936, 1340, 11, 457, 291, 458, 300, 456, 1062, 312, 257, 1154, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16268964494977678, "compression_ratio": 1.775, "no_speech_prob": 0.0638473704457283}, {"id": 159, "seek": 94200, "start": 942.0, "end": 944.0, "text": " You have all other metrics.", "tokens": [50364, 509, 362, 439, 661, 16367, 13, 50464], "temperature": 0.0, "avg_logprob": -0.19142245417055875, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.08436987549066544}, {"id": 160, "seek": 94200, "start": 944.0, "end": 949.0, "text": " You can have metrics from the databases that will show you that flashing was happening or anything.", "tokens": [50464, 509, 393, 362, 16367, 490, 264, 22380, 300, 486, 855, 291, 300, 31049, 390, 2737, 420, 1340, 13, 50714], "temperature": 0.0, "avg_logprob": -0.19142245417055875, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.08436987549066544}, {"id": 161, "seek": 94200, "start": 949.0, "end": 952.0, "text": " But this is what you have to look for.", "tokens": [50714, 583, 341, 307, 437, 291, 362, 281, 574, 337, 13, 50864], "temperature": 0.0, "avg_logprob": -0.19142245417055875, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.08436987549066544}, {"id": 162, "seek": 94200, "start": 952.0, "end": 957.0, "text": " These, it will certainly be different for your application than ours.", "tokens": [50864, 1981, 11, 309, 486, 3297, 312, 819, 337, 428, 3861, 813, 11896, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19142245417055875, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.08436987549066544}, {"id": 163, "seek": 94200, "start": 957.0, "end": 961.0, "text": " You will have to define what you are looking for on the star.", "tokens": [51114, 509, 486, 362, 281, 6964, 437, 291, 366, 1237, 337, 322, 264, 3543, 13, 51314], "temperature": 0.0, "avg_logprob": -0.19142245417055875, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.08436987549066544}, {"id": 164, "seek": 94200, "start": 961.0, "end": 963.0, "text": " But that's the main thing.", "tokens": [51314, 583, 300, 311, 264, 2135, 551, 13, 51414], "temperature": 0.0, "avg_logprob": -0.19142245417055875, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.08436987549066544}, {"id": 165, "seek": 94200, "start": 963.0, "end": 966.0, "text": " And the more funny example is this.", "tokens": [51414, 400, 264, 544, 4074, 1365, 307, 341, 13, 51564], "temperature": 0.0, "avg_logprob": -0.19142245417055875, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.08436987549066544}, {"id": 166, "seek": 94200, "start": 966.0, "end": 967.0, "text": " No, here.", "tokens": [51564, 883, 11, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19142245417055875, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.08436987549066544}, {"id": 167, "seek": 94200, "start": 967.0, "end": 971.0, "text": " This is Jmx metrics from our, from Libyseum.", "tokens": [51614, 639, 307, 508, 76, 87, 16367, 490, 527, 11, 490, 15834, 88, 405, 449, 13, 51814], "temperature": 0.0, "avg_logprob": -0.19142245417055875, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.08436987549066544}, {"id": 168, "seek": 97100, "start": 971.0, "end": 975.0, "text": " And it shows you the size of the queue.", "tokens": [50364, 400, 309, 3110, 291, 264, 2744, 295, 264, 18639, 13, 50564], "temperature": 0.0, "avg_logprob": -0.16295041208681854, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.012967326678335667}, {"id": 169, "seek": 97100, "start": 975.0, "end": 978.0, "text": " Internal queue of the Bizm.", "tokens": [50564, 47836, 18639, 295, 264, 363, 590, 76, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16295041208681854, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.012967326678335667}, {"id": 170, "seek": 97100, "start": 978.0, "end": 984.0, "text": " And that basically means that from the database you are reading to internal queue.", "tokens": [50714, 400, 300, 1936, 1355, 300, 490, 264, 8149, 291, 366, 3760, 281, 6920, 18639, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16295041208681854, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.012967326678335667}, {"id": 171, "seek": 97100, "start": 984.0, "end": 989.0, "text": " So once the queue is zero, you are not reading.", "tokens": [51014, 407, 1564, 264, 18639, 307, 4018, 11, 291, 366, 406, 3760, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16295041208681854, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.012967326678335667}, {"id": 172, "seek": 97100, "start": 989.0, "end": 992.0, "text": " You know, but we are still processing, right?", "tokens": [51264, 509, 458, 11, 457, 321, 366, 920, 9007, 11, 558, 30, 51414], "temperature": 0.0, "avg_logprob": -0.16295041208681854, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.012967326678335667}, {"id": 173, "seek": 97100, "start": 992.0, "end": 995.0, "text": " So there should, there is some mistake.", "tokens": [51414, 407, 456, 820, 11, 456, 307, 512, 6146, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16295041208681854, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.012967326678335667}, {"id": 174, "seek": 97100, "start": 995.0, "end": 999.0, "text": " And this is actually the problem with the scrapers.", "tokens": [51564, 400, 341, 307, 767, 264, 1154, 365, 264, 23138, 433, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16295041208681854, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.012967326678335667}, {"id": 175, "seek": 99900, "start": 999.0, "end": 1006.0, "text": " Because if the scraper takes each second, the database is pretty fast and it can empty up the,", "tokens": [50364, 1436, 498, 264, 13943, 610, 2516, 1184, 1150, 11, 264, 8149, 307, 1238, 2370, 293, 309, 393, 6707, 493, 264, 11, 50714], "temperature": 0.0, "avg_logprob": -0.10145163536071777, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.021137399598956108}, {"id": 176, "seek": 99900, "start": 1006.0, "end": 1009.0, "text": " it can empty the queue during that time.", "tokens": [50714, 309, 393, 6707, 264, 18639, 1830, 300, 565, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10145163536071777, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.021137399598956108}, {"id": 177, "seek": 99900, "start": 1009.0, "end": 1013.0, "text": " And if the scraper hits the right time, it will give you zero.", "tokens": [50864, 400, 498, 264, 13943, 610, 8664, 264, 558, 565, 11, 309, 486, 976, 291, 4018, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10145163536071777, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.021137399598956108}, {"id": 178, "seek": 99900, "start": 1013.0, "end": 1019.0, "text": " You know, so from this until the end, the graph is all wrong.", "tokens": [51064, 509, 458, 11, 370, 490, 341, 1826, 264, 917, 11, 264, 4295, 307, 439, 2085, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10145163536071777, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.021137399598956108}, {"id": 179, "seek": 99900, "start": 1019.0, "end": 1020.0, "text": " It's not true.", "tokens": [51364, 467, 311, 406, 2074, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10145163536071777, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.021137399598956108}, {"id": 180, "seek": 99900, "start": 1020.0, "end": 1023.0, "text": " And it's all because of the speed of the scraper.", "tokens": [51414, 400, 309, 311, 439, 570, 295, 264, 3073, 295, 264, 13943, 610, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10145163536071777, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.021137399598956108}, {"id": 181, "seek": 99900, "start": 1023.0, "end": 1026.0, "text": " Because it basically hit the wrong time.", "tokens": [51564, 1436, 309, 1936, 2045, 264, 2085, 565, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10145163536071777, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.021137399598956108}, {"id": 182, "seek": 102600, "start": 1026.0, "end": 1036.0, "text": " So that is the thing that you have to be worried about because this will happen surely.", "tokens": [50364, 407, 300, 307, 264, 551, 300, 291, 362, 281, 312, 5804, 466, 570, 341, 486, 1051, 11468, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11192987206276883, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.030428968369960785}, {"id": 183, "seek": 102600, "start": 1036.0, "end": 1039.0, "text": " And these are some other graphs.", "tokens": [50864, 400, 613, 366, 512, 661, 24877, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11192987206276883, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.030428968369960785}, {"id": 184, "seek": 102600, "start": 1039.0, "end": 1041.0, "text": " These are, I would say, more wild.", "tokens": [51014, 1981, 366, 11, 286, 576, 584, 11, 544, 4868, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11192987206276883, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.030428968369960785}, {"id": 185, "seek": 102600, "start": 1041.0, "end": 1047.0, "text": " I would say it's from the start of our playing with the performance.", "tokens": [51114, 286, 576, 584, 309, 311, 490, 264, 722, 295, 527, 2433, 365, 264, 3389, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11192987206276883, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.030428968369960785}, {"id": 186, "seek": 102600, "start": 1047.0, "end": 1051.0, "text": " But the top one is also, it's pretty cool.", "tokens": [51414, 583, 264, 1192, 472, 307, 611, 11, 309, 311, 1238, 1627, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11192987206276883, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.030428968369960785}, {"id": 187, "seek": 102600, "start": 1051.0, "end": 1055.0, "text": " It's not that constant as it was for the previous one.", "tokens": [51614, 467, 311, 406, 300, 5754, 382, 309, 390, 337, 264, 3894, 472, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11192987206276883, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.030428968369960785}, {"id": 188, "seek": 105500, "start": 1055.0, "end": 1063.0, "text": " But it's still in some borders, you know, we are somehow oscillating, but there is not really clear way.", "tokens": [50364, 583, 309, 311, 920, 294, 512, 16287, 11, 291, 458, 11, 321, 366, 6063, 18225, 990, 11, 457, 456, 307, 406, 534, 1850, 636, 13, 50764], "temperature": 0.0, "avg_logprob": -0.18670920039830582, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.05260284245014191}, {"id": 189, "seek": 105500, "start": 1063.0, "end": 1065.0, "text": " But the queue size is okay now.", "tokens": [50764, 583, 264, 18639, 2744, 307, 1392, 586, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18670920039830582, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.05260284245014191}, {"id": 190, "seek": 105500, "start": 1065.0, "end": 1069.0, "text": " You can see because you have some data there, but not zero.", "tokens": [50864, 509, 393, 536, 570, 291, 362, 512, 1412, 456, 11, 457, 406, 4018, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18670920039830582, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.05260284245014191}, {"id": 191, "seek": 105500, "start": 1069.0, "end": 1074.0, "text": " So there's an issue with the scrapers, as I said.", "tokens": [51064, 407, 456, 311, 364, 2734, 365, 264, 23138, 433, 11, 382, 286, 848, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18670920039830582, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.05260284245014191}, {"id": 192, "seek": 105500, "start": 1074.0, "end": 1079.0, "text": " And this is actually the thing that you will have to look for the patterns.", "tokens": [51314, 400, 341, 307, 767, 264, 551, 300, 291, 486, 362, 281, 574, 337, 264, 8294, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18670920039830582, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.05260284245014191}, {"id": 193, "seek": 107900, "start": 1080.0, "end": 1082.0, "text": " Really important in graphs.", "tokens": [50414, 4083, 1021, 294, 24877, 13, 50514], "temperature": 0.0, "avg_logprob": -0.13266404108567673, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.3161461651325226}, {"id": 194, "seek": 107900, "start": 1082.0, "end": 1087.0, "text": " You can see all the different ones and there are a lot of papers on the Internet that you can find.", "tokens": [50514, 509, 393, 536, 439, 264, 819, 2306, 293, 456, 366, 257, 688, 295, 10577, 322, 264, 7703, 300, 291, 393, 915, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13266404108567673, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.3161461651325226}, {"id": 195, "seek": 107900, "start": 1087.0, "end": 1090.0, "text": " What to look for on your specific application.", "tokens": [50764, 708, 281, 574, 337, 322, 428, 2685, 3861, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13266404108567673, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.3161461651325226}, {"id": 196, "seek": 107900, "start": 1090.0, "end": 1094.0, "text": " So, yeah, but not look to the numbers.", "tokens": [50914, 407, 11, 1338, 11, 457, 406, 574, 281, 264, 3547, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13266404108567673, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.3161461651325226}, {"id": 197, "seek": 107900, "start": 1094.0, "end": 1096.0, "text": " Numbers don't say you anything.", "tokens": [51114, 22592, 1616, 500, 380, 584, 291, 1340, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13266404108567673, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.3161461651325226}, {"id": 198, "seek": 107900, "start": 1096.0, "end": 1103.0, "text": " You can usually get the higher numbers if you change up the hardware that you are running on.", "tokens": [51214, 509, 393, 2673, 483, 264, 2946, 3547, 498, 291, 1319, 493, 264, 8837, 300, 291, 366, 2614, 322, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13266404108567673, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.3161461651325226}, {"id": 199, "seek": 110300, "start": 1103.0, "end": 1113.0, "text": " But if you can optimize it on the some hardware that you have, you will surely get the big numbers on almost anywhere.", "tokens": [50364, 583, 498, 291, 393, 19719, 309, 322, 264, 512, 8837, 300, 291, 362, 11, 291, 486, 11468, 483, 264, 955, 3547, 322, 1920, 4992, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2006483859703189, "compression_ratio": 1.4792899408284024, "no_speech_prob": 0.058439649641513824}, {"id": 200, "seek": 110300, "start": 1113.0, "end": 1118.0, "text": " So some tips and tricks for me.", "tokens": [50864, 407, 512, 6082, 293, 11733, 337, 385, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2006483859703189, "compression_ratio": 1.4792899408284024, "no_speech_prob": 0.058439649641513824}, {"id": 201, "seek": 110300, "start": 1118.0, "end": 1126.0, "text": " During the way that we have started playing with the performance, we have developed a lot of tools.", "tokens": [51114, 6842, 264, 636, 300, 321, 362, 1409, 2433, 365, 264, 3389, 11, 321, 362, 4743, 257, 688, 295, 3873, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2006483859703189, "compression_ratio": 1.4792899408284024, "no_speech_prob": 0.058439649641513824}, {"id": 202, "seek": 112600, "start": 1126.0, "end": 1135.0, "text": " First is the database manipulation tool, which is effectively giving you a Json API.", "tokens": [50364, 2386, 307, 264, 8149, 26475, 2290, 11, 597, 307, 8659, 2902, 291, 257, 508, 3015, 9362, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17131539583206176, "compression_ratio": 1.4190476190476191, "no_speech_prob": 0.1594020277261734}, {"id": 203, "seek": 112600, "start": 1135.0, "end": 1143.0, "text": " And just with the Json API, you can create DML for almost any databases.", "tokens": [50814, 400, 445, 365, 264, 508, 3015, 9362, 11, 291, 393, 1884, 413, 12683, 337, 1920, 604, 22380, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17131539583206176, "compression_ratio": 1.4190476190476191, "no_speech_prob": 0.1594020277261734}, {"id": 204, "seek": 112600, "start": 1143.0, "end": 1147.0, "text": " We have now probably MySQL, Postgres, and Oracle there.", "tokens": [51214, 492, 362, 586, 1391, 1222, 39934, 11, 10223, 45189, 11, 293, 25654, 456, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17131539583206176, "compression_ratio": 1.4190476190476191, "no_speech_prob": 0.1594020277261734}, {"id": 205, "seek": 112600, "start": 1147.0, "end": 1153.0, "text": " So it's just you don't need to have a lot of different JDBC connectors in your code.", "tokens": [51414, 407, 309, 311, 445, 291, 500, 380, 643, 281, 362, 257, 688, 295, 819, 37082, 7869, 31865, 294, 428, 3089, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17131539583206176, "compression_ratio": 1.4190476190476191, "no_speech_prob": 0.1594020277261734}, {"id": 206, "seek": 115300, "start": 1153.0, "end": 1156.0, "text": " We'll just deploy this and it will take care of it.", "tokens": [50364, 492, 603, 445, 7274, 341, 293, 309, 486, 747, 1127, 295, 309, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2564457654953003, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.15181884169578552}, {"id": 207, "seek": 115300, "start": 1156.0, "end": 1162.0, "text": " We have also implemented some kind of the load generator that can generate you.", "tokens": [50514, 492, 362, 611, 12270, 512, 733, 295, 264, 3677, 19265, 300, 393, 8460, 291, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2564457654953003, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.15181884169578552}, {"id": 208, "seek": 115300, "start": 1162.0, "end": 1167.0, "text": " Load for, I would say, constant load, P-codes and all this kind of stuff.", "tokens": [50814, 48408, 337, 11, 286, 576, 584, 11, 5754, 3677, 11, 430, 12, 66, 4789, 293, 439, 341, 733, 295, 1507, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2564457654953003, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.15181884169578552}, {"id": 209, "seek": 115300, "start": 1167.0, "end": 1173.0, "text": " We have also some automation and the other, like MySQL auto-tune, that's it.", "tokens": [51064, 492, 362, 611, 512, 17769, 293, 264, 661, 11, 411, 1222, 39934, 8399, 12, 83, 2613, 11, 300, 311, 309, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2564457654953003, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.15181884169578552}, {"id": 210, "seek": 117300, "start": 1173.0, "end": 1184.0, "text": " We are pretty proud of that because it can basically tune your MySQL to the whole VM that you are running it or physical machine.", "tokens": [50364, 492, 366, 1238, 4570, 295, 300, 570, 309, 393, 1936, 10864, 428, 1222, 39934, 281, 264, 1379, 18038, 300, 291, 366, 2614, 309, 420, 4001, 3479, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12178274410874096, "compression_ratio": 1.44, "no_speech_prob": 0.3162097930908203}, {"id": 211, "seek": 117300, "start": 1184.0, "end": 1188.0, "text": " And you would say that it's easy, but it's not.", "tokens": [50914, 400, 291, 576, 584, 300, 309, 311, 1858, 11, 457, 309, 311, 406, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12178274410874096, "compression_ratio": 1.44, "no_speech_prob": 0.3162097930908203}, {"id": 212, "seek": 117300, "start": 1188.0, "end": 1195.0, "text": " You know it's hard when you look on the seven or eight page of the Google.", "tokens": [51114, 509, 458, 309, 311, 1152, 562, 291, 574, 322, 264, 3407, 420, 3180, 3028, 295, 264, 3329, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12178274410874096, "compression_ratio": 1.44, "no_speech_prob": 0.3162097930908203}, {"id": 213, "seek": 119500, "start": 1195.0, "end": 1202.0, "text": " You know, in this phase, you know, we are probably not on the good shape and this is one of the things.", "tokens": [50364, 509, 458, 11, 294, 341, 5574, 11, 291, 458, 11, 321, 366, 1391, 406, 322, 264, 665, 3909, 293, 341, 307, 472, 295, 264, 721, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1684701307764593, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.08519542217254639}, {"id": 214, "seek": 119500, "start": 1202.0, "end": 1206.0, "text": " So please take a look if you are working on MySQL.", "tokens": [50714, 407, 1767, 747, 257, 574, 498, 291, 366, 1364, 322, 1222, 39934, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1684701307764593, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.08519542217254639}, {"id": 215, "seek": 119500, "start": 1206.0, "end": 1210.0, "text": " We have some counting of the parameters for the database there.", "tokens": [50914, 492, 362, 512, 13251, 295, 264, 9834, 337, 264, 8149, 456, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1684701307764593, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.08519542217254639}, {"id": 216, "seek": 119500, "start": 1210.0, "end": 1215.0, "text": " And it will save you a lot of time if you want to tune up your database.", "tokens": [51114, 400, 309, 486, 3155, 291, 257, 688, 295, 565, 498, 291, 528, 281, 10864, 493, 428, 8149, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1684701307764593, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.08519542217254639}, {"id": 217, "seek": 119500, "start": 1215.0, "end": 1217.0, "text": " We have spent the time for you.", "tokens": [51364, 492, 362, 4418, 264, 565, 337, 291, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1684701307764593, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.08519542217254639}, {"id": 218, "seek": 119500, "start": 1217.0, "end": 1223.0, "text": " And secondly, we have implemented the metadata to Prometheus Creper.", "tokens": [51464, 400, 26246, 11, 321, 362, 12270, 264, 26603, 281, 2114, 649, 42209, 9549, 610, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1684701307764593, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.08519542217254639}, {"id": 219, "seek": 122300, "start": 1223.0, "end": 1229.0, "text": " So you can get rid of one, one struggle point in your monitoring environment.", "tokens": [50364, 407, 291, 393, 483, 3973, 295, 472, 11, 472, 7799, 935, 294, 428, 11028, 2823, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15253994382660965, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.04742768034338951}, {"id": 220, "seek": 122300, "start": 1229.0, "end": 1238.0, "text": " And we are also starting working on the fast Creper for the, for your monitoring, for our monitoring stack.", "tokens": [50664, 400, 321, 366, 611, 2891, 1364, 322, 264, 2370, 9549, 610, 337, 264, 11, 337, 428, 11028, 11, 337, 527, 11028, 8630, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15253994382660965, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.04742768034338951}, {"id": 221, "seek": 122300, "start": 1238.0, "end": 1242.0, "text": " But it's not done yet because it's quite more complicated than before.", "tokens": [51114, 583, 309, 311, 406, 1096, 1939, 570, 309, 311, 1596, 544, 6179, 813, 949, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15253994382660965, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.04742768034338951}, {"id": 222, "seek": 122300, "start": 1242.0, "end": 1246.0, "text": " So, but yeah, please take a look.", "tokens": [51314, 407, 11, 457, 1338, 11, 1767, 747, 257, 574, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15253994382660965, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.04742768034338951}, {"id": 223, "seek": 122300, "start": 1246.0, "end": 1249.0, "text": " I will have the links on our GitHub and everything.", "tokens": [51514, 286, 486, 362, 264, 6123, 322, 527, 23331, 293, 1203, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15253994382660965, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.04742768034338951}, {"id": 224, "seek": 124900, "start": 1249.0, "end": 1260.0, "text": " It's all open source. So you can just also add some code there if you want.", "tokens": [50364, 467, 311, 439, 1269, 4009, 13, 407, 291, 393, 445, 611, 909, 512, 3089, 456, 498, 291, 528, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18570402341011244, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.09716606140136719}, {"id": 225, "seek": 124900, "start": 1260.0, "end": 1263.0, "text": " Yeah, so, so I have started quite early on than I should.", "tokens": [50914, 865, 11, 370, 11, 370, 286, 362, 1409, 1596, 2440, 322, 813, 286, 820, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18570402341011244, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.09716606140136719}, {"id": 226, "seek": 124900, "start": 1263.0, "end": 1265.0, "text": " So I have some time now then.", "tokens": [51064, 407, 286, 362, 512, 565, 586, 550, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18570402341011244, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.09716606140136719}, {"id": 227, "seek": 124900, "start": 1265.0, "end": 1271.0, "text": " But okay, we can just summarize everything now.", "tokens": [51164, 583, 1392, 11, 321, 393, 445, 20858, 1203, 586, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18570402341011244, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.09716606140136719}, {"id": 228, "seek": 124900, "start": 1271.0, "end": 1278.0, "text": " And I hope for some discussion before you guys who done performance testing.", "tokens": [51464, 400, 286, 1454, 337, 512, 5017, 949, 291, 1074, 567, 1096, 3389, 4997, 13, 51814], "temperature": 0.0, "avg_logprob": -0.18570402341011244, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.09716606140136719}, {"id": 229, "seek": 127800, "start": 1278.0, "end": 1282.0, "text": " So for me, don't be scared around the performance testing.", "tokens": [50364, 407, 337, 385, 11, 500, 380, 312, 5338, 926, 264, 3389, 4997, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08455267057314024, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.10630496591329575}, {"id": 230, "seek": 127800, "start": 1282.0, "end": 1285.0, "text": " He's not like some, some monster.", "tokens": [50564, 634, 311, 406, 411, 512, 11, 512, 10090, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08455267057314024, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.10630496591329575}, {"id": 231, "seek": 127800, "start": 1285.0, "end": 1289.0, "text": " People are mostly just like creating the monster from it.", "tokens": [50714, 3432, 366, 5240, 445, 411, 4084, 264, 10090, 490, 309, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08455267057314024, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.10630496591329575}, {"id": 232, "seek": 127800, "start": 1289.0, "end": 1293.0, "text": " But if you don't need that for some legal options or anything, it's fine.", "tokens": [50914, 583, 498, 291, 500, 380, 643, 300, 337, 512, 5089, 3956, 420, 1340, 11, 309, 311, 2489, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08455267057314024, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.10630496591329575}, {"id": 233, "seek": 127800, "start": 1293.0, "end": 1295.0, "text": " You can play with it. It's funny.", "tokens": [51114, 509, 393, 862, 365, 309, 13, 467, 311, 4074, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08455267057314024, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.10630496591329575}, {"id": 234, "seek": 127800, "start": 1295.0, "end": 1299.0, "text": " You will gain a lot of knowledge about your, about your product on that.", "tokens": [51214, 509, 486, 6052, 257, 688, 295, 3601, 466, 428, 11, 466, 428, 1674, 322, 300, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08455267057314024, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.10630496591329575}, {"id": 235, "seek": 129900, "start": 1299.0, "end": 1307.0, "text": " And especially if you are QE, I mean, a lot of QE folks don't have the", "tokens": [50364, 400, 2318, 498, 291, 366, 1249, 36, 11, 286, 914, 11, 257, 688, 295, 1249, 36, 4024, 500, 380, 362, 264, 50764], "temperature": 0.0, "avg_logprob": -0.13461360012192325, "compression_ratio": 1.55, "no_speech_prob": 0.4098234474658966}, {"id": 236, "seek": 129900, "start": 1307.0, "end": 1309.0, "text": " necessary knowledge about the product itself.", "tokens": [50764, 4818, 3601, 466, 264, 1674, 2564, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13461360012192325, "compression_ratio": 1.55, "no_speech_prob": 0.4098234474658966}, {"id": 237, "seek": 129900, "start": 1309.0, "end": 1313.0, "text": " And this helps a lot to get through everything because in the end,", "tokens": [50864, 400, 341, 3665, 257, 688, 281, 483, 807, 1203, 570, 294, 264, 917, 11, 51064], "temperature": 0.0, "avg_logprob": -0.13461360012192325, "compression_ratio": 1.55, "no_speech_prob": 0.4098234474658966}, {"id": 238, "seek": 129900, "start": 1313.0, "end": 1317.0, "text": " you will, you will just go through the code and look for, for the mistakes or", "tokens": [51064, 291, 486, 11, 291, 486, 445, 352, 807, 264, 3089, 293, 574, 337, 11, 337, 264, 8038, 420, 51264], "temperature": 0.0, "avg_logprob": -0.13461360012192325, "compression_ratio": 1.55, "no_speech_prob": 0.4098234474658966}, {"id": 239, "seek": 129900, "start": 1317.0, "end": 1323.0, "text": " something like that. So that really helps a lot.", "tokens": [51264, 746, 411, 300, 13, 407, 300, 534, 3665, 257, 688, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13461360012192325, "compression_ratio": 1.55, "no_speech_prob": 0.4098234474658966}, {"id": 240, "seek": 132300, "start": 1323.0, "end": 1327.0, "text": " So, gather all the metrics you can.", "tokens": [50364, 407, 11, 5448, 439, 264, 16367, 291, 393, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2391318702697754, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.07724840939044952}, {"id": 241, "seek": 132300, "start": 1327.0, "end": 1333.0, "text": " And well, we are also writing our blog and all the repositories will be on the", "tokens": [50564, 400, 731, 11, 321, 366, 611, 3579, 527, 6968, 293, 439, 264, 22283, 2083, 486, 312, 322, 264, 50864], "temperature": 0.0, "avg_logprob": -0.2391318702697754, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.07724840939044952}, {"id": 242, "seek": 132300, "start": 1333.0, "end": 1336.0, "text": " other side before the two, before the two links.", "tokens": [50864, 661, 1252, 949, 264, 732, 11, 949, 264, 732, 6123, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2391318702697754, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.07724840939044952}, {"id": 243, "seek": 132300, "start": 1336.0, "end": 1341.0, "text": " So I would be happy to hear from you, you know, like repository or organization", "tokens": [51014, 407, 286, 576, 312, 2055, 281, 1568, 490, 291, 11, 291, 458, 11, 411, 25841, 420, 4475, 51264], "temperature": 0.0, "avg_logprob": -0.2391318702697754, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.07724840939044952}, {"id": 244, "seek": 132300, "start": 1341.0, "end": 1343.0, "text": " before the two links.", "tokens": [51264, 949, 264, 732, 6123, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2391318702697754, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.07724840939044952}, {"id": 245, "seek": 132300, "start": 1343.0, "end": 1347.0, "text": " And yeah, that's probably it for, for my talk.", "tokens": [51364, 400, 1338, 11, 300, 311, 1391, 309, 337, 11, 337, 452, 751, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2391318702697754, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.07724840939044952}, {"id": 246, "seek": 132300, "start": 1347.0, "end": 1349.0, "text": " As I said, I have started a lot earlier.", "tokens": [51564, 1018, 286, 848, 11, 286, 362, 1409, 257, 688, 3071, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2391318702697754, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.07724840939044952}, {"id": 247, "seek": 134900, "start": 1349.0, "end": 1354.0, "text": " So thank you very much for listening to me.", "tokens": [50364, 407, 1309, 291, 588, 709, 337, 4764, 281, 385, 13, 50614], "temperature": 0.0, "avg_logprob": -0.3002444597390982, "compression_ratio": 1.0389610389610389, "no_speech_prob": 0.06750985234975815}, {"id": 248, "seek": 134900, "start": 1354.0, "end": 1364.0, "text": " Please do have some questions.", "tokens": [50614, 2555, 360, 362, 512, 1651, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3002444597390982, "compression_ratio": 1.0389610389610389, "no_speech_prob": 0.06750985234975815}, {"id": 249, "seek": 134900, "start": 1364.0, "end": 1368.0, "text": " Yeah.", "tokens": [51114, 865, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3002444597390982, "compression_ratio": 1.0389610389610389, "no_speech_prob": 0.06750985234975815}, {"id": 250, "seek": 136800, "start": 1368.0, "end": 1372.0, "text": " So my question would be, so what kind of experience do you have in your", "tokens": [50364, 407, 452, 1168, 576, 312, 11, 370, 437, 733, 295, 1752, 360, 291, 362, 294, 428, 50564], "temperature": 0.0, "avg_logprob": -0.33438813508446535, "compression_ratio": 1.962962962962963, "no_speech_prob": 0.8048601150512695}, {"id": 251, "seek": 136800, "start": 1372.0, "end": 1376.0, "text": " complex system? And then you see something happens there.", "tokens": [50564, 3997, 1185, 30, 400, 550, 291, 536, 746, 2314, 456, 13, 50764], "temperature": 0.0, "avg_logprob": -0.33438813508446535, "compression_ratio": 1.962962962962963, "no_speech_prob": 0.8048601150512695}, {"id": 252, "seek": 136800, "start": 1376.0, "end": 1379.0, "text": " And say, okay, here, here's the latest EP or something like that.", "tokens": [50764, 400, 584, 11, 1392, 11, 510, 11, 510, 311, 264, 6792, 25330, 420, 746, 411, 300, 13, 50914], "temperature": 0.0, "avg_logprob": -0.33438813508446535, "compression_ratio": 1.962962962962963, "no_speech_prob": 0.8048601150512695}, {"id": 253, "seek": 136800, "start": 1379.0, "end": 1384.0, "text": " Which experience do you have with, let's say, find the cause of the problem?", "tokens": [50914, 3013, 1752, 360, 291, 362, 365, 11, 718, 311, 584, 11, 915, 264, 3082, 295, 264, 1154, 30, 51164], "temperature": 0.0, "avg_logprob": -0.33438813508446535, "compression_ratio": 1.962962962962963, "no_speech_prob": 0.8048601150512695}, {"id": 254, "seek": 136800, "start": 1384.0, "end": 1387.0, "text": " Cause, so when something happens randomly, you will see it with wrap and say,", "tokens": [51164, 10865, 11, 370, 562, 746, 2314, 16979, 11, 291, 486, 536, 309, 365, 7019, 293, 584, 11, 51314], "temperature": 0.0, "avg_logprob": -0.33438813508446535, "compression_ratio": 1.962962962962963, "no_speech_prob": 0.8048601150512695}, {"id": 255, "seek": 136800, "start": 1387.0, "end": 1390.0, "text": " okay, something happens there. And that's maybe annoying, especially when it", "tokens": [51314, 1392, 11, 746, 2314, 456, 13, 400, 300, 311, 1310, 11304, 11, 2318, 562, 309, 51464], "temperature": 0.0, "avg_logprob": -0.33438813508446535, "compression_ratio": 1.962962962962963, "no_speech_prob": 0.8048601150512695}, {"id": 256, "seek": 136800, "start": 1390.0, "end": 1392.0, "text": " happens, happens randomly.", "tokens": [51464, 2314, 11, 2314, 16979, 13, 51564], "temperature": 0.0, "avg_logprob": -0.33438813508446535, "compression_ratio": 1.962962962962963, "no_speech_prob": 0.8048601150512695}, {"id": 257, "seek": 136800, "start": 1392.0, "end": 1395.0, "text": " So what kind of strategies you are using then when you know, okay, there is", "tokens": [51564, 407, 437, 733, 295, 9029, 291, 366, 1228, 550, 562, 291, 458, 11, 1392, 11, 456, 307, 51714], "temperature": 0.0, "avg_logprob": -0.33438813508446535, "compression_ratio": 1.962962962962963, "no_speech_prob": 0.8048601150512695}, {"id": 258, "seek": 139500, "start": 1395.0, "end": 1400.0, "text": " something to find cause of the problem in the complex chain.", "tokens": [50364, 746, 281, 915, 3082, 295, 264, 1154, 294, 264, 3997, 5021, 13, 50614], "temperature": 0.0, "avg_logprob": -0.19455373128255207, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0678887739777565}, {"id": 259, "seek": 139500, "start": 1400.0, "end": 1404.0, "text": " Yeah, so this is, okay.", "tokens": [50614, 865, 11, 370, 341, 307, 11, 1392, 13, 50814], "temperature": 0.0, "avg_logprob": -0.19455373128255207, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0678887739777565}, {"id": 260, "seek": 139500, "start": 1404.0, "end": 1410.0, "text": " So, so the question was actually, if there are some, some changes in the", "tokens": [50814, 407, 11, 370, 264, 1168, 390, 767, 11, 498, 456, 366, 512, 11, 512, 2962, 294, 264, 51114], "temperature": 0.0, "avg_logprob": -0.19455373128255207, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0678887739777565}, {"id": 261, "seek": 139500, "start": 1410.0, "end": 1415.0, "text": " environment, some like latency things or everything, how we can deal with that", "tokens": [51114, 2823, 11, 512, 411, 27043, 721, 420, 1203, 11, 577, 321, 393, 2028, 365, 300, 51364], "temperature": 0.0, "avg_logprob": -0.19455373128255207, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0678887739777565}, {"id": 262, "seek": 139500, "start": 1415.0, "end": 1419.0, "text": " and how we can find the causes of the problems.", "tokens": [51364, 293, 577, 321, 393, 915, 264, 7700, 295, 264, 2740, 13, 51564], "temperature": 0.0, "avg_logprob": -0.19455373128255207, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0678887739777565}, {"id": 263, "seek": 141900, "start": 1419.0, "end": 1424.0, "text": " Yeah, so surely this is the main problem of the whole performance testing", "tokens": [50364, 865, 11, 370, 11468, 341, 307, 264, 2135, 1154, 295, 264, 1379, 3389, 4997, 50614], "temperature": 0.0, "avg_logprob": -0.23511664072672525, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.17400871217250824}, {"id": 264, "seek": 141900, "start": 1424.0, "end": 1430.0, "text": " outside of the like isolating and, you know, well, you need the metrics", "tokens": [50614, 2380, 295, 264, 411, 48912, 293, 11, 291, 458, 11, 731, 11, 291, 643, 264, 16367, 50914], "temperature": 0.0, "avg_logprob": -0.23511664072672525, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.17400871217250824}, {"id": 265, "seek": 141900, "start": 1430.0, "end": 1438.0, "text": " from everything because then you somehow at least it will help you to, to get", "tokens": [50914, 490, 1203, 570, 550, 291, 6063, 412, 1935, 309, 486, 854, 291, 281, 11, 281, 483, 51314], "temperature": 0.0, "avg_logprob": -0.23511664072672525, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.17400871217250824}, {"id": 266, "seek": 141900, "start": 1438.0, "end": 1443.0, "text": " all the things in the right timeline and you can see the need and picks what", "tokens": [51314, 439, 264, 721, 294, 264, 558, 12933, 293, 291, 393, 536, 264, 643, 293, 16137, 437, 51564], "temperature": 0.0, "avg_logprob": -0.23511664072672525, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.17400871217250824}, {"id": 267, "seek": 144300, "start": 1443.0, "end": 1450.0, "text": " could happen. But if it's like something that is really bad, you can find it", "tokens": [50364, 727, 1051, 13, 583, 498, 309, 311, 411, 746, 300, 307, 534, 1578, 11, 291, 393, 915, 309, 50714], "temperature": 0.0, "avg_logprob": -0.1508245772503792, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.37847867608070374}, {"id": 268, "seek": 144300, "start": 1450.0, "end": 1456.0, "text": " usually because it will mostly, it will just disappear in all the logs because", "tokens": [50714, 2673, 570, 309, 486, 5240, 11, 309, 486, 445, 11596, 294, 439, 264, 20820, 570, 51014], "temperature": 0.0, "avg_logprob": -0.1508245772503792, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.37847867608070374}, {"id": 269, "seek": 144300, "start": 1456.0, "end": 1460.0, "text": " it can be something like if you have the smaller machines, I have to write", "tokens": [51014, 309, 393, 312, 746, 411, 498, 291, 362, 264, 4356, 8379, 11, 286, 362, 281, 2464, 51214], "temperature": 0.0, "avg_logprob": -0.1508245772503792, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.37847867608070374}, {"id": 270, "seek": 144300, "start": 1460.0, "end": 1465.0, "text": " once on the, some microchips, you know, it was funny thing that you fill up your", "tokens": [51214, 1564, 322, 264, 11, 512, 4532, 339, 2600, 11, 291, 458, 11, 309, 390, 4074, 551, 300, 291, 2836, 493, 428, 51464], "temperature": 0.0, "avg_logprob": -0.1508245772503792, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.37847867608070374}, {"id": 271, "seek": 144300, "start": 1465.0, "end": 1470.0, "text": " TCP queue. You cannot find that anywhere in the world.", "tokens": [51464, 48965, 18639, 13, 509, 2644, 915, 300, 4992, 294, 264, 1002, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1508245772503792, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.37847867608070374}, {"id": 272, "seek": 147000, "start": 1470.0, "end": 1476.0, "text": " So at that case, you will just repeat the testing, even if it's take long", "tokens": [50364, 407, 412, 300, 1389, 11, 291, 486, 445, 7149, 264, 4997, 11, 754, 498, 309, 311, 747, 938, 50664], "temperature": 0.0, "avg_logprob": -0.14838088077047598, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.07447417080402374}, {"id": 273, "seek": 147000, "start": 1476.0, "end": 1482.0, "text": " and you will see if same thing happens or not. I don't have any other like", "tokens": [50664, 293, 291, 486, 536, 498, 912, 551, 2314, 420, 406, 13, 286, 500, 380, 362, 604, 661, 411, 50964], "temperature": 0.0, "avg_logprob": -0.14838088077047598, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.07447417080402374}, {"id": 274, "seek": 147000, "start": 1482.0, "end": 1486.0, "text": " recommendation for that because this is like really main problem if you are", "tokens": [50964, 11879, 337, 300, 570, 341, 307, 411, 534, 2135, 1154, 498, 291, 366, 51164], "temperature": 0.0, "avg_logprob": -0.14838088077047598, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.07447417080402374}, {"id": 275, "seek": 147000, "start": 1486.0, "end": 1492.0, "text": " doing it outside of the ideal environment. This surely will face this, but", "tokens": [51164, 884, 309, 2380, 295, 264, 7157, 2823, 13, 639, 11468, 486, 1851, 341, 11, 457, 51464], "temperature": 0.0, "avg_logprob": -0.14838088077047598, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.07447417080402374}, {"id": 276, "seek": 147000, "start": 1492.0, "end": 1498.0, "text": " mostly it's not happening that often, I would say, because you can have", "tokens": [51464, 5240, 309, 311, 406, 2737, 300, 2049, 11, 286, 576, 584, 11, 570, 291, 393, 362, 51764], "temperature": 0.0, "avg_logprob": -0.14838088077047598, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.07447417080402374}, {"id": 277, "seek": 149800, "start": 1498.0, "end": 1503.0, "text": " observation for and tracing for a lot of things and most of the times you can", "tokens": [50364, 14816, 337, 293, 25262, 337, 257, 688, 295, 721, 293, 881, 295, 264, 1413, 291, 393, 50614], "temperature": 0.0, "avg_logprob": -0.18243105999835127, "compression_ratio": 1.7094972067039107, "no_speech_prob": 0.07876536250114441}, {"id": 278, "seek": 149800, "start": 1503.0, "end": 1509.0, "text": " like colorate those things together. So you exactly know what is wrong,", "tokens": [50614, 411, 2017, 473, 729, 721, 1214, 13, 407, 291, 2293, 458, 437, 307, 2085, 11, 50914], "temperature": 0.0, "avg_logprob": -0.18243105999835127, "compression_ratio": 1.7094972067039107, "no_speech_prob": 0.07876536250114441}, {"id": 279, "seek": 149800, "start": 1509.0, "end": 1519.0, "text": " especially for the network. You can get a lot of network traffic, like, how is", "tokens": [50914, 2318, 337, 264, 3209, 13, 509, 393, 483, 257, 688, 295, 3209, 6419, 11, 411, 11, 577, 307, 51414], "temperature": 0.0, "avg_logprob": -0.18243105999835127, "compression_ratio": 1.7094972067039107, "no_speech_prob": 0.07876536250114441}, {"id": 280, "seek": 149800, "start": 1519.0, "end": 1525.0, "text": " the word? You can see all the traffic and what is going on, especially on one", "tokens": [51414, 264, 1349, 30, 509, 393, 536, 439, 264, 6419, 293, 437, 307, 516, 322, 11, 2318, 322, 472, 51714], "temperature": 0.0, "avg_logprob": -0.18243105999835127, "compression_ratio": 1.7094972067039107, "no_speech_prob": 0.07876536250114441}, {"id": 281, "seek": 152500, "start": 1525.0, "end": 1532.0, "text": " line. So then you can usually put those graphs together and you know at the time.", "tokens": [50364, 1622, 13, 407, 550, 291, 393, 2673, 829, 729, 24877, 1214, 293, 291, 458, 412, 264, 565, 13, 50714], "temperature": 0.0, "avg_logprob": -0.28808613146765755, "compression_ratio": 1.4125874125874125, "no_speech_prob": 0.07045646011829376}, {"id": 282, "seek": 152500, "start": 1532.0, "end": 1535.0, "text": " If that is okay, answer for you.", "tokens": [50714, 759, 300, 307, 1392, 11, 1867, 337, 291, 13, 50864], "temperature": 0.0, "avg_logprob": -0.28808613146765755, "compression_ratio": 1.4125874125874125, "no_speech_prob": 0.07045646011829376}, {"id": 283, "seek": 152500, "start": 1535.0, "end": 1538.0, "text": " Yeah, yeah, yeah.", "tokens": [50864, 865, 11, 1338, 11, 1338, 13, 51014], "temperature": 0.0, "avg_logprob": -0.28808613146765755, "compression_ratio": 1.4125874125874125, "no_speech_prob": 0.07045646011829376}, {"id": 284, "seek": 152500, "start": 1542.0, "end": 1550.0, "text": " Thanks for the talk. I was just wondering that how to use the traces,", "tokens": [51214, 2561, 337, 264, 751, 13, 286, 390, 445, 6359, 300, 577, 281, 764, 264, 26076, 11, 51614], "temperature": 0.0, "avg_logprob": -0.28808613146765755, "compression_ratio": 1.4125874125874125, "no_speech_prob": 0.07045646011829376}, {"id": 285, "seek": 155000, "start": 1550.0, "end": 1556.0, "text": " analyzing the traces, because I've seen that you mentioned metrics and traces.", "tokens": [50364, 23663, 264, 26076, 11, 570, 286, 600, 1612, 300, 291, 2835, 16367, 293, 26076, 13, 50664], "temperature": 0.0, "avg_logprob": -0.17172445569719588, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.1435200273990631}, {"id": 286, "seek": 155000, "start": 1556.0, "end": 1559.0, "text": " Sorry, can you speak more louder?", "tokens": [50664, 4919, 11, 393, 291, 1710, 544, 22717, 30, 50814], "temperature": 0.0, "avg_logprob": -0.17172445569719588, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.1435200273990631}, {"id": 287, "seek": 155000, "start": 1559.0, "end": 1561.0, "text": " Oh, yeah. Can you hear me now?", "tokens": [50814, 876, 11, 1338, 13, 1664, 291, 1568, 385, 586, 30, 50914], "temperature": 0.0, "avg_logprob": -0.17172445569719588, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.1435200273990631}, {"id": 288, "seek": 155000, "start": 1561.0, "end": 1562.0, "text": " Yes.", "tokens": [50914, 1079, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17172445569719588, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.1435200273990631}, {"id": 289, "seek": 155000, "start": 1562.0, "end": 1568.0, "text": " Yeah, I was wondering how do you use the traces for performance testing,", "tokens": [50964, 865, 11, 286, 390, 6359, 577, 360, 291, 764, 264, 26076, 337, 3389, 4997, 11, 51264], "temperature": 0.0, "avg_logprob": -0.17172445569719588, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.1435200273990631}, {"id": 290, "seek": 155000, "start": 1568.0, "end": 1571.0, "text": " because when you collect the traces, how do you deal with the sampling of the", "tokens": [51264, 570, 562, 291, 2500, 264, 26076, 11, 577, 360, 291, 2028, 365, 264, 21179, 295, 264, 51414], "temperature": 0.0, "avg_logprob": -0.17172445569719588, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.1435200273990631}, {"id": 291, "seek": 155000, "start": 1571.0, "end": 1576.0, "text": " traces? And if you miss something because the sampling is bad or you are not", "tokens": [51414, 26076, 30, 400, 498, 291, 1713, 746, 570, 264, 21179, 307, 1578, 420, 291, 366, 406, 51664], "temperature": 0.0, "avg_logprob": -0.17172445569719588, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.1435200273990631}, {"id": 292, "seek": 157600, "start": 1576.0, "end": 1582.0, "text": " sampling everything, maybe you have to infer something from the metrics and the", "tokens": [50364, 21179, 1203, 11, 1310, 291, 362, 281, 13596, 746, 490, 264, 16367, 293, 264, 50664], "temperature": 0.0, "avg_logprob": -0.14014150982811338, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.03623151779174805}, {"id": 293, "seek": 157600, "start": 1582.0, "end": 1587.0, "text": " traces, I was wondering how do you deal with the traces? And if you use distributed", "tokens": [50664, 26076, 11, 286, 390, 6359, 577, 360, 291, 2028, 365, 264, 26076, 30, 400, 498, 291, 764, 12631, 50914], "temperature": 0.0, "avg_logprob": -0.14014150982811338, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.03623151779174805}, {"id": 294, "seek": 157600, "start": 1587.0, "end": 1594.0, "text": " tracing in a large project like collecting all this kind of stuff.", "tokens": [50914, 25262, 294, 257, 2416, 1716, 411, 12510, 439, 341, 733, 295, 1507, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14014150982811338, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.03623151779174805}, {"id": 295, "seek": 157600, "start": 1594.0, "end": 1599.0, "text": " I'm not sure. I understand the question.", "tokens": [51264, 286, 478, 406, 988, 13, 286, 1223, 264, 1168, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14014150982811338, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.03623151779174805}, {"id": 296, "seek": 157600, "start": 1599.0, "end": 1604.0, "text": " The question actually is, I've seen that you're using collecting the metrics and then", "tokens": [51514, 440, 1168, 767, 307, 11, 286, 600, 1612, 300, 291, 434, 1228, 12510, 264, 16367, 293, 550, 51764], "temperature": 0.0, "avg_logprob": -0.14014150982811338, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.03623151779174805}, {"id": 297, "seek": 160400, "start": 1604.0, "end": 1607.0, "text": " you are analyzing the metrics.", "tokens": [50364, 291, 366, 23663, 264, 16367, 13, 50514], "temperature": 0.0, "avg_logprob": -0.21524577194385314, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.051338277757167816}, {"id": 298, "seek": 160400, "start": 1607.0, "end": 1608.0, "text": " Yeah.", "tokens": [50514, 865, 13, 50564], "temperature": 0.0, "avg_logprob": -0.21524577194385314, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.051338277757167816}, {"id": 299, "seek": 160400, "start": 1608.0, "end": 1610.0, "text": " And what about the traces?", "tokens": [50564, 400, 437, 466, 264, 26076, 30, 50664], "temperature": 0.0, "avg_logprob": -0.21524577194385314, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.051338277757167816}, {"id": 300, "seek": 160400, "start": 1610.0, "end": 1618.0, "text": " Yeah, so, well, the business does not have really that amount of traces that we could", "tokens": [50664, 865, 11, 370, 11, 731, 11, 264, 1606, 775, 406, 362, 534, 300, 2372, 295, 26076, 300, 321, 727, 51064], "temperature": 0.0, "avg_logprob": -0.21524577194385314, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.051338277757167816}, {"id": 301, "seek": 160400, "start": 1618.0, "end": 1625.0, "text": " get from it. We have mostly like JMX metrics, you know, from the Java environment.", "tokens": [51064, 483, 490, 309, 13, 492, 362, 5240, 411, 508, 44, 55, 16367, 11, 291, 458, 11, 490, 264, 10745, 2823, 13, 51414], "temperature": 0.0, "avg_logprob": -0.21524577194385314, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.051338277757167816}, {"id": 302, "seek": 160400, "start": 1625.0, "end": 1632.0, "text": " So that's for us what we analyze. And I'm not sure how can I answer more for the", "tokens": [51414, 407, 300, 311, 337, 505, 437, 321, 12477, 13, 400, 286, 478, 406, 988, 577, 393, 286, 1867, 544, 337, 264, 51764], "temperature": 0.0, "avg_logprob": -0.21524577194385314, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.051338277757167816}, {"id": 303, "seek": 163200, "start": 1632.0, "end": 1636.0, "text": " questions. So I'm sorry. We can discuss it later.", "tokens": [50364, 1651, 13, 407, 286, 478, 2597, 13, 492, 393, 2248, 309, 1780, 13, 50564], "temperature": 0.0, "avg_logprob": -0.21597528457641602, "compression_ratio": 1.2869565217391303, "no_speech_prob": 0.0110851489007473}, {"id": 304, "seek": 163200, "start": 1636.0, "end": 1651.0, "text": " I'm coming to you.", "tokens": [50564, 286, 478, 1348, 281, 291, 13, 51314], "temperature": 0.0, "avg_logprob": -0.21597528457641602, "compression_ratio": 1.2869565217391303, "no_speech_prob": 0.0110851489007473}, {"id": 305, "seek": 163200, "start": 1651.0, "end": 1658.0, "text": " Okay, so my question is about the long running tests. Sometimes the performance", "tokens": [51314, 1033, 11, 370, 452, 1168, 307, 466, 264, 938, 2614, 6921, 13, 4803, 264, 3389, 51664], "temperature": 0.0, "avg_logprob": -0.21597528457641602, "compression_ratio": 1.2869565217391303, "no_speech_prob": 0.0110851489007473}, {"id": 306, "seek": 165800, "start": 1658.0, "end": 1664.0, "text": " validation is visible only after long run, for example, one week, couple of weeks and", "tokens": [50364, 24071, 307, 8974, 787, 934, 938, 1190, 11, 337, 1365, 11, 472, 1243, 11, 1916, 295, 3259, 293, 50664], "temperature": 0.0, "avg_logprob": -0.1846266345701356, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.11846352368593216}, {"id": 307, "seek": 165800, "start": 1664.0, "end": 1670.0, "text": " sometimes even more. So how do you address this in your process or how do you recommend", "tokens": [50664, 2171, 754, 544, 13, 407, 577, 360, 291, 2985, 341, 294, 428, 1399, 420, 577, 360, 291, 2748, 50964], "temperature": 0.0, "avg_logprob": -0.1846266345701356, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.11846352368593216}, {"id": 308, "seek": 165800, "start": 1670.0, "end": 1672.0, "text": " to address this problem?", "tokens": [50964, 281, 2985, 341, 1154, 30, 51064], "temperature": 0.0, "avg_logprob": -0.1846266345701356, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.11846352368593216}, {"id": 309, "seek": 165800, "start": 1672.0, "end": 1681.0, "text": " Yes. So for this, actually colleagues of mine as part of our open source organization,", "tokens": [51064, 1079, 13, 407, 337, 341, 11, 767, 7734, 295, 3892, 382, 644, 295, 527, 1269, 4009, 4475, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1846266345701356, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.11846352368593216}, {"id": 310, "seek": 168100, "start": 1681.0, "end": 1687.0, "text": " they are also developing the long running cluster environment, something like that is", "tokens": [50364, 436, 366, 611, 6416, 264, 938, 2614, 13630, 2823, 11, 746, 411, 300, 307, 50664], "temperature": 0.0, "avg_logprob": -0.16946802434233046, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.33673495054244995}, {"id": 311, "seek": 168100, "start": 1687.0, "end": 1694.0, "text": " like, because, you know, having a long running thing is complicated on itself because you", "tokens": [50664, 411, 11, 570, 11, 291, 458, 11, 1419, 257, 938, 2614, 551, 307, 6179, 322, 2564, 570, 291, 51014], "temperature": 0.0, "avg_logprob": -0.16946802434233046, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.33673495054244995}, {"id": 312, "seek": 168100, "start": 1694.0, "end": 1700.0, "text": " have to manage it a lot, especially on OpenShift or Kubernetes or these kind of things is like", "tokens": [51014, 362, 281, 3067, 309, 257, 688, 11, 2318, 322, 7238, 7774, 2008, 420, 23145, 420, 613, 733, 295, 721, 307, 411, 51314], "temperature": 0.0, "avg_logprob": -0.16946802434233046, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.33673495054244995}, {"id": 313, "seek": 168100, "start": 1700.0, "end": 1704.0, "text": " little problematic before the upgrades and this kind of stuff.", "tokens": [51314, 707, 19011, 949, 264, 24868, 293, 341, 733, 295, 1507, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16946802434233046, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.33673495054244995}, {"id": 314, "seek": 168100, "start": 1704.0, "end": 1710.0, "text": " So we have not dealt with that yet, but we are planning that once we are okay, that we", "tokens": [51514, 407, 321, 362, 406, 15991, 365, 300, 1939, 11, 457, 321, 366, 5038, 300, 1564, 321, 366, 1392, 11, 300, 321, 51814], "temperature": 0.0, "avg_logprob": -0.16946802434233046, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.33673495054244995}, {"id": 315, "seek": 171000, "start": 1710.0, "end": 1717.0, "text": " manage that we have everything prepared for like databases and everything, we want to", "tokens": [50364, 3067, 300, 321, 362, 1203, 4927, 337, 411, 22380, 293, 1203, 11, 321, 528, 281, 50714], "temperature": 0.0, "avg_logprob": -0.14335757871217367, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.16379226744174957}, {"id": 316, "seek": 171000, "start": 1717.0, "end": 1724.0, "text": " get the up running on the long running clusters and like regularly doing the performance tests", "tokens": [50714, 483, 264, 493, 2614, 322, 264, 938, 2614, 23313, 293, 411, 11672, 884, 264, 3389, 6921, 51064], "temperature": 0.0, "avg_logprob": -0.14335757871217367, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.16379226744174957}, {"id": 317, "seek": 171000, "start": 1724.0, "end": 1730.0, "text": " over, I would say a month or something like that or a week, it usually is enough, even", "tokens": [51064, 670, 11, 286, 576, 584, 257, 1618, 420, 746, 411, 300, 420, 257, 1243, 11, 309, 2673, 307, 1547, 11, 754, 51364], "temperature": 0.0, "avg_logprob": -0.14335757871217367, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.16379226744174957}, {"id": 318, "seek": 171000, "start": 1730.0, "end": 1738.0, "text": " especially when you put all the numbers to the low ranges for the retention for the memory", "tokens": [51364, 2318, 562, 291, 829, 439, 264, 3547, 281, 264, 2295, 22526, 337, 264, 22871, 337, 264, 4675, 51764], "temperature": 0.0, "avg_logprob": -0.14335757871217367, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.16379226744174957}, {"id": 319, "seek": 173800, "start": 1738.0, "end": 1743.0, "text": " and all this kind of stuff. It doesn't take too long to fill everything when you will", "tokens": [50364, 293, 439, 341, 733, 295, 1507, 13, 467, 1177, 380, 747, 886, 938, 281, 2836, 1203, 562, 291, 486, 50614], "temperature": 0.0, "avg_logprob": -0.14731884002685547, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.10443033277988434}, {"id": 320, "seek": 173800, "start": 1743.0, "end": 1748.0, "text": " start to see the retention and flashes and everything. So yeah, that's our plan, but we", "tokens": [50614, 722, 281, 536, 264, 22871, 293, 39665, 293, 1203, 13, 407, 1338, 11, 300, 311, 527, 1393, 11, 457, 321, 50864], "temperature": 0.0, "avg_logprob": -0.14731884002685547, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.10443033277988434}, {"id": 321, "seek": 173800, "start": 1748.0, "end": 1755.0, "text": " haven't done it yet. So yeah, but if you are interested in that, you should definitely", "tokens": [50864, 2378, 380, 1096, 309, 1939, 13, 407, 1338, 11, 457, 498, 291, 366, 3102, 294, 300, 11, 291, 820, 2138, 51214], "temperature": 0.0, "avg_logprob": -0.14731884002685547, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.10443033277988434}, {"id": 322, "seek": 173800, "start": 1755.0, "end": 1765.0, "text": " look in the hub that we have in the repository because it could be useful.", "tokens": [51214, 574, 294, 264, 11838, 300, 321, 362, 294, 264, 25841, 570, 309, 727, 312, 4420, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14731884002685547, "compression_ratio": 1.6584158415841583, "no_speech_prob": 0.10443033277988434}, {"id": 323, "seek": 176500, "start": 1765.0, "end": 1772.0, "text": " Do you have any tips for running performance tests in the cloud? Because for me that's quite", "tokens": [50364, 1144, 291, 362, 604, 6082, 337, 2614, 3389, 6921, 294, 264, 4588, 30, 1436, 337, 385, 300, 311, 1596, 50714], "temperature": 0.0, "avg_logprob": -0.15087706164309853, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.354765385389328}, {"id": 324, "seek": 176500, "start": 1772.0, "end": 1777.0, "text": " the opposite of dedicated test runners. But when the software runs finally in the cloud", "tokens": [50714, 264, 6182, 295, 8374, 1500, 33892, 13, 583, 562, 264, 4722, 6676, 2721, 294, 264, 4588, 50964], "temperature": 0.0, "avg_logprob": -0.15087706164309853, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.354765385389328}, {"id": 325, "seek": 176500, "start": 1777.0, "end": 1780.0, "text": " you should probably also performance test it there.", "tokens": [50964, 291, 820, 1391, 611, 3389, 1500, 309, 456, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15087706164309853, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.354765385389328}, {"id": 326, "seek": 176500, "start": 1780.0, "end": 1789.0, "text": " It's a problem. A big one. We have tried it and it is so inconsistent. The results are", "tokens": [51114, 467, 311, 257, 1154, 13, 316, 955, 472, 13, 492, 362, 3031, 309, 293, 309, 307, 370, 36891, 13, 440, 3542, 366, 51564], "temperature": 0.0, "avg_logprob": -0.15087706164309853, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.354765385389328}, {"id": 327, "seek": 178900, "start": 1789.0, "end": 1796.0, "text": " so all over the place. If you run, you have two same clusters, Kubernetes, OpenShifting", "tokens": [50364, 370, 439, 670, 264, 1081, 13, 759, 291, 1190, 11, 291, 362, 732, 912, 23313, 11, 23145, 11, 7238, 7774, 10106, 50714], "temperature": 0.0, "avg_logprob": -0.1650233762017612, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.3853726089000702}, {"id": 328, "seek": 178900, "start": 1796.0, "end": 1801.0, "text": " doesn't matter actually, and you run the tests on the same clusters at the same time,", "tokens": [50714, 1177, 380, 1871, 767, 11, 293, 291, 1190, 264, 6921, 322, 264, 912, 23313, 412, 264, 912, 565, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1650233762017612, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.3853726089000702}, {"id": 329, "seek": 178900, "start": 1801.0, "end": 1808.0, "text": " clusters are in different zones on AWS, you will get entirely different results because", "tokens": [50964, 23313, 366, 294, 819, 16025, 322, 17650, 11, 291, 486, 483, 7696, 819, 3542, 570, 51314], "temperature": 0.0, "avg_logprob": -0.1650233762017612, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.3853726089000702}, {"id": 330, "seek": 178900, "start": 1808.0, "end": 1816.0, "text": " all the load balancers and these kind of stuff, it really, you know, if you have only", "tokens": [51314, 439, 264, 3677, 3119, 4463, 433, 293, 613, 733, 295, 1507, 11, 309, 534, 11, 291, 458, 11, 498, 291, 362, 787, 51714], "temperature": 0.0, "avg_logprob": -0.1650233762017612, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.3853726089000702}, {"id": 331, "seek": 181600, "start": 1816.0, "end": 1821.0, "text": " internal communication on the cluster, we found anything from the outside. It could be", "tokens": [50364, 6920, 6101, 322, 264, 13630, 11, 321, 1352, 1340, 490, 264, 2380, 13, 467, 727, 312, 50614], "temperature": 0.0, "avg_logprob": -0.13636658612419578, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.1727178990840912}, {"id": 332, "seek": 181600, "start": 1821.0, "end": 1828.0, "text": " actually doable, I think, but if you have any communication during the test that is going", "tokens": [50614, 767, 41183, 11, 286, 519, 11, 457, 498, 291, 362, 604, 6101, 1830, 264, 1500, 300, 307, 516, 50964], "temperature": 0.0, "avg_logprob": -0.13636658612419578, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.1727178990840912}, {"id": 333, "seek": 181600, "start": 1828.0, "end": 1833.0, "text": " outside the cluster and it has to go through the load balancers and these kind of stuff,", "tokens": [50964, 2380, 264, 13630, 293, 309, 575, 281, 352, 807, 264, 3677, 3119, 4463, 433, 293, 613, 733, 295, 1507, 11, 51214], "temperature": 0.0, "avg_logprob": -0.13636658612419578, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.1727178990840912}, {"id": 334, "seek": 181600, "start": 1833.0, "end": 1841.0, "text": " I think that's not doable in any way because it's like you don't know what latency we will", "tokens": [51214, 286, 519, 300, 311, 406, 41183, 294, 604, 636, 570, 309, 311, 411, 291, 500, 380, 458, 437, 27043, 321, 486, 51614], "temperature": 0.0, "avg_logprob": -0.13636658612419578, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.1727178990840912}, {"id": 335, "seek": 184100, "start": 1841.0, "end": 1849.0, "text": " have for this kind of the request and travels. So I think that that would be like really", "tokens": [50364, 362, 337, 341, 733, 295, 264, 5308, 293, 19863, 13, 407, 286, 519, 300, 300, 576, 312, 411, 534, 50764], "temperature": 0.0, "avg_logprob": -0.18576936423778534, "compression_ratio": 1.5257142857142858, "no_speech_prob": 0.2444293200969696}, {"id": 336, "seek": 184100, "start": 1849.0, "end": 1856.0, "text": " problematic, but if you could mock up the external communication with just some internal", "tokens": [50764, 19011, 11, 457, 498, 291, 727, 17362, 493, 264, 8320, 6101, 365, 445, 512, 6920, 51114], "temperature": 0.0, "avg_logprob": -0.18576936423778534, "compression_ratio": 1.5257142857142858, "no_speech_prob": 0.2444293200969696}, {"id": 337, "seek": 184100, "start": 1856.0, "end": 1864.0, "text": " endpoint, it should be quite okay-ish, I would say, but you will not get like really good", "tokens": [51114, 35795, 11, 309, 820, 312, 1596, 1392, 12, 742, 11, 286, 576, 584, 11, 457, 291, 486, 406, 483, 411, 534, 665, 51514], "temperature": 0.0, "avg_logprob": -0.18576936423778534, "compression_ratio": 1.5257142857142858, "no_speech_prob": 0.2444293200969696}, {"id": 338, "seek": 186400, "start": 1864.0, "end": 1870.0, "text": " results from that, I think, even if you try more and more. But I think there are some", "tokens": [50364, 3542, 490, 300, 11, 286, 519, 11, 754, 498, 291, 853, 544, 293, 544, 13, 583, 286, 519, 456, 366, 512, 50664], "temperature": 0.0, "avg_logprob": -0.18254239008976864, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.37747862935066223}, {"id": 339, "seek": 186400, "start": 1870.0, "end": 1879.0, "text": " Kubernetes, some special Kubernetes builds that should be used for these kind of measurements,", "tokens": [50664, 23145, 11, 512, 2121, 23145, 15182, 300, 820, 312, 1143, 337, 613, 733, 295, 15383, 11, 51114], "temperature": 0.0, "avg_logprob": -0.18254239008976864, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.37747862935066223}, {"id": 340, "seek": 186400, "start": 1879.0, "end": 1886.0, "text": " but I have never actually tried it, so I cannot recommend it when I try it, when I", "tokens": [51114, 457, 286, 362, 1128, 767, 3031, 309, 11, 370, 286, 2644, 2748, 309, 562, 286, 853, 309, 11, 562, 286, 51464], "temperature": 0.0, "avg_logprob": -0.18254239008976864, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.37747862935066223}, {"id": 341, "seek": 188600, "start": 1886.0, "end": 1894.0, "text": " try it, but yeah, this is definitely a problem. Okay, so I have more questions?", "tokens": [50364, 853, 309, 11, 457, 1338, 11, 341, 307, 2138, 257, 1154, 13, 1033, 11, 370, 286, 362, 544, 1651, 30, 50764], "temperature": 0.0, "avg_logprob": -0.28920630949089327, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.3219788670539856}, {"id": 342, "seek": 188600, "start": 1894.0, "end": 1901.0, "text": " No, we have a few more minutes for questions. Come on. Otherwise, I'm going to ask you to, you know,", "tokens": [50764, 883, 11, 321, 362, 257, 1326, 544, 2077, 337, 1651, 13, 2492, 322, 13, 10328, 11, 286, 478, 516, 281, 1029, 291, 281, 11, 291, 458, 11, 51114], "temperature": 0.0, "avg_logprob": -0.28920630949089327, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.3219788670539856}, {"id": 343, "seek": 188600, "start": 1901.0, "end": 1905.0, "text": " to move your seats. Wait, wait.", "tokens": [51114, 281, 1286, 428, 11069, 13, 3802, 11, 1699, 13, 51314], "temperature": 0.0, "avg_logprob": -0.28920630949089327, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.3219788670539856}, {"id": 344, "seek": 188600, "start": 1905.0, "end": 1911.0, "text": " You said you would want to have a very small statement developed, so in the milliseconds", "tokens": [51314, 509, 848, 291, 576, 528, 281, 362, 257, 588, 1359, 5629, 4743, 11, 370, 294, 264, 34184, 51614], "temperature": 0.0, "avg_logprob": -0.28920630949089327, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.3219788670539856}, {"id": 345, "seek": 191100, "start": 1911.0, "end": 1917.0, "text": " ago. Yeah. So doesn't that create problems on their own, something like noisy neighbors and so on?", "tokens": [50364, 2057, 13, 865, 13, 407, 1177, 380, 300, 1884, 2740, 322, 641, 1065, 11, 746, 411, 24518, 12512, 293, 370, 322, 30, 50664], "temperature": 0.0, "avg_logprob": -0.17495611880687958, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.08933278173208237}, {"id": 346, "seek": 191100, "start": 1917.0, "end": 1921.0, "text": " Yes, it does. It does. Right, but...", "tokens": [50664, 1079, 11, 309, 775, 13, 467, 775, 13, 1779, 11, 457, 485, 50864], "temperature": 0.0, "avg_logprob": -0.17495611880687958, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.08933278173208237}, {"id": 347, "seek": 191100, "start": 1921.0, "end": 1924.0, "text": " How big of a problem that is?", "tokens": [50864, 1012, 955, 295, 257, 1154, 300, 307, 30, 51014], "temperature": 0.0, "avg_logprob": -0.17495611880687958, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.08933278173208237}, {"id": 348, "seek": 191100, "start": 1924.0, "end": 1931.0, "text": " Well, that's the thing. We are really thinking about writing some scraper that is fast enough for this,", "tokens": [51014, 1042, 11, 300, 311, 264, 551, 13, 492, 366, 534, 1953, 466, 3579, 512, 13943, 610, 300, 307, 2370, 1547, 337, 341, 11, 51364], "temperature": 0.0, "avg_logprob": -0.17495611880687958, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.08933278173208237}, {"id": 349, "seek": 191100, "start": 1931.0, "end": 1938.0, "text": " and yes, you will probably generate some problems during the way, especially if you would", "tokens": [51364, 293, 2086, 11, 291, 486, 1391, 8460, 512, 2740, 1830, 264, 636, 11, 2318, 498, 291, 576, 51714], "temperature": 0.0, "avg_logprob": -0.17495611880687958, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.08933278173208237}, {"id": 350, "seek": 193800, "start": 1938.0, "end": 1945.0, "text": " like to send the metrics directly to the Prometheus every millisecond. You will probably fill up the", "tokens": [50364, 411, 281, 2845, 264, 16367, 3838, 281, 264, 2114, 649, 42209, 633, 27940, 18882, 13, 509, 486, 1391, 2836, 493, 264, 50714], "temperature": 0.0, "avg_logprob": -0.1611039211875514, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.14083465933799744}, {"id": 351, "seek": 193800, "start": 1945.0, "end": 1952.0, "text": " network line or the TCP stack or whatever, because it's really fast. So you will...", "tokens": [50714, 3209, 1622, 420, 264, 48965, 8630, 420, 2035, 11, 570, 309, 311, 534, 2370, 13, 407, 291, 486, 485, 51064], "temperature": 0.0, "avg_logprob": -0.1611039211875514, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.14083465933799744}, {"id": 352, "seek": 193800, "start": 1952.0, "end": 1959.0, "text": " It will strongly depend on the machine that you are running and if you have space there, if you have a", "tokens": [51064, 467, 486, 10613, 5672, 322, 264, 3479, 300, 291, 366, 2614, 293, 498, 291, 362, 1901, 456, 11, 498, 291, 362, 257, 51414], "temperature": 0.0, "avg_logprob": -0.1611039211875514, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.14083465933799744}, {"id": 353, "seek": 193800, "start": 1959.0, "end": 1967.0, "text": " lot of RAM, you could actually batch all the metrics and send it like one package after the test is done.", "tokens": [51414, 688, 295, 14561, 11, 291, 727, 767, 15245, 439, 264, 16367, 293, 2845, 309, 411, 472, 7372, 934, 264, 1500, 307, 1096, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1611039211875514, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.14083465933799744}, {"id": 354, "seek": 196700, "start": 1968.0, "end": 1976.0, "text": " But yes, that's actually what we are now fighting with and we are trying to figure out how we are going to", "tokens": [50414, 583, 2086, 11, 300, 311, 767, 437, 321, 366, 586, 5237, 365, 293, 321, 366, 1382, 281, 2573, 484, 577, 321, 366, 516, 281, 50814], "temperature": 0.0, "avg_logprob": -0.1820440654513202, "compression_ratio": 1.64321608040201, "no_speech_prob": 0.050152119249105453}, {"id": 355, "seek": 196700, "start": 1976.0, "end": 1989.0, "text": " aim for this. But mostly we are thinking that we will do somehow a configurable scraper that will either batch the", "tokens": [50814, 5939, 337, 341, 13, 583, 5240, 321, 366, 1953, 300, 321, 486, 360, 6063, 257, 22192, 712, 13943, 610, 300, 486, 2139, 15245, 264, 51464], "temperature": 0.0, "avg_logprob": -0.1820440654513202, "compression_ratio": 1.64321608040201, "no_speech_prob": 0.050152119249105453}, {"id": 356, "seek": 196700, "start": 1989.0, "end": 1995.0, "text": " request or send them or something like that. I cannot say you because we haven't tried it, actually, what", "tokens": [51464, 5308, 420, 2845, 552, 420, 746, 411, 300, 13, 286, 2644, 584, 291, 570, 321, 2378, 380, 3031, 309, 11, 767, 11, 437, 51764], "temperature": 0.0, "avg_logprob": -0.1820440654513202, "compression_ratio": 1.64321608040201, "no_speech_prob": 0.050152119249105453}, {"id": 357, "seek": 199500, "start": 1995.0, "end": 2003.0, "text": " problems it makes, especially with batching, because I have counted it up and metrics aren't small, actually.", "tokens": [50364, 2740, 309, 1669, 11, 2318, 365, 15245, 278, 11, 570, 286, 362, 20150, 309, 493, 293, 16367, 3212, 380, 1359, 11, 767, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11771267729920226, "compression_ratio": 1.57, "no_speech_prob": 0.08853420615196228}, {"id": 358, "seek": 199500, "start": 2003.0, "end": 2011.0, "text": " So it will take a lot of place in the memory. So we will have to try it and somehow figure it out.", "tokens": [50764, 407, 309, 486, 747, 257, 688, 295, 1081, 294, 264, 4675, 13, 407, 321, 486, 362, 281, 853, 309, 293, 6063, 2573, 309, 484, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11771267729920226, "compression_ratio": 1.57, "no_speech_prob": 0.08853420615196228}, {"id": 359, "seek": 199500, "start": 2011.0, "end": 2019.0, "text": " But before the fast scraper, it will give you really headaches because you will try to find something and", "tokens": [51164, 583, 949, 264, 2370, 13943, 610, 11, 309, 486, 976, 291, 534, 35046, 570, 291, 486, 853, 281, 915, 746, 293, 51564], "temperature": 0.0, "avg_logprob": -0.11771267729920226, "compression_ratio": 1.57, "no_speech_prob": 0.08853420615196228}, {"id": 360, "seek": 201900, "start": 2019.0, "end": 2026.0, "text": " fight something and you will spend 20 times debugging it and then you will find that the scraper hit it the wrong", "tokens": [50364, 2092, 746, 293, 291, 486, 3496, 945, 1413, 45592, 309, 293, 550, 291, 486, 915, 300, 264, 13943, 610, 2045, 309, 264, 2085, 50714], "temperature": 0.0, "avg_logprob": -0.18875447530595083, "compression_ratio": 1.5426829268292683, "no_speech_prob": 0.12101864814758301}, {"id": 361, "seek": 201900, "start": 2026.0, "end": 2038.0, "text": " time every time. So we have to deal with this in some way. But it will be hard and problematic.", "tokens": [50714, 565, 633, 565, 13, 407, 321, 362, 281, 2028, 365, 341, 294, 512, 636, 13, 583, 309, 486, 312, 1152, 293, 19011, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18875447530595083, "compression_ratio": 1.5426829268292683, "no_speech_prob": 0.12101864814758301}, {"id": 362, "seek": 201900, "start": 2041.0, "end": 2043.0, "text": " I think we have time for one last question.", "tokens": [51464, 286, 519, 321, 362, 565, 337, 472, 1036, 1168, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18875447530595083, "compression_ratio": 1.5426829268292683, "no_speech_prob": 0.12101864814758301}, {"id": 363, "seek": 204900, "start": 2050.0, "end": 2051.0, "text": " No one?", "tokens": [50414, 883, 472, 30, 50464], "temperature": 0.0, "avg_logprob": -0.16565795739491782, "compression_ratio": 0.8367346938775511, "no_speech_prob": 0.22631874680519104}, {"id": 364, "seek": 204900, "start": 2052.0, "end": 2053.0, "text": " Tough crowd.", "tokens": [50514, 48568, 6919, 13, 50564], "temperature": 0.0, "avg_logprob": -0.16565795739491782, "compression_ratio": 0.8367346938775511, "no_speech_prob": 0.22631874680519104}, {"id": 365, "seek": 204900, "start": 2053.0, "end": 2054.0, "text": " Thank you very much.", "tokens": [50564, 1044, 291, 588, 709, 13, 50614], "temperature": 0.0, "avg_logprob": -0.16565795739491782, "compression_ratio": 0.8367346938775511, "no_speech_prob": 0.22631874680519104}], "language": "en"}