{"text": " Okay, this is going to be interesting. We are relying on the Wi-Fi bit here as well. So it would actually help if you turn off your Wi-Fi. I know that's a big ask. Consider that for the next half an hour. That would be really helpful. So Vanessa is live here through a video call. Give us away, Vanessa. We can... Well, can you try speaking? What's up, folks? Sorry, I'm not working. Is that working? Try again? Still what's up, son of them? Okay, that's really better. Nice. So we'll start your recording, Vanessa, and then we'll try and do live Q&A at the end. Sounds good. I have some answers for the previous Q&A, too, so we can talk a little bit about that. We can try. We can try. By the way, Vanessa is also the one who designed the HPC social logo. So you should thank her for that and take some stickers when you leave. Thank you. Thank you. All right, here comes the talk. Hi, folks. I'm Vanessa Socket, and today we're going to be talking about Kubernetes and HPC, the bare metal bros. So I thought I would open this talk by putting two words on the slide and then I'll go to the next question. So, what is the question that you guys have been asking or very anxious? Those words are cloud and HPC. So probably the question on everyone's mind is what does the future look like? I'm going to answer this question by posing a question back to you. Where is the money going? So we can look at polls from Gartner and Hyperion Research that suggests that cloud is projected to reach $40 billion by 2026 with a smaller CGR of 6.4%. So very superficially speaking, the money is going to cloud. Now, we can also then follow up on this question like, okay, that's great, but who's going to get left behind? We can look at a paper from Reed Gannon and Degar from 2023 that identified some really interesting trends. For HPC, it suggested that the way that we design our system will not be a problem because we're not going to be able to design our system will not continue to work. We cannot depend on dentered scaling and Moore's law. There's increasing rising costs for improved semiconductors. This is going to make it harder and increasingly more expensive and laborious to deploy new systems. And they define something called NREs or Non-Reoccurring Engineering Costs that we are incurring for every new system. Now, cloud, on the other hand, is leading the space of innovation. As we know, there's this massive expansion of large-scale commercial clouds. They are not depending on software vendors or hardware vendors. They're making their own stuff in-house. And guess what? They're hiring away and attracting the talent pool. And they made a really interesting analogy with temperature. They described HPC at endothermic requiring the absorption of heat for survival. And cloud is exothermic and really giving off of heat. And we know that, folks, we're not talking about heat here. We are talking about money. But to continue the heat analogy, you'll know that if you've ever been out in the snow in a cold environment, you are much more likely going to be wanting to give off heat to survive. So who gets left behind? Well, the person that needs to constantly absorb heat that's probably going to run out is the person that needs to absorb heat. And that's the reason that we're all here. It's because we need to ensure that the needs of our science are represented in this new environment. And guess what? The success of our science, the reason that we're all here, really depends on our ability to be collaborative in this space. And so this is really kind of the manifesto of Converge Computing. So if we bring them together, we get this new technology space where we have the best of both worlds. So where do we start? Well, here is how the talk is going to proceed today. We're going to start with models for convergence, talking about patterns for bringing together traditionally disparate environments. We're then going to move into strategies for convergence. So designs that I've noticed allow for easy movement between the spaces. So let's start with those models for convergence. Now, if you've looked in paper land, you've probably seen many different models. There's many different ways to take HPC and cloud and put them together. I'm going to talk about the high-level patterns and from the perspective of someone that's maybe deploying a system. So let's say that's me, and let's say I want my cloud and HPC, I'm going to take my limited set of resources and I'm going to try to split them into two steps. So I spend a ton of money and I do this, and then, I chose poorly. No one's using half my resources, and oh my god, so four years later I come back and I'm like, all right, I want cloud, X or HPC exclusive or HPC. I understand I can't have my cake and you to choose, so I am just going to choose one. We've used HPC for all these years, red and butter, this is why you've always done things. I choose HPC. Great, six months later, someone comes into my office. Are we dinosaurs? You know, everyone over there is using YAML and automation and we have this old setup and ah, so you go back in your office, you contemplate your life choice and you're like, oh right, no, it's okay, I'm not going to wait another four years. I'm going to sneak it in. So this is where you see all of these ideas, like bursting, multi-cluster, and these are generally referring to this idea of having some home base of resources and reaching out to get more. And the problem with this approach as I see it is that the complexity of these approaches often reflects the complexity of the systems. So they tend to be snowflake, they tend to be complex, and this is why there hasn't been like a single leader that has emerged in the space. So here is a different idea that's less common because it doesn't superficially kind of make sense. I want cloud or HPC, meaning I want to be able to run HPC, or cloud, or at the same time, or something together that's more converged, like what the heck am I talking about, don't I? Am I talking about, don't worry, we'll talk about it. Let's first talk about strategies for convergence. So these strategies I need to point out, these are not just about the technology, they are also about the people which is often harder. The first is common goals. In order to get two different communities working together, they have to care about the same things. You can't get around that. The second is modularity. So the degree to which your application or infrastructure can be modular, is that you can use things interchangeably and swap them, be very creative. The third is integration. This is consumption of an entire thing in another thing by way of different strategies. So let me give you some examples. For goals, the best overlap of goals I've seen is with respect to batch workloads. So a few years ago, the Kubernetes community started the batch working group, and this was because this new need to have AI ML workloads in Kubernetes. Traditionally, Kubernetes is where you run services, you keep something running. And there wasn't this concept of starting something and having it complete, but all of a sudden there was this new need, and guess what? We have been doing that in HPC land for like a couple of decades now. Modularity, a really great example, is actually with Kubernetes and Flux Framework. So you may think of Flux as just like this workload manager, but actually it's called a framework because we assemble many different components together to assemble into the workload manager known as Flux. Kubernetes is the same, different set of components, and there is going to be a creative way that we can kind of use these interchangeably. So the final example of integration, the best technologies I can provide are containers and language bindings. Container technologies are literally this vehicle to let you move between spaces, and language bindings are going to let you take it traditionally like C++ HPC project and extend it into a language that is native to the language and extend it into a language that is native to cloud. So for example, Go. Alrighty, let's get into some examples just like eggs three ways. Here are some projects that we've actually been working on at the lab. The first is Fluids. As I alluded to, this is the Flux scheduler, swapped with Coop scheduler. The next is the Flux operator, the entirety of Flux Framework implemented inside of Kubernetes. And then the namesake of this talk about air battle grows, Flux and Kubernetes working side by side. So let's start with the Flux scheduler within Kubernetes. You may be familiar with Kubernetes when you launch a job. You ask for a certain number of resources that's given to the scheduler. The scheduler says, okay, here are four pods. Have a nice day. So what we're going to do is bring in Fluents. So our C++ package, FluxSched, that is mapped with Go bindings into a custom scheduler plugin. We're going to swap it. And so you're basically going to be asking for the same amount of resources, but the scheduling is going to be done by FluxSched. How does this do? Well, we find that the workflows run three times faster. So what you're seeing here is Coop scheduler on the top, Fluents on the bottom. You see a lot of randomness with respect to how Coop scheduler places jobs. What this leads to is a pathological scheduling pattern. So anywhere you see a red box on there, that is a startup delay. And what that means in practice is though, is that although the workloads themselves run in similar times, we have a lot of outliers. We have a lot of jobs that take a really long time to get started. And so Fluents improves upon us. So Fluents is a really great example of modularity because we're taking an HPC technology and we're literally swapping it. And the modularity of the software allows for that. It's also a great example of integration. Because we have those Go bindings, we can speak the language of the cloud need of communities. Alrighty, next project, the Flex Operator. Super cool. All the gophers in Flexland are pretty cool. Alright, so the Flex Operator is implementing the entirety of Flex framework inside of Kubernetes, your own HPC cluster. This happens by way of a custom resource definition of CRD, where you basically give all the parameters that you want for your cluster, whether that's a single job or whether you want an interactive cluster. This creates what we call the mini cluster, which, you know, Flex Operator is a mini cluster, which, you know, Flux doesn't know the difference that it's running in Kubernetes versus on bare metal. There's a lead broker that's connected to several follower brokers. So here you have one pod for one physical node. The tree based overlay network within each pod or node, you have Flux that's added on the fly to your application. And the Operator is just going to basically reconcile until the state that you need for your cluster matches the actual state of the cluster. How well does it do? We added it to the best in the space last year. The MPI Operator and the Flux Operator consistently outperformed the MPI Operator we believe because of the 0MQ bootstrap. So the Flux Operator is a beautiful example of integration because we're taking the entirety of Flux framework and implementing it inside of Kubernetes. Bro, bro, bro, is it time for the bare metal bro? Yeah! Okay, so, warning. I've been saying bare metal, but nobody's going to give me bare metal. Let's be frank about that. So I was using virtual machine. We're using virtual machine as a proxy for bare metal. So just a warning. So what's different about this picture? The orange is on the outside. So we actually have Flux framework on the outside spinning up a Kubernetes cluster and notice that we actually still have compute running on bare metal alongside Kubernetes. How's that possible? Don't worry, I'll tell you. So why do we need this in the first place? As you know, also, there are increasingly more complex heterogeneous workloads that are coming to HPC. So this means not just, you know, embarrassingly parallel stuff, but also adding in services, databases, task queues. Ah! Okay, so I was... This slide is not wrong. I was going to give you an example of such a workload, and apparently this slide is giving you this warning that I'm a bad scientist and I'm not wrong, but I will point out that my example is actually a very good example that is a prototype for this kind of design. Let's talk about that. So let's say that we're running simulations. We're training examples one through N, whatever, doesn't matter, and we want to send them to a machine learning server, a specific endpoint to do the training. We then want to wait till some metric of goodness or perhaps a number of samples, and then we want to flip it around. We want to run simulations again, but we want to instead give this to our machine learning server without the actual values, then we're going to have a vector of the true values and the predictions, and we're going to see how well we did. Now, very superficially, if we match this to HPC versus Kubernetes, this is how we do it. We would expect that the simulations would run better on bare metal, and the service thing would run better in user netties or Kubernetes. This is way to be... We need to prove to ourselves first. So a lot of you are probably out there like, user net, like, Kubernetes? Like, in user things, are you nuts? I'm not nuts. There's actually something called user netties. It's a Kubernetes enhancement proposal or CUP proposal in 2022 by a very talented developer named Akihiro Sudo. Akihiro must point out won the top maintainer award for KUKON last year. He's an incredibly talented developer. If you've used any of these technologies, he's the one behind it. Hats off to Akihiro. So last year, at the beginning of the year, user netties was really a hodgepodge with kind of bash grips. It was really hard to use. So I engaged with Akihiro and we released Generation 2 of user netties in September. And guess what? It is using containerization, which is really great. It has these components that we'll go into in more detail. So what does it mean in practice? Well, it means when you're building a virtual machine, you need to have C groups version to enable. I recommend LIMA or Linux virtual machines if you're prototyping this for the first time. It also means that you need to enable these kernel modules. So very generally speaking, the RNet filter is going to allow you to apply IP tables, rules, bridge traffic. VXLan is going to allow you to connect VXLan devices on different hosts to a standalone bridge. This is important because we actually have different physical nodes. Now it's going to use RULE stocker. This isn't such a crazy idea anymore. Many clusters have podmin these days. And so what does it mean? Actually, when you bring out these VMs, it means that you're going to run a make up command that has two contexts. So both of them are going to build and start a base image that is using kind, kubernetes, and Docker with CNI plugins. And then the two contexts are the control plane and the worker. The control plane is going to install Flano, run kubernetes, and admit. This makes a joint command which is basically a token that you give to the workers, and then the togers can authenticate and join the cluster. And so that's what they do. They're just like, I'm ready to serve. All right, so we created this garbage cluster small and mighty using Overt and Ansible. It is small and mighty because each has eight cores and 30 MBs RAM and a 10-NVVD iterate. And I want to point out that we have seven nodes here because generally speaking, we're going to have six that we run things with compute on and one's going to be an admin node or control plane. Again, warning, not bare metal, you get the deal. All right, so what's in these VMs when we bring them up? We have a complete system install a flux, singularity on bare metal for reasons I'll tell you a little bit. Lamps installed on bare metal and of course user netties ready to be brought up. So once I shell into these VMs, my flux cluster is ready to go. I can do flux resource list and I can see all my nodes. And user netties, again, that administrative node is also a control plane. So we technically have six nodes to work with. And then we have a user netties. So we technically have six nodes to work with. And we can still see them with coop control get nodes. Here's what we're working with. User netties and flux running side by side the bare metal bros. All right, bro, bro, what experiments do we want to run all of them, bro? All right. So we first need to sanity check that what I said earlier about the bare metal and lamps and the simulations is actually true. We need to look at application performance between flux and user netties. So the way we're going to do that is by running a few things. We're first going to run lamps on bare metal with flux. We're then going to do the same thing but in a singularity container. And I did this just to demonstrate that you don't lose anything by using containers. Here's great. We're then going to run lamps in user netties with the flux operator. And then finally we're going to repeat cases one and two, but with user netties running in the background to look to see if there's any overhead of that. And I need to pause for a second because I know how incredibly cool this third case is. We have flux on the outside. Flux is running user netties. Within that we are launching the flux operator which is bringing up another instance of flux and inside there is where lamps is running. So folks, like I know Thanksgiving is over but this is the ultimate production. And we expect lamps to be slower in user netties because as we know it makes MPI collective calls. User netties are using something called SERP 4.NET NS which requires additional processing of packets with a tap device. I have a great paper I can share if you're interested in learning more about that. So drumroll the results as we expected the well actually maybe we didn't expect but guess what the bare metal case is the singularity container is very comparable to actual bare metal. I was very surprised by this. So user netties does not add a lot of overhead. And this is what we'd expected that guy up there running in user netties is about twice as slow as running on bare metal. So what did we learn? Well, we learned that for a setup like this the network sensitive stuff probably should be run on the HPC. But I'll point out there's opportunity for improving this in user netties. If you have experience with networking I'd like you to go over to the GitHub right now and I'm just going to wait a lot for the talk and engage with that to hear it to work on this problem. Now the next thing we want to look at is distributed machine learning specifically two cases one distributed to across six nodes and then the second on one node so the distributed case network is a variable and for the one node obviously network is not a variable. Drum roll results same thing it's about twice as fast on bare metal or twice as slow I guess on user netties. And interestingly when you look at just a single node these are really comparable so there's no issue with running something on a single node in user netties in and of itself it's really when you bring in the networking that it becomes a variable. So it's a network right well let's sanity check one more thing here's I per thing we did one bit of transfer for each node as a client to each node as a server we see bit rate and give you bits per second is between 10 and 30 for bare metal user netties with like non detectable closest here are really really terrible we can look so we can see the same patterns for transfer gigabits per second and so yes it's the network we're pretty confident for the setup it's the network. All right can we do the fun workflow now we absolutely can so guess what I actually prototyped this kind of workflow because I was really excited about it and so what we're going to do is we're going to be launching a batch job with flux batch this means the flux instance that's only by the running user it's going to scope resources using hw lock in this backshot where we can basically bring up and tear down all of user netties. We're going to take that workflow that I mentioned before we're going to map it into our star track cluster space so we're going to run simulations with lamps randomly selecting the problem sizes predict well time we're then going to bring up a machine learning server a special server I made using river a few years ago and then we're going to basically do the test cases we're going to run lamps again but we're going to leave out the actual well time and we're going to ask our models what it is and we're going to do a thousand training samples and 250 testing samples. How do we do? I put no thought into these particular models but I did three kinds of regression the Bayesian and sampling from a probability distribution didn't do super well but for the first two there's an actual kind of pattern between the predicted and the actual time and so although I put no thought into this I was really pleased with this result to see that the general prototype this idea of having bare middle simulations running alongside a service there is something here we can do science this way with actual real scientific questions and I'll point out that there are real heterogeneous workloads out in the wild and you this capability here's Moomi the massively parallel multi-stale machine learn model infrastructure and this is basically simulating biological systems the interact between proteins and plasma membrane I'll also point out that the Moomins are what it's based on the name the finished book comic book series with really cute hippos with often yellow spiky hair very awesome so this is the perfect example the bare metal rows of coexistence adopting technologies to make it possible to go to coexist and continuing to improve upon them so that for example with networking this environment can get even better so what should you remember from this talk if you take nothing else away the first is looking out for opportunities for collaboration look for that alignment of goals between spaces that's an opportunity the second is providing handles for your components so you don't have the bandwidth to look for opportunities add some go bindings to C++ projects because someone else could find you the third is engagement we need to show up at the table we need to go to working groups, conferences places that you haven't traditionally been to engage in to find these opportunities for collaboration and possibly the most important is this mindset we've had this mindset of cloud versus HPC that one has to win but they're different for so long we need to throw that away and get rid of the adversarial thinking and have a more collaborative mindset this is the vision that we have for the future for converge computing and we hope that you like to join us so thank you that's how to reach me my email and social networks and here's some interesting links for the flux and the various projects I think I will take some questions virtually now okay we can take a couple of questions it seems like the wifi is stable enough to let Vanessa answer them do we have any questions okay so Vanessa we may have to repeat a question for you we'll see how that works hi Vanessa amazing talk congrats so I was wondering if your architecture can support sidecars because one of the nightmares I had when I was trying to do something similar was that in order to get the sidecars running I had to spin up a second network stack and that created a lot of overhead so no no just one is on okay did you get the question Vanessa no I didn't hear the question at all neither did I yeah maybe that's better okay let's do it like this you'll come up front and ask it here yeah that's perfect that'd be great I can hear you great hi there hi so I was wondering if your architecture can support sidecar containers because as I was saying when I was trying to do something similar when I tried to create the sidecars I had to create a second network stack within singularity so the network overhead was amazingly high so absolutely a flux operator actually uses a sidecar container on a net container which is similar in concept to add flux on the fly as a view what's going on in Kubernetes is sort of a different thing than the networking issue so the short answer is yes to kind of add to that though I'm not sure that singularity and Kubernetes singularity as the container runtime for Kubernetes would work I have never tried that but it doesn't sound like it would work yeah it needs to be done yeah exactly hi Vanessa thank you hi it was the most fun presentation on the post then so far thank you so when you were saying that the main difference between performance between EBM and bare metal workloads was related to network was that the case also for distributed training and if that's the case were you using infini band or not so this we did not have infini band and you make a really good point that this kind of setup would need to be tested with an actually great network and that is still a very big challenge even for cloud so for example if you use AWS you can bring the elastic fiber adapter which will give you great networking performance but if you go to other clouds and I don't have to name this specifically you tend to only get really good networks when it comes to using like TPUs or GPUs the exception though is Azure which has a lot of really great HPC stuff kind of built in so absolutely you can get that setup with infini band Hi thank you for your talk I had a smile on my face the whole time thank you for having such high energy at the end of the day what was I going to say oh yeah so probably in my workloads I can reduce the network traffic by a very large margin if I can constrain certain jobs to specific nodes because then large files don't have to be moved for certain jobs to across the network is that something that you could keep in mind so if you remember the very quick machine learning experiment that we showed when we're running something on one node and you're not using the network there's no issue so if you're just running something on one node in user netties you won't have an issue in a degree to which you can reduce anything that uses network so moving data MPI etc etc you will get similar performance at least from this small prototype experiment that we've seen as you would on bare metal I have to do this because it wasn't really bare metal thanks one more question hey Vanessa that's Danny I'm gonna die my hair soon so you won't recognize me again I really liked your framing actually I thought I was going to sort of being adversarial and then I actually realized what you were saying and I really appreciated it however though regarding the adversarial framing I have some experience with for example cloud tools and cloud environments being used as platforms for vendor lock-in I think that you described especially with your converged computing kind of the way that you can push back against scientific labs aren't kind of in-depth to corporations I actually think that you kind of made a really useful example of one way to do that in your talk so again I actually was very very impressed by the way you kind of explained that I would like to know in the more general sense how can labs and potentially RSEs make use of cloud tools without getting locked in or becoming beholden again to a corporate environment and again by the way I think that you effectively did that in this talk so I'm more looking for a general kind of thought about that You're totally correct that vendor lock is an issue and when you tend to see many sort of niche APIs in different clouds and then you built your entire thing around them you do face that as an issue but the great thing about Kubernetes is that it is this open source project that is available across clouds there are subtle differences but if you make a workload that can run on Kubernetes you're going to have an easier time to move it between clouds and that's you know speaking from my lab we work on flux framework and one of our goals with flux is to make things portable not just between clouds but between cloud and HPC that's also something like user netties running actually Kubernetes on bare metal alongside HPC is so important because all of a sudden you have the same workload and it runs in all the places that is sort of like the vision we don't we want to make sure that the scientific workloads that we're running today can run in all places not just to one niche specific cloud not just one niche specific center just convergence TLDR that is very exciting and I really appreciate that response thank you so much okay that's all we have time for this workout great Vanessa I hope you agree yeah it was really fun if anyone has further questions and stuff please reach out to me I love chatting it was a pleasure chatting with you and I hope you have a great rest of your fun then thank you and the best way to reach out to Vanessa is via HPC social so don't forget to grab a sticker and you walk out please consider doing a small donation in the box as well to help cover the costs and if you're leaving please check if you see any trash around please take the trash with you bottles anything anything you clean up we don't have to clean up thanks a lot Vanessa this was great bye", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 17.14, "text": " Okay, this is going to be interesting.", "tokens": [50364, 1033, 11, 341, 307, 516, 281, 312, 1880, 13, 51221], "temperature": 0.0, "avg_logprob": -0.3859726941144025, "compression_ratio": 1.2734375, "no_speech_prob": 0.5935758352279663}, {"id": 1, "seek": 0, "start": 17.14, "end": 21.72, "text": " We are relying on the Wi-Fi bit here as well.", "tokens": [51221, 492, 366, 24140, 322, 264, 14035, 12, 13229, 857, 510, 382, 731, 13, 51450], "temperature": 0.0, "avg_logprob": -0.3859726941144025, "compression_ratio": 1.2734375, "no_speech_prob": 0.5935758352279663}, {"id": 2, "seek": 0, "start": 21.72, "end": 24.560000000000002, "text": " So it would actually help if you turn off your Wi-Fi.", "tokens": [51450, 407, 309, 576, 767, 854, 498, 291, 1261, 766, 428, 14035, 12, 13229, 13, 51592], "temperature": 0.0, "avg_logprob": -0.3859726941144025, "compression_ratio": 1.2734375, "no_speech_prob": 0.5935758352279663}, {"id": 3, "seek": 0, "start": 24.560000000000002, "end": 26.560000000000002, "text": " I know that's a big ask.", "tokens": [51592, 286, 458, 300, 311, 257, 955, 1029, 13, 51692], "temperature": 0.0, "avg_logprob": -0.3859726941144025, "compression_ratio": 1.2734375, "no_speech_prob": 0.5935758352279663}, {"id": 4, "seek": 2656, "start": 27.52, "end": 29.68, "text": " Consider that for the next half an hour.", "tokens": [50412, 17416, 300, 337, 264, 958, 1922, 364, 1773, 13, 50520], "temperature": 0.0, "avg_logprob": -0.40172671033190444, "compression_ratio": 1.4131736526946108, "no_speech_prob": 0.11717720329761505}, {"id": 5, "seek": 2656, "start": 29.68, "end": 31.68, "text": " That would be really helpful.", "tokens": [50520, 663, 576, 312, 534, 4961, 13, 50620], "temperature": 0.0, "avg_logprob": -0.40172671033190444, "compression_ratio": 1.4131736526946108, "no_speech_prob": 0.11717720329761505}, {"id": 6, "seek": 2656, "start": 31.68, "end": 35.0, "text": " So Vanessa is live here through a video call.", "tokens": [50620, 407, 27928, 307, 1621, 510, 807, 257, 960, 818, 13, 50786], "temperature": 0.0, "avg_logprob": -0.40172671033190444, "compression_ratio": 1.4131736526946108, "no_speech_prob": 0.11717720329761505}, {"id": 7, "seek": 2656, "start": 35.0, "end": 37.0, "text": " Give us away, Vanessa.", "tokens": [50786, 5303, 505, 1314, 11, 27928, 13, 50886], "temperature": 0.0, "avg_logprob": -0.40172671033190444, "compression_ratio": 1.4131736526946108, "no_speech_prob": 0.11717720329761505}, {"id": 8, "seek": 2656, "start": 37.0, "end": 39.0, "text": " We can...", "tokens": [50886, 492, 393, 485, 50986], "temperature": 0.0, "avg_logprob": -0.40172671033190444, "compression_ratio": 1.4131736526946108, "no_speech_prob": 0.11717720329761505}, {"id": 9, "seek": 2656, "start": 39.0, "end": 41.0, "text": " Well, can you try speaking?", "tokens": [50986, 1042, 11, 393, 291, 853, 4124, 30, 51086], "temperature": 0.0, "avg_logprob": -0.40172671033190444, "compression_ratio": 1.4131736526946108, "no_speech_prob": 0.11717720329761505}, {"id": 10, "seek": 2656, "start": 41.0, "end": 43.0, "text": " What's up, folks?", "tokens": [51086, 708, 311, 493, 11, 4024, 30, 51186], "temperature": 0.0, "avg_logprob": -0.40172671033190444, "compression_ratio": 1.4131736526946108, "no_speech_prob": 0.11717720329761505}, {"id": 11, "seek": 2656, "start": 43.0, "end": 45.0, "text": " Sorry, I'm not working.", "tokens": [51186, 4919, 11, 286, 478, 406, 1364, 13, 51286], "temperature": 0.0, "avg_logprob": -0.40172671033190444, "compression_ratio": 1.4131736526946108, "no_speech_prob": 0.11717720329761505}, {"id": 12, "seek": 2656, "start": 45.0, "end": 47.0, "text": " Is that working?", "tokens": [51286, 1119, 300, 1364, 30, 51386], "temperature": 0.0, "avg_logprob": -0.40172671033190444, "compression_ratio": 1.4131736526946108, "no_speech_prob": 0.11717720329761505}, {"id": 13, "seek": 5656, "start": 57.0, "end": 59.0, "text": " Try again?", "tokens": [50386, 6526, 797, 30, 50486], "temperature": 0.0, "avg_logprob": -0.18898614760368102, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.42888227105140686}, {"id": 14, "seek": 5656, "start": 59.0, "end": 61.0, "text": " Still what's up, son of them?", "tokens": [50486, 8291, 437, 311, 493, 11, 1872, 295, 552, 30, 50586], "temperature": 0.0, "avg_logprob": -0.18898614760368102, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.42888227105140686}, {"id": 15, "seek": 5656, "start": 61.0, "end": 63.0, "text": " Okay, that's really better.", "tokens": [50586, 1033, 11, 300, 311, 534, 1101, 13, 50686], "temperature": 0.0, "avg_logprob": -0.18898614760368102, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.42888227105140686}, {"id": 16, "seek": 5656, "start": 63.0, "end": 65.0, "text": " Nice.", "tokens": [50686, 5490, 13, 50786], "temperature": 0.0, "avg_logprob": -0.18898614760368102, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.42888227105140686}, {"id": 17, "seek": 5656, "start": 65.0, "end": 69.0, "text": " So we'll start your recording, Vanessa,", "tokens": [50786, 407, 321, 603, 722, 428, 6613, 11, 27928, 11, 50986], "temperature": 0.0, "avg_logprob": -0.18898614760368102, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.42888227105140686}, {"id": 18, "seek": 5656, "start": 69.0, "end": 71.0, "text": " and then we'll try and do live Q&A at the end.", "tokens": [50986, 293, 550, 321, 603, 853, 293, 360, 1621, 1249, 5, 32, 412, 264, 917, 13, 51086], "temperature": 0.0, "avg_logprob": -0.18898614760368102, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.42888227105140686}, {"id": 19, "seek": 5656, "start": 71.0, "end": 73.0, "text": " Sounds good.", "tokens": [51086, 14576, 665, 13, 51186], "temperature": 0.0, "avg_logprob": -0.18898614760368102, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.42888227105140686}, {"id": 20, "seek": 5656, "start": 73.0, "end": 75.0, "text": " I have some answers for the previous Q&A, too,", "tokens": [51186, 286, 362, 512, 6338, 337, 264, 3894, 1249, 5, 32, 11, 886, 11, 51286], "temperature": 0.0, "avg_logprob": -0.18898614760368102, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.42888227105140686}, {"id": 21, "seek": 5656, "start": 75.0, "end": 77.0, "text": " so we can talk a little bit about that.", "tokens": [51286, 370, 321, 393, 751, 257, 707, 857, 466, 300, 13, 51386], "temperature": 0.0, "avg_logprob": -0.18898614760368102, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.42888227105140686}, {"id": 22, "seek": 5656, "start": 77.0, "end": 79.0, "text": " We can try.", "tokens": [51386, 492, 393, 853, 13, 51486], "temperature": 0.0, "avg_logprob": -0.18898614760368102, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.42888227105140686}, {"id": 23, "seek": 5656, "start": 79.0, "end": 81.0, "text": " We can try.", "tokens": [51486, 492, 393, 853, 13, 51586], "temperature": 0.0, "avg_logprob": -0.18898614760368102, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.42888227105140686}, {"id": 24, "seek": 5656, "start": 81.0, "end": 83.0, "text": " By the way, Vanessa is also the one who designed", "tokens": [51586, 3146, 264, 636, 11, 27928, 307, 611, 264, 472, 567, 4761, 51686], "temperature": 0.0, "avg_logprob": -0.18898614760368102, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.42888227105140686}, {"id": 25, "seek": 5656, "start": 83.0, "end": 85.0, "text": " the HPC social logo.", "tokens": [51686, 264, 12557, 34, 2093, 9699, 13, 51786], "temperature": 0.0, "avg_logprob": -0.18898614760368102, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.42888227105140686}, {"id": 26, "seek": 8500, "start": 85.44, "end": 87.44, "text": " So you should thank her for that", "tokens": [50386, 407, 291, 820, 1309, 720, 337, 300, 50486], "temperature": 0.0, "avg_logprob": -0.11762868881225585, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.019776463508605957}, {"id": 27, "seek": 8500, "start": 87.44, "end": 89.44, "text": " and take some stickers when you leave.", "tokens": [50486, 293, 747, 512, 21019, 562, 291, 1856, 13, 50586], "temperature": 0.0, "avg_logprob": -0.11762868881225585, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.019776463508605957}, {"id": 28, "seek": 8500, "start": 89.44, "end": 91.44, "text": " Thank you.", "tokens": [50586, 1044, 291, 13, 50686], "temperature": 0.0, "avg_logprob": -0.11762868881225585, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.019776463508605957}, {"id": 29, "seek": 8500, "start": 91.44, "end": 93.44, "text": " Thank you.", "tokens": [50686, 1044, 291, 13, 50786], "temperature": 0.0, "avg_logprob": -0.11762868881225585, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.019776463508605957}, {"id": 30, "seek": 8500, "start": 93.44, "end": 95.44, "text": " All right, here comes the talk.", "tokens": [50786, 1057, 558, 11, 510, 1487, 264, 751, 13, 50886], "temperature": 0.0, "avg_logprob": -0.11762868881225585, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.019776463508605957}, {"id": 31, "seek": 8500, "start": 99.44, "end": 101.44, "text": " Hi, folks.", "tokens": [51086, 2421, 11, 4024, 13, 51186], "temperature": 0.0, "avg_logprob": -0.11762868881225585, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.019776463508605957}, {"id": 32, "seek": 8500, "start": 101.44, "end": 103.44, "text": " I'm Vanessa Socket,", "tokens": [51186, 286, 478, 27928, 407, 4737, 11, 51286], "temperature": 0.0, "avg_logprob": -0.11762868881225585, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.019776463508605957}, {"id": 33, "seek": 8500, "start": 103.44, "end": 105.44, "text": " and today we're going to be talking about", "tokens": [51286, 293, 965, 321, 434, 516, 281, 312, 1417, 466, 51386], "temperature": 0.0, "avg_logprob": -0.11762868881225585, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.019776463508605957}, {"id": 34, "seek": 8500, "start": 105.44, "end": 107.44, "text": " Kubernetes and HPC,", "tokens": [51386, 23145, 293, 12557, 34, 11, 51486], "temperature": 0.0, "avg_logprob": -0.11762868881225585, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.019776463508605957}, {"id": 35, "seek": 8500, "start": 107.44, "end": 109.44, "text": " the bare metal bros.", "tokens": [51486, 264, 6949, 5760, 738, 329, 13, 51586], "temperature": 0.0, "avg_logprob": -0.11762868881225585, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.019776463508605957}, {"id": 36, "seek": 8500, "start": 109.44, "end": 111.44, "text": " So I thought I would open this talk", "tokens": [51586, 407, 286, 1194, 286, 576, 1269, 341, 751, 51686], "temperature": 0.0, "avg_logprob": -0.11762868881225585, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.019776463508605957}, {"id": 37, "seek": 8500, "start": 111.44, "end": 113.44, "text": " by putting two words on the slide", "tokens": [51686, 538, 3372, 732, 2283, 322, 264, 4137, 51786], "temperature": 0.0, "avg_logprob": -0.11762868881225585, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.019776463508605957}, {"id": 38, "seek": 11344, "start": 113.88, "end": 115.88, "text": " and then I'll go to the next question.", "tokens": [50386, 293, 550, 286, 603, 352, 281, 264, 958, 1168, 13, 50486], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 39, "seek": 11344, "start": 115.88, "end": 117.88, "text": " So, what is the question", "tokens": [50486, 407, 11, 437, 307, 264, 1168, 50586], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 40, "seek": 11344, "start": 117.88, "end": 119.88, "text": " that you guys have been asking", "tokens": [50586, 300, 291, 1074, 362, 668, 3365, 50686], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 41, "seek": 11344, "start": 119.88, "end": 121.88, "text": " or very anxious?", "tokens": [50686, 420, 588, 15166, 30, 50786], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 42, "seek": 11344, "start": 121.88, "end": 123.88, "text": " Those words are cloud and HPC.", "tokens": [50786, 3950, 2283, 366, 4588, 293, 12557, 34, 13, 50886], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 43, "seek": 11344, "start": 123.88, "end": 125.88, "text": " So probably the question on everyone's mind", "tokens": [50886, 407, 1391, 264, 1168, 322, 1518, 311, 1575, 50986], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 44, "seek": 11344, "start": 125.88, "end": 127.88, "text": " is what does the future look like?", "tokens": [50986, 307, 437, 775, 264, 2027, 574, 411, 30, 51086], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 45, "seek": 11344, "start": 127.88, "end": 129.88, "text": " I'm going to answer this question", "tokens": [51086, 286, 478, 516, 281, 1867, 341, 1168, 51186], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 46, "seek": 11344, "start": 129.88, "end": 131.88, "text": " by posing a question back to you.", "tokens": [51186, 538, 40378, 257, 1168, 646, 281, 291, 13, 51286], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 47, "seek": 11344, "start": 131.88, "end": 133.88, "text": " Where is the money going?", "tokens": [51286, 2305, 307, 264, 1460, 516, 30, 51386], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 48, "seek": 11344, "start": 133.88, "end": 135.88, "text": " So we can look at polls", "tokens": [51386, 407, 321, 393, 574, 412, 24264, 51486], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 49, "seek": 11344, "start": 135.88, "end": 137.88, "text": " from Gartner and Hyperion Research", "tokens": [51486, 490, 460, 446, 1193, 293, 29592, 313, 10303, 51586], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 50, "seek": 11344, "start": 137.88, "end": 139.88, "text": " that suggests that cloud", "tokens": [51586, 300, 13409, 300, 4588, 51686], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 51, "seek": 11344, "start": 139.88, "end": 141.88, "text": " is projected to reach", "tokens": [51686, 307, 26231, 281, 2524, 51786], "temperature": 0.2, "avg_logprob": -0.3408842629533473, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.037230346351861954}, {"id": 52, "seek": 14188, "start": 142.32, "end": 144.32, "text": " $40 billion by 2026", "tokens": [50386, 1848, 5254, 5218, 538, 945, 10880, 50486], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 53, "seek": 14188, "start": 144.32, "end": 146.32, "text": " with a smaller CGR of 6.4%.", "tokens": [50486, 365, 257, 4356, 383, 23971, 295, 1386, 13, 19, 6856, 50586], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 54, "seek": 14188, "start": 146.32, "end": 148.32, "text": " So very superficially speaking,", "tokens": [50586, 407, 588, 23881, 2270, 4124, 11, 50686], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 55, "seek": 14188, "start": 148.32, "end": 150.32, "text": " the money is going to cloud.", "tokens": [50686, 264, 1460, 307, 516, 281, 4588, 13, 50786], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 56, "seek": 14188, "start": 150.32, "end": 152.32, "text": " Now, we can also then follow up on this question", "tokens": [50786, 823, 11, 321, 393, 611, 550, 1524, 493, 322, 341, 1168, 50886], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 57, "seek": 14188, "start": 152.32, "end": 154.32, "text": " like, okay, that's great,", "tokens": [50886, 411, 11, 1392, 11, 300, 311, 869, 11, 50986], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 58, "seek": 14188, "start": 154.32, "end": 156.32, "text": " but who's going to get left behind?", "tokens": [50986, 457, 567, 311, 516, 281, 483, 1411, 2261, 30, 51086], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 59, "seek": 14188, "start": 156.32, "end": 158.32, "text": " We can look at a paper", "tokens": [51086, 492, 393, 574, 412, 257, 3035, 51186], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 60, "seek": 14188, "start": 158.32, "end": 160.32, "text": " from Reed Gannon and Degar from 2023", "tokens": [51186, 490, 32071, 460, 16138, 293, 413, 1146, 289, 490, 945, 9356, 51286], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 61, "seek": 14188, "start": 160.32, "end": 162.32, "text": " that identified some really interesting trends.", "tokens": [51286, 300, 9234, 512, 534, 1880, 13892, 13, 51386], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 62, "seek": 14188, "start": 162.32, "end": 164.32, "text": " For HPC, it suggested", "tokens": [51386, 1171, 12557, 34, 11, 309, 10945, 51486], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 63, "seek": 14188, "start": 164.32, "end": 166.32, "text": " that the way that we design our system", "tokens": [51486, 300, 264, 636, 300, 321, 1715, 527, 1185, 51586], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 64, "seek": 14188, "start": 166.32, "end": 168.32, "text": " will not be a problem", "tokens": [51586, 486, 406, 312, 257, 1154, 51686], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 65, "seek": 14188, "start": 168.32, "end": 170.32, "text": " because we're not going to be", "tokens": [51686, 570, 321, 434, 406, 516, 281, 312, 51786], "temperature": 0.0, "avg_logprob": -0.29941077127943944, "compression_ratio": 1.5120274914089347, "no_speech_prob": 0.01878598891198635}, {"id": 66, "seek": 17032, "start": 170.76, "end": 172.76, "text": " able to design our system", "tokens": [50386, 1075, 281, 1715, 527, 1185, 50486], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 67, "seek": 17032, "start": 172.76, "end": 174.76, "text": " will not continue to work.", "tokens": [50486, 486, 406, 2354, 281, 589, 13, 50586], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 68, "seek": 17032, "start": 174.76, "end": 176.76, "text": " We cannot depend on dentered scaling", "tokens": [50586, 492, 2644, 5672, 322, 1441, 40665, 21589, 50686], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 69, "seek": 17032, "start": 176.76, "end": 178.76, "text": " and Moore's law.", "tokens": [50686, 293, 21644, 311, 2101, 13, 50786], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 70, "seek": 17032, "start": 178.76, "end": 180.76, "text": " There's increasing rising costs", "tokens": [50786, 821, 311, 5662, 11636, 5497, 50886], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 71, "seek": 17032, "start": 180.76, "end": 182.76, "text": " for improved semiconductors.", "tokens": [50886, 337, 9689, 36924, 5547, 13, 50986], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 72, "seek": 17032, "start": 182.76, "end": 184.76, "text": " This is going to make it harder", "tokens": [50986, 639, 307, 516, 281, 652, 309, 6081, 51086], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 73, "seek": 17032, "start": 184.76, "end": 186.76, "text": " and increasingly more expensive", "tokens": [51086, 293, 12980, 544, 5124, 51186], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 74, "seek": 17032, "start": 186.76, "end": 188.76, "text": " and laborious to deploy new systems.", "tokens": [51186, 293, 5938, 851, 281, 7274, 777, 3652, 13, 51286], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 75, "seek": 17032, "start": 188.76, "end": 190.76, "text": " And they define something called NREs", "tokens": [51286, 400, 436, 6964, 746, 1219, 426, 3850, 82, 51386], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 76, "seek": 17032, "start": 190.76, "end": 192.76, "text": " or Non-Reoccurring Engineering Costs", "tokens": [51386, 420, 8774, 12, 8524, 905, 14112, 2937, 16215, 20863, 82, 51486], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 77, "seek": 17032, "start": 192.76, "end": 194.76, "text": " that we are incurring for every new system.", "tokens": [51486, 300, 321, 366, 35774, 2937, 337, 633, 777, 1185, 13, 51586], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 78, "seek": 17032, "start": 194.76, "end": 196.76, "text": " Now, cloud, on the other hand,", "tokens": [51586, 823, 11, 4588, 11, 322, 264, 661, 1011, 11, 51686], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 79, "seek": 17032, "start": 196.76, "end": 198.76, "text": " is leading the space of innovation.", "tokens": [51686, 307, 5775, 264, 1901, 295, 8504, 13, 51786], "temperature": 0.0, "avg_logprob": -0.1620373801579551, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.04131528362631798}, {"id": 80, "seek": 19876, "start": 199.2, "end": 201.2, "text": " As we know, there's this massive expansion", "tokens": [50386, 1018, 321, 458, 11, 456, 311, 341, 5994, 11260, 50486], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 81, "seek": 19876, "start": 201.2, "end": 203.2, "text": " of large-scale commercial clouds.", "tokens": [50486, 295, 2416, 12, 20033, 6841, 12193, 13, 50586], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 82, "seek": 19876, "start": 203.2, "end": 205.2, "text": " They are not depending", "tokens": [50586, 814, 366, 406, 5413, 50686], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 83, "seek": 19876, "start": 205.2, "end": 207.2, "text": " on software vendors or hardware vendors.", "tokens": [50686, 322, 4722, 22056, 420, 8837, 22056, 13, 50786], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 84, "seek": 19876, "start": 207.2, "end": 209.2, "text": " They're making their own stuff in-house.", "tokens": [50786, 814, 434, 1455, 641, 1065, 1507, 294, 12, 6410, 13, 50886], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 85, "seek": 19876, "start": 209.2, "end": 211.2, "text": " And guess what?", "tokens": [50886, 400, 2041, 437, 30, 50986], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 86, "seek": 19876, "start": 211.2, "end": 213.2, "text": " They're hiring away and attracting the talent pool.", "tokens": [50986, 814, 434, 15335, 1314, 293, 36594, 264, 8301, 7005, 13, 51086], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 87, "seek": 19876, "start": 213.2, "end": 215.2, "text": " And they made a really interesting analogy", "tokens": [51086, 400, 436, 1027, 257, 534, 1880, 21663, 51186], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 88, "seek": 19876, "start": 215.2, "end": 217.2, "text": " with temperature.", "tokens": [51186, 365, 4292, 13, 51286], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 89, "seek": 19876, "start": 217.2, "end": 219.2, "text": " They described HPC at endothermic", "tokens": [51286, 814, 7619, 12557, 34, 412, 917, 802, 13195, 51386], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 90, "seek": 19876, "start": 219.2, "end": 221.2, "text": " requiring the absorption of heat", "tokens": [51386, 24165, 264, 27557, 295, 3738, 51486], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 91, "seek": 19876, "start": 221.2, "end": 223.2, "text": " for survival.", "tokens": [51486, 337, 12559, 13, 51586], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 92, "seek": 19876, "start": 223.2, "end": 225.2, "text": " And cloud is exothermic", "tokens": [51586, 400, 4588, 307, 454, 802, 13195, 51686], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 93, "seek": 19876, "start": 225.2, "end": 227.2, "text": " and really giving off of heat.", "tokens": [51686, 293, 534, 2902, 766, 295, 3738, 13, 51786], "temperature": 0.0, "avg_logprob": -0.10023664633433024, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.015148988924920559}, {"id": 94, "seek": 22720, "start": 227.64, "end": 229.64, "text": " And we know that, folks,", "tokens": [50386, 400, 321, 458, 300, 11, 4024, 11, 50486], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 95, "seek": 22720, "start": 229.64, "end": 231.64, "text": " we're not talking about heat here.", "tokens": [50486, 321, 434, 406, 1417, 466, 3738, 510, 13, 50586], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 96, "seek": 22720, "start": 231.64, "end": 233.64, "text": " We are talking about money.", "tokens": [50586, 492, 366, 1417, 466, 1460, 13, 50686], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 97, "seek": 22720, "start": 233.64, "end": 235.64, "text": " But to continue the heat analogy,", "tokens": [50686, 583, 281, 2354, 264, 3738, 21663, 11, 50786], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 98, "seek": 22720, "start": 235.64, "end": 237.64, "text": " you'll know that if you've ever been", "tokens": [50786, 291, 603, 458, 300, 498, 291, 600, 1562, 668, 50886], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 99, "seek": 22720, "start": 237.64, "end": 239.64, "text": " out in the snow", "tokens": [50886, 484, 294, 264, 5756, 50986], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 100, "seek": 22720, "start": 239.64, "end": 241.64, "text": " in a cold environment,", "tokens": [50986, 294, 257, 3554, 2823, 11, 51086], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 101, "seek": 22720, "start": 241.64, "end": 243.64, "text": " you are much more likely", "tokens": [51086, 291, 366, 709, 544, 3700, 51186], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 102, "seek": 22720, "start": 243.64, "end": 245.64, "text": " going to be wanting to give off heat", "tokens": [51186, 516, 281, 312, 7935, 281, 976, 766, 3738, 51286], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 103, "seek": 22720, "start": 245.64, "end": 247.64, "text": " to survive.", "tokens": [51286, 281, 7867, 13, 51386], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 104, "seek": 22720, "start": 247.64, "end": 249.64, "text": " So who gets left behind?", "tokens": [51386, 407, 567, 2170, 1411, 2261, 30, 51486], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 105, "seek": 22720, "start": 249.64, "end": 251.64, "text": " Well, the person that needs", "tokens": [51486, 1042, 11, 264, 954, 300, 2203, 51586], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 106, "seek": 22720, "start": 251.64, "end": 253.64, "text": " to constantly absorb heat", "tokens": [51586, 281, 6460, 15631, 3738, 51686], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 107, "seek": 22720, "start": 253.64, "end": 255.64, "text": " that's probably going to run out", "tokens": [51686, 300, 311, 1391, 516, 281, 1190, 484, 51786], "temperature": 0.0, "avg_logprob": -0.13539233000382134, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.030567755922675133}, {"id": 108, "seek": 25564, "start": 256.08, "end": 258.08, "text": " is the person that needs to absorb heat.", "tokens": [50386, 307, 264, 954, 300, 2203, 281, 15631, 3738, 13, 50486], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 109, "seek": 25564, "start": 258.08, "end": 260.08, "text": " And that's the reason", "tokens": [50486, 400, 300, 311, 264, 1778, 50586], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 110, "seek": 25564, "start": 260.08, "end": 262.08, "text": " that we're all here.", "tokens": [50586, 300, 321, 434, 439, 510, 13, 50686], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 111, "seek": 25564, "start": 262.08, "end": 264.08, "text": " It's because we need to ensure", "tokens": [50686, 467, 311, 570, 321, 643, 281, 5586, 50786], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 112, "seek": 25564, "start": 264.08, "end": 266.08, "text": " that the needs of our science", "tokens": [50786, 300, 264, 2203, 295, 527, 3497, 50886], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 113, "seek": 25564, "start": 266.08, "end": 268.08, "text": " are represented in this new environment.", "tokens": [50886, 366, 10379, 294, 341, 777, 2823, 13, 50986], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 114, "seek": 25564, "start": 268.08, "end": 270.08, "text": " And guess what?", "tokens": [50986, 400, 2041, 437, 30, 51086], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 115, "seek": 25564, "start": 270.08, "end": 272.08, "text": " The success of our science,", "tokens": [51086, 440, 2245, 295, 527, 3497, 11, 51186], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 116, "seek": 25564, "start": 272.08, "end": 274.08, "text": " the reason that we're all here,", "tokens": [51186, 264, 1778, 300, 321, 434, 439, 510, 11, 51286], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 117, "seek": 25564, "start": 274.08, "end": 276.08, "text": " really depends on our ability", "tokens": [51286, 534, 5946, 322, 527, 3485, 51386], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 118, "seek": 25564, "start": 276.08, "end": 278.08, "text": " to be collaborative", "tokens": [51386, 281, 312, 16555, 51486], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 119, "seek": 25564, "start": 278.08, "end": 280.08, "text": " in this space.", "tokens": [51486, 294, 341, 1901, 13, 51586], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 120, "seek": 25564, "start": 280.08, "end": 282.08, "text": " And so this is really kind of", "tokens": [51586, 400, 370, 341, 307, 534, 733, 295, 51686], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 121, "seek": 25564, "start": 282.08, "end": 284.08, "text": " the manifesto of Converge Computing.", "tokens": [51686, 264, 10067, 78, 295, 2656, 331, 432, 37804, 278, 13, 51786], "temperature": 0.0, "avg_logprob": -0.24927214272001869, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.005981304217129946}, {"id": 122, "seek": 28408, "start": 284.52, "end": 286.52, "text": " So if we bring them together,", "tokens": [50386, 407, 498, 321, 1565, 552, 1214, 11, 50486], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 123, "seek": 28408, "start": 286.52, "end": 288.52, "text": " we get this new technology space", "tokens": [50486, 321, 483, 341, 777, 2899, 1901, 50586], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 124, "seek": 28408, "start": 288.52, "end": 290.52, "text": " where we have the best of both worlds.", "tokens": [50586, 689, 321, 362, 264, 1151, 295, 1293, 13401, 13, 50686], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 125, "seek": 28408, "start": 290.52, "end": 292.52, "text": " So where do we start?", "tokens": [50686, 407, 689, 360, 321, 722, 30, 50786], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 126, "seek": 28408, "start": 292.52, "end": 294.52, "text": " Well, here is how the talk", "tokens": [50786, 1042, 11, 510, 307, 577, 264, 751, 50886], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 127, "seek": 28408, "start": 294.52, "end": 296.52, "text": " is going to proceed today.", "tokens": [50886, 307, 516, 281, 8991, 965, 13, 50986], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 128, "seek": 28408, "start": 296.52, "end": 298.52, "text": " We're going to start with models for convergence,", "tokens": [50986, 492, 434, 516, 281, 722, 365, 5245, 337, 32181, 11, 51086], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 129, "seek": 28408, "start": 298.52, "end": 300.52, "text": " talking about patterns", "tokens": [51086, 1417, 466, 8294, 51186], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 130, "seek": 28408, "start": 300.52, "end": 302.52, "text": " for bringing together traditionally disparate environments.", "tokens": [51186, 337, 5062, 1214, 19067, 14548, 473, 12388, 13, 51286], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 131, "seek": 28408, "start": 302.52, "end": 304.52, "text": " We're then going to move into strategies", "tokens": [51286, 492, 434, 550, 516, 281, 1286, 666, 9029, 51386], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 132, "seek": 28408, "start": 304.52, "end": 306.52, "text": " for convergence.", "tokens": [51386, 337, 32181, 13, 51486], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 133, "seek": 28408, "start": 306.52, "end": 308.52, "text": " So designs that I've noticed", "tokens": [51486, 407, 11347, 300, 286, 600, 5694, 51586], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 134, "seek": 28408, "start": 308.52, "end": 310.52, "text": " allow for easy movement", "tokens": [51586, 2089, 337, 1858, 3963, 51686], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 135, "seek": 28408, "start": 310.52, "end": 312.52, "text": " between the spaces.", "tokens": [51686, 1296, 264, 7673, 13, 51786], "temperature": 0.0, "avg_logprob": -0.10458683763813768, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.02117055095732212}, {"id": 136, "seek": 31252, "start": 312.96, "end": 314.96, "text": " So let's start with", "tokens": [50386, 407, 718, 311, 722, 365, 50486], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 137, "seek": 31252, "start": 314.96, "end": 316.96, "text": " those models for convergence.", "tokens": [50486, 729, 5245, 337, 32181, 13, 50586], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 138, "seek": 31252, "start": 316.96, "end": 318.96, "text": " Now, if you've looked in paper land,", "tokens": [50586, 823, 11, 498, 291, 600, 2956, 294, 3035, 2117, 11, 50686], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 139, "seek": 31252, "start": 318.96, "end": 320.96, "text": " you've probably seen many different models.", "tokens": [50686, 291, 600, 1391, 1612, 867, 819, 5245, 13, 50786], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 140, "seek": 31252, "start": 320.96, "end": 322.96, "text": " There's many different ways", "tokens": [50786, 821, 311, 867, 819, 2098, 50886], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 141, "seek": 31252, "start": 322.96, "end": 324.96, "text": " to take HPC and cloud", "tokens": [50886, 281, 747, 12557, 34, 293, 4588, 50986], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 142, "seek": 31252, "start": 324.96, "end": 326.96, "text": " and put them together.", "tokens": [50986, 293, 829, 552, 1214, 13, 51086], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 143, "seek": 31252, "start": 326.96, "end": 328.96, "text": " I'm going to talk about the high-level patterns", "tokens": [51086, 286, 478, 516, 281, 751, 466, 264, 1090, 12, 12418, 8294, 51186], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 144, "seek": 31252, "start": 328.96, "end": 330.96, "text": " and from the perspective of someone", "tokens": [51186, 293, 490, 264, 4585, 295, 1580, 51286], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 145, "seek": 31252, "start": 330.96, "end": 332.96, "text": " that's maybe deploying a system.", "tokens": [51286, 300, 311, 1310, 34198, 257, 1185, 13, 51386], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 146, "seek": 31252, "start": 332.96, "end": 334.96, "text": " So let's say that's me,", "tokens": [51386, 407, 718, 311, 584, 300, 311, 385, 11, 51486], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 147, "seek": 31252, "start": 334.96, "end": 336.96, "text": " and let's say I want my cloud and HPC,", "tokens": [51486, 293, 718, 311, 584, 286, 528, 452, 4588, 293, 12557, 34, 11, 51586], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 148, "seek": 31252, "start": 336.96, "end": 338.96, "text": " I'm going to take my limited set of resources", "tokens": [51586, 286, 478, 516, 281, 747, 452, 5567, 992, 295, 3593, 51686], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 149, "seek": 31252, "start": 338.96, "end": 340.96, "text": " and I'm going to try to split them", "tokens": [51686, 293, 286, 478, 516, 281, 853, 281, 7472, 552, 51786], "temperature": 0.0, "avg_logprob": -0.14250000723957146, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.01319122314453125}, {"id": 150, "seek": 34096, "start": 340.96, "end": 342.96, "text": " into two steps.", "tokens": [50364, 666, 732, 4439, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 151, "seek": 34096, "start": 342.96, "end": 344.96, "text": " So I spend a ton of money and I do this,", "tokens": [50464, 407, 286, 3496, 257, 2952, 295, 1460, 293, 286, 360, 341, 11, 50564], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 152, "seek": 34096, "start": 344.96, "end": 346.96, "text": " and then,", "tokens": [50564, 293, 550, 11, 50664], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 153, "seek": 34096, "start": 346.96, "end": 348.96, "text": " I chose poorly.", "tokens": [50664, 286, 5111, 22271, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 154, "seek": 34096, "start": 348.96, "end": 350.96, "text": " No one's using half my resources,", "tokens": [50764, 883, 472, 311, 1228, 1922, 452, 3593, 11, 50864], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 155, "seek": 34096, "start": 350.96, "end": 352.96, "text": " and oh my god, so four years later", "tokens": [50864, 293, 1954, 452, 3044, 11, 370, 1451, 924, 1780, 50964], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 156, "seek": 34096, "start": 352.96, "end": 354.96, "text": " I come back and I'm like, all right,", "tokens": [50964, 286, 808, 646, 293, 286, 478, 411, 11, 439, 558, 11, 51064], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 157, "seek": 34096, "start": 354.96, "end": 356.96, "text": " I want cloud, X or HPC exclusive or HPC.", "tokens": [51064, 286, 528, 4588, 11, 1783, 420, 12557, 34, 13005, 420, 12557, 34, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 158, "seek": 34096, "start": 356.96, "end": 358.96, "text": " I understand I can't have my cake and you to choose,", "tokens": [51164, 286, 1223, 286, 393, 380, 362, 452, 5908, 293, 291, 281, 2826, 11, 51264], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 159, "seek": 34096, "start": 358.96, "end": 360.96, "text": " so I am just going to choose one.", "tokens": [51264, 370, 286, 669, 445, 516, 281, 2826, 472, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 160, "seek": 34096, "start": 360.96, "end": 362.96, "text": " We've used HPC for all these years,", "tokens": [51364, 492, 600, 1143, 12557, 34, 337, 439, 613, 924, 11, 51464], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 161, "seek": 34096, "start": 362.96, "end": 364.96, "text": " red and butter, this is why you've always done things.", "tokens": [51464, 2182, 293, 5517, 11, 341, 307, 983, 291, 600, 1009, 1096, 721, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 162, "seek": 34096, "start": 364.96, "end": 366.96, "text": " I choose HPC.", "tokens": [51564, 286, 2826, 12557, 34, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 163, "seek": 34096, "start": 366.96, "end": 368.96, "text": " Great, six months later, someone comes into my office.", "tokens": [51664, 3769, 11, 2309, 2493, 1780, 11, 1580, 1487, 666, 452, 3398, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2160919435562626, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.01741253212094307}, {"id": 164, "seek": 37096, "start": 370.96, "end": 372.96, "text": " Are we dinosaurs?", "tokens": [50364, 2014, 321, 25851, 30, 50464], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 165, "seek": 37096, "start": 372.96, "end": 374.96, "text": " You know, everyone over there is using YAML and automation", "tokens": [50464, 509, 458, 11, 1518, 670, 456, 307, 1228, 398, 2865, 43, 293, 17769, 50564], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 166, "seek": 37096, "start": 374.96, "end": 376.96, "text": " and we have this old setup and", "tokens": [50564, 293, 321, 362, 341, 1331, 8657, 293, 50664], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 167, "seek": 37096, "start": 376.96, "end": 378.96, "text": " ah, so you go back in your office,", "tokens": [50664, 3716, 11, 370, 291, 352, 646, 294, 428, 3398, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 168, "seek": 37096, "start": 378.96, "end": 380.96, "text": " you contemplate your life choice and you're like,", "tokens": [50764, 291, 19935, 473, 428, 993, 3922, 293, 291, 434, 411, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 169, "seek": 37096, "start": 380.96, "end": 382.96, "text": " oh right, no, it's okay, I'm not going to wait another four years.", "tokens": [50864, 1954, 558, 11, 572, 11, 309, 311, 1392, 11, 286, 478, 406, 516, 281, 1699, 1071, 1451, 924, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 170, "seek": 37096, "start": 382.96, "end": 384.96, "text": " I'm going to sneak it in.", "tokens": [50964, 286, 478, 516, 281, 13164, 309, 294, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 171, "seek": 37096, "start": 384.96, "end": 386.96, "text": " So this is where you see all of these ideas,", "tokens": [51064, 407, 341, 307, 689, 291, 536, 439, 295, 613, 3487, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 172, "seek": 37096, "start": 386.96, "end": 388.96, "text": " like bursting, multi-cluster,", "tokens": [51164, 411, 45713, 11, 4825, 12, 3474, 8393, 11, 51264], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 173, "seek": 37096, "start": 388.96, "end": 390.96, "text": " and these are generally referring to this idea", "tokens": [51264, 293, 613, 366, 5101, 13761, 281, 341, 1558, 51364], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 174, "seek": 37096, "start": 390.96, "end": 392.96, "text": " of having some home base of resources", "tokens": [51364, 295, 1419, 512, 1280, 3096, 295, 3593, 51464], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 175, "seek": 37096, "start": 392.96, "end": 394.96, "text": " and reaching out to get more.", "tokens": [51464, 293, 9906, 484, 281, 483, 544, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 176, "seek": 37096, "start": 394.96, "end": 396.96, "text": " And the problem with this approach as I see it", "tokens": [51564, 400, 264, 1154, 365, 341, 3109, 382, 286, 536, 309, 51664], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 177, "seek": 37096, "start": 396.96, "end": 398.96, "text": " is that the complexity of these approaches", "tokens": [51664, 307, 300, 264, 14024, 295, 613, 11587, 51764], "temperature": 0.0, "avg_logprob": -0.11980700492858887, "compression_ratio": 1.7018072289156627, "no_speech_prob": 0.0064794085919857025}, {"id": 178, "seek": 39896, "start": 398.96, "end": 400.96, "text": " often reflects the complexity of the systems.", "tokens": [50364, 2049, 18926, 264, 14024, 295, 264, 3652, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 179, "seek": 39896, "start": 400.96, "end": 402.96, "text": " So they tend to be snowflake,", "tokens": [50464, 407, 436, 3928, 281, 312, 44124, 619, 11, 50564], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 180, "seek": 39896, "start": 402.96, "end": 404.96, "text": " they tend to be complex, and this is why there hasn't been", "tokens": [50564, 436, 3928, 281, 312, 3997, 11, 293, 341, 307, 983, 456, 6132, 380, 668, 50664], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 181, "seek": 39896, "start": 404.96, "end": 406.96, "text": " like a single leader that has emerged in the space.", "tokens": [50664, 411, 257, 2167, 5263, 300, 575, 20178, 294, 264, 1901, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 182, "seek": 39896, "start": 406.96, "end": 408.96, "text": " So here is a different idea", "tokens": [50764, 407, 510, 307, 257, 819, 1558, 50864], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 183, "seek": 39896, "start": 408.96, "end": 410.96, "text": " that's less common because it doesn't", "tokens": [50864, 300, 311, 1570, 2689, 570, 309, 1177, 380, 50964], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 184, "seek": 39896, "start": 410.96, "end": 412.96, "text": " superficially kind of make sense.", "tokens": [50964, 23881, 2270, 733, 295, 652, 2020, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 185, "seek": 39896, "start": 412.96, "end": 414.96, "text": " I want cloud or HPC, meaning I want to be able to run HPC,", "tokens": [51064, 286, 528, 4588, 420, 12557, 34, 11, 3620, 286, 528, 281, 312, 1075, 281, 1190, 12557, 34, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 186, "seek": 39896, "start": 414.96, "end": 416.96, "text": " or cloud,", "tokens": [51164, 420, 4588, 11, 51264], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 187, "seek": 39896, "start": 416.96, "end": 418.96, "text": " or at the same time,", "tokens": [51264, 420, 412, 264, 912, 565, 11, 51364], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 188, "seek": 39896, "start": 418.96, "end": 420.96, "text": " or something together that's more converged,", "tokens": [51364, 420, 746, 1214, 300, 311, 544, 9652, 3004, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 189, "seek": 39896, "start": 420.96, "end": 422.96, "text": " like what the heck", "tokens": [51464, 411, 437, 264, 12872, 51564], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 190, "seek": 39896, "start": 422.96, "end": 424.96, "text": " am I talking about,", "tokens": [51564, 669, 286, 1417, 466, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 191, "seek": 39896, "start": 424.96, "end": 426.96, "text": " don't I?", "tokens": [51664, 500, 380, 286, 30, 51764], "temperature": 0.0, "avg_logprob": -0.1147215687636788, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.005460510961711407}, {"id": 192, "seek": 42696, "start": 426.96, "end": 428.96, "text": " Am I talking about,", "tokens": [50364, 2012, 286, 1417, 466, 11, 50464], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 193, "seek": 42696, "start": 428.96, "end": 430.96, "text": " don't worry, we'll talk about it.", "tokens": [50464, 500, 380, 3292, 11, 321, 603, 751, 466, 309, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 194, "seek": 42696, "start": 430.96, "end": 432.96, "text": " Let's first talk about strategies for convergence.", "tokens": [50564, 961, 311, 700, 751, 466, 9029, 337, 32181, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 195, "seek": 42696, "start": 432.96, "end": 434.96, "text": " So these strategies", "tokens": [50664, 407, 613, 9029, 50764], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 196, "seek": 42696, "start": 434.96, "end": 436.96, "text": " I need to point out, these are not just about the technology,", "tokens": [50764, 286, 643, 281, 935, 484, 11, 613, 366, 406, 445, 466, 264, 2899, 11, 50864], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 197, "seek": 42696, "start": 436.96, "end": 438.96, "text": " they are also about the people", "tokens": [50864, 436, 366, 611, 466, 264, 561, 50964], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 198, "seek": 42696, "start": 438.96, "end": 440.96, "text": " which is often harder.", "tokens": [50964, 597, 307, 2049, 6081, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 199, "seek": 42696, "start": 440.96, "end": 442.96, "text": " The first is common goals.", "tokens": [51064, 440, 700, 307, 2689, 5493, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 200, "seek": 42696, "start": 442.96, "end": 444.96, "text": " In order to get two different communities working together,", "tokens": [51164, 682, 1668, 281, 483, 732, 819, 4456, 1364, 1214, 11, 51264], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 201, "seek": 42696, "start": 444.96, "end": 446.96, "text": " they have to care about the same things.", "tokens": [51264, 436, 362, 281, 1127, 466, 264, 912, 721, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 202, "seek": 42696, "start": 446.96, "end": 448.96, "text": " You can't get around that.", "tokens": [51364, 509, 393, 380, 483, 926, 300, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 203, "seek": 42696, "start": 448.96, "end": 450.96, "text": " The second is modularity.", "tokens": [51464, 440, 1150, 307, 31111, 507, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 204, "seek": 42696, "start": 450.96, "end": 452.96, "text": " So the degree to which your application or infrastructure", "tokens": [51564, 407, 264, 4314, 281, 597, 428, 3861, 420, 6896, 51664], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 205, "seek": 42696, "start": 452.96, "end": 454.96, "text": " can be modular,", "tokens": [51664, 393, 312, 31111, 11, 51764], "temperature": 0.0, "avg_logprob": -0.06657838462886953, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.004192943684756756}, {"id": 206, "seek": 45496, "start": 454.96, "end": 456.96, "text": " is that you can use things interchangeably", "tokens": [50364, 307, 300, 291, 393, 764, 721, 30358, 1188, 50464], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 207, "seek": 45496, "start": 456.96, "end": 458.96, "text": " and swap them, be very creative.", "tokens": [50464, 293, 18135, 552, 11, 312, 588, 5880, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 208, "seek": 45496, "start": 458.96, "end": 460.96, "text": " The third is integration.", "tokens": [50564, 440, 2636, 307, 10980, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 209, "seek": 45496, "start": 460.96, "end": 462.96, "text": " This is consumption of an entire thing", "tokens": [50664, 639, 307, 12126, 295, 364, 2302, 551, 50764], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 210, "seek": 45496, "start": 462.96, "end": 464.96, "text": " in another thing by way of different strategies.", "tokens": [50764, 294, 1071, 551, 538, 636, 295, 819, 9029, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 211, "seek": 45496, "start": 464.96, "end": 466.96, "text": " So let me give you some examples.", "tokens": [50864, 407, 718, 385, 976, 291, 512, 5110, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 212, "seek": 45496, "start": 466.96, "end": 468.96, "text": " For goals,", "tokens": [50964, 1171, 5493, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 213, "seek": 45496, "start": 468.96, "end": 470.96, "text": " the best overlap of goals I've seen", "tokens": [51064, 264, 1151, 19959, 295, 5493, 286, 600, 1612, 51164], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 214, "seek": 45496, "start": 470.96, "end": 472.96, "text": " is with respect to batch workloads.", "tokens": [51164, 307, 365, 3104, 281, 15245, 32452, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 215, "seek": 45496, "start": 472.96, "end": 474.96, "text": " So a few years ago, the Kubernetes community", "tokens": [51264, 407, 257, 1326, 924, 2057, 11, 264, 23145, 1768, 51364], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 216, "seek": 45496, "start": 474.96, "end": 476.96, "text": " started the batch working group,", "tokens": [51364, 1409, 264, 15245, 1364, 1594, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 217, "seek": 45496, "start": 476.96, "end": 478.96, "text": " and this was because this new need to have", "tokens": [51464, 293, 341, 390, 570, 341, 777, 643, 281, 362, 51564], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 218, "seek": 45496, "start": 478.96, "end": 480.96, "text": " AI ML workloads in Kubernetes.", "tokens": [51564, 7318, 21601, 32452, 294, 23145, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 219, "seek": 45496, "start": 480.96, "end": 482.96, "text": " Traditionally, Kubernetes is where you run services,", "tokens": [51664, 22017, 15899, 11, 23145, 307, 689, 291, 1190, 3328, 11, 51764], "temperature": 0.0, "avg_logprob": -0.1592126717244772, "compression_ratio": 1.7090301003344481, "no_speech_prob": 0.016851304098963737}, {"id": 220, "seek": 48296, "start": 482.96, "end": 484.96, "text": " you keep something running.", "tokens": [50364, 291, 1066, 746, 2614, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 221, "seek": 48296, "start": 484.96, "end": 486.96, "text": " And there wasn't this concept of starting something", "tokens": [50464, 400, 456, 2067, 380, 341, 3410, 295, 2891, 746, 50564], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 222, "seek": 48296, "start": 486.96, "end": 488.96, "text": " and having it complete, but all of a sudden", "tokens": [50564, 293, 1419, 309, 3566, 11, 457, 439, 295, 257, 3990, 50664], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 223, "seek": 48296, "start": 488.96, "end": 490.96, "text": " there was this new need, and guess what?", "tokens": [50664, 456, 390, 341, 777, 643, 11, 293, 2041, 437, 30, 50764], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 224, "seek": 48296, "start": 490.96, "end": 492.96, "text": " We have been doing that in HPC land", "tokens": [50764, 492, 362, 668, 884, 300, 294, 12557, 34, 2117, 50864], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 225, "seek": 48296, "start": 492.96, "end": 494.96, "text": " for like a couple of decades now.", "tokens": [50864, 337, 411, 257, 1916, 295, 7878, 586, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 226, "seek": 48296, "start": 494.96, "end": 496.96, "text": " Modularity, a really great example,", "tokens": [50964, 6583, 1040, 507, 11, 257, 534, 869, 1365, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 227, "seek": 48296, "start": 496.96, "end": 498.96, "text": " is actually with Kubernetes and Flux Framework.", "tokens": [51064, 307, 767, 365, 23145, 293, 3235, 2449, 31628, 1902, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 228, "seek": 48296, "start": 498.96, "end": 500.96, "text": " So you may think of Flux as just like", "tokens": [51164, 407, 291, 815, 519, 295, 3235, 2449, 382, 445, 411, 51264], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 229, "seek": 48296, "start": 500.96, "end": 502.96, "text": " this workload manager,", "tokens": [51264, 341, 20139, 6598, 11, 51364], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 230, "seek": 48296, "start": 502.96, "end": 504.96, "text": " but actually it's called a framework", "tokens": [51364, 457, 767, 309, 311, 1219, 257, 8388, 51464], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 231, "seek": 48296, "start": 504.96, "end": 506.96, "text": " because we assemble", "tokens": [51464, 570, 321, 22364, 51564], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 232, "seek": 48296, "start": 506.96, "end": 508.96, "text": " many different components together", "tokens": [51564, 867, 819, 6677, 1214, 51664], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 233, "seek": 48296, "start": 508.96, "end": 510.96, "text": " to assemble into the workload manager", "tokens": [51664, 281, 22364, 666, 264, 20139, 6598, 51764], "temperature": 0.0, "avg_logprob": -0.09459776664847758, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.008431368507444859}, {"id": 234, "seek": 51096, "start": 510.96, "end": 512.96, "text": " known as Flux.", "tokens": [50364, 2570, 382, 3235, 2449, 13, 50464], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 235, "seek": 51096, "start": 512.96, "end": 514.96, "text": " Kubernetes is the same,", "tokens": [50464, 23145, 307, 264, 912, 11, 50564], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 236, "seek": 51096, "start": 514.96, "end": 516.96, "text": " different set of components,", "tokens": [50564, 819, 992, 295, 6677, 11, 50664], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 237, "seek": 51096, "start": 516.96, "end": 518.96, "text": " and there is going to be a creative way", "tokens": [50664, 293, 456, 307, 516, 281, 312, 257, 5880, 636, 50764], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 238, "seek": 51096, "start": 518.96, "end": 520.96, "text": " that we can kind of use these interchangeably.", "tokens": [50764, 300, 321, 393, 733, 295, 764, 613, 30358, 1188, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 239, "seek": 51096, "start": 520.96, "end": 522.96, "text": " So the final example of integration,", "tokens": [50864, 407, 264, 2572, 1365, 295, 10980, 11, 50964], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 240, "seek": 51096, "start": 522.96, "end": 524.96, "text": " the best technologies I can provide", "tokens": [50964, 264, 1151, 7943, 286, 393, 2893, 51064], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 241, "seek": 51096, "start": 524.96, "end": 526.96, "text": " are containers and language bindings.", "tokens": [51064, 366, 17089, 293, 2856, 14786, 1109, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 242, "seek": 51096, "start": 526.96, "end": 528.96, "text": " Container technologies are literally this vehicle", "tokens": [51164, 43732, 260, 7943, 366, 3736, 341, 5864, 51264], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 243, "seek": 51096, "start": 528.96, "end": 530.96, "text": " to let you move between spaces,", "tokens": [51264, 281, 718, 291, 1286, 1296, 7673, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 244, "seek": 51096, "start": 530.96, "end": 532.96, "text": " and language bindings are going to let you take it traditionally", "tokens": [51364, 293, 2856, 14786, 1109, 366, 516, 281, 718, 291, 747, 309, 19067, 51464], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 245, "seek": 51096, "start": 532.96, "end": 534.96, "text": " like C++ HPC project", "tokens": [51464, 411, 383, 25472, 12557, 34, 1716, 51564], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 246, "seek": 51096, "start": 534.96, "end": 536.96, "text": " and extend it into a language", "tokens": [51564, 293, 10101, 309, 666, 257, 2856, 51664], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 247, "seek": 51096, "start": 536.96, "end": 538.96, "text": " that is native to the language", "tokens": [51664, 300, 307, 8470, 281, 264, 2856, 51764], "temperature": 0.0, "avg_logprob": -0.12716850638389587, "compression_ratio": 1.721254355400697, "no_speech_prob": 0.0019250744953751564}, {"id": 248, "seek": 53896, "start": 538.96, "end": 540.96, "text": " and extend it into a language", "tokens": [50364, 293, 10101, 309, 666, 257, 2856, 50464], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 249, "seek": 53896, "start": 540.96, "end": 542.96, "text": " that is native to cloud.", "tokens": [50464, 300, 307, 8470, 281, 4588, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 250, "seek": 53896, "start": 542.96, "end": 544.96, "text": " So for example, Go.", "tokens": [50564, 407, 337, 1365, 11, 1037, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 251, "seek": 53896, "start": 544.96, "end": 546.96, "text": " Alrighty, let's get into some examples", "tokens": [50664, 43301, 11, 718, 311, 483, 666, 512, 5110, 50764], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 252, "seek": 53896, "start": 546.96, "end": 548.96, "text": " just like eggs three ways.", "tokens": [50764, 445, 411, 6466, 1045, 2098, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 253, "seek": 53896, "start": 548.96, "end": 550.96, "text": " Here are some projects that we've actually been working on", "tokens": [50864, 1692, 366, 512, 4455, 300, 321, 600, 767, 668, 1364, 322, 50964], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 254, "seek": 53896, "start": 550.96, "end": 552.96, "text": " at the lab. The first is Fluids.", "tokens": [50964, 412, 264, 2715, 13, 440, 700, 307, 33612, 3742, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 255, "seek": 53896, "start": 552.96, "end": 554.96, "text": " As I alluded to, this is the Flux scheduler,", "tokens": [51064, 1018, 286, 33919, 281, 11, 341, 307, 264, 3235, 2449, 12000, 260, 11, 51164], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 256, "seek": 53896, "start": 554.96, "end": 556.96, "text": " swapped with Coop scheduler.", "tokens": [51164, 50011, 365, 3066, 404, 12000, 260, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 257, "seek": 53896, "start": 556.96, "end": 558.96, "text": " The next is the Flux operator,", "tokens": [51264, 440, 958, 307, 264, 3235, 2449, 12973, 11, 51364], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 258, "seek": 53896, "start": 558.96, "end": 560.96, "text": " the entirety of Flux Framework", "tokens": [51364, 264, 31557, 295, 3235, 2449, 31628, 1902, 51464], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 259, "seek": 53896, "start": 560.96, "end": 562.96, "text": " implemented inside of Kubernetes.", "tokens": [51464, 12270, 1854, 295, 23145, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 260, "seek": 53896, "start": 562.96, "end": 564.96, "text": " And then the namesake", "tokens": [51564, 400, 550, 264, 5288, 619, 51664], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 261, "seek": 53896, "start": 564.96, "end": 566.96, "text": " of this talk about air battle grows,", "tokens": [51664, 295, 341, 751, 466, 1988, 4635, 13156, 11, 51764], "temperature": 0.0, "avg_logprob": -0.15026510463041418, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.0013876346638426185}, {"id": 262, "seek": 56696, "start": 566.96, "end": 568.96, "text": " Flux and Kubernetes working side by side.", "tokens": [50364, 3235, 2449, 293, 23145, 1364, 1252, 538, 1252, 13, 50464], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 263, "seek": 56696, "start": 568.96, "end": 570.96, "text": " So let's start", "tokens": [50464, 407, 718, 311, 722, 50564], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 264, "seek": 56696, "start": 570.96, "end": 572.96, "text": " with the Flux scheduler within Kubernetes.", "tokens": [50564, 365, 264, 3235, 2449, 12000, 260, 1951, 23145, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 265, "seek": 56696, "start": 572.96, "end": 574.96, "text": " You may be familiar with Kubernetes when you launch a job.", "tokens": [50664, 509, 815, 312, 4963, 365, 23145, 562, 291, 4025, 257, 1691, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 266, "seek": 56696, "start": 574.96, "end": 576.96, "text": " You ask for a certain number of resources", "tokens": [50764, 509, 1029, 337, 257, 1629, 1230, 295, 3593, 50864], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 267, "seek": 56696, "start": 576.96, "end": 578.96, "text": " that's given to the scheduler.", "tokens": [50864, 300, 311, 2212, 281, 264, 12000, 260, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 268, "seek": 56696, "start": 578.96, "end": 580.96, "text": " The scheduler says, okay, here are four pods.", "tokens": [50964, 440, 12000, 260, 1619, 11, 1392, 11, 510, 366, 1451, 31925, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 269, "seek": 56696, "start": 580.96, "end": 582.96, "text": " Have a nice day.", "tokens": [51064, 3560, 257, 1481, 786, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 270, "seek": 56696, "start": 582.96, "end": 584.96, "text": " So what we're going to do is bring in Fluents.", "tokens": [51164, 407, 437, 321, 434, 516, 281, 360, 307, 1565, 294, 33612, 791, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 271, "seek": 56696, "start": 584.96, "end": 586.96, "text": " So our C++ package, FluxSched,", "tokens": [51264, 407, 527, 383, 25472, 7372, 11, 3235, 2449, 50, 19318, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 272, "seek": 56696, "start": 586.96, "end": 588.96, "text": " that is mapped with Go bindings", "tokens": [51364, 300, 307, 33318, 365, 1037, 14786, 1109, 51464], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 273, "seek": 56696, "start": 588.96, "end": 590.96, "text": " into a custom scheduler plugin.", "tokens": [51464, 666, 257, 2375, 12000, 260, 23407, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 274, "seek": 56696, "start": 590.96, "end": 592.96, "text": " We're going to swap it.", "tokens": [51564, 492, 434, 516, 281, 18135, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 275, "seek": 56696, "start": 592.96, "end": 594.96, "text": " And so you're basically going to be asking", "tokens": [51664, 400, 370, 291, 434, 1936, 516, 281, 312, 3365, 51764], "temperature": 0.0, "avg_logprob": -0.12871713196204987, "compression_ratio": 1.6936026936026936, "no_speech_prob": 0.0020494055934250355}, {"id": 276, "seek": 59496, "start": 594.96, "end": 596.96, "text": " for the same amount of resources,", "tokens": [50364, 337, 264, 912, 2372, 295, 3593, 11, 50464], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 277, "seek": 59496, "start": 596.96, "end": 598.96, "text": " but the scheduling is going to be done", "tokens": [50464, 457, 264, 29055, 307, 516, 281, 312, 1096, 50564], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 278, "seek": 59496, "start": 598.96, "end": 600.96, "text": " by FluxSched.", "tokens": [50564, 538, 3235, 2449, 50, 19318, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 279, "seek": 59496, "start": 600.96, "end": 602.96, "text": " How does this do?", "tokens": [50664, 1012, 775, 341, 360, 30, 50764], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 280, "seek": 59496, "start": 602.96, "end": 604.96, "text": " Well, we find that the workflows run three times faster.", "tokens": [50764, 1042, 11, 321, 915, 300, 264, 43461, 1190, 1045, 1413, 4663, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 281, "seek": 59496, "start": 604.96, "end": 606.96, "text": " So what you're seeing here is Coop scheduler", "tokens": [50864, 407, 437, 291, 434, 2577, 510, 307, 3066, 404, 12000, 260, 50964], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 282, "seek": 59496, "start": 606.96, "end": 608.96, "text": " on the top, Fluents on the bottom.", "tokens": [50964, 322, 264, 1192, 11, 33612, 791, 322, 264, 2767, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 283, "seek": 59496, "start": 608.96, "end": 610.96, "text": " You see a lot of randomness with respect to", "tokens": [51064, 509, 536, 257, 688, 295, 4974, 1287, 365, 3104, 281, 51164], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 284, "seek": 59496, "start": 610.96, "end": 612.96, "text": " how Coop scheduler places jobs.", "tokens": [51164, 577, 3066, 404, 12000, 260, 3190, 4782, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 285, "seek": 59496, "start": 612.96, "end": 614.96, "text": " What this leads to is a pathological scheduling pattern.", "tokens": [51264, 708, 341, 6689, 281, 307, 257, 3100, 4383, 29055, 5102, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 286, "seek": 59496, "start": 614.96, "end": 616.96, "text": " So anywhere you see a red box on there,", "tokens": [51364, 407, 4992, 291, 536, 257, 2182, 2424, 322, 456, 11, 51464], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 287, "seek": 59496, "start": 616.96, "end": 618.96, "text": " that is a startup delay.", "tokens": [51464, 300, 307, 257, 18578, 8577, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 288, "seek": 59496, "start": 618.96, "end": 620.96, "text": " And what that means in practice is though,", "tokens": [51564, 400, 437, 300, 1355, 294, 3124, 307, 1673, 11, 51664], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 289, "seek": 59496, "start": 620.96, "end": 622.96, "text": " is that although the workloads themselves", "tokens": [51664, 307, 300, 4878, 264, 32452, 2969, 51764], "temperature": 0.0, "avg_logprob": -0.06830133527717334, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0019550910219550133}, {"id": 290, "seek": 62296, "start": 622.96, "end": 624.96, "text": " run in similar times,", "tokens": [50364, 1190, 294, 2531, 1413, 11, 50464], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 291, "seek": 62296, "start": 624.96, "end": 626.96, "text": " we have a lot of outliers.", "tokens": [50464, 321, 362, 257, 688, 295, 484, 23646, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 292, "seek": 62296, "start": 626.96, "end": 628.96, "text": " We have a lot of jobs that take a really long time", "tokens": [50564, 492, 362, 257, 688, 295, 4782, 300, 747, 257, 534, 938, 565, 50664], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 293, "seek": 62296, "start": 628.96, "end": 630.96, "text": " to get started.", "tokens": [50664, 281, 483, 1409, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 294, "seek": 62296, "start": 630.96, "end": 632.96, "text": " And so Fluents improves upon us.", "tokens": [50764, 400, 370, 33612, 791, 24771, 3564, 505, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 295, "seek": 62296, "start": 632.96, "end": 634.96, "text": " So Fluents is a really great example", "tokens": [50864, 407, 33612, 791, 307, 257, 534, 869, 1365, 50964], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 296, "seek": 62296, "start": 634.96, "end": 636.96, "text": " of modularity because we're taking", "tokens": [50964, 295, 31111, 507, 570, 321, 434, 1940, 51064], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 297, "seek": 62296, "start": 636.96, "end": 638.96, "text": " an HPC technology", "tokens": [51064, 364, 12557, 34, 2899, 51164], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 298, "seek": 62296, "start": 638.96, "end": 640.96, "text": " and we're literally swapping it.", "tokens": [51164, 293, 321, 434, 3736, 1693, 10534, 309, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 299, "seek": 62296, "start": 640.96, "end": 642.96, "text": " And the modularity of the software allows for that.", "tokens": [51264, 400, 264, 31111, 507, 295, 264, 4722, 4045, 337, 300, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 300, "seek": 62296, "start": 642.96, "end": 644.96, "text": " It's also a great example of integration.", "tokens": [51364, 467, 311, 611, 257, 869, 1365, 295, 10980, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 301, "seek": 62296, "start": 644.96, "end": 646.96, "text": " Because we have those Go bindings,", "tokens": [51464, 1436, 321, 362, 729, 1037, 14786, 1109, 11, 51564], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 302, "seek": 62296, "start": 646.96, "end": 648.96, "text": " we can speak the language", "tokens": [51564, 321, 393, 1710, 264, 2856, 51664], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 303, "seek": 62296, "start": 648.96, "end": 650.96, "text": " of the cloud need of communities.", "tokens": [51664, 295, 264, 4588, 643, 295, 4456, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0882172940382317, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003169178031384945}, {"id": 304, "seek": 65096, "start": 650.96, "end": 652.96, "text": " Alrighty, next project, the Flex Operator.", "tokens": [50364, 43301, 11, 958, 1716, 11, 264, 29208, 12480, 1639, 13, 50464], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 305, "seek": 65096, "start": 652.96, "end": 654.96, "text": " Super cool.", "tokens": [50464, 4548, 1627, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 306, "seek": 65096, "start": 654.96, "end": 656.96, "text": " All the gophers in Flexland are pretty cool.", "tokens": [50564, 1057, 264, 352, 950, 433, 294, 29208, 1661, 366, 1238, 1627, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 307, "seek": 65096, "start": 656.96, "end": 658.96, "text": " Alright, so the Flex Operator", "tokens": [50664, 2798, 11, 370, 264, 29208, 12480, 1639, 50764], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 308, "seek": 65096, "start": 658.96, "end": 660.96, "text": " is implementing the entirety of Flex", "tokens": [50764, 307, 18114, 264, 31557, 295, 29208, 50864], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 309, "seek": 65096, "start": 660.96, "end": 662.96, "text": " framework inside of Kubernetes,", "tokens": [50864, 8388, 1854, 295, 23145, 11, 50964], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 310, "seek": 65096, "start": 662.96, "end": 664.96, "text": " your own HPC cluster.", "tokens": [50964, 428, 1065, 12557, 34, 13630, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 311, "seek": 65096, "start": 664.96, "end": 666.96, "text": " This happens by way of a custom resource", "tokens": [51064, 639, 2314, 538, 636, 295, 257, 2375, 7684, 51164], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 312, "seek": 65096, "start": 666.96, "end": 668.96, "text": " definition of CRD, where you basically", "tokens": [51164, 7123, 295, 14123, 35, 11, 689, 291, 1936, 51264], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 313, "seek": 65096, "start": 668.96, "end": 670.96, "text": " give all the parameters that you want", "tokens": [51264, 976, 439, 264, 9834, 300, 291, 528, 51364], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 314, "seek": 65096, "start": 670.96, "end": 672.96, "text": " for your cluster, whether that's a single job", "tokens": [51364, 337, 428, 13630, 11, 1968, 300, 311, 257, 2167, 1691, 51464], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 315, "seek": 65096, "start": 672.96, "end": 674.96, "text": " or whether you want an interactive cluster.", "tokens": [51464, 420, 1968, 291, 528, 364, 15141, 13630, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 316, "seek": 65096, "start": 674.96, "end": 676.96, "text": " This creates what we call the mini cluster,", "tokens": [51564, 639, 7829, 437, 321, 818, 264, 8382, 13630, 11, 51664], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 317, "seek": 65096, "start": 676.96, "end": 678.96, "text": " which, you know, Flex Operator", "tokens": [51664, 597, 11, 291, 458, 11, 29208, 12480, 1639, 51764], "temperature": 0.0, "avg_logprob": -0.15751862351911783, "compression_ratio": 1.7711267605633803, "no_speech_prob": 0.007447856944054365}, {"id": 318, "seek": 67896, "start": 678.96, "end": 680.96, "text": " is a mini cluster, which, you know,", "tokens": [50364, 307, 257, 8382, 13630, 11, 597, 11, 291, 458, 11, 50464], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 319, "seek": 67896, "start": 680.96, "end": 682.96, "text": " Flux doesn't know the difference that it's running", "tokens": [50464, 3235, 2449, 1177, 380, 458, 264, 2649, 300, 309, 311, 2614, 50564], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 320, "seek": 67896, "start": 682.96, "end": 684.96, "text": " in Kubernetes versus on bare metal.", "tokens": [50564, 294, 23145, 5717, 322, 6949, 5760, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 321, "seek": 67896, "start": 684.96, "end": 686.96, "text": " There's a lead broker that's connected", "tokens": [50664, 821, 311, 257, 1477, 26502, 300, 311, 4582, 50764], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 322, "seek": 67896, "start": 686.96, "end": 688.96, "text": " to several follower brokers.", "tokens": [50764, 281, 2940, 35413, 47549, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 323, "seek": 67896, "start": 688.96, "end": 690.96, "text": " So here you have one pod for one physical node.", "tokens": [50864, 407, 510, 291, 362, 472, 2497, 337, 472, 4001, 9984, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 324, "seek": 67896, "start": 690.96, "end": 692.96, "text": " The tree based overlay network", "tokens": [50964, 440, 4230, 2361, 31741, 3209, 51064], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 325, "seek": 67896, "start": 692.96, "end": 694.96, "text": " within each pod or node,", "tokens": [51064, 1951, 1184, 2497, 420, 9984, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 326, "seek": 67896, "start": 694.96, "end": 696.96, "text": " you have Flux that's added on the fly to your application.", "tokens": [51164, 291, 362, 3235, 2449, 300, 311, 3869, 322, 264, 3603, 281, 428, 3861, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 327, "seek": 67896, "start": 696.96, "end": 698.96, "text": " And the Operator is just going to basically", "tokens": [51264, 400, 264, 12480, 1639, 307, 445, 516, 281, 1936, 51364], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 328, "seek": 67896, "start": 698.96, "end": 700.96, "text": " reconcile until the state", "tokens": [51364, 41059, 1826, 264, 1785, 51464], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 329, "seek": 67896, "start": 700.96, "end": 702.96, "text": " that you need for your cluster matches", "tokens": [51464, 300, 291, 643, 337, 428, 13630, 10676, 51564], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 330, "seek": 67896, "start": 702.96, "end": 704.96, "text": " the actual state of the cluster.", "tokens": [51564, 264, 3539, 1785, 295, 264, 13630, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 331, "seek": 67896, "start": 704.96, "end": 706.96, "text": " How well does it do?", "tokens": [51664, 1012, 731, 775, 309, 360, 30, 51764], "temperature": 0.0, "avg_logprob": -0.11823553357805525, "compression_ratio": 1.6807817589576548, "no_speech_prob": 0.006284475326538086}, {"id": 332, "seek": 70696, "start": 706.96, "end": 708.96, "text": " We added it to the best in the space last year.", "tokens": [50364, 492, 3869, 309, 281, 264, 1151, 294, 264, 1901, 1036, 1064, 13, 50464], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 333, "seek": 70696, "start": 708.96, "end": 710.96, "text": " The MPI Operator and the Flux Operator", "tokens": [50464, 440, 14146, 40, 12480, 1639, 293, 264, 3235, 2449, 12480, 1639, 50564], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 334, "seek": 70696, "start": 710.96, "end": 712.96, "text": " consistently outperformed the MPI Operator", "tokens": [50564, 14961, 484, 610, 22892, 264, 14146, 40, 12480, 1639, 50664], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 335, "seek": 70696, "start": 712.96, "end": 714.96, "text": " we believe because of the 0MQ bootstrap.", "tokens": [50664, 321, 1697, 570, 295, 264, 1958, 44, 48, 11450, 372, 4007, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 336, "seek": 70696, "start": 714.96, "end": 716.96, "text": " So the Flux Operator", "tokens": [50764, 407, 264, 3235, 2449, 12480, 1639, 50864], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 337, "seek": 70696, "start": 716.96, "end": 718.96, "text": " is a beautiful example of integration", "tokens": [50864, 307, 257, 2238, 1365, 295, 10980, 50964], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 338, "seek": 70696, "start": 718.96, "end": 720.96, "text": " because we're taking the entirety of Flux", "tokens": [50964, 570, 321, 434, 1940, 264, 31557, 295, 3235, 2449, 51064], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 339, "seek": 70696, "start": 720.96, "end": 722.96, "text": " framework and implementing it inside of Kubernetes.", "tokens": [51064, 8388, 293, 18114, 309, 1854, 295, 23145, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 340, "seek": 70696, "start": 722.96, "end": 724.96, "text": " Bro, bro, bro, is it time", "tokens": [51164, 5425, 11, 2006, 11, 2006, 11, 307, 309, 565, 51264], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 341, "seek": 70696, "start": 724.96, "end": 726.96, "text": " for the bare metal bro?", "tokens": [51264, 337, 264, 6949, 5760, 2006, 30, 51364], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 342, "seek": 70696, "start": 726.96, "end": 728.96, "text": " Yeah!", "tokens": [51364, 865, 0, 51464], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 343, "seek": 70696, "start": 728.96, "end": 730.96, "text": " Okay, so, warning.", "tokens": [51464, 1033, 11, 370, 11, 9164, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 344, "seek": 70696, "start": 730.96, "end": 732.96, "text": " I've been saying bare metal,", "tokens": [51564, 286, 600, 668, 1566, 6949, 5760, 11, 51664], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 345, "seek": 70696, "start": 732.96, "end": 734.96, "text": " but nobody's going to give me bare metal.", "tokens": [51664, 457, 5079, 311, 516, 281, 976, 385, 6949, 5760, 13, 51764], "temperature": 0.0, "avg_logprob": -0.17308349876137047, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.010642509907484055}, {"id": 346, "seek": 73496, "start": 734.96, "end": 736.96, "text": " Let's be frank about that.", "tokens": [50364, 961, 311, 312, 10455, 466, 300, 13, 50464], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 347, "seek": 73496, "start": 736.96, "end": 738.96, "text": " So I was using virtual machine.", "tokens": [50464, 407, 286, 390, 1228, 6374, 3479, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 348, "seek": 73496, "start": 738.96, "end": 740.96, "text": " We're using virtual machine as a proxy", "tokens": [50564, 492, 434, 1228, 6374, 3479, 382, 257, 29690, 50664], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 349, "seek": 73496, "start": 740.96, "end": 742.96, "text": " for bare metal.", "tokens": [50664, 337, 6949, 5760, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 350, "seek": 73496, "start": 742.96, "end": 744.96, "text": " So just a warning.", "tokens": [50764, 407, 445, 257, 9164, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 351, "seek": 73496, "start": 744.96, "end": 746.96, "text": " So what's different about this picture?", "tokens": [50864, 407, 437, 311, 819, 466, 341, 3036, 30, 50964], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 352, "seek": 73496, "start": 746.96, "end": 748.96, "text": " The orange is on the outside.", "tokens": [50964, 440, 7671, 307, 322, 264, 2380, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 353, "seek": 73496, "start": 748.96, "end": 750.96, "text": " So we actually have Flux framework on the outside", "tokens": [51064, 407, 321, 767, 362, 3235, 2449, 8388, 322, 264, 2380, 51164], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 354, "seek": 73496, "start": 750.96, "end": 752.96, "text": " spinning up a Kubernetes cluster", "tokens": [51164, 15640, 493, 257, 23145, 13630, 51264], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 355, "seek": 73496, "start": 752.96, "end": 754.96, "text": " and notice that we actually still have", "tokens": [51264, 293, 3449, 300, 321, 767, 920, 362, 51364], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 356, "seek": 73496, "start": 754.96, "end": 756.96, "text": " compute running on bare metal alongside Kubernetes.", "tokens": [51364, 14722, 2614, 322, 6949, 5760, 12385, 23145, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 357, "seek": 73496, "start": 756.96, "end": 758.96, "text": " How's that possible?", "tokens": [51464, 1012, 311, 300, 1944, 30, 51564], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 358, "seek": 73496, "start": 758.96, "end": 760.96, "text": " Don't worry, I'll tell you.", "tokens": [51564, 1468, 380, 3292, 11, 286, 603, 980, 291, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 359, "seek": 73496, "start": 760.96, "end": 762.96, "text": " So why do we need this in the first place?", "tokens": [51664, 407, 983, 360, 321, 643, 341, 294, 264, 700, 1081, 30, 51764], "temperature": 0.0, "avg_logprob": -0.07894001298278343, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001955944811925292}, {"id": 360, "seek": 76296, "start": 762.96, "end": 764.96, "text": " As you know, also,", "tokens": [50364, 1018, 291, 458, 11, 611, 11, 50464], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 361, "seek": 76296, "start": 764.96, "end": 766.96, "text": " there are increasingly more", "tokens": [50464, 456, 366, 12980, 544, 50564], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 362, "seek": 76296, "start": 766.96, "end": 768.96, "text": " complex heterogeneous workloads", "tokens": [50564, 3997, 20789, 31112, 32452, 50664], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 363, "seek": 76296, "start": 768.96, "end": 770.96, "text": " that are coming to HPC.", "tokens": [50664, 300, 366, 1348, 281, 12557, 34, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 364, "seek": 76296, "start": 770.96, "end": 772.96, "text": " So this means not just, you know,", "tokens": [50764, 407, 341, 1355, 406, 445, 11, 291, 458, 11, 50864], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 365, "seek": 76296, "start": 772.96, "end": 774.96, "text": " embarrassingly parallel stuff,", "tokens": [50864, 9187, 12163, 8952, 1507, 11, 50964], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 366, "seek": 76296, "start": 774.96, "end": 776.96, "text": " but also adding in services,", "tokens": [50964, 457, 611, 5127, 294, 3328, 11, 51064], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 367, "seek": 76296, "start": 776.96, "end": 778.96, "text": " databases, task queues.", "tokens": [51064, 22380, 11, 5633, 631, 1247, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 368, "seek": 76296, "start": 778.96, "end": 780.96, "text": " Ah!", "tokens": [51164, 2438, 0, 51264], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 369, "seek": 76296, "start": 780.96, "end": 782.96, "text": " Okay, so I was...", "tokens": [51264, 1033, 11, 370, 286, 390, 485, 51364], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 370, "seek": 76296, "start": 782.96, "end": 784.96, "text": " This slide is not wrong.", "tokens": [51364, 639, 4137, 307, 406, 2085, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 371, "seek": 76296, "start": 784.96, "end": 786.96, "text": " I was going to give you an example", "tokens": [51464, 286, 390, 516, 281, 976, 291, 364, 1365, 51564], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 372, "seek": 76296, "start": 786.96, "end": 788.96, "text": " of such a workload, and apparently this slide", "tokens": [51564, 295, 1270, 257, 20139, 11, 293, 7970, 341, 4137, 51664], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 373, "seek": 76296, "start": 788.96, "end": 790.96, "text": " is giving you this warning that I'm a bad scientist", "tokens": [51664, 307, 2902, 291, 341, 9164, 300, 286, 478, 257, 1578, 12662, 51764], "temperature": 0.0, "avg_logprob": -0.08883432179939847, "compression_ratio": 1.5748031496062993, "no_speech_prob": 0.0027105440385639668}, {"id": 374, "seek": 79096, "start": 790.96, "end": 792.96, "text": " and I'm not wrong, but I will point out", "tokens": [50364, 293, 286, 478, 406, 2085, 11, 457, 286, 486, 935, 484, 50464], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 375, "seek": 79096, "start": 792.96, "end": 794.96, "text": " that my example is actually a very good example", "tokens": [50464, 300, 452, 1365, 307, 767, 257, 588, 665, 1365, 50564], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 376, "seek": 79096, "start": 794.96, "end": 796.96, "text": " that is a prototype for this kind of design.", "tokens": [50564, 300, 307, 257, 19475, 337, 341, 733, 295, 1715, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 377, "seek": 79096, "start": 796.96, "end": 798.96, "text": " Let's talk about that.", "tokens": [50664, 961, 311, 751, 466, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 378, "seek": 79096, "start": 798.96, "end": 800.96, "text": " So let's say that we're running simulations.", "tokens": [50764, 407, 718, 311, 584, 300, 321, 434, 2614, 35138, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 379, "seek": 79096, "start": 800.96, "end": 802.96, "text": " We're training examples", "tokens": [50864, 492, 434, 3097, 5110, 50964], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 380, "seek": 79096, "start": 802.96, "end": 804.96, "text": " one through N, whatever, doesn't matter,", "tokens": [50964, 472, 807, 426, 11, 2035, 11, 1177, 380, 1871, 11, 51064], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 381, "seek": 79096, "start": 804.96, "end": 806.96, "text": " and we want to send them", "tokens": [51064, 293, 321, 528, 281, 2845, 552, 51164], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 382, "seek": 79096, "start": 806.96, "end": 808.96, "text": " to a machine learning server, a specific endpoint", "tokens": [51164, 281, 257, 3479, 2539, 7154, 11, 257, 2685, 35795, 51264], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 383, "seek": 79096, "start": 808.96, "end": 810.96, "text": " to do the training.", "tokens": [51264, 281, 360, 264, 3097, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 384, "seek": 79096, "start": 810.96, "end": 812.96, "text": " We then want to wait till some metric of goodness", "tokens": [51364, 492, 550, 528, 281, 1699, 4288, 512, 20678, 295, 8387, 51464], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 385, "seek": 79096, "start": 812.96, "end": 814.96, "text": " or perhaps a number of samples,", "tokens": [51464, 420, 4317, 257, 1230, 295, 10938, 11, 51564], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 386, "seek": 79096, "start": 814.96, "end": 816.96, "text": " and then we want to flip it around.", "tokens": [51564, 293, 550, 321, 528, 281, 7929, 309, 926, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 387, "seek": 79096, "start": 816.96, "end": 818.96, "text": " We want to run simulations again,", "tokens": [51664, 492, 528, 281, 1190, 35138, 797, 11, 51764], "temperature": 0.0, "avg_logprob": -0.07694723889544293, "compression_ratio": 1.7655172413793103, "no_speech_prob": 0.018519898876547813}, {"id": 388, "seek": 81896, "start": 818.96, "end": 820.96, "text": " but we want to instead give this", "tokens": [50364, 457, 321, 528, 281, 2602, 976, 341, 50464], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 389, "seek": 81896, "start": 820.96, "end": 822.96, "text": " to our machine learning server", "tokens": [50464, 281, 527, 3479, 2539, 7154, 50564], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 390, "seek": 81896, "start": 822.96, "end": 824.96, "text": " without the actual values,", "tokens": [50564, 1553, 264, 3539, 4190, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 391, "seek": 81896, "start": 824.96, "end": 826.96, "text": " then we're going to have a vector of the true values", "tokens": [50664, 550, 321, 434, 516, 281, 362, 257, 8062, 295, 264, 2074, 4190, 50764], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 392, "seek": 81896, "start": 826.96, "end": 828.96, "text": " and the predictions, and we're going to see how well we did.", "tokens": [50764, 293, 264, 21264, 11, 293, 321, 434, 516, 281, 536, 577, 731, 321, 630, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 393, "seek": 81896, "start": 828.96, "end": 830.96, "text": " Now, very superficially,", "tokens": [50864, 823, 11, 588, 23881, 2270, 11, 50964], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 394, "seek": 81896, "start": 830.96, "end": 832.96, "text": " if we match this to HPC", "tokens": [50964, 498, 321, 2995, 341, 281, 12557, 34, 51064], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 395, "seek": 81896, "start": 832.96, "end": 834.96, "text": " versus Kubernetes, this is how we do it.", "tokens": [51064, 5717, 23145, 11, 341, 307, 577, 321, 360, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 396, "seek": 81896, "start": 834.96, "end": 836.96, "text": " We would expect that the simulations", "tokens": [51164, 492, 576, 2066, 300, 264, 35138, 51264], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 397, "seek": 81896, "start": 836.96, "end": 838.96, "text": " would run better on bare metal,", "tokens": [51264, 576, 1190, 1101, 322, 6949, 5760, 11, 51364], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 398, "seek": 81896, "start": 838.96, "end": 840.96, "text": " and the service thing would run better", "tokens": [51364, 293, 264, 2643, 551, 576, 1190, 1101, 51464], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 399, "seek": 81896, "start": 840.96, "end": 842.96, "text": " in user netties or Kubernetes.", "tokens": [51464, 294, 4195, 2533, 6097, 420, 23145, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 400, "seek": 81896, "start": 842.96, "end": 844.96, "text": " This is way to be...", "tokens": [51564, 639, 307, 636, 281, 312, 485, 51664], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 401, "seek": 81896, "start": 844.96, "end": 846.96, "text": " We need to prove to ourselves first.", "tokens": [51664, 492, 643, 281, 7081, 281, 4175, 700, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07796008345009624, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0025489497929811478}, {"id": 402, "seek": 84696, "start": 846.96, "end": 848.96, "text": " So a lot of you are probably out there like,", "tokens": [50364, 407, 257, 688, 295, 291, 366, 1391, 484, 456, 411, 11, 50464], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 403, "seek": 84696, "start": 848.96, "end": 850.96, "text": " user net, like, Kubernetes?", "tokens": [50464, 4195, 2533, 11, 411, 11, 23145, 30, 50564], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 404, "seek": 84696, "start": 850.96, "end": 852.96, "text": " Like, in user things, are you nuts?", "tokens": [50564, 1743, 11, 294, 4195, 721, 11, 366, 291, 10483, 30, 50664], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 405, "seek": 84696, "start": 852.96, "end": 854.96, "text": " I'm not nuts.", "tokens": [50664, 286, 478, 406, 10483, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 406, "seek": 84696, "start": 854.96, "end": 856.96, "text": " There's actually something called user netties.", "tokens": [50764, 821, 311, 767, 746, 1219, 4195, 2533, 6097, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 407, "seek": 84696, "start": 856.96, "end": 858.96, "text": " It's a Kubernetes enhancement proposal", "tokens": [50864, 467, 311, 257, 23145, 40776, 11494, 50964], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 408, "seek": 84696, "start": 858.96, "end": 860.96, "text": " or CUP proposal in 2022", "tokens": [50964, 420, 383, 22917, 11494, 294, 20229, 51064], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 409, "seek": 84696, "start": 860.96, "end": 862.96, "text": " by a very talented developer named", "tokens": [51064, 538, 257, 588, 13467, 10754, 4926, 51164], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 410, "seek": 84696, "start": 862.96, "end": 864.96, "text": " Akihiro Sudo.", "tokens": [51164, 316, 2984, 4954, 340, 318, 6207, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 411, "seek": 84696, "start": 864.96, "end": 866.96, "text": " Akihiro must point out", "tokens": [51264, 316, 2984, 4954, 340, 1633, 935, 484, 51364], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 412, "seek": 84696, "start": 866.96, "end": 868.96, "text": " won the top maintainer award", "tokens": [51364, 1582, 264, 1192, 6909, 260, 7130, 51464], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 413, "seek": 84696, "start": 868.96, "end": 870.96, "text": " for KUKON last year.", "tokens": [51464, 337, 591, 52, 42, 1928, 1036, 1064, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 414, "seek": 84696, "start": 870.96, "end": 872.96, "text": " He's an incredibly talented developer.", "tokens": [51564, 634, 311, 364, 6252, 13467, 10754, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 415, "seek": 84696, "start": 872.96, "end": 874.96, "text": " If you've used any of these technologies,", "tokens": [51664, 759, 291, 600, 1143, 604, 295, 613, 7943, 11, 51764], "temperature": 0.0, "avg_logprob": -0.17054109431024808, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0046015712432563305}, {"id": 416, "seek": 87496, "start": 874.96, "end": 876.96, "text": " he's the one behind it.", "tokens": [50364, 415, 311, 264, 472, 2261, 309, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 417, "seek": 87496, "start": 876.96, "end": 878.96, "text": " Hats off to Akihiro.", "tokens": [50464, 389, 1720, 766, 281, 316, 2984, 4954, 340, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 418, "seek": 87496, "start": 878.96, "end": 880.96, "text": " So last year, at the beginning of the year,", "tokens": [50564, 407, 1036, 1064, 11, 412, 264, 2863, 295, 264, 1064, 11, 50664], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 419, "seek": 87496, "start": 880.96, "end": 882.96, "text": " user netties was really a hodgepodge", "tokens": [50664, 4195, 2533, 6097, 390, 534, 257, 276, 19315, 79, 19315, 50764], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 420, "seek": 87496, "start": 882.96, "end": 884.96, "text": " with kind of bash grips. It was really hard to use.", "tokens": [50764, 365, 733, 295, 46183, 38037, 13, 467, 390, 534, 1152, 281, 764, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 421, "seek": 87496, "start": 884.96, "end": 886.96, "text": " So I engaged with Akihiro", "tokens": [50864, 407, 286, 8237, 365, 316, 2984, 4954, 340, 50964], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 422, "seek": 87496, "start": 886.96, "end": 888.96, "text": " and we released Generation 2", "tokens": [50964, 293, 321, 4736, 23898, 568, 51064], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 423, "seek": 87496, "start": 888.96, "end": 890.96, "text": " of user netties in September.", "tokens": [51064, 295, 4195, 2533, 6097, 294, 7216, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 424, "seek": 87496, "start": 890.96, "end": 892.96, "text": " And guess what?", "tokens": [51164, 400, 2041, 437, 30, 51264], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 425, "seek": 87496, "start": 892.96, "end": 894.96, "text": " It is using containerization, which is really great.", "tokens": [51264, 467, 307, 1228, 10129, 2144, 11, 597, 307, 534, 869, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 426, "seek": 87496, "start": 894.96, "end": 896.96, "text": " It has these components that we'll go into", "tokens": [51364, 467, 575, 613, 6677, 300, 321, 603, 352, 666, 51464], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 427, "seek": 87496, "start": 896.96, "end": 898.96, "text": " in more detail.", "tokens": [51464, 294, 544, 2607, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 428, "seek": 87496, "start": 898.96, "end": 900.96, "text": " So what does it mean in practice?", "tokens": [51564, 407, 437, 775, 309, 914, 294, 3124, 30, 51664], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 429, "seek": 87496, "start": 900.96, "end": 902.96, "text": " Well, it means when you're building a virtual machine,", "tokens": [51664, 1042, 11, 309, 1355, 562, 291, 434, 2390, 257, 6374, 3479, 11, 51764], "temperature": 0.0, "avg_logprob": -0.08124265155276737, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.010630609467625618}, {"id": 430, "seek": 90296, "start": 902.96, "end": 904.96, "text": " you need to have C groups version to enable.", "tokens": [50364, 291, 643, 281, 362, 383, 3935, 3037, 281, 9528, 13, 50464], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 431, "seek": 90296, "start": 904.96, "end": 906.96, "text": " I recommend LIMA or Linux virtual machines", "tokens": [50464, 286, 2748, 441, 6324, 32, 420, 18734, 6374, 8379, 50564], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 432, "seek": 90296, "start": 906.96, "end": 908.96, "text": " if you're prototyping this for the first time.", "tokens": [50564, 498, 291, 434, 46219, 3381, 341, 337, 264, 700, 565, 13, 50664], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 433, "seek": 90296, "start": 908.96, "end": 910.96, "text": " It also means that you need", "tokens": [50664, 467, 611, 1355, 300, 291, 643, 50764], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 434, "seek": 90296, "start": 910.96, "end": 912.96, "text": " to enable these kernel modules.", "tokens": [50764, 281, 9528, 613, 28256, 16679, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 435, "seek": 90296, "start": 912.96, "end": 914.96, "text": " So very generally speaking, the RNet filter", "tokens": [50864, 407, 588, 5101, 4124, 11, 264, 497, 31890, 6608, 50964], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 436, "seek": 90296, "start": 914.96, "end": 916.96, "text": " is going to allow you to apply IP tables, rules, bridge traffic.", "tokens": [50964, 307, 516, 281, 2089, 291, 281, 3079, 8671, 8020, 11, 4474, 11, 7283, 6419, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 437, "seek": 90296, "start": 916.96, "end": 918.96, "text": " VXLan is going to allow you to connect VXLan devices", "tokens": [51064, 691, 55, 43, 282, 307, 516, 281, 2089, 291, 281, 1745, 691, 55, 43, 282, 5759, 51164], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 438, "seek": 90296, "start": 918.96, "end": 920.96, "text": " on different hosts to a standalone bridge.", "tokens": [51164, 322, 819, 21573, 281, 257, 37454, 7283, 13, 51264], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 439, "seek": 90296, "start": 920.96, "end": 922.96, "text": " This is important because we actually have", "tokens": [51264, 639, 307, 1021, 570, 321, 767, 362, 51364], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 440, "seek": 90296, "start": 922.96, "end": 924.96, "text": " different physical nodes.", "tokens": [51364, 819, 4001, 13891, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 441, "seek": 90296, "start": 924.96, "end": 926.96, "text": " Now it's going to use RULE stocker.", "tokens": [51464, 823, 309, 311, 516, 281, 764, 497, 52, 2634, 4127, 260, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 442, "seek": 90296, "start": 926.96, "end": 928.96, "text": " This isn't such a crazy idea anymore.", "tokens": [51564, 639, 1943, 380, 1270, 257, 3219, 1558, 3602, 13, 51664], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 443, "seek": 90296, "start": 928.96, "end": 930.96, "text": " Many clusters have podmin these days.", "tokens": [51664, 5126, 23313, 362, 2497, 2367, 613, 1708, 13, 51764], "temperature": 0.0, "avg_logprob": -0.18681317567825317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.020254021510481834}, {"id": 444, "seek": 93096, "start": 930.96, "end": 932.96, "text": " And so what does it mean?", "tokens": [50364, 400, 370, 437, 775, 309, 914, 30, 50464], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 445, "seek": 93096, "start": 932.96, "end": 934.96, "text": " Actually, when you bring out these VMs,", "tokens": [50464, 5135, 11, 562, 291, 1565, 484, 613, 18038, 82, 11, 50564], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 446, "seek": 93096, "start": 934.96, "end": 936.96, "text": " it means that you're going to run a make up command", "tokens": [50564, 309, 1355, 300, 291, 434, 516, 281, 1190, 257, 652, 493, 5622, 50664], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 447, "seek": 93096, "start": 936.96, "end": 938.96, "text": " that has two contexts.", "tokens": [50664, 300, 575, 732, 30628, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 448, "seek": 93096, "start": 938.96, "end": 940.96, "text": " So both of them are going to build and start a base image", "tokens": [50764, 407, 1293, 295, 552, 366, 516, 281, 1322, 293, 722, 257, 3096, 3256, 50864], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 449, "seek": 93096, "start": 940.96, "end": 942.96, "text": " that is using kind, kubernetes,", "tokens": [50864, 300, 307, 1228, 733, 11, 350, 22457, 11, 50964], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 450, "seek": 93096, "start": 942.96, "end": 944.96, "text": " and Docker with CNI plugins.", "tokens": [50964, 293, 33772, 365, 14589, 40, 33759, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 451, "seek": 93096, "start": 944.96, "end": 946.96, "text": " And then the two contexts are the control plane", "tokens": [51064, 400, 550, 264, 732, 30628, 366, 264, 1969, 5720, 51164], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 452, "seek": 93096, "start": 946.96, "end": 948.96, "text": " and the worker.", "tokens": [51164, 293, 264, 11346, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 453, "seek": 93096, "start": 948.96, "end": 950.96, "text": " The control plane is going to install Flano,", "tokens": [51264, 440, 1969, 5720, 307, 516, 281, 3625, 3235, 3730, 11, 51364], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 454, "seek": 93096, "start": 950.96, "end": 952.96, "text": " run kubernetes, and admit.", "tokens": [51364, 1190, 350, 22457, 11, 293, 9796, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 455, "seek": 93096, "start": 952.96, "end": 954.96, "text": " This makes a joint command which is basically a token", "tokens": [51464, 639, 1669, 257, 7225, 5622, 597, 307, 1936, 257, 14862, 51564], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 456, "seek": 93096, "start": 954.96, "end": 956.96, "text": " that you give to the workers,", "tokens": [51564, 300, 291, 976, 281, 264, 5600, 11, 51664], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 457, "seek": 93096, "start": 956.96, "end": 958.96, "text": " and then the togers can authenticate and join the cluster.", "tokens": [51664, 293, 550, 264, 281, 9458, 393, 9214, 8700, 293, 3917, 264, 13630, 13, 51764], "temperature": 0.0, "avg_logprob": -0.17509988734596654, "compression_ratio": 1.8361774744027304, "no_speech_prob": 0.007330168038606644}, {"id": 458, "seek": 95896, "start": 958.96, "end": 960.96, "text": " And so that's what they do.", "tokens": [50364, 400, 370, 300, 311, 437, 436, 360, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 459, "seek": 95896, "start": 960.96, "end": 962.96, "text": " They're just like, I'm ready to serve.", "tokens": [50464, 814, 434, 445, 411, 11, 286, 478, 1919, 281, 4596, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 460, "seek": 95896, "start": 962.96, "end": 964.96, "text": " All right, so we created this garbage cluster", "tokens": [50564, 1057, 558, 11, 370, 321, 2942, 341, 14150, 13630, 50664], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 461, "seek": 95896, "start": 964.96, "end": 966.96, "text": " small and mighty using Overt and Ansible.", "tokens": [50664, 1359, 293, 21556, 1228, 422, 3281, 293, 14590, 964, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 462, "seek": 95896, "start": 966.96, "end": 968.96, "text": " It is small and mighty because each has", "tokens": [50764, 467, 307, 1359, 293, 21556, 570, 1184, 575, 50864], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 463, "seek": 95896, "start": 968.96, "end": 970.96, "text": " eight cores and 30 MBs RAM", "tokens": [50864, 3180, 24826, 293, 2217, 28866, 82, 14561, 50964], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 464, "seek": 95896, "start": 970.96, "end": 972.96, "text": " and a 10-NVVD iterate.", "tokens": [50964, 293, 257, 1266, 12, 45, 53, 53, 35, 44497, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 465, "seek": 95896, "start": 972.96, "end": 974.96, "text": " And I want to point out that we have", "tokens": [51064, 400, 286, 528, 281, 935, 484, 300, 321, 362, 51164], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 466, "seek": 95896, "start": 974.96, "end": 976.96, "text": " seven nodes here because generally speaking,", "tokens": [51164, 3407, 13891, 510, 570, 5101, 4124, 11, 51264], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 467, "seek": 95896, "start": 976.96, "end": 978.96, "text": " we're going to have six that we run", "tokens": [51264, 321, 434, 516, 281, 362, 2309, 300, 321, 1190, 51364], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 468, "seek": 95896, "start": 978.96, "end": 980.96, "text": " things with compute on and one's going to be", "tokens": [51364, 721, 365, 14722, 322, 293, 472, 311, 516, 281, 312, 51464], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 469, "seek": 95896, "start": 980.96, "end": 982.96, "text": " an admin node or control plane.", "tokens": [51464, 364, 24236, 9984, 420, 1969, 5720, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 470, "seek": 95896, "start": 982.96, "end": 984.96, "text": " Again,", "tokens": [51564, 3764, 11, 51664], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 471, "seek": 95896, "start": 984.96, "end": 986.96, "text": " warning, not bare metal,", "tokens": [51664, 9164, 11, 406, 6949, 5760, 11, 51764], "temperature": 0.0, "avg_logprob": -0.2605294447678786, "compression_ratio": 1.6075085324232081, "no_speech_prob": 0.011671951971948147}, {"id": 472, "seek": 98696, "start": 986.96, "end": 988.96, "text": " you get the deal.", "tokens": [50364, 291, 483, 264, 2028, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 473, "seek": 98696, "start": 988.96, "end": 990.96, "text": " All right, so what's in these VMs when we bring them up?", "tokens": [50464, 1057, 558, 11, 370, 437, 311, 294, 613, 18038, 82, 562, 321, 1565, 552, 493, 30, 50564], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 474, "seek": 98696, "start": 990.96, "end": 992.96, "text": " We have a complete system", "tokens": [50564, 492, 362, 257, 3566, 1185, 50664], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 475, "seek": 98696, "start": 992.96, "end": 994.96, "text": " install a flux,", "tokens": [50664, 3625, 257, 19298, 11, 50764], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 476, "seek": 98696, "start": 994.96, "end": 996.96, "text": " singularity on bare metal for reasons I'll tell you a little bit.", "tokens": [50764, 20010, 507, 322, 6949, 5760, 337, 4112, 286, 603, 980, 291, 257, 707, 857, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 477, "seek": 98696, "start": 996.96, "end": 998.96, "text": " Lamps installed on bare metal", "tokens": [50864, 441, 23150, 8899, 322, 6949, 5760, 50964], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 478, "seek": 98696, "start": 998.96, "end": 1000.96, "text": " and of course user netties ready to be brought up.", "tokens": [50964, 293, 295, 1164, 4195, 2533, 6097, 1919, 281, 312, 3038, 493, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 479, "seek": 98696, "start": 1000.96, "end": 1002.96, "text": " So once I shell into these VMs,", "tokens": [51064, 407, 1564, 286, 8720, 666, 613, 18038, 82, 11, 51164], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 480, "seek": 98696, "start": 1002.96, "end": 1004.96, "text": " my flux cluster is ready to go.", "tokens": [51164, 452, 19298, 13630, 307, 1919, 281, 352, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 481, "seek": 98696, "start": 1004.96, "end": 1006.96, "text": " I can do flux resource list and I can see all my nodes.", "tokens": [51264, 286, 393, 360, 19298, 7684, 1329, 293, 286, 393, 536, 439, 452, 13891, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 482, "seek": 98696, "start": 1006.96, "end": 1008.96, "text": " And user netties, again,", "tokens": [51364, 400, 4195, 2533, 6097, 11, 797, 11, 51464], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 483, "seek": 98696, "start": 1008.96, "end": 1010.96, "text": " that administrative node is also a control plane.", "tokens": [51464, 300, 17900, 9984, 307, 611, 257, 1969, 5720, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 484, "seek": 98696, "start": 1010.96, "end": 1012.96, "text": " So we technically have six nodes to work with.", "tokens": [51564, 407, 321, 12120, 362, 2309, 13891, 281, 589, 365, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 485, "seek": 98696, "start": 1012.96, "end": 1014.96, "text": " And then we have a user netties.", "tokens": [51664, 400, 550, 321, 362, 257, 4195, 2533, 6097, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2571821872543239, "compression_ratio": 1.729903536977492, "no_speech_prob": 0.0026299026794731617}, {"id": 486, "seek": 101496, "start": 1014.96, "end": 1016.96, "text": " So we technically have six nodes to work with.", "tokens": [50364, 407, 321, 12120, 362, 2309, 13891, 281, 589, 365, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 487, "seek": 101496, "start": 1016.96, "end": 1018.96, "text": " And we can still see them with", "tokens": [50464, 400, 321, 393, 920, 536, 552, 365, 50564], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 488, "seek": 101496, "start": 1018.96, "end": 1020.96, "text": " coop control get nodes.", "tokens": [50564, 13215, 1969, 483, 13891, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 489, "seek": 101496, "start": 1020.96, "end": 1022.96, "text": " Here's what we're working with.", "tokens": [50664, 1692, 311, 437, 321, 434, 1364, 365, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 490, "seek": 101496, "start": 1022.96, "end": 1024.96, "text": " User netties and flux running side by side", "tokens": [50764, 32127, 2533, 6097, 293, 19298, 2614, 1252, 538, 1252, 50864], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 491, "seek": 101496, "start": 1024.96, "end": 1026.96, "text": " the bare metal bros.", "tokens": [50864, 264, 6949, 5760, 738, 329, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 492, "seek": 101496, "start": 1026.96, "end": 1028.96, "text": " All right, bro, bro, what experiments", "tokens": [50964, 1057, 558, 11, 2006, 11, 2006, 11, 437, 12050, 51064], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 493, "seek": 101496, "start": 1028.96, "end": 1030.96, "text": " do we want to run all of them, bro?", "tokens": [51064, 360, 321, 528, 281, 1190, 439, 295, 552, 11, 2006, 30, 51164], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 494, "seek": 101496, "start": 1030.96, "end": 1032.96, "text": " All right.", "tokens": [51164, 1057, 558, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 495, "seek": 101496, "start": 1032.96, "end": 1034.96, "text": " So we first need to sanity check", "tokens": [51264, 407, 321, 700, 643, 281, 47892, 1520, 51364], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 496, "seek": 101496, "start": 1034.96, "end": 1036.96, "text": " that what I said earlier about the bare metal", "tokens": [51364, 300, 437, 286, 848, 3071, 466, 264, 6949, 5760, 51464], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 497, "seek": 101496, "start": 1036.96, "end": 1038.96, "text": " and lamps and the simulations is actually true.", "tokens": [51464, 293, 34887, 293, 264, 35138, 307, 767, 2074, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 498, "seek": 101496, "start": 1038.96, "end": 1040.96, "text": " We need to look at", "tokens": [51564, 492, 643, 281, 574, 412, 51664], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 499, "seek": 101496, "start": 1040.96, "end": 1042.96, "text": " application performance between flux", "tokens": [51664, 3861, 3389, 1296, 19298, 51764], "temperature": 0.0, "avg_logprob": -0.1616331013766202, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.005999367218464613}, {"id": 500, "seek": 104296, "start": 1042.96, "end": 1044.96, "text": " and user netties.", "tokens": [50364, 293, 4195, 2533, 6097, 13, 50464], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 501, "seek": 104296, "start": 1044.96, "end": 1046.96, "text": " So the way we're going to do that is by running a few things.", "tokens": [50464, 407, 264, 636, 321, 434, 516, 281, 360, 300, 307, 538, 2614, 257, 1326, 721, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 502, "seek": 104296, "start": 1046.96, "end": 1048.96, "text": " We're first going to run lamps on bare metal with flux.", "tokens": [50564, 492, 434, 700, 516, 281, 1190, 34887, 322, 6949, 5760, 365, 19298, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 503, "seek": 104296, "start": 1048.96, "end": 1050.96, "text": " We're then going to do the same thing", "tokens": [50664, 492, 434, 550, 516, 281, 360, 264, 912, 551, 50764], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 504, "seek": 104296, "start": 1050.96, "end": 1052.96, "text": " but in a singularity container.", "tokens": [50764, 457, 294, 257, 20010, 507, 10129, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 505, "seek": 104296, "start": 1052.96, "end": 1054.96, "text": " And I did this just to demonstrate that you don't", "tokens": [50864, 400, 286, 630, 341, 445, 281, 11698, 300, 291, 500, 380, 50964], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 506, "seek": 104296, "start": 1054.96, "end": 1056.96, "text": " lose anything by using containers.", "tokens": [50964, 3624, 1340, 538, 1228, 17089, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 507, "seek": 104296, "start": 1056.96, "end": 1058.96, "text": " Here's great.", "tokens": [51064, 1692, 311, 869, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 508, "seek": 104296, "start": 1058.96, "end": 1060.96, "text": " We're then going to run lamps in user netties", "tokens": [51164, 492, 434, 550, 516, 281, 1190, 34887, 294, 4195, 2533, 6097, 51264], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 509, "seek": 104296, "start": 1060.96, "end": 1062.96, "text": " with the flux operator.", "tokens": [51264, 365, 264, 19298, 12973, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 510, "seek": 104296, "start": 1062.96, "end": 1064.96, "text": " And then finally we're going to repeat cases one and two,", "tokens": [51364, 400, 550, 2721, 321, 434, 516, 281, 7149, 3331, 472, 293, 732, 11, 51464], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 511, "seek": 104296, "start": 1064.96, "end": 1066.96, "text": " but with user netties running in the background", "tokens": [51464, 457, 365, 4195, 2533, 6097, 2614, 294, 264, 3678, 51564], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 512, "seek": 104296, "start": 1066.96, "end": 1068.96, "text": " to look to see if there's any overhead of that.", "tokens": [51564, 281, 574, 281, 536, 498, 456, 311, 604, 19922, 295, 300, 13, 51664], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 513, "seek": 104296, "start": 1068.96, "end": 1070.96, "text": " And I need to pause for a second", "tokens": [51664, 400, 286, 643, 281, 10465, 337, 257, 1150, 51764], "temperature": 0.0, "avg_logprob": -0.06320977960742495, "compression_ratio": 1.9479166666666667, "no_speech_prob": 0.010478190146386623}, {"id": 514, "seek": 107096, "start": 1070.96, "end": 1072.96, "text": " because I know how incredibly cool this third case is.", "tokens": [50364, 570, 286, 458, 577, 6252, 1627, 341, 2636, 1389, 307, 13, 50464], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 515, "seek": 107096, "start": 1072.96, "end": 1074.96, "text": " We have", "tokens": [50464, 492, 362, 50564], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 516, "seek": 107096, "start": 1074.96, "end": 1076.96, "text": " flux on the outside.", "tokens": [50564, 19298, 322, 264, 2380, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 517, "seek": 107096, "start": 1076.96, "end": 1078.96, "text": " Flux is running", "tokens": [50664, 3235, 2449, 307, 2614, 50764], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 518, "seek": 107096, "start": 1078.96, "end": 1080.96, "text": " user netties.", "tokens": [50764, 4195, 2533, 6097, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 519, "seek": 107096, "start": 1080.96, "end": 1082.96, "text": " Within that we are launching the flux operator", "tokens": [50864, 15996, 300, 321, 366, 18354, 264, 19298, 12973, 50964], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 520, "seek": 107096, "start": 1082.96, "end": 1084.96, "text": " which is bringing up another instance of flux", "tokens": [50964, 597, 307, 5062, 493, 1071, 5197, 295, 19298, 51064], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 521, "seek": 107096, "start": 1084.96, "end": 1086.96, "text": " and inside there is where lamps is running.", "tokens": [51064, 293, 1854, 456, 307, 689, 34887, 307, 2614, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 522, "seek": 107096, "start": 1086.96, "end": 1088.96, "text": " So folks, like I know Thanksgiving is over", "tokens": [51164, 407, 4024, 11, 411, 286, 458, 21230, 307, 670, 51264], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 523, "seek": 107096, "start": 1088.96, "end": 1090.96, "text": " but this is the ultimate", "tokens": [51264, 457, 341, 307, 264, 9705, 51364], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 524, "seek": 107096, "start": 1090.96, "end": 1092.96, "text": " production.", "tokens": [51364, 4265, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 525, "seek": 107096, "start": 1092.96, "end": 1094.96, "text": " And we expect lamps to be slower in user netties", "tokens": [51464, 400, 321, 2066, 34887, 281, 312, 14009, 294, 4195, 2533, 6097, 51564], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 526, "seek": 107096, "start": 1094.96, "end": 1096.96, "text": " because as we know it makes MPI collective calls.", "tokens": [51564, 570, 382, 321, 458, 309, 1669, 14146, 40, 12590, 5498, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 527, "seek": 107096, "start": 1096.96, "end": 1098.96, "text": " User netties are using", "tokens": [51664, 32127, 2533, 6097, 366, 1228, 51764], "temperature": 0.0, "avg_logprob": -0.13214961944087858, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.00679342495277524}, {"id": 528, "seek": 109896, "start": 1098.96, "end": 1100.96, "text": " something called SERP 4.NET NS", "tokens": [50364, 746, 1219, 36772, 47, 1017, 13, 35554, 15943, 50464], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 529, "seek": 109896, "start": 1100.96, "end": 1102.96, "text": " which requires additional processing of packets", "tokens": [50464, 597, 7029, 4497, 9007, 295, 30364, 50564], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 530, "seek": 109896, "start": 1102.96, "end": 1104.96, "text": " with a tap device. I have a great paper I can share", "tokens": [50564, 365, 257, 5119, 4302, 13, 286, 362, 257, 869, 3035, 286, 393, 2073, 50664], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 531, "seek": 109896, "start": 1104.96, "end": 1106.96, "text": " if you're interested in learning more about that.", "tokens": [50664, 498, 291, 434, 3102, 294, 2539, 544, 466, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 532, "seek": 109896, "start": 1106.96, "end": 1108.96, "text": " So drumroll the results", "tokens": [50764, 407, 10206, 3970, 264, 3542, 50864], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 533, "seek": 109896, "start": 1108.96, "end": 1110.96, "text": " as we expected the", "tokens": [50864, 382, 321, 5176, 264, 50964], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 534, "seek": 109896, "start": 1110.96, "end": 1112.96, "text": " well actually maybe we didn't expect but guess what", "tokens": [50964, 731, 767, 1310, 321, 994, 380, 2066, 457, 2041, 437, 51064], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 535, "seek": 109896, "start": 1112.96, "end": 1114.96, "text": " the bare metal case is the singularity container", "tokens": [51064, 264, 6949, 5760, 1389, 307, 264, 20010, 507, 10129, 51164], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 536, "seek": 109896, "start": 1114.96, "end": 1116.96, "text": " is very comparable to actual bare metal.", "tokens": [51164, 307, 588, 25323, 281, 3539, 6949, 5760, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 537, "seek": 109896, "start": 1116.96, "end": 1118.96, "text": " I was very surprised by this.", "tokens": [51264, 286, 390, 588, 6100, 538, 341, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 538, "seek": 109896, "start": 1118.96, "end": 1120.96, "text": " So user netties does not", "tokens": [51364, 407, 4195, 2533, 6097, 775, 406, 51464], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 539, "seek": 109896, "start": 1120.96, "end": 1122.96, "text": " add a lot of overhead.", "tokens": [51464, 909, 257, 688, 295, 19922, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 540, "seek": 109896, "start": 1122.96, "end": 1124.96, "text": " And this is what we'd expected that guy up there", "tokens": [51564, 400, 341, 307, 437, 321, 1116, 5176, 300, 2146, 493, 456, 51664], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 541, "seek": 109896, "start": 1124.96, "end": 1126.96, "text": " running in user netties", "tokens": [51664, 2614, 294, 4195, 2533, 6097, 51764], "temperature": 0.0, "avg_logprob": -0.15733483908832938, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.01663532666862011}, {"id": 542, "seek": 112696, "start": 1126.96, "end": 1128.96, "text": " is about twice as slow as running", "tokens": [50364, 307, 466, 6091, 382, 2964, 382, 2614, 50464], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 543, "seek": 112696, "start": 1128.96, "end": 1130.96, "text": " on bare metal.", "tokens": [50464, 322, 6949, 5760, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 544, "seek": 112696, "start": 1130.96, "end": 1132.96, "text": " So what did we learn?", "tokens": [50564, 407, 437, 630, 321, 1466, 30, 50664], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 545, "seek": 112696, "start": 1132.96, "end": 1134.96, "text": " Well, we learned that for a setup like this", "tokens": [50664, 1042, 11, 321, 3264, 300, 337, 257, 8657, 411, 341, 50764], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 546, "seek": 112696, "start": 1134.96, "end": 1136.96, "text": " the network sensitive stuff", "tokens": [50764, 264, 3209, 9477, 1507, 50864], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 547, "seek": 112696, "start": 1136.96, "end": 1138.96, "text": " probably should be run on the HPC.", "tokens": [50864, 1391, 820, 312, 1190, 322, 264, 12557, 34, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 548, "seek": 112696, "start": 1138.96, "end": 1140.96, "text": " But I'll point out", "tokens": [50964, 583, 286, 603, 935, 484, 51064], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 549, "seek": 112696, "start": 1140.96, "end": 1142.96, "text": " there's opportunity for improving this in user netties.", "tokens": [51064, 456, 311, 2650, 337, 11470, 341, 294, 4195, 2533, 6097, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 550, "seek": 112696, "start": 1142.96, "end": 1144.96, "text": " If you have experience with networking", "tokens": [51164, 759, 291, 362, 1752, 365, 17985, 51264], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 551, "seek": 112696, "start": 1144.96, "end": 1146.96, "text": " I'd like you to go over to the GitHub", "tokens": [51264, 286, 1116, 411, 291, 281, 352, 670, 281, 264, 23331, 51364], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 552, "seek": 112696, "start": 1146.96, "end": 1148.96, "text": " right now and I'm just going to wait a lot for the talk", "tokens": [51364, 558, 586, 293, 286, 478, 445, 516, 281, 1699, 257, 688, 337, 264, 751, 51464], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 553, "seek": 112696, "start": 1148.96, "end": 1150.96, "text": " and engage with that to hear it to work on this problem.", "tokens": [51464, 293, 4683, 365, 300, 281, 1568, 309, 281, 589, 322, 341, 1154, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 554, "seek": 112696, "start": 1150.96, "end": 1152.96, "text": " Now the next thing we want", "tokens": [51564, 823, 264, 958, 551, 321, 528, 51664], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 555, "seek": 112696, "start": 1152.96, "end": 1154.96, "text": " to look at is distributed machine learning", "tokens": [51664, 281, 574, 412, 307, 12631, 3479, 2539, 51764], "temperature": 0.0, "avg_logprob": -0.09913310198716714, "compression_ratio": 1.6357827476038338, "no_speech_prob": 0.007795614656060934}, {"id": 556, "seek": 115496, "start": 1154.96, "end": 1156.96, "text": " specifically two cases", "tokens": [50364, 4682, 732, 3331, 50464], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 557, "seek": 115496, "start": 1156.96, "end": 1158.96, "text": " one distributed to across six nodes", "tokens": [50464, 472, 12631, 281, 2108, 2309, 13891, 50564], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 558, "seek": 115496, "start": 1158.96, "end": 1160.96, "text": " and then the second on one node", "tokens": [50564, 293, 550, 264, 1150, 322, 472, 9984, 50664], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 559, "seek": 115496, "start": 1160.96, "end": 1162.96, "text": " so the distributed case network is a", "tokens": [50664, 370, 264, 12631, 1389, 3209, 307, 257, 50764], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 560, "seek": 115496, "start": 1162.96, "end": 1164.96, "text": " variable and for the one node obviously", "tokens": [50764, 7006, 293, 337, 264, 472, 9984, 2745, 50864], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 561, "seek": 115496, "start": 1164.96, "end": 1166.96, "text": " network is not a variable.", "tokens": [50864, 3209, 307, 406, 257, 7006, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 562, "seek": 115496, "start": 1166.96, "end": 1168.96, "text": " Drum roll results same thing", "tokens": [50964, 40320, 3373, 3542, 912, 551, 51064], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 563, "seek": 115496, "start": 1168.96, "end": 1170.96, "text": " it's about twice as fast", "tokens": [51064, 309, 311, 466, 6091, 382, 2370, 51164], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 564, "seek": 115496, "start": 1170.96, "end": 1172.96, "text": " on bare metal or", "tokens": [51164, 322, 6949, 5760, 420, 51264], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 565, "seek": 115496, "start": 1172.96, "end": 1174.96, "text": " twice as slow I guess on user", "tokens": [51264, 6091, 382, 2964, 286, 2041, 322, 4195, 51364], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 566, "seek": 115496, "start": 1174.96, "end": 1176.96, "text": " netties.", "tokens": [51364, 2533, 6097, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 567, "seek": 115496, "start": 1176.96, "end": 1178.96, "text": " And interestingly when you look at just a", "tokens": [51464, 400, 25873, 562, 291, 574, 412, 445, 257, 51564], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 568, "seek": 115496, "start": 1178.96, "end": 1180.96, "text": " single node these are really", "tokens": [51564, 2167, 9984, 613, 366, 534, 51664], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 569, "seek": 115496, "start": 1180.96, "end": 1182.96, "text": " comparable so there's no issue with running", "tokens": [51664, 25323, 370, 456, 311, 572, 2734, 365, 2614, 51764], "temperature": 0.0, "avg_logprob": -0.11092271975108556, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.019087472930550575}, {"id": 570, "seek": 118296, "start": 1182.96, "end": 1184.96, "text": " something on a single node in user netties", "tokens": [50364, 746, 322, 257, 2167, 9984, 294, 4195, 2533, 6097, 50464], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 571, "seek": 118296, "start": 1184.96, "end": 1186.96, "text": " in and of itself it's really when you bring", "tokens": [50464, 294, 293, 295, 2564, 309, 311, 534, 562, 291, 1565, 50564], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 572, "seek": 118296, "start": 1186.96, "end": 1188.96, "text": " in the networking that it becomes a variable.", "tokens": [50564, 294, 264, 17985, 300, 309, 3643, 257, 7006, 13, 50664], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 573, "seek": 118296, "start": 1188.96, "end": 1190.96, "text": " So it's a network right", "tokens": [50664, 407, 309, 311, 257, 3209, 558, 50764], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 574, "seek": 118296, "start": 1190.96, "end": 1192.96, "text": " well let's sanity check one more thing", "tokens": [50764, 731, 718, 311, 47892, 1520, 472, 544, 551, 50864], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 575, "seek": 118296, "start": 1192.96, "end": 1194.96, "text": " here's I per thing we did one bit of", "tokens": [50864, 510, 311, 286, 680, 551, 321, 630, 472, 857, 295, 50964], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 576, "seek": 118296, "start": 1194.96, "end": 1196.96, "text": " transfer for each node as a client to each node", "tokens": [50964, 5003, 337, 1184, 9984, 382, 257, 6423, 281, 1184, 9984, 51064], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 577, "seek": 118296, "start": 1196.96, "end": 1198.96, "text": " as a server we see bit rate and give you bits", "tokens": [51064, 382, 257, 7154, 321, 536, 857, 3314, 293, 976, 291, 9239, 51164], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 578, "seek": 118296, "start": 1198.96, "end": 1200.96, "text": " per second is between 10 and 30 for bare metal", "tokens": [51164, 680, 1150, 307, 1296, 1266, 293, 2217, 337, 6949, 5760, 51264], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 579, "seek": 118296, "start": 1200.96, "end": 1202.96, "text": " user netties with like", "tokens": [51264, 4195, 2533, 6097, 365, 411, 51364], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 580, "seek": 118296, "start": 1202.96, "end": 1204.96, "text": " non detectable closest here are really really terrible", "tokens": [51364, 2107, 5531, 712, 13699, 510, 366, 534, 534, 6237, 51464], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 581, "seek": 118296, "start": 1204.96, "end": 1206.96, "text": " we can look so we can see the same patterns", "tokens": [51464, 321, 393, 574, 370, 321, 393, 536, 264, 912, 8294, 51564], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 582, "seek": 118296, "start": 1206.96, "end": 1208.96, "text": " for transfer gigabits per second", "tokens": [51564, 337, 5003, 8741, 455, 1208, 680, 1150, 51664], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 583, "seek": 118296, "start": 1208.96, "end": 1210.96, "text": " and so yes", "tokens": [51664, 293, 370, 2086, 51764], "temperature": 0.0, "avg_logprob": -0.18099226919161218, "compression_ratio": 1.852233676975945, "no_speech_prob": 0.007928667590022087}, {"id": 584, "seek": 121096, "start": 1210.96, "end": 1212.96, "text": " it's the network we're pretty confident", "tokens": [50364, 309, 311, 264, 3209, 321, 434, 1238, 6679, 50464], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 585, "seek": 121096, "start": 1212.96, "end": 1214.96, "text": " for the setup it's the network.", "tokens": [50464, 337, 264, 8657, 309, 311, 264, 3209, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 586, "seek": 121096, "start": 1214.96, "end": 1216.96, "text": " All right can we do the fun workflow now", "tokens": [50564, 1057, 558, 393, 321, 360, 264, 1019, 20993, 586, 50664], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 587, "seek": 121096, "start": 1216.96, "end": 1218.96, "text": " we absolutely can so guess", "tokens": [50664, 321, 3122, 393, 370, 2041, 50764], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 588, "seek": 121096, "start": 1218.96, "end": 1220.96, "text": " what I actually prototyped this kind", "tokens": [50764, 437, 286, 767, 46219, 3452, 341, 733, 50864], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 589, "seek": 121096, "start": 1220.96, "end": 1222.96, "text": " of workflow because I was really excited about it", "tokens": [50864, 295, 20993, 570, 286, 390, 534, 2919, 466, 309, 50964], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 590, "seek": 121096, "start": 1222.96, "end": 1224.96, "text": " and so what we're going to do is we're going to", "tokens": [50964, 293, 370, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 51064], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 591, "seek": 121096, "start": 1224.96, "end": 1226.96, "text": " be launching a batch job with flux", "tokens": [51064, 312, 18354, 257, 15245, 1691, 365, 19298, 51164], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 592, "seek": 121096, "start": 1226.96, "end": 1228.96, "text": " batch this means the flux instance", "tokens": [51164, 15245, 341, 1355, 264, 19298, 5197, 51264], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 593, "seek": 121096, "start": 1228.96, "end": 1230.96, "text": " that's only by the running user it's going", "tokens": [51264, 300, 311, 787, 538, 264, 2614, 4195, 309, 311, 516, 51364], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 594, "seek": 121096, "start": 1230.96, "end": 1232.96, "text": " to scope resources using hw lock", "tokens": [51364, 281, 11923, 3593, 1228, 276, 86, 4017, 51464], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 595, "seek": 121096, "start": 1232.96, "end": 1234.96, "text": " in this backshot where we can", "tokens": [51464, 294, 341, 646, 18402, 689, 321, 393, 51564], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 596, "seek": 121096, "start": 1234.96, "end": 1236.96, "text": " basically bring up and tear down all", "tokens": [51564, 1936, 1565, 493, 293, 12556, 760, 439, 51664], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 597, "seek": 121096, "start": 1236.96, "end": 1238.96, "text": " of user netties.", "tokens": [51664, 295, 4195, 2533, 6097, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1183032574860946, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.01822761259973049}, {"id": 598, "seek": 123896, "start": 1238.96, "end": 1240.96, "text": " We're going to take that workflow that I mentioned before", "tokens": [50364, 492, 434, 516, 281, 747, 300, 20993, 300, 286, 2835, 949, 50464], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 599, "seek": 123896, "start": 1240.96, "end": 1242.96, "text": " we're going to map it into our star track cluster", "tokens": [50464, 321, 434, 516, 281, 4471, 309, 666, 527, 3543, 2837, 13630, 50564], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 600, "seek": 123896, "start": 1242.96, "end": 1244.96, "text": " space so we're going to run simulations with lamps", "tokens": [50564, 1901, 370, 321, 434, 516, 281, 1190, 35138, 365, 34887, 50664], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 601, "seek": 123896, "start": 1244.96, "end": 1246.96, "text": " randomly selecting the problem", "tokens": [50664, 16979, 18182, 264, 1154, 50764], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 602, "seek": 123896, "start": 1246.96, "end": 1248.96, "text": " sizes predict well time we're then", "tokens": [50764, 11602, 6069, 731, 565, 321, 434, 550, 50864], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 603, "seek": 123896, "start": 1248.96, "end": 1250.96, "text": " going to bring up a machine learning server a", "tokens": [50864, 516, 281, 1565, 493, 257, 3479, 2539, 7154, 257, 50964], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 604, "seek": 123896, "start": 1250.96, "end": 1252.96, "text": " special server I made using river a few years", "tokens": [50964, 2121, 7154, 286, 1027, 1228, 6810, 257, 1326, 924, 51064], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 605, "seek": 123896, "start": 1252.96, "end": 1254.96, "text": " ago and then we're going to", "tokens": [51064, 2057, 293, 550, 321, 434, 516, 281, 51164], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 606, "seek": 123896, "start": 1254.96, "end": 1256.96, "text": " basically do the test cases we're going to run", "tokens": [51164, 1936, 360, 264, 1500, 3331, 321, 434, 516, 281, 1190, 51264], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 607, "seek": 123896, "start": 1256.96, "end": 1258.96, "text": " lamps again but we're going to leave", "tokens": [51264, 34887, 797, 457, 321, 434, 516, 281, 1856, 51364], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 608, "seek": 123896, "start": 1258.96, "end": 1260.96, "text": " out the actual well time and we're", "tokens": [51364, 484, 264, 3539, 731, 565, 293, 321, 434, 51464], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 609, "seek": 123896, "start": 1260.96, "end": 1262.96, "text": " going to ask our models what it is", "tokens": [51464, 516, 281, 1029, 527, 5245, 437, 309, 307, 51564], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 610, "seek": 123896, "start": 1262.96, "end": 1264.96, "text": " and we're going to do", "tokens": [51564, 293, 321, 434, 516, 281, 360, 51664], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 611, "seek": 123896, "start": 1264.96, "end": 1266.96, "text": " a thousand training samples and 250", "tokens": [51664, 257, 4714, 3097, 10938, 293, 11650, 51764], "temperature": 0.0, "avg_logprob": -0.11850453747643365, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.005550018511712551}, {"id": 612, "seek": 126696, "start": 1266.96, "end": 1268.96, "text": " testing samples.", "tokens": [50364, 4997, 10938, 13, 50464], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 613, "seek": 126696, "start": 1268.96, "end": 1270.96, "text": " How do we do?", "tokens": [50464, 1012, 360, 321, 360, 30, 50564], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 614, "seek": 126696, "start": 1270.96, "end": 1272.96, "text": " I put no thought into these", "tokens": [50564, 286, 829, 572, 1194, 666, 613, 50664], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 615, "seek": 126696, "start": 1272.96, "end": 1274.96, "text": " particular models but I did", "tokens": [50664, 1729, 5245, 457, 286, 630, 50764], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 616, "seek": 126696, "start": 1274.96, "end": 1276.96, "text": " three kinds of regression", "tokens": [50764, 1045, 3685, 295, 24590, 50864], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 617, "seek": 126696, "start": 1276.96, "end": 1278.96, "text": " the Bayesian and sampling from a probability distribution", "tokens": [50864, 264, 7840, 42434, 293, 21179, 490, 257, 8482, 7316, 50964], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 618, "seek": 126696, "start": 1278.96, "end": 1280.96, "text": " didn't do super well but for the", "tokens": [50964, 994, 380, 360, 1687, 731, 457, 337, 264, 51064], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 619, "seek": 126696, "start": 1280.96, "end": 1282.96, "text": " first two there's an actual", "tokens": [51064, 700, 732, 456, 311, 364, 3539, 51164], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 620, "seek": 126696, "start": 1282.96, "end": 1284.96, "text": " kind of pattern between the predicted and the", "tokens": [51164, 733, 295, 5102, 1296, 264, 19147, 293, 264, 51264], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 621, "seek": 126696, "start": 1284.96, "end": 1286.96, "text": " actual time and so although", "tokens": [51264, 3539, 565, 293, 370, 4878, 51364], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 622, "seek": 126696, "start": 1286.96, "end": 1288.96, "text": " I put no thought into this I was really pleased", "tokens": [51364, 286, 829, 572, 1194, 666, 341, 286, 390, 534, 10587, 51464], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 623, "seek": 126696, "start": 1288.96, "end": 1290.96, "text": " with this result to see that the general prototype", "tokens": [51464, 365, 341, 1874, 281, 536, 300, 264, 2674, 19475, 51564], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 624, "seek": 126696, "start": 1290.96, "end": 1292.96, "text": " this idea of having bare", "tokens": [51564, 341, 1558, 295, 1419, 6949, 51664], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 625, "seek": 126696, "start": 1292.96, "end": 1294.96, "text": " middle simulations running alongside a service", "tokens": [51664, 2808, 35138, 2614, 12385, 257, 2643, 51764], "temperature": 0.0, "avg_logprob": -0.14556875148741136, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0036447460297495127}, {"id": 626, "seek": 129496, "start": 1294.96, "end": 1296.96, "text": " there is something", "tokens": [50364, 456, 307, 746, 50464], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 627, "seek": 129496, "start": 1296.96, "end": 1298.96, "text": " here we can do science", "tokens": [50464, 510, 321, 393, 360, 3497, 50564], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 628, "seek": 129496, "start": 1298.96, "end": 1300.96, "text": " this way with actual real scientific questions", "tokens": [50564, 341, 636, 365, 3539, 957, 8134, 1651, 50664], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 629, "seek": 129496, "start": 1300.96, "end": 1302.96, "text": " and I'll point out that there are", "tokens": [50664, 293, 286, 603, 935, 484, 300, 456, 366, 50764], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 630, "seek": 129496, "start": 1302.96, "end": 1304.96, "text": " real heterogeneous workloads out in the wild", "tokens": [50764, 957, 20789, 31112, 32452, 484, 294, 264, 4868, 50864], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 631, "seek": 129496, "start": 1304.96, "end": 1306.96, "text": " and you this capability here's Moomi", "tokens": [50864, 293, 291, 341, 13759, 510, 311, 3335, 9220, 50964], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 632, "seek": 129496, "start": 1306.96, "end": 1308.96, "text": " the massively parallel multi-stale machine", "tokens": [50964, 264, 29379, 8952, 4825, 12, 372, 1220, 3479, 51064], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 633, "seek": 129496, "start": 1308.96, "end": 1310.96, "text": " learn model infrastructure", "tokens": [51064, 1466, 2316, 6896, 51164], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 634, "seek": 129496, "start": 1310.96, "end": 1312.96, "text": " and this is basically simulating biological systems", "tokens": [51164, 293, 341, 307, 1936, 1034, 12162, 13910, 3652, 51264], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 635, "seek": 129496, "start": 1312.96, "end": 1314.96, "text": " the interact between proteins and plasma", "tokens": [51264, 264, 4648, 1296, 15577, 293, 22564, 51364], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 636, "seek": 129496, "start": 1314.96, "end": 1316.96, "text": " membrane I'll also point out that the", "tokens": [51364, 19651, 286, 603, 611, 935, 484, 300, 264, 51464], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 637, "seek": 129496, "start": 1316.96, "end": 1318.96, "text": " Moomins are what it's based on", "tokens": [51464, 3335, 298, 1292, 366, 437, 309, 311, 2361, 322, 51564], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 638, "seek": 129496, "start": 1318.96, "end": 1320.96, "text": " the name the finished book comic book series", "tokens": [51564, 264, 1315, 264, 4335, 1446, 13900, 1446, 2638, 51664], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 639, "seek": 129496, "start": 1320.96, "end": 1322.96, "text": " with really cute hippos with often yellow", "tokens": [51664, 365, 534, 4052, 27745, 329, 365, 2049, 5566, 51764], "temperature": 0.0, "avg_logprob": -0.18435847665381244, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.008558946661651134}, {"id": 640, "seek": 132296, "start": 1322.96, "end": 1324.96, "text": " spiky hair very awesome", "tokens": [50364, 637, 1035, 88, 2578, 588, 3476, 50464], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 641, "seek": 132296, "start": 1324.96, "end": 1326.96, "text": " so this is the perfect example", "tokens": [50464, 370, 341, 307, 264, 2176, 1365, 50564], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 642, "seek": 132296, "start": 1326.96, "end": 1328.96, "text": " the bare metal rows of coexistence", "tokens": [50564, 264, 6949, 5760, 13241, 295, 48086, 655, 50664], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 643, "seek": 132296, "start": 1328.96, "end": 1330.96, "text": " adopting technologies to make it possible", "tokens": [50664, 32328, 7943, 281, 652, 309, 1944, 50764], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 644, "seek": 132296, "start": 1330.96, "end": 1332.96, "text": " to go to coexist and", "tokens": [50764, 281, 352, 281, 48086, 293, 50864], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 645, "seek": 132296, "start": 1332.96, "end": 1334.96, "text": " continuing to improve upon them so that", "tokens": [50864, 9289, 281, 3470, 3564, 552, 370, 300, 50964], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 646, "seek": 132296, "start": 1334.96, "end": 1336.96, "text": " for example with networking", "tokens": [50964, 337, 1365, 365, 17985, 51064], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 647, "seek": 132296, "start": 1336.96, "end": 1338.96, "text": " this environment can get even better", "tokens": [51064, 341, 2823, 393, 483, 754, 1101, 51164], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 648, "seek": 132296, "start": 1338.96, "end": 1340.96, "text": " so what should you", "tokens": [51164, 370, 437, 820, 291, 51264], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 649, "seek": 132296, "start": 1340.96, "end": 1342.96, "text": " remember from this talk if you take nothing", "tokens": [51264, 1604, 490, 341, 751, 498, 291, 747, 1825, 51364], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 650, "seek": 132296, "start": 1342.96, "end": 1344.96, "text": " else away the first is looking out", "tokens": [51364, 1646, 1314, 264, 700, 307, 1237, 484, 51464], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 651, "seek": 132296, "start": 1344.96, "end": 1346.96, "text": " for opportunities for collaboration", "tokens": [51464, 337, 4786, 337, 9363, 51564], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 652, "seek": 132296, "start": 1346.96, "end": 1348.96, "text": " look for that alignment of goals between", "tokens": [51564, 574, 337, 300, 18515, 295, 5493, 1296, 51664], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 653, "seek": 132296, "start": 1348.96, "end": 1350.96, "text": " spaces that's an opportunity", "tokens": [51664, 7673, 300, 311, 364, 2650, 51764], "temperature": 0.0, "avg_logprob": -0.12470286233084542, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.008421406149864197}, {"id": 654, "seek": 135096, "start": 1350.96, "end": 1352.96, "text": " the second is providing handles for your", "tokens": [50364, 264, 1150, 307, 6530, 18722, 337, 428, 50464], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 655, "seek": 135096, "start": 1352.96, "end": 1354.96, "text": " components so you don't have the bandwidth", "tokens": [50464, 6677, 370, 291, 500, 380, 362, 264, 23647, 50564], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 656, "seek": 135096, "start": 1354.96, "end": 1356.96, "text": " to look for opportunities add some", "tokens": [50564, 281, 574, 337, 4786, 909, 512, 50664], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 657, "seek": 135096, "start": 1356.96, "end": 1358.96, "text": " go bindings to C++ projects because", "tokens": [50664, 352, 14786, 1109, 281, 383, 25472, 4455, 570, 50764], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 658, "seek": 135096, "start": 1358.96, "end": 1360.96, "text": " someone else could find you", "tokens": [50764, 1580, 1646, 727, 915, 291, 50864], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 659, "seek": 135096, "start": 1360.96, "end": 1362.96, "text": " the third is engagement", "tokens": [50864, 264, 2636, 307, 8742, 50964], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 660, "seek": 135096, "start": 1362.96, "end": 1364.96, "text": " we need to show up at the table", "tokens": [50964, 321, 643, 281, 855, 493, 412, 264, 3199, 51064], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 661, "seek": 135096, "start": 1364.96, "end": 1366.96, "text": " we need to go to working groups, conferences", "tokens": [51064, 321, 643, 281, 352, 281, 1364, 3935, 11, 22032, 51164], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 662, "seek": 135096, "start": 1366.96, "end": 1368.96, "text": " places that you haven't traditionally been", "tokens": [51164, 3190, 300, 291, 2378, 380, 19067, 668, 51264], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 663, "seek": 135096, "start": 1368.96, "end": 1370.96, "text": " to engage in to find these opportunities", "tokens": [51264, 281, 4683, 294, 281, 915, 613, 4786, 51364], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 664, "seek": 135096, "start": 1370.96, "end": 1372.96, "text": " for collaboration and possibly", "tokens": [51364, 337, 9363, 293, 6264, 51464], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 665, "seek": 135096, "start": 1372.96, "end": 1374.96, "text": " the most important is this", "tokens": [51464, 264, 881, 1021, 307, 341, 51564], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 666, "seek": 135096, "start": 1374.96, "end": 1376.96, "text": " mindset we've had this mindset", "tokens": [51564, 12543, 321, 600, 632, 341, 12543, 51664], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 667, "seek": 135096, "start": 1376.96, "end": 1378.96, "text": " of cloud versus HPC", "tokens": [51664, 295, 4588, 5717, 12557, 34, 51764], "temperature": 0.0, "avg_logprob": -0.10887719002090582, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0024317344650626183}, {"id": 668, "seek": 137896, "start": 1378.96, "end": 1380.96, "text": " that one has to win", "tokens": [50364, 300, 472, 575, 281, 1942, 50464], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 669, "seek": 137896, "start": 1380.96, "end": 1382.96, "text": " but they're different for so long", "tokens": [50464, 457, 436, 434, 819, 337, 370, 938, 50564], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 670, "seek": 137896, "start": 1382.96, "end": 1384.96, "text": " we need to throw that away", "tokens": [50564, 321, 643, 281, 3507, 300, 1314, 50664], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 671, "seek": 137896, "start": 1384.96, "end": 1386.96, "text": " and get rid of the adversarial thinking", "tokens": [50664, 293, 483, 3973, 295, 264, 17641, 44745, 1953, 50764], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 672, "seek": 137896, "start": 1386.96, "end": 1388.96, "text": " and have a more collaborative", "tokens": [50764, 293, 362, 257, 544, 16555, 50864], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 673, "seek": 137896, "start": 1388.96, "end": 1390.96, "text": " mindset this", "tokens": [50864, 12543, 341, 50964], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 674, "seek": 137896, "start": 1390.96, "end": 1392.96, "text": " is the vision that we have for the future", "tokens": [50964, 307, 264, 5201, 300, 321, 362, 337, 264, 2027, 51064], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 675, "seek": 137896, "start": 1392.96, "end": 1394.96, "text": " for converge computing", "tokens": [51064, 337, 41881, 15866, 51164], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 676, "seek": 137896, "start": 1394.96, "end": 1396.96, "text": " and we hope that you like to join us", "tokens": [51164, 293, 321, 1454, 300, 291, 411, 281, 3917, 505, 51264], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 677, "seek": 137896, "start": 1396.96, "end": 1398.96, "text": " so thank you", "tokens": [51264, 370, 1309, 291, 51364], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 678, "seek": 137896, "start": 1398.96, "end": 1400.96, "text": " that's how to reach me my email", "tokens": [51364, 300, 311, 577, 281, 2524, 385, 452, 3796, 51464], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 679, "seek": 137896, "start": 1400.96, "end": 1402.96, "text": " and social networks and here's some interesting links", "tokens": [51464, 293, 2093, 9590, 293, 510, 311, 512, 1880, 6123, 51564], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 680, "seek": 137896, "start": 1402.96, "end": 1404.96, "text": " for the flux and the various projects", "tokens": [51564, 337, 264, 19298, 293, 264, 3683, 4455, 51664], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 681, "seek": 137896, "start": 1404.96, "end": 1406.96, "text": " I think I will take some questions", "tokens": [51664, 286, 519, 286, 486, 747, 512, 1651, 51764], "temperature": 0.0, "avg_logprob": -0.07753972683922719, "compression_ratio": 1.7410358565737052, "no_speech_prob": 0.03710964694619179}, {"id": 682, "seek": 140696, "start": 1406.96, "end": 1408.96, "text": " virtually now", "tokens": [50364, 14103, 586, 50464], "temperature": 0.0, "avg_logprob": -0.14135484846811447, "compression_ratio": 1.5374149659863945, "no_speech_prob": 0.021850304678082466}, {"id": 683, "seek": 140696, "start": 1416.96, "end": 1418.96, "text": " okay we can take a couple of questions", "tokens": [50864, 1392, 321, 393, 747, 257, 1916, 295, 1651, 50964], "temperature": 0.0, "avg_logprob": -0.14135484846811447, "compression_ratio": 1.5374149659863945, "no_speech_prob": 0.021850304678082466}, {"id": 684, "seek": 140696, "start": 1418.96, "end": 1420.96, "text": " it seems like the wifi is stable enough", "tokens": [50964, 309, 2544, 411, 264, 35246, 307, 8351, 1547, 51064], "temperature": 0.0, "avg_logprob": -0.14135484846811447, "compression_ratio": 1.5374149659863945, "no_speech_prob": 0.021850304678082466}, {"id": 685, "seek": 140696, "start": 1420.96, "end": 1422.96, "text": " to let Vanessa answer them", "tokens": [51064, 281, 718, 27928, 1867, 552, 51164], "temperature": 0.0, "avg_logprob": -0.14135484846811447, "compression_ratio": 1.5374149659863945, "no_speech_prob": 0.021850304678082466}, {"id": 686, "seek": 140696, "start": 1422.96, "end": 1424.96, "text": " do we have any questions", "tokens": [51164, 360, 321, 362, 604, 1651, 51264], "temperature": 0.0, "avg_logprob": -0.14135484846811447, "compression_ratio": 1.5374149659863945, "no_speech_prob": 0.021850304678082466}, {"id": 687, "seek": 140696, "start": 1424.96, "end": 1426.96, "text": " okay", "tokens": [51264, 1392, 51364], "temperature": 0.0, "avg_logprob": -0.14135484846811447, "compression_ratio": 1.5374149659863945, "no_speech_prob": 0.021850304678082466}, {"id": 688, "seek": 140696, "start": 1428.96, "end": 1430.96, "text": " so Vanessa we may have to repeat", "tokens": [51464, 370, 27928, 321, 815, 362, 281, 7149, 51564], "temperature": 0.0, "avg_logprob": -0.14135484846811447, "compression_ratio": 1.5374149659863945, "no_speech_prob": 0.021850304678082466}, {"id": 689, "seek": 140696, "start": 1430.96, "end": 1432.96, "text": " a question for you we'll see how that works", "tokens": [51564, 257, 1168, 337, 291, 321, 603, 536, 577, 300, 1985, 51664], "temperature": 0.0, "avg_logprob": -0.14135484846811447, "compression_ratio": 1.5374149659863945, "no_speech_prob": 0.021850304678082466}, {"id": 690, "seek": 143696, "start": 1436.96, "end": 1438.96, "text": " hi Vanessa amazing talk", "tokens": [50364, 4879, 27928, 2243, 751, 50464], "temperature": 0.0, "avg_logprob": -0.13734730180487575, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.018835000693798065}, {"id": 691, "seek": 143696, "start": 1438.96, "end": 1440.96, "text": " congrats", "tokens": [50464, 8882, 1720, 50564], "temperature": 0.0, "avg_logprob": -0.13734730180487575, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.018835000693798065}, {"id": 692, "seek": 143696, "start": 1440.96, "end": 1442.96, "text": " so I was wondering", "tokens": [50564, 370, 286, 390, 6359, 50664], "temperature": 0.0, "avg_logprob": -0.13734730180487575, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.018835000693798065}, {"id": 693, "seek": 143696, "start": 1442.96, "end": 1444.96, "text": " if your architecture", "tokens": [50664, 498, 428, 9482, 50764], "temperature": 0.0, "avg_logprob": -0.13734730180487575, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.018835000693798065}, {"id": 694, "seek": 143696, "start": 1444.96, "end": 1446.96, "text": " can support sidecars", "tokens": [50764, 393, 1406, 1252, 66, 685, 50864], "temperature": 0.0, "avg_logprob": -0.13734730180487575, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.018835000693798065}, {"id": 695, "seek": 143696, "start": 1446.96, "end": 1448.96, "text": " because one of the nightmares I had", "tokens": [50864, 570, 472, 295, 264, 36911, 286, 632, 50964], "temperature": 0.0, "avg_logprob": -0.13734730180487575, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.018835000693798065}, {"id": 696, "seek": 143696, "start": 1448.96, "end": 1450.96, "text": " when I was trying to do something", "tokens": [50964, 562, 286, 390, 1382, 281, 360, 746, 51064], "temperature": 0.0, "avg_logprob": -0.13734730180487575, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.018835000693798065}, {"id": 697, "seek": 143696, "start": 1450.96, "end": 1452.96, "text": " similar was that in order", "tokens": [51064, 2531, 390, 300, 294, 1668, 51164], "temperature": 0.0, "avg_logprob": -0.13734730180487575, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.018835000693798065}, {"id": 698, "seek": 143696, "start": 1452.96, "end": 1454.96, "text": " to get the sidecars running", "tokens": [51164, 281, 483, 264, 1252, 66, 685, 2614, 51264], "temperature": 0.0, "avg_logprob": -0.13734730180487575, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.018835000693798065}, {"id": 699, "seek": 143696, "start": 1454.96, "end": 1456.96, "text": " I had to spin up a second", "tokens": [51264, 286, 632, 281, 6060, 493, 257, 1150, 51364], "temperature": 0.0, "avg_logprob": -0.13734730180487575, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.018835000693798065}, {"id": 700, "seek": 143696, "start": 1456.96, "end": 1458.96, "text": " network stack and that created a lot of overhead", "tokens": [51364, 3209, 8630, 293, 300, 2942, 257, 688, 295, 19922, 51464], "temperature": 0.0, "avg_logprob": -0.13734730180487575, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.018835000693798065}, {"id": 701, "seek": 146696, "start": 1466.96, "end": 1468.96, "text": " so", "tokens": [50364, 370, 50464], "temperature": 0.0, "avg_logprob": -0.5231357018152872, "compression_ratio": 0.9838709677419355, "no_speech_prob": 0.2159610092639923}, {"id": 702, "seek": 146696, "start": 1468.96, "end": 1470.96, "text": " no", "tokens": [50464, 572, 50564], "temperature": 0.0, "avg_logprob": -0.5231357018152872, "compression_ratio": 0.9838709677419355, "no_speech_prob": 0.2159610092639923}, {"id": 703, "seek": 146696, "start": 1488.96, "end": 1490.96, "text": " no just one is on", "tokens": [51464, 572, 445, 472, 307, 322, 51564], "temperature": 0.0, "avg_logprob": -0.5231357018152872, "compression_ratio": 0.9838709677419355, "no_speech_prob": 0.2159610092639923}, {"id": 704, "seek": 146696, "start": 1492.96, "end": 1494.96, "text": " okay did you get the question Vanessa", "tokens": [51664, 1392, 630, 291, 483, 264, 1168, 27928, 51764], "temperature": 0.0, "avg_logprob": -0.5231357018152872, "compression_ratio": 0.9838709677419355, "no_speech_prob": 0.2159610092639923}, {"id": 705, "seek": 149496, "start": 1494.96, "end": 1496.96, "text": " no I didn't hear the question at all", "tokens": [50364, 572, 286, 994, 380, 1568, 264, 1168, 412, 439, 50464], "temperature": 0.0, "avg_logprob": -0.14288972966811236, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.036744125187397}, {"id": 706, "seek": 149496, "start": 1498.96, "end": 1500.96, "text": " neither did I", "tokens": [50564, 9662, 630, 286, 50664], "temperature": 0.0, "avg_logprob": -0.14288972966811236, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.036744125187397}, {"id": 707, "seek": 149496, "start": 1500.96, "end": 1502.96, "text": " yeah maybe that's better", "tokens": [50664, 1338, 1310, 300, 311, 1101, 50764], "temperature": 0.0, "avg_logprob": -0.14288972966811236, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.036744125187397}, {"id": 708, "seek": 149496, "start": 1502.96, "end": 1504.96, "text": " okay let's do it like this", "tokens": [50764, 1392, 718, 311, 360, 309, 411, 341, 50864], "temperature": 0.0, "avg_logprob": -0.14288972966811236, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.036744125187397}, {"id": 709, "seek": 149496, "start": 1504.96, "end": 1506.96, "text": " you'll come up front and ask it here", "tokens": [50864, 291, 603, 808, 493, 1868, 293, 1029, 309, 510, 50964], "temperature": 0.0, "avg_logprob": -0.14288972966811236, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.036744125187397}, {"id": 710, "seek": 149496, "start": 1506.96, "end": 1508.96, "text": " yeah that's perfect that'd be great", "tokens": [50964, 1338, 300, 311, 2176, 300, 1116, 312, 869, 51064], "temperature": 0.0, "avg_logprob": -0.14288972966811236, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.036744125187397}, {"id": 711, "seek": 149496, "start": 1508.96, "end": 1510.96, "text": " I can hear you great", "tokens": [51064, 286, 393, 1568, 291, 869, 51164], "temperature": 0.0, "avg_logprob": -0.14288972966811236, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.036744125187397}, {"id": 712, "seek": 149496, "start": 1514.96, "end": 1516.96, "text": " hi there", "tokens": [51364, 4879, 456, 51464], "temperature": 0.0, "avg_logprob": -0.14288972966811236, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.036744125187397}, {"id": 713, "seek": 149496, "start": 1516.96, "end": 1518.96, "text": " hi", "tokens": [51464, 4879, 51564], "temperature": 0.0, "avg_logprob": -0.14288972966811236, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.036744125187397}, {"id": 714, "seek": 149496, "start": 1518.96, "end": 1520.96, "text": " so I was wondering if your architecture", "tokens": [51564, 370, 286, 390, 6359, 498, 428, 9482, 51664], "temperature": 0.0, "avg_logprob": -0.14288972966811236, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.036744125187397}, {"id": 715, "seek": 149496, "start": 1520.96, "end": 1522.96, "text": " can support sidecar containers", "tokens": [51664, 393, 1406, 1252, 6166, 17089, 51764], "temperature": 0.0, "avg_logprob": -0.14288972966811236, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.036744125187397}, {"id": 716, "seek": 152296, "start": 1522.96, "end": 1524.96, "text": " because as I was saying", "tokens": [50364, 570, 382, 286, 390, 1566, 50464], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 717, "seek": 152296, "start": 1524.96, "end": 1526.96, "text": " when I was trying to do something similar", "tokens": [50464, 562, 286, 390, 1382, 281, 360, 746, 2531, 50564], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 718, "seek": 152296, "start": 1526.96, "end": 1528.96, "text": " when I", "tokens": [50564, 562, 286, 50664], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 719, "seek": 152296, "start": 1528.96, "end": 1530.96, "text": " tried to create the sidecars I had to", "tokens": [50664, 3031, 281, 1884, 264, 1252, 66, 685, 286, 632, 281, 50764], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 720, "seek": 152296, "start": 1530.96, "end": 1532.96, "text": " create a second network stack", "tokens": [50764, 1884, 257, 1150, 3209, 8630, 50864], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 721, "seek": 152296, "start": 1532.96, "end": 1534.96, "text": " within singularity so the", "tokens": [50864, 1951, 20010, 507, 370, 264, 50964], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 722, "seek": 152296, "start": 1534.96, "end": 1536.96, "text": " network overhead was", "tokens": [50964, 3209, 19922, 390, 51064], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 723, "seek": 152296, "start": 1536.96, "end": 1538.96, "text": " amazingly high", "tokens": [51064, 31762, 1090, 51164], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 724, "seek": 152296, "start": 1538.96, "end": 1540.96, "text": " so absolutely a flux", "tokens": [51164, 370, 3122, 257, 19298, 51264], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 725, "seek": 152296, "start": 1540.96, "end": 1542.96, "text": " operator actually uses a sidecar container", "tokens": [51264, 12973, 767, 4960, 257, 1252, 6166, 10129, 51364], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 726, "seek": 152296, "start": 1542.96, "end": 1544.96, "text": " on a net container which is similar", "tokens": [51364, 322, 257, 2533, 10129, 597, 307, 2531, 51464], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 727, "seek": 152296, "start": 1544.96, "end": 1546.96, "text": " in concept to add flux on the fly", "tokens": [51464, 294, 3410, 281, 909, 19298, 322, 264, 3603, 51564], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 728, "seek": 152296, "start": 1546.96, "end": 1548.96, "text": " as a view what's going on", "tokens": [51564, 382, 257, 1910, 437, 311, 516, 322, 51664], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 729, "seek": 152296, "start": 1548.96, "end": 1550.96, "text": " in Kubernetes is sort of a different", "tokens": [51664, 294, 23145, 307, 1333, 295, 257, 819, 51764], "temperature": 0.0, "avg_logprob": -0.0933882713317871, "compression_ratio": 1.7347826086956522, "no_speech_prob": 0.026757244020700455}, {"id": 730, "seek": 155096, "start": 1550.96, "end": 1552.96, "text": " thing than the networking", "tokens": [50364, 551, 813, 264, 17985, 50464], "temperature": 0.0, "avg_logprob": -0.17019143597833042, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.007942047901451588}, {"id": 731, "seek": 155096, "start": 1552.96, "end": 1554.96, "text": " issue so the short answer is yes", "tokens": [50464, 2734, 370, 264, 2099, 1867, 307, 2086, 50564], "temperature": 0.0, "avg_logprob": -0.17019143597833042, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.007942047901451588}, {"id": 732, "seek": 155096, "start": 1558.96, "end": 1560.96, "text": " to kind of add to that though I'm not sure", "tokens": [50764, 281, 733, 295, 909, 281, 300, 1673, 286, 478, 406, 988, 50864], "temperature": 0.0, "avg_logprob": -0.17019143597833042, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.007942047901451588}, {"id": 733, "seek": 155096, "start": 1560.96, "end": 1562.96, "text": " that singularity and Kubernetes", "tokens": [50864, 300, 20010, 507, 293, 23145, 50964], "temperature": 0.0, "avg_logprob": -0.17019143597833042, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.007942047901451588}, {"id": 734, "seek": 155096, "start": 1562.96, "end": 1564.96, "text": " singularity as the container", "tokens": [50964, 20010, 507, 382, 264, 10129, 51064], "temperature": 0.0, "avg_logprob": -0.17019143597833042, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.007942047901451588}, {"id": 735, "seek": 155096, "start": 1564.96, "end": 1566.96, "text": " runtime for Kubernetes would work", "tokens": [51064, 34474, 337, 23145, 576, 589, 51164], "temperature": 0.0, "avg_logprob": -0.17019143597833042, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.007942047901451588}, {"id": 736, "seek": 155096, "start": 1566.96, "end": 1568.96, "text": " I have never tried that but it", "tokens": [51164, 286, 362, 1128, 3031, 300, 457, 309, 51264], "temperature": 0.0, "avg_logprob": -0.17019143597833042, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.007942047901451588}, {"id": 737, "seek": 155096, "start": 1568.96, "end": 1570.96, "text": " doesn't sound like it would work", "tokens": [51264, 1177, 380, 1626, 411, 309, 576, 589, 51364], "temperature": 0.0, "avg_logprob": -0.17019143597833042, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.007942047901451588}, {"id": 738, "seek": 155096, "start": 1570.96, "end": 1572.96, "text": " yeah it needs to be done", "tokens": [51364, 1338, 309, 2203, 281, 312, 1096, 51464], "temperature": 0.0, "avg_logprob": -0.17019143597833042, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.007942047901451588}, {"id": 739, "seek": 155096, "start": 1572.96, "end": 1574.96, "text": " yeah exactly", "tokens": [51464, 1338, 2293, 51564], "temperature": 0.0, "avg_logprob": -0.17019143597833042, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.007942047901451588}, {"id": 740, "seek": 155096, "start": 1576.96, "end": 1578.96, "text": " hi Vanessa thank you", "tokens": [51664, 4879, 27928, 1309, 291, 51764], "temperature": 0.0, "avg_logprob": -0.17019143597833042, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.007942047901451588}, {"id": 741, "seek": 157896, "start": 1578.96, "end": 1580.96, "text": " hi it was the most fun presentation", "tokens": [50364, 4879, 309, 390, 264, 881, 1019, 5860, 50464], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 742, "seek": 157896, "start": 1580.96, "end": 1582.96, "text": " on the post then so far", "tokens": [50464, 322, 264, 2183, 550, 370, 1400, 50564], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 743, "seek": 157896, "start": 1582.96, "end": 1584.96, "text": " thank you", "tokens": [50564, 1309, 291, 50664], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 744, "seek": 157896, "start": 1584.96, "end": 1586.96, "text": " so when you were saying that", "tokens": [50664, 370, 562, 291, 645, 1566, 300, 50764], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 745, "seek": 157896, "start": 1586.96, "end": 1588.96, "text": " the main difference between", "tokens": [50764, 264, 2135, 2649, 1296, 50864], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 746, "seek": 157896, "start": 1588.96, "end": 1590.96, "text": " performance between EBM and bare metal", "tokens": [50864, 3389, 1296, 462, 18345, 293, 6949, 5760, 50964], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 747, "seek": 157896, "start": 1590.96, "end": 1592.96, "text": " workloads was related to network", "tokens": [50964, 32452, 390, 4077, 281, 3209, 51064], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 748, "seek": 157896, "start": 1592.96, "end": 1594.96, "text": " was that the case", "tokens": [51064, 390, 300, 264, 1389, 51164], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 749, "seek": 157896, "start": 1594.96, "end": 1596.96, "text": " also for distributed", "tokens": [51164, 611, 337, 12631, 51264], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 750, "seek": 157896, "start": 1596.96, "end": 1598.96, "text": " training and if that's the case", "tokens": [51264, 3097, 293, 498, 300, 311, 264, 1389, 51364], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 751, "seek": 157896, "start": 1598.96, "end": 1600.96, "text": " were you using infini band or not", "tokens": [51364, 645, 291, 1228, 1536, 3812, 4116, 420, 406, 51464], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 752, "seek": 157896, "start": 1600.96, "end": 1602.96, "text": " so this we did", "tokens": [51464, 370, 341, 321, 630, 51564], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 753, "seek": 157896, "start": 1602.96, "end": 1604.96, "text": " not have infini band and you make a really", "tokens": [51564, 406, 362, 1536, 3812, 4116, 293, 291, 652, 257, 534, 51664], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 754, "seek": 157896, "start": 1604.96, "end": 1606.96, "text": " good point that this kind of setup would", "tokens": [51664, 665, 935, 300, 341, 733, 295, 8657, 576, 51764], "temperature": 0.0, "avg_logprob": -0.22273493242693376, "compression_ratio": 1.7478260869565216, "no_speech_prob": 0.014521499164402485}, {"id": 755, "seek": 160696, "start": 1606.96, "end": 1608.96, "text": " need to be tested with an actually great", "tokens": [50364, 643, 281, 312, 8246, 365, 364, 767, 869, 50464], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 756, "seek": 160696, "start": 1608.96, "end": 1610.96, "text": " network and that is still a very big", "tokens": [50464, 3209, 293, 300, 307, 920, 257, 588, 955, 50564], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 757, "seek": 160696, "start": 1610.96, "end": 1612.96, "text": " challenge even for cloud", "tokens": [50564, 3430, 754, 337, 4588, 50664], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 758, "seek": 160696, "start": 1612.96, "end": 1614.96, "text": " so for example if you use AWS you can bring", "tokens": [50664, 370, 337, 1365, 498, 291, 764, 17650, 291, 393, 1565, 50764], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 759, "seek": 160696, "start": 1614.96, "end": 1616.96, "text": " the elastic fiber adapter which will give you", "tokens": [50764, 264, 17115, 12874, 22860, 597, 486, 976, 291, 50864], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 760, "seek": 160696, "start": 1616.96, "end": 1618.96, "text": " great networking performance but if you go", "tokens": [50864, 869, 17985, 3389, 457, 498, 291, 352, 50964], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 761, "seek": 160696, "start": 1618.96, "end": 1620.96, "text": " to other clouds and I don't have to name this specifically", "tokens": [50964, 281, 661, 12193, 293, 286, 500, 380, 362, 281, 1315, 341, 4682, 51064], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 762, "seek": 160696, "start": 1620.96, "end": 1622.96, "text": " you tend to only get really good networks", "tokens": [51064, 291, 3928, 281, 787, 483, 534, 665, 9590, 51164], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 763, "seek": 160696, "start": 1622.96, "end": 1624.96, "text": " when it comes to using like TPUs", "tokens": [51164, 562, 309, 1487, 281, 1228, 411, 314, 8115, 82, 51264], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 764, "seek": 160696, "start": 1624.96, "end": 1626.96, "text": " or GPUs the exception", "tokens": [51264, 420, 18407, 82, 264, 11183, 51364], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 765, "seek": 160696, "start": 1626.96, "end": 1628.96, "text": " though is Azure which has a lot of really great", "tokens": [51364, 1673, 307, 11969, 597, 575, 257, 688, 295, 534, 869, 51464], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 766, "seek": 160696, "start": 1628.96, "end": 1630.96, "text": " HPC stuff kind of", "tokens": [51464, 12557, 34, 1507, 733, 295, 51564], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 767, "seek": 160696, "start": 1630.96, "end": 1632.96, "text": " built in", "tokens": [51564, 3094, 294, 51664], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 768, "seek": 160696, "start": 1632.96, "end": 1634.96, "text": " so absolutely", "tokens": [51664, 370, 3122, 51764], "temperature": 0.0, "avg_logprob": -0.08640759438276291, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03396027535200119}, {"id": 769, "seek": 163496, "start": 1634.96, "end": 1636.96, "text": " you can get that setup with infini band", "tokens": [50364, 291, 393, 483, 300, 8657, 365, 1536, 3812, 4116, 50464], "temperature": 0.0, "avg_logprob": -0.1956547975540161, "compression_ratio": 1.5730994152046784, "no_speech_prob": 0.016396207734942436}, {"id": 770, "seek": 163496, "start": 1640.96, "end": 1642.96, "text": " Hi thank you for your talk", "tokens": [50664, 2421, 1309, 291, 337, 428, 751, 50764], "temperature": 0.0, "avg_logprob": -0.1956547975540161, "compression_ratio": 1.5730994152046784, "no_speech_prob": 0.016396207734942436}, {"id": 771, "seek": 163496, "start": 1642.96, "end": 1644.96, "text": " I had a smile on my face the whole time", "tokens": [50764, 286, 632, 257, 7563, 322, 452, 1851, 264, 1379, 565, 50864], "temperature": 0.0, "avg_logprob": -0.1956547975540161, "compression_ratio": 1.5730994152046784, "no_speech_prob": 0.016396207734942436}, {"id": 772, "seek": 163496, "start": 1644.96, "end": 1646.96, "text": " thank you for having such high energy", "tokens": [50864, 1309, 291, 337, 1419, 1270, 1090, 2281, 50964], "temperature": 0.0, "avg_logprob": -0.1956547975540161, "compression_ratio": 1.5730994152046784, "no_speech_prob": 0.016396207734942436}, {"id": 773, "seek": 163496, "start": 1646.96, "end": 1648.96, "text": " at the end of the day", "tokens": [50964, 412, 264, 917, 295, 264, 786, 51064], "temperature": 0.0, "avg_logprob": -0.1956547975540161, "compression_ratio": 1.5730994152046784, "no_speech_prob": 0.016396207734942436}, {"id": 774, "seek": 163496, "start": 1652.96, "end": 1654.96, "text": " what was I going to say", "tokens": [51264, 437, 390, 286, 516, 281, 584, 51364], "temperature": 0.0, "avg_logprob": -0.1956547975540161, "compression_ratio": 1.5730994152046784, "no_speech_prob": 0.016396207734942436}, {"id": 775, "seek": 163496, "start": 1656.96, "end": 1658.96, "text": " oh yeah so probably in my workloads", "tokens": [51464, 1954, 1338, 370, 1391, 294, 452, 32452, 51564], "temperature": 0.0, "avg_logprob": -0.1956547975540161, "compression_ratio": 1.5730994152046784, "no_speech_prob": 0.016396207734942436}, {"id": 776, "seek": 163496, "start": 1658.96, "end": 1660.96, "text": " I can reduce the", "tokens": [51564, 286, 393, 5407, 264, 51664], "temperature": 0.0, "avg_logprob": -0.1956547975540161, "compression_ratio": 1.5730994152046784, "no_speech_prob": 0.016396207734942436}, {"id": 777, "seek": 163496, "start": 1660.96, "end": 1662.96, "text": " network traffic by a very", "tokens": [51664, 3209, 6419, 538, 257, 588, 51764], "temperature": 0.0, "avg_logprob": -0.1956547975540161, "compression_ratio": 1.5730994152046784, "no_speech_prob": 0.016396207734942436}, {"id": 778, "seek": 166296, "start": 1662.96, "end": 1664.96, "text": " large margin if I can", "tokens": [50364, 2416, 10270, 498, 286, 393, 50464], "temperature": 0.0, "avg_logprob": -0.12465272965978404, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012131479568779469}, {"id": 779, "seek": 166296, "start": 1664.96, "end": 1666.96, "text": " constrain certain jobs to", "tokens": [50464, 1817, 7146, 1629, 4782, 281, 50564], "temperature": 0.0, "avg_logprob": -0.12465272965978404, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012131479568779469}, {"id": 780, "seek": 166296, "start": 1666.96, "end": 1668.96, "text": " specific nodes because", "tokens": [50564, 2685, 13891, 570, 50664], "temperature": 0.0, "avg_logprob": -0.12465272965978404, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012131479568779469}, {"id": 781, "seek": 166296, "start": 1668.96, "end": 1670.96, "text": " then large files don't have to be moved", "tokens": [50664, 550, 2416, 7098, 500, 380, 362, 281, 312, 4259, 50764], "temperature": 0.0, "avg_logprob": -0.12465272965978404, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012131479568779469}, {"id": 782, "seek": 166296, "start": 1670.96, "end": 1672.96, "text": " for certain jobs to", "tokens": [50764, 337, 1629, 4782, 281, 50864], "temperature": 0.0, "avg_logprob": -0.12465272965978404, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012131479568779469}, {"id": 783, "seek": 166296, "start": 1672.96, "end": 1674.96, "text": " across the network is that something that you", "tokens": [50864, 2108, 264, 3209, 307, 300, 746, 300, 291, 50964], "temperature": 0.0, "avg_logprob": -0.12465272965978404, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012131479568779469}, {"id": 784, "seek": 166296, "start": 1674.96, "end": 1676.96, "text": " could keep in mind", "tokens": [50964, 727, 1066, 294, 1575, 51064], "temperature": 0.0, "avg_logprob": -0.12465272965978404, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012131479568779469}, {"id": 785, "seek": 166296, "start": 1678.96, "end": 1680.96, "text": " so if you remember the very quick", "tokens": [51164, 370, 498, 291, 1604, 264, 588, 1702, 51264], "temperature": 0.0, "avg_logprob": -0.12465272965978404, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012131479568779469}, {"id": 786, "seek": 166296, "start": 1680.96, "end": 1682.96, "text": " machine learning experiment that we showed when we're running something on one node", "tokens": [51264, 3479, 2539, 5120, 300, 321, 4712, 562, 321, 434, 2614, 746, 322, 472, 9984, 51364], "temperature": 0.0, "avg_logprob": -0.12465272965978404, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012131479568779469}, {"id": 787, "seek": 166296, "start": 1682.96, "end": 1684.96, "text": " and you're not using the network", "tokens": [51364, 293, 291, 434, 406, 1228, 264, 3209, 51464], "temperature": 0.0, "avg_logprob": -0.12465272965978404, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012131479568779469}, {"id": 788, "seek": 166296, "start": 1684.96, "end": 1686.96, "text": " there's no issue so if you're just running", "tokens": [51464, 456, 311, 572, 2734, 370, 498, 291, 434, 445, 2614, 51564], "temperature": 0.0, "avg_logprob": -0.12465272965978404, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012131479568779469}, {"id": 789, "seek": 166296, "start": 1686.96, "end": 1688.96, "text": " something on one node in user netties", "tokens": [51564, 746, 322, 472, 9984, 294, 4195, 2533, 6097, 51664], "temperature": 0.0, "avg_logprob": -0.12465272965978404, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012131479568779469}, {"id": 790, "seek": 166296, "start": 1688.96, "end": 1690.96, "text": " you won't have an issue in a degree to", "tokens": [51664, 291, 1582, 380, 362, 364, 2734, 294, 257, 4314, 281, 51764], "temperature": 0.0, "avg_logprob": -0.12465272965978404, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.012131479568779469}, {"id": 791, "seek": 169096, "start": 1690.96, "end": 1692.96, "text": " which you can reduce anything that uses", "tokens": [50364, 597, 291, 393, 5407, 1340, 300, 4960, 50464], "temperature": 0.0, "avg_logprob": -0.13450465202331544, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0032105010468512774}, {"id": 792, "seek": 169096, "start": 1692.96, "end": 1694.96, "text": " network so moving data", "tokens": [50464, 3209, 370, 2684, 1412, 50564], "temperature": 0.0, "avg_logprob": -0.13450465202331544, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0032105010468512774}, {"id": 793, "seek": 169096, "start": 1694.96, "end": 1696.96, "text": " MPI etc etc you will get", "tokens": [50564, 14146, 40, 5183, 5183, 291, 486, 483, 50664], "temperature": 0.0, "avg_logprob": -0.13450465202331544, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0032105010468512774}, {"id": 794, "seek": 169096, "start": 1696.96, "end": 1698.96, "text": " similar performance at least from this small", "tokens": [50664, 2531, 3389, 412, 1935, 490, 341, 1359, 50764], "temperature": 0.0, "avg_logprob": -0.13450465202331544, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0032105010468512774}, {"id": 795, "seek": 169096, "start": 1698.96, "end": 1700.96, "text": " prototype experiment that we've seen", "tokens": [50764, 19475, 5120, 300, 321, 600, 1612, 50864], "temperature": 0.0, "avg_logprob": -0.13450465202331544, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0032105010468512774}, {"id": 796, "seek": 169096, "start": 1700.96, "end": 1702.96, "text": " as you would on bare metal", "tokens": [50864, 382, 291, 576, 322, 6949, 5760, 50964], "temperature": 0.0, "avg_logprob": -0.13450465202331544, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0032105010468512774}, {"id": 797, "seek": 169096, "start": 1702.96, "end": 1704.96, "text": " I have to do this because it wasn't really bare metal", "tokens": [50964, 286, 362, 281, 360, 341, 570, 309, 2067, 380, 534, 6949, 5760, 51064], "temperature": 0.0, "avg_logprob": -0.13450465202331544, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0032105010468512774}, {"id": 798, "seek": 169096, "start": 1708.96, "end": 1710.96, "text": " thanks", "tokens": [51264, 3231, 51364], "temperature": 0.0, "avg_logprob": -0.13450465202331544, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0032105010468512774}, {"id": 799, "seek": 169096, "start": 1712.96, "end": 1714.96, "text": " one more question", "tokens": [51464, 472, 544, 1168, 51564], "temperature": 0.0, "avg_logprob": -0.13450465202331544, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0032105010468512774}, {"id": 800, "seek": 169096, "start": 1714.96, "end": 1716.96, "text": " hey Vanessa that's Danny", "tokens": [51564, 4177, 27928, 300, 311, 16682, 51664], "temperature": 0.0, "avg_logprob": -0.13450465202331544, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.0032105010468512774}, {"id": 801, "seek": 171696, "start": 1716.96, "end": 1718.96, "text": " I'm gonna die my hair", "tokens": [50364, 286, 478, 799, 978, 452, 2578, 50464], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 802, "seek": 171696, "start": 1718.96, "end": 1720.96, "text": " soon so you won't recognize me again", "tokens": [50464, 2321, 370, 291, 1582, 380, 5521, 385, 797, 50564], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 803, "seek": 171696, "start": 1720.96, "end": 1722.96, "text": " I really liked your framing", "tokens": [50564, 286, 534, 4501, 428, 28971, 50664], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 804, "seek": 171696, "start": 1722.96, "end": 1724.96, "text": " actually I thought I was going to", "tokens": [50664, 767, 286, 1194, 286, 390, 516, 281, 50764], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 805, "seek": 171696, "start": 1724.96, "end": 1726.96, "text": " sort of being adversarial and then I actually", "tokens": [50764, 1333, 295, 885, 17641, 44745, 293, 550, 286, 767, 50864], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 806, "seek": 171696, "start": 1726.96, "end": 1728.96, "text": " realized what you were saying and I really appreciated it", "tokens": [50864, 5334, 437, 291, 645, 1566, 293, 286, 534, 17169, 309, 50964], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 807, "seek": 171696, "start": 1728.96, "end": 1730.96, "text": " however though regarding the", "tokens": [50964, 4461, 1673, 8595, 264, 51064], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 808, "seek": 171696, "start": 1730.96, "end": 1732.96, "text": " adversarial framing I have", "tokens": [51064, 17641, 44745, 28971, 286, 362, 51164], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 809, "seek": 171696, "start": 1732.96, "end": 1734.96, "text": " some experience with for example", "tokens": [51164, 512, 1752, 365, 337, 1365, 51264], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 810, "seek": 171696, "start": 1734.96, "end": 1736.96, "text": " cloud tools and cloud environments", "tokens": [51264, 4588, 3873, 293, 4588, 12388, 51364], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 811, "seek": 171696, "start": 1736.96, "end": 1738.96, "text": " being used as", "tokens": [51364, 885, 1143, 382, 51464], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 812, "seek": 171696, "start": 1738.96, "end": 1740.96, "text": " platforms for vendor lock-in", "tokens": [51464, 9473, 337, 24321, 4017, 12, 259, 51564], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 813, "seek": 171696, "start": 1740.96, "end": 1742.96, "text": " I think that you described especially with your converged computing", "tokens": [51564, 286, 519, 300, 291, 7619, 2318, 365, 428, 9652, 3004, 15866, 51664], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 814, "seek": 171696, "start": 1742.96, "end": 1744.96, "text": " kind of the way that you can push back against", "tokens": [51664, 733, 295, 264, 636, 300, 291, 393, 2944, 646, 1970, 51764], "temperature": 0.0, "avg_logprob": -0.22159387219336726, "compression_ratio": 1.7508650519031141, "no_speech_prob": 0.009184638038277626}, {"id": 815, "seek": 174496, "start": 1744.96, "end": 1746.96, "text": " scientific labs aren't", "tokens": [50364, 8134, 20339, 3212, 380, 50464], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 816, "seek": 174496, "start": 1746.96, "end": 1748.96, "text": " kind of in-depth to corporations", "tokens": [50464, 733, 295, 294, 12, 25478, 281, 17676, 50564], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 817, "seek": 174496, "start": 1748.96, "end": 1750.96, "text": " I actually think that you kind of", "tokens": [50564, 286, 767, 519, 300, 291, 733, 295, 50664], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 818, "seek": 174496, "start": 1750.96, "end": 1752.96, "text": " made a really useful example of", "tokens": [50664, 1027, 257, 534, 4420, 1365, 295, 50764], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 819, "seek": 174496, "start": 1752.96, "end": 1754.96, "text": " one way to do that in your talk so", "tokens": [50764, 472, 636, 281, 360, 300, 294, 428, 751, 370, 50864], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 820, "seek": 174496, "start": 1754.96, "end": 1756.96, "text": " again I actually was very very", "tokens": [50864, 797, 286, 767, 390, 588, 588, 50964], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 821, "seek": 174496, "start": 1756.96, "end": 1758.96, "text": " impressed by the way you kind of explained that", "tokens": [50964, 11679, 538, 264, 636, 291, 733, 295, 8825, 300, 51064], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 822, "seek": 174496, "start": 1758.96, "end": 1760.96, "text": " I would like to know in the more general", "tokens": [51064, 286, 576, 411, 281, 458, 294, 264, 544, 2674, 51164], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 823, "seek": 174496, "start": 1760.96, "end": 1762.96, "text": " sense how can labs and", "tokens": [51164, 2020, 577, 393, 20339, 293, 51264], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 824, "seek": 174496, "start": 1762.96, "end": 1764.96, "text": " potentially RSEs make use of", "tokens": [51264, 7263, 497, 5879, 82, 652, 764, 295, 51364], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 825, "seek": 174496, "start": 1764.96, "end": 1766.96, "text": " cloud tools without", "tokens": [51364, 4588, 3873, 1553, 51464], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 826, "seek": 174496, "start": 1766.96, "end": 1768.96, "text": " getting locked in or", "tokens": [51464, 1242, 9376, 294, 420, 51564], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 827, "seek": 174496, "start": 1768.96, "end": 1770.96, "text": " becoming beholden again to a corporate environment", "tokens": [51564, 5617, 27234, 268, 797, 281, 257, 10896, 2823, 51664], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 828, "seek": 174496, "start": 1770.96, "end": 1772.96, "text": " and again by the way I think that you effectively did that in this talk", "tokens": [51664, 293, 797, 538, 264, 636, 286, 519, 300, 291, 8659, 630, 300, 294, 341, 751, 51764], "temperature": 0.0, "avg_logprob": -0.15023196020791696, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.015011613257229328}, {"id": 829, "seek": 177296, "start": 1772.96, "end": 1774.96, "text": " so I'm more looking for a general kind of", "tokens": [50364, 370, 286, 478, 544, 1237, 337, 257, 2674, 733, 295, 50464], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 830, "seek": 177296, "start": 1774.96, "end": 1776.96, "text": " thought about that", "tokens": [50464, 1194, 466, 300, 50564], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 831, "seek": 177296, "start": 1776.96, "end": 1778.96, "text": " You're totally correct that vendor lock", "tokens": [50564, 509, 434, 3879, 3006, 300, 24321, 4017, 50664], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 832, "seek": 177296, "start": 1778.96, "end": 1780.96, "text": " is an issue and when you tend to see", "tokens": [50664, 307, 364, 2734, 293, 562, 291, 3928, 281, 536, 50764], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 833, "seek": 177296, "start": 1780.96, "end": 1782.96, "text": " many sort of niche APIs in different", "tokens": [50764, 867, 1333, 295, 19956, 21445, 294, 819, 50864], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 834, "seek": 177296, "start": 1782.96, "end": 1784.96, "text": " clouds and then you built your entire thing around them", "tokens": [50864, 12193, 293, 550, 291, 3094, 428, 2302, 551, 926, 552, 50964], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 835, "seek": 177296, "start": 1784.96, "end": 1786.96, "text": " you do face that as an issue", "tokens": [50964, 291, 360, 1851, 300, 382, 364, 2734, 51064], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 836, "seek": 177296, "start": 1786.96, "end": 1788.96, "text": " but the great thing about Kubernetes is", "tokens": [51064, 457, 264, 869, 551, 466, 23145, 307, 51164], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 837, "seek": 177296, "start": 1788.96, "end": 1790.96, "text": " that it is this open source project", "tokens": [51164, 300, 309, 307, 341, 1269, 4009, 1716, 51264], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 838, "seek": 177296, "start": 1790.96, "end": 1792.96, "text": " that is available across clouds", "tokens": [51264, 300, 307, 2435, 2108, 12193, 51364], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 839, "seek": 177296, "start": 1792.96, "end": 1794.96, "text": " there are subtle differences but if you make a workload", "tokens": [51364, 456, 366, 13743, 7300, 457, 498, 291, 652, 257, 20139, 51464], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 840, "seek": 177296, "start": 1794.96, "end": 1796.96, "text": " that can run on Kubernetes", "tokens": [51464, 300, 393, 1190, 322, 23145, 51564], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 841, "seek": 177296, "start": 1796.96, "end": 1798.96, "text": " you're going to have an easier time to move it between clouds", "tokens": [51564, 291, 434, 516, 281, 362, 364, 3571, 565, 281, 1286, 309, 1296, 12193, 51664], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 842, "seek": 177296, "start": 1798.96, "end": 1800.96, "text": " and that's you know speaking from my lab", "tokens": [51664, 293, 300, 311, 291, 458, 4124, 490, 452, 2715, 51764], "temperature": 0.0, "avg_logprob": -0.07950364256934296, "compression_ratio": 1.7896440129449838, "no_speech_prob": 0.004091565031558275}, {"id": 843, "seek": 180096, "start": 1800.96, "end": 1802.96, "text": " we work on flux framework and one of our goals with", "tokens": [50364, 321, 589, 322, 19298, 8388, 293, 472, 295, 527, 5493, 365, 50464], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 844, "seek": 180096, "start": 1802.96, "end": 1804.96, "text": " flux is to make things", "tokens": [50464, 19298, 307, 281, 652, 721, 50564], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 845, "seek": 180096, "start": 1804.96, "end": 1806.96, "text": " portable not just between clouds", "tokens": [50564, 21800, 406, 445, 1296, 12193, 50664], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 846, "seek": 180096, "start": 1806.96, "end": 1808.96, "text": " but between cloud and HPC", "tokens": [50664, 457, 1296, 4588, 293, 12557, 34, 50764], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 847, "seek": 180096, "start": 1808.96, "end": 1810.96, "text": " that's also something like", "tokens": [50764, 300, 311, 611, 746, 411, 50864], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 848, "seek": 180096, "start": 1810.96, "end": 1812.96, "text": " user netties running actually", "tokens": [50864, 4195, 2533, 6097, 2614, 767, 50964], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 849, "seek": 180096, "start": 1812.96, "end": 1814.96, "text": " Kubernetes on bare metal alongside HPC", "tokens": [50964, 23145, 322, 6949, 5760, 12385, 12557, 34, 51064], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 850, "seek": 180096, "start": 1814.96, "end": 1816.96, "text": " is so important because all of a sudden", "tokens": [51064, 307, 370, 1021, 570, 439, 295, 257, 3990, 51164], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 851, "seek": 180096, "start": 1816.96, "end": 1818.96, "text": " you have the same workload", "tokens": [51164, 291, 362, 264, 912, 20139, 51264], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 852, "seek": 180096, "start": 1818.96, "end": 1820.96, "text": " and it runs in all the places", "tokens": [51264, 293, 309, 6676, 294, 439, 264, 3190, 51364], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 853, "seek": 180096, "start": 1820.96, "end": 1822.96, "text": " that is sort of like the vision we don't", "tokens": [51364, 300, 307, 1333, 295, 411, 264, 5201, 321, 500, 380, 51464], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 854, "seek": 180096, "start": 1822.96, "end": 1824.96, "text": " we want to make sure that the scientific", "tokens": [51464, 321, 528, 281, 652, 988, 300, 264, 8134, 51564], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 855, "seek": 180096, "start": 1824.96, "end": 1826.96, "text": " workloads that we're running today can run", "tokens": [51564, 32452, 300, 321, 434, 2614, 965, 393, 1190, 51664], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 856, "seek": 180096, "start": 1826.96, "end": 1828.96, "text": " in all places not just to", "tokens": [51664, 294, 439, 3190, 406, 445, 281, 51764], "temperature": 0.0, "avg_logprob": -0.1073870129055447, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.04587936773896217}, {"id": 857, "seek": 182896, "start": 1828.96, "end": 1830.96, "text": " one niche specific cloud", "tokens": [50364, 472, 19956, 2685, 4588, 50464], "temperature": 0.0, "avg_logprob": -0.13303173238580876, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.01174682192504406}, {"id": 858, "seek": 182896, "start": 1830.96, "end": 1832.96, "text": " not just one niche specific center", "tokens": [50464, 406, 445, 472, 19956, 2685, 3056, 50564], "temperature": 0.0, "avg_logprob": -0.13303173238580876, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.01174682192504406}, {"id": 859, "seek": 182896, "start": 1832.96, "end": 1834.96, "text": " just convergence", "tokens": [50564, 445, 32181, 50664], "temperature": 0.0, "avg_logprob": -0.13303173238580876, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.01174682192504406}, {"id": 860, "seek": 182896, "start": 1834.96, "end": 1836.96, "text": " TLDR", "tokens": [50664, 40277, 9301, 50764], "temperature": 0.0, "avg_logprob": -0.13303173238580876, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.01174682192504406}, {"id": 861, "seek": 182896, "start": 1836.96, "end": 1838.96, "text": " that is very exciting and I really appreciate that response", "tokens": [50764, 300, 307, 588, 4670, 293, 286, 534, 4449, 300, 4134, 50864], "temperature": 0.0, "avg_logprob": -0.13303173238580876, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.01174682192504406}, {"id": 862, "seek": 182896, "start": 1838.96, "end": 1840.96, "text": " thank you so much", "tokens": [50864, 1309, 291, 370, 709, 50964], "temperature": 0.0, "avg_logprob": -0.13303173238580876, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.01174682192504406}, {"id": 863, "seek": 182896, "start": 1840.96, "end": 1842.96, "text": " okay that's all we have time for", "tokens": [50964, 1392, 300, 311, 439, 321, 362, 565, 337, 51064], "temperature": 0.0, "avg_logprob": -0.13303173238580876, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.01174682192504406}, {"id": 864, "seek": 182896, "start": 1842.96, "end": 1844.96, "text": " this workout great Vanessa I hope you agree", "tokens": [51064, 341, 12169, 869, 27928, 286, 1454, 291, 3986, 51164], "temperature": 0.0, "avg_logprob": -0.13303173238580876, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.01174682192504406}, {"id": 865, "seek": 182896, "start": 1846.96, "end": 1848.96, "text": " yeah it was really fun if anyone has further questions", "tokens": [51264, 1338, 309, 390, 534, 1019, 498, 2878, 575, 3052, 1651, 51364], "temperature": 0.0, "avg_logprob": -0.13303173238580876, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.01174682192504406}, {"id": 866, "seek": 182896, "start": 1848.96, "end": 1850.96, "text": " and stuff please reach out to me", "tokens": [51364, 293, 1507, 1767, 2524, 484, 281, 385, 51464], "temperature": 0.0, "avg_logprob": -0.13303173238580876, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.01174682192504406}, {"id": 867, "seek": 182896, "start": 1850.96, "end": 1852.96, "text": " I love chatting it was a pleasure chatting", "tokens": [51464, 286, 959, 24654, 309, 390, 257, 6834, 24654, 51564], "temperature": 0.0, "avg_logprob": -0.13303173238580876, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.01174682192504406}, {"id": 868, "seek": 182896, "start": 1852.96, "end": 1854.96, "text": " with you and I hope you have a great rest of your fun", "tokens": [51564, 365, 291, 293, 286, 1454, 291, 362, 257, 869, 1472, 295, 428, 1019, 51664], "temperature": 0.0, "avg_logprob": -0.13303173238580876, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.01174682192504406}, {"id": 869, "seek": 182896, "start": 1854.96, "end": 1856.96, "text": " then", "tokens": [51664, 550, 51764], "temperature": 0.0, "avg_logprob": -0.13303173238580876, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.01174682192504406}, {"id": 870, "seek": 185896, "start": 1858.96, "end": 1860.96, "text": " thank you", "tokens": [50364, 1309, 291, 50464], "temperature": 0.0, "avg_logprob": -0.10174790653613729, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.029705867171287537}, {"id": 871, "seek": 185896, "start": 1862.96, "end": 1864.96, "text": " and the best way", "tokens": [50564, 293, 264, 1151, 636, 50664], "temperature": 0.0, "avg_logprob": -0.10174790653613729, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.029705867171287537}, {"id": 872, "seek": 185896, "start": 1864.96, "end": 1866.96, "text": " to reach out to Vanessa is via HPC", "tokens": [50664, 281, 2524, 484, 281, 27928, 307, 5766, 12557, 34, 50764], "temperature": 0.0, "avg_logprob": -0.10174790653613729, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.029705867171287537}, {"id": 873, "seek": 185896, "start": 1866.96, "end": 1868.96, "text": " social so don't forget to grab a sticker", "tokens": [50764, 2093, 370, 500, 380, 2870, 281, 4444, 257, 20400, 50864], "temperature": 0.0, "avg_logprob": -0.10174790653613729, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.029705867171287537}, {"id": 874, "seek": 185896, "start": 1868.96, "end": 1870.96, "text": " and you walk out please consider", "tokens": [50864, 293, 291, 1792, 484, 1767, 1949, 50964], "temperature": 0.0, "avg_logprob": -0.10174790653613729, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.029705867171287537}, {"id": 875, "seek": 185896, "start": 1870.96, "end": 1872.96, "text": " doing a small donation in the box as well", "tokens": [50964, 884, 257, 1359, 19724, 294, 264, 2424, 382, 731, 51064], "temperature": 0.0, "avg_logprob": -0.10174790653613729, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.029705867171287537}, {"id": 876, "seek": 185896, "start": 1872.96, "end": 1874.96, "text": " to help cover the costs and if you're leaving", "tokens": [51064, 281, 854, 2060, 264, 5497, 293, 498, 291, 434, 5012, 51164], "temperature": 0.0, "avg_logprob": -0.10174790653613729, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.029705867171287537}, {"id": 877, "seek": 185896, "start": 1874.96, "end": 1876.96, "text": " please check if you see any trash", "tokens": [51164, 1767, 1520, 498, 291, 536, 604, 11321, 51264], "temperature": 0.0, "avg_logprob": -0.10174790653613729, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.029705867171287537}, {"id": 878, "seek": 185896, "start": 1876.96, "end": 1878.96, "text": " around please take the trash with you", "tokens": [51264, 926, 1767, 747, 264, 11321, 365, 291, 51364], "temperature": 0.0, "avg_logprob": -0.10174790653613729, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.029705867171287537}, {"id": 879, "seek": 185896, "start": 1878.96, "end": 1880.96, "text": " bottles anything", "tokens": [51364, 15923, 1340, 51464], "temperature": 0.0, "avg_logprob": -0.10174790653613729, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.029705867171287537}, {"id": 880, "seek": 185896, "start": 1880.96, "end": 1882.96, "text": " anything you clean up we don't have to clean up", "tokens": [51464, 1340, 291, 2541, 493, 321, 500, 380, 362, 281, 2541, 493, 51564], "temperature": 0.0, "avg_logprob": -0.10174790653613729, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.029705867171287537}, {"id": 881, "seek": 185896, "start": 1884.96, "end": 1886.96, "text": " thanks a lot Vanessa this was great", "tokens": [51664, 3231, 257, 688, 27928, 341, 390, 869, 51764], "temperature": 0.0, "avg_logprob": -0.10174790653613729, "compression_ratio": 1.7444933920704846, "no_speech_prob": 0.029705867171287537}, {"id": 882, "seek": 188696, "start": 1886.96, "end": 1888.96, "text": " bye", "tokens": [50364, 6543, 50464], "temperature": 0.0, "avg_logprob": -0.2978951632976532, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.2344244420528412}], "language": "en"}