{"text": " Hi everybody, I know you've heard it now, I think about four times about how the type safety and gleam is perfect. But guess what, we're doing it again for a fifth time in case you haven't heard it enough. But before we do that, I'll just introduce myself. My name is Harry Besto, I'm currently a student studying in the UK and looking to move on to university next year. I spoke here last year about gleam again and sort of helped start it off last year and at the moment I currently work with Felicia's Ventures on their research team. But while we get started, I know GraphQL isn't gleam, it isn't Erlang and it isn't Elixir, it's none of this stuff maybe you're here to see, but it is the perfect match to go with gleam as they both care about type safety and they care about how everything works correctly. In case people aren't familiar with GraphQL, I thought I'd just do a quick introduction to that so everybody can be on the same page for the rest of this. Here's an example type in GraphQL, it's a presentation, I thought that might be a fitting for today and we'll just imagine for a second that it's super simple, you only have a title, a set of speakers, in case you were lucky enough to have a friend, I don't. The amount of people who attend, whether it's one of the keynote ones and any speaker notes that people decide to give afterwards. So we'll break this down one by one really quick. The title's a string and it's not nullable. You have to have this as a string. The next one is an array of speakers where each speaker has to be there as well, that's what the estimation mark means. Integer that has to be there as well, a boolean that doesn't have to be there, it could be null, true or false, and then finally notes that also doesn't have to be there, just as a rough sort of guide for how GraphQL works. There's so much more to GraphQL that isn't actually fitting for this dev room and would go into so much more detail, so I think we'll just stick with this for now and we'll go from there. Before you end to using the two together, I'll just do the normal sort of introduction that everybody else has done to Gleam. It has type safe structs, the power of Erlang and JavaScript, and also a lovely friendly community who a lot of actually shown up this time here with the rest of their talks to persuade you as well to use it. So let's get to combining the two together. Here's what looks like very complicated Gleam code if you've never seen it before for a GraphQL request as well as sort of the request itself and then an object inside of it. We'll take for example that it could be a mutation or a query, which isn't too important for this, but it's just there anyway, and that you're requesting a list of objects potentially. Each object has a name, a set of arguments, and a set of fields that you're requesting where the field could be something as simple as just taking the name out of the presentation or as requesting subfields of the speakers that we saw earlier. Here's an example query that we're going to now use for the rest of this. The rest of the presentation is built around this query, so we're trying to query this presentation itself. We want the notes that I've supposedly written as well as my name and my email. This is what that looks like in Gleam, which looks absolutely awful for you guys. So we'll remove some of the stuff that isn't particularly important and we get left with this, which is just a simple function that would in reality take in the query as a string, actually pass it, and then would return something similar to this where we're saying it's not a mutation, we're requesting the presentation and the set of fields that were there before, and then the argument is that the title has to be that. But how on earth are we even going to use this? Everybody knows that GraphQL is normally queried over HTTP as its sort of baseline. Sometimes there's web sockets involved with subscriptions, and sometimes people go and do something a bit interesting with it. But for this example, we're going to use Wisp, which was actually written by Louis, and it I believe one of the only sort of higher level frameworks that Gleam has at the moment for HTTP. So I'll do a quick intro to this just to keep everything so everybody knows what they're doing. Wisp has some really nice functions in it for configuring logging, for getting the secret key for cookies and other hashing algorithms, and it builds on top of Mist, which is built by another one of the Gleam community, and just uses core underlying fundamentals for that. At the bottom, we're just telling it to sleep forever. So let's go through line by line and see what each thing does. This is super simple. It configures sane defaults, so info logging and all that sort of stuff, so you don't have to go over and do all of the annoying Erlang stuff. Then we generate a random string for this example. This isn't great. I guess you will have heard about it with Phoenix, Ruby on Rails, Laval, any of the other frameworks. You need to actually set a secret in real life, but for now we'll just ignore this as it's not particularly important. Then finally, we set up our handler, which is going to use this handle request function, which we'll get onto in a minute, as well as the secret, and then saying it's going to run on port 8000. Finally, the process does sleep forever is something I don't think you'd be used to in Erlang, but it's quite common in Gleam now, which is that you don't want the process just to terminate itself at the end. You want it to stay alive with the HTTP process running in the background. Let's get into the router. There's four imports we need just to start off, which is we need the request and response from Wisp. We need a string builder to actually send some stuff back. We need GraphQL Web, which is just sort of a, is nothing too big. It's just some boilerplate that's included with Wisp, and then also Gleam's HTTP for post for some filtering. Handle request is as simple as this. We're given a request. We have to return a response. We can do whatever we want to it, and as long as that we get the response out of the end, this, yeah, Wisp will handle returning it. Sorry. So in this case, we're using Gleam's powerful pattern matching to match on the tuple of the method and the path segments. That way, if you wanted to, you could have a get request to get info or health or sort of the UI as well be served. But in this case, we're just going to say everything else isn't found except for a post request to slash GraphQL. And then at the top, we have a use statement with web.middleware, which is something fairly new in Gleam but not super new. It's an abstraction for putting, for calling a function with a parameter passed into it. Before this, Gleam code was sort of nested functions on nested functions, which maybe wasn't the nicest to look at. This is sort of simplified that for quite a few things and is now used across everything from the standard library to libraries themselves to people's code. We have this GraphQL request function, which we actually need to do something with, but for now, we're just returning the string GraphQL response and sending that with 200 status code. So now if you were to send a post request to HTTP localhost 8000 slash GraphQL, you'd just get GraphQL response sent back to you. But you're thinking, that isn't what you came here to hear about, and it's absolutely useless. I agree. So let's go on to actually handling a GraphQL request and sending back some actually useful data. When we get the request in from GraphQL, it has the JSON structure of the query, the operation name, and then variables. The variables and operation name aren't important for this, but for a fully featured implementation they would be. Let's say you had a query that had loads of different requests inside of it, operations. You can then specify afterwards which one you want to use, and the variables that you pass stuff into those operations after the fact. Here we're using Gleam's decoder to decode into three, well, to decode three things into a custom type. The three doesn't mean anything like a tuple or an array like you might be used to. All it means is that the constructor of GraphQL full request has three arguments. So we're then saying the field query should be decoded as a string and put into the first field in the constructor. This is entirely type safe. I don't have anything on my slides about it, but let's say I jumbled up the order of these. It wouldn't match what the constructor should be, and as such you'd get a compile time error rather than ending up at the end of this talk going, why is my query looking like an object of variables? So I guess that's one point where Gleam's type safety comes back into being useful. The other part is that we can do something like this using the use statement again to say that the body has to be JSON. It's a case of we could handle all of this ourselves, but Wisp has a function for it. It requires that the content type's application JSON takes out the body and decodes it into a dynamic which goes back into Gleam's dynamic system, which allows you to break out of that box of type safety when you can't trust what you're getting in or you want to send something out in a way that's maybe less structured. When we want to then decode it, we have to, if we want to work with it, we can't just work with it in that dynamic form. It's not how Gleam wants to work, and it's going to make you could, but it would make your life so much harder than it needs to be. In real life, you should also be using result.try, handling this nicely and bubbling errors up, maybe with how Hailey spoke about where you could have your phantom types so you don't have to bubble it all the way back up. But for this short demonstration, I'm just unwrapping it, which will panic if somebody sends the wrong data. I'm unwrapping it with what is a bog standard QL request, which is underscore underscore type name, which just would in this case return query to you. It gives you nothing useful, but it means that the program isn't going to crash. Now that we have our body and we have the query inside of it, which we looked at earlier, we need to do something with it. So let's send it to that passing function that we saw at the very start and turn it from just being a generic string into something that's actually tangible and could be used by Gleam. But once we have that, how are we even going to resolve it? You have your query, you now have it as a Gleam type, but what does that even mean? You need to somehow get all of that speaker information back in a way that keeps your type safety, but also can be with the flexibility that GraphQL provides to people. So let's think about resolvers for a minute. We could go down this approach of having a type of the resolver where it has its key, for example, speaker.notes or just speaker, where you then have granularity over how far down you want to resolve each time. You want to resolve the whole object with one function or do you want an individual function for resolving each field? And then you have that function there, which gets the request as well as the variables and just returns a dynamic value, which can then be sent back to the client as Jason in this case. Being dynamic there, it will be much nicer to use a generic, but in the process of writing this talk, I couldn't figure out a nice way to allow you to have loads of different generics at once, which I think is something that is going to be worked on in the future maybe so that you can have a collection of generics that also are maybe like an interface type thing. Here's an example of how this could look, forgetting the presentation's notes, but there is one key problem that comes with this. Let's say you then have a list of these resolvers. How on earth do I find presentation.notes in a time that's actually suitable? The bigger notation for this is going to be o of n at best, or o of n at worst maybe, because you're going to have to go through all of them checking each one. So maps or lists, a hash map is going to be a thousand times better, maybe not actually a thousand times, don't quote me on that, but it is going to be significantly better than using this sort of list and sorting method. So let's switch it up and say that the resolver now isn't this custom type with a constructor and everything, it's now just a function that takes in a request, has those arguments passed in and still returns the dynamic. And now we can use this in a much simpler way, where we have a simple function that has resolve where it takes in a prefix, which will make a lot more sense in a minute. The HTTP request, the object itself we're trying to resolve, so we took the string, we pass it into this object and now we're going to resolve it. And then a dictionary of all the resolvers that were created when you wrote it. So you'd have your dictionary where you put in each of your resolvers, so you could have presentation.speakers, which resolves that whole array, as well as presentation.notes and any of the other fields that were there. Or you could even just have one resolver that resolves the entire presentation. Example of this is here, where you have your dictionary. We're just creating the dictionary from a list for simplicity, but this is the same as doing dictionary new and then inserting an element with that key and that value. Now that we have it, we can take our prefix, prefix it to the object.name and then try and see if we have that as a resolver. Of course you're thinking, how on earth is this going to work? Glean doesn't have an if statement, but we have something just as good, which is a case statement. We're going to say, if that resolver exists, try and just resolve using that. If it doesn't exist, we're going to check that there's fields. If there aren't fields, then we need to somehow handle this much nicer at the moment. It just returns that there was no resolver set. Then in reality, it should be erroring out properly and returning an error in the GraphQL standard way. And if there are fields, we then map each one, attempt to resolve it, and then we have this function at the bottom called combine results, which I'll come back to in a second. But you can see that the prefix actually ends up being the prefix that was passed in before and then the object's name then a dot. So as you go further down and into it, you get the dots. You still get the objects that were before added in. And then you can have your granularity that I've been speaking about of resolvers. The combine results just simply has the list of them piped in. It then folds the list into a dictionary and then it does dynamic.from to just turn that into that dynamic value to be returned at the end. So finally, I know it's been a lot, but let's finally put it all together into something actually useful. First, we need to resolve multiple things. When we got our request in, we could be requesting the presentation and a speaker separately and maybe an event separately. So we need to take each of those objects, go through the resolution process for them and get the values. So to do that, we call that function. Then we do, we say the responses after we've resolved them all, passing in the HTTP request, the past query, and your set of resolvers. And then this is what you finally end up with, the basic GraphQL request function in the start that just returned a string. Now it requires the JSON, unwraps the body, passes it, resolves everything, and then finally sends it as a HTML response, which probably should have been changed to JSON response. Small details there, and sends that back to the client. So yay, you might think we're finally finished. But in reality, we're actually missing so much from GraphQL that I wouldn't have even had time to discuss or make for this talk, such as proper error handling. When you return data from GraphQL, you should return data and errors separately. We don't even have the concept of an error here. Mutations at the moment, a mutation in the query, I just treated as the exact same thing. We should really have them as separate things that are resolved differently and handled differently. Subscriptions as well, that's using web sockets, and that's a whole nother layer of GraphQL that people, there's fairly, I guess, divisive opinions on how it should be implemented. So I didn't even touch that. But you might be thinking, overall, that's an absolute ton of work if you want to implement it yourself. So the goal with all of this is, let's make a package out of it. We'll have a green GraphQL package that you can just plug in, pass your resolvers, and it will pass and manage all of the query inside for you, similar to what Elixir has for Phoenix, as well as what Laval and the other major frameworks have, which makes it easier, which makes it super easy to use GraphQL and sometimes even easier than REST itself as it handles most of that abstraction for you. So thank you so much for listening as I ran through that really quickly. And if anybody has any questions, I think I have just enough time for a couple. Any questions? Thank you very much. So I know that GraphQL also has a schema, and I'm guessing from that API that you would write your schema alongside that. Do you have any thoughts on maybe generating some of that blue code from the schema, or maybe just the schema from the blue code? Yeah, when I first started, I thought, oh, the question was before I go, GraphQL has schemas, so is there any way for us to generate that code, either the code that I showed today or even the schema itself from Glean? And the answer to that is I looked at it when I started, and you could generate the schema from Glean and as such interpret some of this code, but long term I think the best bet would be sort of code generation similar to how it's done in JavaScript or TypeScript, where you either write your schema and then it generates some of the other Glean for you as well as a .graphql file, or you pass in a .graphql file to sort of a CLI, and it then spits out all of the boilerplate Glean you need. But yeah, schema validation is something else that sort of, I didn't really have time to do for this. Any other questions? Okay, thank you then.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.72, "text": " Hi everybody, I know you've heard it now, I think about four times about how the type", "tokens": [50364, 2421, 2201, 11, 286, 458, 291, 600, 2198, 309, 586, 11, 286, 519, 466, 1451, 1413, 466, 577, 264, 2010, 51200], "temperature": 0.0, "avg_logprob": -0.24869666301028828, "compression_ratio": 1.4463276836158192, "no_speech_prob": 0.28162291646003723}, {"id": 1, "seek": 0, "start": 16.72, "end": 18.84, "text": " safety and gleam is perfect.", "tokens": [51200, 4514, 293, 48956, 335, 307, 2176, 13, 51306], "temperature": 0.0, "avg_logprob": -0.24869666301028828, "compression_ratio": 1.4463276836158192, "no_speech_prob": 0.28162291646003723}, {"id": 2, "seek": 0, "start": 18.84, "end": 23.2, "text": " But guess what, we're doing it again for a fifth time in case you haven't heard it enough.", "tokens": [51306, 583, 2041, 437, 11, 321, 434, 884, 309, 797, 337, 257, 9266, 565, 294, 1389, 291, 2378, 380, 2198, 309, 1547, 13, 51524], "temperature": 0.0, "avg_logprob": -0.24869666301028828, "compression_ratio": 1.4463276836158192, "no_speech_prob": 0.28162291646003723}, {"id": 3, "seek": 0, "start": 23.2, "end": 27.72, "text": " But before we do that, I'll just introduce myself.", "tokens": [51524, 583, 949, 321, 360, 300, 11, 286, 603, 445, 5366, 2059, 13, 51750], "temperature": 0.0, "avg_logprob": -0.24869666301028828, "compression_ratio": 1.4463276836158192, "no_speech_prob": 0.28162291646003723}, {"id": 4, "seek": 2772, "start": 27.72, "end": 32.839999999999996, "text": " My name is Harry Besto, I'm currently a student studying in the UK and looking to move on", "tokens": [50364, 1222, 1315, 307, 9378, 363, 18465, 11, 286, 478, 4362, 257, 3107, 7601, 294, 264, 7051, 293, 1237, 281, 1286, 322, 50620], "temperature": 0.0, "avg_logprob": -0.2098563995361328, "compression_ratio": 1.625, "no_speech_prob": 0.1074969470500946}, {"id": 5, "seek": 2772, "start": 32.839999999999996, "end": 36.0, "text": " to university next year.", "tokens": [50620, 281, 5454, 958, 1064, 13, 50778], "temperature": 0.0, "avg_logprob": -0.2098563995361328, "compression_ratio": 1.625, "no_speech_prob": 0.1074969470500946}, {"id": 6, "seek": 2772, "start": 36.0, "end": 40.72, "text": " I spoke here last year about gleam again and sort of helped start it off last year and", "tokens": [50778, 286, 7179, 510, 1036, 1064, 466, 48956, 335, 797, 293, 1333, 295, 4254, 722, 309, 766, 1036, 1064, 293, 51014], "temperature": 0.0, "avg_logprob": -0.2098563995361328, "compression_ratio": 1.625, "no_speech_prob": 0.1074969470500946}, {"id": 7, "seek": 2772, "start": 40.72, "end": 45.879999999999995, "text": " at the moment I currently work with Felicia's Ventures on their research team.", "tokens": [51014, 412, 264, 1623, 286, 4362, 589, 365, 13298, 15341, 311, 28290, 1303, 322, 641, 2132, 1469, 13, 51272], "temperature": 0.0, "avg_logprob": -0.2098563995361328, "compression_ratio": 1.625, "no_speech_prob": 0.1074969470500946}, {"id": 8, "seek": 2772, "start": 45.879999999999995, "end": 50.28, "text": " But while we get started, I know GraphQL isn't gleam, it isn't Erlang and it isn't Elixir,", "tokens": [51272, 583, 1339, 321, 483, 1409, 11, 286, 458, 21884, 13695, 1943, 380, 48956, 335, 11, 309, 1943, 380, 3300, 25241, 293, 309, 1943, 380, 2699, 970, 347, 11, 51492], "temperature": 0.0, "avg_logprob": -0.2098563995361328, "compression_ratio": 1.625, "no_speech_prob": 0.1074969470500946}, {"id": 9, "seek": 2772, "start": 50.28, "end": 54.68, "text": " it's none of this stuff maybe you're here to see, but it is the perfect match to go", "tokens": [51492, 309, 311, 6022, 295, 341, 1507, 1310, 291, 434, 510, 281, 536, 11, 457, 309, 307, 264, 2176, 2995, 281, 352, 51712], "temperature": 0.0, "avg_logprob": -0.2098563995361328, "compression_ratio": 1.625, "no_speech_prob": 0.1074969470500946}, {"id": 10, "seek": 5468, "start": 54.68, "end": 60.0, "text": " with gleam as they both care about type safety and they care about how everything works correctly.", "tokens": [50364, 365, 48956, 335, 382, 436, 1293, 1127, 466, 2010, 4514, 293, 436, 1127, 466, 577, 1203, 1985, 8944, 13, 50630], "temperature": 0.0, "avg_logprob": -0.16069097737319596, "compression_ratio": 1.7009966777408638, "no_speech_prob": 0.18881171941757202}, {"id": 11, "seek": 5468, "start": 60.0, "end": 63.44, "text": " In case people aren't familiar with GraphQL, I thought I'd just do a quick introduction", "tokens": [50630, 682, 1389, 561, 3212, 380, 4963, 365, 21884, 13695, 11, 286, 1194, 286, 1116, 445, 360, 257, 1702, 9339, 50802], "temperature": 0.0, "avg_logprob": -0.16069097737319596, "compression_ratio": 1.7009966777408638, "no_speech_prob": 0.18881171941757202}, {"id": 12, "seek": 5468, "start": 63.44, "end": 66.44, "text": " to that so everybody can be on the same page for the rest of this.", "tokens": [50802, 281, 300, 370, 2201, 393, 312, 322, 264, 912, 3028, 337, 264, 1472, 295, 341, 13, 50952], "temperature": 0.0, "avg_logprob": -0.16069097737319596, "compression_ratio": 1.7009966777408638, "no_speech_prob": 0.18881171941757202}, {"id": 13, "seek": 5468, "start": 66.44, "end": 71.88, "text": " Here's an example type in GraphQL, it's a presentation, I thought that might be a fitting", "tokens": [50952, 1692, 311, 364, 1365, 2010, 294, 21884, 13695, 11, 309, 311, 257, 5860, 11, 286, 1194, 300, 1062, 312, 257, 15669, 51224], "temperature": 0.0, "avg_logprob": -0.16069097737319596, "compression_ratio": 1.7009966777408638, "no_speech_prob": 0.18881171941757202}, {"id": 14, "seek": 5468, "start": 71.88, "end": 76.92, "text": " for today and we'll just imagine for a second that it's super simple, you only have a title,", "tokens": [51224, 337, 965, 293, 321, 603, 445, 3811, 337, 257, 1150, 300, 309, 311, 1687, 2199, 11, 291, 787, 362, 257, 4876, 11, 51476], "temperature": 0.0, "avg_logprob": -0.16069097737319596, "compression_ratio": 1.7009966777408638, "no_speech_prob": 0.18881171941757202}, {"id": 15, "seek": 5468, "start": 76.92, "end": 81.44, "text": " a set of speakers, in case you were lucky enough to have a friend, I don't.", "tokens": [51476, 257, 992, 295, 9518, 11, 294, 1389, 291, 645, 6356, 1547, 281, 362, 257, 1277, 11, 286, 500, 380, 13, 51702], "temperature": 0.0, "avg_logprob": -0.16069097737319596, "compression_ratio": 1.7009966777408638, "no_speech_prob": 0.18881171941757202}, {"id": 16, "seek": 8144, "start": 81.48, "end": 86.8, "text": " The amount of people who attend, whether it's one of the keynote ones and any speaker notes", "tokens": [50366, 440, 2372, 295, 561, 567, 6888, 11, 1968, 309, 311, 472, 295, 264, 33896, 2306, 293, 604, 8145, 5570, 50632], "temperature": 0.0, "avg_logprob": -0.18135951459407806, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.3312798738479614}, {"id": 17, "seek": 8144, "start": 86.8, "end": 88.8, "text": " that people decide to give afterwards.", "tokens": [50632, 300, 561, 4536, 281, 976, 10543, 13, 50732], "temperature": 0.0, "avg_logprob": -0.18135951459407806, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.3312798738479614}, {"id": 18, "seek": 8144, "start": 88.8, "end": 91.16, "text": " So we'll break this down one by one really quick.", "tokens": [50732, 407, 321, 603, 1821, 341, 760, 472, 538, 472, 534, 1702, 13, 50850], "temperature": 0.0, "avg_logprob": -0.18135951459407806, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.3312798738479614}, {"id": 19, "seek": 8144, "start": 91.16, "end": 94.2, "text": " The title's a string and it's not nullable.", "tokens": [50850, 440, 4876, 311, 257, 6798, 293, 309, 311, 406, 18184, 712, 13, 51002], "temperature": 0.0, "avg_logprob": -0.18135951459407806, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.3312798738479614}, {"id": 20, "seek": 8144, "start": 94.2, "end": 96.48, "text": " You have to have this as a string.", "tokens": [51002, 509, 362, 281, 362, 341, 382, 257, 6798, 13, 51116], "temperature": 0.0, "avg_logprob": -0.18135951459407806, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.3312798738479614}, {"id": 21, "seek": 8144, "start": 96.48, "end": 101.0, "text": " The next one is an array of speakers where each speaker has to be there as well, that's", "tokens": [51116, 440, 958, 472, 307, 364, 10225, 295, 9518, 689, 1184, 8145, 575, 281, 312, 456, 382, 731, 11, 300, 311, 51342], "temperature": 0.0, "avg_logprob": -0.18135951459407806, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.3312798738479614}, {"id": 22, "seek": 8144, "start": 101.0, "end": 103.47999999999999, "text": " what the estimation mark means.", "tokens": [51342, 437, 264, 35701, 1491, 1355, 13, 51466], "temperature": 0.0, "avg_logprob": -0.18135951459407806, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.3312798738479614}, {"id": 23, "seek": 8144, "start": 103.47999999999999, "end": 108.0, "text": " Integer that has to be there as well, a boolean that doesn't have to be there, it could be", "tokens": [51466, 5681, 30744, 300, 575, 281, 312, 456, 382, 731, 11, 257, 748, 4812, 282, 300, 1177, 380, 362, 281, 312, 456, 11, 309, 727, 312, 51692], "temperature": 0.0, "avg_logprob": -0.18135951459407806, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.3312798738479614}, {"id": 24, "seek": 10800, "start": 108.04, "end": 113.64, "text": " null, true or false, and then finally notes that also doesn't have to be there, just as", "tokens": [50366, 18184, 11, 2074, 420, 7908, 11, 293, 550, 2721, 5570, 300, 611, 1177, 380, 362, 281, 312, 456, 11, 445, 382, 50646], "temperature": 0.0, "avg_logprob": -0.19640602864010234, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.013245713897049427}, {"id": 25, "seek": 10800, "start": 113.64, "end": 116.96000000000001, "text": " a rough sort of guide for how GraphQL works.", "tokens": [50646, 257, 5903, 1333, 295, 5934, 337, 577, 21884, 13695, 1985, 13, 50812], "temperature": 0.0, "avg_logprob": -0.19640602864010234, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.013245713897049427}, {"id": 26, "seek": 10800, "start": 116.96000000000001, "end": 121.2, "text": " There's so much more to GraphQL that isn't actually fitting for this dev room and would", "tokens": [50812, 821, 311, 370, 709, 544, 281, 21884, 13695, 300, 1943, 380, 767, 15669, 337, 341, 1905, 1808, 293, 576, 51024], "temperature": 0.0, "avg_logprob": -0.19640602864010234, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.013245713897049427}, {"id": 27, "seek": 10800, "start": 121.2, "end": 124.96000000000001, "text": " go into so much more detail, so I think we'll just stick with this for now and we'll go", "tokens": [51024, 352, 666, 370, 709, 544, 2607, 11, 370, 286, 519, 321, 603, 445, 2897, 365, 341, 337, 586, 293, 321, 603, 352, 51212], "temperature": 0.0, "avg_logprob": -0.19640602864010234, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.013245713897049427}, {"id": 28, "seek": 10800, "start": 124.96000000000001, "end": 126.96000000000001, "text": " from there.", "tokens": [51212, 490, 456, 13, 51312], "temperature": 0.0, "avg_logprob": -0.19640602864010234, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.013245713897049427}, {"id": 29, "seek": 10800, "start": 126.96000000000001, "end": 130.28, "text": " Before you end to using the two together, I'll just do the normal sort of introduction", "tokens": [51312, 4546, 291, 917, 281, 1228, 264, 732, 1214, 11, 286, 603, 445, 360, 264, 2710, 1333, 295, 9339, 51478], "temperature": 0.0, "avg_logprob": -0.19640602864010234, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.013245713897049427}, {"id": 30, "seek": 10800, "start": 130.28, "end": 131.84, "text": " that everybody else has done to Gleam.", "tokens": [51478, 300, 2201, 1646, 575, 1096, 281, 460, 306, 335, 13, 51556], "temperature": 0.0, "avg_logprob": -0.19640602864010234, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.013245713897049427}, {"id": 31, "seek": 10800, "start": 131.84, "end": 137.92000000000002, "text": " It has type safe structs, the power of Erlang and JavaScript, and also a lovely friendly", "tokens": [51556, 467, 575, 2010, 3273, 6594, 82, 11, 264, 1347, 295, 3300, 25241, 293, 15778, 11, 293, 611, 257, 7496, 9208, 51860], "temperature": 0.0, "avg_logprob": -0.19640602864010234, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.013245713897049427}, {"id": 32, "seek": 13792, "start": 137.92, "end": 142.83999999999997, "text": " community who a lot of actually shown up this time here with the rest of their talks", "tokens": [50364, 1768, 567, 257, 688, 295, 767, 4898, 493, 341, 565, 510, 365, 264, 1472, 295, 641, 6686, 50610], "temperature": 0.0, "avg_logprob": -0.18715160264881378, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.0007374128908850253}, {"id": 33, "seek": 13792, "start": 142.83999999999997, "end": 146.32, "text": " to persuade you as well to use it.", "tokens": [50610, 281, 31781, 291, 382, 731, 281, 764, 309, 13, 50784], "temperature": 0.0, "avg_logprob": -0.18715160264881378, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.0007374128908850253}, {"id": 34, "seek": 13792, "start": 146.32, "end": 150.23999999999998, "text": " So let's get to combining the two together.", "tokens": [50784, 407, 718, 311, 483, 281, 21928, 264, 732, 1214, 13, 50980], "temperature": 0.0, "avg_logprob": -0.18715160264881378, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.0007374128908850253}, {"id": 35, "seek": 13792, "start": 150.23999999999998, "end": 154.76, "text": " Here's what looks like very complicated Gleam code if you've never seen it before for a", "tokens": [50980, 1692, 311, 437, 1542, 411, 588, 6179, 460, 306, 335, 3089, 498, 291, 600, 1128, 1612, 309, 949, 337, 257, 51206], "temperature": 0.0, "avg_logprob": -0.18715160264881378, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.0007374128908850253}, {"id": 36, "seek": 13792, "start": 154.76, "end": 161.16, "text": " GraphQL request as well as sort of the request itself and then an object inside of it.", "tokens": [51206, 21884, 13695, 5308, 382, 731, 382, 1333, 295, 264, 5308, 2564, 293, 550, 364, 2657, 1854, 295, 309, 13, 51526], "temperature": 0.0, "avg_logprob": -0.18715160264881378, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.0007374128908850253}, {"id": 37, "seek": 13792, "start": 161.16, "end": 165.0, "text": " We'll take for example that it could be a mutation or a query, which isn't too important", "tokens": [51526, 492, 603, 747, 337, 1365, 300, 309, 727, 312, 257, 27960, 420, 257, 14581, 11, 597, 1943, 380, 886, 1021, 51718], "temperature": 0.0, "avg_logprob": -0.18715160264881378, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.0007374128908850253}, {"id": 38, "seek": 16500, "start": 165.0, "end": 169.6, "text": " for this, but it's just there anyway, and that you're requesting a list of objects potentially.", "tokens": [50364, 337, 341, 11, 457, 309, 311, 445, 456, 4033, 11, 293, 300, 291, 434, 31937, 257, 1329, 295, 6565, 7263, 13, 50594], "temperature": 0.0, "avg_logprob": -0.17307445651195089, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.018477747216820717}, {"id": 39, "seek": 16500, "start": 169.6, "end": 174.36, "text": " Each object has a name, a set of arguments, and a set of fields that you're requesting", "tokens": [50594, 6947, 2657, 575, 257, 1315, 11, 257, 992, 295, 12869, 11, 293, 257, 992, 295, 7909, 300, 291, 434, 31937, 50832], "temperature": 0.0, "avg_logprob": -0.17307445651195089, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.018477747216820717}, {"id": 40, "seek": 16500, "start": 174.36, "end": 178.92, "text": " where the field could be something as simple as just taking the name out of the presentation", "tokens": [50832, 689, 264, 2519, 727, 312, 746, 382, 2199, 382, 445, 1940, 264, 1315, 484, 295, 264, 5860, 51060], "temperature": 0.0, "avg_logprob": -0.17307445651195089, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.018477747216820717}, {"id": 41, "seek": 16500, "start": 178.92, "end": 182.96, "text": " or as requesting subfields of the speakers that we saw earlier.", "tokens": [51060, 420, 382, 31937, 1422, 7610, 82, 295, 264, 9518, 300, 321, 1866, 3071, 13, 51262], "temperature": 0.0, "avg_logprob": -0.17307445651195089, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.018477747216820717}, {"id": 42, "seek": 16500, "start": 182.96, "end": 187.88, "text": " Here's an example query that we're going to now use for the rest of this.", "tokens": [51262, 1692, 311, 364, 1365, 14581, 300, 321, 434, 516, 281, 586, 764, 337, 264, 1472, 295, 341, 13, 51508], "temperature": 0.0, "avg_logprob": -0.17307445651195089, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.018477747216820717}, {"id": 43, "seek": 16500, "start": 187.88, "end": 192.32, "text": " The rest of the presentation is built around this query, so we're trying to query this", "tokens": [51508, 440, 1472, 295, 264, 5860, 307, 3094, 926, 341, 14581, 11, 370, 321, 434, 1382, 281, 14581, 341, 51730], "temperature": 0.0, "avg_logprob": -0.17307445651195089, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.018477747216820717}, {"id": 44, "seek": 19232, "start": 192.32, "end": 193.76, "text": " presentation itself.", "tokens": [50364, 5860, 2564, 13, 50436], "temperature": 0.0, "avg_logprob": -0.14220101124531515, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.04813018813729286}, {"id": 45, "seek": 19232, "start": 193.76, "end": 201.07999999999998, "text": " We want the notes that I've supposedly written as well as my name and my email.", "tokens": [50436, 492, 528, 264, 5570, 300, 286, 600, 20581, 3720, 382, 731, 382, 452, 1315, 293, 452, 3796, 13, 50802], "temperature": 0.0, "avg_logprob": -0.14220101124531515, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.04813018813729286}, {"id": 46, "seek": 19232, "start": 201.07999999999998, "end": 205.16, "text": " This is what that looks like in Gleam, which looks absolutely awful for you guys.", "tokens": [50802, 639, 307, 437, 300, 1542, 411, 294, 460, 306, 335, 11, 597, 1542, 3122, 11232, 337, 291, 1074, 13, 51006], "temperature": 0.0, "avg_logprob": -0.14220101124531515, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.04813018813729286}, {"id": 47, "seek": 19232, "start": 205.16, "end": 208.44, "text": " So we'll remove some of the stuff that isn't particularly important and we get left with", "tokens": [51006, 407, 321, 603, 4159, 512, 295, 264, 1507, 300, 1943, 380, 4098, 1021, 293, 321, 483, 1411, 365, 51170], "temperature": 0.0, "avg_logprob": -0.14220101124531515, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.04813018813729286}, {"id": 48, "seek": 19232, "start": 208.44, "end": 214.6, "text": " this, which is just a simple function that would in reality take in the query as a string,", "tokens": [51170, 341, 11, 597, 307, 445, 257, 2199, 2445, 300, 576, 294, 4103, 747, 294, 264, 14581, 382, 257, 6798, 11, 51478], "temperature": 0.0, "avg_logprob": -0.14220101124531515, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.04813018813729286}, {"id": 49, "seek": 19232, "start": 214.6, "end": 219.24, "text": " actually pass it, and then would return something similar to this where we're saying it's not", "tokens": [51478, 767, 1320, 309, 11, 293, 550, 576, 2736, 746, 2531, 281, 341, 689, 321, 434, 1566, 309, 311, 406, 51710], "temperature": 0.0, "avg_logprob": -0.14220101124531515, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.04813018813729286}, {"id": 50, "seek": 21924, "start": 219.32000000000002, "end": 224.64000000000001, "text": " a mutation, we're requesting the presentation and the set of fields that were there before,", "tokens": [50368, 257, 27960, 11, 321, 434, 31937, 264, 5860, 293, 264, 992, 295, 7909, 300, 645, 456, 949, 11, 50634], "temperature": 0.0, "avg_logprob": -0.17152324392775858, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.2496732920408249}, {"id": 51, "seek": 21924, "start": 224.64000000000001, "end": 228.56, "text": " and then the argument is that the title has to be that.", "tokens": [50634, 293, 550, 264, 6770, 307, 300, 264, 4876, 575, 281, 312, 300, 13, 50830], "temperature": 0.0, "avg_logprob": -0.17152324392775858, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.2496732920408249}, {"id": 52, "seek": 21924, "start": 228.56, "end": 231.28, "text": " But how on earth are we even going to use this?", "tokens": [50830, 583, 577, 322, 4120, 366, 321, 754, 516, 281, 764, 341, 30, 50966], "temperature": 0.0, "avg_logprob": -0.17152324392775858, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.2496732920408249}, {"id": 53, "seek": 21924, "start": 231.28, "end": 236.84, "text": " Everybody knows that GraphQL is normally queried over HTTP as its sort of baseline.", "tokens": [50966, 7646, 3255, 300, 21884, 13695, 307, 5646, 7083, 1091, 670, 33283, 382, 1080, 1333, 295, 20518, 13, 51244], "temperature": 0.0, "avg_logprob": -0.17152324392775858, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.2496732920408249}, {"id": 54, "seek": 21924, "start": 236.84, "end": 241.4, "text": " Sometimes there's web sockets involved with subscriptions, and sometimes people go and", "tokens": [51244, 4803, 456, 311, 3670, 370, 11984, 3288, 365, 44951, 11, 293, 2171, 561, 352, 293, 51472], "temperature": 0.0, "avg_logprob": -0.17152324392775858, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.2496732920408249}, {"id": 55, "seek": 21924, "start": 241.4, "end": 243.88, "text": " do something a bit interesting with it.", "tokens": [51472, 360, 746, 257, 857, 1880, 365, 309, 13, 51596], "temperature": 0.0, "avg_logprob": -0.17152324392775858, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.2496732920408249}, {"id": 56, "seek": 21924, "start": 243.88, "end": 248.60000000000002, "text": " But for this example, we're going to use Wisp, which was actually written by Louis, and it", "tokens": [51596, 583, 337, 341, 1365, 11, 321, 434, 516, 281, 764, 343, 7631, 11, 597, 390, 767, 3720, 538, 9763, 11, 293, 309, 51832], "temperature": 0.0, "avg_logprob": -0.17152324392775858, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.2496732920408249}, {"id": 57, "seek": 24860, "start": 249.07999999999998, "end": 254.72, "text": " I believe one of the only sort of higher level frameworks that Gleam has at the moment", "tokens": [50388, 286, 1697, 472, 295, 264, 787, 1333, 295, 2946, 1496, 29834, 300, 460, 306, 335, 575, 412, 264, 1623, 50670], "temperature": 0.0, "avg_logprob": -0.17894221160371424, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.004019514657557011}, {"id": 58, "seek": 24860, "start": 254.72, "end": 256.28, "text": " for HTTP.", "tokens": [50670, 337, 33283, 13, 50748], "temperature": 0.0, "avg_logprob": -0.17894221160371424, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.004019514657557011}, {"id": 59, "seek": 24860, "start": 256.28, "end": 261.24, "text": " So I'll do a quick intro to this just to keep everything so everybody knows what they're", "tokens": [50748, 407, 286, 603, 360, 257, 1702, 12897, 281, 341, 445, 281, 1066, 1203, 370, 2201, 3255, 437, 436, 434, 50996], "temperature": 0.0, "avg_logprob": -0.17894221160371424, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.004019514657557011}, {"id": 60, "seek": 24860, "start": 261.24, "end": 262.24, "text": " doing.", "tokens": [50996, 884, 13, 51046], "temperature": 0.0, "avg_logprob": -0.17894221160371424, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.004019514657557011}, {"id": 61, "seek": 24860, "start": 262.24, "end": 265.92, "text": " Wisp has some really nice functions in it for configuring logging, for getting the secret", "tokens": [51046, 343, 7631, 575, 512, 534, 1481, 6828, 294, 309, 337, 6662, 1345, 27991, 11, 337, 1242, 264, 4054, 51230], "temperature": 0.0, "avg_logprob": -0.17894221160371424, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.004019514657557011}, {"id": 62, "seek": 24860, "start": 265.92, "end": 270.76, "text": " key for cookies and other hashing algorithms, and it builds on top of Mist, which is built", "tokens": [51230, 2141, 337, 13670, 293, 661, 575, 571, 14642, 11, 293, 309, 15182, 322, 1192, 295, 20166, 11, 597, 307, 3094, 51472], "temperature": 0.0, "avg_logprob": -0.17894221160371424, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.004019514657557011}, {"id": 63, "seek": 24860, "start": 270.76, "end": 277.28, "text": " by another one of the Gleam community, and just uses core underlying fundamentals for", "tokens": [51472, 538, 1071, 472, 295, 264, 460, 306, 335, 1768, 11, 293, 445, 4960, 4965, 14217, 29505, 337, 51798], "temperature": 0.0, "avg_logprob": -0.17894221160371424, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.004019514657557011}, {"id": 64, "seek": 24860, "start": 277.28, "end": 278.28, "text": " that.", "tokens": [51798, 300, 13, 51848], "temperature": 0.0, "avg_logprob": -0.17894221160371424, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.004019514657557011}, {"id": 65, "seek": 27828, "start": 278.28, "end": 280.67999999999995, "text": " At the bottom, we're just telling it to sleep forever.", "tokens": [50364, 1711, 264, 2767, 11, 321, 434, 445, 3585, 309, 281, 2817, 5680, 13, 50484], "temperature": 0.0, "avg_logprob": -0.16285811814685797, "compression_ratio": 1.6186186186186187, "no_speech_prob": 0.015906210988759995}, {"id": 66, "seek": 27828, "start": 280.67999999999995, "end": 283.35999999999996, "text": " So let's go through line by line and see what each thing does.", "tokens": [50484, 407, 718, 311, 352, 807, 1622, 538, 1622, 293, 536, 437, 1184, 551, 775, 13, 50618], "temperature": 0.0, "avg_logprob": -0.16285811814685797, "compression_ratio": 1.6186186186186187, "no_speech_prob": 0.015906210988759995}, {"id": 67, "seek": 27828, "start": 283.35999999999996, "end": 285.59999999999997, "text": " This is super simple.", "tokens": [50618, 639, 307, 1687, 2199, 13, 50730], "temperature": 0.0, "avg_logprob": -0.16285811814685797, "compression_ratio": 1.6186186186186187, "no_speech_prob": 0.015906210988759995}, {"id": 68, "seek": 27828, "start": 285.59999999999997, "end": 290.35999999999996, "text": " It configures sane defaults, so info logging and all that sort of stuff, so you don't have", "tokens": [50730, 467, 6662, 1303, 45610, 7576, 82, 11, 370, 13614, 27991, 293, 439, 300, 1333, 295, 1507, 11, 370, 291, 500, 380, 362, 50968], "temperature": 0.0, "avg_logprob": -0.16285811814685797, "compression_ratio": 1.6186186186186187, "no_speech_prob": 0.015906210988759995}, {"id": 69, "seek": 27828, "start": 290.35999999999996, "end": 294.79999999999995, "text": " to go over and do all of the annoying Erlang stuff.", "tokens": [50968, 281, 352, 670, 293, 360, 439, 295, 264, 11304, 3300, 25241, 1507, 13, 51190], "temperature": 0.0, "avg_logprob": -0.16285811814685797, "compression_ratio": 1.6186186186186187, "no_speech_prob": 0.015906210988759995}, {"id": 70, "seek": 27828, "start": 294.79999999999995, "end": 297.03999999999996, "text": " Then we generate a random string for this example.", "tokens": [51190, 1396, 321, 8460, 257, 4974, 6798, 337, 341, 1365, 13, 51302], "temperature": 0.0, "avg_logprob": -0.16285811814685797, "compression_ratio": 1.6186186186186187, "no_speech_prob": 0.015906210988759995}, {"id": 71, "seek": 27828, "start": 297.03999999999996, "end": 298.03999999999996, "text": " This isn't great.", "tokens": [51302, 639, 1943, 380, 869, 13, 51352], "temperature": 0.0, "avg_logprob": -0.16285811814685797, "compression_ratio": 1.6186186186186187, "no_speech_prob": 0.015906210988759995}, {"id": 72, "seek": 27828, "start": 298.03999999999996, "end": 301.84, "text": " I guess you will have heard about it with Phoenix, Ruby on Rails, Laval, any of the", "tokens": [51352, 286, 2041, 291, 486, 362, 2198, 466, 309, 365, 18383, 11, 19907, 322, 48526, 11, 30966, 304, 11, 604, 295, 264, 51542], "temperature": 0.0, "avg_logprob": -0.16285811814685797, "compression_ratio": 1.6186186186186187, "no_speech_prob": 0.015906210988759995}, {"id": 73, "seek": 27828, "start": 301.84, "end": 302.84, "text": " other frameworks.", "tokens": [51542, 661, 29834, 13, 51592], "temperature": 0.0, "avg_logprob": -0.16285811814685797, "compression_ratio": 1.6186186186186187, "no_speech_prob": 0.015906210988759995}, {"id": 74, "seek": 27828, "start": 302.84, "end": 306.35999999999996, "text": " You need to actually set a secret in real life, but for now we'll just ignore this as", "tokens": [51592, 509, 643, 281, 767, 992, 257, 4054, 294, 957, 993, 11, 457, 337, 586, 321, 603, 445, 11200, 341, 382, 51768], "temperature": 0.0, "avg_logprob": -0.16285811814685797, "compression_ratio": 1.6186186186186187, "no_speech_prob": 0.015906210988759995}, {"id": 75, "seek": 30636, "start": 306.44, "end": 308.6, "text": " it's not particularly important.", "tokens": [50368, 309, 311, 406, 4098, 1021, 13, 50476], "temperature": 0.0, "avg_logprob": -0.16152192554334655, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.005244449246674776}, {"id": 76, "seek": 30636, "start": 308.6, "end": 312.8, "text": " Then finally, we set up our handler, which is going to use this handle request function,", "tokens": [50476, 1396, 2721, 11, 321, 992, 493, 527, 41967, 11, 597, 307, 516, 281, 764, 341, 4813, 5308, 2445, 11, 50686], "temperature": 0.0, "avg_logprob": -0.16152192554334655, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.005244449246674776}, {"id": 77, "seek": 30636, "start": 312.8, "end": 317.36, "text": " which we'll get onto in a minute, as well as the secret, and then saying it's going", "tokens": [50686, 597, 321, 603, 483, 3911, 294, 257, 3456, 11, 382, 731, 382, 264, 4054, 11, 293, 550, 1566, 309, 311, 516, 50914], "temperature": 0.0, "avg_logprob": -0.16152192554334655, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.005244449246674776}, {"id": 78, "seek": 30636, "start": 317.36, "end": 319.32, "text": " to run on port 8000.", "tokens": [50914, 281, 1190, 322, 2436, 1649, 1360, 13, 51012], "temperature": 0.0, "avg_logprob": -0.16152192554334655, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.005244449246674776}, {"id": 79, "seek": 30636, "start": 319.32, "end": 324.76, "text": " Finally, the process does sleep forever is something I don't think you'd be used to", "tokens": [51012, 6288, 11, 264, 1399, 775, 2817, 5680, 307, 746, 286, 500, 380, 519, 291, 1116, 312, 1143, 281, 51284], "temperature": 0.0, "avg_logprob": -0.16152192554334655, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.005244449246674776}, {"id": 80, "seek": 30636, "start": 324.76, "end": 329.08000000000004, "text": " in Erlang, but it's quite common in Gleam now, which is that you don't want the process", "tokens": [51284, 294, 3300, 25241, 11, 457, 309, 311, 1596, 2689, 294, 460, 306, 335, 586, 11, 597, 307, 300, 291, 500, 380, 528, 264, 1399, 51500], "temperature": 0.0, "avg_logprob": -0.16152192554334655, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.005244449246674776}, {"id": 81, "seek": 30636, "start": 329.08000000000004, "end": 330.40000000000003, "text": " just to terminate itself at the end.", "tokens": [51500, 445, 281, 10761, 473, 2564, 412, 264, 917, 13, 51566], "temperature": 0.0, "avg_logprob": -0.16152192554334655, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.005244449246674776}, {"id": 82, "seek": 30636, "start": 330.40000000000003, "end": 334.36, "text": " You want it to stay alive with the HTTP process running in the background.", "tokens": [51566, 509, 528, 309, 281, 1754, 5465, 365, 264, 33283, 1399, 2614, 294, 264, 3678, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16152192554334655, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.005244449246674776}, {"id": 83, "seek": 33436, "start": 335.36, "end": 338.04, "text": " Let's get into the router.", "tokens": [50414, 961, 311, 483, 666, 264, 22492, 13, 50548], "temperature": 0.0, "avg_logprob": -0.19842056099695105, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0014409504365175962}, {"id": 84, "seek": 33436, "start": 338.04, "end": 341.76, "text": " There's four imports we need just to start off, which is we need the request and response", "tokens": [50548, 821, 311, 1451, 41596, 321, 643, 445, 281, 722, 766, 11, 597, 307, 321, 643, 264, 5308, 293, 4134, 50734], "temperature": 0.0, "avg_logprob": -0.19842056099695105, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0014409504365175962}, {"id": 85, "seek": 33436, "start": 341.76, "end": 342.76, "text": " from Wisp.", "tokens": [50734, 490, 343, 7631, 13, 50784], "temperature": 0.0, "avg_logprob": -0.19842056099695105, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0014409504365175962}, {"id": 86, "seek": 33436, "start": 342.76, "end": 346.36, "text": " We need a string builder to actually send some stuff back.", "tokens": [50784, 492, 643, 257, 6798, 27377, 281, 767, 2845, 512, 1507, 646, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19842056099695105, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0014409504365175962}, {"id": 87, "seek": 33436, "start": 346.36, "end": 350.8, "text": " We need GraphQL Web, which is just sort of a, is nothing too big.", "tokens": [50964, 492, 643, 21884, 13695, 9573, 11, 597, 307, 445, 1333, 295, 257, 11, 307, 1825, 886, 955, 13, 51186], "temperature": 0.0, "avg_logprob": -0.19842056099695105, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0014409504365175962}, {"id": 88, "seek": 33436, "start": 350.8, "end": 355.2, "text": " It's just some boilerplate that's included with Wisp, and then also Gleam's HTTP for", "tokens": [51186, 467, 311, 445, 512, 39228, 37008, 300, 311, 5556, 365, 343, 7631, 11, 293, 550, 611, 460, 306, 335, 311, 33283, 337, 51406], "temperature": 0.0, "avg_logprob": -0.19842056099695105, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0014409504365175962}, {"id": 89, "seek": 33436, "start": 355.2, "end": 358.6, "text": " post for some filtering.", "tokens": [51406, 2183, 337, 512, 30822, 13, 51576], "temperature": 0.0, "avg_logprob": -0.19842056099695105, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0014409504365175962}, {"id": 90, "seek": 33436, "start": 358.6, "end": 360.0, "text": " Handle request is as simple as this.", "tokens": [51576, 8854, 306, 5308, 307, 382, 2199, 382, 341, 13, 51646], "temperature": 0.0, "avg_logprob": -0.19842056099695105, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0014409504365175962}, {"id": 91, "seek": 33436, "start": 360.0, "end": 361.0, "text": " We're given a request.", "tokens": [51646, 492, 434, 2212, 257, 5308, 13, 51696], "temperature": 0.0, "avg_logprob": -0.19842056099695105, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0014409504365175962}, {"id": 92, "seek": 33436, "start": 361.0, "end": 363.04, "text": " We have to return a response.", "tokens": [51696, 492, 362, 281, 2736, 257, 4134, 13, 51798], "temperature": 0.0, "avg_logprob": -0.19842056099695105, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0014409504365175962}, {"id": 93, "seek": 36304, "start": 363.04, "end": 368.72, "text": " We can do whatever we want to it, and as long as that we get the response out of the end,", "tokens": [50364, 492, 393, 360, 2035, 321, 528, 281, 309, 11, 293, 382, 938, 382, 300, 321, 483, 264, 4134, 484, 295, 264, 917, 11, 50648], "temperature": 0.0, "avg_logprob": -0.19109031444287483, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.03478217124938965}, {"id": 94, "seek": 36304, "start": 368.72, "end": 372.92, "text": " this, yeah, Wisp will handle returning it.", "tokens": [50648, 341, 11, 1338, 11, 343, 7631, 486, 4813, 12678, 309, 13, 50858], "temperature": 0.0, "avg_logprob": -0.19109031444287483, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.03478217124938965}, {"id": 95, "seek": 36304, "start": 372.92, "end": 373.92, "text": " Sorry.", "tokens": [50858, 4919, 13, 50908], "temperature": 0.0, "avg_logprob": -0.19109031444287483, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.03478217124938965}, {"id": 96, "seek": 36304, "start": 373.92, "end": 378.92, "text": " So in this case, we're using Gleam's powerful pattern matching to match on the tuple of", "tokens": [50908, 407, 294, 341, 1389, 11, 321, 434, 1228, 460, 306, 335, 311, 4005, 5102, 14324, 281, 2995, 322, 264, 2604, 781, 295, 51158], "temperature": 0.0, "avg_logprob": -0.19109031444287483, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.03478217124938965}, {"id": 97, "seek": 36304, "start": 378.92, "end": 381.52000000000004, "text": " the method and the path segments.", "tokens": [51158, 264, 3170, 293, 264, 3100, 19904, 13, 51288], "temperature": 0.0, "avg_logprob": -0.19109031444287483, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.03478217124938965}, {"id": 98, "seek": 36304, "start": 381.52000000000004, "end": 386.92, "text": " That way, if you wanted to, you could have a get request to get info or health or sort", "tokens": [51288, 663, 636, 11, 498, 291, 1415, 281, 11, 291, 727, 362, 257, 483, 5308, 281, 483, 13614, 420, 1585, 420, 1333, 51558], "temperature": 0.0, "avg_logprob": -0.19109031444287483, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.03478217124938965}, {"id": 99, "seek": 36304, "start": 386.92, "end": 388.88, "text": " of the UI as well be served.", "tokens": [51558, 295, 264, 15682, 382, 731, 312, 7584, 13, 51656], "temperature": 0.0, "avg_logprob": -0.19109031444287483, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.03478217124938965}, {"id": 100, "seek": 36304, "start": 388.88, "end": 391.92, "text": " But in this case, we're just going to say everything else isn't found except for a", "tokens": [51656, 583, 294, 341, 1389, 11, 321, 434, 445, 516, 281, 584, 1203, 1646, 1943, 380, 1352, 3993, 337, 257, 51808], "temperature": 0.0, "avg_logprob": -0.19109031444287483, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.03478217124938965}, {"id": 101, "seek": 39192, "start": 391.92, "end": 394.44, "text": " post request to slash GraphQL.", "tokens": [50364, 2183, 5308, 281, 17330, 21884, 13695, 13, 50490], "temperature": 0.0, "avg_logprob": -0.14523604764776715, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0033174133859574795}, {"id": 102, "seek": 39192, "start": 394.44, "end": 399.48, "text": " And then at the top, we have a use statement with web.middleware, which is something fairly", "tokens": [50490, 400, 550, 412, 264, 1192, 11, 321, 362, 257, 764, 5629, 365, 3670, 13, 25394, 2285, 3039, 11, 597, 307, 746, 6457, 50742], "temperature": 0.0, "avg_logprob": -0.14523604764776715, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0033174133859574795}, {"id": 103, "seek": 39192, "start": 399.48, "end": 401.6, "text": " new in Gleam but not super new.", "tokens": [50742, 777, 294, 460, 306, 335, 457, 406, 1687, 777, 13, 50848], "temperature": 0.0, "avg_logprob": -0.14523604764776715, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0033174133859574795}, {"id": 104, "seek": 39192, "start": 401.6, "end": 409.44, "text": " It's an abstraction for putting, for calling a function with a parameter passed into it.", "tokens": [50848, 467, 311, 364, 37765, 337, 3372, 11, 337, 5141, 257, 2445, 365, 257, 13075, 4678, 666, 309, 13, 51240], "temperature": 0.0, "avg_logprob": -0.14523604764776715, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0033174133859574795}, {"id": 105, "seek": 39192, "start": 409.44, "end": 413.40000000000003, "text": " Before this, Gleam code was sort of nested functions on nested functions, which maybe", "tokens": [51240, 4546, 341, 11, 460, 306, 335, 3089, 390, 1333, 295, 15646, 292, 6828, 322, 15646, 292, 6828, 11, 597, 1310, 51438], "temperature": 0.0, "avg_logprob": -0.14523604764776715, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0033174133859574795}, {"id": 106, "seek": 39192, "start": 413.40000000000003, "end": 415.04, "text": " wasn't the nicest to look at.", "tokens": [51438, 2067, 380, 264, 45516, 281, 574, 412, 13, 51520], "temperature": 0.0, "avg_logprob": -0.14523604764776715, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0033174133859574795}, {"id": 107, "seek": 39192, "start": 415.04, "end": 419.36, "text": " This is sort of simplified that for quite a few things and is now used across everything", "tokens": [51520, 639, 307, 1333, 295, 26335, 300, 337, 1596, 257, 1326, 721, 293, 307, 586, 1143, 2108, 1203, 51736], "temperature": 0.0, "avg_logprob": -0.14523604764776715, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0033174133859574795}, {"id": 108, "seek": 41936, "start": 419.36, "end": 425.16, "text": " from the standard library to libraries themselves to people's code.", "tokens": [50364, 490, 264, 3832, 6405, 281, 15148, 2969, 281, 561, 311, 3089, 13, 50654], "temperature": 0.0, "avg_logprob": -0.17411327362060547, "compression_ratio": 1.65625, "no_speech_prob": 0.24789005517959595}, {"id": 109, "seek": 41936, "start": 425.16, "end": 429.16, "text": " We have this GraphQL request function, which we actually need to do something with, but", "tokens": [50654, 492, 362, 341, 21884, 13695, 5308, 2445, 11, 597, 321, 767, 643, 281, 360, 746, 365, 11, 457, 50854], "temperature": 0.0, "avg_logprob": -0.17411327362060547, "compression_ratio": 1.65625, "no_speech_prob": 0.24789005517959595}, {"id": 110, "seek": 41936, "start": 429.16, "end": 434.8, "text": " for now, we're just returning the string GraphQL response and sending that with 200 status", "tokens": [50854, 337, 586, 11, 321, 434, 445, 12678, 264, 6798, 21884, 13695, 4134, 293, 7750, 300, 365, 2331, 6558, 51136], "temperature": 0.0, "avg_logprob": -0.17411327362060547, "compression_ratio": 1.65625, "no_speech_prob": 0.24789005517959595}, {"id": 111, "seek": 41936, "start": 434.8, "end": 435.8, "text": " code.", "tokens": [51136, 3089, 13, 51186], "temperature": 0.0, "avg_logprob": -0.17411327362060547, "compression_ratio": 1.65625, "no_speech_prob": 0.24789005517959595}, {"id": 112, "seek": 41936, "start": 435.8, "end": 441.36, "text": " So now if you were to send a post request to HTTP localhost 8000 slash GraphQL, you'd", "tokens": [51186, 407, 586, 498, 291, 645, 281, 2845, 257, 2183, 5308, 281, 33283, 2654, 6037, 1649, 1360, 17330, 21884, 13695, 11, 291, 1116, 51464], "temperature": 0.0, "avg_logprob": -0.17411327362060547, "compression_ratio": 1.65625, "no_speech_prob": 0.24789005517959595}, {"id": 113, "seek": 41936, "start": 441.36, "end": 444.44, "text": " just get GraphQL response sent back to you.", "tokens": [51464, 445, 483, 21884, 13695, 4134, 2279, 646, 281, 291, 13, 51618], "temperature": 0.0, "avg_logprob": -0.17411327362060547, "compression_ratio": 1.65625, "no_speech_prob": 0.24789005517959595}, {"id": 114, "seek": 41936, "start": 444.44, "end": 448.6, "text": " But you're thinking, that isn't what you came here to hear about, and it's absolutely useless.", "tokens": [51618, 583, 291, 434, 1953, 11, 300, 1943, 380, 437, 291, 1361, 510, 281, 1568, 466, 11, 293, 309, 311, 3122, 14115, 13, 51826], "temperature": 0.0, "avg_logprob": -0.17411327362060547, "compression_ratio": 1.65625, "no_speech_prob": 0.24789005517959595}, {"id": 115, "seek": 44860, "start": 448.64000000000004, "end": 449.64000000000004, "text": " I agree.", "tokens": [50366, 286, 3986, 13, 50416], "temperature": 0.0, "avg_logprob": -0.13823065051326044, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.034381456673145294}, {"id": 116, "seek": 44860, "start": 449.64000000000004, "end": 454.96000000000004, "text": " So let's go on to actually handling a GraphQL request and sending back some actually useful", "tokens": [50416, 407, 718, 311, 352, 322, 281, 767, 13175, 257, 21884, 13695, 5308, 293, 7750, 646, 512, 767, 4420, 50682], "temperature": 0.0, "avg_logprob": -0.13823065051326044, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.034381456673145294}, {"id": 117, "seek": 44860, "start": 454.96000000000004, "end": 458.04, "text": " data.", "tokens": [50682, 1412, 13, 50836], "temperature": 0.0, "avg_logprob": -0.13823065051326044, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.034381456673145294}, {"id": 118, "seek": 44860, "start": 458.04, "end": 463.20000000000005, "text": " When we get the request in from GraphQL, it has the JSON structure of the query, the", "tokens": [50836, 1133, 321, 483, 264, 5308, 294, 490, 21884, 13695, 11, 309, 575, 264, 31828, 3877, 295, 264, 14581, 11, 264, 51094], "temperature": 0.0, "avg_logprob": -0.13823065051326044, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.034381456673145294}, {"id": 119, "seek": 44860, "start": 463.20000000000005, "end": 466.28000000000003, "text": " operation name, and then variables.", "tokens": [51094, 6916, 1315, 11, 293, 550, 9102, 13, 51248], "temperature": 0.0, "avg_logprob": -0.13823065051326044, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.034381456673145294}, {"id": 120, "seek": 44860, "start": 466.28000000000003, "end": 470.48, "text": " The variables and operation name aren't important for this, but for a fully featured implementation", "tokens": [51248, 440, 9102, 293, 6916, 1315, 3212, 380, 1021, 337, 341, 11, 457, 337, 257, 4498, 13822, 11420, 51458], "temperature": 0.0, "avg_logprob": -0.13823065051326044, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.034381456673145294}, {"id": 121, "seek": 44860, "start": 470.48, "end": 471.48, "text": " they would be.", "tokens": [51458, 436, 576, 312, 13, 51508], "temperature": 0.0, "avg_logprob": -0.13823065051326044, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.034381456673145294}, {"id": 122, "seek": 44860, "start": 471.48, "end": 477.52000000000004, "text": " Let's say you had a query that had loads of different requests inside of it, operations.", "tokens": [51508, 961, 311, 584, 291, 632, 257, 14581, 300, 632, 12668, 295, 819, 12475, 1854, 295, 309, 11, 7705, 13, 51810], "temperature": 0.0, "avg_logprob": -0.13823065051326044, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.034381456673145294}, {"id": 123, "seek": 47752, "start": 477.52, "end": 481.35999999999996, "text": " You can then specify afterwards which one you want to use, and the variables that you", "tokens": [50364, 509, 393, 550, 16500, 10543, 597, 472, 291, 528, 281, 764, 11, 293, 264, 9102, 300, 291, 50556], "temperature": 0.0, "avg_logprob": -0.15898986963125375, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.001996614970266819}, {"id": 124, "seek": 47752, "start": 481.35999999999996, "end": 485.59999999999997, "text": " pass stuff into those operations after the fact.", "tokens": [50556, 1320, 1507, 666, 729, 7705, 934, 264, 1186, 13, 50768], "temperature": 0.0, "avg_logprob": -0.15898986963125375, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.001996614970266819}, {"id": 125, "seek": 47752, "start": 485.59999999999997, "end": 492.59999999999997, "text": " Here we're using Gleam's decoder to decode into three, well, to decode three things into", "tokens": [50768, 1692, 321, 434, 1228, 460, 306, 335, 311, 979, 19866, 281, 979, 1429, 666, 1045, 11, 731, 11, 281, 979, 1429, 1045, 721, 666, 51118], "temperature": 0.0, "avg_logprob": -0.15898986963125375, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.001996614970266819}, {"id": 126, "seek": 47752, "start": 492.59999999999997, "end": 494.79999999999995, "text": " a custom type.", "tokens": [51118, 257, 2375, 2010, 13, 51228], "temperature": 0.0, "avg_logprob": -0.15898986963125375, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.001996614970266819}, {"id": 127, "seek": 47752, "start": 494.79999999999995, "end": 498.84, "text": " The three doesn't mean anything like a tuple or an array like you might be used to.", "tokens": [51228, 440, 1045, 1177, 380, 914, 1340, 411, 257, 2604, 781, 420, 364, 10225, 411, 291, 1062, 312, 1143, 281, 13, 51430], "temperature": 0.0, "avg_logprob": -0.15898986963125375, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.001996614970266819}, {"id": 128, "seek": 47752, "start": 498.84, "end": 504.12, "text": " All it means is that the constructor of GraphQL full request has three arguments.", "tokens": [51430, 1057, 309, 1355, 307, 300, 264, 47479, 295, 21884, 13695, 1577, 5308, 575, 1045, 12869, 13, 51694], "temperature": 0.0, "avg_logprob": -0.15898986963125375, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.001996614970266819}, {"id": 129, "seek": 50412, "start": 504.12, "end": 510.12, "text": " So we're then saying the field query should be decoded as a string and put into the first", "tokens": [50364, 407, 321, 434, 550, 1566, 264, 2519, 14581, 820, 312, 979, 12340, 382, 257, 6798, 293, 829, 666, 264, 700, 50664], "temperature": 0.0, "avg_logprob": -0.16281475937157347, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.000700416334439069}, {"id": 130, "seek": 50412, "start": 510.12, "end": 511.64, "text": " field in the constructor.", "tokens": [50664, 2519, 294, 264, 47479, 13, 50740], "temperature": 0.0, "avg_logprob": -0.16281475937157347, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.000700416334439069}, {"id": 131, "seek": 50412, "start": 511.64, "end": 513.24, "text": " This is entirely type safe.", "tokens": [50740, 639, 307, 7696, 2010, 3273, 13, 50820], "temperature": 0.0, "avg_logprob": -0.16281475937157347, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.000700416334439069}, {"id": 132, "seek": 50412, "start": 513.24, "end": 517.76, "text": " I don't have anything on my slides about it, but let's say I jumbled up the order of these.", "tokens": [50820, 286, 500, 380, 362, 1340, 322, 452, 9788, 466, 309, 11, 457, 718, 311, 584, 286, 361, 19928, 493, 264, 1668, 295, 613, 13, 51046], "temperature": 0.0, "avg_logprob": -0.16281475937157347, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.000700416334439069}, {"id": 133, "seek": 50412, "start": 517.76, "end": 521.52, "text": " It wouldn't match what the constructor should be, and as such you'd get a compile time error", "tokens": [51046, 467, 2759, 380, 2995, 437, 264, 47479, 820, 312, 11, 293, 382, 1270, 291, 1116, 483, 257, 31413, 565, 6713, 51234], "temperature": 0.0, "avg_logprob": -0.16281475937157347, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.000700416334439069}, {"id": 134, "seek": 50412, "start": 521.52, "end": 528.72, "text": " rather than ending up at the end of this talk going, why is my query looking like an object", "tokens": [51234, 2831, 813, 8121, 493, 412, 264, 917, 295, 341, 751, 516, 11, 983, 307, 452, 14581, 1237, 411, 364, 2657, 51594], "temperature": 0.0, "avg_logprob": -0.16281475937157347, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.000700416334439069}, {"id": 135, "seek": 50412, "start": 528.72, "end": 529.88, "text": " of variables?", "tokens": [51594, 295, 9102, 30, 51652], "temperature": 0.0, "avg_logprob": -0.16281475937157347, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.000700416334439069}, {"id": 136, "seek": 52988, "start": 529.88, "end": 535.92, "text": " So I guess that's one point where Gleam's type safety comes back into being useful.", "tokens": [50364, 407, 286, 2041, 300, 311, 472, 935, 689, 460, 306, 335, 311, 2010, 4514, 1487, 646, 666, 885, 4420, 13, 50666], "temperature": 0.0, "avg_logprob": -0.11553219386509486, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.019921690225601196}, {"id": 137, "seek": 52988, "start": 535.92, "end": 540.8, "text": " The other part is that we can do something like this using the use statement again to", "tokens": [50666, 440, 661, 644, 307, 300, 321, 393, 360, 746, 411, 341, 1228, 264, 764, 5629, 797, 281, 50910], "temperature": 0.0, "avg_logprob": -0.11553219386509486, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.019921690225601196}, {"id": 138, "seek": 52988, "start": 540.8, "end": 543.48, "text": " say that the body has to be JSON.", "tokens": [50910, 584, 300, 264, 1772, 575, 281, 312, 31828, 13, 51044], "temperature": 0.0, "avg_logprob": -0.11553219386509486, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.019921690225601196}, {"id": 139, "seek": 52988, "start": 543.48, "end": 550.48, "text": " It's a case of we could handle all of this ourselves, but Wisp has a function for it.", "tokens": [51044, 467, 311, 257, 1389, 295, 321, 727, 4813, 439, 295, 341, 4175, 11, 457, 343, 7631, 575, 257, 2445, 337, 309, 13, 51394], "temperature": 0.0, "avg_logprob": -0.11553219386509486, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.019921690225601196}, {"id": 140, "seek": 52988, "start": 550.48, "end": 556.12, "text": " It requires that the content type's application JSON takes out the body and decodes it into", "tokens": [51394, 467, 7029, 300, 264, 2701, 2010, 311, 3861, 31828, 2516, 484, 264, 1772, 293, 979, 4789, 309, 666, 51676], "temperature": 0.0, "avg_logprob": -0.11553219386509486, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.019921690225601196}, {"id": 141, "seek": 55612, "start": 556.12, "end": 561.44, "text": " a dynamic which goes back into Gleam's dynamic system, which allows you to break out of that", "tokens": [50364, 257, 8546, 597, 1709, 646, 666, 460, 306, 335, 311, 8546, 1185, 11, 597, 4045, 291, 281, 1821, 484, 295, 300, 50630], "temperature": 0.0, "avg_logprob": -0.10643747117784289, "compression_ratio": 1.788104089219331, "no_speech_prob": 0.05581022426486015}, {"id": 142, "seek": 55612, "start": 561.44, "end": 566.12, "text": " box of type safety when you can't trust what you're getting in or you want to send something", "tokens": [50630, 2424, 295, 2010, 4514, 562, 291, 393, 380, 3361, 437, 291, 434, 1242, 294, 420, 291, 528, 281, 2845, 746, 50864], "temperature": 0.0, "avg_logprob": -0.10643747117784289, "compression_ratio": 1.788104089219331, "no_speech_prob": 0.05581022426486015}, {"id": 143, "seek": 55612, "start": 566.12, "end": 570.44, "text": " out in a way that's maybe less structured.", "tokens": [50864, 484, 294, 257, 636, 300, 311, 1310, 1570, 18519, 13, 51080], "temperature": 0.0, "avg_logprob": -0.10643747117784289, "compression_ratio": 1.788104089219331, "no_speech_prob": 0.05581022426486015}, {"id": 144, "seek": 55612, "start": 570.44, "end": 575.12, "text": " When we want to then decode it, we have to, if we want to work with it, we can't just", "tokens": [51080, 1133, 321, 528, 281, 550, 979, 1429, 309, 11, 321, 362, 281, 11, 498, 321, 528, 281, 589, 365, 309, 11, 321, 393, 380, 445, 51314], "temperature": 0.0, "avg_logprob": -0.10643747117784289, "compression_ratio": 1.788104089219331, "no_speech_prob": 0.05581022426486015}, {"id": 145, "seek": 55612, "start": 575.12, "end": 577.12, "text": " work with it in that dynamic form.", "tokens": [51314, 589, 365, 309, 294, 300, 8546, 1254, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10643747117784289, "compression_ratio": 1.788104089219331, "no_speech_prob": 0.05581022426486015}, {"id": 146, "seek": 55612, "start": 577.12, "end": 581.44, "text": " It's not how Gleam wants to work, and it's going to make you could, but it would make", "tokens": [51414, 467, 311, 406, 577, 460, 306, 335, 2738, 281, 589, 11, 293, 309, 311, 516, 281, 652, 291, 727, 11, 457, 309, 576, 652, 51630], "temperature": 0.0, "avg_logprob": -0.10643747117784289, "compression_ratio": 1.788104089219331, "no_speech_prob": 0.05581022426486015}, {"id": 147, "seek": 55612, "start": 581.44, "end": 584.6, "text": " your life so much harder than it needs to be.", "tokens": [51630, 428, 993, 370, 709, 6081, 813, 309, 2203, 281, 312, 13, 51788], "temperature": 0.0, "avg_logprob": -0.10643747117784289, "compression_ratio": 1.788104089219331, "no_speech_prob": 0.05581022426486015}, {"id": 148, "seek": 58460, "start": 584.6, "end": 588.76, "text": " In real life, you should also be using result.try, handling this nicely and bubbling errors", "tokens": [50364, 682, 957, 993, 11, 291, 820, 611, 312, 1228, 1874, 13, 83, 627, 11, 13175, 341, 9594, 293, 46360, 13603, 50572], "temperature": 0.0, "avg_logprob": -0.20921640014648438, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.4012971520423889}, {"id": 149, "seek": 58460, "start": 588.76, "end": 594.12, "text": " up, maybe with how Hailey spoke about where you could have your phantom types so you don't", "tokens": [50572, 493, 11, 1310, 365, 577, 4064, 12062, 7179, 466, 689, 291, 727, 362, 428, 903, 25796, 3467, 370, 291, 500, 380, 50840], "temperature": 0.0, "avg_logprob": -0.20921640014648438, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.4012971520423889}, {"id": 150, "seek": 58460, "start": 594.12, "end": 596.28, "text": " have to bubble it all the way back up.", "tokens": [50840, 362, 281, 12212, 309, 439, 264, 636, 646, 493, 13, 50948], "temperature": 0.0, "avg_logprob": -0.20921640014648438, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.4012971520423889}, {"id": 151, "seek": 58460, "start": 596.28, "end": 600.24, "text": " But for this short demonstration, I'm just unwrapping it, which will panic if somebody", "tokens": [50948, 583, 337, 341, 2099, 16520, 11, 286, 478, 445, 14853, 424, 3759, 309, 11, 597, 486, 14783, 498, 2618, 51146], "temperature": 0.0, "avg_logprob": -0.20921640014648438, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.4012971520423889}, {"id": 152, "seek": 58460, "start": 600.24, "end": 601.84, "text": " sends the wrong data.", "tokens": [51146, 14790, 264, 2085, 1412, 13, 51226], "temperature": 0.0, "avg_logprob": -0.20921640014648438, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.4012971520423889}, {"id": 153, "seek": 58460, "start": 601.84, "end": 607.4, "text": " I'm unwrapping it with what is a bog standard QL request, which is underscore underscore", "tokens": [51226, 286, 478, 14853, 424, 3759, 309, 365, 437, 307, 257, 26132, 3832, 1249, 43, 5308, 11, 597, 307, 37556, 37556, 51504], "temperature": 0.0, "avg_logprob": -0.20921640014648438, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.4012971520423889}, {"id": 154, "seek": 58460, "start": 607.4, "end": 611.1600000000001, "text": " type name, which just would in this case return query to you.", "tokens": [51504, 2010, 1315, 11, 597, 445, 576, 294, 341, 1389, 2736, 14581, 281, 291, 13, 51692], "temperature": 0.0, "avg_logprob": -0.20921640014648438, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.4012971520423889}, {"id": 155, "seek": 61116, "start": 611.16, "end": 616.8399999999999, "text": " It gives you nothing useful, but it means that the program isn't going to crash.", "tokens": [50364, 467, 2709, 291, 1825, 4420, 11, 457, 309, 1355, 300, 264, 1461, 1943, 380, 516, 281, 8252, 13, 50648], "temperature": 0.0, "avg_logprob": -0.12089962878469694, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.1516561210155487}, {"id": 156, "seek": 61116, "start": 616.8399999999999, "end": 622.0, "text": " Now that we have our body and we have the query inside of it, which we looked at earlier,", "tokens": [50648, 823, 300, 321, 362, 527, 1772, 293, 321, 362, 264, 14581, 1854, 295, 309, 11, 597, 321, 2956, 412, 3071, 11, 50906], "temperature": 0.0, "avg_logprob": -0.12089962878469694, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.1516561210155487}, {"id": 157, "seek": 61116, "start": 622.0, "end": 623.88, "text": " we need to do something with it.", "tokens": [50906, 321, 643, 281, 360, 746, 365, 309, 13, 51000], "temperature": 0.0, "avg_logprob": -0.12089962878469694, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.1516561210155487}, {"id": 158, "seek": 61116, "start": 623.88, "end": 628.9599999999999, "text": " So let's send it to that passing function that we saw at the very start and turn it", "tokens": [51000, 407, 718, 311, 2845, 309, 281, 300, 8437, 2445, 300, 321, 1866, 412, 264, 588, 722, 293, 1261, 309, 51254], "temperature": 0.0, "avg_logprob": -0.12089962878469694, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.1516561210155487}, {"id": 159, "seek": 61116, "start": 628.9599999999999, "end": 634.1999999999999, "text": " from just being a generic string into something that's actually tangible and could be used", "tokens": [51254, 490, 445, 885, 257, 19577, 6798, 666, 746, 300, 311, 767, 27094, 293, 727, 312, 1143, 51516], "temperature": 0.0, "avg_logprob": -0.12089962878469694, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.1516561210155487}, {"id": 160, "seek": 61116, "start": 634.1999999999999, "end": 636.64, "text": " by Gleam.", "tokens": [51516, 538, 460, 306, 335, 13, 51638], "temperature": 0.0, "avg_logprob": -0.12089962878469694, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.1516561210155487}, {"id": 161, "seek": 61116, "start": 636.64, "end": 639.8, "text": " But once we have that, how are we even going to resolve it?", "tokens": [51638, 583, 1564, 321, 362, 300, 11, 577, 366, 321, 754, 516, 281, 14151, 309, 30, 51796], "temperature": 0.0, "avg_logprob": -0.12089962878469694, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.1516561210155487}, {"id": 162, "seek": 63980, "start": 639.8, "end": 643.92, "text": " You have your query, you now have it as a Gleam type, but what does that even mean?", "tokens": [50364, 509, 362, 428, 14581, 11, 291, 586, 362, 309, 382, 257, 460, 306, 335, 2010, 11, 457, 437, 775, 300, 754, 914, 30, 50570], "temperature": 0.0, "avg_logprob": -0.13634655493816347, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.10902951657772064}, {"id": 163, "seek": 63980, "start": 643.92, "end": 647.7199999999999, "text": " You need to somehow get all of that speaker information back in a way that keeps your", "tokens": [50570, 509, 643, 281, 6063, 483, 439, 295, 300, 8145, 1589, 646, 294, 257, 636, 300, 5965, 428, 50760], "temperature": 0.0, "avg_logprob": -0.13634655493816347, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.10902951657772064}, {"id": 164, "seek": 63980, "start": 647.7199999999999, "end": 653.92, "text": " type safety, but also can be with the flexibility that GraphQL provides to people.", "tokens": [50760, 2010, 4514, 11, 457, 611, 393, 312, 365, 264, 12635, 300, 21884, 13695, 6417, 281, 561, 13, 51070], "temperature": 0.0, "avg_logprob": -0.13634655493816347, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.10902951657772064}, {"id": 165, "seek": 63980, "start": 653.92, "end": 655.7199999999999, "text": " So let's think about resolvers for a minute.", "tokens": [51070, 407, 718, 311, 519, 466, 7923, 840, 337, 257, 3456, 13, 51160], "temperature": 0.0, "avg_logprob": -0.13634655493816347, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.10902951657772064}, {"id": 166, "seek": 63980, "start": 655.7199999999999, "end": 660.9599999999999, "text": " We could go down this approach of having a type of the resolver where it has its key,", "tokens": [51160, 492, 727, 352, 760, 341, 3109, 295, 1419, 257, 2010, 295, 264, 34480, 689, 309, 575, 1080, 2141, 11, 51422], "temperature": 0.0, "avg_logprob": -0.13634655493816347, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.10902951657772064}, {"id": 167, "seek": 63980, "start": 660.9599999999999, "end": 666.1999999999999, "text": " for example, speaker.notes or just speaker, where you then have granularity over how far", "tokens": [51422, 337, 1365, 11, 8145, 13, 2247, 279, 420, 445, 8145, 11, 689, 291, 550, 362, 39962, 507, 670, 577, 1400, 51684], "temperature": 0.0, "avg_logprob": -0.13634655493816347, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.10902951657772064}, {"id": 168, "seek": 63980, "start": 666.1999999999999, "end": 668.0, "text": " down you want to resolve each time.", "tokens": [51684, 760, 291, 528, 281, 14151, 1184, 565, 13, 51774], "temperature": 0.0, "avg_logprob": -0.13634655493816347, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.10902951657772064}, {"id": 169, "seek": 66800, "start": 668.0, "end": 671.88, "text": " You want to resolve the whole object with one function or do you want an individual function", "tokens": [50364, 509, 528, 281, 14151, 264, 1379, 2657, 365, 472, 2445, 420, 360, 291, 528, 364, 2609, 2445, 50558], "temperature": 0.0, "avg_logprob": -0.13841915130615234, "compression_ratio": 1.712280701754386, "no_speech_prob": 0.16386443376541138}, {"id": 170, "seek": 66800, "start": 671.88, "end": 673.72, "text": " for resolving each field?", "tokens": [50558, 337, 49940, 1184, 2519, 30, 50650], "temperature": 0.0, "avg_logprob": -0.13841915130615234, "compression_ratio": 1.712280701754386, "no_speech_prob": 0.16386443376541138}, {"id": 171, "seek": 66800, "start": 673.72, "end": 677.56, "text": " And then you have that function there, which gets the request as well as the variables", "tokens": [50650, 400, 550, 291, 362, 300, 2445, 456, 11, 597, 2170, 264, 5308, 382, 731, 382, 264, 9102, 50842], "temperature": 0.0, "avg_logprob": -0.13841915130615234, "compression_ratio": 1.712280701754386, "no_speech_prob": 0.16386443376541138}, {"id": 172, "seek": 66800, "start": 677.56, "end": 682.88, "text": " and just returns a dynamic value, which can then be sent back to the client as Jason in", "tokens": [50842, 293, 445, 11247, 257, 8546, 2158, 11, 597, 393, 550, 312, 2279, 646, 281, 264, 6423, 382, 11181, 294, 51108], "temperature": 0.0, "avg_logprob": -0.13841915130615234, "compression_ratio": 1.712280701754386, "no_speech_prob": 0.16386443376541138}, {"id": 173, "seek": 66800, "start": 682.88, "end": 684.64, "text": " this case.", "tokens": [51108, 341, 1389, 13, 51196], "temperature": 0.0, "avg_logprob": -0.13841915130615234, "compression_ratio": 1.712280701754386, "no_speech_prob": 0.16386443376541138}, {"id": 174, "seek": 66800, "start": 684.64, "end": 689.08, "text": " Being dynamic there, it will be much nicer to use a generic, but in the process of writing", "tokens": [51196, 8891, 8546, 456, 11, 309, 486, 312, 709, 22842, 281, 764, 257, 19577, 11, 457, 294, 264, 1399, 295, 3579, 51418], "temperature": 0.0, "avg_logprob": -0.13841915130615234, "compression_ratio": 1.712280701754386, "no_speech_prob": 0.16386443376541138}, {"id": 175, "seek": 66800, "start": 689.08, "end": 693.8, "text": " this talk, I couldn't figure out a nice way to allow you to have loads of different generics", "tokens": [51418, 341, 751, 11, 286, 2809, 380, 2573, 484, 257, 1481, 636, 281, 2089, 291, 281, 362, 12668, 295, 819, 1337, 1167, 51654], "temperature": 0.0, "avg_logprob": -0.13841915130615234, "compression_ratio": 1.712280701754386, "no_speech_prob": 0.16386443376541138}, {"id": 176, "seek": 69380, "start": 693.8, "end": 698.7199999999999, "text": " at once, which I think is something that is going to be worked on in the future maybe", "tokens": [50364, 412, 1564, 11, 597, 286, 519, 307, 746, 300, 307, 516, 281, 312, 2732, 322, 294, 264, 2027, 1310, 50610], "temperature": 0.0, "avg_logprob": -0.18969782263831755, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.17617934942245483}, {"id": 177, "seek": 69380, "start": 698.7199999999999, "end": 703.64, "text": " so that you can have a collection of generics that also are maybe like an interface type", "tokens": [50610, 370, 300, 291, 393, 362, 257, 5765, 295, 1337, 1167, 300, 611, 366, 1310, 411, 364, 9226, 2010, 50856], "temperature": 0.0, "avg_logprob": -0.18969782263831755, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.17617934942245483}, {"id": 178, "seek": 69380, "start": 703.64, "end": 704.64, "text": " thing.", "tokens": [50856, 551, 13, 50906], "temperature": 0.0, "avg_logprob": -0.18969782263831755, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.17617934942245483}, {"id": 179, "seek": 69380, "start": 704.64, "end": 710.24, "text": " Here's an example of how this could look, forgetting the presentation's notes, but there", "tokens": [50906, 1692, 311, 364, 1365, 295, 577, 341, 727, 574, 11, 25428, 264, 5860, 311, 5570, 11, 457, 456, 51186], "temperature": 0.0, "avg_logprob": -0.18969782263831755, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.17617934942245483}, {"id": 180, "seek": 69380, "start": 710.24, "end": 712.56, "text": " is one key problem that comes with this.", "tokens": [51186, 307, 472, 2141, 1154, 300, 1487, 365, 341, 13, 51302], "temperature": 0.0, "avg_logprob": -0.18969782263831755, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.17617934942245483}, {"id": 181, "seek": 69380, "start": 712.56, "end": 714.9599999999999, "text": " Let's say you then have a list of these resolvers.", "tokens": [51302, 961, 311, 584, 291, 550, 362, 257, 1329, 295, 613, 7923, 840, 13, 51422], "temperature": 0.0, "avg_logprob": -0.18969782263831755, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.17617934942245483}, {"id": 182, "seek": 69380, "start": 714.9599999999999, "end": 720.88, "text": " How on earth do I find presentation.notes in a time that's actually suitable?", "tokens": [51422, 1012, 322, 4120, 360, 286, 915, 5860, 13, 2247, 279, 294, 257, 565, 300, 311, 767, 12873, 30, 51718], "temperature": 0.0, "avg_logprob": -0.18969782263831755, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.17617934942245483}, {"id": 183, "seek": 72088, "start": 720.88, "end": 724.8, "text": " The bigger notation for this is going to be o of n at best, or o of n at worst maybe,", "tokens": [50364, 440, 3801, 24657, 337, 341, 307, 516, 281, 312, 277, 295, 297, 412, 1151, 11, 420, 277, 295, 297, 412, 5855, 1310, 11, 50560], "temperature": 0.0, "avg_logprob": -0.19027653678518827, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.1084955632686615}, {"id": 184, "seek": 72088, "start": 724.8, "end": 729.36, "text": " because you're going to have to go through all of them checking each one.", "tokens": [50560, 570, 291, 434, 516, 281, 362, 281, 352, 807, 439, 295, 552, 8568, 1184, 472, 13, 50788], "temperature": 0.0, "avg_logprob": -0.19027653678518827, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.1084955632686615}, {"id": 185, "seek": 72088, "start": 729.36, "end": 735.04, "text": " So maps or lists, a hash map is going to be a thousand times better, maybe not actually", "tokens": [50788, 407, 11317, 420, 14511, 11, 257, 22019, 4471, 307, 516, 281, 312, 257, 4714, 1413, 1101, 11, 1310, 406, 767, 51072], "temperature": 0.0, "avg_logprob": -0.19027653678518827, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.1084955632686615}, {"id": 186, "seek": 72088, "start": 735.04, "end": 738.88, "text": " a thousand times, don't quote me on that, but it is going to be significantly better", "tokens": [51072, 257, 4714, 1413, 11, 500, 380, 6513, 385, 322, 300, 11, 457, 309, 307, 516, 281, 312, 10591, 1101, 51264], "temperature": 0.0, "avg_logprob": -0.19027653678518827, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.1084955632686615}, {"id": 187, "seek": 72088, "start": 738.88, "end": 742.12, "text": " than using this sort of list and sorting method.", "tokens": [51264, 813, 1228, 341, 1333, 295, 1329, 293, 32411, 3170, 13, 51426], "temperature": 0.0, "avg_logprob": -0.19027653678518827, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.1084955632686615}, {"id": 188, "seek": 72088, "start": 742.12, "end": 748.16, "text": " So let's switch it up and say that the resolver now isn't this custom type with a constructor", "tokens": [51426, 407, 718, 311, 3679, 309, 493, 293, 584, 300, 264, 34480, 586, 1943, 380, 341, 2375, 2010, 365, 257, 47479, 51728], "temperature": 0.0, "avg_logprob": -0.19027653678518827, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.1084955632686615}, {"id": 189, "seek": 74816, "start": 748.16, "end": 753.36, "text": " and everything, it's now just a function that takes in a request, has those arguments", "tokens": [50364, 293, 1203, 11, 309, 311, 586, 445, 257, 2445, 300, 2516, 294, 257, 5308, 11, 575, 729, 12869, 50624], "temperature": 0.0, "avg_logprob": -0.1530245000665838, "compression_ratio": 1.8159722222222223, "no_speech_prob": 0.1505635529756546}, {"id": 190, "seek": 74816, "start": 753.36, "end": 756.68, "text": " passed in and still returns the dynamic.", "tokens": [50624, 4678, 294, 293, 920, 11247, 264, 8546, 13, 50790], "temperature": 0.0, "avg_logprob": -0.1530245000665838, "compression_ratio": 1.8159722222222223, "no_speech_prob": 0.1505635529756546}, {"id": 191, "seek": 74816, "start": 756.68, "end": 760.5799999999999, "text": " And now we can use this in a much simpler way, where we have a simple function that has", "tokens": [50790, 400, 586, 321, 393, 764, 341, 294, 257, 709, 18587, 636, 11, 689, 321, 362, 257, 2199, 2445, 300, 575, 50985], "temperature": 0.0, "avg_logprob": -0.1530245000665838, "compression_ratio": 1.8159722222222223, "no_speech_prob": 0.1505635529756546}, {"id": 192, "seek": 74816, "start": 760.5799999999999, "end": 764.76, "text": " resolve where it takes in a prefix, which will make a lot more sense in a minute.", "tokens": [50985, 14151, 689, 309, 2516, 294, 257, 46969, 11, 597, 486, 652, 257, 688, 544, 2020, 294, 257, 3456, 13, 51194], "temperature": 0.0, "avg_logprob": -0.1530245000665838, "compression_ratio": 1.8159722222222223, "no_speech_prob": 0.1505635529756546}, {"id": 193, "seek": 74816, "start": 764.76, "end": 770.68, "text": " The HTTP request, the object itself we're trying to resolve, so we took the string,", "tokens": [51194, 440, 33283, 5308, 11, 264, 2657, 2564, 321, 434, 1382, 281, 14151, 11, 370, 321, 1890, 264, 6798, 11, 51490], "temperature": 0.0, "avg_logprob": -0.1530245000665838, "compression_ratio": 1.8159722222222223, "no_speech_prob": 0.1505635529756546}, {"id": 194, "seek": 74816, "start": 770.68, "end": 773.88, "text": " we pass it into this object and now we're going to resolve it.", "tokens": [51490, 321, 1320, 309, 666, 341, 2657, 293, 586, 321, 434, 516, 281, 14151, 309, 13, 51650], "temperature": 0.0, "avg_logprob": -0.1530245000665838, "compression_ratio": 1.8159722222222223, "no_speech_prob": 0.1505635529756546}, {"id": 195, "seek": 74816, "start": 773.88, "end": 777.8399999999999, "text": " And then a dictionary of all the resolvers that were created when you wrote it.", "tokens": [51650, 400, 550, 257, 25890, 295, 439, 264, 7923, 840, 300, 645, 2942, 562, 291, 4114, 309, 13, 51848], "temperature": 0.0, "avg_logprob": -0.1530245000665838, "compression_ratio": 1.8159722222222223, "no_speech_prob": 0.1505635529756546}, {"id": 196, "seek": 77784, "start": 777.84, "end": 781.72, "text": " So you'd have your dictionary where you put in each of your resolvers, so you could have", "tokens": [50364, 407, 291, 1116, 362, 428, 25890, 689, 291, 829, 294, 1184, 295, 428, 7923, 840, 11, 370, 291, 727, 362, 50558], "temperature": 0.0, "avg_logprob": -0.16740155653520064, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.008232837542891502}, {"id": 197, "seek": 77784, "start": 781.72, "end": 787.12, "text": " presentation.speakers, which resolves that whole array, as well as presentation.notes", "tokens": [50558, 5860, 13, 7053, 19552, 11, 597, 7923, 977, 300, 1379, 10225, 11, 382, 731, 382, 5860, 13, 2247, 279, 50828], "temperature": 0.0, "avg_logprob": -0.16740155653520064, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.008232837542891502}, {"id": 198, "seek": 77784, "start": 787.12, "end": 789.12, "text": " and any of the other fields that were there.", "tokens": [50828, 293, 604, 295, 264, 661, 7909, 300, 645, 456, 13, 50928], "temperature": 0.0, "avg_logprob": -0.16740155653520064, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.008232837542891502}, {"id": 199, "seek": 77784, "start": 789.12, "end": 793.9200000000001, "text": " Or you could even just have one resolver that resolves the entire presentation.", "tokens": [50928, 1610, 291, 727, 754, 445, 362, 472, 34480, 300, 7923, 977, 264, 2302, 5860, 13, 51168], "temperature": 0.0, "avg_logprob": -0.16740155653520064, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.008232837542891502}, {"id": 200, "seek": 77784, "start": 793.9200000000001, "end": 797.2800000000001, "text": " Example of this is here, where you have your dictionary.", "tokens": [51168, 24755, 781, 295, 341, 307, 510, 11, 689, 291, 362, 428, 25890, 13, 51336], "temperature": 0.0, "avg_logprob": -0.16740155653520064, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.008232837542891502}, {"id": 201, "seek": 77784, "start": 797.2800000000001, "end": 801.6, "text": " We're just creating the dictionary from a list for simplicity, but this is the same as doing", "tokens": [51336, 492, 434, 445, 4084, 264, 25890, 490, 257, 1329, 337, 25632, 11, 457, 341, 307, 264, 912, 382, 884, 51552], "temperature": 0.0, "avg_logprob": -0.16740155653520064, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.008232837542891502}, {"id": 202, "seek": 80160, "start": 801.6, "end": 808.28, "text": " dictionary new and then inserting an element with that key and that value.", "tokens": [50364, 25890, 777, 293, 550, 46567, 364, 4478, 365, 300, 2141, 293, 300, 2158, 13, 50698], "temperature": 0.0, "avg_logprob": -0.16806660668324616, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.17502200603485107}, {"id": 203, "seek": 80160, "start": 808.28, "end": 815.5600000000001, "text": " Now that we have it, we can take our prefix, prefix it to the object.name and then try", "tokens": [50698, 823, 300, 321, 362, 309, 11, 321, 393, 747, 527, 46969, 11, 46969, 309, 281, 264, 2657, 13, 16344, 293, 550, 853, 51062], "temperature": 0.0, "avg_logprob": -0.16806660668324616, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.17502200603485107}, {"id": 204, "seek": 80160, "start": 815.5600000000001, "end": 818.36, "text": " and see if we have that as a resolver.", "tokens": [51062, 293, 536, 498, 321, 362, 300, 382, 257, 34480, 13, 51202], "temperature": 0.0, "avg_logprob": -0.16806660668324616, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.17502200603485107}, {"id": 205, "seek": 80160, "start": 818.36, "end": 820.44, "text": " Of course you're thinking, how on earth is this going to work?", "tokens": [51202, 2720, 1164, 291, 434, 1953, 11, 577, 322, 4120, 307, 341, 516, 281, 589, 30, 51306], "temperature": 0.0, "avg_logprob": -0.16806660668324616, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.17502200603485107}, {"id": 206, "seek": 80160, "start": 820.44, "end": 824.0400000000001, "text": " Glean doesn't have an if statement, but we have something just as good, which is a case", "tokens": [51306, 460, 28499, 1177, 380, 362, 364, 498, 5629, 11, 457, 321, 362, 746, 445, 382, 665, 11, 597, 307, 257, 1389, 51486], "temperature": 0.0, "avg_logprob": -0.16806660668324616, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.17502200603485107}, {"id": 207, "seek": 80160, "start": 824.0400000000001, "end": 825.0400000000001, "text": " statement.", "tokens": [51486, 5629, 13, 51536], "temperature": 0.0, "avg_logprob": -0.16806660668324616, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.17502200603485107}, {"id": 208, "seek": 80160, "start": 825.0400000000001, "end": 829.96, "text": " We're going to say, if that resolver exists, try and just resolve using that.", "tokens": [51536, 492, 434, 516, 281, 584, 11, 498, 300, 34480, 8198, 11, 853, 293, 445, 14151, 1228, 300, 13, 51782], "temperature": 0.0, "avg_logprob": -0.16806660668324616, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.17502200603485107}, {"id": 209, "seek": 82996, "start": 829.96, "end": 833.76, "text": " If it doesn't exist, we're going to check that there's fields.", "tokens": [50364, 759, 309, 1177, 380, 2514, 11, 321, 434, 516, 281, 1520, 300, 456, 311, 7909, 13, 50554], "temperature": 0.0, "avg_logprob": -0.1599883099535962, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.14899323880672455}, {"id": 210, "seek": 82996, "start": 833.76, "end": 838.2800000000001, "text": " If there aren't fields, then we need to somehow handle this much nicer at the moment.", "tokens": [50554, 759, 456, 3212, 380, 7909, 11, 550, 321, 643, 281, 6063, 4813, 341, 709, 22842, 412, 264, 1623, 13, 50780], "temperature": 0.0, "avg_logprob": -0.1599883099535962, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.14899323880672455}, {"id": 211, "seek": 82996, "start": 838.2800000000001, "end": 840.24, "text": " It just returns that there was no resolver set.", "tokens": [50780, 467, 445, 11247, 300, 456, 390, 572, 34480, 992, 13, 50878], "temperature": 0.0, "avg_logprob": -0.1599883099535962, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.14899323880672455}, {"id": 212, "seek": 82996, "start": 840.24, "end": 845.24, "text": " Then in reality, it should be erroring out properly and returning an error in the GraphQL", "tokens": [50878, 1396, 294, 4103, 11, 309, 820, 312, 6713, 278, 484, 6108, 293, 12678, 364, 6713, 294, 264, 21884, 13695, 51128], "temperature": 0.0, "avg_logprob": -0.1599883099535962, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.14899323880672455}, {"id": 213, "seek": 82996, "start": 845.24, "end": 846.48, "text": " standard way.", "tokens": [51128, 3832, 636, 13, 51190], "temperature": 0.0, "avg_logprob": -0.1599883099535962, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.14899323880672455}, {"id": 214, "seek": 82996, "start": 846.48, "end": 850.96, "text": " And if there are fields, we then map each one, attempt to resolve it, and then we have this", "tokens": [51190, 400, 498, 456, 366, 7909, 11, 321, 550, 4471, 1184, 472, 11, 5217, 281, 14151, 309, 11, 293, 550, 321, 362, 341, 51414], "temperature": 0.0, "avg_logprob": -0.1599883099535962, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.14899323880672455}, {"id": 215, "seek": 82996, "start": 850.96, "end": 855.24, "text": " function at the bottom called combine results, which I'll come back to in a second.", "tokens": [51414, 2445, 412, 264, 2767, 1219, 10432, 3542, 11, 597, 286, 603, 808, 646, 281, 294, 257, 1150, 13, 51628], "temperature": 0.0, "avg_logprob": -0.1599883099535962, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.14899323880672455}, {"id": 216, "seek": 82996, "start": 855.24, "end": 859.8000000000001, "text": " But you can see that the prefix actually ends up being the prefix that was passed in before", "tokens": [51628, 583, 291, 393, 536, 300, 264, 46969, 767, 5314, 493, 885, 264, 46969, 300, 390, 4678, 294, 949, 51856], "temperature": 0.0, "avg_logprob": -0.1599883099535962, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.14899323880672455}, {"id": 217, "seek": 85980, "start": 860.56, "end": 862.3199999999999, "text": " and then the object's name then a dot.", "tokens": [50402, 293, 550, 264, 2657, 311, 1315, 550, 257, 5893, 13, 50490], "temperature": 0.0, "avg_logprob": -0.18203305361563699, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.055036548525094986}, {"id": 218, "seek": 85980, "start": 862.3199999999999, "end": 865.8, "text": " So as you go further down and into it, you get the dots.", "tokens": [50490, 407, 382, 291, 352, 3052, 760, 293, 666, 309, 11, 291, 483, 264, 15026, 13, 50664], "temperature": 0.0, "avg_logprob": -0.18203305361563699, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.055036548525094986}, {"id": 219, "seek": 85980, "start": 865.8, "end": 868.92, "text": " You still get the objects that were before added in.", "tokens": [50664, 509, 920, 483, 264, 6565, 300, 645, 949, 3869, 294, 13, 50820], "temperature": 0.0, "avg_logprob": -0.18203305361563699, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.055036548525094986}, {"id": 220, "seek": 85980, "start": 868.92, "end": 873.28, "text": " And then you can have your granularity that I've been speaking about of resolvers.", "tokens": [50820, 400, 550, 291, 393, 362, 428, 39962, 507, 300, 286, 600, 668, 4124, 466, 295, 7923, 840, 13, 51038], "temperature": 0.0, "avg_logprob": -0.18203305361563699, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.055036548525094986}, {"id": 221, "seek": 85980, "start": 873.28, "end": 877.24, "text": " The combine results just simply has the list of them piped in.", "tokens": [51038, 440, 10432, 3542, 445, 2935, 575, 264, 1329, 295, 552, 8489, 292, 294, 13, 51236], "temperature": 0.0, "avg_logprob": -0.18203305361563699, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.055036548525094986}, {"id": 222, "seek": 85980, "start": 877.24, "end": 882.28, "text": " It then folds the list into a dictionary and then it does dynamic.from to just turn that", "tokens": [51236, 467, 550, 31341, 264, 1329, 666, 257, 25890, 293, 550, 309, 775, 8546, 13, 20579, 281, 445, 1261, 300, 51488], "temperature": 0.0, "avg_logprob": -0.18203305361563699, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.055036548525094986}, {"id": 223, "seek": 85980, "start": 882.28, "end": 886.4, "text": " into that dynamic value to be returned at the end.", "tokens": [51488, 666, 300, 8546, 2158, 281, 312, 8752, 412, 264, 917, 13, 51694], "temperature": 0.0, "avg_logprob": -0.18203305361563699, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.055036548525094986}, {"id": 224, "seek": 88640, "start": 886.4399999999999, "end": 890.72, "text": " So finally, I know it's been a lot, but let's finally put it all together into something", "tokens": [50366, 407, 2721, 11, 286, 458, 309, 311, 668, 257, 688, 11, 457, 718, 311, 2721, 829, 309, 439, 1214, 666, 746, 50580], "temperature": 0.0, "avg_logprob": -0.17123867005340812, "compression_ratio": 1.733788395904437, "no_speech_prob": 0.004786329343914986}, {"id": 225, "seek": 88640, "start": 890.72, "end": 891.72, "text": " actually useful.", "tokens": [50580, 767, 4420, 13, 50630], "temperature": 0.0, "avg_logprob": -0.17123867005340812, "compression_ratio": 1.733788395904437, "no_speech_prob": 0.004786329343914986}, {"id": 226, "seek": 88640, "start": 891.72, "end": 895.4, "text": " First, we need to resolve multiple things.", "tokens": [50630, 2386, 11, 321, 643, 281, 14151, 3866, 721, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17123867005340812, "compression_ratio": 1.733788395904437, "no_speech_prob": 0.004786329343914986}, {"id": 227, "seek": 88640, "start": 895.4, "end": 899.64, "text": " When we got our request in, we could be requesting the presentation and a speaker separately", "tokens": [50814, 1133, 321, 658, 527, 5308, 294, 11, 321, 727, 312, 31937, 264, 5860, 293, 257, 8145, 14759, 51026], "temperature": 0.0, "avg_logprob": -0.17123867005340812, "compression_ratio": 1.733788395904437, "no_speech_prob": 0.004786329343914986}, {"id": 228, "seek": 88640, "start": 899.64, "end": 901.6, "text": " and maybe an event separately.", "tokens": [51026, 293, 1310, 364, 2280, 14759, 13, 51124], "temperature": 0.0, "avg_logprob": -0.17123867005340812, "compression_ratio": 1.733788395904437, "no_speech_prob": 0.004786329343914986}, {"id": 229, "seek": 88640, "start": 901.6, "end": 905.4, "text": " So we need to take each of those objects, go through the resolution process for them", "tokens": [51124, 407, 321, 643, 281, 747, 1184, 295, 729, 6565, 11, 352, 807, 264, 8669, 1399, 337, 552, 51314], "temperature": 0.0, "avg_logprob": -0.17123867005340812, "compression_ratio": 1.733788395904437, "no_speech_prob": 0.004786329343914986}, {"id": 230, "seek": 88640, "start": 905.4, "end": 907.12, "text": " and get the values.", "tokens": [51314, 293, 483, 264, 4190, 13, 51400], "temperature": 0.0, "avg_logprob": -0.17123867005340812, "compression_ratio": 1.733788395904437, "no_speech_prob": 0.004786329343914986}, {"id": 231, "seek": 88640, "start": 907.12, "end": 909.64, "text": " So to do that, we call that function.", "tokens": [51400, 407, 281, 360, 300, 11, 321, 818, 300, 2445, 13, 51526], "temperature": 0.0, "avg_logprob": -0.17123867005340812, "compression_ratio": 1.733788395904437, "no_speech_prob": 0.004786329343914986}, {"id": 232, "seek": 88640, "start": 909.64, "end": 914.76, "text": " Then we do, we say the responses after we've resolved them all, passing in the HTTP request,", "tokens": [51526, 1396, 321, 360, 11, 321, 584, 264, 13019, 934, 321, 600, 20772, 552, 439, 11, 8437, 294, 264, 33283, 5308, 11, 51782], "temperature": 0.0, "avg_logprob": -0.17123867005340812, "compression_ratio": 1.733788395904437, "no_speech_prob": 0.004786329343914986}, {"id": 233, "seek": 91476, "start": 914.76, "end": 918.4399999999999, "text": " the past query, and your set of resolvers.", "tokens": [50364, 264, 1791, 14581, 11, 293, 428, 992, 295, 7923, 840, 13, 50548], "temperature": 0.0, "avg_logprob": -0.23479497198965035, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.01146148331463337}, {"id": 234, "seek": 91476, "start": 918.4399999999999, "end": 922.72, "text": " And then this is what you finally end up with, the basic GraphQL request function in the", "tokens": [50548, 400, 550, 341, 307, 437, 291, 2721, 917, 493, 365, 11, 264, 3875, 21884, 13695, 5308, 2445, 294, 264, 50762], "temperature": 0.0, "avg_logprob": -0.23479497198965035, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.01146148331463337}, {"id": 235, "seek": 91476, "start": 922.72, "end": 924.64, "text": " start that just returned a string.", "tokens": [50762, 722, 300, 445, 8752, 257, 6798, 13, 50858], "temperature": 0.0, "avg_logprob": -0.23479497198965035, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.01146148331463337}, {"id": 236, "seek": 91476, "start": 924.64, "end": 930.88, "text": " Now it requires the JSON, unwraps the body, passes it, resolves everything, and then finally", "tokens": [50858, 823, 309, 7029, 264, 31828, 11, 14853, 424, 1878, 264, 1772, 11, 11335, 309, 11, 7923, 977, 1203, 11, 293, 550, 2721, 51170], "temperature": 0.0, "avg_logprob": -0.23479497198965035, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.01146148331463337}, {"id": 237, "seek": 91476, "start": 930.88, "end": 936.12, "text": " sends it as a HTML response, which probably should have been changed to JSON response.", "tokens": [51170, 14790, 309, 382, 257, 17995, 4134, 11, 597, 1391, 820, 362, 668, 3105, 281, 31828, 4134, 13, 51432], "temperature": 0.0, "avg_logprob": -0.23479497198965035, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.01146148331463337}, {"id": 238, "seek": 91476, "start": 936.12, "end": 941.92, "text": " Small details there, and sends that back to the client.", "tokens": [51432, 15287, 4365, 456, 11, 293, 14790, 300, 646, 281, 264, 6423, 13, 51722], "temperature": 0.0, "avg_logprob": -0.23479497198965035, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.01146148331463337}, {"id": 239, "seek": 94192, "start": 941.92, "end": 945.36, "text": " So yay, you might think we're finally finished.", "tokens": [50364, 407, 23986, 11, 291, 1062, 519, 321, 434, 2721, 4335, 13, 50536], "temperature": 0.0, "avg_logprob": -0.13968692016601564, "compression_ratio": 1.7281879194630871, "no_speech_prob": 0.032698944211006165}, {"id": 240, "seek": 94192, "start": 945.36, "end": 950.04, "text": " But in reality, we're actually missing so much from GraphQL that I wouldn't have even", "tokens": [50536, 583, 294, 4103, 11, 321, 434, 767, 5361, 370, 709, 490, 21884, 13695, 300, 286, 2759, 380, 362, 754, 50770], "temperature": 0.0, "avg_logprob": -0.13968692016601564, "compression_ratio": 1.7281879194630871, "no_speech_prob": 0.032698944211006165}, {"id": 241, "seek": 94192, "start": 950.04, "end": 955.76, "text": " had time to discuss or make for this talk, such as proper error handling.", "tokens": [50770, 632, 565, 281, 2248, 420, 652, 337, 341, 751, 11, 1270, 382, 2296, 6713, 13175, 13, 51056], "temperature": 0.0, "avg_logprob": -0.13968692016601564, "compression_ratio": 1.7281879194630871, "no_speech_prob": 0.032698944211006165}, {"id": 242, "seek": 94192, "start": 955.76, "end": 960.64, "text": " When you return data from GraphQL, you should return data and errors separately.", "tokens": [51056, 1133, 291, 2736, 1412, 490, 21884, 13695, 11, 291, 820, 2736, 1412, 293, 13603, 14759, 13, 51300], "temperature": 0.0, "avg_logprob": -0.13968692016601564, "compression_ratio": 1.7281879194630871, "no_speech_prob": 0.032698944211006165}, {"id": 243, "seek": 94192, "start": 960.64, "end": 964.0, "text": " We don't even have the concept of an error here.", "tokens": [51300, 492, 500, 380, 754, 362, 264, 3410, 295, 364, 6713, 510, 13, 51468], "temperature": 0.0, "avg_logprob": -0.13968692016601564, "compression_ratio": 1.7281879194630871, "no_speech_prob": 0.032698944211006165}, {"id": 244, "seek": 94192, "start": 964.0, "end": 968.04, "text": " Mutations at the moment, a mutation in the query, I just treated as the exact same thing.", "tokens": [51468, 18517, 763, 412, 264, 1623, 11, 257, 27960, 294, 264, 14581, 11, 286, 445, 8668, 382, 264, 1900, 912, 551, 13, 51670], "temperature": 0.0, "avg_logprob": -0.13968692016601564, "compression_ratio": 1.7281879194630871, "no_speech_prob": 0.032698944211006165}, {"id": 245, "seek": 94192, "start": 968.04, "end": 971.16, "text": " We should really have them as separate things that are resolved differently and handled", "tokens": [51670, 492, 820, 534, 362, 552, 382, 4994, 721, 300, 366, 20772, 7614, 293, 18033, 51826], "temperature": 0.0, "avg_logprob": -0.13968692016601564, "compression_ratio": 1.7281879194630871, "no_speech_prob": 0.032698944211006165}, {"id": 246, "seek": 97116, "start": 971.16, "end": 972.16, "text": " differently.", "tokens": [50364, 7614, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1998681376751204, "compression_ratio": 1.6182432432432432, "no_speech_prob": 0.05519239231944084}, {"id": 247, "seek": 97116, "start": 972.16, "end": 978.6, "text": " Subscriptions as well, that's using web sockets, and that's a whole nother layer of GraphQL", "tokens": [50414, 37471, 34173, 382, 731, 11, 300, 311, 1228, 3670, 370, 11984, 11, 293, 300, 311, 257, 1379, 406, 511, 4583, 295, 21884, 13695, 50736], "temperature": 0.0, "avg_logprob": -0.1998681376751204, "compression_ratio": 1.6182432432432432, "no_speech_prob": 0.05519239231944084}, {"id": 248, "seek": 97116, "start": 978.6, "end": 983.24, "text": " that people, there's fairly, I guess, divisive opinions on how it should be implemented.", "tokens": [50736, 300, 561, 11, 456, 311, 6457, 11, 286, 2041, 11, 25974, 488, 11819, 322, 577, 309, 820, 312, 12270, 13, 50968], "temperature": 0.0, "avg_logprob": -0.1998681376751204, "compression_ratio": 1.6182432432432432, "no_speech_prob": 0.05519239231944084}, {"id": 249, "seek": 97116, "start": 983.24, "end": 985.0, "text": " So I didn't even touch that.", "tokens": [50968, 407, 286, 994, 380, 754, 2557, 300, 13, 51056], "temperature": 0.0, "avg_logprob": -0.1998681376751204, "compression_ratio": 1.6182432432432432, "no_speech_prob": 0.05519239231944084}, {"id": 250, "seek": 97116, "start": 985.0, "end": 989.24, "text": " But you might be thinking, overall, that's an absolute ton of work if you want to implement", "tokens": [51056, 583, 291, 1062, 312, 1953, 11, 4787, 11, 300, 311, 364, 8236, 2952, 295, 589, 498, 291, 528, 281, 4445, 51268], "temperature": 0.0, "avg_logprob": -0.1998681376751204, "compression_ratio": 1.6182432432432432, "no_speech_prob": 0.05519239231944084}, {"id": 251, "seek": 97116, "start": 989.24, "end": 990.54, "text": " it yourself.", "tokens": [51268, 309, 1803, 13, 51333], "temperature": 0.0, "avg_logprob": -0.1998681376751204, "compression_ratio": 1.6182432432432432, "no_speech_prob": 0.05519239231944084}, {"id": 252, "seek": 97116, "start": 990.54, "end": 993.6, "text": " So the goal with all of this is, let's make a package out of it.", "tokens": [51333, 407, 264, 3387, 365, 439, 295, 341, 307, 11, 718, 311, 652, 257, 7372, 484, 295, 309, 13, 51486], "temperature": 0.0, "avg_logprob": -0.1998681376751204, "compression_ratio": 1.6182432432432432, "no_speech_prob": 0.05519239231944084}, {"id": 253, "seek": 97116, "start": 993.6, "end": 997.16, "text": " We'll have a green GraphQL package that you can just plug in, pass your resolvers, and", "tokens": [51486, 492, 603, 362, 257, 3092, 21884, 13695, 7372, 300, 291, 393, 445, 5452, 294, 11, 1320, 428, 7923, 840, 11, 293, 51664], "temperature": 0.0, "avg_logprob": -0.1998681376751204, "compression_ratio": 1.6182432432432432, "no_speech_prob": 0.05519239231944084}, {"id": 254, "seek": 99716, "start": 997.16, "end": 1004.36, "text": " it will pass and manage all of the query inside for you, similar to what Elixir has for Phoenix,", "tokens": [50364, 309, 486, 1320, 293, 3067, 439, 295, 264, 14581, 1854, 337, 291, 11, 2531, 281, 437, 2699, 970, 347, 575, 337, 18383, 11, 50724], "temperature": 0.0, "avg_logprob": -0.20091404831200316, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.2946130931377411}, {"id": 255, "seek": 99716, "start": 1004.36, "end": 1009.28, "text": " as well as what Laval and the other major frameworks have, which makes it easier, which", "tokens": [50724, 382, 731, 382, 437, 30966, 304, 293, 264, 661, 2563, 29834, 362, 11, 597, 1669, 309, 3571, 11, 597, 50970], "temperature": 0.0, "avg_logprob": -0.20091404831200316, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.2946130931377411}, {"id": 256, "seek": 99716, "start": 1009.28, "end": 1014.12, "text": " makes it super easy to use GraphQL and sometimes even easier than REST itself as it handles", "tokens": [50970, 1669, 309, 1687, 1858, 281, 764, 21884, 13695, 293, 2171, 754, 3571, 813, 497, 14497, 2564, 382, 309, 18722, 51212], "temperature": 0.0, "avg_logprob": -0.20091404831200316, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.2946130931377411}, {"id": 257, "seek": 99716, "start": 1014.12, "end": 1016.48, "text": " most of that abstraction for you.", "tokens": [51212, 881, 295, 300, 37765, 337, 291, 13, 51330], "temperature": 0.0, "avg_logprob": -0.20091404831200316, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.2946130931377411}, {"id": 258, "seek": 99716, "start": 1016.48, "end": 1020.8399999999999, "text": " So thank you so much for listening as I ran through that really quickly.", "tokens": [51330, 407, 1309, 291, 370, 709, 337, 4764, 382, 286, 5872, 807, 300, 534, 2661, 13, 51548], "temperature": 0.0, "avg_logprob": -0.20091404831200316, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.2946130931377411}, {"id": 259, "seek": 99716, "start": 1020.8399999999999, "end": 1024.52, "text": " And if anybody has any questions, I think I have just enough time for a couple.", "tokens": [51548, 400, 498, 4472, 575, 604, 1651, 11, 286, 519, 286, 362, 445, 1547, 565, 337, 257, 1916, 13, 51732], "temperature": 0.0, "avg_logprob": -0.20091404831200316, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.2946130931377411}, {"id": 260, "seek": 102716, "start": 1027.16, "end": 1029.16, "text": " Any questions?", "tokens": [50364, 2639, 1651, 30, 50464], "temperature": 0.0, "avg_logprob": -0.45560108987908615, "compression_ratio": 1.53475935828877, "no_speech_prob": 0.1541488617658615}, {"id": 261, "seek": 102716, "start": 1029.16, "end": 1034.16, "text": " Thank you very much.", "tokens": [50464, 1044, 291, 588, 709, 13, 50714], "temperature": 0.0, "avg_logprob": -0.45560108987908615, "compression_ratio": 1.53475935828877, "no_speech_prob": 0.1541488617658615}, {"id": 262, "seek": 102716, "start": 1034.16, "end": 1047.16, "text": " So I know that GraphQL also has a schema, and I'm guessing from that API that you would", "tokens": [50714, 407, 286, 458, 300, 21884, 13695, 611, 575, 257, 34078, 11, 293, 286, 478, 17939, 490, 300, 9362, 300, 291, 576, 51364], "temperature": 0.0, "avg_logprob": -0.45560108987908615, "compression_ratio": 1.53475935828877, "no_speech_prob": 0.1541488617658615}, {"id": 263, "seek": 102716, "start": 1047.16, "end": 1049.16, "text": " write your schema alongside that.", "tokens": [51364, 2464, 428, 34078, 12385, 300, 13, 51464], "temperature": 0.0, "avg_logprob": -0.45560108987908615, "compression_ratio": 1.53475935828877, "no_speech_prob": 0.1541488617658615}, {"id": 264, "seek": 102716, "start": 1049.16, "end": 1053.16, "text": " Do you have any thoughts on maybe generating some of that blue code from the schema, or", "tokens": [51464, 1144, 291, 362, 604, 4598, 322, 1310, 17746, 512, 295, 300, 3344, 3089, 490, 264, 34078, 11, 420, 51664], "temperature": 0.0, "avg_logprob": -0.45560108987908615, "compression_ratio": 1.53475935828877, "no_speech_prob": 0.1541488617658615}, {"id": 265, "seek": 102716, "start": 1053.16, "end": 1056.16, "text": " maybe just the schema from the blue code?", "tokens": [51664, 1310, 445, 264, 34078, 490, 264, 3344, 3089, 30, 51814], "temperature": 0.0, "avg_logprob": -0.45560108987908615, "compression_ratio": 1.53475935828877, "no_speech_prob": 0.1541488617658615}, {"id": 266, "seek": 105616, "start": 1056.16, "end": 1063.16, "text": " Yeah, when I first started, I thought, oh, the question was before I go, GraphQL has", "tokens": [50364, 865, 11, 562, 286, 700, 1409, 11, 286, 1194, 11, 1954, 11, 264, 1168, 390, 949, 286, 352, 11, 21884, 13695, 575, 50714], "temperature": 0.0, "avg_logprob": -0.12182407559088941, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.02041631005704403}, {"id": 267, "seek": 105616, "start": 1063.16, "end": 1069.16, "text": " schemas, so is there any way for us to generate that code, either the code that I showed today", "tokens": [50714, 22627, 296, 11, 370, 307, 456, 604, 636, 337, 505, 281, 8460, 300, 3089, 11, 2139, 264, 3089, 300, 286, 4712, 965, 51014], "temperature": 0.0, "avg_logprob": -0.12182407559088941, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.02041631005704403}, {"id": 268, "seek": 105616, "start": 1069.16, "end": 1071.16, "text": " or even the schema itself from Glean?", "tokens": [51014, 420, 754, 264, 34078, 2564, 490, 460, 28499, 30, 51114], "temperature": 0.0, "avg_logprob": -0.12182407559088941, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.02041631005704403}, {"id": 269, "seek": 105616, "start": 1071.16, "end": 1076.16, "text": " And the answer to that is I looked at it when I started, and you could generate the schema", "tokens": [51114, 400, 264, 1867, 281, 300, 307, 286, 2956, 412, 309, 562, 286, 1409, 11, 293, 291, 727, 8460, 264, 34078, 51364], "temperature": 0.0, "avg_logprob": -0.12182407559088941, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.02041631005704403}, {"id": 270, "seek": 105616, "start": 1076.16, "end": 1082.16, "text": " from Glean and as such interpret some of this code, but long term I think the best bet would", "tokens": [51364, 490, 460, 28499, 293, 382, 1270, 7302, 512, 295, 341, 3089, 11, 457, 938, 1433, 286, 519, 264, 1151, 778, 576, 51664], "temperature": 0.0, "avg_logprob": -0.12182407559088941, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.02041631005704403}, {"id": 271, "seek": 108216, "start": 1082.16, "end": 1086.16, "text": " be sort of code generation similar to how it's done in JavaScript or TypeScript, where", "tokens": [50364, 312, 1333, 295, 3089, 5125, 2531, 281, 577, 309, 311, 1096, 294, 15778, 420, 15576, 14237, 11, 689, 50564], "temperature": 0.0, "avg_logprob": -0.11509645710820737, "compression_ratio": 1.683127572016461, "no_speech_prob": 0.13746514916419983}, {"id": 272, "seek": 108216, "start": 1086.16, "end": 1092.16, "text": " you either write your schema and then it generates some of the other Glean for you as well as", "tokens": [50564, 291, 2139, 2464, 428, 34078, 293, 550, 309, 23815, 512, 295, 264, 661, 460, 28499, 337, 291, 382, 731, 382, 50864], "temperature": 0.0, "avg_logprob": -0.11509645710820737, "compression_ratio": 1.683127572016461, "no_speech_prob": 0.13746514916419983}, {"id": 273, "seek": 108216, "start": 1092.16, "end": 1099.16, "text": " a .graphql file, or you pass in a .graphql file to sort of a CLI, and it then spits out", "tokens": [50864, 257, 2411, 34091, 80, 75, 3991, 11, 420, 291, 1320, 294, 257, 2411, 34091, 80, 75, 3991, 281, 1333, 295, 257, 12855, 40, 11, 293, 309, 550, 637, 1208, 484, 51214], "temperature": 0.0, "avg_logprob": -0.11509645710820737, "compression_ratio": 1.683127572016461, "no_speech_prob": 0.13746514916419983}, {"id": 274, "seek": 108216, "start": 1099.16, "end": 1101.16, "text": " all of the boilerplate Glean you need.", "tokens": [51214, 439, 295, 264, 39228, 37008, 460, 28499, 291, 643, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11509645710820737, "compression_ratio": 1.683127572016461, "no_speech_prob": 0.13746514916419983}, {"id": 275, "seek": 108216, "start": 1101.16, "end": 1106.16, "text": " But yeah, schema validation is something else that sort of, I didn't really have time to", "tokens": [51314, 583, 1338, 11, 34078, 24071, 307, 746, 1646, 300, 1333, 295, 11, 286, 994, 380, 534, 362, 565, 281, 51564], "temperature": 0.0, "avg_logprob": -0.11509645710820737, "compression_ratio": 1.683127572016461, "no_speech_prob": 0.13746514916419983}, {"id": 276, "seek": 108216, "start": 1106.16, "end": 1109.16, "text": " do for this.", "tokens": [51564, 360, 337, 341, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11509645710820737, "compression_ratio": 1.683127572016461, "no_speech_prob": 0.13746514916419983}, {"id": 277, "seek": 110916, "start": 1109.16, "end": 1113.16, "text": " Any other questions?", "tokens": [50364, 2639, 661, 1651, 30, 50564], "temperature": 0.0, "avg_logprob": -0.24349881807963053, "compression_ratio": 0.875, "no_speech_prob": 0.03988473117351532}, {"id": 278, "seek": 110916, "start": 1113.16, "end": 1115.16, "text": " Okay, thank you then.", "tokens": [50564, 1033, 11, 1309, 291, 550, 13, 50664], "temperature": 0.0, "avg_logprob": -0.24349881807963053, "compression_ratio": 0.875, "no_speech_prob": 0.03988473117351532}], "language": "en"}