{"text": " We'll get started. Sylvester will introduce us to Piepart MC. Thank you for coming. I'm Sylvester Arrabas. I work at the AGH University in Krak\u00f3w in Poland. And this is a project carried out together with a team from the University of Illinois, Urbana-Champaign in US. So Piepart MC is the highlight here. But from the perspective of this conference, probably I should read the subtitle, namely How to engineer a Python to Fortran binding in C++ for use in Julia and MATLAB and why to do it. So the package that this tool is interfacing is called Piepart MC. It's a Monte Carlo simulation package for air resolves that are, for example, floating in the air. It's an open source tool developed for more than 20 years at Urbana-Champaign. And just one line about the physics. So usually it's kind of a box model, so studying just processes without a spatial context. But it also has an option to be coupled with the Worf weather simulation for a cast. So here is the HPC context. And it simulates things like air pollution, evolution due to collisions of particles, condensation, chemical reactions, et cetera. And on the technical side, it's actually an object-oriented code base written in quite classic, using quite classic subset of Fortran, but still in very much object-oriented manner. And despite 20 years of heritage, it has a very comprehensive test suite. And I would say it could be an example of best practices in Fortran. However, its usage poses several challenges, for example, to students who intend to start off using it, for example, from a Jupyter notebook. And these challenges are related with, first of all, multiple dependencies. The need to compile it. Getting updates doesn't have really a workflow ready. The automation of simulations, analysis, et cetera, usually involves Shell. The input output is handled through multiple text files. And to analyze output from these simulations, usually one needs to actually look or use some of the Fortran code the simulation is based on. So the question that was posed when we started was how to bring together these two seemingly separate worlds. So on the right-hand side, this is the simulation package, part MC, with its Fortran code base, a bit of C code base, different dependencies. And then a perspective of a modern student, let's say, who starts with Jupyter and expects basically everything to be importable and interoperable with other libraries, scipy, numpy, et cetera. So the goals would be to lower the entry threshold for installation and usage. To ensure that the same experience is doable on different operating systems. And also to streamline the dissemination of studies based on the simulation tool, for example, for peer review with scientific journals. So the status of the project, as of now, of part MC, this Python bindings, is that we released after two years of development version one, it's on PyPy. And we also published a description of the package in the software X journal. So we are kind of ready for a rollout. And today I will talk more about the internals. And the internals start with PyBind 11. So despite we are talking about Python and Fortran, we actually, we picked PyBind 11, which is a C++ tool for developing Python packages as our backbone. So here's some highlights. The project actually is for those who are new to it, it's quite a remarkable success, I would say, with over 300 contributors on GitHub, 2,000 forks and 14,000 stars. Congratulations to PyBind 11. And it's very useful. So it fits here into the picture. So essentially we developed in C++, in C and in Fortran, so it's a triple language project, something that uses PyBind 11 and a few other components to automate building of this part of C and offering the Python package. So probably what's also worth mentioning is here that most of the work on PyPartnC was around substituting this text file input output with JSON-like Python native, let's say, or Python-like Pythonic input output layer. And as I mentioned, the original project has the object-oriented structure, so we tried to also couple Python's garbage collector with the Fortran functions that are provided for creating and deallocating objects. And there are many, many dependencies that the project has in Fortran, in C, in C++. And here, let me just mention that we picked Git submodules as a tool to pin versions of these dependencies, which is useful because the pip install command is able to grab packages from a Git repository, and this would include all the submodules with their versions. So let me now present a bit of code and how it looks from a user perspective. So this example here, please don't look particularly on the license of code, maybe just on the bulk of code, and the type of code. So here on the left, we have the Fortran Hello World for using the PartMC package, and on the right, three text files that would be the minimum to start a simplest simulation. So now this is the end result that uses the PyPartnC layer, so essentially the same can be obtained with a single file, starting with importing from this PyPartnC wrapper, and then using this kind of JSON-like notation, essentially here, list and dictionaries that are wrapped. So one achievement kind of, and one big advantage of using Python is that actually providing Python wrappers, you are catering also to Julia users, for example, here through the PyCall.jl package, essentially the same code and the same logic can be obtained for Julia users using PyPartnC. And finally, example with using Matlap, which ships with built-in Python bridge, and then which allows also to use PyPartnC to access the Fortran code from Matlap. So these three examples I've shown are actually part of our CI, so we have them in the readme file, and on CI we are executing the Julia, the Python, the Fortran, and the Matlap example, uploading the output as artifacts, and there is an assert stage that checks if the output from all these languages match. By the way, the timings here are essentially compilation and set up, so it's not that Fortran takes much shorter, the execution is always done through the Fortran code base and binary, but clearly compiling just the Fortran code is faster than setting up the Python, Julia, or Matlap environment, and how it works actually in practice when looking at the code. So here, this diagram might be not perfectly visible, but the right column is C++ layer, here is the C layer, here is Fortran layer, and here is the user code either in Julia, Matlap, or Python. And the different color here is to depict the package that we are interfacing with. So if we start with this readme code here, the user's Python code, we have set up the some import and instantiation of a single object of this arrow data class as an example, and what happens if we call it, first it goes through barely visible, I guess. So anyhow, this is the kind of outer layer for the C++ implemented Python package, and now I hope it's more visible. This is how PyBind 11, how one works with PyBind 11. So this is the C++ code where we define a module for Python, creating a Python class from C++ code looks roughly like this, with some templates defining the class that we interface how to handle memory allocation and defining particular methods. Here there is an init method, so a kind of constructor, and this constructor, when called, goes through C++ code, this arrow data class that we wrap, but quickly we need on our way to Fortran to go into what is written here up at the top, C binded signatures for the Fortran function. So they cannot take exceptions, exception handling through, across these languages is essentially undefined behavior, depending on the compiler. This is how it looks from the C++ perspective. So when we look now on the C signatures here at the top, they match to what is later defined in Fortran with the Fortran built in C binding module. So whenever you see this bind C or C underscore types, these ensure within Fortran code that we can access this code from C, and each of these routines is written for our wrapper and essentially calls quickly as a fin wrapper around the original Fortran routines that we wanted to wrap. So for example, the one below spec file read arrow data. So now we go finally to the wrapped code. This is the unmodified code that we access, and it sits in a Git submodule of the Pypartmc project. Now the fun starts when this Fortran code actually calls its input output layer, and there is like, usually a simulation takes something like 20 different text files to be read through, and these text files are nested. So what we've done is we replaced one of the components of the original Fortran package with our implementation that starts in Fortran, then goes through a C layer back to C++, which then uses JSON for Fortran. So this is a C++ library that helps get very readable C++ code for using Fortran, and this was our solution to replacing the multiple text files with what from user perspective are essentially in memory, MATLAB, Julia, or Python objects. We also have online documentation for the project generated from the source code, and as you can see here, for example, the types are hinted correctly. So despite in principle the Fortran parameter ordering is the key, we do inform Python users for the types of the arguments. So to start a summary, what we achieved with the Pypartmc wrapper is that we have a list of different types of the wrapper, and we have a single command pip installation on Windows Linux and OS X, with the exception that from Apple Silicon we are still struggling to get it done and help welcome, if any of you is a Fortran hacker who could help us produce universal binaries. We provide access to unmodified internals of the Pypartmc underlying package from Python, MATLAB, and also C++. So as a side effect by product of this goal of providing Python interface, we got also Julia MATLAB and C++ layer. Probably something that might not be obvious from the original plan, and we ended up actually using extensively is that this provides us with a nice tool for development of other Python packages because we can use part mc in test shoots to verify against the established simulation package. And also probably it's maybe a non-trivial way to use pip, but since C and Fortran are probably not the best, are not the solutions, not the technologies where you see mainstream package managers coming in or being established here, we managed to ship Fortran codes to users of Windows 6 Linux different variants of binary packages through pip. So it's essentially probably one way of thinking of the PyPy.org platform. And from the point of view of what I mentioned earlier, providing students or researchers using this package with tool to disseminate their research workflows, including input data, output data analysis workflow in a single, for example, Jupyter file for a paper peer review. And finally, PyPy.org mc allows to extend the Fortran code with some Python logic. So since we interface with, we expose the internals of the package, we can do in a simulation the time stepping can actually be done from Python. And you can add to, let's say, if you have 10 different steps of the simulation done in Fortran, you can add an 11th one that is in Python, Julia or whatever. And the final point is probably one of the key things here is that having statically linked all the dependencies, we can actually use the package on platforms such as Colab or Jupyter Hubs of various institutions by doing just pip install and importing what otherwise would require getting a lot of dependencies and a lot of compile time stuff available. Take home messages. So I wanted to kind of give you a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit kind of underline that PyBind 11, despite being a C++ tool is actually a valuable thing for interfacing Fortran with Python. And this is linked to the fact that PyBind 11 offers CMake integration. So your C++ projects can have build automation in CMake, and CMake handles Fortran well, so this was the key thing here. The glue language role of Python is, I think, nicely exemplified here with Julia and Matlap, including CI. Static linkage of the dependencies was essential for us, for example, due to the fact that there is no standardized ABI for four different versions, even of the same compiler, have different binary incompatibilities, and this was essential to get it working on on platforms such as Colab or other Jupyter Hubs. But this prevented us from from publishing the package on KONDA due to KONDA policy of no static linkage. We've used more than 10 Git submodules for tracking our dependencies from the GitHub repo. As I mentioned, help welcome in getting the universal binaries generated with G4tran. The CI on using MATLAB is possible thanks to the MATLAB actions. So the producer of MATLAB MapWorks offers CI, GitHub actions that actually do not require any MATLAB license. So if one wants to run MATLAB code on GitHub, this is important and just wanted to thank them. And finally, a fun fact or the positive thing that actually when we submitted the paper about the project to the Software X Journal, just reporting that during the peer review, the reviewers indeed tried the code and provided us with feedback that also helped. So this was kind of positive that it did work. Let me acknowledge funding from US National Science Foundation and Polish National Science Center and thank you for your attention. Any questions? Yes, thank you for that presentation. My question was exactly did you keep in Fortran and what did you pass to Python site? So it's a race or some or just single values? So the question is about if I understand correctly what kind of data we are tackling with passing us during the simulation. So it's a the Monte Carlo simulations here are tracking particles in kind of attribute space that tracks their physical and chemical properties. So it's usually 20, 30 dimensional attribute space that is randomly sampled. So we have vectors of these particles in this attribute space. So usually this could be from thousands to hundreds of thousands of particles that each of the particle has like 30 attributes. From Python perspective, usually the user does not really use the roll data of the simulation, the state vector, just some aggregate information which is passed back to Python as enumerables that can be used with NAMPy, but we don't actually assume that it must be NAMPy. So one can use just lists if they are enough. I hope that answers. My question is just because we need some roll data from Fortran site to Python site and then it's just some two dimensional matter. Here we have some problems that we need to know where we keep the data. We are not exposing particle locations in memory. They are always returned as new objects to Python because this is it is never the state vector of the simulation. It's just a some aggregate information that characterizes it in a simpler way. So usually we have just one dimensional enumerable. For you it's much more simple. Thank you. Time for one more question. If there is one. Okay, if not we'll wrap up here because apparently there's a queue outside to get in for the next talks. Thank you. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.16, "text": " We'll get started.", "tokens": [50364, 492, 603, 483, 1409, 13, 50672], "temperature": 0.0, "avg_logprob": -0.351153450450678, "compression_ratio": 1.3383084577114428, "no_speech_prob": 0.5184693336486816}, {"id": 1, "seek": 0, "start": 6.16, "end": 10.56, "text": " Sylvester will introduce us to Piepart MC.", "tokens": [50672, 3902, 14574, 3011, 486, 5366, 505, 281, 22914, 6971, 8797, 13, 50892], "temperature": 0.0, "avg_logprob": -0.351153450450678, "compression_ratio": 1.3383084577114428, "no_speech_prob": 0.5184693336486816}, {"id": 2, "seek": 0, "start": 10.56, "end": 11.56, "text": " Thank you for coming.", "tokens": [50892, 1044, 291, 337, 1348, 13, 50942], "temperature": 0.0, "avg_logprob": -0.351153450450678, "compression_ratio": 1.3383084577114428, "no_speech_prob": 0.5184693336486816}, {"id": 3, "seek": 0, "start": 11.56, "end": 13.4, "text": " I'm Sylvester Arrabas.", "tokens": [50942, 286, 478, 3902, 14574, 3011, 1587, 5305, 296, 13, 51034], "temperature": 0.0, "avg_logprob": -0.351153450450678, "compression_ratio": 1.3383084577114428, "no_speech_prob": 0.5184693336486816}, {"id": 4, "seek": 0, "start": 13.4, "end": 17.240000000000002, "text": " I work at the AGH University in Krak\u00f3w in Poland.", "tokens": [51034, 286, 589, 412, 264, 316, 4269, 3535, 294, 591, 11272, 3901, 294, 15950, 13, 51226], "temperature": 0.0, "avg_logprob": -0.351153450450678, "compression_ratio": 1.3383084577114428, "no_speech_prob": 0.5184693336486816}, {"id": 5, "seek": 0, "start": 17.240000000000002, "end": 23.6, "text": " And this is a project carried out together with a team from the University of Illinois,", "tokens": [51226, 400, 341, 307, 257, 1716, 9094, 484, 1214, 365, 257, 1469, 490, 264, 3535, 295, 17508, 11, 51544], "temperature": 0.0, "avg_logprob": -0.351153450450678, "compression_ratio": 1.3383084577114428, "no_speech_prob": 0.5184693336486816}, {"id": 6, "seek": 0, "start": 23.6, "end": 26.04, "text": " Urbana-Champaign in US.", "tokens": [51544, 9533, 65, 2095, 12, 6546, 1215, 4897, 294, 2546, 13, 51666], "temperature": 0.0, "avg_logprob": -0.351153450450678, "compression_ratio": 1.3383084577114428, "no_speech_prob": 0.5184693336486816}, {"id": 7, "seek": 2604, "start": 26.599999999999998, "end": 32.04, "text": " So Piepart MC is the highlight here.", "tokens": [50392, 407, 22914, 6971, 8797, 307, 264, 5078, 510, 13, 50664], "temperature": 0.0, "avg_logprob": -0.23522730796567856, "compression_ratio": 1.3373493975903614, "no_speech_prob": 0.3231109082698822}, {"id": 8, "seek": 2604, "start": 32.04, "end": 39.48, "text": " But from the perspective of this conference, probably I should read the subtitle, namely", "tokens": [50664, 583, 490, 264, 4585, 295, 341, 7586, 11, 1391, 286, 820, 1401, 264, 30706, 306, 11, 20926, 51036], "temperature": 0.0, "avg_logprob": -0.23522730796567856, "compression_ratio": 1.3373493975903614, "no_speech_prob": 0.3231109082698822}, {"id": 9, "seek": 2604, "start": 39.48, "end": 46.8, "text": " How to engineer a Python to Fortran binding in C++ for use in Julia and MATLAB and why", "tokens": [51036, 1012, 281, 11403, 257, 15329, 281, 11002, 4257, 17359, 294, 383, 25472, 337, 764, 294, 18551, 293, 5904, 11435, 33, 293, 983, 51402], "temperature": 0.0, "avg_logprob": -0.23522730796567856, "compression_ratio": 1.3373493975903614, "no_speech_prob": 0.3231109082698822}, {"id": 10, "seek": 2604, "start": 46.8, "end": 48.239999999999995, "text": " to do it.", "tokens": [51402, 281, 360, 309, 13, 51474], "temperature": 0.0, "avg_logprob": -0.23522730796567856, "compression_ratio": 1.3373493975903614, "no_speech_prob": 0.3231109082698822}, {"id": 11, "seek": 4824, "start": 48.24, "end": 57.400000000000006, "text": " So the package that this tool is interfacing is called Piepart MC.", "tokens": [50364, 407, 264, 7372, 300, 341, 2290, 307, 14510, 5615, 307, 1219, 22914, 6971, 8797, 13, 50822], "temperature": 0.0, "avg_logprob": -0.156093621865297, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.16576646268367767}, {"id": 12, "seek": 4824, "start": 57.400000000000006, "end": 63.160000000000004, "text": " It's a Monte Carlo simulation package for air resolves that are, for example, floating", "tokens": [50822, 467, 311, 257, 38105, 45112, 16575, 7372, 337, 1988, 7923, 977, 300, 366, 11, 337, 1365, 11, 12607, 51110], "temperature": 0.0, "avg_logprob": -0.156093621865297, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.16576646268367767}, {"id": 13, "seek": 4824, "start": 63.160000000000004, "end": 64.16, "text": " in the air.", "tokens": [51110, 294, 264, 1988, 13, 51160], "temperature": 0.0, "avg_logprob": -0.156093621865297, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.16576646268367767}, {"id": 14, "seek": 4824, "start": 64.16, "end": 71.96000000000001, "text": " It's an open source tool developed for more than 20 years at Urbana-Champaign.", "tokens": [51160, 467, 311, 364, 1269, 4009, 2290, 4743, 337, 544, 813, 945, 924, 412, 9533, 65, 2095, 12, 6546, 1215, 4897, 13, 51550], "temperature": 0.0, "avg_logprob": -0.156093621865297, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.16576646268367767}, {"id": 15, "seek": 4824, "start": 71.96000000000001, "end": 74.28, "text": " And just one line about the physics.", "tokens": [51550, 400, 445, 472, 1622, 466, 264, 10649, 13, 51666], "temperature": 0.0, "avg_logprob": -0.156093621865297, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.16576646268367767}, {"id": 16, "seek": 7428, "start": 74.28, "end": 81.36, "text": " So usually it's kind of a box model, so studying just processes without a spatial context.", "tokens": [50364, 407, 2673, 309, 311, 733, 295, 257, 2424, 2316, 11, 370, 7601, 445, 7555, 1553, 257, 23598, 4319, 13, 50718], "temperature": 0.0, "avg_logprob": -0.2596502878579749, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.04052755981683731}, {"id": 17, "seek": 7428, "start": 81.36, "end": 85.92, "text": " But it also has an option to be coupled with the Worf weather simulation for a cast.", "tokens": [50718, 583, 309, 611, 575, 364, 3614, 281, 312, 29482, 365, 264, 26363, 69, 5503, 16575, 337, 257, 4193, 13, 50946], "temperature": 0.0, "avg_logprob": -0.2596502878579749, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.04052755981683731}, {"id": 18, "seek": 7428, "start": 85.92, "end": 89.28, "text": " So here is the HPC context.", "tokens": [50946, 407, 510, 307, 264, 12557, 34, 4319, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2596502878579749, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.04052755981683731}, {"id": 19, "seek": 7428, "start": 89.28, "end": 95.2, "text": " And it simulates things like air pollution, evolution due to collisions of particles,", "tokens": [51114, 400, 309, 1034, 26192, 721, 411, 1988, 16727, 11, 9303, 3462, 281, 46537, 295, 10007, 11, 51410], "temperature": 0.0, "avg_logprob": -0.2596502878579749, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.04052755981683731}, {"id": 20, "seek": 7428, "start": 95.2, "end": 99.44, "text": " condensation, chemical reactions, et cetera.", "tokens": [51410, 2224, 35292, 11, 7313, 12215, 11, 1030, 11458, 13, 51622], "temperature": 0.0, "avg_logprob": -0.2596502878579749, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.04052755981683731}, {"id": 21, "seek": 9944, "start": 99.44, "end": 107.44, "text": " And on the technical side, it's actually an object-oriented code base written in quite", "tokens": [50364, 400, 322, 264, 6191, 1252, 11, 309, 311, 767, 364, 2657, 12, 27414, 3089, 3096, 3720, 294, 1596, 50764], "temperature": 0.0, "avg_logprob": -0.16686455207534984, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.195319265127182}, {"id": 22, "seek": 9944, "start": 107.44, "end": 114.67999999999999, "text": " classic, using quite classic subset of Fortran, but still in very much object-oriented manner.", "tokens": [50764, 7230, 11, 1228, 1596, 7230, 25993, 295, 11002, 4257, 11, 457, 920, 294, 588, 709, 2657, 12, 27414, 9060, 13, 51126], "temperature": 0.0, "avg_logprob": -0.16686455207534984, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.195319265127182}, {"id": 23, "seek": 9944, "start": 114.67999999999999, "end": 123.2, "text": " And despite 20 years of heritage, it has a very comprehensive test suite.", "tokens": [51126, 400, 7228, 945, 924, 295, 16040, 11, 309, 575, 257, 588, 13914, 1500, 14205, 13, 51552], "temperature": 0.0, "avg_logprob": -0.16686455207534984, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.195319265127182}, {"id": 24, "seek": 9944, "start": 123.2, "end": 128.56, "text": " And I would say it could be an example of best practices in Fortran.", "tokens": [51552, 400, 286, 576, 584, 309, 727, 312, 364, 1365, 295, 1151, 7525, 294, 11002, 4257, 13, 51820], "temperature": 0.0, "avg_logprob": -0.16686455207534984, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.195319265127182}, {"id": 25, "seek": 12856, "start": 128.56, "end": 135.76, "text": " However, its usage poses several challenges, for example, to students who intend to start", "tokens": [50364, 2908, 11, 1080, 14924, 26059, 2940, 4759, 11, 337, 1365, 11, 281, 1731, 567, 19759, 281, 722, 50724], "temperature": 0.0, "avg_logprob": -0.1795208105880223, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.03822089359164238}, {"id": 26, "seek": 12856, "start": 135.76, "end": 140.2, "text": " off using it, for example, from a Jupyter notebook.", "tokens": [50724, 766, 1228, 309, 11, 337, 1365, 11, 490, 257, 22125, 88, 391, 21060, 13, 50946], "temperature": 0.0, "avg_logprob": -0.1795208105880223, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.03822089359164238}, {"id": 27, "seek": 12856, "start": 140.2, "end": 144.44, "text": " And these challenges are related with, first of all, multiple dependencies.", "tokens": [50946, 400, 613, 4759, 366, 4077, 365, 11, 700, 295, 439, 11, 3866, 36606, 13, 51158], "temperature": 0.0, "avg_logprob": -0.1795208105880223, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.03822089359164238}, {"id": 28, "seek": 12856, "start": 144.44, "end": 147.08, "text": " The need to compile it.", "tokens": [51158, 440, 643, 281, 31413, 309, 13, 51290], "temperature": 0.0, "avg_logprob": -0.1795208105880223, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.03822089359164238}, {"id": 29, "seek": 12856, "start": 147.08, "end": 151.6, "text": " Getting updates doesn't have really a workflow ready.", "tokens": [51290, 13674, 9205, 1177, 380, 362, 534, 257, 20993, 1919, 13, 51516], "temperature": 0.0, "avg_logprob": -0.1795208105880223, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.03822089359164238}, {"id": 30, "seek": 12856, "start": 151.6, "end": 157.28, "text": " The automation of simulations, analysis, et cetera, usually involves Shell.", "tokens": [51516, 440, 17769, 295, 35138, 11, 5215, 11, 1030, 11458, 11, 2673, 11626, 22863, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1795208105880223, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.03822089359164238}, {"id": 31, "seek": 15728, "start": 158.28, "end": 165.04, "text": " The input output is handled through multiple text files.", "tokens": [50414, 440, 4846, 5598, 307, 18033, 807, 3866, 2487, 7098, 13, 50752], "temperature": 0.0, "avg_logprob": -0.1560683114188058, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.07073946297168732}, {"id": 32, "seek": 15728, "start": 165.04, "end": 171.36, "text": " And to analyze output from these simulations, usually one needs to actually look or use", "tokens": [50752, 400, 281, 12477, 5598, 490, 613, 35138, 11, 2673, 472, 2203, 281, 767, 574, 420, 764, 51068], "temperature": 0.0, "avg_logprob": -0.1560683114188058, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.07073946297168732}, {"id": 33, "seek": 15728, "start": 171.36, "end": 175.64, "text": " some of the Fortran code the simulation is based on.", "tokens": [51068, 512, 295, 264, 11002, 4257, 3089, 264, 16575, 307, 2361, 322, 13, 51282], "temperature": 0.0, "avg_logprob": -0.1560683114188058, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.07073946297168732}, {"id": 34, "seek": 15728, "start": 175.64, "end": 182.36, "text": " So the question that was posed when we started was how to bring together these two seemingly", "tokens": [51282, 407, 264, 1168, 300, 390, 31399, 562, 321, 1409, 390, 577, 281, 1565, 1214, 613, 732, 18709, 51618], "temperature": 0.0, "avg_logprob": -0.1560683114188058, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.07073946297168732}, {"id": 35, "seek": 15728, "start": 182.36, "end": 183.8, "text": " separate worlds.", "tokens": [51618, 4994, 13401, 13, 51690], "temperature": 0.0, "avg_logprob": -0.1560683114188058, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.07073946297168732}, {"id": 36, "seek": 18380, "start": 183.84, "end": 188.52, "text": " So on the right-hand side, this is the simulation package, part MC, with its Fortran code base,", "tokens": [50366, 407, 322, 264, 558, 12, 5543, 1252, 11, 341, 307, 264, 16575, 7372, 11, 644, 8797, 11, 365, 1080, 11002, 4257, 3089, 3096, 11, 50600], "temperature": 0.0, "avg_logprob": -0.23736955517920377, "compression_ratio": 1.5622641509433963, "no_speech_prob": 0.01612665504217148}, {"id": 37, "seek": 18380, "start": 188.52, "end": 193.16000000000003, "text": " a bit of C code base, different dependencies.", "tokens": [50600, 257, 857, 295, 383, 3089, 3096, 11, 819, 36606, 13, 50832], "temperature": 0.0, "avg_logprob": -0.23736955517920377, "compression_ratio": 1.5622641509433963, "no_speech_prob": 0.01612665504217148}, {"id": 38, "seek": 18380, "start": 193.16000000000003, "end": 199.60000000000002, "text": " And then a perspective of a modern student, let's say, who starts with Jupyter and expects", "tokens": [50832, 400, 550, 257, 4585, 295, 257, 4363, 3107, 11, 718, 311, 584, 11, 567, 3719, 365, 22125, 88, 391, 293, 33280, 51154], "temperature": 0.0, "avg_logprob": -0.23736955517920377, "compression_ratio": 1.5622641509433963, "no_speech_prob": 0.01612665504217148}, {"id": 39, "seek": 18380, "start": 199.60000000000002, "end": 206.36, "text": " basically everything to be importable and interoperable with other libraries, scipy,", "tokens": [51154, 1936, 1203, 281, 312, 974, 712, 293, 728, 7192, 712, 365, 661, 15148, 11, 2180, 8200, 11, 51492], "temperature": 0.0, "avg_logprob": -0.23736955517920377, "compression_ratio": 1.5622641509433963, "no_speech_prob": 0.01612665504217148}, {"id": 40, "seek": 18380, "start": 206.36, "end": 207.92000000000002, "text": " numpy, et cetera.", "tokens": [51492, 1031, 8200, 11, 1030, 11458, 13, 51570], "temperature": 0.0, "avg_logprob": -0.23736955517920377, "compression_ratio": 1.5622641509433963, "no_speech_prob": 0.01612665504217148}, {"id": 41, "seek": 18380, "start": 207.92000000000002, "end": 213.64000000000001, "text": " So the goals would be to lower the entry threshold for installation and usage.", "tokens": [51570, 407, 264, 5493, 576, 312, 281, 3126, 264, 8729, 14678, 337, 13260, 293, 14924, 13, 51856], "temperature": 0.0, "avg_logprob": -0.23736955517920377, "compression_ratio": 1.5622641509433963, "no_speech_prob": 0.01612665504217148}, {"id": 42, "seek": 21364, "start": 213.64, "end": 221.27999999999997, "text": " To ensure that the same experience is doable on different operating systems.", "tokens": [50364, 1407, 5586, 300, 264, 912, 1752, 307, 41183, 322, 819, 7447, 3652, 13, 50746], "temperature": 0.0, "avg_logprob": -0.19552287249497965, "compression_ratio": 1.4875621890547264, "no_speech_prob": 0.021327681839466095}, {"id": 43, "seek": 21364, "start": 221.27999999999997, "end": 228.32, "text": " And also to streamline the dissemination of studies based on the simulation tool, for", "tokens": [50746, 400, 611, 281, 47141, 264, 34585, 399, 295, 5313, 2361, 322, 264, 16575, 2290, 11, 337, 51098], "temperature": 0.0, "avg_logprob": -0.19552287249497965, "compression_ratio": 1.4875621890547264, "no_speech_prob": 0.021327681839466095}, {"id": 44, "seek": 21364, "start": 228.32, "end": 235.35999999999999, "text": " example, for peer review with scientific journals.", "tokens": [51098, 1365, 11, 337, 15108, 3131, 365, 8134, 29621, 13, 51450], "temperature": 0.0, "avg_logprob": -0.19552287249497965, "compression_ratio": 1.4875621890547264, "no_speech_prob": 0.021327681839466095}, {"id": 45, "seek": 21364, "start": 235.35999999999999, "end": 240.04, "text": " So the status of the project, as of now, of part MC, this Python bindings, is that we", "tokens": [51450, 407, 264, 6558, 295, 264, 1716, 11, 382, 295, 586, 11, 295, 644, 8797, 11, 341, 15329, 14786, 1109, 11, 307, 300, 321, 51684], "temperature": 0.0, "avg_logprob": -0.19552287249497965, "compression_ratio": 1.4875621890547264, "no_speech_prob": 0.021327681839466095}, {"id": 46, "seek": 24004, "start": 240.07999999999998, "end": 247.12, "text": " released after two years of development version one, it's on PyPy.", "tokens": [50366, 4736, 934, 732, 924, 295, 3250, 3037, 472, 11, 309, 311, 322, 9953, 47, 88, 13, 50718], "temperature": 0.0, "avg_logprob": -0.19212808107074938, "compression_ratio": 1.456989247311828, "no_speech_prob": 0.04878390580415726}, {"id": 47, "seek": 24004, "start": 247.12, "end": 253.95999999999998, "text": " And we also published a description of the package in the software X journal.", "tokens": [50718, 400, 321, 611, 6572, 257, 3855, 295, 264, 7372, 294, 264, 4722, 1783, 6708, 13, 51060], "temperature": 0.0, "avg_logprob": -0.19212808107074938, "compression_ratio": 1.456989247311828, "no_speech_prob": 0.04878390580415726}, {"id": 48, "seek": 24004, "start": 253.95999999999998, "end": 257.64, "text": " So we are kind of ready for a rollout.", "tokens": [51060, 407, 321, 366, 733, 295, 1919, 337, 257, 3373, 346, 13, 51244], "temperature": 0.0, "avg_logprob": -0.19212808107074938, "compression_ratio": 1.456989247311828, "no_speech_prob": 0.04878390580415726}, {"id": 49, "seek": 24004, "start": 257.64, "end": 261.15999999999997, "text": " And today I will talk more about the internals.", "tokens": [51244, 400, 965, 286, 486, 751, 544, 466, 264, 2154, 1124, 13, 51420], "temperature": 0.0, "avg_logprob": -0.19212808107074938, "compression_ratio": 1.456989247311828, "no_speech_prob": 0.04878390580415726}, {"id": 50, "seek": 24004, "start": 261.15999999999997, "end": 263.48, "text": " And the internals start with PyBind 11.", "tokens": [51420, 400, 264, 2154, 1124, 722, 365, 9953, 33, 471, 2975, 13, 51536], "temperature": 0.0, "avg_logprob": -0.19212808107074938, "compression_ratio": 1.456989247311828, "no_speech_prob": 0.04878390580415726}, {"id": 51, "seek": 26348, "start": 263.48, "end": 271.52000000000004, "text": " So despite we are talking about Python and Fortran, we actually, we picked PyBind 11,", "tokens": [50364, 407, 7228, 321, 366, 1417, 466, 15329, 293, 11002, 4257, 11, 321, 767, 11, 321, 6183, 9953, 33, 471, 2975, 11, 50766], "temperature": 0.0, "avg_logprob": -0.1878004482814244, "compression_ratio": 1.4, "no_speech_prob": 0.07530783116817474}, {"id": 52, "seek": 26348, "start": 271.52000000000004, "end": 279.28000000000003, "text": " which is a C++ tool for developing Python packages as our backbone.", "tokens": [50766, 597, 307, 257, 383, 25472, 2290, 337, 6416, 15329, 17401, 382, 527, 34889, 13, 51154], "temperature": 0.0, "avg_logprob": -0.1878004482814244, "compression_ratio": 1.4, "no_speech_prob": 0.07530783116817474}, {"id": 53, "seek": 26348, "start": 279.28000000000003, "end": 280.40000000000003, "text": " So here's some highlights.", "tokens": [51154, 407, 510, 311, 512, 14254, 13, 51210], "temperature": 0.0, "avg_logprob": -0.1878004482814244, "compression_ratio": 1.4, "no_speech_prob": 0.07530783116817474}, {"id": 54, "seek": 26348, "start": 280.40000000000003, "end": 287.28000000000003, "text": " The project actually is for those who are new to it, it's quite a remarkable success,", "tokens": [51210, 440, 1716, 767, 307, 337, 729, 567, 366, 777, 281, 309, 11, 309, 311, 1596, 257, 12802, 2245, 11, 51554], "temperature": 0.0, "avg_logprob": -0.1878004482814244, "compression_ratio": 1.4, "no_speech_prob": 0.07530783116817474}, {"id": 55, "seek": 28728, "start": 287.28, "end": 294.64, "text": " I would say, with over 300 contributors on GitHub, 2,000 forks and 14,000 stars.", "tokens": [50364, 286, 576, 584, 11, 365, 670, 6641, 45627, 322, 23331, 11, 568, 11, 1360, 337, 1694, 293, 3499, 11, 1360, 6105, 13, 50732], "temperature": 0.0, "avg_logprob": -0.2394688859277842, "compression_ratio": 1.4481327800829875, "no_speech_prob": 0.07916373759508133}, {"id": 56, "seek": 28728, "start": 294.64, "end": 296.23999999999995, "text": " Congratulations to PyBind 11.", "tokens": [50732, 9694, 281, 9953, 33, 471, 2975, 13, 50812], "temperature": 0.0, "avg_logprob": -0.2394688859277842, "compression_ratio": 1.4481327800829875, "no_speech_prob": 0.07916373759508133}, {"id": 57, "seek": 28728, "start": 296.23999999999995, "end": 297.67999999999995, "text": " And it's very useful.", "tokens": [50812, 400, 309, 311, 588, 4420, 13, 50884], "temperature": 0.0, "avg_logprob": -0.2394688859277842, "compression_ratio": 1.4481327800829875, "no_speech_prob": 0.07916373759508133}, {"id": 58, "seek": 28728, "start": 297.67999999999995, "end": 301.15999999999997, "text": " So it fits here into the picture.", "tokens": [50884, 407, 309, 9001, 510, 666, 264, 3036, 13, 51058], "temperature": 0.0, "avg_logprob": -0.2394688859277842, "compression_ratio": 1.4481327800829875, "no_speech_prob": 0.07916373759508133}, {"id": 59, "seek": 28728, "start": 301.15999999999997, "end": 307.71999999999997, "text": " So essentially we developed in C++, in C and in Fortran, so it's a triple language project,", "tokens": [51058, 407, 4476, 321, 4743, 294, 383, 25472, 11, 294, 383, 293, 294, 11002, 4257, 11, 370, 309, 311, 257, 15508, 2856, 1716, 11, 51386], "temperature": 0.0, "avg_logprob": -0.2394688859277842, "compression_ratio": 1.4481327800829875, "no_speech_prob": 0.07916373759508133}, {"id": 60, "seek": 28728, "start": 307.71999999999997, "end": 315.0, "text": " something that uses PyBind 11 and a few other components to automate building of this part", "tokens": [51386, 746, 300, 4960, 9953, 33, 471, 2975, 293, 257, 1326, 661, 6677, 281, 31605, 2390, 295, 341, 644, 51750], "temperature": 0.0, "avg_logprob": -0.2394688859277842, "compression_ratio": 1.4481327800829875, "no_speech_prob": 0.07916373759508133}, {"id": 61, "seek": 31500, "start": 315.08, "end": 319.48, "text": " of C and offering the Python package.", "tokens": [50368, 295, 383, 293, 8745, 264, 15329, 7372, 13, 50588], "temperature": 0.0, "avg_logprob": -0.3102520899986153, "compression_ratio": 1.494186046511628, "no_speech_prob": 0.07520533353090286}, {"id": 62, "seek": 31500, "start": 319.48, "end": 328.64, "text": " So probably what's also worth mentioning is here that most of the work on PyPartnC was", "tokens": [50588, 407, 1391, 437, 311, 611, 3163, 18315, 307, 510, 300, 881, 295, 264, 589, 322, 9953, 37012, 77, 34, 390, 51046], "temperature": 0.0, "avg_logprob": -0.3102520899986153, "compression_ratio": 1.494186046511628, "no_speech_prob": 0.07520533353090286}, {"id": 63, "seek": 31500, "start": 328.64, "end": 336.28, "text": " around substituting this text file input output with JSON-like Python native, let's say,", "tokens": [51046, 926, 26441, 10861, 341, 2487, 3991, 4846, 5598, 365, 31828, 12, 4092, 15329, 8470, 11, 718, 311, 584, 11, 51428], "temperature": 0.0, "avg_logprob": -0.3102520899986153, "compression_ratio": 1.494186046511628, "no_speech_prob": 0.07520533353090286}, {"id": 64, "seek": 31500, "start": 336.28, "end": 344.0, "text": " or Python-like Pythonic input output layer.", "tokens": [51428, 420, 15329, 12, 4092, 15329, 299, 4846, 5598, 4583, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3102520899986153, "compression_ratio": 1.494186046511628, "no_speech_prob": 0.07520533353090286}, {"id": 65, "seek": 34400, "start": 344.0, "end": 350.6, "text": " And as I mentioned, the original project has the object-oriented structure, so we tried", "tokens": [50364, 400, 382, 286, 2835, 11, 264, 3380, 1716, 575, 264, 2657, 12, 27414, 3877, 11, 370, 321, 3031, 50694], "temperature": 0.0, "avg_logprob": -0.1412610621065707, "compression_ratio": 1.544041450777202, "no_speech_prob": 0.009030448272824287}, {"id": 66, "seek": 34400, "start": 350.6, "end": 357.68, "text": " to also couple Python's garbage collector with the Fortran functions that are provided", "tokens": [50694, 281, 611, 1916, 15329, 311, 14150, 23960, 365, 264, 11002, 4257, 6828, 300, 366, 5649, 51048], "temperature": 0.0, "avg_logprob": -0.1412610621065707, "compression_ratio": 1.544041450777202, "no_speech_prob": 0.009030448272824287}, {"id": 67, "seek": 34400, "start": 357.68, "end": 360.96, "text": " for creating and deallocating objects.", "tokens": [51048, 337, 4084, 293, 368, 336, 905, 990, 6565, 13, 51212], "temperature": 0.0, "avg_logprob": -0.1412610621065707, "compression_ratio": 1.544041450777202, "no_speech_prob": 0.009030448272824287}, {"id": 68, "seek": 34400, "start": 360.96, "end": 368.88, "text": " And there are many, many dependencies that the project has in Fortran, in C, in C++.", "tokens": [51212, 400, 456, 366, 867, 11, 867, 36606, 300, 264, 1716, 575, 294, 11002, 4257, 11, 294, 383, 11, 294, 383, 25472, 13, 51608], "temperature": 0.0, "avg_logprob": -0.1412610621065707, "compression_ratio": 1.544041450777202, "no_speech_prob": 0.009030448272824287}, {"id": 69, "seek": 36888, "start": 368.88, "end": 379.48, "text": " And here, let me just mention that we picked Git submodules as a tool to pin versions of", "tokens": [50364, 400, 510, 11, 718, 385, 445, 2152, 300, 321, 6183, 16939, 1422, 8014, 3473, 382, 257, 2290, 281, 5447, 9606, 295, 50894], "temperature": 0.0, "avg_logprob": -0.13346520389419003, "compression_ratio": 1.5898617511520738, "no_speech_prob": 0.12051356583833694}, {"id": 70, "seek": 36888, "start": 379.48, "end": 384.8, "text": " these dependencies, which is useful because the pip install command is able to grab packages", "tokens": [50894, 613, 36606, 11, 597, 307, 4420, 570, 264, 8489, 3625, 5622, 307, 1075, 281, 4444, 17401, 51160], "temperature": 0.0, "avg_logprob": -0.13346520389419003, "compression_ratio": 1.5898617511520738, "no_speech_prob": 0.12051356583833694}, {"id": 71, "seek": 36888, "start": 384.8, "end": 392.2, "text": " from a Git repository, and this would include all the submodules with their versions.", "tokens": [51160, 490, 257, 16939, 25841, 11, 293, 341, 576, 4090, 439, 264, 1422, 8014, 3473, 365, 641, 9606, 13, 51530], "temperature": 0.0, "avg_logprob": -0.13346520389419003, "compression_ratio": 1.5898617511520738, "no_speech_prob": 0.12051356583833694}, {"id": 72, "seek": 36888, "start": 392.2, "end": 397.52, "text": " So let me now present a bit of code and how it looks from a user perspective.", "tokens": [51530, 407, 718, 385, 586, 1974, 257, 857, 295, 3089, 293, 577, 309, 1542, 490, 257, 4195, 4585, 13, 51796], "temperature": 0.0, "avg_logprob": -0.13346520389419003, "compression_ratio": 1.5898617511520738, "no_speech_prob": 0.12051356583833694}, {"id": 73, "seek": 39752, "start": 397.52, "end": 402.47999999999996, "text": " So this example here, please don't look particularly on the license of code, maybe just on the bulk", "tokens": [50364, 407, 341, 1365, 510, 11, 1767, 500, 380, 574, 4098, 322, 264, 10476, 295, 3089, 11, 1310, 445, 322, 264, 16139, 50612], "temperature": 0.0, "avg_logprob": -0.1840348898195753, "compression_ratio": 1.628099173553719, "no_speech_prob": 0.04159983992576599}, {"id": 74, "seek": 39752, "start": 402.47999999999996, "end": 405.59999999999997, "text": " of code, and the type of code.", "tokens": [50612, 295, 3089, 11, 293, 264, 2010, 295, 3089, 13, 50768], "temperature": 0.0, "avg_logprob": -0.1840348898195753, "compression_ratio": 1.628099173553719, "no_speech_prob": 0.04159983992576599}, {"id": 75, "seek": 39752, "start": 405.59999999999997, "end": 411.79999999999995, "text": " So here on the left, we have the Fortran Hello World for using the PartMC package, and on", "tokens": [50768, 407, 510, 322, 264, 1411, 11, 321, 362, 264, 11002, 4257, 2425, 3937, 337, 1228, 264, 4100, 39261, 7372, 11, 293, 322, 51078], "temperature": 0.0, "avg_logprob": -0.1840348898195753, "compression_ratio": 1.628099173553719, "no_speech_prob": 0.04159983992576599}, {"id": 76, "seek": 39752, "start": 411.79999999999995, "end": 417.44, "text": " the right, three text files that would be the minimum to start a simplest simulation.", "tokens": [51078, 264, 558, 11, 1045, 2487, 7098, 300, 576, 312, 264, 7285, 281, 722, 257, 22811, 16575, 13, 51360], "temperature": 0.0, "avg_logprob": -0.1840348898195753, "compression_ratio": 1.628099173553719, "no_speech_prob": 0.04159983992576599}, {"id": 77, "seek": 39752, "start": 417.44, "end": 425.44, "text": " So now this is the end result that uses the PyPartnC layer, so essentially the same can", "tokens": [51360, 407, 586, 341, 307, 264, 917, 1874, 300, 4960, 264, 9953, 37012, 77, 34, 4583, 11, 370, 4476, 264, 912, 393, 51760], "temperature": 0.0, "avg_logprob": -0.1840348898195753, "compression_ratio": 1.628099173553719, "no_speech_prob": 0.04159983992576599}, {"id": 78, "seek": 42544, "start": 425.52, "end": 432.52, "text": " be obtained with a single file, starting with importing from this PyPartnC wrapper, and then", "tokens": [50368, 312, 14879, 365, 257, 2167, 3991, 11, 2891, 365, 43866, 490, 341, 9953, 37012, 77, 34, 46906, 11, 293, 550, 50718], "temperature": 0.0, "avg_logprob": -0.21184833233173078, "compression_ratio": 1.5326086956521738, "no_speech_prob": 0.08283320814371109}, {"id": 79, "seek": 42544, "start": 433.88, "end": 440.88, "text": " using this kind of JSON-like notation, essentially here, list and dictionaries that are wrapped.", "tokens": [50786, 1228, 341, 733, 295, 31828, 12, 4092, 24657, 11, 4476, 510, 11, 1329, 293, 22352, 4889, 300, 366, 14226, 13, 51136], "temperature": 0.0, "avg_logprob": -0.21184833233173078, "compression_ratio": 1.5326086956521738, "no_speech_prob": 0.08283320814371109}, {"id": 80, "seek": 42544, "start": 444.36, "end": 451.36, "text": " So one achievement kind of, and one big advantage of using Python is that actually providing", "tokens": [51310, 407, 472, 15838, 733, 295, 11, 293, 472, 955, 5002, 295, 1228, 15329, 307, 300, 767, 6530, 51660], "temperature": 0.0, "avg_logprob": -0.21184833233173078, "compression_ratio": 1.5326086956521738, "no_speech_prob": 0.08283320814371109}, {"id": 81, "seek": 45136, "start": 452.36, "end": 459.36, "text": " Python wrappers, you are catering also to Julia users, for example, here through the", "tokens": [50414, 15329, 7843, 15226, 11, 291, 366, 21557, 278, 611, 281, 18551, 5022, 11, 337, 1365, 11, 510, 807, 264, 50764], "temperature": 0.0, "avg_logprob": -0.29508977187307256, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.018902292475104332}, {"id": 82, "seek": 45136, "start": 461.36, "end": 468.36, "text": " PyCall.jl package, essentially the same code and the same logic can be obtained for Julia", "tokens": [50864, 9953, 46113, 13, 73, 75, 7372, 11, 4476, 264, 912, 3089, 293, 264, 912, 9952, 393, 312, 14879, 337, 18551, 51214], "temperature": 0.0, "avg_logprob": -0.29508977187307256, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.018902292475104332}, {"id": 83, "seek": 45136, "start": 471.44, "end": 474.32, "text": " users using PyPartnC.", "tokens": [51368, 5022, 1228, 9953, 37012, 77, 34, 13, 51512], "temperature": 0.0, "avg_logprob": -0.29508977187307256, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.018902292475104332}, {"id": 84, "seek": 45136, "start": 474.32, "end": 481.32, "text": " And finally, example with using Matlap, which ships with built-in Python bridge, and then", "tokens": [51512, 400, 2721, 11, 1365, 365, 1228, 6789, 75, 569, 11, 597, 11434, 365, 3094, 12, 259, 15329, 7283, 11, 293, 550, 51862], "temperature": 0.0, "avg_logprob": -0.29508977187307256, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.018902292475104332}, {"id": 85, "seek": 48136, "start": 481.44, "end": 488.44, "text": " which allows also to use PyPartnC to access the Fortran code from Matlap.", "tokens": [50368, 597, 4045, 611, 281, 764, 9953, 37012, 77, 34, 281, 2105, 264, 11002, 4257, 3089, 490, 6789, 75, 569, 13, 50718], "temperature": 0.0, "avg_logprob": -0.16881052991177173, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.014882484450936317}, {"id": 86, "seek": 48136, "start": 490.0, "end": 496.0, "text": " So these three examples I've shown are actually part of our CI, so we have them in the readme", "tokens": [50796, 407, 613, 1045, 5110, 286, 600, 4898, 366, 767, 644, 295, 527, 37777, 11, 370, 321, 362, 552, 294, 264, 1401, 1398, 51096], "temperature": 0.0, "avg_logprob": -0.16881052991177173, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.014882484450936317}, {"id": 87, "seek": 48136, "start": 496.0, "end": 503.0, "text": " file, and on CI we are executing the Julia, the Python, the Fortran, and the Matlap example,", "tokens": [51096, 3991, 11, 293, 322, 37777, 321, 366, 32368, 264, 18551, 11, 264, 15329, 11, 264, 11002, 4257, 11, 293, 264, 6789, 75, 569, 1365, 11, 51446], "temperature": 0.0, "avg_logprob": -0.16881052991177173, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.014882484450936317}, {"id": 88, "seek": 48136, "start": 505.48, "end": 509.92, "text": " uploading the output as artifacts, and there is an assert stage that checks if the output", "tokens": [51570, 27301, 264, 5598, 382, 24617, 11, 293, 456, 307, 364, 19810, 3233, 300, 13834, 498, 264, 5598, 51792], "temperature": 0.0, "avg_logprob": -0.16881052991177173, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.014882484450936317}, {"id": 89, "seek": 50992, "start": 509.92, "end": 513.04, "text": " from all these languages match.", "tokens": [50364, 490, 439, 613, 8650, 2995, 13, 50520], "temperature": 0.0, "avg_logprob": -0.17253877719243368, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.018836401402950287}, {"id": 90, "seek": 50992, "start": 513.04, "end": 517.5600000000001, "text": " By the way, the timings here are essentially compilation and set up, so it's not that Fortran", "tokens": [50520, 3146, 264, 636, 11, 264, 524, 1109, 510, 366, 4476, 40261, 293, 992, 493, 11, 370, 309, 311, 406, 300, 11002, 4257, 50746], "temperature": 0.0, "avg_logprob": -0.17253877719243368, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.018836401402950287}, {"id": 91, "seek": 50992, "start": 517.5600000000001, "end": 524.5600000000001, "text": " takes much shorter, the execution is always done through the Fortran code base and binary,", "tokens": [50746, 2516, 709, 11639, 11, 264, 15058, 307, 1009, 1096, 807, 264, 11002, 4257, 3089, 3096, 293, 17434, 11, 51096], "temperature": 0.0, "avg_logprob": -0.17253877719243368, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.018836401402950287}, {"id": 92, "seek": 50992, "start": 525.5600000000001, "end": 532.0, "text": " but clearly compiling just the Fortran code is faster than setting up the Python, Julia,", "tokens": [51146, 457, 4448, 715, 4883, 445, 264, 11002, 4257, 3089, 307, 4663, 813, 3287, 493, 264, 15329, 11, 18551, 11, 51468], "temperature": 0.0, "avg_logprob": -0.17253877719243368, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.018836401402950287}, {"id": 93, "seek": 50992, "start": 532.0, "end": 537.12, "text": " or Matlap environment, and how it works actually in practice when looking at the code.", "tokens": [51468, 420, 6789, 75, 569, 2823, 11, 293, 577, 309, 1985, 767, 294, 3124, 562, 1237, 412, 264, 3089, 13, 51724], "temperature": 0.0, "avg_logprob": -0.17253877719243368, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.018836401402950287}, {"id": 94, "seek": 53712, "start": 537.12, "end": 544.12, "text": " So here, this diagram might be not perfectly visible, but the right column is C++ layer,", "tokens": [50364, 407, 510, 11, 341, 10686, 1062, 312, 406, 6239, 8974, 11, 457, 264, 558, 7738, 307, 383, 25472, 4583, 11, 50714], "temperature": 0.0, "avg_logprob": -0.17438033132842093, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.03954417631030083}, {"id": 95, "seek": 53712, "start": 545.08, "end": 551.16, "text": " here is the C layer, here is Fortran layer, and here is the user code either in Julia,", "tokens": [50762, 510, 307, 264, 383, 4583, 11, 510, 307, 11002, 4257, 4583, 11, 293, 510, 307, 264, 4195, 3089, 2139, 294, 18551, 11, 51066], "temperature": 0.0, "avg_logprob": -0.17438033132842093, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.03954417631030083}, {"id": 96, "seek": 53712, "start": 551.16, "end": 553.24, "text": " Matlap, or Python.", "tokens": [51066, 6789, 75, 569, 11, 420, 15329, 13, 51170], "temperature": 0.0, "avg_logprob": -0.17438033132842093, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.03954417631030083}, {"id": 97, "seek": 53712, "start": 553.24, "end": 559.08, "text": " And the different color here is to depict the package that we are interfacing with.", "tokens": [51170, 400, 264, 819, 2017, 510, 307, 281, 31553, 264, 7372, 300, 321, 366, 14510, 5615, 365, 13, 51462], "temperature": 0.0, "avg_logprob": -0.17438033132842093, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.03954417631030083}, {"id": 98, "seek": 53712, "start": 559.08, "end": 566.08, "text": " So if we start with this readme code here, the user's Python code, we have set up the", "tokens": [51462, 407, 498, 321, 722, 365, 341, 1401, 1398, 3089, 510, 11, 264, 4195, 311, 15329, 3089, 11, 321, 362, 992, 493, 264, 51812], "temperature": 0.0, "avg_logprob": -0.17438033132842093, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.03954417631030083}, {"id": 99, "seek": 56712, "start": 567.12, "end": 574.12, "text": " some import and instantiation of a single object of this arrow data class as an example,", "tokens": [50364, 512, 974, 293, 9836, 6642, 295, 257, 2167, 2657, 295, 341, 11610, 1412, 1508, 382, 364, 1365, 11, 50714], "temperature": 0.0, "avg_logprob": -0.18206345516702402, "compression_ratio": 1.491150442477876, "no_speech_prob": 0.023771416395902634}, {"id": 100, "seek": 56712, "start": 574.52, "end": 580.16, "text": " and what happens if we call it, first it goes through barely visible, I guess.", "tokens": [50734, 293, 437, 2314, 498, 321, 818, 309, 11, 700, 309, 1709, 807, 10268, 8974, 11, 286, 2041, 13, 51016], "temperature": 0.0, "avg_logprob": -0.18206345516702402, "compression_ratio": 1.491150442477876, "no_speech_prob": 0.023771416395902634}, {"id": 101, "seek": 56712, "start": 580.16, "end": 587.16, "text": " So anyhow, this is the kind of outer layer for the C++ implemented Python package, and", "tokens": [51016, 407, 44995, 11, 341, 307, 264, 733, 295, 10847, 4583, 337, 264, 383, 25472, 12270, 15329, 7372, 11, 293, 51366], "temperature": 0.0, "avg_logprob": -0.18206345516702402, "compression_ratio": 1.491150442477876, "no_speech_prob": 0.023771416395902634}, {"id": 102, "seek": 56712, "start": 588.2, "end": 589.84, "text": " now I hope it's more visible.", "tokens": [51418, 586, 286, 1454, 309, 311, 544, 8974, 13, 51500], "temperature": 0.0, "avg_logprob": -0.18206345516702402, "compression_ratio": 1.491150442477876, "no_speech_prob": 0.023771416395902634}, {"id": 103, "seek": 56712, "start": 589.84, "end": 593.24, "text": " This is how PyBind 11, how one works with PyBind 11.", "tokens": [51500, 639, 307, 577, 9953, 33, 471, 2975, 11, 577, 472, 1985, 365, 9953, 33, 471, 2975, 13, 51670], "temperature": 0.0, "avg_logprob": -0.18206345516702402, "compression_ratio": 1.491150442477876, "no_speech_prob": 0.023771416395902634}, {"id": 104, "seek": 59324, "start": 593.24, "end": 600.16, "text": " So this is the C++ code where we define a module for Python, creating a Python class", "tokens": [50364, 407, 341, 307, 264, 383, 25472, 3089, 689, 321, 6964, 257, 10088, 337, 15329, 11, 4084, 257, 15329, 1508, 50710], "temperature": 0.0, "avg_logprob": -0.21046997664810776, "compression_ratio": 1.6076555023923444, "no_speech_prob": 0.06290785223245621}, {"id": 105, "seek": 59324, "start": 600.16, "end": 607.16, "text": " from C++ code looks roughly like this, with some templates defining the class that we", "tokens": [50710, 490, 383, 25472, 3089, 1542, 9810, 411, 341, 11, 365, 512, 21165, 17827, 264, 1508, 300, 321, 51060], "temperature": 0.0, "avg_logprob": -0.21046997664810776, "compression_ratio": 1.6076555023923444, "no_speech_prob": 0.06290785223245621}, {"id": 106, "seek": 59324, "start": 607.92, "end": 614.92, "text": " interface how to handle memory allocation and defining particular methods.", "tokens": [51098, 9226, 577, 281, 4813, 4675, 27599, 293, 17827, 1729, 7150, 13, 51448], "temperature": 0.0, "avg_logprob": -0.21046997664810776, "compression_ratio": 1.6076555023923444, "no_speech_prob": 0.06290785223245621}, {"id": 107, "seek": 59324, "start": 617.24, "end": 622.76, "text": " Here there is an init method, so a kind of constructor, and this constructor, when called,", "tokens": [51564, 1692, 456, 307, 364, 3157, 3170, 11, 370, 257, 733, 295, 47479, 11, 293, 341, 47479, 11, 562, 1219, 11, 51840], "temperature": 0.0, "avg_logprob": -0.21046997664810776, "compression_ratio": 1.6076555023923444, "no_speech_prob": 0.06290785223245621}, {"id": 108, "seek": 62276, "start": 622.8, "end": 629.8, "text": " goes through C++ code, this arrow data class that we wrap, but quickly we need on our way", "tokens": [50366, 1709, 807, 383, 25472, 3089, 11, 341, 11610, 1412, 1508, 300, 321, 7019, 11, 457, 2661, 321, 643, 322, 527, 636, 50716], "temperature": 0.0, "avg_logprob": -0.23712776367922864, "compression_ratio": 1.5504587155963303, "no_speech_prob": 0.0368986539542675}, {"id": 109, "seek": 62276, "start": 630.3199999999999, "end": 637.3199999999999, "text": " to Fortran to go into what is written here up at the top, C binded signatures for the", "tokens": [50742, 281, 11002, 4257, 281, 352, 666, 437, 307, 3720, 510, 493, 412, 264, 1192, 11, 383, 14786, 292, 32322, 337, 264, 51092], "temperature": 0.0, "avg_logprob": -0.23712776367922864, "compression_ratio": 1.5504587155963303, "no_speech_prob": 0.0368986539542675}, {"id": 110, "seek": 62276, "start": 638.2, "end": 639.36, "text": " Fortran function.", "tokens": [51136, 11002, 4257, 2445, 13, 51194], "temperature": 0.0, "avg_logprob": -0.23712776367922864, "compression_ratio": 1.5504587155963303, "no_speech_prob": 0.0368986539542675}, {"id": 111, "seek": 62276, "start": 639.36, "end": 646.36, "text": " So they cannot take exceptions, exception handling through, across these languages is essentially", "tokens": [51194, 407, 436, 2644, 747, 22847, 11, 11183, 13175, 807, 11, 2108, 613, 8650, 307, 4476, 51544], "temperature": 0.0, "avg_logprob": -0.23712776367922864, "compression_ratio": 1.5504587155963303, "no_speech_prob": 0.0368986539542675}, {"id": 112, "seek": 62276, "start": 646.36, "end": 650.92, "text": " undefined behavior, depending on the compiler.", "tokens": [51544, 674, 5666, 2001, 5223, 11, 5413, 322, 264, 31958, 13, 51772], "temperature": 0.0, "avg_logprob": -0.23712776367922864, "compression_ratio": 1.5504587155963303, "no_speech_prob": 0.0368986539542675}, {"id": 113, "seek": 65092, "start": 651.0, "end": 653.36, "text": " This is how it looks from the C++ perspective.", "tokens": [50368, 639, 307, 577, 309, 1542, 490, 264, 383, 25472, 4585, 13, 50486], "temperature": 0.0, "avg_logprob": -0.17135254112449852, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.030238816514611244}, {"id": 114, "seek": 65092, "start": 653.36, "end": 660.36, "text": " So when we look now on the C signatures here at the top, they match to what is later defined", "tokens": [50486, 407, 562, 321, 574, 586, 322, 264, 383, 32322, 510, 412, 264, 1192, 11, 436, 2995, 281, 437, 307, 1780, 7642, 50836], "temperature": 0.0, "avg_logprob": -0.17135254112449852, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.030238816514611244}, {"id": 115, "seek": 65092, "start": 663.0, "end": 667.0, "text": " in Fortran with the Fortran built in C binding module.", "tokens": [50968, 294, 11002, 4257, 365, 264, 11002, 4257, 3094, 294, 383, 17359, 10088, 13, 51168], "temperature": 0.0, "avg_logprob": -0.17135254112449852, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.030238816514611244}, {"id": 116, "seek": 65092, "start": 667.0, "end": 674.0, "text": " So whenever you see this bind C or C underscore types, these ensure within Fortran code that", "tokens": [51168, 407, 5699, 291, 536, 341, 14786, 383, 420, 383, 37556, 3467, 11, 613, 5586, 1951, 11002, 4257, 3089, 300, 51518], "temperature": 0.0, "avg_logprob": -0.17135254112449852, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.030238816514611244}, {"id": 117, "seek": 67400, "start": 675.0, "end": 682.0, "text": " we can access this code from C, and each of these routines is written for our wrapper", "tokens": [50414, 321, 393, 2105, 341, 3089, 490, 383, 11, 293, 1184, 295, 613, 33827, 307, 3720, 337, 527, 46906, 50764], "temperature": 0.0, "avg_logprob": -0.2409621066734439, "compression_ratio": 1.4970059880239521, "no_speech_prob": 0.056131020188331604}, {"id": 118, "seek": 67400, "start": 685.08, "end": 691.36, "text": " and essentially calls quickly as a fin wrapper around the original Fortran routines that", "tokens": [50918, 293, 4476, 5498, 2661, 382, 257, 962, 46906, 926, 264, 3380, 11002, 4257, 33827, 300, 51232], "temperature": 0.0, "avg_logprob": -0.2409621066734439, "compression_ratio": 1.4970059880239521, "no_speech_prob": 0.056131020188331604}, {"id": 119, "seek": 67400, "start": 691.36, "end": 692.8, "text": " we wanted to wrap.", "tokens": [51232, 321, 1415, 281, 7019, 13, 51304], "temperature": 0.0, "avg_logprob": -0.2409621066734439, "compression_ratio": 1.4970059880239521, "no_speech_prob": 0.056131020188331604}, {"id": 120, "seek": 67400, "start": 692.8, "end": 697.84, "text": " So for example, the one below spec file read arrow data.", "tokens": [51304, 407, 337, 1365, 11, 264, 472, 2507, 1608, 3991, 1401, 11610, 1412, 13, 51556], "temperature": 0.0, "avg_logprob": -0.2409621066734439, "compression_ratio": 1.4970059880239521, "no_speech_prob": 0.056131020188331604}, {"id": 121, "seek": 69784, "start": 697.84, "end": 700.96, "text": " So now we go finally to the wrapped code.", "tokens": [50364, 407, 586, 321, 352, 2721, 281, 264, 14226, 3089, 13, 50520], "temperature": 0.0, "avg_logprob": -0.25693323498680476, "compression_ratio": 1.5142857142857142, "no_speech_prob": 0.04812755808234215}, {"id": 122, "seek": 69784, "start": 700.96, "end": 707.96, "text": " This is the unmodified code that we access, and it sits in a Git submodule of the Pypartmc", "tokens": [50520, 639, 307, 264, 517, 8014, 2587, 3089, 300, 321, 2105, 11, 293, 309, 12696, 294, 257, 16939, 1422, 8014, 2271, 295, 264, 9953, 6971, 76, 66, 50870], "temperature": 0.0, "avg_logprob": -0.25693323498680476, "compression_ratio": 1.5142857142857142, "no_speech_prob": 0.04812755808234215}, {"id": 123, "seek": 69784, "start": 707.96, "end": 709.44, "text": " project.", "tokens": [50870, 1716, 13, 50944], "temperature": 0.0, "avg_logprob": -0.25693323498680476, "compression_ratio": 1.5142857142857142, "no_speech_prob": 0.04812755808234215}, {"id": 124, "seek": 69784, "start": 709.44, "end": 716.44, "text": " Now the fun starts when this Fortran code actually calls its input output layer, and", "tokens": [50944, 823, 264, 1019, 3719, 562, 341, 11002, 4257, 3089, 767, 5498, 1080, 4846, 5598, 4583, 11, 293, 51294], "temperature": 0.0, "avg_logprob": -0.25693323498680476, "compression_ratio": 1.5142857142857142, "no_speech_prob": 0.04812755808234215}, {"id": 125, "seek": 69784, "start": 720.0, "end": 727.0, "text": " there is like, usually a simulation takes something like 20 different text files to be read", "tokens": [51472, 456, 307, 411, 11, 2673, 257, 16575, 2516, 746, 411, 945, 819, 2487, 7098, 281, 312, 1401, 51822], "temperature": 0.0, "avg_logprob": -0.25693323498680476, "compression_ratio": 1.5142857142857142, "no_speech_prob": 0.04812755808234215}, {"id": 126, "seek": 72700, "start": 727.0, "end": 730.4, "text": " through, and these text files are nested.", "tokens": [50364, 807, 11, 293, 613, 2487, 7098, 366, 15646, 292, 13, 50534], "temperature": 0.0, "avg_logprob": -0.13456013708403616, "compression_ratio": 1.4482758620689655, "no_speech_prob": 0.023158272728323936}, {"id": 127, "seek": 72700, "start": 730.4, "end": 736.6, "text": " So what we've done is we replaced one of the components of the original Fortran package", "tokens": [50534, 407, 437, 321, 600, 1096, 307, 321, 10772, 472, 295, 264, 6677, 295, 264, 3380, 11002, 4257, 7372, 50844], "temperature": 0.0, "avg_logprob": -0.13456013708403616, "compression_ratio": 1.4482758620689655, "no_speech_prob": 0.023158272728323936}, {"id": 128, "seek": 72700, "start": 736.6, "end": 743.6, "text": " with our implementation that starts in Fortran, then goes through a C layer back to C++, which", "tokens": [50844, 365, 527, 11420, 300, 3719, 294, 11002, 4257, 11, 550, 1709, 807, 257, 383, 4583, 646, 281, 383, 25472, 11, 597, 51194], "temperature": 0.0, "avg_logprob": -0.13456013708403616, "compression_ratio": 1.4482758620689655, "no_speech_prob": 0.023158272728323936}, {"id": 129, "seek": 72700, "start": 750.0, "end": 752.52, "text": " then uses JSON for Fortran.", "tokens": [51514, 550, 4960, 31828, 337, 11002, 4257, 13, 51640], "temperature": 0.0, "avg_logprob": -0.13456013708403616, "compression_ratio": 1.4482758620689655, "no_speech_prob": 0.023158272728323936}, {"id": 130, "seek": 75252, "start": 752.52, "end": 759.52, "text": " So this is a C++ library that helps get very readable C++ code for using Fortran, and this", "tokens": [50364, 407, 341, 307, 257, 383, 25472, 6405, 300, 3665, 483, 588, 49857, 383, 25472, 3089, 337, 1228, 11002, 4257, 11, 293, 341, 50714], "temperature": 0.0, "avg_logprob": -0.2031587048580772, "compression_ratio": 1.454954954954955, "no_speech_prob": 0.03889432176947594}, {"id": 131, "seek": 75252, "start": 764.3199999999999, "end": 771.3199999999999, "text": " was our solution to replacing the multiple text files with what from user perspective", "tokens": [50954, 390, 527, 3827, 281, 19139, 264, 3866, 2487, 7098, 365, 437, 490, 4195, 4585, 51304], "temperature": 0.0, "avg_logprob": -0.2031587048580772, "compression_ratio": 1.454954954954955, "no_speech_prob": 0.03889432176947594}, {"id": 132, "seek": 75252, "start": 771.68, "end": 776.64, "text": " are essentially in memory, MATLAB, Julia, or Python objects.", "tokens": [51322, 366, 4476, 294, 4675, 11, 5904, 11435, 33, 11, 18551, 11, 420, 15329, 6565, 13, 51570], "temperature": 0.0, "avg_logprob": -0.2031587048580772, "compression_ratio": 1.454954954954955, "no_speech_prob": 0.03889432176947594}, {"id": 133, "seek": 75252, "start": 776.64, "end": 782.4, "text": " We also have online documentation for the project generated from the source code, and", "tokens": [51570, 492, 611, 362, 2950, 14333, 337, 264, 1716, 10833, 490, 264, 4009, 3089, 11, 293, 51858], "temperature": 0.0, "avg_logprob": -0.2031587048580772, "compression_ratio": 1.454954954954955, "no_speech_prob": 0.03889432176947594}, {"id": 134, "seek": 78240, "start": 782.52, "end": 789.52, "text": " as you can see here, for example, the types are hinted correctly.", "tokens": [50370, 382, 291, 393, 536, 510, 11, 337, 1365, 11, 264, 3467, 366, 12075, 292, 8944, 13, 50720], "temperature": 0.0, "avg_logprob": -0.32182863023546004, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.007760966196656227}, {"id": 135, "seek": 78240, "start": 791.12, "end": 798.12, "text": " So despite in principle the Fortran parameter ordering is the key, we do inform Python users", "tokens": [50800, 407, 7228, 294, 8665, 264, 11002, 4257, 13075, 21739, 307, 264, 2141, 11, 321, 360, 1356, 15329, 5022, 51150], "temperature": 0.0, "avg_logprob": -0.32182863023546004, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.007760966196656227}, {"id": 136, "seek": 78240, "start": 803.04, "end": 805.24, "text": " for the types of the arguments.", "tokens": [51396, 337, 264, 3467, 295, 264, 12869, 13, 51506], "temperature": 0.0, "avg_logprob": -0.32182863023546004, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.007760966196656227}, {"id": 137, "seek": 78240, "start": 805.24, "end": 812.24, "text": " So to start a summary, what we achieved with the Pypartmc wrapper is that we have a list", "tokens": [51506, 407, 281, 722, 257, 12691, 11, 437, 321, 11042, 365, 264, 9953, 6971, 76, 66, 46906, 307, 300, 321, 362, 257, 1329, 51856], "temperature": 0.0, "avg_logprob": -0.32182863023546004, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.007760966196656227}, {"id": 138, "seek": 81240, "start": 812.4, "end": 819.04, "text": " of different types of the wrapper, and we have a single command pip installation on Windows", "tokens": [50364, 295, 819, 3467, 295, 264, 46906, 11, 293, 321, 362, 257, 2167, 5622, 8489, 13260, 322, 8591, 50696], "temperature": 0.0, "avg_logprob": -0.569812266031901, "compression_ratio": 1.50199203187251, "no_speech_prob": 0.011644819751381874}, {"id": 139, "seek": 81240, "start": 819.04, "end": 826.04, "text": " Linux and OS X, with the exception that from Apple Silicon we are still struggling to get", "tokens": [50696, 18734, 293, 12731, 1783, 11, 365, 264, 11183, 300, 490, 6373, 25351, 321, 366, 920, 9314, 281, 483, 51046], "temperature": 0.0, "avg_logprob": -0.569812266031901, "compression_ratio": 1.50199203187251, "no_speech_prob": 0.011644819751381874}, {"id": 140, "seek": 81240, "start": 826.16, "end": 833.16, "text": " it done and help welcome, if any of you is a Fortran hacker who could help us produce", "tokens": [51052, 309, 1096, 293, 854, 2928, 11, 498, 604, 295, 291, 307, 257, 11002, 4257, 38155, 567, 727, 854, 505, 5258, 51402], "temperature": 0.0, "avg_logprob": -0.569812266031901, "compression_ratio": 1.50199203187251, "no_speech_prob": 0.011644819751381874}, {"id": 141, "seek": 81240, "start": 833.16, "end": 835.16, "text": " universal binaries.", "tokens": [51402, 11455, 5171, 4889, 13, 51502], "temperature": 0.0, "avg_logprob": -0.569812266031901, "compression_ratio": 1.50199203187251, "no_speech_prob": 0.011644819751381874}, {"id": 142, "seek": 81240, "start": 835.16, "end": 842.16, "text": " We provide access to unmodified internals of the Pypartmc underlying package from Python,", "tokens": [51502, 492, 2893, 2105, 281, 517, 8014, 2587, 2154, 1124, 295, 264, 9953, 6971, 76, 66, 14217, 7372, 490, 15329, 11, 51852], "temperature": 0.0, "avg_logprob": -0.569812266031901, "compression_ratio": 1.50199203187251, "no_speech_prob": 0.011644819751381874}, {"id": 143, "seek": 84240, "start": 842.4, "end": 844.4, "text": " MATLAB, and also C++.", "tokens": [50364, 5904, 11435, 33, 11, 293, 611, 383, 25472, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2444221019744873, "compression_ratio": 1.47196261682243, "no_speech_prob": 0.019587159156799316}, {"id": 144, "seek": 84240, "start": 844.4, "end": 850.92, "text": " So as a side effect by product of this goal of providing Python interface, we got also", "tokens": [50464, 407, 382, 257, 1252, 1802, 538, 1674, 295, 341, 3387, 295, 6530, 15329, 9226, 11, 321, 658, 611, 50790], "temperature": 0.0, "avg_logprob": -0.2444221019744873, "compression_ratio": 1.47196261682243, "no_speech_prob": 0.019587159156799316}, {"id": 145, "seek": 84240, "start": 850.92, "end": 855.92, "text": " Julia MATLAB and C++ layer.", "tokens": [50790, 18551, 5904, 11435, 33, 293, 383, 25472, 4583, 13, 51040], "temperature": 0.0, "avg_logprob": -0.2444221019744873, "compression_ratio": 1.47196261682243, "no_speech_prob": 0.019587159156799316}, {"id": 146, "seek": 84240, "start": 855.92, "end": 862.92, "text": " Probably something that might not be obvious from the original plan, and we ended up actually", "tokens": [51040, 9210, 746, 300, 1062, 406, 312, 6322, 490, 264, 3380, 1393, 11, 293, 321, 4590, 493, 767, 51390], "temperature": 0.0, "avg_logprob": -0.2444221019744873, "compression_ratio": 1.47196261682243, "no_speech_prob": 0.019587159156799316}, {"id": 147, "seek": 84240, "start": 864.52, "end": 871.04, "text": " using extensively is that this provides us with a nice tool for development of other", "tokens": [51470, 1228, 32636, 307, 300, 341, 6417, 505, 365, 257, 1481, 2290, 337, 3250, 295, 661, 51796], "temperature": 0.0, "avg_logprob": -0.2444221019744873, "compression_ratio": 1.47196261682243, "no_speech_prob": 0.019587159156799316}, {"id": 148, "seek": 87104, "start": 871.04, "end": 878.04, "text": " Python packages because we can use part mc in test shoots to verify against the established", "tokens": [50364, 15329, 17401, 570, 321, 393, 764, 644, 275, 66, 294, 1500, 20704, 281, 16888, 1970, 264, 7545, 50714], "temperature": 0.0, "avg_logprob": -0.2794218980349027, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.031933944672346115}, {"id": 149, "seek": 87104, "start": 880.88, "end": 882.88, "text": " simulation package.", "tokens": [50856, 16575, 7372, 13, 50956], "temperature": 0.0, "avg_logprob": -0.2794218980349027, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.031933944672346115}, {"id": 150, "seek": 87104, "start": 882.88, "end": 889.88, "text": " And also probably it's maybe a non-trivial way to use pip, but since C and Fortran are", "tokens": [50956, 400, 611, 1391, 309, 311, 1310, 257, 2107, 12, 83, 470, 22640, 636, 281, 764, 8489, 11, 457, 1670, 383, 293, 11002, 4257, 366, 51306], "temperature": 0.0, "avg_logprob": -0.2794218980349027, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.031933944672346115}, {"id": 151, "seek": 88988, "start": 890.04, "end": 897.04, "text": " probably not the best, are not the solutions, not the technologies where you see mainstream", "tokens": [50372, 1391, 406, 264, 1151, 11, 366, 406, 264, 6547, 11, 406, 264, 7943, 689, 291, 536, 15960, 50722], "temperature": 0.0, "avg_logprob": -0.29841973781585696, "compression_ratio": 1.4596774193548387, "no_speech_prob": 0.08738979697227478}, {"id": 152, "seek": 88988, "start": 906.76, "end": 913.76, "text": " package managers coming in or being established here, we managed to ship Fortran codes to", "tokens": [51208, 7372, 14084, 1348, 294, 420, 885, 7545, 510, 11, 321, 6453, 281, 5374, 11002, 4257, 14211, 281, 51558], "temperature": 0.0, "avg_logprob": -0.29841973781585696, "compression_ratio": 1.4596774193548387, "no_speech_prob": 0.08738979697227478}, {"id": 153, "seek": 91376, "start": 914.52, "end": 921.52, "text": " users of Windows 6 Linux different variants of binary packages through pip.", "tokens": [50402, 5022, 295, 8591, 1386, 18734, 819, 21669, 295, 17434, 17401, 807, 8489, 13, 50752], "temperature": 0.0, "avg_logprob": -0.1824847481467507, "compression_ratio": 1.348314606741573, "no_speech_prob": 0.023059498518705368}, {"id": 154, "seek": 91376, "start": 921.88, "end": 928.88, "text": " So it's essentially probably one way of thinking of the PyPy.org platform.", "tokens": [50770, 407, 309, 311, 4476, 1391, 472, 636, 295, 1953, 295, 264, 9953, 47, 88, 13, 4646, 3663, 13, 51120], "temperature": 0.0, "avg_logprob": -0.1824847481467507, "compression_ratio": 1.348314606741573, "no_speech_prob": 0.023059498518705368}, {"id": 155, "seek": 91376, "start": 932.48, "end": 937.68, "text": " And from the point of view of what I mentioned earlier, providing students or researchers", "tokens": [51300, 400, 490, 264, 935, 295, 1910, 295, 437, 286, 2835, 3071, 11, 6530, 1731, 420, 10309, 51560], "temperature": 0.0, "avg_logprob": -0.1824847481467507, "compression_ratio": 1.348314606741573, "no_speech_prob": 0.023059498518705368}, {"id": 156, "seek": 93768, "start": 937.68, "end": 944.68, "text": " using this package with tool to disseminate their research workflows, including input", "tokens": [50364, 1228, 341, 7372, 365, 2290, 281, 34585, 473, 641, 2132, 43461, 11, 3009, 4846, 50714], "temperature": 0.0, "avg_logprob": -0.2392623113549274, "compression_ratio": 1.587719298245614, "no_speech_prob": 0.0662304237484932}, {"id": 157, "seek": 93768, "start": 945.56, "end": 950.9599999999999, "text": " data, output data analysis workflow in a single, for example, Jupyter file for a paper peer", "tokens": [50758, 1412, 11, 5598, 1412, 5215, 20993, 294, 257, 2167, 11, 337, 1365, 11, 22125, 88, 391, 3991, 337, 257, 3035, 15108, 51028], "temperature": 0.0, "avg_logprob": -0.2392623113549274, "compression_ratio": 1.587719298245614, "no_speech_prob": 0.0662304237484932}, {"id": 158, "seek": 93768, "start": 950.9599999999999, "end": 952.5999999999999, "text": " review.", "tokens": [51028, 3131, 13, 51110], "temperature": 0.0, "avg_logprob": -0.2392623113549274, "compression_ratio": 1.587719298245614, "no_speech_prob": 0.0662304237484932}, {"id": 159, "seek": 93768, "start": 952.5999999999999, "end": 959.5999999999999, "text": " And finally, PyPy.org mc allows to extend the Fortran code with some Python logic.", "tokens": [51110, 400, 2721, 11, 9953, 47, 88, 13, 4646, 275, 66, 4045, 281, 10101, 264, 11002, 4257, 3089, 365, 512, 15329, 9952, 13, 51460], "temperature": 0.0, "avg_logprob": -0.2392623113549274, "compression_ratio": 1.587719298245614, "no_speech_prob": 0.0662304237484932}, {"id": 160, "seek": 93768, "start": 959.68, "end": 966.68, "text": " So since we interface with, we expose the internals of the package, we can do in a simulation", "tokens": [51464, 407, 1670, 321, 9226, 365, 11, 321, 19219, 264, 2154, 1124, 295, 264, 7372, 11, 321, 393, 360, 294, 257, 16575, 51814], "temperature": 0.0, "avg_logprob": -0.2392623113549274, "compression_ratio": 1.587719298245614, "no_speech_prob": 0.0662304237484932}, {"id": 161, "seek": 96668, "start": 967.5999999999999, "end": 970.5999999999999, "text": " the time stepping can actually be done from Python.", "tokens": [50410, 264, 565, 16821, 393, 767, 312, 1096, 490, 15329, 13, 50560], "temperature": 0.0, "avg_logprob": -0.20260875256030592, "compression_ratio": 1.528497409326425, "no_speech_prob": 0.02196885272860527}, {"id": 162, "seek": 96668, "start": 970.5999999999999, "end": 977.5999999999999, "text": " And you can add to, let's say, if you have 10 different steps of the simulation done", "tokens": [50560, 400, 291, 393, 909, 281, 11, 718, 311, 584, 11, 498, 291, 362, 1266, 819, 4439, 295, 264, 16575, 1096, 50910], "temperature": 0.0, "avg_logprob": -0.20260875256030592, "compression_ratio": 1.528497409326425, "no_speech_prob": 0.02196885272860527}, {"id": 163, "seek": 96668, "start": 977.64, "end": 982.64, "text": " in Fortran, you can add an 11th one that is in Python, Julia or whatever.", "tokens": [50912, 294, 11002, 4257, 11, 291, 393, 909, 364, 2975, 392, 472, 300, 307, 294, 15329, 11, 18551, 420, 2035, 13, 51162], "temperature": 0.0, "avg_logprob": -0.20260875256030592, "compression_ratio": 1.528497409326425, "no_speech_prob": 0.02196885272860527}, {"id": 164, "seek": 96668, "start": 982.64, "end": 989.64, "text": " And the final point is probably one of the key things here is that having statically", "tokens": [51162, 400, 264, 2572, 935, 307, 1391, 472, 295, 264, 2141, 721, 510, 307, 300, 1419, 2219, 984, 51512], "temperature": 0.0, "avg_logprob": -0.20260875256030592, "compression_ratio": 1.528497409326425, "no_speech_prob": 0.02196885272860527}, {"id": 165, "seek": 98964, "start": 989.92, "end": 994.92, "text": " linked all the dependencies, we can actually use the package on platforms such as Colab", "tokens": [50378, 9408, 439, 264, 36606, 11, 321, 393, 767, 764, 264, 7372, 322, 9473, 1270, 382, 4004, 455, 50628], "temperature": 0.0, "avg_logprob": -0.4282745633806501, "compression_ratio": 2.0625, "no_speech_prob": 0.023037506267428398}, {"id": 166, "seek": 98964, "start": 994.92, "end": 1000.92, "text": " or Jupyter Hubs of various institutions by doing just pip install and importing what", "tokens": [50628, 420, 22125, 88, 391, 389, 5432, 295, 3683, 8142, 538, 884, 445, 8489, 3625, 293, 43866, 437, 50928], "temperature": 0.0, "avg_logprob": -0.4282745633806501, "compression_ratio": 2.0625, "no_speech_prob": 0.023037506267428398}, {"id": 167, "seek": 98964, "start": 1000.92, "end": 1007.92, "text": " otherwise would require getting a lot of dependencies and a lot of compile time stuff available.", "tokens": [50928, 5911, 576, 3651, 1242, 257, 688, 295, 36606, 293, 257, 688, 295, 31413, 565, 1507, 2435, 13, 51278], "temperature": 0.0, "avg_logprob": -0.4282745633806501, "compression_ratio": 2.0625, "no_speech_prob": 0.023037506267428398}, {"id": 168, "seek": 98964, "start": 1008.76, "end": 1009.92, "text": " Take home messages.", "tokens": [51320, 3664, 1280, 7897, 13, 51378], "temperature": 0.0, "avg_logprob": -0.4282745633806501, "compression_ratio": 2.0625, "no_speech_prob": 0.023037506267428398}, {"id": 169, "seek": 98964, "start": 1009.92, "end": 1014.92, "text": " So I wanted to kind of give you a little bit of a little bit of a little bit of a little", "tokens": [51378, 407, 286, 1415, 281, 733, 295, 976, 291, 257, 707, 857, 295, 257, 707, 857, 295, 257, 707, 857, 295, 257, 707, 51628], "temperature": 0.0, "avg_logprob": -0.4282745633806501, "compression_ratio": 2.0625, "no_speech_prob": 0.023037506267428398}, {"id": 170, "seek": 98964, "start": 1014.92, "end": 1018.92, "text": " bit of a little bit of a little bit of a little bit of a little bit of a little bit", "tokens": [51628, 857, 295, 257, 707, 857, 295, 257, 707, 857, 295, 257, 707, 857, 295, 257, 707, 857, 295, 257, 707, 857, 51828], "temperature": 0.0, "avg_logprob": -0.4282745633806501, "compression_ratio": 2.0625, "no_speech_prob": 0.023037506267428398}, {"id": 171, "seek": 101892, "start": 1019.06, "end": 1025.32, "text": " kind of underline that PyBind 11, despite being a C++ tool is actually a valuable thing", "tokens": [50371, 733, 295, 833, 1889, 300, 9953, 33, 471, 2975, 11, 7228, 885, 257, 383, 25472, 2290, 307, 767, 257, 8263, 551, 50684], "temperature": 0.8, "avg_logprob": -0.36907271651534346, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.07894294708967209}, {"id": 172, "seek": 101892, "start": 1025.32, "end": 1028.48, "text": " for interfacing Fortran with Python.", "tokens": [50684, 337, 14510, 5615, 11002, 4257, 365, 15329, 13, 50842], "temperature": 0.8, "avg_logprob": -0.36907271651534346, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.07894294708967209}, {"id": 173, "seek": 101892, "start": 1028.48, "end": 1032.44, "text": " And this is linked to the fact that PyBind 11 offers CMake integration.", "tokens": [50842, 400, 341, 307, 9408, 281, 264, 1186, 300, 9953, 33, 471, 2975, 7736, 20424, 619, 10980, 13, 51040], "temperature": 0.8, "avg_logprob": -0.36907271651534346, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.07894294708967209}, {"id": 174, "seek": 101892, "start": 1032.44, "end": 1038.28, "text": " So your C++ projects can have build automation in CMake, and CMake handles Fortran well,", "tokens": [51040, 407, 428, 383, 25472, 4455, 393, 362, 1322, 17769, 294, 20424, 619, 11, 293, 20424, 619, 18722, 11002, 4257, 731, 11, 51332], "temperature": 0.8, "avg_logprob": -0.36907271651534346, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.07894294708967209}, {"id": 175, "seek": 101892, "start": 1038.28, "end": 1040.74, "text": " so this was the key thing here.", "tokens": [51332, 370, 341, 390, 264, 2141, 551, 510, 13, 51455], "temperature": 0.8, "avg_logprob": -0.36907271651534346, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.07894294708967209}, {"id": 176, "seek": 101892, "start": 1040.74, "end": 1047.74, "text": " The glue language role of Python is, I think, nicely exemplified here with Julia and Matlap,", "tokens": [51455, 440, 8998, 2856, 3090, 295, 15329, 307, 11, 286, 519, 11, 9594, 24112, 2587, 510, 365, 18551, 293, 6789, 75, 569, 11, 51805], "temperature": 0.8, "avg_logprob": -0.36907271651534346, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.07894294708967209}, {"id": 177, "seek": 104774, "start": 1047.74, "end": 1049.74, "text": " including CI.", "tokens": [50364, 3009, 37777, 13, 50464], "temperature": 0.0, "avg_logprob": -0.33163254681755516, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.11405805498361588}, {"id": 178, "seek": 104774, "start": 1050.6200000000001, "end": 1055.26, "text": " Static linkage of the dependencies was essential for us, for example, due to the fact that", "tokens": [50508, 745, 2399, 49118, 295, 264, 36606, 390, 7115, 337, 505, 11, 337, 1365, 11, 3462, 281, 264, 1186, 300, 50740], "temperature": 0.0, "avg_logprob": -0.33163254681755516, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.11405805498361588}, {"id": 179, "seek": 104774, "start": 1055.86, "end": 1059.02, "text": " there is no standardized ABI for four different", "tokens": [50770, 456, 307, 572, 31677, 316, 11291, 337, 1451, 819, 50928], "temperature": 0.0, "avg_logprob": -0.33163254681755516, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.11405805498361588}, {"id": 180, "seek": 104774, "start": 1060.22, "end": 1062.22, "text": " versions, even of the same compiler,", "tokens": [50988, 9606, 11, 754, 295, 264, 912, 31958, 11, 51088], "temperature": 0.0, "avg_logprob": -0.33163254681755516, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.11405805498361588}, {"id": 181, "seek": 104774, "start": 1063.42, "end": 1065.42, "text": " have different", "tokens": [51148, 362, 819, 51248], "temperature": 0.0, "avg_logprob": -0.33163254681755516, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.11405805498361588}, {"id": 182, "seek": 104774, "start": 1067.54, "end": 1071.34, "text": " binary incompatibilities, and this was essential to get it working on on", "tokens": [51354, 17434, 40393, 267, 8261, 11, 293, 341, 390, 7115, 281, 483, 309, 1364, 322, 322, 51544], "temperature": 0.0, "avg_logprob": -0.33163254681755516, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.11405805498361588}, {"id": 183, "seek": 104774, "start": 1072.06, "end": 1074.7, "text": " platforms such as Colab or other Jupyter Hubs.", "tokens": [51580, 9473, 1270, 382, 4004, 455, 420, 661, 22125, 88, 391, 389, 5432, 13, 51712], "temperature": 0.0, "avg_logprob": -0.33163254681755516, "compression_ratio": 1.528301886792453, "no_speech_prob": 0.11405805498361588}, {"id": 184, "seek": 107470, "start": 1075.26, "end": 1081.3400000000001, "text": " But this prevented us from from publishing the package on KONDA due to KONDA policy of no static linkage.", "tokens": [50392, 583, 341, 27314, 505, 490, 490, 17832, 264, 7372, 322, 591, 1928, 7509, 3462, 281, 591, 1928, 7509, 3897, 295, 572, 13437, 49118, 13, 50696], "temperature": 0.0, "avg_logprob": -0.2951221466064453, "compression_ratio": 1.4267241379310345, "no_speech_prob": 0.028758931905031204}, {"id": 185, "seek": 107470, "start": 1082.06, "end": 1085.42, "text": " We've used more than 10 Git submodules for", "tokens": [50732, 492, 600, 1143, 544, 813, 1266, 16939, 1422, 8014, 3473, 337, 50900], "temperature": 0.0, "avg_logprob": -0.2951221466064453, "compression_ratio": 1.4267241379310345, "no_speech_prob": 0.028758931905031204}, {"id": 186, "seek": 107470, "start": 1087.18, "end": 1090.02, "text": " tracking our dependencies from the GitHub repo.", "tokens": [50988, 11603, 527, 36606, 490, 264, 23331, 49040, 13, 51130], "temperature": 0.0, "avg_logprob": -0.2951221466064453, "compression_ratio": 1.4267241379310345, "no_speech_prob": 0.028758931905031204}, {"id": 187, "seek": 107470, "start": 1090.82, "end": 1094.14, "text": " As I mentioned, help welcome in getting the universal binaries", "tokens": [51170, 1018, 286, 2835, 11, 854, 2928, 294, 1242, 264, 11455, 5171, 4889, 51336], "temperature": 0.0, "avg_logprob": -0.2951221466064453, "compression_ratio": 1.4267241379310345, "no_speech_prob": 0.028758931905031204}, {"id": 188, "seek": 107470, "start": 1095.26, "end": 1097.26, "text": " generated with G4tran.", "tokens": [51392, 10833, 365, 460, 19, 83, 4257, 13, 51492], "temperature": 0.0, "avg_logprob": -0.2951221466064453, "compression_ratio": 1.4267241379310345, "no_speech_prob": 0.028758931905031204}, {"id": 189, "seek": 107470, "start": 1098.98, "end": 1100.98, "text": " The CI on", "tokens": [51578, 440, 37777, 322, 51678], "temperature": 0.0, "avg_logprob": -0.2951221466064453, "compression_ratio": 1.4267241379310345, "no_speech_prob": 0.028758931905031204}, {"id": 190, "seek": 107470, "start": 1100.98, "end": 1103.54, "text": " using MATLAB is possible thanks to the", "tokens": [51678, 1228, 5904, 11435, 33, 307, 1944, 3231, 281, 264, 51806], "temperature": 0.0, "avg_logprob": -0.2951221466064453, "compression_ratio": 1.4267241379310345, "no_speech_prob": 0.028758931905031204}, {"id": 191, "seek": 110354, "start": 1104.02, "end": 1108.6599999999999, "text": " MATLAB actions. So the producer of MATLAB MapWorks offers", "tokens": [50388, 5904, 11435, 33, 5909, 13, 407, 264, 12314, 295, 5904, 11435, 33, 22053, 28846, 82, 7736, 50620], "temperature": 0.0, "avg_logprob": -0.27134566836886936, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.025720901787281036}, {"id": 192, "seek": 110354, "start": 1111.94, "end": 1116.7, "text": " CI, GitHub actions that actually do not require any MATLAB license.", "tokens": [50784, 37777, 11, 23331, 5909, 300, 767, 360, 406, 3651, 604, 5904, 11435, 33, 10476, 13, 51022], "temperature": 0.0, "avg_logprob": -0.27134566836886936, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.025720901787281036}, {"id": 193, "seek": 110354, "start": 1116.7, "end": 1122.86, "text": " So if one wants to run MATLAB code on GitHub, this is important and just wanted to thank them. And finally,", "tokens": [51022, 407, 498, 472, 2738, 281, 1190, 5904, 11435, 33, 3089, 322, 23331, 11, 341, 307, 1021, 293, 445, 1415, 281, 1309, 552, 13, 400, 2721, 11, 51330], "temperature": 0.0, "avg_logprob": -0.27134566836886936, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.025720901787281036}, {"id": 194, "seek": 110354, "start": 1123.3, "end": 1129.02, "text": " a fun fact or the positive thing that actually when we submitted the paper about the project to the Software X Journal,", "tokens": [51352, 257, 1019, 1186, 420, 264, 3353, 551, 300, 767, 562, 321, 14405, 264, 3035, 466, 264, 1716, 281, 264, 27428, 1783, 16936, 11, 51638], "temperature": 0.0, "avg_logprob": -0.27134566836886936, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.025720901787281036}, {"id": 195, "seek": 112902, "start": 1129.86, "end": 1134.98, "text": " just reporting that during the peer review, the reviewers indeed tried the code and", "tokens": [50406, 445, 10031, 300, 1830, 264, 15108, 3131, 11, 264, 45837, 6451, 3031, 264, 3089, 293, 50662], "temperature": 0.0, "avg_logprob": -0.21891318261623383, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.05407320335507393}, {"id": 196, "seek": 112902, "start": 1135.34, "end": 1138.78, "text": " provided us with feedback that also helped. So this was kind of positive that", "tokens": [50680, 5649, 505, 365, 5824, 300, 611, 4254, 13, 407, 341, 390, 733, 295, 3353, 300, 50852], "temperature": 0.0, "avg_logprob": -0.21891318261623383, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.05407320335507393}, {"id": 197, "seek": 112902, "start": 1139.34, "end": 1147.7, "text": " it did work. Let me acknowledge funding from US National Science Foundation and Polish National Science Center", "tokens": [50880, 309, 630, 589, 13, 961, 385, 10692, 6137, 490, 2546, 4862, 8976, 10335, 293, 18504, 4862, 8976, 5169, 51298], "temperature": 0.0, "avg_logprob": -0.21891318261623383, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.05407320335507393}, {"id": 198, "seek": 112902, "start": 1148.1399999999999, "end": 1150.54, "text": " and thank you for your attention.", "tokens": [51320, 293, 1309, 291, 337, 428, 3202, 13, 51440], "temperature": 0.0, "avg_logprob": -0.21891318261623383, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.05407320335507393}, {"id": 199, "seek": 115902, "start": 1160.02, "end": 1162.02, "text": " Any questions?", "tokens": [50414, 2639, 1651, 30, 50514], "temperature": 0.0, "avg_logprob": -0.43915821075439454, "compression_ratio": 1.3237410071942446, "no_speech_prob": 0.0029304474592208862}, {"id": 200, "seek": 115902, "start": 1170.02, "end": 1178.02, "text": " Yes, thank you for that presentation. My question was exactly did you keep in Fortran and what did you pass to", "tokens": [50914, 1079, 11, 1309, 291, 337, 300, 5860, 13, 1222, 1168, 390, 2293, 630, 291, 1066, 294, 11002, 4257, 293, 437, 630, 291, 1320, 281, 51314], "temperature": 0.0, "avg_logprob": -0.43915821075439454, "compression_ratio": 1.3237410071942446, "no_speech_prob": 0.0029304474592208862}, {"id": 201, "seek": 115902, "start": 1178.02, "end": 1184.02, "text": " Python site? So it's a race or some or just single values?", "tokens": [51314, 15329, 3621, 30, 407, 309, 311, 257, 4569, 420, 512, 420, 445, 2167, 4190, 30, 51614], "temperature": 0.0, "avg_logprob": -0.43915821075439454, "compression_ratio": 1.3237410071942446, "no_speech_prob": 0.0029304474592208862}, {"id": 202, "seek": 118402, "start": 1184.5, "end": 1192.02, "text": " So the question is about if I understand correctly what kind of data we are tackling with", "tokens": [50388, 407, 264, 1168, 307, 466, 498, 286, 1223, 8944, 437, 733, 295, 1412, 321, 366, 34415, 365, 50764], "temperature": 0.0, "avg_logprob": -0.3611858523621851, "compression_ratio": 1.4539473684210527, "no_speech_prob": 0.006177200935781002}, {"id": 203, "seek": 118402, "start": 1194.02, "end": 1198.02, "text": " passing us during the simulation. So it's a", "tokens": [50864, 8437, 505, 1830, 264, 16575, 13, 407, 309, 311, 257, 51064], "temperature": 0.0, "avg_logprob": -0.3611858523621851, "compression_ratio": 1.4539473684210527, "no_speech_prob": 0.006177200935781002}, {"id": 204, "seek": 118402, "start": 1200.02, "end": 1206.02, "text": " the Monte Carlo simulations here are tracking particles in kind of attribute space that", "tokens": [51164, 264, 38105, 45112, 35138, 510, 366, 11603, 10007, 294, 733, 295, 19667, 1901, 300, 51464], "temperature": 0.0, "avg_logprob": -0.3611858523621851, "compression_ratio": 1.4539473684210527, "no_speech_prob": 0.006177200935781002}, {"id": 205, "seek": 120602, "start": 1207.02, "end": 1216.02, "text": " tracks their physical and chemical properties. So it's usually 20, 30 dimensional attribute space that is randomly", "tokens": [50414, 10218, 641, 4001, 293, 7313, 7221, 13, 407, 309, 311, 2673, 945, 11, 2217, 18795, 19667, 1901, 300, 307, 16979, 50864], "temperature": 0.0, "avg_logprob": -0.13665464345146627, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.042282141745090485}, {"id": 206, "seek": 120602, "start": 1216.02, "end": 1222.02, "text": " sampled. So we have vectors of these particles in this attribute space. So usually this could be", "tokens": [50864, 3247, 15551, 13, 407, 321, 362, 18875, 295, 613, 10007, 294, 341, 19667, 1901, 13, 407, 2673, 341, 727, 312, 51164], "temperature": 0.0, "avg_logprob": -0.13665464345146627, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.042282141745090485}, {"id": 207, "seek": 120602, "start": 1223.02, "end": 1224.02, "text": " from thousands to", "tokens": [51214, 490, 5383, 281, 51264], "temperature": 0.0, "avg_logprob": -0.13665464345146627, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.042282141745090485}, {"id": 208, "seek": 120602, "start": 1226.02, "end": 1232.02, "text": " hundreds of thousands of particles that each of the particle has like 30 attributes.", "tokens": [51364, 6779, 295, 5383, 295, 10007, 300, 1184, 295, 264, 12359, 575, 411, 2217, 17212, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13665464345146627, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.042282141745090485}, {"id": 209, "seek": 123202, "start": 1233.02, "end": 1244.02, "text": " From Python perspective, usually the user does not really use the roll data of the simulation, the state vector, just some aggregate information", "tokens": [50414, 3358, 15329, 4585, 11, 2673, 264, 4195, 775, 406, 534, 764, 264, 3373, 1412, 295, 264, 16575, 11, 264, 1785, 8062, 11, 445, 512, 26118, 1589, 50964], "temperature": 0.0, "avg_logprob": -0.24268786112467447, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.019753793254494667}, {"id": 210, "seek": 123202, "start": 1245.02, "end": 1259.02, "text": " which is passed back to Python as enumerables that can be used with NAMPy, but we don't actually assume that it must be NAMPy. So one can use just lists if they are enough.", "tokens": [51014, 597, 307, 4678, 646, 281, 15329, 382, 465, 15583, 2965, 300, 393, 312, 1143, 365, 426, 2865, 47, 88, 11, 457, 321, 500, 380, 767, 6552, 300, 309, 1633, 312, 426, 2865, 47, 88, 13, 407, 472, 393, 764, 445, 14511, 498, 436, 366, 1547, 13, 51714], "temperature": 0.0, "avg_logprob": -0.24268786112467447, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.019753793254494667}, {"id": 211, "seek": 125902, "start": 1260.02, "end": 1262.02, "text": " I hope that answers.", "tokens": [50414, 286, 1454, 300, 6338, 13, 50514], "temperature": 0.0, "avg_logprob": -0.26081214480929904, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.009641493670642376}, {"id": 212, "seek": 125902, "start": 1263.02, "end": 1276.02, "text": " My question is just because we need some roll data from Fortran site to Python site and then it's just some two dimensional matter. Here we have some problems that we need to know where we keep the data.", "tokens": [50564, 1222, 1168, 307, 445, 570, 321, 643, 512, 3373, 1412, 490, 11002, 4257, 3621, 281, 15329, 3621, 293, 550, 309, 311, 445, 512, 732, 18795, 1871, 13, 1692, 321, 362, 512, 2740, 300, 321, 643, 281, 458, 689, 321, 1066, 264, 1412, 13, 51214], "temperature": 0.0, "avg_logprob": -0.26081214480929904, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.009641493670642376}, {"id": 213, "seek": 125902, "start": 1278.02, "end": 1288.02, "text": " We are not exposing particle locations in memory. They are always returned as new objects to Python because this is it is never the state vector of the simulation. It's just a", "tokens": [51314, 492, 366, 406, 33178, 12359, 9253, 294, 4675, 13, 814, 366, 1009, 8752, 382, 777, 6565, 281, 15329, 570, 341, 307, 309, 307, 1128, 264, 1785, 8062, 295, 264, 16575, 13, 467, 311, 445, 257, 51814], "temperature": 0.0, "avg_logprob": -0.26081214480929904, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.009641493670642376}, {"id": 214, "seek": 128902, "start": 1289.02, "end": 1297.02, "text": " some aggregate information that characterizes it in a simpler way. So usually we have just one dimensional enumerable.", "tokens": [50364, 512, 26118, 1589, 300, 2517, 5660, 309, 294, 257, 18587, 636, 13, 407, 2673, 321, 362, 445, 472, 18795, 465, 15583, 712, 13, 50764], "temperature": 0.0, "avg_logprob": -0.23443260192871093, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.06471553444862366}, {"id": 215, "seek": 128902, "start": 1298.02, "end": 1300.02, "text": " For you it's much more simple. Thank you.", "tokens": [50814, 1171, 291, 309, 311, 709, 544, 2199, 13, 1044, 291, 13, 50914], "temperature": 0.0, "avg_logprob": -0.23443260192871093, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.06471553444862366}, {"id": 216, "seek": 128902, "start": 1307.02, "end": 1308.02, "text": " Time for one more question.", "tokens": [51264, 6161, 337, 472, 544, 1168, 13, 51314], "temperature": 0.0, "avg_logprob": -0.23443260192871093, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.06471553444862366}, {"id": 217, "seek": 128902, "start": 1310.02, "end": 1311.02, "text": " If there is one.", "tokens": [51414, 759, 456, 307, 472, 13, 51464], "temperature": 0.0, "avg_logprob": -0.23443260192871093, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.06471553444862366}, {"id": 218, "seek": 131102, "start": 1311.02, "end": 1323.02, "text": " Okay, if not we'll wrap up here because apparently there's a queue outside to get in for the next talks. Thank you. Thank you very much.", "tokens": [50414, 1033, 11, 498, 406, 321, 603, 7019, 493, 510, 570, 7970, 456, 311, 257, 18639, 2380, 281, 483, 294, 337, 264, 958, 6686, 13, 1044, 291, 13, 1044, 291, 588, 709, 13, 50964], "temperature": 0.0, "avg_logprob": -0.24504855019705638, "compression_ratio": 1.2477064220183487, "no_speech_prob": 0.09258869290351868}], "language": "en"}