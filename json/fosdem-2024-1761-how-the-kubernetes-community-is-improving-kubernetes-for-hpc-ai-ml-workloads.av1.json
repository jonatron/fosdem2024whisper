{"text": " about Kubernetes and HBC and AI. Hello everyone. Yeah, so today I'm going to be talking to you about what the Kubernetes community is doing to improve batch workloads in general. So just a brief background about who I am. I work as a senior software engineer at Red Hat. I'm a big upstream developer in Kubernetes and OpenShift. At Red Hat I focus mostly on Cryo and KubeLit now, but I also dabble where I'm also a reviewer in the job area in Kubernetes and a project I'll talk about also called JobSet. I was a maintainer of a batch project called Armada, which was for running batch jobs across multiple Kubernetes clusters. And generally I actually started my Kubernetes experience by trying to run, trying to build a platform that could run jobs on Slurm and Kubernetes. So I kind of liked the Kubernetes aspect a little bit better in some ways, but the Slurm scheduler was a lot more easier to use in Kubernetes. But I think I saw a gap in Kubernetes and I've been kind of helping try to contribute since. So just to give a little outline, I'm going to kind of give a historical perspective about Kubernetes and how it developed and why we're in this area that we are now. I will not really be talking too much about how best to get the most performance out of your cloud vendor or what other things you need to do to get Kubernetes. I'm going to be kind of focusing on the APIs that users could use in Kubernetes. So this is my couple slides of what is Kubernetes. It's pretty complicated. But generally I've noticed that when people start using Kubernetes as a library, I like to kind of think of it as sort of a react, but for distributed systems. So you're kind of using all the Kubernetes client libraries, you're using the APIs, you're composing custom resources on top of objects and exposing them to your customers. That's kind of where I've seen a lot of companies start using Kubernetes, especially when you're trying to build like a quote-unquote Kubernetes native platform. So what does that mean really for most people? Well generally I think the benefit for in this community is you have declarative API for workloads. If you're running on the cloud, failures happen, it sucks, but it does. And a lot of times your users also don't want to be told, oh yeah, you had a network failure so your job failed. Sorry, restart it. And a lot of users are pesky and they ask more and more of you as time goes on. We all know this. So and also for better or for worse, everything starts with YAML. You take it with what you want. But generally what that really means is that we have a big focus in Kubernetes on what is your API, backwards compatibility, most of the time, and also how to make it useful for people. So generally a Kubernetes cluster has not too many components, but I want to try to focus a little bit on a couple of components for this talk. So generally you have the API server which everyone talks to, CLI, whatever. NCD is your database essentially for storing all your objects in Kubernetes. The scheduler is an interesting component because it's, I think, the hardest thing for the HVC community to kind of grasp with the Kubernetes scheduler versus Slurm is Kubernetes is a scheduler focus for the node. You don't get as much fine-grained control in a slur, you get a lot more control in a slurm scheduler than you would in Kubernetes because slurm can actually target like, I don't know, sockets and everything on a node. It's much more fine-grained than Kubernetes. So I like to think of the Kubernetes scheduler as kind of a heat-seeking missile for a node. You give it hints and it just, it targets it and then your pod is on a node. So in the node, what is actually on a node? Well, there's this thing called KubeLit which talks to the container runtime and actually I will talk about that next slide. So the point of KubeLit is to actually start a pod, but I want to walk through what actually happens with a pod. Like this is, you know, step one, a user creates a pod that's a workload and it goes to the API server, the API server stores it in that CD and then the scheduler says, oh, you don't have a node specified on your pod. Okay, let me do a little scheduling loop, finding a node. And then once it's, once your pod is located on a node, KubeLit will pick it up and actually start running it and if you're running a batch job, it will run into completion. If you're running a microservices, it's just there and it keeps running. And KubeLit actually talks to a container runtime and the host. KubeLit also handles a lot of stuff with volumes. It's a pretty, it does a lot. So now you saw the pod lifecycle and I'll be honest, my first time using Kubernetes, I was like, deployment, stateful sets, this is so complicated. I'm just going to use a pod. Unfortunately, I learned pretty early on that you kind of lose a lot of the benefits of Kubernetes if you're using pods directly. Pods are stateless, so if your node goes down, you essentially lose your pod. And a lot of times if your cluster is overworked, you're actually going to lose, you, well, not overworked, but your pods will get deleted after a while. You also don't get self-healing. That is an important part of Kubernetes, even in, I think, the batch community. It just means that when you define an API, things are going to keep running and if you have, like, a job, you are going to keep retrying, is one example. The more pragmatic thing is the pod API fits the need in both microservices and the batch area, and you cannot really change it for one area, not the other. So generally, I don't recommend people using learning stuff that people like. Unicorn is actually, it's more popular in Spark community. It's trying to bring the yarn scheduler to Kubernetes by replacing or by adding a separate scheduler. And then MCAT is a project from IBM around trying to deploy arbitrary objects to multiple Kubernetes clusters and adding its own queuing. So now, what does this mean when you have all these projects? Well, you have chaos. You have Kubeflow, I'll pick on Kubeflow a little bit. I only have two machine learning frameworks, but from the last I checked, there's like six different APIs for representing a machine learning job in Kubeflow. And that means that there is a lot of APIs for running a batch job from Kubeflow. They are trying to consolidate most of them into a single one called a training operator. Still, you have a new API. You have two versions of running MPI jobs on Kubeflow. Now, it isn't as, I actually don't know if that MPI operator fits for all the use cases that people can give with MPI, but it is, as far as I know, the only public open-source way of running MPI on Kubernetes. And you also have things from Armada and Volcano that have their own representation of jobs. Well, this is honestly pretty chaotic. It's not really fun as a developer to be told, like, you know, how many, like if people want to bring a new API, can you support them? And you say no, because we don't really want to install all of Kubeflow just so you could run a PyTorch job or whatever, or install the controller. And it gets kind of complicated. So this group was founded, it's like a working group in the Kubernetes community. Batch workloads run the full gamut on Kubernetes from the scheduling all the way to the node to some representation of the batch APIs. So they actually had to form a working group to kind of coordinate, not really have to, but it's kind of a way to sort of allow you to focus multiple people on a single area and try to improve it. And some of the goals of this group are, let's make the batch API useful again. Let's allow people to actually use these APIs without having to install something like Kubeflow or Volcano to run a batch job. And also, the other one I'll talk about is queuing. Carlos over there could probably talk to you all about DRA, which is another exciting area that's happening, and that's about getting more use out of the GPUs, and that is in scope of this group, but that is actually mostly led by NVIDIA and Intel right now. And I'll be focusing on the two bullet points for the rest of this talk. So what is the job API? Well, this is generally a pretty simple way of representing a batch job, and I think that's one of the downsides of it, is that it was really focused originally on kind of simple use cases. I have an example here of computing Pi, and I'll just walk through the API so you'll see it kind of repeated again and again. So generally, Kubernetes has this concept where you define a template and you define a replica. And the job API that's called parallelism, and that just means how many pods do you want running in parallel? So the first thing that I want to talk about with this group is how many of these do you want to actually are complete before you consider my job successful? Active deadline is just how long the job takes to run, and then back off limit is retry. It's kind of how the job gets some self-healing, if you will, because it just says if the job fails for any reason, I want to retry, in this case, up to the back off limit, or the default is six. And one of the first features that this group added is a pod failure policy. It's essentially a way to kind of short-circuit the retry limit, because let's say your user has a segmentation fault and they're using a GPU. You probably don't want them to be using that resource when other people could be using it, and you probably don't want to keep retrying. And there's no limit on these retries, so someone could say 10,000 retries and kind of be on that node forever or whatever. So generally, that pod failure policy was kind of a way to short-circuit that. Now, how do we actually make the job API useful for workloads that need to talk to one another, which is pretty much most of the most interesting use cases in the HPC world? Well, this is kind of this idea of an index job, is can we provide a static name and environment variable so that the applications can actually refer to a replica of a pod and not have to worry about not being able to communicate to it and not be able to say, you know, my replica zero, that's my index zero, is always going to be this, and so then you can kind of talk to it. So you could think of this as sort of being a common way in like an MPI area where you have maybe like a rank zero pod and you have a series of workers and you probably want to make sure you have a rank zero. And that's kind of the idea of an index job. Now, I should wish I would have shown a slide here, but when you couple an index job and a headless service in Kubernetes speak, you're actually able to get all these pods to talk to one another. So when the last area is if you're trying to build queuing in Kubernetes, you kind of run into this problem where this pod lifecycle, I like to kind of joke, the way I envision this lifecycle is it's kind of like a racehorse. Once you create the pod, it's just, it's running and it's never going to stop. And effectively, the why this can take down a cluster is because if you have a million of these things running, it's just an infinite loop and it's going to kind of drain all your resources of your cluster. But you still need to know kind of how many objects are being created, but you also do not want when you create the object to start this loop. So this was kind of this idea of suspend in the Kubernetes community, adding suspend to our essential queue supports, a wide range of jobs via this use of suspend. So kubere, all the kubeflow operators, a project I'll talk about next called jobset, job, and then another project called a flux, which is, I don't know what I'm going to add, but, and so this is kind of a nice thing that queue provides. So what do you do about representing a more complicated job? Well, generally the job API is only, is, you kind of have to have the same pod definition for all of your workloads. And that may not fit for a lot of use cases. So the job set was kind of created as this way to say, can we create a representation of a single job that could have maybe different pod templates and then also have its own kind of failure and success policies. So when you run these jobs at large scale, you're going to see failures and you may want to restart some jobs in case, or maybe you don't want to restart, or you want to, and I'll talk about one interesting use case of success policies. And one of our goals is Kubernetes is kind of an implementation detail. Most people don't want to know about it if you're a researcher, you just want to know I'm running this. So we kind of want to streamline the creation of stuff like index job and headless services, because we know people want to communicate with their pods. And so at a high level, the API for a job set looks very close to a job to a pod. Instead of replicating pods, we are replicating jobs. And I didn't have it specified here, but there's a replica field under the spec, which says how many replicas of my replicated job I want to create. And then inside of the inside of a replicated job is a job template. And so this job is a PyTorch job. It creates an index job with a headless service, and then it creates a single job that has four pods. And I'll show in a little demo why this is useful. And the other area that we've actually gotten quite a bit, it's one of both Volcano and Kubeflow have implemented this in their projects, is one of the main reasons why they kind of created these projects, is what do you do if you kind of have this leader worker paradigm, where your leader, let's say, is a Redis database and your workers are talking to it, or whatever, you know, a message queue. Well, I want my workers just to finish. Like, I want to say, hey, once my workers are done, my job is successful and I don't really care about the progress of the leader. And so this is kind of one of the use cases we had in mind with this project, or not, there's a lot of them, but this was one, like, can we use something called a success policy to say, I only really care about one set of jobs completion, the rest are fodder, essentially, or not fodder, but they play an important role until the workers are done, and then they're also taken down. So, how am I doing on time? Okay, so I'll walk through the demo a little bit. So generally, with a job set, you have this controller, a job set controller manager. Right now, you can check it's running, great. And I kind of, in this demo, I tried to take the PyTorch job and kind of show the template, and then try to run it as just a normal job and kind of show you what happens. You can't communicate to the service, because if you try to create this job normally, there is no service for the communicate with, and it just automatically fails. So then, what do you do? Well, you can use job set. Woo-hoo. And so, I already created the job set, and you can kind of see that with the kube control logs, I'm actually able to, the job set is running, it's doing training, using PyTorch. And also, I created a headless service called a PyTorch that's there. And so, this allows you to kind of hide all this stuff from the user. And then, I think in the next part of the demo, I'll show the success policy. Come on. Oh, well. So, I guess, I mean, it will go on for a little bit, but does anyone have any questions? Any questions? There's a couple up there. Wait, wait, wait. Who was first? Hi. Yeah, I'm very much from the Slurrem bioinformatics snake make next-flow world. So, and we have an IT department, and they have a Kubernetes cluster, so this is very interesting talk to me. But are you thinking about these kind of workflow managers that typical researchers like that use, because I was just in a high-energy physics session, they also use snake make, and they have schedulers, of course, but somehow that also has to interface. Do you have any comments on that? So, generally, we don't want to get into the... We don't want to add another workflow engine, and there's too many of them, but what I kind of view the job set is like a single node of a DAG, and one of our goals could be this, like either this job or a job set could be added to something like Airflow or Argo workflows. It's another example to kind of be like, this is a single element that you could run, rather than having, like, Argo has their own way of representing, like, what they actually run on Kubernetes, which is, you know, fine for pods, Airflow is also pods. There are a lot of other workflow engines out there. I've actually... We took a lot of inspiration and two jobs ago for me in applying bioinformatics, some of their workflow languages, and trying to get... Trying to standardize a workflow language so we could actually run across different environments. And so I'm familiar with the area, but we're trying not to be a workflow engine for this project. Thank you for the talk. I noticed that a lot of the things you were talking about seemed to play kind of in the same field, sort of where the Slurm plays. So, I don't know, a few years down the road, do you see Slurm kind of giving way to, you know, this Kubernetes-based infrastructure, or do you think they're targeting kind of different tasks, and Slurm will always have its place? That's a really good question. I was not at KubeCon North America this year, but I heard of a company called CoreWeave that was actually collaborating with SchedumD to try and kind of provide Slurm on Kubernetes. From what I understand, kind of using the Slurm scheduler, but also allowing people to run some of the more popular Kubernetes stuff, like have Kubernetes for services or Slurm for batch. Generally, everyone is kind of converging in this area. Our motto is actually taking from inspiration of HT Condor and trying to apply that to Kubernetes. And then I know that the... Sorry, I'm pulling a blank. The University of Wisconsin, who kind of created HT Condor, they're big on trying to actually use Kubernetes for a lot of some of their infrastructure also. But, and also, we do talk pretty closely with the SchedumD folks, at least in my last role, and there is a lot of interest in trying to bring Kubernetes to Slurm. And part of it is Slurm has been around a long time, and so they had to do a lot of work to just even to get in the fact of, I want to containerize Slurm in Kubernetes. Okay, great. Now, do I want to schedule a pod, or do I want to schedule a single container? And that's kind of where I can see... That's also what's kind of challenging, and the other thing is convincing more and more people to use containers, because it's great, but it's also a pain to change everything that you want to go to a container. Okay. Any more questions? So, if I understand it correctly, you're primarily optimizing that I do not schedule 10,000 pods, and then have job sets, right? Because when I think about batch processing, I do think about, let's say, CI, and then we are running like 5,000 jobs per day, and we do this with Jenkins, which actually works super great with Kubernetes plugin, but I'm not seeing enough features on this proposal to get rid of Jenkins or any other components. I'm primarily seeing a way of not overloading the cluster with pending pods. Is that right? No, I would say the main thing is trying, if you want to say run a PyTorch job, the one option is let's create, let's use Kubeflow. Fine, that will work. But what if I don't really want to use Kubeflow? What if I have my own representation? What if I want to add my own...", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " about Kubernetes and HBC and AI.", "tokens": [50364, 466, 23145, 293, 389, 7869, 293, 7318, 13, 50714], "temperature": 0.0, "avg_logprob": -0.3172397052540499, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.22468625009059906}, {"id": 1, "seek": 0, "start": 7.0, "end": 11.0, "text": " Hello everyone.", "tokens": [50714, 2425, 1518, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3172397052540499, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.22468625009059906}, {"id": 2, "seek": 0, "start": 11.0, "end": 20.0, "text": " Yeah, so today I'm going to be talking to you about what the Kubernetes community is doing to improve batch workloads in general.", "tokens": [50914, 865, 11, 370, 965, 286, 478, 516, 281, 312, 1417, 281, 291, 466, 437, 264, 23145, 1768, 307, 884, 281, 3470, 15245, 32452, 294, 2674, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3172397052540499, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.22468625009059906}, {"id": 3, "seek": 0, "start": 20.0, "end": 25.0, "text": " So just a brief background about who I am. I work as a senior software engineer at Red Hat.", "tokens": [51364, 407, 445, 257, 5353, 3678, 466, 567, 286, 669, 13, 286, 589, 382, 257, 7965, 4722, 11403, 412, 4477, 15867, 13, 51614], "temperature": 0.0, "avg_logprob": -0.3172397052540499, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.22468625009059906}, {"id": 4, "seek": 2500, "start": 25.0, "end": 30.0, "text": " I'm a big upstream developer in Kubernetes and OpenShift.", "tokens": [50364, 286, 478, 257, 955, 33915, 10754, 294, 23145, 293, 7238, 7774, 2008, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1305473072012675, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.10345558077096939}, {"id": 5, "seek": 2500, "start": 30.0, "end": 35.0, "text": " At Red Hat I focus mostly on Cryo and KubeLit now,", "tokens": [50614, 1711, 4477, 15867, 286, 1879, 5240, 322, 12267, 78, 293, 591, 1977, 43, 270, 586, 11, 50864], "temperature": 0.0, "avg_logprob": -0.1305473072012675, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.10345558077096939}, {"id": 6, "seek": 2500, "start": 35.0, "end": 42.0, "text": " but I also dabble where I'm also a reviewer in the job area in Kubernetes", "tokens": [50864, 457, 286, 611, 28964, 638, 689, 286, 478, 611, 257, 3131, 260, 294, 264, 1691, 1859, 294, 23145, 51214], "temperature": 0.0, "avg_logprob": -0.1305473072012675, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.10345558077096939}, {"id": 7, "seek": 2500, "start": 42.0, "end": 45.0, "text": " and a project I'll talk about also called JobSet.", "tokens": [51214, 293, 257, 1716, 286, 603, 751, 466, 611, 1219, 18602, 42718, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1305473072012675, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.10345558077096939}, {"id": 8, "seek": 2500, "start": 45.0, "end": 48.0, "text": " I was a maintainer of a batch project called Armada,", "tokens": [51364, 286, 390, 257, 6909, 260, 295, 257, 15245, 1716, 1219, 11893, 1538, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1305473072012675, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.10345558077096939}, {"id": 9, "seek": 2500, "start": 48.0, "end": 52.0, "text": " which was for running batch jobs across multiple Kubernetes clusters.", "tokens": [51514, 597, 390, 337, 2614, 15245, 4782, 2108, 3866, 23145, 23313, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1305473072012675, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.10345558077096939}, {"id": 10, "seek": 5200, "start": 52.0, "end": 57.0, "text": " And generally I actually started my Kubernetes experience by trying to run,", "tokens": [50364, 400, 5101, 286, 767, 1409, 452, 23145, 1752, 538, 1382, 281, 1190, 11, 50614], "temperature": 0.0, "avg_logprob": -0.1030746583015688, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.04071579501032829}, {"id": 11, "seek": 5200, "start": 57.0, "end": 62.0, "text": " trying to build a platform that could run jobs on Slurm and Kubernetes.", "tokens": [50614, 1382, 281, 1322, 257, 3663, 300, 727, 1190, 4782, 322, 6187, 26717, 293, 23145, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1030746583015688, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.04071579501032829}, {"id": 12, "seek": 5200, "start": 62.0, "end": 67.0, "text": " So I kind of liked the Kubernetes aspect a little bit better in some ways,", "tokens": [50864, 407, 286, 733, 295, 4501, 264, 23145, 4171, 257, 707, 857, 1101, 294, 512, 2098, 11, 51114], "temperature": 0.0, "avg_logprob": -0.1030746583015688, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.04071579501032829}, {"id": 13, "seek": 5200, "start": 67.0, "end": 71.0, "text": " but the Slurm scheduler was a lot more easier to use in Kubernetes.", "tokens": [51114, 457, 264, 6187, 26717, 12000, 260, 390, 257, 688, 544, 3571, 281, 764, 294, 23145, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1030746583015688, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.04071579501032829}, {"id": 14, "seek": 5200, "start": 71.0, "end": 77.0, "text": " But I think I saw a gap in Kubernetes and I've been kind of helping try to contribute since.", "tokens": [51314, 583, 286, 519, 286, 1866, 257, 7417, 294, 23145, 293, 286, 600, 668, 733, 295, 4315, 853, 281, 10586, 1670, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1030746583015688, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.04071579501032829}, {"id": 15, "seek": 7700, "start": 78.0, "end": 81.0, "text": " So just to give a little outline,", "tokens": [50414, 407, 445, 281, 976, 257, 707, 16387, 11, 50564], "temperature": 0.0, "avg_logprob": -0.05722453301413018, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.004192663822323084}, {"id": 16, "seek": 7700, "start": 81.0, "end": 84.0, "text": " I'm going to kind of give a historical perspective about Kubernetes", "tokens": [50564, 286, 478, 516, 281, 733, 295, 976, 257, 8584, 4585, 466, 23145, 50714], "temperature": 0.0, "avg_logprob": -0.05722453301413018, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.004192663822323084}, {"id": 17, "seek": 7700, "start": 84.0, "end": 88.0, "text": " and how it developed and why we're in this area that we are now.", "tokens": [50714, 293, 577, 309, 4743, 293, 983, 321, 434, 294, 341, 1859, 300, 321, 366, 586, 13, 50914], "temperature": 0.0, "avg_logprob": -0.05722453301413018, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.004192663822323084}, {"id": 18, "seek": 7700, "start": 88.0, "end": 93.0, "text": " I will not really be talking too much about how best to get the most performance", "tokens": [50914, 286, 486, 406, 534, 312, 1417, 886, 709, 466, 577, 1151, 281, 483, 264, 881, 3389, 51164], "temperature": 0.0, "avg_logprob": -0.05722453301413018, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.004192663822323084}, {"id": 19, "seek": 7700, "start": 93.0, "end": 97.0, "text": " out of your cloud vendor or what other things you need to do to get Kubernetes.", "tokens": [51164, 484, 295, 428, 4588, 24321, 420, 437, 661, 721, 291, 643, 281, 360, 281, 483, 23145, 13, 51364], "temperature": 0.0, "avg_logprob": -0.05722453301413018, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.004192663822323084}, {"id": 20, "seek": 7700, "start": 97.0, "end": 102.0, "text": " I'm going to be kind of focusing on the APIs that users could use in Kubernetes.", "tokens": [51364, 286, 478, 516, 281, 312, 733, 295, 8416, 322, 264, 21445, 300, 5022, 727, 764, 294, 23145, 13, 51614], "temperature": 0.0, "avg_logprob": -0.05722453301413018, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.004192663822323084}, {"id": 21, "seek": 7700, "start": 102.0, "end": 105.0, "text": " So this is my couple slides of what is Kubernetes.", "tokens": [51614, 407, 341, 307, 452, 1916, 9788, 295, 437, 307, 23145, 13, 51764], "temperature": 0.0, "avg_logprob": -0.05722453301413018, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.004192663822323084}, {"id": 22, "seek": 10500, "start": 105.0, "end": 108.0, "text": " It's pretty complicated.", "tokens": [50364, 467, 311, 1238, 6179, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07180177490666227, "compression_ratio": 1.7035573122529644, "no_speech_prob": 0.0032679818104952574}, {"id": 23, "seek": 10500, "start": 108.0, "end": 113.0, "text": " But generally I've noticed that when people start using Kubernetes as a library,", "tokens": [50514, 583, 5101, 286, 600, 5694, 300, 562, 561, 722, 1228, 23145, 382, 257, 6405, 11, 50764], "temperature": 0.0, "avg_logprob": -0.07180177490666227, "compression_ratio": 1.7035573122529644, "no_speech_prob": 0.0032679818104952574}, {"id": 24, "seek": 10500, "start": 113.0, "end": 117.0, "text": " I like to kind of think of it as sort of a react, but for distributed systems.", "tokens": [50764, 286, 411, 281, 733, 295, 519, 295, 309, 382, 1333, 295, 257, 4515, 11, 457, 337, 12631, 3652, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07180177490666227, "compression_ratio": 1.7035573122529644, "no_speech_prob": 0.0032679818104952574}, {"id": 25, "seek": 10500, "start": 117.0, "end": 121.0, "text": " So you're kind of using all the Kubernetes client libraries,", "tokens": [50964, 407, 291, 434, 733, 295, 1228, 439, 264, 23145, 6423, 15148, 11, 51164], "temperature": 0.0, "avg_logprob": -0.07180177490666227, "compression_ratio": 1.7035573122529644, "no_speech_prob": 0.0032679818104952574}, {"id": 26, "seek": 10500, "start": 121.0, "end": 126.0, "text": " you're using the APIs, you're composing custom resources on top of objects", "tokens": [51164, 291, 434, 1228, 264, 21445, 11, 291, 434, 715, 6110, 2375, 3593, 322, 1192, 295, 6565, 51414], "temperature": 0.0, "avg_logprob": -0.07180177490666227, "compression_ratio": 1.7035573122529644, "no_speech_prob": 0.0032679818104952574}, {"id": 27, "seek": 10500, "start": 126.0, "end": 128.0, "text": " and exposing them to your customers.", "tokens": [51414, 293, 33178, 552, 281, 428, 4581, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07180177490666227, "compression_ratio": 1.7035573122529644, "no_speech_prob": 0.0032679818104952574}, {"id": 28, "seek": 10500, "start": 128.0, "end": 132.0, "text": " That's kind of where I've seen a lot of companies start using Kubernetes,", "tokens": [51514, 663, 311, 733, 295, 689, 286, 600, 1612, 257, 688, 295, 3431, 722, 1228, 23145, 11, 51714], "temperature": 0.0, "avg_logprob": -0.07180177490666227, "compression_ratio": 1.7035573122529644, "no_speech_prob": 0.0032679818104952574}, {"id": 29, "seek": 13200, "start": 132.0, "end": 137.0, "text": " especially when you're trying to build like a quote-unquote Kubernetes native platform.", "tokens": [50364, 2318, 562, 291, 434, 1382, 281, 1322, 411, 257, 6513, 12, 409, 25016, 23145, 8470, 3663, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13376180260582307, "compression_ratio": 1.5927272727272728, "no_speech_prob": 0.002469264203682542}, {"id": 30, "seek": 13200, "start": 137.0, "end": 140.0, "text": " So what does that mean really for most people?", "tokens": [50614, 407, 437, 775, 300, 914, 534, 337, 881, 561, 30, 50764], "temperature": 0.0, "avg_logprob": -0.13376180260582307, "compression_ratio": 1.5927272727272728, "no_speech_prob": 0.002469264203682542}, {"id": 31, "seek": 13200, "start": 140.0, "end": 145.0, "text": " Well generally I think the benefit for in this community is you have declarative API for workloads.", "tokens": [50764, 1042, 5101, 286, 519, 264, 5121, 337, 294, 341, 1768, 307, 291, 362, 16694, 1166, 9362, 337, 32452, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13376180260582307, "compression_ratio": 1.5927272727272728, "no_speech_prob": 0.002469264203682542}, {"id": 32, "seek": 13200, "start": 145.0, "end": 150.0, "text": " If you're running on the cloud, failures happen, it sucks, but it does.", "tokens": [51014, 759, 291, 434, 2614, 322, 264, 4588, 11, 20774, 1051, 11, 309, 15846, 11, 457, 309, 775, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13376180260582307, "compression_ratio": 1.5927272727272728, "no_speech_prob": 0.002469264203682542}, {"id": 33, "seek": 13200, "start": 150.0, "end": 153.0, "text": " And a lot of times your users also don't want to be told,", "tokens": [51264, 400, 257, 688, 295, 1413, 428, 5022, 611, 500, 380, 528, 281, 312, 1907, 11, 51414], "temperature": 0.0, "avg_logprob": -0.13376180260582307, "compression_ratio": 1.5927272727272728, "no_speech_prob": 0.002469264203682542}, {"id": 34, "seek": 13200, "start": 153.0, "end": 155.0, "text": " oh yeah, you had a network failure so your job failed.", "tokens": [51414, 1954, 1338, 11, 291, 632, 257, 3209, 7763, 370, 428, 1691, 7612, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13376180260582307, "compression_ratio": 1.5927272727272728, "no_speech_prob": 0.002469264203682542}, {"id": 35, "seek": 13200, "start": 155.0, "end": 157.0, "text": " Sorry, restart it.", "tokens": [51514, 4919, 11, 21022, 309, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13376180260582307, "compression_ratio": 1.5927272727272728, "no_speech_prob": 0.002469264203682542}, {"id": 36, "seek": 15700, "start": 157.0, "end": 163.0, "text": " And a lot of users are pesky and they ask more and more of you as time goes on.", "tokens": [50364, 400, 257, 688, 295, 5022, 366, 9262, 4133, 293, 436, 1029, 544, 293, 544, 295, 291, 382, 565, 1709, 322, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08736680348714193, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.010306834243237972}, {"id": 37, "seek": 15700, "start": 163.0, "end": 165.0, "text": " We all know this.", "tokens": [50664, 492, 439, 458, 341, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08736680348714193, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.010306834243237972}, {"id": 38, "seek": 15700, "start": 165.0, "end": 169.0, "text": " So and also for better or for worse, everything starts with YAML.", "tokens": [50764, 407, 293, 611, 337, 1101, 420, 337, 5324, 11, 1203, 3719, 365, 398, 2865, 43, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08736680348714193, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.010306834243237972}, {"id": 39, "seek": 15700, "start": 169.0, "end": 172.0, "text": " You take it with what you want.", "tokens": [50964, 509, 747, 309, 365, 437, 291, 528, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08736680348714193, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.010306834243237972}, {"id": 40, "seek": 15700, "start": 172.0, "end": 176.0, "text": " But generally what that really means is that we have a big focus in Kubernetes", "tokens": [51114, 583, 5101, 437, 300, 534, 1355, 307, 300, 321, 362, 257, 955, 1879, 294, 23145, 51314], "temperature": 0.0, "avg_logprob": -0.08736680348714193, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.010306834243237972}, {"id": 41, "seek": 15700, "start": 176.0, "end": 181.0, "text": " on what is your API, backwards compatibility, most of the time,", "tokens": [51314, 322, 437, 307, 428, 9362, 11, 12204, 34237, 11, 881, 295, 264, 565, 11, 51564], "temperature": 0.0, "avg_logprob": -0.08736680348714193, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.010306834243237972}, {"id": 42, "seek": 15700, "start": 181.0, "end": 184.0, "text": " and also how to make it useful for people.", "tokens": [51564, 293, 611, 577, 281, 652, 309, 4420, 337, 561, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08736680348714193, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.010306834243237972}, {"id": 43, "seek": 18400, "start": 184.0, "end": 190.0, "text": " So generally a Kubernetes cluster has not too many components,", "tokens": [50364, 407, 5101, 257, 23145, 13630, 575, 406, 886, 867, 6677, 11, 50664], "temperature": 0.0, "avg_logprob": -0.15475541695781136, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0009841021383181214}, {"id": 44, "seek": 18400, "start": 190.0, "end": 195.0, "text": " but I want to try to focus a little bit on a couple of components for this talk.", "tokens": [50664, 457, 286, 528, 281, 853, 281, 1879, 257, 707, 857, 322, 257, 1916, 295, 6677, 337, 341, 751, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15475541695781136, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0009841021383181214}, {"id": 45, "seek": 18400, "start": 195.0, "end": 201.0, "text": " So generally you have the API server which everyone talks to, CLI, whatever.", "tokens": [50914, 407, 5101, 291, 362, 264, 9362, 7154, 597, 1518, 6686, 281, 11, 12855, 40, 11, 2035, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15475541695781136, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0009841021383181214}, {"id": 46, "seek": 18400, "start": 201.0, "end": 206.0, "text": " NCD is your database essentially for storing all your objects in Kubernetes.", "tokens": [51214, 426, 16508, 307, 428, 8149, 4476, 337, 26085, 439, 428, 6565, 294, 23145, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15475541695781136, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0009841021383181214}, {"id": 47, "seek": 18400, "start": 206.0, "end": 209.0, "text": " The scheduler is an interesting component because it's, I think,", "tokens": [51464, 440, 12000, 260, 307, 364, 1880, 6542, 570, 309, 311, 11, 286, 519, 11, 51614], "temperature": 0.0, "avg_logprob": -0.15475541695781136, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0009841021383181214}, {"id": 48, "seek": 20900, "start": 209.0, "end": 213.0, "text": " the hardest thing for the HVC community to kind of grasp with the Kubernetes scheduler", "tokens": [50364, 264, 13158, 551, 337, 264, 389, 53, 34, 1768, 281, 733, 295, 21743, 365, 264, 23145, 12000, 260, 50564], "temperature": 0.0, "avg_logprob": -0.1281295103185317, "compression_ratio": 1.8843283582089552, "no_speech_prob": 0.030167855322360992}, {"id": 49, "seek": 20900, "start": 213.0, "end": 218.0, "text": " versus Slurm is Kubernetes is a scheduler focus for the node.", "tokens": [50564, 5717, 6187, 26717, 307, 23145, 307, 257, 12000, 260, 1879, 337, 264, 9984, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1281295103185317, "compression_ratio": 1.8843283582089552, "no_speech_prob": 0.030167855322360992}, {"id": 50, "seek": 20900, "start": 218.0, "end": 221.0, "text": " You don't get as much fine-grained control in a slur,", "tokens": [50814, 509, 500, 380, 483, 382, 709, 2489, 12, 20735, 2001, 1969, 294, 257, 1061, 374, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1281295103185317, "compression_ratio": 1.8843283582089552, "no_speech_prob": 0.030167855322360992}, {"id": 51, "seek": 20900, "start": 221.0, "end": 225.0, "text": " you get a lot more control in a slurm scheduler than you would in Kubernetes", "tokens": [50964, 291, 483, 257, 688, 544, 1969, 294, 257, 1061, 26717, 12000, 260, 813, 291, 576, 294, 23145, 51164], "temperature": 0.0, "avg_logprob": -0.1281295103185317, "compression_ratio": 1.8843283582089552, "no_speech_prob": 0.030167855322360992}, {"id": 52, "seek": 20900, "start": 225.0, "end": 229.0, "text": " because slurm can actually target like, I don't know, sockets and everything on a node.", "tokens": [51164, 570, 1061, 26717, 393, 767, 3779, 411, 11, 286, 500, 380, 458, 11, 370, 11984, 293, 1203, 322, 257, 9984, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1281295103185317, "compression_ratio": 1.8843283582089552, "no_speech_prob": 0.030167855322360992}, {"id": 53, "seek": 20900, "start": 229.0, "end": 232.0, "text": " It's much more fine-grained than Kubernetes.", "tokens": [51364, 467, 311, 709, 544, 2489, 12, 20735, 2001, 813, 23145, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1281295103185317, "compression_ratio": 1.8843283582089552, "no_speech_prob": 0.030167855322360992}, {"id": 54, "seek": 20900, "start": 232.0, "end": 236.0, "text": " So I like to think of the Kubernetes scheduler as kind of a heat-seeking missile for a node.", "tokens": [51514, 407, 286, 411, 281, 519, 295, 264, 23145, 12000, 260, 382, 733, 295, 257, 3738, 12, 405, 38437, 19321, 337, 257, 9984, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1281295103185317, "compression_ratio": 1.8843283582089552, "no_speech_prob": 0.030167855322360992}, {"id": 55, "seek": 23600, "start": 236.0, "end": 243.0, "text": " You give it hints and it just, it targets it and then your pod is on a node.", "tokens": [50364, 509, 976, 309, 27271, 293, 309, 445, 11, 309, 12911, 309, 293, 550, 428, 2497, 307, 322, 257, 9984, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1354031171955046, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.014265886507928371}, {"id": 56, "seek": 23600, "start": 243.0, "end": 246.0, "text": " So in the node, what is actually on a node?", "tokens": [50714, 407, 294, 264, 9984, 11, 437, 307, 767, 322, 257, 9984, 30, 50864], "temperature": 0.0, "avg_logprob": -0.1354031171955046, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.014265886507928371}, {"id": 57, "seek": 23600, "start": 246.0, "end": 251.0, "text": " Well, there's this thing called KubeLit which talks to the container runtime", "tokens": [50864, 1042, 11, 456, 311, 341, 551, 1219, 591, 1977, 43, 270, 597, 6686, 281, 264, 10129, 34474, 51114], "temperature": 0.0, "avg_logprob": -0.1354031171955046, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.014265886507928371}, {"id": 58, "seek": 23600, "start": 251.0, "end": 253.0, "text": " and actually I will talk about that next slide.", "tokens": [51114, 293, 767, 286, 486, 751, 466, 300, 958, 4137, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1354031171955046, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.014265886507928371}, {"id": 59, "seek": 23600, "start": 253.0, "end": 257.0, "text": " So the point of KubeLit is to actually start a pod,", "tokens": [51214, 407, 264, 935, 295, 591, 1977, 43, 270, 307, 281, 767, 722, 257, 2497, 11, 51414], "temperature": 0.0, "avg_logprob": -0.1354031171955046, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.014265886507928371}, {"id": 60, "seek": 23600, "start": 257.0, "end": 260.0, "text": " but I want to walk through what actually happens with a pod.", "tokens": [51414, 457, 286, 528, 281, 1792, 807, 437, 767, 2314, 365, 257, 2497, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1354031171955046, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.014265886507928371}, {"id": 61, "seek": 23600, "start": 260.0, "end": 265.0, "text": " Like this is, you know, step one, a user creates a pod that's a workload", "tokens": [51564, 1743, 341, 307, 11, 291, 458, 11, 1823, 472, 11, 257, 4195, 7829, 257, 2497, 300, 311, 257, 20139, 51814], "temperature": 0.0, "avg_logprob": -0.1354031171955046, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.014265886507928371}, {"id": 62, "seek": 26500, "start": 265.0, "end": 270.0, "text": " and it goes to the API server, the API server stores it in that CD", "tokens": [50364, 293, 309, 1709, 281, 264, 9362, 7154, 11, 264, 9362, 7154, 9512, 309, 294, 300, 6743, 50614], "temperature": 0.0, "avg_logprob": -0.08738300535413954, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.003169193398207426}, {"id": 63, "seek": 26500, "start": 270.0, "end": 274.0, "text": " and then the scheduler says, oh, you don't have a node specified on your pod.", "tokens": [50614, 293, 550, 264, 12000, 260, 1619, 11, 1954, 11, 291, 500, 380, 362, 257, 9984, 22206, 322, 428, 2497, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08738300535413954, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.003169193398207426}, {"id": 64, "seek": 26500, "start": 274.0, "end": 278.0, "text": " Okay, let me do a little scheduling loop, finding a node.", "tokens": [50814, 1033, 11, 718, 385, 360, 257, 707, 29055, 6367, 11, 5006, 257, 9984, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08738300535413954, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.003169193398207426}, {"id": 65, "seek": 26500, "start": 278.0, "end": 282.0, "text": " And then once it's, once your pod is located on a node,", "tokens": [51014, 400, 550, 1564, 309, 311, 11, 1564, 428, 2497, 307, 6870, 322, 257, 9984, 11, 51214], "temperature": 0.0, "avg_logprob": -0.08738300535413954, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.003169193398207426}, {"id": 66, "seek": 26500, "start": 282.0, "end": 285.0, "text": " KubeLit will pick it up and actually start running it", "tokens": [51214, 591, 1977, 43, 270, 486, 1888, 309, 493, 293, 767, 722, 2614, 309, 51364], "temperature": 0.0, "avg_logprob": -0.08738300535413954, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.003169193398207426}, {"id": 67, "seek": 26500, "start": 285.0, "end": 288.0, "text": " and if you're running a batch job, it will run into completion.", "tokens": [51364, 293, 498, 291, 434, 2614, 257, 15245, 1691, 11, 309, 486, 1190, 666, 19372, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08738300535413954, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.003169193398207426}, {"id": 68, "seek": 26500, "start": 288.0, "end": 291.0, "text": " If you're running a microservices, it's just there and it keeps running.", "tokens": [51514, 759, 291, 434, 2614, 257, 15547, 47480, 11, 309, 311, 445, 456, 293, 309, 5965, 2614, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08738300535413954, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.003169193398207426}, {"id": 69, "seek": 29100, "start": 291.0, "end": 295.0, "text": " And KubeLit actually talks to a container runtime and the host.", "tokens": [50364, 400, 591, 1977, 43, 270, 767, 6686, 281, 257, 10129, 34474, 293, 264, 3975, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13511187744140624, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.002082099439576268}, {"id": 70, "seek": 29100, "start": 295.0, "end": 298.0, "text": " KubeLit also handles a lot of stuff with volumes.", "tokens": [50564, 591, 1977, 43, 270, 611, 18722, 257, 688, 295, 1507, 365, 22219, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13511187744140624, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.002082099439576268}, {"id": 71, "seek": 29100, "start": 298.0, "end": 301.0, "text": " It's a pretty, it does a lot.", "tokens": [50714, 467, 311, 257, 1238, 11, 309, 775, 257, 688, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13511187744140624, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.002082099439576268}, {"id": 72, "seek": 29100, "start": 301.0, "end": 304.0, "text": " So now you saw the pod lifecycle and I'll be honest,", "tokens": [50864, 407, 586, 291, 1866, 264, 2497, 45722, 293, 286, 603, 312, 3245, 11, 51014], "temperature": 0.0, "avg_logprob": -0.13511187744140624, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.002082099439576268}, {"id": 73, "seek": 29100, "start": 304.0, "end": 308.0, "text": " my first time using Kubernetes, I was like, deployment, stateful sets,", "tokens": [51014, 452, 700, 565, 1228, 23145, 11, 286, 390, 411, 11, 19317, 11, 1785, 906, 6352, 11, 51214], "temperature": 0.0, "avg_logprob": -0.13511187744140624, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.002082099439576268}, {"id": 74, "seek": 29100, "start": 308.0, "end": 311.0, "text": " this is so complicated. I'm just going to use a pod.", "tokens": [51214, 341, 307, 370, 6179, 13, 286, 478, 445, 516, 281, 764, 257, 2497, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13511187744140624, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.002082099439576268}, {"id": 75, "seek": 29100, "start": 311.0, "end": 317.0, "text": " Unfortunately, I learned pretty early on that you kind of lose a lot of the benefits of Kubernetes", "tokens": [51364, 8590, 11, 286, 3264, 1238, 2440, 322, 300, 291, 733, 295, 3624, 257, 688, 295, 264, 5311, 295, 23145, 51664], "temperature": 0.0, "avg_logprob": -0.13511187744140624, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.002082099439576268}, {"id": 76, "seek": 29100, "start": 317.0, "end": 319.0, "text": " if you're using pods directly.", "tokens": [51664, 498, 291, 434, 1228, 31925, 3838, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13511187744140624, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.002082099439576268}, {"id": 77, "seek": 31900, "start": 319.0, "end": 323.0, "text": " Pods are stateless, so if your node goes down, you essentially lose your pod.", "tokens": [50364, 12646, 82, 366, 2219, 4272, 11, 370, 498, 428, 9984, 1709, 760, 11, 291, 4476, 3624, 428, 2497, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10400637264909415, "compression_ratio": 1.6030534351145038, "no_speech_prob": 0.0014316499000415206}, {"id": 78, "seek": 31900, "start": 323.0, "end": 327.0, "text": " And a lot of times if your cluster is overworked, you're actually going to lose,", "tokens": [50564, 400, 257, 688, 295, 1413, 498, 428, 13630, 307, 670, 1902, 292, 11, 291, 434, 767, 516, 281, 3624, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10400637264909415, "compression_ratio": 1.6030534351145038, "no_speech_prob": 0.0014316499000415206}, {"id": 79, "seek": 31900, "start": 327.0, "end": 332.0, "text": " you, well, not overworked, but your pods will get deleted after a while.", "tokens": [50764, 291, 11, 731, 11, 406, 670, 1902, 292, 11, 457, 428, 31925, 486, 483, 22981, 934, 257, 1339, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10400637264909415, "compression_ratio": 1.6030534351145038, "no_speech_prob": 0.0014316499000415206}, {"id": 80, "seek": 31900, "start": 332.0, "end": 335.0, "text": " You also don't get self-healing.", "tokens": [51014, 509, 611, 500, 380, 483, 2698, 12, 675, 4270, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10400637264909415, "compression_ratio": 1.6030534351145038, "no_speech_prob": 0.0014316499000415206}, {"id": 81, "seek": 31900, "start": 335.0, "end": 339.0, "text": " That is an important part of Kubernetes, even in, I think, the batch community.", "tokens": [51164, 663, 307, 364, 1021, 644, 295, 23145, 11, 754, 294, 11, 286, 519, 11, 264, 15245, 1768, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10400637264909415, "compression_ratio": 1.6030534351145038, "no_speech_prob": 0.0014316499000415206}, {"id": 82, "seek": 31900, "start": 339.0, "end": 343.0, "text": " It just means that when you define an API, things are going to keep running", "tokens": [51364, 467, 445, 1355, 300, 562, 291, 6964, 364, 9362, 11, 721, 366, 516, 281, 1066, 2614, 51564], "temperature": 0.0, "avg_logprob": -0.10400637264909415, "compression_ratio": 1.6030534351145038, "no_speech_prob": 0.0014316499000415206}, {"id": 83, "seek": 34300, "start": 343.0, "end": 351.0, "text": " and if you have, like, a job, you are going to keep retrying, is one example.", "tokens": [50364, 293, 498, 291, 362, 11, 411, 11, 257, 1691, 11, 291, 366, 516, 281, 1066, 1533, 19076, 11, 307, 472, 1365, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14037985603014627, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.006898239720612764}, {"id": 84, "seek": 34300, "start": 351.0, "end": 356.0, "text": " The more pragmatic thing is the pod API fits the need in both microservices", "tokens": [50764, 440, 544, 46904, 551, 307, 264, 2497, 9362, 9001, 264, 643, 294, 1293, 15547, 47480, 51014], "temperature": 0.0, "avg_logprob": -0.14037985603014627, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.006898239720612764}, {"id": 85, "seek": 34300, "start": 356.0, "end": 361.0, "text": " and the batch area, and you cannot really change it for one area, not the other.", "tokens": [51014, 293, 264, 15245, 1859, 11, 293, 291, 2644, 534, 1319, 309, 337, 472, 1859, 11, 406, 264, 661, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14037985603014627, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.006898239720612764}, {"id": 86, "seek": 34300, "start": 361.0, "end": 366.0, "text": " So generally, I don't recommend people using learning stuff that people like.", "tokens": [51264, 407, 5101, 11, 286, 500, 380, 2748, 561, 1228, 2539, 1507, 300, 561, 411, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14037985603014627, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.006898239720612764}, {"id": 87, "seek": 34300, "start": 366.0, "end": 369.0, "text": " Unicorn is actually, it's more popular in Spark community.", "tokens": [51514, 1156, 23115, 307, 767, 11, 309, 311, 544, 3743, 294, 23424, 1768, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14037985603014627, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.006898239720612764}, {"id": 88, "seek": 36900, "start": 369.0, "end": 374.0, "text": " It's trying to bring the yarn scheduler to Kubernetes by replacing", "tokens": [50364, 467, 311, 1382, 281, 1565, 264, 11400, 12000, 260, 281, 23145, 538, 19139, 50614], "temperature": 0.0, "avg_logprob": -0.10255097389221192, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015154222026467323}, {"id": 89, "seek": 36900, "start": 374.0, "end": 377.0, "text": " or by adding a separate scheduler.", "tokens": [50614, 420, 538, 5127, 257, 4994, 12000, 260, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10255097389221192, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015154222026467323}, {"id": 90, "seek": 36900, "start": 377.0, "end": 384.0, "text": " And then MCAT is a project from IBM around trying to deploy arbitrary objects", "tokens": [50764, 400, 550, 8797, 2218, 307, 257, 1716, 490, 23487, 926, 1382, 281, 7274, 23211, 6565, 51114], "temperature": 0.0, "avg_logprob": -0.10255097389221192, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015154222026467323}, {"id": 91, "seek": 36900, "start": 384.0, "end": 387.0, "text": " to multiple Kubernetes clusters and adding its own queuing.", "tokens": [51114, 281, 3866, 23145, 23313, 293, 5127, 1080, 1065, 631, 9635, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10255097389221192, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015154222026467323}, {"id": 92, "seek": 36900, "start": 387.0, "end": 390.0, "text": " So now, what does this mean when you have all these projects?", "tokens": [51264, 407, 586, 11, 437, 775, 341, 914, 562, 291, 362, 439, 613, 4455, 30, 51414], "temperature": 0.0, "avg_logprob": -0.10255097389221192, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015154222026467323}, {"id": 93, "seek": 36900, "start": 390.0, "end": 392.0, "text": " Well, you have chaos.", "tokens": [51414, 1042, 11, 291, 362, 14158, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10255097389221192, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015154222026467323}, {"id": 94, "seek": 36900, "start": 392.0, "end": 396.0, "text": " You have Kubeflow, I'll pick on Kubeflow a little bit.", "tokens": [51514, 509, 362, 591, 1977, 10565, 11, 286, 603, 1888, 322, 591, 1977, 10565, 257, 707, 857, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10255097389221192, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015154222026467323}, {"id": 95, "seek": 39600, "start": 396.0, "end": 399.0, "text": " I only have two machine learning frameworks, but from the last I checked,", "tokens": [50364, 286, 787, 362, 732, 3479, 2539, 29834, 11, 457, 490, 264, 1036, 286, 10033, 11, 50514], "temperature": 0.0, "avg_logprob": -0.09826245674720177, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.011852297000586987}, {"id": 96, "seek": 39600, "start": 399.0, "end": 404.0, "text": " there's like six different APIs for representing a machine learning job in Kubeflow.", "tokens": [50514, 456, 311, 411, 2309, 819, 21445, 337, 13460, 257, 3479, 2539, 1691, 294, 591, 1977, 10565, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09826245674720177, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.011852297000586987}, {"id": 97, "seek": 39600, "start": 404.0, "end": 409.0, "text": " And that means that there is a lot of APIs for running a batch job from Kubeflow.", "tokens": [50764, 400, 300, 1355, 300, 456, 307, 257, 688, 295, 21445, 337, 2614, 257, 15245, 1691, 490, 591, 1977, 10565, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09826245674720177, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.011852297000586987}, {"id": 98, "seek": 39600, "start": 409.0, "end": 414.0, "text": " They are trying to consolidate most of them into a single one called a training operator.", "tokens": [51014, 814, 366, 1382, 281, 49521, 881, 295, 552, 666, 257, 2167, 472, 1219, 257, 3097, 12973, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09826245674720177, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.011852297000586987}, {"id": 99, "seek": 39600, "start": 414.0, "end": 417.0, "text": " Still, you have a new API.", "tokens": [51264, 8291, 11, 291, 362, 257, 777, 9362, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09826245674720177, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.011852297000586987}, {"id": 100, "seek": 39600, "start": 417.0, "end": 420.0, "text": " You have two versions of running MPI jobs on Kubeflow.", "tokens": [51414, 509, 362, 732, 9606, 295, 2614, 14146, 40, 4782, 322, 591, 1977, 10565, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09826245674720177, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.011852297000586987}, {"id": 101, "seek": 39600, "start": 420.0, "end": 425.0, "text": " Now, it isn't as, I actually don't know if that MPI operator fits for all the use cases", "tokens": [51564, 823, 11, 309, 1943, 380, 382, 11, 286, 767, 500, 380, 458, 498, 300, 14146, 40, 12973, 9001, 337, 439, 264, 764, 3331, 51814], "temperature": 0.0, "avg_logprob": -0.09826245674720177, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.011852297000586987}, {"id": 102, "seek": 42500, "start": 425.0, "end": 428.0, "text": " that people can give with MPI, but it is, as far as I know,", "tokens": [50364, 300, 561, 393, 976, 365, 14146, 40, 11, 457, 309, 307, 11, 382, 1400, 382, 286, 458, 11, 50514], "temperature": 0.0, "avg_logprob": -0.10658691823482513, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.00453000795096159}, {"id": 103, "seek": 42500, "start": 428.0, "end": 433.0, "text": " the only public open-source way of running MPI on Kubernetes.", "tokens": [50514, 264, 787, 1908, 1269, 12, 41676, 636, 295, 2614, 14146, 40, 322, 23145, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10658691823482513, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.00453000795096159}, {"id": 104, "seek": 42500, "start": 433.0, "end": 438.0, "text": " And you also have things from Armada and Volcano that have their own representation of jobs.", "tokens": [50764, 400, 291, 611, 362, 721, 490, 11893, 1538, 293, 8911, 18651, 300, 362, 641, 1065, 10290, 295, 4782, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10658691823482513, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.00453000795096159}, {"id": 105, "seek": 42500, "start": 438.0, "end": 442.0, "text": " Well, this is honestly pretty chaotic.", "tokens": [51014, 1042, 11, 341, 307, 6095, 1238, 27013, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10658691823482513, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.00453000795096159}, {"id": 106, "seek": 42500, "start": 442.0, "end": 446.0, "text": " It's not really fun as a developer to be told, like, you know, how many,", "tokens": [51214, 467, 311, 406, 534, 1019, 382, 257, 10754, 281, 312, 1907, 11, 411, 11, 291, 458, 11, 577, 867, 11, 51414], "temperature": 0.0, "avg_logprob": -0.10658691823482513, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.00453000795096159}, {"id": 107, "seek": 42500, "start": 446.0, "end": 449.0, "text": " like if people want to bring a new API, can you support them?", "tokens": [51414, 411, 498, 561, 528, 281, 1565, 257, 777, 9362, 11, 393, 291, 1406, 552, 30, 51564], "temperature": 0.0, "avg_logprob": -0.10658691823482513, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.00453000795096159}, {"id": 108, "seek": 42500, "start": 449.0, "end": 454.0, "text": " And you say no, because we don't really want to install all of Kubeflow", "tokens": [51564, 400, 291, 584, 572, 11, 570, 321, 500, 380, 534, 528, 281, 3625, 439, 295, 591, 1977, 10565, 51814], "temperature": 0.0, "avg_logprob": -0.10658691823482513, "compression_ratio": 1.6083916083916083, "no_speech_prob": 0.00453000795096159}, {"id": 109, "seek": 45400, "start": 454.0, "end": 458.0, "text": " just so you could run a PyTorch job or whatever, or install the controller.", "tokens": [50364, 445, 370, 291, 727, 1190, 257, 9953, 51, 284, 339, 1691, 420, 2035, 11, 420, 3625, 264, 10561, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11526733515213947, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002544693648815155}, {"id": 110, "seek": 45400, "start": 458.0, "end": 461.0, "text": " And it gets kind of complicated.", "tokens": [50564, 400, 309, 2170, 733, 295, 6179, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11526733515213947, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002544693648815155}, {"id": 111, "seek": 45400, "start": 461.0, "end": 467.0, "text": " So this group was founded, it's like a working group in the Kubernetes community.", "tokens": [50714, 407, 341, 1594, 390, 13234, 11, 309, 311, 411, 257, 1364, 1594, 294, 264, 23145, 1768, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11526733515213947, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002544693648815155}, {"id": 112, "seek": 45400, "start": 467.0, "end": 473.0, "text": " Batch workloads run the full gamut on Kubernetes from the scheduling all the way to the node", "tokens": [51014, 363, 852, 32452, 1190, 264, 1577, 8019, 325, 322, 23145, 490, 264, 29055, 439, 264, 636, 281, 264, 9984, 51314], "temperature": 0.0, "avg_logprob": -0.11526733515213947, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002544693648815155}, {"id": 113, "seek": 45400, "start": 473.0, "end": 476.0, "text": " to some representation of the batch APIs.", "tokens": [51314, 281, 512, 10290, 295, 264, 15245, 21445, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11526733515213947, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002544693648815155}, {"id": 114, "seek": 45400, "start": 476.0, "end": 479.0, "text": " So they actually had to form a working group to kind of coordinate,", "tokens": [51464, 407, 436, 767, 632, 281, 1254, 257, 1364, 1594, 281, 733, 295, 15670, 11, 51614], "temperature": 0.0, "avg_logprob": -0.11526733515213947, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002544693648815155}, {"id": 115, "seek": 47900, "start": 479.0, "end": 485.0, "text": " not really have to, but it's kind of a way to sort of allow you to focus", "tokens": [50364, 406, 534, 362, 281, 11, 457, 309, 311, 733, 295, 257, 636, 281, 1333, 295, 2089, 291, 281, 1879, 50664], "temperature": 0.0, "avg_logprob": -0.06918716430664062, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.028393050655722618}, {"id": 116, "seek": 47900, "start": 485.0, "end": 488.0, "text": " multiple people on a single area and try to improve it.", "tokens": [50664, 3866, 561, 322, 257, 2167, 1859, 293, 853, 281, 3470, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06918716430664062, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.028393050655722618}, {"id": 117, "seek": 47900, "start": 488.0, "end": 494.0, "text": " And some of the goals of this group are, let's make the batch API useful again.", "tokens": [50814, 400, 512, 295, 264, 5493, 295, 341, 1594, 366, 11, 718, 311, 652, 264, 15245, 9362, 4420, 797, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06918716430664062, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.028393050655722618}, {"id": 118, "seek": 47900, "start": 494.0, "end": 499.0, "text": " Let's allow people to actually use these APIs without having to install", "tokens": [51114, 961, 311, 2089, 561, 281, 767, 764, 613, 21445, 1553, 1419, 281, 3625, 51364], "temperature": 0.0, "avg_logprob": -0.06918716430664062, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.028393050655722618}, {"id": 119, "seek": 47900, "start": 499.0, "end": 503.0, "text": " something like Kubeflow or Volcano to run a batch job.", "tokens": [51364, 746, 411, 591, 1977, 10565, 420, 8911, 18651, 281, 1190, 257, 15245, 1691, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06918716430664062, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.028393050655722618}, {"id": 120, "seek": 47900, "start": 503.0, "end": 507.0, "text": " And also, the other one I'll talk about is queuing.", "tokens": [51564, 400, 611, 11, 264, 661, 472, 286, 603, 751, 466, 307, 631, 9635, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06918716430664062, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.028393050655722618}, {"id": 121, "seek": 50700, "start": 507.0, "end": 510.0, "text": " Carlos over there could probably talk to you all about DRA,", "tokens": [50364, 19646, 670, 456, 727, 1391, 751, 281, 291, 439, 466, 413, 3750, 11, 50514], "temperature": 0.0, "avg_logprob": -0.09524937333731816, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.003880277508869767}, {"id": 122, "seek": 50700, "start": 510.0, "end": 512.0, "text": " which is another exciting area that's happening,", "tokens": [50514, 597, 307, 1071, 4670, 1859, 300, 311, 2737, 11, 50614], "temperature": 0.0, "avg_logprob": -0.09524937333731816, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.003880277508869767}, {"id": 123, "seek": 50700, "start": 512.0, "end": 515.0, "text": " and that's about getting more use out of the GPUs,", "tokens": [50614, 293, 300, 311, 466, 1242, 544, 764, 484, 295, 264, 18407, 82, 11, 50764], "temperature": 0.0, "avg_logprob": -0.09524937333731816, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.003880277508869767}, {"id": 124, "seek": 50700, "start": 515.0, "end": 518.0, "text": " and that is in scope of this group,", "tokens": [50764, 293, 300, 307, 294, 11923, 295, 341, 1594, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09524937333731816, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.003880277508869767}, {"id": 125, "seek": 50700, "start": 518.0, "end": 523.0, "text": " but that is actually mostly led by NVIDIA and Intel right now.", "tokens": [50914, 457, 300, 307, 767, 5240, 4684, 538, 426, 3958, 6914, 293, 19762, 558, 586, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09524937333731816, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.003880277508869767}, {"id": 126, "seek": 50700, "start": 523.0, "end": 527.0, "text": " And I'll be focusing on the two bullet points for the rest of this talk.", "tokens": [51164, 400, 286, 603, 312, 8416, 322, 264, 732, 11632, 2793, 337, 264, 1472, 295, 341, 751, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09524937333731816, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.003880277508869767}, {"id": 127, "seek": 50700, "start": 527.0, "end": 529.0, "text": " So what is the job API?", "tokens": [51364, 407, 437, 307, 264, 1691, 9362, 30, 51464], "temperature": 0.0, "avg_logprob": -0.09524937333731816, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.003880277508869767}, {"id": 128, "seek": 50700, "start": 529.0, "end": 535.0, "text": " Well, this is generally a pretty simple way of representing a batch job,", "tokens": [51464, 1042, 11, 341, 307, 5101, 257, 1238, 2199, 636, 295, 13460, 257, 15245, 1691, 11, 51764], "temperature": 0.0, "avg_logprob": -0.09524937333731816, "compression_ratio": 1.5970149253731343, "no_speech_prob": 0.003880277508869767}, {"id": 129, "seek": 53500, "start": 535.0, "end": 537.0, "text": " and I think that's one of the downsides of it,", "tokens": [50364, 293, 286, 519, 300, 311, 472, 295, 264, 21554, 1875, 295, 309, 11, 50464], "temperature": 0.0, "avg_logprob": -0.09644010971332419, "compression_ratio": 1.6531365313653137, "no_speech_prob": 0.0016725099412724376}, {"id": 130, "seek": 53500, "start": 537.0, "end": 542.0, "text": " is that it was really focused originally on kind of simple use cases.", "tokens": [50464, 307, 300, 309, 390, 534, 5178, 7993, 322, 733, 295, 2199, 764, 3331, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09644010971332419, "compression_ratio": 1.6531365313653137, "no_speech_prob": 0.0016725099412724376}, {"id": 131, "seek": 53500, "start": 542.0, "end": 545.0, "text": " I have an example here of computing Pi,", "tokens": [50714, 286, 362, 364, 1365, 510, 295, 15866, 17741, 11, 50864], "temperature": 0.0, "avg_logprob": -0.09644010971332419, "compression_ratio": 1.6531365313653137, "no_speech_prob": 0.0016725099412724376}, {"id": 132, "seek": 53500, "start": 545.0, "end": 549.0, "text": " and I'll just walk through the API so you'll see it kind of repeated again and again.", "tokens": [50864, 293, 286, 603, 445, 1792, 807, 264, 9362, 370, 291, 603, 536, 309, 733, 295, 10477, 797, 293, 797, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09644010971332419, "compression_ratio": 1.6531365313653137, "no_speech_prob": 0.0016725099412724376}, {"id": 133, "seek": 53500, "start": 549.0, "end": 553.0, "text": " So generally, Kubernetes has this concept where you define a template", "tokens": [51064, 407, 5101, 11, 23145, 575, 341, 3410, 689, 291, 6964, 257, 12379, 51264], "temperature": 0.0, "avg_logprob": -0.09644010971332419, "compression_ratio": 1.6531365313653137, "no_speech_prob": 0.0016725099412724376}, {"id": 134, "seek": 53500, "start": 553.0, "end": 556.0, "text": " and you define a replica.", "tokens": [51264, 293, 291, 6964, 257, 35456, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09644010971332419, "compression_ratio": 1.6531365313653137, "no_speech_prob": 0.0016725099412724376}, {"id": 135, "seek": 53500, "start": 556.0, "end": 558.0, "text": " And the job API that's called parallelism,", "tokens": [51414, 400, 264, 1691, 9362, 300, 311, 1219, 8952, 1434, 11, 51514], "temperature": 0.0, "avg_logprob": -0.09644010971332419, "compression_ratio": 1.6531365313653137, "no_speech_prob": 0.0016725099412724376}, {"id": 136, "seek": 53500, "start": 558.0, "end": 562.0, "text": " and that just means how many pods do you want running in parallel?", "tokens": [51514, 293, 300, 445, 1355, 577, 867, 31925, 360, 291, 528, 2614, 294, 8952, 30, 51714], "temperature": 0.0, "avg_logprob": -0.09644010971332419, "compression_ratio": 1.6531365313653137, "no_speech_prob": 0.0016725099412724376}, {"id": 137, "seek": 56200, "start": 562.0, "end": 566.0, "text": " So the first thing that I want to talk about with this group is how many of these", "tokens": [50364, 407, 264, 700, 551, 300, 286, 528, 281, 751, 466, 365, 341, 1594, 307, 577, 867, 295, 613, 50564], "temperature": 0.0, "avg_logprob": -0.272668439950516, "compression_ratio": 1.7152777777777777, "no_speech_prob": 0.006896419450640678}, {"id": 138, "seek": 56200, "start": 566.0, "end": 570.0, "text": " do you want to actually are complete before you consider my job successful?", "tokens": [50564, 360, 291, 528, 281, 767, 366, 3566, 949, 291, 1949, 452, 1691, 4406, 30, 50764], "temperature": 0.0, "avg_logprob": -0.272668439950516, "compression_ratio": 1.7152777777777777, "no_speech_prob": 0.006896419450640678}, {"id": 139, "seek": 56200, "start": 570.0, "end": 573.0, "text": " Active deadline is just how long the job takes to run,", "tokens": [50764, 26635, 20615, 307, 445, 577, 938, 264, 1691, 2516, 281, 1190, 11, 50914], "temperature": 0.0, "avg_logprob": -0.272668439950516, "compression_ratio": 1.7152777777777777, "no_speech_prob": 0.006896419450640678}, {"id": 140, "seek": 56200, "start": 573.0, "end": 576.0, "text": " and then back off limit is retry.", "tokens": [50914, 293, 550, 646, 766, 4948, 307, 1533, 627, 13, 51064], "temperature": 0.0, "avg_logprob": -0.272668439950516, "compression_ratio": 1.7152777777777777, "no_speech_prob": 0.006896419450640678}, {"id": 141, "seek": 56200, "start": 576.0, "end": 579.0, "text": " It's kind of how the job gets some self-healing, if you will,", "tokens": [51064, 467, 311, 733, 295, 577, 264, 1691, 2170, 512, 2698, 12, 675, 4270, 11, 498, 291, 486, 11, 51214], "temperature": 0.0, "avg_logprob": -0.272668439950516, "compression_ratio": 1.7152777777777777, "no_speech_prob": 0.006896419450640678}, {"id": 142, "seek": 56200, "start": 579.0, "end": 582.0, "text": " because it just says if the job fails for any reason,", "tokens": [51214, 570, 309, 445, 1619, 498, 264, 1691, 18199, 337, 604, 1778, 11, 51364], "temperature": 0.0, "avg_logprob": -0.272668439950516, "compression_ratio": 1.7152777777777777, "no_speech_prob": 0.006896419450640678}, {"id": 143, "seek": 56200, "start": 582.0, "end": 586.0, "text": " I want to retry, in this case, up to the back off limit, or the default is six.", "tokens": [51364, 286, 528, 281, 1533, 627, 11, 294, 341, 1389, 11, 493, 281, 264, 646, 766, 4948, 11, 420, 264, 7576, 307, 2309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.272668439950516, "compression_ratio": 1.7152777777777777, "no_speech_prob": 0.006896419450640678}, {"id": 144, "seek": 56200, "start": 586.0, "end": 589.0, "text": " And one of the first features that this group added", "tokens": [51564, 400, 472, 295, 264, 700, 4122, 300, 341, 1594, 3869, 51714], "temperature": 0.0, "avg_logprob": -0.272668439950516, "compression_ratio": 1.7152777777777777, "no_speech_prob": 0.006896419450640678}, {"id": 145, "seek": 58900, "start": 589.0, "end": 591.0, "text": " is a pod failure policy.", "tokens": [50364, 307, 257, 2497, 7763, 3897, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09399537358965193, "compression_ratio": 1.795053003533569, "no_speech_prob": 0.004066111985594034}, {"id": 146, "seek": 58900, "start": 591.0, "end": 595.0, "text": " It's essentially a way to kind of short-circuit the retry limit,", "tokens": [50464, 467, 311, 4476, 257, 636, 281, 733, 295, 2099, 12, 23568, 66, 1983, 264, 1533, 627, 4948, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09399537358965193, "compression_ratio": 1.795053003533569, "no_speech_prob": 0.004066111985594034}, {"id": 147, "seek": 58900, "start": 595.0, "end": 600.0, "text": " because let's say your user has a segmentation fault and they're using a GPU.", "tokens": [50664, 570, 718, 311, 584, 428, 4195, 575, 257, 9469, 399, 7441, 293, 436, 434, 1228, 257, 18407, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09399537358965193, "compression_ratio": 1.795053003533569, "no_speech_prob": 0.004066111985594034}, {"id": 148, "seek": 58900, "start": 600.0, "end": 604.0, "text": " You probably don't want them to be using that resource when other people could be using it,", "tokens": [50914, 509, 1391, 500, 380, 528, 552, 281, 312, 1228, 300, 7684, 562, 661, 561, 727, 312, 1228, 309, 11, 51114], "temperature": 0.0, "avg_logprob": -0.09399537358965193, "compression_ratio": 1.795053003533569, "no_speech_prob": 0.004066111985594034}, {"id": 149, "seek": 58900, "start": 604.0, "end": 606.0, "text": " and you probably don't want to keep retrying.", "tokens": [51114, 293, 291, 1391, 500, 380, 528, 281, 1066, 1533, 19076, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09399537358965193, "compression_ratio": 1.795053003533569, "no_speech_prob": 0.004066111985594034}, {"id": 150, "seek": 58900, "start": 606.0, "end": 610.0, "text": " And there's no limit on these retries, so someone could say 10,000 retries", "tokens": [51214, 400, 456, 311, 572, 4948, 322, 613, 1533, 2244, 11, 370, 1580, 727, 584, 1266, 11, 1360, 1533, 2244, 51414], "temperature": 0.0, "avg_logprob": -0.09399537358965193, "compression_ratio": 1.795053003533569, "no_speech_prob": 0.004066111985594034}, {"id": 151, "seek": 58900, "start": 610.0, "end": 612.0, "text": " and kind of be on that node forever or whatever.", "tokens": [51414, 293, 733, 295, 312, 322, 300, 9984, 5680, 420, 2035, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09399537358965193, "compression_ratio": 1.795053003533569, "no_speech_prob": 0.004066111985594034}, {"id": 152, "seek": 58900, "start": 612.0, "end": 618.0, "text": " So generally, that pod failure policy was kind of a way to short-circuit that.", "tokens": [51514, 407, 5101, 11, 300, 2497, 7763, 3897, 390, 733, 295, 257, 636, 281, 2099, 12, 23568, 66, 1983, 300, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09399537358965193, "compression_ratio": 1.795053003533569, "no_speech_prob": 0.004066111985594034}, {"id": 153, "seek": 61800, "start": 618.0, "end": 627.0, "text": " Now, how do we actually make the job API useful for workloads", "tokens": [50364, 823, 11, 577, 360, 321, 767, 652, 264, 1691, 9362, 4420, 337, 32452, 50814], "temperature": 0.0, "avg_logprob": -0.11370510404760187, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.0006663000676780939}, {"id": 154, "seek": 61800, "start": 627.0, "end": 631.0, "text": " that need to talk to one another, which is pretty much most of the most interesting use cases", "tokens": [50814, 300, 643, 281, 751, 281, 472, 1071, 11, 597, 307, 1238, 709, 881, 295, 264, 881, 1880, 764, 3331, 51014], "temperature": 0.0, "avg_logprob": -0.11370510404760187, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.0006663000676780939}, {"id": 155, "seek": 61800, "start": 631.0, "end": 633.0, "text": " in the HPC world?", "tokens": [51014, 294, 264, 12557, 34, 1002, 30, 51114], "temperature": 0.0, "avg_logprob": -0.11370510404760187, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.0006663000676780939}, {"id": 156, "seek": 61800, "start": 633.0, "end": 636.0, "text": " Well, this is kind of this idea of an index job,", "tokens": [51114, 1042, 11, 341, 307, 733, 295, 341, 1558, 295, 364, 8186, 1691, 11, 51264], "temperature": 0.0, "avg_logprob": -0.11370510404760187, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.0006663000676780939}, {"id": 157, "seek": 61800, "start": 636.0, "end": 640.0, "text": " is can we provide a static name and environment variable", "tokens": [51264, 307, 393, 321, 2893, 257, 13437, 1315, 293, 2823, 7006, 51464], "temperature": 0.0, "avg_logprob": -0.11370510404760187, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.0006663000676780939}, {"id": 158, "seek": 61800, "start": 640.0, "end": 644.0, "text": " so that the applications can actually refer to a replica of a pod", "tokens": [51464, 370, 300, 264, 5821, 393, 767, 2864, 281, 257, 35456, 295, 257, 2497, 51664], "temperature": 0.0, "avg_logprob": -0.11370510404760187, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.0006663000676780939}, {"id": 159, "seek": 64400, "start": 644.0, "end": 647.0, "text": " and not have to worry about not being able to communicate to it", "tokens": [50364, 293, 406, 362, 281, 3292, 466, 406, 885, 1075, 281, 7890, 281, 309, 50514], "temperature": 0.0, "avg_logprob": -0.07482722425085353, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0222505833953619}, {"id": 160, "seek": 64400, "start": 647.0, "end": 652.0, "text": " and not be able to say, you know, my replica zero, that's my index zero,", "tokens": [50514, 293, 406, 312, 1075, 281, 584, 11, 291, 458, 11, 452, 35456, 4018, 11, 300, 311, 452, 8186, 4018, 11, 50764], "temperature": 0.0, "avg_logprob": -0.07482722425085353, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0222505833953619}, {"id": 161, "seek": 64400, "start": 652.0, "end": 656.0, "text": " is always going to be this, and so then you can kind of talk to it.", "tokens": [50764, 307, 1009, 516, 281, 312, 341, 11, 293, 370, 550, 291, 393, 733, 295, 751, 281, 309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07482722425085353, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0222505833953619}, {"id": 162, "seek": 64400, "start": 656.0, "end": 661.0, "text": " So you could think of this as sort of being a common way in like an MPI area", "tokens": [50964, 407, 291, 727, 519, 295, 341, 382, 1333, 295, 885, 257, 2689, 636, 294, 411, 364, 14146, 40, 1859, 51214], "temperature": 0.0, "avg_logprob": -0.07482722425085353, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0222505833953619}, {"id": 163, "seek": 64400, "start": 661.0, "end": 666.0, "text": " where you have maybe like a rank zero pod and you have a series of workers", "tokens": [51214, 689, 291, 362, 1310, 411, 257, 6181, 4018, 2497, 293, 291, 362, 257, 2638, 295, 5600, 51464], "temperature": 0.0, "avg_logprob": -0.07482722425085353, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0222505833953619}, {"id": 164, "seek": 64400, "start": 666.0, "end": 669.0, "text": " and you probably want to make sure you have a rank zero.", "tokens": [51464, 293, 291, 1391, 528, 281, 652, 988, 291, 362, 257, 6181, 4018, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07482722425085353, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0222505833953619}, {"id": 165, "seek": 64400, "start": 669.0, "end": 671.0, "text": " And that's kind of the idea of an index job.", "tokens": [51614, 400, 300, 311, 733, 295, 264, 1558, 295, 364, 8186, 1691, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07482722425085353, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0222505833953619}, {"id": 166, "seek": 67100, "start": 671.0, "end": 674.0, "text": " Now, I should wish I would have shown a slide here,", "tokens": [50364, 823, 11, 286, 820, 3172, 286, 576, 362, 4898, 257, 4137, 510, 11, 50514], "temperature": 0.0, "avg_logprob": -0.1164607741615989, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.001753779943101108}, {"id": 167, "seek": 67100, "start": 674.0, "end": 679.0, "text": " but when you couple an index job and a headless service in Kubernetes speak,", "tokens": [50514, 457, 562, 291, 1916, 364, 8186, 1691, 293, 257, 1378, 1832, 2643, 294, 23145, 1710, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1164607741615989, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.001753779943101108}, {"id": 168, "seek": 67100, "start": 679.0, "end": 682.0, "text": " you're actually able to get all these pods to talk to one another.", "tokens": [50764, 291, 434, 767, 1075, 281, 483, 439, 613, 31925, 281, 751, 281, 472, 1071, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1164607741615989, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.001753779943101108}, {"id": 169, "seek": 67100, "start": 682.0, "end": 690.0, "text": " So when the last area is if you're trying to build queuing in Kubernetes,", "tokens": [50914, 407, 562, 264, 1036, 1859, 307, 498, 291, 434, 1382, 281, 1322, 631, 9635, 294, 23145, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1164607741615989, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.001753779943101108}, {"id": 170, "seek": 67100, "start": 690.0, "end": 694.0, "text": " you kind of run into this problem where this pod lifecycle,", "tokens": [51314, 291, 733, 295, 1190, 666, 341, 1154, 689, 341, 2497, 45722, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1164607741615989, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.001753779943101108}, {"id": 171, "seek": 67100, "start": 694.0, "end": 698.0, "text": " I like to kind of joke, the way I envision this lifecycle is it's kind of like a racehorse.", "tokens": [51514, 286, 411, 281, 733, 295, 7647, 11, 264, 636, 286, 24739, 341, 45722, 307, 309, 311, 733, 295, 411, 257, 4569, 45079, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1164607741615989, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.001753779943101108}, {"id": 172, "seek": 69800, "start": 698.0, "end": 702.0, "text": " Once you create the pod, it's just, it's running and it's never going to stop.", "tokens": [50364, 3443, 291, 1884, 264, 2497, 11, 309, 311, 445, 11, 309, 311, 2614, 293, 309, 311, 1128, 516, 281, 1590, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07169807524908156, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.005907407030463219}, {"id": 173, "seek": 69800, "start": 702.0, "end": 706.0, "text": " And effectively, the why this can take down a cluster is because if you have", "tokens": [50564, 400, 8659, 11, 264, 983, 341, 393, 747, 760, 257, 13630, 307, 570, 498, 291, 362, 50764], "temperature": 0.0, "avg_logprob": -0.07169807524908156, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.005907407030463219}, {"id": 174, "seek": 69800, "start": 706.0, "end": 709.0, "text": " a million of these things running, it's just an infinite loop", "tokens": [50764, 257, 2459, 295, 613, 721, 2614, 11, 309, 311, 445, 364, 13785, 6367, 50914], "temperature": 0.0, "avg_logprob": -0.07169807524908156, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.005907407030463219}, {"id": 175, "seek": 69800, "start": 709.0, "end": 712.0, "text": " and it's going to kind of drain all your resources of your cluster.", "tokens": [50914, 293, 309, 311, 516, 281, 733, 295, 12339, 439, 428, 3593, 295, 428, 13630, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07169807524908156, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.005907407030463219}, {"id": 176, "seek": 69800, "start": 712.0, "end": 716.0, "text": " But you still need to know kind of how many objects are being created,", "tokens": [51064, 583, 291, 920, 643, 281, 458, 733, 295, 577, 867, 6565, 366, 885, 2942, 11, 51264], "temperature": 0.0, "avg_logprob": -0.07169807524908156, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.005907407030463219}, {"id": 177, "seek": 69800, "start": 716.0, "end": 720.0, "text": " but you also do not want when you create the object to start this loop.", "tokens": [51264, 457, 291, 611, 360, 406, 528, 562, 291, 1884, 264, 2657, 281, 722, 341, 6367, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07169807524908156, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.005907407030463219}, {"id": 178, "seek": 69800, "start": 720.0, "end": 724.0, "text": " So this was kind of this idea of suspend in the Kubernetes community,", "tokens": [51464, 407, 341, 390, 733, 295, 341, 1558, 295, 42546, 294, 264, 23145, 1768, 11, 51664], "temperature": 0.0, "avg_logprob": -0.07169807524908156, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.005907407030463219}, {"id": 179, "seek": 72400, "start": 724.0, "end": 728.0, "text": " adding suspend to our essential queue supports, a wide range of jobs", "tokens": [50364, 5127, 42546, 281, 527, 7115, 18639, 9346, 11, 257, 4874, 3613, 295, 4782, 50564], "temperature": 0.0, "avg_logprob": -0.257075657354337, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.007568030152469873}, {"id": 180, "seek": 72400, "start": 728.0, "end": 730.0, "text": " via this use of suspend.", "tokens": [50564, 5766, 341, 764, 295, 42546, 13, 50664], "temperature": 0.0, "avg_logprob": -0.257075657354337, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.007568030152469873}, {"id": 181, "seek": 72400, "start": 730.0, "end": 736.0, "text": " So kubere, all the kubeflow operators, a project I'll talk about next called jobset,", "tokens": [50664, 407, 350, 836, 323, 11, 439, 264, 350, 1977, 10565, 19077, 11, 257, 1716, 286, 603, 751, 466, 958, 1219, 1691, 3854, 11, 50964], "temperature": 0.0, "avg_logprob": -0.257075657354337, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.007568030152469873}, {"id": 182, "seek": 72400, "start": 736.0, "end": 742.0, "text": " job, and then another project called a flux, which is, I don't know what I'm going to add,", "tokens": [50964, 1691, 11, 293, 550, 1071, 1716, 1219, 257, 19298, 11, 597, 307, 11, 286, 500, 380, 458, 437, 286, 478, 516, 281, 909, 11, 51264], "temperature": 0.0, "avg_logprob": -0.257075657354337, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.007568030152469873}, {"id": 183, "seek": 72400, "start": 742.0, "end": 746.0, "text": " but, and so this is kind of a nice thing that queue provides.", "tokens": [51264, 457, 11, 293, 370, 341, 307, 733, 295, 257, 1481, 551, 300, 18639, 6417, 13, 51464], "temperature": 0.0, "avg_logprob": -0.257075657354337, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.007568030152469873}, {"id": 184, "seek": 72400, "start": 746.0, "end": 751.0, "text": " So what do you do about representing a more complicated job?", "tokens": [51464, 407, 437, 360, 291, 360, 466, 13460, 257, 544, 6179, 1691, 30, 51714], "temperature": 0.0, "avg_logprob": -0.257075657354337, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.007568030152469873}, {"id": 185, "seek": 75100, "start": 751.0, "end": 757.0, "text": " Well, generally the job API is only, is, you kind of have to have the same pod definition", "tokens": [50364, 1042, 11, 5101, 264, 1691, 9362, 307, 787, 11, 307, 11, 291, 733, 295, 362, 281, 362, 264, 912, 2497, 7123, 50664], "temperature": 0.0, "avg_logprob": -0.10527788128769189, "compression_ratio": 1.7088122605363985, "no_speech_prob": 0.004974524024873972}, {"id": 186, "seek": 75100, "start": 757.0, "end": 760.0, "text": " for all of your workloads.", "tokens": [50664, 337, 439, 295, 428, 32452, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10527788128769189, "compression_ratio": 1.7088122605363985, "no_speech_prob": 0.004974524024873972}, {"id": 187, "seek": 75100, "start": 760.0, "end": 763.0, "text": " And that may not fit for a lot of use cases.", "tokens": [50814, 400, 300, 815, 406, 3318, 337, 257, 688, 295, 764, 3331, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10527788128769189, "compression_ratio": 1.7088122605363985, "no_speech_prob": 0.004974524024873972}, {"id": 188, "seek": 75100, "start": 763.0, "end": 766.0, "text": " So the job set was kind of created as this way to say,", "tokens": [50964, 407, 264, 1691, 992, 390, 733, 295, 2942, 382, 341, 636, 281, 584, 11, 51114], "temperature": 0.0, "avg_logprob": -0.10527788128769189, "compression_ratio": 1.7088122605363985, "no_speech_prob": 0.004974524024873972}, {"id": 189, "seek": 75100, "start": 766.0, "end": 772.0, "text": " can we create a representation of a single job that could have maybe different pod templates", "tokens": [51114, 393, 321, 1884, 257, 10290, 295, 257, 2167, 1691, 300, 727, 362, 1310, 819, 2497, 21165, 51414], "temperature": 0.0, "avg_logprob": -0.10527788128769189, "compression_ratio": 1.7088122605363985, "no_speech_prob": 0.004974524024873972}, {"id": 190, "seek": 75100, "start": 772.0, "end": 775.0, "text": " and then also have its own kind of failure and success policies.", "tokens": [51414, 293, 550, 611, 362, 1080, 1065, 733, 295, 7763, 293, 2245, 7657, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10527788128769189, "compression_ratio": 1.7088122605363985, "no_speech_prob": 0.004974524024873972}, {"id": 191, "seek": 75100, "start": 775.0, "end": 779.0, "text": " So when you run these jobs at large scale, you're going to see failures", "tokens": [51564, 407, 562, 291, 1190, 613, 4782, 412, 2416, 4373, 11, 291, 434, 516, 281, 536, 20774, 51764], "temperature": 0.0, "avg_logprob": -0.10527788128769189, "compression_ratio": 1.7088122605363985, "no_speech_prob": 0.004974524024873972}, {"id": 192, "seek": 77900, "start": 779.0, "end": 784.0, "text": " and you may want to restart some jobs in case, or maybe you don't want to restart,", "tokens": [50364, 293, 291, 815, 528, 281, 21022, 512, 4782, 294, 1389, 11, 420, 1310, 291, 500, 380, 528, 281, 21022, 11, 50614], "temperature": 0.0, "avg_logprob": -0.08767786258604468, "compression_ratio": 1.7733812949640289, "no_speech_prob": 0.00869913399219513}, {"id": 193, "seek": 77900, "start": 784.0, "end": 789.0, "text": " or you want to, and I'll talk about one interesting use case of success policies.", "tokens": [50614, 420, 291, 528, 281, 11, 293, 286, 603, 751, 466, 472, 1880, 764, 1389, 295, 2245, 7657, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08767786258604468, "compression_ratio": 1.7733812949640289, "no_speech_prob": 0.00869913399219513}, {"id": 194, "seek": 77900, "start": 789.0, "end": 794.0, "text": " And one of our goals is Kubernetes is kind of an implementation detail.", "tokens": [50864, 400, 472, 295, 527, 5493, 307, 23145, 307, 733, 295, 364, 11420, 2607, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08767786258604468, "compression_ratio": 1.7733812949640289, "no_speech_prob": 0.00869913399219513}, {"id": 195, "seek": 77900, "start": 794.0, "end": 796.0, "text": " Most people don't want to know about it if you're a researcher,", "tokens": [51114, 4534, 561, 500, 380, 528, 281, 458, 466, 309, 498, 291, 434, 257, 21751, 11, 51214], "temperature": 0.0, "avg_logprob": -0.08767786258604468, "compression_ratio": 1.7733812949640289, "no_speech_prob": 0.00869913399219513}, {"id": 196, "seek": 77900, "start": 796.0, "end": 798.0, "text": " you just want to know I'm running this.", "tokens": [51214, 291, 445, 528, 281, 458, 286, 478, 2614, 341, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08767786258604468, "compression_ratio": 1.7733812949640289, "no_speech_prob": 0.00869913399219513}, {"id": 197, "seek": 77900, "start": 798.0, "end": 802.0, "text": " So we kind of want to streamline the creation of stuff like index job and headless services,", "tokens": [51314, 407, 321, 733, 295, 528, 281, 47141, 264, 8016, 295, 1507, 411, 8186, 1691, 293, 1378, 1832, 3328, 11, 51514], "temperature": 0.0, "avg_logprob": -0.08767786258604468, "compression_ratio": 1.7733812949640289, "no_speech_prob": 0.00869913399219513}, {"id": 198, "seek": 77900, "start": 802.0, "end": 806.0, "text": " because we know people want to communicate with their pods.", "tokens": [51514, 570, 321, 458, 561, 528, 281, 7890, 365, 641, 31925, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08767786258604468, "compression_ratio": 1.7733812949640289, "no_speech_prob": 0.00869913399219513}, {"id": 199, "seek": 80600, "start": 806.0, "end": 814.0, "text": " And so at a high level, the API for a job set looks very close to a job to a pod.", "tokens": [50364, 400, 370, 412, 257, 1090, 1496, 11, 264, 9362, 337, 257, 1691, 992, 1542, 588, 1998, 281, 257, 1691, 281, 257, 2497, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08710884054501851, "compression_ratio": 1.6971153846153846, "no_speech_prob": 0.0006355800433084369}, {"id": 200, "seek": 80600, "start": 814.0, "end": 818.0, "text": " Instead of replicating pods, we are replicating jobs.", "tokens": [50764, 7156, 295, 3248, 30541, 31925, 11, 321, 366, 3248, 30541, 4782, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08710884054501851, "compression_ratio": 1.6971153846153846, "no_speech_prob": 0.0006355800433084369}, {"id": 201, "seek": 80600, "start": 818.0, "end": 822.0, "text": " And I didn't have it specified here, but there's a replica field under the spec,", "tokens": [50964, 400, 286, 994, 380, 362, 309, 22206, 510, 11, 457, 456, 311, 257, 35456, 2519, 833, 264, 1608, 11, 51164], "temperature": 0.0, "avg_logprob": -0.08710884054501851, "compression_ratio": 1.6971153846153846, "no_speech_prob": 0.0006355800433084369}, {"id": 202, "seek": 80600, "start": 822.0, "end": 827.0, "text": " which says how many replicas of my replicated job I want to create.", "tokens": [51164, 597, 1619, 577, 867, 3248, 9150, 295, 452, 46365, 1691, 286, 528, 281, 1884, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08710884054501851, "compression_ratio": 1.6971153846153846, "no_speech_prob": 0.0006355800433084369}, {"id": 203, "seek": 80600, "start": 827.0, "end": 831.0, "text": " And then inside of the inside of a replicated job is a job template.", "tokens": [51414, 400, 550, 1854, 295, 264, 1854, 295, 257, 46365, 1691, 307, 257, 1691, 12379, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08710884054501851, "compression_ratio": 1.6971153846153846, "no_speech_prob": 0.0006355800433084369}, {"id": 204, "seek": 83100, "start": 831.0, "end": 834.0, "text": " And so this job is a PyTorch job.", "tokens": [50364, 400, 370, 341, 1691, 307, 257, 9953, 51, 284, 339, 1691, 13, 50514], "temperature": 0.0, "avg_logprob": -0.0985844157566534, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.002547186566516757}, {"id": 205, "seek": 83100, "start": 834.0, "end": 841.0, "text": " It creates an index job with a headless service, and then it creates a single job that has four pods.", "tokens": [50514, 467, 7829, 364, 8186, 1691, 365, 257, 1378, 1832, 2643, 11, 293, 550, 309, 7829, 257, 2167, 1691, 300, 575, 1451, 31925, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0985844157566534, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.002547186566516757}, {"id": 206, "seek": 83100, "start": 841.0, "end": 846.0, "text": " And I'll show in a little demo why this is useful.", "tokens": [50864, 400, 286, 603, 855, 294, 257, 707, 10723, 983, 341, 307, 4420, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0985844157566534, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.002547186566516757}, {"id": 207, "seek": 83100, "start": 846.0, "end": 849.0, "text": " And the other area that we've actually gotten quite a bit,", "tokens": [51114, 400, 264, 661, 1859, 300, 321, 600, 767, 5768, 1596, 257, 857, 11, 51264], "temperature": 0.0, "avg_logprob": -0.0985844157566534, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.002547186566516757}, {"id": 208, "seek": 83100, "start": 849.0, "end": 854.0, "text": " it's one of both Volcano and Kubeflow have implemented this in their projects,", "tokens": [51264, 309, 311, 472, 295, 1293, 8911, 18651, 293, 591, 1977, 10565, 362, 12270, 341, 294, 641, 4455, 11, 51514], "temperature": 0.0, "avg_logprob": -0.0985844157566534, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.002547186566516757}, {"id": 209, "seek": 83100, "start": 854.0, "end": 857.0, "text": " is one of the main reasons why they kind of created these projects,", "tokens": [51514, 307, 472, 295, 264, 2135, 4112, 983, 436, 733, 295, 2942, 613, 4455, 11, 51664], "temperature": 0.0, "avg_logprob": -0.0985844157566534, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.002547186566516757}, {"id": 210, "seek": 85700, "start": 857.0, "end": 861.0, "text": " is what do you do if you kind of have this leader worker paradigm,", "tokens": [50364, 307, 437, 360, 291, 360, 498, 291, 733, 295, 362, 341, 5263, 11346, 24709, 11, 50564], "temperature": 0.0, "avg_logprob": -0.10924507879441785, "compression_ratio": 1.770967741935484, "no_speech_prob": 0.03836123272776604}, {"id": 211, "seek": 85700, "start": 861.0, "end": 865.0, "text": " where your leader, let's say, is a Redis database and your workers are talking to it,", "tokens": [50564, 689, 428, 5263, 11, 718, 311, 584, 11, 307, 257, 4477, 271, 8149, 293, 428, 5600, 366, 1417, 281, 309, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10924507879441785, "compression_ratio": 1.770967741935484, "no_speech_prob": 0.03836123272776604}, {"id": 212, "seek": 85700, "start": 865.0, "end": 867.0, "text": " or whatever, you know, a message queue.", "tokens": [50764, 420, 2035, 11, 291, 458, 11, 257, 3636, 18639, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10924507879441785, "compression_ratio": 1.770967741935484, "no_speech_prob": 0.03836123272776604}, {"id": 213, "seek": 85700, "start": 867.0, "end": 870.0, "text": " Well, I want my workers just to finish.", "tokens": [50864, 1042, 11, 286, 528, 452, 5600, 445, 281, 2413, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10924507879441785, "compression_ratio": 1.770967741935484, "no_speech_prob": 0.03836123272776604}, {"id": 214, "seek": 85700, "start": 870.0, "end": 873.0, "text": " Like, I want to say, hey, once my workers are done,", "tokens": [51014, 1743, 11, 286, 528, 281, 584, 11, 4177, 11, 1564, 452, 5600, 366, 1096, 11, 51164], "temperature": 0.0, "avg_logprob": -0.10924507879441785, "compression_ratio": 1.770967741935484, "no_speech_prob": 0.03836123272776604}, {"id": 215, "seek": 85700, "start": 873.0, "end": 878.0, "text": " my job is successful and I don't really care about the progress of the leader.", "tokens": [51164, 452, 1691, 307, 4406, 293, 286, 500, 380, 534, 1127, 466, 264, 4205, 295, 264, 5263, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10924507879441785, "compression_ratio": 1.770967741935484, "no_speech_prob": 0.03836123272776604}, {"id": 216, "seek": 85700, "start": 878.0, "end": 881.0, "text": " And so this is kind of one of the use cases we had in mind with this project,", "tokens": [51414, 400, 370, 341, 307, 733, 295, 472, 295, 264, 764, 3331, 321, 632, 294, 1575, 365, 341, 1716, 11, 51564], "temperature": 0.0, "avg_logprob": -0.10924507879441785, "compression_ratio": 1.770967741935484, "no_speech_prob": 0.03836123272776604}, {"id": 217, "seek": 85700, "start": 881.0, "end": 883.0, "text": " or not, there's a lot of them, but this was one,", "tokens": [51564, 420, 406, 11, 456, 311, 257, 688, 295, 552, 11, 457, 341, 390, 472, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10924507879441785, "compression_ratio": 1.770967741935484, "no_speech_prob": 0.03836123272776604}, {"id": 218, "seek": 85700, "start": 883.0, "end": 886.0, "text": " like, can we use something called a success policy to say,", "tokens": [51664, 411, 11, 393, 321, 764, 746, 1219, 257, 2245, 3897, 281, 584, 11, 51814], "temperature": 0.0, "avg_logprob": -0.10924507879441785, "compression_ratio": 1.770967741935484, "no_speech_prob": 0.03836123272776604}, {"id": 219, "seek": 88600, "start": 886.0, "end": 891.0, "text": " I only really care about one set of jobs completion,", "tokens": [50364, 286, 787, 534, 1127, 466, 472, 992, 295, 4782, 19372, 11, 50614], "temperature": 0.0, "avg_logprob": -0.1212001458192483, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.000829194497782737}, {"id": 220, "seek": 88600, "start": 891.0, "end": 894.0, "text": " the rest are fodder, essentially, or not fodder,", "tokens": [50614, 264, 1472, 366, 47698, 1068, 11, 4476, 11, 420, 406, 47698, 1068, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1212001458192483, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.000829194497782737}, {"id": 221, "seek": 88600, "start": 894.0, "end": 897.0, "text": " but they play an important role until the workers are done,", "tokens": [50764, 457, 436, 862, 364, 1021, 3090, 1826, 264, 5600, 366, 1096, 11, 50914], "temperature": 0.0, "avg_logprob": -0.1212001458192483, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.000829194497782737}, {"id": 222, "seek": 88600, "start": 897.0, "end": 900.0, "text": " and then they're also taken down.", "tokens": [50914, 293, 550, 436, 434, 611, 2726, 760, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1212001458192483, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.000829194497782737}, {"id": 223, "seek": 88600, "start": 900.0, "end": 904.0, "text": " So, how am I doing on time?", "tokens": [51064, 407, 11, 577, 669, 286, 884, 322, 565, 30, 51264], "temperature": 0.0, "avg_logprob": -0.1212001458192483, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.000829194497782737}, {"id": 224, "seek": 88600, "start": 904.0, "end": 906.0, "text": " Okay, so I'll walk through the demo a little bit.", "tokens": [51264, 1033, 11, 370, 286, 603, 1792, 807, 264, 10723, 257, 707, 857, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1212001458192483, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.000829194497782737}, {"id": 225, "seek": 88600, "start": 906.0, "end": 910.0, "text": " So generally, with a job set, you have this controller,", "tokens": [51364, 407, 5101, 11, 365, 257, 1691, 992, 11, 291, 362, 341, 10561, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1212001458192483, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.000829194497782737}, {"id": 226, "seek": 88600, "start": 910.0, "end": 912.0, "text": " a job set controller manager.", "tokens": [51564, 257, 1691, 992, 10561, 6598, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1212001458192483, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.000829194497782737}, {"id": 227, "seek": 88600, "start": 912.0, "end": 915.0, "text": " Right now, you can check it's running, great.", "tokens": [51664, 1779, 586, 11, 291, 393, 1520, 309, 311, 2614, 11, 869, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1212001458192483, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.000829194497782737}, {"id": 228, "seek": 91500, "start": 915.0, "end": 920.0, "text": " And I kind of, in this demo, I tried to take the PyTorch job", "tokens": [50364, 400, 286, 733, 295, 11, 294, 341, 10723, 11, 286, 3031, 281, 747, 264, 9953, 51, 284, 339, 1691, 50614], "temperature": 0.0, "avg_logprob": -0.11883407433827718, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0008957060053944588}, {"id": 229, "seek": 91500, "start": 920.0, "end": 923.0, "text": " and kind of show the template,", "tokens": [50614, 293, 733, 295, 855, 264, 12379, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11883407433827718, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0008957060053944588}, {"id": 230, "seek": 91500, "start": 923.0, "end": 927.0, "text": " and then try to run it as just a normal job and kind of show you what happens.", "tokens": [50764, 293, 550, 853, 281, 1190, 309, 382, 445, 257, 2710, 1691, 293, 733, 295, 855, 291, 437, 2314, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11883407433827718, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0008957060053944588}, {"id": 231, "seek": 91500, "start": 927.0, "end": 929.0, "text": " You can't communicate to the service,", "tokens": [50964, 509, 393, 380, 7890, 281, 264, 2643, 11, 51064], "temperature": 0.0, "avg_logprob": -0.11883407433827718, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0008957060053944588}, {"id": 232, "seek": 91500, "start": 929.0, "end": 932.0, "text": " because if you try to create this job normally,", "tokens": [51064, 570, 498, 291, 853, 281, 1884, 341, 1691, 5646, 11, 51214], "temperature": 0.0, "avg_logprob": -0.11883407433827718, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0008957060053944588}, {"id": 233, "seek": 91500, "start": 932.0, "end": 934.0, "text": " there is no service for the communicate with,", "tokens": [51214, 456, 307, 572, 2643, 337, 264, 7890, 365, 11, 51314], "temperature": 0.0, "avg_logprob": -0.11883407433827718, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0008957060053944588}, {"id": 234, "seek": 91500, "start": 934.0, "end": 937.0, "text": " and it just automatically fails.", "tokens": [51314, 293, 309, 445, 6772, 18199, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11883407433827718, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0008957060053944588}, {"id": 235, "seek": 91500, "start": 937.0, "end": 939.0, "text": " So then, what do you do?", "tokens": [51464, 407, 550, 11, 437, 360, 291, 360, 30, 51564], "temperature": 0.0, "avg_logprob": -0.11883407433827718, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0008957060053944588}, {"id": 236, "seek": 91500, "start": 939.0, "end": 941.0, "text": " Well, you can use job set.", "tokens": [51564, 1042, 11, 291, 393, 764, 1691, 992, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11883407433827718, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0008957060053944588}, {"id": 237, "seek": 91500, "start": 941.0, "end": 942.0, "text": " Woo-hoo.", "tokens": [51664, 10468, 12, 19069, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11883407433827718, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0008957060053944588}, {"id": 238, "seek": 94200, "start": 942.0, "end": 946.0, "text": " And so, I already created the job set,", "tokens": [50364, 400, 370, 11, 286, 1217, 2942, 264, 1691, 992, 11, 50564], "temperature": 0.0, "avg_logprob": -0.14005846829758478, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.0011863582767546177}, {"id": 239, "seek": 94200, "start": 946.0, "end": 948.0, "text": " and you can kind of see that with the kube control logs,", "tokens": [50564, 293, 291, 393, 733, 295, 536, 300, 365, 264, 350, 1977, 1969, 20820, 11, 50664], "temperature": 0.0, "avg_logprob": -0.14005846829758478, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.0011863582767546177}, {"id": 240, "seek": 94200, "start": 948.0, "end": 952.0, "text": " I'm actually able to, the job set is running,", "tokens": [50664, 286, 478, 767, 1075, 281, 11, 264, 1691, 992, 307, 2614, 11, 50864], "temperature": 0.0, "avg_logprob": -0.14005846829758478, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.0011863582767546177}, {"id": 241, "seek": 94200, "start": 952.0, "end": 955.0, "text": " it's doing training, using PyTorch.", "tokens": [50864, 309, 311, 884, 3097, 11, 1228, 9953, 51, 284, 339, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14005846829758478, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.0011863582767546177}, {"id": 242, "seek": 94200, "start": 955.0, "end": 962.0, "text": " And also, I created a headless service called a PyTorch that's there.", "tokens": [51014, 400, 611, 11, 286, 2942, 257, 1378, 1832, 2643, 1219, 257, 9953, 51, 284, 339, 300, 311, 456, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14005846829758478, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.0011863582767546177}, {"id": 243, "seek": 94200, "start": 962.0, "end": 969.0, "text": " And so, this allows you to kind of hide all this stuff from the user.", "tokens": [51364, 400, 370, 11, 341, 4045, 291, 281, 733, 295, 6479, 439, 341, 1507, 490, 264, 4195, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14005846829758478, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.0011863582767546177}, {"id": 244, "seek": 96900, "start": 969.0, "end": 972.0, "text": " And then, I think in the next part of the demo,", "tokens": [50364, 400, 550, 11, 286, 519, 294, 264, 958, 644, 295, 264, 10723, 11, 50514], "temperature": 0.0, "avg_logprob": -0.19404298342191256, "compression_ratio": 1.3028169014084507, "no_speech_prob": 0.0018638757755979896}, {"id": 245, "seek": 96900, "start": 972.0, "end": 976.0, "text": " I'll show the success policy.", "tokens": [50514, 286, 603, 855, 264, 2245, 3897, 13, 50714], "temperature": 0.0, "avg_logprob": -0.19404298342191256, "compression_ratio": 1.3028169014084507, "no_speech_prob": 0.0018638757755979896}, {"id": 246, "seek": 96900, "start": 976.0, "end": 982.0, "text": " Come on.", "tokens": [50714, 2492, 322, 13, 51014], "temperature": 0.0, "avg_logprob": -0.19404298342191256, "compression_ratio": 1.3028169014084507, "no_speech_prob": 0.0018638757755979896}, {"id": 247, "seek": 96900, "start": 982.0, "end": 984.0, "text": " Oh, well.", "tokens": [51014, 876, 11, 731, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19404298342191256, "compression_ratio": 1.3028169014084507, "no_speech_prob": 0.0018638757755979896}, {"id": 248, "seek": 96900, "start": 984.0, "end": 989.0, "text": " So, I guess, I mean, it will go on for a little bit,", "tokens": [51114, 407, 11, 286, 2041, 11, 286, 914, 11, 309, 486, 352, 322, 337, 257, 707, 857, 11, 51364], "temperature": 0.0, "avg_logprob": -0.19404298342191256, "compression_ratio": 1.3028169014084507, "no_speech_prob": 0.0018638757755979896}, {"id": 249, "seek": 96900, "start": 989.0, "end": 992.0, "text": " but does anyone have any questions?", "tokens": [51364, 457, 775, 2878, 362, 604, 1651, 30, 51514], "temperature": 0.0, "avg_logprob": -0.19404298342191256, "compression_ratio": 1.3028169014084507, "no_speech_prob": 0.0018638757755979896}, {"id": 250, "seek": 99200, "start": 993.0, "end": 998.0, "text": " Any questions?", "tokens": [50414, 2639, 1651, 30, 50664], "temperature": 0.0, "avg_logprob": -0.3440582275390625, "compression_ratio": 1.1504424778761062, "no_speech_prob": 0.05035873502492905}, {"id": 251, "seek": 99200, "start": 998.0, "end": 1001.0, "text": " There's a couple up there.", "tokens": [50664, 821, 311, 257, 1916, 493, 456, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3440582275390625, "compression_ratio": 1.1504424778761062, "no_speech_prob": 0.05035873502492905}, {"id": 252, "seek": 99200, "start": 1008.0, "end": 1010.0, "text": " Wait, wait, wait.", "tokens": [51164, 3802, 11, 1699, 11, 1699, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3440582275390625, "compression_ratio": 1.1504424778761062, "no_speech_prob": 0.05035873502492905}, {"id": 253, "seek": 99200, "start": 1010.0, "end": 1012.0, "text": " Who was first?", "tokens": [51264, 2102, 390, 700, 30, 51364], "temperature": 0.0, "avg_logprob": -0.3440582275390625, "compression_ratio": 1.1504424778761062, "no_speech_prob": 0.05035873502492905}, {"id": 254, "seek": 99200, "start": 1015.0, "end": 1016.0, "text": " Hi.", "tokens": [51514, 2421, 13, 51564], "temperature": 0.0, "avg_logprob": -0.3440582275390625, "compression_ratio": 1.1504424778761062, "no_speech_prob": 0.05035873502492905}, {"id": 255, "seek": 99200, "start": 1016.0, "end": 1020.0, "text": " Yeah, I'm very much from the Slurrem bioinformatics", "tokens": [51564, 865, 11, 286, 478, 588, 709, 490, 264, 6187, 374, 2579, 12198, 37811, 30292, 51764], "temperature": 0.0, "avg_logprob": -0.3440582275390625, "compression_ratio": 1.1504424778761062, "no_speech_prob": 0.05035873502492905}, {"id": 256, "seek": 102000, "start": 1020.0, "end": 1022.0, "text": " snake make next-flow world.", "tokens": [50364, 12650, 652, 958, 12, 10565, 1002, 13, 50464], "temperature": 0.0, "avg_logprob": -0.14752776863039002, "compression_ratio": 1.6401384083044983, "no_speech_prob": 0.09999558329582214}, {"id": 257, "seek": 102000, "start": 1022.0, "end": 1024.0, "text": " So, and we have an IT department,", "tokens": [50464, 407, 11, 293, 321, 362, 364, 6783, 5882, 11, 50564], "temperature": 0.0, "avg_logprob": -0.14752776863039002, "compression_ratio": 1.6401384083044983, "no_speech_prob": 0.09999558329582214}, {"id": 258, "seek": 102000, "start": 1024.0, "end": 1026.0, "text": " and they have a Kubernetes cluster,", "tokens": [50564, 293, 436, 362, 257, 23145, 13630, 11, 50664], "temperature": 0.0, "avg_logprob": -0.14752776863039002, "compression_ratio": 1.6401384083044983, "no_speech_prob": 0.09999558329582214}, {"id": 259, "seek": 102000, "start": 1026.0, "end": 1028.0, "text": " so this is very interesting talk to me.", "tokens": [50664, 370, 341, 307, 588, 1880, 751, 281, 385, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14752776863039002, "compression_ratio": 1.6401384083044983, "no_speech_prob": 0.09999558329582214}, {"id": 260, "seek": 102000, "start": 1028.0, "end": 1031.0, "text": " But are you thinking about these kind of workflow managers", "tokens": [50764, 583, 366, 291, 1953, 466, 613, 733, 295, 20993, 14084, 50914], "temperature": 0.0, "avg_logprob": -0.14752776863039002, "compression_ratio": 1.6401384083044983, "no_speech_prob": 0.09999558329582214}, {"id": 261, "seek": 102000, "start": 1031.0, "end": 1034.0, "text": " that typical researchers like that use,", "tokens": [50914, 300, 7476, 10309, 411, 300, 764, 11, 51064], "temperature": 0.0, "avg_logprob": -0.14752776863039002, "compression_ratio": 1.6401384083044983, "no_speech_prob": 0.09999558329582214}, {"id": 262, "seek": 102000, "start": 1034.0, "end": 1036.0, "text": " because I was just in a high-energy physics session,", "tokens": [51064, 570, 286, 390, 445, 294, 257, 1090, 12, 49016, 10649, 5481, 11, 51164], "temperature": 0.0, "avg_logprob": -0.14752776863039002, "compression_ratio": 1.6401384083044983, "no_speech_prob": 0.09999558329582214}, {"id": 263, "seek": 102000, "start": 1036.0, "end": 1039.0, "text": " they also use snake make, and they have schedulers, of course,", "tokens": [51164, 436, 611, 764, 12650, 652, 11, 293, 436, 362, 12000, 433, 11, 295, 1164, 11, 51314], "temperature": 0.0, "avg_logprob": -0.14752776863039002, "compression_ratio": 1.6401384083044983, "no_speech_prob": 0.09999558329582214}, {"id": 264, "seek": 102000, "start": 1039.0, "end": 1041.0, "text": " but somehow that also has to interface.", "tokens": [51314, 457, 6063, 300, 611, 575, 281, 9226, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14752776863039002, "compression_ratio": 1.6401384083044983, "no_speech_prob": 0.09999558329582214}, {"id": 265, "seek": 102000, "start": 1041.0, "end": 1043.0, "text": " Do you have any comments on that?", "tokens": [51414, 1144, 291, 362, 604, 3053, 322, 300, 30, 51514], "temperature": 0.0, "avg_logprob": -0.14752776863039002, "compression_ratio": 1.6401384083044983, "no_speech_prob": 0.09999558329582214}, {"id": 266, "seek": 102000, "start": 1043.0, "end": 1048.0, "text": " So, generally, we don't want to get into the...", "tokens": [51514, 407, 11, 5101, 11, 321, 500, 380, 528, 281, 483, 666, 264, 485, 51764], "temperature": 0.0, "avg_logprob": -0.14752776863039002, "compression_ratio": 1.6401384083044983, "no_speech_prob": 0.09999558329582214}, {"id": 267, "seek": 104800, "start": 1048.0, "end": 1050.0, "text": " We don't want to add another workflow engine,", "tokens": [50364, 492, 500, 380, 528, 281, 909, 1071, 20993, 2848, 11, 50464], "temperature": 0.0, "avg_logprob": -0.11909642613920055, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0082953916862607}, {"id": 268, "seek": 104800, "start": 1050.0, "end": 1052.0, "text": " and there's too many of them,", "tokens": [50464, 293, 456, 311, 886, 867, 295, 552, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11909642613920055, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0082953916862607}, {"id": 269, "seek": 104800, "start": 1052.0, "end": 1056.0, "text": " but what I kind of view the job set is like a single node of a DAG,", "tokens": [50564, 457, 437, 286, 733, 295, 1910, 264, 1691, 992, 307, 411, 257, 2167, 9984, 295, 257, 9578, 38, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11909642613920055, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0082953916862607}, {"id": 270, "seek": 104800, "start": 1056.0, "end": 1058.0, "text": " and one of our goals could be this,", "tokens": [50764, 293, 472, 295, 527, 5493, 727, 312, 341, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11909642613920055, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0082953916862607}, {"id": 271, "seek": 104800, "start": 1058.0, "end": 1061.0, "text": " like either this job or a job set could be added", "tokens": [50864, 411, 2139, 341, 1691, 420, 257, 1691, 992, 727, 312, 3869, 51014], "temperature": 0.0, "avg_logprob": -0.11909642613920055, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0082953916862607}, {"id": 272, "seek": 104800, "start": 1061.0, "end": 1066.0, "text": " to something like Airflow or Argo workflows.", "tokens": [51014, 281, 746, 411, 5774, 10565, 420, 1587, 1571, 43461, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11909642613920055, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0082953916862607}, {"id": 273, "seek": 104800, "start": 1066.0, "end": 1068.0, "text": " It's another example to kind of be like,", "tokens": [51264, 467, 311, 1071, 1365, 281, 733, 295, 312, 411, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11909642613920055, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0082953916862607}, {"id": 274, "seek": 104800, "start": 1068.0, "end": 1070.0, "text": " this is a single element that you could run,", "tokens": [51364, 341, 307, 257, 2167, 4478, 300, 291, 727, 1190, 11, 51464], "temperature": 0.0, "avg_logprob": -0.11909642613920055, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0082953916862607}, {"id": 275, "seek": 104800, "start": 1070.0, "end": 1074.0, "text": " rather than having, like, Argo has their own way of representing,", "tokens": [51464, 2831, 813, 1419, 11, 411, 11, 1587, 1571, 575, 641, 1065, 636, 295, 13460, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11909642613920055, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0082953916862607}, {"id": 276, "seek": 104800, "start": 1074.0, "end": 1076.0, "text": " like, what they actually run on Kubernetes,", "tokens": [51664, 411, 11, 437, 436, 767, 1190, 322, 23145, 11, 51764], "temperature": 0.0, "avg_logprob": -0.11909642613920055, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0082953916862607}, {"id": 277, "seek": 107600, "start": 1076.0, "end": 1081.0, "text": " which is, you know, fine for pods, Airflow is also pods.", "tokens": [50364, 597, 307, 11, 291, 458, 11, 2489, 337, 31925, 11, 5774, 10565, 307, 611, 31925, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09812229701450893, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.004384932573884726}, {"id": 278, "seek": 107600, "start": 1081.0, "end": 1084.0, "text": " There are a lot of other workflow engines out there.", "tokens": [50614, 821, 366, 257, 688, 295, 661, 20993, 12982, 484, 456, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09812229701450893, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.004384932573884726}, {"id": 279, "seek": 107600, "start": 1084.0, "end": 1086.0, "text": " I've actually...", "tokens": [50764, 286, 600, 767, 485, 50864], "temperature": 0.0, "avg_logprob": -0.09812229701450893, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.004384932573884726}, {"id": 280, "seek": 107600, "start": 1086.0, "end": 1090.0, "text": " We took a lot of inspiration and two jobs ago for me", "tokens": [50864, 492, 1890, 257, 688, 295, 10249, 293, 732, 4782, 2057, 337, 385, 51064], "temperature": 0.0, "avg_logprob": -0.09812229701450893, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.004384932573884726}, {"id": 281, "seek": 107600, "start": 1090.0, "end": 1095.0, "text": " in applying bioinformatics, some of their workflow languages,", "tokens": [51064, 294, 9275, 12198, 37811, 30292, 11, 512, 295, 641, 20993, 8650, 11, 51314], "temperature": 0.0, "avg_logprob": -0.09812229701450893, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.004384932573884726}, {"id": 282, "seek": 107600, "start": 1095.0, "end": 1097.0, "text": " and trying to get...", "tokens": [51314, 293, 1382, 281, 483, 485, 51414], "temperature": 0.0, "avg_logprob": -0.09812229701450893, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.004384932573884726}, {"id": 283, "seek": 107600, "start": 1097.0, "end": 1099.0, "text": " Trying to standardize a workflow language", "tokens": [51414, 20180, 281, 3832, 1125, 257, 20993, 2856, 51514], "temperature": 0.0, "avg_logprob": -0.09812229701450893, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.004384932573884726}, {"id": 284, "seek": 107600, "start": 1099.0, "end": 1102.0, "text": " so we could actually run across different environments.", "tokens": [51514, 370, 321, 727, 767, 1190, 2108, 819, 12388, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09812229701450893, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.004384932573884726}, {"id": 285, "seek": 107600, "start": 1102.0, "end": 1105.0, "text": " And so I'm familiar with the area,", "tokens": [51664, 400, 370, 286, 478, 4963, 365, 264, 1859, 11, 51814], "temperature": 0.0, "avg_logprob": -0.09812229701450893, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.004384932573884726}, {"id": 286, "seek": 110500, "start": 1105.0, "end": 1109.0, "text": " but we're trying not to be a workflow engine for this project.", "tokens": [50364, 457, 321, 434, 1382, 406, 281, 312, 257, 20993, 2848, 337, 341, 1716, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08029684042319274, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.00529803242534399}, {"id": 287, "seek": 110500, "start": 1119.0, "end": 1121.0, "text": " Thank you for the talk.", "tokens": [51064, 1044, 291, 337, 264, 751, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08029684042319274, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.00529803242534399}, {"id": 288, "seek": 110500, "start": 1121.0, "end": 1125.0, "text": " I noticed that a lot of the things you were talking about", "tokens": [51164, 286, 5694, 300, 257, 688, 295, 264, 721, 291, 645, 1417, 466, 51364], "temperature": 0.0, "avg_logprob": -0.08029684042319274, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.00529803242534399}, {"id": 289, "seek": 110500, "start": 1125.0, "end": 1128.0, "text": " seemed to play kind of in the same field,", "tokens": [51364, 6576, 281, 862, 733, 295, 294, 264, 912, 2519, 11, 51514], "temperature": 0.0, "avg_logprob": -0.08029684042319274, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.00529803242534399}, {"id": 290, "seek": 110500, "start": 1128.0, "end": 1130.0, "text": " sort of where the Slurm plays.", "tokens": [51514, 1333, 295, 689, 264, 6187, 26717, 5749, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08029684042319274, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.00529803242534399}, {"id": 291, "seek": 110500, "start": 1130.0, "end": 1133.0, "text": " So, I don't know, a few years down the road,", "tokens": [51614, 407, 11, 286, 500, 380, 458, 11, 257, 1326, 924, 760, 264, 3060, 11, 51764], "temperature": 0.0, "avg_logprob": -0.08029684042319274, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.00529803242534399}, {"id": 292, "seek": 113300, "start": 1133.0, "end": 1136.0, "text": " do you see Slurm kind of giving way to, you know,", "tokens": [50364, 360, 291, 536, 6187, 26717, 733, 295, 2902, 636, 281, 11, 291, 458, 11, 50514], "temperature": 0.0, "avg_logprob": -0.09036687098511863, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.027379514649510384}, {"id": 293, "seek": 113300, "start": 1136.0, "end": 1138.0, "text": " this Kubernetes-based infrastructure,", "tokens": [50514, 341, 23145, 12, 6032, 6896, 11, 50614], "temperature": 0.0, "avg_logprob": -0.09036687098511863, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.027379514649510384}, {"id": 294, "seek": 113300, "start": 1138.0, "end": 1143.0, "text": " or do you think they're targeting kind of different tasks,", "tokens": [50614, 420, 360, 291, 519, 436, 434, 17918, 733, 295, 819, 9608, 11, 50864], "temperature": 0.0, "avg_logprob": -0.09036687098511863, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.027379514649510384}, {"id": 295, "seek": 113300, "start": 1143.0, "end": 1146.0, "text": " and Slurm will always have its place?", "tokens": [50864, 293, 6187, 26717, 486, 1009, 362, 1080, 1081, 30, 51014], "temperature": 0.0, "avg_logprob": -0.09036687098511863, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.027379514649510384}, {"id": 296, "seek": 113300, "start": 1146.0, "end": 1148.0, "text": " That's a really good question.", "tokens": [51014, 663, 311, 257, 534, 665, 1168, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09036687098511863, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.027379514649510384}, {"id": 297, "seek": 113300, "start": 1148.0, "end": 1151.0, "text": " I was not at KubeCon North America this year,", "tokens": [51114, 286, 390, 406, 412, 591, 1977, 9838, 4067, 3374, 341, 1064, 11, 51264], "temperature": 0.0, "avg_logprob": -0.09036687098511863, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.027379514649510384}, {"id": 298, "seek": 113300, "start": 1151.0, "end": 1154.0, "text": " but I heard of a company called CoreWeave", "tokens": [51264, 457, 286, 2198, 295, 257, 2237, 1219, 14798, 4360, 946, 51414], "temperature": 0.0, "avg_logprob": -0.09036687098511863, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.027379514649510384}, {"id": 299, "seek": 113300, "start": 1154.0, "end": 1156.0, "text": " that was actually collaborating with SchedumD", "tokens": [51414, 300, 390, 767, 30188, 365, 44926, 449, 35, 51514], "temperature": 0.0, "avg_logprob": -0.09036687098511863, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.027379514649510384}, {"id": 300, "seek": 113300, "start": 1156.0, "end": 1161.0, "text": " to try and kind of provide Slurm on Kubernetes.", "tokens": [51514, 281, 853, 293, 733, 295, 2893, 6187, 26717, 322, 23145, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09036687098511863, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.027379514649510384}, {"id": 301, "seek": 116100, "start": 1161.0, "end": 1164.0, "text": " From what I understand, kind of using the Slurm scheduler,", "tokens": [50364, 3358, 437, 286, 1223, 11, 733, 295, 1228, 264, 6187, 26717, 12000, 260, 11, 50514], "temperature": 0.0, "avg_logprob": -0.1184119512868482, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.002250273944810033}, {"id": 302, "seek": 116100, "start": 1164.0, "end": 1166.0, "text": " but also allowing people to run", "tokens": [50514, 457, 611, 8293, 561, 281, 1190, 50614], "temperature": 0.0, "avg_logprob": -0.1184119512868482, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.002250273944810033}, {"id": 303, "seek": 116100, "start": 1166.0, "end": 1168.0, "text": " some of the more popular Kubernetes stuff,", "tokens": [50614, 512, 295, 264, 544, 3743, 23145, 1507, 11, 50714], "temperature": 0.0, "avg_logprob": -0.1184119512868482, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.002250273944810033}, {"id": 304, "seek": 116100, "start": 1168.0, "end": 1173.0, "text": " like have Kubernetes for services or Slurm for batch.", "tokens": [50714, 411, 362, 23145, 337, 3328, 420, 6187, 26717, 337, 15245, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1184119512868482, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.002250273944810033}, {"id": 305, "seek": 116100, "start": 1173.0, "end": 1179.0, "text": " Generally, everyone is kind of converging in this area.", "tokens": [50964, 21082, 11, 1518, 307, 733, 295, 9652, 3249, 294, 341, 1859, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1184119512868482, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.002250273944810033}, {"id": 306, "seek": 116100, "start": 1179.0, "end": 1184.0, "text": " Our motto is actually taking from inspiration of HT Condor", "tokens": [51264, 2621, 32680, 307, 767, 1940, 490, 10249, 295, 11751, 21793, 284, 51514], "temperature": 0.0, "avg_logprob": -0.1184119512868482, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.002250273944810033}, {"id": 307, "seek": 116100, "start": 1184.0, "end": 1186.0, "text": " and trying to apply that to Kubernetes.", "tokens": [51514, 293, 1382, 281, 3079, 300, 281, 23145, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1184119512868482, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.002250273944810033}, {"id": 308, "seek": 118600, "start": 1186.0, "end": 1189.0, "text": " And then I know that the...", "tokens": [50364, 400, 550, 286, 458, 300, 264, 485, 50514], "temperature": 0.0, "avg_logprob": -0.14467877887544178, "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.0024323598481714725}, {"id": 309, "seek": 118600, "start": 1189.0, "end": 1191.0, "text": " Sorry, I'm pulling a blank.", "tokens": [50514, 4919, 11, 286, 478, 8407, 257, 8247, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14467877887544178, "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.0024323598481714725}, {"id": 310, "seek": 118600, "start": 1191.0, "end": 1195.0, "text": " The University of Wisconsin, who kind of created HT Condor,", "tokens": [50614, 440, 3535, 295, 17977, 11, 567, 733, 295, 2942, 11751, 21793, 284, 11, 50814], "temperature": 0.0, "avg_logprob": -0.14467877887544178, "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.0024323598481714725}, {"id": 311, "seek": 118600, "start": 1195.0, "end": 1197.0, "text": " they're big on trying to actually use Kubernetes", "tokens": [50814, 436, 434, 955, 322, 1382, 281, 767, 764, 23145, 50914], "temperature": 0.0, "avg_logprob": -0.14467877887544178, "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.0024323598481714725}, {"id": 312, "seek": 118600, "start": 1197.0, "end": 1201.0, "text": " for a lot of some of their infrastructure also.", "tokens": [50914, 337, 257, 688, 295, 512, 295, 641, 6896, 611, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14467877887544178, "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.0024323598481714725}, {"id": 313, "seek": 118600, "start": 1201.0, "end": 1206.0, "text": " But, and also, we do talk pretty closely with the SchedumD folks,", "tokens": [51114, 583, 11, 293, 611, 11, 321, 360, 751, 1238, 8185, 365, 264, 44926, 449, 35, 4024, 11, 51364], "temperature": 0.0, "avg_logprob": -0.14467877887544178, "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.0024323598481714725}, {"id": 314, "seek": 118600, "start": 1206.0, "end": 1208.0, "text": " at least in my last role,", "tokens": [51364, 412, 1935, 294, 452, 1036, 3090, 11, 51464], "temperature": 0.0, "avg_logprob": -0.14467877887544178, "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.0024323598481714725}, {"id": 315, "seek": 118600, "start": 1208.0, "end": 1214.0, "text": " and there is a lot of interest in trying to bring Kubernetes to Slurm.", "tokens": [51464, 293, 456, 307, 257, 688, 295, 1179, 294, 1382, 281, 1565, 23145, 281, 6187, 26717, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14467877887544178, "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.0024323598481714725}, {"id": 316, "seek": 121400, "start": 1214.0, "end": 1218.0, "text": " And part of it is Slurm has been around a long time,", "tokens": [50364, 400, 644, 295, 309, 307, 6187, 26717, 575, 668, 926, 257, 938, 565, 11, 50564], "temperature": 0.0, "avg_logprob": -0.09557931606586163, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.0014770458219572902}, {"id": 317, "seek": 121400, "start": 1218.0, "end": 1220.0, "text": " and so they had to do a lot of work", "tokens": [50564, 293, 370, 436, 632, 281, 360, 257, 688, 295, 589, 50664], "temperature": 0.0, "avg_logprob": -0.09557931606586163, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.0014770458219572902}, {"id": 318, "seek": 121400, "start": 1220.0, "end": 1222.0, "text": " to just even to get in the fact of,", "tokens": [50664, 281, 445, 754, 281, 483, 294, 264, 1186, 295, 11, 50764], "temperature": 0.0, "avg_logprob": -0.09557931606586163, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.0014770458219572902}, {"id": 319, "seek": 121400, "start": 1222.0, "end": 1225.0, "text": " I want to containerize Slurm in Kubernetes.", "tokens": [50764, 286, 528, 281, 10129, 1125, 6187, 26717, 294, 23145, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09557931606586163, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.0014770458219572902}, {"id": 320, "seek": 121400, "start": 1225.0, "end": 1226.0, "text": " Okay, great.", "tokens": [50914, 1033, 11, 869, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09557931606586163, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.0014770458219572902}, {"id": 321, "seek": 121400, "start": 1226.0, "end": 1229.0, "text": " Now, do I want to schedule a pod,", "tokens": [50964, 823, 11, 360, 286, 528, 281, 7567, 257, 2497, 11, 51114], "temperature": 0.0, "avg_logprob": -0.09557931606586163, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.0014770458219572902}, {"id": 322, "seek": 121400, "start": 1229.0, "end": 1231.0, "text": " or do I want to schedule a single container?", "tokens": [51114, 420, 360, 286, 528, 281, 7567, 257, 2167, 10129, 30, 51214], "temperature": 0.0, "avg_logprob": -0.09557931606586163, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.0014770458219572902}, {"id": 323, "seek": 121400, "start": 1231.0, "end": 1235.0, "text": " And that's kind of where I can see...", "tokens": [51214, 400, 300, 311, 733, 295, 689, 286, 393, 536, 485, 51414], "temperature": 0.0, "avg_logprob": -0.09557931606586163, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.0014770458219572902}, {"id": 324, "seek": 121400, "start": 1235.0, "end": 1237.0, "text": " That's also what's kind of challenging,", "tokens": [51414, 663, 311, 611, 437, 311, 733, 295, 7595, 11, 51514], "temperature": 0.0, "avg_logprob": -0.09557931606586163, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.0014770458219572902}, {"id": 325, "seek": 121400, "start": 1237.0, "end": 1239.0, "text": " and the other thing is convincing more and more people", "tokens": [51514, 293, 264, 661, 551, 307, 24823, 544, 293, 544, 561, 51614], "temperature": 0.0, "avg_logprob": -0.09557931606586163, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.0014770458219572902}, {"id": 326, "seek": 121400, "start": 1239.0, "end": 1241.0, "text": " to use containers, because it's great,", "tokens": [51614, 281, 764, 17089, 11, 570, 309, 311, 869, 11, 51714], "temperature": 0.0, "avg_logprob": -0.09557931606586163, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.0014770458219572902}, {"id": 327, "seek": 124100, "start": 1241.0, "end": 1244.0, "text": " but it's also a pain to change everything that you want", "tokens": [50364, 457, 309, 311, 611, 257, 1822, 281, 1319, 1203, 300, 291, 528, 50514], "temperature": 0.0, "avg_logprob": -0.12403142580422022, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0018932800740003586}, {"id": 328, "seek": 124100, "start": 1244.0, "end": 1246.0, "text": " to go to a container.", "tokens": [50514, 281, 352, 281, 257, 10129, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12403142580422022, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0018932800740003586}, {"id": 329, "seek": 124100, "start": 1246.0, "end": 1248.0, "text": " Okay.", "tokens": [50614, 1033, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12403142580422022, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0018932800740003586}, {"id": 330, "seek": 124100, "start": 1248.0, "end": 1250.0, "text": " Any more questions?", "tokens": [50714, 2639, 544, 1651, 30, 50814], "temperature": 0.0, "avg_logprob": -0.12403142580422022, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0018932800740003586}, {"id": 331, "seek": 124100, "start": 1253.0, "end": 1255.0, "text": " So, if I understand it correctly,", "tokens": [50964, 407, 11, 498, 286, 1223, 309, 8944, 11, 51064], "temperature": 0.0, "avg_logprob": -0.12403142580422022, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0018932800740003586}, {"id": 332, "seek": 124100, "start": 1255.0, "end": 1259.0, "text": " you're primarily optimizing that I do not schedule 10,000 pods,", "tokens": [51064, 291, 434, 10029, 40425, 300, 286, 360, 406, 7567, 1266, 11, 1360, 31925, 11, 51264], "temperature": 0.0, "avg_logprob": -0.12403142580422022, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0018932800740003586}, {"id": 333, "seek": 124100, "start": 1259.0, "end": 1261.0, "text": " and then have job sets, right?", "tokens": [51264, 293, 550, 362, 1691, 6352, 11, 558, 30, 51364], "temperature": 0.0, "avg_logprob": -0.12403142580422022, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0018932800740003586}, {"id": 334, "seek": 124100, "start": 1261.0, "end": 1264.0, "text": " Because when I think about batch processing,", "tokens": [51364, 1436, 562, 286, 519, 466, 15245, 9007, 11, 51514], "temperature": 0.0, "avg_logprob": -0.12403142580422022, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0018932800740003586}, {"id": 335, "seek": 124100, "start": 1264.0, "end": 1267.0, "text": " I do think about, let's say, CI,", "tokens": [51514, 286, 360, 519, 466, 11, 718, 311, 584, 11, 37777, 11, 51664], "temperature": 0.0, "avg_logprob": -0.12403142580422022, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0018932800740003586}, {"id": 336, "seek": 126700, "start": 1267.0, "end": 1270.0, "text": " and then we are running like 5,000 jobs per day,", "tokens": [50364, 293, 550, 321, 366, 2614, 411, 1025, 11, 1360, 4782, 680, 786, 11, 50514], "temperature": 0.0, "avg_logprob": -0.09530022621154785, "compression_ratio": 1.5420168067226891, "no_speech_prob": 0.04764406383037567}, {"id": 337, "seek": 126700, "start": 1270.0, "end": 1272.0, "text": " and we do this with Jenkins,", "tokens": [50514, 293, 321, 360, 341, 365, 41273, 11, 50614], "temperature": 0.0, "avg_logprob": -0.09530022621154785, "compression_ratio": 1.5420168067226891, "no_speech_prob": 0.04764406383037567}, {"id": 338, "seek": 126700, "start": 1272.0, "end": 1276.0, "text": " which actually works super great with Kubernetes plugin,", "tokens": [50614, 597, 767, 1985, 1687, 869, 365, 23145, 23407, 11, 50814], "temperature": 0.0, "avg_logprob": -0.09530022621154785, "compression_ratio": 1.5420168067226891, "no_speech_prob": 0.04764406383037567}, {"id": 339, "seek": 126700, "start": 1276.0, "end": 1279.0, "text": " but I'm not seeing enough features on this proposal", "tokens": [50814, 457, 286, 478, 406, 2577, 1547, 4122, 322, 341, 11494, 50964], "temperature": 0.0, "avg_logprob": -0.09530022621154785, "compression_ratio": 1.5420168067226891, "no_speech_prob": 0.04764406383037567}, {"id": 340, "seek": 126700, "start": 1279.0, "end": 1283.0, "text": " to get rid of Jenkins or any other components.", "tokens": [50964, 281, 483, 3973, 295, 41273, 420, 604, 661, 6677, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09530022621154785, "compression_ratio": 1.5420168067226891, "no_speech_prob": 0.04764406383037567}, {"id": 341, "seek": 126700, "start": 1283.0, "end": 1286.0, "text": " I'm primarily seeing a way of not overloading", "tokens": [51164, 286, 478, 10029, 2577, 257, 636, 295, 406, 28777, 278, 51314], "temperature": 0.0, "avg_logprob": -0.09530022621154785, "compression_ratio": 1.5420168067226891, "no_speech_prob": 0.04764406383037567}, {"id": 342, "seek": 126700, "start": 1286.0, "end": 1289.0, "text": " the cluster with pending pods.", "tokens": [51314, 264, 13630, 365, 32110, 31925, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09530022621154785, "compression_ratio": 1.5420168067226891, "no_speech_prob": 0.04764406383037567}, {"id": 343, "seek": 126700, "start": 1289.0, "end": 1291.0, "text": " Is that right?", "tokens": [51464, 1119, 300, 558, 30, 51564], "temperature": 0.0, "avg_logprob": -0.09530022621154785, "compression_ratio": 1.5420168067226891, "no_speech_prob": 0.04764406383037567}, {"id": 344, "seek": 126700, "start": 1291.0, "end": 1294.0, "text": " No, I would say the main thing is trying,", "tokens": [51564, 883, 11, 286, 576, 584, 264, 2135, 551, 307, 1382, 11, 51714], "temperature": 0.0, "avg_logprob": -0.09530022621154785, "compression_ratio": 1.5420168067226891, "no_speech_prob": 0.04764406383037567}, {"id": 345, "seek": 129400, "start": 1294.0, "end": 1297.0, "text": " if you want to say run a PyTorch job,", "tokens": [50364, 498, 291, 528, 281, 584, 1190, 257, 9953, 51, 284, 339, 1691, 11, 50514], "temperature": 0.0, "avg_logprob": -0.12289327230208959, "compression_ratio": 1.4935064935064934, "no_speech_prob": 0.019093504175543785}, {"id": 346, "seek": 129400, "start": 1297.0, "end": 1302.0, "text": " the one option is let's create, let's use Kubeflow.", "tokens": [50514, 264, 472, 3614, 307, 718, 311, 1884, 11, 718, 311, 764, 591, 1977, 10565, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12289327230208959, "compression_ratio": 1.4935064935064934, "no_speech_prob": 0.019093504175543785}, {"id": 347, "seek": 129400, "start": 1302.0, "end": 1304.0, "text": " Fine, that will work.", "tokens": [50764, 12024, 11, 300, 486, 589, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12289327230208959, "compression_ratio": 1.4935064935064934, "no_speech_prob": 0.019093504175543785}, {"id": 348, "seek": 129400, "start": 1304.0, "end": 1306.0, "text": " But what if I don't really want to use Kubeflow?", "tokens": [50864, 583, 437, 498, 286, 500, 380, 534, 528, 281, 764, 591, 1977, 10565, 30, 50964], "temperature": 0.0, "avg_logprob": -0.12289327230208959, "compression_ratio": 1.4935064935064934, "no_speech_prob": 0.019093504175543785}, {"id": 349, "seek": 129400, "start": 1306.0, "end": 1308.0, "text": " What if I have my own representation?", "tokens": [50964, 708, 498, 286, 362, 452, 1065, 10290, 30, 51064], "temperature": 0.0, "avg_logprob": -0.12289327230208959, "compression_ratio": 1.4935064935064934, "no_speech_prob": 0.019093504175543785}, {"id": 350, "seek": 129400, "start": 1308.0, "end": 1310.0, "text": " What if I want to add my own...", "tokens": [51064, 708, 498, 286, 528, 281, 909, 452, 1065, 485, 51164], "temperature": 0.0, "avg_logprob": -0.12289327230208959, "compression_ratio": 1.4935064935064934, "no_speech_prob": 0.019093504175543785}], "language": "en"}