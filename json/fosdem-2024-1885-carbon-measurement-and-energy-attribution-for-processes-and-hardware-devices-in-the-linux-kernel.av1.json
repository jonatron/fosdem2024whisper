{"text": " All right, everyone. I hope the mic is working. It's great to be here. This is my first fast time, by the way. And I'm very happy to talk to you all about current measurement and energy attribution for processes and hardware devices in Linux. My name's Aditya, but you can call me Adi. That's the first three letters. I'm a grad student. And yeah, that's my contact. I'm always very happy to talk to people during, before, and after my talk. Please reach out. Please. I would love to hear from you. So a bit of background. I'm a graduate student at ETHEHA Zirik in Switzerland. And I do research at the intersection of computer architecture and operating systems. I love this stuff very much. Great. What do we want to talk about? Let's get a bit of a brief background to bring everyone on the same page. Now, when we talk about energy sources and computing systems, you can have a bunch of options. You can have direct inputs from DC, from USB. You can have battery power systems. And if you're really exotic, you can even have energy harvesting devices. Okay? Now, we want to use the maximum. No, I'm sorry. We want to use the minimum amount of energy to perform our task. Why do we want to use the minimum amount of energy? Because energy consumption correlates with battery capacity. And battery capacity is a major, a significant design constraint for your consumers. Okay? All of us have cell phones. We have the recent buzz around Apple Vision Pro, AR devices. These devices are significantly restricted by the amount of battery capacity. So we want to minimize the energy that we use to get the job done. Okay? Now, what is the problem here? What do we want to solve? Let's flesh it out. Energy consumption is defined as power times latency. Power is determined by your hardware. Okay? Latency is determined by your software. Okay? Now, how do we measure this? How do we get this data? Programmers often measure latency using well-established tools. I'm guessing many of you would be familiar with Linux Perf, or you would have timed your own software using wall clock time using CPU clock cycles, right? Now, these are well-established metrics and well-established tools to quantify your latency. What if I ask you, do you know of any tools to calculate your application's energy? What comes to your mind when I pose this question to you? How would you calculate your application's energy consumption? You would say, okay, Ali, I know. This is very simple, right? Energy is power times latency, right? We just talked about this. I'll get the power from the CPU. My CPU has this magical interface called Rappel, which stands for running average power limit. I'm going to get the value and they, oh, voila, my CPU says 15 watts right now. Great. Then I'll time my application and my application turns out to be, let's say, five milliseconds. Okay? And we put these values into this formula and great, we have 75 mJ of energy consumption. Stop done. Let's go home. Unfortunately, this is too simplistic. Let's try to dive into what we missed here. This does not reflect the ground reality, okay? And now I'm going to deconstruct what happened here and what we missed. The first step, we saw the power was 15 watts. And unfortunately, this model assumes a linear power draw over time. That is not the case. If you actually look at the system, this is what it looks like. You have these values and you have these peaks. And if you measure your power at the wrong time, you will end up with a significantly different number than what you should have. On the x-axis, you have time value. On the y-axis, you have the power for CPU. And power consumption is not linear. So this assumption that we have a linear power draw is incorrect. Second, we got the power value from Rappel. Remember running average power limit? It turns out that Rappel is only available on Intel or sometimes on AMD. ARM, for example, has a very, very different interface to report power. So I would love to share a story. I was doing energy profiling on a server-class system back in university and I said, oh, I've built this great infrastructure on my Intel platform, right? Let me just use it to run on ARM and see what happens. And the moment I ran it on ARM, Linux Perf said, I'm sorry, I don't recognize the CPU. I can't give you any numbers. And it just crashed, okay? So all of these interfaces are really different and you need a significant amount of engineering to make sense of it across different platforms. Second limitation is that we do not have uniform interfaces or the formats to measure power reliably. All right. Let's try to go deeper. Let's try to get more into the closer to the ground truth. Our model got the power value from the CPU. What about the other devices? I'm right now broadcasting from this device and oh my God. I'm sorry for this. I hope not. Give me a sec. Beautiful. Beautiful. Okay, okay. So back to the presentation. We were talking about the impact of devices like the screen, the memory, the network cards, right? We don't know how to quantify them. So we did a lot of experiments and it turns out that these devices very often dominate your power consumption and our findings are also correlated from some similar observations at Google. So what Google did was they were trying to optimize their data centers and did a huge amount of profiling on their server class CPUs. Server class CPUs are the heaviest CPUs that you can get in the market. And it turns out that they observed that DRAM is dominating their power because DRAM is burning power all the time. CPU turns on and off, but the DRAM you cannot turn off. Remember, it is volatile. So you need to break out of this mindset that CPU is the end all be all. Okay. So let me try to summarize everything. We are inaccurately calculating only a fraction of the system's actual energy consumption. Okay. And I would love to put this in a take of a code for you. This is not from me, but I like this very much. We cannot improve what we cannot measure. So we first need to understand how to measure energy correctly. And that's what my project is all about. That's what I love to do. What is the goal of what I'm trying to do here? My goal is to develop a framework to accurately and reliably measure the energy consumption of processes in the kernel. All right. All of us can get this data. What is the use for this? Because data without use is it does not get used. Okay. Once we have this data, we want to report it to the end users in an easy to understand format. Right. End users should be able to make sense of the number. Right. What does this number mean for me? We wanted to report it to the programmers which improve their action ability, which enable them to move their code up and down to change their code to move the numbers. Right. And we want to report it to system designers to enable them to iterate much faster over low energy designs over low carbon designs. Okay. So let's try to dive deeper. What do we mean by a framework? What are we trying to do? Let's flesh it out. A framework comprises models and tools. Let's break down these two words. A power model is how we think about a device. When I say that I want to measure power, a power model is the mental model that I have that I will use to get the value. Okay. And it turns out that these power models are often very poorly understood for a number of devices. For example, DRAM power models are often not available to the public. They're not available to academia. They're, let's say, a proprietary trade secret. Don't quote me on that. And okay, once we have these power models, we can build tools which accurately calculate power based on these models. A tool that I would like to mention would be the NVIDIA SMI utility. It allows you to calculate the power of a GPU using this tool. It's a good tool. And okay, so let's pull it all in. What I would like you to take away is that we need accurate models, first and foremost accurate models, and second, reliable tools to calculate the energy consumption correctly. So we defined our problem and we defined our goal post where we want to go. And now let's see how are we going to get from point A to point B. Great. So before I dive into what is the mechanism, I would like to bring to knowledge what has been done before. All of us have been here for the entire day, right? We love energy and we love efficiency. If this is such an important problem, why didn't people solve it before? People did. People did try to solve it before and I'm going to describe to you right now what they did before and why that is insufficient, why we need to do better. Okay, on the screen you can see a screenshot from a tool from Intel that is known as PowerTop. And you can see the first column here which reports power estimate. And on the right side you have the description of the particular device, interrupt, process for which this power estimate is calculated. Now, what are the challenges? Well, first of all, I believe in energy. It turns out that power is a discrete time event. What do we mean by discrete time event? Let's try to break this down. If you have a graph, a power is a single point on that graph. Energy is the area under that graph, okay? We want to calculate energy because energy is what correlates to your battery drain. Your battery supplies you energy. Power is just one particular instance in that time. Second, PowerTop has a vendor-specific implementation. I hope that is clear. Third, what is the actionability? So I just showed you this data. I just showed you the screenshot. It says, oh, my display backlight is taking 350 milliwatts. Great. This particular process is consuming 292 milliwatts. Okay, fine. The question that comes to mind is what is the use for me? What is the actionability for the programmer for this data? How does the programmer change the code to move this number? And I don't know. How do I fix that? How do I fix something that I don't know how to fix? And that is a gap that I would like to bridge, right? So let me dive into the guts of the system. This is a system design. On the screen you can see an elementary flow chart which summarizes the system at a very high level. And this is a regression-based system. A regression-based system has two inputs. You have the parameters and you have the inputs to the parameters. First, we calculate the parameters and then we calculate the inputs to the parameters. Great, we have time. I will go into details now. Please bear with me. Okay, let's first look into the parameters. How do we determine the regression model's parameters? There's an algorithm for this. First of all, we turn off everything. We turn off everything that we can turn off in the system. We measure the baseline draw. This is what we refer to as the minimizing the system load. Then we pick each device one by one. We isolate the impact of the device on the baseline load. And we measure the drain over multiple times. So we turn on a single time. Let's say that I turn off everything and then I turn on just the screen. Okay? And I measure the difference between these two values. The difference is the impact of the screen on my baseline. And then I also do one thing. I sweep the screen. So I change the brightness of the screen from minimum to the maximum because obviously the minimum brightness is going to have different power than the maximum brightness, right? I hope this makes sense to me. Are you guys still with me? Okay, so this was just an example. But what we're trying to do is we're trying to quantify the impact of each device on the baseline. Now, I would love to give a metaphor to help explain this better. Imagine that you have a water tank. And in this water tank there's one single input and there are 10,000 tiny outputs. And the problem that you're trying to solve is what is the rate for each of the output pipes? You cannot measure it directly. So you have these 10,000 outputs which go on their own anytime. They can go off and you don't have levers to control them. What you're trying to figure out is what is the drain rate for each of the output pipes? That is essentially the problem that you're trying to solve. So what you do is you turn off all the outputs, okay? You turn off all the outputs and you turn on one single output and then you see the difference in the tank level before and after turning it off, okay? And that is essentially what we call as an isolation or well in academic terms it's also sometimes known as an ablation study. But we try to isolate the device and measure the impact. Next, we repeat this process for all the pipes in the system and we try to get a reasonable estimate of what is the impact of each pipe. Great. So that was the first step, the device-specific measurements. The second step would be the kernel process accounting step. This would be the inputs to the regression model. So we have the parameters that we got from this step and now we need the inputs. Now how do we determine these inputs? Sorry, did I hear a question? Okay, great. Right, how do we determine the inputs? What we do is we isolate the impact of each process. So we identify how much time the process used for the CPU. We identify how much was the network activity, the screen wakeups, file handles, memory usage and we put all of these numbers together into the model. And this is what gives us a predicted energy consumption value for that process. Okay, so what are the challenges? This seems very simple. This seems, okay, you've done this work but what did you not tell us? Here comes the part that I did not tell you. First part, estimated value. This is not the reality. It is really hard to find out the reality. And there's a very famous line in machine learning community. It's known as all models are wrong but some are useful. So my goal here is to build a useful model that I hope is less wrong. I would love to make it perfect but unfortunately we cannot make it perfect. But yes, I would love to make a useful model first. Second, there's a bit of a cash 22 situation here if you observe that. What is the cash 22? I am running a measurement process. There's a process that is doing measurement on my system. Okay, that is also going to create a load. So there's going to be a skew in the values that I get because of my measurement. Okay, and the more accurate I want it to be, the more skew it is going to create. So we want to understand what is the right amount of accuracy that we can use to also be useful while also minimizing the bias. So this is very challenging, right? Because this is different for every system. And that's a problem that I'm almost struggling to solve. I would love to get your inputs if you have. Great, next challenge. There are millions of devices out there and these millions of devices have billions of ICs inside them. Very often we don't even have the data sheets for these ICs to correlate the values that we see. The estimates that we get can range across two to three orders of magnitude. One device can say, oh, I use one microjoule and the second one can say, oh, I use 10 milliwatts and those numbers don't make sense. Those numbers really blow you away. So how do we maintain our sanity in the face of the variance that we see here? And one more challenge would be that, assume that you can say, oh, let the users supply this data, let me get the data and then build a centralized farm of this data and then try to make sense of it. Should the users share this data? Would the users share their device users' data to you and allow you to put it on a centralized server? Who will own that data? Because there's enormous value in it. So this is, I would love to get your inputs on. One more challenge here would be the validation. So we got a value that we estimated. How do we make sure this value is as close as possible to the ground truth? In an ideal world, I would have infinite money and I would go to every computer in this world and take a probe and put it next to their CPU and say, oh, this says 17.5 watts and my tool says 17.5 watts. Great job. Let's go. I cannot do that because I don't have that much time. Okay, so we want to minimize the difference from the ground truth and what we see in the tool. There's a significant challenge in making sure that what we see is what is the reality. Right? Remember, there's accuracy and there's precision and there's correctness. And all of these trifecta come together and make this a very difficult tool to get right. But still, I believe it's going to be great. I'm very happy to work on it. Great. So once we have the energy consumption, how do we link it to the carbon emissions? We just saw that we can calculate energy consumption using power time flatancy. The carbon footprint can be calculated by multiplying this number by the composition of the energy. Where did the energy come from that you used to power the device that you were running? And this composition depends on multiple factors. It can include the geography. It can include the time of availability. It can include the cost of generation of that energy. Right? So fortunately, there are good tools and libraries out there which can simplify this problem for you. So energy composition is, let's say, something that I believe people will solve faster than I can solve this one. That is why I would love to focus on this one. Great. All done. Let's get back to the good stuff. How is this going to look like? How is this going to make your life better? If you're an end user, I would love to ship to you an application like this, an application which tells you how much energy your inkscape usage consumed, how much energy your screen was dissipating. So as an end user, you can remember to turn off inkscape when you're not using it. Or you can figure out, oh, I need to deliver a presentation to so many people in five minutes. I'd better save my battery. Otherwise, I'll be in deep trouble. So it's for those use cases when you want to maximize your battery life as an end user. As a programmer, if you want to expose an API that enables programmers to take action, if you want to indicate the devices and the code regions which consume the maximum amount of power in the code and enable the programmers to change it, to modify it, to fix it. So actionability is the primary concern for programmers. In an ideal world, I would love to have direct suggestions in the IDE that tell the programmer, oh, this code is not, this code is going to burn this much carbon. You'd better change it. And for the system designers, what we want to do is we want to enable them to iterate our designs faster. We want them to enable this, we want them, we want to enable system designers to discover designs which are really low on energy, which are really high on performance, which are really high on carbon efficiency. So there's typically a design space that designers explore. And we want to enable them to explore the design space faster. That would be the end goal from this tool. Great. So what is the takeaway from this talk? If there's two things that I would love for you to take away, that if you forget everything else, okay, just remember these two things and I'd be very happy. First, we cannot improve what we cannot measure. We must measure correctly, okay, to improve things. And second, we need to break out of the CPU mindset, okay, non-CPU system components can dominate your power. Please remember that. Please remember these two things. And the next time I see you, please come say hi and I'll, I'll buy you lunch. Okay. Great. Thank you very much for listening to me. It's great to be here. It's great to talk to you. Please be in touch. Please reach out and oh, boy, we're out of time, but I'm very, very happy to get your questions. Come talk to me. There's still like two minutes for questions. So if there are any questions, please. Go for it. There's one in the back. Yes. So. So hello and thank you for this presentation. I hope you're not going to hate me for this question because I'm a primary infrastructure guy. And one thing I was always concerned about is redundancy, like a scale twice. So if one dies, is this part of your thinking and scope? Or does the question make sense? I'm really sorry. I don't fully understand what you mean by redundancy. I mean, sure, I understand, but redundancy is trying to solve the problem of fault tolerance. Okay. It's not trying to solve the problem of efficiency. I'm trying to solve the problem of efficiency. So redundancy is an orthogonal concern to mine. Does that, does that mean it makes sense? Yeah. Thank you. But thank you for the question. I really appreciate questions. Yes. Did you try to monitor the hover head of monitoring the energy consumption? Yes, that's a great question. No, we did not. On one side, I'm afraid it's going to be huge. On one side, I don't know. It's like an infinite recursion, you know, like how can I measure the impact of my tool itself? Like the tool is what measures the impact. But how do I measure the impact of the tool? I don't know. I hope, I hope that, I would love to believe that. That's what I, yes, that's what I want to believe. Yes, please. Thank you very much. It was great to be here. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.4, "text": " All right, everyone. I hope the mic is working. It's great to be here. This is my first fast", "tokens": [50364, 1057, 558, 11, 1518, 13, 286, 1454, 264, 3123, 307, 1364, 13, 467, 311, 869, 281, 312, 510, 13, 639, 307, 452, 700, 2370, 50934], "temperature": 0.0, "avg_logprob": -0.1942278795903272, "compression_ratio": 1.525, "no_speech_prob": 0.14242741465568542}, {"id": 1, "seek": 0, "start": 11.4, "end": 18.0, "text": " time, by the way. And I'm very happy to talk to you all about current measurement and energy", "tokens": [50934, 565, 11, 538, 264, 636, 13, 400, 286, 478, 588, 2055, 281, 751, 281, 291, 439, 466, 2190, 13160, 293, 2281, 51264], "temperature": 0.0, "avg_logprob": -0.1942278795903272, "compression_ratio": 1.525, "no_speech_prob": 0.14242741465568542}, {"id": 2, "seek": 0, "start": 18.0, "end": 24.14, "text": " attribution for processes and hardware devices in Linux. My name's Aditya, but you can call", "tokens": [51264, 9080, 1448, 337, 7555, 293, 8837, 5759, 294, 18734, 13, 1222, 1315, 311, 1999, 507, 64, 11, 457, 291, 393, 818, 51571], "temperature": 0.0, "avg_logprob": -0.1942278795903272, "compression_ratio": 1.525, "no_speech_prob": 0.14242741465568542}, {"id": 3, "seek": 0, "start": 24.14, "end": 29.42, "text": " me Adi. That's the first three letters. I'm a grad student. And yeah, that's my contact.", "tokens": [51571, 385, 1999, 72, 13, 663, 311, 264, 700, 1045, 7825, 13, 286, 478, 257, 2771, 3107, 13, 400, 1338, 11, 300, 311, 452, 3385, 13, 51835], "temperature": 0.0, "avg_logprob": -0.1942278795903272, "compression_ratio": 1.525, "no_speech_prob": 0.14242741465568542}, {"id": 4, "seek": 2942, "start": 29.42, "end": 35.84, "text": " I'm always very happy to talk to people during, before, and after my talk. Please reach out.", "tokens": [50364, 286, 478, 1009, 588, 2055, 281, 751, 281, 561, 1830, 11, 949, 11, 293, 934, 452, 751, 13, 2555, 2524, 484, 13, 50685], "temperature": 0.0, "avg_logprob": -0.22145520260459498, "compression_ratio": 1.4696356275303644, "no_speech_prob": 0.002712896326556802}, {"id": 5, "seek": 2942, "start": 35.84, "end": 40.64, "text": " Please. I would love to hear from you. So a bit of background. I'm a graduate student", "tokens": [50685, 2555, 13, 286, 576, 959, 281, 1568, 490, 291, 13, 407, 257, 857, 295, 3678, 13, 286, 478, 257, 8080, 3107, 50925], "temperature": 0.0, "avg_logprob": -0.22145520260459498, "compression_ratio": 1.4696356275303644, "no_speech_prob": 0.002712896326556802}, {"id": 6, "seek": 2942, "start": 40.64, "end": 47.82000000000001, "text": " at ETHEHA Zirik in Switzerland. And I do research at the intersection of computer architecture", "tokens": [50925, 412, 462, 9620, 36, 4983, 1176, 347, 1035, 294, 23312, 13, 400, 286, 360, 2132, 412, 264, 15236, 295, 3820, 9482, 51284], "temperature": 0.0, "avg_logprob": -0.22145520260459498, "compression_ratio": 1.4696356275303644, "no_speech_prob": 0.002712896326556802}, {"id": 7, "seek": 2942, "start": 47.82000000000001, "end": 55.1, "text": " and operating systems. I love this stuff very much. Great. What do we want to talk about?", "tokens": [51284, 293, 7447, 3652, 13, 286, 959, 341, 1507, 588, 709, 13, 3769, 13, 708, 360, 321, 528, 281, 751, 466, 30, 51648], "temperature": 0.0, "avg_logprob": -0.22145520260459498, "compression_ratio": 1.4696356275303644, "no_speech_prob": 0.002712896326556802}, {"id": 8, "seek": 5510, "start": 55.1, "end": 59.480000000000004, "text": " Let's get a bit of a brief background to bring everyone on the same page. Now, when we talk", "tokens": [50364, 961, 311, 483, 257, 857, 295, 257, 5353, 3678, 281, 1565, 1518, 322, 264, 912, 3028, 13, 823, 11, 562, 321, 751, 50583], "temperature": 0.0, "avg_logprob": -0.17471237182617189, "compression_ratio": 1.6544117647058822, "no_speech_prob": 0.14788317680358887}, {"id": 9, "seek": 5510, "start": 59.480000000000004, "end": 63.980000000000004, "text": " about energy sources and computing systems, you can have a bunch of options. You can have", "tokens": [50583, 466, 2281, 7139, 293, 15866, 3652, 11, 291, 393, 362, 257, 3840, 295, 3956, 13, 509, 393, 362, 50808], "temperature": 0.0, "avg_logprob": -0.17471237182617189, "compression_ratio": 1.6544117647058822, "no_speech_prob": 0.14788317680358887}, {"id": 10, "seek": 5510, "start": 63.980000000000004, "end": 70.34, "text": " direct inputs from DC, from USB. You can have battery power systems. And if you're really", "tokens": [50808, 2047, 15743, 490, 9114, 11, 490, 10109, 13, 509, 393, 362, 5809, 1347, 3652, 13, 400, 498, 291, 434, 534, 51126], "temperature": 0.0, "avg_logprob": -0.17471237182617189, "compression_ratio": 1.6544117647058822, "no_speech_prob": 0.14788317680358887}, {"id": 11, "seek": 5510, "start": 70.34, "end": 77.14, "text": " exotic, you can even have energy harvesting devices. Okay? Now, we want to use the maximum.", "tokens": [51126, 27063, 11, 291, 393, 754, 362, 2281, 35679, 5759, 13, 1033, 30, 823, 11, 321, 528, 281, 764, 264, 6674, 13, 51466], "temperature": 0.0, "avg_logprob": -0.17471237182617189, "compression_ratio": 1.6544117647058822, "no_speech_prob": 0.14788317680358887}, {"id": 12, "seek": 5510, "start": 77.14, "end": 82.54, "text": " No, I'm sorry. We want to use the minimum amount of energy to perform our task. Why do", "tokens": [51466, 883, 11, 286, 478, 2597, 13, 492, 528, 281, 764, 264, 7285, 2372, 295, 2281, 281, 2042, 527, 5633, 13, 1545, 360, 51736], "temperature": 0.0, "avg_logprob": -0.17471237182617189, "compression_ratio": 1.6544117647058822, "no_speech_prob": 0.14788317680358887}, {"id": 13, "seek": 8254, "start": 82.54, "end": 88.5, "text": " we want to use the minimum amount of energy? Because energy consumption correlates with", "tokens": [50364, 321, 528, 281, 764, 264, 7285, 2372, 295, 2281, 30, 1436, 2281, 12126, 13983, 1024, 365, 50662], "temperature": 0.0, "avg_logprob": -0.11484001159667968, "compression_ratio": 1.5898617511520738, "no_speech_prob": 0.005462971981614828}, {"id": 14, "seek": 8254, "start": 88.5, "end": 95.58000000000001, "text": " battery capacity. And battery capacity is a major, a significant design constraint for", "tokens": [50662, 5809, 6042, 13, 400, 5809, 6042, 307, 257, 2563, 11, 257, 4776, 1715, 25534, 337, 51016], "temperature": 0.0, "avg_logprob": -0.11484001159667968, "compression_ratio": 1.5898617511520738, "no_speech_prob": 0.005462971981614828}, {"id": 15, "seek": 8254, "start": 95.58000000000001, "end": 101.98, "text": " your consumers. Okay? All of us have cell phones. We have the recent buzz around Apple", "tokens": [51016, 428, 11883, 13, 1033, 30, 1057, 295, 505, 362, 2815, 10216, 13, 492, 362, 264, 5162, 13036, 926, 6373, 51336], "temperature": 0.0, "avg_logprob": -0.11484001159667968, "compression_ratio": 1.5898617511520738, "no_speech_prob": 0.005462971981614828}, {"id": 16, "seek": 8254, "start": 101.98, "end": 108.86000000000001, "text": " Vision Pro, AR devices. These devices are significantly restricted by the amount of", "tokens": [51336, 25170, 1705, 11, 8943, 5759, 13, 1981, 5759, 366, 10591, 20608, 538, 264, 2372, 295, 51680], "temperature": 0.0, "avg_logprob": -0.11484001159667968, "compression_ratio": 1.5898617511520738, "no_speech_prob": 0.005462971981614828}, {"id": 17, "seek": 10886, "start": 108.86, "end": 114.74, "text": " battery capacity. So we want to minimize the energy that we use to get the job done.", "tokens": [50364, 5809, 6042, 13, 407, 321, 528, 281, 17522, 264, 2281, 300, 321, 764, 281, 483, 264, 1691, 1096, 13, 50658], "temperature": 0.0, "avg_logprob": -0.15041246133692124, "compression_ratio": 1.456043956043956, "no_speech_prob": 0.2114393413066864}, {"id": 18, "seek": 10886, "start": 114.74, "end": 123.06, "text": " Okay? Now, what is the problem here? What do we want to solve? Let's flesh it out. Energy", "tokens": [50658, 1033, 30, 823, 11, 437, 307, 264, 1154, 510, 30, 708, 360, 321, 528, 281, 5039, 30, 961, 311, 12497, 309, 484, 13, 14939, 51074], "temperature": 0.0, "avg_logprob": -0.15041246133692124, "compression_ratio": 1.456043956043956, "no_speech_prob": 0.2114393413066864}, {"id": 19, "seek": 10886, "start": 123.06, "end": 132.94, "text": " consumption is defined as power times latency. Power is determined by your hardware. Okay?", "tokens": [51074, 12126, 307, 7642, 382, 1347, 1413, 27043, 13, 7086, 307, 9540, 538, 428, 8837, 13, 1033, 30, 51568], "temperature": 0.0, "avg_logprob": -0.15041246133692124, "compression_ratio": 1.456043956043956, "no_speech_prob": 0.2114393413066864}, {"id": 20, "seek": 13294, "start": 132.94, "end": 139.85999999999999, "text": " Latency is determined by your software. Okay? Now, how do we measure this? How do we get", "tokens": [50364, 7354, 3020, 307, 9540, 538, 428, 4722, 13, 1033, 30, 823, 11, 577, 360, 321, 3481, 341, 30, 1012, 360, 321, 483, 50710], "temperature": 0.0, "avg_logprob": -0.1432058998707975, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.31350287795066833}, {"id": 21, "seek": 13294, "start": 139.85999999999999, "end": 145.78, "text": " this data? Programmers often measure latency using well-established tools. I'm guessing", "tokens": [50710, 341, 1412, 30, 8338, 18552, 2049, 3481, 27043, 1228, 731, 12, 33542, 4173, 3873, 13, 286, 478, 17939, 51006], "temperature": 0.0, "avg_logprob": -0.1432058998707975, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.31350287795066833}, {"id": 22, "seek": 13294, "start": 145.78, "end": 151.38, "text": " many of you would be familiar with Linux Perf, or you would have timed your own software", "tokens": [51006, 867, 295, 291, 576, 312, 4963, 365, 18734, 3026, 69, 11, 420, 291, 576, 362, 44696, 428, 1065, 4722, 51286], "temperature": 0.0, "avg_logprob": -0.1432058998707975, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.31350287795066833}, {"id": 23, "seek": 13294, "start": 151.38, "end": 157.02, "text": " using wall clock time using CPU clock cycles, right? Now, these are well-established metrics", "tokens": [51286, 1228, 2929, 7830, 565, 1228, 13199, 7830, 17796, 11, 558, 30, 823, 11, 613, 366, 731, 12, 33542, 4173, 16367, 51568], "temperature": 0.0, "avg_logprob": -0.1432058998707975, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.31350287795066833}, {"id": 24, "seek": 15702, "start": 157.02, "end": 164.54000000000002, "text": " and well-established tools to quantify your latency. What if I ask you, do you know of", "tokens": [50364, 293, 731, 12, 33542, 4173, 3873, 281, 40421, 428, 27043, 13, 708, 498, 286, 1029, 291, 11, 360, 291, 458, 295, 50740], "temperature": 0.0, "avg_logprob": -0.1334787317224451, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.4331801235675812}, {"id": 25, "seek": 15702, "start": 164.54000000000002, "end": 170.06, "text": " any tools to calculate your application's energy? What comes to your mind when I pose", "tokens": [50740, 604, 3873, 281, 8873, 428, 3861, 311, 2281, 30, 708, 1487, 281, 428, 1575, 562, 286, 10774, 51016], "temperature": 0.0, "avg_logprob": -0.1334787317224451, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.4331801235675812}, {"id": 26, "seek": 15702, "start": 170.06, "end": 175.82000000000002, "text": " this question to you? How would you calculate your application's energy consumption? You", "tokens": [51016, 341, 1168, 281, 291, 30, 1012, 576, 291, 8873, 428, 3861, 311, 2281, 12126, 30, 509, 51304], "temperature": 0.0, "avg_logprob": -0.1334787317224451, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.4331801235675812}, {"id": 27, "seek": 15702, "start": 175.82000000000002, "end": 181.22, "text": " would say, okay, Ali, I know. This is very simple, right? Energy is power times latency,", "tokens": [51304, 576, 584, 11, 1392, 11, 12020, 11, 286, 458, 13, 639, 307, 588, 2199, 11, 558, 30, 14939, 307, 1347, 1413, 27043, 11, 51574], "temperature": 0.0, "avg_logprob": -0.1334787317224451, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.4331801235675812}, {"id": 28, "seek": 15702, "start": 181.22, "end": 186.98000000000002, "text": " right? We just talked about this. I'll get the power from the CPU. My CPU has this magical", "tokens": [51574, 558, 30, 492, 445, 2825, 466, 341, 13, 286, 603, 483, 264, 1347, 490, 264, 13199, 13, 1222, 13199, 575, 341, 12066, 51862], "temperature": 0.0, "avg_logprob": -0.1334787317224451, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.4331801235675812}, {"id": 29, "seek": 18698, "start": 186.98, "end": 193.14, "text": " interface called Rappel, which stands for running average power limit. I'm going to", "tokens": [50364, 9226, 1219, 497, 1746, 338, 11, 597, 7382, 337, 2614, 4274, 1347, 4948, 13, 286, 478, 516, 281, 50672], "temperature": 0.0, "avg_logprob": -0.23212952666230255, "compression_ratio": 1.4725738396624473, "no_speech_prob": 0.024022717028856277}, {"id": 30, "seek": 18698, "start": 193.14, "end": 199.7, "text": " get the value and they, oh, voila, my CPU says 15 watts right now. Great. Then I'll time", "tokens": [50672, 483, 264, 2158, 293, 436, 11, 1954, 11, 45565, 11, 452, 13199, 1619, 2119, 31247, 558, 586, 13, 3769, 13, 1396, 286, 603, 565, 51000], "temperature": 0.0, "avg_logprob": -0.23212952666230255, "compression_ratio": 1.4725738396624473, "no_speech_prob": 0.024022717028856277}, {"id": 31, "seek": 18698, "start": 199.7, "end": 206.82, "text": " my application and my application turns out to be, let's say, five milliseconds. Okay?", "tokens": [51000, 452, 3861, 293, 452, 3861, 4523, 484, 281, 312, 11, 718, 311, 584, 11, 1732, 34184, 13, 1033, 30, 51356], "temperature": 0.0, "avg_logprob": -0.23212952666230255, "compression_ratio": 1.4725738396624473, "no_speech_prob": 0.024022717028856277}, {"id": 32, "seek": 18698, "start": 206.82, "end": 213.66, "text": " And we put these values into this formula and great, we have 75 mJ of energy consumption.", "tokens": [51356, 400, 321, 829, 613, 4190, 666, 341, 8513, 293, 869, 11, 321, 362, 9562, 275, 41, 295, 2281, 12126, 13, 51698], "temperature": 0.0, "avg_logprob": -0.23212952666230255, "compression_ratio": 1.4725738396624473, "no_speech_prob": 0.024022717028856277}, {"id": 33, "seek": 21366, "start": 213.66, "end": 220.57999999999998, "text": " Stop done. Let's go home. Unfortunately, this is too simplistic. Let's try to dive into", "tokens": [50364, 5535, 1096, 13, 961, 311, 352, 1280, 13, 8590, 11, 341, 307, 886, 44199, 13, 961, 311, 853, 281, 9192, 666, 50710], "temperature": 0.0, "avg_logprob": -0.1467578411102295, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.040793366730213165}, {"id": 34, "seek": 21366, "start": 220.57999999999998, "end": 225.42, "text": " what we missed here. This does not reflect the ground reality, okay? And now I'm going", "tokens": [50710, 437, 321, 6721, 510, 13, 639, 775, 406, 5031, 264, 2727, 4103, 11, 1392, 30, 400, 586, 286, 478, 516, 50952], "temperature": 0.0, "avg_logprob": -0.1467578411102295, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.040793366730213165}, {"id": 35, "seek": 21366, "start": 225.42, "end": 232.82, "text": " to deconstruct what happened here and what we missed. The first step, we saw the power", "tokens": [50952, 281, 49473, 1757, 437, 2011, 510, 293, 437, 321, 6721, 13, 440, 700, 1823, 11, 321, 1866, 264, 1347, 51322], "temperature": 0.0, "avg_logprob": -0.1467578411102295, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.040793366730213165}, {"id": 36, "seek": 21366, "start": 232.82, "end": 239.62, "text": " was 15 watts. And unfortunately, this model assumes a linear power draw over time. That", "tokens": [51322, 390, 2119, 31247, 13, 400, 7015, 11, 341, 2316, 37808, 257, 8213, 1347, 2642, 670, 565, 13, 663, 51662], "temperature": 0.0, "avg_logprob": -0.1467578411102295, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.040793366730213165}, {"id": 37, "seek": 23962, "start": 239.62, "end": 245.24, "text": " is not the case. If you actually look at the system, this is what it looks like. You", "tokens": [50364, 307, 406, 264, 1389, 13, 759, 291, 767, 574, 412, 264, 1185, 11, 341, 307, 437, 309, 1542, 411, 13, 509, 50645], "temperature": 0.0, "avg_logprob": -0.126946645123618, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.04143732413649559}, {"id": 38, "seek": 23962, "start": 245.24, "end": 250.46, "text": " have these values and you have these peaks. And if you measure your power at the wrong", "tokens": [50645, 362, 613, 4190, 293, 291, 362, 613, 26897, 13, 400, 498, 291, 3481, 428, 1347, 412, 264, 2085, 50906], "temperature": 0.0, "avg_logprob": -0.126946645123618, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.04143732413649559}, {"id": 39, "seek": 23962, "start": 250.46, "end": 255.8, "text": " time, you will end up with a significantly different number than what you should have.", "tokens": [50906, 565, 11, 291, 486, 917, 493, 365, 257, 10591, 819, 1230, 813, 437, 291, 820, 362, 13, 51173], "temperature": 0.0, "avg_logprob": -0.126946645123618, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.04143732413649559}, {"id": 40, "seek": 23962, "start": 255.8, "end": 262.22, "text": " On the x-axis, you have time value. On the y-axis, you have the power for CPU. And power", "tokens": [51173, 1282, 264, 2031, 12, 24633, 11, 291, 362, 565, 2158, 13, 1282, 264, 288, 12, 24633, 11, 291, 362, 264, 1347, 337, 13199, 13, 400, 1347, 51494], "temperature": 0.0, "avg_logprob": -0.126946645123618, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.04143732413649559}, {"id": 41, "seek": 23962, "start": 262.22, "end": 269.58, "text": " consumption is not linear. So this assumption that we have a linear power draw is incorrect.", "tokens": [51494, 12126, 307, 406, 8213, 13, 407, 341, 15302, 300, 321, 362, 257, 8213, 1347, 2642, 307, 18424, 13, 51862], "temperature": 0.0, "avg_logprob": -0.126946645123618, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.04143732413649559}, {"id": 42, "seek": 26958, "start": 270.53999999999996, "end": 276.41999999999996, "text": " Second, we got the power value from Rappel. Remember running average power limit? It turns", "tokens": [50412, 5736, 11, 321, 658, 264, 1347, 2158, 490, 497, 1746, 338, 13, 5459, 2614, 4274, 1347, 4948, 30, 467, 4523, 50706], "temperature": 0.0, "avg_logprob": -0.1737198528942761, "compression_ratio": 1.5335689045936396, "no_speech_prob": 0.007687119767069817}, {"id": 43, "seek": 26958, "start": 276.41999999999996, "end": 283.09999999999997, "text": " out that Rappel is only available on Intel or sometimes on AMD. ARM, for example, has", "tokens": [50706, 484, 300, 497, 1746, 338, 307, 787, 2435, 322, 19762, 420, 2171, 322, 34808, 13, 45209, 11, 337, 1365, 11, 575, 51040], "temperature": 0.0, "avg_logprob": -0.1737198528942761, "compression_ratio": 1.5335689045936396, "no_speech_prob": 0.007687119767069817}, {"id": 44, "seek": 26958, "start": 283.09999999999997, "end": 289.34, "text": " a very, very different interface to report power. So I would love to share a story. I", "tokens": [51040, 257, 588, 11, 588, 819, 9226, 281, 2275, 1347, 13, 407, 286, 576, 959, 281, 2073, 257, 1657, 13, 286, 51352], "temperature": 0.0, "avg_logprob": -0.1737198528942761, "compression_ratio": 1.5335689045936396, "no_speech_prob": 0.007687119767069817}, {"id": 45, "seek": 26958, "start": 289.34, "end": 294.14, "text": " was doing energy profiling on a server-class system back in university and I said, oh,", "tokens": [51352, 390, 884, 2281, 1740, 4883, 322, 257, 7154, 12, 11665, 1185, 646, 294, 5454, 293, 286, 848, 11, 1954, 11, 51592], "temperature": 0.0, "avg_logprob": -0.1737198528942761, "compression_ratio": 1.5335689045936396, "no_speech_prob": 0.007687119767069817}, {"id": 46, "seek": 26958, "start": 294.14, "end": 298.94, "text": " I've built this great infrastructure on my Intel platform, right? Let me just use it", "tokens": [51592, 286, 600, 3094, 341, 869, 6896, 322, 452, 19762, 3663, 11, 558, 30, 961, 385, 445, 764, 309, 51832], "temperature": 0.0, "avg_logprob": -0.1737198528942761, "compression_ratio": 1.5335689045936396, "no_speech_prob": 0.007687119767069817}, {"id": 47, "seek": 29894, "start": 298.98, "end": 305.18, "text": " to run on ARM and see what happens. And the moment I ran it on ARM, Linux Perf said, I'm", "tokens": [50366, 281, 1190, 322, 45209, 293, 536, 437, 2314, 13, 400, 264, 1623, 286, 5872, 309, 322, 45209, 11, 18734, 3026, 69, 848, 11, 286, 478, 50676], "temperature": 0.0, "avg_logprob": -0.2021783498617319, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.002888323040679097}, {"id": 48, "seek": 29894, "start": 305.18, "end": 309.58, "text": " sorry, I don't recognize the CPU. I can't give you any numbers. And it just crashed,", "tokens": [50676, 2597, 11, 286, 500, 380, 5521, 264, 13199, 13, 286, 393, 380, 976, 291, 604, 3547, 13, 400, 309, 445, 24190, 11, 50896], "temperature": 0.0, "avg_logprob": -0.2021783498617319, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.002888323040679097}, {"id": 49, "seek": 29894, "start": 309.58, "end": 315.42, "text": " okay? So all of these interfaces are really different and you need a significant amount", "tokens": [50896, 1392, 30, 407, 439, 295, 613, 28416, 366, 534, 819, 293, 291, 643, 257, 4776, 2372, 51188], "temperature": 0.0, "avg_logprob": -0.2021783498617319, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.002888323040679097}, {"id": 50, "seek": 29894, "start": 315.42, "end": 320.42, "text": " of engineering to make sense of it across different platforms. Second limitation is that", "tokens": [51188, 295, 7043, 281, 652, 2020, 295, 309, 2108, 819, 9473, 13, 5736, 27432, 307, 300, 51438], "temperature": 0.0, "avg_logprob": -0.2021783498617319, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.002888323040679097}, {"id": 51, "seek": 29894, "start": 320.42, "end": 325.5, "text": " we do not have uniform interfaces or the formats to measure power reliably.", "tokens": [51438, 321, 360, 406, 362, 9452, 28416, 420, 264, 25879, 281, 3481, 1347, 49927, 13, 51692], "temperature": 0.0, "avg_logprob": -0.2021783498617319, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.002888323040679097}, {"id": 52, "seek": 32550, "start": 326.5, "end": 332.5, "text": " All right. Let's try to go deeper. Let's try to get more into the closer to the ground", "tokens": [50414, 1057, 558, 13, 961, 311, 853, 281, 352, 7731, 13, 961, 311, 853, 281, 483, 544, 666, 264, 4966, 281, 264, 2727, 50714], "temperature": 0.0, "avg_logprob": -0.22174267335371536, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.0008957163081504405}, {"id": 53, "seek": 32550, "start": 332.7, "end": 339.7, "text": " truth. Our model got the power value from the CPU. What about the other devices? I'm right", "tokens": [50724, 3494, 13, 2621, 2316, 658, 264, 1347, 2158, 490, 264, 13199, 13, 708, 466, 264, 661, 5759, 30, 286, 478, 558, 51074], "temperature": 0.0, "avg_logprob": -0.22174267335371536, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.0008957163081504405}, {"id": 54, "seek": 32550, "start": 341.3, "end": 348.3, "text": " now broadcasting from this device and oh my God. I'm sorry for this.", "tokens": [51154, 586, 30024, 490, 341, 4302, 293, 1954, 452, 1265, 13, 286, 478, 2597, 337, 341, 13, 51504], "temperature": 0.0, "avg_logprob": -0.22174267335371536, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.0008957163081504405}, {"id": 55, "seek": 35550, "start": 356.26, "end": 361.26, "text": " I hope not. Give me a sec.", "tokens": [50402, 286, 1454, 406, 13, 5303, 385, 257, 907, 13, 50652], "temperature": 0.0, "avg_logprob": -0.24611317027698865, "compression_ratio": 1.3493150684931507, "no_speech_prob": 0.008831380866467953}, {"id": 56, "seek": 35550, "start": 369.26, "end": 376.26, "text": " Beautiful. Beautiful. Okay, okay. So back to the presentation. We were talking about", "tokens": [51052, 14724, 13, 14724, 13, 1033, 11, 1392, 13, 407, 646, 281, 264, 5860, 13, 492, 645, 1417, 466, 51402], "temperature": 0.0, "avg_logprob": -0.24611317027698865, "compression_ratio": 1.3493150684931507, "no_speech_prob": 0.008831380866467953}, {"id": 57, "seek": 35550, "start": 376.26, "end": 380.78, "text": " the impact of devices like the screen, the memory, the network cards, right? We don't", "tokens": [51402, 264, 2712, 295, 5759, 411, 264, 2568, 11, 264, 4675, 11, 264, 3209, 5632, 11, 558, 30, 492, 500, 380, 51628], "temperature": 0.0, "avg_logprob": -0.24611317027698865, "compression_ratio": 1.3493150684931507, "no_speech_prob": 0.008831380866467953}, {"id": 58, "seek": 38078, "start": 380.82, "end": 387.17999999999995, "text": " know how to quantify them. So we did a lot of experiments and it turns out that these", "tokens": [50366, 458, 577, 281, 40421, 552, 13, 407, 321, 630, 257, 688, 295, 12050, 293, 309, 4523, 484, 300, 613, 50684], "temperature": 0.0, "avg_logprob": -0.11963213191312902, "compression_ratio": 1.7038461538461538, "no_speech_prob": 0.01185538712888956}, {"id": 59, "seek": 38078, "start": 387.17999999999995, "end": 393.78, "text": " devices very often dominate your power consumption and our findings are also correlated from", "tokens": [50684, 5759, 588, 2049, 28246, 428, 1347, 12126, 293, 527, 16483, 366, 611, 38574, 490, 51014], "temperature": 0.0, "avg_logprob": -0.11963213191312902, "compression_ratio": 1.7038461538461538, "no_speech_prob": 0.01185538712888956}, {"id": 60, "seek": 38078, "start": 393.78, "end": 397.5, "text": " some similar observations at Google. So what Google did was they were trying to optimize", "tokens": [51014, 512, 2531, 18163, 412, 3329, 13, 407, 437, 3329, 630, 390, 436, 645, 1382, 281, 19719, 51200], "temperature": 0.0, "avg_logprob": -0.11963213191312902, "compression_ratio": 1.7038461538461538, "no_speech_prob": 0.01185538712888956}, {"id": 61, "seek": 38078, "start": 397.5, "end": 402.46, "text": " their data centers and did a huge amount of profiling on their server class CPUs. Server", "tokens": [51200, 641, 1412, 10898, 293, 630, 257, 2603, 2372, 295, 1740, 4883, 322, 641, 7154, 1508, 13199, 82, 13, 25684, 51448], "temperature": 0.0, "avg_logprob": -0.11963213191312902, "compression_ratio": 1.7038461538461538, "no_speech_prob": 0.01185538712888956}, {"id": 62, "seek": 38078, "start": 402.46, "end": 407.97999999999996, "text": " class CPUs are the heaviest CPUs that you can get in the market. And it turns out that", "tokens": [51448, 1508, 13199, 82, 366, 264, 3577, 6495, 13199, 82, 300, 291, 393, 483, 294, 264, 2142, 13, 400, 309, 4523, 484, 300, 51724], "temperature": 0.0, "avg_logprob": -0.11963213191312902, "compression_ratio": 1.7038461538461538, "no_speech_prob": 0.01185538712888956}, {"id": 63, "seek": 40798, "start": 408.06, "end": 415.06, "text": " they observed that DRAM is dominating their power because DRAM is burning power all the", "tokens": [50368, 436, 13095, 300, 12118, 2865, 307, 43306, 641, 1347, 570, 12118, 2865, 307, 9488, 1347, 439, 264, 50718], "temperature": 0.0, "avg_logprob": -0.13303252628871373, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0009695831686258316}, {"id": 64, "seek": 40798, "start": 415.14000000000004, "end": 421.3, "text": " time. CPU turns on and off, but the DRAM you cannot turn off. Remember, it is volatile.", "tokens": [50722, 565, 13, 13199, 4523, 322, 293, 766, 11, 457, 264, 12118, 2865, 291, 2644, 1261, 766, 13, 5459, 11, 309, 307, 34377, 13, 51030], "temperature": 0.0, "avg_logprob": -0.13303252628871373, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0009695831686258316}, {"id": 65, "seek": 40798, "start": 421.3, "end": 428.3, "text": " So you need to break out of this mindset that CPU is the end all be all. Okay. So let me", "tokens": [51030, 407, 291, 643, 281, 1821, 484, 295, 341, 12543, 300, 13199, 307, 264, 917, 439, 312, 439, 13, 1033, 13, 407, 718, 385, 51380], "temperature": 0.0, "avg_logprob": -0.13303252628871373, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0009695831686258316}, {"id": 66, "seek": 40798, "start": 428.86, "end": 435.86, "text": " try to summarize everything. We are inaccurately calculating only a fraction of the system's", "tokens": [51408, 853, 281, 20858, 1203, 13, 492, 366, 37957, 374, 1592, 28258, 787, 257, 14135, 295, 264, 1185, 311, 51758], "temperature": 0.0, "avg_logprob": -0.13303252628871373, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0009695831686258316}, {"id": 67, "seek": 43586, "start": 436.1, "end": 443.18, "text": " actual energy consumption. Okay. And I would love to put this in a take of a code for you.", "tokens": [50376, 3539, 2281, 12126, 13, 1033, 13, 400, 286, 576, 959, 281, 829, 341, 294, 257, 747, 295, 257, 3089, 337, 291, 13, 50730], "temperature": 0.0, "avg_logprob": -0.09786413113276164, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.004130731336772442}, {"id": 68, "seek": 43586, "start": 443.18, "end": 448.54, "text": " This is not from me, but I like this very much. We cannot improve what we cannot measure.", "tokens": [50730, 639, 307, 406, 490, 385, 11, 457, 286, 411, 341, 588, 709, 13, 492, 2644, 3470, 437, 321, 2644, 3481, 13, 50998], "temperature": 0.0, "avg_logprob": -0.09786413113276164, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.004130731336772442}, {"id": 69, "seek": 43586, "start": 448.54, "end": 453.78000000000003, "text": " So we first need to understand how to measure energy correctly. And that's what my project", "tokens": [50998, 407, 321, 700, 643, 281, 1223, 577, 281, 3481, 2281, 8944, 13, 400, 300, 311, 437, 452, 1716, 51260], "temperature": 0.0, "avg_logprob": -0.09786413113276164, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.004130731336772442}, {"id": 70, "seek": 43586, "start": 453.78000000000003, "end": 460.34000000000003, "text": " is all about. That's what I love to do. What is the goal of what I'm trying to do here?", "tokens": [51260, 307, 439, 466, 13, 663, 311, 437, 286, 959, 281, 360, 13, 708, 307, 264, 3387, 295, 437, 286, 478, 1382, 281, 360, 510, 30, 51588], "temperature": 0.0, "avg_logprob": -0.09786413113276164, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.004130731336772442}, {"id": 71, "seek": 46034, "start": 460.34, "end": 466.9, "text": " My goal is to develop a framework to accurately and reliably measure the energy consumption", "tokens": [50364, 1222, 3387, 307, 281, 1499, 257, 8388, 281, 20095, 293, 49927, 3481, 264, 2281, 12126, 50692], "temperature": 0.0, "avg_logprob": -0.14704560149799695, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.0015483360039070249}, {"id": 72, "seek": 46034, "start": 466.9, "end": 473.9, "text": " of processes in the kernel. All right. All of us can get this data. What is the use for", "tokens": [50692, 295, 7555, 294, 264, 28256, 13, 1057, 558, 13, 1057, 295, 505, 393, 483, 341, 1412, 13, 708, 307, 264, 764, 337, 51042], "temperature": 0.0, "avg_logprob": -0.14704560149799695, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.0015483360039070249}, {"id": 73, "seek": 46034, "start": 474.29999999999995, "end": 481.29999999999995, "text": " this? Because data without use is it does not get used. Okay. Once we have this data,", "tokens": [51062, 341, 30, 1436, 1412, 1553, 764, 307, 309, 775, 406, 483, 1143, 13, 1033, 13, 3443, 321, 362, 341, 1412, 11, 51412], "temperature": 0.0, "avg_logprob": -0.14704560149799695, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.0015483360039070249}, {"id": 74, "seek": 46034, "start": 481.29999999999995, "end": 486.73999999999995, "text": " we want to report it to the end users in an easy to understand format. Right. End users", "tokens": [51412, 321, 528, 281, 2275, 309, 281, 264, 917, 5022, 294, 364, 1858, 281, 1223, 7877, 13, 1779, 13, 6967, 5022, 51684], "temperature": 0.0, "avg_logprob": -0.14704560149799695, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.0015483360039070249}, {"id": 75, "seek": 48674, "start": 486.78000000000003, "end": 490.66, "text": " should be able to make sense of the number. Right. What does this number mean for me?", "tokens": [50366, 820, 312, 1075, 281, 652, 2020, 295, 264, 1230, 13, 1779, 13, 708, 775, 341, 1230, 914, 337, 385, 30, 50560], "temperature": 0.0, "avg_logprob": -0.1460799811021337, "compression_ratio": 1.8122448979591836, "no_speech_prob": 0.005904376972466707}, {"id": 76, "seek": 48674, "start": 490.66, "end": 495.46000000000004, "text": " We wanted to report it to the programmers which improve their action ability, which enable", "tokens": [50560, 492, 1415, 281, 2275, 309, 281, 264, 41504, 597, 3470, 641, 3069, 3485, 11, 597, 9528, 50800], "temperature": 0.0, "avg_logprob": -0.1460799811021337, "compression_ratio": 1.8122448979591836, "no_speech_prob": 0.005904376972466707}, {"id": 77, "seek": 48674, "start": 495.46000000000004, "end": 501.54, "text": " them to move their code up and down to change their code to move the numbers. Right. And", "tokens": [50800, 552, 281, 1286, 641, 3089, 493, 293, 760, 281, 1319, 641, 3089, 281, 1286, 264, 3547, 13, 1779, 13, 400, 51104], "temperature": 0.0, "avg_logprob": -0.1460799811021337, "compression_ratio": 1.8122448979591836, "no_speech_prob": 0.005904376972466707}, {"id": 78, "seek": 48674, "start": 501.54, "end": 506.64, "text": " we want to report it to system designers to enable them to iterate much faster over low", "tokens": [51104, 321, 528, 281, 2275, 309, 281, 1185, 16196, 281, 9528, 552, 281, 44497, 709, 4663, 670, 2295, 51359], "temperature": 0.0, "avg_logprob": -0.1460799811021337, "compression_ratio": 1.8122448979591836, "no_speech_prob": 0.005904376972466707}, {"id": 79, "seek": 48674, "start": 506.64, "end": 513.14, "text": " energy designs over low carbon designs. Okay. So let's try to dive deeper. What do we mean", "tokens": [51359, 2281, 11347, 670, 2295, 5954, 11347, 13, 1033, 13, 407, 718, 311, 853, 281, 9192, 7731, 13, 708, 360, 321, 914, 51684], "temperature": 0.0, "avg_logprob": -0.1460799811021337, "compression_ratio": 1.8122448979591836, "no_speech_prob": 0.005904376972466707}, {"id": 80, "seek": 51314, "start": 513.14, "end": 519.98, "text": " by a framework? What are we trying to do? Let's flesh it out. A framework comprises models", "tokens": [50364, 538, 257, 8388, 30, 708, 366, 321, 1382, 281, 360, 30, 961, 311, 12497, 309, 484, 13, 316, 8388, 16802, 3598, 5245, 50706], "temperature": 0.0, "avg_logprob": -0.10218785938463713, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0061892252415418625}, {"id": 81, "seek": 51314, "start": 519.98, "end": 526.98, "text": " and tools. Let's break down these two words. A power model is how we think about a device.", "tokens": [50706, 293, 3873, 13, 961, 311, 1821, 760, 613, 732, 2283, 13, 316, 1347, 2316, 307, 577, 321, 519, 466, 257, 4302, 13, 51056], "temperature": 0.0, "avg_logprob": -0.10218785938463713, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0061892252415418625}, {"id": 82, "seek": 51314, "start": 526.98, "end": 532.78, "text": " When I say that I want to measure power, a power model is the mental model that I have", "tokens": [51056, 1133, 286, 584, 300, 286, 528, 281, 3481, 1347, 11, 257, 1347, 2316, 307, 264, 4973, 2316, 300, 286, 362, 51346], "temperature": 0.0, "avg_logprob": -0.10218785938463713, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0061892252415418625}, {"id": 83, "seek": 51314, "start": 532.78, "end": 539.02, "text": " that I will use to get the value. Okay. And it turns out that these power models are often", "tokens": [51346, 300, 286, 486, 764, 281, 483, 264, 2158, 13, 1033, 13, 400, 309, 4523, 484, 300, 613, 1347, 5245, 366, 2049, 51658], "temperature": 0.0, "avg_logprob": -0.10218785938463713, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0061892252415418625}, {"id": 84, "seek": 53902, "start": 539.06, "end": 543.8199999999999, "text": " very poorly understood for a number of devices. For example, DRAM power models are often not", "tokens": [50366, 588, 22271, 7320, 337, 257, 1230, 295, 5759, 13, 1171, 1365, 11, 12118, 2865, 1347, 5245, 366, 2049, 406, 50604], "temperature": 0.0, "avg_logprob": -0.17027761254991805, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.0011327601969242096}, {"id": 85, "seek": 53902, "start": 543.8199999999999, "end": 549.1, "text": " available to the public. They're not available to academia. They're, let's say, a proprietary", "tokens": [50604, 2435, 281, 264, 1908, 13, 814, 434, 406, 2435, 281, 28937, 13, 814, 434, 11, 718, 311, 584, 11, 257, 38992, 50868], "temperature": 0.0, "avg_logprob": -0.17027761254991805, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.0011327601969242096}, {"id": 86, "seek": 53902, "start": 549.1, "end": 556.1, "text": " trade secret. Don't quote me on that. And okay, once we have these power models, we can build", "tokens": [50868, 4923, 4054, 13, 1468, 380, 6513, 385, 322, 300, 13, 400, 1392, 11, 1564, 321, 362, 613, 1347, 5245, 11, 321, 393, 1322, 51218], "temperature": 0.0, "avg_logprob": -0.17027761254991805, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.0011327601969242096}, {"id": 87, "seek": 53902, "start": 556.26, "end": 561.66, "text": " tools which accurately calculate power based on these models. A tool that I would like", "tokens": [51226, 3873, 597, 20095, 8873, 1347, 2361, 322, 613, 5245, 13, 316, 2290, 300, 286, 576, 411, 51496], "temperature": 0.0, "avg_logprob": -0.17027761254991805, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.0011327601969242096}, {"id": 88, "seek": 53902, "start": 561.66, "end": 566.5, "text": " to mention would be the NVIDIA SMI utility. It allows you to calculate the power of a", "tokens": [51496, 281, 2152, 576, 312, 264, 426, 3958, 6914, 13115, 40, 14877, 13, 467, 4045, 291, 281, 8873, 264, 1347, 295, 257, 51738], "temperature": 0.0, "avg_logprob": -0.17027761254991805, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.0011327601969242096}, {"id": 89, "seek": 56650, "start": 566.5, "end": 573.5, "text": " GPU using this tool. It's a good tool. And okay, so let's pull it all in. What I would", "tokens": [50364, 18407, 1228, 341, 2290, 13, 467, 311, 257, 665, 2290, 13, 400, 1392, 11, 370, 718, 311, 2235, 309, 439, 294, 13, 708, 286, 576, 50714], "temperature": 0.0, "avg_logprob": -0.15898427963256836, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0027962890453636646}, {"id": 90, "seek": 56650, "start": 578.1, "end": 584.18, "text": " like you to take away is that we need accurate models, first and foremost accurate models,", "tokens": [50944, 411, 291, 281, 747, 1314, 307, 300, 321, 643, 8559, 5245, 11, 700, 293, 18864, 8559, 5245, 11, 51248], "temperature": 0.0, "avg_logprob": -0.15898427963256836, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0027962890453636646}, {"id": 91, "seek": 56650, "start": 584.18, "end": 589.42, "text": " and second, reliable tools to calculate the energy consumption correctly. So we defined", "tokens": [51248, 293, 1150, 11, 12924, 3873, 281, 8873, 264, 2281, 12126, 8944, 13, 407, 321, 7642, 51510], "temperature": 0.0, "avg_logprob": -0.15898427963256836, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0027962890453636646}, {"id": 92, "seek": 56650, "start": 589.42, "end": 594.74, "text": " our problem and we defined our goal post where we want to go. And now let's see how are we", "tokens": [51510, 527, 1154, 293, 321, 7642, 527, 3387, 2183, 689, 321, 528, 281, 352, 13, 400, 586, 718, 311, 536, 577, 366, 321, 51776], "temperature": 0.0, "avg_logprob": -0.15898427963256836, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0027962890453636646}, {"id": 93, "seek": 59474, "start": 594.74, "end": 601.74, "text": " going to get from point A to point B. Great. So before I dive into what is the mechanism,", "tokens": [50364, 516, 281, 483, 490, 935, 316, 281, 935, 363, 13, 3769, 13, 407, 949, 286, 9192, 666, 437, 307, 264, 7513, 11, 50714], "temperature": 0.0, "avg_logprob": -0.08846407538061743, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0035334809217602015}, {"id": 94, "seek": 59474, "start": 602.78, "end": 608.58, "text": " I would like to bring to knowledge what has been done before. All of us have been here", "tokens": [50766, 286, 576, 411, 281, 1565, 281, 3601, 437, 575, 668, 1096, 949, 13, 1057, 295, 505, 362, 668, 510, 51056], "temperature": 0.0, "avg_logprob": -0.08846407538061743, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0035334809217602015}, {"id": 95, "seek": 59474, "start": 608.58, "end": 612.66, "text": " for the entire day, right? We love energy and we love efficiency. If this is such an", "tokens": [51056, 337, 264, 2302, 786, 11, 558, 30, 492, 959, 2281, 293, 321, 959, 10493, 13, 759, 341, 307, 1270, 364, 51260], "temperature": 0.0, "avg_logprob": -0.08846407538061743, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0035334809217602015}, {"id": 96, "seek": 59474, "start": 612.66, "end": 616.86, "text": " important problem, why didn't people solve it before? People did. People did try to solve", "tokens": [51260, 1021, 1154, 11, 983, 994, 380, 561, 5039, 309, 949, 30, 3432, 630, 13, 3432, 630, 853, 281, 5039, 51470], "temperature": 0.0, "avg_logprob": -0.08846407538061743, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0035334809217602015}, {"id": 97, "seek": 59474, "start": 616.86, "end": 620.86, "text": " it before and I'm going to describe to you right now what they did before and why that", "tokens": [51470, 309, 949, 293, 286, 478, 516, 281, 6786, 281, 291, 558, 586, 437, 436, 630, 949, 293, 983, 300, 51670], "temperature": 0.0, "avg_logprob": -0.08846407538061743, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0035334809217602015}, {"id": 98, "seek": 62086, "start": 620.86, "end": 626.1800000000001, "text": " is insufficient, why we need to do better. Okay, on the screen you can see a screenshot", "tokens": [50364, 307, 41709, 11, 983, 321, 643, 281, 360, 1101, 13, 1033, 11, 322, 264, 2568, 291, 393, 536, 257, 27712, 50630], "temperature": 0.0, "avg_logprob": -0.16136168298267184, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.0057194228284060955}, {"id": 99, "seek": 62086, "start": 626.1800000000001, "end": 631.98, "text": " from a tool from Intel that is known as PowerTop. And you can see the first column here which", "tokens": [50630, 490, 257, 2290, 490, 19762, 300, 307, 2570, 382, 7086, 50118, 13, 400, 291, 393, 536, 264, 700, 7738, 510, 597, 50920], "temperature": 0.0, "avg_logprob": -0.16136168298267184, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.0057194228284060955}, {"id": 100, "seek": 62086, "start": 631.98, "end": 636.86, "text": " reports power estimate. And on the right side you have the description of the particular", "tokens": [50920, 7122, 1347, 12539, 13, 400, 322, 264, 558, 1252, 291, 362, 264, 3855, 295, 264, 1729, 51164], "temperature": 0.0, "avg_logprob": -0.16136168298267184, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.0057194228284060955}, {"id": 101, "seek": 62086, "start": 636.86, "end": 643.86, "text": " device, interrupt, process for which this power estimate is calculated. Now, what are", "tokens": [51164, 4302, 11, 12729, 11, 1399, 337, 597, 341, 1347, 12539, 307, 15598, 13, 823, 11, 437, 366, 51514], "temperature": 0.0, "avg_logprob": -0.16136168298267184, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.0057194228284060955}, {"id": 102, "seek": 64386, "start": 644.86, "end": 651.86, "text": " the challenges? Well, first of all, I believe in energy. It turns out that power is a discrete", "tokens": [50414, 264, 4759, 30, 1042, 11, 700, 295, 439, 11, 286, 1697, 294, 2281, 13, 467, 4523, 484, 300, 1347, 307, 257, 27706, 50764], "temperature": 0.0, "avg_logprob": -0.13925206142923105, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.0016994287725538015}, {"id": 103, "seek": 64386, "start": 654.78, "end": 658.46, "text": " time event. What do we mean by discrete time event? Let's try to break this down. If you", "tokens": [50910, 565, 2280, 13, 708, 360, 321, 914, 538, 27706, 565, 2280, 30, 961, 311, 853, 281, 1821, 341, 760, 13, 759, 291, 51094], "temperature": 0.0, "avg_logprob": -0.13925206142923105, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.0016994287725538015}, {"id": 104, "seek": 64386, "start": 658.46, "end": 664.9, "text": " have a graph, a power is a single point on that graph. Energy is the area under that", "tokens": [51094, 362, 257, 4295, 11, 257, 1347, 307, 257, 2167, 935, 322, 300, 4295, 13, 14939, 307, 264, 1859, 833, 300, 51416], "temperature": 0.0, "avg_logprob": -0.13925206142923105, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.0016994287725538015}, {"id": 105, "seek": 64386, "start": 664.9, "end": 669.4200000000001, "text": " graph, okay? We want to calculate energy because energy is what correlates to your battery", "tokens": [51416, 4295, 11, 1392, 30, 492, 528, 281, 8873, 2281, 570, 2281, 307, 437, 13983, 1024, 281, 428, 5809, 51642], "temperature": 0.0, "avg_logprob": -0.13925206142923105, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.0016994287725538015}, {"id": 106, "seek": 66942, "start": 669.62, "end": 674.62, "text": " drain. Your battery supplies you energy. Power is just one particular instance in that time.", "tokens": [50374, 12339, 13, 2260, 5809, 11768, 291, 2281, 13, 7086, 307, 445, 472, 1729, 5197, 294, 300, 565, 13, 50624], "temperature": 0.4, "avg_logprob": -0.23819630416398196, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.008311192505061626}, {"id": 107, "seek": 66942, "start": 674.62, "end": 681.62, "text": " Second, PowerTop has a vendor-specific implementation. I hope that is clear. Third, what is the actionability?", "tokens": [50624, 5736, 11, 7086, 50118, 575, 257, 24321, 12, 29258, 11420, 13, 286, 1454, 300, 307, 1850, 13, 12548, 11, 437, 307, 264, 3069, 2310, 30, 50974], "temperature": 0.4, "avg_logprob": -0.23819630416398196, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.008311192505061626}, {"id": 108, "seek": 66942, "start": 684.5, "end": 688.8199999999999, "text": " So I just showed you this data. I just showed you the screenshot. It says, oh, my display", "tokens": [51118, 407, 286, 445, 4712, 291, 341, 1412, 13, 286, 445, 4712, 291, 264, 27712, 13, 467, 1619, 11, 1954, 11, 452, 4674, 51334], "temperature": 0.4, "avg_logprob": -0.23819630416398196, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.008311192505061626}, {"id": 109, "seek": 66942, "start": 688.8199999999999, "end": 695.8199999999999, "text": " backlight is taking 350 milliwatts. Great. This particular process is consuming 292 milliwatts.", "tokens": [51334, 646, 2764, 307, 1940, 18065, 26176, 44824, 1373, 13, 3769, 13, 639, 1729, 1399, 307, 19867, 9413, 17, 26176, 44824, 1373, 13, 51684], "temperature": 0.4, "avg_logprob": -0.23819630416398196, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.008311192505061626}, {"id": 110, "seek": 69582, "start": 695.82, "end": 701.86, "text": " Okay, fine. The question that comes to mind is what is the use for me? What is the actionability", "tokens": [50364, 1033, 11, 2489, 13, 440, 1168, 300, 1487, 281, 1575, 307, 437, 307, 264, 764, 337, 385, 30, 708, 307, 264, 3069, 2310, 50666], "temperature": 0.0, "avg_logprob": -0.09939265737728196, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.0012835547095164657}, {"id": 111, "seek": 69582, "start": 701.86, "end": 708.46, "text": " for the programmer for this data? How does the programmer change the code to move this", "tokens": [50666, 337, 264, 32116, 337, 341, 1412, 30, 1012, 775, 264, 32116, 1319, 264, 3089, 281, 1286, 341, 50996], "temperature": 0.0, "avg_logprob": -0.09939265737728196, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.0012835547095164657}, {"id": 112, "seek": 69582, "start": 708.46, "end": 714.3000000000001, "text": " number? And I don't know. How do I fix that? How do I fix something that I don't know how", "tokens": [50996, 1230, 30, 400, 286, 500, 380, 458, 13, 1012, 360, 286, 3191, 300, 30, 1012, 360, 286, 3191, 746, 300, 286, 500, 380, 458, 577, 51288], "temperature": 0.0, "avg_logprob": -0.09939265737728196, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.0012835547095164657}, {"id": 113, "seek": 69582, "start": 714.3000000000001, "end": 721.3000000000001, "text": " to fix? And that is a gap that I would like to bridge, right? So let me dive into the", "tokens": [51288, 281, 3191, 30, 400, 300, 307, 257, 7417, 300, 286, 576, 411, 281, 7283, 11, 558, 30, 407, 718, 385, 9192, 666, 264, 51638], "temperature": 0.0, "avg_logprob": -0.09939265737728196, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.0012835547095164657}, {"id": 114, "seek": 72130, "start": 721.8599999999999, "end": 727.6999999999999, "text": " guts of the system. This is a system design. On the screen you can see an elementary flow", "tokens": [50392, 28560, 295, 264, 1185, 13, 639, 307, 257, 1185, 1715, 13, 1282, 264, 2568, 291, 393, 536, 364, 16429, 3095, 50684], "temperature": 0.0, "avg_logprob": -0.14116968267104205, "compression_ratio": 1.8512820512820514, "no_speech_prob": 0.019092541188001633}, {"id": 115, "seek": 72130, "start": 727.6999999999999, "end": 733.8199999999999, "text": " chart which summarizes the system at a very high level. And this is a regression-based", "tokens": [50684, 6927, 597, 14611, 5660, 264, 1185, 412, 257, 588, 1090, 1496, 13, 400, 341, 307, 257, 24590, 12, 6032, 50990], "temperature": 0.0, "avg_logprob": -0.14116968267104205, "compression_ratio": 1.8512820512820514, "no_speech_prob": 0.019092541188001633}, {"id": 116, "seek": 72130, "start": 733.8199999999999, "end": 739.54, "text": " system. A regression-based system has two inputs. You have the parameters and you have", "tokens": [50990, 1185, 13, 316, 24590, 12, 6032, 1185, 575, 732, 15743, 13, 509, 362, 264, 9834, 293, 291, 362, 51276], "temperature": 0.0, "avg_logprob": -0.14116968267104205, "compression_ratio": 1.8512820512820514, "no_speech_prob": 0.019092541188001633}, {"id": 117, "seek": 72130, "start": 739.54, "end": 746.54, "text": " the inputs to the parameters. First, we calculate the parameters and then we calculate the inputs", "tokens": [51276, 264, 15743, 281, 264, 9834, 13, 2386, 11, 321, 8873, 264, 9834, 293, 550, 321, 8873, 264, 15743, 51626], "temperature": 0.0, "avg_logprob": -0.14116968267104205, "compression_ratio": 1.8512820512820514, "no_speech_prob": 0.019092541188001633}, {"id": 118, "seek": 74654, "start": 747.18, "end": 754.18, "text": " to the parameters. Great, we have time. I will go into details now. Please bear with", "tokens": [50396, 281, 264, 9834, 13, 3769, 11, 321, 362, 565, 13, 286, 486, 352, 666, 4365, 586, 13, 2555, 6155, 365, 50746], "temperature": 0.0, "avg_logprob": -0.19476984528934255, "compression_ratio": 1.4915254237288136, "no_speech_prob": 0.002431831555441022}, {"id": 119, "seek": 74654, "start": 755.9399999999999, "end": 761.98, "text": " me. Okay, let's first look into the parameters. How do we determine the regression model's", "tokens": [50834, 385, 13, 1033, 11, 718, 311, 700, 574, 666, 264, 9834, 13, 1012, 360, 321, 6997, 264, 24590, 2316, 311, 51136], "temperature": 0.0, "avg_logprob": -0.19476984528934255, "compression_ratio": 1.4915254237288136, "no_speech_prob": 0.002431831555441022}, {"id": 120, "seek": 74654, "start": 761.98, "end": 768.98, "text": " parameters? There's an algorithm for this. First of all, we turn off everything. We turn", "tokens": [51136, 9834, 30, 821, 311, 364, 9284, 337, 341, 13, 2386, 295, 439, 11, 321, 1261, 766, 1203, 13, 492, 1261, 51486], "temperature": 0.0, "avg_logprob": -0.19476984528934255, "compression_ratio": 1.4915254237288136, "no_speech_prob": 0.002431831555441022}, {"id": 121, "seek": 76898, "start": 769.66, "end": 776.66, "text": " off everything that we can turn off in the system. We measure the baseline draw. This", "tokens": [50398, 766, 1203, 300, 321, 393, 1261, 766, 294, 264, 1185, 13, 492, 3481, 264, 20518, 2642, 13, 639, 50748], "temperature": 0.0, "avg_logprob": -0.16006493013958598, "compression_ratio": 1.7178217821782178, "no_speech_prob": 0.04528426006436348}, {"id": 122, "seek": 76898, "start": 777.58, "end": 783.3000000000001, "text": " is what we refer to as the minimizing the system load. Then we pick each device one", "tokens": [50794, 307, 437, 321, 2864, 281, 382, 264, 46608, 264, 1185, 3677, 13, 1396, 321, 1888, 1184, 4302, 472, 51080], "temperature": 0.0, "avg_logprob": -0.16006493013958598, "compression_ratio": 1.7178217821782178, "no_speech_prob": 0.04528426006436348}, {"id": 123, "seek": 76898, "start": 783.3000000000001, "end": 790.3000000000001, "text": " by one. We isolate the impact of the device on the baseline load. And we measure the", "tokens": [51080, 538, 472, 13, 492, 25660, 264, 2712, 295, 264, 4302, 322, 264, 20518, 3677, 13, 400, 321, 3481, 264, 51430], "temperature": 0.0, "avg_logprob": -0.16006493013958598, "compression_ratio": 1.7178217821782178, "no_speech_prob": 0.04528426006436348}, {"id": 124, "seek": 76898, "start": 790.46, "end": 797.38, "text": " drain over multiple times. So we turn on a single time. Let's say that I turn off everything", "tokens": [51438, 12339, 670, 3866, 1413, 13, 407, 321, 1261, 322, 257, 2167, 565, 13, 961, 311, 584, 300, 286, 1261, 766, 1203, 51784], "temperature": 0.0, "avg_logprob": -0.16006493013958598, "compression_ratio": 1.7178217821782178, "no_speech_prob": 0.04528426006436348}, {"id": 125, "seek": 79738, "start": 797.38, "end": 801.7, "text": " and then I turn on just the screen. Okay? And I measure the difference between these", "tokens": [50364, 293, 550, 286, 1261, 322, 445, 264, 2568, 13, 1033, 30, 400, 286, 3481, 264, 2649, 1296, 613, 50580], "temperature": 0.0, "avg_logprob": -0.12641289234161376, "compression_ratio": 1.74, "no_speech_prob": 0.002280383137986064}, {"id": 126, "seek": 79738, "start": 801.7, "end": 808.7, "text": " two values. The difference is the impact of the screen on my baseline. And then I also", "tokens": [50580, 732, 4190, 13, 440, 2649, 307, 264, 2712, 295, 264, 2568, 322, 452, 20518, 13, 400, 550, 286, 611, 50930], "temperature": 0.0, "avg_logprob": -0.12641289234161376, "compression_ratio": 1.74, "no_speech_prob": 0.002280383137986064}, {"id": 127, "seek": 79738, "start": 810.14, "end": 817.14, "text": " do one thing. I sweep the screen. So I change the brightness of the screen from minimum", "tokens": [51002, 360, 472, 551, 13, 286, 22169, 264, 2568, 13, 407, 286, 1319, 264, 21367, 295, 264, 2568, 490, 7285, 51352], "temperature": 0.0, "avg_logprob": -0.12641289234161376, "compression_ratio": 1.74, "no_speech_prob": 0.002280383137986064}, {"id": 128, "seek": 79738, "start": 817.38, "end": 822.06, "text": " to the maximum because obviously the minimum brightness is going to have different power", "tokens": [51364, 281, 264, 6674, 570, 2745, 264, 7285, 21367, 307, 516, 281, 362, 819, 1347, 51598], "temperature": 0.0, "avg_logprob": -0.12641289234161376, "compression_ratio": 1.74, "no_speech_prob": 0.002280383137986064}, {"id": 129, "seek": 82206, "start": 822.06, "end": 825.9399999999999, "text": " than the maximum brightness, right? I hope this makes sense to me. Are you guys still", "tokens": [50364, 813, 264, 6674, 21367, 11, 558, 30, 286, 1454, 341, 1669, 2020, 281, 385, 13, 2014, 291, 1074, 920, 50558], "temperature": 0.0, "avg_logprob": -0.13302916222876246, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.0031212822068482637}, {"id": 130, "seek": 82206, "start": 825.9399999999999, "end": 832.06, "text": " with me? Okay, so this was just an example. But what we're trying to do is we're trying", "tokens": [50558, 365, 385, 30, 1033, 11, 370, 341, 390, 445, 364, 1365, 13, 583, 437, 321, 434, 1382, 281, 360, 307, 321, 434, 1382, 50864], "temperature": 0.0, "avg_logprob": -0.13302916222876246, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.0031212822068482637}, {"id": 131, "seek": 82206, "start": 832.06, "end": 839.06, "text": " to quantify the impact of each device on the baseline. Now, I would love to give a metaphor", "tokens": [50864, 281, 40421, 264, 2712, 295, 1184, 4302, 322, 264, 20518, 13, 823, 11, 286, 576, 959, 281, 976, 257, 19157, 51214], "temperature": 0.0, "avg_logprob": -0.13302916222876246, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.0031212822068482637}, {"id": 132, "seek": 82206, "start": 839.2199999999999, "end": 846.2199999999999, "text": " to help explain this better. Imagine that you have a water tank. And in this water tank", "tokens": [51222, 281, 854, 2903, 341, 1101, 13, 11739, 300, 291, 362, 257, 1281, 5466, 13, 400, 294, 341, 1281, 5466, 51572], "temperature": 0.0, "avg_logprob": -0.13302916222876246, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.0031212822068482637}, {"id": 133, "seek": 84622, "start": 847.22, "end": 853.22, "text": " there's one single input and there are 10,000 tiny outputs. And the problem that you're", "tokens": [50414, 456, 311, 472, 2167, 4846, 293, 456, 366, 1266, 11, 1360, 5870, 23930, 13, 400, 264, 1154, 300, 291, 434, 50714], "temperature": 0.0, "avg_logprob": -0.17226205305619674, "compression_ratio": 1.86864406779661, "no_speech_prob": 0.003025469835847616}, {"id": 134, "seek": 84622, "start": 853.58, "end": 860.58, "text": " trying to solve is what is the rate for each of the output pipes? You cannot measure it", "tokens": [50732, 1382, 281, 5039, 307, 437, 307, 264, 3314, 337, 1184, 295, 264, 5598, 21882, 30, 509, 2644, 3481, 309, 51082], "temperature": 0.0, "avg_logprob": -0.17226205305619674, "compression_ratio": 1.86864406779661, "no_speech_prob": 0.003025469835847616}, {"id": 135, "seek": 84622, "start": 862.78, "end": 867.14, "text": " directly. So you have these 10,000 outputs which go on their own anytime. They can go", "tokens": [51192, 3838, 13, 407, 291, 362, 613, 1266, 11, 1360, 23930, 597, 352, 322, 641, 1065, 13038, 13, 814, 393, 352, 51410], "temperature": 0.0, "avg_logprob": -0.17226205305619674, "compression_ratio": 1.86864406779661, "no_speech_prob": 0.003025469835847616}, {"id": 136, "seek": 84622, "start": 867.14, "end": 871.6600000000001, "text": " off and you don't have levers to control them. What you're trying to figure out is what is", "tokens": [51410, 766, 293, 291, 500, 380, 362, 45571, 281, 1969, 552, 13, 708, 291, 434, 1382, 281, 2573, 484, 307, 437, 307, 51636], "temperature": 0.0, "avg_logprob": -0.17226205305619674, "compression_ratio": 1.86864406779661, "no_speech_prob": 0.003025469835847616}, {"id": 137, "seek": 84622, "start": 871.6600000000001, "end": 875.5400000000001, "text": " the drain rate for each of the output pipes? That is essentially the problem that you're", "tokens": [51636, 264, 12339, 3314, 337, 1184, 295, 264, 5598, 21882, 30, 663, 307, 4476, 264, 1154, 300, 291, 434, 51830], "temperature": 0.0, "avg_logprob": -0.17226205305619674, "compression_ratio": 1.86864406779661, "no_speech_prob": 0.003025469835847616}, {"id": 138, "seek": 87554, "start": 875.54, "end": 881.38, "text": " trying to solve. So what you do is you turn off all the outputs, okay? You turn off all", "tokens": [50364, 1382, 281, 5039, 13, 407, 437, 291, 360, 307, 291, 1261, 766, 439, 264, 23930, 11, 1392, 30, 509, 1261, 766, 439, 50656], "temperature": 0.0, "avg_logprob": -0.15222881580221242, "compression_ratio": 1.7053140096618358, "no_speech_prob": 0.0015234444290399551}, {"id": 139, "seek": 87554, "start": 881.38, "end": 888.38, "text": " the outputs and you turn on one single output and then you see the difference in the tank", "tokens": [50656, 264, 23930, 293, 291, 1261, 322, 472, 2167, 5598, 293, 550, 291, 536, 264, 2649, 294, 264, 5466, 51006], "temperature": 0.0, "avg_logprob": -0.15222881580221242, "compression_ratio": 1.7053140096618358, "no_speech_prob": 0.0015234444290399551}, {"id": 140, "seek": 87554, "start": 888.62, "end": 895.62, "text": " level before and after turning it off, okay? And that is essentially what we call as an", "tokens": [51018, 1496, 949, 293, 934, 6246, 309, 766, 11, 1392, 30, 400, 300, 307, 4476, 437, 321, 818, 382, 364, 51368], "temperature": 0.0, "avg_logprob": -0.15222881580221242, "compression_ratio": 1.7053140096618358, "no_speech_prob": 0.0015234444290399551}, {"id": 141, "seek": 87554, "start": 896.02, "end": 902.18, "text": " isolation or well in academic terms it's also sometimes known as an ablation study. But", "tokens": [51388, 16001, 420, 731, 294, 7778, 2115, 309, 311, 611, 2171, 2570, 382, 364, 410, 24278, 2979, 13, 583, 51696], "temperature": 0.0, "avg_logprob": -0.15222881580221242, "compression_ratio": 1.7053140096618358, "no_speech_prob": 0.0015234444290399551}, {"id": 142, "seek": 90218, "start": 902.18, "end": 908.8199999999999, "text": " we try to isolate the device and measure the impact. Next, we repeat this process for all", "tokens": [50364, 321, 853, 281, 25660, 264, 4302, 293, 3481, 264, 2712, 13, 3087, 11, 321, 7149, 341, 1399, 337, 439, 50696], "temperature": 0.0, "avg_logprob": -0.12579450046314913, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.0006357933743856847}, {"id": 143, "seek": 90218, "start": 908.8199999999999, "end": 915.8199999999999, "text": " the pipes in the system and we try to get a reasonable estimate of what is the impact", "tokens": [50696, 264, 21882, 294, 264, 1185, 293, 321, 853, 281, 483, 257, 10585, 12539, 295, 437, 307, 264, 2712, 51046], "temperature": 0.0, "avg_logprob": -0.12579450046314913, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.0006357933743856847}, {"id": 144, "seek": 90218, "start": 915.8199999999999, "end": 922.3399999999999, "text": " of each pipe. Great. So that was the first step, the device-specific measurements. The", "tokens": [51046, 295, 1184, 11240, 13, 3769, 13, 407, 300, 390, 264, 700, 1823, 11, 264, 4302, 12, 29258, 15383, 13, 440, 51372], "temperature": 0.0, "avg_logprob": -0.12579450046314913, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.0006357933743856847}, {"id": 145, "seek": 90218, "start": 922.3399999999999, "end": 928.3399999999999, "text": " second step would be the kernel process accounting step. This would be the inputs to the regression", "tokens": [51372, 1150, 1823, 576, 312, 264, 28256, 1399, 19163, 1823, 13, 639, 576, 312, 264, 15743, 281, 264, 24590, 51672], "temperature": 0.0, "avg_logprob": -0.12579450046314913, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.0006357933743856847}, {"id": 146, "seek": 92834, "start": 928.34, "end": 934.7, "text": " model. So we have the parameters that we got from this step and now we need the inputs.", "tokens": [50364, 2316, 13, 407, 321, 362, 264, 9834, 300, 321, 658, 490, 341, 1823, 293, 586, 321, 643, 264, 15743, 13, 50682], "temperature": 0.0, "avg_logprob": -0.13362790213690864, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.004825328942388296}, {"id": 147, "seek": 92834, "start": 934.7, "end": 941.7, "text": " Now how do we determine these inputs? Sorry, did I hear a question? Okay, great. Right,", "tokens": [50682, 823, 577, 360, 321, 6997, 613, 15743, 30, 4919, 11, 630, 286, 1568, 257, 1168, 30, 1033, 11, 869, 13, 1779, 11, 51032], "temperature": 0.0, "avg_logprob": -0.13362790213690864, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.004825328942388296}, {"id": 148, "seek": 92834, "start": 942.5400000000001, "end": 946.98, "text": " how do we determine the inputs? What we do is we isolate the impact of each process.", "tokens": [51074, 577, 360, 321, 6997, 264, 15743, 30, 708, 321, 360, 307, 321, 25660, 264, 2712, 295, 1184, 1399, 13, 51296], "temperature": 0.0, "avg_logprob": -0.13362790213690864, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.004825328942388296}, {"id": 149, "seek": 92834, "start": 946.98, "end": 953.98, "text": " So we identify how much time the process used for the CPU. We identify how much was the", "tokens": [51296, 407, 321, 5876, 577, 709, 565, 264, 1399, 1143, 337, 264, 13199, 13, 492, 5876, 577, 709, 390, 264, 51646], "temperature": 0.0, "avg_logprob": -0.13362790213690864, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.004825328942388296}, {"id": 150, "seek": 95398, "start": 953.98, "end": 960.98, "text": " network activity, the screen wakeups, file handles, memory usage and we put all of these", "tokens": [50364, 3209, 5191, 11, 264, 2568, 6634, 7528, 11, 3991, 18722, 11, 4675, 14924, 293, 321, 829, 439, 295, 613, 50714], "temperature": 0.0, "avg_logprob": -0.15679552388745685, "compression_ratio": 1.5367965367965368, "no_speech_prob": 0.002247667172923684}, {"id": 151, "seek": 95398, "start": 962.1, "end": 969.1, "text": " numbers together into the model. And this is what gives us a predicted energy consumption", "tokens": [50770, 3547, 1214, 666, 264, 2316, 13, 400, 341, 307, 437, 2709, 505, 257, 19147, 2281, 12126, 51120], "temperature": 0.0, "avg_logprob": -0.15679552388745685, "compression_ratio": 1.5367965367965368, "no_speech_prob": 0.002247667172923684}, {"id": 152, "seek": 95398, "start": 969.54, "end": 976.54, "text": " value for that process. Okay, so what are the challenges? This seems very simple. This", "tokens": [51142, 2158, 337, 300, 1399, 13, 1033, 11, 370, 437, 366, 264, 4759, 30, 639, 2544, 588, 2199, 13, 639, 51492], "temperature": 0.0, "avg_logprob": -0.15679552388745685, "compression_ratio": 1.5367965367965368, "no_speech_prob": 0.002247667172923684}, {"id": 153, "seek": 95398, "start": 977.54, "end": 982.7, "text": " seems, okay, you've done this work but what did you not tell us? Here comes the part that", "tokens": [51542, 2544, 11, 1392, 11, 291, 600, 1096, 341, 589, 457, 437, 630, 291, 406, 980, 505, 30, 1692, 1487, 264, 644, 300, 51800], "temperature": 0.0, "avg_logprob": -0.15679552388745685, "compression_ratio": 1.5367965367965368, "no_speech_prob": 0.002247667172923684}, {"id": 154, "seek": 98270, "start": 982.7, "end": 989.7, "text": " I did not tell you. First part, estimated value. This is not the reality. It is really", "tokens": [50364, 286, 630, 406, 980, 291, 13, 2386, 644, 11, 14109, 2158, 13, 639, 307, 406, 264, 4103, 13, 467, 307, 534, 50714], "temperature": 0.0, "avg_logprob": -0.13110608733102178, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.0018365547293797135}, {"id": 155, "seek": 98270, "start": 990.38, "end": 995.86, "text": " hard to find out the reality. And there's a very famous line in machine learning community.", "tokens": [50748, 1152, 281, 915, 484, 264, 4103, 13, 400, 456, 311, 257, 588, 4618, 1622, 294, 3479, 2539, 1768, 13, 51022], "temperature": 0.0, "avg_logprob": -0.13110608733102178, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.0018365547293797135}, {"id": 156, "seek": 98270, "start": 995.86, "end": 1001.3000000000001, "text": " It's known as all models are wrong but some are useful. So my goal here is to build a", "tokens": [51022, 467, 311, 2570, 382, 439, 5245, 366, 2085, 457, 512, 366, 4420, 13, 407, 452, 3387, 510, 307, 281, 1322, 257, 51294], "temperature": 0.0, "avg_logprob": -0.13110608733102178, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.0018365547293797135}, {"id": 157, "seek": 98270, "start": 1001.3000000000001, "end": 1006.82, "text": " useful model that I hope is less wrong. I would love to make it perfect but unfortunately", "tokens": [51294, 4420, 2316, 300, 286, 1454, 307, 1570, 2085, 13, 286, 576, 959, 281, 652, 309, 2176, 457, 7015, 51570], "temperature": 0.0, "avg_logprob": -0.13110608733102178, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.0018365547293797135}, {"id": 158, "seek": 100682, "start": 1006.86, "end": 1013.0200000000001, "text": " we cannot make it perfect. But yes, I would love to make a useful model first. Second,", "tokens": [50366, 321, 2644, 652, 309, 2176, 13, 583, 2086, 11, 286, 576, 959, 281, 652, 257, 4420, 2316, 700, 13, 5736, 11, 50674], "temperature": 0.0, "avg_logprob": -0.1399985385197465, "compression_ratio": 1.6, "no_speech_prob": 0.007116237655282021}, {"id": 159, "seek": 100682, "start": 1013.0200000000001, "end": 1019.7800000000001, "text": " there's a bit of a cash 22 situation here if you observe that. What is the cash 22?", "tokens": [50674, 456, 311, 257, 857, 295, 257, 6388, 5853, 2590, 510, 498, 291, 11441, 300, 13, 708, 307, 264, 6388, 5853, 30, 51012], "temperature": 0.0, "avg_logprob": -0.1399985385197465, "compression_ratio": 1.6, "no_speech_prob": 0.007116237655282021}, {"id": 160, "seek": 100682, "start": 1019.7800000000001, "end": 1024.38, "text": " I am running a measurement process. There's a process that is doing measurement on my", "tokens": [51012, 286, 669, 2614, 257, 13160, 1399, 13, 821, 311, 257, 1399, 300, 307, 884, 13160, 322, 452, 51242], "temperature": 0.0, "avg_logprob": -0.1399985385197465, "compression_ratio": 1.6, "no_speech_prob": 0.007116237655282021}, {"id": 161, "seek": 100682, "start": 1024.38, "end": 1031.38, "text": " system. Okay, that is also going to create a load. So there's going to be a skew in the", "tokens": [51242, 1185, 13, 1033, 11, 300, 307, 611, 516, 281, 1884, 257, 3677, 13, 407, 456, 311, 516, 281, 312, 257, 8756, 86, 294, 264, 51592], "temperature": 0.0, "avg_logprob": -0.1399985385197465, "compression_ratio": 1.6, "no_speech_prob": 0.007116237655282021}, {"id": 162, "seek": 103138, "start": 1032.2600000000002, "end": 1039.2600000000002, "text": " values that I get because of my measurement. Okay, and the more accurate I want it to be,", "tokens": [50408, 4190, 300, 286, 483, 570, 295, 452, 13160, 13, 1033, 11, 293, 264, 544, 8559, 286, 528, 309, 281, 312, 11, 50758], "temperature": 0.0, "avg_logprob": -0.13917305890251608, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.004454342648386955}, {"id": 163, "seek": 103138, "start": 1041.3000000000002, "end": 1048.14, "text": " the more skew it is going to create. So we want to understand what is the right amount", "tokens": [50860, 264, 544, 8756, 86, 309, 307, 516, 281, 1884, 13, 407, 321, 528, 281, 1223, 437, 307, 264, 558, 2372, 51202], "temperature": 0.0, "avg_logprob": -0.13917305890251608, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.004454342648386955}, {"id": 164, "seek": 103138, "start": 1048.14, "end": 1055.14, "text": " of accuracy that we can use to also be useful while also minimizing the bias. So this is", "tokens": [51202, 295, 14170, 300, 321, 393, 764, 281, 611, 312, 4420, 1339, 611, 46608, 264, 12577, 13, 407, 341, 307, 51552], "temperature": 0.0, "avg_logprob": -0.13917305890251608, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.004454342648386955}, {"id": 165, "seek": 105514, "start": 1055.38, "end": 1062.38, "text": " very challenging, right? Because this is different for every system. And that's a problem that", "tokens": [50376, 588, 7595, 11, 558, 30, 1436, 341, 307, 819, 337, 633, 1185, 13, 400, 300, 311, 257, 1154, 300, 50726], "temperature": 0.0, "avg_logprob": -0.2121975855393843, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.005210593342781067}, {"id": 166, "seek": 105514, "start": 1063.38, "end": 1070.38, "text": " I'm almost struggling to solve. I would love to get your inputs if you have. Great, next", "tokens": [50776, 286, 478, 1920, 9314, 281, 5039, 13, 286, 576, 959, 281, 483, 428, 15743, 498, 291, 362, 13, 3769, 11, 958, 51126], "temperature": 0.0, "avg_logprob": -0.2121975855393843, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.005210593342781067}, {"id": 167, "seek": 105514, "start": 1070.38, "end": 1076.0600000000002, "text": " challenge. There are millions of devices out there and these millions of devices have billions", "tokens": [51126, 3430, 13, 821, 366, 6803, 295, 5759, 484, 456, 293, 613, 6803, 295, 5759, 362, 17375, 51410], "temperature": 0.0, "avg_logprob": -0.2121975855393843, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.005210593342781067}, {"id": 168, "seek": 105514, "start": 1076.0600000000002, "end": 1082.5, "text": " of ICs inside them. Very often we don't even have the data sheets for these ICs to correlate", "tokens": [51410, 295, 14360, 82, 1854, 552, 13, 4372, 2049, 321, 500, 380, 754, 362, 264, 1412, 15421, 337, 613, 14360, 82, 281, 48742, 51732], "temperature": 0.0, "avg_logprob": -0.2121975855393843, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.005210593342781067}, {"id": 169, "seek": 108250, "start": 1082.5, "end": 1087.7, "text": " the values that we see. The estimates that we get can range across two to three orders", "tokens": [50364, 264, 4190, 300, 321, 536, 13, 440, 20561, 300, 321, 483, 393, 3613, 2108, 732, 281, 1045, 9470, 50624], "temperature": 0.0, "avg_logprob": -0.12362803910907946, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0031708902679383755}, {"id": 170, "seek": 108250, "start": 1087.7, "end": 1092.5, "text": " of magnitude. One device can say, oh, I use one microjoule and the second one can say,", "tokens": [50624, 295, 15668, 13, 1485, 4302, 393, 584, 11, 1954, 11, 286, 764, 472, 4532, 73, 26757, 293, 264, 1150, 472, 393, 584, 11, 50864], "temperature": 0.0, "avg_logprob": -0.12362803910907946, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0031708902679383755}, {"id": 171, "seek": 108250, "start": 1092.5, "end": 1096.54, "text": " oh, I use 10 milliwatts and those numbers don't make sense. Those numbers really blow", "tokens": [50864, 1954, 11, 286, 764, 1266, 26176, 44824, 1373, 293, 729, 3547, 500, 380, 652, 2020, 13, 3950, 3547, 534, 6327, 51066], "temperature": 0.0, "avg_logprob": -0.12362803910907946, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0031708902679383755}, {"id": 172, "seek": 108250, "start": 1096.54, "end": 1103.54, "text": " you away. So how do we maintain our sanity in the face of the variance that we see here?", "tokens": [51066, 291, 1314, 13, 407, 577, 360, 321, 6909, 527, 47892, 294, 264, 1851, 295, 264, 21977, 300, 321, 536, 510, 30, 51416], "temperature": 0.0, "avg_logprob": -0.12362803910907946, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0031708902679383755}, {"id": 173, "seek": 108250, "start": 1105.74, "end": 1111.14, "text": " And one more challenge would be that, assume that you can say, oh, let the users supply", "tokens": [51526, 400, 472, 544, 3430, 576, 312, 300, 11, 6552, 300, 291, 393, 584, 11, 1954, 11, 718, 264, 5022, 5847, 51796], "temperature": 0.0, "avg_logprob": -0.12362803910907946, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0031708902679383755}, {"id": 174, "seek": 111114, "start": 1111.22, "end": 1117.6200000000001, "text": " this data, let me get the data and then build a centralized farm of this data and then try", "tokens": [50368, 341, 1412, 11, 718, 385, 483, 264, 1412, 293, 550, 1322, 257, 32395, 5421, 295, 341, 1412, 293, 550, 853, 50688], "temperature": 0.0, "avg_logprob": -0.1178906758626302, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0035351805854588747}, {"id": 175, "seek": 111114, "start": 1117.6200000000001, "end": 1124.6200000000001, "text": " to make sense of it. Should the users share this data? Would the users share their device", "tokens": [50688, 281, 652, 2020, 295, 309, 13, 6454, 264, 5022, 2073, 341, 1412, 30, 6068, 264, 5022, 2073, 641, 4302, 51038], "temperature": 0.0, "avg_logprob": -0.1178906758626302, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0035351805854588747}, {"id": 176, "seek": 111114, "start": 1125.5400000000002, "end": 1130.8600000000001, "text": " users' data to you and allow you to put it on a centralized server? Who will own that", "tokens": [51084, 5022, 6, 1412, 281, 291, 293, 2089, 291, 281, 829, 309, 322, 257, 32395, 7154, 30, 2102, 486, 1065, 300, 51350], "temperature": 0.0, "avg_logprob": -0.1178906758626302, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0035351805854588747}, {"id": 177, "seek": 111114, "start": 1130.8600000000001, "end": 1137.8600000000001, "text": " data? Because there's enormous value in it. So this is, I would love to get your inputs", "tokens": [51350, 1412, 30, 1436, 456, 311, 11322, 2158, 294, 309, 13, 407, 341, 307, 11, 286, 576, 959, 281, 483, 428, 15743, 51700], "temperature": 0.0, "avg_logprob": -0.1178906758626302, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0035351805854588747}, {"id": 178, "seek": 113786, "start": 1138.02, "end": 1145.02, "text": " on. One more challenge here would be the validation. So we got a value that we estimated. How do", "tokens": [50372, 322, 13, 1485, 544, 3430, 510, 576, 312, 264, 24071, 13, 407, 321, 658, 257, 2158, 300, 321, 14109, 13, 1012, 360, 50722], "temperature": 0.0, "avg_logprob": -0.12800453106562296, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.0041292160749435425}, {"id": 179, "seek": 113786, "start": 1149.02, "end": 1155.34, "text": " we make sure this value is as close as possible to the ground truth? In an ideal world, I", "tokens": [50922, 321, 652, 988, 341, 2158, 307, 382, 1998, 382, 1944, 281, 264, 2727, 3494, 30, 682, 364, 7157, 1002, 11, 286, 51238], "temperature": 0.0, "avg_logprob": -0.12800453106562296, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.0041292160749435425}, {"id": 180, "seek": 113786, "start": 1155.34, "end": 1160.3799999999999, "text": " would have infinite money and I would go to every computer in this world and take a probe", "tokens": [51238, 576, 362, 13785, 1460, 293, 286, 576, 352, 281, 633, 3820, 294, 341, 1002, 293, 747, 257, 22715, 51490], "temperature": 0.0, "avg_logprob": -0.12800453106562296, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.0041292160749435425}, {"id": 181, "seek": 113786, "start": 1160.3799999999999, "end": 1167.1799999999998, "text": " and put it next to their CPU and say, oh, this says 17.5 watts and my tool says 17.5", "tokens": [51490, 293, 829, 309, 958, 281, 641, 13199, 293, 584, 11, 1954, 11, 341, 1619, 3282, 13, 20, 31247, 293, 452, 2290, 1619, 3282, 13, 20, 51830], "temperature": 0.0, "avg_logprob": -0.12800453106562296, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.0041292160749435425}, {"id": 182, "seek": 116718, "start": 1167.18, "end": 1173.3, "text": " watts. Great job. Let's go. I cannot do that because I don't have that much time. Okay,", "tokens": [50364, 31247, 13, 3769, 1691, 13, 961, 311, 352, 13, 286, 2644, 360, 300, 570, 286, 500, 380, 362, 300, 709, 565, 13, 1033, 11, 50670], "temperature": 0.0, "avg_logprob": -0.17684151331583658, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.0018654793966561556}, {"id": 183, "seek": 116718, "start": 1173.3, "end": 1180.3, "text": " so we want to minimize the difference from the ground truth and what we see in the tool.", "tokens": [50670, 370, 321, 528, 281, 17522, 264, 2649, 490, 264, 2727, 3494, 293, 437, 321, 536, 294, 264, 2290, 13, 51020], "temperature": 0.0, "avg_logprob": -0.17684151331583658, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.0018654793966561556}, {"id": 184, "seek": 116718, "start": 1183.0600000000002, "end": 1189.0600000000002, "text": " There's a significant challenge in making sure that what we see is what is the reality.", "tokens": [51158, 821, 311, 257, 4776, 3430, 294, 1455, 988, 300, 437, 321, 536, 307, 437, 307, 264, 4103, 13, 51458], "temperature": 0.0, "avg_logprob": -0.17684151331583658, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.0018654793966561556}, {"id": 185, "seek": 116718, "start": 1189.0600000000002, "end": 1194.66, "text": " Right? Remember, there's accuracy and there's precision and there's correctness. And all", "tokens": [51458, 1779, 30, 5459, 11, 456, 311, 14170, 293, 456, 311, 18356, 293, 456, 311, 3006, 1287, 13, 400, 439, 51738], "temperature": 0.0, "avg_logprob": -0.17684151331583658, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.0018654793966561556}, {"id": 186, "seek": 119466, "start": 1194.68, "end": 1201.68, "text": " of these trifecta come together and make this a very difficult tool to get right. But still,", "tokens": [50365, 295, 613, 36956, 557, 64, 808, 1214, 293, 652, 341, 257, 588, 2252, 2290, 281, 483, 558, 13, 583, 920, 11, 50715], "temperature": 0.0, "avg_logprob": -0.1507684992647719, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.001302905147895217}, {"id": 187, "seek": 119466, "start": 1202.5400000000002, "end": 1208.46, "text": " I believe it's going to be great. I'm very happy to work on it. Great. So once we have", "tokens": [50758, 286, 1697, 309, 311, 516, 281, 312, 869, 13, 286, 478, 588, 2055, 281, 589, 322, 309, 13, 3769, 13, 407, 1564, 321, 362, 51054], "temperature": 0.0, "avg_logprob": -0.1507684992647719, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.001302905147895217}, {"id": 188, "seek": 119466, "start": 1208.46, "end": 1214.3000000000002, "text": " the energy consumption, how do we link it to the carbon emissions? We just saw that", "tokens": [51054, 264, 2281, 12126, 11, 577, 360, 321, 2113, 309, 281, 264, 5954, 14607, 30, 492, 445, 1866, 300, 51346], "temperature": 0.0, "avg_logprob": -0.1507684992647719, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.001302905147895217}, {"id": 189, "seek": 119466, "start": 1214.3000000000002, "end": 1221.3000000000002, "text": " we can calculate energy consumption using power time flatancy. The carbon footprint", "tokens": [51346, 321, 393, 8873, 2281, 12126, 1228, 1347, 565, 4962, 6717, 13, 440, 5954, 24222, 51696], "temperature": 0.0, "avg_logprob": -0.1507684992647719, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.001302905147895217}, {"id": 190, "seek": 122130, "start": 1221.3, "end": 1226.46, "text": " can be calculated by multiplying this number by the composition of the energy. Where did", "tokens": [50364, 393, 312, 15598, 538, 30955, 341, 1230, 538, 264, 12686, 295, 264, 2281, 13, 2305, 630, 50622], "temperature": 0.0, "avg_logprob": -0.11357184251149495, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.0008820677176117897}, {"id": 191, "seek": 122130, "start": 1226.46, "end": 1232.34, "text": " the energy come from that you used to power the device that you were running? And this", "tokens": [50622, 264, 2281, 808, 490, 300, 291, 1143, 281, 1347, 264, 4302, 300, 291, 645, 2614, 30, 400, 341, 50916], "temperature": 0.0, "avg_logprob": -0.11357184251149495, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.0008820677176117897}, {"id": 192, "seek": 122130, "start": 1232.34, "end": 1237.74, "text": " composition depends on multiple factors. It can include the geography. It can include", "tokens": [50916, 12686, 5946, 322, 3866, 6771, 13, 467, 393, 4090, 264, 26695, 13, 467, 393, 4090, 51186], "temperature": 0.0, "avg_logprob": -0.11357184251149495, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.0008820677176117897}, {"id": 193, "seek": 122130, "start": 1237.74, "end": 1243.98, "text": " the time of availability. It can include the cost of generation of that energy. Right?", "tokens": [51186, 264, 565, 295, 17945, 13, 467, 393, 4090, 264, 2063, 295, 5125, 295, 300, 2281, 13, 1779, 30, 51498], "temperature": 0.0, "avg_logprob": -0.11357184251149495, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.0008820677176117897}, {"id": 194, "seek": 122130, "start": 1243.98, "end": 1248.7, "text": " So fortunately, there are good tools and libraries out there which can simplify this problem for", "tokens": [51498, 407, 25511, 11, 456, 366, 665, 3873, 293, 15148, 484, 456, 597, 393, 20460, 341, 1154, 337, 51734], "temperature": 0.0, "avg_logprob": -0.11357184251149495, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.0008820677176117897}, {"id": 195, "seek": 124870, "start": 1248.7, "end": 1254.66, "text": " you. So energy composition is, let's say, something that I believe people will solve", "tokens": [50364, 291, 13, 407, 2281, 12686, 307, 11, 718, 311, 584, 11, 746, 300, 286, 1697, 561, 486, 5039, 50662], "temperature": 0.0, "avg_logprob": -0.1229869240208676, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.004535824526101351}, {"id": 196, "seek": 124870, "start": 1254.66, "end": 1261.66, "text": " faster than I can solve this one. That is why I would love to focus on this one. Great.", "tokens": [50662, 4663, 813, 286, 393, 5039, 341, 472, 13, 663, 307, 983, 286, 576, 959, 281, 1879, 322, 341, 472, 13, 3769, 13, 51012], "temperature": 0.0, "avg_logprob": -0.1229869240208676, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.004535824526101351}, {"id": 197, "seek": 124870, "start": 1262.5, "end": 1267.5, "text": " All done. Let's get back to the good stuff. How is this going to look like? How is this", "tokens": [51054, 1057, 1096, 13, 961, 311, 483, 646, 281, 264, 665, 1507, 13, 1012, 307, 341, 516, 281, 574, 411, 30, 1012, 307, 341, 51304], "temperature": 0.0, "avg_logprob": -0.1229869240208676, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.004535824526101351}, {"id": 198, "seek": 124870, "start": 1267.5, "end": 1273.78, "text": " going to make your life better? If you're an end user, I would love to ship to you an", "tokens": [51304, 516, 281, 652, 428, 993, 1101, 30, 759, 291, 434, 364, 917, 4195, 11, 286, 576, 959, 281, 5374, 281, 291, 364, 51618], "temperature": 0.0, "avg_logprob": -0.1229869240208676, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.004535824526101351}, {"id": 199, "seek": 127378, "start": 1273.78, "end": 1280.78, "text": " application like this, an application which tells you how much energy your inkscape usage", "tokens": [50364, 3861, 411, 341, 11, 364, 3861, 597, 5112, 291, 577, 709, 2281, 428, 294, 1694, 4747, 14924, 50714], "temperature": 0.0, "avg_logprob": -0.15174325307210287, "compression_ratio": 1.6902985074626866, "no_speech_prob": 0.061750832945108414}, {"id": 200, "seek": 127378, "start": 1281.54, "end": 1288.34, "text": " consumed, how much energy your screen was dissipating. So as an end user, you can remember to turn", "tokens": [50752, 21226, 11, 577, 709, 2281, 428, 2568, 390, 29544, 990, 13, 407, 382, 364, 917, 4195, 11, 291, 393, 1604, 281, 1261, 51092], "temperature": 0.0, "avg_logprob": -0.15174325307210287, "compression_ratio": 1.6902985074626866, "no_speech_prob": 0.061750832945108414}, {"id": 201, "seek": 127378, "start": 1288.34, "end": 1292.62, "text": " off inkscape when you're not using it. Or you can figure out, oh, I need to deliver a", "tokens": [51092, 766, 294, 1694, 4747, 562, 291, 434, 406, 1228, 309, 13, 1610, 291, 393, 2573, 484, 11, 1954, 11, 286, 643, 281, 4239, 257, 51306], "temperature": 0.0, "avg_logprob": -0.15174325307210287, "compression_ratio": 1.6902985074626866, "no_speech_prob": 0.061750832945108414}, {"id": 202, "seek": 127378, "start": 1292.62, "end": 1297.8999999999999, "text": " presentation to so many people in five minutes. I'd better save my battery. Otherwise, I'll", "tokens": [51306, 5860, 281, 370, 867, 561, 294, 1732, 2077, 13, 286, 1116, 1101, 3155, 452, 5809, 13, 10328, 11, 286, 603, 51570], "temperature": 0.0, "avg_logprob": -0.15174325307210287, "compression_ratio": 1.6902985074626866, "no_speech_prob": 0.061750832945108414}, {"id": 203, "seek": 127378, "start": 1297.8999999999999, "end": 1303.3799999999999, "text": " be in deep trouble. So it's for those use cases when you want to maximize your battery", "tokens": [51570, 312, 294, 2452, 5253, 13, 407, 309, 311, 337, 729, 764, 3331, 562, 291, 528, 281, 19874, 428, 5809, 51844], "temperature": 0.0, "avg_logprob": -0.15174325307210287, "compression_ratio": 1.6902985074626866, "no_speech_prob": 0.061750832945108414}, {"id": 204, "seek": 130338, "start": 1303.38, "end": 1310.38, "text": " life as an end user. As a programmer, if you want to expose an API that enables programmers", "tokens": [50364, 993, 382, 364, 917, 4195, 13, 1018, 257, 32116, 11, 498, 291, 528, 281, 19219, 364, 9362, 300, 17077, 41504, 50714], "temperature": 0.0, "avg_logprob": -0.1330175843349723, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.0016215101350098848}, {"id": 205, "seek": 130338, "start": 1311.8200000000002, "end": 1318.2600000000002, "text": " to take action, if you want to indicate the devices and the code regions which consume", "tokens": [50786, 281, 747, 3069, 11, 498, 291, 528, 281, 13330, 264, 5759, 293, 264, 3089, 10682, 597, 14732, 51108], "temperature": 0.0, "avg_logprob": -0.1330175843349723, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.0016215101350098848}, {"id": 206, "seek": 130338, "start": 1318.2600000000002, "end": 1323.2600000000002, "text": " the maximum amount of power in the code and enable the programmers to change it, to modify", "tokens": [51108, 264, 6674, 2372, 295, 1347, 294, 264, 3089, 293, 9528, 264, 41504, 281, 1319, 309, 11, 281, 16927, 51358], "temperature": 0.0, "avg_logprob": -0.1330175843349723, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.0016215101350098848}, {"id": 207, "seek": 130338, "start": 1323.2600000000002, "end": 1330.2600000000002, "text": " it, to fix it. So actionability is the primary concern for programmers. In an ideal world,", "tokens": [51358, 309, 11, 281, 3191, 309, 13, 407, 3069, 2310, 307, 264, 6194, 3136, 337, 41504, 13, 682, 364, 7157, 1002, 11, 51708], "temperature": 0.0, "avg_logprob": -0.1330175843349723, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.0016215101350098848}, {"id": 208, "seek": 133026, "start": 1331.1, "end": 1336.9, "text": " I would love to have direct suggestions in the IDE that tell the programmer, oh, this", "tokens": [50406, 286, 576, 959, 281, 362, 2047, 13396, 294, 264, 40930, 300, 980, 264, 32116, 11, 1954, 11, 341, 50696], "temperature": 0.0, "avg_logprob": -0.133536109217891, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.005638152826577425}, {"id": 209, "seek": 133026, "start": 1336.9, "end": 1343.5, "text": " code is not, this code is going to burn this much carbon. You'd better change it. And for", "tokens": [50696, 3089, 307, 406, 11, 341, 3089, 307, 516, 281, 5064, 341, 709, 5954, 13, 509, 1116, 1101, 1319, 309, 13, 400, 337, 51026], "temperature": 0.0, "avg_logprob": -0.133536109217891, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.005638152826577425}, {"id": 210, "seek": 133026, "start": 1343.5, "end": 1347.9, "text": " the system designers, what we want to do is we want to enable them to iterate our designs", "tokens": [51026, 264, 1185, 16196, 11, 437, 321, 528, 281, 360, 307, 321, 528, 281, 9528, 552, 281, 44497, 527, 11347, 51246], "temperature": 0.0, "avg_logprob": -0.133536109217891, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.005638152826577425}, {"id": 211, "seek": 133026, "start": 1347.9, "end": 1353.86, "text": " faster. We want them to enable this, we want them, we want to enable system designers to", "tokens": [51246, 4663, 13, 492, 528, 552, 281, 9528, 341, 11, 321, 528, 552, 11, 321, 528, 281, 9528, 1185, 16196, 281, 51544], "temperature": 0.0, "avg_logprob": -0.133536109217891, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.005638152826577425}, {"id": 212, "seek": 133026, "start": 1353.86, "end": 1359.9, "text": " discover designs which are really low on energy, which are really high on performance, which", "tokens": [51544, 4411, 11347, 597, 366, 534, 2295, 322, 2281, 11, 597, 366, 534, 1090, 322, 3389, 11, 597, 51846], "temperature": 0.0, "avg_logprob": -0.133536109217891, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.005638152826577425}, {"id": 213, "seek": 135990, "start": 1359.94, "end": 1364.94, "text": " are really high on carbon efficiency. So there's typically a design space that designers explore.", "tokens": [50366, 366, 534, 1090, 322, 5954, 10493, 13, 407, 456, 311, 5850, 257, 1715, 1901, 300, 16196, 6839, 13, 50616], "temperature": 0.0, "avg_logprob": -0.1350759400261773, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.001453839591704309}, {"id": 214, "seek": 135990, "start": 1364.94, "end": 1369.6200000000001, "text": " And we want to enable them to explore the design space faster. That would be the end", "tokens": [50616, 400, 321, 528, 281, 9528, 552, 281, 6839, 264, 1715, 1901, 4663, 13, 663, 576, 312, 264, 917, 50850], "temperature": 0.0, "avg_logprob": -0.1350759400261773, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.001453839591704309}, {"id": 215, "seek": 135990, "start": 1369.6200000000001, "end": 1376.6200000000001, "text": " goal from this tool. Great. So what is the takeaway from this talk? If there's two things", "tokens": [50850, 3387, 490, 341, 2290, 13, 3769, 13, 407, 437, 307, 264, 30681, 490, 341, 751, 30, 759, 456, 311, 732, 721, 51200], "temperature": 0.0, "avg_logprob": -0.1350759400261773, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.001453839591704309}, {"id": 216, "seek": 135990, "start": 1378.3000000000002, "end": 1384.0600000000002, "text": " that I would love for you to take away, that if you forget everything else, okay, just", "tokens": [51284, 300, 286, 576, 959, 337, 291, 281, 747, 1314, 11, 300, 498, 291, 2870, 1203, 1646, 11, 1392, 11, 445, 51572], "temperature": 0.0, "avg_logprob": -0.1350759400261773, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.001453839591704309}, {"id": 217, "seek": 135990, "start": 1384.0600000000002, "end": 1388.94, "text": " remember these two things and I'd be very happy. First, we cannot improve what we cannot", "tokens": [51572, 1604, 613, 732, 721, 293, 286, 1116, 312, 588, 2055, 13, 2386, 11, 321, 2644, 3470, 437, 321, 2644, 51816], "temperature": 0.0, "avg_logprob": -0.1350759400261773, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.001453839591704309}, {"id": 218, "seek": 138894, "start": 1388.98, "end": 1395.98, "text": " measure. We must measure correctly, okay, to improve things. And second, we need to break", "tokens": [50366, 3481, 13, 492, 1633, 3481, 8944, 11, 1392, 11, 281, 3470, 721, 13, 400, 1150, 11, 321, 643, 281, 1821, 50716], "temperature": 0.0, "avg_logprob": -0.13672422369321188, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.006675093434751034}, {"id": 219, "seek": 138894, "start": 1396.14, "end": 1402.42, "text": " out of the CPU mindset, okay, non-CPU system components can dominate your power. Please", "tokens": [50724, 484, 295, 264, 13199, 12543, 11, 1392, 11, 2107, 12, 34, 8115, 1185, 6677, 393, 28246, 428, 1347, 13, 2555, 51038], "temperature": 0.0, "avg_logprob": -0.13672422369321188, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.006675093434751034}, {"id": 220, "seek": 138894, "start": 1402.42, "end": 1406.3400000000001, "text": " remember that. Please remember these two things. And the next time I see you, please come say", "tokens": [51038, 1604, 300, 13, 2555, 1604, 613, 732, 721, 13, 400, 264, 958, 565, 286, 536, 291, 11, 1767, 808, 584, 51234], "temperature": 0.0, "avg_logprob": -0.13672422369321188, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.006675093434751034}, {"id": 221, "seek": 138894, "start": 1406.3400000000001, "end": 1413.26, "text": " hi and I'll, I'll buy you lunch. Okay. Great. Thank you very much for listening to me. It's", "tokens": [51234, 4879, 293, 286, 603, 11, 286, 603, 2256, 291, 6349, 13, 1033, 13, 3769, 13, 1044, 291, 588, 709, 337, 4764, 281, 385, 13, 467, 311, 51580], "temperature": 0.0, "avg_logprob": -0.13672422369321188, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.006675093434751034}, {"id": 222, "seek": 141326, "start": 1413.26, "end": 1420.26, "text": " great to be here. It's great to talk to you. Please be in touch. Please reach out and oh,", "tokens": [50364, 869, 281, 312, 510, 13, 467, 311, 869, 281, 751, 281, 291, 13, 2555, 312, 294, 2557, 13, 2555, 2524, 484, 293, 1954, 11, 50714], "temperature": 0.0, "avg_logprob": -0.3749049828977001, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.045615293085575104}, {"id": 223, "seek": 141326, "start": 1423.54, "end": 1427.86, "text": " boy, we're out of time, but I'm very, very happy to get your questions. Come talk to", "tokens": [50878, 3237, 11, 321, 434, 484, 295, 565, 11, 457, 286, 478, 588, 11, 588, 2055, 281, 483, 428, 1651, 13, 2492, 751, 281, 51094], "temperature": 0.0, "avg_logprob": -0.3749049828977001, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.045615293085575104}, {"id": 224, "seek": 141326, "start": 1427.86, "end": 1428.86, "text": " me.", "tokens": [51094, 385, 13, 51144], "temperature": 0.0, "avg_logprob": -0.3749049828977001, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.045615293085575104}, {"id": 225, "seek": 141326, "start": 1428.86, "end": 1432.18, "text": " There's still like two minutes for questions. So if there are any questions, please.", "tokens": [51144, 821, 311, 920, 411, 732, 2077, 337, 1651, 13, 407, 498, 456, 366, 604, 1651, 11, 1767, 13, 51310], "temperature": 0.0, "avg_logprob": -0.3749049828977001, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.045615293085575104}, {"id": 226, "seek": 141326, "start": 1432.18, "end": 1433.18, "text": " Go for it.", "tokens": [51310, 1037, 337, 309, 13, 51360], "temperature": 0.0, "avg_logprob": -0.3749049828977001, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.045615293085575104}, {"id": 227, "seek": 141326, "start": 1433.18, "end": 1435.18, "text": " There's one in the back. Yes.", "tokens": [51360, 821, 311, 472, 294, 264, 646, 13, 1079, 13, 51460], "temperature": 0.0, "avg_logprob": -0.3749049828977001, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.045615293085575104}, {"id": 228, "seek": 141326, "start": 1435.18, "end": 1436.18, "text": " So.", "tokens": [51460, 407, 13, 51510], "temperature": 0.0, "avg_logprob": -0.3749049828977001, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.045615293085575104}, {"id": 229, "seek": 143618, "start": 1436.3, "end": 1443.3, "text": " So hello and thank you for this presentation. I hope you're not going to hate me for this", "tokens": [50370, 407, 7751, 293, 1309, 291, 337, 341, 5860, 13, 286, 1454, 291, 434, 406, 516, 281, 4700, 385, 337, 341, 50720], "temperature": 0.0, "avg_logprob": -0.20619273866925922, "compression_ratio": 1.4627659574468086, "no_speech_prob": 0.012103351764380932}, {"id": 230, "seek": 143618, "start": 1446.0600000000002, "end": 1451.78, "text": " question because I'm a primary infrastructure guy. And one thing I was always concerned", "tokens": [50858, 1168, 570, 286, 478, 257, 6194, 6896, 2146, 13, 400, 472, 551, 286, 390, 1009, 5922, 51144], "temperature": 0.0, "avg_logprob": -0.20619273866925922, "compression_ratio": 1.4627659574468086, "no_speech_prob": 0.012103351764380932}, {"id": 231, "seek": 143618, "start": 1451.78, "end": 1458.78, "text": " about is redundancy, like a scale twice. So if one dies, is this part of your thinking", "tokens": [51144, 466, 307, 27830, 6717, 11, 411, 257, 4373, 6091, 13, 407, 498, 472, 2714, 11, 307, 341, 644, 295, 428, 1953, 51494], "temperature": 0.0, "avg_logprob": -0.20619273866925922, "compression_ratio": 1.4627659574468086, "no_speech_prob": 0.012103351764380932}, {"id": 232, "seek": 143618, "start": 1458.78, "end": 1459.78, "text": " and scope?", "tokens": [51494, 293, 11923, 30, 51544], "temperature": 0.0, "avg_logprob": -0.20619273866925922, "compression_ratio": 1.4627659574468086, "no_speech_prob": 0.012103351764380932}, {"id": 233, "seek": 145978, "start": 1460.3799999999999, "end": 1467.3799999999999, "text": " Or does the question make sense?", "tokens": [50394, 1610, 775, 264, 1168, 652, 2020, 30, 50744], "temperature": 0.0, "avg_logprob": -0.3224819117578967, "compression_ratio": 1.1063829787234043, "no_speech_prob": 0.026364753022789955}, {"id": 234, "seek": 145978, "start": 1467.3799999999999, "end": 1474.3799999999999, "text": " I'm really sorry. I don't fully understand what you mean by redundancy.", "tokens": [50744, 286, 478, 534, 2597, 13, 286, 500, 380, 4498, 1223, 437, 291, 914, 538, 27830, 6717, 13, 51094], "temperature": 0.0, "avg_logprob": -0.3224819117578967, "compression_ratio": 1.1063829787234043, "no_speech_prob": 0.026364753022789955}, {"id": 235, "seek": 148978, "start": 1489.78, "end": 1496.78, "text": " I mean, sure, I understand, but redundancy is trying to solve the problem of fault tolerance.", "tokens": [50364, 286, 914, 11, 988, 11, 286, 1223, 11, 457, 27830, 6717, 307, 1382, 281, 5039, 264, 1154, 295, 7441, 23368, 13, 50714], "temperature": 0.0, "avg_logprob": -0.26422470092773437, "compression_ratio": 1.1071428571428572, "no_speech_prob": 0.019242143258452415}, {"id": 236, "seek": 149678, "start": 1496.78, "end": 1503.78, "text": " Okay. It's not trying to solve the problem of efficiency. I'm trying to solve the problem", "tokens": [50364, 1033, 13, 467, 311, 406, 1382, 281, 5039, 264, 1154, 295, 10493, 13, 286, 478, 1382, 281, 5039, 264, 1154, 50714], "temperature": 0.0, "avg_logprob": -0.2817215052517978, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.010353882797062397}, {"id": 237, "seek": 149678, "start": 1519.3799999999999, "end": 1526.3799999999999, "text": " of efficiency. So redundancy is an orthogonal concern to mine. Does that, does that mean", "tokens": [51494, 295, 10493, 13, 407, 27830, 6717, 307, 364, 41488, 3136, 281, 3892, 13, 4402, 300, 11, 775, 300, 914, 51844], "temperature": 0.0, "avg_logprob": -0.2817215052517978, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.010353882797062397}, {"id": 238, "seek": 152678, "start": 1527.62, "end": 1531.12, "text": " it makes sense? Yeah. Thank you. But thank you for the question. I really appreciate", "tokens": [50406, 309, 1669, 2020, 30, 865, 13, 1044, 291, 13, 583, 1309, 291, 337, 264, 1168, 13, 286, 534, 4449, 50581], "temperature": 0.0, "avg_logprob": -0.2527417036203238, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.004173429682850838}, {"id": 239, "seek": 152678, "start": 1531.12, "end": 1535.5, "text": " questions. Yes.", "tokens": [50581, 1651, 13, 1079, 13, 50800], "temperature": 0.0, "avg_logprob": -0.2527417036203238, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.004173429682850838}, {"id": 240, "seek": 152678, "start": 1535.5, "end": 1542.5, "text": " Did you try to monitor the hover head of monitoring the energy consumption? Yes, that's a great", "tokens": [50800, 2589, 291, 853, 281, 6002, 264, 20076, 1378, 295, 11028, 264, 2281, 12126, 30, 1079, 11, 300, 311, 257, 869, 51150], "temperature": 0.0, "avg_logprob": -0.2527417036203238, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.004173429682850838}, {"id": 241, "seek": 152678, "start": 1543.5, "end": 1549.1, "text": " question. No, we did not. On one side, I'm afraid it's going to be huge. On one side,", "tokens": [51200, 1168, 13, 883, 11, 321, 630, 406, 13, 1282, 472, 1252, 11, 286, 478, 4638, 309, 311, 516, 281, 312, 2603, 13, 1282, 472, 1252, 11, 51480], "temperature": 0.0, "avg_logprob": -0.2527417036203238, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.004173429682850838}, {"id": 242, "seek": 152678, "start": 1549.1, "end": 1556.1, "text": " I don't know. It's like an infinite recursion, you know, like how can I measure the impact", "tokens": [51480, 286, 500, 380, 458, 13, 467, 311, 411, 364, 13785, 20560, 313, 11, 291, 458, 11, 411, 577, 393, 286, 3481, 264, 2712, 51830], "temperature": 0.0, "avg_logprob": -0.2527417036203238, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.004173429682850838}, {"id": 243, "seek": 155610, "start": 1556.3799999999999, "end": 1561.6599999999999, "text": " of my tool itself? Like the tool is what measures the impact. But how do I measure the impact", "tokens": [50378, 295, 452, 2290, 2564, 30, 1743, 264, 2290, 307, 437, 8000, 264, 2712, 13, 583, 577, 360, 286, 3481, 264, 2712, 50642], "temperature": 0.0, "avg_logprob": -0.19618749618530273, "compression_ratio": 1.6337209302325582, "no_speech_prob": 0.008990272879600525}, {"id": 244, "seek": 155610, "start": 1561.6599999999999, "end": 1568.6599999999999, "text": " of the tool? I don't know. I hope, I hope that, I would love to believe that. That's", "tokens": [50642, 295, 264, 2290, 30, 286, 500, 380, 458, 13, 286, 1454, 11, 286, 1454, 300, 11, 286, 576, 959, 281, 1697, 300, 13, 663, 311, 50992], "temperature": 0.0, "avg_logprob": -0.19618749618530273, "compression_ratio": 1.6337209302325582, "no_speech_prob": 0.008990272879600525}, {"id": 245, "seek": 155610, "start": 1569.9399999999998, "end": 1576.9399999999998, "text": " what I, yes, that's what I want to believe. Yes, please. Thank you very much. It was great", "tokens": [51056, 437, 286, 11, 2086, 11, 300, 311, 437, 286, 528, 281, 1697, 13, 1079, 11, 1767, 13, 1044, 291, 588, 709, 13, 467, 390, 869, 51406], "temperature": 0.0, "avg_logprob": -0.19618749618530273, "compression_ratio": 1.6337209302325582, "no_speech_prob": 0.008990272879600525}, {"id": 246, "seek": 155610, "start": 1577.26, "end": 1577.74, "text": " to be here.", "tokens": [51422, 281, 312, 510, 13, 51446], "temperature": 0.0, "avg_logprob": -0.19618749618530273, "compression_ratio": 1.6337209302325582, "no_speech_prob": 0.008990272879600525}, {"id": 247, "seek": 158610, "start": 1586.1, "end": 1593.1, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50714], "temperature": 0.0, "avg_logprob": -0.9419424533843994, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9948045611381531}], "language": "en"}