{"text": " I'm here as someone who's just interested in this stuff. I'm definitely not an expert in AI or a machine learning expert. I'm just a developer writing docs like a lot of people here in the room and I have been experimenting a bit and I want to show you what I've done. I'm Frank. That was my first computer so I'm programming how many people are the same generation? Not that much, okay? That's a long time ago. I'm involved in an open source project which is called Pi4J. I'm in the library to have interaction on the Raspberry Pi with electronic components with Java. Yes, I'm a Java developer who loves Java. Not enough. I even read a book about it. I have been programming for many, many years, over 20 years, but by writing the book I've become a bit of a writer, I contributed a few of these articles of these chapters to a website, Fujay.io, a website for friends of OpenJDK. And that's how I eventually landed a new job. So by starting to write about the projects I love and work on, I actually got hired by Azul. Azul is one of the distributors of Java, so you can have a Java runtime created by Azul. And my job is I'm half of the documentation team. And we live on docs.azul.com, so we have several parts there about these different products. And from time to time we also block about experiments with Java, what's changed, how performance, the stuff, or that we built and things like that. And of course, last year we had chatGPT, the new thing, it knew everything. It's a damn good liar too, so you have to be careful with it. But what do you know about Azul? Azul is one of the distributions we have with our company. It's reasonably good. I'm happy with this answer. But it also says, this information is based on what I know that existed on January 2022. For software that's a problem. We are already two years further. We have every three months a new security release. We have a new version every six months. So although the basic information is correct, it is outdated. And that's a bit the problem with large language models. A large language model works completely different than our brain. The only thing that it does is it predicts the most plausible world, word, after the previous ones. So it's based on a lot of knowledge, that's true. But it doesn't do real reasoning. If it tells you a lie and you say it, that it tells a lie, it will give you another answer. And it will continue doing that until you're happy. And luckily we have a lot of evolutions in these large language models. So we have these GPT evolutions. GPT-5 is around the corner. We have no data there. But each of these models is trained on more and more data and gets better. Now this GPT-5, what they say about it is it will also understand video. And I think that's an important one to realize. They will also train the new model, the GPT-5, on videos. So if you are a documentation writer and you're from time to time create a video of a blog post or something else, all those sources will be used as part of the new models. By the way, who knows Find.com? Only a few people. I like it a lot more than chat GPT. It gives you the links of where it has found a few of the sources that it's using. So that's one of the lacks of chat GPT. It doesn't tell you where it has found the information because it didn't find the information. It's just reasoning on your question and what is the most plausible answer. Now if you are a bit familiar with Java and Java Spring, that Sean Carter is one of those developer advocates. What he says is true. The documentation that you are writing, that you are publishing on the public web is the source for these language models. So they can only become as smart as possible about your product if the information is available somewhere. And then of course as a docs writer, you definitely heard this question. Someone from your management comes to you. Can we make a chat version of our docs? Who had the question? Okay, not that much. Luckily people are researching or trying out. Vadin is a web framework on Java and one of the developer advocates has written a nice blog post. He has done exactly that. Vadin has very good docs on the website. So he has taken the website and he has described this whole thing and he takes two steps. Again, open source stuff. It's available online. By the way, all the links from my presentation, they are on my website. If you go to webtechie.be, the damn password thing is there again. You will find all the links. Sorry for my voice. So what he did is he created a little application that went through all their docs and created vectors. Vectors are the base of a large language model. They are a conversion of text to some kind of a mathematical model. I don't understand a bit of it, but it's amazing. It works. And then he had the second thing is from these vectors. If you ask a question, it will first filter out the documentation which is related and will then do a search or create an example, an answer based on that. And it works pretty good. He was pretty happy with how it answered questions about his own doc set. But there are two problems. We are at an open source event. It has a dependency on two paid services. The first one is pointcone.io, which is a vector database. Definitely you can find online an alternative. And openAI, which is actually providing the chat API. He found out that training it, and I tried the same example he created with our own docs of Azure. You still need to do some training and rewrite some of your documentation to really find the good answers. And it doesn't provide you, again, the same thing as chat GPT. It doesn't provide you links to your documentation. So when I tried this and when he tried this last summer, it's not really the right time to do such a thing. So go to your management. No, not yet. But last October, we had the DevOps conference in Antwerp. It's an amazing event. If you love Java, if you love software development, it sells out faster than Tomorrowland. They have 3,000 tickets. They were sold out in five minutes. Then they had 500 additional tickets. It sold out in two seconds. So it's easier to be a speaker than an attendee at that event. That's how I fix it. Now, Lisa Rass, she's from Belgium. She's one of the developers of LangChain 4j. LangChain is a Python library for doing stuff with OpenAI and all these chat-based things and machine learning. LangChain 4j is a Java version of that Python library. Now, during that talk, she gave 12 demos in one hour. The last one was, how do I interact with an existing text? So she gives the chat system a text, a story about, I don't know, I cannot even remember, and then she asks specific questions about that story. And she gets answers of that story. So that's what we're looking for. How can we look and interact with our own documentation? So this looked promising. And again, when you're at the conference and you get inspiration, like I had a few tools that I already took a picture of that I want to try out. This stuck to my head. And luckily we have Fosdm and then the tool, the Docs Devroom, so I had a reason to try something out. And that's exactly what I did. If you go to the LangChain 4j examples repository on GitHub, since two weeks I have added a little Java evix application there as one of the examples. Java evix is a tool to create user interfaces. Yes, I'm a Java champion. I have to sell Java today. So what this application does, it still relies on OpenAI, sorry, but you have to buy a few credits. With all the experiments that I've done, I spent a few dollars. Not that big of an effort, but... So it remembers your previous questions. So I asked you to pick a random boy name and a random girl name and then tell me a fairy tale of five sentences. And you see that the fairy tale is again over. There were two children named Etna and Olivia, the answers of the previous questions. So you can have a chat with an application within Java and reasoning. But this is based on OpenAI and what it already knows. So then I went a bit a step further. With the docs that we create for the Azure Docs website, we use the Algolia search machine. It was already mentioned a few times here. We are a company so we can afford to use a third party for this. But to feed it with data, we already created a little tool that breaks our docs into sections. Every header becomes a JSON. I know it's hard to read, but every header of our docs becomes a JSON block with the title of the header, a link to the page, a link to the specific anchor on that page, and then the content which is under the header. So we already have that JSON. We have a data set, a structured data set of our docs. We can use this. Can we chat with something with an application against this documentation set? So that was my idea. Can I do that? I know this is not a coding conference, but still let me dive into it. Because I like Java. I think that's clear. It allows you, thanks to these amazing libraries, to create powerful applications with minimal code. So the thing you see here is actually about the UI, so I will go to the chat service. So what I do here in a few things is first I have this JSON. It has over 1,500 records. And it creates an object of each of these records and puts them in, it's called an embedding store, I think. So here it creates, where is it? The embedding model. So it has an embedding model. Again, I'm not an expert. I have no idea what it's doing behind the scenes. I just found out it works and it does some great things. And then I have, if you ask it a question, then it will, of all these 1,500 blocks, search the 10 most relevant ones. And then give those 10 text blocks to chat.gpt. And that chat.gpt will create an answer out of it. And when we ask for chat.gpt to create an answer, we also give it some rules. Do not provide any additional information. I will show you why later. I try to do not provide answers about other programming languages, but it just ignores it. It still answers me questions about Python, for instance. I don't know why. I said it's a damn good liar, but also a cheater. And if you don't find the answers in those 1500 elements, then just say, sorry, I could not find an answer because you don't want your chat system to come up with something else. So this is the application. I should have probably made it a bigger fund. So you see we have 1,522 embeddings. So if I ask it, what do you know about as a prime? So it you see those are the 10 links I have now to the specific information. It's a demo. So it should fail. No exception. It's going to the network indeed. But in cases like that, we have, of course, video too many open windows exception. Okay. Good. I recorded this, this noon, just in case. Converting those 1500 elements to vectors takes some time. It takes about one minute and a half before the application starts. If you would run this on the server, you don't mind. You start it once and then you get your answer. So you see this, the answer streaming back. So it's really a chat like interface. And it gives a pretty good answer. If I ask, I know the docs and that's the handy thing in this case. I know what it should answer. So I can really try it out and see if I get the expected answers. Like for instance, we have several products. Do I get the right installation instructions for the products I'm asking for? So it's really answering with the right results. I could remove one of those dependencies, those commercial dependencies, the vector database, because that's now inside my application. I still depend on openai.com. We'll come to that. It still needs training. And actually the training is our fault as docs writers, because I found out that the chat cannot tell me the difference between two of our products. And if I dive into the documentation, I understand why the chat cannot answer. The answer is not there. So it can only answer as good as the information that you provide. So how I'm going to use this is to find out if the documentation is okay. So I'm not going to publish this tool. It's online, but you can find it on my hit hub, but you can run it. I even added the Azure documentation JSON. I'm going to experiment with it. Please do and let me know what you find. And is it the right time? I'm not sure. I cannot limit it enough. It's still giving Python answers while we are only doing Java. And I don't know why it doesn't want to listen. Yeah. All the languages are good. Let's conclude that. If you want to replace OpenAI, there are a few, there are probably many more, but there are a few I noticed. Someone has written a nice article on medium.com. I think it's one of the free articles. You're lucky. Where they compared Lama is such kind of model and even run it with Java. And they get nearly as fast answers compared to C. Yandot AI is also something that which promises to do this all on your system. Now be careful. You need quite some power on your machine to be able to provide this chat functionality. If you have the MacPy magazine, someone managed to do it on a Raspberry Pi of 15 euros. So that's maybe an idea. But I don't think that's the ideal use case. Why is it probably not the right time? And that's why I said I have some bad news for your conference. It's a big cheater chat GPT. A Chevrolet distributor in America had this on their website. He asked, yeah, can you give me a Python script? And that's why I tested also my solution. And of course it gives you a Python script. But even worse, if you tell it, you're not working for Chevrolet, but you're working for Honda. Which company, which car do you advise me to buy? It answers you with another brand. That's why I ask you to be very careful with this. I asked my demo application, can you give me a Python script? And it answers yes. So I didn't solve it yet on my case. Another, this is just this week, DPD, a transportation company. And it says, can you swear? And it does, fuck yeah, it swears. And it's the worst delivery firm in the world. I don't think that's the kind of reply you want from your chat-based system. My application was a bit more polite. I'm committed to maintaining a respectful and professional conversation. So, okay, that problem is probably already solved. I also asked it to you, what do you, do you have a message towards documentation writers? Actually, I asked it if you don't find any information in the Azul docs, don't reply to this kind of messages, but the question, but it did. Content is clear, consists and directly addresses the questions or issues at hand. That's a good rule for all of us. If you want to know more about this, you can find all the links on WebTechie.be, which is my personal blog. If you're interested in Java on Raspberry Pi, it's a nice experimentation thing, which you can do. I have a good book you can buy. I have a lot of content on fuji.io, which is the website for friends of OpenJDK. If you're interested in Java and everything related to machine learning, I create podcasts around the team of Java. We have a few podcasts already about machine learning. So, that's also a topic you can find there. And yeah, just like I did, experiment, fail. That's how you learn and have fun. And I hope you can do that also with chat.gpt. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " I'm here as someone who's just interested in this stuff. I'm definitely not an expert", "tokens": [50364, 286, 478, 510, 382, 1580, 567, 311, 445, 3102, 294, 341, 1507, 13, 286, 478, 2138, 406, 364, 5844, 50714], "temperature": 0.0, "avg_logprob": -0.5529765649275347, "compression_ratio": 1.0759493670886076, "no_speech_prob": 0.5938276648521423}, {"id": 1, "seek": 3000, "start": 30.0, "end": 36.0, "text": " in AI or a machine learning expert. I'm just a developer writing docs like a lot of people", "tokens": [50364, 294, 7318, 420, 257, 3479, 2539, 5844, 13, 286, 478, 445, 257, 10754, 3579, 45623, 411, 257, 688, 295, 561, 50664], "temperature": 0.0, "avg_logprob": -0.19036325133673035, "compression_ratio": 1.5256916996047432, "no_speech_prob": 0.8692067861557007}, {"id": 2, "seek": 3000, "start": 36.0, "end": 41.0, "text": " here in the room and I have been experimenting a bit and I want to show you what I've done.", "tokens": [50664, 510, 294, 264, 1808, 293, 286, 362, 668, 29070, 257, 857, 293, 286, 528, 281, 855, 291, 437, 286, 600, 1096, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19036325133673035, "compression_ratio": 1.5256916996047432, "no_speech_prob": 0.8692067861557007}, {"id": 3, "seek": 3000, "start": 41.0, "end": 48.0, "text": " I'm Frank. That was my first computer so I'm programming how many people are the same generation?", "tokens": [50914, 286, 478, 6823, 13, 663, 390, 452, 700, 3820, 370, 286, 478, 9410, 577, 867, 561, 366, 264, 912, 5125, 30, 51264], "temperature": 0.0, "avg_logprob": -0.19036325133673035, "compression_ratio": 1.5256916996047432, "no_speech_prob": 0.8692067861557007}, {"id": 4, "seek": 3000, "start": 48.0, "end": 56.0, "text": " Not that much, okay? That's a long time ago. I'm involved in an open source project which is called Pi4J.", "tokens": [51264, 1726, 300, 709, 11, 1392, 30, 663, 311, 257, 938, 565, 2057, 13, 286, 478, 3288, 294, 364, 1269, 4009, 1716, 597, 307, 1219, 17741, 19, 41, 13, 51664], "temperature": 0.0, "avg_logprob": -0.19036325133673035, "compression_ratio": 1.5256916996047432, "no_speech_prob": 0.8692067861557007}, {"id": 5, "seek": 5600, "start": 56.0, "end": 62.0, "text": " I'm in the library to have interaction on the Raspberry Pi with electronic components with Java.", "tokens": [50364, 286, 478, 294, 264, 6405, 281, 362, 9285, 322, 264, 41154, 17741, 365, 10092, 6677, 365, 10745, 13, 50664], "temperature": 0.0, "avg_logprob": -0.20877737747995476, "compression_ratio": 1.4898989898989898, "no_speech_prob": 0.1436685472726822}, {"id": 6, "seek": 5600, "start": 62.0, "end": 70.0, "text": " Yes, I'm a Java developer who loves Java. Not enough. I even read a book about it.", "tokens": [50664, 1079, 11, 286, 478, 257, 10745, 10754, 567, 6752, 10745, 13, 1726, 1547, 13, 286, 754, 1401, 257, 1446, 466, 309, 13, 51064], "temperature": 0.0, "avg_logprob": -0.20877737747995476, "compression_ratio": 1.4898989898989898, "no_speech_prob": 0.1436685472726822}, {"id": 7, "seek": 5600, "start": 70.0, "end": 80.0, "text": " I have been programming for many, many years, over 20 years, but by writing the book I've become a bit of a writer,", "tokens": [51064, 286, 362, 668, 9410, 337, 867, 11, 867, 924, 11, 670, 945, 924, 11, 457, 538, 3579, 264, 1446, 286, 600, 1813, 257, 857, 295, 257, 9936, 11, 51564], "temperature": 0.0, "avg_logprob": -0.20877737747995476, "compression_ratio": 1.4898989898989898, "no_speech_prob": 0.1436685472726822}, {"id": 8, "seek": 8000, "start": 80.0, "end": 88.0, "text": " I contributed a few of these articles of these chapters to a website, Fujay.io, a website for friends of OpenJDK.", "tokens": [50364, 286, 18434, 257, 1326, 295, 613, 11290, 295, 613, 20013, 281, 257, 3144, 11, 43915, 320, 13, 1004, 11, 257, 3144, 337, 1855, 295, 7238, 41, 35, 42, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14183448463357906, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.3098110258579254}, {"id": 9, "seek": 8000, "start": 88.0, "end": 96.0, "text": " And that's how I eventually landed a new job. So by starting to write about the projects I love and work on,", "tokens": [50764, 400, 300, 311, 577, 286, 4728, 15336, 257, 777, 1691, 13, 407, 538, 2891, 281, 2464, 466, 264, 4455, 286, 959, 293, 589, 322, 11, 51164], "temperature": 0.0, "avg_logprob": -0.14183448463357906, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.3098110258579254}, {"id": 10, "seek": 8000, "start": 96.0, "end": 104.0, "text": " I actually got hired by Azul. Azul is one of the distributors of Java, so you can have a Java runtime created by Azul.", "tokens": [51164, 286, 767, 658, 13144, 538, 7607, 425, 13, 7607, 425, 307, 472, 295, 264, 4400, 30751, 295, 10745, 11, 370, 291, 393, 362, 257, 10745, 34474, 2942, 538, 7607, 425, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14183448463357906, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.3098110258579254}, {"id": 11, "seek": 10400, "start": 104.0, "end": 114.0, "text": " And my job is I'm half of the documentation team. And we live on docs.azul.com, so we have several parts there about these different products.", "tokens": [50364, 400, 452, 1691, 307, 286, 478, 1922, 295, 264, 14333, 1469, 13, 400, 321, 1621, 322, 45623, 13, 921, 425, 13, 1112, 11, 370, 321, 362, 2940, 3166, 456, 466, 613, 819, 3383, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15833161936865914, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.19054770469665527}, {"id": 12, "seek": 10400, "start": 114.0, "end": 125.0, "text": " And from time to time we also block about experiments with Java, what's changed, how performance, the stuff, or that we built and things like that.", "tokens": [50864, 400, 490, 565, 281, 565, 321, 611, 3461, 466, 12050, 365, 10745, 11, 437, 311, 3105, 11, 577, 3389, 11, 264, 1507, 11, 420, 300, 321, 3094, 293, 721, 411, 300, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15833161936865914, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.19054770469665527}, {"id": 13, "seek": 12500, "start": 126.0, "end": 140.0, "text": " And of course, last year we had chatGPT, the new thing, it knew everything. It's a damn good liar too, so you have to be careful with it.", "tokens": [50414, 400, 295, 1164, 11, 1036, 1064, 321, 632, 5081, 38, 47, 51, 11, 264, 777, 551, 11, 309, 2586, 1203, 13, 467, 311, 257, 8151, 665, 27323, 886, 11, 370, 291, 362, 281, 312, 5026, 365, 309, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1571748223103268, "compression_ratio": 1.4101123595505618, "no_speech_prob": 0.19397318363189697}, {"id": 14, "seek": 12500, "start": 140.0, "end": 149.0, "text": " But what do you know about Azul? Azul is one of the distributions we have with our company. It's reasonably good.", "tokens": [51114, 583, 437, 360, 291, 458, 466, 7607, 425, 30, 7607, 425, 307, 472, 295, 264, 37870, 321, 362, 365, 527, 2237, 13, 467, 311, 23551, 665, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1571748223103268, "compression_ratio": 1.4101123595505618, "no_speech_prob": 0.19397318363189697}, {"id": 15, "seek": 14900, "start": 149.0, "end": 162.0, "text": " I'm happy with this answer. But it also says, this information is based on what I know that existed on January 2022.", "tokens": [50364, 286, 478, 2055, 365, 341, 1867, 13, 583, 309, 611, 1619, 11, 341, 1589, 307, 2361, 322, 437, 286, 458, 300, 13135, 322, 7061, 20229, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10064435910574998, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.2103709727525711}, {"id": 16, "seek": 14900, "start": 162.0, "end": 169.0, "text": " For software that's a problem. We are already two years further. We have every three months a new security release.", "tokens": [51014, 1171, 4722, 300, 311, 257, 1154, 13, 492, 366, 1217, 732, 924, 3052, 13, 492, 362, 633, 1045, 2493, 257, 777, 3825, 4374, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10064435910574998, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.2103709727525711}, {"id": 17, "seek": 14900, "start": 169.0, "end": 177.0, "text": " We have a new version every six months. So although the basic information is correct, it is outdated.", "tokens": [51364, 492, 362, 257, 777, 3037, 633, 2309, 2493, 13, 407, 4878, 264, 3875, 1589, 307, 3006, 11, 309, 307, 36313, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10064435910574998, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.2103709727525711}, {"id": 18, "seek": 17700, "start": 177.0, "end": 187.0, "text": " And that's a bit the problem with large language models. A large language model works completely different than our brain.", "tokens": [50364, 400, 300, 311, 257, 857, 264, 1154, 365, 2416, 2856, 5245, 13, 316, 2416, 2856, 2316, 1985, 2584, 819, 813, 527, 3567, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11440774281819661, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.013305083848536015}, {"id": 19, "seek": 17700, "start": 187.0, "end": 195.0, "text": " The only thing that it does is it predicts the most plausible world, word, after the previous ones.", "tokens": [50864, 440, 787, 551, 300, 309, 775, 307, 309, 6069, 82, 264, 881, 39925, 1002, 11, 1349, 11, 934, 264, 3894, 2306, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11440774281819661, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.013305083848536015}, {"id": 20, "seek": 17700, "start": 195.0, "end": 201.0, "text": " So it's based on a lot of knowledge, that's true. But it doesn't do real reasoning.", "tokens": [51264, 407, 309, 311, 2361, 322, 257, 688, 295, 3601, 11, 300, 311, 2074, 13, 583, 309, 1177, 380, 360, 957, 21577, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11440774281819661, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.013305083848536015}, {"id": 21, "seek": 20100, "start": 202.0, "end": 210.0, "text": " If it tells you a lie and you say it, that it tells a lie, it will give you another answer. And it will continue doing that until you're happy.", "tokens": [50414, 759, 309, 5112, 291, 257, 4544, 293, 291, 584, 309, 11, 300, 309, 5112, 257, 4544, 11, 309, 486, 976, 291, 1071, 1867, 13, 400, 309, 486, 2354, 884, 300, 1826, 291, 434, 2055, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16809726342922304, "compression_ratio": 1.5736842105263158, "no_speech_prob": 0.022474462166428566}, {"id": 22, "seek": 20100, "start": 210.0, "end": 216.0, "text": " And luckily we have a lot of evolutions in these large language models.", "tokens": [50814, 400, 22880, 321, 362, 257, 688, 295, 1073, 15892, 294, 613, 2416, 2856, 5245, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16809726342922304, "compression_ratio": 1.5736842105263158, "no_speech_prob": 0.022474462166428566}, {"id": 23, "seek": 20100, "start": 216.0, "end": 224.0, "text": " So we have these GPT evolutions. GPT-5 is around the corner. We have no data there.", "tokens": [51114, 407, 321, 362, 613, 26039, 51, 1073, 15892, 13, 26039, 51, 12, 20, 307, 926, 264, 4538, 13, 492, 362, 572, 1412, 456, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16809726342922304, "compression_ratio": 1.5736842105263158, "no_speech_prob": 0.022474462166428566}, {"id": 24, "seek": 22400, "start": 225.0, "end": 230.0, "text": " But each of these models is trained on more and more data and gets better.", "tokens": [50414, 583, 1184, 295, 613, 5245, 307, 8895, 322, 544, 293, 544, 1412, 293, 2170, 1101, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08041483919385453, "compression_ratio": 1.4624277456647399, "no_speech_prob": 0.1301058530807495}, {"id": 25, "seek": 22400, "start": 230.0, "end": 238.0, "text": " Now this GPT-5, what they say about it is it will also understand video.", "tokens": [50664, 823, 341, 26039, 51, 12, 20, 11, 437, 436, 584, 466, 309, 307, 309, 486, 611, 1223, 960, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08041483919385453, "compression_ratio": 1.4624277456647399, "no_speech_prob": 0.1301058530807495}, {"id": 26, "seek": 22400, "start": 238.0, "end": 246.0, "text": " And I think that's an important one to realize. They will also train the new model, the GPT-5, on videos.", "tokens": [51064, 400, 286, 519, 300, 311, 364, 1021, 472, 281, 4325, 13, 814, 486, 611, 3847, 264, 777, 2316, 11, 264, 26039, 51, 12, 20, 11, 322, 2145, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08041483919385453, "compression_ratio": 1.4624277456647399, "no_speech_prob": 0.1301058530807495}, {"id": 27, "seek": 24600, "start": 246.0, "end": 259.0, "text": " So if you are a documentation writer and you're from time to time create a video of a blog post or something else, all those sources will be used as part of the new models.", "tokens": [50364, 407, 498, 291, 366, 257, 14333, 9936, 293, 291, 434, 490, 565, 281, 565, 1884, 257, 960, 295, 257, 6968, 2183, 420, 746, 1646, 11, 439, 729, 7139, 486, 312, 1143, 382, 644, 295, 264, 777, 5245, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1648878000550351, "compression_ratio": 1.3515151515151516, "no_speech_prob": 0.17402561008930206}, {"id": 28, "seek": 24600, "start": 259.0, "end": 267.0, "text": " By the way, who knows Find.com? Only a few people.", "tokens": [51014, 3146, 264, 636, 11, 567, 3255, 11809, 13, 1112, 30, 5686, 257, 1326, 561, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1648878000550351, "compression_ratio": 1.3515151515151516, "no_speech_prob": 0.17402561008930206}, {"id": 29, "seek": 26700, "start": 267.0, "end": 275.0, "text": " I like it a lot more than chat GPT. It gives you the links of where it has found a few of the sources that it's using.", "tokens": [50364, 286, 411, 309, 257, 688, 544, 813, 5081, 26039, 51, 13, 467, 2709, 291, 264, 6123, 295, 689, 309, 575, 1352, 257, 1326, 295, 264, 7139, 300, 309, 311, 1228, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09078624949735754, "compression_ratio": 1.7046632124352332, "no_speech_prob": 0.2812786400318146}, {"id": 30, "seek": 26700, "start": 275.0, "end": 285.0, "text": " So that's one of the lacks of chat GPT. It doesn't tell you where it has found the information because it didn't find the information.", "tokens": [50764, 407, 300, 311, 472, 295, 264, 31132, 295, 5081, 26039, 51, 13, 467, 1177, 380, 980, 291, 689, 309, 575, 1352, 264, 1589, 570, 309, 994, 380, 915, 264, 1589, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09078624949735754, "compression_ratio": 1.7046632124352332, "no_speech_prob": 0.2812786400318146}, {"id": 31, "seek": 26700, "start": 285.0, "end": 290.0, "text": " It's just reasoning on your question and what is the most plausible answer.", "tokens": [51264, 467, 311, 445, 21577, 322, 428, 1168, 293, 437, 307, 264, 881, 39925, 1867, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09078624949735754, "compression_ratio": 1.7046632124352332, "no_speech_prob": 0.2812786400318146}, {"id": 32, "seek": 29000, "start": 291.0, "end": 300.0, "text": " Now if you are a bit familiar with Java and Java Spring, that Sean Carter is one of those developer advocates.", "tokens": [50414, 823, 498, 291, 366, 257, 857, 4963, 365, 10745, 293, 10745, 14013, 11, 300, 14839, 21622, 307, 472, 295, 729, 10754, 25160, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14090658056324926, "compression_ratio": 1.5088757396449703, "no_speech_prob": 0.0451870821416378}, {"id": 33, "seek": 29000, "start": 300.0, "end": 310.0, "text": " What he says is true. The documentation that you are writing, that you are publishing on the public web is the source for these language models.", "tokens": [50864, 708, 415, 1619, 307, 2074, 13, 440, 14333, 300, 291, 366, 3579, 11, 300, 291, 366, 17832, 322, 264, 1908, 3670, 307, 264, 4009, 337, 613, 2856, 5245, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14090658056324926, "compression_ratio": 1.5088757396449703, "no_speech_prob": 0.0451870821416378}, {"id": 34, "seek": 31000, "start": 311.0, "end": 319.0, "text": " So they can only become as smart as possible about your product if the information is available somewhere.", "tokens": [50414, 407, 436, 393, 787, 1813, 382, 4069, 382, 1944, 466, 428, 1674, 498, 264, 1589, 307, 2435, 4079, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15896009298471303, "compression_ratio": 1.4947368421052631, "no_speech_prob": 0.11716992408037186}, {"id": 35, "seek": 31000, "start": 322.0, "end": 326.0, "text": " And then of course as a docs writer, you definitely heard this question.", "tokens": [50964, 400, 550, 295, 1164, 382, 257, 45623, 9936, 11, 291, 2138, 2198, 341, 1168, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15896009298471303, "compression_ratio": 1.4947368421052631, "no_speech_prob": 0.11716992408037186}, {"id": 36, "seek": 31000, "start": 326.0, "end": 334.0, "text": " Someone from your management comes to you. Can we make a chat version of our docs? Who had the question?", "tokens": [51164, 8734, 490, 428, 4592, 1487, 281, 291, 13, 1664, 321, 652, 257, 5081, 3037, 295, 527, 45623, 30, 2102, 632, 264, 1168, 30, 51564], "temperature": 0.0, "avg_logprob": -0.15896009298471303, "compression_ratio": 1.4947368421052631, "no_speech_prob": 0.11716992408037186}, {"id": 37, "seek": 33400, "start": 334.0, "end": 341.0, "text": " Okay, not that much. Luckily people are researching or trying out.", "tokens": [50364, 1033, 11, 406, 300, 709, 13, 19726, 561, 366, 24176, 420, 1382, 484, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11758112907409668, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.032211050391197205}, {"id": 38, "seek": 33400, "start": 341.0, "end": 347.0, "text": " Vadin is a web framework on Java and one of the developer advocates has written a nice blog post.", "tokens": [50714, 691, 39961, 307, 257, 3670, 8388, 322, 10745, 293, 472, 295, 264, 10754, 25160, 575, 3720, 257, 1481, 6968, 2183, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11758112907409668, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.032211050391197205}, {"id": 39, "seek": 33400, "start": 347.0, "end": 352.0, "text": " He has done exactly that. Vadin has very good docs on the website.", "tokens": [51014, 634, 575, 1096, 2293, 300, 13, 691, 39961, 575, 588, 665, 45623, 322, 264, 3144, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11758112907409668, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.032211050391197205}, {"id": 40, "seek": 33400, "start": 352.0, "end": 359.0, "text": " So he has taken the website and he has described this whole thing and he takes two steps.", "tokens": [51264, 407, 415, 575, 2726, 264, 3144, 293, 415, 575, 7619, 341, 1379, 551, 293, 415, 2516, 732, 4439, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11758112907409668, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.032211050391197205}, {"id": 41, "seek": 35900, "start": 360.0, "end": 363.0, "text": " Again, open source stuff. It's available online.", "tokens": [50414, 3764, 11, 1269, 4009, 1507, 13, 467, 311, 2435, 2950, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12881692250569662, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.07253103703260422}, {"id": 42, "seek": 35900, "start": 363.0, "end": 368.0, "text": " By the way, all the links from my presentation, they are on my website.", "tokens": [50564, 3146, 264, 636, 11, 439, 264, 6123, 490, 452, 5860, 11, 436, 366, 322, 452, 3144, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12881692250569662, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.07253103703260422}, {"id": 43, "seek": 35900, "start": 368.0, "end": 372.0, "text": " If you go to webtechie.be, the damn password thing is there again.", "tokens": [50814, 759, 291, 352, 281, 3670, 25970, 414, 13, 650, 11, 264, 8151, 11524, 551, 307, 456, 797, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12881692250569662, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.07253103703260422}, {"id": 44, "seek": 35900, "start": 372.0, "end": 375.0, "text": " You will find all the links. Sorry for my voice.", "tokens": [51014, 509, 486, 915, 439, 264, 6123, 13, 4919, 337, 452, 3177, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12881692250569662, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.07253103703260422}, {"id": 45, "seek": 35900, "start": 375.0, "end": 384.0, "text": " So what he did is he created a little application that went through all their docs and created vectors.", "tokens": [51164, 407, 437, 415, 630, 307, 415, 2942, 257, 707, 3861, 300, 1437, 807, 439, 641, 45623, 293, 2942, 18875, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12881692250569662, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.07253103703260422}, {"id": 46, "seek": 38400, "start": 384.0, "end": 388.0, "text": " Vectors are the base of a large language model.", "tokens": [50364, 691, 557, 830, 366, 264, 3096, 295, 257, 2416, 2856, 2316, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09749459645834314, "compression_ratio": 1.5169082125603865, "no_speech_prob": 0.02152653969824314}, {"id": 47, "seek": 38400, "start": 388.0, "end": 393.0, "text": " They are a conversion of text to some kind of a mathematical model.", "tokens": [50564, 814, 366, 257, 14298, 295, 2487, 281, 512, 733, 295, 257, 18894, 2316, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09749459645834314, "compression_ratio": 1.5169082125603865, "no_speech_prob": 0.02152653969824314}, {"id": 48, "seek": 38400, "start": 393.0, "end": 398.0, "text": " I don't understand a bit of it, but it's amazing. It works.", "tokens": [50814, 286, 500, 380, 1223, 257, 857, 295, 309, 11, 457, 309, 311, 2243, 13, 467, 1985, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09749459645834314, "compression_ratio": 1.5169082125603865, "no_speech_prob": 0.02152653969824314}, {"id": 49, "seek": 38400, "start": 398.0, "end": 403.0, "text": " And then he had the second thing is from these vectors.", "tokens": [51064, 400, 550, 415, 632, 264, 1150, 551, 307, 490, 613, 18875, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09749459645834314, "compression_ratio": 1.5169082125603865, "no_speech_prob": 0.02152653969824314}, {"id": 50, "seek": 38400, "start": 403.0, "end": 409.0, "text": " If you ask a question, it will first filter out the documentation which is related", "tokens": [51314, 759, 291, 1029, 257, 1168, 11, 309, 486, 700, 6608, 484, 264, 14333, 597, 307, 4077, 51614], "temperature": 0.0, "avg_logprob": -0.09749459645834314, "compression_ratio": 1.5169082125603865, "no_speech_prob": 0.02152653969824314}, {"id": 51, "seek": 40900, "start": 409.0, "end": 415.0, "text": " and will then do a search or create an example, an answer based on that.", "tokens": [50364, 293, 486, 550, 360, 257, 3164, 420, 1884, 364, 1365, 11, 364, 1867, 2361, 322, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10560774541163183, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.009256215766072273}, {"id": 52, "seek": 40900, "start": 415.0, "end": 419.0, "text": " And it works pretty good.", "tokens": [50664, 400, 309, 1985, 1238, 665, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10560774541163183, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.009256215766072273}, {"id": 53, "seek": 40900, "start": 419.0, "end": 425.0, "text": " He was pretty happy with how it answered questions about his own doc set.", "tokens": [50864, 634, 390, 1238, 2055, 365, 577, 309, 10103, 1651, 466, 702, 1065, 3211, 992, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10560774541163183, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.009256215766072273}, {"id": 54, "seek": 40900, "start": 425.0, "end": 429.0, "text": " But there are two problems. We are at an open source event.", "tokens": [51164, 583, 456, 366, 732, 2740, 13, 492, 366, 412, 364, 1269, 4009, 2280, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10560774541163183, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.009256215766072273}, {"id": 55, "seek": 40900, "start": 429.0, "end": 432.0, "text": " It has a dependency on two paid services.", "tokens": [51364, 467, 575, 257, 33621, 322, 732, 4835, 3328, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10560774541163183, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.009256215766072273}, {"id": 56, "seek": 40900, "start": 432.0, "end": 436.0, "text": " The first one is pointcone.io, which is a vector database.", "tokens": [51514, 440, 700, 472, 307, 935, 66, 546, 13, 1004, 11, 597, 307, 257, 8062, 8149, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10560774541163183, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.009256215766072273}, {"id": 57, "seek": 43600, "start": 436.0, "end": 440.0, "text": " Definitely you can find online an alternative.", "tokens": [50364, 12151, 291, 393, 915, 2950, 364, 8535, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10535862710740831, "compression_ratio": 1.4854368932038835, "no_speech_prob": 0.012130515649914742}, {"id": 58, "seek": 43600, "start": 440.0, "end": 446.0, "text": " And openAI, which is actually providing the chat API.", "tokens": [50564, 400, 1269, 48698, 11, 597, 307, 767, 6530, 264, 5081, 9362, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10535862710740831, "compression_ratio": 1.4854368932038835, "no_speech_prob": 0.012130515649914742}, {"id": 59, "seek": 43600, "start": 446.0, "end": 453.0, "text": " He found out that training it, and I tried the same example he created with our own docs of Azure.", "tokens": [50864, 634, 1352, 484, 300, 3097, 309, 11, 293, 286, 3031, 264, 912, 1365, 415, 2942, 365, 527, 1065, 45623, 295, 11969, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10535862710740831, "compression_ratio": 1.4854368932038835, "no_speech_prob": 0.012130515649914742}, {"id": 60, "seek": 43600, "start": 453.0, "end": 460.0, "text": " You still need to do some training and rewrite some of your documentation to really find the good answers.", "tokens": [51214, 509, 920, 643, 281, 360, 512, 3097, 293, 28132, 512, 295, 428, 14333, 281, 534, 915, 264, 665, 6338, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10535862710740831, "compression_ratio": 1.4854368932038835, "no_speech_prob": 0.012130515649914742}, {"id": 61, "seek": 46000, "start": 460.0, "end": 464.0, "text": " And it doesn't provide you, again, the same thing as chat GPT.", "tokens": [50364, 400, 309, 1177, 380, 2893, 291, 11, 797, 11, 264, 912, 551, 382, 5081, 26039, 51, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11396022464918054, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.008818527683615685}, {"id": 62, "seek": 46000, "start": 464.0, "end": 467.0, "text": " It doesn't provide you links to your documentation.", "tokens": [50564, 467, 1177, 380, 2893, 291, 6123, 281, 428, 14333, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11396022464918054, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.008818527683615685}, {"id": 63, "seek": 46000, "start": 467.0, "end": 472.0, "text": " So when I tried this and when he tried this last summer,", "tokens": [50714, 407, 562, 286, 3031, 341, 293, 562, 415, 3031, 341, 1036, 4266, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11396022464918054, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.008818527683615685}, {"id": 64, "seek": 46000, "start": 472.0, "end": 476.0, "text": " it's not really the right time to do such a thing.", "tokens": [50964, 309, 311, 406, 534, 264, 558, 565, 281, 360, 1270, 257, 551, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11396022464918054, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.008818527683615685}, {"id": 65, "seek": 46000, "start": 476.0, "end": 481.0, "text": " So go to your management. No, not yet.", "tokens": [51164, 407, 352, 281, 428, 4592, 13, 883, 11, 406, 1939, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11396022464918054, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.008818527683615685}, {"id": 66, "seek": 46000, "start": 481.0, "end": 485.0, "text": " But last October, we had the DevOps conference in Antwerp.", "tokens": [51414, 583, 1036, 7617, 11, 321, 632, 264, 43051, 7586, 294, 5130, 1554, 79, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11396022464918054, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.008818527683615685}, {"id": 67, "seek": 48500, "start": 485.0, "end": 490.0, "text": " It's an amazing event. If you love Java, if you love software development,", "tokens": [50364, 467, 311, 364, 2243, 2280, 13, 759, 291, 959, 10745, 11, 498, 291, 959, 4722, 3250, 11, 50614], "temperature": 0.0, "avg_logprob": -0.08345589270958534, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.06359292566776276}, {"id": 68, "seek": 48500, "start": 490.0, "end": 494.0, "text": " it sells out faster than Tomorrowland.", "tokens": [50614, 309, 20897, 484, 4663, 813, 17499, 1661, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08345589270958534, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.06359292566776276}, {"id": 69, "seek": 48500, "start": 494.0, "end": 498.0, "text": " They have 3,000 tickets. They were sold out in five minutes.", "tokens": [50814, 814, 362, 805, 11, 1360, 12628, 13, 814, 645, 3718, 484, 294, 1732, 2077, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08345589270958534, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.06359292566776276}, {"id": 70, "seek": 48500, "start": 498.0, "end": 502.0, "text": " Then they had 500 additional tickets. It sold out in two seconds.", "tokens": [51014, 1396, 436, 632, 5923, 4497, 12628, 13, 467, 3718, 484, 294, 732, 3949, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08345589270958534, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.06359292566776276}, {"id": 71, "seek": 48500, "start": 502.0, "end": 507.0, "text": " So it's easier to be a speaker than an attendee at that event.", "tokens": [51214, 407, 309, 311, 3571, 281, 312, 257, 8145, 813, 364, 6888, 1653, 412, 300, 2280, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08345589270958534, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.06359292566776276}, {"id": 72, "seek": 48500, "start": 507.0, "end": 509.0, "text": " That's how I fix it.", "tokens": [51464, 663, 311, 577, 286, 3191, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08345589270958534, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.06359292566776276}, {"id": 73, "seek": 50900, "start": 509.0, "end": 513.0, "text": " Now, Lisa Rass, she's from Belgium.", "tokens": [50364, 823, 11, 12252, 497, 640, 11, 750, 311, 490, 28094, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12664132648044163, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.023055510595440865}, {"id": 74, "seek": 50900, "start": 513.0, "end": 516.0, "text": " She's one of the developers of LangChain 4j.", "tokens": [50564, 1240, 311, 472, 295, 264, 8849, 295, 13313, 6546, 491, 1017, 73, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12664132648044163, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.023055510595440865}, {"id": 75, "seek": 50900, "start": 516.0, "end": 522.0, "text": " LangChain is a Python library for doing stuff with OpenAI", "tokens": [50714, 13313, 6546, 491, 307, 257, 15329, 6405, 337, 884, 1507, 365, 7238, 48698, 51014], "temperature": 0.0, "avg_logprob": -0.12664132648044163, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.023055510595440865}, {"id": 76, "seek": 50900, "start": 522.0, "end": 526.0, "text": " and all these chat-based things and machine learning.", "tokens": [51014, 293, 439, 613, 5081, 12, 6032, 721, 293, 3479, 2539, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12664132648044163, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.023055510595440865}, {"id": 77, "seek": 50900, "start": 526.0, "end": 531.0, "text": " LangChain 4j is a Java version of that Python library.", "tokens": [51214, 13313, 6546, 491, 1017, 73, 307, 257, 10745, 3037, 295, 300, 15329, 6405, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12664132648044163, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.023055510595440865}, {"id": 78, "seek": 50900, "start": 531.0, "end": 536.0, "text": " Now, during that talk, she gave 12 demos in one hour.", "tokens": [51464, 823, 11, 1830, 300, 751, 11, 750, 2729, 2272, 33788, 294, 472, 1773, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12664132648044163, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.023055510595440865}, {"id": 79, "seek": 53600, "start": 536.0, "end": 542.0, "text": " The last one was, how do I interact with an existing text?", "tokens": [50364, 440, 1036, 472, 390, 11, 577, 360, 286, 4648, 365, 364, 6741, 2487, 30, 50664], "temperature": 0.0, "avg_logprob": -0.13278958201408386, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.021221458911895752}, {"id": 80, "seek": 53600, "start": 542.0, "end": 547.0, "text": " So she gives the chat system a text, a story about, I don't know,", "tokens": [50664, 407, 750, 2709, 264, 5081, 1185, 257, 2487, 11, 257, 1657, 466, 11, 286, 500, 380, 458, 11, 50914], "temperature": 0.0, "avg_logprob": -0.13278958201408386, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.021221458911895752}, {"id": 81, "seek": 53600, "start": 547.0, "end": 552.0, "text": " I cannot even remember, and then she asks specific questions about that story.", "tokens": [50914, 286, 2644, 754, 1604, 11, 293, 550, 750, 8962, 2685, 1651, 466, 300, 1657, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13278958201408386, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.021221458911895752}, {"id": 82, "seek": 53600, "start": 552.0, "end": 554.0, "text": " And she gets answers of that story.", "tokens": [51164, 400, 750, 2170, 6338, 295, 300, 1657, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13278958201408386, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.021221458911895752}, {"id": 83, "seek": 53600, "start": 554.0, "end": 556.0, "text": " So that's what we're looking for.", "tokens": [51264, 407, 300, 311, 437, 321, 434, 1237, 337, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13278958201408386, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.021221458911895752}, {"id": 84, "seek": 53600, "start": 556.0, "end": 561.0, "text": " How can we look and interact with our own documentation?", "tokens": [51364, 1012, 393, 321, 574, 293, 4648, 365, 527, 1065, 14333, 30, 51614], "temperature": 0.0, "avg_logprob": -0.13278958201408386, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.021221458911895752}, {"id": 85, "seek": 53600, "start": 561.0, "end": 564.0, "text": " So this looked promising.", "tokens": [51614, 407, 341, 2956, 20257, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13278958201408386, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.021221458911895752}, {"id": 86, "seek": 56400, "start": 564.0, "end": 568.0, "text": " And again, when you're at the conference and you get inspiration,", "tokens": [50364, 400, 797, 11, 562, 291, 434, 412, 264, 7586, 293, 291, 483, 10249, 11, 50564], "temperature": 0.0, "avg_logprob": -0.17398021839283132, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.012200740166008472}, {"id": 87, "seek": 56400, "start": 568.0, "end": 574.0, "text": " like I had a few tools that I already took a picture of that I want to try out.", "tokens": [50564, 411, 286, 632, 257, 1326, 3873, 300, 286, 1217, 1890, 257, 3036, 295, 300, 286, 528, 281, 853, 484, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17398021839283132, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.012200740166008472}, {"id": 88, "seek": 56400, "start": 574.0, "end": 576.0, "text": " This stuck to my head.", "tokens": [50864, 639, 5541, 281, 452, 1378, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17398021839283132, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.012200740166008472}, {"id": 89, "seek": 56400, "start": 576.0, "end": 580.0, "text": " And luckily we have Fosdm and then the tool, the Docs Devroom,", "tokens": [50964, 400, 22880, 321, 362, 479, 329, 67, 76, 293, 550, 264, 2290, 11, 264, 16024, 82, 9096, 2861, 11, 51164], "temperature": 0.0, "avg_logprob": -0.17398021839283132, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.012200740166008472}, {"id": 90, "seek": 56400, "start": 580.0, "end": 583.0, "text": " so I had a reason to try something out.", "tokens": [51164, 370, 286, 632, 257, 1778, 281, 853, 746, 484, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17398021839283132, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.012200740166008472}, {"id": 91, "seek": 56400, "start": 583.0, "end": 585.0, "text": " And that's exactly what I did.", "tokens": [51314, 400, 300, 311, 2293, 437, 286, 630, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17398021839283132, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.012200740166008472}, {"id": 92, "seek": 56400, "start": 585.0, "end": 589.0, "text": " If you go to the LangChain 4j examples repository on GitHub,", "tokens": [51414, 759, 291, 352, 281, 264, 13313, 6546, 491, 1017, 73, 5110, 25841, 322, 23331, 11, 51614], "temperature": 0.0, "avg_logprob": -0.17398021839283132, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.012200740166008472}, {"id": 93, "seek": 58900, "start": 589.0, "end": 595.0, "text": " since two weeks I have added a little Java evix application there as one of the examples.", "tokens": [50364, 1670, 732, 3259, 286, 362, 3869, 257, 707, 10745, 1073, 970, 3861, 456, 382, 472, 295, 264, 5110, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12215107917785645, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.05114037171006203}, {"id": 94, "seek": 58900, "start": 595.0, "end": 599.0, "text": " Java evix is a tool to create user interfaces.", "tokens": [50664, 10745, 1073, 970, 307, 257, 2290, 281, 1884, 4195, 28416, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12215107917785645, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.05114037171006203}, {"id": 95, "seek": 58900, "start": 599.0, "end": 601.0, "text": " Yes, I'm a Java champion.", "tokens": [50864, 1079, 11, 286, 478, 257, 10745, 10971, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12215107917785645, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.05114037171006203}, {"id": 96, "seek": 58900, "start": 601.0, "end": 605.0, "text": " I have to sell Java today.", "tokens": [50964, 286, 362, 281, 3607, 10745, 965, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12215107917785645, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.05114037171006203}, {"id": 97, "seek": 58900, "start": 605.0, "end": 611.0, "text": " So what this application does, it still relies on OpenAI, sorry,", "tokens": [51164, 407, 437, 341, 3861, 775, 11, 309, 920, 30910, 322, 7238, 48698, 11, 2597, 11, 51464], "temperature": 0.0, "avg_logprob": -0.12215107917785645, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.05114037171006203}, {"id": 98, "seek": 58900, "start": 611.0, "end": 614.0, "text": " but you have to buy a few credits.", "tokens": [51464, 457, 291, 362, 281, 2256, 257, 1326, 16816, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12215107917785645, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.05114037171006203}, {"id": 99, "seek": 58900, "start": 614.0, "end": 617.0, "text": " With all the experiments that I've done, I spent a few dollars.", "tokens": [51614, 2022, 439, 264, 12050, 300, 286, 600, 1096, 11, 286, 4418, 257, 1326, 3808, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12215107917785645, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.05114037171006203}, {"id": 100, "seek": 61700, "start": 617.0, "end": 620.0, "text": " Not that big of an effort, but...", "tokens": [50364, 1726, 300, 955, 295, 364, 4630, 11, 457, 485, 50514], "temperature": 0.0, "avg_logprob": -0.14511713981628419, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03188974782824516}, {"id": 101, "seek": 61700, "start": 620.0, "end": 623.0, "text": " So it remembers your previous questions.", "tokens": [50514, 407, 309, 26228, 428, 3894, 1651, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14511713981628419, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03188974782824516}, {"id": 102, "seek": 61700, "start": 623.0, "end": 627.0, "text": " So I asked you to pick a random boy name and a random girl name", "tokens": [50664, 407, 286, 2351, 291, 281, 1888, 257, 4974, 3237, 1315, 293, 257, 4974, 2013, 1315, 50864], "temperature": 0.0, "avg_logprob": -0.14511713981628419, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03188974782824516}, {"id": 103, "seek": 61700, "start": 627.0, "end": 630.0, "text": " and then tell me a fairy tale of five sentences.", "tokens": [50864, 293, 550, 980, 385, 257, 19104, 17172, 295, 1732, 16579, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14511713981628419, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03188974782824516}, {"id": 104, "seek": 61700, "start": 630.0, "end": 633.0, "text": " And you see that the fairy tale is again over.", "tokens": [51014, 400, 291, 536, 300, 264, 19104, 17172, 307, 797, 670, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14511713981628419, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03188974782824516}, {"id": 105, "seek": 61700, "start": 633.0, "end": 638.0, "text": " There were two children named Etna and Olivia, the answers of the previous questions.", "tokens": [51164, 821, 645, 732, 2227, 4926, 3790, 629, 293, 26023, 11, 264, 6338, 295, 264, 3894, 1651, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14511713981628419, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03188974782824516}, {"id": 106, "seek": 61700, "start": 638.0, "end": 644.0, "text": " So you can have a chat with an application within Java and reasoning.", "tokens": [51414, 407, 291, 393, 362, 257, 5081, 365, 364, 3861, 1951, 10745, 293, 21577, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14511713981628419, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03188974782824516}, {"id": 107, "seek": 64400, "start": 644.0, "end": 648.0, "text": " But this is based on OpenAI and what it already knows.", "tokens": [50364, 583, 341, 307, 2361, 322, 7238, 48698, 293, 437, 309, 1217, 3255, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09792524065290178, "compression_ratio": 1.5884773662551441, "no_speech_prob": 0.012439288198947906}, {"id": 108, "seek": 64400, "start": 648.0, "end": 651.0, "text": " So then I went a bit a step further.", "tokens": [50564, 407, 550, 286, 1437, 257, 857, 257, 1823, 3052, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09792524065290178, "compression_ratio": 1.5884773662551441, "no_speech_prob": 0.012439288198947906}, {"id": 109, "seek": 64400, "start": 651.0, "end": 655.0, "text": " With the docs that we create for the Azure Docs website,", "tokens": [50714, 2022, 264, 45623, 300, 321, 1884, 337, 264, 11969, 16024, 82, 3144, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09792524065290178, "compression_ratio": 1.5884773662551441, "no_speech_prob": 0.012439288198947906}, {"id": 110, "seek": 64400, "start": 655.0, "end": 657.0, "text": " we use the Algolia search machine.", "tokens": [50914, 321, 764, 264, 35014, 29760, 3164, 3479, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09792524065290178, "compression_ratio": 1.5884773662551441, "no_speech_prob": 0.012439288198947906}, {"id": 111, "seek": 64400, "start": 657.0, "end": 661.0, "text": " It was already mentioned a few times here.", "tokens": [51014, 467, 390, 1217, 2835, 257, 1326, 1413, 510, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09792524065290178, "compression_ratio": 1.5884773662551441, "no_speech_prob": 0.012439288198947906}, {"id": 112, "seek": 64400, "start": 661.0, "end": 665.0, "text": " We are a company so we can afford to use a third party for this.", "tokens": [51214, 492, 366, 257, 2237, 370, 321, 393, 6157, 281, 764, 257, 2636, 3595, 337, 341, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09792524065290178, "compression_ratio": 1.5884773662551441, "no_speech_prob": 0.012439288198947906}, {"id": 113, "seek": 64400, "start": 665.0, "end": 673.0, "text": " But to feed it with data, we already created a little tool that breaks our docs into sections.", "tokens": [51414, 583, 281, 3154, 309, 365, 1412, 11, 321, 1217, 2942, 257, 707, 2290, 300, 9857, 527, 45623, 666, 10863, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09792524065290178, "compression_ratio": 1.5884773662551441, "no_speech_prob": 0.012439288198947906}, {"id": 114, "seek": 67300, "start": 673.0, "end": 676.0, "text": " Every header becomes a JSON.", "tokens": [50364, 2048, 23117, 3643, 257, 31828, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08928892923438031, "compression_ratio": 1.7421052631578948, "no_speech_prob": 0.10692961513996124}, {"id": 115, "seek": 67300, "start": 676.0, "end": 683.0, "text": " I know it's hard to read, but every header of our docs becomes a JSON block", "tokens": [50514, 286, 458, 309, 311, 1152, 281, 1401, 11, 457, 633, 23117, 295, 527, 45623, 3643, 257, 31828, 3461, 50864], "temperature": 0.0, "avg_logprob": -0.08928892923438031, "compression_ratio": 1.7421052631578948, "no_speech_prob": 0.10692961513996124}, {"id": 116, "seek": 67300, "start": 683.0, "end": 689.0, "text": " with the title of the header, a link to the page, a link to the specific anchor on that page,", "tokens": [50864, 365, 264, 4876, 295, 264, 23117, 11, 257, 2113, 281, 264, 3028, 11, 257, 2113, 281, 264, 2685, 18487, 322, 300, 3028, 11, 51164], "temperature": 0.0, "avg_logprob": -0.08928892923438031, "compression_ratio": 1.7421052631578948, "no_speech_prob": 0.10692961513996124}, {"id": 117, "seek": 67300, "start": 689.0, "end": 692.0, "text": " and then the content which is under the header.", "tokens": [51164, 293, 550, 264, 2701, 597, 307, 833, 264, 23117, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08928892923438031, "compression_ratio": 1.7421052631578948, "no_speech_prob": 0.10692961513996124}, {"id": 118, "seek": 67300, "start": 692.0, "end": 694.0, "text": " So we already have that JSON.", "tokens": [51314, 407, 321, 1217, 362, 300, 31828, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08928892923438031, "compression_ratio": 1.7421052631578948, "no_speech_prob": 0.10692961513996124}, {"id": 119, "seek": 67300, "start": 694.0, "end": 699.0, "text": " We have a data set, a structured data set of our docs.", "tokens": [51414, 492, 362, 257, 1412, 992, 11, 257, 18519, 1412, 992, 295, 527, 45623, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08928892923438031, "compression_ratio": 1.7421052631578948, "no_speech_prob": 0.10692961513996124}, {"id": 120, "seek": 69900, "start": 699.0, "end": 701.0, "text": " We can use this.", "tokens": [50364, 492, 393, 764, 341, 13, 50464], "temperature": 0.0, "avg_logprob": -0.15217938630477243, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.15179333090782166}, {"id": 121, "seek": 69900, "start": 701.0, "end": 706.0, "text": " Can we chat with something with an application against this documentation set?", "tokens": [50464, 1664, 321, 5081, 365, 746, 365, 364, 3861, 1970, 341, 14333, 992, 30, 50714], "temperature": 0.0, "avg_logprob": -0.15217938630477243, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.15179333090782166}, {"id": 122, "seek": 69900, "start": 706.0, "end": 711.0, "text": " So that was my idea. Can I do that?", "tokens": [50714, 407, 300, 390, 452, 1558, 13, 1664, 286, 360, 300, 30, 50964], "temperature": 0.0, "avg_logprob": -0.15217938630477243, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.15179333090782166}, {"id": 123, "seek": 69900, "start": 711.0, "end": 718.0, "text": " I know this is not a coding conference, but still let me dive into it.", "tokens": [50964, 286, 458, 341, 307, 406, 257, 17720, 7586, 11, 457, 920, 718, 385, 9192, 666, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15217938630477243, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.15179333090782166}, {"id": 124, "seek": 69900, "start": 718.0, "end": 724.0, "text": " Because I like Java. I think that's clear.", "tokens": [51314, 1436, 286, 411, 10745, 13, 286, 519, 300, 311, 1850, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15217938630477243, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.15179333090782166}, {"id": 125, "seek": 72400, "start": 724.0, "end": 731.0, "text": " It allows you, thanks to these amazing libraries, to create powerful applications with minimal code.", "tokens": [50364, 467, 4045, 291, 11, 3231, 281, 613, 2243, 15148, 11, 281, 1884, 4005, 5821, 365, 13206, 3089, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14001471375765867, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.15898513793945312}, {"id": 126, "seek": 72400, "start": 731.0, "end": 740.0, "text": " So the thing you see here is actually about the UI, so I will go to the chat service.", "tokens": [50714, 407, 264, 551, 291, 536, 510, 307, 767, 466, 264, 15682, 11, 370, 286, 486, 352, 281, 264, 5081, 2643, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14001471375765867, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.15898513793945312}, {"id": 127, "seek": 72400, "start": 740.0, "end": 745.0, "text": " So what I do here in a few things is first I have this JSON.", "tokens": [51164, 407, 437, 286, 360, 510, 294, 257, 1326, 721, 307, 700, 286, 362, 341, 31828, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14001471375765867, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.15898513793945312}, {"id": 128, "seek": 72400, "start": 745.0, "end": 750.0, "text": " It has over 1,500 records.", "tokens": [51414, 467, 575, 670, 502, 11, 7526, 7724, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14001471375765867, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.15898513793945312}, {"id": 129, "seek": 75000, "start": 750.0, "end": 755.0, "text": " And it creates an object of each of these records and puts them in,", "tokens": [50364, 400, 309, 7829, 364, 2657, 295, 1184, 295, 613, 7724, 293, 8137, 552, 294, 11, 50614], "temperature": 0.0, "avg_logprob": -0.10364787111577299, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.005930623039603233}, {"id": 130, "seek": 75000, "start": 755.0, "end": 760.0, "text": " it's called an embedding store, I think.", "tokens": [50614, 309, 311, 1219, 364, 12240, 3584, 3531, 11, 286, 519, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10364787111577299, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.005930623039603233}, {"id": 131, "seek": 75000, "start": 760.0, "end": 768.0, "text": " So here it creates, where is it?", "tokens": [50864, 407, 510, 309, 7829, 11, 689, 307, 309, 30, 51264], "temperature": 0.0, "avg_logprob": -0.10364787111577299, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.005930623039603233}, {"id": 132, "seek": 75000, "start": 768.0, "end": 770.0, "text": " The embedding model.", "tokens": [51264, 440, 12240, 3584, 2316, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10364787111577299, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.005930623039603233}, {"id": 133, "seek": 75000, "start": 770.0, "end": 772.0, "text": " So it has an embedding model.", "tokens": [51364, 407, 309, 575, 364, 12240, 3584, 2316, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10364787111577299, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.005930623039603233}, {"id": 134, "seek": 75000, "start": 772.0, "end": 776.0, "text": " Again, I'm not an expert. I have no idea what it's doing behind the scenes.", "tokens": [51464, 3764, 11, 286, 478, 406, 364, 5844, 13, 286, 362, 572, 1558, 437, 309, 311, 884, 2261, 264, 8026, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10364787111577299, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.005930623039603233}, {"id": 135, "seek": 75000, "start": 776.0, "end": 779.0, "text": " I just found out it works and it does some great things.", "tokens": [51664, 286, 445, 1352, 484, 309, 1985, 293, 309, 775, 512, 869, 721, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10364787111577299, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.005930623039603233}, {"id": 136, "seek": 77900, "start": 779.0, "end": 794.0, "text": " And then I have, if you ask it a question, then it will, of all these 1,500 blocks, search the 10 most relevant ones.", "tokens": [50364, 400, 550, 286, 362, 11, 498, 291, 1029, 309, 257, 1168, 11, 550, 309, 486, 11, 295, 439, 613, 502, 11, 7526, 8474, 11, 3164, 264, 1266, 881, 7340, 2306, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18804139801950165, "compression_ratio": 1.4496644295302012, "no_speech_prob": 0.01746133342385292}, {"id": 137, "seek": 77900, "start": 794.0, "end": 799.0, "text": " And then give those 10 text blocks to chat.gpt.", "tokens": [51114, 400, 550, 976, 729, 1266, 2487, 8474, 281, 5081, 13, 70, 662, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18804139801950165, "compression_ratio": 1.4496644295302012, "no_speech_prob": 0.01746133342385292}, {"id": 138, "seek": 77900, "start": 799.0, "end": 803.0, "text": " And that chat.gpt will create an answer out of it.", "tokens": [51364, 400, 300, 5081, 13, 70, 662, 486, 1884, 364, 1867, 484, 295, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18804139801950165, "compression_ratio": 1.4496644295302012, "no_speech_prob": 0.01746133342385292}, {"id": 139, "seek": 80300, "start": 804.0, "end": 811.0, "text": " And when we ask for chat.gpt to create an answer, we also give it some rules.", "tokens": [50414, 400, 562, 321, 1029, 337, 5081, 13, 70, 662, 281, 1884, 364, 1867, 11, 321, 611, 976, 309, 512, 4474, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09324582417805989, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.03904427960515022}, {"id": 140, "seek": 80300, "start": 811.0, "end": 814.0, "text": " Do not provide any additional information.", "tokens": [50764, 1144, 406, 2893, 604, 4497, 1589, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09324582417805989, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.03904427960515022}, {"id": 141, "seek": 80300, "start": 814.0, "end": 818.0, "text": " I will show you why later.", "tokens": [50914, 286, 486, 855, 291, 983, 1780, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09324582417805989, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.03904427960515022}, {"id": 142, "seek": 80300, "start": 818.0, "end": 823.0, "text": " I try to do not provide answers about other programming languages, but it just ignores it.", "tokens": [51114, 286, 853, 281, 360, 406, 2893, 6338, 466, 661, 9410, 8650, 11, 457, 309, 445, 5335, 2706, 309, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09324582417805989, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.03904427960515022}, {"id": 143, "seek": 80300, "start": 823.0, "end": 826.0, "text": " It still answers me questions about Python, for instance.", "tokens": [51364, 467, 920, 6338, 385, 1651, 466, 15329, 11, 337, 5197, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09324582417805989, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.03904427960515022}, {"id": 144, "seek": 80300, "start": 826.0, "end": 828.0, "text": " I don't know why.", "tokens": [51514, 286, 500, 380, 458, 983, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09324582417805989, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.03904427960515022}, {"id": 145, "seek": 80300, "start": 828.0, "end": 832.0, "text": " I said it's a damn good liar, but also a cheater.", "tokens": [51614, 286, 848, 309, 311, 257, 8151, 665, 27323, 11, 457, 611, 257, 947, 771, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09324582417805989, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.03904427960515022}, {"id": 146, "seek": 83200, "start": 832.0, "end": 845.0, "text": " And if you don't find the answers in those 1500 elements, then just say, sorry, I could not find an answer because you don't want your chat system to come up with something else.", "tokens": [50364, 400, 498, 291, 500, 380, 915, 264, 6338, 294, 729, 22671, 4959, 11, 550, 445, 584, 11, 2597, 11, 286, 727, 406, 915, 364, 1867, 570, 291, 500, 380, 528, 428, 5081, 1185, 281, 808, 493, 365, 746, 1646, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10020039631770207, "compression_ratio": 1.467005076142132, "no_speech_prob": 0.01915036514401436}, {"id": 147, "seek": 83200, "start": 845.0, "end": 848.0, "text": " So this is the application.", "tokens": [51014, 407, 341, 307, 264, 3861, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10020039631770207, "compression_ratio": 1.467005076142132, "no_speech_prob": 0.01915036514401436}, {"id": 148, "seek": 83200, "start": 848.0, "end": 850.0, "text": " I should have probably made it a bigger fund.", "tokens": [51164, 286, 820, 362, 1391, 1027, 309, 257, 3801, 2374, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10020039631770207, "compression_ratio": 1.467005076142132, "no_speech_prob": 0.01915036514401436}, {"id": 149, "seek": 83200, "start": 850.0, "end": 855.0, "text": " So you see we have 1,522 embeddings.", "tokens": [51264, 407, 291, 536, 321, 362, 502, 11, 20, 7490, 12240, 29432, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10020039631770207, "compression_ratio": 1.467005076142132, "no_speech_prob": 0.01915036514401436}, {"id": 150, "seek": 85500, "start": 855.0, "end": 863.0, "text": " So if I ask it, what do you know about as a prime?", "tokens": [50364, 407, 498, 286, 1029, 309, 11, 437, 360, 291, 458, 466, 382, 257, 5835, 30, 50764], "temperature": 0.0, "avg_logprob": -0.1933514444451583, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.025474855676293373}, {"id": 151, "seek": 85500, "start": 863.0, "end": 872.0, "text": " So it you see those are the 10 links I have now to the specific information.", "tokens": [50764, 407, 309, 291, 536, 729, 366, 264, 1266, 6123, 286, 362, 586, 281, 264, 2685, 1589, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1933514444451583, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.025474855676293373}, {"id": 152, "seek": 85500, "start": 872.0, "end": 873.0, "text": " It's a demo.", "tokens": [51214, 467, 311, 257, 10723, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1933514444451583, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.025474855676293373}, {"id": 153, "seek": 85500, "start": 873.0, "end": 881.0, "text": " So it should fail.", "tokens": [51264, 407, 309, 820, 3061, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1933514444451583, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.025474855676293373}, {"id": 154, "seek": 85500, "start": 881.0, "end": 883.0, "text": " No exception.", "tokens": [51664, 883, 11183, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1933514444451583, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.025474855676293373}, {"id": 155, "seek": 88300, "start": 883.0, "end": 887.0, "text": " It's going to the network indeed.", "tokens": [50364, 467, 311, 516, 281, 264, 3209, 6451, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2262277956362124, "compression_ratio": 1.2878787878787878, "no_speech_prob": 0.11574122309684753}, {"id": 156, "seek": 88300, "start": 887.0, "end": 899.0, "text": " But in cases like that, we have, of course, video too many open windows exception.", "tokens": [50564, 583, 294, 3331, 411, 300, 11, 321, 362, 11, 295, 1164, 11, 960, 886, 867, 1269, 9309, 11183, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2262277956362124, "compression_ratio": 1.2878787878787878, "no_speech_prob": 0.11574122309684753}, {"id": 157, "seek": 88300, "start": 899.0, "end": 901.0, "text": " Okay.", "tokens": [51164, 1033, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2262277956362124, "compression_ratio": 1.2878787878787878, "no_speech_prob": 0.11574122309684753}, {"id": 158, "seek": 88300, "start": 901.0, "end": 903.0, "text": " Good.", "tokens": [51264, 2205, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2262277956362124, "compression_ratio": 1.2878787878787878, "no_speech_prob": 0.11574122309684753}, {"id": 159, "seek": 88300, "start": 903.0, "end": 909.0, "text": " I recorded this, this noon, just in case.", "tokens": [51364, 286, 8287, 341, 11, 341, 24040, 11, 445, 294, 1389, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2262277956362124, "compression_ratio": 1.2878787878787878, "no_speech_prob": 0.11574122309684753}, {"id": 160, "seek": 90900, "start": 909.0, "end": 916.0, "text": " Converting those 1500 elements to vectors takes some time.", "tokens": [50364, 2656, 331, 783, 729, 22671, 4959, 281, 18875, 2516, 512, 565, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13045303795927315, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.12325642257928848}, {"id": 161, "seek": 90900, "start": 916.0, "end": 921.0, "text": " It takes about one minute and a half before the application starts.", "tokens": [50714, 467, 2516, 466, 472, 3456, 293, 257, 1922, 949, 264, 3861, 3719, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13045303795927315, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.12325642257928848}, {"id": 162, "seek": 90900, "start": 921.0, "end": 924.0, "text": " If you would run this on the server, you don't mind.", "tokens": [50964, 759, 291, 576, 1190, 341, 322, 264, 7154, 11, 291, 500, 380, 1575, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13045303795927315, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.12325642257928848}, {"id": 163, "seek": 90900, "start": 924.0, "end": 927.0, "text": " You start it once and then you get your answer.", "tokens": [51114, 509, 722, 309, 1564, 293, 550, 291, 483, 428, 1867, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13045303795927315, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.12325642257928848}, {"id": 164, "seek": 90900, "start": 927.0, "end": 929.0, "text": " So you see this, the answer streaming back.", "tokens": [51264, 407, 291, 536, 341, 11, 264, 1867, 11791, 646, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13045303795927315, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.12325642257928848}, {"id": 165, "seek": 90900, "start": 929.0, "end": 932.0, "text": " So it's really a chat like interface.", "tokens": [51364, 407, 309, 311, 534, 257, 5081, 411, 9226, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13045303795927315, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.12325642257928848}, {"id": 166, "seek": 90900, "start": 932.0, "end": 935.0, "text": " And it gives a pretty good answer.", "tokens": [51514, 400, 309, 2709, 257, 1238, 665, 1867, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13045303795927315, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.12325642257928848}, {"id": 167, "seek": 93500, "start": 935.0, "end": 942.0, "text": " If I ask, I know the docs and that's the handy thing in this case.", "tokens": [50364, 759, 286, 1029, 11, 286, 458, 264, 45623, 293, 300, 311, 264, 13239, 551, 294, 341, 1389, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08666687333182002, "compression_ratio": 1.6616915422885572, "no_speech_prob": 0.07503359019756317}, {"id": 168, "seek": 93500, "start": 942.0, "end": 944.0, "text": " I know what it should answer.", "tokens": [50714, 286, 458, 437, 309, 820, 1867, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08666687333182002, "compression_ratio": 1.6616915422885572, "no_speech_prob": 0.07503359019756317}, {"id": 169, "seek": 93500, "start": 944.0, "end": 947.0, "text": " So I can really try it out and see if I get the expected answers.", "tokens": [50814, 407, 286, 393, 534, 853, 309, 484, 293, 536, 498, 286, 483, 264, 5176, 6338, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08666687333182002, "compression_ratio": 1.6616915422885572, "no_speech_prob": 0.07503359019756317}, {"id": 170, "seek": 93500, "start": 947.0, "end": 949.0, "text": " Like for instance, we have several products.", "tokens": [50964, 1743, 337, 5197, 11, 321, 362, 2940, 3383, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08666687333182002, "compression_ratio": 1.6616915422885572, "no_speech_prob": 0.07503359019756317}, {"id": 171, "seek": 93500, "start": 949.0, "end": 954.0, "text": " Do I get the right installation instructions for the products I'm asking for?", "tokens": [51064, 1144, 286, 483, 264, 558, 13260, 9415, 337, 264, 3383, 286, 478, 3365, 337, 30, 51314], "temperature": 0.0, "avg_logprob": -0.08666687333182002, "compression_ratio": 1.6616915422885572, "no_speech_prob": 0.07503359019756317}, {"id": 172, "seek": 93500, "start": 954.0, "end": 964.0, "text": " So it's really answering with the right results.", "tokens": [51314, 407, 309, 311, 534, 13430, 365, 264, 558, 3542, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08666687333182002, "compression_ratio": 1.6616915422885572, "no_speech_prob": 0.07503359019756317}, {"id": 173, "seek": 96400, "start": 964.0, "end": 968.0, "text": " I could remove one of those dependencies, those commercial dependencies,", "tokens": [50364, 286, 727, 4159, 472, 295, 729, 36606, 11, 729, 6841, 36606, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11244826370410704, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.019083574414253235}, {"id": 174, "seek": 96400, "start": 968.0, "end": 972.0, "text": " the vector database, because that's now inside my application.", "tokens": [50564, 264, 8062, 8149, 11, 570, 300, 311, 586, 1854, 452, 3861, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11244826370410704, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.019083574414253235}, {"id": 175, "seek": 96400, "start": 972.0, "end": 976.0, "text": " I still depend on openai.com.", "tokens": [50764, 286, 920, 5672, 322, 1269, 1301, 13, 1112, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11244826370410704, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.019083574414253235}, {"id": 176, "seek": 96400, "start": 976.0, "end": 979.0, "text": " We'll come to that.", "tokens": [50964, 492, 603, 808, 281, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11244826370410704, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.019083574414253235}, {"id": 177, "seek": 96400, "start": 979.0, "end": 981.0, "text": " It still needs training.", "tokens": [51114, 467, 920, 2203, 3097, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11244826370410704, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.019083574414253235}, {"id": 178, "seek": 96400, "start": 981.0, "end": 985.0, "text": " And actually the training is our fault as docs writers,", "tokens": [51214, 400, 767, 264, 3097, 307, 527, 7441, 382, 45623, 13491, 11, 51414], "temperature": 0.0, "avg_logprob": -0.11244826370410704, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.019083574414253235}, {"id": 179, "seek": 96400, "start": 985.0, "end": 992.0, "text": " because I found out that the chat cannot tell me the difference between two of our products.", "tokens": [51414, 570, 286, 1352, 484, 300, 264, 5081, 2644, 980, 385, 264, 2649, 1296, 732, 295, 527, 3383, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11244826370410704, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.019083574414253235}, {"id": 180, "seek": 99200, "start": 992.0, "end": 996.0, "text": " And if I dive into the documentation, I understand why the chat cannot answer.", "tokens": [50364, 400, 498, 286, 9192, 666, 264, 14333, 11, 286, 1223, 983, 264, 5081, 2644, 1867, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08896600758587872, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.02230137400329113}, {"id": 181, "seek": 99200, "start": 996.0, "end": 998.0, "text": " The answer is not there.", "tokens": [50564, 440, 1867, 307, 406, 456, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08896600758587872, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.02230137400329113}, {"id": 182, "seek": 99200, "start": 998.0, "end": 1003.0, "text": " So it can only answer as good as the information that you provide.", "tokens": [50664, 407, 309, 393, 787, 1867, 382, 665, 382, 264, 1589, 300, 291, 2893, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08896600758587872, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.02230137400329113}, {"id": 183, "seek": 99200, "start": 1003.0, "end": 1009.0, "text": " So how I'm going to use this is to find out if the documentation is okay.", "tokens": [50914, 407, 577, 286, 478, 516, 281, 764, 341, 307, 281, 915, 484, 498, 264, 14333, 307, 1392, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08896600758587872, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.02230137400329113}, {"id": 184, "seek": 99200, "start": 1009.0, "end": 1011.0, "text": " So I'm not going to publish this tool.", "tokens": [51214, 407, 286, 478, 406, 516, 281, 11374, 341, 2290, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08896600758587872, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.02230137400329113}, {"id": 185, "seek": 99200, "start": 1011.0, "end": 1017.0, "text": " It's online, but you can find it on my hit hub, but you can run it.", "tokens": [51314, 467, 311, 2950, 11, 457, 291, 393, 915, 309, 322, 452, 2045, 11838, 11, 457, 291, 393, 1190, 309, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08896600758587872, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.02230137400329113}, {"id": 186, "seek": 99200, "start": 1017.0, "end": 1020.0, "text": " I even added the Azure documentation JSON.", "tokens": [51614, 286, 754, 3869, 264, 11969, 14333, 31828, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08896600758587872, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.02230137400329113}, {"id": 187, "seek": 102000, "start": 1020.0, "end": 1022.0, "text": " I'm going to experiment with it.", "tokens": [50364, 286, 478, 516, 281, 5120, 365, 309, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1721466276380751, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.04178055748343468}, {"id": 188, "seek": 102000, "start": 1022.0, "end": 1024.0, "text": " Please do and let me know what you find.", "tokens": [50464, 2555, 360, 293, 718, 385, 458, 437, 291, 915, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1721466276380751, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.04178055748343468}, {"id": 189, "seek": 102000, "start": 1024.0, "end": 1026.0, "text": " And is it the right time?", "tokens": [50564, 400, 307, 309, 264, 558, 565, 30, 50664], "temperature": 0.0, "avg_logprob": -0.1721466276380751, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.04178055748343468}, {"id": 190, "seek": 102000, "start": 1026.0, "end": 1028.0, "text": " I'm not sure.", "tokens": [50664, 286, 478, 406, 988, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1721466276380751, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.04178055748343468}, {"id": 191, "seek": 102000, "start": 1028.0, "end": 1031.0, "text": " I cannot limit it enough.", "tokens": [50764, 286, 2644, 4948, 309, 1547, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1721466276380751, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.04178055748343468}, {"id": 192, "seek": 102000, "start": 1031.0, "end": 1035.0, "text": " It's still giving Python answers while we are only doing Java.", "tokens": [50914, 467, 311, 920, 2902, 15329, 6338, 1339, 321, 366, 787, 884, 10745, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1721466276380751, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.04178055748343468}, {"id": 193, "seek": 102000, "start": 1035.0, "end": 1041.0, "text": " And I don't know why it doesn't want to listen.", "tokens": [51114, 400, 286, 500, 380, 458, 983, 309, 1177, 380, 528, 281, 2140, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1721466276380751, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.04178055748343468}, {"id": 194, "seek": 102000, "start": 1041.0, "end": 1045.0, "text": " Yeah.", "tokens": [51414, 865, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1721466276380751, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.04178055748343468}, {"id": 195, "seek": 102000, "start": 1045.0, "end": 1047.0, "text": " All the languages are good.", "tokens": [51614, 1057, 264, 8650, 366, 665, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1721466276380751, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.04178055748343468}, {"id": 196, "seek": 104700, "start": 1047.0, "end": 1049.0, "text": " Let's conclude that.", "tokens": [50364, 961, 311, 16886, 300, 13, 50464], "temperature": 0.0, "avg_logprob": -0.14784188800387912, "compression_ratio": 1.4926829268292683, "no_speech_prob": 0.04956000670790672}, {"id": 197, "seek": 104700, "start": 1049.0, "end": 1055.0, "text": " If you want to replace OpenAI, there are a few, there are probably many more,", "tokens": [50464, 759, 291, 528, 281, 7406, 7238, 48698, 11, 456, 366, 257, 1326, 11, 456, 366, 1391, 867, 544, 11, 50764], "temperature": 0.0, "avg_logprob": -0.14784188800387912, "compression_ratio": 1.4926829268292683, "no_speech_prob": 0.04956000670790672}, {"id": 198, "seek": 104700, "start": 1055.0, "end": 1057.0, "text": " but there are a few I noticed.", "tokens": [50764, 457, 456, 366, 257, 1326, 286, 5694, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14784188800387912, "compression_ratio": 1.4926829268292683, "no_speech_prob": 0.04956000670790672}, {"id": 199, "seek": 104700, "start": 1057.0, "end": 1061.0, "text": " Someone has written a nice article on medium.com.", "tokens": [50864, 8734, 575, 3720, 257, 1481, 7222, 322, 6399, 13, 1112, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14784188800387912, "compression_ratio": 1.4926829268292683, "no_speech_prob": 0.04956000670790672}, {"id": 200, "seek": 104700, "start": 1061.0, "end": 1063.0, "text": " I think it's one of the free articles.", "tokens": [51064, 286, 519, 309, 311, 472, 295, 264, 1737, 11290, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14784188800387912, "compression_ratio": 1.4926829268292683, "no_speech_prob": 0.04956000670790672}, {"id": 201, "seek": 104700, "start": 1063.0, "end": 1064.0, "text": " You're lucky.", "tokens": [51164, 509, 434, 6356, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14784188800387912, "compression_ratio": 1.4926829268292683, "no_speech_prob": 0.04956000670790672}, {"id": 202, "seek": 104700, "start": 1064.0, "end": 1071.0, "text": " Where they compared Lama is such kind of model and even run it with Java.", "tokens": [51214, 2305, 436, 5347, 441, 2404, 307, 1270, 733, 295, 2316, 293, 754, 1190, 309, 365, 10745, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14784188800387912, "compression_ratio": 1.4926829268292683, "no_speech_prob": 0.04956000670790672}, {"id": 203, "seek": 107100, "start": 1072.0, "end": 1077.0, "text": " And they get nearly as fast answers compared to C.", "tokens": [50414, 400, 436, 483, 6217, 382, 2370, 6338, 5347, 281, 383, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16002499262491862, "compression_ratio": 1.4268292682926829, "no_speech_prob": 0.04825542867183685}, {"id": 204, "seek": 107100, "start": 1077.0, "end": 1087.0, "text": " Yandot AI is also something that which promises to do this all on your system.", "tokens": [50664, 398, 474, 310, 7318, 307, 611, 746, 300, 597, 16403, 281, 360, 341, 439, 322, 428, 1185, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16002499262491862, "compression_ratio": 1.4268292682926829, "no_speech_prob": 0.04825542867183685}, {"id": 205, "seek": 107100, "start": 1087.0, "end": 1089.0, "text": " Now be careful.", "tokens": [51164, 823, 312, 5026, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16002499262491862, "compression_ratio": 1.4268292682926829, "no_speech_prob": 0.04825542867183685}, {"id": 206, "seek": 107100, "start": 1089.0, "end": 1096.0, "text": " You need quite some power on your machine to be able to provide this chat functionality.", "tokens": [51264, 509, 643, 1596, 512, 1347, 322, 428, 3479, 281, 312, 1075, 281, 2893, 341, 5081, 14980, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16002499262491862, "compression_ratio": 1.4268292682926829, "no_speech_prob": 0.04825542867183685}, {"id": 207, "seek": 109600, "start": 1096.0, "end": 1101.0, "text": " If you have the MacPy magazine, someone managed to do it on a Raspberry Pi of 15 euros.", "tokens": [50364, 759, 291, 362, 264, 5707, 47, 88, 11332, 11, 1580, 6453, 281, 360, 309, 322, 257, 41154, 17741, 295, 2119, 14160, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1317455034989577, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.2803401052951813}, {"id": 208, "seek": 109600, "start": 1101.0, "end": 1103.0, "text": " So that's maybe an idea.", "tokens": [50614, 407, 300, 311, 1310, 364, 1558, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1317455034989577, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.2803401052951813}, {"id": 209, "seek": 109600, "start": 1103.0, "end": 1109.0, "text": " But I don't think that's the ideal use case.", "tokens": [50714, 583, 286, 500, 380, 519, 300, 311, 264, 7157, 764, 1389, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1317455034989577, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.2803401052951813}, {"id": 210, "seek": 109600, "start": 1109.0, "end": 1111.0, "text": " Why is it probably not the right time?", "tokens": [51014, 1545, 307, 309, 1391, 406, 264, 558, 565, 30, 51114], "temperature": 0.0, "avg_logprob": -0.1317455034989577, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.2803401052951813}, {"id": 211, "seek": 109600, "start": 1111.0, "end": 1115.0, "text": " And that's why I said I have some bad news for your conference.", "tokens": [51114, 400, 300, 311, 983, 286, 848, 286, 362, 512, 1578, 2583, 337, 428, 7586, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1317455034989577, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.2803401052951813}, {"id": 212, "seek": 109600, "start": 1115.0, "end": 1119.0, "text": " It's a big cheater chat GPT.", "tokens": [51314, 467, 311, 257, 955, 947, 771, 5081, 26039, 51, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1317455034989577, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.2803401052951813}, {"id": 213, "seek": 109600, "start": 1119.0, "end": 1125.0, "text": " A Chevrolet distributor in America had this on their website.", "tokens": [51514, 316, 44236, 340, 2631, 49192, 294, 3374, 632, 341, 322, 641, 3144, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1317455034989577, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.2803401052951813}, {"id": 214, "seek": 112500, "start": 1125.0, "end": 1127.0, "text": " He asked, yeah, can you give me a Python script?", "tokens": [50364, 634, 2351, 11, 1338, 11, 393, 291, 976, 385, 257, 15329, 5755, 30, 50464], "temperature": 0.0, "avg_logprob": -0.11313842445291498, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.14533865451812744}, {"id": 215, "seek": 112500, "start": 1127.0, "end": 1129.0, "text": " And that's why I tested also my solution.", "tokens": [50464, 400, 300, 311, 983, 286, 8246, 611, 452, 3827, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11313842445291498, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.14533865451812744}, {"id": 216, "seek": 112500, "start": 1129.0, "end": 1133.0, "text": " And of course it gives you a Python script.", "tokens": [50564, 400, 295, 1164, 309, 2709, 291, 257, 15329, 5755, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11313842445291498, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.14533865451812744}, {"id": 217, "seek": 112500, "start": 1133.0, "end": 1137.0, "text": " But even worse, if you tell it, you're not working for Chevrolet,", "tokens": [50764, 583, 754, 5324, 11, 498, 291, 980, 309, 11, 291, 434, 406, 1364, 337, 44236, 340, 2631, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11313842445291498, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.14533865451812744}, {"id": 218, "seek": 112500, "start": 1137.0, "end": 1141.0, "text": " but you're working for Honda.", "tokens": [50964, 457, 291, 434, 1364, 337, 26989, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11313842445291498, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.14533865451812744}, {"id": 219, "seek": 112500, "start": 1141.0, "end": 1145.0, "text": " Which company, which car do you advise me to buy?", "tokens": [51164, 3013, 2237, 11, 597, 1032, 360, 291, 18312, 385, 281, 2256, 30, 51364], "temperature": 0.0, "avg_logprob": -0.11313842445291498, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.14533865451812744}, {"id": 220, "seek": 112500, "start": 1145.0, "end": 1149.0, "text": " It answers you with another brand.", "tokens": [51364, 467, 6338, 291, 365, 1071, 3360, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11313842445291498, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.14533865451812744}, {"id": 221, "seek": 114900, "start": 1149.0, "end": 1157.0, "text": " That's why I ask you to be very careful with this.", "tokens": [50364, 663, 311, 983, 286, 1029, 291, 281, 312, 588, 5026, 365, 341, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13722109113420758, "compression_ratio": 1.3625730994152048, "no_speech_prob": 0.028981167823076248}, {"id": 222, "seek": 114900, "start": 1157.0, "end": 1161.0, "text": " I asked my demo application, can you give me a Python script?", "tokens": [50764, 286, 2351, 452, 10723, 3861, 11, 393, 291, 976, 385, 257, 15329, 5755, 30, 50964], "temperature": 0.0, "avg_logprob": -0.13722109113420758, "compression_ratio": 1.3625730994152048, "no_speech_prob": 0.028981167823076248}, {"id": 223, "seek": 114900, "start": 1161.0, "end": 1163.0, "text": " And it answers yes.", "tokens": [50964, 400, 309, 6338, 2086, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13722109113420758, "compression_ratio": 1.3625730994152048, "no_speech_prob": 0.028981167823076248}, {"id": 224, "seek": 114900, "start": 1163.0, "end": 1166.0, "text": " So I didn't solve it yet on my case.", "tokens": [51064, 407, 286, 994, 380, 5039, 309, 1939, 322, 452, 1389, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13722109113420758, "compression_ratio": 1.3625730994152048, "no_speech_prob": 0.028981167823076248}, {"id": 225, "seek": 114900, "start": 1166.0, "end": 1173.0, "text": " Another, this is just this week, DPD, a transportation company.", "tokens": [51214, 3996, 11, 341, 307, 445, 341, 1243, 11, 413, 17349, 11, 257, 11328, 2237, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13722109113420758, "compression_ratio": 1.3625730994152048, "no_speech_prob": 0.028981167823076248}, {"id": 226, "seek": 117300, "start": 1173.0, "end": 1176.0, "text": " And it says, can you swear?", "tokens": [50364, 400, 309, 1619, 11, 393, 291, 11902, 30, 50514], "temperature": 0.0, "avg_logprob": -0.10130019304228992, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.017002426087856293}, {"id": 227, "seek": 117300, "start": 1176.0, "end": 1180.0, "text": " And it does, fuck yeah, it swears.", "tokens": [50514, 400, 309, 775, 11, 3275, 1338, 11, 309, 2484, 685, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10130019304228992, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.017002426087856293}, {"id": 228, "seek": 117300, "start": 1180.0, "end": 1184.0, "text": " And it's the worst delivery firm in the world.", "tokens": [50714, 400, 309, 311, 264, 5855, 8982, 6174, 294, 264, 1002, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10130019304228992, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.017002426087856293}, {"id": 229, "seek": 117300, "start": 1184.0, "end": 1191.0, "text": " I don't think that's the kind of reply you want from your chat-based system.", "tokens": [50914, 286, 500, 380, 519, 300, 311, 264, 733, 295, 16972, 291, 528, 490, 428, 5081, 12, 6032, 1185, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10130019304228992, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.017002426087856293}, {"id": 230, "seek": 117300, "start": 1191.0, "end": 1198.0, "text": " My application was a bit more polite.", "tokens": [51264, 1222, 3861, 390, 257, 857, 544, 25171, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10130019304228992, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.017002426087856293}, {"id": 231, "seek": 117300, "start": 1198.0, "end": 1202.0, "text": " I'm committed to maintaining a respectful and professional conversation.", "tokens": [51614, 286, 478, 7784, 281, 14916, 257, 26205, 293, 4843, 3761, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10130019304228992, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.017002426087856293}, {"id": 232, "seek": 120200, "start": 1202.0, "end": 1206.0, "text": " So, okay, that problem is probably already solved.", "tokens": [50364, 407, 11, 1392, 11, 300, 1154, 307, 1391, 1217, 13041, 13, 50564], "temperature": 0.0, "avg_logprob": -0.17082899360246556, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.059468887746334076}, {"id": 233, "seek": 120200, "start": 1206.0, "end": 1212.0, "text": " I also asked it to you, what do you, do you have a message towards documentation writers?", "tokens": [50564, 286, 611, 2351, 309, 281, 291, 11, 437, 360, 291, 11, 360, 291, 362, 257, 3636, 3030, 14333, 13491, 30, 50864], "temperature": 0.0, "avg_logprob": -0.17082899360246556, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.059468887746334076}, {"id": 234, "seek": 120200, "start": 1212.0, "end": 1216.0, "text": " Actually, I asked it if you don't find any information in the Azul docs,", "tokens": [50864, 5135, 11, 286, 2351, 309, 498, 291, 500, 380, 915, 604, 1589, 294, 264, 7607, 425, 45623, 11, 51064], "temperature": 0.0, "avg_logprob": -0.17082899360246556, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.059468887746334076}, {"id": 235, "seek": 120200, "start": 1216.0, "end": 1221.0, "text": " don't reply to this kind of messages, but the question, but it did.", "tokens": [51064, 500, 380, 16972, 281, 341, 733, 295, 7897, 11, 457, 264, 1168, 11, 457, 309, 630, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17082899360246556, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.059468887746334076}, {"id": 236, "seek": 120200, "start": 1221.0, "end": 1227.0, "text": " Content is clear, consists and directly addresses the questions or issues at hand.", "tokens": [51314, 30078, 307, 1850, 11, 14689, 293, 3838, 16862, 264, 1651, 420, 2663, 412, 1011, 13, 51614], "temperature": 0.0, "avg_logprob": -0.17082899360246556, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.059468887746334076}, {"id": 237, "seek": 122700, "start": 1227.0, "end": 1232.0, "text": " That's a good rule for all of us.", "tokens": [50364, 663, 311, 257, 665, 4978, 337, 439, 295, 505, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12170713178573116, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.4400724172592163}, {"id": 238, "seek": 122700, "start": 1232.0, "end": 1237.0, "text": " If you want to know more about this, you can find all the links on WebTechie.be,", "tokens": [50614, 759, 291, 528, 281, 458, 544, 466, 341, 11, 291, 393, 915, 439, 264, 6123, 322, 9573, 36050, 414, 13, 650, 11, 50864], "temperature": 0.0, "avg_logprob": -0.12170713178573116, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.4400724172592163}, {"id": 239, "seek": 122700, "start": 1237.0, "end": 1239.0, "text": " which is my personal blog.", "tokens": [50864, 597, 307, 452, 2973, 6968, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12170713178573116, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.4400724172592163}, {"id": 240, "seek": 122700, "start": 1239.0, "end": 1243.0, "text": " If you're interested in Java on Raspberry Pi, it's a nice experimentation thing,", "tokens": [50964, 759, 291, 434, 3102, 294, 10745, 322, 41154, 17741, 11, 309, 311, 257, 1481, 37142, 551, 11, 51164], "temperature": 0.0, "avg_logprob": -0.12170713178573116, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.4400724172592163}, {"id": 241, "seek": 122700, "start": 1243.0, "end": 1245.0, "text": " which you can do.", "tokens": [51164, 597, 291, 393, 360, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12170713178573116, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.4400724172592163}, {"id": 242, "seek": 122700, "start": 1245.0, "end": 1247.0, "text": " I have a good book you can buy.", "tokens": [51264, 286, 362, 257, 665, 1446, 291, 393, 2256, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12170713178573116, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.4400724172592163}, {"id": 243, "seek": 122700, "start": 1247.0, "end": 1253.0, "text": " I have a lot of content on fuji.io, which is the website for friends of OpenJDK.", "tokens": [51364, 286, 362, 257, 688, 295, 2701, 322, 8536, 4013, 13, 1004, 11, 597, 307, 264, 3144, 337, 1855, 295, 7238, 41, 35, 42, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12170713178573116, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.4400724172592163}, {"id": 244, "seek": 122700, "start": 1253.0, "end": 1256.0, "text": " If you're interested in Java and everything related to machine learning,", "tokens": [51664, 759, 291, 434, 3102, 294, 10745, 293, 1203, 4077, 281, 3479, 2539, 11, 51814], "temperature": 0.0, "avg_logprob": -0.12170713178573116, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.4400724172592163}, {"id": 245, "seek": 125600, "start": 1256.0, "end": 1261.0, "text": " I create podcasts around the team of Java.", "tokens": [50364, 286, 1884, 24045, 926, 264, 1469, 295, 10745, 13, 50614], "temperature": 0.0, "avg_logprob": -0.23437960007611444, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.04974821209907532}, {"id": 246, "seek": 125600, "start": 1261.0, "end": 1265.0, "text": " We have a few podcasts already about machine learning.", "tokens": [50614, 492, 362, 257, 1326, 24045, 1217, 466, 3479, 2539, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23437960007611444, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.04974821209907532}, {"id": 247, "seek": 125600, "start": 1265.0, "end": 1268.0, "text": " So, that's also a topic you can find there.", "tokens": [50814, 407, 11, 300, 311, 611, 257, 4829, 291, 393, 915, 456, 13, 50964], "temperature": 0.0, "avg_logprob": -0.23437960007611444, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.04974821209907532}, {"id": 248, "seek": 125600, "start": 1268.0, "end": 1272.0, "text": " And yeah, just like I did, experiment, fail.", "tokens": [50964, 400, 1338, 11, 445, 411, 286, 630, 11, 5120, 11, 3061, 13, 51164], "temperature": 0.0, "avg_logprob": -0.23437960007611444, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.04974821209907532}, {"id": 249, "seek": 125600, "start": 1272.0, "end": 1276.0, "text": " That's how you learn and have fun.", "tokens": [51164, 663, 311, 577, 291, 1466, 293, 362, 1019, 13, 51364], "temperature": 0.0, "avg_logprob": -0.23437960007611444, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.04974821209907532}, {"id": 250, "seek": 125600, "start": 1276.0, "end": 1279.0, "text": " And I hope you can do that also with chat.gpt.", "tokens": [51364, 400, 286, 1454, 291, 393, 360, 300, 611, 365, 5081, 13, 70, 79, 83, 13, 51514], "temperature": 0.0, "avg_logprob": -0.23437960007611444, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.04974821209907532}, {"id": 251, "seek": 125600, "start": 1279.0, "end": 1281.0, "text": " Thank you.", "tokens": [51514, 1044, 291, 13, 51614], "temperature": 0.0, "avg_logprob": -0.23437960007611444, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.04974821209907532}, {"id": 252, "seek": 128600, "start": 1286.0, "end": 1289.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50514], "temperature": 0.0, "avg_logprob": -0.6830453077952067, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9919141530990601}], "language": "en"}