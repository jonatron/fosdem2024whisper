{"text": " Thank you so much. My name is Likol Goczewski and I'm here with my colleague Mario Matias. I think I pronounced your name right. Yeah, you pronounced it very well. We work on open source project at Grafana called Grafana Vela about software engineers. We didn't practice this presentation much because we live on two different continents so you get what you get. It's always not too bad but yeah, we'll give it a shot. Let's go. So we will first do a very quick introduction to what is distributed tracing. I know most of you already know but just to try to get a common mindset even for people that is new to observability or to distributed tracing. Then we will explain a bit how it is implemented and how do we implement it in Grafana Vela using ABPF. So if you want to instrument a server, you might add an instrumentation library like for example the OpenTelemetry SDK and insert some instrumentation points in your server to get on each request a span containing data like the start and the end or some extra information about the request like client ID, the path of an HTTP, the response, etc. Then you can send that to an OpenTelemetry collector and visualize that. If we have a distributed service in which one service calls another, gets responses and so on, you could still do the same instrument each point and then send them to an OpenTelemetry collector for example. But the spans themselves could give information but separately may lack a lot of context. So if you get just a bunch of front end database back in the span separate, it will not be as useful as for example getting for each span which is the request that invoked that other request so you can see everything in context. This is what we say name distributed tracing or context propagation. In OpenTelemetry concretely we use the W3C standard that is using a trace parent header in the request. So you can insert into your request, you can insert headers with the trace ID and the parent span ID and then their services getting these or receiving those invocations can read this trace parent and add it to their own request. So that way you can always track the context. This is not any real SDK, any real language, it's just an example on how could you do it. You have a service and on each request you can read this trace parent, create your span, the part of the trace and when you have to call other services you will add this trace parent in the headers and then in the span. This can be manually done by code, be an SDK or this can be injected by your instrumentation or SDK agent like the OpenTelemetry Java or OpenTelemetry.net agents. Bayla, those are another or follows a similar approach especially for these services that are written in a language that is not so easy to instrument be an external agent. I'm thinking of for example, compiler languages like Go, Rust and C. In that case, Grafana Bayla can be deployed in the host, in the same host as the services you want to instrument and it will use the EVPF technology, we will talk later a bit about it, to hook and inspect the runtimes and libraries of your application or the functions of your application and as well some points of the Linux kernel. Then compose metrics, traces and forward them to your OpenTelemetry collector. What is CVPF? I mentioned it before. It's just in time, virtual machine that is shipped inside the Linux kernel. This allows you to efficiently hook programs to multiple events in the kernel, libraries and the user space programs. For example, Bayla can hook every time an HTTP request is received in the instrumented application. Bayla can execute immediately a piece of code, a probe and then inspect and even modify the memory, the runtime memory of your process or even the kernel. This way is able to know when request, service request starts and ends and even inspect some arguments about them. Bayla has two ways to provide a span information. One is to inspect at the language level. At the language level, we only currently support Go and it hooks user probes into the Go runtime and the Go libraries to inspect them. To support other languages, this is compiler languages but also Python, Ruby or other interpreted languages. It hooks K probes in several kernel functions and libraries to know when connections are started to read the arguments of the requests and the responses and so on. We are able to do that in Go. We are currently inspecting HTTP, HTTPS, GRPC, HTTP2 and soon SQL. At the kernel level, at the moment, we are inspecting HTTP and HTTPS but other protocols will come at some point. We will talk about how to provide the spans but Nicola will talk about how the context is propagated with Bayla. I think you can hear me here. You can hear me, right? Yeah, this is working. We showed a previous example where we had this done by manual introduction in that logic in the program about reading the trace information coming in on a request and then how we send that over which is effectively what most of the open telemetry SDK instrumentations do or the agents in Java or .NET, they do that injection for you automatically but we do it with eBPF so you don't have to have an SDK added to your packages or languages when that doesn't exist or languages where maybe your library dependencies don't quite work with the SDK because of different versions or it's not up to date or whatever the reason. We hook into the program like Mario mentioned in different ways and when a request starts we actually read the memory with eBPF and what is in that trace parent. If there isn't one, we'll generate one according to the W3 stack. Then what we do next is that we notice an outgoing call and then in that outgoing call, if we can find the information about the headers, we will inject the outgoing trace header just like the SDK would do. This is what happens in Go currently with Vela. This is exactly what we do. Now internally, how this all works? Well, to make sure that we can tag an incoming request on a server accepted something like slash ping for example and it did an outgoing request to slash ping me too and in that case we need to track that this incoming request matches this outgoing request by the call maybe async. Maybe somebody wrote a library and said, well, I don't want to wait for this request. I just want to do it async for whatever the reason. I'm using some reactive library. In that case for Go, we track the Go routine creation and termination essentially. Because the Go runtime and the standard libraries are very standardized and everybody uses that, we're able to do this kind of stuff. It doesn't need to be the first argument, needs to be the context. None of that stuff. We just track Go routine creation and we're able to match it later on. That's how we propagate the context. Now for the other languages, we thought, well, how are we going to do that for other languages? People use number of libraries. How do you do this on compile languages? Somebody does just think on time compile language. It's kind of hard. For that we wrote additional support that does something more sneaky or if you will, something more interesting. Land 2 servers or two processes talk to each other over HTTP, for example. They have a unique pair of information and they identify every connection. I have a client, opens a remote connection to a server. It has a source port, which is typically a femoral. I have a destination port, which is a server port. When we see that connection pair, we use it as a unique key and we say, we'll store it in the eBPF map. Then when the server on the other side gets that request, they look up that map and say, well, I have this connection pair. Does that match any client that made this connection? It does require that one single baler monitors both processes. If that is true, then we can actually tag these requests between servers without actually using this transparent propagation. For languages where we haven't written additional support to inject the headers information, we use this as a backup option. This context propagation correlates internally requests through the kernel. Here's an example. We start the client call. It may read the transparent information that was present from a previous call, but if there isn't one, it's just going to generate it right there in eBPF and then store that information. Then later on, when another server request happens based on the client call, we'll read that map, read the transparent information, create the spans, just like if you will, that transparent logic flew through the HTTP headers. More or less the same. There's restrictions, of course. Obviously, for this to actually work, we have to have a single node. Now, these eBPF maps can be shared on a volume and maybe there's a way to use that, but we don't do that and support that right now. This is also not released yet, so we just have it in the main branch. It's one of the newer things we added. But with this, I think I'm more of a person. I'll believe it when I see it. I think we want to try to do a demo to show you everything's running off the laptop that Mario has here. We're not going to connect to any cloud services, but what we want to demonstrate is a few HTTP services here. And GRPC also. They're using GRPC in this case. They're returning Go. We're going to have one Bayline instance. Look at all of them. We're going to use this little tool that actually Fabian made, this little Docker Compose LGTN, which has the full Grafana stack with all our open source products, with the OpenTelemetry collector setup that it can ingest and do traces, metrics, and everything you need. Very convenient for testing. Very convenient for testing or spinning up your own Dockerfana cluster at home. So it's just one Dockerfile with all of it. I also wanted to mention, because we didn't say, it's obvious the presentation is about distributed traces, but Bayline does support metrics too. So HTTP metrics were included from the Star Door product. Traces distributed traces is some of the newer stuff we're working on. Okay, so for this demo, we will show a simple distributed application. It means a synthetic application is just a frontend sending a request to a backend, and the backend asking for distributing some load on the workers and then getting a response. Do you need to hold that? No, it's okay. It's okay. Thank you. Then I have added everything into a Docker compose file just for facilitating the demo in my laptop. So we have this OpenTelemetry collector, which is the hotel LGTM container that Fabian did. And we just dropped Bayline as a container. You can drop Bayline there as a host process, but for convenience also as a container. We need to give access to the pit name space of the host, because it will have to instrument all the processes in that host, and also privilege access because loading EVPF programs requires administrative privileges. Then we set here some the OpenTelemetry endpoint in a standard configuration. Bayline accepts the standard OpenTelemetry configurations for setting up many values. And also we are providing a configuration file. Basically here we say how to group the HTTP routes. For example, there is a route that calculates a factorial, and you will pass in the request, you will set factorial and the number to calculate. We don't get a cardinality explosion because we don't want to create a different route value for every number we calculate. So we say, okay, just group all the URLs matching this pattern, group them in factorial number. And then we tell Bayline how to discover the services to instrument. We have a frontend, a backend, and a worker container, and then we pass that. This accepts any regular expression. So if we say just a dot, it will instrument or try to instrument all the processes in the host. But in that case it will also instrument some parts of the Docker API, the Docker Compose API. So to not generate noise, we are just providing the services we want to instrument. And let me then run this Docker Compose file. Okay, this application is a very simple application. It's a huge factorial calculator application. I will just write a number, and it will calculate the factorial. And if you need more numbers, okay, you calculate. Boom! This is an error introduced as on purpose because I also use this application to track errors from Bayline. But it usually works. Then, doing that, we have, Bayline was already running. We have been generating some traces. So let me go to the local Grafana. Let's see. I go to, for example, explore. Here I selected the tempo, and let me search for all the traces. Okay, beautiful. It's strange because here we can see that Bayline... Oh, yeah? Okay, let me check. No data. Okay, it happens in the best families. No, but we have this... I mean, it is able to... Okay, I don't know what happened. But... For sure, it's a book in Grafana. So I have here many, many requests. Or many traces. Let me just instrument this, submit trace, which is the one that triggers the backend and the workers. If we enter here, you will see the trace information. How the front end invokes the backend. You can track also an internal status of the request, like how much time the queue is in... Or the request is in queue or is being processed. And you can see how, for example, the backend might invoke the worker multiple times. So we got distributed traces automatically. We can even see the node graph of all the requests. How this process invokes or the relation of all the traces as a graph. How the front end as a server, because we instrument either server or client side spans. How the front end invokes the backend, the backend invokes different workers and so on. I just want to add something here. So we're here, if you see, when you look at the Bayla stuff that we produce, we produce these two spans for some of the server requests. We have in queue and processing. And for most people, that's like, what is this two things? Like why are you tracking two times? And if you have a typical application server that saves with an in-go, and you accept the request and as soon as that happens, go or launch a go routine for this. But how long before this go routine gets scheduled on a physical thread, which is M in the world of go, and how long before this physical thread actually gets CPU time? So from a traditional instrumentation, you instrument the handler of the server request. This handler of the server request is the time the handler started running, not the time that the runtime accepted the request coming in from the kernel. Well, at the EVPF, because we're at a low level, we can actually track that time. We can actually see where the request actually came from the kernel, when the go routine was launched, and when you finally got the handler to run. So in a situation where you have a server which is overloaded and it's not able to serve the request, you'll get the actual request time, much closer to what the client sees on the other end. Rather than the fake time, which is what the application server would see normal. Okay, so that was the demo. Let's summarize something that is that, using EVPF, you can capture distributed traces, as we, as Nicole explained it, with some limitations. The advantage is that it requires almost no effort from the developer or operator, in the sense that you don't need to reconfigure your service, you don't need to change the code, you don't need to redeploy, just drop it and get whatever Rela can get. Yeah, and it's, another conclusion is combining this packet tracing with language level support, is what we, we allow Bayla get those distributed traces. So if you like it and want to give a try, Bayla is available to download freely, to test it. You can, you can connect to our GitHub page, or, and then you will see instructions and links to the documentation or the main open source page of Bayla. Yeah, and on the GitHub page is what we start with, we have a link to our community Slack, if you want to chat with us, and we also are soon going to start organizing the community call. So once every month we have a call where you can just join in and chat or yell at us, for whatever reason, but yeah, that's it. Thank you. Thanks a lot. Oh, so many questions. I'm running. You said that when you're tracing in Go, you, you are, you are tracing the coroutines that are, that are handling requests, but in Go you don't have ideas of these coroutines and you don't have the relationship between them. And to, to make it worse, the go around time actually reuses coroutines for something completely different. So how do you, how do you do that without constantly handing pretty much all the coroutines all the time in order to get your trace? Yeah, okay. So like with EVPF you get superpowers. So from a regular goal developer perspective, you never actually have the access to this information. Yeah, for whatever reason, they won't give it to you. But with EVPF, I attached the go runtime. So the address in memory of the go routine is my ID. Now I can tell when the go routine starts and when it gets parked back, when it's reused for something else, it can be reused and that's fine. But at that time I'll clear up all the information because I know the go routine is done. Because like superpowers. Hey, thank you for your talk. I'm one of those guys that manage a lot of infrastructure in code in general. And when you say that, hey, you just have to eat that and just work sort of a box, it's kind of scares me because potentially it can cause problems. And one of the issues that we saw with both kind of solutions usually is if you inject into request a tracing header, potentially the request might be changed. And some protocols do signing and request like AWS signature free, for example. And they don't really like you injecting headers in the middle of request, especially at a lower level. So how do you envision if you have some kind of like agent in the code itself, then you can work on that by disabling the tracing on both specific endpoints. But if you do that at a lower level, then you don't really have a visibility to be able to disable that or recognize that you are creating a request to such a back end. How do you envision like working around those issues in the future? Because this is one example, but this will happen many, many times. Yeah, yeah. So that's true. So if you need sign some IDs and whatever, it's not letting you change the header information, then disable that feature. Don't use what we do right now for propagating using the headers. Use the black box. This is the back boxes are sort of the full back. We've been toying with the idea that maybe in the future we'll let it work with an external storage of some kind that we can actually make past the one node restriction we have with the black box right now. But that's the very reason we're designing for because in so many environments, injecting the header information is just not possible. I'm dealing with interpreter language. No compiled methods, no dice. So I can't do anything with you. Thanks. Good question. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.92, "text": " Thank you so much.", "tokens": [50364, 1044, 291, 370, 709, 13, 50710], "temperature": 0.0, "avg_logprob": -0.3714316782325205, "compression_ratio": 1.5020746887966805, "no_speech_prob": 0.32594287395477295}, {"id": 1, "seek": 0, "start": 6.92, "end": 11.4, "text": " My name is Likol Goczewski and I'm here with my colleague Mario Matias.", "tokens": [50710, 1222, 1315, 307, 441, 1035, 401, 460, 905, 43551, 18020, 293, 286, 478, 510, 365, 452, 13532, 9343, 6789, 4609, 13, 50934], "temperature": 0.0, "avg_logprob": -0.3714316782325205, "compression_ratio": 1.5020746887966805, "no_speech_prob": 0.32594287395477295}, {"id": 2, "seek": 0, "start": 11.4, "end": 12.88, "text": " I think I pronounced your name right.", "tokens": [50934, 286, 519, 286, 23155, 428, 1315, 558, 13, 51008], "temperature": 0.0, "avg_logprob": -0.3714316782325205, "compression_ratio": 1.5020746887966805, "no_speech_prob": 0.32594287395477295}, {"id": 3, "seek": 0, "start": 12.88, "end": 15.76, "text": " Yeah, you pronounced it very well.", "tokens": [51008, 865, 11, 291, 23155, 309, 588, 731, 13, 51152], "temperature": 0.0, "avg_logprob": -0.3714316782325205, "compression_ratio": 1.5020746887966805, "no_speech_prob": 0.32594287395477295}, {"id": 4, "seek": 0, "start": 15.76, "end": 21.96, "text": " We work on open source project at Grafana called Grafana Vela about software engineers.", "tokens": [51152, 492, 589, 322, 1269, 4009, 1716, 412, 8985, 69, 2095, 1219, 8985, 69, 2095, 691, 4053, 466, 4722, 11955, 13, 51462], "temperature": 0.0, "avg_logprob": -0.3714316782325205, "compression_ratio": 1.5020746887966805, "no_speech_prob": 0.32594287395477295}, {"id": 5, "seek": 0, "start": 21.96, "end": 25.32, "text": " We didn't practice this presentation much because we live on two different continents", "tokens": [51462, 492, 994, 380, 3124, 341, 5860, 709, 570, 321, 1621, 322, 732, 819, 38598, 51630], "temperature": 0.0, "avg_logprob": -0.3714316782325205, "compression_ratio": 1.5020746887966805, "no_speech_prob": 0.32594287395477295}, {"id": 6, "seek": 0, "start": 25.32, "end": 27.48, "text": " so you get what you get.", "tokens": [51630, 370, 291, 483, 437, 291, 483, 13, 51738], "temperature": 0.0, "avg_logprob": -0.3714316782325205, "compression_ratio": 1.5020746887966805, "no_speech_prob": 0.32594287395477295}, {"id": 7, "seek": 2748, "start": 27.48, "end": 31.64, "text": " It's always not too bad but yeah, we'll give it a shot.", "tokens": [50364, 467, 311, 1009, 406, 886, 1578, 457, 1338, 11, 321, 603, 976, 309, 257, 3347, 13, 50572], "temperature": 0.0, "avg_logprob": -0.18404934193828318, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.13523104786872864}, {"id": 8, "seek": 2748, "start": 31.64, "end": 34.16, "text": " Let's go.", "tokens": [50572, 961, 311, 352, 13, 50698], "temperature": 0.0, "avg_logprob": -0.18404934193828318, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.13523104786872864}, {"id": 9, "seek": 2748, "start": 34.16, "end": 39.16, "text": " So we will first do a very quick introduction to what is distributed tracing.", "tokens": [50698, 407, 321, 486, 700, 360, 257, 588, 1702, 9339, 281, 437, 307, 12631, 25262, 13, 50948], "temperature": 0.0, "avg_logprob": -0.18404934193828318, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.13523104786872864}, {"id": 10, "seek": 2748, "start": 39.16, "end": 46.28, "text": " I know most of you already know but just to try to get a common mindset even for people", "tokens": [50948, 286, 458, 881, 295, 291, 1217, 458, 457, 445, 281, 853, 281, 483, 257, 2689, 12543, 754, 337, 561, 51304], "temperature": 0.0, "avg_logprob": -0.18404934193828318, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.13523104786872864}, {"id": 11, "seek": 2748, "start": 46.28, "end": 50.480000000000004, "text": " that is new to observability or to distributed tracing.", "tokens": [51304, 300, 307, 777, 281, 9951, 2310, 420, 281, 12631, 25262, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18404934193828318, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.13523104786872864}, {"id": 12, "seek": 2748, "start": 50.480000000000004, "end": 57.16, "text": " Then we will explain a bit how it is implemented and how do we implement it in Grafana Vela", "tokens": [51514, 1396, 321, 486, 2903, 257, 857, 577, 309, 307, 12270, 293, 577, 360, 321, 4445, 309, 294, 8985, 69, 2095, 691, 4053, 51848], "temperature": 0.0, "avg_logprob": -0.18404934193828318, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.13523104786872864}, {"id": 13, "seek": 5716, "start": 57.16, "end": 60.44, "text": " using ABPF.", "tokens": [50364, 1228, 13838, 47, 37, 13, 50528], "temperature": 0.0, "avg_logprob": -0.25976085662841797, "compression_ratio": 1.373134328358209, "no_speech_prob": 0.06760478019714355}, {"id": 14, "seek": 5716, "start": 60.44, "end": 70.52, "text": " So if you want to instrument a server, you might add an instrumentation library like", "tokens": [50528, 407, 498, 291, 528, 281, 7198, 257, 7154, 11, 291, 1062, 909, 364, 7198, 399, 6405, 411, 51032], "temperature": 0.0, "avg_logprob": -0.25976085662841797, "compression_ratio": 1.373134328358209, "no_speech_prob": 0.06760478019714355}, {"id": 15, "seek": 5716, "start": 70.52, "end": 81.72, "text": " for example the OpenTelemetry SDK and insert some instrumentation points in your server", "tokens": [51032, 337, 1365, 264, 7238, 14233, 306, 5537, 627, 37135, 293, 8969, 512, 7198, 399, 2793, 294, 428, 7154, 51592], "temperature": 0.0, "avg_logprob": -0.25976085662841797, "compression_ratio": 1.373134328358209, "no_speech_prob": 0.06760478019714355}, {"id": 16, "seek": 8172, "start": 81.72, "end": 90.52, "text": " to get on each request a span containing data like the start and the end or some extra", "tokens": [50364, 281, 483, 322, 1184, 5308, 257, 16174, 19273, 1412, 411, 264, 722, 293, 264, 917, 420, 512, 2857, 50804], "temperature": 0.0, "avg_logprob": -0.19163387052474484, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.19209206104278564}, {"id": 17, "seek": 8172, "start": 90.52, "end": 96.72, "text": " information about the request like client ID, the path of an HTTP, the response, etc.", "tokens": [50804, 1589, 466, 264, 5308, 411, 6423, 7348, 11, 264, 3100, 295, 364, 33283, 11, 264, 4134, 11, 5183, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19163387052474484, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.19209206104278564}, {"id": 18, "seek": 8172, "start": 96.72, "end": 102.8, "text": " Then you can send that to an OpenTelemetry collector and visualize that.", "tokens": [51114, 1396, 291, 393, 2845, 300, 281, 364, 7238, 14233, 306, 5537, 627, 23960, 293, 23273, 300, 13, 51418], "temperature": 0.0, "avg_logprob": -0.19163387052474484, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.19209206104278564}, {"id": 19, "seek": 10280, "start": 102.92, "end": 112.36, "text": " If we have a distributed service in which one service calls another, gets responses and", "tokens": [50370, 759, 321, 362, 257, 12631, 2643, 294, 597, 472, 2643, 5498, 1071, 11, 2170, 13019, 293, 50842], "temperature": 0.0, "avg_logprob": -0.25760216806449143, "compression_ratio": 1.4109589041095891, "no_speech_prob": 0.2821692228317261}, {"id": 20, "seek": 10280, "start": 112.36, "end": 119.72, "text": " so on, you could still do the same instrument each point and then send them to an OpenTelemetry", "tokens": [50842, 370, 322, 11, 291, 727, 920, 360, 264, 912, 7198, 1184, 935, 293, 550, 2845, 552, 281, 364, 7238, 14233, 306, 5537, 627, 51210], "temperature": 0.0, "avg_logprob": -0.25760216806449143, "compression_ratio": 1.4109589041095891, "no_speech_prob": 0.2821692228317261}, {"id": 21, "seek": 10280, "start": 119.72, "end": 123.44, "text": " collector for example.", "tokens": [51210, 23960, 337, 1365, 13, 51396], "temperature": 0.0, "avg_logprob": -0.25760216806449143, "compression_ratio": 1.4109589041095891, "no_speech_prob": 0.2821692228317261}, {"id": 22, "seek": 12344, "start": 123.44, "end": 133.92, "text": " But the spans themselves could give information but separately may lack a lot of context.", "tokens": [50364, 583, 264, 44086, 2969, 727, 976, 1589, 457, 14759, 815, 5011, 257, 688, 295, 4319, 13, 50888], "temperature": 0.0, "avg_logprob": -0.2043059564405872, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.49230295419692993}, {"id": 23, "seek": 12344, "start": 133.92, "end": 140.56, "text": " So if you get just a bunch of front end database back in the span separate, it will not be", "tokens": [50888, 407, 498, 291, 483, 445, 257, 3840, 295, 1868, 917, 8149, 646, 294, 264, 16174, 4994, 11, 309, 486, 406, 312, 51220], "temperature": 0.0, "avg_logprob": -0.2043059564405872, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.49230295419692993}, {"id": 24, "seek": 12344, "start": 140.56, "end": 150.76, "text": " as useful as for example getting for each span which is the request that invoked that", "tokens": [51220, 382, 4420, 382, 337, 1365, 1242, 337, 1184, 16174, 597, 307, 264, 5308, 300, 1048, 9511, 300, 51730], "temperature": 0.0, "avg_logprob": -0.2043059564405872, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.49230295419692993}, {"id": 25, "seek": 15076, "start": 150.76, "end": 154.56, "text": " other request so you can see everything in context.", "tokens": [50364, 661, 5308, 370, 291, 393, 536, 1203, 294, 4319, 13, 50554], "temperature": 0.0, "avg_logprob": -0.18888565589641704, "compression_ratio": 1.4177215189873418, "no_speech_prob": 0.26556211709976196}, {"id": 26, "seek": 15076, "start": 154.56, "end": 162.23999999999998, "text": " This is what we say name distributed tracing or context propagation.", "tokens": [50554, 639, 307, 437, 321, 584, 1315, 12631, 25262, 420, 4319, 38377, 13, 50938], "temperature": 0.0, "avg_logprob": -0.18888565589641704, "compression_ratio": 1.4177215189873418, "no_speech_prob": 0.26556211709976196}, {"id": 27, "seek": 15076, "start": 162.23999999999998, "end": 172.44, "text": " In OpenTelemetry concretely we use the W3C standard that is using a trace parent header", "tokens": [50938, 682, 7238, 14233, 306, 5537, 627, 39481, 736, 321, 764, 264, 343, 18, 34, 3832, 300, 307, 1228, 257, 13508, 2596, 23117, 51448], "temperature": 0.0, "avg_logprob": -0.18888565589641704, "compression_ratio": 1.4177215189873418, "no_speech_prob": 0.26556211709976196}, {"id": 28, "seek": 15076, "start": 172.44, "end": 173.68, "text": " in the request.", "tokens": [51448, 294, 264, 5308, 13, 51510], "temperature": 0.0, "avg_logprob": -0.18888565589641704, "compression_ratio": 1.4177215189873418, "no_speech_prob": 0.26556211709976196}, {"id": 29, "seek": 17368, "start": 173.68, "end": 181.32, "text": " So you can insert into your request, you can insert headers with the trace ID and the", "tokens": [50364, 407, 291, 393, 8969, 666, 428, 5308, 11, 291, 393, 8969, 45101, 365, 264, 13508, 7348, 293, 264, 50746], "temperature": 0.0, "avg_logprob": -0.19687847514728923, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.1238788366317749}, {"id": 30, "seek": 17368, "start": 181.32, "end": 188.52, "text": " parent span ID and then their services getting these or receiving those invocations can read", "tokens": [50746, 2596, 16174, 7348, 293, 550, 641, 3328, 1242, 613, 420, 10040, 729, 1048, 905, 763, 393, 1401, 51106], "temperature": 0.0, "avg_logprob": -0.19687847514728923, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.1238788366317749}, {"id": 31, "seek": 17368, "start": 188.52, "end": 191.52, "text": " this trace parent and add it to their own request.", "tokens": [51106, 341, 13508, 2596, 293, 909, 309, 281, 641, 1065, 5308, 13, 51256], "temperature": 0.0, "avg_logprob": -0.19687847514728923, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.1238788366317749}, {"id": 32, "seek": 17368, "start": 191.52, "end": 195.64000000000001, "text": " So that way you can always track the context.", "tokens": [51256, 407, 300, 636, 291, 393, 1009, 2837, 264, 4319, 13, 51462], "temperature": 0.0, "avg_logprob": -0.19687847514728923, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.1238788366317749}, {"id": 33, "seek": 17368, "start": 195.64000000000001, "end": 201.84, "text": " This is not any real SDK, any real language, it's just an example on how could you do it.", "tokens": [51462, 639, 307, 406, 604, 957, 37135, 11, 604, 957, 2856, 11, 309, 311, 445, 364, 1365, 322, 577, 727, 291, 360, 309, 13, 51772], "temperature": 0.0, "avg_logprob": -0.19687847514728923, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.1238788366317749}, {"id": 34, "seek": 20184, "start": 201.84, "end": 209.76, "text": " You have a service and on each request you can read this trace parent, create your span,", "tokens": [50364, 509, 362, 257, 2643, 293, 322, 1184, 5308, 291, 393, 1401, 341, 13508, 2596, 11, 1884, 428, 16174, 11, 50760], "temperature": 0.0, "avg_logprob": -0.16961345424899807, "compression_ratio": 1.7049180327868851, "no_speech_prob": 0.16214945912361145}, {"id": 35, "seek": 20184, "start": 209.76, "end": 216.36, "text": " the part of the trace and when you have to call other services you will add this trace", "tokens": [50760, 264, 644, 295, 264, 13508, 293, 562, 291, 362, 281, 818, 661, 3328, 291, 486, 909, 341, 13508, 51090], "temperature": 0.0, "avg_logprob": -0.16961345424899807, "compression_ratio": 1.7049180327868851, "no_speech_prob": 0.16214945912361145}, {"id": 36, "seek": 20184, "start": 216.36, "end": 221.0, "text": " parent in the headers and then in the span.", "tokens": [51090, 2596, 294, 264, 45101, 293, 550, 294, 264, 16174, 13, 51322], "temperature": 0.0, "avg_logprob": -0.16961345424899807, "compression_ratio": 1.7049180327868851, "no_speech_prob": 0.16214945912361145}, {"id": 37, "seek": 20184, "start": 221.0, "end": 226.92000000000002, "text": " This can be manually done by code, be an SDK or this can be injected by your instrumentation", "tokens": [51322, 639, 393, 312, 16945, 1096, 538, 3089, 11, 312, 364, 37135, 420, 341, 393, 312, 36967, 538, 428, 7198, 399, 51618], "temperature": 0.0, "avg_logprob": -0.16961345424899807, "compression_ratio": 1.7049180327868851, "no_speech_prob": 0.16214945912361145}, {"id": 38, "seek": 22692, "start": 226.92, "end": 233.44, "text": " or SDK agent like the OpenTelemetry Java or OpenTelemetry.net agents.", "tokens": [50364, 420, 37135, 9461, 411, 264, 7238, 14233, 306, 5537, 627, 10745, 420, 7238, 14233, 306, 5537, 627, 13, 7129, 12554, 13, 50690], "temperature": 0.0, "avg_logprob": -0.2703996417166173, "compression_ratio": 1.5159817351598173, "no_speech_prob": 0.15295156836509705}, {"id": 39, "seek": 22692, "start": 233.44, "end": 242.44, "text": " Bayla, those are another or follows a similar approach especially for these services that", "tokens": [50690, 7840, 875, 11, 729, 366, 1071, 420, 10002, 257, 2531, 3109, 2318, 337, 613, 3328, 300, 51140], "temperature": 0.0, "avg_logprob": -0.2703996417166173, "compression_ratio": 1.5159817351598173, "no_speech_prob": 0.15295156836509705}, {"id": 40, "seek": 22692, "start": 242.44, "end": 248.07999999999998, "text": " are written in a language that is not so easy to instrument be an external agent.", "tokens": [51140, 366, 3720, 294, 257, 2856, 300, 307, 406, 370, 1858, 281, 7198, 312, 364, 8320, 9461, 13, 51422], "temperature": 0.0, "avg_logprob": -0.2703996417166173, "compression_ratio": 1.5159817351598173, "no_speech_prob": 0.15295156836509705}, {"id": 41, "seek": 22692, "start": 248.07999999999998, "end": 256.28, "text": " I'm thinking of for example, compiler languages like Go, Rust and C. In that case, Grafana", "tokens": [51422, 286, 478, 1953, 295, 337, 1365, 11, 31958, 8650, 411, 1037, 11, 34952, 293, 383, 13, 682, 300, 1389, 11, 8985, 69, 2095, 51832], "temperature": 0.0, "avg_logprob": -0.2703996417166173, "compression_ratio": 1.5159817351598173, "no_speech_prob": 0.15295156836509705}, {"id": 42, "seek": 25628, "start": 256.35999999999996, "end": 262.76, "text": " Bayla can be deployed in the host, in the same host as the services you want to instrument", "tokens": [50368, 7840, 875, 393, 312, 17826, 294, 264, 3975, 11, 294, 264, 912, 3975, 382, 264, 3328, 291, 528, 281, 7198, 50688], "temperature": 0.0, "avg_logprob": -0.22023030070515423, "compression_ratio": 1.6030927835051547, "no_speech_prob": 0.036988407373428345}, {"id": 43, "seek": 25628, "start": 262.76, "end": 272.55999999999995, "text": " and it will use the EVPF technology, we will talk later a bit about it, to hook and inspect", "tokens": [50688, 293, 309, 486, 764, 264, 15733, 47, 37, 2899, 11, 321, 486, 751, 1780, 257, 857, 466, 309, 11, 281, 6328, 293, 15018, 51178], "temperature": 0.0, "avg_logprob": -0.22023030070515423, "compression_ratio": 1.6030927835051547, "no_speech_prob": 0.036988407373428345}, {"id": 44, "seek": 25628, "start": 272.55999999999995, "end": 278.96, "text": " the runtimes and libraries of your application or the functions of your application and as", "tokens": [51178, 264, 49435, 1532, 293, 15148, 295, 428, 3861, 420, 264, 6828, 295, 428, 3861, 293, 382, 51498], "temperature": 0.0, "avg_logprob": -0.22023030070515423, "compression_ratio": 1.6030927835051547, "no_speech_prob": 0.036988407373428345}, {"id": 45, "seek": 25628, "start": 278.96, "end": 282.44, "text": " well some points of the Linux kernel.", "tokens": [51498, 731, 512, 2793, 295, 264, 18734, 28256, 13, 51672], "temperature": 0.0, "avg_logprob": -0.22023030070515423, "compression_ratio": 1.6030927835051547, "no_speech_prob": 0.036988407373428345}, {"id": 46, "seek": 28244, "start": 282.44, "end": 289.96, "text": " Then compose metrics, traces and forward them to your OpenTelemetry collector.", "tokens": [50364, 1396, 35925, 16367, 11, 26076, 293, 2128, 552, 281, 428, 7238, 14233, 306, 5537, 627, 23960, 13, 50740], "temperature": 0.0, "avg_logprob": -0.2968051971927766, "compression_ratio": 1.3421052631578947, "no_speech_prob": 0.22112835943698883}, {"id": 47, "seek": 28244, "start": 289.96, "end": 299.6, "text": " What is CVPF? I mentioned it before. It's just in time, virtual machine that is shipped", "tokens": [50740, 708, 307, 22995, 47, 37, 30, 286, 2835, 309, 949, 13, 467, 311, 445, 294, 565, 11, 6374, 3479, 300, 307, 25312, 51222], "temperature": 0.0, "avg_logprob": -0.2968051971927766, "compression_ratio": 1.3421052631578947, "no_speech_prob": 0.22112835943698883}, {"id": 48, "seek": 28244, "start": 299.6, "end": 310.4, "text": " inside the Linux kernel. This allows you to efficiently hook programs to multiple events", "tokens": [51222, 1854, 264, 18734, 28256, 13, 639, 4045, 291, 281, 19621, 6328, 4268, 281, 3866, 3931, 51762], "temperature": 0.0, "avg_logprob": -0.2968051971927766, "compression_ratio": 1.3421052631578947, "no_speech_prob": 0.22112835943698883}, {"id": 49, "seek": 31040, "start": 310.44, "end": 316.28, "text": " in the kernel, libraries and the user space programs. For example, Bayla can hook every", "tokens": [50366, 294, 264, 28256, 11, 15148, 293, 264, 4195, 1901, 4268, 13, 1171, 1365, 11, 7840, 875, 393, 6328, 633, 50658], "temperature": 0.0, "avg_logprob": -0.21482676551455543, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.04014599323272705}, {"id": 50, "seek": 31040, "start": 316.28, "end": 323.28, "text": " time an HTTP request is received in the instrumented application. Bayla can execute immediately", "tokens": [50658, 565, 364, 33283, 5308, 307, 4613, 294, 264, 7198, 292, 3861, 13, 7840, 875, 393, 14483, 4258, 51008], "temperature": 0.0, "avg_logprob": -0.21482676551455543, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.04014599323272705}, {"id": 51, "seek": 31040, "start": 323.28, "end": 332.44, "text": " a piece of code, a probe and then inspect and even modify the memory, the runtime memory", "tokens": [51008, 257, 2522, 295, 3089, 11, 257, 22715, 293, 550, 15018, 293, 754, 16927, 264, 4675, 11, 264, 34474, 4675, 51466], "temperature": 0.0, "avg_logprob": -0.21482676551455543, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.04014599323272705}, {"id": 52, "seek": 33244, "start": 332.44, "end": 343.84, "text": " of your process or even the kernel. This way is able to know when request, service request", "tokens": [50364, 295, 428, 1399, 420, 754, 264, 28256, 13, 639, 636, 307, 1075, 281, 458, 562, 5308, 11, 2643, 5308, 50934], "temperature": 0.0, "avg_logprob": -0.3324399158872407, "compression_ratio": 1.478527607361963, "no_speech_prob": 0.06337905675172806}, {"id": 53, "seek": 33244, "start": 343.84, "end": 348.88, "text": " starts and ends and even inspect some arguments about them.", "tokens": [50934, 3719, 293, 5314, 293, 754, 15018, 512, 12869, 466, 552, 13, 51186], "temperature": 0.0, "avg_logprob": -0.3324399158872407, "compression_ratio": 1.478527607361963, "no_speech_prob": 0.06337905675172806}, {"id": 54, "seek": 33244, "start": 348.88, "end": 360.88, "text": " Bayla has two ways to provide a span information. One is to inspect at the language level.", "tokens": [51186, 7840, 875, 575, 732, 2098, 281, 2893, 257, 16174, 1589, 13, 1485, 307, 281, 15018, 412, 264, 2856, 1496, 13, 51786], "temperature": 0.0, "avg_logprob": -0.3324399158872407, "compression_ratio": 1.478527607361963, "no_speech_prob": 0.06337905675172806}, {"id": 55, "seek": 36088, "start": 361.32, "end": 368.68, "text": " At the language level, we only currently support Go and it hooks user probes into the Go runtime", "tokens": [50386, 1711, 264, 2856, 1496, 11, 321, 787, 4362, 1406, 1037, 293, 309, 26485, 4195, 1239, 279, 666, 264, 1037, 34474, 50754], "temperature": 0.0, "avg_logprob": -0.2765302311290394, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.023564424365758896}, {"id": 56, "seek": 36088, "start": 368.68, "end": 376.15999999999997, "text": " and the Go libraries to inspect them. To support other languages, this is compiler languages", "tokens": [50754, 293, 264, 1037, 15148, 281, 15018, 552, 13, 1407, 1406, 661, 8650, 11, 341, 307, 31958, 8650, 51128], "temperature": 0.0, "avg_logprob": -0.2765302311290394, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.023564424365758896}, {"id": 57, "seek": 36088, "start": 376.15999999999997, "end": 383.28, "text": " but also Python, Ruby or other interpreted languages.", "tokens": [51128, 457, 611, 15329, 11, 19907, 420, 661, 26749, 8650, 13, 51484], "temperature": 0.0, "avg_logprob": -0.2765302311290394, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.023564424365758896}, {"id": 58, "seek": 38328, "start": 383.67999999999995, "end": 391.08, "text": " It hooks K probes in several kernel functions and libraries to know when connections are", "tokens": [50384, 467, 26485, 591, 1239, 279, 294, 2940, 28256, 6828, 293, 15148, 281, 458, 562, 9271, 366, 50754], "temperature": 0.0, "avg_logprob": -0.28588624196509793, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.29821211099624634}, {"id": 59, "seek": 38328, "start": 391.08, "end": 400.67999999999995, "text": " started to read the arguments of the requests and the responses and so on. We are able to do that", "tokens": [50754, 1409, 281, 1401, 264, 12869, 295, 264, 12475, 293, 264, 13019, 293, 370, 322, 13, 492, 366, 1075, 281, 360, 300, 51234], "temperature": 0.0, "avg_logprob": -0.28588624196509793, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.29821211099624634}, {"id": 60, "seek": 38328, "start": 400.67999999999995, "end": 410.64, "text": " in Go. We are currently inspecting HTTP, HTTPS, GRPC, HTTP2 and soon SQL. At the kernel level,", "tokens": [51234, 294, 1037, 13, 492, 366, 4362, 15018, 278, 33283, 11, 11751, 51, 6273, 11, 10903, 12986, 11, 33283, 17, 293, 2321, 19200, 13, 1711, 264, 28256, 1496, 11, 51732], "temperature": 0.0, "avg_logprob": -0.28588624196509793, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.29821211099624634}, {"id": 61, "seek": 41064, "start": 410.64, "end": 420.03999999999996, "text": " at the moment, we are inspecting HTTP and HTTPS but other protocols will come at some point.", "tokens": [50364, 412, 264, 1623, 11, 321, 366, 15018, 278, 33283, 293, 11751, 51, 6273, 457, 661, 20618, 486, 808, 412, 512, 935, 13, 50834], "temperature": 0.0, "avg_logprob": -0.3250968405540953, "compression_ratio": 1.4330708661417322, "no_speech_prob": 0.016119617968797684}, {"id": 62, "seek": 41064, "start": 424.03999999999996, "end": 433.56, "text": " We will talk about how to provide the spans but Nicola will talk about how the context is", "tokens": [51034, 492, 486, 751, 466, 577, 281, 2893, 264, 44086, 457, 14776, 4711, 486, 751, 466, 577, 264, 4319, 307, 51510], "temperature": 0.0, "avg_logprob": -0.3250968405540953, "compression_ratio": 1.4330708661417322, "no_speech_prob": 0.016119617968797684}, {"id": 63, "seek": 43356, "start": 433.56, "end": 435.96, "text": " propagated with Bayla.", "tokens": [50364, 12425, 770, 365, 7840, 875, 13, 50484], "temperature": 0.0, "avg_logprob": -0.35471069678831635, "compression_ratio": 1.5355648535564854, "no_speech_prob": 0.23790986835956573}, {"id": 64, "seek": 43356, "start": 435.96, "end": 440.96, "text": " I think you can hear me here. You can hear me, right?", "tokens": [50484, 286, 519, 291, 393, 1568, 385, 510, 13, 509, 393, 1568, 385, 11, 558, 30, 50734], "temperature": 0.0, "avg_logprob": -0.35471069678831635, "compression_ratio": 1.5355648535564854, "no_speech_prob": 0.23790986835956573}, {"id": 65, "seek": 43356, "start": 440.96, "end": 449.96, "text": " Yeah, this is working. We showed a previous example where we had this done by manual introduction", "tokens": [50734, 865, 11, 341, 307, 1364, 13, 492, 4712, 257, 3894, 1365, 689, 321, 632, 341, 1096, 538, 9688, 9339, 51184], "temperature": 0.0, "avg_logprob": -0.35471069678831635, "compression_ratio": 1.5355648535564854, "no_speech_prob": 0.23790986835956573}, {"id": 66, "seek": 43356, "start": 449.96, "end": 455.96, "text": " in that logic in the program about reading the trace information coming in on a request", "tokens": [51184, 294, 300, 9952, 294, 264, 1461, 466, 3760, 264, 13508, 1589, 1348, 294, 322, 257, 5308, 51484], "temperature": 0.0, "avg_logprob": -0.35471069678831635, "compression_ratio": 1.5355648535564854, "no_speech_prob": 0.23790986835956573}, {"id": 67, "seek": 43356, "start": 455.96, "end": 462.36, "text": " and then how we send that over which is effectively what most of the open telemetry SDK instrumentations", "tokens": [51484, 293, 550, 577, 321, 2845, 300, 670, 597, 307, 8659, 437, 881, 295, 264, 1269, 4304, 5537, 627, 37135, 7198, 763, 51804], "temperature": 0.0, "avg_logprob": -0.35471069678831635, "compression_ratio": 1.5355648535564854, "no_speech_prob": 0.23790986835956573}, {"id": 68, "seek": 46236, "start": 462.36, "end": 469.76, "text": " do or the agents in Java or .NET, they do that injection for you automatically but we", "tokens": [50364, 360, 420, 264, 12554, 294, 10745, 420, 2411, 35554, 11, 436, 360, 300, 22873, 337, 291, 6772, 457, 321, 50734], "temperature": 0.0, "avg_logprob": -0.1856757541035497, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.05245380476117134}, {"id": 69, "seek": 46236, "start": 469.76, "end": 476.68, "text": " do it with eBPF so you don't have to have an SDK added to your packages or languages", "tokens": [50734, 360, 309, 365, 308, 33, 47, 37, 370, 291, 500, 380, 362, 281, 362, 364, 37135, 3869, 281, 428, 17401, 420, 8650, 51080], "temperature": 0.0, "avg_logprob": -0.1856757541035497, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.05245380476117134}, {"id": 70, "seek": 46236, "start": 476.68, "end": 481.8, "text": " when that doesn't exist or languages where maybe your library dependencies don't quite", "tokens": [51080, 562, 300, 1177, 380, 2514, 420, 8650, 689, 1310, 428, 6405, 36606, 500, 380, 1596, 51336], "temperature": 0.0, "avg_logprob": -0.1856757541035497, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.05245380476117134}, {"id": 71, "seek": 46236, "start": 481.8, "end": 490.40000000000003, "text": " work with the SDK because of different versions or it's not up to date or whatever the reason.", "tokens": [51336, 589, 365, 264, 37135, 570, 295, 819, 9606, 420, 309, 311, 406, 493, 281, 4002, 420, 2035, 264, 1778, 13, 51766], "temperature": 0.0, "avg_logprob": -0.1856757541035497, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.05245380476117134}, {"id": 72, "seek": 49040, "start": 490.44, "end": 496.56, "text": " We hook into the program like Mario mentioned in different ways and when a request starts", "tokens": [50366, 492, 6328, 666, 264, 1461, 411, 9343, 2835, 294, 819, 2098, 293, 562, 257, 5308, 3719, 50672], "temperature": 0.0, "avg_logprob": -0.16478030983058886, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.0029787879902869463}, {"id": 73, "seek": 49040, "start": 496.56, "end": 503.23999999999995, "text": " we actually read the memory with eBPF and what is in that trace parent. If there isn't", "tokens": [50672, 321, 767, 1401, 264, 4675, 365, 308, 33, 47, 37, 293, 437, 307, 294, 300, 13508, 2596, 13, 759, 456, 1943, 380, 51006], "temperature": 0.0, "avg_logprob": -0.16478030983058886, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.0029787879902869463}, {"id": 74, "seek": 49040, "start": 503.23999999999995, "end": 510.47999999999996, "text": " one, we'll generate one according to the W3 stack. Then what we do next is that we notice", "tokens": [51006, 472, 11, 321, 603, 8460, 472, 4650, 281, 264, 343, 18, 8630, 13, 1396, 437, 321, 360, 958, 307, 300, 321, 3449, 51368], "temperature": 0.0, "avg_logprob": -0.16478030983058886, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.0029787879902869463}, {"id": 75, "seek": 49040, "start": 510.47999999999996, "end": 517.0, "text": " an outgoing call and then in that outgoing call, if we can find the information about", "tokens": [51368, 364, 41565, 818, 293, 550, 294, 300, 41565, 818, 11, 498, 321, 393, 915, 264, 1589, 466, 51694], "temperature": 0.0, "avg_logprob": -0.16478030983058886, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.0029787879902869463}, {"id": 76, "seek": 51700, "start": 517.04, "end": 524.0, "text": " the headers, we will inject the outgoing trace header just like the SDK would do. This is", "tokens": [50366, 264, 45101, 11, 321, 486, 10711, 264, 41565, 13508, 23117, 445, 411, 264, 37135, 576, 360, 13, 639, 307, 50714], "temperature": 0.0, "avg_logprob": -0.21071276553841525, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.06090153381228447}, {"id": 77, "seek": 51700, "start": 524.0, "end": 530.12, "text": " what happens in Go currently with Vela. This is exactly what we do. Now internally, how", "tokens": [50714, 437, 2314, 294, 1037, 4362, 365, 691, 4053, 13, 639, 307, 2293, 437, 321, 360, 13, 823, 19501, 11, 577, 51020], "temperature": 0.0, "avg_logprob": -0.21071276553841525, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.06090153381228447}, {"id": 78, "seek": 51700, "start": 530.12, "end": 537.76, "text": " this all works? Well, to make sure that we can tag an incoming request on a server accepted", "tokens": [51020, 341, 439, 1985, 30, 1042, 11, 281, 652, 988, 300, 321, 393, 6162, 364, 22341, 5308, 322, 257, 7154, 9035, 51402], "temperature": 0.0, "avg_logprob": -0.21071276553841525, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.06090153381228447}, {"id": 79, "seek": 51700, "start": 537.76, "end": 543.36, "text": " something like slash ping for example and it did an outgoing request to slash ping me too", "tokens": [51402, 746, 411, 17330, 26151, 337, 1365, 293, 309, 630, 364, 41565, 5308, 281, 17330, 26151, 385, 886, 51682], "temperature": 0.0, "avg_logprob": -0.21071276553841525, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.06090153381228447}, {"id": 80, "seek": 54336, "start": 543.4, "end": 549.96, "text": " and in that case we need to track that this incoming request matches this outgoing request", "tokens": [50366, 293, 294, 300, 1389, 321, 643, 281, 2837, 300, 341, 22341, 5308, 10676, 341, 41565, 5308, 50694], "temperature": 0.0, "avg_logprob": -0.24492467533458362, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.0003149916883558035}, {"id": 81, "seek": 54336, "start": 549.96, "end": 556.5600000000001, "text": " by the call maybe async. Maybe somebody wrote a library and said, well, I don't want to", "tokens": [50694, 538, 264, 818, 1310, 382, 34015, 13, 2704, 2618, 4114, 257, 6405, 293, 848, 11, 731, 11, 286, 500, 380, 528, 281, 51024], "temperature": 0.0, "avg_logprob": -0.24492467533458362, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.0003149916883558035}, {"id": 82, "seek": 54336, "start": 556.5600000000001, "end": 560.52, "text": " wait for this request. I just want to do it async for whatever the reason. I'm using", "tokens": [51024, 1699, 337, 341, 5308, 13, 286, 445, 528, 281, 360, 309, 382, 34015, 337, 2035, 264, 1778, 13, 286, 478, 1228, 51222], "temperature": 0.0, "avg_logprob": -0.24492467533458362, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.0003149916883558035}, {"id": 83, "seek": 54336, "start": 560.52, "end": 568.16, "text": " some reactive library. In that case for Go, we track the Go routine creation and termination", "tokens": [51222, 512, 28897, 6405, 13, 682, 300, 1389, 337, 1037, 11, 321, 2837, 264, 1037, 9927, 8016, 293, 1433, 2486, 51604], "temperature": 0.0, "avg_logprob": -0.24492467533458362, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.0003149916883558035}, {"id": 84, "seek": 56816, "start": 568.16, "end": 574.1999999999999, "text": " essentially. Because the Go runtime and the standard libraries are very standardized and", "tokens": [50364, 4476, 13, 1436, 264, 1037, 34474, 293, 264, 3832, 15148, 366, 588, 31677, 293, 50666], "temperature": 0.0, "avg_logprob": -0.20501190793197765, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.005300136748701334}, {"id": 85, "seek": 56816, "start": 574.1999999999999, "end": 580.8399999999999, "text": " everybody uses that, we're able to do this kind of stuff. It doesn't need to be the first argument,", "tokens": [50666, 2201, 4960, 300, 11, 321, 434, 1075, 281, 360, 341, 733, 295, 1507, 13, 467, 1177, 380, 643, 281, 312, 264, 700, 6770, 11, 50998], "temperature": 0.0, "avg_logprob": -0.20501190793197765, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.005300136748701334}, {"id": 86, "seek": 56816, "start": 580.8399999999999, "end": 584.8399999999999, "text": " needs to be the context. None of that stuff. We just track Go routine creation and we're", "tokens": [50998, 2203, 281, 312, 264, 4319, 13, 14492, 295, 300, 1507, 13, 492, 445, 2837, 1037, 9927, 8016, 293, 321, 434, 51198], "temperature": 0.0, "avg_logprob": -0.20501190793197765, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.005300136748701334}, {"id": 87, "seek": 56816, "start": 584.8399999999999, "end": 593.3199999999999, "text": " able to match it later on. That's how we propagate the context. Now for the other languages,", "tokens": [51198, 1075, 281, 2995, 309, 1780, 322, 13, 663, 311, 577, 321, 48256, 264, 4319, 13, 823, 337, 264, 661, 8650, 11, 51622], "temperature": 0.0, "avg_logprob": -0.20501190793197765, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.005300136748701334}, {"id": 88, "seek": 56816, "start": 593.3199999999999, "end": 597.4, "text": " we thought, well, how are we going to do that for other languages? People use number of libraries.", "tokens": [51622, 321, 1194, 11, 731, 11, 577, 366, 321, 516, 281, 360, 300, 337, 661, 8650, 30, 3432, 764, 1230, 295, 15148, 13, 51826], "temperature": 0.0, "avg_logprob": -0.20501190793197765, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.005300136748701334}, {"id": 89, "seek": 59740, "start": 598.0, "end": 605.0799999999999, "text": " How do you do this on compile languages? Somebody does just think on time compile language. It's", "tokens": [50394, 1012, 360, 291, 360, 341, 322, 31413, 8650, 30, 13463, 775, 445, 519, 322, 565, 31413, 2856, 13, 467, 311, 50748], "temperature": 0.0, "avg_logprob": -0.30342115357864735, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0034820460714399815}, {"id": 90, "seek": 59740, "start": 605.0799999999999, "end": 612.0, "text": " kind of hard. For that we wrote additional support that does something more sneaky or if you will,", "tokens": [50748, 733, 295, 1152, 13, 1171, 300, 321, 4114, 4497, 1406, 300, 775, 746, 544, 39518, 420, 498, 291, 486, 11, 51094], "temperature": 0.0, "avg_logprob": -0.30342115357864735, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0034820460714399815}, {"id": 91, "seek": 59740, "start": 612.0, "end": 618.68, "text": " something more interesting. Land 2 servers or two processes talk to each other over HTTP,", "tokens": [51094, 746, 544, 1880, 13, 6607, 568, 15909, 420, 732, 7555, 751, 281, 1184, 661, 670, 33283, 11, 51428], "temperature": 0.0, "avg_logprob": -0.30342115357864735, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0034820460714399815}, {"id": 92, "seek": 59740, "start": 618.68, "end": 624.36, "text": " for example. They have a unique pair of information and they identify every connection. I have a", "tokens": [51428, 337, 1365, 13, 814, 362, 257, 3845, 6119, 295, 1589, 293, 436, 5876, 633, 4984, 13, 286, 362, 257, 51712], "temperature": 0.0, "avg_logprob": -0.30342115357864735, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0034820460714399815}, {"id": 93, "seek": 62436, "start": 624.36, "end": 630.36, "text": " client, opens a remote connection to a server. It has a source port, which is typically a", "tokens": [50364, 6423, 11, 9870, 257, 8607, 4984, 281, 257, 7154, 13, 467, 575, 257, 4009, 2436, 11, 597, 307, 5850, 257, 50664], "temperature": 0.0, "avg_logprob": -0.2002240803616106, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.003943944349884987}, {"id": 94, "seek": 62436, "start": 630.36, "end": 635.32, "text": " femoral. I have a destination port, which is a server port. When we see that connection pair,", "tokens": [50664, 4010, 16819, 13, 286, 362, 257, 12236, 2436, 11, 597, 307, 257, 7154, 2436, 13, 1133, 321, 536, 300, 4984, 6119, 11, 50912], "temperature": 0.0, "avg_logprob": -0.2002240803616106, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.003943944349884987}, {"id": 95, "seek": 62436, "start": 635.32, "end": 640.96, "text": " we use it as a unique key and we say, we'll store it in the eBPF map. Then when the server on the", "tokens": [50912, 321, 764, 309, 382, 257, 3845, 2141, 293, 321, 584, 11, 321, 603, 3531, 309, 294, 264, 308, 33, 47, 37, 4471, 13, 1396, 562, 264, 7154, 322, 264, 51194], "temperature": 0.0, "avg_logprob": -0.2002240803616106, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.003943944349884987}, {"id": 96, "seek": 62436, "start": 640.96, "end": 646.6800000000001, "text": " other side gets that request, they look up that map and say, well, I have this connection pair.", "tokens": [51194, 661, 1252, 2170, 300, 5308, 11, 436, 574, 493, 300, 4471, 293, 584, 11, 731, 11, 286, 362, 341, 4984, 6119, 13, 51480], "temperature": 0.0, "avg_logprob": -0.2002240803616106, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.003943944349884987}, {"id": 97, "seek": 62436, "start": 646.6800000000001, "end": 651.96, "text": " Does that match any client that made this connection? It does require that one single", "tokens": [51480, 4402, 300, 2995, 604, 6423, 300, 1027, 341, 4984, 30, 467, 775, 3651, 300, 472, 2167, 51744], "temperature": 0.0, "avg_logprob": -0.2002240803616106, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.003943944349884987}, {"id": 98, "seek": 65196, "start": 651.96, "end": 660.2800000000001, "text": " baler monitors both processes. If that is true, then we can actually tag these requests between", "tokens": [50364, 3119, 260, 26518, 1293, 7555, 13, 759, 300, 307, 2074, 11, 550, 321, 393, 767, 6162, 613, 12475, 1296, 50780], "temperature": 0.0, "avg_logprob": -0.23508639084665398, "compression_ratio": 1.613733905579399, "no_speech_prob": 0.0022865517530590296}, {"id": 99, "seek": 65196, "start": 660.2800000000001, "end": 665.96, "text": " servers without actually using this transparent propagation. For languages where we haven't", "tokens": [50780, 15909, 1553, 767, 1228, 341, 12737, 38377, 13, 1171, 8650, 689, 321, 2378, 380, 51064], "temperature": 0.0, "avg_logprob": -0.23508639084665398, "compression_ratio": 1.613733905579399, "no_speech_prob": 0.0022865517530590296}, {"id": 100, "seek": 65196, "start": 665.96, "end": 670.84, "text": " written additional support to inject the headers information, we use this as a backup option.", "tokens": [51064, 3720, 4497, 1406, 281, 10711, 264, 45101, 1589, 11, 321, 764, 341, 382, 257, 14807, 3614, 13, 51308], "temperature": 0.0, "avg_logprob": -0.23508639084665398, "compression_ratio": 1.613733905579399, "no_speech_prob": 0.0022865517530590296}, {"id": 101, "seek": 65196, "start": 670.84, "end": 679.52, "text": " This context propagation correlates internally requests through the kernel. Here's an example.", "tokens": [51308, 639, 4319, 38377, 13983, 1024, 19501, 12475, 807, 264, 28256, 13, 1692, 311, 364, 1365, 13, 51742], "temperature": 0.0, "avg_logprob": -0.23508639084665398, "compression_ratio": 1.613733905579399, "no_speech_prob": 0.0022865517530590296}, {"id": 102, "seek": 67952, "start": 679.64, "end": 685.8, "text": " We start the client call. It may read the transparent information that was present from a previous call,", "tokens": [50370, 492, 722, 264, 6423, 818, 13, 467, 815, 1401, 264, 12737, 1589, 300, 390, 1974, 490, 257, 3894, 818, 11, 50678], "temperature": 0.0, "avg_logprob": -0.20133283514725533, "compression_ratio": 1.7787610619469028, "no_speech_prob": 0.0049804262816905975}, {"id": 103, "seek": 67952, "start": 685.8, "end": 691.36, "text": " but if there isn't one, it's just going to generate it right there in eBPF and then store that information.", "tokens": [50678, 457, 498, 456, 1943, 380, 472, 11, 309, 311, 445, 516, 281, 8460, 309, 558, 456, 294, 308, 33, 47, 37, 293, 550, 3531, 300, 1589, 13, 50956], "temperature": 0.0, "avg_logprob": -0.20133283514725533, "compression_ratio": 1.7787610619469028, "no_speech_prob": 0.0049804262816905975}, {"id": 104, "seek": 67952, "start": 691.36, "end": 696.4399999999999, "text": " Then later on, when another server request happens based on the client call, we'll read that map,", "tokens": [50956, 1396, 1780, 322, 11, 562, 1071, 7154, 5308, 2314, 2361, 322, 264, 6423, 818, 11, 321, 603, 1401, 300, 4471, 11, 51210], "temperature": 0.0, "avg_logprob": -0.20133283514725533, "compression_ratio": 1.7787610619469028, "no_speech_prob": 0.0049804262816905975}, {"id": 105, "seek": 67952, "start": 696.4399999999999, "end": 702.1999999999999, "text": " read the transparent information, create the spans, just like if you will, that transparent", "tokens": [51210, 1401, 264, 12737, 1589, 11, 1884, 264, 44086, 11, 445, 411, 498, 291, 486, 11, 300, 12737, 51498], "temperature": 0.0, "avg_logprob": -0.20133283514725533, "compression_ratio": 1.7787610619469028, "no_speech_prob": 0.0049804262816905975}, {"id": 106, "seek": 70220, "start": 702.32, "end": 709.2, "text": " logic flew through the HTTP headers. More or less the same. There's restrictions, of course.", "tokens": [50370, 9952, 15728, 807, 264, 33283, 45101, 13, 5048, 420, 1570, 264, 912, 13, 821, 311, 14191, 11, 295, 1164, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2574939727783203, "compression_ratio": 1.5381679389312977, "no_speech_prob": 0.06556960195302963}, {"id": 107, "seek": 70220, "start": 709.2, "end": 718.2, "text": " Obviously, for this to actually work, we have to have a single node. Now, these eBPF maps can be shared", "tokens": [50714, 7580, 11, 337, 341, 281, 767, 589, 11, 321, 362, 281, 362, 257, 2167, 9984, 13, 823, 11, 613, 308, 33, 47, 37, 11317, 393, 312, 5507, 51164], "temperature": 0.0, "avg_logprob": -0.2574939727783203, "compression_ratio": 1.5381679389312977, "no_speech_prob": 0.06556960195302963}, {"id": 108, "seek": 70220, "start": 718.2, "end": 724.6, "text": " on a volume and maybe there's a way to use that, but we don't do that and support that right now.", "tokens": [51164, 322, 257, 5523, 293, 1310, 456, 311, 257, 636, 281, 764, 300, 11, 457, 321, 500, 380, 360, 300, 293, 1406, 300, 558, 586, 13, 51484], "temperature": 0.0, "avg_logprob": -0.2574939727783203, "compression_ratio": 1.5381679389312977, "no_speech_prob": 0.06556960195302963}, {"id": 109, "seek": 70220, "start": 724.6, "end": 731.0, "text": " This is also not released yet, so we just have it in the main branch. It's one of the newer things we added.", "tokens": [51484, 639, 307, 611, 406, 4736, 1939, 11, 370, 321, 445, 362, 309, 294, 264, 2135, 9819, 13, 467, 311, 472, 295, 264, 17628, 721, 321, 3869, 13, 51804], "temperature": 0.0, "avg_logprob": -0.2574939727783203, "compression_ratio": 1.5381679389312977, "no_speech_prob": 0.06556960195302963}, {"id": 110, "seek": 73100, "start": 731.96, "end": 739.32, "text": " But with this, I think I'm more of a person. I'll believe it when I see it.", "tokens": [50412, 583, 365, 341, 11, 286, 519, 286, 478, 544, 295, 257, 954, 13, 286, 603, 1697, 309, 562, 286, 536, 309, 13, 50780], "temperature": 0.0, "avg_logprob": -0.27219108132755054, "compression_ratio": 1.4926829268292683, "no_speech_prob": 0.0045754811726510525}, {"id": 111, "seek": 73100, "start": 739.32, "end": 746.64, "text": " I think we want to try to do a demo to show you everything's running off the laptop that Mario has here.", "tokens": [50780, 286, 519, 321, 528, 281, 853, 281, 360, 257, 10723, 281, 855, 291, 1203, 311, 2614, 766, 264, 10732, 300, 9343, 575, 510, 13, 51146], "temperature": 0.0, "avg_logprob": -0.27219108132755054, "compression_ratio": 1.4926829268292683, "no_speech_prob": 0.0045754811726510525}, {"id": 112, "seek": 73100, "start": 746.64, "end": 753.48, "text": " We're not going to connect to any cloud services, but what we want to demonstrate is a few HTTP services here.", "tokens": [51146, 492, 434, 406, 516, 281, 1745, 281, 604, 4588, 3328, 11, 457, 437, 321, 528, 281, 11698, 307, 257, 1326, 33283, 3328, 510, 13, 51488], "temperature": 0.0, "avg_logprob": -0.27219108132755054, "compression_ratio": 1.4926829268292683, "no_speech_prob": 0.0045754811726510525}, {"id": 113, "seek": 73100, "start": 753.48, "end": 754.88, "text": " And GRPC also.", "tokens": [51488, 400, 10903, 12986, 611, 13, 51558], "temperature": 0.0, "avg_logprob": -0.27219108132755054, "compression_ratio": 1.4926829268292683, "no_speech_prob": 0.0045754811726510525}, {"id": 114, "seek": 75488, "start": 755.76, "end": 761.24, "text": " They're using GRPC in this case. They're returning Go. We're going to have one Bayline instance.", "tokens": [50408, 814, 434, 1228, 10903, 12986, 294, 341, 1389, 13, 814, 434, 12678, 1037, 13, 492, 434, 516, 281, 362, 472, 7840, 1889, 5197, 13, 50682], "temperature": 0.0, "avg_logprob": -0.32301749072028596, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.02251356653869152}, {"id": 115, "seek": 75488, "start": 761.24, "end": 767.72, "text": " Look at all of them. We're going to use this little tool that actually Fabian made,", "tokens": [50682, 2053, 412, 439, 295, 552, 13, 492, 434, 516, 281, 764, 341, 707, 2290, 300, 767, 17440, 952, 1027, 11, 51006], "temperature": 0.0, "avg_logprob": -0.32301749072028596, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.02251356653869152}, {"id": 116, "seek": 75488, "start": 767.72, "end": 773.72, "text": " this little Docker Compose LGTN, which has the full Grafana stack with all our open source products,", "tokens": [51006, 341, 707, 33772, 6620, 541, 25449, 51, 45, 11, 597, 575, 264, 1577, 8985, 69, 2095, 8630, 365, 439, 527, 1269, 4009, 3383, 11, 51306], "temperature": 0.0, "avg_logprob": -0.32301749072028596, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.02251356653869152}, {"id": 117, "seek": 75488, "start": 773.72, "end": 780.16, "text": " with the OpenTelemetry collector setup that it can ingest and do traces, metrics, and everything you need.", "tokens": [51306, 365, 264, 7238, 14233, 306, 5537, 627, 23960, 8657, 300, 309, 393, 3957, 377, 293, 360, 26076, 11, 16367, 11, 293, 1203, 291, 643, 13, 51628], "temperature": 0.0, "avg_logprob": -0.32301749072028596, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.02251356653869152}, {"id": 118, "seek": 78016, "start": 780.8, "end": 782.76, "text": " Very convenient for testing.", "tokens": [50396, 4372, 10851, 337, 4997, 13, 50494], "temperature": 0.0, "avg_logprob": -0.3947723676573555, "compression_ratio": 1.66796875, "no_speech_prob": 0.035557862371206284}, {"id": 119, "seek": 78016, "start": 782.76, "end": 786.24, "text": " Very convenient for testing or spinning up your own Dockerfana cluster at home.", "tokens": [50494, 4372, 10851, 337, 4997, 420, 15640, 493, 428, 1065, 33772, 69, 2095, 13630, 412, 1280, 13, 50668], "temperature": 0.0, "avg_logprob": -0.3947723676573555, "compression_ratio": 1.66796875, "no_speech_prob": 0.035557862371206284}, {"id": 120, "seek": 78016, "start": 786.24, "end": 788.64, "text": " So it's just one Dockerfile with all of it.", "tokens": [50668, 407, 309, 311, 445, 472, 33772, 69, 794, 365, 439, 295, 309, 13, 50788], "temperature": 0.0, "avg_logprob": -0.3947723676573555, "compression_ratio": 1.66796875, "no_speech_prob": 0.035557862371206284}, {"id": 121, "seek": 78016, "start": 790.64, "end": 795.12, "text": " I also wanted to mention, because we didn't say, it's obvious the presentation is about distributed traces,", "tokens": [50888, 286, 611, 1415, 281, 2152, 11, 570, 321, 994, 380, 584, 11, 309, 311, 6322, 264, 5860, 307, 466, 12631, 26076, 11, 51112], "temperature": 0.0, "avg_logprob": -0.3947723676573555, "compression_ratio": 1.66796875, "no_speech_prob": 0.035557862371206284}, {"id": 122, "seek": 78016, "start": 795.12, "end": 797.24, "text": " but Bayline does support metrics too.", "tokens": [51112, 457, 7840, 1889, 775, 1406, 16367, 886, 13, 51218], "temperature": 0.0, "avg_logprob": -0.3947723676573555, "compression_ratio": 1.66796875, "no_speech_prob": 0.035557862371206284}, {"id": 123, "seek": 78016, "start": 797.24, "end": 801.04, "text": " So HTTP metrics were included from the Star Door product.", "tokens": [51218, 407, 33283, 16367, 645, 5556, 490, 264, 5705, 29636, 1674, 13, 51408], "temperature": 0.0, "avg_logprob": -0.3947723676573555, "compression_ratio": 1.66796875, "no_speech_prob": 0.035557862371206284}, {"id": 124, "seek": 78016, "start": 801.04, "end": 805.04, "text": " Traces distributed traces is some of the newer stuff we're working on.", "tokens": [51408, 1765, 2116, 12631, 26076, 307, 512, 295, 264, 17628, 1507, 321, 434, 1364, 322, 13, 51608], "temperature": 0.0, "avg_logprob": -0.3947723676573555, "compression_ratio": 1.66796875, "no_speech_prob": 0.035557862371206284}, {"id": 125, "seek": 80504, "start": 805.76, "end": 812.76, "text": " Okay, so for this demo, we will show a simple distributed application.", "tokens": [50400, 1033, 11, 370, 337, 341, 10723, 11, 321, 486, 855, 257, 2199, 12631, 3861, 13, 50750], "temperature": 0.0, "avg_logprob": -0.3513659489007644, "compression_ratio": 1.5707070707070707, "no_speech_prob": 0.016577113419771194}, {"id": 126, "seek": 80504, "start": 812.76, "end": 818.28, "text": " It means a synthetic application is just a frontend sending a request to a backend,", "tokens": [50750, 467, 1355, 257, 23420, 3861, 307, 445, 257, 1868, 521, 7750, 257, 5308, 281, 257, 38087, 11, 51026], "temperature": 0.0, "avg_logprob": -0.3513659489007644, "compression_ratio": 1.5707070707070707, "no_speech_prob": 0.016577113419771194}, {"id": 127, "seek": 80504, "start": 818.28, "end": 825.28, "text": " and the backend asking for distributing some load on the workers and then getting a response.", "tokens": [51026, 293, 264, 38087, 3365, 337, 41406, 512, 3677, 322, 264, 5600, 293, 550, 1242, 257, 4134, 13, 51376], "temperature": 0.0, "avg_logprob": -0.3513659489007644, "compression_ratio": 1.5707070707070707, "no_speech_prob": 0.016577113419771194}, {"id": 128, "seek": 80504, "start": 826.9599999999999, "end": 827.9599999999999, "text": " Do you need to hold that?", "tokens": [51460, 1144, 291, 643, 281, 1797, 300, 30, 51510], "temperature": 0.0, "avg_logprob": -0.3513659489007644, "compression_ratio": 1.5707070707070707, "no_speech_prob": 0.016577113419771194}, {"id": 129, "seek": 80504, "start": 827.9599999999999, "end": 829.52, "text": " No, it's okay. It's okay. Thank you.", "tokens": [51510, 883, 11, 309, 311, 1392, 13, 467, 311, 1392, 13, 1044, 291, 13, 51588], "temperature": 0.0, "avg_logprob": -0.3513659489007644, "compression_ratio": 1.5707070707070707, "no_speech_prob": 0.016577113419771194}, {"id": 130, "seek": 82952, "start": 830.52, "end": 843.52, "text": " Then I have added everything into a Docker compose file just for facilitating the demo in my laptop.", "tokens": [50414, 1396, 286, 362, 3869, 1203, 666, 257, 33772, 35925, 3991, 445, 337, 47558, 264, 10723, 294, 452, 10732, 13, 51064], "temperature": 0.0, "avg_logprob": -0.34168608983357746, "compression_ratio": 1.2972972972972974, "no_speech_prob": 0.03787067532539368}, {"id": 131, "seek": 82952, "start": 843.52, "end": 853.52, "text": " So we have this OpenTelemetry collector, which is the hotel LGTM container that Fabian did.", "tokens": [51064, 407, 321, 362, 341, 7238, 14233, 306, 5537, 627, 23960, 11, 597, 307, 264, 7622, 25449, 42023, 10129, 300, 17440, 952, 630, 13, 51564], "temperature": 0.0, "avg_logprob": -0.34168608983357746, "compression_ratio": 1.2972972972972974, "no_speech_prob": 0.03787067532539368}, {"id": 132, "seek": 85352, "start": 854.52, "end": 857.52, "text": " And we just dropped Bayline as a container.", "tokens": [50414, 400, 321, 445, 8119, 7840, 1889, 382, 257, 10129, 13, 50564], "temperature": 0.0, "avg_logprob": -0.17761616273359818, "compression_ratio": 1.6037735849056605, "no_speech_prob": 0.044925201684236526}, {"id": 133, "seek": 85352, "start": 857.52, "end": 864.52, "text": " You can drop Bayline there as a host process, but for convenience also as a container.", "tokens": [50564, 509, 393, 3270, 7840, 1889, 456, 382, 257, 3975, 1399, 11, 457, 337, 19283, 611, 382, 257, 10129, 13, 50914], "temperature": 0.0, "avg_logprob": -0.17761616273359818, "compression_ratio": 1.6037735849056605, "no_speech_prob": 0.044925201684236526}, {"id": 134, "seek": 85352, "start": 864.52, "end": 870.52, "text": " We need to give access to the pit name space of the host,", "tokens": [50914, 492, 643, 281, 976, 2105, 281, 264, 10147, 1315, 1901, 295, 264, 3975, 11, 51214], "temperature": 0.0, "avg_logprob": -0.17761616273359818, "compression_ratio": 1.6037735849056605, "no_speech_prob": 0.044925201684236526}, {"id": 135, "seek": 85352, "start": 870.52, "end": 876.52, "text": " because it will have to instrument all the processes in that host,", "tokens": [51214, 570, 309, 486, 362, 281, 7198, 439, 264, 7555, 294, 300, 3975, 11, 51514], "temperature": 0.0, "avg_logprob": -0.17761616273359818, "compression_ratio": 1.6037735849056605, "no_speech_prob": 0.044925201684236526}, {"id": 136, "seek": 87652, "start": 876.52, "end": 886.52, "text": " and also privilege access because loading EVPF programs requires administrative privileges.", "tokens": [50364, 293, 611, 12122, 2105, 570, 15114, 15733, 47, 37, 4268, 7029, 17900, 32588, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17828520706721715, "compression_ratio": 1.526946107784431, "no_speech_prob": 0.06517910957336426}, {"id": 137, "seek": 87652, "start": 886.52, "end": 897.52, "text": " Then we set here some the OpenTelemetry endpoint in a standard configuration.", "tokens": [50864, 1396, 321, 992, 510, 512, 264, 7238, 14233, 306, 5537, 627, 35795, 294, 257, 3832, 11694, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17828520706721715, "compression_ratio": 1.526946107784431, "no_speech_prob": 0.06517910957336426}, {"id": 138, "seek": 87652, "start": 897.52, "end": 902.52, "text": " Bayline accepts the standard OpenTelemetry configurations for setting up many values.", "tokens": [51414, 7840, 1889, 33538, 264, 3832, 7238, 14233, 306, 5537, 627, 31493, 337, 3287, 493, 867, 4190, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17828520706721715, "compression_ratio": 1.526946107784431, "no_speech_prob": 0.06517910957336426}, {"id": 139, "seek": 90252, "start": 903.52, "end": 908.52, "text": " And also we are providing a configuration file.", "tokens": [50414, 400, 611, 321, 366, 6530, 257, 11694, 3991, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1564180811897653, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.06611253321170807}, {"id": 140, "seek": 90252, "start": 908.52, "end": 915.52, "text": " Basically here we say how to group the HTTP routes.", "tokens": [50664, 8537, 510, 321, 584, 577, 281, 1594, 264, 33283, 18242, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1564180811897653, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.06611253321170807}, {"id": 141, "seek": 90252, "start": 915.52, "end": 924.52, "text": " For example, there is a route that calculates a factorial, and you will pass in the request,", "tokens": [51014, 1171, 1365, 11, 456, 307, 257, 7955, 300, 4322, 1024, 257, 36916, 11, 293, 291, 486, 1320, 294, 264, 5308, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1564180811897653, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.06611253321170807}, {"id": 142, "seek": 90252, "start": 924.52, "end": 927.52, "text": " you will set factorial and the number to calculate.", "tokens": [51464, 291, 486, 992, 36916, 293, 264, 1230, 281, 8873, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1564180811897653, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.06611253321170807}, {"id": 143, "seek": 92752, "start": 927.52, "end": 935.52, "text": " We don't get a cardinality explosion because we don't want to create a different route value for every number we calculate.", "tokens": [50364, 492, 500, 380, 483, 257, 2920, 259, 1860, 15673, 570, 321, 500, 380, 528, 281, 1884, 257, 819, 7955, 2158, 337, 633, 1230, 321, 8873, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11348202608633733, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.07265836745500565}, {"id": 144, "seek": 92752, "start": 935.52, "end": 946.52, "text": " So we say, okay, just group all the URLs matching this pattern, group them in factorial number.", "tokens": [50764, 407, 321, 584, 11, 1392, 11, 445, 1594, 439, 264, 43267, 14324, 341, 5102, 11, 1594, 552, 294, 36916, 1230, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11348202608633733, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.07265836745500565}, {"id": 145, "seek": 92752, "start": 946.52, "end": 951.52, "text": " And then we tell Bayline how to discover the services to instrument.", "tokens": [51314, 400, 550, 321, 980, 7840, 1889, 577, 281, 4411, 264, 3328, 281, 7198, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11348202608633733, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.07265836745500565}, {"id": 146, "seek": 95152, "start": 951.52, "end": 958.52, "text": " We have a frontend, a backend, and a worker container, and then we pass that.", "tokens": [50364, 492, 362, 257, 1868, 521, 11, 257, 38087, 11, 293, 257, 11346, 10129, 11, 293, 550, 321, 1320, 300, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11192281940315343, "compression_ratio": 1.5803108808290156, "no_speech_prob": 0.14849969744682312}, {"id": 147, "seek": 95152, "start": 958.52, "end": 961.52, "text": " This accepts any regular expression.", "tokens": [50714, 639, 33538, 604, 3890, 6114, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11192281940315343, "compression_ratio": 1.5803108808290156, "no_speech_prob": 0.14849969744682312}, {"id": 148, "seek": 95152, "start": 961.52, "end": 969.52, "text": " So if we say just a dot, it will instrument or try to instrument all the processes in the host.", "tokens": [50864, 407, 498, 321, 584, 445, 257, 5893, 11, 309, 486, 7198, 420, 853, 281, 7198, 439, 264, 7555, 294, 264, 3975, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11192281940315343, "compression_ratio": 1.5803108808290156, "no_speech_prob": 0.14849969744682312}, {"id": 149, "seek": 95152, "start": 969.52, "end": 976.52, "text": " But in that case it will also instrument some parts of the Docker API, the Docker Compose API.", "tokens": [51264, 583, 294, 300, 1389, 309, 486, 611, 7198, 512, 3166, 295, 264, 33772, 9362, 11, 264, 33772, 6620, 541, 9362, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11192281940315343, "compression_ratio": 1.5803108808290156, "no_speech_prob": 0.14849969744682312}, {"id": 150, "seek": 97652, "start": 976.52, "end": 982.52, "text": " So to not generate noise, we are just providing the services we want to instrument.", "tokens": [50364, 407, 281, 406, 8460, 5658, 11, 321, 366, 445, 6530, 264, 3328, 321, 528, 281, 7198, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10123041603300306, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.027474040165543556}, {"id": 151, "seek": 97652, "start": 982.52, "end": 987.52, "text": " And let me then run this Docker Compose file.", "tokens": [50664, 400, 718, 385, 550, 1190, 341, 33772, 6620, 541, 3991, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10123041603300306, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.027474040165543556}, {"id": 152, "seek": 97652, "start": 989.52, "end": 992.52, "text": " Okay, this application is a very simple application.", "tokens": [51014, 1033, 11, 341, 3861, 307, 257, 588, 2199, 3861, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10123041603300306, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.027474040165543556}, {"id": 153, "seek": 97652, "start": 992.52, "end": 997.52, "text": " It's a huge factorial calculator application.", "tokens": [51164, 467, 311, 257, 2603, 36916, 24993, 3861, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10123041603300306, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.027474040165543556}, {"id": 154, "seek": 97652, "start": 997.52, "end": 1002.52, "text": " I will just write a number, and it will calculate the factorial.", "tokens": [51414, 286, 486, 445, 2464, 257, 1230, 11, 293, 309, 486, 8873, 264, 36916, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10123041603300306, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.027474040165543556}, {"id": 155, "seek": 100252, "start": 1002.52, "end": 1006.52, "text": " And if you need more numbers, okay, you calculate.", "tokens": [50364, 400, 498, 291, 643, 544, 3547, 11, 1392, 11, 291, 8873, 13, 50564], "temperature": 0.0, "avg_logprob": -0.17921781539916992, "compression_ratio": 1.4639639639639639, "no_speech_prob": 0.015412135049700737}, {"id": 156, "seek": 100252, "start": 1006.52, "end": 1012.52, "text": " Boom! This is an error introduced as on purpose because I also use this application to track errors from Bayline.", "tokens": [50564, 15523, 0, 639, 307, 364, 6713, 7268, 382, 322, 4334, 570, 286, 611, 764, 341, 3861, 281, 2837, 13603, 490, 7840, 1889, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17921781539916992, "compression_ratio": 1.4639639639639639, "no_speech_prob": 0.015412135049700737}, {"id": 157, "seek": 100252, "start": 1012.52, "end": 1015.52, "text": " But it usually works.", "tokens": [50864, 583, 309, 2673, 1985, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17921781539916992, "compression_ratio": 1.4639639639639639, "no_speech_prob": 0.015412135049700737}, {"id": 158, "seek": 100252, "start": 1015.52, "end": 1019.52, "text": " Then, doing that, we have, Bayline was already running.", "tokens": [51014, 1396, 11, 884, 300, 11, 321, 362, 11, 7840, 1889, 390, 1217, 2614, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17921781539916992, "compression_ratio": 1.4639639639639639, "no_speech_prob": 0.015412135049700737}, {"id": 159, "seek": 100252, "start": 1019.52, "end": 1022.52, "text": " We have been generating some traces.", "tokens": [51214, 492, 362, 668, 17746, 512, 26076, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17921781539916992, "compression_ratio": 1.4639639639639639, "no_speech_prob": 0.015412135049700737}, {"id": 160, "seek": 100252, "start": 1022.52, "end": 1025.52, "text": " So let me go to the local Grafana.", "tokens": [51364, 407, 718, 385, 352, 281, 264, 2654, 8985, 69, 2095, 13, 51514], "temperature": 0.0, "avg_logprob": -0.17921781539916992, "compression_ratio": 1.4639639639639639, "no_speech_prob": 0.015412135049700737}, {"id": 161, "seek": 100252, "start": 1025.52, "end": 1027.52, "text": " Let's see.", "tokens": [51514, 961, 311, 536, 13, 51614], "temperature": 0.0, "avg_logprob": -0.17921781539916992, "compression_ratio": 1.4639639639639639, "no_speech_prob": 0.015412135049700737}, {"id": 162, "seek": 102752, "start": 1028.52, "end": 1031.52, "text": " I go to, for example, explore.", "tokens": [50414, 286, 352, 281, 11, 337, 1365, 11, 6839, 13, 50564], "temperature": 0.0, "avg_logprob": -0.21463394165039062, "compression_ratio": 1.3691275167785235, "no_speech_prob": 0.03234093636274338}, {"id": 163, "seek": 102752, "start": 1031.52, "end": 1036.52, "text": " Here I selected the tempo, and let me search for all the traces.", "tokens": [50564, 1692, 286, 8209, 264, 8972, 11, 293, 718, 385, 3164, 337, 439, 264, 26076, 13, 50814], "temperature": 0.0, "avg_logprob": -0.21463394165039062, "compression_ratio": 1.3691275167785235, "no_speech_prob": 0.03234093636274338}, {"id": 164, "seek": 102752, "start": 1036.52, "end": 1039.52, "text": " Okay, beautiful.", "tokens": [50814, 1033, 11, 2238, 13, 50964], "temperature": 0.0, "avg_logprob": -0.21463394165039062, "compression_ratio": 1.3691275167785235, "no_speech_prob": 0.03234093636274338}, {"id": 165, "seek": 102752, "start": 1039.52, "end": 1046.52, "text": " It's strange because here we can see that Bayline...", "tokens": [50964, 467, 311, 5861, 570, 510, 321, 393, 536, 300, 7840, 1889, 485, 51314], "temperature": 0.0, "avg_logprob": -0.21463394165039062, "compression_ratio": 1.3691275167785235, "no_speech_prob": 0.03234093636274338}, {"id": 166, "seek": 102752, "start": 1046.52, "end": 1049.52, "text": " Oh, yeah?", "tokens": [51314, 876, 11, 1338, 30, 51464], "temperature": 0.0, "avg_logprob": -0.21463394165039062, "compression_ratio": 1.3691275167785235, "no_speech_prob": 0.03234093636274338}, {"id": 167, "seek": 102752, "start": 1049.52, "end": 1052.52, "text": " Okay, let me check.", "tokens": [51464, 1033, 11, 718, 385, 1520, 13, 51614], "temperature": 0.0, "avg_logprob": -0.21463394165039062, "compression_ratio": 1.3691275167785235, "no_speech_prob": 0.03234093636274338}, {"id": 168, "seek": 102752, "start": 1052.52, "end": 1054.52, "text": " No data.", "tokens": [51614, 883, 1412, 13, 51714], "temperature": 0.0, "avg_logprob": -0.21463394165039062, "compression_ratio": 1.3691275167785235, "no_speech_prob": 0.03234093636274338}, {"id": 169, "seek": 105452, "start": 1054.52, "end": 1057.52, "text": " Okay, it happens in the best families.", "tokens": [50364, 1033, 11, 309, 2314, 294, 264, 1151, 4466, 13, 50514], "temperature": 0.0, "avg_logprob": -0.19961265722910562, "compression_ratio": 1.337837837837838, "no_speech_prob": 0.0880461186170578}, {"id": 170, "seek": 105452, "start": 1057.52, "end": 1060.52, "text": " No, but we have this...", "tokens": [50514, 883, 11, 457, 321, 362, 341, 485, 50664], "temperature": 0.0, "avg_logprob": -0.19961265722910562, "compression_ratio": 1.337837837837838, "no_speech_prob": 0.0880461186170578}, {"id": 171, "seek": 105452, "start": 1060.52, "end": 1063.52, "text": " I mean, it is able to...", "tokens": [50664, 286, 914, 11, 309, 307, 1075, 281, 485, 50814], "temperature": 0.0, "avg_logprob": -0.19961265722910562, "compression_ratio": 1.337837837837838, "no_speech_prob": 0.0880461186170578}, {"id": 172, "seek": 105452, "start": 1063.52, "end": 1065.52, "text": " Okay, I don't know what happened.", "tokens": [50814, 1033, 11, 286, 500, 380, 458, 437, 2011, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19961265722910562, "compression_ratio": 1.337837837837838, "no_speech_prob": 0.0880461186170578}, {"id": 173, "seek": 105452, "start": 1065.52, "end": 1067.52, "text": " But...", "tokens": [50914, 583, 485, 51014], "temperature": 0.0, "avg_logprob": -0.19961265722910562, "compression_ratio": 1.337837837837838, "no_speech_prob": 0.0880461186170578}, {"id": 174, "seek": 105452, "start": 1072.52, "end": 1075.52, "text": " For sure, it's a book in Grafana.", "tokens": [51264, 1171, 988, 11, 309, 311, 257, 1446, 294, 8985, 69, 2095, 13, 51414], "temperature": 0.0, "avg_logprob": -0.19961265722910562, "compression_ratio": 1.337837837837838, "no_speech_prob": 0.0880461186170578}, {"id": 175, "seek": 105452, "start": 1075.52, "end": 1082.52, "text": " So I have here many, many requests.", "tokens": [51414, 407, 286, 362, 510, 867, 11, 867, 12475, 13, 51764], "temperature": 0.0, "avg_logprob": -0.19961265722910562, "compression_ratio": 1.337837837837838, "no_speech_prob": 0.0880461186170578}, {"id": 176, "seek": 108252, "start": 1082.52, "end": 1084.52, "text": " Or many traces.", "tokens": [50364, 1610, 867, 26076, 13, 50464], "temperature": 0.0, "avg_logprob": -0.13042662693903997, "compression_ratio": 1.6267281105990783, "no_speech_prob": 0.1157740131020546}, {"id": 177, "seek": 108252, "start": 1084.52, "end": 1092.52, "text": " Let me just instrument this, submit trace, which is the one that triggers the backend and the workers.", "tokens": [50464, 961, 385, 445, 7198, 341, 11, 10315, 13508, 11, 597, 307, 264, 472, 300, 22827, 264, 38087, 293, 264, 5600, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13042662693903997, "compression_ratio": 1.6267281105990783, "no_speech_prob": 0.1157740131020546}, {"id": 178, "seek": 108252, "start": 1092.52, "end": 1098.52, "text": " If we enter here, you will see the trace information.", "tokens": [50864, 759, 321, 3242, 510, 11, 291, 486, 536, 264, 13508, 1589, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13042662693903997, "compression_ratio": 1.6267281105990783, "no_speech_prob": 0.1157740131020546}, {"id": 179, "seek": 108252, "start": 1098.52, "end": 1102.52, "text": " How the front end invokes the backend.", "tokens": [51164, 1012, 264, 1868, 917, 1048, 8606, 264, 38087, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13042662693903997, "compression_ratio": 1.6267281105990783, "no_speech_prob": 0.1157740131020546}, {"id": 180, "seek": 108252, "start": 1102.52, "end": 1108.52, "text": " You can track also an internal status of the request, like how much time the queue is in...", "tokens": [51364, 509, 393, 2837, 611, 364, 6920, 6558, 295, 264, 5308, 11, 411, 577, 709, 565, 264, 18639, 307, 294, 485, 51664], "temperature": 0.0, "avg_logprob": -0.13042662693903997, "compression_ratio": 1.6267281105990783, "no_speech_prob": 0.1157740131020546}, {"id": 181, "seek": 108252, "start": 1108.52, "end": 1110.52, "text": " Or the request is in queue or is being processed.", "tokens": [51664, 1610, 264, 5308, 307, 294, 18639, 420, 307, 885, 18846, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13042662693903997, "compression_ratio": 1.6267281105990783, "no_speech_prob": 0.1157740131020546}, {"id": 182, "seek": 111052, "start": 1110.52, "end": 1116.52, "text": " And you can see how, for example, the backend might invoke the worker multiple times.", "tokens": [50364, 400, 291, 393, 536, 577, 11, 337, 1365, 11, 264, 38087, 1062, 41117, 264, 11346, 3866, 1413, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09640281919449095, "compression_ratio": 1.5089820359281436, "no_speech_prob": 0.02989431656897068}, {"id": 183, "seek": 111052, "start": 1116.52, "end": 1120.52, "text": " So we got distributed traces automatically.", "tokens": [50664, 407, 321, 658, 12631, 26076, 6772, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09640281919449095, "compression_ratio": 1.5089820359281436, "no_speech_prob": 0.02989431656897068}, {"id": 184, "seek": 111052, "start": 1120.52, "end": 1125.52, "text": " We can even see the node graph of all the requests.", "tokens": [50864, 492, 393, 754, 536, 264, 9984, 4295, 295, 439, 264, 12475, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09640281919449095, "compression_ratio": 1.5089820359281436, "no_speech_prob": 0.02989431656897068}, {"id": 185, "seek": 111052, "start": 1125.52, "end": 1133.52, "text": " How this process invokes or the relation of all the traces as a graph.", "tokens": [51114, 1012, 341, 1399, 1048, 8606, 420, 264, 9721, 295, 439, 264, 26076, 382, 257, 4295, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09640281919449095, "compression_ratio": 1.5089820359281436, "no_speech_prob": 0.02989431656897068}, {"id": 186, "seek": 113352, "start": 1133.52, "end": 1140.52, "text": " How the front end as a server, because we instrument either server or client side spans.", "tokens": [50364, 1012, 264, 1868, 917, 382, 257, 7154, 11, 570, 321, 7198, 2139, 7154, 420, 6423, 1252, 44086, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15776171003069198, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.09447290003299713}, {"id": 187, "seek": 113352, "start": 1140.52, "end": 1145.52, "text": " How the front end invokes the backend, the backend invokes different workers and so on.", "tokens": [50714, 1012, 264, 1868, 917, 1048, 8606, 264, 38087, 11, 264, 38087, 1048, 8606, 819, 5600, 293, 370, 322, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15776171003069198, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.09447290003299713}, {"id": 188, "seek": 113352, "start": 1145.52, "end": 1147.52, "text": " I just want to add something here.", "tokens": [50964, 286, 445, 528, 281, 909, 746, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15776171003069198, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.09447290003299713}, {"id": 189, "seek": 113352, "start": 1147.52, "end": 1150.52, "text": " So we're here, if you see, when you look at the Bayla stuff that we produce,", "tokens": [51064, 407, 321, 434, 510, 11, 498, 291, 536, 11, 562, 291, 574, 412, 264, 7840, 875, 1507, 300, 321, 5258, 11, 51214], "temperature": 0.0, "avg_logprob": -0.15776171003069198, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.09447290003299713}, {"id": 190, "seek": 113352, "start": 1150.52, "end": 1153.52, "text": " we produce these two spans for some of the server requests.", "tokens": [51214, 321, 5258, 613, 732, 44086, 337, 512, 295, 264, 7154, 12475, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15776171003069198, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.09447290003299713}, {"id": 191, "seek": 113352, "start": 1153.52, "end": 1155.52, "text": " We have in queue and processing.", "tokens": [51364, 492, 362, 294, 18639, 293, 9007, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15776171003069198, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.09447290003299713}, {"id": 192, "seek": 113352, "start": 1155.52, "end": 1157.52, "text": " And for most people, that's like, what is this two things?", "tokens": [51464, 400, 337, 881, 561, 11, 300, 311, 411, 11, 437, 307, 341, 732, 721, 30, 51564], "temperature": 0.0, "avg_logprob": -0.15776171003069198, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.09447290003299713}, {"id": 193, "seek": 113352, "start": 1157.52, "end": 1159.52, "text": " Like why are you tracking two times?", "tokens": [51564, 1743, 983, 366, 291, 11603, 732, 1413, 30, 51664], "temperature": 0.0, "avg_logprob": -0.15776171003069198, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.09447290003299713}, {"id": 194, "seek": 115952, "start": 1159.52, "end": 1164.52, "text": " And if you have a typical application server that saves with an in-go,", "tokens": [50364, 400, 498, 291, 362, 257, 7476, 3861, 7154, 300, 19155, 365, 364, 294, 12, 1571, 11, 50614], "temperature": 0.0, "avg_logprob": -0.1450655174255371, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010810213163495064}, {"id": 195, "seek": 115952, "start": 1164.52, "end": 1170.52, "text": " and you accept the request and as soon as that happens, go or launch a go routine for this.", "tokens": [50614, 293, 291, 3241, 264, 5308, 293, 382, 2321, 382, 300, 2314, 11, 352, 420, 4025, 257, 352, 9927, 337, 341, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1450655174255371, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010810213163495064}, {"id": 196, "seek": 115952, "start": 1170.52, "end": 1178.52, "text": " But how long before this go routine gets scheduled on a physical thread, which is M in the world of go,", "tokens": [50914, 583, 577, 938, 949, 341, 352, 9927, 2170, 15678, 322, 257, 4001, 7207, 11, 597, 307, 376, 294, 264, 1002, 295, 352, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1450655174255371, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010810213163495064}, {"id": 197, "seek": 115952, "start": 1178.52, "end": 1181.52, "text": " and how long before this physical thread actually gets CPU time?", "tokens": [51314, 293, 577, 938, 949, 341, 4001, 7207, 767, 2170, 13199, 565, 30, 51464], "temperature": 0.0, "avg_logprob": -0.1450655174255371, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010810213163495064}, {"id": 198, "seek": 115952, "start": 1181.52, "end": 1187.52, "text": " So from a traditional instrumentation, you instrument the handler of the server request.", "tokens": [51464, 407, 490, 257, 5164, 7198, 399, 11, 291, 7198, 264, 41967, 295, 264, 7154, 5308, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1450655174255371, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.010810213163495064}, {"id": 199, "seek": 118752, "start": 1187.52, "end": 1190.52, "text": " This handler of the server request is the time the handler started running,", "tokens": [50364, 639, 41967, 295, 264, 7154, 5308, 307, 264, 565, 264, 41967, 1409, 2614, 11, 50514], "temperature": 0.0, "avg_logprob": -0.10027966600783328, "compression_ratio": 1.936026936026936, "no_speech_prob": 0.005908811464905739}, {"id": 200, "seek": 118752, "start": 1190.52, "end": 1195.52, "text": " not the time that the runtime accepted the request coming in from the kernel.", "tokens": [50514, 406, 264, 565, 300, 264, 34474, 9035, 264, 5308, 1348, 294, 490, 264, 28256, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10027966600783328, "compression_ratio": 1.936026936026936, "no_speech_prob": 0.005908811464905739}, {"id": 201, "seek": 118752, "start": 1195.52, "end": 1199.52, "text": " Well, at the EVPF, because we're at a low level, we can actually track that time.", "tokens": [50764, 1042, 11, 412, 264, 15733, 47, 37, 11, 570, 321, 434, 412, 257, 2295, 1496, 11, 321, 393, 767, 2837, 300, 565, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10027966600783328, "compression_ratio": 1.936026936026936, "no_speech_prob": 0.005908811464905739}, {"id": 202, "seek": 118752, "start": 1199.52, "end": 1204.52, "text": " We can actually see where the request actually came from the kernel, when the go routine was launched,", "tokens": [50964, 492, 393, 767, 536, 689, 264, 5308, 767, 1361, 490, 264, 28256, 11, 562, 264, 352, 9927, 390, 8730, 11, 51214], "temperature": 0.0, "avg_logprob": -0.10027966600783328, "compression_ratio": 1.936026936026936, "no_speech_prob": 0.005908811464905739}, {"id": 203, "seek": 118752, "start": 1204.52, "end": 1206.52, "text": " and when you finally got the handler to run.", "tokens": [51214, 293, 562, 291, 2721, 658, 264, 41967, 281, 1190, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10027966600783328, "compression_ratio": 1.936026936026936, "no_speech_prob": 0.005908811464905739}, {"id": 204, "seek": 118752, "start": 1206.52, "end": 1212.52, "text": " So in a situation where you have a server which is overloaded and it's not able to serve the request,", "tokens": [51314, 407, 294, 257, 2590, 689, 291, 362, 257, 7154, 597, 307, 28777, 292, 293, 309, 311, 406, 1075, 281, 4596, 264, 5308, 11, 51614], "temperature": 0.0, "avg_logprob": -0.10027966600783328, "compression_ratio": 1.936026936026936, "no_speech_prob": 0.005908811464905739}, {"id": 205, "seek": 118752, "start": 1212.52, "end": 1216.52, "text": " you'll get the actual request time, much closer to what the client sees on the other end.", "tokens": [51614, 291, 603, 483, 264, 3539, 5308, 565, 11, 709, 4966, 281, 437, 264, 6423, 8194, 322, 264, 661, 917, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10027966600783328, "compression_ratio": 1.936026936026936, "no_speech_prob": 0.005908811464905739}, {"id": 206, "seek": 121652, "start": 1216.52, "end": 1221.52, "text": " Rather than the fake time, which is what the application server would see normal.", "tokens": [50364, 16571, 813, 264, 7592, 565, 11, 597, 307, 437, 264, 3861, 7154, 576, 536, 2710, 13, 50614], "temperature": 0.0, "avg_logprob": -0.20567901317889875, "compression_ratio": 1.4180790960451977, "no_speech_prob": 0.01500190980732441}, {"id": 207, "seek": 121652, "start": 1221.52, "end": 1226.52, "text": " Okay, so that was the demo.", "tokens": [50614, 1033, 11, 370, 300, 390, 264, 10723, 13, 50864], "temperature": 0.0, "avg_logprob": -0.20567901317889875, "compression_ratio": 1.4180790960451977, "no_speech_prob": 0.01500190980732441}, {"id": 208, "seek": 121652, "start": 1226.52, "end": 1234.52, "text": " Let's summarize something that is that, using EVPF, you can capture distributed traces,", "tokens": [50864, 961, 311, 20858, 746, 300, 307, 300, 11, 1228, 15733, 47, 37, 11, 291, 393, 7983, 12631, 26076, 11, 51264], "temperature": 0.0, "avg_logprob": -0.20567901317889875, "compression_ratio": 1.4180790960451977, "no_speech_prob": 0.01500190980732441}, {"id": 209, "seek": 121652, "start": 1234.52, "end": 1238.52, "text": " as we, as Nicole explained it, with some limitations.", "tokens": [51264, 382, 321, 11, 382, 18532, 8825, 309, 11, 365, 512, 15705, 13, 51464], "temperature": 0.0, "avg_logprob": -0.20567901317889875, "compression_ratio": 1.4180790960451977, "no_speech_prob": 0.01500190980732441}, {"id": 210, "seek": 123852, "start": 1239.52, "end": 1246.52, "text": " The advantage is that it requires almost no effort from the developer or operator,", "tokens": [50414, 440, 5002, 307, 300, 309, 7029, 1920, 572, 4630, 490, 264, 10754, 420, 12973, 11, 50764], "temperature": 0.0, "avg_logprob": -0.13639195760091147, "compression_ratio": 1.608974358974359, "no_speech_prob": 0.07284671068191528}, {"id": 211, "seek": 123852, "start": 1246.52, "end": 1250.52, "text": " in the sense that you don't need to reconfigure your service, you don't need to change the code,", "tokens": [50764, 294, 264, 2020, 300, 291, 500, 380, 643, 281, 9993, 20646, 540, 428, 2643, 11, 291, 500, 380, 643, 281, 1319, 264, 3089, 11, 50964], "temperature": 0.0, "avg_logprob": -0.13639195760091147, "compression_ratio": 1.608974358974359, "no_speech_prob": 0.07284671068191528}, {"id": 212, "seek": 123852, "start": 1250.52, "end": 1257.52, "text": " you don't need to redeploy, just drop it and get whatever Rela can get.", "tokens": [50964, 291, 500, 380, 643, 281, 14328, 2384, 11, 445, 3270, 309, 293, 483, 2035, 497, 4053, 393, 483, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13639195760091147, "compression_ratio": 1.608974358974359, "no_speech_prob": 0.07284671068191528}, {"id": 213, "seek": 125752, "start": 1258.52, "end": 1269.52, "text": " Yeah, and it's, another conclusion is combining this packet tracing with language level support,", "tokens": [50414, 865, 11, 293, 309, 311, 11, 1071, 10063, 307, 21928, 341, 20300, 25262, 365, 2856, 1496, 1406, 11, 50964], "temperature": 0.0, "avg_logprob": -0.2360823495047433, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.05641991272568703}, {"id": 214, "seek": 125752, "start": 1269.52, "end": 1274.52, "text": " is what we, we allow Bayla get those distributed traces.", "tokens": [50964, 307, 437, 321, 11, 321, 2089, 7840, 875, 483, 729, 12631, 26076, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2360823495047433, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.05641991272568703}, {"id": 215, "seek": 125752, "start": 1274.52, "end": 1283.52, "text": " So if you like it and want to give a try, Bayla is available to download freely, to test it.", "tokens": [51214, 407, 498, 291, 411, 309, 293, 528, 281, 976, 257, 853, 11, 7840, 875, 307, 2435, 281, 5484, 16433, 11, 281, 1500, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2360823495047433, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.05641991272568703}, {"id": 216, "seek": 128352, "start": 1283.52, "end": 1293.52, "text": " You can, you can connect to our GitHub page, or, and then you will see instructions and links to the documentation", "tokens": [50364, 509, 393, 11, 291, 393, 1745, 281, 527, 23331, 3028, 11, 420, 11, 293, 550, 291, 486, 536, 9415, 293, 6123, 281, 264, 14333, 50864], "temperature": 0.0, "avg_logprob": -0.14288456656716086, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.02695632539689541}, {"id": 217, "seek": 128352, "start": 1293.52, "end": 1296.52, "text": " or the main open source page of Bayla.", "tokens": [50864, 420, 264, 2135, 1269, 4009, 3028, 295, 7840, 875, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14288456656716086, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.02695632539689541}, {"id": 218, "seek": 128352, "start": 1296.52, "end": 1301.52, "text": " Yeah, and on the GitHub page is what we start with, we have a link to our community Slack,", "tokens": [51014, 865, 11, 293, 322, 264, 23331, 3028, 307, 437, 321, 722, 365, 11, 321, 362, 257, 2113, 281, 527, 1768, 37211, 11, 51264], "temperature": 0.0, "avg_logprob": -0.14288456656716086, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.02695632539689541}, {"id": 219, "seek": 128352, "start": 1301.52, "end": 1306.52, "text": " if you want to chat with us, and we also are soon going to start organizing the community call.", "tokens": [51264, 498, 291, 528, 281, 5081, 365, 505, 11, 293, 321, 611, 366, 2321, 516, 281, 722, 17608, 264, 1768, 818, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14288456656716086, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.02695632539689541}, {"id": 220, "seek": 128352, "start": 1306.52, "end": 1311.52, "text": " So once every month we have a call where you can just join in and chat or yell at us,", "tokens": [51514, 407, 1564, 633, 1618, 321, 362, 257, 818, 689, 291, 393, 445, 3917, 294, 293, 5081, 420, 20525, 412, 505, 11, 51764], "temperature": 0.0, "avg_logprob": -0.14288456656716086, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.02695632539689541}, {"id": 221, "seek": 131152, "start": 1311.52, "end": 1316.52, "text": " for whatever reason, but yeah, that's it. Thank you.", "tokens": [50364, 337, 2035, 1778, 11, 457, 1338, 11, 300, 311, 309, 13, 1044, 291, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17775263505823471, "compression_ratio": 1.108695652173913, "no_speech_prob": 0.021989913657307625}, {"id": 222, "seek": 131152, "start": 1324.52, "end": 1327.52, "text": " Thanks a lot. Oh, so many questions. I'm running.", "tokens": [51014, 2561, 257, 688, 13, 876, 11, 370, 867, 1651, 13, 286, 478, 2614, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17775263505823471, "compression_ratio": 1.108695652173913, "no_speech_prob": 0.021989913657307625}, {"id": 223, "seek": 132752, "start": 1328.52, "end": 1343.52, "text": " You said that when you're tracing in Go, you, you are, you are tracing the coroutines that are, that are handling requests,", "tokens": [50414, 509, 848, 300, 562, 291, 434, 25262, 294, 1037, 11, 291, 11, 291, 366, 11, 291, 366, 25262, 264, 1181, 346, 1652, 300, 366, 11, 300, 366, 13175, 12475, 11, 51164], "temperature": 0.0, "avg_logprob": -0.20297207266597425, "compression_ratio": 1.635036496350365, "no_speech_prob": 0.05083855986595154}, {"id": 224, "seek": 132752, "start": 1343.52, "end": 1350.52, "text": " but in Go you don't have ideas of these coroutines and you don't have the relationship between them.", "tokens": [51164, 457, 294, 1037, 291, 500, 380, 362, 3487, 295, 613, 1181, 346, 1652, 293, 291, 500, 380, 362, 264, 2480, 1296, 552, 13, 51514], "temperature": 0.0, "avg_logprob": -0.20297207266597425, "compression_ratio": 1.635036496350365, "no_speech_prob": 0.05083855986595154}, {"id": 225, "seek": 135052, "start": 1351.52, "end": 1358.52, "text": " And to, to make it worse, the go around time actually reuses coroutines for something completely different.", "tokens": [50414, 400, 281, 11, 281, 652, 309, 5324, 11, 264, 352, 926, 565, 767, 319, 8355, 1181, 346, 1652, 337, 746, 2584, 819, 13, 50764], "temperature": 0.0, "avg_logprob": -0.19653087854385376, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.22427664697170258}, {"id": 226, "seek": 135052, "start": 1358.52, "end": 1365.52, "text": " So how do you, how do you do that without constantly handing pretty much all the coroutines all the time in order to get your trace?", "tokens": [50764, 407, 577, 360, 291, 11, 577, 360, 291, 360, 300, 1553, 6460, 34774, 1238, 709, 439, 264, 1181, 346, 1652, 439, 264, 565, 294, 1668, 281, 483, 428, 13508, 30, 51114], "temperature": 0.0, "avg_logprob": -0.19653087854385376, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.22427664697170258}, {"id": 227, "seek": 135052, "start": 1365.52, "end": 1370.52, "text": " Yeah, okay. So like with EVPF you get superpowers.", "tokens": [51114, 865, 11, 1392, 13, 407, 411, 365, 15733, 47, 37, 291, 483, 1687, 47953, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19653087854385376, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.22427664697170258}, {"id": 228, "seek": 135052, "start": 1370.52, "end": 1375.52, "text": " So from a regular goal developer perspective, you never actually have the access to this information.", "tokens": [51364, 407, 490, 257, 3890, 3387, 10754, 4585, 11, 291, 1128, 767, 362, 264, 2105, 281, 341, 1589, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19653087854385376, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.22427664697170258}, {"id": 229, "seek": 137552, "start": 1375.52, "end": 1378.52, "text": " Yeah, for whatever reason, they won't give it to you.", "tokens": [50364, 865, 11, 337, 2035, 1778, 11, 436, 1582, 380, 976, 309, 281, 291, 13, 50514], "temperature": 0.0, "avg_logprob": -0.13907136326342556, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.02692260593175888}, {"id": 230, "seek": 137552, "start": 1378.52, "end": 1381.52, "text": " But with EVPF, I attached the go runtime.", "tokens": [50514, 583, 365, 15733, 47, 37, 11, 286, 8570, 264, 352, 34474, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13907136326342556, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.02692260593175888}, {"id": 231, "seek": 137552, "start": 1381.52, "end": 1385.52, "text": " So the address in memory of the go routine is my ID.", "tokens": [50664, 407, 264, 2985, 294, 4675, 295, 264, 352, 9927, 307, 452, 7348, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13907136326342556, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.02692260593175888}, {"id": 232, "seek": 137552, "start": 1385.52, "end": 1392.52, "text": " Now I can tell when the go routine starts and when it gets parked back, when it's reused for something else, it can be reused and that's fine.", "tokens": [50864, 823, 286, 393, 980, 562, 264, 352, 9927, 3719, 293, 562, 309, 2170, 28491, 646, 11, 562, 309, 311, 319, 4717, 337, 746, 1646, 11, 309, 393, 312, 319, 4717, 293, 300, 311, 2489, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13907136326342556, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.02692260593175888}, {"id": 233, "seek": 137552, "start": 1392.52, "end": 1396.52, "text": " But at that time I'll clear up all the information because I know the go routine is done.", "tokens": [51214, 583, 412, 300, 565, 286, 603, 1850, 493, 439, 264, 1589, 570, 286, 458, 264, 352, 9927, 307, 1096, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13907136326342556, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.02692260593175888}, {"id": 234, "seek": 137552, "start": 1396.52, "end": 1399.52, "text": " Because like superpowers.", "tokens": [51414, 1436, 411, 1687, 47953, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13907136326342556, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.02692260593175888}, {"id": 235, "seek": 139952, "start": 1399.52, "end": 1403.52, "text": " Hey, thank you for your talk.", "tokens": [50364, 1911, 11, 1309, 291, 337, 428, 751, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1990204612807472, "compression_ratio": 1.664, "no_speech_prob": 0.014786739833652973}, {"id": 236, "seek": 139952, "start": 1403.52, "end": 1408.52, "text": " I'm one of those guys that manage a lot of infrastructure in code in general.", "tokens": [50564, 286, 478, 472, 295, 729, 1074, 300, 3067, 257, 688, 295, 6896, 294, 3089, 294, 2674, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1990204612807472, "compression_ratio": 1.664, "no_speech_prob": 0.014786739833652973}, {"id": 237, "seek": 139952, "start": 1408.52, "end": 1415.52, "text": " And when you say that, hey, you just have to eat that and just work sort of a box, it's kind of scares me because potentially it can cause problems.", "tokens": [50814, 400, 562, 291, 584, 300, 11, 4177, 11, 291, 445, 362, 281, 1862, 300, 293, 445, 589, 1333, 295, 257, 2424, 11, 309, 311, 733, 295, 35721, 385, 570, 7263, 309, 393, 3082, 2740, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1990204612807472, "compression_ratio": 1.664, "no_speech_prob": 0.014786739833652973}, {"id": 238, "seek": 139952, "start": 1415.52, "end": 1426.52, "text": " And one of the issues that we saw with both kind of solutions usually is if you inject into request a tracing header, potentially the request might be changed.", "tokens": [51164, 400, 472, 295, 264, 2663, 300, 321, 1866, 365, 1293, 733, 295, 6547, 2673, 307, 498, 291, 10711, 666, 5308, 257, 25262, 23117, 11, 7263, 264, 5308, 1062, 312, 3105, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1990204612807472, "compression_ratio": 1.664, "no_speech_prob": 0.014786739833652973}, {"id": 239, "seek": 142652, "start": 1426.52, "end": 1431.52, "text": " And some protocols do signing and request like AWS signature free, for example.", "tokens": [50364, 400, 512, 20618, 360, 13393, 293, 5308, 411, 17650, 13397, 1737, 11, 337, 1365, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1650019645690918, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.09229671210050583}, {"id": 240, "seek": 142652, "start": 1431.52, "end": 1436.52, "text": " And they don't really like you injecting headers in the middle of request, especially at a lower level.", "tokens": [50614, 400, 436, 500, 380, 534, 411, 291, 10711, 278, 45101, 294, 264, 2808, 295, 5308, 11, 2318, 412, 257, 3126, 1496, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1650019645690918, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.09229671210050583}, {"id": 241, "seek": 142652, "start": 1436.52, "end": 1445.52, "text": " So how do you envision if you have some kind of like agent in the code itself, then you can work on that by disabling the tracing on both specific endpoints.", "tokens": [50864, 407, 577, 360, 291, 24739, 498, 291, 362, 512, 733, 295, 411, 9461, 294, 264, 3089, 2564, 11, 550, 291, 393, 589, 322, 300, 538, 717, 20112, 264, 25262, 322, 1293, 2685, 917, 20552, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1650019645690918, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.09229671210050583}, {"id": 242, "seek": 142652, "start": 1445.52, "end": 1453.52, "text": " But if you do that at a lower level, then you don't really have a visibility to be able to disable that or recognize that you are creating a request to such a back end.", "tokens": [51314, 583, 498, 291, 360, 300, 412, 257, 3126, 1496, 11, 550, 291, 500, 380, 534, 362, 257, 19883, 281, 312, 1075, 281, 28362, 300, 420, 5521, 300, 291, 366, 4084, 257, 5308, 281, 1270, 257, 646, 917, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1650019645690918, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.09229671210050583}, {"id": 243, "seek": 145352, "start": 1453.52, "end": 1457.52, "text": " How do you envision like working around those issues in the future?", "tokens": [50364, 1012, 360, 291, 24739, 411, 1364, 926, 729, 2663, 294, 264, 2027, 30, 50564], "temperature": 0.0, "avg_logprob": -0.1944849756028917, "compression_ratio": 1.596958174904943, "no_speech_prob": 0.09033985435962677}, {"id": 244, "seek": 145352, "start": 1457.52, "end": 1460.52, "text": " Because this is one example, but this will happen many, many times.", "tokens": [50564, 1436, 341, 307, 472, 1365, 11, 457, 341, 486, 1051, 867, 11, 867, 1413, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1944849756028917, "compression_ratio": 1.596958174904943, "no_speech_prob": 0.09033985435962677}, {"id": 245, "seek": 145352, "start": 1460.52, "end": 1462.52, "text": " Yeah, yeah. So that's true.", "tokens": [50714, 865, 11, 1338, 13, 407, 300, 311, 2074, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1944849756028917, "compression_ratio": 1.596958174904943, "no_speech_prob": 0.09033985435962677}, {"id": 246, "seek": 145352, "start": 1462.52, "end": 1470.52, "text": " So if you need sign some IDs and whatever, it's not letting you change the header information, then disable that feature.", "tokens": [50814, 407, 498, 291, 643, 1465, 512, 48212, 293, 2035, 11, 309, 311, 406, 8295, 291, 1319, 264, 23117, 1589, 11, 550, 28362, 300, 4111, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1944849756028917, "compression_ratio": 1.596958174904943, "no_speech_prob": 0.09033985435962677}, {"id": 247, "seek": 145352, "start": 1470.52, "end": 1474.52, "text": " Don't use what we do right now for propagating using the headers. Use the black box.", "tokens": [51214, 1468, 380, 764, 437, 321, 360, 558, 586, 337, 12425, 990, 1228, 264, 45101, 13, 8278, 264, 2211, 2424, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1944849756028917, "compression_ratio": 1.596958174904943, "no_speech_prob": 0.09033985435962677}, {"id": 248, "seek": 145352, "start": 1474.52, "end": 1476.52, "text": " This is the back boxes are sort of the full back.", "tokens": [51414, 639, 307, 264, 646, 9002, 366, 1333, 295, 264, 1577, 646, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1944849756028917, "compression_ratio": 1.596958174904943, "no_speech_prob": 0.09033985435962677}, {"id": 249, "seek": 147652, "start": 1476.52, "end": 1486.52, "text": " We've been toying with the idea that maybe in the future we'll let it work with an external storage of some kind that we can actually make past the one node restriction we have with the black box right now.", "tokens": [50364, 492, 600, 668, 12058, 278, 365, 264, 1558, 300, 1310, 294, 264, 2027, 321, 603, 718, 309, 589, 365, 364, 8320, 6725, 295, 512, 733, 300, 321, 393, 767, 652, 1791, 264, 472, 9984, 29529, 321, 362, 365, 264, 2211, 2424, 558, 586, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14979496336819834, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.14257225394248962}, {"id": 250, "seek": 147652, "start": 1486.52, "end": 1492.52, "text": " But that's the very reason we're designing for because in so many environments, injecting the header information is just not possible.", "tokens": [50864, 583, 300, 311, 264, 588, 1778, 321, 434, 14685, 337, 570, 294, 370, 867, 12388, 11, 10711, 278, 264, 23117, 1589, 307, 445, 406, 1944, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14979496336819834, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.14257225394248962}, {"id": 251, "seek": 147652, "start": 1492.52, "end": 1495.52, "text": " I'm dealing with interpreter language.", "tokens": [51164, 286, 478, 6260, 365, 34132, 2856, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14979496336819834, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.14257225394248962}, {"id": 252, "seek": 147652, "start": 1495.52, "end": 1501.52, "text": " No compiled methods, no dice. So I can't do anything with you.", "tokens": [51314, 883, 36548, 7150, 11, 572, 10313, 13, 407, 286, 393, 380, 360, 1340, 365, 291, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14979496336819834, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.14257225394248962}, {"id": 253, "seek": 147652, "start": 1501.52, "end": 1503.52, "text": " Thanks. Good question.", "tokens": [51614, 2561, 13, 2205, 1168, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14979496336819834, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.14257225394248962}, {"id": 254, "seek": 147652, "start": 1503.52, "end": 1504.52, "text": " Thank you.", "tokens": [51714, 1044, 291, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14979496336819834, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.14257225394248962}, {"id": 255, "seek": 150652, "start": 1506.52, "end": 1507.52, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.5273807843526205, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9860324263572693}], "language": "en"}