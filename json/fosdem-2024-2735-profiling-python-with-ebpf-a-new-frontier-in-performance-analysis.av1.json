{"text": " Well, everyone, we're about to start the next session. So we have here Kemal Okoyom, who's going to be speaking about profiling Python with EBF, profiling certainly one of the most challenging parts I had with Python in the last 20 years when I've been coaching with the programming language. So I really can't wait to hear what you have to share with us. Thank you very much and welcome. Hello everyone. So let's start with some questions. Who does anyone here like knows anything about EBF? Okay, quite a lot of people. Have you ever used profiling? Wow, nice. Do you know anything about Python? Great, everyone. That's nice. Okay, first, who I am. I'm not Prometheus, but I'm a Prometheus maintainer. Do you know or use Prometheus? Okay, that's also great. So I'm a maintainer of Prometheus. I'm also a maintainer of Thanos and recently I'm a maintainer of Parkout Project. These are all open source projects and they are all focused on observability. And I think I know something about observability, so today I will tell you about that. So let's start always with why. Why do we need profiling? It's either for some performance optimization. This graph can be anything generic. This could be CPU. This could be memory. When you see some spikes and you try to understand what's going on actually on those spikes. Or this could be something about an incident. This is graph is specifically for a umkil that your process and you don't know what that happened at that certain point and you would like to know about like who, what function or which component of your process actually allocates the memory in that particular moment. So some of you already know that there exists some profiling solution in Python. This is not an exhaustive list, but most of the libraries or projects that you see in here you actually need to instrument your code. So either you need to import a library or you need to specifically write some code and then start profiling your application, Python application. This is not always the ideal case because you would like sometimes you don't have access to code itself and sometimes you would like to do this from outside. So how do you do that? This is where eBPF actually comes into place and helps us. So eBPF is a, it's originally for networking applications. It's called Berkeley package filtering, but now eBPF totally something else. It's basically an event-based system where you can hook into some events that Linux kernel issues and then you can just run code as a reaction to those events. There is a runtime, there's a virtual machine inside and there's a verifier before you load your program that verifier needs to check your program that it doesn't do anything harmful like infinite loops whatnot in the kernel space. Then compiler kicks in and compile your provided code and you actually run those code against any events that you issue. That's the one fancy part of the solution. So then it comes another subsystem of Linux which is super cool is the perf event. You can have perf subsystem. From the perf subsystem you can have, you can hook into various parts of your stack and you can run code against these events. In this particle talk we are going to talk about the CPU events, but you don't need to use only the CPU events necessarily. You can do this for the IO events, you can do this for the memory allocation. Practically anything that you see in here, you can run, hook into that event and run a code piece to that. So what makes perf events special? It's actually performance monitoring units. These are very efficiently implemented units in the Linux kernel so that they keep track of the cycles and then like you can actually take measurements and you can react to those measurements. That's why EVPF plus perf events is actually faster than the other solutions that we have in basically ecosystem. Because with the Linux you can already have some syscalls, you can just hook into that introps and do all these things that I'm going to tell you in a minute in the user space. But the most of the things we do is using the PMUs and because the PMUs are efficient and EVPF codes also efficient and run in the kernel space, this gives us a bit of a headroom for the performance. So we are not the only ones that actually implemented that. This is quite a journey. I don't remember when is the first time that it actually introduced the PyPerf code inside the Linux kernel. You can check out, these are actual links and I'm going to share the slides and you can see basically the git comments against that. There's a set of tools called BCC tools in the EVPF space and there's also another implementation of PyPerf in there. But what is the downside of all these tooling? These are first, they are dated. They don't cover all the recent changes in the Python runtime itself and they are just one of tools. So I'm going to show you some cool things that you can actually do profiling in the production itself in a continuous manner, which we call continuous profiling. We're going to come to that in a minute. So to make the tools work, you need to just wrap your Python interpreter around with these tools and then collect your profiles. So that gets us to the Parker project. It's an open source project. It's a continuous profiling project using EVPF and perf events. We can run your profiling workloads directly in production and there is no runtime, nearly none of the runtime overhead in this approach. There's a tiny bit, but it's really negligible. So how does the Parker EVPF agent actually work? Other things that I mentioned previously, the hook into perf events, we have some unwinded programs that actually unwinds the stack, which I will tell you about. And we then keep track of what happens in the CPU for that stack. And then we aggregate those information and put in an EVPF map, which are the special data structures that you talk between Kernel and the user space. And then we read that data, we convert that some open profiling formats and push that in a server site where we can just aggregate and visualize that and let the users actually make sense of their programs. So this is how that whole thing actually works. There are a lot of details, but this talk is not actually about the internals of Parker, but we are doing a lot of cool stuff to make the stack collection and symbolization very efficient. And then the end result is a UI like this. In a continuous timeline, you can see that what's going on on your CPU for each process. Then we collect a lot of metadata and enrich those information for you so that you can query, compare, and see how you can improve your program. And the agent is kind of super cool because you can just install any host machine and any process that you have on that machine, we can just collect data and send to the server and you can see that in the UI. This doesn't necessarily, it's not scoped to the Python itself, but it does a lot of cool stuff with the Python as well. So there's not a Python stack, but we will see some examples, but this is some, I think this is a Go one, but you can see that the stacks are easily getting really deep. So what is the stack unwinding? This is the next critical thing that we need to talk about because the whole, like the what makes profiling challenging, especially from the Python side, is actually to be able to unwind the stack. So when a program gets executed, probably you all heard these in your start of your education. There are specific structures when the process actually allocates in the memory, which is one is stack and one is heap and the stack actually tracks the execution of the program and whenever you call a function, you open a frame and you change the states of your registers and then you keep adding everything to the stack and when one of the functions that returns from the leaf, you just go back and return the data to your user. So I might be oversimplified that, but it's a diagram just to show you how it looked like, but the end result when you unwind the stack and aggregate all these function addresses, you get something like that. It's just the machine addresses and now you need to find a way to translate those machine addresses to the human readable format. So all these parts for the native code, so anything that actually runs on your CPU. So that brings us actually the next step. So this is a state where we didn't implement the Python unwind for parka and you can actually, this is an interactive one. So you can see anything that gets from like your kernel, there's a start thread, this is coming from libc and all these green things that you see, these are coming from the Python interpreter itself because Python interpreter is written in C and it's compiled and then directly gets executed on the CPU, but probably this is not useful for you, right? You are Python developers. You actually want to see what's happening in the Python process itself, not the underlying infrastructure. That being said, we also know that most of the Python applications also rely on the C bits and the native code bits and it could be calling some C function here and there. Then when that happens, these are actually, gets like super important, right? For example, PyTorch, it's very popular nowadays in the machine learning workload, but it's actually funneling everything into a native code and when you want to see what's going on the native code, parka actually can do that as well. And we do that in a very efficient way. You don't need to have, there's a whole concept of frame pointers and that actually helps us to unwind the stack. We just gave another talk in the observability room like why frame pointers are cool, but you don't have to have a need to have the frame pointers itself because there's also another facility with the devolve debug information, you can unwind the stack. So parka actually utilizes that. This is important because most of the packages that you can find on any of the Linux distribution, you wouldn't find frame pointers. But with the devolve information, you can actually unwind the stack and you can see all these goals. So, but we want more, right? We want to see the Python code, so how we do actually do that. So this is where it comes to, where we unwind the stack virtually, with virtual stack, we mean that anything that gets executed in the Python interpreter, we need to find those stacks and put those things in our flame graphs so we can see that like where is the problematic areas in our Python code itself. So everything starts with opening the Python runtime and reading the code. This is the huge structure, like if you know the Python internals, like it's long. There are a lot of comments, but it's not the easiest code to read and it's not the easiest code to reason about. Let's focus on like what is the important bits, right? We care about the interpreter state and from that bit, we want to capture what's going on in each thread, right? It comes from the interpreter state and then we try to find the PyState itself. The PyThread structure, it's like a link list, so whenever you have multiple threads running in an interpreter, you need to traverse the whole link list and for each thread, you actually need to do that. But also you need to find out which thread actually captures the guild and globally interpreter luck so that that's the one actually executing the code. So from finding all those information from thread state, you check the thread state. Oh, yeah, it's another like pages long C code that we need to reason about. It's not the easiest thing, but this is how reverse engineering kind of works. And again, I extracted the important bits. So from that thread state, we need to find what is the current frame is actually executing so that we can online from there, right? That's actually the same thing that we are doing with the native stack, but rather than checking some registries and reading roll memory addresses, we are actually checking the Python in turn itself. So from the interpreter frame, it's actually easy, like whatever we need, it's here. So all the information we need is here. We have the pointers to the previous stack and we can actually do the same thing. So I'm going to speed things up. Yeah, we have the map. We know the source code, but where do we actually start? When we have an object file from the Python interpreter, we first need to find the where does all these tracks are actually live. So we check the entry point of a Python interpreter. We see that it's linked against a live Python. We go and check the names for one of these tracks symbols and we actually see that there are some offsets that are located there. But this is just from the binary. We don't know what these addresses mean when a process started. So this happens because this is just one of the reasons, but when you get a binary and run the process out of the binary, there is some address randomization and all those addresses need to be translated to that. So how do we do that? We just run a Python interpreter and check what's going on in the process. This is basically memory mappings and it shows you where actually Linux maps the certain objects and we check out the live Python. Grab the base address and from the addresses that we find from the symbol or draft, the morph information, we actually find where the structs are actually located in the memory. We are looking for those. And from that, now we need to read that data. So here comes the GDB. GDB is like an amazing debugging tool and we jump into the process and start to poke around, we define a macro and to calculate the offsets of a struct which reads from the devolve information, you say that, okay, give me this struct and this field and it gives you the offset of that. Since we already have the start address of the memory, we just calculate the next address and read the data from that. But as you can see, this is very manual labor. We cannot do this for each and every Python version or implementation out there. So we do this with another project ahead of time. We use Rust-Pyne-Gen for that which was super convenient because PySpy was using that. We just grabbed some offsets and generated all these things for a couple of versions. But we are also working on a devolve-based reader which is more scalable. You just grab any binary, read the debug information and calculate all the offsets. From those offsets, we generate this struct which we think over the kernel space. It's like a map, where to find the fields and everything. And the nice part that the whole things that we are working on, it's going to be deprecated soon because Python... This is life of a software engineer when you do reverse engineering. So something super cool happened in the Python main branch. Now they have this debug offset data structure value. They generate all those sets and put that in just the beginning of the py runtime. We just can grab the address and just read the first chunk of the thing. How we got those that we don't need to do this ahead of time things right now. So this is already a merge and it's going to be released with the Python 3.0 in 13.0. It's also huge, lots of stuff that you need to find out. Okay, actual unwinding stack. So these are where the EVPS comes from. We did all the magic, we got the offsets, we put that thing into interpreter info structure and put that in an EVPF map so that the EVPF program can read in the kernel. And this C code is in the kernel itself. You check something and get the interpreter info. This is the user space code where we actually calculate all the addresses and send and put to the EVPF map itself. And from that we also grab this offset data that we calculated. We just do check some versions and find the runtime version of that particular Python interpreter, read the offsets and from that offsets we calculate that. But again, like 3.13, this is, this will be futile. So then we read, try to read the, oops, rate data from the thread state, find those structures and read the pointers and try to find out where to go from there. Five minutes left, I need to be super quick. Okay, so we find the, we try to find the initial pointer for the virtual frames. This is how we do it. And then from that we start walking the stack. The key points here is just from the previous code that you can see, we actually put just line 13, put something into a state frame pointer and we then read that frame pointer from another EVPF program. And from that pointer we basically find the offset of the frame object where the code points to, read that row address with some VPF helper code. And from that we read the symbol because like the addresses that we saw that they don't mean anything to us. We are humans and we need some human readable data. And from that to be efficient because we keep seeing the same stack traces, we just like hash it and put that symbol somewhere so that if we see that we don't need to send that same symbol to the user space because it's costly. And then we also encode the line number because like symbols just represents a class, then a function and then there's a line number in that function that is different. This API also recently stabilizing the Python. So for the old Python versions this line number could be wrong but the read after 3.10 it's actually, it should be accurate. And then we encode that as well and send it to the user space. This is the reading the symbol parts. Like the code is like super complicated. I just highlighted the GDP outputs because it's like easier to read. So you read this nested structures and find the actual type name, then the file name, then the name of the object code and the first line in that function and put an encode that is a symbol so that it means something for the humans. Voila, now we finally have Python unwinded stack. But as you can see there are lots of things going, happening and most of the things are interpreters and it doesn't make any sense. But we have this cool UI, you can actually, these are like color coded, you can, from the color code you can actually highlight like what's going on in the interpreter, what happens to the libc, libpython, you can see everything. Again, we want to focus on the Python bits, right? You can just filter out the Python code and see that it's recursively calling and calculating some Fibonacci numbers. Apparently, yeah, it's inefficient so you need to optimize this. Yeah, you can just tell by like that list of the stack, okay, yeah. You don't need to know the details of how to read the flame graph. But good thing for you, we also have a blog post for that. You can check it out. So I guess we are nearly out of time. So we support a couple of interpreters like 2.7, we still support that. So if you happen to have that, we support everything until 3.11. We are working on the 3.12 because 3.12 changes where the actual trade state also stored, which is the trade local storage. We are working on the facilities to read the state from the trade local storage. It shouldn't take more than a couple of weeks to be that support actually landed. And 3.13 will be there so we don't need to do this again, basically, for the next version of the Python. So everything you can see, please try, install and give us a feedback. There's this QR code. It's a GitHub discussion. You can just like engage with us and report bugs. And we can try to help you to profile and optimize your Python workloads. All the things that you see here, there's also a blog post if you want to catch up. You can check the company's blog post. And also we find the DevoreFind on winding bits like super cool because it's a niche thing that we do. And if you especially like the, if you have an application that follows everything to the native code, this will be super useful. Thank you for listening. Thank you very much, Camille. I'm afraid we're quite tight on the schedule, but please feel free to reach out to him with any questions. And thank you very much. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.36, "text": " Well, everyone, we're about to start the next session.", "tokens": [50364, 1042, 11, 1518, 11, 321, 434, 466, 281, 722, 264, 958, 5481, 13, 50782], "temperature": 0.0, "avg_logprob": -0.2528648376464844, "compression_ratio": 1.517094017094017, "no_speech_prob": 0.2750011682510376}, {"id": 1, "seek": 0, "start": 8.36, "end": 14.76, "text": " So we have here Kemal Okoyom, who's going to be speaking about profiling Python with", "tokens": [50782, 407, 321, 362, 510, 30097, 304, 3477, 939, 298, 11, 567, 311, 516, 281, 312, 4124, 466, 1740, 4883, 15329, 365, 51102], "temperature": 0.0, "avg_logprob": -0.2528648376464844, "compression_ratio": 1.517094017094017, "no_speech_prob": 0.2750011682510376}, {"id": 2, "seek": 0, "start": 14.76, "end": 22.88, "text": " EBF, profiling certainly one of the most challenging parts I had with Python in the last 20 years", "tokens": [51102, 462, 33, 37, 11, 1740, 4883, 3297, 472, 295, 264, 881, 7595, 3166, 286, 632, 365, 15329, 294, 264, 1036, 945, 924, 51508], "temperature": 0.0, "avg_logprob": -0.2528648376464844, "compression_ratio": 1.517094017094017, "no_speech_prob": 0.2750011682510376}, {"id": 3, "seek": 0, "start": 22.88, "end": 25.36, "text": " when I've been coaching with the programming language.", "tokens": [51508, 562, 286, 600, 668, 15818, 365, 264, 9410, 2856, 13, 51632], "temperature": 0.0, "avg_logprob": -0.2528648376464844, "compression_ratio": 1.517094017094017, "no_speech_prob": 0.2750011682510376}, {"id": 4, "seek": 0, "start": 25.36, "end": 28.76, "text": " So I really can't wait to hear what you have to share with us.", "tokens": [51632, 407, 286, 534, 393, 380, 1699, 281, 1568, 437, 291, 362, 281, 2073, 365, 505, 13, 51802], "temperature": 0.0, "avg_logprob": -0.2528648376464844, "compression_ratio": 1.517094017094017, "no_speech_prob": 0.2750011682510376}, {"id": 5, "seek": 2876, "start": 28.76, "end": 36.44, "text": " Thank you very much and welcome.", "tokens": [50364, 1044, 291, 588, 709, 293, 2928, 13, 50748], "temperature": 0.0, "avg_logprob": -0.4381077225143845, "compression_ratio": 1.409356725146199, "no_speech_prob": 0.003036148613318801}, {"id": 6, "seek": 2876, "start": 36.44, "end": 37.44, "text": " Hello everyone.", "tokens": [50748, 2425, 1518, 13, 50798], "temperature": 0.0, "avg_logprob": -0.4381077225143845, "compression_ratio": 1.409356725146199, "no_speech_prob": 0.003036148613318801}, {"id": 7, "seek": 2876, "start": 37.44, "end": 41.120000000000005, "text": " So let's start with some questions.", "tokens": [50798, 407, 718, 311, 722, 365, 512, 1651, 13, 50982], "temperature": 0.0, "avg_logprob": -0.4381077225143845, "compression_ratio": 1.409356725146199, "no_speech_prob": 0.003036148613318801}, {"id": 8, "seek": 2876, "start": 41.120000000000005, "end": 45.36, "text": " Who does anyone here like knows anything about EBF?", "tokens": [50982, 2102, 775, 2878, 510, 411, 3255, 1340, 466, 462, 33, 37, 30, 51194], "temperature": 0.0, "avg_logprob": -0.4381077225143845, "compression_ratio": 1.409356725146199, "no_speech_prob": 0.003036148613318801}, {"id": 9, "seek": 2876, "start": 45.36, "end": 49.24, "text": " Okay, quite a lot of people.", "tokens": [51194, 1033, 11, 1596, 257, 688, 295, 561, 13, 51388], "temperature": 0.0, "avg_logprob": -0.4381077225143845, "compression_ratio": 1.409356725146199, "no_speech_prob": 0.003036148613318801}, {"id": 10, "seek": 2876, "start": 49.24, "end": 51.88, "text": " Have you ever used profiling?", "tokens": [51388, 3560, 291, 1562, 1143, 1740, 4883, 30, 51520], "temperature": 0.0, "avg_logprob": -0.4381077225143845, "compression_ratio": 1.409356725146199, "no_speech_prob": 0.003036148613318801}, {"id": 11, "seek": 2876, "start": 51.88, "end": 53.88, "text": " Wow, nice.", "tokens": [51520, 3153, 11, 1481, 13, 51620], "temperature": 0.0, "avg_logprob": -0.4381077225143845, "compression_ratio": 1.409356725146199, "no_speech_prob": 0.003036148613318801}, {"id": 12, "seek": 2876, "start": 53.88, "end": 57.88, "text": " Do you know anything about Python?", "tokens": [51620, 1144, 291, 458, 1340, 466, 15329, 30, 51820], "temperature": 0.0, "avg_logprob": -0.4381077225143845, "compression_ratio": 1.409356725146199, "no_speech_prob": 0.003036148613318801}, {"id": 13, "seek": 5788, "start": 57.88, "end": 60.52, "text": " Great, everyone.", "tokens": [50364, 3769, 11, 1518, 13, 50496], "temperature": 0.0, "avg_logprob": -0.25020949694575095, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.005816702730953693}, {"id": 14, "seek": 5788, "start": 60.52, "end": 61.52, "text": " That's nice.", "tokens": [50496, 663, 311, 1481, 13, 50546], "temperature": 0.0, "avg_logprob": -0.25020949694575095, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.005816702730953693}, {"id": 15, "seek": 5788, "start": 61.52, "end": 65.0, "text": " Okay, first, who I am.", "tokens": [50546, 1033, 11, 700, 11, 567, 286, 669, 13, 50720], "temperature": 0.0, "avg_logprob": -0.25020949694575095, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.005816702730953693}, {"id": 16, "seek": 5788, "start": 65.0, "end": 69.4, "text": " I'm not Prometheus, but I'm a Prometheus maintainer.", "tokens": [50720, 286, 478, 406, 2114, 649, 42209, 11, 457, 286, 478, 257, 2114, 649, 42209, 6909, 260, 13, 50940], "temperature": 0.0, "avg_logprob": -0.25020949694575095, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.005816702730953693}, {"id": 17, "seek": 5788, "start": 69.4, "end": 71.8, "text": " Do you know or use Prometheus?", "tokens": [50940, 1144, 291, 458, 420, 764, 2114, 649, 42209, 30, 51060], "temperature": 0.0, "avg_logprob": -0.25020949694575095, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.005816702730953693}, {"id": 18, "seek": 5788, "start": 71.8, "end": 75.08, "text": " Okay, that's also great.", "tokens": [51060, 1033, 11, 300, 311, 611, 869, 13, 51224], "temperature": 0.0, "avg_logprob": -0.25020949694575095, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.005816702730953693}, {"id": 19, "seek": 5788, "start": 75.08, "end": 78.48, "text": " So I'm a maintainer of Prometheus.", "tokens": [51224, 407, 286, 478, 257, 6909, 260, 295, 2114, 649, 42209, 13, 51394], "temperature": 0.0, "avg_logprob": -0.25020949694575095, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.005816702730953693}, {"id": 20, "seek": 5788, "start": 78.48, "end": 83.52000000000001, "text": " I'm also a maintainer of Thanos and recently I'm a maintainer of Parkout Project.", "tokens": [51394, 286, 478, 611, 257, 6909, 260, 295, 35993, 293, 3938, 286, 478, 257, 6909, 260, 295, 4964, 346, 9849, 13, 51646], "temperature": 0.0, "avg_logprob": -0.25020949694575095, "compression_ratio": 1.7267080745341614, "no_speech_prob": 0.005816702730953693}, {"id": 21, "seek": 8352, "start": 83.52, "end": 88.47999999999999, "text": " These are all open source projects and they are all focused on observability.", "tokens": [50364, 1981, 366, 439, 1269, 4009, 4455, 293, 436, 366, 439, 5178, 322, 9951, 2310, 13, 50612], "temperature": 0.0, "avg_logprob": -0.15707567463750424, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.021569840610027313}, {"id": 22, "seek": 8352, "start": 88.47999999999999, "end": 96.72, "text": " And I think I know something about observability, so today I will tell you about that.", "tokens": [50612, 400, 286, 519, 286, 458, 746, 466, 9951, 2310, 11, 370, 965, 286, 486, 980, 291, 466, 300, 13, 51024], "temperature": 0.0, "avg_logprob": -0.15707567463750424, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.021569840610027313}, {"id": 23, "seek": 8352, "start": 96.72, "end": 99.75999999999999, "text": " So let's start always with why.", "tokens": [51024, 407, 718, 311, 722, 1009, 365, 983, 13, 51176], "temperature": 0.0, "avg_logprob": -0.15707567463750424, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.021569840610027313}, {"id": 24, "seek": 8352, "start": 99.75999999999999, "end": 103.64, "text": " Why do we need profiling?", "tokens": [51176, 1545, 360, 321, 643, 1740, 4883, 30, 51370], "temperature": 0.0, "avg_logprob": -0.15707567463750424, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.021569840610027313}, {"id": 25, "seek": 8352, "start": 103.64, "end": 107.32, "text": " It's either for some performance optimization.", "tokens": [51370, 467, 311, 2139, 337, 512, 3389, 19618, 13, 51554], "temperature": 0.0, "avg_logprob": -0.15707567463750424, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.021569840610027313}, {"id": 26, "seek": 8352, "start": 107.32, "end": 109.28, "text": " This graph can be anything generic.", "tokens": [51554, 639, 4295, 393, 312, 1340, 19577, 13, 51652], "temperature": 0.0, "avg_logprob": -0.15707567463750424, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.021569840610027313}, {"id": 27, "seek": 8352, "start": 109.28, "end": 111.0, "text": " This could be CPU.", "tokens": [51652, 639, 727, 312, 13199, 13, 51738], "temperature": 0.0, "avg_logprob": -0.15707567463750424, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.021569840610027313}, {"id": 28, "seek": 8352, "start": 111.0, "end": 112.44, "text": " This could be memory.", "tokens": [51738, 639, 727, 312, 4675, 13, 51810], "temperature": 0.0, "avg_logprob": -0.15707567463750424, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.021569840610027313}, {"id": 29, "seek": 11244, "start": 112.44, "end": 119.16, "text": " When you see some spikes and you try to understand what's going on actually on those spikes.", "tokens": [50364, 1133, 291, 536, 512, 28997, 293, 291, 853, 281, 1223, 437, 311, 516, 322, 767, 322, 729, 28997, 13, 50700], "temperature": 0.0, "avg_logprob": -0.26662745683089545, "compression_ratio": 1.7172995780590716, "no_speech_prob": 0.015401766635477543}, {"id": 30, "seek": 11244, "start": 119.16, "end": 122.44, "text": " Or this could be something about an incident.", "tokens": [50700, 1610, 341, 727, 312, 746, 466, 364, 9348, 13, 50864], "temperature": 0.0, "avg_logprob": -0.26662745683089545, "compression_ratio": 1.7172995780590716, "no_speech_prob": 0.015401766635477543}, {"id": 31, "seek": 11244, "start": 122.44, "end": 128.12, "text": " This is graph is specifically for a umkil that your process and you don't know what", "tokens": [50864, 639, 307, 4295, 307, 4682, 337, 257, 1105, 33534, 300, 428, 1399, 293, 291, 500, 380, 458, 437, 51148], "temperature": 0.0, "avg_logprob": -0.26662745683089545, "compression_ratio": 1.7172995780590716, "no_speech_prob": 0.015401766635477543}, {"id": 32, "seek": 11244, "start": 128.12, "end": 133.56, "text": " that happened at that certain point and you would like to know about like who, what function", "tokens": [51148, 300, 2011, 412, 300, 1629, 935, 293, 291, 576, 411, 281, 458, 466, 411, 567, 11, 437, 2445, 51420], "temperature": 0.0, "avg_logprob": -0.26662745683089545, "compression_ratio": 1.7172995780590716, "no_speech_prob": 0.015401766635477543}, {"id": 33, "seek": 11244, "start": 133.56, "end": 140.96, "text": " or which component of your process actually allocates the memory in that particular moment.", "tokens": [51420, 420, 597, 6542, 295, 428, 1399, 767, 12660, 1024, 264, 4675, 294, 300, 1729, 1623, 13, 51790], "temperature": 0.0, "avg_logprob": -0.26662745683089545, "compression_ratio": 1.7172995780590716, "no_speech_prob": 0.015401766635477543}, {"id": 34, "seek": 14096, "start": 140.96, "end": 148.8, "text": " So some of you already know that there exists some profiling solution in Python.", "tokens": [50364, 407, 512, 295, 291, 1217, 458, 300, 456, 8198, 512, 1740, 4883, 3827, 294, 15329, 13, 50756], "temperature": 0.0, "avg_logprob": -0.16473279516380954, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.0028352681547403336}, {"id": 35, "seek": 14096, "start": 148.8, "end": 154.0, "text": " This is not an exhaustive list, but most of the libraries or projects that you see in", "tokens": [50756, 639, 307, 406, 364, 14687, 488, 1329, 11, 457, 881, 295, 264, 15148, 420, 4455, 300, 291, 536, 294, 51016], "temperature": 0.0, "avg_logprob": -0.16473279516380954, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.0028352681547403336}, {"id": 36, "seek": 14096, "start": 154.0, "end": 157.72, "text": " here you actually need to instrument your code.", "tokens": [51016, 510, 291, 767, 643, 281, 7198, 428, 3089, 13, 51202], "temperature": 0.0, "avg_logprob": -0.16473279516380954, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.0028352681547403336}, {"id": 37, "seek": 14096, "start": 157.72, "end": 163.12, "text": " So either you need to import a library or you need to specifically write some code and", "tokens": [51202, 407, 2139, 291, 643, 281, 974, 257, 6405, 420, 291, 643, 281, 4682, 2464, 512, 3089, 293, 51472], "temperature": 0.0, "avg_logprob": -0.16473279516380954, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.0028352681547403336}, {"id": 38, "seek": 14096, "start": 163.12, "end": 168.4, "text": " then start profiling your application, Python application.", "tokens": [51472, 550, 722, 1740, 4883, 428, 3861, 11, 15329, 3861, 13, 51736], "temperature": 0.0, "avg_logprob": -0.16473279516380954, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.0028352681547403336}, {"id": 39, "seek": 16840, "start": 168.68, "end": 174.68, "text": " This is not always the ideal case because you would like sometimes you don't have access", "tokens": [50378, 639, 307, 406, 1009, 264, 7157, 1389, 570, 291, 576, 411, 2171, 291, 500, 380, 362, 2105, 50678], "temperature": 0.0, "avg_logprob": -0.1946643590927124, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.003876362694427371}, {"id": 40, "seek": 16840, "start": 174.68, "end": 178.96, "text": " to code itself and sometimes you would like to do this from outside.", "tokens": [50678, 281, 3089, 2564, 293, 2171, 291, 576, 411, 281, 360, 341, 490, 2380, 13, 50892], "temperature": 0.0, "avg_logprob": -0.1946643590927124, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.003876362694427371}, {"id": 41, "seek": 16840, "start": 178.96, "end": 181.8, "text": " So how do you do that?", "tokens": [50892, 407, 577, 360, 291, 360, 300, 30, 51034], "temperature": 0.0, "avg_logprob": -0.1946643590927124, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.003876362694427371}, {"id": 42, "seek": 16840, "start": 181.8, "end": 187.44, "text": " This is where eBPF actually comes into place and helps us.", "tokens": [51034, 639, 307, 689, 308, 33, 47, 37, 767, 1487, 666, 1081, 293, 3665, 505, 13, 51316], "temperature": 0.0, "avg_logprob": -0.1946643590927124, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.003876362694427371}, {"id": 43, "seek": 16840, "start": 187.44, "end": 194.52, "text": " So eBPF is a, it's originally for networking applications.", "tokens": [51316, 407, 308, 33, 47, 37, 307, 257, 11, 309, 311, 7993, 337, 17985, 5821, 13, 51670], "temperature": 0.0, "avg_logprob": -0.1946643590927124, "compression_ratio": 1.528205128205128, "no_speech_prob": 0.003876362694427371}, {"id": 44, "seek": 19452, "start": 194.52, "end": 201.12, "text": " It's called Berkeley package filtering, but now eBPF totally something else.", "tokens": [50364, 467, 311, 1219, 23684, 7372, 30822, 11, 457, 586, 308, 33, 47, 37, 3879, 746, 1646, 13, 50694], "temperature": 0.0, "avg_logprob": -0.16537334943058515, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.007189295254647732}, {"id": 45, "seek": 19452, "start": 201.12, "end": 207.04000000000002, "text": " It's basically an event-based system where you can hook into some events that Linux kernel", "tokens": [50694, 467, 311, 1936, 364, 2280, 12, 6032, 1185, 689, 291, 393, 6328, 666, 512, 3931, 300, 18734, 28256, 50990], "temperature": 0.0, "avg_logprob": -0.16537334943058515, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.007189295254647732}, {"id": 46, "seek": 19452, "start": 207.04000000000002, "end": 211.68, "text": " issues and then you can just run code as a reaction to those events.", "tokens": [50990, 2663, 293, 550, 291, 393, 445, 1190, 3089, 382, 257, 5480, 281, 729, 3931, 13, 51222], "temperature": 0.0, "avg_logprob": -0.16537334943058515, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.007189295254647732}, {"id": 47, "seek": 19452, "start": 211.68, "end": 217.48000000000002, "text": " There is a runtime, there's a virtual machine inside and there's a verifier before you load", "tokens": [51222, 821, 307, 257, 34474, 11, 456, 311, 257, 6374, 3479, 1854, 293, 456, 311, 257, 1306, 9902, 949, 291, 3677, 51512], "temperature": 0.0, "avg_logprob": -0.16537334943058515, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.007189295254647732}, {"id": 48, "seek": 19452, "start": 217.48000000000002, "end": 222.28, "text": " your program that verifier needs to check your program that it doesn't do anything harmful", "tokens": [51512, 428, 1461, 300, 1306, 9902, 2203, 281, 1520, 428, 1461, 300, 309, 1177, 380, 360, 1340, 19727, 51752], "temperature": 0.0, "avg_logprob": -0.16537334943058515, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.007189295254647732}, {"id": 49, "seek": 22228, "start": 222.32, "end": 225.84, "text": " like infinite loops whatnot in the kernel space.", "tokens": [50366, 411, 13785, 16121, 25882, 294, 264, 28256, 1901, 13, 50542], "temperature": 0.0, "avg_logprob": -0.22224607467651367, "compression_ratio": 1.62, "no_speech_prob": 0.0024246536195278168}, {"id": 50, "seek": 22228, "start": 225.84, "end": 232.76, "text": " Then compiler kicks in and compile your provided code and you actually run those code against", "tokens": [50542, 1396, 31958, 21293, 294, 293, 31413, 428, 5649, 3089, 293, 291, 767, 1190, 729, 3089, 1970, 50888], "temperature": 0.0, "avg_logprob": -0.22224607467651367, "compression_ratio": 1.62, "no_speech_prob": 0.0024246536195278168}, {"id": 51, "seek": 22228, "start": 232.76, "end": 235.32, "text": " any events that you issue.", "tokens": [50888, 604, 3931, 300, 291, 2734, 13, 51016], "temperature": 0.0, "avg_logprob": -0.22224607467651367, "compression_ratio": 1.62, "no_speech_prob": 0.0024246536195278168}, {"id": 52, "seek": 22228, "start": 235.32, "end": 240.12, "text": " That's the one fancy part of the solution.", "tokens": [51016, 663, 311, 264, 472, 10247, 644, 295, 264, 3827, 13, 51256], "temperature": 0.0, "avg_logprob": -0.22224607467651367, "compression_ratio": 1.62, "no_speech_prob": 0.0024246536195278168}, {"id": 53, "seek": 22228, "start": 240.12, "end": 246.24, "text": " So then it comes another subsystem of Linux which is super cool is the perf event.", "tokens": [51256, 407, 550, 309, 1487, 1071, 2090, 9321, 295, 18734, 597, 307, 1687, 1627, 307, 264, 13826, 2280, 13, 51562], "temperature": 0.0, "avg_logprob": -0.22224607467651367, "compression_ratio": 1.62, "no_speech_prob": 0.0024246536195278168}, {"id": 54, "seek": 22228, "start": 246.24, "end": 248.68, "text": " You can have perf subsystem.", "tokens": [51562, 509, 393, 362, 13826, 2090, 9321, 13, 51684], "temperature": 0.0, "avg_logprob": -0.22224607467651367, "compression_ratio": 1.62, "no_speech_prob": 0.0024246536195278168}, {"id": 55, "seek": 24868, "start": 248.68, "end": 255.04000000000002, "text": " From the perf subsystem you can have, you can hook into various parts of your stack", "tokens": [50364, 3358, 264, 13826, 2090, 9321, 291, 393, 362, 11, 291, 393, 6328, 666, 3683, 3166, 295, 428, 8630, 50682], "temperature": 0.0, "avg_logprob": -0.1589112599690755, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0009760679677128792}, {"id": 56, "seek": 24868, "start": 255.04000000000002, "end": 258.6, "text": " and you can run code against these events.", "tokens": [50682, 293, 291, 393, 1190, 3089, 1970, 613, 3931, 13, 50860], "temperature": 0.0, "avg_logprob": -0.1589112599690755, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0009760679677128792}, {"id": 57, "seek": 24868, "start": 258.6, "end": 263.56, "text": " In this particle talk we are going to talk about the CPU events, but you don't need to", "tokens": [50860, 682, 341, 12359, 751, 321, 366, 516, 281, 751, 466, 264, 13199, 3931, 11, 457, 291, 500, 380, 643, 281, 51108], "temperature": 0.0, "avg_logprob": -0.1589112599690755, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0009760679677128792}, {"id": 58, "seek": 24868, "start": 263.56, "end": 266.68, "text": " use only the CPU events necessarily.", "tokens": [51108, 764, 787, 264, 13199, 3931, 4725, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1589112599690755, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0009760679677128792}, {"id": 59, "seek": 24868, "start": 266.68, "end": 272.48, "text": " You can do this for the IO events, you can do this for the memory allocation.", "tokens": [51264, 509, 393, 360, 341, 337, 264, 39839, 3931, 11, 291, 393, 360, 341, 337, 264, 4675, 27599, 13, 51554], "temperature": 0.0, "avg_logprob": -0.1589112599690755, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0009760679677128792}, {"id": 60, "seek": 24868, "start": 272.48, "end": 276.92, "text": " Practically anything that you see in here, you can run, hook into that event and run", "tokens": [51554, 19170, 984, 1340, 300, 291, 536, 294, 510, 11, 291, 393, 1190, 11, 6328, 666, 300, 2280, 293, 1190, 51776], "temperature": 0.0, "avg_logprob": -0.1589112599690755, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0009760679677128792}, {"id": 61, "seek": 27692, "start": 276.96000000000004, "end": 279.8, "text": " a code piece to that.", "tokens": [50366, 257, 3089, 2522, 281, 300, 13, 50508], "temperature": 0.0, "avg_logprob": -0.2195255475885728, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.001970502780750394}, {"id": 62, "seek": 27692, "start": 279.8, "end": 282.88, "text": " So what makes perf events special?", "tokens": [50508, 407, 437, 1669, 13826, 3931, 2121, 30, 50662], "temperature": 0.0, "avg_logprob": -0.2195255475885728, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.001970502780750394}, {"id": 63, "seek": 27692, "start": 282.88, "end": 285.44, "text": " It's actually performance monitoring units.", "tokens": [50662, 467, 311, 767, 3389, 11028, 6815, 13, 50790], "temperature": 0.0, "avg_logprob": -0.2195255475885728, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.001970502780750394}, {"id": 64, "seek": 27692, "start": 285.44, "end": 291.12, "text": " These are very efficiently implemented units in the Linux kernel so that they keep track", "tokens": [50790, 1981, 366, 588, 19621, 12270, 6815, 294, 264, 18734, 28256, 370, 300, 436, 1066, 2837, 51074], "temperature": 0.0, "avg_logprob": -0.2195255475885728, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.001970502780750394}, {"id": 65, "seek": 27692, "start": 291.12, "end": 299.44, "text": " of the cycles and then like you can actually take measurements and you can react to those", "tokens": [51074, 295, 264, 17796, 293, 550, 411, 291, 393, 767, 747, 15383, 293, 291, 393, 4515, 281, 729, 51490], "temperature": 0.0, "avg_logprob": -0.2195255475885728, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.001970502780750394}, {"id": 66, "seek": 27692, "start": 299.44, "end": 300.44, "text": " measurements.", "tokens": [51490, 15383, 13, 51540], "temperature": 0.0, "avg_logprob": -0.2195255475885728, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.001970502780750394}, {"id": 67, "seek": 30044, "start": 300.44, "end": 307.84, "text": " That's why EVPF plus perf events is actually faster than the other solutions that we have", "tokens": [50364, 663, 311, 983, 15733, 47, 37, 1804, 13826, 3931, 307, 767, 4663, 813, 264, 661, 6547, 300, 321, 362, 50734], "temperature": 0.0, "avg_logprob": -0.2788092426418029, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.01326976902782917}, {"id": 68, "seek": 30044, "start": 307.84, "end": 309.64, "text": " in basically ecosystem.", "tokens": [50734, 294, 1936, 11311, 13, 50824], "temperature": 0.0, "avg_logprob": -0.2788092426418029, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.01326976902782917}, {"id": 69, "seek": 30044, "start": 309.64, "end": 316.28, "text": " Because with the Linux you can already have some syscalls, you can just hook into that", "tokens": [50824, 1436, 365, 264, 18734, 291, 393, 1217, 362, 512, 262, 749, 66, 39655, 11, 291, 393, 445, 6328, 666, 300, 51156], "temperature": 0.0, "avg_logprob": -0.2788092426418029, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.01326976902782917}, {"id": 70, "seek": 30044, "start": 316.28, "end": 322.6, "text": " introps and do all these things that I'm going to tell you in a minute in the user space.", "tokens": [51156, 560, 49715, 293, 360, 439, 613, 721, 300, 286, 478, 516, 281, 980, 291, 294, 257, 3456, 294, 264, 4195, 1901, 13, 51472], "temperature": 0.0, "avg_logprob": -0.2788092426418029, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.01326976902782917}, {"id": 71, "seek": 30044, "start": 322.6, "end": 328.88, "text": " But the most of the things we do is using the PMUs and because the PMUs are efficient", "tokens": [51472, 583, 264, 881, 295, 264, 721, 321, 360, 307, 1228, 264, 12499, 29211, 293, 570, 264, 12499, 29211, 366, 7148, 51786], "temperature": 0.0, "avg_logprob": -0.2788092426418029, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.01326976902782917}, {"id": 72, "seek": 32888, "start": 328.88, "end": 335.44, "text": " and EVPF codes also efficient and run in the kernel space, this gives us a bit of a headroom", "tokens": [50364, 293, 15733, 47, 37, 14211, 611, 7148, 293, 1190, 294, 264, 28256, 1901, 11, 341, 2709, 505, 257, 857, 295, 257, 1378, 2861, 50692], "temperature": 0.0, "avg_logprob": -0.20577951079433404, "compression_ratio": 1.564, "no_speech_prob": 0.0009058539872057736}, {"id": 73, "seek": 32888, "start": 335.44, "end": 338.0, "text": " for the performance.", "tokens": [50692, 337, 264, 3389, 13, 50820], "temperature": 0.0, "avg_logprob": -0.20577951079433404, "compression_ratio": 1.564, "no_speech_prob": 0.0009058539872057736}, {"id": 74, "seek": 32888, "start": 338.0, "end": 341.71999999999997, "text": " So we are not the only ones that actually implemented that.", "tokens": [50820, 407, 321, 366, 406, 264, 787, 2306, 300, 767, 12270, 300, 13, 51006], "temperature": 0.0, "avg_logprob": -0.20577951079433404, "compression_ratio": 1.564, "no_speech_prob": 0.0009058539872057736}, {"id": 75, "seek": 32888, "start": 341.71999999999997, "end": 344.68, "text": " This is quite a journey.", "tokens": [51006, 639, 307, 1596, 257, 4671, 13, 51154], "temperature": 0.0, "avg_logprob": -0.20577951079433404, "compression_ratio": 1.564, "no_speech_prob": 0.0009058539872057736}, {"id": 76, "seek": 32888, "start": 344.68, "end": 351.15999999999997, "text": " I don't remember when is the first time that it actually introduced the PyPerf code inside", "tokens": [51154, 286, 500, 380, 1604, 562, 307, 264, 700, 565, 300, 309, 767, 7268, 264, 9953, 23232, 69, 3089, 1854, 51478], "temperature": 0.0, "avg_logprob": -0.20577951079433404, "compression_ratio": 1.564, "no_speech_prob": 0.0009058539872057736}, {"id": 77, "seek": 32888, "start": 351.15999999999997, "end": 352.15999999999997, "text": " the Linux kernel.", "tokens": [51478, 264, 18734, 28256, 13, 51528], "temperature": 0.0, "avg_logprob": -0.20577951079433404, "compression_ratio": 1.564, "no_speech_prob": 0.0009058539872057736}, {"id": 78, "seek": 32888, "start": 352.15999999999997, "end": 357.15999999999997, "text": " You can check out, these are actual links and I'm going to share the slides and you", "tokens": [51528, 509, 393, 1520, 484, 11, 613, 366, 3539, 6123, 293, 286, 478, 516, 281, 2073, 264, 9788, 293, 291, 51778], "temperature": 0.0, "avg_logprob": -0.20577951079433404, "compression_ratio": 1.564, "no_speech_prob": 0.0009058539872057736}, {"id": 79, "seek": 35716, "start": 357.16, "end": 363.56, "text": " can see basically the git comments against that.", "tokens": [50364, 393, 536, 1936, 264, 18331, 3053, 1970, 300, 13, 50684], "temperature": 0.0, "avg_logprob": -0.23323102151193925, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.03935835883021355}, {"id": 80, "seek": 35716, "start": 363.56, "end": 369.68, "text": " There's a set of tools called BCC tools in the EVPF space and there's also another implementation", "tokens": [50684, 821, 311, 257, 992, 295, 3873, 1219, 363, 11717, 3873, 294, 264, 15733, 47, 37, 1901, 293, 456, 311, 611, 1071, 11420, 50990], "temperature": 0.0, "avg_logprob": -0.23323102151193925, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.03935835883021355}, {"id": 81, "seek": 35716, "start": 369.68, "end": 372.64000000000004, "text": " of PyPerf in there.", "tokens": [50990, 295, 9953, 23232, 69, 294, 456, 13, 51138], "temperature": 0.0, "avg_logprob": -0.23323102151193925, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.03935835883021355}, {"id": 82, "seek": 35716, "start": 372.64000000000004, "end": 376.92, "text": " But what is the downside of all these tooling?", "tokens": [51138, 583, 437, 307, 264, 25060, 295, 439, 613, 46593, 30, 51352], "temperature": 0.0, "avg_logprob": -0.23323102151193925, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.03935835883021355}, {"id": 83, "seek": 35716, "start": 376.92, "end": 379.16, "text": " These are first, they are dated.", "tokens": [51352, 1981, 366, 700, 11, 436, 366, 23804, 13, 51464], "temperature": 0.0, "avg_logprob": -0.23323102151193925, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.03935835883021355}, {"id": 84, "seek": 35716, "start": 379.16, "end": 384.64000000000004, "text": " They don't cover all the recent changes in the Python runtime itself and they are just", "tokens": [51464, 814, 500, 380, 2060, 439, 264, 5162, 2962, 294, 264, 15329, 34474, 2564, 293, 436, 366, 445, 51738], "temperature": 0.0, "avg_logprob": -0.23323102151193925, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.03935835883021355}, {"id": 85, "seek": 35716, "start": 384.64000000000004, "end": 385.64000000000004, "text": " one of tools.", "tokens": [51738, 472, 295, 3873, 13, 51788], "temperature": 0.0, "avg_logprob": -0.23323102151193925, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.03935835883021355}, {"id": 86, "seek": 38564, "start": 385.64, "end": 392.0, "text": " So I'm going to show you some cool things that you can actually do profiling in the", "tokens": [50364, 407, 286, 478, 516, 281, 855, 291, 512, 1627, 721, 300, 291, 393, 767, 360, 1740, 4883, 294, 264, 50682], "temperature": 0.0, "avg_logprob": -0.228174625056805, "compression_ratio": 1.7198275862068966, "no_speech_prob": 0.002352494979277253}, {"id": 87, "seek": 38564, "start": 392.0, "end": 397.24, "text": " production itself in a continuous manner, which we call continuous profiling.", "tokens": [50682, 4265, 2564, 294, 257, 10957, 9060, 11, 597, 321, 818, 10957, 1740, 4883, 13, 50944], "temperature": 0.0, "avg_logprob": -0.228174625056805, "compression_ratio": 1.7198275862068966, "no_speech_prob": 0.002352494979277253}, {"id": 88, "seek": 38564, "start": 397.24, "end": 400.44, "text": " We're going to come to that in a minute.", "tokens": [50944, 492, 434, 516, 281, 808, 281, 300, 294, 257, 3456, 13, 51104], "temperature": 0.0, "avg_logprob": -0.228174625056805, "compression_ratio": 1.7198275862068966, "no_speech_prob": 0.002352494979277253}, {"id": 89, "seek": 38564, "start": 400.44, "end": 406.08, "text": " So to make the tools work, you need to just wrap your Python interpreter around with these", "tokens": [51104, 407, 281, 652, 264, 3873, 589, 11, 291, 643, 281, 445, 7019, 428, 15329, 34132, 926, 365, 613, 51386], "temperature": 0.0, "avg_logprob": -0.228174625056805, "compression_ratio": 1.7198275862068966, "no_speech_prob": 0.002352494979277253}, {"id": 90, "seek": 38564, "start": 406.08, "end": 409.59999999999997, "text": " tools and then collect your profiles.", "tokens": [51386, 3873, 293, 550, 2500, 428, 23693, 13, 51562], "temperature": 0.0, "avg_logprob": -0.228174625056805, "compression_ratio": 1.7198275862068966, "no_speech_prob": 0.002352494979277253}, {"id": 91, "seek": 38564, "start": 409.59999999999997, "end": 413.84, "text": " So that gets us to the Parker project.", "tokens": [51562, 407, 300, 2170, 505, 281, 264, 20155, 1716, 13, 51774], "temperature": 0.0, "avg_logprob": -0.228174625056805, "compression_ratio": 1.7198275862068966, "no_speech_prob": 0.002352494979277253}, {"id": 92, "seek": 38564, "start": 413.84, "end": 415.36, "text": " It's an open source project.", "tokens": [51774, 467, 311, 364, 1269, 4009, 1716, 13, 51850], "temperature": 0.0, "avg_logprob": -0.228174625056805, "compression_ratio": 1.7198275862068966, "no_speech_prob": 0.002352494979277253}, {"id": 93, "seek": 41536, "start": 415.36, "end": 420.32, "text": " It's a continuous profiling project using EVPF and perf events.", "tokens": [50364, 467, 311, 257, 10957, 1740, 4883, 1716, 1228, 15733, 47, 37, 293, 13826, 3931, 13, 50612], "temperature": 0.0, "avg_logprob": -0.23488795451628855, "compression_ratio": 1.49, "no_speech_prob": 0.0030436587985605}, {"id": 94, "seek": 41536, "start": 420.32, "end": 427.32, "text": " We can run your profiling workloads directly in production and there is no runtime, nearly", "tokens": [50612, 492, 393, 1190, 428, 1740, 4883, 32452, 3838, 294, 4265, 293, 456, 307, 572, 34474, 11, 6217, 50962], "temperature": 0.0, "avg_logprob": -0.23488795451628855, "compression_ratio": 1.49, "no_speech_prob": 0.0030436587985605}, {"id": 95, "seek": 41536, "start": 427.32, "end": 431.76, "text": " none of the runtime overhead in this approach.", "tokens": [50962, 6022, 295, 264, 34474, 19922, 294, 341, 3109, 13, 51184], "temperature": 0.0, "avg_logprob": -0.23488795451628855, "compression_ratio": 1.49, "no_speech_prob": 0.0030436587985605}, {"id": 96, "seek": 41536, "start": 431.76, "end": 436.32, "text": " There's a tiny bit, but it's really negligible.", "tokens": [51184, 821, 311, 257, 5870, 857, 11, 457, 309, 311, 534, 32570, 964, 13, 51412], "temperature": 0.0, "avg_logprob": -0.23488795451628855, "compression_ratio": 1.49, "no_speech_prob": 0.0030436587985605}, {"id": 97, "seek": 41536, "start": 436.32, "end": 442.24, "text": " So how does the Parker EVPF agent actually work?", "tokens": [51412, 407, 577, 775, 264, 20155, 15733, 47, 37, 9461, 767, 589, 30, 51708], "temperature": 0.0, "avg_logprob": -0.23488795451628855, "compression_ratio": 1.49, "no_speech_prob": 0.0030436587985605}, {"id": 98, "seek": 44224, "start": 442.24, "end": 449.08, "text": " Other things that I mentioned previously, the hook into perf events, we have some unwinded", "tokens": [50364, 5358, 721, 300, 286, 2835, 8046, 11, 264, 6328, 666, 13826, 3931, 11, 321, 362, 512, 517, 12199, 292, 50706], "temperature": 0.0, "avg_logprob": -0.18767017654225795, "compression_ratio": 1.526829268292683, "no_speech_prob": 0.038577500730752945}, {"id": 99, "seek": 44224, "start": 449.08, "end": 455.6, "text": " programs that actually unwinds the stack, which I will tell you about.", "tokens": [50706, 4268, 300, 767, 517, 12199, 82, 264, 8630, 11, 597, 286, 486, 980, 291, 466, 13, 51032], "temperature": 0.0, "avg_logprob": -0.18767017654225795, "compression_ratio": 1.526829268292683, "no_speech_prob": 0.038577500730752945}, {"id": 100, "seek": 44224, "start": 455.6, "end": 463.04, "text": " And we then keep track of what happens in the CPU for that stack.", "tokens": [51032, 400, 321, 550, 1066, 2837, 295, 437, 2314, 294, 264, 13199, 337, 300, 8630, 13, 51404], "temperature": 0.0, "avg_logprob": -0.18767017654225795, "compression_ratio": 1.526829268292683, "no_speech_prob": 0.038577500730752945}, {"id": 101, "seek": 44224, "start": 463.04, "end": 468.68, "text": " And then we aggregate those information and put in an EVPF map, which are the special", "tokens": [51404, 400, 550, 321, 26118, 729, 1589, 293, 829, 294, 364, 15733, 47, 37, 4471, 11, 597, 366, 264, 2121, 51686], "temperature": 0.0, "avg_logprob": -0.18767017654225795, "compression_ratio": 1.526829268292683, "no_speech_prob": 0.038577500730752945}, {"id": 102, "seek": 46868, "start": 468.72, "end": 474.2, "text": " data structures that you talk between Kernel and the user space.", "tokens": [50366, 1412, 9227, 300, 291, 751, 1296, 40224, 338, 293, 264, 4195, 1901, 13, 50640], "temperature": 0.0, "avg_logprob": -0.23046558781674034, "compression_ratio": 1.6030150753768844, "no_speech_prob": 0.036094069480895996}, {"id": 103, "seek": 46868, "start": 474.2, "end": 479.56, "text": " And then we read that data, we convert that some open profiling formats and push that", "tokens": [50640, 400, 550, 321, 1401, 300, 1412, 11, 321, 7620, 300, 512, 1269, 1740, 4883, 25879, 293, 2944, 300, 50908], "temperature": 0.0, "avg_logprob": -0.23046558781674034, "compression_ratio": 1.6030150753768844, "no_speech_prob": 0.036094069480895996}, {"id": 104, "seek": 46868, "start": 479.56, "end": 486.56, "text": " in a server site where we can just aggregate and visualize that and let the users actually", "tokens": [50908, 294, 257, 7154, 3621, 689, 321, 393, 445, 26118, 293, 23273, 300, 293, 718, 264, 5022, 767, 51258], "temperature": 0.0, "avg_logprob": -0.23046558781674034, "compression_ratio": 1.6030150753768844, "no_speech_prob": 0.036094069480895996}, {"id": 105, "seek": 46868, "start": 488.16, "end": 491.24, "text": " make sense of their programs.", "tokens": [51338, 652, 2020, 295, 641, 4268, 13, 51492], "temperature": 0.0, "avg_logprob": -0.23046558781674034, "compression_ratio": 1.6030150753768844, "no_speech_prob": 0.036094069480895996}, {"id": 106, "seek": 46868, "start": 491.24, "end": 494.88, "text": " So this is how that whole thing actually works.", "tokens": [51492, 407, 341, 307, 577, 300, 1379, 551, 767, 1985, 13, 51674], "temperature": 0.0, "avg_logprob": -0.23046558781674034, "compression_ratio": 1.6030150753768844, "no_speech_prob": 0.036094069480895996}, {"id": 107, "seek": 49488, "start": 495.84, "end": 501.48, "text": " There are a lot of details, but this talk is not actually about the internals of Parker,", "tokens": [50412, 821, 366, 257, 688, 295, 4365, 11, 457, 341, 751, 307, 406, 767, 466, 264, 2154, 1124, 295, 20155, 11, 50694], "temperature": 0.0, "avg_logprob": -0.23588683806270003, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.004382943734526634}, {"id": 108, "seek": 49488, "start": 501.48, "end": 508.48, "text": " but we are doing a lot of cool stuff to make the stack collection and symbolization very", "tokens": [50694, 457, 321, 366, 884, 257, 688, 295, 1627, 1507, 281, 652, 264, 8630, 5765, 293, 5986, 2144, 588, 51044], "temperature": 0.0, "avg_logprob": -0.23588683806270003, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.004382943734526634}, {"id": 109, "seek": 49488, "start": 508.48, "end": 509.96, "text": " efficient.", "tokens": [51044, 7148, 13, 51118], "temperature": 0.0, "avg_logprob": -0.23588683806270003, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.004382943734526634}, {"id": 110, "seek": 49488, "start": 509.96, "end": 513.96, "text": " And then the end result is a UI like this.", "tokens": [51118, 400, 550, 264, 917, 1874, 307, 257, 15682, 411, 341, 13, 51318], "temperature": 0.0, "avg_logprob": -0.23588683806270003, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.004382943734526634}, {"id": 111, "seek": 49488, "start": 513.96, "end": 520.96, "text": " In a continuous timeline, you can see that what's going on on your CPU for each process.", "tokens": [51318, 682, 257, 10957, 12933, 11, 291, 393, 536, 300, 437, 311, 516, 322, 322, 428, 13199, 337, 1184, 1399, 13, 51668], "temperature": 0.0, "avg_logprob": -0.23588683806270003, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.004382943734526634}, {"id": 112, "seek": 52096, "start": 520.96, "end": 527.76, "text": " Then we collect a lot of metadata and enrich those information for you so that you can", "tokens": [50364, 1396, 321, 2500, 257, 688, 295, 26603, 293, 18849, 729, 1589, 337, 291, 370, 300, 291, 393, 50704], "temperature": 0.0, "avg_logprob": -0.18010113157075028, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.005047907121479511}, {"id": 113, "seek": 52096, "start": 527.76, "end": 533.2800000000001, "text": " query, compare, and see how you can improve your program.", "tokens": [50704, 14581, 11, 6794, 11, 293, 536, 577, 291, 393, 3470, 428, 1461, 13, 50980], "temperature": 0.0, "avg_logprob": -0.18010113157075028, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.005047907121479511}, {"id": 114, "seek": 52096, "start": 533.2800000000001, "end": 537.84, "text": " And the agent is kind of super cool because you can just install any host machine and", "tokens": [50980, 400, 264, 9461, 307, 733, 295, 1687, 1627, 570, 291, 393, 445, 3625, 604, 3975, 3479, 293, 51208], "temperature": 0.0, "avg_logprob": -0.18010113157075028, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.005047907121479511}, {"id": 115, "seek": 52096, "start": 537.84, "end": 542.48, "text": " any process that you have on that machine, we can just collect data and send to the server", "tokens": [51208, 604, 1399, 300, 291, 362, 322, 300, 3479, 11, 321, 393, 445, 2500, 1412, 293, 2845, 281, 264, 7154, 51440], "temperature": 0.0, "avg_logprob": -0.18010113157075028, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.005047907121479511}, {"id": 116, "seek": 52096, "start": 542.48, "end": 545.4000000000001, "text": " and you can see that in the UI.", "tokens": [51440, 293, 291, 393, 536, 300, 294, 264, 15682, 13, 51586], "temperature": 0.0, "avg_logprob": -0.18010113157075028, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.005047907121479511}, {"id": 117, "seek": 54540, "start": 545.4, "end": 551.52, "text": " This doesn't necessarily, it's not scoped to the Python itself, but it does a lot of", "tokens": [50364, 639, 1177, 380, 4725, 11, 309, 311, 406, 795, 27277, 281, 264, 15329, 2564, 11, 457, 309, 775, 257, 688, 295, 50670], "temperature": 0.0, "avg_logprob": -0.24774320390489366, "compression_ratio": 1.6180904522613064, "no_speech_prob": 0.018500152975320816}, {"id": 118, "seek": 54540, "start": 551.52, "end": 554.4, "text": " cool stuff with the Python as well.", "tokens": [50670, 1627, 1507, 365, 264, 15329, 382, 731, 13, 50814], "temperature": 0.0, "avg_logprob": -0.24774320390489366, "compression_ratio": 1.6180904522613064, "no_speech_prob": 0.018500152975320816}, {"id": 119, "seek": 54540, "start": 554.4, "end": 561.0, "text": " So there's not a Python stack, but we will see some examples, but this is some, I think", "tokens": [50814, 407, 456, 311, 406, 257, 15329, 8630, 11, 457, 321, 486, 536, 512, 5110, 11, 457, 341, 307, 512, 11, 286, 519, 51144], "temperature": 0.0, "avg_logprob": -0.24774320390489366, "compression_ratio": 1.6180904522613064, "no_speech_prob": 0.018500152975320816}, {"id": 120, "seek": 54540, "start": 561.0, "end": 568.0, "text": " this is a Go one, but you can see that the stacks are easily getting really deep.", "tokens": [51144, 341, 307, 257, 1037, 472, 11, 457, 291, 393, 536, 300, 264, 30792, 366, 3612, 1242, 534, 2452, 13, 51494], "temperature": 0.0, "avg_logprob": -0.24774320390489366, "compression_ratio": 1.6180904522613064, "no_speech_prob": 0.018500152975320816}, {"id": 121, "seek": 54540, "start": 568.04, "end": 570.56, "text": " So what is the stack unwinding?", "tokens": [51496, 407, 437, 307, 264, 8630, 14853, 9245, 30, 51622], "temperature": 0.0, "avg_logprob": -0.24774320390489366, "compression_ratio": 1.6180904522613064, "no_speech_prob": 0.018500152975320816}, {"id": 122, "seek": 57056, "start": 570.56, "end": 576.68, "text": " This is the next critical thing that we need to talk about because the whole, like the", "tokens": [50364, 639, 307, 264, 958, 4924, 551, 300, 321, 643, 281, 751, 466, 570, 264, 1379, 11, 411, 264, 50670], "temperature": 0.0, "avg_logprob": -0.2182102636857466, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.0006755343056283891}, {"id": 123, "seek": 57056, "start": 576.68, "end": 581.76, "text": " what makes profiling challenging, especially from the Python side, is actually to be able", "tokens": [50670, 437, 1669, 1740, 4883, 7595, 11, 2318, 490, 264, 15329, 1252, 11, 307, 767, 281, 312, 1075, 50924], "temperature": 0.0, "avg_logprob": -0.2182102636857466, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.0006755343056283891}, {"id": 124, "seek": 57056, "start": 581.76, "end": 583.92, "text": " to unwind the stack.", "tokens": [50924, 281, 517, 12199, 264, 8630, 13, 51032], "temperature": 0.0, "avg_logprob": -0.2182102636857466, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.0006755343056283891}, {"id": 125, "seek": 57056, "start": 583.92, "end": 592.92, "text": " So when a program gets executed, probably you all heard these in your start of your education.", "tokens": [51032, 407, 562, 257, 1461, 2170, 17577, 11, 1391, 291, 439, 2198, 613, 294, 428, 722, 295, 428, 3309, 13, 51482], "temperature": 0.0, "avg_logprob": -0.2182102636857466, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.0006755343056283891}, {"id": 126, "seek": 57056, "start": 593.52, "end": 598.7199999999999, "text": " There are specific structures when the process actually allocates in the memory, which is", "tokens": [51512, 821, 366, 2685, 9227, 562, 264, 1399, 767, 12660, 1024, 294, 264, 4675, 11, 597, 307, 51772], "temperature": 0.0, "avg_logprob": -0.2182102636857466, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.0006755343056283891}, {"id": 127, "seek": 59872, "start": 598.84, "end": 604.32, "text": " one is stack and one is heap and the stack actually tracks the execution of the program", "tokens": [50370, 472, 307, 8630, 293, 472, 307, 33591, 293, 264, 8630, 767, 10218, 264, 15058, 295, 264, 1461, 50644], "temperature": 0.0, "avg_logprob": -0.19841153108620946, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.0029695904813706875}, {"id": 128, "seek": 59872, "start": 604.32, "end": 610.5600000000001, "text": " and whenever you call a function, you open a frame and you change the states of your", "tokens": [50644, 293, 5699, 291, 818, 257, 2445, 11, 291, 1269, 257, 3920, 293, 291, 1319, 264, 4368, 295, 428, 50956], "temperature": 0.0, "avg_logprob": -0.19841153108620946, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.0029695904813706875}, {"id": 129, "seek": 59872, "start": 610.5600000000001, "end": 616.84, "text": " registers and then you keep adding everything to the stack and when one of the functions", "tokens": [50956, 38351, 293, 550, 291, 1066, 5127, 1203, 281, 264, 8630, 293, 562, 472, 295, 264, 6828, 51270], "temperature": 0.0, "avg_logprob": -0.19841153108620946, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.0029695904813706875}, {"id": 130, "seek": 59872, "start": 616.84, "end": 623.32, "text": " that returns from the leaf, you just go back and return the data to your user.", "tokens": [51270, 300, 11247, 490, 264, 10871, 11, 291, 445, 352, 646, 293, 2736, 264, 1412, 281, 428, 4195, 13, 51594], "temperature": 0.0, "avg_logprob": -0.19841153108620946, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.0029695904813706875}, {"id": 131, "seek": 62332, "start": 623.32, "end": 633.32, "text": " So I might be oversimplified that, but it's a diagram just to show you how it looked like,", "tokens": [50364, 407, 286, 1062, 312, 15488, 332, 564, 2587, 300, 11, 457, 309, 311, 257, 10686, 445, 281, 855, 291, 577, 309, 2956, 411, 11, 50864], "temperature": 0.0, "avg_logprob": -0.1684267975035168, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.0018655013991519809}, {"id": 132, "seek": 62332, "start": 633.32, "end": 638.8000000000001, "text": " but the end result when you unwind the stack and aggregate all these function addresses,", "tokens": [50864, 457, 264, 917, 1874, 562, 291, 517, 12199, 264, 8630, 293, 26118, 439, 613, 2445, 16862, 11, 51138], "temperature": 0.0, "avg_logprob": -0.1684267975035168, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.0018655013991519809}, {"id": 133, "seek": 62332, "start": 638.8000000000001, "end": 640.48, "text": " you get something like that.", "tokens": [51138, 291, 483, 746, 411, 300, 13, 51222], "temperature": 0.0, "avg_logprob": -0.1684267975035168, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.0018655013991519809}, {"id": 134, "seek": 62332, "start": 640.48, "end": 646.48, "text": " It's just the machine addresses and now you need to find a way to translate those machine", "tokens": [51222, 467, 311, 445, 264, 3479, 16862, 293, 586, 291, 643, 281, 915, 257, 636, 281, 13799, 729, 3479, 51522], "temperature": 0.0, "avg_logprob": -0.1684267975035168, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.0018655013991519809}, {"id": 135, "seek": 62332, "start": 646.48, "end": 649.8800000000001, "text": " addresses to the human readable format.", "tokens": [51522, 16862, 281, 264, 1952, 49857, 7877, 13, 51692], "temperature": 0.0, "avg_logprob": -0.1684267975035168, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.0018655013991519809}, {"id": 136, "seek": 64988, "start": 649.92, "end": 655.56, "text": " So all these parts for the native code, so anything that actually runs on your CPU.", "tokens": [50366, 407, 439, 613, 3166, 337, 264, 8470, 3089, 11, 370, 1340, 300, 767, 6676, 322, 428, 13199, 13, 50648], "temperature": 0.0, "avg_logprob": -0.25769133900487146, "compression_ratio": 1.67, "no_speech_prob": 0.011412706226110458}, {"id": 137, "seek": 64988, "start": 655.56, "end": 661.24, "text": " So that brings us actually the next step.", "tokens": [50648, 407, 300, 5607, 505, 767, 264, 958, 1823, 13, 50932], "temperature": 0.0, "avg_logprob": -0.25769133900487146, "compression_ratio": 1.67, "no_speech_prob": 0.011412706226110458}, {"id": 138, "seek": 64988, "start": 661.24, "end": 668.72, "text": " So this is a state where we didn't implement the Python unwind for parka and you can actually,", "tokens": [50932, 407, 341, 307, 257, 1785, 689, 321, 994, 380, 4445, 264, 15329, 517, 12199, 337, 3884, 64, 293, 291, 393, 767, 11, 51306], "temperature": 0.0, "avg_logprob": -0.25769133900487146, "compression_ratio": 1.67, "no_speech_prob": 0.011412706226110458}, {"id": 139, "seek": 64988, "start": 668.72, "end": 670.56, "text": " this is an interactive one.", "tokens": [51306, 341, 307, 364, 15141, 472, 13, 51398], "temperature": 0.0, "avg_logprob": -0.25769133900487146, "compression_ratio": 1.67, "no_speech_prob": 0.011412706226110458}, {"id": 140, "seek": 64988, "start": 670.56, "end": 676.32, "text": " So you can see anything that gets from like your kernel, there's a start thread, this", "tokens": [51398, 407, 291, 393, 536, 1340, 300, 2170, 490, 411, 428, 28256, 11, 456, 311, 257, 722, 7207, 11, 341, 51686], "temperature": 0.0, "avg_logprob": -0.25769133900487146, "compression_ratio": 1.67, "no_speech_prob": 0.011412706226110458}, {"id": 141, "seek": 67632, "start": 676.32, "end": 683.1600000000001, "text": " is coming from libc and all these green things that you see, these are coming from the Python", "tokens": [50364, 307, 1348, 490, 22854, 66, 293, 439, 613, 3092, 721, 300, 291, 536, 11, 613, 366, 1348, 490, 264, 15329, 50706], "temperature": 0.0, "avg_logprob": -0.19130738226922003, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0024154805578291416}, {"id": 142, "seek": 67632, "start": 683.1600000000001, "end": 688.5200000000001, "text": " interpreter itself because Python interpreter is written in C and it's compiled and then", "tokens": [50706, 34132, 2564, 570, 15329, 34132, 307, 3720, 294, 383, 293, 309, 311, 36548, 293, 550, 50974], "temperature": 0.0, "avg_logprob": -0.19130738226922003, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0024154805578291416}, {"id": 143, "seek": 67632, "start": 688.5200000000001, "end": 696.2, "text": " directly gets executed on the CPU, but probably this is not useful for you, right?", "tokens": [50974, 3838, 2170, 17577, 322, 264, 13199, 11, 457, 1391, 341, 307, 406, 4420, 337, 291, 11, 558, 30, 51358], "temperature": 0.0, "avg_logprob": -0.19130738226922003, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0024154805578291416}, {"id": 144, "seek": 67632, "start": 696.2, "end": 697.5200000000001, "text": " You are Python developers.", "tokens": [51358, 509, 366, 15329, 8849, 13, 51424], "temperature": 0.0, "avg_logprob": -0.19130738226922003, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0024154805578291416}, {"id": 145, "seek": 67632, "start": 697.5200000000001, "end": 702.48, "text": " You actually want to see what's happening in the Python process itself, not the underlying", "tokens": [51424, 509, 767, 528, 281, 536, 437, 311, 2737, 294, 264, 15329, 1399, 2564, 11, 406, 264, 14217, 51672], "temperature": 0.0, "avg_logprob": -0.19130738226922003, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0024154805578291416}, {"id": 146, "seek": 67632, "start": 702.48, "end": 704.9200000000001, "text": " infrastructure.", "tokens": [51672, 6896, 13, 51794], "temperature": 0.0, "avg_logprob": -0.19130738226922003, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0024154805578291416}, {"id": 147, "seek": 70492, "start": 704.92, "end": 710.24, "text": " That being said, we also know that most of the Python applications also rely on the", "tokens": [50364, 663, 885, 848, 11, 321, 611, 458, 300, 881, 295, 264, 15329, 5821, 611, 10687, 322, 264, 50630], "temperature": 0.0, "avg_logprob": -0.24781726655505953, "compression_ratio": 1.5155555555555555, "no_speech_prob": 0.004188511520624161}, {"id": 148, "seek": 70492, "start": 710.24, "end": 719.24, "text": " C bits and the native code bits and it could be calling some C function here and there.", "tokens": [50630, 383, 9239, 293, 264, 8470, 3089, 9239, 293, 309, 727, 312, 5141, 512, 383, 2445, 510, 293, 456, 13, 51080], "temperature": 0.0, "avg_logprob": -0.24781726655505953, "compression_ratio": 1.5155555555555555, "no_speech_prob": 0.004188511520624161}, {"id": 149, "seek": 70492, "start": 719.24, "end": 724.3199999999999, "text": " Then when that happens, these are actually, gets like super important, right?", "tokens": [51080, 1396, 562, 300, 2314, 11, 613, 366, 767, 11, 2170, 411, 1687, 1021, 11, 558, 30, 51334], "temperature": 0.0, "avg_logprob": -0.24781726655505953, "compression_ratio": 1.5155555555555555, "no_speech_prob": 0.004188511520624161}, {"id": 150, "seek": 70492, "start": 724.3199999999999, "end": 729.48, "text": " For example, PyTorch, it's very popular nowadays in the machine learning workload, but it's", "tokens": [51334, 1171, 1365, 11, 9953, 51, 284, 339, 11, 309, 311, 588, 3743, 13434, 294, 264, 3479, 2539, 20139, 11, 457, 309, 311, 51592], "temperature": 0.0, "avg_logprob": -0.24781726655505953, "compression_ratio": 1.5155555555555555, "no_speech_prob": 0.004188511520624161}, {"id": 151, "seek": 72948, "start": 729.48, "end": 735.04, "text": " actually funneling everything into a native code and when you want to see what's going", "tokens": [50364, 767, 24515, 278, 1203, 666, 257, 8470, 3089, 293, 562, 291, 528, 281, 536, 437, 311, 516, 50642], "temperature": 0.0, "avg_logprob": -0.2115758645414102, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.021564679220318794}, {"id": 152, "seek": 72948, "start": 735.04, "end": 739.04, "text": " on the native code, parka actually can do that as well.", "tokens": [50642, 322, 264, 8470, 3089, 11, 3884, 64, 767, 393, 360, 300, 382, 731, 13, 50842], "temperature": 0.0, "avg_logprob": -0.2115758645414102, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.021564679220318794}, {"id": 153, "seek": 72948, "start": 739.04, "end": 742.72, "text": " And we do that in a very efficient way.", "tokens": [50842, 400, 321, 360, 300, 294, 257, 588, 7148, 636, 13, 51026], "temperature": 0.0, "avg_logprob": -0.2115758645414102, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.021564679220318794}, {"id": 154, "seek": 72948, "start": 742.72, "end": 748.48, "text": " You don't need to have, there's a whole concept of frame pointers and that actually helps us", "tokens": [51026, 509, 500, 380, 643, 281, 362, 11, 456, 311, 257, 1379, 3410, 295, 3920, 44548, 293, 300, 767, 3665, 505, 51314], "temperature": 0.0, "avg_logprob": -0.2115758645414102, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.021564679220318794}, {"id": 155, "seek": 72948, "start": 748.48, "end": 749.9200000000001, "text": " to unwind the stack.", "tokens": [51314, 281, 517, 12199, 264, 8630, 13, 51386], "temperature": 0.0, "avg_logprob": -0.2115758645414102, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.021564679220318794}, {"id": 156, "seek": 72948, "start": 749.9200000000001, "end": 756.9200000000001, "text": " We just gave another talk in the observability room like why frame pointers are cool, but", "tokens": [51386, 492, 445, 2729, 1071, 751, 294, 264, 9951, 2310, 1808, 411, 983, 3920, 44548, 366, 1627, 11, 457, 51736], "temperature": 0.0, "avg_logprob": -0.2115758645414102, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.021564679220318794}, {"id": 157, "seek": 75692, "start": 756.92, "end": 760.8, "text": " you don't have to have a need to have the frame pointers itself because there's also", "tokens": [50364, 291, 500, 380, 362, 281, 362, 257, 643, 281, 362, 264, 3920, 44548, 2564, 570, 456, 311, 611, 50558], "temperature": 0.0, "avg_logprob": -0.22513971573267227, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.001079944078810513}, {"id": 158, "seek": 75692, "start": 760.8, "end": 765.5999999999999, "text": " another facility with the devolve debug information, you can unwind the stack.", "tokens": [50558, 1071, 8973, 365, 264, 1905, 37361, 24083, 1589, 11, 291, 393, 517, 12199, 264, 8630, 13, 50798], "temperature": 0.0, "avg_logprob": -0.22513971573267227, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.001079944078810513}, {"id": 159, "seek": 75692, "start": 765.5999999999999, "end": 768.24, "text": " So parka actually utilizes that.", "tokens": [50798, 407, 3884, 64, 767, 4976, 5660, 300, 13, 50930], "temperature": 0.0, "avg_logprob": -0.22513971573267227, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.001079944078810513}, {"id": 160, "seek": 75692, "start": 768.24, "end": 773.9599999999999, "text": " This is important because most of the packages that you can find on any of the Linux distribution,", "tokens": [50930, 639, 307, 1021, 570, 881, 295, 264, 17401, 300, 291, 393, 915, 322, 604, 295, 264, 18734, 7316, 11, 51216], "temperature": 0.0, "avg_logprob": -0.22513971573267227, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.001079944078810513}, {"id": 161, "seek": 75692, "start": 773.9599999999999, "end": 778.12, "text": " you wouldn't find frame pointers.", "tokens": [51216, 291, 2759, 380, 915, 3920, 44548, 13, 51424], "temperature": 0.0, "avg_logprob": -0.22513971573267227, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.001079944078810513}, {"id": 162, "seek": 75692, "start": 778.12, "end": 781.16, "text": " But with the devolve information, you can actually unwind the stack and you can see", "tokens": [51424, 583, 365, 264, 1905, 37361, 1589, 11, 291, 393, 767, 517, 12199, 264, 8630, 293, 291, 393, 536, 51576], "temperature": 0.0, "avg_logprob": -0.22513971573267227, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.001079944078810513}, {"id": 163, "seek": 75692, "start": 781.16, "end": 783.1999999999999, "text": " all these goals.", "tokens": [51576, 439, 613, 5493, 13, 51678], "temperature": 0.0, "avg_logprob": -0.22513971573267227, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.001079944078810513}, {"id": 164, "seek": 75692, "start": 783.1999999999999, "end": 785.52, "text": " So, but we want more, right?", "tokens": [51678, 407, 11, 457, 321, 528, 544, 11, 558, 30, 51794], "temperature": 0.0, "avg_logprob": -0.22513971573267227, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.001079944078810513}, {"id": 165, "seek": 78552, "start": 785.52, "end": 789.3199999999999, "text": " We want to see the Python code, so how we do actually do that.", "tokens": [50364, 492, 528, 281, 536, 264, 15329, 3089, 11, 370, 577, 321, 360, 767, 360, 300, 13, 50554], "temperature": 0.0, "avg_logprob": -0.22817010145920974, "compression_ratio": 1.732394366197183, "no_speech_prob": 0.015631115064024925}, {"id": 166, "seek": 78552, "start": 789.3199999999999, "end": 796.24, "text": " So this is where it comes to, where we unwind the stack virtually, with virtual stack, we", "tokens": [50554, 407, 341, 307, 689, 309, 1487, 281, 11, 689, 321, 517, 12199, 264, 8630, 14103, 11, 365, 6374, 8630, 11, 321, 50900], "temperature": 0.0, "avg_logprob": -0.22817010145920974, "compression_ratio": 1.732394366197183, "no_speech_prob": 0.015631115064024925}, {"id": 167, "seek": 78552, "start": 796.24, "end": 802.4399999999999, "text": " mean that anything that gets executed in the Python interpreter, we need to find those", "tokens": [50900, 914, 300, 1340, 300, 2170, 17577, 294, 264, 15329, 34132, 11, 321, 643, 281, 915, 729, 51210], "temperature": 0.0, "avg_logprob": -0.22817010145920974, "compression_ratio": 1.732394366197183, "no_speech_prob": 0.015631115064024925}, {"id": 168, "seek": 78552, "start": 802.4399999999999, "end": 809.04, "text": " stacks and put those things in our flame graphs so we can see that like where is the problematic", "tokens": [51210, 30792, 293, 829, 729, 721, 294, 527, 13287, 24877, 370, 321, 393, 536, 300, 411, 689, 307, 264, 19011, 51540], "temperature": 0.0, "avg_logprob": -0.22817010145920974, "compression_ratio": 1.732394366197183, "no_speech_prob": 0.015631115064024925}, {"id": 169, "seek": 78552, "start": 809.04, "end": 812.76, "text": " areas in our Python code itself.", "tokens": [51540, 3179, 294, 527, 15329, 3089, 2564, 13, 51726], "temperature": 0.0, "avg_logprob": -0.22817010145920974, "compression_ratio": 1.732394366197183, "no_speech_prob": 0.015631115064024925}, {"id": 170, "seek": 81276, "start": 812.76, "end": 820.04, "text": " So everything starts with opening the Python runtime and reading the code.", "tokens": [50364, 407, 1203, 3719, 365, 5193, 264, 15329, 34474, 293, 3760, 264, 3089, 13, 50728], "temperature": 0.0, "avg_logprob": -0.23606603285845587, "compression_ratio": 1.6979166666666667, "no_speech_prob": 0.002464765915647149}, {"id": 171, "seek": 81276, "start": 820.04, "end": 828.2, "text": " This is the huge structure, like if you know the Python internals, like it's long.", "tokens": [50728, 639, 307, 264, 2603, 3877, 11, 411, 498, 291, 458, 264, 15329, 2154, 1124, 11, 411, 309, 311, 938, 13, 51136], "temperature": 0.0, "avg_logprob": -0.23606603285845587, "compression_ratio": 1.6979166666666667, "no_speech_prob": 0.002464765915647149}, {"id": 172, "seek": 81276, "start": 828.2, "end": 834.24, "text": " There are a lot of comments, but it's not the easiest code to read and it's not the", "tokens": [51136, 821, 366, 257, 688, 295, 3053, 11, 457, 309, 311, 406, 264, 12889, 3089, 281, 1401, 293, 309, 311, 406, 264, 51438], "temperature": 0.0, "avg_logprob": -0.23606603285845587, "compression_ratio": 1.6979166666666667, "no_speech_prob": 0.002464765915647149}, {"id": 173, "seek": 81276, "start": 834.24, "end": 837.56, "text": " easiest code to reason about.", "tokens": [51438, 12889, 3089, 281, 1778, 466, 13, 51604], "temperature": 0.0, "avg_logprob": -0.23606603285845587, "compression_ratio": 1.6979166666666667, "no_speech_prob": 0.002464765915647149}, {"id": 174, "seek": 81276, "start": 837.56, "end": 840.92, "text": " Let's focus on like what is the important bits, right?", "tokens": [51604, 961, 311, 1879, 322, 411, 437, 307, 264, 1021, 9239, 11, 558, 30, 51772], "temperature": 0.0, "avg_logprob": -0.23606603285845587, "compression_ratio": 1.6979166666666667, "no_speech_prob": 0.002464765915647149}, {"id": 175, "seek": 84092, "start": 840.92, "end": 849.7199999999999, "text": " We care about the interpreter state and from that bit, we want to capture what's going", "tokens": [50364, 492, 1127, 466, 264, 34132, 1785, 293, 490, 300, 857, 11, 321, 528, 281, 7983, 437, 311, 516, 50804], "temperature": 0.0, "avg_logprob": -0.2080184858139247, "compression_ratio": 1.565934065934066, "no_speech_prob": 0.012602372094988823}, {"id": 176, "seek": 84092, "start": 849.7199999999999, "end": 851.1999999999999, "text": " on in each thread, right?", "tokens": [50804, 322, 294, 1184, 7207, 11, 558, 30, 50878], "temperature": 0.0, "avg_logprob": -0.2080184858139247, "compression_ratio": 1.565934065934066, "no_speech_prob": 0.012602372094988823}, {"id": 177, "seek": 84092, "start": 851.1999999999999, "end": 858.12, "text": " It comes from the interpreter state and then we try to find the PyState itself.", "tokens": [50878, 467, 1487, 490, 264, 34132, 1785, 293, 550, 321, 853, 281, 915, 264, 9953, 4520, 473, 2564, 13, 51224], "temperature": 0.0, "avg_logprob": -0.2080184858139247, "compression_ratio": 1.565934065934066, "no_speech_prob": 0.012602372094988823}, {"id": 178, "seek": 84092, "start": 858.12, "end": 864.8, "text": " The PyThread structure, it's like a link list, so whenever you have multiple threads running", "tokens": [51224, 440, 9953, 2434, 2538, 3877, 11, 309, 311, 411, 257, 2113, 1329, 11, 370, 5699, 291, 362, 3866, 19314, 2614, 51558], "temperature": 0.0, "avg_logprob": -0.2080184858139247, "compression_ratio": 1.565934065934066, "no_speech_prob": 0.012602372094988823}, {"id": 179, "seek": 86480, "start": 864.8, "end": 873.3199999999999, "text": " in an interpreter, you need to traverse the whole link list and for each thread, you actually", "tokens": [50364, 294, 364, 34132, 11, 291, 643, 281, 45674, 264, 1379, 2113, 1329, 293, 337, 1184, 7207, 11, 291, 767, 50790], "temperature": 0.0, "avg_logprob": -0.27477476596832273, "compression_ratio": 1.8125, "no_speech_prob": 0.0259037297219038}, {"id": 180, "seek": 86480, "start": 873.3199999999999, "end": 875.24, "text": " need to do that.", "tokens": [50790, 643, 281, 360, 300, 13, 50886], "temperature": 0.0, "avg_logprob": -0.27477476596832273, "compression_ratio": 1.8125, "no_speech_prob": 0.0259037297219038}, {"id": 181, "seek": 86480, "start": 875.24, "end": 881.3199999999999, "text": " But also you need to find out which thread actually captures the guild and globally interpreter", "tokens": [50886, 583, 611, 291, 643, 281, 915, 484, 597, 7207, 767, 27986, 264, 37435, 293, 18958, 34132, 51190], "temperature": 0.0, "avg_logprob": -0.27477476596832273, "compression_ratio": 1.8125, "no_speech_prob": 0.0259037297219038}, {"id": 182, "seek": 86480, "start": 881.3199999999999, "end": 885.92, "text": " luck so that that's the one actually executing the code.", "tokens": [51190, 3668, 370, 300, 300, 311, 264, 472, 767, 32368, 264, 3089, 13, 51420], "temperature": 0.0, "avg_logprob": -0.27477476596832273, "compression_ratio": 1.8125, "no_speech_prob": 0.0259037297219038}, {"id": 183, "seek": 86480, "start": 885.92, "end": 892.4799999999999, "text": " So from finding all those information from thread state, you check the thread state.", "tokens": [51420, 407, 490, 5006, 439, 729, 1589, 490, 7207, 1785, 11, 291, 1520, 264, 7207, 1785, 13, 51748], "temperature": 0.0, "avg_logprob": -0.27477476596832273, "compression_ratio": 1.8125, "no_speech_prob": 0.0259037297219038}, {"id": 184, "seek": 89248, "start": 892.48, "end": 899.4, "text": " Oh, yeah, it's another like pages long C code that we need to reason about.", "tokens": [50364, 876, 11, 1338, 11, 309, 311, 1071, 411, 7183, 938, 383, 3089, 300, 321, 643, 281, 1778, 466, 13, 50710], "temperature": 0.0, "avg_logprob": -0.22205743335542225, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.0032156421802937984}, {"id": 185, "seek": 89248, "start": 899.4, "end": 904.12, "text": " It's not the easiest thing, but this is how reverse engineering kind of works.", "tokens": [50710, 467, 311, 406, 264, 12889, 551, 11, 457, 341, 307, 577, 9943, 7043, 733, 295, 1985, 13, 50946], "temperature": 0.0, "avg_logprob": -0.22205743335542225, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.0032156421802937984}, {"id": 186, "seek": 89248, "start": 904.12, "end": 907.32, "text": " And again, I extracted the important bits.", "tokens": [50946, 400, 797, 11, 286, 34086, 264, 1021, 9239, 13, 51106], "temperature": 0.0, "avg_logprob": -0.22205743335542225, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.0032156421802937984}, {"id": 187, "seek": 89248, "start": 907.32, "end": 913.24, "text": " So from that thread state, we need to find what is the current frame is actually executing", "tokens": [51106, 407, 490, 300, 7207, 1785, 11, 321, 643, 281, 915, 437, 307, 264, 2190, 3920, 307, 767, 32368, 51402], "temperature": 0.0, "avg_logprob": -0.22205743335542225, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.0032156421802937984}, {"id": 188, "seek": 89248, "start": 913.24, "end": 915.44, "text": " so that we can online from there, right?", "tokens": [51402, 370, 300, 321, 393, 2950, 490, 456, 11, 558, 30, 51512], "temperature": 0.0, "avg_logprob": -0.22205743335542225, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.0032156421802937984}, {"id": 189, "seek": 89248, "start": 915.44, "end": 919.5600000000001, "text": " That's actually the same thing that we are doing with the native stack, but rather than", "tokens": [51512, 663, 311, 767, 264, 912, 551, 300, 321, 366, 884, 365, 264, 8470, 8630, 11, 457, 2831, 813, 51718], "temperature": 0.0, "avg_logprob": -0.22205743335542225, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.0032156421802937984}, {"id": 190, "seek": 91956, "start": 919.9599999999999, "end": 924.92, "text": " checking some registries and reading roll memory addresses, we are actually checking the Python", "tokens": [50384, 8568, 512, 11376, 2244, 293, 3760, 3373, 4675, 16862, 11, 321, 366, 767, 8568, 264, 15329, 50632], "temperature": 0.0, "avg_logprob": -0.2654631673073282, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.004962707404047251}, {"id": 191, "seek": 91956, "start": 924.92, "end": 926.88, "text": " in turn itself.", "tokens": [50632, 294, 1261, 2564, 13, 50730], "temperature": 0.0, "avg_logprob": -0.2654631673073282, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.004962707404047251}, {"id": 192, "seek": 91956, "start": 926.88, "end": 935.68, "text": " So from the interpreter frame, it's actually easy, like whatever we need, it's here.", "tokens": [50730, 407, 490, 264, 34132, 3920, 11, 309, 311, 767, 1858, 11, 411, 2035, 321, 643, 11, 309, 311, 510, 13, 51170], "temperature": 0.0, "avg_logprob": -0.2654631673073282, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.004962707404047251}, {"id": 193, "seek": 91956, "start": 935.68, "end": 938.56, "text": " So all the information we need is here.", "tokens": [51170, 407, 439, 264, 1589, 321, 643, 307, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2654631673073282, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.004962707404047251}, {"id": 194, "seek": 91956, "start": 938.56, "end": 942.64, "text": " We have the pointers to the previous stack and we can actually do the same thing.", "tokens": [51314, 492, 362, 264, 44548, 281, 264, 3894, 8630, 293, 321, 393, 767, 360, 264, 912, 551, 13, 51518], "temperature": 0.0, "avg_logprob": -0.2654631673073282, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.004962707404047251}, {"id": 195, "seek": 91956, "start": 942.64, "end": 945.28, "text": " So I'm going to speed things up.", "tokens": [51518, 407, 286, 478, 516, 281, 3073, 721, 493, 13, 51650], "temperature": 0.0, "avg_logprob": -0.2654631673073282, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.004962707404047251}, {"id": 196, "seek": 91956, "start": 945.28, "end": 946.28, "text": " Yeah, we have the map.", "tokens": [51650, 865, 11, 321, 362, 264, 4471, 13, 51700], "temperature": 0.0, "avg_logprob": -0.2654631673073282, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.004962707404047251}, {"id": 197, "seek": 94628, "start": 946.28, "end": 949.0, "text": " We know the source code, but where do we actually start?", "tokens": [50364, 492, 458, 264, 4009, 3089, 11, 457, 689, 360, 321, 767, 722, 30, 50500], "temperature": 0.0, "avg_logprob": -0.2817174342640659, "compression_ratio": 1.792828685258964, "no_speech_prob": 0.011829907074570656}, {"id": 198, "seek": 94628, "start": 949.0, "end": 954.4, "text": " When we have an object file from the Python interpreter, we first need to find the where", "tokens": [50500, 1133, 321, 362, 364, 2657, 3991, 490, 264, 15329, 34132, 11, 321, 700, 643, 281, 915, 264, 689, 50770], "temperature": 0.0, "avg_logprob": -0.2817174342640659, "compression_ratio": 1.792828685258964, "no_speech_prob": 0.011829907074570656}, {"id": 199, "seek": 94628, "start": 954.4, "end": 956.76, "text": " does all these tracks are actually live.", "tokens": [50770, 775, 439, 613, 10218, 366, 767, 1621, 13, 50888], "temperature": 0.0, "avg_logprob": -0.2817174342640659, "compression_ratio": 1.792828685258964, "no_speech_prob": 0.011829907074570656}, {"id": 200, "seek": 94628, "start": 956.76, "end": 959.6, "text": " So we check the entry point of a Python interpreter.", "tokens": [50888, 407, 321, 1520, 264, 8729, 935, 295, 257, 15329, 34132, 13, 51030], "temperature": 0.0, "avg_logprob": -0.2817174342640659, "compression_ratio": 1.792828685258964, "no_speech_prob": 0.011829907074570656}, {"id": 201, "seek": 94628, "start": 959.6, "end": 962.92, "text": " We see that it's linked against a live Python.", "tokens": [51030, 492, 536, 300, 309, 311, 9408, 1970, 257, 1621, 15329, 13, 51196], "temperature": 0.0, "avg_logprob": -0.2817174342640659, "compression_ratio": 1.792828685258964, "no_speech_prob": 0.011829907074570656}, {"id": 202, "seek": 94628, "start": 962.92, "end": 968.76, "text": " We go and check the names for one of these tracks symbols and we actually see that there", "tokens": [51196, 492, 352, 293, 1520, 264, 5288, 337, 472, 295, 613, 10218, 16944, 293, 321, 767, 536, 300, 456, 51488], "temperature": 0.0, "avg_logprob": -0.2817174342640659, "compression_ratio": 1.792828685258964, "no_speech_prob": 0.011829907074570656}, {"id": 203, "seek": 94628, "start": 968.76, "end": 970.92, "text": " are some offsets that are located there.", "tokens": [51488, 366, 512, 39457, 1385, 300, 366, 6870, 456, 13, 51596], "temperature": 0.0, "avg_logprob": -0.2817174342640659, "compression_ratio": 1.792828685258964, "no_speech_prob": 0.011829907074570656}, {"id": 204, "seek": 94628, "start": 970.92, "end": 973.16, "text": " But this is just from the binary.", "tokens": [51596, 583, 341, 307, 445, 490, 264, 17434, 13, 51708], "temperature": 0.0, "avg_logprob": -0.2817174342640659, "compression_ratio": 1.792828685258964, "no_speech_prob": 0.011829907074570656}, {"id": 205, "seek": 97316, "start": 973.4, "end": 977.4399999999999, "text": " We don't know what these addresses mean when a process started.", "tokens": [50376, 492, 500, 380, 458, 437, 613, 16862, 914, 562, 257, 1399, 1409, 13, 50578], "temperature": 0.0, "avg_logprob": -0.23164085631674908, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0046159252524375916}, {"id": 206, "seek": 97316, "start": 977.4399999999999, "end": 985.4399999999999, "text": " So this happens because this is just one of the reasons, but when you get a binary and", "tokens": [50578, 407, 341, 2314, 570, 341, 307, 445, 472, 295, 264, 4112, 11, 457, 562, 291, 483, 257, 17434, 293, 50978], "temperature": 0.0, "avg_logprob": -0.23164085631674908, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0046159252524375916}, {"id": 207, "seek": 97316, "start": 985.4399999999999, "end": 992.92, "text": " run the process out of the binary, there is some address randomization and all those", "tokens": [50978, 1190, 264, 1399, 484, 295, 264, 17434, 11, 456, 307, 512, 2985, 4974, 2144, 293, 439, 729, 51352], "temperature": 0.0, "avg_logprob": -0.23164085631674908, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0046159252524375916}, {"id": 208, "seek": 97316, "start": 992.92, "end": 995.88, "text": " addresses need to be translated to that.", "tokens": [51352, 16862, 643, 281, 312, 16805, 281, 300, 13, 51500], "temperature": 0.0, "avg_logprob": -0.23164085631674908, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0046159252524375916}, {"id": 209, "seek": 97316, "start": 995.88, "end": 996.88, "text": " So how do we do that?", "tokens": [51500, 407, 577, 360, 321, 360, 300, 30, 51550], "temperature": 0.0, "avg_logprob": -0.23164085631674908, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0046159252524375916}, {"id": 210, "seek": 97316, "start": 996.88, "end": 1002.3199999999999, "text": " We just run a Python interpreter and check what's going on in the process.", "tokens": [51550, 492, 445, 1190, 257, 15329, 34132, 293, 1520, 437, 311, 516, 322, 294, 264, 1399, 13, 51822], "temperature": 0.0, "avg_logprob": -0.23164085631674908, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0046159252524375916}, {"id": 211, "seek": 100232, "start": 1003.2800000000001, "end": 1008.5600000000001, "text": " This is basically memory mappings and it shows you where actually Linux maps the certain", "tokens": [50412, 639, 307, 1936, 4675, 463, 28968, 293, 309, 3110, 291, 689, 767, 18734, 11317, 264, 1629, 50676], "temperature": 0.0, "avg_logprob": -0.2901956917035698, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0019179285736754537}, {"id": 212, "seek": 100232, "start": 1008.5600000000001, "end": 1011.96, "text": " objects and we check out the live Python.", "tokens": [50676, 6565, 293, 321, 1520, 484, 264, 1621, 15329, 13, 50846], "temperature": 0.0, "avg_logprob": -0.2901956917035698, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0019179285736754537}, {"id": 213, "seek": 100232, "start": 1011.96, "end": 1016.5600000000001, "text": " Grab the base address and from the addresses that we find from the symbol or draft, the", "tokens": [50846, 20357, 264, 3096, 2985, 293, 490, 264, 16862, 300, 321, 915, 490, 264, 5986, 420, 11206, 11, 264, 51076], "temperature": 0.0, "avg_logprob": -0.2901956917035698, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0019179285736754537}, {"id": 214, "seek": 100232, "start": 1016.5600000000001, "end": 1024.16, "text": " morph information, we actually find where the structs are actually located in the memory.", "tokens": [51076, 25778, 1589, 11, 321, 767, 915, 689, 264, 6594, 82, 366, 767, 6870, 294, 264, 4675, 13, 51456], "temperature": 0.0, "avg_logprob": -0.2901956917035698, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0019179285736754537}, {"id": 215, "seek": 100232, "start": 1024.16, "end": 1026.04, "text": " We are looking for those.", "tokens": [51456, 492, 366, 1237, 337, 729, 13, 51550], "temperature": 0.0, "avg_logprob": -0.2901956917035698, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0019179285736754537}, {"id": 216, "seek": 100232, "start": 1026.04, "end": 1028.6000000000001, "text": " And from that, now we need to read that data.", "tokens": [51550, 400, 490, 300, 11, 586, 321, 643, 281, 1401, 300, 1412, 13, 51678], "temperature": 0.0, "avg_logprob": -0.2901956917035698, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0019179285736754537}, {"id": 217, "seek": 100232, "start": 1028.6000000000001, "end": 1030.2, "text": " So here comes the GDB.", "tokens": [51678, 407, 510, 1487, 264, 460, 27735, 13, 51758], "temperature": 0.0, "avg_logprob": -0.2901956917035698, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0019179285736754537}, {"id": 218, "seek": 103020, "start": 1030.2, "end": 1037.48, "text": " GDB is like an amazing debugging tool and we jump into the process and start to poke", "tokens": [50364, 460, 27735, 307, 411, 364, 2243, 45592, 2290, 293, 321, 3012, 666, 264, 1399, 293, 722, 281, 19712, 50728], "temperature": 0.0, "avg_logprob": -0.21722174162911898, "compression_ratio": 1.7191489361702128, "no_speech_prob": 0.0018040367867797613}, {"id": 219, "seek": 103020, "start": 1037.48, "end": 1042.8400000000001, "text": " around, we define a macro and to calculate the offsets of a struct which reads from the", "tokens": [50728, 926, 11, 321, 6964, 257, 18887, 293, 281, 8873, 264, 39457, 1385, 295, 257, 6594, 597, 15700, 490, 264, 50996], "temperature": 0.0, "avg_logprob": -0.21722174162911898, "compression_ratio": 1.7191489361702128, "no_speech_prob": 0.0018040367867797613}, {"id": 220, "seek": 103020, "start": 1042.8400000000001, "end": 1048.6000000000001, "text": " devolve information, you say that, okay, give me this struct and this field and it gives", "tokens": [50996, 1905, 37361, 1589, 11, 291, 584, 300, 11, 1392, 11, 976, 385, 341, 6594, 293, 341, 2519, 293, 309, 2709, 51284], "temperature": 0.0, "avg_logprob": -0.21722174162911898, "compression_ratio": 1.7191489361702128, "no_speech_prob": 0.0018040367867797613}, {"id": 221, "seek": 103020, "start": 1048.6000000000001, "end": 1050.3600000000001, "text": " you the offset of that.", "tokens": [51284, 291, 264, 18687, 295, 300, 13, 51372], "temperature": 0.0, "avg_logprob": -0.21722174162911898, "compression_ratio": 1.7191489361702128, "no_speech_prob": 0.0018040367867797613}, {"id": 222, "seek": 103020, "start": 1050.3600000000001, "end": 1054.4, "text": " Since we already have the start address of the memory, we just calculate the next address", "tokens": [51372, 4162, 321, 1217, 362, 264, 722, 2985, 295, 264, 4675, 11, 321, 445, 8873, 264, 958, 2985, 51574], "temperature": 0.0, "avg_logprob": -0.21722174162911898, "compression_ratio": 1.7191489361702128, "no_speech_prob": 0.0018040367867797613}, {"id": 223, "seek": 103020, "start": 1054.4, "end": 1056.8, "text": " and read the data from that.", "tokens": [51574, 293, 1401, 264, 1412, 490, 300, 13, 51694], "temperature": 0.0, "avg_logprob": -0.21722174162911898, "compression_ratio": 1.7191489361702128, "no_speech_prob": 0.0018040367867797613}, {"id": 224, "seek": 105680, "start": 1056.8, "end": 1059.28, "text": " But as you can see, this is very manual labor.", "tokens": [50364, 583, 382, 291, 393, 536, 11, 341, 307, 588, 9688, 5938, 13, 50488], "temperature": 0.0, "avg_logprob": -0.2821507410171929, "compression_ratio": 1.593984962406015, "no_speech_prob": 0.0036987410858273506}, {"id": 225, "seek": 105680, "start": 1059.28, "end": 1064.08, "text": " We cannot do this for each and every Python version or implementation out there.", "tokens": [50488, 492, 2644, 360, 341, 337, 1184, 293, 633, 15329, 3037, 420, 11420, 484, 456, 13, 50728], "temperature": 0.0, "avg_logprob": -0.2821507410171929, "compression_ratio": 1.593984962406015, "no_speech_prob": 0.0036987410858273506}, {"id": 226, "seek": 105680, "start": 1064.08, "end": 1067.72, "text": " So we do this with another project ahead of time.", "tokens": [50728, 407, 321, 360, 341, 365, 1071, 1716, 2286, 295, 565, 13, 50910], "temperature": 0.0, "avg_logprob": -0.2821507410171929, "compression_ratio": 1.593984962406015, "no_speech_prob": 0.0036987410858273506}, {"id": 227, "seek": 105680, "start": 1067.72, "end": 1073.52, "text": " We use Rust-Pyne-Gen for that which was super convenient because PySpy was using that.", "tokens": [50910, 492, 764, 34952, 12, 47, 88, 716, 12, 26647, 337, 300, 597, 390, 1687, 10851, 570, 9953, 14014, 88, 390, 1228, 300, 13, 51200], "temperature": 0.0, "avg_logprob": -0.2821507410171929, "compression_ratio": 1.593984962406015, "no_speech_prob": 0.0036987410858273506}, {"id": 228, "seek": 105680, "start": 1073.52, "end": 1078.48, "text": " We just grabbed some offsets and generated all these things for a couple of versions.", "tokens": [51200, 492, 445, 18607, 512, 39457, 1385, 293, 10833, 439, 613, 721, 337, 257, 1916, 295, 9606, 13, 51448], "temperature": 0.0, "avg_logprob": -0.2821507410171929, "compression_ratio": 1.593984962406015, "no_speech_prob": 0.0036987410858273506}, {"id": 229, "seek": 105680, "start": 1078.48, "end": 1082.12, "text": " But we are also working on a devolve-based reader which is more scalable.", "tokens": [51448, 583, 321, 366, 611, 1364, 322, 257, 1905, 37361, 12, 6032, 15149, 597, 307, 544, 38481, 13, 51630], "temperature": 0.0, "avg_logprob": -0.2821507410171929, "compression_ratio": 1.593984962406015, "no_speech_prob": 0.0036987410858273506}, {"id": 230, "seek": 108212, "start": 1082.12, "end": 1087.9199999999998, "text": " You just grab any binary, read the debug information and calculate all the offsets.", "tokens": [50364, 509, 445, 4444, 604, 17434, 11, 1401, 264, 24083, 1589, 293, 8873, 439, 264, 39457, 1385, 13, 50654], "temperature": 0.0, "avg_logprob": -0.22230363600324876, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.01524490024894476}, {"id": 231, "seek": 108212, "start": 1087.9199999999998, "end": 1092.28, "text": " From those offsets, we generate this struct which we think over the kernel space.", "tokens": [50654, 3358, 729, 39457, 1385, 11, 321, 8460, 341, 6594, 597, 321, 519, 670, 264, 28256, 1901, 13, 50872], "temperature": 0.0, "avg_logprob": -0.22230363600324876, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.01524490024894476}, {"id": 232, "seek": 108212, "start": 1092.28, "end": 1096.6399999999999, "text": " It's like a map, where to find the fields and everything.", "tokens": [50872, 467, 311, 411, 257, 4471, 11, 689, 281, 915, 264, 7909, 293, 1203, 13, 51090], "temperature": 0.0, "avg_logprob": -0.22230363600324876, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.01524490024894476}, {"id": 233, "seek": 108212, "start": 1096.6399999999999, "end": 1102.32, "text": " And the nice part that the whole things that we are working on, it's going to be deprecated", "tokens": [51090, 400, 264, 1481, 644, 300, 264, 1379, 721, 300, 321, 366, 1364, 322, 11, 309, 311, 516, 281, 312, 1367, 13867, 770, 51374], "temperature": 0.0, "avg_logprob": -0.22230363600324876, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.01524490024894476}, {"id": 234, "seek": 108212, "start": 1102.32, "end": 1103.8, "text": " soon because Python...", "tokens": [51374, 2321, 570, 15329, 485, 51448], "temperature": 0.0, "avg_logprob": -0.22230363600324876, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.01524490024894476}, {"id": 235, "seek": 108212, "start": 1103.8, "end": 1109.1999999999998, "text": " This is life of a software engineer when you do reverse engineering.", "tokens": [51448, 639, 307, 993, 295, 257, 4722, 11403, 562, 291, 360, 9943, 7043, 13, 51718], "temperature": 0.0, "avg_logprob": -0.22230363600324876, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.01524490024894476}, {"id": 236, "seek": 110920, "start": 1109.2, "end": 1114.48, "text": " So something super cool happened in the Python main branch.", "tokens": [50364, 407, 746, 1687, 1627, 2011, 294, 264, 15329, 2135, 9819, 13, 50628], "temperature": 0.0, "avg_logprob": -0.30301308846688485, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.025256358087062836}, {"id": 237, "seek": 110920, "start": 1114.48, "end": 1118.72, "text": " Now they have this debug offset data structure value.", "tokens": [50628, 823, 436, 362, 341, 24083, 18687, 1412, 3877, 2158, 13, 50840], "temperature": 0.0, "avg_logprob": -0.30301308846688485, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.025256358087062836}, {"id": 238, "seek": 110920, "start": 1118.72, "end": 1124.04, "text": " They generate all those sets and put that in just the beginning of the py runtime.", "tokens": [50840, 814, 8460, 439, 729, 6352, 293, 829, 300, 294, 445, 264, 2863, 295, 264, 10664, 34474, 13, 51106], "temperature": 0.0, "avg_logprob": -0.30301308846688485, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.025256358087062836}, {"id": 239, "seek": 110920, "start": 1124.04, "end": 1127.88, "text": " We just can grab the address and just read the first chunk of the thing.", "tokens": [51106, 492, 445, 393, 4444, 264, 2985, 293, 445, 1401, 264, 700, 16635, 295, 264, 551, 13, 51298], "temperature": 0.0, "avg_logprob": -0.30301308846688485, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.025256358087062836}, {"id": 240, "seek": 110920, "start": 1127.88, "end": 1132.92, "text": " How we got those that we don't need to do this ahead of time things right now.", "tokens": [51298, 1012, 321, 658, 729, 300, 321, 500, 380, 643, 281, 360, 341, 2286, 295, 565, 721, 558, 586, 13, 51550], "temperature": 0.0, "avg_logprob": -0.30301308846688485, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.025256358087062836}, {"id": 241, "seek": 110920, "start": 1132.92, "end": 1138.56, "text": " So this is already a merge and it's going to be released with the Python 3.0 in 13.0.", "tokens": [51550, 407, 341, 307, 1217, 257, 22183, 293, 309, 311, 516, 281, 312, 4736, 365, 264, 15329, 805, 13, 15, 294, 3705, 13, 15, 13, 51832], "temperature": 0.0, "avg_logprob": -0.30301308846688485, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.025256358087062836}, {"id": 242, "seek": 113856, "start": 1138.6, "end": 1143.32, "text": " It's also huge, lots of stuff that you need to find out.", "tokens": [50366, 467, 311, 611, 2603, 11, 3195, 295, 1507, 300, 291, 643, 281, 915, 484, 13, 50602], "temperature": 0.0, "avg_logprob": -0.304652187802376, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.011017221957445145}, {"id": 243, "seek": 113856, "start": 1143.32, "end": 1145.48, "text": " Okay, actual unwinding stack.", "tokens": [50602, 1033, 11, 3539, 14853, 9245, 8630, 13, 50710], "temperature": 0.0, "avg_logprob": -0.304652187802376, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.011017221957445145}, {"id": 244, "seek": 113856, "start": 1145.48, "end": 1149.52, "text": " So these are where the EVPS comes from.", "tokens": [50710, 407, 613, 366, 689, 264, 15733, 6273, 1487, 490, 13, 50912], "temperature": 0.0, "avg_logprob": -0.304652187802376, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.011017221957445145}, {"id": 245, "seek": 113856, "start": 1149.52, "end": 1155.6399999999999, "text": " We did all the magic, we got the offsets, we put that thing into interpreter info structure", "tokens": [50912, 492, 630, 439, 264, 5585, 11, 321, 658, 264, 39457, 1385, 11, 321, 829, 300, 551, 666, 34132, 13614, 3877, 51218], "temperature": 0.0, "avg_logprob": -0.304652187802376, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.011017221957445145}, {"id": 246, "seek": 113856, "start": 1155.6399999999999, "end": 1160.72, "text": " and put that in an EVPF map so that the EVPF program can read in the kernel.", "tokens": [51218, 293, 829, 300, 294, 364, 15733, 47, 37, 4471, 370, 300, 264, 15733, 47, 37, 1461, 393, 1401, 294, 264, 28256, 13, 51472], "temperature": 0.0, "avg_logprob": -0.304652187802376, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.011017221957445145}, {"id": 247, "seek": 113856, "start": 1160.72, "end": 1162.84, "text": " And this C code is in the kernel itself.", "tokens": [51472, 400, 341, 383, 3089, 307, 294, 264, 28256, 2564, 13, 51578], "temperature": 0.0, "avg_logprob": -0.304652187802376, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.011017221957445145}, {"id": 248, "seek": 113856, "start": 1162.84, "end": 1166.24, "text": " You check something and get the interpreter info.", "tokens": [51578, 509, 1520, 746, 293, 483, 264, 34132, 13614, 13, 51748], "temperature": 0.0, "avg_logprob": -0.304652187802376, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.011017221957445145}, {"id": 249, "seek": 116624, "start": 1166.24, "end": 1171.96, "text": " This is the user space code where we actually calculate all the addresses and send and put", "tokens": [50364, 639, 307, 264, 4195, 1901, 3089, 689, 321, 767, 8873, 439, 264, 16862, 293, 2845, 293, 829, 50650], "temperature": 0.0, "avg_logprob": -0.18742052714029947, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.0014910884201526642}, {"id": 250, "seek": 116624, "start": 1171.96, "end": 1175.28, "text": " to the EVPF map itself.", "tokens": [50650, 281, 264, 15733, 47, 37, 4471, 2564, 13, 50816], "temperature": 0.0, "avg_logprob": -0.18742052714029947, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.0014910884201526642}, {"id": 251, "seek": 116624, "start": 1175.28, "end": 1179.28, "text": " And from that we also grab this offset data that we calculated.", "tokens": [50816, 400, 490, 300, 321, 611, 4444, 341, 18687, 1412, 300, 321, 15598, 13, 51016], "temperature": 0.0, "avg_logprob": -0.18742052714029947, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.0014910884201526642}, {"id": 252, "seek": 116624, "start": 1179.28, "end": 1187.88, "text": " We just do check some versions and find the runtime version of that particular Python interpreter,", "tokens": [51016, 492, 445, 360, 1520, 512, 9606, 293, 915, 264, 34474, 3037, 295, 300, 1729, 15329, 34132, 11, 51446], "temperature": 0.0, "avg_logprob": -0.18742052714029947, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.0014910884201526642}, {"id": 253, "seek": 116624, "start": 1187.88, "end": 1191.48, "text": " read the offsets and from that offsets we calculate that.", "tokens": [51446, 1401, 264, 39457, 1385, 293, 490, 300, 39457, 1385, 321, 8873, 300, 13, 51626], "temperature": 0.0, "avg_logprob": -0.18742052714029947, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.0014910884201526642}, {"id": 254, "seek": 116624, "start": 1191.48, "end": 1196.0, "text": " But again, like 3.13, this is, this will be futile.", "tokens": [51626, 583, 797, 11, 411, 805, 13, 7668, 11, 341, 307, 11, 341, 486, 312, 1877, 794, 13, 51852], "temperature": 0.0, "avg_logprob": -0.18742052714029947, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.0014910884201526642}, {"id": 255, "seek": 119600, "start": 1196.0, "end": 1205.84, "text": " So then we read, try to read the, oops, rate data from the thread state, find those structures", "tokens": [50364, 407, 550, 321, 1401, 11, 853, 281, 1401, 264, 11, 34166, 11, 3314, 1412, 490, 264, 7207, 1785, 11, 915, 729, 9227, 50856], "temperature": 0.0, "avg_logprob": -0.23158918108258927, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.001075609470717609}, {"id": 256, "seek": 119600, "start": 1205.84, "end": 1210.12, "text": " and read the pointers and try to find out where to go from there.", "tokens": [50856, 293, 1401, 264, 44548, 293, 853, 281, 915, 484, 689, 281, 352, 490, 456, 13, 51070], "temperature": 0.0, "avg_logprob": -0.23158918108258927, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.001075609470717609}, {"id": 257, "seek": 119600, "start": 1210.12, "end": 1213.36, "text": " Five minutes left, I need to be super quick.", "tokens": [51070, 9436, 2077, 1411, 11, 286, 643, 281, 312, 1687, 1702, 13, 51232], "temperature": 0.0, "avg_logprob": -0.23158918108258927, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.001075609470717609}, {"id": 258, "seek": 119600, "start": 1213.36, "end": 1219.72, "text": " Okay, so we find the, we try to find the initial pointer for the virtual frames.", "tokens": [51232, 1033, 11, 370, 321, 915, 264, 11, 321, 853, 281, 915, 264, 5883, 23918, 337, 264, 6374, 12083, 13, 51550], "temperature": 0.0, "avg_logprob": -0.23158918108258927, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.001075609470717609}, {"id": 259, "seek": 119600, "start": 1219.72, "end": 1221.48, "text": " This is how we do it.", "tokens": [51550, 639, 307, 577, 321, 360, 309, 13, 51638], "temperature": 0.0, "avg_logprob": -0.23158918108258927, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.001075609470717609}, {"id": 260, "seek": 119600, "start": 1221.48, "end": 1224.64, "text": " And then from that we start walking the stack.", "tokens": [51638, 400, 550, 490, 300, 321, 722, 4494, 264, 8630, 13, 51796], "temperature": 0.0, "avg_logprob": -0.23158918108258927, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.001075609470717609}, {"id": 261, "seek": 122464, "start": 1224.64, "end": 1232.0, "text": " The key points here is just from the previous code that you can see, we actually put just", "tokens": [50364, 440, 2141, 2793, 510, 307, 445, 490, 264, 3894, 3089, 300, 291, 393, 536, 11, 321, 767, 829, 445, 50732], "temperature": 0.0, "avg_logprob": -0.18614053035127945, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0014168742345646024}, {"id": 262, "seek": 122464, "start": 1232.0, "end": 1239.0800000000002, "text": " line 13, put something into a state frame pointer and we then read that frame pointer", "tokens": [50732, 1622, 3705, 11, 829, 746, 666, 257, 1785, 3920, 23918, 293, 321, 550, 1401, 300, 3920, 23918, 51086], "temperature": 0.0, "avg_logprob": -0.18614053035127945, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0014168742345646024}, {"id": 263, "seek": 122464, "start": 1239.0800000000002, "end": 1241.16, "text": " from another EVPF program.", "tokens": [51086, 490, 1071, 15733, 47, 37, 1461, 13, 51190], "temperature": 0.0, "avg_logprob": -0.18614053035127945, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0014168742345646024}, {"id": 264, "seek": 122464, "start": 1241.16, "end": 1250.0, "text": " And from that pointer we basically find the offset of the frame object where the code", "tokens": [51190, 400, 490, 300, 23918, 321, 1936, 915, 264, 18687, 295, 264, 3920, 2657, 689, 264, 3089, 51632], "temperature": 0.0, "avg_logprob": -0.18614053035127945, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0014168742345646024}, {"id": 265, "seek": 125000, "start": 1250.0, "end": 1257.52, "text": " points to, read that row address with some VPF helper code.", "tokens": [50364, 2793, 281, 11, 1401, 300, 5386, 2985, 365, 512, 35812, 37, 36133, 3089, 13, 50740], "temperature": 0.0, "avg_logprob": -0.234737420693422, "compression_ratio": 1.6263157894736842, "no_speech_prob": 0.03812474384903908}, {"id": 266, "seek": 125000, "start": 1257.52, "end": 1264.36, "text": " And from that we read the symbol because like the addresses that we saw that they don't", "tokens": [50740, 400, 490, 300, 321, 1401, 264, 5986, 570, 411, 264, 16862, 300, 321, 1866, 300, 436, 500, 380, 51082], "temperature": 0.0, "avg_logprob": -0.234737420693422, "compression_ratio": 1.6263157894736842, "no_speech_prob": 0.03812474384903908}, {"id": 267, "seek": 125000, "start": 1264.36, "end": 1265.36, "text": " mean anything to us.", "tokens": [51082, 914, 1340, 281, 505, 13, 51132], "temperature": 0.0, "avg_logprob": -0.234737420693422, "compression_ratio": 1.6263157894736842, "no_speech_prob": 0.03812474384903908}, {"id": 268, "seek": 125000, "start": 1265.36, "end": 1269.2, "text": " We are humans and we need some human readable data.", "tokens": [51132, 492, 366, 6255, 293, 321, 643, 512, 1952, 49857, 1412, 13, 51324], "temperature": 0.0, "avg_logprob": -0.234737420693422, "compression_ratio": 1.6263157894736842, "no_speech_prob": 0.03812474384903908}, {"id": 269, "seek": 125000, "start": 1269.2, "end": 1276.32, "text": " And from that to be efficient because we keep seeing the same stack traces, we just like", "tokens": [51324, 400, 490, 300, 281, 312, 7148, 570, 321, 1066, 2577, 264, 912, 8630, 26076, 11, 321, 445, 411, 51680], "temperature": 0.0, "avg_logprob": -0.234737420693422, "compression_ratio": 1.6263157894736842, "no_speech_prob": 0.03812474384903908}, {"id": 270, "seek": 127632, "start": 1276.32, "end": 1280.56, "text": " hash it and put that symbol somewhere so that if we see that we don't need to send", "tokens": [50364, 22019, 309, 293, 829, 300, 5986, 4079, 370, 300, 498, 321, 536, 300, 321, 500, 380, 643, 281, 2845, 50576], "temperature": 0.0, "avg_logprob": -0.2040159742710954, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.47778528928756714}, {"id": 271, "seek": 127632, "start": 1280.56, "end": 1284.4399999999998, "text": " that same symbol to the user space because it's costly.", "tokens": [50576, 300, 912, 5986, 281, 264, 4195, 1901, 570, 309, 311, 28328, 13, 50770], "temperature": 0.0, "avg_logprob": -0.2040159742710954, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.47778528928756714}, {"id": 272, "seek": 127632, "start": 1284.4399999999998, "end": 1290.36, "text": " And then we also encode the line number because like symbols just represents a class, then", "tokens": [50770, 400, 550, 321, 611, 2058, 1429, 264, 1622, 1230, 570, 411, 16944, 445, 8855, 257, 1508, 11, 550, 51066], "temperature": 0.0, "avg_logprob": -0.2040159742710954, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.47778528928756714}, {"id": 273, "seek": 127632, "start": 1290.36, "end": 1294.84, "text": " a function and then there's a line number in that function that is different.", "tokens": [51066, 257, 2445, 293, 550, 456, 311, 257, 1622, 1230, 294, 300, 2445, 300, 307, 819, 13, 51290], "temperature": 0.0, "avg_logprob": -0.2040159742710954, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.47778528928756714}, {"id": 274, "seek": 127632, "start": 1294.84, "end": 1297.72, "text": " This API also recently stabilizing the Python.", "tokens": [51290, 639, 9362, 611, 3938, 11652, 3319, 264, 15329, 13, 51434], "temperature": 0.0, "avg_logprob": -0.2040159742710954, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.47778528928756714}, {"id": 275, "seek": 127632, "start": 1297.72, "end": 1303.28, "text": " So for the old Python versions this line number could be wrong but the read after 3.10 it's", "tokens": [51434, 407, 337, 264, 1331, 15329, 9606, 341, 1622, 1230, 727, 312, 2085, 457, 264, 1401, 934, 805, 13, 3279, 309, 311, 51712], "temperature": 0.0, "avg_logprob": -0.2040159742710954, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.47778528928756714}, {"id": 276, "seek": 127632, "start": 1303.28, "end": 1305.96, "text": " actually, it should be accurate.", "tokens": [51712, 767, 11, 309, 820, 312, 8559, 13, 51846], "temperature": 0.0, "avg_logprob": -0.2040159742710954, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.47778528928756714}, {"id": 277, "seek": 130596, "start": 1305.96, "end": 1311.1200000000001, "text": " And then we encode that as well and send it to the user space.", "tokens": [50364, 400, 550, 321, 2058, 1429, 300, 382, 731, 293, 2845, 309, 281, 264, 4195, 1901, 13, 50622], "temperature": 0.0, "avg_logprob": -0.18281334058373375, "compression_ratio": 1.7237354085603114, "no_speech_prob": 0.0074447509832680225}, {"id": 278, "seek": 130596, "start": 1311.1200000000001, "end": 1313.4, "text": " This is the reading the symbol parts.", "tokens": [50622, 639, 307, 264, 3760, 264, 5986, 3166, 13, 50736], "temperature": 0.0, "avg_logprob": -0.18281334058373375, "compression_ratio": 1.7237354085603114, "no_speech_prob": 0.0074447509832680225}, {"id": 279, "seek": 130596, "start": 1313.4, "end": 1315.76, "text": " Like the code is like super complicated.", "tokens": [50736, 1743, 264, 3089, 307, 411, 1687, 6179, 13, 50854], "temperature": 0.0, "avg_logprob": -0.18281334058373375, "compression_ratio": 1.7237354085603114, "no_speech_prob": 0.0074447509832680225}, {"id": 280, "seek": 130596, "start": 1315.76, "end": 1320.08, "text": " I just highlighted the GDP outputs because it's like easier to read.", "tokens": [50854, 286, 445, 17173, 264, 19599, 23930, 570, 309, 311, 411, 3571, 281, 1401, 13, 51070], "temperature": 0.0, "avg_logprob": -0.18281334058373375, "compression_ratio": 1.7237354085603114, "no_speech_prob": 0.0074447509832680225}, {"id": 281, "seek": 130596, "start": 1320.08, "end": 1325.52, "text": " So you read this nested structures and find the actual type name, then the file name, then", "tokens": [51070, 407, 291, 1401, 341, 15646, 292, 9227, 293, 915, 264, 3539, 2010, 1315, 11, 550, 264, 3991, 1315, 11, 550, 51342], "temperature": 0.0, "avg_logprob": -0.18281334058373375, "compression_ratio": 1.7237354085603114, "no_speech_prob": 0.0074447509832680225}, {"id": 282, "seek": 130596, "start": 1325.52, "end": 1330.8, "text": " the name of the object code and the first line in that function and put an encode that", "tokens": [51342, 264, 1315, 295, 264, 2657, 3089, 293, 264, 700, 1622, 294, 300, 2445, 293, 829, 364, 2058, 1429, 300, 51606], "temperature": 0.0, "avg_logprob": -0.18281334058373375, "compression_ratio": 1.7237354085603114, "no_speech_prob": 0.0074447509832680225}, {"id": 283, "seek": 130596, "start": 1330.8, "end": 1335.3600000000001, "text": " is a symbol so that it means something for the humans.", "tokens": [51606, 307, 257, 5986, 370, 300, 309, 1355, 746, 337, 264, 6255, 13, 51834], "temperature": 0.0, "avg_logprob": -0.18281334058373375, "compression_ratio": 1.7237354085603114, "no_speech_prob": 0.0074447509832680225}, {"id": 284, "seek": 133536, "start": 1335.7199999999998, "end": 1340.84, "text": " Voila, now we finally have Python unwinded stack.", "tokens": [50382, 7518, 7371, 11, 586, 321, 2721, 362, 15329, 517, 12199, 292, 8630, 13, 50638], "temperature": 0.0, "avg_logprob": -0.26051894021690436, "compression_ratio": 1.794759825327511, "no_speech_prob": 0.10756246000528336}, {"id": 285, "seek": 133536, "start": 1340.84, "end": 1346.52, "text": " But as you can see there are lots of things going, happening and most of the things are", "tokens": [50638, 583, 382, 291, 393, 536, 456, 366, 3195, 295, 721, 516, 11, 2737, 293, 881, 295, 264, 721, 366, 50922], "temperature": 0.0, "avg_logprob": -0.26051894021690436, "compression_ratio": 1.794759825327511, "no_speech_prob": 0.10756246000528336}, {"id": 286, "seek": 133536, "start": 1346.52, "end": 1349.8, "text": " interpreters and it doesn't make any sense.", "tokens": [50922, 17489, 1559, 293, 309, 1177, 380, 652, 604, 2020, 13, 51086], "temperature": 0.0, "avg_logprob": -0.26051894021690436, "compression_ratio": 1.794759825327511, "no_speech_prob": 0.10756246000528336}, {"id": 287, "seek": 133536, "start": 1349.8, "end": 1355.8799999999999, "text": " But we have this cool UI, you can actually, these are like color coded, you can, from", "tokens": [51086, 583, 321, 362, 341, 1627, 15682, 11, 291, 393, 767, 11, 613, 366, 411, 2017, 34874, 11, 291, 393, 11, 490, 51390], "temperature": 0.0, "avg_logprob": -0.26051894021690436, "compression_ratio": 1.794759825327511, "no_speech_prob": 0.10756246000528336}, {"id": 288, "seek": 133536, "start": 1355.8799999999999, "end": 1360.08, "text": " the color code you can actually highlight like what's going on in the interpreter, what", "tokens": [51390, 264, 2017, 3089, 291, 393, 767, 5078, 411, 437, 311, 516, 322, 294, 264, 34132, 11, 437, 51600], "temperature": 0.0, "avg_logprob": -0.26051894021690436, "compression_ratio": 1.794759825327511, "no_speech_prob": 0.10756246000528336}, {"id": 289, "seek": 133536, "start": 1360.08, "end": 1363.9599999999998, "text": " happens to the libc, libpython, you can see everything.", "tokens": [51600, 2314, 281, 264, 22854, 66, 11, 22854, 8200, 11943, 11, 291, 393, 536, 1203, 13, 51794], "temperature": 0.0, "avg_logprob": -0.26051894021690436, "compression_ratio": 1.794759825327511, "no_speech_prob": 0.10756246000528336}, {"id": 290, "seek": 136396, "start": 1364.4, "end": 1366.76, "text": " Again, we want to focus on the Python bits, right?", "tokens": [50386, 3764, 11, 321, 528, 281, 1879, 322, 264, 15329, 9239, 11, 558, 30, 50504], "temperature": 0.0, "avg_logprob": -0.29124134422367454, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.005075730383396149}, {"id": 291, "seek": 136396, "start": 1366.76, "end": 1374.52, "text": " You can just filter out the Python code and see that it's recursively calling and calculating", "tokens": [50504, 509, 393, 445, 6608, 484, 264, 15329, 3089, 293, 536, 300, 309, 311, 20560, 3413, 5141, 293, 28258, 50892], "temperature": 0.0, "avg_logprob": -0.29124134422367454, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.005075730383396149}, {"id": 292, "seek": 136396, "start": 1374.52, "end": 1375.96, "text": " some Fibonacci numbers.", "tokens": [50892, 512, 479, 897, 266, 43870, 3547, 13, 50964], "temperature": 0.0, "avg_logprob": -0.29124134422367454, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.005075730383396149}, {"id": 293, "seek": 136396, "start": 1375.96, "end": 1379.68, "text": " Apparently, yeah, it's inefficient so you need to optimize this.", "tokens": [50964, 16755, 11, 1338, 11, 309, 311, 43495, 370, 291, 643, 281, 19719, 341, 13, 51150], "temperature": 0.0, "avg_logprob": -0.29124134422367454, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.005075730383396149}, {"id": 294, "seek": 136396, "start": 1381.68, "end": 1386.24, "text": " Yeah, you can just tell by like that list of the stack, okay, yeah.", "tokens": [51250, 865, 11, 291, 393, 445, 980, 538, 411, 300, 1329, 295, 264, 8630, 11, 1392, 11, 1338, 13, 51478], "temperature": 0.0, "avg_logprob": -0.29124134422367454, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.005075730383396149}, {"id": 295, "seek": 136396, "start": 1386.24, "end": 1390.3600000000001, "text": " You don't need to know the details of how to read the flame graph.", "tokens": [51478, 509, 500, 380, 643, 281, 458, 264, 4365, 295, 577, 281, 1401, 264, 13287, 4295, 13, 51684], "temperature": 0.0, "avg_logprob": -0.29124134422367454, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.005075730383396149}, {"id": 296, "seek": 136396, "start": 1390.3600000000001, "end": 1393.04, "text": " But good thing for you, we also have a blog post for that.", "tokens": [51684, 583, 665, 551, 337, 291, 11, 321, 611, 362, 257, 6968, 2183, 337, 300, 13, 51818], "temperature": 0.0, "avg_logprob": -0.29124134422367454, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.005075730383396149}, {"id": 297, "seek": 139304, "start": 1393.28, "end": 1394.92, "text": " You can check it out.", "tokens": [50376, 509, 393, 1520, 309, 484, 13, 50458], "temperature": 0.0, "avg_logprob": -0.272580623626709, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.004647007677704096}, {"id": 298, "seek": 139304, "start": 1394.92, "end": 1398.36, "text": " So I guess we are nearly out of time.", "tokens": [50458, 407, 286, 2041, 321, 366, 6217, 484, 295, 565, 13, 50630], "temperature": 0.0, "avg_logprob": -0.272580623626709, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.004647007677704096}, {"id": 299, "seek": 139304, "start": 1398.36, "end": 1404.28, "text": " So we support a couple of interpreters like 2.7, we still support that.", "tokens": [50630, 407, 321, 1406, 257, 1916, 295, 17489, 1559, 411, 568, 13, 22, 11, 321, 920, 1406, 300, 13, 50926], "temperature": 0.0, "avg_logprob": -0.272580623626709, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.004647007677704096}, {"id": 300, "seek": 139304, "start": 1404.28, "end": 1409.8799999999999, "text": " So if you happen to have that, we support everything until 3.11.", "tokens": [50926, 407, 498, 291, 1051, 281, 362, 300, 11, 321, 1406, 1203, 1826, 805, 13, 5348, 13, 51206], "temperature": 0.0, "avg_logprob": -0.272580623626709, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.004647007677704096}, {"id": 301, "seek": 139304, "start": 1409.8799999999999, "end": 1417.1599999999999, "text": " We are working on the 3.12 because 3.12 changes where the actual trade state also stored,", "tokens": [51206, 492, 366, 1364, 322, 264, 805, 13, 4762, 570, 805, 13, 4762, 2962, 689, 264, 3539, 4923, 1785, 611, 12187, 11, 51570], "temperature": 0.0, "avg_logprob": -0.272580623626709, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.004647007677704096}, {"id": 302, "seek": 139304, "start": 1417.1599999999999, "end": 1419.52, "text": " which is the trade local storage.", "tokens": [51570, 597, 307, 264, 4923, 2654, 6725, 13, 51688], "temperature": 0.0, "avg_logprob": -0.272580623626709, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.004647007677704096}, {"id": 303, "seek": 141952, "start": 1419.52, "end": 1424.08, "text": " We are working on the facilities to read the state from the trade local storage.", "tokens": [50364, 492, 366, 1364, 322, 264, 9406, 281, 1401, 264, 1785, 490, 264, 4923, 2654, 6725, 13, 50592], "temperature": 0.0, "avg_logprob": -0.1999518637563668, "compression_ratio": 1.5137254901960784, "no_speech_prob": 0.0014249561354517937}, {"id": 304, "seek": 141952, "start": 1424.08, "end": 1428.12, "text": " It shouldn't take more than a couple of weeks to be that support actually landed.", "tokens": [50592, 467, 4659, 380, 747, 544, 813, 257, 1916, 295, 3259, 281, 312, 300, 1406, 767, 15336, 13, 50794], "temperature": 0.0, "avg_logprob": -0.1999518637563668, "compression_ratio": 1.5137254901960784, "no_speech_prob": 0.0014249561354517937}, {"id": 305, "seek": 141952, "start": 1428.12, "end": 1435.28, "text": " And 3.13 will be there so we don't need to do this again, basically, for the next version of the Python.", "tokens": [50794, 400, 805, 13, 7668, 486, 312, 456, 370, 321, 500, 380, 643, 281, 360, 341, 797, 11, 1936, 11, 337, 264, 958, 3037, 295, 264, 15329, 13, 51152], "temperature": 0.0, "avg_logprob": -0.1999518637563668, "compression_ratio": 1.5137254901960784, "no_speech_prob": 0.0014249561354517937}, {"id": 306, "seek": 141952, "start": 1435.28, "end": 1441.68, "text": " So everything you can see, please try, install and give us a feedback.", "tokens": [51152, 407, 1203, 291, 393, 536, 11, 1767, 853, 11, 3625, 293, 976, 505, 257, 5824, 13, 51472], "temperature": 0.0, "avg_logprob": -0.1999518637563668, "compression_ratio": 1.5137254901960784, "no_speech_prob": 0.0014249561354517937}, {"id": 307, "seek": 141952, "start": 1441.68, "end": 1443.32, "text": " There's this QR code.", "tokens": [51472, 821, 311, 341, 32784, 3089, 13, 51554], "temperature": 0.0, "avg_logprob": -0.1999518637563668, "compression_ratio": 1.5137254901960784, "no_speech_prob": 0.0014249561354517937}, {"id": 308, "seek": 141952, "start": 1443.32, "end": 1444.84, "text": " It's a GitHub discussion.", "tokens": [51554, 467, 311, 257, 23331, 5017, 13, 51630], "temperature": 0.0, "avg_logprob": -0.1999518637563668, "compression_ratio": 1.5137254901960784, "no_speech_prob": 0.0014249561354517937}, {"id": 309, "seek": 144484, "start": 1444.84, "end": 1447.56, "text": " You can just like engage with us and report bugs.", "tokens": [50364, 509, 393, 445, 411, 4683, 365, 505, 293, 2275, 15120, 13, 50500], "temperature": 0.0, "avg_logprob": -0.18481010768724523, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.017775006592273712}, {"id": 310, "seek": 144484, "start": 1447.56, "end": 1453.6799999999998, "text": " And we can try to help you to profile and optimize your Python workloads.", "tokens": [50500, 400, 321, 393, 853, 281, 854, 291, 281, 7964, 293, 19719, 428, 15329, 32452, 13, 50806], "temperature": 0.0, "avg_logprob": -0.18481010768724523, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.017775006592273712}, {"id": 311, "seek": 144484, "start": 1453.6799999999998, "end": 1458.1999999999998, "text": " All the things that you see here, there's also a blog post if you want to catch up.", "tokens": [50806, 1057, 264, 721, 300, 291, 536, 510, 11, 456, 311, 611, 257, 6968, 2183, 498, 291, 528, 281, 3745, 493, 13, 51032], "temperature": 0.0, "avg_logprob": -0.18481010768724523, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.017775006592273712}, {"id": 312, "seek": 144484, "start": 1458.1999999999998, "end": 1460.76, "text": " You can check the company's blog post.", "tokens": [51032, 509, 393, 1520, 264, 2237, 311, 6968, 2183, 13, 51160], "temperature": 0.0, "avg_logprob": -0.18481010768724523, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.017775006592273712}, {"id": 313, "seek": 144484, "start": 1460.76, "end": 1466.4399999999998, "text": " And also we find the DevoreFind on winding bits like super cool because it's a niche thing that we do.", "tokens": [51160, 400, 611, 321, 915, 264, 9096, 418, 37, 471, 322, 29775, 9239, 411, 1687, 1627, 570, 309, 311, 257, 19956, 551, 300, 321, 360, 13, 51444], "temperature": 0.0, "avg_logprob": -0.18481010768724523, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.017775006592273712}, {"id": 314, "seek": 144484, "start": 1466.4399999999998, "end": 1473.24, "text": " And if you especially like the, if you have an application that follows everything to the native code,", "tokens": [51444, 400, 498, 291, 2318, 411, 264, 11, 498, 291, 362, 364, 3861, 300, 10002, 1203, 281, 264, 8470, 3089, 11, 51784], "temperature": 0.0, "avg_logprob": -0.18481010768724523, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.017775006592273712}, {"id": 315, "seek": 147324, "start": 1473.24, "end": 1475.1200000000001, "text": " this will be super useful.", "tokens": [50364, 341, 486, 312, 1687, 4420, 13, 50458], "temperature": 0.0, "avg_logprob": -0.32895792447603667, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.019193625077605247}, {"id": 316, "seek": 147324, "start": 1475.1200000000001, "end": 1477.0, "text": " Thank you for listening.", "tokens": [50458, 1044, 291, 337, 4764, 13, 50552], "temperature": 0.0, "avg_logprob": -0.32895792447603667, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.019193625077605247}, {"id": 317, "seek": 147324, "start": 1477.0, "end": 1485.88, "text": " Thank you very much, Camille.", "tokens": [50552, 1044, 291, 588, 709, 11, 6886, 3409, 13, 50996], "temperature": 0.0, "avg_logprob": -0.32895792447603667, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.019193625077605247}, {"id": 318, "seek": 147324, "start": 1485.88, "end": 1495.04, "text": " I'm afraid we're quite tight on the schedule, but please feel free to reach out to him with any questions.", "tokens": [50996, 286, 478, 4638, 321, 434, 1596, 4524, 322, 264, 7567, 11, 457, 1767, 841, 1737, 281, 2524, 484, 281, 796, 365, 604, 1651, 13, 51454], "temperature": 0.0, "avg_logprob": -0.32895792447603667, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.019193625077605247}, {"id": 319, "seek": 147324, "start": 1495.04, "end": 1496.2, "text": " And thank you very much.", "tokens": [51454, 400, 1309, 291, 588, 709, 13, 51512], "temperature": 0.0, "avg_logprob": -0.32895792447603667, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.019193625077605247}, {"id": 320, "seek": 147324, "start": 1496.2, "end": 1497.2, "text": " Thank you.", "tokens": [51512, 1044, 291, 13, 51562], "temperature": 0.0, "avg_logprob": -0.32895792447603667, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.019193625077605247}], "language": "en"}