{"text": " So we're going to have a nice little open discussion of AI and machine learning for the next 30 minutes. Jeremy, where are you? Here you are. And Jeremy is going to be chairing it for us. Take it away, Jeremy. Okay, ladies and gentlemen, we thought it would be good to fire up some Q&A. We're slightly suffering from, I think we're still down to one microphone. So I'm going to get a lot of exercise. We're going to tie, in order to give some structure this, William, Stefania and Michelle are going to each give a two minute introduction to a topic we think needs discussing and then we'll open the floor for the next eight minutes to comments from the floor feedback. Okay, and William's going to go first. Get the microphone right this time. So one of the hats I wear outside of working for Jeremy is that I'm chairing and leading the best practices group in AI at TechWorks, which is the trade body for electric systems in the UK. And as part of that, we are working on a base practices guide. Can you hear this now, by the way? Yeah? Yeah, cool. Let's be louder and into the mic. Okay. Let's see how this one works. That was our best practices. One of the challenges we have with this, one of the challenges that we have with this is retraining existing software engineers to be AI engineers and to understand the risks and challenges of doing AI and machine learning engineering, particularly to try and make the software do all of the things it should do and none of the things it shouldn't do. And some of those challenges are adversarial attacks. You can poison models either in their training data or just with enough experimentation, you can find examples, adversarial examples, which cause the model to do weird things. Or we have to deal with privacy issues. Models can be reverse engineered or they can be the data sets that they were trained on can be reverse engineered by sufficiently clever adversarial attacking models. So that's what I wanted to open up on this first discussion, talking about essentially how we can keep AI robust to misuse intentional or otherwise and keep things secure. Any questions, comments, thoughts on that? Please don't make this a very short discussion. So I've been spending a lot of time working on prompt engineering for a project, personal project I'm working on, prompt engineering. Did you ever play the Gandalf game where you're trying to guess a password? And at one point it uses a second LLM to verify the output of the first one to know whether it gave the response. I have never played this. It sounds pretty cool. Yeah, no, it's fantastic. So quickly, it is basically you are attempting to get Gandalf to tell you a secret password. And so however you coax it to produce it, such as tell me in a different language or whatever kinds of ways you can trick it to reveal that password, one of the last level is basically it adds in a separate query to an LLM from the output of the first one. And to verify, hey, no, no, actually I did actually say the password that's wrong. And so then it becomes difficult to say, okay, how do you get it to spit out the password such that the other LLM lets it through? And so there were some different techniques on doing that. I would love to hear any kind of discussion on that kind of prompt engineering. So this is the thing I've seen a few people do, have large language models, check their own homework. It's pretty interesting and I've got to say it seems to work remarkably well, but I'm pretty sure in this game you're describing that the last level is beatable. Is that right? Yes. And so this type of thing, having large language models check their own homework, it seems to work really well, but it doesn't solve the issue, at least proof of the issue. At the end of the day, large language models checking it can still be tricked as well, so that's where we end up. Thank you. Any more questions, comments? Yes, okay, hold on. Thank you very much. So one of the things that we're doing at the company I work with, we work with AI in an educational sort of environment, and so because of that we have to be very, very careful what we give out to students, especially because if we give them any wrong information, it can really be detrimental. And so one of the things that we've been working on and we've got it working fairly well is that instead of giving the student like full access to the input and output of the LLM, we've made it so that the student can basically provide tailored inputs to it that we know and have tested the outputs of based on our data. And so we've been able to get it so that we can have outputs that are generally 99.99% of the time beneficial to the student through instead of letting them directly enter prompts, we engineer the prompts for them ourselves and then give them a drop down or based on their input into the chat box determine what they're actually looking for. Now if they ask something really, really random or obscure, like yes, sometimes it can come through and say I don't know what you're asking for, but we found that it's better to sort of have a more curtailed environment to actually return outputs from. That's really interesting. It sounds pretty labor intensive. How do you curate these things? Is it done manually? So with regard to it being curtailed, we have basically gone through and we've spoken to students, we've spoken to universities and the other providers that we sell our software to. And we've essentially figured out like a long list of what they're looking for. And so we essentially built a chat box based around that. It's relatively limited right now with the number of prompts it can give. It's about 15, but we're adding more as time goes on. So it is intensive in building it, but the end sort of goal that it improves trustworthiness and robustness in our system, which is really important for our clients, is like it's worth it. That's really cool. So this is a chat box you've built yourself and is this like the chat box you've built, not a large language model, it just is a curation of a large language model? Yeah, it's not. The chat box itself, I didn't actually build it myself. Someone else on the team built it, but yeah, I believe it uses either a very basic LLM or it's just purely statistical. So something I was going to ask is if this was using a large language model and you were restricting the input and output, I think a really clever student could play around with the order you ask things and probably still get bad things to come out, which is, which would be, I mean, it would be bad, but it would be interesting. It would be interesting. Yeah. There's something that we're continually testing. But yeah, we, as the software works, it will only allow prompts to go to the actual LLM through one of the channels that we've laid out for it through the prompts. And so they might be able to be clever and get it to return a wrong prompt, but it still wouldn't return like things that are detrimental per se. That's interesting. Thank you. Okay. Thank you for that. Stefania, would you like to introduce your topic for discussion? Hello. Can you hear me well from outside? So my topic of discussion is the important to share your projects and to get exposed to conference from a national and community level as well and the local community as well. So to give you some inputs, I personally came from physics, decided to go in data science. And my very first exposure was Picon Italia seven years ago. How many of you actually programming Python? Cool. How many of you have been in Italy? So if you want to join actually on 22 to 25 of May, there will be Picon Italia. And personally, when I joined seven years ago, I was a volunteer presenting speakers. And at one point that I was studying a lot, I was saying, okay, I can do that. And that gave me a big push in applying to data science job and I've been teaching Python data science for a long time. And so I encouraged you to go to national conferences, but also on the local point of view. So for example, I'm based in Turin, Northwest Italy. And a great example was for example, Shishari Kat, an assistant. We are by, if I don't remember wrong, Piero Stefani and Savastani. And there was a contributor, Alessandro Spallina, that came to give his first talk with a demo inside a Python Turin on my city. So it was very interesting to see how from one person showing up a project can inspire others and also collect more and more volunteers to their own project. And to add on that also, I want to also inspire you to be networking across different communities. For example, another example of an event was working with OpenStreetMap data and collaborating with Wikimedia Italia in order to make this happen. So I want to inspire you to enhance your local community, especially also to give opportunities to students to showcase your product and your project in machine learning. And what I saw from my side is when you're able to create a space for people to network and to collaborate, it's also easy to study, understand better complex knowledge and collaborate together. So I'm very open to help you with your local community or national chapter and hoping for questions, even if you want to share your own experience in starting or being inspired by local events. Okay, thank you, Stefania. Any questions, comments or anyone wish to take up the offer of help from Stefania? Hands, come on. Yep. Help me hearing because I don't hear you. Hello. So thanks for the interview. I was kind of worried because I'm Belgian and I think there is a lack of student community in tech, especially informatics. I was a leader of a Google group last year for students and it's kind of hard to attract students to learn AI a part of their course. So I kind of agree with your view to improve this communication, to attract some people and motivate them to participate into some conference like that and other conference and other hackathon or something like in Italy, for example. And so I would really like to see more group like that, like you told and I think it's really important because some people don't even think that, some people think that I have to be, to study AI, to do AI and to be specializing in this thing, to find a job in this thing. And when I speak with recruiter and something, they have an opposite discourse. So I can agree with your point of view and I can't, like, no, I want to motivate people to join group, to create group because for example the GDSC, Google group, we are the first one was in Mons, in Belgium, a second in Liege, but for foreign students. And in Belgium it's really hard to create those groups and I think schools should help students to create that and to entertain the culture about techs and AI, etc. And I could wish to, with older societies or older more professional group to maybe ask to study, like university for example, to help those groups to preferrate and multiple and to attract more and more people like that. So thank you. Yes, thank you for the input and I have a lot of tips to give you that maybe can help other people too. So first of all, also encourage students, volunteer for conference, for example, an example of Picon Italia is to get also students from Florence, from high school and university to get and help. So it's very important, I'm also leading an nonprofit association, so it's very challenging to get that volunteer work. So if you are the organizer and what I encourage all of you is to not wait for something to happen, just make it happen, just create once. And the first thing that I will do is also to contact perhaps speakers from other university. For example, in Italy we have a tour in Milan that are quite close to each other. Milan is doing the first pay data, that's another great network of events, especially for students, have something very clear and attractive to them. And I have to be honest, sometimes also free food helps. For beers, for example, there is a format called Data beers, I don't know if you ever heard about it. It started from data scientists in Spain and there is a beer estrella that kindly sponsored beers. And the format is a free lighting talks. So we started to do it more and more in Italy, there is different city that doing it and also across Europe. And that could be an extra push to make people come to events and then really network to each other. So my suggestion is if you can get a professor on board, that's great. But then also another last tip that can give you is the last event that we did was in collaboration with a high school because sometimes it's very challenging to find spaces. So for example, you can look out the different community, even a Linux group that can give also particular topics in machine learning and say, okay, I'm in contact with the professor that can offer the space to host an open source community here. And then the students will start to know about that and start to get around and talk about that and from there can start another group. And there are a few tips, but feel free to contact me if you need more tips on that because it's very good to also share your experience so all the local groups and national community can also learn from each other. Out. So I was the one of the main organizers of the University of Birmingham Computer Science Society for several years. And so my tip about making a community is definitely focus on some organic kind of community growth of pizza and beer and interesting talks. And then from that, see if you can, once you've got enough of a group of people, you can then approach companies for sponsorship, which will only make it bigger and bigger because people are desperate, or companies are desperate for computer science graduates. You've got a genuine currency there that companies will definitely respond to. So I think build organic growth to begin with and then you can really get much more money and greater support by utilizing companies like that. Yes, I do agree with that. And also there's someone that is waving in here, you can get it. And also in terms of workshop, I do agree in getting involved also with companies, local companies, and in terms of kind of events, it could be more social events or more workshop events. For example, for the one about special data, it was very, very hardcore workshop with not both shared and then discussion of it. And he could be also hack and tell with someone is coding and other people are watching it and asking questions along the way. So hello, thank you for mentioning the cat. I am the guy that started it. So thank you. I want to get a little political if it is permitted. So I think in this place, it's really important to, and I ask you a comment on this, to focus on open source and standards because they go hand in hand. And it's a way to invite our territory in Europe to build without falling into the fun boy style around open AI, around US services. So it's time to build in open and create standards, not only the laws, because we are good at laws and standards and open source. It's the only way we can build our own AI economy. Thank you. Thank you. Thank you. Yeah, I do do agree completely. Actually, maybe he knows that I give some talks last year by potentially about a safety but later on at two 15 we are going to talk about exactly that. So standards AI governments. So if you want to stick around, there will be a wonderful panel to talk about that. And I completely agree that we need that more centralization and more discussion, even an event in university. And not only computer science, also policymakers need to have more and more exposure in the past. I worked with the open data and we have policymakers ask us, can you please explain that the science because we have to make the law. And yeah, so communication between the two sector are extremely important. Thank you for the inputs. Can I just ask a question? I was lovely to hear the discussion about graduates and everything. What's your view on how you bring forward the older engineer, of which I'm a representative member who's been working in software engineering for a long time. How do you bring them into the AI world? Well, I think there will be a space for everyone in terms of, well, I'm from the data science part of you and recently more about handling their risk of AI. So I think there will be a great space to understand how to make it more accessible to everyone. And also, I don't know your specific, because from UX to development to understanding more also cross part about languages, for example, so it came to my mind, for example, Rust became more and more popular in data science from pollers. This is a library in in used in data science through a Python binding as well. So it's very important. And let me know if I could request it as well to have some of the tools to have an understanding of different ways of approaching in this case, data science for different technology from especially now that we need more power, recorrosy, more paradigms that sometimes is less used in a high level program languages. So I think there's space to collaborate in that as well. Yes, so this work I've been looking at with best practices and I for tech works is actually quite a lot about this. And it's really interesting how different it is trying to write a guide or a course for somebody who already knows a lot of computer science paradigms. There's so much you can skip. But there's so many things you have to also be careful to not skip because you need to save them again or reemphasize things. For example, source control is one you need to think about not just source control for your data, sorry, source control for your code, you need to think about source control for your data to make sure your whole pipeline doesn't get messed up. And it's just very interesting. Might be some interesting things to talk about that by the way. It's so nice to be at FOSDOM. I haven't been here for years. So in 2017 I was doing repotential build, neurodebian, any other neuroscientists? No. Yeah, neurodebian. Yeah, good. Sorry, I could not find this building so I only saw the last slide. But so in 2017, a group of computational neuroscientists led by me put together a fully open and available set of training. And it had to be asynchronous as well, right? Code has to be fully available. The classes have to be pre-recorded so that anyone anywhere on earth can access it at any time. That is a extended definition of open and you have to create that. So we trot them everything up from reinforcement learning agents to GANs in 2017, right? And I think this space is under exploited in open education, right? And if you take this, what we did was say here's the criteria to create reproducible science. Here are the skills that you're going to learn. Go out and find and validate an open data set. And if you do all of that because of reproducible science, you in advance have a pre-registered paper. You will produce a peer-reviewed paper. In this space, concerningly, a lot of people, if they get to masters and PhD, have done at this time a lot of rote education and no critical thinking or no extending into new knowledge. But the second you tell them, these are tools. And they're tools to solving new problem. And if you solve that problem in science, right? In science, it's the only place where you say, I solved a problem, it's peer-reviewed, it's open and public. If you pre-register it, you're perfect. If you can motivate them with something real, it will be applied. And I just want that to be much, much more in the forefront of people's minds as they communicate. Okay. We've only got a couple more minutes. Will, would you and Stephanie just like to wind up the discussion and then we'll hand back to JJ to run the next talk. Just a quick comment on what you just said. It was always something we struggled with, or I struggled with when I was a neuroscience researcher, because there's quite a lot of not quite so good neuroscience researchers who don't do what they should and pre-register experiments. And they just sort of make it up as they go along and that causes a lot of problems. And I really like this idea of pre-register everything first and then when everything's pre-registered, then do it and then publish it no matter what. I think it's quite important. Yes. And do you want to just wrap up your bit on education? Sorry. Just quickly wrap up your bit on education. Okay. I think we're, okay. I think we're done. Okay. We're done. Thank you very much indeed. Bye. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.4, "text": " So we're going to have a nice little open discussion of AI and machine learning for the next 30", "tokens": [50364, 407, 321, 434, 516, 281, 362, 257, 1481, 707, 1269, 5017, 295, 7318, 293, 3479, 2539, 337, 264, 958, 2217, 51184], "temperature": 0.0, "avg_logprob": -0.19891457259655, "compression_ratio": 1.375, "no_speech_prob": 0.2800966799259186}, {"id": 1, "seek": 0, "start": 16.4, "end": 17.400000000000002, "text": " minutes.", "tokens": [51184, 2077, 13, 51234], "temperature": 0.0, "avg_logprob": -0.19891457259655, "compression_ratio": 1.375, "no_speech_prob": 0.2800966799259186}, {"id": 2, "seek": 0, "start": 17.400000000000002, "end": 18.400000000000002, "text": " Jeremy, where are you?", "tokens": [51234, 17809, 11, 689, 366, 291, 30, 51284], "temperature": 0.0, "avg_logprob": -0.19891457259655, "compression_ratio": 1.375, "no_speech_prob": 0.2800966799259186}, {"id": 3, "seek": 0, "start": 18.400000000000002, "end": 19.400000000000002, "text": " Here you are.", "tokens": [51284, 1692, 291, 366, 13, 51334], "temperature": 0.0, "avg_logprob": -0.19891457259655, "compression_ratio": 1.375, "no_speech_prob": 0.2800966799259186}, {"id": 4, "seek": 0, "start": 19.400000000000002, "end": 20.400000000000002, "text": " And Jeremy is going to be chairing it for us.", "tokens": [51334, 400, 17809, 307, 516, 281, 312, 6090, 278, 309, 337, 505, 13, 51384], "temperature": 0.0, "avg_logprob": -0.19891457259655, "compression_ratio": 1.375, "no_speech_prob": 0.2800966799259186}, {"id": 5, "seek": 0, "start": 20.400000000000002, "end": 21.400000000000002, "text": " Take it away, Jeremy.", "tokens": [51384, 3664, 309, 1314, 11, 17809, 13, 51434], "temperature": 0.0, "avg_logprob": -0.19891457259655, "compression_ratio": 1.375, "no_speech_prob": 0.2800966799259186}, {"id": 6, "seek": 2140, "start": 22.4, "end": 33.6, "text": " Okay, ladies and gentlemen, we thought it would be good to fire up some Q&A.", "tokens": [50414, 1033, 11, 9974, 293, 11669, 11, 321, 1194, 309, 576, 312, 665, 281, 2610, 493, 512, 1249, 5, 32, 13, 50974], "temperature": 0.0, "avg_logprob": -0.29507946658444095, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.06894771754741669}, {"id": 7, "seek": 2140, "start": 33.6, "end": 39.64, "text": " We're slightly suffering from, I think we're still down to one microphone.", "tokens": [50974, 492, 434, 4748, 7755, 490, 11, 286, 519, 321, 434, 920, 760, 281, 472, 10952, 13, 51276], "temperature": 0.0, "avg_logprob": -0.29507946658444095, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.06894771754741669}, {"id": 8, "seek": 2140, "start": 39.64, "end": 42.519999999999996, "text": " So I'm going to get a lot of exercise.", "tokens": [51276, 407, 286, 478, 516, 281, 483, 257, 688, 295, 5380, 13, 51420], "temperature": 0.0, "avg_logprob": -0.29507946658444095, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.06894771754741669}, {"id": 9, "seek": 2140, "start": 42.519999999999996, "end": 51.12, "text": " We're going to tie, in order to give some structure this, William, Stefania and Michelle", "tokens": [51420, 492, 434, 516, 281, 7582, 11, 294, 1668, 281, 976, 512, 3877, 341, 11, 6740, 11, 43421, 5609, 293, 14933, 51850], "temperature": 0.0, "avg_logprob": -0.29507946658444095, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.06894771754741669}, {"id": 10, "seek": 5112, "start": 51.12, "end": 58.839999999999996, "text": " are going to each give a two minute introduction to a topic we think needs discussing and then", "tokens": [50364, 366, 516, 281, 1184, 976, 257, 732, 3456, 9339, 281, 257, 4829, 321, 519, 2203, 10850, 293, 550, 50750], "temperature": 0.0, "avg_logprob": -0.22730098451886857, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.011655646376311779}, {"id": 11, "seek": 5112, "start": 58.839999999999996, "end": 64.88, "text": " we'll open the floor for the next eight minutes to comments from the floor feedback.", "tokens": [50750, 321, 603, 1269, 264, 4123, 337, 264, 958, 3180, 2077, 281, 3053, 490, 264, 4123, 5824, 13, 51052], "temperature": 0.0, "avg_logprob": -0.22730098451886857, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.011655646376311779}, {"id": 12, "seek": 5112, "start": 64.88, "end": 67.24, "text": " Okay, and William's going to go first.", "tokens": [51052, 1033, 11, 293, 6740, 311, 516, 281, 352, 700, 13, 51170], "temperature": 0.0, "avg_logprob": -0.22730098451886857, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.011655646376311779}, {"id": 13, "seek": 5112, "start": 67.24, "end": 72.72, "text": " Get the microphone right this time.", "tokens": [51170, 3240, 264, 10952, 558, 341, 565, 13, 51444], "temperature": 0.0, "avg_logprob": -0.22730098451886857, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.011655646376311779}, {"id": 14, "seek": 5112, "start": 72.72, "end": 80.12, "text": " So one of the hats I wear outside of working for Jeremy is that I'm chairing and leading", "tokens": [51444, 407, 472, 295, 264, 20549, 286, 3728, 2380, 295, 1364, 337, 17809, 307, 300, 286, 478, 6090, 278, 293, 5775, 51814], "temperature": 0.0, "avg_logprob": -0.22730098451886857, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.011655646376311779}, {"id": 15, "seek": 8012, "start": 80.12, "end": 85.92, "text": " the best practices group in AI at TechWorks, which is the trade body for electric systems", "tokens": [50364, 264, 1151, 7525, 1594, 294, 7318, 412, 13795, 28846, 82, 11, 597, 307, 264, 4923, 1772, 337, 5210, 3652, 50654], "temperature": 0.0, "avg_logprob": -0.33434139389589607, "compression_ratio": 1.3763440860215055, "no_speech_prob": 0.07286861538887024}, {"id": 16, "seek": 8012, "start": 85.92, "end": 86.92, "text": " in the UK.", "tokens": [50654, 294, 264, 7051, 13, 50704], "temperature": 0.0, "avg_logprob": -0.33434139389589607, "compression_ratio": 1.3763440860215055, "no_speech_prob": 0.07286861538887024}, {"id": 17, "seek": 8012, "start": 86.92, "end": 92.08000000000001, "text": " And as part of that, we are working on a base practices guide.", "tokens": [50704, 400, 382, 644, 295, 300, 11, 321, 366, 1364, 322, 257, 3096, 7525, 5934, 13, 50962], "temperature": 0.0, "avg_logprob": -0.33434139389589607, "compression_ratio": 1.3763440860215055, "no_speech_prob": 0.07286861538887024}, {"id": 18, "seek": 8012, "start": 92.08000000000001, "end": 94.08000000000001, "text": " Can you hear this now, by the way?", "tokens": [50962, 1664, 291, 1568, 341, 586, 11, 538, 264, 636, 30, 51062], "temperature": 0.0, "avg_logprob": -0.33434139389589607, "compression_ratio": 1.3763440860215055, "no_speech_prob": 0.07286861538887024}, {"id": 19, "seek": 8012, "start": 94.08000000000001, "end": 95.08000000000001, "text": " Yeah?", "tokens": [51062, 865, 30, 51112], "temperature": 0.0, "avg_logprob": -0.33434139389589607, "compression_ratio": 1.3763440860215055, "no_speech_prob": 0.07286861538887024}, {"id": 20, "seek": 8012, "start": 95.08000000000001, "end": 96.08000000000001, "text": " Yeah, cool.", "tokens": [51112, 865, 11, 1627, 13, 51162], "temperature": 0.0, "avg_logprob": -0.33434139389589607, "compression_ratio": 1.3763440860215055, "no_speech_prob": 0.07286861538887024}, {"id": 21, "seek": 8012, "start": 96.08000000000001, "end": 98.08000000000001, "text": " Let's be louder and into the mic.", "tokens": [51162, 961, 311, 312, 22717, 293, 666, 264, 3123, 13, 51262], "temperature": 0.0, "avg_logprob": -0.33434139389589607, "compression_ratio": 1.3763440860215055, "no_speech_prob": 0.07286861538887024}, {"id": 22, "seek": 8012, "start": 98.08000000000001, "end": 99.08000000000001, "text": " Okay.", "tokens": [51262, 1033, 13, 51312], "temperature": 0.0, "avg_logprob": -0.33434139389589607, "compression_ratio": 1.3763440860215055, "no_speech_prob": 0.07286861538887024}, {"id": 23, "seek": 9908, "start": 99.08, "end": 109.08, "text": " Let's see how this one works.", "tokens": [50364, 961, 311, 536, 577, 341, 472, 1985, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3455514250130489, "compression_ratio": 1.6013513513513513, "no_speech_prob": 0.14736171066761017}, {"id": 24, "seek": 9908, "start": 109.08, "end": 112.08, "text": " That was our best practices.", "tokens": [50864, 663, 390, 527, 1151, 7525, 13, 51014], "temperature": 0.0, "avg_logprob": -0.3455514250130489, "compression_ratio": 1.6013513513513513, "no_speech_prob": 0.14736171066761017}, {"id": 25, "seek": 9908, "start": 112.08, "end": 121.48, "text": " One of the challenges we have with this, one of the challenges that we have with this is", "tokens": [51014, 1485, 295, 264, 4759, 321, 362, 365, 341, 11, 472, 295, 264, 4759, 300, 321, 362, 365, 341, 307, 51484], "temperature": 0.0, "avg_logprob": -0.3455514250130489, "compression_ratio": 1.6013513513513513, "no_speech_prob": 0.14736171066761017}, {"id": 26, "seek": 9908, "start": 121.48, "end": 127.48, "text": " retraining existing software engineers to be AI engineers and to understand the risks and", "tokens": [51484, 49356, 1760, 6741, 4722, 11955, 281, 312, 7318, 11955, 293, 281, 1223, 264, 10888, 293, 51784], "temperature": 0.0, "avg_logprob": -0.3455514250130489, "compression_ratio": 1.6013513513513513, "no_speech_prob": 0.14736171066761017}, {"id": 27, "seek": 12748, "start": 127.48, "end": 133.08, "text": " challenges of doing AI and machine learning engineering, particularly to try and make", "tokens": [50364, 4759, 295, 884, 7318, 293, 3479, 2539, 7043, 11, 4098, 281, 853, 293, 652, 50644], "temperature": 0.0, "avg_logprob": -0.2963461500875066, "compression_ratio": 1.7253218884120172, "no_speech_prob": 0.38726091384887695}, {"id": 28, "seek": 12748, "start": 133.08, "end": 137.48000000000002, "text": " the software do all of the things it should do and none of the things it shouldn't do.", "tokens": [50644, 264, 4722, 360, 439, 295, 264, 721, 309, 820, 360, 293, 6022, 295, 264, 721, 309, 4659, 380, 360, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2963461500875066, "compression_ratio": 1.7253218884120172, "no_speech_prob": 0.38726091384887695}, {"id": 29, "seek": 12748, "start": 137.48000000000002, "end": 141.28, "text": " And some of those challenges are adversarial attacks.", "tokens": [50864, 400, 512, 295, 729, 4759, 366, 17641, 44745, 8122, 13, 51054], "temperature": 0.0, "avg_logprob": -0.2963461500875066, "compression_ratio": 1.7253218884120172, "no_speech_prob": 0.38726091384887695}, {"id": 30, "seek": 12748, "start": 141.28, "end": 146.28, "text": " You can poison models either in their training data or just with enough experimentation, you", "tokens": [51054, 509, 393, 10836, 5245, 2139, 294, 641, 3097, 1412, 420, 445, 365, 1547, 37142, 11, 291, 51304], "temperature": 0.0, "avg_logprob": -0.2963461500875066, "compression_ratio": 1.7253218884120172, "no_speech_prob": 0.38726091384887695}, {"id": 31, "seek": 12748, "start": 146.28, "end": 153.48000000000002, "text": " can find examples, adversarial examples, which cause the model to do weird things.", "tokens": [51304, 393, 915, 5110, 11, 17641, 44745, 5110, 11, 597, 3082, 264, 2316, 281, 360, 3657, 721, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2963461500875066, "compression_ratio": 1.7253218884120172, "no_speech_prob": 0.38726091384887695}, {"id": 32, "seek": 15348, "start": 153.48, "end": 156.48, "text": " Or we have to deal with privacy issues.", "tokens": [50364, 1610, 321, 362, 281, 2028, 365, 11427, 2663, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16559713085492453, "compression_ratio": 1.6254826254826256, "no_speech_prob": 0.16272523999214172}, {"id": 33, "seek": 15348, "start": 156.48, "end": 162.48, "text": " Models can be reverse engineered or they can be the data sets that they were trained on", "tokens": [50514, 6583, 1625, 393, 312, 9943, 38648, 420, 436, 393, 312, 264, 1412, 6352, 300, 436, 645, 8895, 322, 50814], "temperature": 0.0, "avg_logprob": -0.16559713085492453, "compression_ratio": 1.6254826254826256, "no_speech_prob": 0.16272523999214172}, {"id": 34, "seek": 15348, "start": 162.48, "end": 167.48, "text": " can be reverse engineered by sufficiently clever adversarial attacking models.", "tokens": [50814, 393, 312, 9943, 38648, 538, 31868, 13494, 17641, 44745, 15010, 5245, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16559713085492453, "compression_ratio": 1.6254826254826256, "no_speech_prob": 0.16272523999214172}, {"id": 35, "seek": 15348, "start": 167.48, "end": 171.48, "text": " So that's what I wanted to open up on this first discussion, talking about essentially", "tokens": [51064, 407, 300, 311, 437, 286, 1415, 281, 1269, 493, 322, 341, 700, 5017, 11, 1417, 466, 4476, 51264], "temperature": 0.0, "avg_logprob": -0.16559713085492453, "compression_ratio": 1.6254826254826256, "no_speech_prob": 0.16272523999214172}, {"id": 36, "seek": 15348, "start": 171.48, "end": 180.48, "text": " how we can keep AI robust to misuse intentional or otherwise and keep things secure.", "tokens": [51264, 577, 321, 393, 1066, 7318, 13956, 281, 3346, 438, 21935, 420, 5911, 293, 1066, 721, 7144, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16559713085492453, "compression_ratio": 1.6254826254826256, "no_speech_prob": 0.16272523999214172}, {"id": 37, "seek": 15348, "start": 180.48, "end": 182.48, "text": " Any questions, comments, thoughts on that?", "tokens": [51714, 2639, 1651, 11, 3053, 11, 4598, 322, 300, 30, 51814], "temperature": 0.0, "avg_logprob": -0.16559713085492453, "compression_ratio": 1.6254826254826256, "no_speech_prob": 0.16272523999214172}, {"id": 38, "seek": 18248, "start": 183.48, "end": 186.48, "text": " Please don't make this a very short discussion.", "tokens": [50414, 2555, 500, 380, 652, 341, 257, 588, 2099, 5017, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1669105953640408, "compression_ratio": 1.4365079365079365, "no_speech_prob": 0.042409129440784454}, {"id": 39, "seek": 18248, "start": 196.48, "end": 201.48, "text": " So I've been spending a lot of time working on prompt engineering for a project, personal", "tokens": [51064, 407, 286, 600, 668, 6434, 257, 688, 295, 565, 1364, 322, 12391, 7043, 337, 257, 1716, 11, 2973, 51314], "temperature": 0.0, "avg_logprob": -0.1669105953640408, "compression_ratio": 1.4365079365079365, "no_speech_prob": 0.042409129440784454}, {"id": 40, "seek": 18248, "start": 201.48, "end": 206.48, "text": " project I'm working on, prompt engineering.", "tokens": [51314, 1716, 286, 478, 1364, 322, 11, 12391, 7043, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1669105953640408, "compression_ratio": 1.4365079365079365, "no_speech_prob": 0.042409129440784454}, {"id": 41, "seek": 20648, "start": 207.48, "end": 213.48, "text": " Did you ever play the Gandalf game where you're trying to guess a password?", "tokens": [50414, 2589, 291, 1562, 862, 264, 23962, 1678, 1216, 689, 291, 434, 1382, 281, 2041, 257, 11524, 30, 50714], "temperature": 0.0, "avg_logprob": -0.1673551286969866, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.08079461008310318}, {"id": 42, "seek": 20648, "start": 213.48, "end": 222.48, "text": " And at one point it uses a second LLM to verify the output of the first one to know whether", "tokens": [50714, 400, 412, 472, 935, 309, 4960, 257, 1150, 441, 43, 44, 281, 16888, 264, 5598, 295, 264, 700, 472, 281, 458, 1968, 51164], "temperature": 0.0, "avg_logprob": -0.1673551286969866, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.08079461008310318}, {"id": 43, "seek": 20648, "start": 222.48, "end": 224.48, "text": " it gave the response.", "tokens": [51164, 309, 2729, 264, 4134, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1673551286969866, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.08079461008310318}, {"id": 44, "seek": 20648, "start": 225.48, "end": 227.48, "text": " I have never played this.", "tokens": [51314, 286, 362, 1128, 3737, 341, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1673551286969866, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.08079461008310318}, {"id": 45, "seek": 20648, "start": 227.48, "end": 229.48, "text": " It sounds pretty cool.", "tokens": [51414, 467, 3263, 1238, 1627, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1673551286969866, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.08079461008310318}, {"id": 46, "seek": 20648, "start": 229.48, "end": 230.48, "text": " Yeah, no, it's fantastic.", "tokens": [51514, 865, 11, 572, 11, 309, 311, 5456, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1673551286969866, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.08079461008310318}, {"id": 47, "seek": 23048, "start": 230.48, "end": 237.48, "text": " So quickly, it is basically you are attempting to get Gandalf to tell you a secret password.", "tokens": [50364, 407, 2661, 11, 309, 307, 1936, 291, 366, 22001, 281, 483, 23962, 1678, 281, 980, 291, 257, 4054, 11524, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14917879946091595, "compression_ratio": 1.5303867403314917, "no_speech_prob": 0.007857739925384521}, {"id": 48, "seek": 23048, "start": 237.48, "end": 246.48, "text": " And so however you coax it to produce it, such as tell me in a different language or whatever", "tokens": [50714, 400, 370, 4461, 291, 598, 2797, 309, 281, 5258, 309, 11, 1270, 382, 980, 385, 294, 257, 819, 2856, 420, 2035, 51164], "temperature": 0.0, "avg_logprob": -0.14917879946091595, "compression_ratio": 1.5303867403314917, "no_speech_prob": 0.007857739925384521}, {"id": 49, "seek": 23048, "start": 246.48, "end": 254.48, "text": " kinds of ways you can trick it to reveal that password, one of the last level is basically", "tokens": [51164, 3685, 295, 2098, 291, 393, 4282, 309, 281, 10658, 300, 11524, 11, 472, 295, 264, 1036, 1496, 307, 1936, 51564], "temperature": 0.0, "avg_logprob": -0.14917879946091595, "compression_ratio": 1.5303867403314917, "no_speech_prob": 0.007857739925384521}, {"id": 50, "seek": 25448, "start": 254.48, "end": 260.48, "text": " it adds in a separate query to an LLM from the output of the first one.", "tokens": [50364, 309, 10860, 294, 257, 4994, 14581, 281, 364, 441, 43, 44, 490, 264, 5598, 295, 264, 700, 472, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11095734646445826, "compression_ratio": 1.638095238095238, "no_speech_prob": 0.02135053090751171}, {"id": 51, "seek": 25448, "start": 260.48, "end": 266.48, "text": " And to verify, hey, no, no, actually I did actually say the password that's wrong.", "tokens": [50664, 400, 281, 16888, 11, 4177, 11, 572, 11, 572, 11, 767, 286, 630, 767, 584, 264, 11524, 300, 311, 2085, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11095734646445826, "compression_ratio": 1.638095238095238, "no_speech_prob": 0.02135053090751171}, {"id": 52, "seek": 25448, "start": 266.48, "end": 271.48, "text": " And so then it becomes difficult to say, okay, how do you get it to spit out the password", "tokens": [50964, 400, 370, 550, 309, 3643, 2252, 281, 584, 11, 1392, 11, 577, 360, 291, 483, 309, 281, 22127, 484, 264, 11524, 51214], "temperature": 0.0, "avg_logprob": -0.11095734646445826, "compression_ratio": 1.638095238095238, "no_speech_prob": 0.02135053090751171}, {"id": 53, "seek": 25448, "start": 271.48, "end": 274.48, "text": " such that the other LLM lets it through?", "tokens": [51214, 1270, 300, 264, 661, 441, 43, 44, 6653, 309, 807, 30, 51364], "temperature": 0.0, "avg_logprob": -0.11095734646445826, "compression_ratio": 1.638095238095238, "no_speech_prob": 0.02135053090751171}, {"id": 54, "seek": 25448, "start": 274.48, "end": 277.48, "text": " And so there were some different techniques on doing that.", "tokens": [51364, 400, 370, 456, 645, 512, 819, 7512, 322, 884, 300, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11095734646445826, "compression_ratio": 1.638095238095238, "no_speech_prob": 0.02135053090751171}, {"id": 55, "seek": 27748, "start": 277.48, "end": 283.48, "text": " I would love to hear any kind of discussion on that kind of prompt engineering.", "tokens": [50364, 286, 576, 959, 281, 1568, 604, 733, 295, 5017, 322, 300, 733, 295, 12391, 7043, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1498316385412729, "compression_ratio": 1.5565217391304347, "no_speech_prob": 0.005874865688383579}, {"id": 56, "seek": 27748, "start": 287.48, "end": 292.48, "text": " So this is the thing I've seen a few people do, have large language models, check their", "tokens": [50864, 407, 341, 307, 264, 551, 286, 600, 1612, 257, 1326, 561, 360, 11, 362, 2416, 2856, 5245, 11, 1520, 641, 51114], "temperature": 0.0, "avg_logprob": -0.1498316385412729, "compression_ratio": 1.5565217391304347, "no_speech_prob": 0.005874865688383579}, {"id": 57, "seek": 27748, "start": 292.48, "end": 293.48, "text": " own homework.", "tokens": [51114, 1065, 14578, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1498316385412729, "compression_ratio": 1.5565217391304347, "no_speech_prob": 0.005874865688383579}, {"id": 58, "seek": 27748, "start": 293.48, "end": 300.48, "text": " It's pretty interesting and I've got to say it seems to work remarkably well, but I'm", "tokens": [51164, 467, 311, 1238, 1880, 293, 286, 600, 658, 281, 584, 309, 2544, 281, 589, 37381, 731, 11, 457, 286, 478, 51514], "temperature": 0.0, "avg_logprob": -0.1498316385412729, "compression_ratio": 1.5565217391304347, "no_speech_prob": 0.005874865688383579}, {"id": 59, "seek": 27748, "start": 300.48, "end": 305.48, "text": " pretty sure in this game you're describing that the last level is beatable.", "tokens": [51514, 1238, 988, 294, 341, 1216, 291, 434, 16141, 300, 264, 1036, 1496, 307, 4224, 712, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1498316385412729, "compression_ratio": 1.5565217391304347, "no_speech_prob": 0.005874865688383579}, {"id": 60, "seek": 27748, "start": 305.48, "end": 306.48, "text": " Is that right?", "tokens": [51764, 1119, 300, 558, 30, 51814], "temperature": 0.0, "avg_logprob": -0.1498316385412729, "compression_ratio": 1.5565217391304347, "no_speech_prob": 0.005874865688383579}, {"id": 61, "seek": 30648, "start": 306.48, "end": 307.48, "text": " Yes.", "tokens": [50364, 1079, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1872331428527832, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.002400839002802968}, {"id": 62, "seek": 30648, "start": 307.48, "end": 312.48, "text": " And so this type of thing, having large language models check their own homework, it seems", "tokens": [50414, 400, 370, 341, 2010, 295, 551, 11, 1419, 2416, 2856, 5245, 1520, 641, 1065, 14578, 11, 309, 2544, 50664], "temperature": 0.0, "avg_logprob": -0.1872331428527832, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.002400839002802968}, {"id": 63, "seek": 30648, "start": 312.48, "end": 317.48, "text": " to work really well, but it doesn't solve the issue, at least proof of the issue.", "tokens": [50664, 281, 589, 534, 731, 11, 457, 309, 1177, 380, 5039, 264, 2734, 11, 412, 1935, 8177, 295, 264, 2734, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1872331428527832, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.002400839002802968}, {"id": 64, "seek": 30648, "start": 317.48, "end": 321.48, "text": " At the end of the day, large language models checking it can still be tricked as well,", "tokens": [50914, 1711, 264, 917, 295, 264, 786, 11, 2416, 2856, 5245, 8568, 309, 393, 920, 312, 39345, 382, 731, 11, 51114], "temperature": 0.0, "avg_logprob": -0.1872331428527832, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.002400839002802968}, {"id": 65, "seek": 30648, "start": 321.48, "end": 324.48, "text": " so that's where we end up.", "tokens": [51114, 370, 300, 311, 689, 321, 917, 493, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1872331428527832, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.002400839002802968}, {"id": 66, "seek": 30648, "start": 324.48, "end": 325.48, "text": " Thank you.", "tokens": [51264, 1044, 291, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1872331428527832, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.002400839002802968}, {"id": 67, "seek": 30648, "start": 325.48, "end": 327.48, "text": " Any more questions, comments?", "tokens": [51314, 2639, 544, 1651, 11, 3053, 30, 51414], "temperature": 0.0, "avg_logprob": -0.1872331428527832, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.002400839002802968}, {"id": 68, "seek": 30648, "start": 327.48, "end": 329.48, "text": " Yes, okay, hold on.", "tokens": [51414, 1079, 11, 1392, 11, 1797, 322, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1872331428527832, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.002400839002802968}, {"id": 69, "seek": 33648, "start": 336.48, "end": 342.48, "text": " Thank you very much.", "tokens": [50364, 1044, 291, 588, 709, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1316465377807617, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.005233608186244965}, {"id": 70, "seek": 33648, "start": 342.48, "end": 348.48, "text": " So one of the things that we're doing at the company I work with, we work with AI in", "tokens": [50664, 407, 472, 295, 264, 721, 300, 321, 434, 884, 412, 264, 2237, 286, 589, 365, 11, 321, 589, 365, 7318, 294, 50964], "temperature": 0.0, "avg_logprob": -0.1316465377807617, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.005233608186244965}, {"id": 71, "seek": 33648, "start": 348.48, "end": 353.48, "text": " an educational sort of environment, and so because of that we have to be very, very careful", "tokens": [50964, 364, 10189, 1333, 295, 2823, 11, 293, 370, 570, 295, 300, 321, 362, 281, 312, 588, 11, 588, 5026, 51214], "temperature": 0.0, "avg_logprob": -0.1316465377807617, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.005233608186244965}, {"id": 72, "seek": 33648, "start": 353.48, "end": 359.48, "text": " what we give out to students, especially because if we give them any wrong information, it", "tokens": [51214, 437, 321, 976, 484, 281, 1731, 11, 2318, 570, 498, 321, 976, 552, 604, 2085, 1589, 11, 309, 51514], "temperature": 0.0, "avg_logprob": -0.1316465377807617, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.005233608186244965}, {"id": 73, "seek": 33648, "start": 359.48, "end": 361.48, "text": " can really be detrimental.", "tokens": [51514, 393, 534, 312, 45694, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1316465377807617, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.005233608186244965}, {"id": 74, "seek": 36148, "start": 361.48, "end": 367.48, "text": " And so one of the things that we've been working on and we've got it working fairly well is", "tokens": [50364, 400, 370, 472, 295, 264, 721, 300, 321, 600, 668, 1364, 322, 293, 321, 600, 658, 309, 1364, 6457, 731, 307, 50664], "temperature": 0.0, "avg_logprob": -0.09238952856797439, "compression_ratio": 1.8008849557522124, "no_speech_prob": 0.19685406982898712}, {"id": 75, "seek": 36148, "start": 367.48, "end": 374.48, "text": " that instead of giving the student like full access to the input and output of the LLM,", "tokens": [50664, 300, 2602, 295, 2902, 264, 3107, 411, 1577, 2105, 281, 264, 4846, 293, 5598, 295, 264, 441, 43, 44, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09238952856797439, "compression_ratio": 1.8008849557522124, "no_speech_prob": 0.19685406982898712}, {"id": 76, "seek": 36148, "start": 374.48, "end": 380.48, "text": " we've made it so that the student can basically provide tailored inputs to it that we know", "tokens": [51014, 321, 600, 1027, 309, 370, 300, 264, 3107, 393, 1936, 2893, 34858, 15743, 281, 309, 300, 321, 458, 51314], "temperature": 0.0, "avg_logprob": -0.09238952856797439, "compression_ratio": 1.8008849557522124, "no_speech_prob": 0.19685406982898712}, {"id": 77, "seek": 36148, "start": 380.48, "end": 384.48, "text": " and have tested the outputs of based on our data.", "tokens": [51314, 293, 362, 8246, 264, 23930, 295, 2361, 322, 527, 1412, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09238952856797439, "compression_ratio": 1.8008849557522124, "no_speech_prob": 0.19685406982898712}, {"id": 78, "seek": 36148, "start": 384.48, "end": 390.48, "text": " And so we've been able to get it so that we can have outputs that are generally 99.99%", "tokens": [51514, 400, 370, 321, 600, 668, 1075, 281, 483, 309, 370, 300, 321, 393, 362, 23930, 300, 366, 5101, 11803, 13, 8494, 4, 51814], "temperature": 0.0, "avg_logprob": -0.09238952856797439, "compression_ratio": 1.8008849557522124, "no_speech_prob": 0.19685406982898712}, {"id": 79, "seek": 39048, "start": 390.48, "end": 396.48, "text": " of the time beneficial to the student through instead of letting them directly enter prompts,", "tokens": [50364, 295, 264, 565, 14072, 281, 264, 3107, 807, 2602, 295, 8295, 552, 3838, 3242, 41095, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10298273823048809, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.018138837069272995}, {"id": 80, "seek": 39048, "start": 396.48, "end": 401.48, "text": " we engineer the prompts for them ourselves and then give them a drop down or based on", "tokens": [50664, 321, 11403, 264, 41095, 337, 552, 4175, 293, 550, 976, 552, 257, 3270, 760, 420, 2361, 322, 50914], "temperature": 0.0, "avg_logprob": -0.10298273823048809, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.018138837069272995}, {"id": 81, "seek": 39048, "start": 401.48, "end": 406.48, "text": " their input into the chat box determine what they're actually looking for.", "tokens": [50914, 641, 4846, 666, 264, 5081, 2424, 6997, 437, 436, 434, 767, 1237, 337, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10298273823048809, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.018138837069272995}, {"id": 82, "seek": 39048, "start": 406.48, "end": 411.48, "text": " Now if they ask something really, really random or obscure, like yes, sometimes it can come", "tokens": [51164, 823, 498, 436, 1029, 746, 534, 11, 534, 4974, 420, 34443, 11, 411, 2086, 11, 2171, 309, 393, 808, 51414], "temperature": 0.0, "avg_logprob": -0.10298273823048809, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.018138837069272995}, {"id": 83, "seek": 39048, "start": 411.48, "end": 417.48, "text": " through and say I don't know what you're asking for, but we found that it's better to sort of", "tokens": [51414, 807, 293, 584, 286, 500, 380, 458, 437, 291, 434, 3365, 337, 11, 457, 321, 1352, 300, 309, 311, 1101, 281, 1333, 295, 51714], "temperature": 0.0, "avg_logprob": -0.10298273823048809, "compression_ratio": 1.673003802281369, "no_speech_prob": 0.018138837069272995}, {"id": 84, "seek": 41748, "start": 417.48, "end": 423.48, "text": " have a more curtailed environment to actually return outputs from.", "tokens": [50364, 362, 257, 544, 1262, 14430, 292, 2823, 281, 767, 2736, 23930, 490, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09292122335994944, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.013940544798970222}, {"id": 85, "seek": 41748, "start": 423.48, "end": 432.48, "text": " That's really interesting. It sounds pretty labor intensive. How do you curate these things? Is it done manually?", "tokens": [50664, 663, 311, 534, 1880, 13, 467, 3263, 1238, 5938, 18957, 13, 1012, 360, 291, 1262, 473, 613, 721, 30, 1119, 309, 1096, 16945, 30, 51114], "temperature": 0.0, "avg_logprob": -0.09292122335994944, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.013940544798970222}, {"id": 86, "seek": 41748, "start": 432.48, "end": 440.48, "text": " So with regard to it being curtailed, we have basically gone through and we've spoken to students,", "tokens": [51114, 407, 365, 3843, 281, 309, 885, 1262, 14430, 292, 11, 321, 362, 1936, 2780, 807, 293, 321, 600, 10759, 281, 1731, 11, 51514], "temperature": 0.0, "avg_logprob": -0.09292122335994944, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.013940544798970222}, {"id": 87, "seek": 41748, "start": 440.48, "end": 445.48, "text": " we've spoken to universities and the other providers that we sell our software to.", "tokens": [51514, 321, 600, 10759, 281, 11779, 293, 264, 661, 11330, 300, 321, 3607, 527, 4722, 281, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09292122335994944, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.013940544798970222}, {"id": 88, "seek": 44548, "start": 445.48, "end": 450.48, "text": " And we've essentially figured out like a long list of what they're looking for.", "tokens": [50364, 400, 321, 600, 4476, 8932, 484, 411, 257, 938, 1329, 295, 437, 436, 434, 1237, 337, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10312758926796702, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.008800255134701729}, {"id": 89, "seek": 44548, "start": 450.48, "end": 457.48, "text": " And so we essentially built a chat box based around that. It's relatively limited right now with the number of prompts", "tokens": [50614, 400, 370, 321, 4476, 3094, 257, 5081, 2424, 2361, 926, 300, 13, 467, 311, 7226, 5567, 558, 586, 365, 264, 1230, 295, 41095, 50964], "temperature": 0.0, "avg_logprob": -0.10312758926796702, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.008800255134701729}, {"id": 90, "seek": 44548, "start": 457.48, "end": 461.48, "text": " it can give. It's about 15, but we're adding more as time goes on.", "tokens": [50964, 309, 393, 976, 13, 467, 311, 466, 2119, 11, 457, 321, 434, 5127, 544, 382, 565, 1709, 322, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10312758926796702, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.008800255134701729}, {"id": 91, "seek": 44548, "start": 461.48, "end": 468.48, "text": " So it is intensive in building it, but the end sort of goal that it improves trustworthiness", "tokens": [51164, 407, 309, 307, 18957, 294, 2390, 309, 11, 457, 264, 917, 1333, 295, 3387, 300, 309, 24771, 3361, 13136, 1324, 51514], "temperature": 0.0, "avg_logprob": -0.10312758926796702, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.008800255134701729}, {"id": 92, "seek": 44548, "start": 468.48, "end": 474.48, "text": " and robustness in our system, which is really important for our clients, is like it's worth it.", "tokens": [51514, 293, 13956, 1287, 294, 527, 1185, 11, 597, 307, 534, 1021, 337, 527, 6982, 11, 307, 411, 309, 311, 3163, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10312758926796702, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.008800255134701729}, {"id": 93, "seek": 47448, "start": 474.48, "end": 479.48, "text": " That's really cool. So this is a chat box you've built yourself and is this like the chat box you've built,", "tokens": [50364, 663, 311, 534, 1627, 13, 407, 341, 307, 257, 5081, 2424, 291, 600, 3094, 1803, 293, 307, 341, 411, 264, 5081, 2424, 291, 600, 3094, 11, 50614], "temperature": 0.0, "avg_logprob": -0.16386492112103632, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.038775939494371414}, {"id": 94, "seek": 47448, "start": 479.48, "end": 483.48, "text": " not a large language model, it just is a curation of a large language model?", "tokens": [50614, 406, 257, 2416, 2856, 2316, 11, 309, 445, 307, 257, 1262, 399, 295, 257, 2416, 2856, 2316, 30, 50814], "temperature": 0.0, "avg_logprob": -0.16386492112103632, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.038775939494371414}, {"id": 95, "seek": 47448, "start": 483.48, "end": 490.48, "text": " Yeah, it's not. The chat box itself, I didn't actually build it myself. Someone else on the team built it,", "tokens": [50814, 865, 11, 309, 311, 406, 13, 440, 5081, 2424, 2564, 11, 286, 994, 380, 767, 1322, 309, 2059, 13, 8734, 1646, 322, 264, 1469, 3094, 309, 11, 51164], "temperature": 0.0, "avg_logprob": -0.16386492112103632, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.038775939494371414}, {"id": 96, "seek": 47448, "start": 490.48, "end": 498.48, "text": " but yeah, I believe it uses either a very basic LLM or it's just purely statistical.", "tokens": [51164, 457, 1338, 11, 286, 1697, 309, 4960, 2139, 257, 588, 3875, 441, 43, 44, 420, 309, 311, 445, 17491, 22820, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16386492112103632, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.038775939494371414}, {"id": 97, "seek": 49848, "start": 499.48, "end": 505.48, "text": " So something I was going to ask is if this was using a large language model and you were restricting the input and output,", "tokens": [50414, 407, 746, 286, 390, 516, 281, 1029, 307, 498, 341, 390, 1228, 257, 2416, 2856, 2316, 293, 291, 645, 1472, 37714, 264, 4846, 293, 5598, 11, 50714], "temperature": 0.0, "avg_logprob": -0.13297135489327566, "compression_ratio": 1.759656652360515, "no_speech_prob": 0.01590130291879177}, {"id": 98, "seek": 49848, "start": 505.48, "end": 510.48, "text": " I think a really clever student could play around with the order you ask things and probably still get bad things to come out,", "tokens": [50714, 286, 519, 257, 534, 13494, 3107, 727, 862, 926, 365, 264, 1668, 291, 1029, 721, 293, 1391, 920, 483, 1578, 721, 281, 808, 484, 11, 50964], "temperature": 0.0, "avg_logprob": -0.13297135489327566, "compression_ratio": 1.759656652360515, "no_speech_prob": 0.01590130291879177}, {"id": 99, "seek": 49848, "start": 510.48, "end": 513.48, "text": " which is, which would be, I mean, it would be bad, but it would be interesting.", "tokens": [50964, 597, 307, 11, 597, 576, 312, 11, 286, 914, 11, 309, 576, 312, 1578, 11, 457, 309, 576, 312, 1880, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13297135489327566, "compression_ratio": 1.759656652360515, "no_speech_prob": 0.01590130291879177}, {"id": 100, "seek": 49848, "start": 513.48, "end": 517.48, "text": " It would be interesting. Yeah. There's something that we're continually testing.", "tokens": [51114, 467, 576, 312, 1880, 13, 865, 13, 821, 311, 746, 300, 321, 434, 22277, 4997, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13297135489327566, "compression_ratio": 1.759656652360515, "no_speech_prob": 0.01590130291879177}, {"id": 101, "seek": 51748, "start": 518.48, "end": 535.48, "text": " But yeah, we, as the software works, it will only allow prompts to go to the actual LLM through one of the channels that we've laid out for it through the prompts.", "tokens": [50414, 583, 1338, 11, 321, 11, 382, 264, 4722, 1985, 11, 309, 486, 787, 2089, 41095, 281, 352, 281, 264, 3539, 441, 43, 44, 807, 472, 295, 264, 9235, 300, 321, 600, 9897, 484, 337, 309, 807, 264, 41095, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13982079245827414, "compression_ratio": 1.5527638190954773, "no_speech_prob": 0.3565707802772522}, {"id": 102, "seek": 51748, "start": 535.48, "end": 543.48, "text": " And so they might be able to be clever and get it to return a wrong prompt, but it still wouldn't return like things that are detrimental per se.", "tokens": [51264, 400, 370, 436, 1062, 312, 1075, 281, 312, 13494, 293, 483, 309, 281, 2736, 257, 2085, 12391, 11, 457, 309, 920, 2759, 380, 2736, 411, 721, 300, 366, 45694, 680, 369, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13982079245827414, "compression_ratio": 1.5527638190954773, "no_speech_prob": 0.3565707802772522}, {"id": 103, "seek": 54348, "start": 544.48, "end": 546.48, "text": " That's interesting. Thank you.", "tokens": [50414, 663, 311, 1880, 13, 1044, 291, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16133147723054234, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.022196391597390175}, {"id": 104, "seek": 54348, "start": 546.48, "end": 552.48, "text": " Okay. Thank you for that. Stefania, would you like to introduce your topic for discussion?", "tokens": [50514, 1033, 13, 1044, 291, 337, 300, 13, 43421, 5609, 11, 576, 291, 411, 281, 5366, 428, 4829, 337, 5017, 30, 50814], "temperature": 0.0, "avg_logprob": -0.16133147723054234, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.022196391597390175}, {"id": 105, "seek": 54348, "start": 554.48, "end": 558.48, "text": " Hello. Can you hear me well from outside?", "tokens": [50914, 2425, 13, 1664, 291, 1568, 385, 731, 490, 2380, 30, 51114], "temperature": 0.0, "avg_logprob": -0.16133147723054234, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.022196391597390175}, {"id": 106, "seek": 54348, "start": 558.48, "end": 569.48, "text": " So my topic of discussion is the important to share your projects and to get exposed to conference from a national and community level as well", "tokens": [51114, 407, 452, 4829, 295, 5017, 307, 264, 1021, 281, 2073, 428, 4455, 293, 281, 483, 9495, 281, 7586, 490, 257, 4048, 293, 1768, 1496, 382, 731, 51664], "temperature": 0.0, "avg_logprob": -0.16133147723054234, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.022196391597390175}, {"id": 107, "seek": 56948, "start": 569.48, "end": 578.48, "text": " and the local community as well. So to give you some inputs, I personally came from physics, decided to go in data science.", "tokens": [50364, 293, 264, 2654, 1768, 382, 731, 13, 407, 281, 976, 291, 512, 15743, 11, 286, 5665, 1361, 490, 10649, 11, 3047, 281, 352, 294, 1412, 3497, 13, 50814], "temperature": 0.0, "avg_logprob": -0.19073856410695544, "compression_ratio": 1.4491978609625669, "no_speech_prob": 0.09745772182941437}, {"id": 108, "seek": 56948, "start": 578.48, "end": 586.48, "text": " And my very first exposure was Picon Italia seven years ago. How many of you actually programming Python?", "tokens": [50814, 400, 452, 588, 700, 10420, 390, 430, 11911, 41355, 3407, 924, 2057, 13, 1012, 867, 295, 291, 767, 9410, 15329, 30, 51214], "temperature": 0.0, "avg_logprob": -0.19073856410695544, "compression_ratio": 1.4491978609625669, "no_speech_prob": 0.09745772182941437}, {"id": 109, "seek": 56948, "start": 586.48, "end": 590.48, "text": " Cool. How many of you have been in Italy?", "tokens": [51214, 8561, 13, 1012, 867, 295, 291, 362, 668, 294, 10705, 30, 51414], "temperature": 0.0, "avg_logprob": -0.19073856410695544, "compression_ratio": 1.4491978609625669, "no_speech_prob": 0.09745772182941437}, {"id": 110, "seek": 59048, "start": 590.48, "end": 598.48, "text": " So if you want to join actually on 22 to 25 of May, there will be Picon Italia.", "tokens": [50364, 407, 498, 291, 528, 281, 3917, 767, 322, 5853, 281, 3552, 295, 1891, 11, 456, 486, 312, 430, 11911, 41355, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1274966597557068, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.14048074185848236}, {"id": 111, "seek": 59048, "start": 598.48, "end": 604.48, "text": " And personally, when I joined seven years ago, I was a volunteer presenting speakers.", "tokens": [50764, 400, 5665, 11, 562, 286, 6869, 3407, 924, 2057, 11, 286, 390, 257, 13835, 15578, 9518, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1274966597557068, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.14048074185848236}, {"id": 112, "seek": 59048, "start": 604.48, "end": 609.48, "text": " And at one point that I was studying a lot, I was saying, okay, I can do that.", "tokens": [51064, 400, 412, 472, 935, 300, 286, 390, 7601, 257, 688, 11, 286, 390, 1566, 11, 1392, 11, 286, 393, 360, 300, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1274966597557068, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.14048074185848236}, {"id": 113, "seek": 59048, "start": 609.48, "end": 618.48, "text": " And that gave me a big push in applying to data science job and I've been teaching Python data science for a long time.", "tokens": [51314, 400, 300, 2729, 385, 257, 955, 2944, 294, 9275, 281, 1412, 3497, 1691, 293, 286, 600, 668, 4571, 15329, 1412, 3497, 337, 257, 938, 565, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1274966597557068, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.14048074185848236}, {"id": 114, "seek": 61848, "start": 618.48, "end": 624.48, "text": " And so I encouraged you to go to national conferences, but also on the local point of view.", "tokens": [50364, 400, 370, 286, 14658, 291, 281, 352, 281, 4048, 22032, 11, 457, 611, 322, 264, 2654, 935, 295, 1910, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2635066509246826, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.02712925523519516}, {"id": 115, "seek": 61848, "start": 624.48, "end": 627.48, "text": " So for example, I'm based in Turin, Northwest Italy.", "tokens": [50664, 407, 337, 1365, 11, 286, 478, 2361, 294, 5712, 259, 11, 26068, 10705, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2635066509246826, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.02712925523519516}, {"id": 116, "seek": 61848, "start": 627.48, "end": 632.48, "text": " And a great example was for example, Shishari Kat, an assistant.", "tokens": [50814, 400, 257, 869, 1365, 390, 337, 1365, 11, 1160, 742, 3504, 8365, 11, 364, 10994, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2635066509246826, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.02712925523519516}, {"id": 117, "seek": 61848, "start": 632.48, "end": 640.48, "text": " We are by, if I don't remember wrong, Piero Stefani and Savastani.", "tokens": [51064, 492, 366, 538, 11, 498, 286, 500, 380, 1604, 2085, 11, 430, 12030, 43421, 3782, 293, 12346, 525, 3782, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2635066509246826, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.02712925523519516}, {"id": 118, "seek": 64048, "start": 640.48, "end": 652.48, "text": " And there was a contributor, Alessandro Spallina, that came to give his first talk with a demo inside a Python Turin on my city.", "tokens": [50364, 400, 456, 390, 257, 42859, 11, 967, 442, 29173, 1738, 336, 1426, 11, 300, 1361, 281, 976, 702, 700, 751, 365, 257, 10723, 1854, 257, 15329, 5712, 259, 322, 452, 2307, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13516900118659525, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.2409495860338211}, {"id": 119, "seek": 64048, "start": 652.48, "end": 665.48, "text": " So it was very interesting to see how from one person showing up a project can inspire others and also collect more and more volunteers to their own project.", "tokens": [50964, 407, 309, 390, 588, 1880, 281, 536, 577, 490, 472, 954, 4099, 493, 257, 1716, 393, 15638, 2357, 293, 611, 2500, 544, 293, 544, 14352, 281, 641, 1065, 1716, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13516900118659525, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.2409495860338211}, {"id": 120, "seek": 66548, "start": 665.48, "end": 674.48, "text": " And to add on that also, I want to also inspire you to be networking across different communities.", "tokens": [50364, 400, 281, 909, 322, 300, 611, 11, 286, 528, 281, 611, 15638, 291, 281, 312, 17985, 2108, 819, 4456, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11483697233528926, "compression_ratio": 1.4080459770114941, "no_speech_prob": 0.13249872624874115}, {"id": 121, "seek": 66548, "start": 674.48, "end": 685.48, "text": " For example, another example of an event was working with OpenStreetMap data and collaborating with Wikimedia Italia in order to make this happen.", "tokens": [50814, 1171, 1365, 11, 1071, 1365, 295, 364, 2280, 390, 1364, 365, 7238, 50, 3599, 302, 44, 569, 1412, 293, 30188, 365, 23377, 332, 14212, 41355, 294, 1668, 281, 652, 341, 1051, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11483697233528926, "compression_ratio": 1.4080459770114941, "no_speech_prob": 0.13249872624874115}, {"id": 122, "seek": 68548, "start": 686.48, "end": 698.48, "text": " So I want to inspire you to enhance your local community, especially also to give opportunities to students to showcase your product and your project in machine learning.", "tokens": [50414, 407, 286, 528, 281, 15638, 291, 281, 11985, 428, 2654, 1768, 11, 2318, 611, 281, 976, 4786, 281, 1731, 281, 20388, 428, 1674, 293, 428, 1716, 294, 3479, 2539, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10350597381591797, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.29916220903396606}, {"id": 123, "seek": 68548, "start": 698.48, "end": 713.48, "text": " And what I saw from my side is when you're able to create a space for people to network and to collaborate, it's also easy to study, understand better complex knowledge and collaborate together.", "tokens": [51014, 400, 437, 286, 1866, 490, 452, 1252, 307, 562, 291, 434, 1075, 281, 1884, 257, 1901, 337, 561, 281, 3209, 293, 281, 18338, 11, 309, 311, 611, 1858, 281, 2979, 11, 1223, 1101, 3997, 3601, 293, 18338, 1214, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10350597381591797, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.29916220903396606}, {"id": 124, "seek": 71348, "start": 713.48, "end": 727.48, "text": " So I'm very open to help you with your local community or national chapter and hoping for questions, even if you want to share your own experience in starting or being inspired by local events.", "tokens": [50364, 407, 286, 478, 588, 1269, 281, 854, 291, 365, 428, 2654, 1768, 420, 4048, 7187, 293, 7159, 337, 1651, 11, 754, 498, 291, 528, 281, 2073, 428, 1065, 1752, 294, 2891, 420, 885, 7547, 538, 2654, 3931, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15703874979263696, "compression_ratio": 1.5502392344497609, "no_speech_prob": 0.11730272322893143}, {"id": 125, "seek": 71348, "start": 727.48, "end": 734.48, "text": " Okay, thank you, Stefania. Any questions, comments or anyone wish to take up the offer of help from Stefania?", "tokens": [51064, 1033, 11, 1309, 291, 11, 43421, 5609, 13, 2639, 1651, 11, 3053, 420, 2878, 3172, 281, 747, 493, 264, 2626, 295, 854, 490, 43421, 5609, 30, 51414], "temperature": 0.0, "avg_logprob": -0.15703874979263696, "compression_ratio": 1.5502392344497609, "no_speech_prob": 0.11730272322893143}, {"id": 126, "seek": 71348, "start": 734.48, "end": 738.48, "text": " Hands, come on. Yep.", "tokens": [51414, 21369, 11, 808, 322, 13, 7010, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15703874979263696, "compression_ratio": 1.5502392344497609, "no_speech_prob": 0.11730272322893143}, {"id": 127, "seek": 74348, "start": 743.48, "end": 751.48, "text": " Help me hearing because I don't hear you.", "tokens": [50364, 10773, 385, 4763, 570, 286, 500, 380, 1568, 291, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20799479117760292, "compression_ratio": 1.34, "no_speech_prob": 0.03965035453438759}, {"id": 128, "seek": 74348, "start": 751.48, "end": 755.48, "text": " Hello. So thanks for the interview.", "tokens": [50764, 2425, 13, 407, 3231, 337, 264, 4049, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20799479117760292, "compression_ratio": 1.34, "no_speech_prob": 0.03965035453438759}, {"id": 129, "seek": 74348, "start": 755.48, "end": 767.48, "text": " I was kind of worried because I'm Belgian and I think there is a lack of student community in tech, especially informatics.", "tokens": [50964, 286, 390, 733, 295, 5804, 570, 286, 478, 47127, 293, 286, 519, 456, 307, 257, 5011, 295, 3107, 1768, 294, 7553, 11, 2318, 1356, 30292, 13, 51564], "temperature": 0.0, "avg_logprob": -0.20799479117760292, "compression_ratio": 1.34, "no_speech_prob": 0.03965035453438759}, {"id": 130, "seek": 76748, "start": 767.48, "end": 780.48, "text": " I was a leader of a Google group last year for students and it's kind of hard to attract students to learn AI a part of their course.", "tokens": [50364, 286, 390, 257, 5263, 295, 257, 3329, 1594, 1036, 1064, 337, 1731, 293, 309, 311, 733, 295, 1152, 281, 5049, 1731, 281, 1466, 7318, 257, 644, 295, 641, 1164, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10882512962116915, "compression_ratio": 1.2547169811320755, "no_speech_prob": 0.4130476713180542}, {"id": 131, "seek": 78048, "start": 780.48, "end": 802.48, "text": " So I kind of agree with your view to improve this communication, to attract some people and motivate them to participate into some conference like that and other conference and other hackathon or something like in Italy, for example.", "tokens": [50364, 407, 286, 733, 295, 3986, 365, 428, 1910, 281, 3470, 341, 6101, 11, 281, 5049, 512, 561, 293, 28497, 552, 281, 8197, 666, 512, 7586, 411, 300, 293, 661, 7586, 293, 661, 10339, 18660, 420, 746, 411, 294, 10705, 11, 337, 1365, 13, 51464], "temperature": 0.0, "avg_logprob": -0.22903671670467296, "compression_ratio": 1.5328947368421053, "no_speech_prob": 0.7721607685089111}, {"id": 132, "seek": 80248, "start": 802.48, "end": 821.48, "text": " And so I would really like to see more group like that, like you told and I think it's really important because some people don't even think that, some people think that I have to be, to study AI, to do AI and to be specializing in this thing, to find a job in this thing.", "tokens": [50364, 400, 370, 286, 576, 534, 411, 281, 536, 544, 1594, 411, 300, 11, 411, 291, 1907, 293, 286, 519, 309, 311, 534, 1021, 570, 512, 561, 500, 380, 754, 519, 300, 11, 512, 561, 519, 300, 286, 362, 281, 312, 11, 281, 2979, 7318, 11, 281, 360, 7318, 293, 281, 312, 2121, 3319, 294, 341, 551, 11, 281, 915, 257, 1691, 294, 341, 551, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18022421131963315, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.5297876000404358}, {"id": 133, "seek": 82148, "start": 821.48, "end": 847.48, "text": " And when I speak with recruiter and something, they have an opposite discourse. So I can agree with your point of view and I can't, like, no, I want to motivate people to join group, to create group because for example the GDSC, Google group, we are the first one was in Mons, in Belgium, a second in Liege, but for foreign students.", "tokens": [50364, 400, 562, 286, 1710, 365, 9372, 1681, 293, 746, 11, 436, 362, 364, 6182, 23938, 13, 407, 286, 393, 3986, 365, 428, 935, 295, 1910, 293, 286, 393, 380, 11, 411, 11, 572, 11, 286, 528, 281, 28497, 561, 281, 3917, 1594, 11, 281, 1884, 1594, 570, 337, 1365, 264, 460, 11844, 34, 11, 3329, 1594, 11, 321, 366, 264, 700, 472, 390, 294, 376, 892, 11, 294, 28094, 11, 257, 1150, 294, 11197, 432, 11, 457, 337, 5329, 1731, 13, 51664], "temperature": 0.0, "avg_logprob": -0.24535681780646829, "compression_ratio": 1.493273542600897, "no_speech_prob": 0.5776665806770325}, {"id": 134, "seek": 84748, "start": 847.48, "end": 860.48, "text": " And in Belgium it's really hard to create those groups and I think schools should help students to create that and to entertain the culture about techs and AI, etc.", "tokens": [50364, 400, 294, 28094, 309, 311, 534, 1152, 281, 1884, 729, 3935, 293, 286, 519, 4656, 820, 854, 1731, 281, 1884, 300, 293, 281, 7655, 264, 3713, 466, 7553, 82, 293, 7318, 11, 5183, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16176070665058337, "compression_ratio": 1.3666666666666667, "no_speech_prob": 0.5938751101493835}, {"id": 135, "seek": 86048, "start": 860.48, "end": 880.48, "text": " And I could wish to, with older societies or older more professional group to maybe ask to study, like university for example, to help those groups to preferrate and multiple and to attract more and more people like that.", "tokens": [50364, 400, 286, 727, 3172, 281, 11, 365, 4906, 19329, 420, 4906, 544, 4843, 1594, 281, 1310, 1029, 281, 2979, 11, 411, 5454, 337, 1365, 11, 281, 854, 729, 3935, 281, 659, 612, 4404, 293, 3866, 293, 281, 5049, 544, 293, 544, 561, 411, 300, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3059197859330611, "compression_ratio": 1.4779874213836477, "no_speech_prob": 0.8288729786872864}, {"id": 136, "seek": 86048, "start": 880.48, "end": 882.48, "text": " So thank you.", "tokens": [51364, 407, 1309, 291, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3059197859330611, "compression_ratio": 1.4779874213836477, "no_speech_prob": 0.8288729786872864}, {"id": 137, "seek": 88248, "start": 882.48, "end": 904.48, "text": " Yes, thank you for the input and I have a lot of tips to give you that maybe can help other people too. So first of all, also encourage students, volunteer for conference, for example, an example of Picon Italia is to get also students from Florence, from high school and university to get and help.", "tokens": [50364, 1079, 11, 1309, 291, 337, 264, 4846, 293, 286, 362, 257, 688, 295, 6082, 281, 976, 291, 300, 1310, 393, 854, 661, 561, 886, 13, 407, 700, 295, 439, 11, 611, 5373, 1731, 11, 13835, 337, 7586, 11, 337, 1365, 11, 364, 1365, 295, 430, 11911, 41355, 307, 281, 483, 611, 1731, 490, 34631, 11, 490, 1090, 1395, 293, 5454, 281, 483, 293, 854, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16111358697863593, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.6645347476005554}, {"id": 138, "seek": 90448, "start": 904.48, "end": 922.48, "text": " So it's very important, I'm also leading an nonprofit association, so it's very challenging to get that volunteer work. So if you are the organizer and what I encourage all of you is to not wait for something to happen, just make it happen, just create once.", "tokens": [50364, 407, 309, 311, 588, 1021, 11, 286, 478, 611, 5775, 364, 23348, 14598, 11, 370, 309, 311, 588, 7595, 281, 483, 300, 13835, 589, 13, 407, 498, 291, 366, 264, 41363, 293, 437, 286, 5373, 439, 295, 291, 307, 281, 406, 1699, 337, 746, 281, 1051, 11, 445, 652, 309, 1051, 11, 445, 1884, 1564, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1819028377532959, "compression_ratio": 1.5176470588235293, "no_speech_prob": 0.6042498350143433}, {"id": 139, "seek": 92248, "start": 922.48, "end": 946.48, "text": " And the first thing that I will do is also to contact perhaps speakers from other university. For example, in Italy we have a tour in Milan that are quite close to each other. Milan is doing the first pay data, that's another great network of events, especially for students, have something very clear and attractive to them.", "tokens": [50364, 400, 264, 700, 551, 300, 286, 486, 360, 307, 611, 281, 3385, 4317, 9518, 490, 661, 5454, 13, 1171, 1365, 11, 294, 10705, 321, 362, 257, 3512, 294, 32874, 300, 366, 1596, 1998, 281, 1184, 661, 13, 32874, 307, 884, 264, 700, 1689, 1412, 11, 300, 311, 1071, 869, 3209, 295, 3931, 11, 2318, 337, 1731, 11, 362, 746, 588, 1850, 293, 12609, 281, 552, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17748797280447823, "compression_ratio": 1.5402843601895735, "no_speech_prob": 0.5484698414802551}, {"id": 140, "seek": 94648, "start": 946.48, "end": 966.48, "text": " And I have to be honest, sometimes also free food helps. For beers, for example, there is a format called Data beers, I don't know if you ever heard about it. It started from data scientists in Spain and there is a beer estrella that kindly sponsored beers.", "tokens": [50364, 400, 286, 362, 281, 312, 3245, 11, 2171, 611, 1737, 1755, 3665, 13, 1171, 34159, 11, 337, 1365, 11, 456, 307, 257, 7877, 1219, 11888, 34159, 11, 286, 500, 380, 458, 498, 291, 1562, 2198, 466, 309, 13, 467, 1409, 490, 1412, 7708, 294, 12838, 293, 456, 307, 257, 8795, 871, 265, 3505, 300, 29736, 16621, 34159, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2145827201104933, "compression_ratio": 1.4438202247191012, "no_speech_prob": 0.6544752717018127}, {"id": 141, "seek": 96648, "start": 966.48, "end": 983.48, "text": " And the format is a free lighting talks. So we started to do it more and more in Italy, there is different city that doing it and also across Europe. And that could be an extra push to make people come to events and then really network to each other.", "tokens": [50364, 400, 264, 7877, 307, 257, 1737, 9577, 6686, 13, 407, 321, 1409, 281, 360, 309, 544, 293, 544, 294, 10705, 11, 456, 307, 819, 2307, 300, 884, 309, 293, 611, 2108, 3315, 13, 400, 300, 727, 312, 364, 2857, 2944, 281, 652, 561, 808, 281, 3931, 293, 550, 534, 3209, 281, 1184, 661, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1978327488077098, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.7373573780059814}, {"id": 142, "seek": 98348, "start": 983.48, "end": 1005.48, "text": " So my suggestion is if you can get a professor on board, that's great. But then also another last tip that can give you is the last event that we did was in collaboration with a high school because sometimes it's very challenging to find spaces.", "tokens": [50364, 407, 452, 16541, 307, 498, 291, 393, 483, 257, 8304, 322, 3150, 11, 300, 311, 869, 13, 583, 550, 611, 1071, 1036, 4125, 300, 393, 976, 291, 307, 264, 1036, 2280, 300, 321, 630, 390, 294, 9363, 365, 257, 1090, 1395, 570, 2171, 309, 311, 588, 7595, 281, 915, 7673, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12698440551757811, "compression_ratio": 1.467065868263473, "no_speech_prob": 0.514602541923523}, {"id": 143, "seek": 100548, "start": 1005.48, "end": 1022.48, "text": " So for example, you can look out the different community, even a Linux group that can give also particular topics in machine learning and say, okay, I'm in contact with the professor that can offer the space to host an open source community here.", "tokens": [50364, 407, 337, 1365, 11, 291, 393, 574, 484, 264, 819, 1768, 11, 754, 257, 18734, 1594, 300, 393, 976, 611, 1729, 8378, 294, 3479, 2539, 293, 584, 11, 1392, 11, 286, 478, 294, 3385, 365, 264, 8304, 300, 393, 2626, 264, 1901, 281, 3975, 364, 1269, 4009, 1768, 510, 13, 51214], "temperature": 0.0, "avg_logprob": -0.21196439531114367, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.4720919132232666}, {"id": 144, "seek": 102248, "start": 1022.48, "end": 1045.48, "text": " And then the students will start to know about that and start to get around and talk about that and from there can start another group. And there are a few tips, but feel free to contact me if you need more tips on that because it's very good to also share your experience so all the local groups and national community can also learn from each other.", "tokens": [50364, 400, 550, 264, 1731, 486, 722, 281, 458, 466, 300, 293, 722, 281, 483, 926, 293, 751, 466, 300, 293, 490, 456, 393, 722, 1071, 1594, 13, 400, 456, 366, 257, 1326, 6082, 11, 457, 841, 1737, 281, 3385, 385, 498, 291, 643, 544, 6082, 322, 300, 570, 309, 311, 588, 665, 281, 611, 2073, 428, 1752, 370, 439, 264, 2654, 3935, 293, 4048, 1768, 393, 611, 1466, 490, 1184, 661, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15402634520279734, "compression_ratio": 1.6875, "no_speech_prob": 0.44891196489334106}, {"id": 145, "seek": 104548, "start": 1046.48, "end": 1050.48, "text": " Out.", "tokens": [50414, 5925, 13, 50614], "temperature": 0.0, "avg_logprob": -0.21718537180047287, "compression_ratio": 1.45, "no_speech_prob": 0.29683780670166016}, {"id": 146, "seek": 104548, "start": 1050.48, "end": 1057.48, "text": " So I was the one of the main organizers of the University of Birmingham Computer Science Society for several years.", "tokens": [50614, 407, 286, 390, 264, 472, 295, 264, 2135, 35071, 295, 264, 3535, 295, 34673, 22289, 8976, 13742, 337, 2940, 924, 13, 50964], "temperature": 0.0, "avg_logprob": -0.21718537180047287, "compression_ratio": 1.45, "no_speech_prob": 0.29683780670166016}, {"id": 147, "seek": 104548, "start": 1057.48, "end": 1067.48, "text": " And so my tip about making a community is definitely focus on some organic kind of community growth of pizza and beer and interesting talks.", "tokens": [50964, 400, 370, 452, 4125, 466, 1455, 257, 1768, 307, 2138, 1879, 322, 512, 10220, 733, 295, 1768, 4599, 295, 8298, 293, 8795, 293, 1880, 6686, 13, 51464], "temperature": 0.0, "avg_logprob": -0.21718537180047287, "compression_ratio": 1.45, "no_speech_prob": 0.29683780670166016}, {"id": 148, "seek": 106748, "start": 1068.48, "end": 1082.48, "text": " And then from that, see if you can, once you've got enough of a group of people, you can then approach companies for sponsorship, which will only make it bigger and bigger because people are desperate, or companies are desperate for computer science graduates.", "tokens": [50414, 400, 550, 490, 300, 11, 536, 498, 291, 393, 11, 1564, 291, 600, 658, 1547, 295, 257, 1594, 295, 561, 11, 291, 393, 550, 3109, 3431, 337, 42922, 11, 597, 486, 787, 652, 309, 3801, 293, 3801, 570, 561, 366, 17601, 11, 420, 3431, 366, 17601, 337, 3820, 3497, 13577, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1163073372595089, "compression_ratio": 1.75, "no_speech_prob": 0.6909756660461426}, {"id": 149, "seek": 106748, "start": 1082.48, "end": 1096.48, "text": " You've got a genuine currency there that companies will definitely respond to. So I think build organic growth to begin with and then you can really get much more money and greater support by utilizing companies like that.", "tokens": [51114, 509, 600, 658, 257, 16699, 13346, 456, 300, 3431, 486, 2138, 4196, 281, 13, 407, 286, 519, 1322, 10220, 4599, 281, 1841, 365, 293, 550, 291, 393, 534, 483, 709, 544, 1460, 293, 5044, 1406, 538, 26775, 3431, 411, 300, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1163073372595089, "compression_ratio": 1.75, "no_speech_prob": 0.6909756660461426}, {"id": 150, "seek": 109748, "start": 1097.48, "end": 1106.48, "text": " Yes, I do agree with that. And also there's someone that is waving in here, you can get it.", "tokens": [50364, 1079, 11, 286, 360, 3986, 365, 300, 13, 400, 611, 456, 311, 1580, 300, 307, 35347, 294, 510, 11, 291, 393, 483, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18328691565472147, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.034255094826221466}, {"id": 151, "seek": 109748, "start": 1106.48, "end": 1120.48, "text": " And also in terms of workshop, I do agree in getting involved also with companies, local companies, and in terms of kind of events, it could be more social events or more workshop events.", "tokens": [50814, 400, 611, 294, 2115, 295, 13541, 11, 286, 360, 3986, 294, 1242, 3288, 611, 365, 3431, 11, 2654, 3431, 11, 293, 294, 2115, 295, 733, 295, 3931, 11, 309, 727, 312, 544, 2093, 3931, 420, 544, 13541, 3931, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18328691565472147, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.034255094826221466}, {"id": 152, "seek": 112048, "start": 1120.48, "end": 1129.48, "text": " For example, for the one about special data, it was very, very hardcore workshop with not both shared and then discussion of it.", "tokens": [50364, 1171, 1365, 11, 337, 264, 472, 466, 2121, 1412, 11, 309, 390, 588, 11, 588, 28196, 13541, 365, 406, 1293, 5507, 293, 550, 5017, 295, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18871847788492838, "compression_ratio": 1.5446009389671362, "no_speech_prob": 0.06832575052976608}, {"id": 153, "seek": 112048, "start": 1129.48, "end": 1137.48, "text": " And he could be also hack and tell with someone is coding and other people are watching it and asking questions along the way.", "tokens": [50814, 400, 415, 727, 312, 611, 10339, 293, 980, 365, 1580, 307, 17720, 293, 661, 561, 366, 1976, 309, 293, 3365, 1651, 2051, 264, 636, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18871847788492838, "compression_ratio": 1.5446009389671362, "no_speech_prob": 0.06832575052976608}, {"id": 154, "seek": 112048, "start": 1139.48, "end": 1144.48, "text": " So hello, thank you for mentioning the cat. I am the guy that started it.", "tokens": [51314, 407, 7751, 11, 1309, 291, 337, 18315, 264, 3857, 13, 286, 669, 264, 2146, 300, 1409, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18871847788492838, "compression_ratio": 1.5446009389671362, "no_speech_prob": 0.06832575052976608}, {"id": 155, "seek": 114448, "start": 1144.48, "end": 1160.48, "text": " So thank you. I want to get a little political if it is permitted. So I think in this place, it's really important to, and I ask you a comment on this, to focus on open source and standards because they go hand in hand.", "tokens": [50364, 407, 1309, 291, 13, 286, 528, 281, 483, 257, 707, 3905, 498, 309, 307, 28658, 13, 407, 286, 519, 294, 341, 1081, 11, 309, 311, 534, 1021, 281, 11, 293, 286, 1029, 291, 257, 2871, 322, 341, 11, 281, 1879, 322, 1269, 4009, 293, 7787, 570, 436, 352, 1011, 294, 1011, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11363255977630615, "compression_ratio": 1.4407894736842106, "no_speech_prob": 0.0200776606798172}, {"id": 156, "seek": 116048, "start": 1160.48, "end": 1176.48, "text": " And it's a way to invite our territory in Europe to build without falling into the fun boy style around open AI, around US services.", "tokens": [50364, 400, 309, 311, 257, 636, 281, 7980, 527, 11360, 294, 3315, 281, 1322, 1553, 7440, 666, 264, 1019, 3237, 3758, 926, 1269, 7318, 11, 926, 2546, 3328, 13, 51164], "temperature": 0.0, "avg_logprob": -0.134237140417099, "compression_ratio": 1.1785714285714286, "no_speech_prob": 0.16256268322467804}, {"id": 157, "seek": 117648, "start": 1176.48, "end": 1186.48, "text": " So it's time to build in open and create standards, not only the laws, because we are good at laws and standards and open source.", "tokens": [50364, 407, 309, 311, 565, 281, 1322, 294, 1269, 293, 1884, 7787, 11, 406, 787, 264, 6064, 11, 570, 321, 366, 665, 412, 6064, 293, 7787, 293, 1269, 4009, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15409389196657666, "compression_ratio": 1.4253731343283582, "no_speech_prob": 0.34001773595809937}, {"id": 158, "seek": 117648, "start": 1186.48, "end": 1191.48, "text": " It's the only way we can build our own AI economy. Thank you.", "tokens": [50864, 467, 311, 264, 787, 636, 321, 393, 1322, 527, 1065, 7318, 5010, 13, 1044, 291, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15409389196657666, "compression_ratio": 1.4253731343283582, "no_speech_prob": 0.34001773595809937}, {"id": 159, "seek": 119148, "start": 1191.48, "end": 1195.48, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50564], "temperature": 0.0, "avg_logprob": -0.3557589237506573, "compression_ratio": 1.3666666666666667, "no_speech_prob": 0.4509069621562958}, {"id": 160, "seek": 119148, "start": 1195.48, "end": 1209.48, "text": " Thank you. Yeah, I do do agree completely. Actually, maybe he knows that I give some talks last year by potentially about a safety but later on at two 15 we are going to talk about exactly that.", "tokens": [50564, 1044, 291, 13, 865, 11, 286, 360, 360, 3986, 2584, 13, 5135, 11, 1310, 415, 3255, 300, 286, 976, 512, 6686, 1036, 1064, 538, 7263, 466, 257, 4514, 457, 1780, 322, 412, 732, 2119, 321, 366, 516, 281, 751, 466, 2293, 300, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3557589237506573, "compression_ratio": 1.3666666666666667, "no_speech_prob": 0.4509069621562958}, {"id": 161, "seek": 120948, "start": 1209.48, "end": 1222.48, "text": " So standards AI governments. So if you want to stick around, there will be a wonderful panel to talk about that. And I completely agree that we need that more centralization and more discussion, even an event in university.", "tokens": [50364, 407, 7787, 7318, 11280, 13, 407, 498, 291, 528, 281, 2897, 926, 11, 456, 486, 312, 257, 3715, 4831, 281, 751, 466, 300, 13, 400, 286, 2584, 3986, 300, 321, 643, 300, 544, 5777, 2144, 293, 544, 5017, 11, 754, 364, 2280, 294, 5454, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17569903938137754, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.3265683650970459}, {"id": 162, "seek": 120948, "start": 1222.48, "end": 1237.48, "text": " And not only computer science, also policymakers need to have more and more exposure in the past. I worked with the open data and we have policymakers ask us, can you please explain that the science because we have to make the law.", "tokens": [51014, 400, 406, 787, 3820, 3497, 11, 611, 47325, 643, 281, 362, 544, 293, 544, 10420, 294, 264, 1791, 13, 286, 2732, 365, 264, 1269, 1412, 293, 321, 362, 47325, 1029, 505, 11, 393, 291, 1767, 2903, 300, 264, 3497, 570, 321, 362, 281, 652, 264, 2101, 13, 51764], "temperature": 0.0, "avg_logprob": -0.17569903938137754, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.3265683650970459}, {"id": 163, "seek": 123748, "start": 1237.48, "end": 1243.48, "text": " And yeah, so communication between the two sector are extremely important. Thank you for the inputs.", "tokens": [50364, 400, 1338, 11, 370, 6101, 1296, 264, 732, 6977, 366, 4664, 1021, 13, 1044, 291, 337, 264, 15743, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12650378183885055, "compression_ratio": 1.5310077519379846, "no_speech_prob": 0.20778878033161163}, {"id": 164, "seek": 123748, "start": 1243.48, "end": 1257.48, "text": " Can I just ask a question? I was lovely to hear the discussion about graduates and everything. What's your view on how you bring forward the older engineer, of which I'm a representative member who's been working in software engineering for a long time.", "tokens": [50664, 1664, 286, 445, 1029, 257, 1168, 30, 286, 390, 7496, 281, 1568, 264, 5017, 466, 13577, 293, 1203, 13, 708, 311, 428, 1910, 322, 577, 291, 1565, 2128, 264, 4906, 11403, 11, 295, 597, 286, 478, 257, 12424, 4006, 567, 311, 668, 1364, 294, 4722, 7043, 337, 257, 938, 565, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12650378183885055, "compression_ratio": 1.5310077519379846, "no_speech_prob": 0.20778878033161163}, {"id": 165, "seek": 123748, "start": 1257.48, "end": 1261.48, "text": " How do you bring them into the AI world?", "tokens": [51364, 1012, 360, 291, 1565, 552, 666, 264, 7318, 1002, 30, 51564], "temperature": 0.0, "avg_logprob": -0.12650378183885055, "compression_ratio": 1.5310077519379846, "no_speech_prob": 0.20778878033161163}, {"id": 166, "seek": 126148, "start": 1261.48, "end": 1284.48, "text": " Well, I think there will be a space for everyone in terms of, well, I'm from the data science part of you and recently more about handling their risk of AI. So I think there will be a great space to understand how to make it more accessible to everyone.", "tokens": [50364, 1042, 11, 286, 519, 456, 486, 312, 257, 1901, 337, 1518, 294, 2115, 295, 11, 731, 11, 286, 478, 490, 264, 1412, 3497, 644, 295, 291, 293, 3938, 544, 466, 13175, 641, 3148, 295, 7318, 13, 407, 286, 519, 456, 486, 312, 257, 869, 1901, 281, 1223, 577, 281, 652, 309, 544, 9515, 281, 1518, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1518399238586426, "compression_ratio": 1.5521472392638036, "no_speech_prob": 0.24344676733016968}, {"id": 167, "seek": 128448, "start": 1284.48, "end": 1302.48, "text": " And also, I don't know your specific, because from UX to development to understanding more also cross part about languages, for example, so it came to my mind, for example, Rust became more and more popular in data science from pollers.", "tokens": [50364, 400, 611, 11, 286, 500, 380, 458, 428, 2685, 11, 570, 490, 40176, 281, 3250, 281, 3701, 544, 611, 3278, 644, 466, 8650, 11, 337, 1365, 11, 370, 309, 1361, 281, 452, 1575, 11, 337, 1365, 11, 34952, 3062, 544, 293, 544, 3743, 294, 1412, 3497, 490, 6418, 433, 13, 51264], "temperature": 0.0, "avg_logprob": -0.24571088508323388, "compression_ratio": 1.4478527607361964, "no_speech_prob": 0.47298818826675415}, {"id": 168, "seek": 130248, "start": 1302.48, "end": 1327.48, "text": " This is a library in in used in data science through a Python binding as well. So it's very important. And let me know if I could request it as well to have some of the tools to have an understanding of different ways of approaching in this case, data science for different technology from especially now that we need more power,", "tokens": [50364, 639, 307, 257, 6405, 294, 294, 1143, 294, 1412, 3497, 807, 257, 15329, 17359, 382, 731, 13, 407, 309, 311, 588, 1021, 13, 400, 718, 385, 458, 498, 286, 727, 5308, 309, 382, 731, 281, 362, 512, 295, 264, 3873, 281, 362, 364, 3701, 295, 819, 2098, 295, 14908, 294, 341, 1389, 11, 1412, 3497, 337, 819, 2899, 490, 2318, 586, 300, 321, 643, 544, 1347, 11, 51614], "temperature": 0.0, "avg_logprob": -0.32288535212127256, "compression_ratio": 1.5518867924528301, "no_speech_prob": 0.5217655301094055}, {"id": 169, "seek": 132748, "start": 1327.48, "end": 1337.48, "text": " recorrosy, more paradigms that sometimes is less used in a high level program languages. So I think there's space to collaborate in that as well.", "tokens": [50364, 850, 284, 2635, 88, 11, 544, 13480, 328, 2592, 300, 2171, 307, 1570, 1143, 294, 257, 1090, 1496, 1461, 8650, 13, 407, 286, 519, 456, 311, 1901, 281, 18338, 294, 300, 382, 731, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3339733324552837, "compression_ratio": 1.25, "no_speech_prob": 0.08064834028482437}, {"id": 170, "seek": 133748, "start": 1338.48, "end": 1357.48, "text": " Yes, so this work I've been looking at with best practices and I for tech works is actually quite a lot about this. And it's really interesting how different it is trying to write a guide or a course for somebody who already knows a lot of computer science paradigms.", "tokens": [50414, 1079, 11, 370, 341, 589, 286, 600, 668, 1237, 412, 365, 1151, 7525, 293, 286, 337, 7553, 1985, 307, 767, 1596, 257, 688, 466, 341, 13, 400, 309, 311, 534, 1880, 577, 819, 309, 307, 1382, 281, 2464, 257, 5934, 420, 257, 1164, 337, 2618, 567, 1217, 3255, 257, 688, 295, 3820, 3497, 13480, 328, 2592, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12630519710603308, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.34379836916923523}, {"id": 171, "seek": 135748, "start": 1357.48, "end": 1378.48, "text": " There's so much you can skip. But there's so many things you have to also be careful to not skip because you need to save them again or reemphasize things. For example, source control is one you need to think about not just source control for your data, sorry, source control for your code, you need to think about source control for your data to make sure your whole pipeline doesn't get messed up.", "tokens": [50364, 821, 311, 370, 709, 291, 393, 10023, 13, 583, 456, 311, 370, 867, 721, 291, 362, 281, 611, 312, 5026, 281, 406, 10023, 570, 291, 643, 281, 3155, 552, 797, 420, 319, 443, 7485, 1125, 721, 13, 1171, 1365, 11, 4009, 1969, 307, 472, 291, 643, 281, 519, 466, 406, 445, 4009, 1969, 337, 428, 1412, 11, 2597, 11, 4009, 1969, 337, 428, 3089, 11, 291, 643, 281, 519, 466, 4009, 1969, 337, 428, 1412, 281, 652, 988, 428, 1379, 15517, 1177, 380, 483, 16507, 493, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1585695560161884, "compression_ratio": 1.9182692307692308, "no_speech_prob": 0.35430991649627686}, {"id": 172, "seek": 137848, "start": 1379.48, "end": 1383.48, "text": " And it's just very interesting. Might be some interesting things to talk about that by the way.", "tokens": [50414, 400, 309, 311, 445, 588, 1880, 13, 23964, 312, 512, 1880, 721, 281, 751, 466, 300, 538, 264, 636, 13, 50614], "temperature": 0.0, "avg_logprob": -0.3782546001931895, "compression_ratio": 1.3519553072625698, "no_speech_prob": 0.12866514921188354}, {"id": 173, "seek": 137848, "start": 1386.48, "end": 1399.48, "text": " It's so nice to be at FOSDOM. I haven't been here for years. So in 2017 I was doing repotential build, neurodebian, any other neuroscientists? No.", "tokens": [50764, 467, 311, 370, 1481, 281, 312, 412, 479, 4367, 35, 5251, 13, 286, 2378, 380, 668, 510, 337, 924, 13, 407, 294, 6591, 286, 390, 884, 1085, 310, 2549, 1322, 11, 16499, 1479, 20196, 11, 604, 661, 28813, 5412, 1751, 30, 883, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3782546001931895, "compression_ratio": 1.3519553072625698, "no_speech_prob": 0.12866514921188354}, {"id": 174, "seek": 139948, "start": 1400.48, "end": 1418.48, "text": " Yeah, neurodebian. Yeah, good. Sorry, I could not find this building so I only saw the last slide. But so in 2017, a group of computational neuroscientists led by me put together a fully open and available set of training.", "tokens": [50414, 865, 11, 16499, 1479, 20196, 13, 865, 11, 665, 13, 4919, 11, 286, 727, 406, 915, 341, 2390, 370, 286, 787, 1866, 264, 1036, 4137, 13, 583, 370, 294, 6591, 11, 257, 1594, 295, 28270, 28813, 5412, 1751, 4684, 538, 385, 829, 1214, 257, 4498, 1269, 293, 2435, 992, 295, 3097, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17674980844770158, "compression_ratio": 1.353658536585366, "no_speech_prob": 0.04090145230293274}, {"id": 175, "seek": 141848, "start": 1419.48, "end": 1434.48, "text": " And it had to be asynchronous as well, right? Code has to be fully available. The classes have to be pre-recorded so that anyone anywhere on earth can access it at any time. That is a extended definition of open and you have to create that.", "tokens": [50414, 400, 309, 632, 281, 312, 49174, 382, 731, 11, 558, 30, 15549, 575, 281, 312, 4498, 2435, 13, 440, 5359, 362, 281, 312, 659, 12, 38500, 292, 370, 300, 2878, 4992, 322, 4120, 393, 2105, 309, 412, 604, 565, 13, 663, 307, 257, 10913, 7123, 295, 1269, 293, 291, 362, 281, 1884, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13071042093737373, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.10317464172840118}, {"id": 176, "seek": 143448, "start": 1435.48, "end": 1454.48, "text": " So we trot them everything up from reinforcement learning agents to GANs in 2017, right? And I think this space is under exploited in open education, right? And if you take this, what we did was say here's the criteria to create reproducible science.", "tokens": [50414, 407, 321, 504, 310, 552, 1203, 493, 490, 29280, 2539, 12554, 281, 460, 1770, 82, 294, 6591, 11, 558, 30, 400, 286, 519, 341, 1901, 307, 833, 40918, 294, 1269, 3309, 11, 558, 30, 400, 498, 291, 747, 341, 11, 437, 321, 630, 390, 584, 510, 311, 264, 11101, 281, 1884, 11408, 32128, 3497, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1523576833434024, "compression_ratio": 1.3812154696132597, "no_speech_prob": 0.027850491926074028}, {"id": 177, "seek": 145448, "start": 1455.48, "end": 1470.48, "text": " Here are the skills that you're going to learn. Go out and find and validate an open data set. And if you do all of that because of reproducible science, you in advance have a pre-registered paper. You will produce a peer-reviewed paper.", "tokens": [50414, 1692, 366, 264, 3942, 300, 291, 434, 516, 281, 1466, 13, 1037, 484, 293, 915, 293, 29562, 364, 1269, 1412, 992, 13, 400, 498, 291, 360, 439, 295, 300, 570, 295, 11408, 32128, 3497, 11, 291, 294, 7295, 362, 257, 659, 12, 3375, 1964, 292, 3035, 13, 509, 486, 5258, 257, 15108, 12, 265, 1759, 292, 3035, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08741388782378166, "compression_ratio": 1.462962962962963, "no_speech_prob": 0.1858346313238144}, {"id": 178, "seek": 147048, "start": 1471.48, "end": 1485.48, "text": " In this space, concerningly, a lot of people, if they get to masters and PhD, have done at this time a lot of rote education and no critical thinking or no extending into new knowledge.", "tokens": [50414, 682, 341, 1901, 11, 18087, 356, 11, 257, 688, 295, 561, 11, 498, 436, 483, 281, 19294, 293, 14476, 11, 362, 1096, 412, 341, 565, 257, 688, 295, 367, 1370, 3309, 293, 572, 4924, 1953, 420, 572, 24360, 666, 777, 3601, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11447064772896144, "compression_ratio": 1.3805970149253732, "no_speech_prob": 0.04708114638924599}, {"id": 179, "seek": 148548, "start": 1486.48, "end": 1500.48, "text": " But the second you tell them, these are tools. And they're tools to solving new problem. And if you solve that problem in science, right? In science, it's the only place where you say, I solved a problem, it's peer-reviewed, it's open and public.", "tokens": [50414, 583, 264, 1150, 291, 980, 552, 11, 613, 366, 3873, 13, 400, 436, 434, 3873, 281, 12606, 777, 1154, 13, 400, 498, 291, 5039, 300, 1154, 294, 3497, 11, 558, 30, 682, 3497, 11, 309, 311, 264, 787, 1081, 689, 291, 584, 11, 286, 13041, 257, 1154, 11, 309, 311, 15108, 12, 265, 1759, 292, 11, 309, 311, 1269, 293, 1908, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13792536475441672, "compression_ratio": 1.5668789808917198, "no_speech_prob": 0.19117195904254913}, {"id": 180, "seek": 150048, "start": 1500.48, "end": 1513.48, "text": " If you pre-register it, you're perfect. If you can motivate them with something real, it will be applied. And I just want that to be much, much more in the forefront of people's minds as they communicate.", "tokens": [50364, 759, 291, 659, 12, 3375, 1964, 309, 11, 291, 434, 2176, 13, 759, 291, 393, 28497, 552, 365, 746, 957, 11, 309, 486, 312, 6456, 13, 400, 286, 445, 528, 300, 281, 312, 709, 11, 709, 544, 294, 264, 27287, 295, 561, 311, 9634, 382, 436, 7890, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13521895201309866, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.26523953676223755}, {"id": 181, "seek": 150048, "start": 1516.48, "end": 1525.48, "text": " Okay. We've only got a couple more minutes. Will, would you and Stephanie just like to wind up the discussion and then we'll hand back to JJ to run the next talk.", "tokens": [51164, 1033, 13, 492, 600, 787, 658, 257, 1916, 544, 2077, 13, 3099, 11, 576, 291, 293, 18634, 445, 411, 281, 2468, 493, 264, 5017, 293, 550, 321, 603, 1011, 646, 281, 21386, 281, 1190, 264, 958, 751, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13521895201309866, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.26523953676223755}, {"id": 182, "seek": 152548, "start": 1526.48, "end": 1543.48, "text": " Just a quick comment on what you just said. It was always something we struggled with, or I struggled with when I was a neuroscience researcher, because there's quite a lot of not quite so good neuroscience researchers who don't do what they should and pre-register experiments.", "tokens": [50414, 1449, 257, 1702, 2871, 322, 437, 291, 445, 848, 13, 467, 390, 1009, 746, 321, 19023, 365, 11, 420, 286, 19023, 365, 562, 286, 390, 257, 42762, 21751, 11, 570, 456, 311, 1596, 257, 688, 295, 406, 1596, 370, 665, 42762, 10309, 567, 500, 380, 360, 437, 436, 820, 293, 659, 12, 3375, 1964, 12050, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1618848482767741, "compression_ratio": 1.5885714285714285, "no_speech_prob": 0.14740392565727234}, {"id": 183, "seek": 154348, "start": 1543.48, "end": 1554.48, "text": " And they just sort of make it up as they go along and that causes a lot of problems. And I really like this idea of pre-register everything first and then when everything's pre-registered, then do it and then publish it no matter what. I think it's quite important.", "tokens": [50364, 400, 436, 445, 1333, 295, 652, 309, 493, 382, 436, 352, 2051, 293, 300, 7700, 257, 688, 295, 2740, 13, 400, 286, 534, 411, 341, 1558, 295, 659, 12, 3375, 1964, 1203, 700, 293, 550, 562, 1203, 311, 659, 12, 3375, 1964, 292, 11, 550, 360, 309, 293, 550, 11374, 309, 572, 1871, 437, 13, 286, 519, 309, 311, 1596, 1021, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2573275228184978, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.24175205826759338}, {"id": 184, "seek": 154348, "start": 1557.48, "end": 1561.48, "text": " Yes. And do you want to just wrap up your bit on education?", "tokens": [51064, 1079, 13, 400, 360, 291, 528, 281, 445, 7019, 493, 428, 857, 322, 3309, 30, 51264], "temperature": 0.0, "avg_logprob": -0.2573275228184978, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.24175205826759338}, {"id": 185, "seek": 154348, "start": 1561.48, "end": 1562.48, "text": " Sorry.", "tokens": [51264, 4919, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2573275228184978, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.24175205826759338}, {"id": 186, "seek": 154348, "start": 1562.48, "end": 1564.48, "text": " Just quickly wrap up your bit on education.", "tokens": [51314, 1449, 2661, 7019, 493, 428, 857, 322, 3309, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2573275228184978, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.24175205826759338}, {"id": 187, "seek": 154348, "start": 1567.48, "end": 1572.48, "text": " Okay. I think we're, okay. I think we're done. Okay. We're done. Thank you very much indeed.", "tokens": [51564, 1033, 13, 286, 519, 321, 434, 11, 1392, 13, 286, 519, 321, 434, 1096, 13, 1033, 13, 492, 434, 1096, 13, 1044, 291, 588, 709, 6451, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2573275228184978, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.24175205826759338}, {"id": 188, "seek": 157248, "start": 1572.48, "end": 1573.48, "text": " Bye. Thank you.", "tokens": [50364, 4621, 13, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2590591311454773, "compression_ratio": 0.6521739130434783, "no_speech_prob": 0.738457977771759}], "language": "en"}