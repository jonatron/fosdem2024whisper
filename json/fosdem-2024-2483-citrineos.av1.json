{"text": " Hello? All right. I'm going to start my talk. My name is Christian. I'm a software developer at a company called S44. We make software for ChargePoint operators and mobility service providers. So basically the cloud space of the EV stuff. All right. And today I'm going to talk about OCBP implementation and the clicker works. It worked a second ago. All right. So if you take a look around at chargers and charging networks, what you'll find often is a broken charger, a charger with a black screen, and especially payment terminals saying oops. I found a study from 2022 that said in the US that 75% or less than 75% of the chargers weren't working or when the users came up, they couldn't get a charge started. So now governments have gotten involved, right? There is uptime guarantees in the UK and the US Neve funding also relies on uptime guarantees. If I remember correctly, I think the off here also has uptime guarantee, but I'm not 100%. And so the most recent thing I found for the US and the company I work at is mainly US based. So that's why there's a little focus there is that in 2023 broken chargers was like the major concern for users to use public infrastructure for charging. And then maybe most importantly, Reddit users are super unhappy. I think some subreddits even banned talking about broken chargers because they were really annoying. And I'm going to click. All right. So one thing that we found or our thoughts on why this happens is a lot of proprietary implementations. So you can see Dolly's interpretation of OCBP proprietary stuff. So if you're not Tesla, which owns the entire vertical, right, they know what's happening at the charging station in the car and in the cloud. Then what do you do? Well, what happens right now is there is a bunch of different vendors. Wherever you sneeze in the EV charging cloud stuff, there's a different vendor. And most of them don't really share what's happening under the hood, which results in, well, a bunch of uncalled for behavior unknown what's about to happen, especially later in the field when it's a user interacting with something and you don't have known input. Then of course, we have OCBP 1.6, which leaves up a lot of stuff to the imagination on when which message should be sent. And then maybe the CSMS thinks, well, I'm expecting ID token now, but get some other message. But one thing that I think is one of the most big problems with OCBP 1.6 is around monitoring. So right now, each hardware vendor builds in their own obscure monitoring messages. And if you want to integrate with like five different hardware vendors, well, then you have to work out how to understand all five different messages. And that basically mean the same thing. Well, that leads to broken parts in the fields and no one knowing about them, which then leads to Reddit users being angry because the charging station has been broken for like a week. No one really noticed. Thanks. All right. So what can we do to improve the state? Well, OCBP 2.01, I think is already a huge step in the right direction. You can see Dolly thinks as well. OCBP 2.01 winning strongly. One thing that I really like about OCBP 2.01 is it has a lot of use cases and it's super structured and you can build your test cases on them. And then of course, there's much more monitoring around the device model that helps in identifying, oh, there's something about to go wrong with the charger instead of just it's broken. But that still doesn't help with transparency. So if everyone just reinvents the wheel once again, just like with 1.6, well, you're still going to run into different interpretations. So we think there should be something that's open source, that's transparent, that you know what's happening under the hood. And we hope that with something like that, there is better cross compatibility between different vendors and the CSMSs can easily integrate with a bunch of different hardware vendors. And next one. All right. So we looked around. We didn't find something that we were super happy with. So we came up with the project Citrine open source. It's written in TypeScript. I know in this room that might not be the most popular choice, but on the internet it is. So that's why we went with it. It runs on Node. We have a API based modular architecture. So similar to what Achim was saying, there's some microservices and you can set it up that, for instance, transactions is super scalable, but maybe provisioning is maybe not as needed. It's released under the Apache 2 license. And most recently it's been adopted by the Linux Foundation Energy. And it's in their hands now. Yeah. So in general, we think OCPP shouldn't be like something that everyone works on once again and again, but like a stable cornerstone that you can adopt, that you can drop into what you want, where you need it. Because the messages are there, the protocol is really specified and redoing the same thing. Well, I can spend my time better. So taking a quick look at what we envision for the system architecture and how it works right now, going from the left to the right. So charging stations connect via WebSockets to the central system that helps us with scalability. You can have a bunch of different instances of the central system that manage the individual chargers. Then we publish on a message broker. What was important to us is to have our underlying technology kind of agnostic. So you can set up Kafka, you can set up PubSub, whatever you want. Just like with memory cache. So you can use your address in memory cache. At least that's what we've implemented for now. And then you can adapt whatever interface you want. And for relational databases right now, we have it hooked up to PostgreSQL, but you can set up whatever relational database you want. Then comes down here, the maybe more interesting part. So we have our modules. And like I mentioned, transactions is a big one. Most of the bandwidth goes there. So we set up the modules based on how much we think they're used. One second, one back. One thing I forgot to mention is we use Fastify as the web framework to interact with our setup. All right. So looking one step further under the hood, we have a JSON schema generation JavaScript that we take the set up, the part three of the OZP spec and use that to validate all incoming and outgoing messages. And we generate our TypeScript interfaces out of that. Then to run, for the implementation of the modules, we work a lot with decorators and metadata on which decorator is used for which message. And that's how we route the messages within the modules. And then one thing that I think is quite nice is that we have some open API documentation that's generated. And you can easily try out some OZP messages from the REST API. So you can either use the API generation, click try, or use postman and just straight up send OZP messages that then get forwarded to the charger. And our system does the interaction with the charger for it. All right. So then looking up and looking at a UI, so right now we've hooked it up to Directis, which is an open source project that gives you some nice UI on top of a relational database that helps with keeping it simple. But you can go crazy on it. You can build your own flows in Directis and do whatever complex things you want. For now, we have it set up so that we have a little testing set up with our app that we whipped up to try charging. Yeah. All right. So where are we at right now? So a few days ago, we released the 1.0 version that goes through the OCTP protocol's testing cases of core and advanced security. We're quite happy that that's working. It's been working for a while, but we only got to release recently. Then right now we're under development is the advanced device management and advanced UI. We also have a few other people that we're talking to about integrating some payment and just general, we've generated quite some buzz with people that would like to add some modules or add just on functionality. And so moving forward from there, we're looking to ISO 1511.8 support. And hopefully in July, that's what we anticipate is that we have the full OCP 201 implemented. And then for the future, of course, similar to what Ahim was saying, well, you can build on your BI tools or whatnot. And we hope that this is a nice interface for innovation on top of and not that you have to hook yourself as a machine in the middle or something similar. And I'm really happy that so many people were interested in this topic. So maybe you also want to contribute. We're fairly fresh. You can find us on GitHub. The top right is QR code to our Citroen OS core GitHub page. The first technical steering committee will happen on March 14th. So get involved, join, bring ideas. And we have a Discord server. So drop by and ask questions. Sometimes we're fast. Sometimes we're slow, depending on our workload in responding. All right. Does anyone have questions? One simple question. We all know every vendor does its own shit. On the other hand, you generate everything from the JSON schema. So how do you implement extensibility? When a message or an unknown message comes in, do you drop it or can you handle it in a smarter way knowing, okay, it's coming from this vendor and therefore I should interpret it somehow? So right now I believe we drop it. Our major taste has been the Everest. And they send normal messages. Am I in the wrong spot? All right. And for the detail on how it will be handled in the future, I'll get back to you on Discord for that. I got to check with a few people on what's happening, what's going to happen there. So you said you can make an API call and you send the, for example, start charging message to the charger. So do you use like then you get the API call, you use Kafka or something and then from Kafka it goes to the charging station? Okay, that's very cool. I'm also doing that. Yeah, exactly. I've seen implementations where they are just white. I've seen implementations where they are just white like a flag into a database that is like very, very important time. And I think that's very ugly. And I think like message brokers are very elegant solution. Yep, we agree. Okay. With message brokers and 15118 you have very strict timing. How do you ensure that your message brokers not too slow? I got a, I got a pun that one. I'm too nervous for that right now. I'm sorry.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 1.0, "text": " Hello?", "tokens": [50364, 2425, 30, 50414], "temperature": 0.0, "avg_logprob": -0.3184114743585456, "compression_ratio": 1.3681318681318682, "no_speech_prob": 0.10158645361661911}, {"id": 1, "seek": 0, "start": 1.0, "end": 2.0, "text": " All right.", "tokens": [50414, 1057, 558, 13, 50464], "temperature": 0.0, "avg_logprob": -0.3184114743585456, "compression_ratio": 1.3681318681318682, "no_speech_prob": 0.10158645361661911}, {"id": 2, "seek": 0, "start": 2.0, "end": 5.96, "text": " I'm going to start my talk.", "tokens": [50464, 286, 478, 516, 281, 722, 452, 751, 13, 50662], "temperature": 0.0, "avg_logprob": -0.3184114743585456, "compression_ratio": 1.3681318681318682, "no_speech_prob": 0.10158645361661911}, {"id": 3, "seek": 0, "start": 5.96, "end": 10.92, "text": " My name is Christian.", "tokens": [50662, 1222, 1315, 307, 5778, 13, 50910], "temperature": 0.0, "avg_logprob": -0.3184114743585456, "compression_ratio": 1.3681318681318682, "no_speech_prob": 0.10158645361661911}, {"id": 4, "seek": 0, "start": 10.92, "end": 14.56, "text": " I'm a software developer at a company called S44.", "tokens": [50910, 286, 478, 257, 4722, 10754, 412, 257, 2237, 1219, 318, 13912, 13, 51092], "temperature": 0.0, "avg_logprob": -0.3184114743585456, "compression_ratio": 1.3681318681318682, "no_speech_prob": 0.10158645361661911}, {"id": 5, "seek": 0, "start": 14.56, "end": 19.78, "text": " We make software for ChargePoint operators and mobility service providers.", "tokens": [51092, 492, 652, 4722, 337, 40546, 18705, 19077, 293, 16199, 2643, 11330, 13, 51353], "temperature": 0.0, "avg_logprob": -0.3184114743585456, "compression_ratio": 1.3681318681318682, "no_speech_prob": 0.10158645361661911}, {"id": 6, "seek": 0, "start": 19.78, "end": 24.080000000000002, "text": " So basically the cloud space of the EV stuff.", "tokens": [51353, 407, 1936, 264, 4588, 1901, 295, 264, 15733, 1507, 13, 51568], "temperature": 0.0, "avg_logprob": -0.3184114743585456, "compression_ratio": 1.3681318681318682, "no_speech_prob": 0.10158645361661911}, {"id": 7, "seek": 0, "start": 24.080000000000002, "end": 26.04, "text": " All right.", "tokens": [51568, 1057, 558, 13, 51666], "temperature": 0.0, "avg_logprob": -0.3184114743585456, "compression_ratio": 1.3681318681318682, "no_speech_prob": 0.10158645361661911}, {"id": 8, "seek": 2604, "start": 26.04, "end": 35.16, "text": " And today I'm going to talk about OCBP implementation and the clicker works.", "tokens": [50364, 400, 965, 286, 478, 516, 281, 751, 466, 42278, 33, 47, 11420, 293, 264, 2052, 260, 1985, 13, 50820], "temperature": 0.0, "avg_logprob": -0.2073191147816332, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.0013008808018639684}, {"id": 9, "seek": 2604, "start": 35.16, "end": 38.04, "text": " It worked a second ago.", "tokens": [50820, 467, 2732, 257, 1150, 2057, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2073191147816332, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.0013008808018639684}, {"id": 10, "seek": 2604, "start": 38.04, "end": 40.56, "text": " All right.", "tokens": [50964, 1057, 558, 13, 51090], "temperature": 0.0, "avg_logprob": -0.2073191147816332, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.0013008808018639684}, {"id": 11, "seek": 2604, "start": 40.56, "end": 47.239999999999995, "text": " So if you take a look around at chargers and charging networks, what you'll find often", "tokens": [51090, 407, 498, 291, 747, 257, 574, 926, 412, 1290, 9458, 293, 11379, 9590, 11, 437, 291, 603, 915, 2049, 51424], "temperature": 0.0, "avg_logprob": -0.2073191147816332, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.0013008808018639684}, {"id": 12, "seek": 2604, "start": 47.239999999999995, "end": 52.28, "text": " is a broken charger, a charger with a black screen, and especially payment terminals", "tokens": [51424, 307, 257, 5463, 22213, 11, 257, 22213, 365, 257, 2211, 2568, 11, 293, 2318, 10224, 38579, 51676], "temperature": 0.0, "avg_logprob": -0.2073191147816332, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.0013008808018639684}, {"id": 13, "seek": 2604, "start": 52.28, "end": 55.32, "text": " saying oops.", "tokens": [51676, 1566, 34166, 13, 51828], "temperature": 0.0, "avg_logprob": -0.2073191147816332, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.0013008808018639684}, {"id": 14, "seek": 5532, "start": 55.32, "end": 64.0, "text": " I found a study from 2022 that said in the US that 75% or less than 75% of the chargers", "tokens": [50364, 286, 1352, 257, 2979, 490, 20229, 300, 848, 294, 264, 2546, 300, 9562, 4, 420, 1570, 813, 9562, 4, 295, 264, 1290, 9458, 50798], "temperature": 0.0, "avg_logprob": -0.1595264599647051, "compression_ratio": 1.5147058823529411, "no_speech_prob": 0.0011644409969449043}, {"id": 15, "seek": 5532, "start": 64.0, "end": 70.16, "text": " weren't working or when the users came up, they couldn't get a charge started.", "tokens": [50798, 4999, 380, 1364, 420, 562, 264, 5022, 1361, 493, 11, 436, 2809, 380, 483, 257, 4602, 1409, 13, 51106], "temperature": 0.0, "avg_logprob": -0.1595264599647051, "compression_ratio": 1.5147058823529411, "no_speech_prob": 0.0011644409969449043}, {"id": 16, "seek": 5532, "start": 70.16, "end": 73.28, "text": " So now governments have gotten involved, right?", "tokens": [51106, 407, 586, 11280, 362, 5768, 3288, 11, 558, 30, 51262], "temperature": 0.0, "avg_logprob": -0.1595264599647051, "compression_ratio": 1.5147058823529411, "no_speech_prob": 0.0011644409969449043}, {"id": 17, "seek": 5532, "start": 73.28, "end": 82.16, "text": " There is uptime guarantees in the UK and the US Neve funding also relies on uptime guarantees.", "tokens": [51262, 821, 307, 493, 3766, 32567, 294, 264, 7051, 293, 264, 2546, 1734, 303, 6137, 611, 30910, 322, 493, 3766, 32567, 13, 51706], "temperature": 0.0, "avg_logprob": -0.1595264599647051, "compression_ratio": 1.5147058823529411, "no_speech_prob": 0.0011644409969449043}, {"id": 18, "seek": 8216, "start": 82.16, "end": 87.0, "text": " If I remember correctly, I think the off here also has uptime guarantee, but I'm not", "tokens": [50364, 759, 286, 1604, 8944, 11, 286, 519, 264, 766, 510, 611, 575, 493, 3766, 10815, 11, 457, 286, 478, 406, 50606], "temperature": 0.0, "avg_logprob": -0.17424485875272203, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.005784069653600454}, {"id": 19, "seek": 8216, "start": 87.0, "end": 89.84, "text": " 100%.", "tokens": [50606, 2319, 6856, 50748], "temperature": 0.0, "avg_logprob": -0.17424485875272203, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.005784069653600454}, {"id": 20, "seek": 8216, "start": 89.84, "end": 97.8, "text": " And so the most recent thing I found for the US and the company I work at is mainly US based.", "tokens": [50748, 400, 370, 264, 881, 5162, 551, 286, 1352, 337, 264, 2546, 293, 264, 2237, 286, 589, 412, 307, 8704, 2546, 2361, 13, 51146], "temperature": 0.0, "avg_logprob": -0.17424485875272203, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.005784069653600454}, {"id": 21, "seek": 8216, "start": 97.8, "end": 105.0, "text": " So that's why there's a little focus there is that in 2023 broken chargers was like the", "tokens": [51146, 407, 300, 311, 983, 456, 311, 257, 707, 1879, 456, 307, 300, 294, 44377, 5463, 1290, 9458, 390, 411, 264, 51506], "temperature": 0.0, "avg_logprob": -0.17424485875272203, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.005784069653600454}, {"id": 22, "seek": 8216, "start": 105.0, "end": 112.12, "text": " major concern for users to use public infrastructure for charging.", "tokens": [51506, 2563, 3136, 337, 5022, 281, 764, 1908, 6896, 337, 11379, 13, 51862], "temperature": 0.0, "avg_logprob": -0.17424485875272203, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.005784069653600454}, {"id": 23, "seek": 11212, "start": 112.12, "end": 115.64, "text": " And then maybe most importantly, Reddit users are super unhappy.", "tokens": [50364, 400, 550, 1310, 881, 8906, 11, 32210, 5022, 366, 1687, 22172, 13, 50540], "temperature": 0.0, "avg_logprob": -0.19997407213042054, "compression_ratio": 1.4757281553398058, "no_speech_prob": 0.008330415934324265}, {"id": 24, "seek": 11212, "start": 115.64, "end": 120.56, "text": " I think some subreddits even banned talking about broken chargers because they were really", "tokens": [50540, 286, 519, 512, 1422, 986, 67, 1208, 754, 19564, 1417, 466, 5463, 1290, 9458, 570, 436, 645, 534, 50786], "temperature": 0.0, "avg_logprob": -0.19997407213042054, "compression_ratio": 1.4757281553398058, "no_speech_prob": 0.008330415934324265}, {"id": 25, "seek": 11212, "start": 120.56, "end": 125.96000000000001, "text": " annoying.", "tokens": [50786, 11304, 13, 51056], "temperature": 0.0, "avg_logprob": -0.19997407213042054, "compression_ratio": 1.4757281553398058, "no_speech_prob": 0.008330415934324265}, {"id": 26, "seek": 11212, "start": 125.96000000000001, "end": 130.24, "text": " And I'm going to click.", "tokens": [51056, 400, 286, 478, 516, 281, 2052, 13, 51270], "temperature": 0.0, "avg_logprob": -0.19997407213042054, "compression_ratio": 1.4757281553398058, "no_speech_prob": 0.008330415934324265}, {"id": 27, "seek": 11212, "start": 130.24, "end": 131.24, "text": " All right.", "tokens": [51270, 1057, 558, 13, 51320], "temperature": 0.0, "avg_logprob": -0.19997407213042054, "compression_ratio": 1.4757281553398058, "no_speech_prob": 0.008330415934324265}, {"id": 28, "seek": 11212, "start": 131.24, "end": 138.16, "text": " So one thing that we found or our thoughts on why this happens is a lot of proprietary", "tokens": [51320, 407, 472, 551, 300, 321, 1352, 420, 527, 4598, 322, 983, 341, 2314, 307, 257, 688, 295, 38992, 51666], "temperature": 0.0, "avg_logprob": -0.19997407213042054, "compression_ratio": 1.4757281553398058, "no_speech_prob": 0.008330415934324265}, {"id": 29, "seek": 11212, "start": 138.16, "end": 139.88, "text": " implementations.", "tokens": [51666, 4445, 763, 13, 51752], "temperature": 0.0, "avg_logprob": -0.19997407213042054, "compression_ratio": 1.4757281553398058, "no_speech_prob": 0.008330415934324265}, {"id": 30, "seek": 13988, "start": 139.88, "end": 147.32, "text": " So you can see Dolly's interpretation of OCBP proprietary stuff.", "tokens": [50364, 407, 291, 393, 536, 1144, 13020, 311, 14174, 295, 42278, 33, 47, 38992, 1507, 13, 50736], "temperature": 0.0, "avg_logprob": -0.17008370769267178, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.035353682935237885}, {"id": 31, "seek": 13988, "start": 147.32, "end": 152.44, "text": " So if you're not Tesla, which owns the entire vertical, right, they know what's happening", "tokens": [50736, 407, 498, 291, 434, 406, 13666, 11, 597, 19143, 264, 2302, 9429, 11, 558, 11, 436, 458, 437, 311, 2737, 50992], "temperature": 0.0, "avg_logprob": -0.17008370769267178, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.035353682935237885}, {"id": 32, "seek": 13988, "start": 152.44, "end": 155.56, "text": " at the charging station in the car and in the cloud.", "tokens": [50992, 412, 264, 11379, 5214, 294, 264, 1032, 293, 294, 264, 4588, 13, 51148], "temperature": 0.0, "avg_logprob": -0.17008370769267178, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.035353682935237885}, {"id": 33, "seek": 13988, "start": 155.56, "end": 156.96, "text": " Then what do you do?", "tokens": [51148, 1396, 437, 360, 291, 360, 30, 51218], "temperature": 0.0, "avg_logprob": -0.17008370769267178, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.035353682935237885}, {"id": 34, "seek": 13988, "start": 156.96, "end": 162.35999999999999, "text": " Well, what happens right now is there is a bunch of different vendors.", "tokens": [51218, 1042, 11, 437, 2314, 558, 586, 307, 456, 307, 257, 3840, 295, 819, 22056, 13, 51488], "temperature": 0.0, "avg_logprob": -0.17008370769267178, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.035353682935237885}, {"id": 35, "seek": 13988, "start": 162.35999999999999, "end": 168.44, "text": " Wherever you sneeze in the EV charging cloud stuff, there's a different vendor.", "tokens": [51488, 30903, 291, 50076, 294, 264, 15733, 11379, 4588, 1507, 11, 456, 311, 257, 819, 24321, 13, 51792], "temperature": 0.0, "avg_logprob": -0.17008370769267178, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.035353682935237885}, {"id": 36, "seek": 16844, "start": 168.44, "end": 173.56, "text": " And most of them don't really share what's happening under the hood, which results in,", "tokens": [50364, 400, 881, 295, 552, 500, 380, 534, 2073, 437, 311, 2737, 833, 264, 13376, 11, 597, 3542, 294, 11, 50620], "temperature": 0.0, "avg_logprob": -0.16109025347363817, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0691213384270668}, {"id": 37, "seek": 16844, "start": 173.56, "end": 180.64, "text": " well, a bunch of uncalled for behavior unknown what's about to happen, especially later in", "tokens": [50620, 731, 11, 257, 3840, 295, 6219, 8907, 337, 5223, 9841, 437, 311, 466, 281, 1051, 11, 2318, 1780, 294, 50974], "temperature": 0.0, "avg_logprob": -0.16109025347363817, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0691213384270668}, {"id": 38, "seek": 16844, "start": 180.64, "end": 188.16, "text": " the field when it's a user interacting with something and you don't have known input.", "tokens": [50974, 264, 2519, 562, 309, 311, 257, 4195, 18017, 365, 746, 293, 291, 500, 380, 362, 2570, 4846, 13, 51350], "temperature": 0.0, "avg_logprob": -0.16109025347363817, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0691213384270668}, {"id": 39, "seek": 16844, "start": 188.16, "end": 194.52, "text": " Then of course, we have OCBP 1.6, which leaves up a lot of stuff to the imagination on when", "tokens": [51350, 1396, 295, 1164, 11, 321, 362, 42278, 33, 47, 502, 13, 21, 11, 597, 5510, 493, 257, 688, 295, 1507, 281, 264, 12938, 322, 562, 51668], "temperature": 0.0, "avg_logprob": -0.16109025347363817, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0691213384270668}, {"id": 40, "seek": 19452, "start": 194.52, "end": 197.04000000000002, "text": " which message should be sent.", "tokens": [50364, 597, 3636, 820, 312, 2279, 13, 50490], "temperature": 0.0, "avg_logprob": -0.11699143804685035, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.04507347196340561}, {"id": 41, "seek": 19452, "start": 197.04000000000002, "end": 203.48000000000002, "text": " And then maybe the CSMS thinks, well, I'm expecting ID token now, but get some other", "tokens": [50490, 400, 550, 1310, 264, 9460, 10288, 7309, 11, 731, 11, 286, 478, 9650, 7348, 14862, 586, 11, 457, 483, 512, 661, 50812], "temperature": 0.0, "avg_logprob": -0.11699143804685035, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.04507347196340561}, {"id": 42, "seek": 19452, "start": 203.48000000000002, "end": 205.92000000000002, "text": " message.", "tokens": [50812, 3636, 13, 50934], "temperature": 0.0, "avg_logprob": -0.11699143804685035, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.04507347196340561}, {"id": 43, "seek": 19452, "start": 205.92000000000002, "end": 213.96, "text": " But one thing that I think is one of the most big problems with OCBP 1.6 is around monitoring.", "tokens": [50934, 583, 472, 551, 300, 286, 519, 307, 472, 295, 264, 881, 955, 2740, 365, 42278, 33, 47, 502, 13, 21, 307, 926, 11028, 13, 51336], "temperature": 0.0, "avg_logprob": -0.11699143804685035, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.04507347196340561}, {"id": 44, "seek": 19452, "start": 213.96, "end": 219.52, "text": " So right now, each hardware vendor builds in their own obscure monitoring messages.", "tokens": [51336, 407, 558, 586, 11, 1184, 8837, 24321, 15182, 294, 641, 1065, 34443, 11028, 7897, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11699143804685035, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.04507347196340561}, {"id": 45, "seek": 19452, "start": 219.52, "end": 223.12, "text": " And if you want to integrate with like five different hardware vendors, well, then you", "tokens": [51614, 400, 498, 291, 528, 281, 13365, 365, 411, 1732, 819, 8837, 22056, 11, 731, 11, 550, 291, 51794], "temperature": 0.0, "avg_logprob": -0.11699143804685035, "compression_ratio": 1.5748987854251013, "no_speech_prob": 0.04507347196340561}, {"id": 46, "seek": 22312, "start": 223.12, "end": 227.88, "text": " have to work out how to understand all five different messages.", "tokens": [50364, 362, 281, 589, 484, 577, 281, 1223, 439, 1732, 819, 7897, 13, 50602], "temperature": 0.0, "avg_logprob": -0.2555261395641209, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.016126753762364388}, {"id": 47, "seek": 22312, "start": 227.88, "end": 230.56, "text": " And that basically mean the same thing.", "tokens": [50602, 400, 300, 1936, 914, 264, 912, 551, 13, 50736], "temperature": 0.0, "avg_logprob": -0.2555261395641209, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.016126753762364388}, {"id": 48, "seek": 22312, "start": 230.56, "end": 236.20000000000002, "text": " Well, that leads to broken parts in the fields and no one knowing about them, which then", "tokens": [50736, 1042, 11, 300, 6689, 281, 5463, 3166, 294, 264, 7909, 293, 572, 472, 5276, 466, 552, 11, 597, 550, 51018], "temperature": 0.0, "avg_logprob": -0.2555261395641209, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.016126753762364388}, {"id": 49, "seek": 22312, "start": 236.20000000000002, "end": 240.24, "text": " leads to Reddit users being angry because the charging station has been broken for like", "tokens": [51018, 6689, 281, 32210, 5022, 885, 6884, 570, 264, 11379, 5214, 575, 668, 5463, 337, 411, 51220], "temperature": 0.0, "avg_logprob": -0.2555261395641209, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.016126753762364388}, {"id": 50, "seek": 22312, "start": 240.24, "end": 241.24, "text": " a week.", "tokens": [51220, 257, 1243, 13, 51270], "temperature": 0.0, "avg_logprob": -0.2555261395641209, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.016126753762364388}, {"id": 51, "seek": 22312, "start": 241.24, "end": 243.68, "text": " No one really noticed.", "tokens": [51270, 883, 472, 534, 5694, 13, 51392], "temperature": 0.0, "avg_logprob": -0.2555261395641209, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.016126753762364388}, {"id": 52, "seek": 22312, "start": 243.68, "end": 245.68, "text": " Thanks.", "tokens": [51392, 2561, 13, 51492], "temperature": 0.0, "avg_logprob": -0.2555261395641209, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.016126753762364388}, {"id": 53, "seek": 22312, "start": 245.68, "end": 248.72, "text": " All right.", "tokens": [51492, 1057, 558, 13, 51644], "temperature": 0.0, "avg_logprob": -0.2555261395641209, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.016126753762364388}, {"id": 54, "seek": 22312, "start": 248.72, "end": 251.52, "text": " So what can we do to improve the state?", "tokens": [51644, 407, 437, 393, 321, 360, 281, 3470, 264, 1785, 30, 51784], "temperature": 0.0, "avg_logprob": -0.2555261395641209, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.016126753762364388}, {"id": 55, "seek": 25152, "start": 251.96, "end": 256.96000000000004, "text": " Well, OCBP 2.01, I think is already a huge step in the right direction.", "tokens": [50386, 1042, 11, 42278, 33, 47, 568, 13, 10607, 11, 286, 519, 307, 1217, 257, 2603, 1823, 294, 264, 558, 3513, 13, 50636], "temperature": 0.0, "avg_logprob": -0.1723751491970486, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.014694004319608212}, {"id": 56, "seek": 25152, "start": 256.96000000000004, "end": 259.48, "text": " You can see Dolly thinks as well.", "tokens": [50636, 509, 393, 536, 1144, 13020, 7309, 382, 731, 13, 50762], "temperature": 0.0, "avg_logprob": -0.1723751491970486, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.014694004319608212}, {"id": 57, "seek": 25152, "start": 259.48, "end": 264.08, "text": " OCBP 2.01 winning strongly.", "tokens": [50762, 42278, 33, 47, 568, 13, 10607, 8224, 10613, 13, 50992], "temperature": 0.0, "avg_logprob": -0.1723751491970486, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.014694004319608212}, {"id": 58, "seek": 25152, "start": 264.08, "end": 269.28000000000003, "text": " One thing that I really like about OCBP 2.01 is it has a lot of use cases and it's super", "tokens": [50992, 1485, 551, 300, 286, 534, 411, 466, 42278, 33, 47, 568, 13, 10607, 307, 309, 575, 257, 688, 295, 764, 3331, 293, 309, 311, 1687, 51252], "temperature": 0.0, "avg_logprob": -0.1723751491970486, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.014694004319608212}, {"id": 59, "seek": 25152, "start": 269.28000000000003, "end": 273.64, "text": " structured and you can build your test cases on them.", "tokens": [51252, 18519, 293, 291, 393, 1322, 428, 1500, 3331, 322, 552, 13, 51470], "temperature": 0.0, "avg_logprob": -0.1723751491970486, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.014694004319608212}, {"id": 60, "seek": 25152, "start": 273.64, "end": 279.56, "text": " And then of course, there's much more monitoring around the device model that helps in identifying,", "tokens": [51470, 400, 550, 295, 1164, 11, 456, 311, 709, 544, 11028, 926, 264, 4302, 2316, 300, 3665, 294, 16696, 11, 51766], "temperature": 0.0, "avg_logprob": -0.1723751491970486, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.014694004319608212}, {"id": 61, "seek": 27956, "start": 279.6, "end": 286.32, "text": " oh, there's something about to go wrong with the charger instead of just it's broken.", "tokens": [50366, 1954, 11, 456, 311, 746, 466, 281, 352, 2085, 365, 264, 22213, 2602, 295, 445, 309, 311, 5463, 13, 50702], "temperature": 0.0, "avg_logprob": -0.12094928279067531, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.003999000880867243}, {"id": 62, "seek": 27956, "start": 286.32, "end": 289.76, "text": " But that still doesn't help with transparency.", "tokens": [50702, 583, 300, 920, 1177, 380, 854, 365, 17131, 13, 50874], "temperature": 0.0, "avg_logprob": -0.12094928279067531, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.003999000880867243}, {"id": 63, "seek": 27956, "start": 289.76, "end": 296.04, "text": " So if everyone just reinvents the wheel once again, just like with 1.6, well, you're still", "tokens": [50874, 407, 498, 1518, 445, 6561, 85, 791, 264, 5589, 1564, 797, 11, 445, 411, 365, 502, 13, 21, 11, 731, 11, 291, 434, 920, 51188], "temperature": 0.0, "avg_logprob": -0.12094928279067531, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.003999000880867243}, {"id": 64, "seek": 27956, "start": 296.04, "end": 299.0, "text": " going to run into different interpretations.", "tokens": [51188, 516, 281, 1190, 666, 819, 37547, 13, 51336], "temperature": 0.0, "avg_logprob": -0.12094928279067531, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.003999000880867243}, {"id": 65, "seek": 27956, "start": 299.0, "end": 303.52, "text": " So we think there should be something that's open source, that's transparent, that you", "tokens": [51336, 407, 321, 519, 456, 820, 312, 746, 300, 311, 1269, 4009, 11, 300, 311, 12737, 11, 300, 291, 51562], "temperature": 0.0, "avg_logprob": -0.12094928279067531, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.003999000880867243}, {"id": 66, "seek": 27956, "start": 303.52, "end": 306.08, "text": " know what's happening under the hood.", "tokens": [51562, 458, 437, 311, 2737, 833, 264, 13376, 13, 51690], "temperature": 0.0, "avg_logprob": -0.12094928279067531, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.003999000880867243}, {"id": 67, "seek": 30608, "start": 306.12, "end": 311.8, "text": " And we hope that with something like that, there is better cross compatibility between", "tokens": [50366, 400, 321, 1454, 300, 365, 746, 411, 300, 11, 456, 307, 1101, 3278, 34237, 1296, 50650], "temperature": 0.0, "avg_logprob": -0.2670938968658447, "compression_ratio": 1.5466101694915255, "no_speech_prob": 0.0022484136279672384}, {"id": 68, "seek": 30608, "start": 311.8, "end": 316.59999999999997, "text": " different vendors and the CSMSs can easily integrate with a bunch of different hardware", "tokens": [50650, 819, 22056, 293, 264, 9460, 10288, 82, 393, 3612, 13365, 365, 257, 3840, 295, 819, 8837, 50890], "temperature": 0.0, "avg_logprob": -0.2670938968658447, "compression_ratio": 1.5466101694915255, "no_speech_prob": 0.0022484136279672384}, {"id": 69, "seek": 30608, "start": 316.59999999999997, "end": 317.59999999999997, "text": " vendors.", "tokens": [50890, 22056, 13, 50940], "temperature": 0.0, "avg_logprob": -0.2670938968658447, "compression_ratio": 1.5466101694915255, "no_speech_prob": 0.0022484136279672384}, {"id": 70, "seek": 30608, "start": 317.59999999999997, "end": 320.59999999999997, "text": " And next one.", "tokens": [50940, 400, 958, 472, 13, 51090], "temperature": 0.0, "avg_logprob": -0.2670938968658447, "compression_ratio": 1.5466101694915255, "no_speech_prob": 0.0022484136279672384}, {"id": 71, "seek": 30608, "start": 320.59999999999997, "end": 321.84, "text": " All right.", "tokens": [51090, 1057, 558, 13, 51152], "temperature": 0.0, "avg_logprob": -0.2670938968658447, "compression_ratio": 1.5466101694915255, "no_speech_prob": 0.0022484136279672384}, {"id": 72, "seek": 30608, "start": 321.84, "end": 323.88, "text": " So we looked around.", "tokens": [51152, 407, 321, 2956, 926, 13, 51254], "temperature": 0.0, "avg_logprob": -0.2670938968658447, "compression_ratio": 1.5466101694915255, "no_speech_prob": 0.0022484136279672384}, {"id": 73, "seek": 30608, "start": 323.88, "end": 326.59999999999997, "text": " We didn't find something that we were super happy with.", "tokens": [51254, 492, 994, 380, 915, 746, 300, 321, 645, 1687, 2055, 365, 13, 51390], "temperature": 0.0, "avg_logprob": -0.2670938968658447, "compression_ratio": 1.5466101694915255, "no_speech_prob": 0.0022484136279672384}, {"id": 74, "seek": 30608, "start": 326.59999999999997, "end": 331.12, "text": " So we came up with the project Citrine open source.", "tokens": [51390, 407, 321, 1361, 493, 365, 264, 1716, 18435, 15140, 1269, 4009, 13, 51616], "temperature": 0.0, "avg_logprob": -0.2670938968658447, "compression_ratio": 1.5466101694915255, "no_speech_prob": 0.0022484136279672384}, {"id": 75, "seek": 30608, "start": 331.12, "end": 332.47999999999996, "text": " It's written in TypeScript.", "tokens": [51616, 467, 311, 3720, 294, 15576, 14237, 13, 51684], "temperature": 0.0, "avg_logprob": -0.2670938968658447, "compression_ratio": 1.5466101694915255, "no_speech_prob": 0.0022484136279672384}, {"id": 76, "seek": 33248, "start": 332.52000000000004, "end": 338.0, "text": " I know in this room that might not be the most popular choice, but on the internet it is.", "tokens": [50366, 286, 458, 294, 341, 1808, 300, 1062, 406, 312, 264, 881, 3743, 3922, 11, 457, 322, 264, 4705, 309, 307, 13, 50640], "temperature": 0.0, "avg_logprob": -0.1853923207705783, "compression_ratio": 1.5230125523012552, "no_speech_prob": 0.0016661762492731214}, {"id": 77, "seek": 33248, "start": 338.0, "end": 339.68, "text": " So that's why we went with it.", "tokens": [50640, 407, 300, 311, 983, 321, 1437, 365, 309, 13, 50724], "temperature": 0.0, "avg_logprob": -0.1853923207705783, "compression_ratio": 1.5230125523012552, "no_speech_prob": 0.0016661762492731214}, {"id": 78, "seek": 33248, "start": 339.68, "end": 342.08000000000004, "text": " It runs on Node.", "tokens": [50724, 467, 6676, 322, 38640, 13, 50844], "temperature": 0.0, "avg_logprob": -0.1853923207705783, "compression_ratio": 1.5230125523012552, "no_speech_prob": 0.0016661762492731214}, {"id": 79, "seek": 33248, "start": 342.08000000000004, "end": 345.20000000000005, "text": " We have a API based modular architecture.", "tokens": [50844, 492, 362, 257, 9362, 2361, 31111, 9482, 13, 51000], "temperature": 0.0, "avg_logprob": -0.1853923207705783, "compression_ratio": 1.5230125523012552, "no_speech_prob": 0.0016661762492731214}, {"id": 80, "seek": 33248, "start": 345.20000000000005, "end": 352.48, "text": " So similar to what Achim was saying, there's some microservices and you can set it up that,", "tokens": [51000, 407, 2531, 281, 437, 15847, 332, 390, 1566, 11, 456, 311, 512, 15547, 47480, 293, 291, 393, 992, 309, 493, 300, 11, 51364], "temperature": 0.0, "avg_logprob": -0.1853923207705783, "compression_ratio": 1.5230125523012552, "no_speech_prob": 0.0016661762492731214}, {"id": 81, "seek": 33248, "start": 352.48, "end": 360.36, "text": " for instance, transactions is super scalable, but maybe provisioning is maybe not as needed.", "tokens": [51364, 337, 5197, 11, 16856, 307, 1687, 38481, 11, 457, 1310, 17225, 278, 307, 1310, 406, 382, 2978, 13, 51758], "temperature": 0.0, "avg_logprob": -0.1853923207705783, "compression_ratio": 1.5230125523012552, "no_speech_prob": 0.0016661762492731214}, {"id": 82, "seek": 36036, "start": 360.36, "end": 363.56, "text": " It's released under the Apache 2 license.", "tokens": [50364, 467, 311, 4736, 833, 264, 46597, 568, 10476, 13, 50524], "temperature": 0.0, "avg_logprob": -0.2216992974281311, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.004588566720485687}, {"id": 83, "seek": 36036, "start": 363.56, "end": 368.16, "text": " And most recently it's been adopted by the Linux Foundation Energy.", "tokens": [50524, 400, 881, 3938, 309, 311, 668, 12175, 538, 264, 18734, 10335, 14939, 13, 50754], "temperature": 0.0, "avg_logprob": -0.2216992974281311, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.004588566720485687}, {"id": 84, "seek": 36036, "start": 368.16, "end": 371.44, "text": " And it's in their hands now.", "tokens": [50754, 400, 309, 311, 294, 641, 2377, 586, 13, 50918], "temperature": 0.0, "avg_logprob": -0.2216992974281311, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.004588566720485687}, {"id": 85, "seek": 36036, "start": 371.44, "end": 373.44, "text": " Yeah.", "tokens": [50918, 865, 13, 51018], "temperature": 0.0, "avg_logprob": -0.2216992974281311, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.004588566720485687}, {"id": 86, "seek": 36036, "start": 373.44, "end": 379.52000000000004, "text": " So in general, we think OCPP shouldn't be like something that everyone works on once again", "tokens": [51018, 407, 294, 2674, 11, 321, 519, 422, 20049, 47, 4659, 380, 312, 411, 746, 300, 1518, 1985, 322, 1564, 797, 51322], "temperature": 0.0, "avg_logprob": -0.2216992974281311, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.004588566720485687}, {"id": 87, "seek": 36036, "start": 379.52000000000004, "end": 385.68, "text": " and again, but like a stable cornerstone that you can adopt, that you can drop into what", "tokens": [51322, 293, 797, 11, 457, 411, 257, 8351, 4538, 11243, 300, 291, 393, 6878, 11, 300, 291, 393, 3270, 666, 437, 51630], "temperature": 0.0, "avg_logprob": -0.2216992974281311, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.004588566720485687}, {"id": 88, "seek": 36036, "start": 385.68, "end": 388.24, "text": " you want, where you need it.", "tokens": [51630, 291, 528, 11, 689, 291, 643, 309, 13, 51758], "temperature": 0.0, "avg_logprob": -0.2216992974281311, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.004588566720485687}, {"id": 89, "seek": 38824, "start": 388.24, "end": 393.92, "text": " Because the messages are there, the protocol is really specified and redoing the same thing.", "tokens": [50364, 1436, 264, 7897, 366, 456, 11, 264, 10336, 307, 534, 22206, 293, 29956, 278, 264, 912, 551, 13, 50648], "temperature": 0.0, "avg_logprob": -0.1716299495477786, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.006262434180825949}, {"id": 90, "seek": 38824, "start": 393.92, "end": 398.04, "text": " Well, I can spend my time better.", "tokens": [50648, 1042, 11, 286, 393, 3496, 452, 565, 1101, 13, 50854], "temperature": 0.0, "avg_logprob": -0.1716299495477786, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.006262434180825949}, {"id": 91, "seek": 38824, "start": 398.04, "end": 403.32, "text": " So taking a quick look at what we envision for the system architecture and how it works", "tokens": [50854, 407, 1940, 257, 1702, 574, 412, 437, 321, 24739, 337, 264, 1185, 9482, 293, 577, 309, 1985, 51118], "temperature": 0.0, "avg_logprob": -0.1716299495477786, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.006262434180825949}, {"id": 92, "seek": 38824, "start": 403.32, "end": 406.40000000000003, "text": " right now, going from the left to the right.", "tokens": [51118, 558, 586, 11, 516, 490, 264, 1411, 281, 264, 558, 13, 51272], "temperature": 0.0, "avg_logprob": -0.1716299495477786, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.006262434180825949}, {"id": 93, "seek": 38824, "start": 406.40000000000003, "end": 415.68, "text": " So charging stations connect via WebSockets to the central system that helps us with scalability.", "tokens": [51272, 407, 11379, 13390, 1745, 5766, 9573, 50, 1560, 1385, 281, 264, 5777, 1185, 300, 3665, 505, 365, 15664, 2310, 13, 51736], "temperature": 0.0, "avg_logprob": -0.1716299495477786, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.006262434180825949}, {"id": 94, "seek": 41568, "start": 415.68, "end": 420.56, "text": " You can have a bunch of different instances of the central system that manage the individual", "tokens": [50364, 509, 393, 362, 257, 3840, 295, 819, 14519, 295, 264, 5777, 1185, 300, 3067, 264, 2609, 50608], "temperature": 0.0, "avg_logprob": -0.1813784863086457, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.09837512671947479}, {"id": 95, "seek": 41568, "start": 420.56, "end": 421.96, "text": " chargers.", "tokens": [50608, 1290, 9458, 13, 50678], "temperature": 0.0, "avg_logprob": -0.1813784863086457, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.09837512671947479}, {"id": 96, "seek": 41568, "start": 421.96, "end": 424.68, "text": " Then we publish on a message broker.", "tokens": [50678, 1396, 321, 11374, 322, 257, 3636, 26502, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1813784863086457, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.09837512671947479}, {"id": 97, "seek": 41568, "start": 424.68, "end": 431.96000000000004, "text": " What was important to us is to have our underlying technology kind of agnostic.", "tokens": [50814, 708, 390, 1021, 281, 505, 307, 281, 362, 527, 14217, 2899, 733, 295, 623, 77, 19634, 13, 51178], "temperature": 0.0, "avg_logprob": -0.1813784863086457, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.09837512671947479}, {"id": 98, "seek": 41568, "start": 431.96000000000004, "end": 438.96000000000004, "text": " So you can set up Kafka, you can set up PubSub, whatever you want.", "tokens": [51178, 407, 291, 393, 992, 493, 47064, 11, 291, 393, 992, 493, 21808, 39582, 11, 2035, 291, 528, 13, 51528], "temperature": 0.0, "avg_logprob": -0.1813784863086457, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.09837512671947479}, {"id": 99, "seek": 41568, "start": 438.96000000000004, "end": 441.08, "text": " Just like with memory cache.", "tokens": [51528, 1449, 411, 365, 4675, 19459, 13, 51634], "temperature": 0.0, "avg_logprob": -0.1813784863086457, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.09837512671947479}, {"id": 100, "seek": 41568, "start": 441.08, "end": 443.0, "text": " So you can use your address in memory cache.", "tokens": [51634, 407, 291, 393, 764, 428, 2985, 294, 4675, 19459, 13, 51730], "temperature": 0.0, "avg_logprob": -0.1813784863086457, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.09837512671947479}, {"id": 101, "seek": 44300, "start": 443.12, "end": 445.72, "text": " At least that's what we've implemented for now.", "tokens": [50370, 1711, 1935, 300, 311, 437, 321, 600, 12270, 337, 586, 13, 50500], "temperature": 0.0, "avg_logprob": -0.15467395969465667, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.005980643909424543}, {"id": 102, "seek": 44300, "start": 445.72, "end": 448.56, "text": " And then you can adapt whatever interface you want.", "tokens": [50500, 400, 550, 291, 393, 6231, 2035, 9226, 291, 528, 13, 50642], "temperature": 0.0, "avg_logprob": -0.15467395969465667, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.005980643909424543}, {"id": 103, "seek": 44300, "start": 448.56, "end": 453.64, "text": " And for relational databases right now, we have it hooked up to PostgreSQL, but you can", "tokens": [50642, 400, 337, 38444, 22380, 558, 586, 11, 321, 362, 309, 20410, 493, 281, 10223, 33248, 39934, 11, 457, 291, 393, 50896], "temperature": 0.0, "avg_logprob": -0.15467395969465667, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.005980643909424543}, {"id": 104, "seek": 44300, "start": 453.64, "end": 457.36, "text": " set up whatever relational database you want.", "tokens": [50896, 992, 493, 2035, 38444, 8149, 291, 528, 13, 51082], "temperature": 0.0, "avg_logprob": -0.15467395969465667, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.005980643909424543}, {"id": 105, "seek": 44300, "start": 457.36, "end": 460.88, "text": " Then comes down here, the maybe more interesting part.", "tokens": [51082, 1396, 1487, 760, 510, 11, 264, 1310, 544, 1880, 644, 13, 51258], "temperature": 0.0, "avg_logprob": -0.15467395969465667, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.005980643909424543}, {"id": 106, "seek": 44300, "start": 460.88, "end": 463.2, "text": " So we have our modules.", "tokens": [51258, 407, 321, 362, 527, 16679, 13, 51374], "temperature": 0.0, "avg_logprob": -0.15467395969465667, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.005980643909424543}, {"id": 107, "seek": 44300, "start": 463.2, "end": 467.8, "text": " And like I mentioned, transactions is a big one.", "tokens": [51374, 400, 411, 286, 2835, 11, 16856, 307, 257, 955, 472, 13, 51604], "temperature": 0.0, "avg_logprob": -0.15467395969465667, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.005980643909424543}, {"id": 108, "seek": 44300, "start": 467.8, "end": 469.48, "text": " Most of the bandwidth goes there.", "tokens": [51604, 4534, 295, 264, 23647, 1709, 456, 13, 51688], "temperature": 0.0, "avg_logprob": -0.15467395969465667, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.005980643909424543}, {"id": 109, "seek": 46948, "start": 469.48, "end": 473.64000000000004, "text": " So we set up the modules based on how much we think they're used.", "tokens": [50364, 407, 321, 992, 493, 264, 16679, 2361, 322, 577, 709, 321, 519, 436, 434, 1143, 13, 50572], "temperature": 0.0, "avg_logprob": -0.23160967073942484, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.001863726880401373}, {"id": 110, "seek": 46948, "start": 473.64000000000004, "end": 481.24, "text": " One second, one back.", "tokens": [50572, 1485, 1150, 11, 472, 646, 13, 50952], "temperature": 0.0, "avg_logprob": -0.23160967073942484, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.001863726880401373}, {"id": 111, "seek": 46948, "start": 481.24, "end": 485.56, "text": " One thing I forgot to mention is we use Fastify as the web framework to interact with our", "tokens": [50952, 1485, 551, 286, 5298, 281, 2152, 307, 321, 764, 15968, 2505, 382, 264, 3670, 8388, 281, 4648, 365, 527, 51168], "temperature": 0.0, "avg_logprob": -0.23160967073942484, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.001863726880401373}, {"id": 112, "seek": 46948, "start": 485.56, "end": 486.56, "text": " setup.", "tokens": [51168, 8657, 13, 51218], "temperature": 0.0, "avg_logprob": -0.23160967073942484, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.001863726880401373}, {"id": 113, "seek": 46948, "start": 486.56, "end": 489.08000000000004, "text": " All right.", "tokens": [51218, 1057, 558, 13, 51344], "temperature": 0.0, "avg_logprob": -0.23160967073942484, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.001863726880401373}, {"id": 114, "seek": 46948, "start": 489.08000000000004, "end": 497.96000000000004, "text": " So looking one step further under the hood, we have a JSON schema generation JavaScript", "tokens": [51344, 407, 1237, 472, 1823, 3052, 833, 264, 13376, 11, 321, 362, 257, 31828, 34078, 5125, 15778, 51788], "temperature": 0.0, "avg_logprob": -0.23160967073942484, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.001863726880401373}, {"id": 115, "seek": 49796, "start": 497.96, "end": 505.0, "text": " that we take the set up, the part three of the OZP spec and use that to validate all incoming", "tokens": [50364, 300, 321, 747, 264, 992, 493, 11, 264, 644, 1045, 295, 264, 422, 57, 47, 1608, 293, 764, 300, 281, 29562, 439, 22341, 50716], "temperature": 0.0, "avg_logprob": -0.20473687192226978, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.031002700328826904}, {"id": 116, "seek": 49796, "start": 505.0, "end": 506.96, "text": " and outgoing messages.", "tokens": [50716, 293, 41565, 7897, 13, 50814], "temperature": 0.0, "avg_logprob": -0.20473687192226978, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.031002700328826904}, {"id": 117, "seek": 49796, "start": 506.96, "end": 511.71999999999997, "text": " And we generate our TypeScript interfaces out of that.", "tokens": [50814, 400, 321, 8460, 527, 15576, 14237, 28416, 484, 295, 300, 13, 51052], "temperature": 0.0, "avg_logprob": -0.20473687192226978, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.031002700328826904}, {"id": 118, "seek": 49796, "start": 511.71999999999997, "end": 517.3199999999999, "text": " Then to run, for the implementation of the modules, we work a lot with decorators and", "tokens": [51052, 1396, 281, 1190, 11, 337, 264, 11420, 295, 264, 16679, 11, 321, 589, 257, 688, 365, 7919, 3391, 293, 51332], "temperature": 0.0, "avg_logprob": -0.20473687192226978, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.031002700328826904}, {"id": 119, "seek": 49796, "start": 517.3199999999999, "end": 521.8, "text": " metadata on which decorator is used for which message.", "tokens": [51332, 26603, 322, 597, 7919, 1639, 307, 1143, 337, 597, 3636, 13, 51556], "temperature": 0.0, "avg_logprob": -0.20473687192226978, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.031002700328826904}, {"id": 120, "seek": 49796, "start": 521.8, "end": 526.28, "text": " And that's how we route the messages within the modules.", "tokens": [51556, 400, 300, 311, 577, 321, 7955, 264, 7897, 1951, 264, 16679, 13, 51780], "temperature": 0.0, "avg_logprob": -0.20473687192226978, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.031002700328826904}, {"id": 121, "seek": 52628, "start": 526.28, "end": 533.64, "text": " And then one thing that I think is quite nice is that we have some open API documentation", "tokens": [50364, 400, 550, 472, 551, 300, 286, 519, 307, 1596, 1481, 307, 300, 321, 362, 512, 1269, 9362, 14333, 50732], "temperature": 0.0, "avg_logprob": -0.16373754389145795, "compression_ratio": 1.5288461538461537, "no_speech_prob": 0.024375278502702713}, {"id": 122, "seek": 52628, "start": 533.64, "end": 535.36, "text": " that's generated.", "tokens": [50732, 300, 311, 10833, 13, 50818], "temperature": 0.0, "avg_logprob": -0.16373754389145795, "compression_ratio": 1.5288461538461537, "no_speech_prob": 0.024375278502702713}, {"id": 123, "seek": 52628, "start": 535.36, "end": 541.76, "text": " And you can easily try out some OZP messages from the REST API.", "tokens": [50818, 400, 291, 393, 3612, 853, 484, 512, 422, 57, 47, 7897, 490, 264, 497, 14497, 9362, 13, 51138], "temperature": 0.0, "avg_logprob": -0.16373754389145795, "compression_ratio": 1.5288461538461537, "no_speech_prob": 0.024375278502702713}, {"id": 124, "seek": 52628, "start": 541.76, "end": 547.66, "text": " So you can either use the API generation, click try, or use postman and just straight", "tokens": [51138, 407, 291, 393, 2139, 764, 264, 9362, 5125, 11, 2052, 853, 11, 420, 764, 2183, 1601, 293, 445, 2997, 51433], "temperature": 0.0, "avg_logprob": -0.16373754389145795, "compression_ratio": 1.5288461538461537, "no_speech_prob": 0.024375278502702713}, {"id": 125, "seek": 52628, "start": 547.66, "end": 551.24, "text": " up send OZP messages that then get forwarded to the charger.", "tokens": [51433, 493, 2845, 422, 57, 47, 7897, 300, 550, 483, 2128, 292, 281, 264, 22213, 13, 51612], "temperature": 0.0, "avg_logprob": -0.16373754389145795, "compression_ratio": 1.5288461538461537, "no_speech_prob": 0.024375278502702713}, {"id": 126, "seek": 55124, "start": 551.24, "end": 556.36, "text": " And our system does the interaction with the charger for it.", "tokens": [50364, 400, 527, 1185, 775, 264, 9285, 365, 264, 22213, 337, 309, 13, 50620], "temperature": 0.0, "avg_logprob": -0.17377743266877674, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.09651566296815872}, {"id": 127, "seek": 55124, "start": 556.36, "end": 558.4, "text": " All right.", "tokens": [50620, 1057, 558, 13, 50722], "temperature": 0.0, "avg_logprob": -0.17377743266877674, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.09651566296815872}, {"id": 128, "seek": 55124, "start": 558.4, "end": 564.04, "text": " So then looking up and looking at a UI, so right now we've hooked it up to Directis, which", "tokens": [50722, 407, 550, 1237, 493, 293, 1237, 412, 257, 15682, 11, 370, 558, 586, 321, 600, 20410, 309, 493, 281, 18308, 271, 11, 597, 51004], "temperature": 0.0, "avg_logprob": -0.17377743266877674, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.09651566296815872}, {"id": 129, "seek": 55124, "start": 564.04, "end": 572.32, "text": " is an open source project that gives you some nice UI on top of a relational database that", "tokens": [51004, 307, 364, 1269, 4009, 1716, 300, 2709, 291, 512, 1481, 15682, 322, 1192, 295, 257, 38444, 8149, 300, 51418], "temperature": 0.0, "avg_logprob": -0.17377743266877674, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.09651566296815872}, {"id": 130, "seek": 55124, "start": 572.32, "end": 575.6, "text": " helps with keeping it simple.", "tokens": [51418, 3665, 365, 5145, 309, 2199, 13, 51582], "temperature": 0.0, "avg_logprob": -0.17377743266877674, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.09651566296815872}, {"id": 131, "seek": 55124, "start": 575.6, "end": 577.12, "text": " But you can go crazy on it.", "tokens": [51582, 583, 291, 393, 352, 3219, 322, 309, 13, 51658], "temperature": 0.0, "avg_logprob": -0.17377743266877674, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.09651566296815872}, {"id": 132, "seek": 57712, "start": 577.12, "end": 583.44, "text": " You can build your own flows in Directis and do whatever complex things you want.", "tokens": [50364, 509, 393, 1322, 428, 1065, 12867, 294, 18308, 271, 293, 360, 2035, 3997, 721, 291, 528, 13, 50680], "temperature": 0.0, "avg_logprob": -0.2103252410888672, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.011144781485199928}, {"id": 133, "seek": 57712, "start": 583.44, "end": 589.64, "text": " For now, we have it set up so that we have a little testing set up with our app that", "tokens": [50680, 1171, 586, 11, 321, 362, 309, 992, 493, 370, 300, 321, 362, 257, 707, 4997, 992, 493, 365, 527, 724, 300, 50990], "temperature": 0.0, "avg_logprob": -0.2103252410888672, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.011144781485199928}, {"id": 134, "seek": 57712, "start": 589.64, "end": 594.48, "text": " we whipped up to try charging.", "tokens": [50990, 321, 27918, 493, 281, 853, 11379, 13, 51232], "temperature": 0.0, "avg_logprob": -0.2103252410888672, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.011144781485199928}, {"id": 135, "seek": 57712, "start": 594.48, "end": 595.48, "text": " Yeah.", "tokens": [51232, 865, 13, 51282], "temperature": 0.0, "avg_logprob": -0.2103252410888672, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.011144781485199928}, {"id": 136, "seek": 57712, "start": 595.48, "end": 598.2, "text": " All right.", "tokens": [51282, 1057, 558, 13, 51418], "temperature": 0.0, "avg_logprob": -0.2103252410888672, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.011144781485199928}, {"id": 137, "seek": 57712, "start": 598.2, "end": 600.84, "text": " So where are we at right now?", "tokens": [51418, 407, 689, 366, 321, 412, 558, 586, 30, 51550], "temperature": 0.0, "avg_logprob": -0.2103252410888672, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.011144781485199928}, {"id": 138, "seek": 60084, "start": 600.84, "end": 614.12, "text": " So a few days ago, we released the 1.0 version that goes through the OCTP protocol's testing", "tokens": [50364, 407, 257, 1326, 1708, 2057, 11, 321, 4736, 264, 502, 13, 15, 3037, 300, 1709, 807, 264, 422, 10259, 47, 10336, 311, 4997, 51028], "temperature": 0.0, "avg_logprob": -0.20429644951453577, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.023634281009435654}, {"id": 139, "seek": 60084, "start": 614.12, "end": 617.2, "text": " cases of core and advanced security.", "tokens": [51028, 3331, 295, 4965, 293, 7339, 3825, 13, 51182], "temperature": 0.0, "avg_logprob": -0.20429644951453577, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.023634281009435654}, {"id": 140, "seek": 60084, "start": 617.2, "end": 619.76, "text": " We're quite happy that that's working.", "tokens": [51182, 492, 434, 1596, 2055, 300, 300, 311, 1364, 13, 51310], "temperature": 0.0, "avg_logprob": -0.20429644951453577, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.023634281009435654}, {"id": 141, "seek": 60084, "start": 619.76, "end": 624.72, "text": " It's been working for a while, but we only got to release recently.", "tokens": [51310, 467, 311, 668, 1364, 337, 257, 1339, 11, 457, 321, 787, 658, 281, 4374, 3938, 13, 51558], "temperature": 0.0, "avg_logprob": -0.20429644951453577, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.023634281009435654}, {"id": 142, "seek": 62472, "start": 624.72, "end": 630.08, "text": " Then right now we're under development is the advanced device management and advanced", "tokens": [50364, 1396, 558, 586, 321, 434, 833, 3250, 307, 264, 7339, 4302, 4592, 293, 7339, 50632], "temperature": 0.0, "avg_logprob": -0.18094854769499405, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.055721234530210495}, {"id": 143, "seek": 62472, "start": 630.08, "end": 631.32, "text": " UI.", "tokens": [50632, 15682, 13, 50694], "temperature": 0.0, "avg_logprob": -0.18094854769499405, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.055721234530210495}, {"id": 144, "seek": 62472, "start": 631.32, "end": 636.44, "text": " We also have a few other people that we're talking to about integrating some payment", "tokens": [50694, 492, 611, 362, 257, 1326, 661, 561, 300, 321, 434, 1417, 281, 466, 26889, 512, 10224, 50950], "temperature": 0.0, "avg_logprob": -0.18094854769499405, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.055721234530210495}, {"id": 145, "seek": 62472, "start": 636.44, "end": 643.28, "text": " and just general, we've generated quite some buzz with people that would like to add some", "tokens": [50950, 293, 445, 2674, 11, 321, 600, 10833, 1596, 512, 13036, 365, 561, 300, 576, 411, 281, 909, 512, 51292], "temperature": 0.0, "avg_logprob": -0.18094854769499405, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.055721234530210495}, {"id": 146, "seek": 62472, "start": 643.28, "end": 648.52, "text": " modules or add just on functionality.", "tokens": [51292, 16679, 420, 909, 445, 322, 14980, 13, 51554], "temperature": 0.0, "avg_logprob": -0.18094854769499405, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.055721234530210495}, {"id": 147, "seek": 64852, "start": 648.52, "end": 654.12, "text": " And so moving forward from there, we're looking to ISO 1511.8 support.", "tokens": [50364, 400, 370, 2684, 2128, 490, 456, 11, 321, 434, 1237, 281, 25042, 2119, 5348, 13, 23, 1406, 13, 50644], "temperature": 0.0, "avg_logprob": -0.16916568911805444, "compression_ratio": 1.5102880658436213, "no_speech_prob": 0.08190447092056274}, {"id": 148, "seek": 64852, "start": 654.12, "end": 663.04, "text": " And hopefully in July, that's what we anticipate is that we have the full OCP 201 implemented.", "tokens": [50644, 400, 4696, 294, 7370, 11, 300, 311, 437, 321, 21685, 307, 300, 321, 362, 264, 1577, 422, 20049, 1525, 12270, 13, 51090], "temperature": 0.0, "avg_logprob": -0.16916568911805444, "compression_ratio": 1.5102880658436213, "no_speech_prob": 0.08190447092056274}, {"id": 149, "seek": 64852, "start": 663.04, "end": 668.0799999999999, "text": " And then for the future, of course, similar to what Ahim was saying, well, you can build", "tokens": [51090, 400, 550, 337, 264, 2027, 11, 295, 1164, 11, 2531, 281, 437, 2438, 332, 390, 1566, 11, 731, 11, 291, 393, 1322, 51342], "temperature": 0.0, "avg_logprob": -0.16916568911805444, "compression_ratio": 1.5102880658436213, "no_speech_prob": 0.08190447092056274}, {"id": 150, "seek": 64852, "start": 668.0799999999999, "end": 670.76, "text": " on your BI tools or whatnot.", "tokens": [51342, 322, 428, 23524, 3873, 420, 25882, 13, 51476], "temperature": 0.0, "avg_logprob": -0.16916568911805444, "compression_ratio": 1.5102880658436213, "no_speech_prob": 0.08190447092056274}, {"id": 151, "seek": 64852, "start": 670.76, "end": 677.92, "text": " And we hope that this is a nice interface for innovation on top of and not that you", "tokens": [51476, 400, 321, 1454, 300, 341, 307, 257, 1481, 9226, 337, 8504, 322, 1192, 295, 293, 406, 300, 291, 51834], "temperature": 0.0, "avg_logprob": -0.16916568911805444, "compression_ratio": 1.5102880658436213, "no_speech_prob": 0.08190447092056274}, {"id": 152, "seek": 67792, "start": 677.92, "end": 684.64, "text": " have to hook yourself as a machine in the middle or something similar.", "tokens": [50364, 362, 281, 6328, 1803, 382, 257, 3479, 294, 264, 2808, 420, 746, 2531, 13, 50700], "temperature": 0.0, "avg_logprob": -0.21997676318204856, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.007772720884531736}, {"id": 153, "seek": 67792, "start": 684.64, "end": 690.56, "text": " And I'm really happy that so many people were interested in this topic.", "tokens": [50700, 400, 286, 478, 534, 2055, 300, 370, 867, 561, 645, 3102, 294, 341, 4829, 13, 50996], "temperature": 0.0, "avg_logprob": -0.21997676318204856, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.007772720884531736}, {"id": 154, "seek": 67792, "start": 690.56, "end": 693.8399999999999, "text": " So maybe you also want to contribute.", "tokens": [50996, 407, 1310, 291, 611, 528, 281, 10586, 13, 51160], "temperature": 0.0, "avg_logprob": -0.21997676318204856, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.007772720884531736}, {"id": 155, "seek": 67792, "start": 693.8399999999999, "end": 695.4, "text": " We're fairly fresh.", "tokens": [51160, 492, 434, 6457, 4451, 13, 51238], "temperature": 0.0, "avg_logprob": -0.21997676318204856, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.007772720884531736}, {"id": 156, "seek": 67792, "start": 695.4, "end": 697.8, "text": " You can find us on GitHub.", "tokens": [51238, 509, 393, 915, 505, 322, 23331, 13, 51358], "temperature": 0.0, "avg_logprob": -0.21997676318204856, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.007772720884531736}, {"id": 157, "seek": 67792, "start": 697.8, "end": 705.3199999999999, "text": " The top right is QR code to our Citroen OS core GitHub page.", "tokens": [51358, 440, 1192, 558, 307, 32784, 3089, 281, 527, 18435, 340, 268, 12731, 4965, 23331, 3028, 13, 51734], "temperature": 0.0, "avg_logprob": -0.21997676318204856, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.007772720884531736}, {"id": 158, "seek": 70532, "start": 705.32, "end": 709.6400000000001, "text": " The first technical steering committee will happen on March 14th.", "tokens": [50364, 440, 700, 6191, 14823, 7482, 486, 1051, 322, 6129, 3499, 392, 13, 50580], "temperature": 0.0, "avg_logprob": -0.29363441467285156, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.03124191239476204}, {"id": 159, "seek": 70532, "start": 709.6400000000001, "end": 713.8000000000001, "text": " So get involved, join, bring ideas.", "tokens": [50580, 407, 483, 3288, 11, 3917, 11, 1565, 3487, 13, 50788], "temperature": 0.0, "avg_logprob": -0.29363441467285156, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.03124191239476204}, {"id": 160, "seek": 70532, "start": 713.8000000000001, "end": 715.4000000000001, "text": " And we have a Discord server.", "tokens": [50788, 400, 321, 362, 257, 32623, 7154, 13, 50868], "temperature": 0.0, "avg_logprob": -0.29363441467285156, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.03124191239476204}, {"id": 161, "seek": 70532, "start": 715.4000000000001, "end": 718.72, "text": " So drop by and ask questions.", "tokens": [50868, 407, 3270, 538, 293, 1029, 1651, 13, 51034], "temperature": 0.0, "avg_logprob": -0.29363441467285156, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.03124191239476204}, {"id": 162, "seek": 70532, "start": 718.72, "end": 719.72, "text": " Sometimes we're fast.", "tokens": [51034, 4803, 321, 434, 2370, 13, 51084], "temperature": 0.0, "avg_logprob": -0.29363441467285156, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.03124191239476204}, {"id": 163, "seek": 70532, "start": 719.72, "end": 722.6800000000001, "text": " Sometimes we're slow, depending on our workload in responding.", "tokens": [51084, 4803, 321, 434, 2964, 11, 5413, 322, 527, 20139, 294, 16670, 13, 51232], "temperature": 0.0, "avg_logprob": -0.29363441467285156, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.03124191239476204}, {"id": 164, "seek": 70532, "start": 722.6800000000001, "end": 725.6800000000001, "text": " All right.", "tokens": [51232, 1057, 558, 13, 51382], "temperature": 0.0, "avg_logprob": -0.29363441467285156, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.03124191239476204}, {"id": 165, "seek": 70532, "start": 725.6800000000001, "end": 731.6800000000001, "text": " Does anyone have questions?", "tokens": [51382, 4402, 2878, 362, 1651, 30, 51682], "temperature": 0.0, "avg_logprob": -0.29363441467285156, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.03124191239476204}, {"id": 166, "seek": 73168, "start": 731.68, "end": 748.0, "text": " One simple question.", "tokens": [50364, 1485, 2199, 1168, 13, 51180], "temperature": 0.0, "avg_logprob": -0.23110532760620117, "compression_ratio": 1.282442748091603, "no_speech_prob": 0.027846157550811768}, {"id": 167, "seek": 73168, "start": 748.0, "end": 753.28, "text": " We all know every vendor does its own shit.", "tokens": [51180, 492, 439, 458, 633, 24321, 775, 1080, 1065, 4611, 13, 51444], "temperature": 0.0, "avg_logprob": -0.23110532760620117, "compression_ratio": 1.282442748091603, "no_speech_prob": 0.027846157550811768}, {"id": 168, "seek": 73168, "start": 753.28, "end": 757.1999999999999, "text": " On the other hand, you generate everything from the JSON schema.", "tokens": [51444, 1282, 264, 661, 1011, 11, 291, 8460, 1203, 490, 264, 31828, 34078, 13, 51640], "temperature": 0.0, "avg_logprob": -0.23110532760620117, "compression_ratio": 1.282442748091603, "no_speech_prob": 0.027846157550811768}, {"id": 169, "seek": 73168, "start": 757.1999999999999, "end": 760.4399999999999, "text": " So how do you implement extensibility?", "tokens": [51640, 407, 577, 360, 291, 4445, 1279, 694, 2841, 30, 51802], "temperature": 0.0, "avg_logprob": -0.23110532760620117, "compression_ratio": 1.282442748091603, "no_speech_prob": 0.027846157550811768}, {"id": 170, "seek": 76044, "start": 760.44, "end": 766.96, "text": " When a message or an unknown message comes in, do you drop it or can you handle it in", "tokens": [50364, 1133, 257, 3636, 420, 364, 9841, 3636, 1487, 294, 11, 360, 291, 3270, 309, 420, 393, 291, 4813, 309, 294, 50690], "temperature": 0.0, "avg_logprob": -0.3433018820626395, "compression_ratio": 1.458100558659218, "no_speech_prob": 0.11766084283590317}, {"id": 171, "seek": 76044, "start": 766.96, "end": 772.48, "text": " a smarter way knowing, okay, it's coming from this vendor and therefore I should interpret", "tokens": [50690, 257, 20294, 636, 5276, 11, 1392, 11, 309, 311, 1348, 490, 341, 24321, 293, 4412, 286, 820, 7302, 50966], "temperature": 0.0, "avg_logprob": -0.3433018820626395, "compression_ratio": 1.458100558659218, "no_speech_prob": 0.11766084283590317}, {"id": 172, "seek": 76044, "start": 772.48, "end": 775.48, "text": " it somehow?", "tokens": [50966, 309, 6063, 30, 51116], "temperature": 0.0, "avg_logprob": -0.3433018820626395, "compression_ratio": 1.458100558659218, "no_speech_prob": 0.11766084283590317}, {"id": 173, "seek": 76044, "start": 775.48, "end": 778.0400000000001, "text": " So right now I believe we drop it.", "tokens": [51116, 407, 558, 586, 286, 1697, 321, 3270, 309, 13, 51244], "temperature": 0.0, "avg_logprob": -0.3433018820626395, "compression_ratio": 1.458100558659218, "no_speech_prob": 0.11766084283590317}, {"id": 174, "seek": 76044, "start": 778.0400000000001, "end": 782.36, "text": " Our major taste has been the Everest.", "tokens": [51244, 2621, 2563, 3939, 575, 668, 264, 47591, 13, 51460], "temperature": 0.0, "avg_logprob": -0.3433018820626395, "compression_ratio": 1.458100558659218, "no_speech_prob": 0.11766084283590317}, {"id": 175, "seek": 78236, "start": 782.36, "end": 786.4, "text": " And they send normal messages.", "tokens": [50364, 400, 436, 2845, 2710, 7897, 13, 50566], "temperature": 0.0, "avg_logprob": -0.25708765665690103, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.18495291471481323}, {"id": 176, "seek": 78236, "start": 786.4, "end": 790.36, "text": " Am I in the wrong spot?", "tokens": [50566, 2012, 286, 294, 264, 2085, 4008, 30, 50764], "temperature": 0.0, "avg_logprob": -0.25708765665690103, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.18495291471481323}, {"id": 177, "seek": 78236, "start": 790.36, "end": 791.64, "text": " All right.", "tokens": [50764, 1057, 558, 13, 50828], "temperature": 0.0, "avg_logprob": -0.25708765665690103, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.18495291471481323}, {"id": 178, "seek": 78236, "start": 791.64, "end": 797.84, "text": " And for the detail on how it will be handled in the future, I'll get back to you on Discord", "tokens": [50828, 400, 337, 264, 2607, 322, 577, 309, 486, 312, 18033, 294, 264, 2027, 11, 286, 603, 483, 646, 281, 291, 322, 32623, 51138], "temperature": 0.0, "avg_logprob": -0.25708765665690103, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.18495291471481323}, {"id": 179, "seek": 78236, "start": 797.84, "end": 798.84, "text": " for that.", "tokens": [51138, 337, 300, 13, 51188], "temperature": 0.0, "avg_logprob": -0.25708765665690103, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.18495291471481323}, {"id": 180, "seek": 78236, "start": 798.84, "end": 808.44, "text": " I got to check with a few people on what's happening, what's going to happen there.", "tokens": [51188, 286, 658, 281, 1520, 365, 257, 1326, 561, 322, 437, 311, 2737, 11, 437, 311, 516, 281, 1051, 456, 13, 51668], "temperature": 0.0, "avg_logprob": -0.25708765665690103, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.18495291471481323}, {"id": 181, "seek": 80844, "start": 808.44, "end": 814.08, "text": " So you said you can make an API call and you send the, for example, start charging message", "tokens": [50364, 407, 291, 848, 291, 393, 652, 364, 9362, 818, 293, 291, 2845, 264, 11, 337, 1365, 11, 722, 11379, 3636, 50646], "temperature": 0.0, "avg_logprob": -0.3383591538768704, "compression_ratio": 1.8425531914893618, "no_speech_prob": 0.22111360728740692}, {"id": 182, "seek": 80844, "start": 814.08, "end": 815.5600000000001, "text": " to the charger.", "tokens": [50646, 281, 264, 22213, 13, 50720], "temperature": 0.0, "avg_logprob": -0.3383591538768704, "compression_ratio": 1.8425531914893618, "no_speech_prob": 0.22111360728740692}, {"id": 183, "seek": 80844, "start": 815.5600000000001, "end": 821.44, "text": " So do you use like then you get the API call, you use Kafka or something and then from Kafka", "tokens": [50720, 407, 360, 291, 764, 411, 550, 291, 483, 264, 9362, 818, 11, 291, 764, 47064, 420, 746, 293, 550, 490, 47064, 51014], "temperature": 0.0, "avg_logprob": -0.3383591538768704, "compression_ratio": 1.8425531914893618, "no_speech_prob": 0.22111360728740692}, {"id": 184, "seek": 80844, "start": 821.44, "end": 823.4000000000001, "text": " it goes to the charging station?", "tokens": [51014, 309, 1709, 281, 264, 11379, 5214, 30, 51112], "temperature": 0.0, "avg_logprob": -0.3383591538768704, "compression_ratio": 1.8425531914893618, "no_speech_prob": 0.22111360728740692}, {"id": 185, "seek": 80844, "start": 823.4000000000001, "end": 824.84, "text": " Okay, that's very cool.", "tokens": [51112, 1033, 11, 300, 311, 588, 1627, 13, 51184], "temperature": 0.0, "avg_logprob": -0.3383591538768704, "compression_ratio": 1.8425531914893618, "no_speech_prob": 0.22111360728740692}, {"id": 186, "seek": 80844, "start": 824.84, "end": 826.96, "text": " I'm also doing that.", "tokens": [51184, 286, 478, 611, 884, 300, 13, 51290], "temperature": 0.0, "avg_logprob": -0.3383591538768704, "compression_ratio": 1.8425531914893618, "no_speech_prob": 0.22111360728740692}, {"id": 187, "seek": 80844, "start": 826.96, "end": 827.96, "text": " Yeah, exactly.", "tokens": [51290, 865, 11, 2293, 13, 51340], "temperature": 0.0, "avg_logprob": -0.3383591538768704, "compression_ratio": 1.8425531914893618, "no_speech_prob": 0.22111360728740692}, {"id": 188, "seek": 80844, "start": 827.96, "end": 833.08, "text": " I've seen implementations where they are just white.", "tokens": [51340, 286, 600, 1612, 4445, 763, 689, 436, 366, 445, 2418, 13, 51596], "temperature": 0.0, "avg_logprob": -0.3383591538768704, "compression_ratio": 1.8425531914893618, "no_speech_prob": 0.22111360728740692}, {"id": 189, "seek": 80844, "start": 833.08, "end": 837.7600000000001, "text": " I've seen implementations where they are just white like a flag into a database that is", "tokens": [51596, 286, 600, 1612, 4445, 763, 689, 436, 366, 445, 2418, 411, 257, 7166, 666, 257, 8149, 300, 307, 51830], "temperature": 0.0, "avg_logprob": -0.3383591538768704, "compression_ratio": 1.8425531914893618, "no_speech_prob": 0.22111360728740692}, {"id": 190, "seek": 83776, "start": 837.76, "end": 839.3199999999999, "text": " like very, very important time.", "tokens": [50364, 411, 588, 11, 588, 1021, 565, 13, 50442], "temperature": 0.0, "avg_logprob": -0.4704973944302263, "compression_ratio": 1.6077348066298343, "no_speech_prob": 0.007956483401358128}, {"id": 191, "seek": 83776, "start": 839.3199999999999, "end": 840.48, "text": " And I think that's very ugly.", "tokens": [50442, 400, 286, 519, 300, 311, 588, 12246, 13, 50500], "temperature": 0.0, "avg_logprob": -0.4704973944302263, "compression_ratio": 1.6077348066298343, "no_speech_prob": 0.007956483401358128}, {"id": 192, "seek": 83776, "start": 840.48, "end": 844.72, "text": " And I think like message brokers are very elegant solution.", "tokens": [50500, 400, 286, 519, 411, 3636, 47549, 366, 588, 21117, 3827, 13, 50712], "temperature": 0.0, "avg_logprob": -0.4704973944302263, "compression_ratio": 1.6077348066298343, "no_speech_prob": 0.007956483401358128}, {"id": 193, "seek": 83776, "start": 844.72, "end": 847.92, "text": " Yep, we agree.", "tokens": [50712, 7010, 11, 321, 3986, 13, 50872], "temperature": 0.0, "avg_logprob": -0.4704973944302263, "compression_ratio": 1.6077348066298343, "no_speech_prob": 0.007956483401358128}, {"id": 194, "seek": 83776, "start": 847.92, "end": 849.76, "text": " Okay.", "tokens": [50872, 1033, 13, 50964], "temperature": 0.0, "avg_logprob": -0.4704973944302263, "compression_ratio": 1.6077348066298343, "no_speech_prob": 0.007956483401358128}, {"id": 195, "seek": 83776, "start": 849.76, "end": 854.04, "text": " With message brokers and 15118 you have very strict timing.", "tokens": [50964, 2022, 3636, 47549, 293, 2119, 5348, 23, 291, 362, 588, 10910, 10822, 13, 51178], "temperature": 0.0, "avg_logprob": -0.4704973944302263, "compression_ratio": 1.6077348066298343, "no_speech_prob": 0.007956483401358128}, {"id": 196, "seek": 83776, "start": 854.04, "end": 864.08, "text": " How do you ensure that your message brokers not too slow?", "tokens": [51178, 1012, 360, 291, 5586, 300, 428, 3636, 47549, 406, 886, 2964, 30, 51680], "temperature": 0.0, "avg_logprob": -0.4704973944302263, "compression_ratio": 1.6077348066298343, "no_speech_prob": 0.007956483401358128}, {"id": 197, "seek": 83776, "start": 864.08, "end": 866.3199999999999, "text": " I got a, I got a pun that one.", "tokens": [51680, 286, 658, 257, 11, 286, 658, 257, 4468, 300, 472, 13, 51792], "temperature": 0.0, "avg_logprob": -0.4704973944302263, "compression_ratio": 1.6077348066298343, "no_speech_prob": 0.007956483401358128}, {"id": 198, "seek": 86632, "start": 866.32, "end": 868.9200000000001, "text": " I'm too nervous for that right now.", "tokens": [50364, 286, 478, 886, 6296, 337, 300, 558, 586, 13, 50494], "temperature": 0.0, "avg_logprob": -0.47281021542019314, "compression_ratio": 0.8679245283018868, "no_speech_prob": 0.2550256848335266}, {"id": 199, "seek": 86632, "start": 868.9200000000001, "end": 870.44, "text": " I'm sorry.", "tokens": [50494, 286, 478, 2597, 13, 50570], "temperature": 0.0, "avg_logprob": -0.47281021542019314, "compression_ratio": 0.8679245283018868, "no_speech_prob": 0.2550256848335266}], "language": "en"}