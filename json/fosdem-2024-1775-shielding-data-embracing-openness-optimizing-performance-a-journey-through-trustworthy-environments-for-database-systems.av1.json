{"text": " Thank you, thank you everybody for coming. Before we start, I need to say two things. First of all, I'm sorry for this dev room. I'll try to speak as loud as I can, but if you don't see the slides, they are available online. Second, this is a talk about databases. We are database researchers. So first of all, we don't know everything. Second of all, we might also not understand everything, but regardless, we hope to give you a different perspective about this important problem, which is how to store data securely using trusted execution environments as a technology. Sorry. So we are PSD students. We work at CWI Amsterdam, and specifically our research focuses on secure databases. In particular, we do stuff like encrypted query processing, secure multi-party computation, data privacy, and so on. Here our research question is how to protect data in use. In fact, it's very easy to protect data at rest, but we also want to hide it while it's being processed. Our example here is related to cloud. So nowadays it is very common practice to outsource data management to cloud providers, but the thing is we also need to protect information from people who have access to the servers and internal attacks. There are some techniques to analyze data while keeping it encrypted, but like homomorphic encryption for instance, but unfortunately this field doesn't yet have encouraging performance results. So we here need to look for something simpler and more efficient to protect our data while it's being processed. In this talk, of course, we're here. So we talk about trusted execution environments, and we want to employ them as a technology to ensure confidentiality and isolation of the data. But before we start with this, we need to first understand the different technologies and different techniques to split the components of a database system in a trusted execution environment. In this talk, we focus about Intel SGX, and for who didn't know, it's basically a series of hardware instructions to split memory in a secure and insecure part, where the secure part we're going to call Enclave. And in the database field, Intel SGX specifically the first one is a very popular choice for development because it's the most mature one, and there is the most research on it. But at the same time, there are some performance limitations to run workloads that are very typical for database systems. In particular, the biggest problem here is the limited-page cache size, which is 120 megabytes in Intel SGX1. That being said, we're going to explain here many different models to split our DBMS. We have the... does this work? We have the full DBMS split, which means that basically we're going to put all the database inside the Enclave with just a very tiny layer of IO library to handle system calls. Then we have the middle DBMS split, which is something in between. It allows more fine-grained optimization and code splits. Usually approaches just put the query execution engine inside the Enclave, and everything else is going to stay out. And then we have the minimal DBMS split, where only the operators and the comparators are inside the Enclave, where with operators and comparators, I mean plus, minus, equal, and so on. Now we have a general understanding of the different models. We can start with some practical examples. So here's a personal favorite, it's called StelDB, and it's a Postgre's extension. We have some Postgre's people here. I'm very biased on this, but basically, StelDB is employing the third model that I mentioned, which is the minimal DBMS split. So basically it's only implementing operators and comparators. This choice was probably made because of the very limited memory that we can use. And so of course there are some trade-offs. If we do not have the full DBMS, of course there is more information leakage. For instance, people might be able to infer the size of the database and the operations that we are making inside the Enclave. And at the same time, even though the secure part is so limited, the performance is kind of bad. So here we are going to have here 5% to 30% overhead in transactional queries, where transactional queries are workloads that are very heavy in inserts and updates on current data. So, yeah, so this is a very good project, but still not quite what we would like to have if we are running actual real-world workloads. There are more examples here of other databases. We have a lot of implementations of SQLites. And they are, I think all of them are full DBMS split, but regardless, they add at least one or two orders of magnitude of overhead to the queries. We have a MariaDB kind of encrypted database, which is called Ageless. I think I saw some people from Ageless here, or there was a, it was at FOSDEM a few years back. And yeah, Ageless is basically this database that is designed to run inside an Enclave and uses MariaDB and ROXDB storage. It also has encrypted authenticated data in disk and in memory as well, and encrypted network connection. So it's a very nice project. Then we have an implementation of Microsoft SQL Server. I'm sorry about this. It's not open source, I know, but unfortunately it's one of the most relevant works in the field because it actually implements the query engine in the Enclave and splits the data between sensitive and insensitive tables. So it's a very novel idea, but unfortunately it doesn't work because this kind of models also assumes a very big Enclave size and due to the limitations of AgX1, this is not pretty feasible in practice. And then we have one analytical engine where with analytic, I mean doing analytics, so business intelligence workloads on a lot of data and historical data. And yeah, this is called OblityB and it implements Oblivious Physical Operator for analytical processing in the cloud. But yeah, once again, this is really, really slow because of the Enclave size. So our contribution here is taking all of this that we have and we notice two things in here. First of all, the big majority of these implementations on AgX1 are transactional and because analytical workloads really don't scale because of the volume of the data and they overhead called by last level cache misses and EPC swapping. The second problem is that there is no research on SGX2. So SGX2 was released a couple years ago, but still all the prototypes that I mentioned were made for SGX1. I'm not saying they don't work, but I'm saying there are no benchmarks, there are no implementations, so there are specifically tailored for SGX2. So here our contribution is to try and bridge the gap between efficient and secure analytical processing. To do so, we use our database, DacDB. Disclaimer here, we are not affiliated with DacDB, we are not paid by them. It's just an open source database that we happen to use because it's developed in our research center. So DacDB is open source, it's embedded, columnar analytical system, I'm sorry, there are a lot of buzzwords here, I'm going to explain that later. It's in C++11 without additional dependencies and it's actually been ported to SGX1 in 2022 by some, our master student. And before explaining what we did with DacDB and SGX1 and 2, I need to give you some fundamental concepts about database internals. We start here with column storage. So the difference between row and column storage is that basically data in column storage is stored in columns because if you do analytical workloads, we don't need usually all the columns that we have, we just need a few of them. So it is more efficient to store the columns all together such that we can only fetch what we need. And also this kind of column format is also very much, can very much benefit from compression because usually there is a lot of correlation between the data and our data is also huge. So we can definitely implement some sort of compression and DacDB specifically implements column lever compression where data is stored in column and then compressed. Now we also need to talk a little bit about vectorized execution. This is similar to the CMD instructions that you probably know of but applied to databases. So instead of performing operations to one row at a time, we perform it in batches. So instead of having a row fetching it, elaborating it and returning it, we do the same process with batches. So you can see this example, very, very simple query. And here our function next is going to return many tuples rather than one. And we push only the relevant blocks of data up and down the query plan. And this is more efficient because we have less system calls and we can also take advantage of the CPU more efficiently. Now thank you for the attention. Now Lotte is going to explain you how we ported DacDB to SGAX. Thank you, Illa. Okay, so before we go directly to SGAX2 and how we did it, we first are going to pay some attention to how it actually has been done to SGAX1 because the master student, he ported DacDB in two different ways. The first one is the full database management split. The main issue was here of course because of the low memory capacity that not the whole database or not all the data would fit in the enclave. And the second issue here is that system calls are not directly callable inside the enclave. So you either need to reimplement all system calls or the necessary system calls or you need to use some kind of library which maintains this kind of IOS\uc2ec layer. So the master student, he used Graphene. Nowadays, Graphene is actually called Grameen. And I think last year and the year before there were also talks at FOSDEM about Grameen, how this exactly works. So with Grameen and with fully porting DacDB into the enclave, there was a 20 time slowdown actually and this is mainly cause because of the expensive EPC swapping. So to mitigate this, the student tried to instead of keeping all memory buffers inside the enclave, pull some memory buffers outside the enclave and crib them outside the enclave and this way try to run DacDB. And this already gave a significant speed up but still there was a 30 time slowdown. The second approach that he did was the minimal DbMS split. He put basically all the operators inside the enclave and left the rest out of the enclave because this enabled to have factorized processing still and that really increases performance. And a second optimization that he did was replacing Ecos and Ocos by a synchronous request in a shared buffer also called the switches mode I think. And this also helped but still there was a 10 time slowdown. So a couple years later, now there is of course SGX2 and it doesn't suffer from the main memory limitation. So now it's basically easier to port DacDB as a whole to the enclave to SGX2 and we did it also with the use of Grameen which also improved the last years a lot so that made it actually surprisingly easy. And we did some benchmarks to actually see okay so what performance difference is there if you run a database fully inside the enclave. So before going into the results we did the benchmarks with TPCH which is a standard industry benchmark for analytical workloads so basically for data science workloads so there are no inputs inserts or updates but just analytics basically. And we compared it first with Grameen itself because since Grameen replace a system calls it also incurs some overhead but as you can see most overhead is caused by SGX self. On average we would say there is a 10 to 20 percent overhead but here we normalize baseline DacDB so that you actually can see the actual overhead per query and there are some specific queries such as query 12 and query 15 where the overhead is actually more than twice. So this might be a bit problematic. So we did some research we tried to identify okay so what is it in these queries that causes the overhead and we found that mainly strangely enough the overhead is introduced by O-calls so by E-enters and E-exits and we tried to investigate a bit further which system called it then was but there was some kind of timing function that seems to be executed outside of the enclave. And also within these queries there are two times as much page faults and well one optimization that we tried we're still working on it but was increasing the factor size in DacDB because usually in DacDB the factor size consists of 2048 tuples and usually this gives low L1 cache misses but it can incur many EPC calls so with increasing the factor size you basically maximize IO and IO is very expensive in the enclave and we actually found that if you increase the factor size to 16384 that the performance overhead is actually minimized for this workload and a small note is that not for all queries actually the performance improved but just for the queries with a lot of overhead it seems to be really beneficial to increase the factor size in DacDB. So this is very much work in process it's more a prototype than something you can actually use in production so please don't do it yet but we can conclude that analytics can actually perform people from the relatively efficient in SGX2 and the overhead seems to be acceptable but the question now is we can protect data in use so data in secure memory but what about the data in unsecure memory right now because if you go outside the enclave the data is not protected by default and so we will actually need some kind of encryption mechanism and DacDB right now has actually parquet encryption so we are already capable of encrypting parquet files and decrypting them inside the enclave and then perform secure analytics but in the end our goal is to design to build something that is fully functional and that is fully secure actually for users that want to do secure analytics with DacDB. So yeah this is our plan for the future we will of course open source everything but yeah thank you for your attention. Hi thank you for a very nice talk so I was wondering you talked about like this overhead that you were attributing to the old calls going out of the enclave and some of the commercial SGX frameworks use these techniques where you actually batch these together and they are commonly called asynchronous old calls so did you look into that at all and or do you have like some insights how that could affect the performance. Okay so your question basically is if we looked into the asynchronous old calls right or the asynchronous buffer basically well the master student from he looked into that and indeed improved performance we were planning on actually doing some benchmarks with this specific mode but we just didn't do it yet but we are still investigating but as far as I understand it is a little bit less secure to use this mode so yeah it will always be trade off but I suspect that it will improve performance quite a bit so reduced the overhead in the end. Yeah probably a stupid and provocative question have you tried shoving the whole database in a secure instance like 7S and P or TDX and comparing the performance between like SGX and TDX or 7S and P solution. Okay so the question is did we use other secure environments basically the answer is no so we have no performance comparisons yet but the plan is actually to do that indeed because if you want like not everybody is able to run SGX to write so the hardware field is pretty fragmented and we also want to kind of find solutions or at least have comparisons of which one is the best to use and or maybe even made some kind of framework that people can adopt to easily run also on different kind of hardware instructions. Yeah. Thank you I want to ask about the fully secure on the slide. Have you talked about side channels and what's your vision on that? Do you want to answer? Yeah in short yes this is a problem because all the research that we found there is always a trade off between performance and security and literally all the papers build this sort of model like cost model in not in terms of cost but in terms of information leakage. So a lot of people papers just say that yes we acknowledge that there are going to be some trade off some attacks in fact yes yes this is absolutely the case that it can happen but right now the goal was first to have something that is somewhat functional on some sort of database workloads because as I said the big limitations of SGX-1 made the whole thing completely invisible but now that this is actually possible we can also focus on how to fix these issues but unfortunately research tended not to acknowledge this issue so much in the past but for future yes we will.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.08, "text": " Thank you, thank you everybody for coming.", "tokens": [50364, 1044, 291, 11, 1309, 291, 2201, 337, 1348, 13, 50768], "temperature": 0.0, "avg_logprob": -0.1873440424601237, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.1316537857055664}, {"id": 1, "seek": 0, "start": 8.08, "end": 10.36, "text": " Before we start, I need to say two things.", "tokens": [50768, 4546, 321, 722, 11, 286, 643, 281, 584, 732, 721, 13, 50882], "temperature": 0.0, "avg_logprob": -0.1873440424601237, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.1316537857055664}, {"id": 2, "seek": 0, "start": 10.36, "end": 12.200000000000001, "text": " First of all, I'm sorry for this dev room.", "tokens": [50882, 2386, 295, 439, 11, 286, 478, 2597, 337, 341, 1905, 1808, 13, 50974], "temperature": 0.0, "avg_logprob": -0.1873440424601237, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.1316537857055664}, {"id": 3, "seek": 0, "start": 12.200000000000001, "end": 16.68, "text": " I'll try to speak as loud as I can, but if you don't see the slides, they are available", "tokens": [50974, 286, 603, 853, 281, 1710, 382, 6588, 382, 286, 393, 11, 457, 498, 291, 500, 380, 536, 264, 9788, 11, 436, 366, 2435, 51198], "temperature": 0.0, "avg_logprob": -0.1873440424601237, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.1316537857055664}, {"id": 4, "seek": 0, "start": 16.68, "end": 17.68, "text": " online.", "tokens": [51198, 2950, 13, 51248], "temperature": 0.0, "avg_logprob": -0.1873440424601237, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.1316537857055664}, {"id": 5, "seek": 0, "start": 17.68, "end": 19.64, "text": " Second, this is a talk about databases.", "tokens": [51248, 5736, 11, 341, 307, 257, 751, 466, 22380, 13, 51346], "temperature": 0.0, "avg_logprob": -0.1873440424601237, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.1316537857055664}, {"id": 6, "seek": 0, "start": 19.64, "end": 21.28, "text": " We are database researchers.", "tokens": [51346, 492, 366, 8149, 10309, 13, 51428], "temperature": 0.0, "avg_logprob": -0.1873440424601237, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.1316537857055664}, {"id": 7, "seek": 0, "start": 21.28, "end": 23.32, "text": " So first of all, we don't know everything.", "tokens": [51428, 407, 700, 295, 439, 11, 321, 500, 380, 458, 1203, 13, 51530], "temperature": 0.0, "avg_logprob": -0.1873440424601237, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.1316537857055664}, {"id": 8, "seek": 0, "start": 23.32, "end": 28.0, "text": " Second of all, we might also not understand everything, but regardless, we hope to give", "tokens": [51530, 5736, 295, 439, 11, 321, 1062, 611, 406, 1223, 1203, 11, 457, 10060, 11, 321, 1454, 281, 976, 51764], "temperature": 0.0, "avg_logprob": -0.1873440424601237, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.1316537857055664}, {"id": 9, "seek": 2800, "start": 28.0, "end": 32.64, "text": " you a different perspective about this important problem, which is how to store data securely", "tokens": [50364, 291, 257, 819, 4585, 466, 341, 1021, 1154, 11, 597, 307, 577, 281, 3531, 1412, 38348, 50596], "temperature": 0.0, "avg_logprob": -0.2507121239179446, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.008943608961999416}, {"id": 10, "seek": 2800, "start": 32.64, "end": 37.16, "text": " using trusted execution environments as a technology.", "tokens": [50596, 1228, 16034, 15058, 12388, 382, 257, 2899, 13, 50822], "temperature": 0.0, "avg_logprob": -0.2507121239179446, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.008943608961999416}, {"id": 11, "seek": 2800, "start": 37.16, "end": 39.88, "text": " Sorry.", "tokens": [50822, 4919, 13, 50958], "temperature": 0.0, "avg_logprob": -0.2507121239179446, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.008943608961999416}, {"id": 12, "seek": 2800, "start": 39.88, "end": 45.68, "text": " So we are PSD students.", "tokens": [50958, 407, 321, 366, 8168, 35, 1731, 13, 51248], "temperature": 0.0, "avg_logprob": -0.2507121239179446, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.008943608961999416}, {"id": 13, "seek": 2800, "start": 45.68, "end": 51.120000000000005, "text": " We work at CWI Amsterdam, and specifically our research focuses on secure databases.", "tokens": [51248, 492, 589, 412, 383, 54, 40, 28291, 11, 293, 4682, 527, 2132, 16109, 322, 7144, 22380, 13, 51520], "temperature": 0.0, "avg_logprob": -0.2507121239179446, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.008943608961999416}, {"id": 14, "seek": 2800, "start": 51.120000000000005, "end": 55.760000000000005, "text": " In particular, we do stuff like encrypted query processing, secure multi-party computation,", "tokens": [51520, 682, 1729, 11, 321, 360, 1507, 411, 36663, 14581, 9007, 11, 7144, 4825, 12, 23409, 24903, 11, 51752], "temperature": 0.0, "avg_logprob": -0.2507121239179446, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.008943608961999416}, {"id": 15, "seek": 5576, "start": 55.76, "end": 58.32, "text": " data privacy, and so on.", "tokens": [50364, 1412, 11427, 11, 293, 370, 322, 13, 50492], "temperature": 0.0, "avg_logprob": -0.1626221893030569, "compression_ratio": 1.68359375, "no_speech_prob": 0.013438788242638111}, {"id": 16, "seek": 5576, "start": 58.32, "end": 61.519999999999996, "text": " Here our research question is how to protect data in use.", "tokens": [50492, 1692, 527, 2132, 1168, 307, 577, 281, 2371, 1412, 294, 764, 13, 50652], "temperature": 0.0, "avg_logprob": -0.1626221893030569, "compression_ratio": 1.68359375, "no_speech_prob": 0.013438788242638111}, {"id": 17, "seek": 5576, "start": 61.519999999999996, "end": 65.8, "text": " In fact, it's very easy to protect data at rest, but we also want to hide it while it's", "tokens": [50652, 682, 1186, 11, 309, 311, 588, 1858, 281, 2371, 1412, 412, 1472, 11, 457, 321, 611, 528, 281, 6479, 309, 1339, 309, 311, 50866], "temperature": 0.0, "avg_logprob": -0.1626221893030569, "compression_ratio": 1.68359375, "no_speech_prob": 0.013438788242638111}, {"id": 18, "seek": 5576, "start": 65.8, "end": 67.56, "text": " being processed.", "tokens": [50866, 885, 18846, 13, 50954], "temperature": 0.0, "avg_logprob": -0.1626221893030569, "compression_ratio": 1.68359375, "no_speech_prob": 0.013438788242638111}, {"id": 19, "seek": 5576, "start": 67.56, "end": 70.03999999999999, "text": " Our example here is related to cloud.", "tokens": [50954, 2621, 1365, 510, 307, 4077, 281, 4588, 13, 51078], "temperature": 0.0, "avg_logprob": -0.1626221893030569, "compression_ratio": 1.68359375, "no_speech_prob": 0.013438788242638111}, {"id": 20, "seek": 5576, "start": 70.03999999999999, "end": 76.36, "text": " So nowadays it is very common practice to outsource data management to cloud providers,", "tokens": [51078, 407, 13434, 309, 307, 588, 2689, 3124, 281, 14758, 2948, 1412, 4592, 281, 4588, 11330, 11, 51394], "temperature": 0.0, "avg_logprob": -0.1626221893030569, "compression_ratio": 1.68359375, "no_speech_prob": 0.013438788242638111}, {"id": 21, "seek": 5576, "start": 76.36, "end": 81.4, "text": " but the thing is we also need to protect information from people who have access to the servers", "tokens": [51394, 457, 264, 551, 307, 321, 611, 643, 281, 2371, 1589, 490, 561, 567, 362, 2105, 281, 264, 15909, 51646], "temperature": 0.0, "avg_logprob": -0.1626221893030569, "compression_ratio": 1.68359375, "no_speech_prob": 0.013438788242638111}, {"id": 22, "seek": 5576, "start": 81.4, "end": 83.68, "text": " and internal attacks.", "tokens": [51646, 293, 6920, 8122, 13, 51760], "temperature": 0.0, "avg_logprob": -0.1626221893030569, "compression_ratio": 1.68359375, "no_speech_prob": 0.013438788242638111}, {"id": 23, "seek": 8368, "start": 83.68, "end": 89.80000000000001, "text": " There are some techniques to analyze data while keeping it encrypted, but like homomorphic", "tokens": [50364, 821, 366, 512, 7512, 281, 12477, 1412, 1339, 5145, 309, 36663, 11, 457, 411, 3655, 32702, 299, 50670], "temperature": 0.0, "avg_logprob": -0.1322682313244752, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.009521125815808773}, {"id": 24, "seek": 8368, "start": 89.80000000000001, "end": 96.28, "text": " encryption for instance, but unfortunately this field doesn't yet have encouraging performance", "tokens": [50670, 29575, 337, 5197, 11, 457, 7015, 341, 2519, 1177, 380, 1939, 362, 14580, 3389, 50994], "temperature": 0.0, "avg_logprob": -0.1322682313244752, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.009521125815808773}, {"id": 25, "seek": 8368, "start": 96.28, "end": 97.28, "text": " results.", "tokens": [50994, 3542, 13, 51044], "temperature": 0.0, "avg_logprob": -0.1322682313244752, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.009521125815808773}, {"id": 26, "seek": 8368, "start": 97.28, "end": 102.80000000000001, "text": " So we here need to look for something simpler and more efficient to protect our data while", "tokens": [51044, 407, 321, 510, 643, 281, 574, 337, 746, 18587, 293, 544, 7148, 281, 2371, 527, 1412, 1339, 51320], "temperature": 0.0, "avg_logprob": -0.1322682313244752, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.009521125815808773}, {"id": 27, "seek": 8368, "start": 102.80000000000001, "end": 103.80000000000001, "text": " it's being processed.", "tokens": [51320, 309, 311, 885, 18846, 13, 51370], "temperature": 0.0, "avg_logprob": -0.1322682313244752, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.009521125815808773}, {"id": 28, "seek": 8368, "start": 103.80000000000001, "end": 106.32000000000001, "text": " In this talk, of course, we're here.", "tokens": [51370, 682, 341, 751, 11, 295, 1164, 11, 321, 434, 510, 13, 51496], "temperature": 0.0, "avg_logprob": -0.1322682313244752, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.009521125815808773}, {"id": 29, "seek": 8368, "start": 106.32000000000001, "end": 112.32000000000001, "text": " So we talk about trusted execution environments, and we want to employ them as a technology", "tokens": [51496, 407, 321, 751, 466, 16034, 15058, 12388, 11, 293, 321, 528, 281, 3188, 552, 382, 257, 2899, 51796], "temperature": 0.0, "avg_logprob": -0.1322682313244752, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.009521125815808773}, {"id": 30, "seek": 11232, "start": 112.32, "end": 118.44, "text": " to ensure confidentiality and isolation of the data.", "tokens": [50364, 281, 5586, 27054, 507, 293, 16001, 295, 264, 1412, 13, 50670], "temperature": 0.0, "avg_logprob": -0.16210188614694696, "compression_ratio": 1.6484375, "no_speech_prob": 0.011259379796683788}, {"id": 31, "seek": 11232, "start": 118.44, "end": 122.83999999999999, "text": " But before we start with this, we need to first understand the different technologies", "tokens": [50670, 583, 949, 321, 722, 365, 341, 11, 321, 643, 281, 700, 1223, 264, 819, 7943, 50890], "temperature": 0.0, "avg_logprob": -0.16210188614694696, "compression_ratio": 1.6484375, "no_speech_prob": 0.011259379796683788}, {"id": 32, "seek": 11232, "start": 122.83999999999999, "end": 127.91999999999999, "text": " and different techniques to split the components of a database system in a trusted execution", "tokens": [50890, 293, 819, 7512, 281, 7472, 264, 6677, 295, 257, 8149, 1185, 294, 257, 16034, 15058, 51144], "temperature": 0.0, "avg_logprob": -0.16210188614694696, "compression_ratio": 1.6484375, "no_speech_prob": 0.011259379796683788}, {"id": 33, "seek": 11232, "start": 127.91999999999999, "end": 129.07999999999998, "text": " environment.", "tokens": [51144, 2823, 13, 51202], "temperature": 0.0, "avg_logprob": -0.16210188614694696, "compression_ratio": 1.6484375, "no_speech_prob": 0.011259379796683788}, {"id": 34, "seek": 11232, "start": 129.07999999999998, "end": 136.4, "text": " In this talk, we focus about Intel SGX, and for who didn't know, it's basically a series", "tokens": [51202, 682, 341, 751, 11, 321, 1879, 466, 19762, 34520, 55, 11, 293, 337, 567, 994, 380, 458, 11, 309, 311, 1936, 257, 2638, 51568], "temperature": 0.0, "avg_logprob": -0.16210188614694696, "compression_ratio": 1.6484375, "no_speech_prob": 0.011259379796683788}, {"id": 35, "seek": 11232, "start": 136.4, "end": 141.32, "text": " of hardware instructions to split memory in a secure and insecure part, where the secure", "tokens": [51568, 295, 8837, 9415, 281, 7472, 4675, 294, 257, 7144, 293, 32215, 644, 11, 689, 264, 7144, 51814], "temperature": 0.0, "avg_logprob": -0.16210188614694696, "compression_ratio": 1.6484375, "no_speech_prob": 0.011259379796683788}, {"id": 36, "seek": 14132, "start": 141.32, "end": 144.2, "text": " part we're going to call Enclave.", "tokens": [50364, 644, 321, 434, 516, 281, 818, 29584, 27995, 13, 50508], "temperature": 0.0, "avg_logprob": -0.1909598263827237, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.02568887732923031}, {"id": 37, "seek": 14132, "start": 144.2, "end": 149.56, "text": " And in the database field, Intel SGX specifically the first one is a very popular choice for", "tokens": [50508, 400, 294, 264, 8149, 2519, 11, 19762, 34520, 55, 4682, 264, 700, 472, 307, 257, 588, 3743, 3922, 337, 50776], "temperature": 0.0, "avg_logprob": -0.1909598263827237, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.02568887732923031}, {"id": 38, "seek": 14132, "start": 149.56, "end": 154.2, "text": " development because it's the most mature one, and there is the most research on it.", "tokens": [50776, 3250, 570, 309, 311, 264, 881, 14442, 472, 11, 293, 456, 307, 264, 881, 2132, 322, 309, 13, 51008], "temperature": 0.0, "avg_logprob": -0.1909598263827237, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.02568887732923031}, {"id": 39, "seek": 14132, "start": 154.2, "end": 158.64, "text": " But at the same time, there are some performance limitations to run workloads that are very", "tokens": [51008, 583, 412, 264, 912, 565, 11, 456, 366, 512, 3389, 15705, 281, 1190, 32452, 300, 366, 588, 51230], "temperature": 0.0, "avg_logprob": -0.1909598263827237, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.02568887732923031}, {"id": 40, "seek": 14132, "start": 158.64, "end": 160.64, "text": " typical for database systems.", "tokens": [51230, 7476, 337, 8149, 3652, 13, 51330], "temperature": 0.0, "avg_logprob": -0.1909598263827237, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.02568887732923031}, {"id": 41, "seek": 14132, "start": 160.64, "end": 165.72, "text": " In particular, the biggest problem here is the limited-page cache size, which is 120", "tokens": [51330, 682, 1729, 11, 264, 3880, 1154, 510, 307, 264, 5567, 12, 15161, 19459, 2744, 11, 597, 307, 10411, 51584], "temperature": 0.0, "avg_logprob": -0.1909598263827237, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.02568887732923031}, {"id": 42, "seek": 14132, "start": 165.72, "end": 169.72, "text": " megabytes in Intel SGX1.", "tokens": [51584, 10816, 24538, 294, 19762, 34520, 55, 16, 13, 51784], "temperature": 0.0, "avg_logprob": -0.1909598263827237, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.02568887732923031}, {"id": 43, "seek": 16972, "start": 169.72, "end": 175.52, "text": " That being said, we're going to explain here many different models to split our DBMS.", "tokens": [50364, 663, 885, 848, 11, 321, 434, 516, 281, 2903, 510, 867, 819, 5245, 281, 7472, 527, 26754, 10288, 13, 50654], "temperature": 0.0, "avg_logprob": -0.17915265456489896, "compression_ratio": 1.6, "no_speech_prob": 0.029741162434220314}, {"id": 44, "seek": 16972, "start": 175.52, "end": 180.42, "text": " We have the... does this work?", "tokens": [50654, 492, 362, 264, 485, 775, 341, 589, 30, 50899], "temperature": 0.0, "avg_logprob": -0.17915265456489896, "compression_ratio": 1.6, "no_speech_prob": 0.029741162434220314}, {"id": 45, "seek": 16972, "start": 180.42, "end": 184.6, "text": " We have the full DBMS split, which means that basically we're going to put all the database", "tokens": [50899, 492, 362, 264, 1577, 26754, 10288, 7472, 11, 597, 1355, 300, 1936, 321, 434, 516, 281, 829, 439, 264, 8149, 51108], "temperature": 0.0, "avg_logprob": -0.17915265456489896, "compression_ratio": 1.6, "no_speech_prob": 0.029741162434220314}, {"id": 46, "seek": 16972, "start": 184.6, "end": 191.24, "text": " inside the Enclave with just a very tiny layer of IO library to handle system calls.", "tokens": [51108, 1854, 264, 29584, 27995, 365, 445, 257, 588, 5870, 4583, 295, 39839, 6405, 281, 4813, 1185, 5498, 13, 51440], "temperature": 0.0, "avg_logprob": -0.17915265456489896, "compression_ratio": 1.6, "no_speech_prob": 0.029741162434220314}, {"id": 47, "seek": 16972, "start": 191.24, "end": 194.64, "text": " Then we have the middle DBMS split, which is something in between.", "tokens": [51440, 1396, 321, 362, 264, 2808, 26754, 10288, 7472, 11, 597, 307, 746, 294, 1296, 13, 51610], "temperature": 0.0, "avg_logprob": -0.17915265456489896, "compression_ratio": 1.6, "no_speech_prob": 0.029741162434220314}, {"id": 48, "seek": 19464, "start": 194.64, "end": 199.0, "text": " It allows more fine-grained optimization and code splits.", "tokens": [50364, 467, 4045, 544, 2489, 12, 20735, 2001, 19618, 293, 3089, 37741, 13, 50582], "temperature": 0.0, "avg_logprob": -0.18563201401259874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.006471042986959219}, {"id": 49, "seek": 19464, "start": 199.0, "end": 203.6, "text": " Usually approaches just put the query execution engine inside the Enclave, and everything", "tokens": [50582, 11419, 11587, 445, 829, 264, 14581, 15058, 2848, 1854, 264, 29584, 27995, 11, 293, 1203, 50812], "temperature": 0.0, "avg_logprob": -0.18563201401259874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.006471042986959219}, {"id": 50, "seek": 19464, "start": 203.6, "end": 205.72, "text": " else is going to stay out.", "tokens": [50812, 1646, 307, 516, 281, 1754, 484, 13, 50918], "temperature": 0.0, "avg_logprob": -0.18563201401259874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.006471042986959219}, {"id": 51, "seek": 19464, "start": 205.72, "end": 212.2, "text": " And then we have the minimal DBMS split, where only the operators and the comparators are", "tokens": [50918, 400, 550, 321, 362, 264, 13206, 26754, 10288, 7472, 11, 689, 787, 264, 19077, 293, 264, 6311, 3391, 366, 51242], "temperature": 0.0, "avg_logprob": -0.18563201401259874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.006471042986959219}, {"id": 52, "seek": 19464, "start": 212.2, "end": 217.2, "text": " inside the Enclave, where with operators and comparators, I mean plus, minus, equal,", "tokens": [51242, 1854, 264, 29584, 27995, 11, 689, 365, 19077, 293, 6311, 3391, 11, 286, 914, 1804, 11, 3175, 11, 2681, 11, 51492], "temperature": 0.0, "avg_logprob": -0.18563201401259874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.006471042986959219}, {"id": 53, "seek": 19464, "start": 217.2, "end": 219.11999999999998, "text": " and so on.", "tokens": [51492, 293, 370, 322, 13, 51588], "temperature": 0.0, "avg_logprob": -0.18563201401259874, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.006471042986959219}, {"id": 54, "seek": 21912, "start": 220.08, "end": 223.68, "text": " Now we have a general understanding of the different models.", "tokens": [50412, 823, 321, 362, 257, 2674, 3701, 295, 264, 819, 5245, 13, 50592], "temperature": 0.0, "avg_logprob": -0.22933478749126468, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.003703997004777193}, {"id": 55, "seek": 21912, "start": 223.68, "end": 226.84, "text": " We can start with some practical examples.", "tokens": [50592, 492, 393, 722, 365, 512, 8496, 5110, 13, 50750], "temperature": 0.0, "avg_logprob": -0.22933478749126468, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.003703997004777193}, {"id": 56, "seek": 21912, "start": 226.84, "end": 231.76, "text": " So here's a personal favorite, it's called StelDB, and it's a Postgre's extension.", "tokens": [50750, 407, 510, 311, 257, 2973, 2954, 11, 309, 311, 1219, 745, 338, 27735, 11, 293, 309, 311, 257, 10223, 33248, 311, 10320, 13, 50996], "temperature": 0.0, "avg_logprob": -0.22933478749126468, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.003703997004777193}, {"id": 57, "seek": 21912, "start": 231.76, "end": 233.28, "text": " We have some Postgre's people here.", "tokens": [50996, 492, 362, 512, 10223, 33248, 311, 561, 510, 13, 51072], "temperature": 0.0, "avg_logprob": -0.22933478749126468, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.003703997004777193}, {"id": 58, "seek": 21912, "start": 233.28, "end": 241.76, "text": " I'm very biased on this, but basically, StelDB is employing the third model that I mentioned,", "tokens": [51072, 286, 478, 588, 28035, 322, 341, 11, 457, 1936, 11, 745, 338, 27735, 307, 3188, 278, 264, 2636, 2316, 300, 286, 2835, 11, 51496], "temperature": 0.0, "avg_logprob": -0.22933478749126468, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.003703997004777193}, {"id": 59, "seek": 21912, "start": 241.76, "end": 244.52, "text": " which is the minimal DBMS split.", "tokens": [51496, 597, 307, 264, 13206, 26754, 10288, 7472, 13, 51634], "temperature": 0.0, "avg_logprob": -0.22933478749126468, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.003703997004777193}, {"id": 60, "seek": 21912, "start": 244.52, "end": 248.64000000000001, "text": " So basically it's only implementing operators and comparators.", "tokens": [51634, 407, 1936, 309, 311, 787, 18114, 19077, 293, 6311, 3391, 13, 51840], "temperature": 0.0, "avg_logprob": -0.22933478749126468, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.003703997004777193}, {"id": 61, "seek": 24864, "start": 248.67999999999998, "end": 254.0, "text": " This choice was probably made because of the very limited memory that we can use.", "tokens": [50366, 639, 3922, 390, 1391, 1027, 570, 295, 264, 588, 5567, 4675, 300, 321, 393, 764, 13, 50632], "temperature": 0.0, "avg_logprob": -0.1499705533368872, "compression_ratio": 1.6423076923076922, "no_speech_prob": 0.03045617789030075}, {"id": 62, "seek": 24864, "start": 254.0, "end": 256.76, "text": " And so of course there are some trade-offs.", "tokens": [50632, 400, 370, 295, 1164, 456, 366, 512, 4923, 12, 19231, 13, 50770], "temperature": 0.0, "avg_logprob": -0.1499705533368872, "compression_ratio": 1.6423076923076922, "no_speech_prob": 0.03045617789030075}, {"id": 63, "seek": 24864, "start": 256.76, "end": 260.64, "text": " If we do not have the full DBMS, of course there is more information leakage.", "tokens": [50770, 759, 321, 360, 406, 362, 264, 1577, 26754, 10288, 11, 295, 1164, 456, 307, 544, 1589, 47799, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1499705533368872, "compression_ratio": 1.6423076923076922, "no_speech_prob": 0.03045617789030075}, {"id": 64, "seek": 24864, "start": 260.64, "end": 266.12, "text": " For instance, people might be able to infer the size of the database and the operations", "tokens": [50964, 1171, 5197, 11, 561, 1062, 312, 1075, 281, 13596, 264, 2744, 295, 264, 8149, 293, 264, 7705, 51238], "temperature": 0.0, "avg_logprob": -0.1499705533368872, "compression_ratio": 1.6423076923076922, "no_speech_prob": 0.03045617789030075}, {"id": 65, "seek": 24864, "start": 266.12, "end": 268.59999999999997, "text": " that we are making inside the Enclave.", "tokens": [51238, 300, 321, 366, 1455, 1854, 264, 29584, 27995, 13, 51362], "temperature": 0.0, "avg_logprob": -0.1499705533368872, "compression_ratio": 1.6423076923076922, "no_speech_prob": 0.03045617789030075}, {"id": 66, "seek": 24864, "start": 268.59999999999997, "end": 274.2, "text": " And at the same time, even though the secure part is so limited, the performance is kind", "tokens": [51362, 400, 412, 264, 912, 565, 11, 754, 1673, 264, 7144, 644, 307, 370, 5567, 11, 264, 3389, 307, 733, 51642], "temperature": 0.0, "avg_logprob": -0.1499705533368872, "compression_ratio": 1.6423076923076922, "no_speech_prob": 0.03045617789030075}, {"id": 67, "seek": 24864, "start": 274.2, "end": 275.2, "text": " of bad.", "tokens": [51642, 295, 1578, 13, 51692], "temperature": 0.0, "avg_logprob": -0.1499705533368872, "compression_ratio": 1.6423076923076922, "no_speech_prob": 0.03045617789030075}, {"id": 68, "seek": 27520, "start": 275.24, "end": 280.36, "text": " So here we are going to have here 5% to 30% overhead in transactional queries, where", "tokens": [50366, 407, 510, 321, 366, 516, 281, 362, 510, 1025, 4, 281, 2217, 4, 19922, 294, 46688, 1966, 24109, 11, 689, 50622], "temperature": 0.0, "avg_logprob": -0.2011419205438523, "compression_ratio": 1.64, "no_speech_prob": 0.01106958370655775}, {"id": 69, "seek": 27520, "start": 280.36, "end": 285.28, "text": " transactional queries are workloads that are very heavy in inserts and updates on current", "tokens": [50622, 46688, 1966, 24109, 366, 32452, 300, 366, 588, 4676, 294, 49163, 293, 9205, 322, 2190, 50868], "temperature": 0.0, "avg_logprob": -0.2011419205438523, "compression_ratio": 1.64, "no_speech_prob": 0.01106958370655775}, {"id": 70, "seek": 27520, "start": 285.28, "end": 286.28, "text": " data.", "tokens": [50868, 1412, 13, 50918], "temperature": 0.0, "avg_logprob": -0.2011419205438523, "compression_ratio": 1.64, "no_speech_prob": 0.01106958370655775}, {"id": 71, "seek": 27520, "start": 286.28, "end": 292.28, "text": " So, yeah, so this is a very good project, but still not quite what we would like to have", "tokens": [50918, 407, 11, 1338, 11, 370, 341, 307, 257, 588, 665, 1716, 11, 457, 920, 406, 1596, 437, 321, 576, 411, 281, 362, 51218], "temperature": 0.0, "avg_logprob": -0.2011419205438523, "compression_ratio": 1.64, "no_speech_prob": 0.01106958370655775}, {"id": 72, "seek": 27520, "start": 292.28, "end": 295.8, "text": " if we are running actual real-world workloads.", "tokens": [51218, 498, 321, 366, 2614, 3539, 957, 12, 13217, 32452, 13, 51394], "temperature": 0.0, "avg_logprob": -0.2011419205438523, "compression_ratio": 1.64, "no_speech_prob": 0.01106958370655775}, {"id": 73, "seek": 27520, "start": 295.8, "end": 298.56, "text": " There are more examples here of other databases.", "tokens": [51394, 821, 366, 544, 5110, 510, 295, 661, 22380, 13, 51532], "temperature": 0.0, "avg_logprob": -0.2011419205438523, "compression_ratio": 1.64, "no_speech_prob": 0.01106958370655775}, {"id": 74, "seek": 27520, "start": 298.56, "end": 301.88, "text": " We have a lot of implementations of SQLites.", "tokens": [51532, 492, 362, 257, 688, 295, 4445, 763, 295, 19200, 3324, 13, 51698], "temperature": 0.0, "avg_logprob": -0.2011419205438523, "compression_ratio": 1.64, "no_speech_prob": 0.01106958370655775}, {"id": 75, "seek": 30188, "start": 301.88, "end": 307.96, "text": " And they are, I think all of them are full DBMS split, but regardless, they add at least", "tokens": [50364, 400, 436, 366, 11, 286, 519, 439, 295, 552, 366, 1577, 26754, 10288, 7472, 11, 457, 10060, 11, 436, 909, 412, 1935, 50668], "temperature": 0.0, "avg_logprob": -0.20857317812807924, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.004614707548171282}, {"id": 76, "seek": 30188, "start": 307.96, "end": 311.8, "text": " one or two orders of magnitude of overhead to the queries.", "tokens": [50668, 472, 420, 732, 9470, 295, 15668, 295, 19922, 281, 264, 24109, 13, 50860], "temperature": 0.0, "avg_logprob": -0.20857317812807924, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.004614707548171282}, {"id": 77, "seek": 30188, "start": 311.8, "end": 316.76, "text": " We have a MariaDB kind of encrypted database, which is called Ageless.", "tokens": [50860, 492, 362, 257, 12734, 27735, 733, 295, 36663, 8149, 11, 597, 307, 1219, 2725, 4272, 13, 51108], "temperature": 0.0, "avg_logprob": -0.20857317812807924, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.004614707548171282}, {"id": 78, "seek": 30188, "start": 316.76, "end": 324.36, "text": " I think I saw some people from Ageless here, or there was a, it was at FOSDEM a few years", "tokens": [51108, 286, 519, 286, 1866, 512, 561, 490, 2725, 4272, 510, 11, 420, 456, 390, 257, 11, 309, 390, 412, 479, 4367, 35, 6683, 257, 1326, 924, 51488], "temperature": 0.0, "avg_logprob": -0.20857317812807924, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.004614707548171282}, {"id": 79, "seek": 30188, "start": 324.36, "end": 325.36, "text": " back.", "tokens": [51488, 646, 13, 51538], "temperature": 0.0, "avg_logprob": -0.20857317812807924, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.004614707548171282}, {"id": 80, "seek": 30188, "start": 325.36, "end": 330.52, "text": " And yeah, Ageless is basically this database that is designed to run inside an Enclave and", "tokens": [51538, 400, 1338, 11, 2725, 4272, 307, 1936, 341, 8149, 300, 307, 4761, 281, 1190, 1854, 364, 29584, 27995, 293, 51796], "temperature": 0.0, "avg_logprob": -0.20857317812807924, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.004614707548171282}, {"id": 81, "seek": 33052, "start": 330.56, "end": 333.76, "text": " uses MariaDB and ROXDB storage.", "tokens": [50366, 4960, 12734, 27735, 293, 9025, 55, 27735, 6725, 13, 50526], "temperature": 0.0, "avg_logprob": -0.22141721693135924, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.004916517063975334}, {"id": 82, "seek": 33052, "start": 333.76, "end": 338.71999999999997, "text": " It also has encrypted authenticated data in disk and in memory as well, and encrypted", "tokens": [50526, 467, 611, 575, 36663, 9214, 3587, 1412, 294, 12355, 293, 294, 4675, 382, 731, 11, 293, 36663, 50774], "temperature": 0.0, "avg_logprob": -0.22141721693135924, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.004916517063975334}, {"id": 83, "seek": 33052, "start": 338.71999999999997, "end": 339.71999999999997, "text": " network connection.", "tokens": [50774, 3209, 4984, 13, 50824], "temperature": 0.0, "avg_logprob": -0.22141721693135924, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.004916517063975334}, {"id": 84, "seek": 33052, "start": 339.71999999999997, "end": 341.79999999999995, "text": " So it's a very nice project.", "tokens": [50824, 407, 309, 311, 257, 588, 1481, 1716, 13, 50928], "temperature": 0.0, "avg_logprob": -0.22141721693135924, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.004916517063975334}, {"id": 85, "seek": 33052, "start": 341.79999999999995, "end": 344.84, "text": " Then we have an implementation of Microsoft SQL Server.", "tokens": [50928, 1396, 321, 362, 364, 11420, 295, 8116, 19200, 25684, 13, 51080], "temperature": 0.0, "avg_logprob": -0.22141721693135924, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.004916517063975334}, {"id": 86, "seek": 33052, "start": 344.84, "end": 345.84, "text": " I'm sorry about this.", "tokens": [51080, 286, 478, 2597, 466, 341, 13, 51130], "temperature": 0.0, "avg_logprob": -0.22141721693135924, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.004916517063975334}, {"id": 87, "seek": 33052, "start": 345.84, "end": 350.47999999999996, "text": " It's not open source, I know, but unfortunately it's one of the most relevant works in the", "tokens": [51130, 467, 311, 406, 1269, 4009, 11, 286, 458, 11, 457, 7015, 309, 311, 472, 295, 264, 881, 7340, 1985, 294, 264, 51362], "temperature": 0.0, "avg_logprob": -0.22141721693135924, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.004916517063975334}, {"id": 88, "seek": 33052, "start": 350.47999999999996, "end": 356.79999999999995, "text": " field because it actually implements the query engine in the Enclave and splits the data", "tokens": [51362, 2519, 570, 309, 767, 704, 17988, 264, 14581, 2848, 294, 264, 29584, 27995, 293, 37741, 264, 1412, 51678], "temperature": 0.0, "avg_logprob": -0.22141721693135924, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.004916517063975334}, {"id": 89, "seek": 33052, "start": 356.79999999999995, "end": 359.32, "text": " between sensitive and insensitive tables.", "tokens": [51678, 1296, 9477, 293, 1028, 34465, 8020, 13, 51804], "temperature": 0.0, "avg_logprob": -0.22141721693135924, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.004916517063975334}, {"id": 90, "seek": 35932, "start": 359.32, "end": 364.04, "text": " So it's a very novel idea, but unfortunately it doesn't work because this kind of models", "tokens": [50364, 407, 309, 311, 257, 588, 7613, 1558, 11, 457, 7015, 309, 1177, 380, 589, 570, 341, 733, 295, 5245, 50600], "temperature": 0.0, "avg_logprob": -0.24374306418678976, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0027400280814617872}, {"id": 91, "seek": 35932, "start": 364.04, "end": 369.76, "text": " also assumes a very big Enclave size and due to the limitations of AgX1, this is not", "tokens": [50600, 611, 37808, 257, 588, 955, 29584, 27995, 2744, 293, 3462, 281, 264, 15705, 295, 2725, 55, 16, 11, 341, 307, 406, 50886], "temperature": 0.0, "avg_logprob": -0.24374306418678976, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0027400280814617872}, {"id": 92, "seek": 35932, "start": 369.76, "end": 371.64, "text": " pretty feasible in practice.", "tokens": [50886, 1238, 26648, 294, 3124, 13, 50980], "temperature": 0.0, "avg_logprob": -0.24374306418678976, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0027400280814617872}, {"id": 93, "seek": 35932, "start": 371.64, "end": 377.4, "text": " And then we have one analytical engine where with analytic, I mean doing analytics, so", "tokens": [50980, 400, 550, 321, 362, 472, 29579, 2848, 689, 365, 40358, 11, 286, 914, 884, 15370, 11, 370, 51268], "temperature": 0.0, "avg_logprob": -0.24374306418678976, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0027400280814617872}, {"id": 94, "seek": 35932, "start": 377.4, "end": 383.08, "text": " business intelligence workloads on a lot of data and historical data.", "tokens": [51268, 1606, 7599, 32452, 322, 257, 688, 295, 1412, 293, 8584, 1412, 13, 51552], "temperature": 0.0, "avg_logprob": -0.24374306418678976, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0027400280814617872}, {"id": 95, "seek": 35932, "start": 383.08, "end": 389.2, "text": " And yeah, this is called OblityB and it implements Oblivious Physical Operator for analytical", "tokens": [51552, 400, 1338, 11, 341, 307, 1219, 4075, 75, 507, 33, 293, 309, 704, 17988, 4075, 45997, 851, 31918, 12480, 1639, 337, 29579, 51858], "temperature": 0.0, "avg_logprob": -0.24374306418678976, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0027400280814617872}, {"id": 96, "seek": 38920, "start": 389.2, "end": 390.64, "text": " processing in the cloud.", "tokens": [50364, 9007, 294, 264, 4588, 13, 50436], "temperature": 0.0, "avg_logprob": -0.1802347501118978, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.010383747518062592}, {"id": 97, "seek": 38920, "start": 390.64, "end": 395.64, "text": " But yeah, once again, this is really, really slow because of the Enclave size.", "tokens": [50436, 583, 1338, 11, 1564, 797, 11, 341, 307, 534, 11, 534, 2964, 570, 295, 264, 29584, 27995, 2744, 13, 50686], "temperature": 0.0, "avg_logprob": -0.1802347501118978, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.010383747518062592}, {"id": 98, "seek": 38920, "start": 395.64, "end": 402.12, "text": " So our contribution here is taking all of this that we have and we notice two things", "tokens": [50686, 407, 527, 13150, 510, 307, 1940, 439, 295, 341, 300, 321, 362, 293, 321, 3449, 732, 721, 51010], "temperature": 0.0, "avg_logprob": -0.1802347501118978, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.010383747518062592}, {"id": 99, "seek": 38920, "start": 402.12, "end": 403.12, "text": " in here.", "tokens": [51010, 294, 510, 13, 51060], "temperature": 0.0, "avg_logprob": -0.1802347501118978, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.010383747518062592}, {"id": 100, "seek": 38920, "start": 403.12, "end": 408.9, "text": " First of all, the big majority of these implementations on AgX1 are transactional and because", "tokens": [51060, 2386, 295, 439, 11, 264, 955, 6286, 295, 613, 4445, 763, 322, 2725, 55, 16, 366, 46688, 1966, 293, 570, 51349], "temperature": 0.0, "avg_logprob": -0.1802347501118978, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.010383747518062592}, {"id": 101, "seek": 38920, "start": 408.9, "end": 413.71999999999997, "text": " analytical workloads really don't scale because of the volume of the data and they overhead", "tokens": [51349, 29579, 32452, 534, 500, 380, 4373, 570, 295, 264, 5523, 295, 264, 1412, 293, 436, 19922, 51590], "temperature": 0.0, "avg_logprob": -0.1802347501118978, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.010383747518062592}, {"id": 102, "seek": 38920, "start": 413.71999999999997, "end": 417.71999999999997, "text": " called by last level cache misses and EPC swapping.", "tokens": [51590, 1219, 538, 1036, 1496, 19459, 29394, 293, 462, 12986, 1693, 10534, 13, 51790], "temperature": 0.0, "avg_logprob": -0.1802347501118978, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.010383747518062592}, {"id": 103, "seek": 41772, "start": 418.16, "end": 421.84000000000003, "text": " The second problem is that there is no research on SGX2.", "tokens": [50386, 440, 1150, 1154, 307, 300, 456, 307, 572, 2132, 322, 34520, 55, 17, 13, 50570], "temperature": 0.0, "avg_logprob": -0.18103968655621563, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.02735353261232376}, {"id": 104, "seek": 41772, "start": 421.84000000000003, "end": 428.04, "text": " So SGX2 was released a couple years ago, but still all the prototypes that I mentioned", "tokens": [50570, 407, 34520, 55, 17, 390, 4736, 257, 1916, 924, 2057, 11, 457, 920, 439, 264, 42197, 300, 286, 2835, 50880], "temperature": 0.0, "avg_logprob": -0.18103968655621563, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.02735353261232376}, {"id": 105, "seek": 41772, "start": 428.04, "end": 429.88000000000005, "text": " were made for SGX1.", "tokens": [50880, 645, 1027, 337, 34520, 55, 16, 13, 50972], "temperature": 0.0, "avg_logprob": -0.18103968655621563, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.02735353261232376}, {"id": 106, "seek": 41772, "start": 429.88000000000005, "end": 434.48, "text": " I'm not saying they don't work, but I'm saying there are no benchmarks, there are no implementations,", "tokens": [50972, 286, 478, 406, 1566, 436, 500, 380, 589, 11, 457, 286, 478, 1566, 456, 366, 572, 43751, 11, 456, 366, 572, 4445, 763, 11, 51202], "temperature": 0.0, "avg_logprob": -0.18103968655621563, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.02735353261232376}, {"id": 107, "seek": 41772, "start": 434.48, "end": 438.12, "text": " so there are specifically tailored for SGX2.", "tokens": [51202, 370, 456, 366, 4682, 34858, 337, 34520, 55, 17, 13, 51384], "temperature": 0.0, "avg_logprob": -0.18103968655621563, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.02735353261232376}, {"id": 108, "seek": 41772, "start": 438.12, "end": 443.32000000000005, "text": " So here our contribution is to try and bridge the gap between efficient and secure analytical", "tokens": [51384, 407, 510, 527, 13150, 307, 281, 853, 293, 7283, 264, 7417, 1296, 7148, 293, 7144, 29579, 51644], "temperature": 0.0, "avg_logprob": -0.18103968655621563, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.02735353261232376}, {"id": 109, "seek": 41772, "start": 443.32000000000005, "end": 444.6, "text": " processing.", "tokens": [51644, 9007, 13, 51708], "temperature": 0.0, "avg_logprob": -0.18103968655621563, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.02735353261232376}, {"id": 110, "seek": 44460, "start": 444.6, "end": 447.72, "text": " To do so, we use our database, DacDB.", "tokens": [50364, 1407, 360, 370, 11, 321, 764, 527, 8149, 11, 413, 326, 27735, 13, 50520], "temperature": 0.0, "avg_logprob": -0.20294331770676832, "compression_ratio": 1.602112676056338, "no_speech_prob": 0.01872039958834648}, {"id": 111, "seek": 44460, "start": 447.72, "end": 452.0, "text": " Disclaimer here, we are not affiliated with DacDB, we are not paid by them.", "tokens": [50520, 19839, 35220, 510, 11, 321, 366, 406, 42174, 365, 413, 326, 27735, 11, 321, 366, 406, 4835, 538, 552, 13, 50734], "temperature": 0.0, "avg_logprob": -0.20294331770676832, "compression_ratio": 1.602112676056338, "no_speech_prob": 0.01872039958834648}, {"id": 112, "seek": 44460, "start": 452.0, "end": 456.36, "text": " It's just an open source database that we happen to use because it's developed in our", "tokens": [50734, 467, 311, 445, 364, 1269, 4009, 8149, 300, 321, 1051, 281, 764, 570, 309, 311, 4743, 294, 527, 50952], "temperature": 0.0, "avg_logprob": -0.20294331770676832, "compression_ratio": 1.602112676056338, "no_speech_prob": 0.01872039958834648}, {"id": 113, "seek": 44460, "start": 456.36, "end": 457.88, "text": " research center.", "tokens": [50952, 2132, 3056, 13, 51028], "temperature": 0.0, "avg_logprob": -0.20294331770676832, "compression_ratio": 1.602112676056338, "no_speech_prob": 0.01872039958834648}, {"id": 114, "seek": 44460, "start": 457.88, "end": 463.08000000000004, "text": " So DacDB is open source, it's embedded, columnar analytical system, I'm sorry, there are a", "tokens": [51028, 407, 413, 326, 27735, 307, 1269, 4009, 11, 309, 311, 16741, 11, 7738, 289, 29579, 1185, 11, 286, 478, 2597, 11, 456, 366, 257, 51288], "temperature": 0.0, "avg_logprob": -0.20294331770676832, "compression_ratio": 1.602112676056338, "no_speech_prob": 0.01872039958834648}, {"id": 115, "seek": 44460, "start": 463.08000000000004, "end": 466.08000000000004, "text": " lot of buzzwords here, I'm going to explain that later.", "tokens": [51288, 688, 295, 13036, 13832, 510, 11, 286, 478, 516, 281, 2903, 300, 1780, 13, 51438], "temperature": 0.0, "avg_logprob": -0.20294331770676832, "compression_ratio": 1.602112676056338, "no_speech_prob": 0.01872039958834648}, {"id": 116, "seek": 44460, "start": 466.08000000000004, "end": 472.56, "text": " It's in C++11 without additional dependencies and it's actually been ported to SGX1 in 2022", "tokens": [51438, 467, 311, 294, 383, 25472, 5348, 1553, 4497, 36606, 293, 309, 311, 767, 668, 2436, 292, 281, 34520, 55, 16, 294, 20229, 51762], "temperature": 0.0, "avg_logprob": -0.20294331770676832, "compression_ratio": 1.602112676056338, "no_speech_prob": 0.01872039958834648}, {"id": 117, "seek": 47256, "start": 472.56, "end": 474.96, "text": " by some, our master student.", "tokens": [50364, 538, 512, 11, 527, 4505, 3107, 13, 50484], "temperature": 0.0, "avg_logprob": -0.16601308786644126, "compression_ratio": 1.6575875486381324, "no_speech_prob": 0.0057983919978141785}, {"id": 118, "seek": 47256, "start": 474.96, "end": 481.44, "text": " And before explaining what we did with DacDB and SGX1 and 2, I need to give you some fundamental", "tokens": [50484, 400, 949, 13468, 437, 321, 630, 365, 413, 326, 27735, 293, 34520, 55, 16, 293, 568, 11, 286, 643, 281, 976, 291, 512, 8088, 50808], "temperature": 0.0, "avg_logprob": -0.16601308786644126, "compression_ratio": 1.6575875486381324, "no_speech_prob": 0.0057983919978141785}, {"id": 119, "seek": 47256, "start": 481.44, "end": 484.04, "text": " concepts about database internals.", "tokens": [50808, 10392, 466, 8149, 2154, 1124, 13, 50938], "temperature": 0.0, "avg_logprob": -0.16601308786644126, "compression_ratio": 1.6575875486381324, "no_speech_prob": 0.0057983919978141785}, {"id": 120, "seek": 47256, "start": 484.04, "end": 486.2, "text": " We start here with column storage.", "tokens": [50938, 492, 722, 510, 365, 7738, 6725, 13, 51046], "temperature": 0.0, "avg_logprob": -0.16601308786644126, "compression_ratio": 1.6575875486381324, "no_speech_prob": 0.0057983919978141785}, {"id": 121, "seek": 47256, "start": 486.2, "end": 491.24, "text": " So the difference between row and column storage is that basically data in column storage is", "tokens": [51046, 407, 264, 2649, 1296, 5386, 293, 7738, 6725, 307, 300, 1936, 1412, 294, 7738, 6725, 307, 51298], "temperature": 0.0, "avg_logprob": -0.16601308786644126, "compression_ratio": 1.6575875486381324, "no_speech_prob": 0.0057983919978141785}, {"id": 122, "seek": 47256, "start": 491.24, "end": 496.4, "text": " stored in columns because if you do analytical workloads, we don't need usually all the columns", "tokens": [51298, 12187, 294, 13766, 570, 498, 291, 360, 29579, 32452, 11, 321, 500, 380, 643, 2673, 439, 264, 13766, 51556], "temperature": 0.0, "avg_logprob": -0.16601308786644126, "compression_ratio": 1.6575875486381324, "no_speech_prob": 0.0057983919978141785}, {"id": 123, "seek": 47256, "start": 496.4, "end": 499.0, "text": " that we have, we just need a few of them.", "tokens": [51556, 300, 321, 362, 11, 321, 445, 643, 257, 1326, 295, 552, 13, 51686], "temperature": 0.0, "avg_logprob": -0.16601308786644126, "compression_ratio": 1.6575875486381324, "no_speech_prob": 0.0057983919978141785}, {"id": 124, "seek": 49900, "start": 499.0, "end": 503.36, "text": " So it is more efficient to store the columns all together such that we can only fetch what", "tokens": [50364, 407, 309, 307, 544, 7148, 281, 3531, 264, 13766, 439, 1214, 1270, 300, 321, 393, 787, 23673, 437, 50582], "temperature": 0.0, "avg_logprob": -0.13193326950073242, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.004586648661643267}, {"id": 125, "seek": 49900, "start": 503.36, "end": 504.76, "text": " we need.", "tokens": [50582, 321, 643, 13, 50652], "temperature": 0.0, "avg_logprob": -0.13193326950073242, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.004586648661643267}, {"id": 126, "seek": 49900, "start": 504.76, "end": 511.72, "text": " And also this kind of column format is also very much, can very much benefit from compression", "tokens": [50652, 400, 611, 341, 733, 295, 7738, 7877, 307, 611, 588, 709, 11, 393, 588, 709, 5121, 490, 19355, 51000], "temperature": 0.0, "avg_logprob": -0.13193326950073242, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.004586648661643267}, {"id": 127, "seek": 49900, "start": 511.72, "end": 516.56, "text": " because usually there is a lot of correlation between the data and our data is also huge.", "tokens": [51000, 570, 2673, 456, 307, 257, 688, 295, 20009, 1296, 264, 1412, 293, 527, 1412, 307, 611, 2603, 13, 51242], "temperature": 0.0, "avg_logprob": -0.13193326950073242, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.004586648661643267}, {"id": 128, "seek": 49900, "start": 516.56, "end": 521.12, "text": " So we can definitely implement some sort of compression and DacDB specifically implements", "tokens": [51242, 407, 321, 393, 2138, 4445, 512, 1333, 295, 19355, 293, 413, 326, 27735, 4682, 704, 17988, 51470], "temperature": 0.0, "avg_logprob": -0.13193326950073242, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.004586648661643267}, {"id": 129, "seek": 49900, "start": 521.12, "end": 525.88, "text": " column lever compression where data is stored in column and then compressed.", "tokens": [51470, 7738, 12451, 19355, 689, 1412, 307, 12187, 294, 7738, 293, 550, 30353, 13, 51708], "temperature": 0.0, "avg_logprob": -0.13193326950073242, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.004586648661643267}, {"id": 130, "seek": 52588, "start": 525.88, "end": 530.0, "text": " Now we also need to talk a little bit about vectorized execution.", "tokens": [50364, 823, 321, 611, 643, 281, 751, 257, 707, 857, 466, 8062, 1602, 15058, 13, 50570], "temperature": 0.0, "avg_logprob": -0.11955904960632324, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.009591075591742992}, {"id": 131, "seek": 52588, "start": 530.0, "end": 535.92, "text": " This is similar to the CMD instructions that you probably know of but applied to databases.", "tokens": [50570, 639, 307, 2531, 281, 264, 20424, 35, 9415, 300, 291, 1391, 458, 295, 457, 6456, 281, 22380, 13, 50866], "temperature": 0.0, "avg_logprob": -0.11955904960632324, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.009591075591742992}, {"id": 132, "seek": 52588, "start": 535.92, "end": 542.0, "text": " So instead of performing operations to one row at a time, we perform it in batches.", "tokens": [50866, 407, 2602, 295, 10205, 7705, 281, 472, 5386, 412, 257, 565, 11, 321, 2042, 309, 294, 15245, 279, 13, 51170], "temperature": 0.0, "avg_logprob": -0.11955904960632324, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.009591075591742992}, {"id": 133, "seek": 52588, "start": 542.0, "end": 547.8, "text": " So instead of having a row fetching it, elaborating it and returning it, we do the same process", "tokens": [51170, 407, 2602, 295, 1419, 257, 5386, 23673, 278, 309, 11, 16298, 990, 309, 293, 12678, 309, 11, 321, 360, 264, 912, 1399, 51460], "temperature": 0.0, "avg_logprob": -0.11955904960632324, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.009591075591742992}, {"id": 134, "seek": 52588, "start": 547.8, "end": 549.24, "text": " with batches.", "tokens": [51460, 365, 15245, 279, 13, 51532], "temperature": 0.0, "avg_logprob": -0.11955904960632324, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.009591075591742992}, {"id": 135, "seek": 52588, "start": 549.24, "end": 552.92, "text": " So you can see this example, very, very simple query.", "tokens": [51532, 407, 291, 393, 536, 341, 1365, 11, 588, 11, 588, 2199, 14581, 13, 51716], "temperature": 0.0, "avg_logprob": -0.11955904960632324, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.009591075591742992}, {"id": 136, "seek": 55292, "start": 552.92, "end": 557.52, "text": " And here our function next is going to return many tuples rather than one.", "tokens": [50364, 400, 510, 527, 2445, 958, 307, 516, 281, 2736, 867, 2604, 2622, 2831, 813, 472, 13, 50594], "temperature": 0.0, "avg_logprob": -0.23153246366060698, "compression_ratio": 1.5679012345679013, "no_speech_prob": 0.03243501856923103}, {"id": 137, "seek": 55292, "start": 557.52, "end": 561.7199999999999, "text": " And we push only the relevant blocks of data up and down the query plan.", "tokens": [50594, 400, 321, 2944, 787, 264, 7340, 8474, 295, 1412, 493, 293, 760, 264, 14581, 1393, 13, 50804], "temperature": 0.0, "avg_logprob": -0.23153246366060698, "compression_ratio": 1.5679012345679013, "no_speech_prob": 0.03243501856923103}, {"id": 138, "seek": 55292, "start": 561.7199999999999, "end": 566.7199999999999, "text": " And this is more efficient because we have less system calls and we can also take advantage", "tokens": [50804, 400, 341, 307, 544, 7148, 570, 321, 362, 1570, 1185, 5498, 293, 321, 393, 611, 747, 5002, 51054], "temperature": 0.0, "avg_logprob": -0.23153246366060698, "compression_ratio": 1.5679012345679013, "no_speech_prob": 0.03243501856923103}, {"id": 139, "seek": 55292, "start": 566.7199999999999, "end": 568.3199999999999, "text": " of the CPU more efficiently.", "tokens": [51054, 295, 264, 13199, 544, 19621, 13, 51134], "temperature": 0.0, "avg_logprob": -0.23153246366060698, "compression_ratio": 1.5679012345679013, "no_speech_prob": 0.03243501856923103}, {"id": 140, "seek": 55292, "start": 568.3199999999999, "end": 570.36, "text": " Now thank you for the attention.", "tokens": [51134, 823, 1309, 291, 337, 264, 3202, 13, 51236], "temperature": 0.0, "avg_logprob": -0.23153246366060698, "compression_ratio": 1.5679012345679013, "no_speech_prob": 0.03243501856923103}, {"id": 141, "seek": 55292, "start": 570.36, "end": 574.8, "text": " Now Lotte is going to explain you how we ported DacDB to SGAX.", "tokens": [51236, 823, 441, 13079, 307, 516, 281, 2903, 291, 577, 321, 2436, 292, 413, 326, 27735, 281, 318, 12570, 55, 13, 51458], "temperature": 0.0, "avg_logprob": -0.23153246366060698, "compression_ratio": 1.5679012345679013, "no_speech_prob": 0.03243501856923103}, {"id": 142, "seek": 55292, "start": 574.8, "end": 578.92, "text": " Thank you, Illa.", "tokens": [51458, 1044, 291, 11, 286, 3505, 13, 51664], "temperature": 0.0, "avg_logprob": -0.23153246366060698, "compression_ratio": 1.5679012345679013, "no_speech_prob": 0.03243501856923103}, {"id": 143, "seek": 57892, "start": 578.92, "end": 585.28, "text": " Okay, so before we go directly to SGAX2 and how we did it, we first are going to pay some", "tokens": [50364, 1033, 11, 370, 949, 321, 352, 3838, 281, 318, 12570, 55, 17, 293, 577, 321, 630, 309, 11, 321, 700, 366, 516, 281, 1689, 512, 50682], "temperature": 0.0, "avg_logprob": -0.14511777736522533, "compression_ratio": 1.6031746031746033, "no_speech_prob": 0.10387684404850006}, {"id": 144, "seek": 57892, "start": 585.28, "end": 590.92, "text": " attention to how it actually has been done to SGAX1 because the master student, he ported", "tokens": [50682, 3202, 281, 577, 309, 767, 575, 668, 1096, 281, 318, 12570, 55, 16, 570, 264, 4505, 3107, 11, 415, 2436, 292, 50964], "temperature": 0.0, "avg_logprob": -0.14511777736522533, "compression_ratio": 1.6031746031746033, "no_speech_prob": 0.10387684404850006}, {"id": 145, "seek": 57892, "start": 590.92, "end": 593.4, "text": " DacDB in two different ways.", "tokens": [50964, 413, 326, 27735, 294, 732, 819, 2098, 13, 51088], "temperature": 0.0, "avg_logprob": -0.14511777736522533, "compression_ratio": 1.6031746031746033, "no_speech_prob": 0.10387684404850006}, {"id": 146, "seek": 57892, "start": 593.4, "end": 596.3199999999999, "text": " The first one is the full database management split.", "tokens": [51088, 440, 700, 472, 307, 264, 1577, 8149, 4592, 7472, 13, 51234], "temperature": 0.0, "avg_logprob": -0.14511777736522533, "compression_ratio": 1.6031746031746033, "no_speech_prob": 0.10387684404850006}, {"id": 147, "seek": 57892, "start": 596.3199999999999, "end": 600.52, "text": " The main issue was here of course because of the low memory capacity that not the whole", "tokens": [51234, 440, 2135, 2734, 390, 510, 295, 1164, 570, 295, 264, 2295, 4675, 6042, 300, 406, 264, 1379, 51444], "temperature": 0.0, "avg_logprob": -0.14511777736522533, "compression_ratio": 1.6031746031746033, "no_speech_prob": 0.10387684404850006}, {"id": 148, "seek": 57892, "start": 600.52, "end": 604.52, "text": " database or not all the data would fit in the enclave.", "tokens": [51444, 8149, 420, 406, 439, 264, 1412, 576, 3318, 294, 264, 2058, 27995, 13, 51644], "temperature": 0.0, "avg_logprob": -0.14511777736522533, "compression_ratio": 1.6031746031746033, "no_speech_prob": 0.10387684404850006}, {"id": 149, "seek": 60452, "start": 604.52, "end": 608.68, "text": " And the second issue here is that system calls are not directly callable inside the", "tokens": [50364, 400, 264, 1150, 2734, 510, 307, 300, 1185, 5498, 366, 406, 3838, 818, 712, 1854, 264, 50572], "temperature": 0.0, "avg_logprob": -0.24631118774414062, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.02448229305446148}, {"id": 150, "seek": 60452, "start": 608.68, "end": 609.68, "text": " enclave.", "tokens": [50572, 2058, 27995, 13, 50622], "temperature": 0.0, "avg_logprob": -0.24631118774414062, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.02448229305446148}, {"id": 151, "seek": 60452, "start": 609.68, "end": 613.8, "text": " So you either need to reimplement all system calls or the necessary system calls or you", "tokens": [50622, 407, 291, 2139, 643, 281, 33433, 43704, 439, 1185, 5498, 420, 264, 4818, 1185, 5498, 420, 291, 50828], "temperature": 0.0, "avg_logprob": -0.24631118774414062, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.02448229305446148}, {"id": 152, "seek": 60452, "start": 613.8, "end": 623.0, "text": " need to use some kind of library which maintains this kind of IOS\uc2ec layer.", "tokens": [50828, 643, 281, 764, 512, 733, 295, 6405, 597, 33385, 341, 733, 295, 286, 4367, 12766, 4583, 13, 51288], "temperature": 0.0, "avg_logprob": -0.24631118774414062, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.02448229305446148}, {"id": 153, "seek": 60452, "start": 623.0, "end": 625.36, "text": " So the master student, he used Graphene.", "tokens": [51288, 407, 264, 4505, 3107, 11, 415, 1143, 21884, 1450, 13, 51406], "temperature": 0.0, "avg_logprob": -0.24631118774414062, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.02448229305446148}, {"id": 154, "seek": 60452, "start": 625.36, "end": 628.04, "text": " Nowadays, Graphene is actually called Grameen.", "tokens": [51406, 28908, 11, 21884, 1450, 307, 767, 1219, 2606, 529, 268, 13, 51540], "temperature": 0.0, "avg_logprob": -0.24631118774414062, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.02448229305446148}, {"id": 155, "seek": 60452, "start": 628.04, "end": 632.24, "text": " And I think last year and the year before there were also talks at FOSDEM about Grameen,", "tokens": [51540, 400, 286, 519, 1036, 1064, 293, 264, 1064, 949, 456, 645, 611, 6686, 412, 479, 4367, 35, 6683, 466, 2606, 529, 268, 11, 51750], "temperature": 0.0, "avg_logprob": -0.24631118774414062, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.02448229305446148}, {"id": 156, "seek": 60452, "start": 632.24, "end": 634.12, "text": " how this exactly works.", "tokens": [51750, 577, 341, 2293, 1985, 13, 51844], "temperature": 0.0, "avg_logprob": -0.24631118774414062, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.02448229305446148}, {"id": 157, "seek": 63412, "start": 634.12, "end": 640.5600000000001, "text": " So with Grameen and with fully porting DacDB into the enclave, there was a 20 time slowdown", "tokens": [50364, 407, 365, 2606, 529, 268, 293, 365, 4498, 2436, 278, 413, 326, 27735, 666, 264, 2058, 27995, 11, 456, 390, 257, 945, 565, 2964, 5093, 50686], "temperature": 0.0, "avg_logprob": -0.1744730207655165, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.02132105641067028}, {"id": 158, "seek": 63412, "start": 640.5600000000001, "end": 646.04, "text": " actually and this is mainly cause because of the expensive EPC swapping.", "tokens": [50686, 767, 293, 341, 307, 8704, 3082, 570, 295, 264, 5124, 462, 12986, 1693, 10534, 13, 50960], "temperature": 0.0, "avg_logprob": -0.1744730207655165, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.02132105641067028}, {"id": 159, "seek": 63412, "start": 646.04, "end": 652.04, "text": " So to mitigate this, the student tried to instead of keeping all memory buffers inside", "tokens": [50960, 407, 281, 27336, 341, 11, 264, 3107, 3031, 281, 2602, 295, 5145, 439, 4675, 9204, 433, 1854, 51260], "temperature": 0.0, "avg_logprob": -0.1744730207655165, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.02132105641067028}, {"id": 160, "seek": 63412, "start": 652.04, "end": 657.16, "text": " the enclave, pull some memory buffers outside the enclave and crib them outside the enclave", "tokens": [51260, 264, 2058, 27995, 11, 2235, 512, 4675, 9204, 433, 2380, 264, 2058, 27995, 293, 47163, 552, 2380, 264, 2058, 27995, 51516], "temperature": 0.0, "avg_logprob": -0.1744730207655165, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.02132105641067028}, {"id": 161, "seek": 63412, "start": 657.16, "end": 659.12, "text": " and this way try to run DacDB.", "tokens": [51516, 293, 341, 636, 853, 281, 1190, 413, 326, 27735, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1744730207655165, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.02132105641067028}, {"id": 162, "seek": 65912, "start": 659.12, "end": 666.2, "text": " And this already gave a significant speed up but still there was a 30 time slowdown.", "tokens": [50364, 400, 341, 1217, 2729, 257, 4776, 3073, 493, 457, 920, 456, 390, 257, 2217, 565, 2964, 5093, 13, 50718], "temperature": 0.0, "avg_logprob": -0.19133461438692534, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.11245716363191605}, {"id": 163, "seek": 65912, "start": 666.2, "end": 670.04, "text": " The second approach that he did was the minimal DbMS split.", "tokens": [50718, 440, 1150, 3109, 300, 415, 630, 390, 264, 13206, 413, 65, 10288, 7472, 13, 50910], "temperature": 0.0, "avg_logprob": -0.19133461438692534, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.11245716363191605}, {"id": 164, "seek": 65912, "start": 670.04, "end": 674.64, "text": " He put basically all the operators inside the enclave and left the rest out of the", "tokens": [50910, 634, 829, 1936, 439, 264, 19077, 1854, 264, 2058, 27995, 293, 1411, 264, 1472, 484, 295, 264, 51140], "temperature": 0.0, "avg_logprob": -0.19133461438692534, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.11245716363191605}, {"id": 165, "seek": 65912, "start": 674.64, "end": 681.88, "text": " enclave because this enabled to have factorized processing still and that really increases", "tokens": [51140, 2058, 27995, 570, 341, 15172, 281, 362, 5952, 1602, 9007, 920, 293, 300, 534, 8637, 51502], "temperature": 0.0, "avg_logprob": -0.19133461438692534, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.11245716363191605}, {"id": 166, "seek": 65912, "start": 681.88, "end": 683.6800000000001, "text": " performance.", "tokens": [51502, 3389, 13, 51592], "temperature": 0.0, "avg_logprob": -0.19133461438692534, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.11245716363191605}, {"id": 167, "seek": 68368, "start": 683.68, "end": 690.3599999999999, "text": " And a second optimization that he did was replacing Ecos and Ocos by a synchronous request", "tokens": [50364, 400, 257, 1150, 19618, 300, 415, 630, 390, 19139, 462, 6877, 293, 422, 6877, 538, 257, 44743, 5308, 50698], "temperature": 0.0, "avg_logprob": -0.19781027107595284, "compression_ratio": 1.554263565891473, "no_speech_prob": 0.02238006703555584}, {"id": 168, "seek": 68368, "start": 690.3599999999999, "end": 695.0, "text": " in a shared buffer also called the switches mode I think.", "tokens": [50698, 294, 257, 5507, 21762, 611, 1219, 264, 19458, 4391, 286, 519, 13, 50930], "temperature": 0.0, "avg_logprob": -0.19781027107595284, "compression_ratio": 1.554263565891473, "no_speech_prob": 0.02238006703555584}, {"id": 169, "seek": 68368, "start": 695.0, "end": 700.0, "text": " And this also helped but still there was a 10 time slowdown.", "tokens": [50930, 400, 341, 611, 4254, 457, 920, 456, 390, 257, 1266, 565, 2964, 5093, 13, 51180], "temperature": 0.0, "avg_logprob": -0.19781027107595284, "compression_ratio": 1.554263565891473, "no_speech_prob": 0.02238006703555584}, {"id": 170, "seek": 68368, "start": 700.0, "end": 705.2399999999999, "text": " So a couple years later, now there is of course SGX2 and it doesn't suffer from the main", "tokens": [51180, 407, 257, 1916, 924, 1780, 11, 586, 456, 307, 295, 1164, 34520, 55, 17, 293, 309, 1177, 380, 9753, 490, 264, 2135, 51442], "temperature": 0.0, "avg_logprob": -0.19781027107595284, "compression_ratio": 1.554263565891473, "no_speech_prob": 0.02238006703555584}, {"id": 171, "seek": 68368, "start": 705.2399999999999, "end": 706.76, "text": " memory limitation.", "tokens": [51442, 4675, 27432, 13, 51518], "temperature": 0.0, "avg_logprob": -0.19781027107595284, "compression_ratio": 1.554263565891473, "no_speech_prob": 0.02238006703555584}, {"id": 172, "seek": 68368, "start": 706.76, "end": 713.0799999999999, "text": " So now it's basically easier to port DacDB as a whole to the enclave to SGX2 and we", "tokens": [51518, 407, 586, 309, 311, 1936, 3571, 281, 2436, 413, 326, 27735, 382, 257, 1379, 281, 264, 2058, 27995, 281, 34520, 55, 17, 293, 321, 51834], "temperature": 0.0, "avg_logprob": -0.19781027107595284, "compression_ratio": 1.554263565891473, "no_speech_prob": 0.02238006703555584}, {"id": 173, "seek": 71308, "start": 713.08, "end": 717.84, "text": " did it also with the use of Grameen which also improved the last years a lot so that", "tokens": [50364, 630, 309, 611, 365, 264, 764, 295, 2606, 529, 268, 597, 611, 9689, 264, 1036, 924, 257, 688, 370, 300, 50602], "temperature": 0.0, "avg_logprob": -0.16363605820989036, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00492032989859581}, {"id": 174, "seek": 71308, "start": 717.84, "end": 721.8000000000001, "text": " made it actually surprisingly easy.", "tokens": [50602, 1027, 309, 767, 17600, 1858, 13, 50800], "temperature": 0.0, "avg_logprob": -0.16363605820989036, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00492032989859581}, {"id": 175, "seek": 71308, "start": 721.8000000000001, "end": 728.2, "text": " And we did some benchmarks to actually see okay so what performance difference is there", "tokens": [50800, 400, 321, 630, 512, 43751, 281, 767, 536, 1392, 370, 437, 3389, 2649, 307, 456, 51120], "temperature": 0.0, "avg_logprob": -0.16363605820989036, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00492032989859581}, {"id": 176, "seek": 71308, "start": 728.2, "end": 731.0, "text": " if you run a database fully inside the enclave.", "tokens": [51120, 498, 291, 1190, 257, 8149, 4498, 1854, 264, 2058, 27995, 13, 51260], "temperature": 0.0, "avg_logprob": -0.16363605820989036, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00492032989859581}, {"id": 177, "seek": 71308, "start": 731.0, "end": 738.36, "text": " So before going into the results we did the benchmarks with TPCH which is a standard industry", "tokens": [51260, 407, 949, 516, 666, 264, 3542, 321, 630, 264, 43751, 365, 314, 12986, 39, 597, 307, 257, 3832, 3518, 51628], "temperature": 0.0, "avg_logprob": -0.16363605820989036, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00492032989859581}, {"id": 178, "seek": 73836, "start": 738.36, "end": 744.6, "text": " benchmark for analytical workloads so basically for data science workloads so there are no", "tokens": [50364, 18927, 337, 29579, 32452, 370, 1936, 337, 1412, 3497, 32452, 370, 456, 366, 572, 50676], "temperature": 0.0, "avg_logprob": -0.2360126707288954, "compression_ratio": 1.6294416243654823, "no_speech_prob": 0.38086384534835815}, {"id": 179, "seek": 73836, "start": 744.6, "end": 749.96, "text": " inputs inserts or updates but just analytics basically.", "tokens": [50676, 15743, 49163, 420, 9205, 457, 445, 15370, 1936, 13, 50944], "temperature": 0.0, "avg_logprob": -0.2360126707288954, "compression_ratio": 1.6294416243654823, "no_speech_prob": 0.38086384534835815}, {"id": 180, "seek": 73836, "start": 749.96, "end": 755.04, "text": " And we compared it first with Grameen itself because since Grameen replace a system calls", "tokens": [50944, 400, 321, 5347, 309, 700, 365, 2606, 529, 268, 2564, 570, 1670, 2606, 529, 268, 7406, 257, 1185, 5498, 51198], "temperature": 0.0, "avg_logprob": -0.2360126707288954, "compression_ratio": 1.6294416243654823, "no_speech_prob": 0.38086384534835815}, {"id": 181, "seek": 73836, "start": 755.04, "end": 763.9200000000001, "text": " it also incurs some overhead but as you can see most overhead is caused by SGX self.", "tokens": [51198, 309, 611, 834, 2156, 512, 19922, 457, 382, 291, 393, 536, 881, 19922, 307, 7008, 538, 34520, 55, 2698, 13, 51642], "temperature": 0.0, "avg_logprob": -0.2360126707288954, "compression_ratio": 1.6294416243654823, "no_speech_prob": 0.38086384534835815}, {"id": 182, "seek": 76392, "start": 763.92, "end": 771.92, "text": " On average we would say there is a 10 to 20 percent overhead but here we normalize baseline", "tokens": [50364, 1282, 4274, 321, 576, 584, 456, 307, 257, 1266, 281, 945, 3043, 19922, 457, 510, 321, 2710, 1125, 20518, 50764], "temperature": 0.0, "avg_logprob": -0.13239026838733303, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.16971318423748016}, {"id": 183, "seek": 76392, "start": 771.92, "end": 777.88, "text": " DacDB so that you actually can see the actual overhead per query and there are some specific", "tokens": [50764, 413, 326, 27735, 370, 300, 291, 767, 393, 536, 264, 3539, 19922, 680, 14581, 293, 456, 366, 512, 2685, 51062], "temperature": 0.0, "avg_logprob": -0.13239026838733303, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.16971318423748016}, {"id": 184, "seek": 76392, "start": 777.88, "end": 785.64, "text": " queries such as query 12 and query 15 where the overhead is actually more than twice.", "tokens": [51062, 24109, 1270, 382, 14581, 2272, 293, 14581, 2119, 689, 264, 19922, 307, 767, 544, 813, 6091, 13, 51450], "temperature": 0.0, "avg_logprob": -0.13239026838733303, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.16971318423748016}, {"id": 185, "seek": 76392, "start": 785.64, "end": 789.56, "text": " So this might be a bit problematic.", "tokens": [51450, 407, 341, 1062, 312, 257, 857, 19011, 13, 51646], "temperature": 0.0, "avg_logprob": -0.13239026838733303, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.16971318423748016}, {"id": 186, "seek": 76392, "start": 789.56, "end": 793.64, "text": " So we did some research we tried to identify okay so what is it in these queries that", "tokens": [51646, 407, 321, 630, 512, 2132, 321, 3031, 281, 5876, 1392, 370, 437, 307, 309, 294, 613, 24109, 300, 51850], "temperature": 0.0, "avg_logprob": -0.13239026838733303, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.16971318423748016}, {"id": 187, "seek": 79364, "start": 793.64, "end": 800.88, "text": " causes the overhead and we found that mainly strangely enough the overhead is introduced", "tokens": [50364, 7700, 264, 19922, 293, 321, 1352, 300, 8704, 39851, 1547, 264, 19922, 307, 7268, 50726], "temperature": 0.0, "avg_logprob": -0.212362351624862, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.0435703881084919}, {"id": 188, "seek": 79364, "start": 800.88, "end": 807.68, "text": " by O-calls so by E-enters and E-exits and we tried to investigate a bit further which", "tokens": [50726, 538, 422, 12, 66, 39655, 370, 538, 462, 12, 317, 433, 293, 462, 12, 3121, 1208, 293, 321, 3031, 281, 15013, 257, 857, 3052, 597, 51066], "temperature": 0.0, "avg_logprob": -0.212362351624862, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.0435703881084919}, {"id": 189, "seek": 79364, "start": 807.68, "end": 812.4399999999999, "text": " system called it then was but there was some kind of timing function that seems to be executed", "tokens": [51066, 1185, 1219, 309, 550, 390, 457, 456, 390, 512, 733, 295, 10822, 2445, 300, 2544, 281, 312, 17577, 51304], "temperature": 0.0, "avg_logprob": -0.212362351624862, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.0435703881084919}, {"id": 190, "seek": 79364, "start": 812.4399999999999, "end": 814.64, "text": " outside of the enclave.", "tokens": [51304, 2380, 295, 264, 2058, 27995, 13, 51414], "temperature": 0.0, "avg_logprob": -0.212362351624862, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.0435703881084919}, {"id": 191, "seek": 79364, "start": 814.64, "end": 822.56, "text": " And also within these queries there are two times as much page faults and well one optimization", "tokens": [51414, 400, 611, 1951, 613, 24109, 456, 366, 732, 1413, 382, 709, 3028, 36090, 293, 731, 472, 19618, 51810], "temperature": 0.0, "avg_logprob": -0.212362351624862, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.0435703881084919}, {"id": 192, "seek": 82256, "start": 822.56, "end": 830.92, "text": " that we tried we're still working on it but was increasing the factor size in DacDB because", "tokens": [50364, 300, 321, 3031, 321, 434, 920, 1364, 322, 309, 457, 390, 5662, 264, 5952, 2744, 294, 413, 326, 27735, 570, 50782], "temperature": 0.0, "avg_logprob": -0.1592851526596967, "compression_ratio": 1.625, "no_speech_prob": 0.1887558549642563}, {"id": 193, "seek": 82256, "start": 830.92, "end": 837.76, "text": " usually in DacDB the factor size consists of 2048 tuples and usually this gives low", "tokens": [50782, 2673, 294, 413, 326, 27735, 264, 5952, 2744, 14689, 295, 945, 13318, 2604, 2622, 293, 2673, 341, 2709, 2295, 51124], "temperature": 0.0, "avg_logprob": -0.1592851526596967, "compression_ratio": 1.625, "no_speech_prob": 0.1887558549642563}, {"id": 194, "seek": 82256, "start": 837.76, "end": 843.92, "text": " L1 cache misses but it can incur many EPC calls so with increasing the factor size you", "tokens": [51124, 441, 16, 19459, 29394, 457, 309, 393, 35774, 867, 462, 12986, 5498, 370, 365, 5662, 264, 5952, 2744, 291, 51432], "temperature": 0.0, "avg_logprob": -0.1592851526596967, "compression_ratio": 1.625, "no_speech_prob": 0.1887558549642563}, {"id": 195, "seek": 82256, "start": 843.92, "end": 849.8, "text": " basically maximize IO and IO is very expensive in the enclave and we actually found that", "tokens": [51432, 1936, 19874, 39839, 293, 39839, 307, 588, 5124, 294, 264, 2058, 27995, 293, 321, 767, 1352, 300, 51726], "temperature": 0.0, "avg_logprob": -0.1592851526596967, "compression_ratio": 1.625, "no_speech_prob": 0.1887558549642563}, {"id": 196, "seek": 84980, "start": 849.8, "end": 860.1999999999999, "text": " if you increase the factor size to 16384 that the performance overhead is actually minimized", "tokens": [50364, 498, 291, 3488, 264, 5952, 2744, 281, 3165, 12625, 19, 300, 264, 3389, 19922, 307, 767, 4464, 1602, 50884], "temperature": 0.0, "avg_logprob": -0.13163469605526681, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0490214079618454}, {"id": 197, "seek": 84980, "start": 860.1999999999999, "end": 867.92, "text": " for this workload and a small note is that not for all queries actually the performance", "tokens": [50884, 337, 341, 20139, 293, 257, 1359, 3637, 307, 300, 406, 337, 439, 24109, 767, 264, 3389, 51270], "temperature": 0.0, "avg_logprob": -0.13163469605526681, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0490214079618454}, {"id": 198, "seek": 84980, "start": 867.92, "end": 874.16, "text": " improved but just for the queries with a lot of overhead it seems to be really beneficial", "tokens": [51270, 9689, 457, 445, 337, 264, 24109, 365, 257, 688, 295, 19922, 309, 2544, 281, 312, 534, 14072, 51582], "temperature": 0.0, "avg_logprob": -0.13163469605526681, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0490214079618454}, {"id": 199, "seek": 87416, "start": 874.16, "end": 881.88, "text": " to increase the factor size in DacDB.", "tokens": [50364, 281, 3488, 264, 5952, 2744, 294, 413, 326, 27735, 13, 50750], "temperature": 0.0, "avg_logprob": -0.13539810703225333, "compression_ratio": 1.5048076923076923, "no_speech_prob": 0.05648241937160492}, {"id": 200, "seek": 87416, "start": 881.88, "end": 886.4399999999999, "text": " So this is very much work in process it's more a prototype than something you can actually", "tokens": [50750, 407, 341, 307, 588, 709, 589, 294, 1399, 309, 311, 544, 257, 19475, 813, 746, 291, 393, 767, 50978], "temperature": 0.0, "avg_logprob": -0.13539810703225333, "compression_ratio": 1.5048076923076923, "no_speech_prob": 0.05648241937160492}, {"id": 201, "seek": 87416, "start": 886.4399999999999, "end": 893.3199999999999, "text": " use in production so please don't do it yet but we can conclude that analytics can actually", "tokens": [50978, 764, 294, 4265, 370, 1767, 500, 380, 360, 309, 1939, 457, 321, 393, 16886, 300, 15370, 393, 767, 51322], "temperature": 0.0, "avg_logprob": -0.13539810703225333, "compression_ratio": 1.5048076923076923, "no_speech_prob": 0.05648241937160492}, {"id": 202, "seek": 87416, "start": 893.3199999999999, "end": 901.12, "text": " perform people from the relatively efficient in SGX2 and the overhead seems to be acceptable", "tokens": [51322, 2042, 561, 490, 264, 7226, 7148, 294, 34520, 55, 17, 293, 264, 19922, 2544, 281, 312, 15513, 51712], "temperature": 0.0, "avg_logprob": -0.13539810703225333, "compression_ratio": 1.5048076923076923, "no_speech_prob": 0.05648241937160492}, {"id": 203, "seek": 90112, "start": 901.4, "end": 906.96, "text": " but the question now is we can protect data in use so data in secure memory but what about", "tokens": [50378, 457, 264, 1168, 586, 307, 321, 393, 2371, 1412, 294, 764, 370, 1412, 294, 7144, 4675, 457, 437, 466, 50656], "temperature": 0.0, "avg_logprob": -0.1410429311353107, "compression_ratio": 1.8578680203045685, "no_speech_prob": 0.10323123633861542}, {"id": 204, "seek": 90112, "start": 906.96, "end": 911.52, "text": " the data in unsecure memory right now because if you go outside the enclave the data is", "tokens": [50656, 264, 1412, 294, 517, 8159, 540, 4675, 558, 586, 570, 498, 291, 352, 2380, 264, 2058, 27995, 264, 1412, 307, 50884], "temperature": 0.0, "avg_logprob": -0.1410429311353107, "compression_ratio": 1.8578680203045685, "no_speech_prob": 0.10323123633861542}, {"id": 205, "seek": 90112, "start": 911.52, "end": 919.04, "text": " not protected by default and so we will actually need some kind of encryption mechanism and", "tokens": [50884, 406, 10594, 538, 7576, 293, 370, 321, 486, 767, 643, 512, 733, 295, 29575, 7513, 293, 51260], "temperature": 0.0, "avg_logprob": -0.1410429311353107, "compression_ratio": 1.8578680203045685, "no_speech_prob": 0.10323123633861542}, {"id": 206, "seek": 90112, "start": 919.04, "end": 925.12, "text": " DacDB right now has actually parquet encryption so we are already capable of encrypting parquet", "tokens": [51260, 413, 326, 27735, 558, 586, 575, 767, 971, 19343, 29575, 370, 321, 366, 1217, 8189, 295, 17972, 662, 278, 971, 19343, 51564], "temperature": 0.0, "avg_logprob": -0.1410429311353107, "compression_ratio": 1.8578680203045685, "no_speech_prob": 0.10323123633861542}, {"id": 207, "seek": 92512, "start": 925.12, "end": 931.04, "text": " files and decrypting them inside the enclave and then perform secure analytics but in the", "tokens": [50364, 7098, 293, 979, 627, 662, 278, 552, 1854, 264, 2058, 27995, 293, 550, 2042, 7144, 15370, 457, 294, 264, 50660], "temperature": 0.0, "avg_logprob": -0.15691646309786064, "compression_ratio": 1.727699530516432, "no_speech_prob": 0.13320539891719818}, {"id": 208, "seek": 92512, "start": 931.04, "end": 936.96, "text": " end our goal is to design to build something that is fully functional and that is fully", "tokens": [50660, 917, 527, 3387, 307, 281, 1715, 281, 1322, 746, 300, 307, 4498, 11745, 293, 300, 307, 4498, 50956], "temperature": 0.0, "avg_logprob": -0.15691646309786064, "compression_ratio": 1.727699530516432, "no_speech_prob": 0.13320539891719818}, {"id": 209, "seek": 92512, "start": 936.96, "end": 942.84, "text": " secure actually for users that want to do secure analytics with DacDB.", "tokens": [50956, 7144, 767, 337, 5022, 300, 528, 281, 360, 7144, 15370, 365, 413, 326, 27735, 13, 51250], "temperature": 0.0, "avg_logprob": -0.15691646309786064, "compression_ratio": 1.727699530516432, "no_speech_prob": 0.13320539891719818}, {"id": 210, "seek": 92512, "start": 942.84, "end": 948.88, "text": " So yeah this is our plan for the future we will of course open source everything but", "tokens": [51250, 407, 1338, 341, 307, 527, 1393, 337, 264, 2027, 321, 486, 295, 1164, 1269, 4009, 1203, 457, 51552], "temperature": 0.0, "avg_logprob": -0.15691646309786064, "compression_ratio": 1.727699530516432, "no_speech_prob": 0.13320539891719818}, {"id": 211, "seek": 92512, "start": 948.88, "end": 951.64, "text": " yeah thank you for your attention.", "tokens": [51552, 1338, 1309, 291, 337, 428, 3202, 13, 51690], "temperature": 0.0, "avg_logprob": -0.15691646309786064, "compression_ratio": 1.727699530516432, "no_speech_prob": 0.13320539891719818}, {"id": 212, "seek": 95512, "start": 955.12, "end": 972.08, "text": " Hi thank you for a very nice talk so I was wondering you talked about like this overhead", "tokens": [50364, 2421, 1309, 291, 337, 257, 588, 1481, 751, 370, 286, 390, 6359, 291, 2825, 466, 411, 341, 19922, 51212], "temperature": 0.0, "avg_logprob": -0.175477688962763, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.011728061363101006}, {"id": 213, "seek": 95512, "start": 972.08, "end": 977.64, "text": " that you were attributing to the old calls going out of the enclave and some of the commercial", "tokens": [51212, 300, 291, 645, 9080, 10861, 281, 264, 1331, 5498, 516, 484, 295, 264, 2058, 27995, 293, 512, 295, 264, 6841, 51490], "temperature": 0.0, "avg_logprob": -0.175477688962763, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.011728061363101006}, {"id": 214, "seek": 97764, "start": 977.64, "end": 985.28, "text": " SGX frameworks use these techniques where you actually batch these together and they", "tokens": [50364, 34520, 55, 29834, 764, 613, 7512, 689, 291, 767, 15245, 613, 1214, 293, 436, 50746], "temperature": 0.0, "avg_logprob": -0.28154656546456475, "compression_ratio": 1.6582914572864322, "no_speech_prob": 0.19760137796401978}, {"id": 215, "seek": 97764, "start": 985.28, "end": 991.16, "text": " are commonly called asynchronous old calls so did you look into that at all and or do", "tokens": [50746, 366, 12719, 1219, 49174, 1331, 5498, 370, 630, 291, 574, 666, 300, 412, 439, 293, 420, 360, 51040], "temperature": 0.0, "avg_logprob": -0.28154656546456475, "compression_ratio": 1.6582914572864322, "no_speech_prob": 0.19760137796401978}, {"id": 216, "seek": 97764, "start": 991.16, "end": 996.68, "text": " you have like some insights how that could affect the performance.", "tokens": [51040, 291, 362, 411, 512, 14310, 577, 300, 727, 3345, 264, 3389, 13, 51316], "temperature": 0.0, "avg_logprob": -0.28154656546456475, "compression_ratio": 1.6582914572864322, "no_speech_prob": 0.19760137796401978}, {"id": 217, "seek": 97764, "start": 996.68, "end": 1001.84, "text": " Okay so your question basically is if we looked into the asynchronous old calls right or the", "tokens": [51316, 1033, 370, 428, 1168, 1936, 307, 498, 321, 2956, 666, 264, 49174, 1331, 5498, 558, 420, 264, 51574], "temperature": 0.0, "avg_logprob": -0.28154656546456475, "compression_ratio": 1.6582914572864322, "no_speech_prob": 0.19760137796401978}, {"id": 218, "seek": 100184, "start": 1001.84, "end": 1009.6, "text": " asynchronous buffer basically well the master student from he looked into that and indeed", "tokens": [50364, 49174, 21762, 1936, 731, 264, 4505, 3107, 490, 415, 2956, 666, 300, 293, 6451, 50752], "temperature": 0.0, "avg_logprob": -0.21391103206536707, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.3507094979286194}, {"id": 219, "seek": 100184, "start": 1009.6, "end": 1015.6800000000001, "text": " improved performance we were planning on actually doing some benchmarks with this specific mode", "tokens": [50752, 9689, 3389, 321, 645, 5038, 322, 767, 884, 512, 43751, 365, 341, 2685, 4391, 51056], "temperature": 0.0, "avg_logprob": -0.21391103206536707, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.3507094979286194}, {"id": 220, "seek": 100184, "start": 1015.6800000000001, "end": 1020.6800000000001, "text": " but we just didn't do it yet but we are still investigating but as far as I understand it", "tokens": [51056, 457, 321, 445, 994, 380, 360, 309, 1939, 457, 321, 366, 920, 22858, 457, 382, 1400, 382, 286, 1223, 309, 51306], "temperature": 0.0, "avg_logprob": -0.21391103206536707, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.3507094979286194}, {"id": 221, "seek": 100184, "start": 1020.6800000000001, "end": 1028.48, "text": " is a little bit less secure to use this mode so yeah it will always be trade off but I", "tokens": [51306, 307, 257, 707, 857, 1570, 7144, 281, 764, 341, 4391, 370, 1338, 309, 486, 1009, 312, 4923, 766, 457, 286, 51696], "temperature": 0.0, "avg_logprob": -0.21391103206536707, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.3507094979286194}, {"id": 222, "seek": 102848, "start": 1028.52, "end": 1033.0, "text": " suspect that it will improve performance quite a bit so reduced the overhead in the end.", "tokens": [50366, 9091, 300, 309, 486, 3470, 3389, 1596, 257, 857, 370, 9212, 264, 19922, 294, 264, 917, 13, 50590], "temperature": 0.0, "avg_logprob": -0.3625885645548503, "compression_ratio": 1.5026737967914439, "no_speech_prob": 0.01640189252793789}, {"id": 223, "seek": 102848, "start": 1033.0, "end": 1047.1200000000001, "text": " Yeah probably a stupid and provocative question have you tried shoving the whole database in a", "tokens": [50590, 865, 1391, 257, 6631, 293, 47663, 1168, 362, 291, 3031, 2223, 798, 264, 1379, 8149, 294, 257, 51296], "temperature": 0.0, "avg_logprob": -0.3625885645548503, "compression_ratio": 1.5026737967914439, "no_speech_prob": 0.01640189252793789}, {"id": 224, "seek": 102848, "start": 1047.1200000000001, "end": 1054.56, "text": " secure instance like 7S and P or TDX and comparing the performance between like SGX and TDX or 7S", "tokens": [51296, 7144, 5197, 411, 1614, 50, 293, 430, 420, 42606, 55, 293, 15763, 264, 3389, 1296, 411, 34520, 55, 293, 42606, 55, 420, 1614, 50, 51668], "temperature": 0.0, "avg_logprob": -0.3625885645548503, "compression_ratio": 1.5026737967914439, "no_speech_prob": 0.01640189252793789}, {"id": 225, "seek": 105456, "start": 1054.6, "end": 1064.04, "text": " and P solution. Okay so the question is did we use other secure environments basically the answer", "tokens": [50366, 293, 430, 3827, 13, 1033, 370, 264, 1168, 307, 630, 321, 764, 661, 7144, 12388, 1936, 264, 1867, 50838], "temperature": 0.0, "avg_logprob": -0.1764477673698874, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.02149469032883644}, {"id": 226, "seek": 105456, "start": 1064.04, "end": 1071.04, "text": " is no so we have no performance comparisons yet but the plan is actually to do that indeed because", "tokens": [50838, 307, 572, 370, 321, 362, 572, 3389, 33157, 1939, 457, 264, 1393, 307, 767, 281, 360, 300, 6451, 570, 51188], "temperature": 0.0, "avg_logprob": -0.1764477673698874, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.02149469032883644}, {"id": 227, "seek": 105456, "start": 1071.04, "end": 1077.24, "text": " if you want like not everybody is able to run SGX to write so the hardware field is pretty", "tokens": [51188, 498, 291, 528, 411, 406, 2201, 307, 1075, 281, 1190, 34520, 55, 281, 2464, 370, 264, 8837, 2519, 307, 1238, 51498], "temperature": 0.0, "avg_logprob": -0.1764477673698874, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.02149469032883644}, {"id": 228, "seek": 105456, "start": 1077.24, "end": 1081.9199999999998, "text": " fragmented and we also want to kind of find solutions or at least have comparisons of which", "tokens": [51498, 9241, 14684, 293, 321, 611, 528, 281, 733, 295, 915, 6547, 420, 412, 1935, 362, 33157, 295, 597, 51732], "temperature": 0.0, "avg_logprob": -0.1764477673698874, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.02149469032883644}, {"id": 229, "seek": 108192, "start": 1081.92, "end": 1088.04, "text": " one is the best to use and or maybe even made some kind of framework that people can adopt to", "tokens": [50364, 472, 307, 264, 1151, 281, 764, 293, 420, 1310, 754, 1027, 512, 733, 295, 8388, 300, 561, 393, 6878, 281, 50670], "temperature": 0.0, "avg_logprob": -0.2216109073523319, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.008498969487845898}, {"id": 230, "seek": 108192, "start": 1088.04, "end": 1103.76, "text": " easily run also on different kind of hardware instructions. Yeah. Thank you I want to ask", "tokens": [50670, 3612, 1190, 611, 322, 819, 733, 295, 8837, 9415, 13, 865, 13, 1044, 291, 286, 528, 281, 1029, 51456], "temperature": 0.0, "avg_logprob": -0.2216109073523319, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.008498969487845898}, {"id": 231, "seek": 108192, "start": 1103.76, "end": 1110.04, "text": " about the fully secure on the slide. Have you talked about side channels and what's your vision on", "tokens": [51456, 466, 264, 4498, 7144, 322, 264, 4137, 13, 3560, 291, 2825, 466, 1252, 9235, 293, 437, 311, 428, 5201, 322, 51770], "temperature": 0.0, "avg_logprob": -0.2216109073523319, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.008498969487845898}, {"id": 232, "seek": 111004, "start": 1110.08, "end": 1118.84, "text": " that? Do you want to answer? Yeah in short yes this is a problem because all the research that we", "tokens": [50366, 300, 30, 1144, 291, 528, 281, 1867, 30, 865, 294, 2099, 2086, 341, 307, 257, 1154, 570, 439, 264, 2132, 300, 321, 50804], "temperature": 0.0, "avg_logprob": -0.18834046884016556, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.01601424440741539}, {"id": 233, "seek": 111004, "start": 1118.84, "end": 1124.08, "text": " found there is always a trade off between performance and security and literally all the papers", "tokens": [50804, 1352, 456, 307, 1009, 257, 4923, 766, 1296, 3389, 293, 3825, 293, 3736, 439, 264, 10577, 51066], "temperature": 0.0, "avg_logprob": -0.18834046884016556, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.01601424440741539}, {"id": 234, "seek": 111004, "start": 1124.08, "end": 1129.84, "text": " build this sort of model like cost model in not in terms of cost but in terms of information leakage.", "tokens": [51066, 1322, 341, 1333, 295, 2316, 411, 2063, 2316, 294, 406, 294, 2115, 295, 2063, 457, 294, 2115, 295, 1589, 47799, 13, 51354], "temperature": 0.0, "avg_logprob": -0.18834046884016556, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.01601424440741539}, {"id": 235, "seek": 111004, "start": 1129.84, "end": 1136.36, "text": " So a lot of people papers just say that yes we acknowledge that there are going to be some", "tokens": [51354, 407, 257, 688, 295, 561, 10577, 445, 584, 300, 2086, 321, 10692, 300, 456, 366, 516, 281, 312, 512, 51680], "temperature": 0.0, "avg_logprob": -0.18834046884016556, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.01601424440741539}, {"id": 236, "seek": 113636, "start": 1136.3999999999999, "end": 1142.6399999999999, "text": " trade off some attacks in fact yes yes this is absolutely the case that it can happen but", "tokens": [50366, 4923, 766, 512, 8122, 294, 1186, 2086, 2086, 341, 307, 3122, 264, 1389, 300, 309, 393, 1051, 457, 50678], "temperature": 0.0, "avg_logprob": -0.15145282285759248, "compression_ratio": 1.6347826086956523, "no_speech_prob": 0.00736114801838994}, {"id": 237, "seek": 113636, "start": 1142.6399999999999, "end": 1150.6, "text": " right now the goal was first to have something that is somewhat functional on some sort of", "tokens": [50678, 558, 586, 264, 3387, 390, 700, 281, 362, 746, 300, 307, 8344, 11745, 322, 512, 1333, 295, 51076], "temperature": 0.0, "avg_logprob": -0.15145282285759248, "compression_ratio": 1.6347826086956523, "no_speech_prob": 0.00736114801838994}, {"id": 238, "seek": 113636, "start": 1150.6, "end": 1155.9199999999998, "text": " database workloads because as I said the big limitations of SGX-1 made the whole thing completely", "tokens": [51076, 8149, 32452, 570, 382, 286, 848, 264, 955, 15705, 295, 34520, 55, 12, 16, 1027, 264, 1379, 551, 2584, 51342], "temperature": 0.0, "avg_logprob": -0.15145282285759248, "compression_ratio": 1.6347826086956523, "no_speech_prob": 0.00736114801838994}, {"id": 239, "seek": 113636, "start": 1155.9199999999998, "end": 1162.36, "text": " invisible but now that this is actually possible we can also focus on how to fix these issues but", "tokens": [51342, 14603, 457, 586, 300, 341, 307, 767, 1944, 321, 393, 611, 1879, 322, 577, 281, 3191, 613, 2663, 457, 51664], "temperature": 0.0, "avg_logprob": -0.15145282285759248, "compression_ratio": 1.6347826086956523, "no_speech_prob": 0.00736114801838994}, {"id": 240, "seek": 116236, "start": 1162.36, "end": 1170.6, "text": " unfortunately research tended not to acknowledge this issue so much in the past but for future", "tokens": [50364, 7015, 2132, 34732, 406, 281, 10692, 341, 2734, 370, 709, 294, 264, 1791, 457, 337, 2027, 50776], "temperature": 0.0, "avg_logprob": -0.3539376068115234, "compression_ratio": 1.1888888888888889, "no_speech_prob": 0.011399844661355019}, {"id": 241, "seek": 116236, "start": 1170.6, "end": 1173.4399999999998, "text": " yes we will.", "tokens": [50776, 2086, 321, 486, 13, 50918], "temperature": 0.0, "avg_logprob": -0.3539376068115234, "compression_ratio": 1.1888888888888889, "no_speech_prob": 0.011399844661355019}], "language": "en"}