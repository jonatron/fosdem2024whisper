{"text": " Hello, hey everyone. Welcome to Making It Easy to Get to Salsa Level 2. Thanks for sticking around. It's last day of the conference, send me last talk. So, yeah, today we're going to be talking about salsa and compliance and hopefully how you can meet those compliance requirements as a play. My name is Theophilus and I'm going to be talking about Choc and open source framework. We developed that crash override. So I come from a security background and every time I hear the word compliance I get bored to death. It's kind of like a book sticking exercise. But hopefully we can discuss today about this and see how you can do this in your own organization easily while also get value for your org. Before jumping into the topic, let me kind of quickly set the scene and talk a little bit about software supply chain attacks. So in a software supply chain attack, the attackers compromise the build system or the package registry and get a foothold there. And over the past years we've been seeing an increase in these types of attacks. So there was a report say from Sonatype that said since 2019, year after year, we've been seeing a sevenfold increase in this type of attacks. The report came out in 2022 that said supply chain attacks basically surpassed malware based attacks by 40%. And last year around two out of three of US businesses were impacted by a supply chain attack. So you can take these numbers with a grain of salt, but the fact of the matter is there is a surge in these types of attacks. And this popularity on the attack realm drives policy changes. So in May 2021, there was an executive order by the White House that said software vendors must be provided and purchased with software of materials and provenance information. And quick show of hands. How many are familiar with the term S-bombs or provenance? Cool. How many of you have been deploying these in your pipelines or your organizations? Okay, great. There's also an S-bomb room. So today we're going to jump into these topics real quick. So we're going to discuss some concepts and then talk about the challenges people face when trying to deploy these things to production. Then we're going to talk about CHOC and how CHOC can help you solve these problems but achieve many, many more things and hopefully have a discussion in the end. So for those of you who are not familiar with software bill of materials or S-bombs, you can think of it like a list of ingredients for software. So you go to the supermarket, you see a package, then you read the labels and you get a list of all the ingredients that are in there. So an S-bomb is pretty much the same thing but for your software applications. So you get either an XML or a JSON and from that you can get a list of the packages, their versions, etc., etc. When we're talking about provenance, what we're talking about really is how did the artifact get here? Like who created it, who packaged it, how was it modified along the way until it actually reaches the user basically. So that is all good but if we think about a list of ingredients, what are the guarantees that what we get is actually what we're promised? So for instance you could have an NPM package and you can generate an S-bomb for your NPM package saying that these are the ingredients that are there but then in a package you could get a foothold somewhere in your build pipeline and inject something that was not originally there. So another key component here besides generating the S-bomb and the provenance formation is really having some attestation around the integrity of the generated artifacts. So anyone should be able to cryptographically verify that at least what we're promised has not been tampered with and that the contents of the S-bomb were coming from an original author, etc., etc. And what's really important here is we need to have some clear assumptions around the threat model aka what can an attacker compromise and what are the security guarantees we're getting depending on that. So do we require the attacker say to compromise our build pipeline or do we require the attacker to get a foothold on developers boxes? What's our threat model? And that's really important because if we think about DevOps pipelines in practice you have many components like developers are pushing codes, that code ends up in some provider like GitHub or GitLab, you have open source packages, you have container images, you have infrastructure code that modifies this code and pushes it out and then somehow it ends up in the service or the cloud. And as we're building out all these graphs of components attackers could get a foothold at various places. So this is where Salsa comes in. Salsa is an open SSF project and essentially gives us some framework to talk about the security posture of our applications. And we have different levels for the supply chain security of our artifacts. At level one essentially all we're doing is we are providing information about how the package was built and have a report but we don't really have guarantees around the report whether it has been tampered with or not. At level two we get signed provenance. Essentially at this point we say okay once the thing has been generated there has not been tampering on that artifact but you don't get guarantees around the build platform etc. And as you move up the layers you get stronger and stronger security guarantees. So today we're going to be talking about chocking how easy it is to get to Salsa level two if you deploy chocking to your built pipelines. So how does one start to do this? This is good, we all want to improve the security posture of our applications, we want to deploy these things in our organization, how does one start to do it? One could think that okay that surely a solved problem there must be tools for this already and you're right to some extent but the tooling ecosystem is really in its infancy and it's largely fragmented at this point. So it's not necessarily obvious to a newcomer which tool or framework they should pick and even if you say select a space like S-Bombs the outputs of different tools are inconsistent with each other. One tool get a certain report, another tool get some different report and there might be assumptions around how these things should be getting set up, how you should be deploying all these things so it's not a straightforward way and what's really really hard is thinking about how can you do this at scale. If you have a large organization with multiple repositories, different providers, how do you make it easy for your teams to just set this and let it run and have it be easy to consume the data and also generate data that are of interest to you. So yeah it's not an easy problem and hopefully Chalk will help here. The main idea behind Chalk is really we have some metadata that we care about and then we want to embed that metadata that we call Chalkmarks into the artifact. So the artifact could be a binary or it could be a docker image and you want to embed this metadata into the artifact during the build time or post build time. So you could have an L file in a box and then you can inject metadata into that L file and say okay that was indeed here, you can have information that you care about like the security settings on that box like if a partner is enabled or what are the users or the network connections, you embed that metadata on it and now that artifact is tagged and once you have that tagged artifact you basically let it go and it gets deployed somewhere in production. So think of Chalk pretty much like air tags for your code so you embed the air tags and then you're tracking it across the ecosystem of your infrastructure and once the artifact actually gets executed what's interesting is you can get back reports with metadata that you configured there. So essentially you can scan what has been out there in production, you can grab for all this metadata that has been embedded in the artifacts or you could configure the artifacts some cases to phone home and give you the report themselves and you could do this once or you could do it periodically for instance configuring Chalk to send you heartbeat reports. So let's see this in action. I have set up here a very very basic git repository and this repository all it does is it deploys a lambda function. So we have the main code of the lambda function here and as you can see there's nothing really special to it, we just sleep and return it to 100k and we're building this lambda function using a docker file and there's nothing specific to Chalk in this docker file pulling from a well known image and we're actually building the lambda using a github action. So during the github action we check out the code, we set up the build environment and then here we're setting up Chalk. So we're telling our build ecosystem that Chalk should drop this build of the image and embed metadata on it and what sort of metadata we choose to embed is completely up to us. It comes like Chalk comes with defaults. So these are the only lines of code we ever need to do for our build pipeline so that Chalk can embed sbombs and actually use, you know, provide cryptographic guarantees around the integrity of the generated reports and we're also creating attestation manifests using SIGS-Tor for those of you who are aware of that framework. So cool, let's go ahead and trigger this. I'm going to go here in the action, kind of re-trigger the action once more and what we're doing here is we're building a docker image and we're telling Chalk to encapsulate the whole build and inject metadata in here. And that metadata, we can choose how we want to emit it. So we can choose to emit a report in there or the CLI or in some destination that we care like S3 or some server. So I have here a dummy server that's running and it's waiting for reports. There's nothing here currently. And I'm going to go back into one of the previous actions and show you a report that was emitted by Chalk on the CLI. So during the build, after we've actually pushed the image, you can see down here we have a Chalk report and this is basically a JSON file that has metadata that we care about. So here we know that some image could build, that was a daytime, that was a docker file path, the exact contents of the docker file, the commit ID, the author of the committer, but you also get a cryptographic signature about the integrity of this report essentially. You get interesting things like the environment variables, arguments. You can configure this to be however you like it. And this is generated on the CLI, but we can send the exact report, the exact same report or variance of that report in other destinations. So going back here to the action we just triggered, hopefully once this completes, we will be seeing a report populated to our server. So not only will we see a report here on the CLI, but we'll also get the metadata in the endpoint we configured. What could possibly go wrong? This is just a live demo here. So you can make this as fine-grained as you like. So Chalk supports plugins. So if you want to run, say, your static analysis tools like SemGrid or CodeQL, you can embed this metadata into the report as part of your regular other metadata that you're tracking. So it looks like this got finished. So we did get a report here. And if I go here, essentially we see that we got a build operation, so that got sent over to our server. And this is essentially just a pre-defined rendering of the JSON, right? You can send it wherever you see fit and render it however you would like. But we get some interesting information. We get some signal that we collected S-Bomb and Signing data. And indeed, if I scroll down here, I do see that I have the full S-Bomb. And I can fetch information about the attestation of the artifact. But I also get a bunch of interesting metadata that might not have been obvious just by seeing the build. So I see here CloudProvider is Azure. And we have information about the actual Azure instance metadata in which the build happened. So essentially what happens here is GitHub runs their machines on Azure in this particular instance. And so that build triggered in one of the Azure instances. So that's nice. We can also see the build command. And you can see here how the normal build command is now wrapped under Choc. So Choc is in charge of the build and embeds the metadata into your image. So that's nice. What we did do here is we pushed this demo lambda essentially. You can see this was modified just now. So I'm going to go ahead and execute the image. And hopefully, if things work as expected, the lambda will execute. And I'm going to get a second report here. And that second report is an exec. And if I zoom into the exec, you now see that the command that got executed is actually running within the lambda environment. So Choc is wrapping the entry point of the execution for that Docker image and tells you like, hey, this Choc mark that you inserted, the metadata that you've all captured is still here, but now I'm executing in lambda. And indeed, if I go here and see the Cloud metadata, you can see the region, the role they are in, the account ID, et cetera, et cetera. So with this, we can basically say the metadata that we injected in our build pipeline here is still present throughout wherever we deploy the image. And we can keep track of where the thing actually executes. So if I take into this Choc mark, I'm sorry, let me zoom out here. I can see that there's two reports essentially associated with this. One was a build and the other one was an exec for the exact same hash. So the exact same hash that I build in that machine has been executed in the other machine. So what did we do here? First of all, with four lines of YAML in our GitHub action, we generate and distribute the desktops. And we also have provenance information because we can track where the build happened and where the actual image got executed. And we also get artifact integrity. So in our case, we're using cosine. You could use different frameworks to do this. But essentially, we're meeting the basic requirements here. So we're checking those boxes. And that's with minimal effort, in my opinion. Like all you need to do is you need to configure whatever destinations you want for these reports to be sent. So you say, OK, that's cool. What more can you do? So let's think about typical scenarios that happen during kind of live production environments. You might be on call for a given service. And you get a page in the middle of the night. And there is some issue. There's a bug. There's a vulnerability. Something is off. And you want to figure out, OK, what's the component that's responsible for this? You could have, say, a pretty complex application with multiple teams pushing code. And for large organizations, usually the pattern for resolving these issues is you cut the tickets to the team. You wait for somebody else to be seen and be like, hey, that's the responsibility of that person. Potentially, you grab into code. You say, OK, what was the last commit? Or you have metrics. And you track from your metrics what chains. And you try to correlate it to somebody else. If you're using Chalk for your build pipelines, it's much, much easier to correlate what exact version of an image is running where and what the components are. And potentially, like, who are the code owners, et cetera. Because if we go back here, you see that we have things like the cometer and the commit ID. So we have the commit ID. You can start building these profiles about ownership incrementally as you go. So instead of having a process which could potentially take a couple of hours to determine the root cause of an outage or an issue, you now can have this in a few clicks, hopefully. Another common use case is application inventory and change management. So say, for instance, you're part of a large organization. You want to deprecate the framework. You want to deprecate, say, AngularJS. So AngularJS is running production. And you want to figure out, OK, what is the impact? How many teams are using it? Is the code even live? And what was the last time things got executed? You can figure out, you can get reports around these things. More importantly, you can see how applications change over time. So many of the people we've been talking to have processes where, for instance, they do a sort of change management meeting. Like, once a week, they say, OK, what has changed? What has been deployed? Do we need to go through a security review? What's the exact list of changes? And that process is manual to a large extent. Using Choc, you can automate this, because you can generate an exact report of the diff and you can get some integrity guarantees around that report. But more importantly, besides these things, you can do much, much more rightly. It's not necessary that you can only Choc containers. You can run essentially tools of your choosing, or you can submit custom plugins for metadata surfacing. And currently, the open source implementation that we have on GitHub only supports the entry pod wrapping for containers, but we're working to expand Choc functionality with more and more features. You can still Choc L files and PYC files and jars, et cetera. So yeah, the framework is out there. It's written in NIM. NIM is a very, very cool statically compiled type safe language. So any fans of NIM here, feel free to contribute. And we're welcoming feature requests. And I think that's my talk. I'm happy to take questions or discuss this. Thank you. Thank you. Yep. You talked about large organization. I'm very open to second. Yes. So the question here is I brought up large organizations, but given a concrete example of what are some use cases that this would apply in, right? So just to make this clear, this does not only apply to a larger, a small organization. It applies to everyone. It's just that if you are having a single application with a single repository, pretty much you know exactly what version is deployed where. The complexities of these situations start to be amplified the bigger and bigger you get, right? So if you have, say, a web application, and that web application has multiple components that are live at any given time, or say you have a distributed service and you have microservices running, you have multiple teams committing different versions of their component at any given time. And potentially some of these teams change. So you could end up with a repository having outdated code, right? There's a mission now, something has failed, and you go into the code, say, what was the last commit? It was six months ago. The committer of that application has left the team, potentially has left the company. Who do you contact? How do you know that's actually that part that has been outdated? But if you keep track of your builds and your executions, you have the ability now to tap into all the history of, like, all the provenance of a certain artifact and surface metrics that you care about. So if you cared about, say, show me all the components that haven't been updated in the last month or that haven't been executed the last month, it's way, way easier to do this. I'm not sure if I answered the question. Yeah. Yeah. So you showed how to do it in a GitHub action, but could you generalize and do this manually in a one-prime or in a different pipeline environment as well? Yes, yes. Chalk, you can actually, if you go now, if you visit the GitHub repo or the website, you can just download it. And it's a binary that runs. You can run locally and embed metadata into any artifact that you care about on your machine. So you can download on your laptop and scan all the L files in your system or the jar files or whatever, or even scan a whole directory. You can specify whatever you want. And then you can configure metadata that you care about, and these will be embedded there. And you can then extract it. So you don't necessarily need to have Chalk report back to you or run it in a GitHub action. You can just use it to embed information and then surface it. So you can both insert and extract if that makes sense. Yep. So that's a great question. I think one of the big benefits of Chalk is that you can embed information even in generated images or artifacts, right? So if you're using, say you have some third party software like a library that you're consuming, perhaps you don't know where it came from, but you know that you saw it in a certain machine at a certain hash. And then you can use Chalk to encapsulate that information for your artifact. And basically, if you run a query across all your applications that say are importing a given library, you can see all the versions of that library that are running. So you can start building these application inventories very easily, even if it's a third party software. Is it the total of the bottom third party container? It's still the same premise, right? Because if you have a container, you have several layers. So you can start saying, okay, these are the layers I have seen here. And potentially you don't have the full information, but you can at least ensure that you can attest that, okay, these are, this is the hash that I have seen. We are starting to add support to actually wrap entry points of different layers if you'd like to. So you should be able to interpose yourself in another layer should you like to, but that's not currently up yet. It's not up on the open source limitation. Yeah? How does Chalk play together with the useful bits? Are they to include them in the library? That's a great question. No, you don't need to include any compiler. All you need is the binary. And then if you have a reducible build for, in your pipeline, you should still be able to achieve the same guarantees. For instance, if you have, say, an L file, we'll embed metadata into a section and that will survive stripping and all that. So once you have a build, then assuming you know that you're running with Chalk, right, and you don't modify the thing later on inappropriately, you would at least know that you're running with Chalk, right? So that if you're getting a report, that report has not been tampered with. Yep? Let's imagine I have a jar which I have Chalk, right? Then I modify it and zip change it. So at which point Q and V8 and then I Chalk it again. At which point how do you pull the code? Right. So the question is, suppose you have a jar, you Chalk it, and then you modify it and then you Chalk it again. How does the tool help you here? So Chalk does not allow you to have just a single Chalk mark within a binary. You can wrap Chalk marks within Chalk marks within Chalk marks essentially. So if you're making modifications and you'd want it to, you can maintain past information about past Chalk marks. Or if you're building a jar, say, out of other jars and those have Chalk marks, you can use this information and embed them into your final jar, if that makes sense. So you can wrap and encapsulate all the metadata from all the components. So I need to focus more on this. Well it wouldn't be more complex than just saying Chalk insert. Like Chalk would take care of all the build dependencies and make sure it injects it automatically. At least that's where we're heading at. It might not be full feature for all the flavors of what can be choked currently, but that's where we want to go for sure. Cool. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.64, "text": " Hello, hey everyone.", "tokens": [50364, 2425, 11, 4177, 1518, 13, 50796], "temperature": 0.0, "avg_logprob": -0.3896939937884991, "compression_ratio": 1.4455445544554455, "no_speech_prob": 0.2547304630279541}, {"id": 1, "seek": 0, "start": 8.64, "end": 13.32, "text": " Welcome to Making It Easy to Get to Salsa Level 2.", "tokens": [50796, 4027, 281, 14595, 467, 16002, 281, 3240, 281, 318, 24816, 16872, 568, 13, 51030], "temperature": 0.0, "avg_logprob": -0.3896939937884991, "compression_ratio": 1.4455445544554455, "no_speech_prob": 0.2547304630279541}, {"id": 2, "seek": 0, "start": 13.32, "end": 14.32, "text": " Thanks for sticking around.", "tokens": [51030, 2561, 337, 13465, 926, 13, 51080], "temperature": 0.0, "avg_logprob": -0.3896939937884991, "compression_ratio": 1.4455445544554455, "no_speech_prob": 0.2547304630279541}, {"id": 3, "seek": 0, "start": 14.32, "end": 17.72, "text": " It's last day of the conference, send me last talk.", "tokens": [51080, 467, 311, 1036, 786, 295, 264, 7586, 11, 2845, 385, 1036, 751, 13, 51250], "temperature": 0.0, "avg_logprob": -0.3896939937884991, "compression_ratio": 1.4455445544554455, "no_speech_prob": 0.2547304630279541}, {"id": 4, "seek": 0, "start": 17.72, "end": 25.04, "text": " So, yeah, today we're going to be talking about salsa and compliance and hopefully how you", "tokens": [51250, 407, 11, 1338, 11, 965, 321, 434, 516, 281, 312, 1417, 466, 32428, 293, 15882, 293, 4696, 577, 291, 51616], "temperature": 0.0, "avg_logprob": -0.3896939937884991, "compression_ratio": 1.4455445544554455, "no_speech_prob": 0.2547304630279541}, {"id": 5, "seek": 0, "start": 25.04, "end": 28.04, "text": " can meet those compliance requirements as a play.", "tokens": [51616, 393, 1677, 729, 15882, 7728, 382, 257, 862, 13, 51766], "temperature": 0.0, "avg_logprob": -0.3896939937884991, "compression_ratio": 1.4455445544554455, "no_speech_prob": 0.2547304630279541}, {"id": 6, "seek": 2804, "start": 28.04, "end": 32.44, "text": " My name is Theophilus and I'm going to be talking about Choc and open source framework.", "tokens": [50364, 1222, 1315, 307, 440, 5317, 388, 301, 293, 286, 478, 516, 281, 312, 1417, 466, 761, 905, 293, 1269, 4009, 8388, 13, 50584], "temperature": 0.0, "avg_logprob": -0.26638908756589424, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.07956346869468689}, {"id": 7, "seek": 2804, "start": 32.44, "end": 35.2, "text": " We developed that crash override.", "tokens": [50584, 492, 4743, 300, 8252, 42321, 13, 50722], "temperature": 0.0, "avg_logprob": -0.26638908756589424, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.07956346869468689}, {"id": 8, "seek": 2804, "start": 35.2, "end": 40.56, "text": " So I come from a security background and every time I hear the word compliance I get bored", "tokens": [50722, 407, 286, 808, 490, 257, 3825, 3678, 293, 633, 565, 286, 1568, 264, 1349, 15882, 286, 483, 13521, 50990], "temperature": 0.0, "avg_logprob": -0.26638908756589424, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.07956346869468689}, {"id": 9, "seek": 2804, "start": 40.56, "end": 41.56, "text": " to death.", "tokens": [50990, 281, 2966, 13, 51040], "temperature": 0.0, "avg_logprob": -0.26638908756589424, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.07956346869468689}, {"id": 10, "seek": 2804, "start": 41.56, "end": 44.28, "text": " It's kind of like a book sticking exercise.", "tokens": [51040, 467, 311, 733, 295, 411, 257, 1446, 13465, 5380, 13, 51176], "temperature": 0.0, "avg_logprob": -0.26638908756589424, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.07956346869468689}, {"id": 11, "seek": 2804, "start": 44.28, "end": 49.879999999999995, "text": " But hopefully we can discuss today about this and see how you can do this in your own organization", "tokens": [51176, 583, 4696, 321, 393, 2248, 965, 466, 341, 293, 536, 577, 291, 393, 360, 341, 294, 428, 1065, 4475, 51456], "temperature": 0.0, "avg_logprob": -0.26638908756589424, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.07956346869468689}, {"id": 12, "seek": 2804, "start": 49.879999999999995, "end": 53.879999999999995, "text": " easily while also get value for your org.", "tokens": [51456, 3612, 1339, 611, 483, 2158, 337, 428, 14045, 13, 51656], "temperature": 0.0, "avg_logprob": -0.26638908756589424, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.07956346869468689}, {"id": 13, "seek": 5388, "start": 53.88, "end": 57.56, "text": " Before jumping into the topic, let me kind of quickly set the scene and talk a little", "tokens": [50364, 4546, 11233, 666, 264, 4829, 11, 718, 385, 733, 295, 2661, 992, 264, 4145, 293, 751, 257, 707, 50548], "temperature": 0.0, "avg_logprob": -0.2248190203283587, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.15157465636730194}, {"id": 14, "seek": 5388, "start": 57.56, "end": 60.040000000000006, "text": " bit about software supply chain attacks.", "tokens": [50548, 857, 466, 4722, 5847, 5021, 8122, 13, 50672], "temperature": 0.0, "avg_logprob": -0.2248190203283587, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.15157465636730194}, {"id": 15, "seek": 5388, "start": 60.040000000000006, "end": 64.44, "text": " So in a software supply chain attack, the attackers compromise the build system or the", "tokens": [50672, 407, 294, 257, 4722, 5847, 5021, 2690, 11, 264, 45129, 18577, 264, 1322, 1185, 420, 264, 50892], "temperature": 0.0, "avg_logprob": -0.2248190203283587, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.15157465636730194}, {"id": 16, "seek": 5388, "start": 64.44, "end": 68.64, "text": " package registry and get a foothold there.", "tokens": [50892, 7372, 36468, 293, 483, 257, 726, 900, 2641, 456, 13, 51102], "temperature": 0.0, "avg_logprob": -0.2248190203283587, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.15157465636730194}, {"id": 17, "seek": 5388, "start": 68.64, "end": 72.24000000000001, "text": " And over the past years we've been seeing an increase in these types of attacks.", "tokens": [51102, 400, 670, 264, 1791, 924, 321, 600, 668, 2577, 364, 3488, 294, 613, 3467, 295, 8122, 13, 51282], "temperature": 0.0, "avg_logprob": -0.2248190203283587, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.15157465636730194}, {"id": 18, "seek": 5388, "start": 72.24000000000001, "end": 77.28, "text": " So there was a report say from Sonatype that said since 2019, year after year, we've been", "tokens": [51282, 407, 456, 390, 257, 2275, 584, 490, 5185, 21398, 494, 300, 848, 1670, 6071, 11, 1064, 934, 1064, 11, 321, 600, 668, 51534], "temperature": 0.0, "avg_logprob": -0.2248190203283587, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.15157465636730194}, {"id": 19, "seek": 5388, "start": 77.28, "end": 81.32000000000001, "text": " seeing a sevenfold increase in this type of attacks.", "tokens": [51534, 2577, 257, 3407, 18353, 3488, 294, 341, 2010, 295, 8122, 13, 51736], "temperature": 0.0, "avg_logprob": -0.2248190203283587, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.15157465636730194}, {"id": 20, "seek": 8132, "start": 81.32, "end": 86.63999999999999, "text": " The report came out in 2022 that said supply chain attacks basically surpassed malware", "tokens": [50364, 440, 2275, 1361, 484, 294, 20229, 300, 848, 5847, 5021, 8122, 1936, 27650, 292, 40747, 50630], "temperature": 0.0, "avg_logprob": -0.14843456870631166, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.06094794347882271}, {"id": 21, "seek": 8132, "start": 86.63999999999999, "end": 89.36, "text": " based attacks by 40%.", "tokens": [50630, 2361, 8122, 538, 3356, 6856, 50766], "temperature": 0.0, "avg_logprob": -0.14843456870631166, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.06094794347882271}, {"id": 22, "seek": 8132, "start": 89.36, "end": 94.63999999999999, "text": " And last year around two out of three of US businesses were impacted by a supply chain", "tokens": [50766, 400, 1036, 1064, 926, 732, 484, 295, 1045, 295, 2546, 6011, 645, 15653, 538, 257, 5847, 5021, 51030], "temperature": 0.0, "avg_logprob": -0.14843456870631166, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.06094794347882271}, {"id": 23, "seek": 8132, "start": 94.63999999999999, "end": 95.96, "text": " attack.", "tokens": [51030, 2690, 13, 51096], "temperature": 0.0, "avg_logprob": -0.14843456870631166, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.06094794347882271}, {"id": 24, "seek": 8132, "start": 95.96, "end": 101.0, "text": " So you can take these numbers with a grain of salt, but the fact of the matter is there", "tokens": [51096, 407, 291, 393, 747, 613, 3547, 365, 257, 12837, 295, 5139, 11, 457, 264, 1186, 295, 264, 1871, 307, 456, 51348], "temperature": 0.0, "avg_logprob": -0.14843456870631166, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.06094794347882271}, {"id": 25, "seek": 8132, "start": 101.0, "end": 104.96, "text": " is a surge in these types of attacks.", "tokens": [51348, 307, 257, 18989, 294, 613, 3467, 295, 8122, 13, 51546], "temperature": 0.0, "avg_logprob": -0.14843456870631166, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.06094794347882271}, {"id": 26, "seek": 8132, "start": 104.96, "end": 110.19999999999999, "text": " And this popularity on the attack realm drives policy changes.", "tokens": [51546, 400, 341, 19301, 322, 264, 2690, 15355, 11754, 3897, 2962, 13, 51808], "temperature": 0.0, "avg_logprob": -0.14843456870631166, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.06094794347882271}, {"id": 27, "seek": 11020, "start": 110.2, "end": 117.84, "text": " So in May 2021, there was an executive order by the White House that said software vendors", "tokens": [50364, 407, 294, 1891, 7201, 11, 456, 390, 364, 10140, 1668, 538, 264, 5552, 4928, 300, 848, 4722, 22056, 50746], "temperature": 0.0, "avg_logprob": -0.23954330370264146, "compression_ratio": 1.5393700787401574, "no_speech_prob": 0.013897879049181938}, {"id": 28, "seek": 11020, "start": 117.84, "end": 124.2, "text": " must be provided and purchased with software of materials and provenance information.", "tokens": [50746, 1633, 312, 5649, 293, 14734, 365, 4722, 295, 5319, 293, 12785, 719, 1589, 13, 51064], "temperature": 0.0, "avg_logprob": -0.23954330370264146, "compression_ratio": 1.5393700787401574, "no_speech_prob": 0.013897879049181938}, {"id": 29, "seek": 11020, "start": 124.2, "end": 126.44, "text": " And quick show of hands.", "tokens": [51064, 400, 1702, 855, 295, 2377, 13, 51176], "temperature": 0.0, "avg_logprob": -0.23954330370264146, "compression_ratio": 1.5393700787401574, "no_speech_prob": 0.013897879049181938}, {"id": 30, "seek": 11020, "start": 126.44, "end": 129.56, "text": " How many are familiar with the term S-bombs or provenance?", "tokens": [51176, 1012, 867, 366, 4963, 365, 264, 1433, 318, 12, 65, 298, 929, 420, 12785, 719, 30, 51332], "temperature": 0.0, "avg_logprob": -0.23954330370264146, "compression_ratio": 1.5393700787401574, "no_speech_prob": 0.013897879049181938}, {"id": 31, "seek": 11020, "start": 129.56, "end": 130.72, "text": " Cool.", "tokens": [51332, 8561, 13, 51390], "temperature": 0.0, "avg_logprob": -0.23954330370264146, "compression_ratio": 1.5393700787401574, "no_speech_prob": 0.013897879049181938}, {"id": 32, "seek": 11020, "start": 130.72, "end": 135.12, "text": " How many of you have been deploying these in your pipelines or your organizations?", "tokens": [51390, 1012, 867, 295, 291, 362, 668, 34198, 613, 294, 428, 40168, 420, 428, 6150, 30, 51610], "temperature": 0.0, "avg_logprob": -0.23954330370264146, "compression_ratio": 1.5393700787401574, "no_speech_prob": 0.013897879049181938}, {"id": 33, "seek": 11020, "start": 135.12, "end": 136.88, "text": " Okay, great.", "tokens": [51610, 1033, 11, 869, 13, 51698], "temperature": 0.0, "avg_logprob": -0.23954330370264146, "compression_ratio": 1.5393700787401574, "no_speech_prob": 0.013897879049181938}, {"id": 34, "seek": 11020, "start": 136.88, "end": 137.88, "text": " There's also an S-bomb room.", "tokens": [51698, 821, 311, 611, 364, 318, 12, 65, 3548, 1808, 13, 51748], "temperature": 0.0, "avg_logprob": -0.23954330370264146, "compression_ratio": 1.5393700787401574, "no_speech_prob": 0.013897879049181938}, {"id": 35, "seek": 13788, "start": 137.88, "end": 140.68, "text": " So today we're going to jump into these topics real quick.", "tokens": [50364, 407, 965, 321, 434, 516, 281, 3012, 666, 613, 8378, 957, 1702, 13, 50504], "temperature": 0.0, "avg_logprob": -0.13880480014211763, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0037032112013548613}, {"id": 36, "seek": 13788, "start": 140.68, "end": 146.04, "text": " So we're going to discuss some concepts and then talk about the challenges people face", "tokens": [50504, 407, 321, 434, 516, 281, 2248, 512, 10392, 293, 550, 751, 466, 264, 4759, 561, 1851, 50772], "temperature": 0.0, "avg_logprob": -0.13880480014211763, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0037032112013548613}, {"id": 37, "seek": 13788, "start": 146.04, "end": 149.72, "text": " when trying to deploy these things to production.", "tokens": [50772, 562, 1382, 281, 7274, 613, 721, 281, 4265, 13, 50956], "temperature": 0.0, "avg_logprob": -0.13880480014211763, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0037032112013548613}, {"id": 38, "seek": 13788, "start": 149.72, "end": 154.28, "text": " Then we're going to talk about CHOC and how CHOC can help you solve these problems but", "tokens": [50956, 1396, 321, 434, 516, 281, 751, 466, 5995, 30087, 293, 577, 5995, 30087, 393, 854, 291, 5039, 613, 2740, 457, 51184], "temperature": 0.0, "avg_logprob": -0.13880480014211763, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0037032112013548613}, {"id": 39, "seek": 13788, "start": 154.28, "end": 158.4, "text": " achieve many, many more things and hopefully have a discussion in the end.", "tokens": [51184, 4584, 867, 11, 867, 544, 721, 293, 4696, 362, 257, 5017, 294, 264, 917, 13, 51390], "temperature": 0.0, "avg_logprob": -0.13880480014211763, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0037032112013548613}, {"id": 40, "seek": 13788, "start": 158.4, "end": 163.32, "text": " So for those of you who are not familiar with software bill of materials or S-bombs, you", "tokens": [51390, 407, 337, 729, 295, 291, 567, 366, 406, 4963, 365, 4722, 2961, 295, 5319, 420, 318, 12, 65, 298, 929, 11, 291, 51636], "temperature": 0.0, "avg_logprob": -0.13880480014211763, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0037032112013548613}, {"id": 41, "seek": 13788, "start": 163.32, "end": 166.12, "text": " can think of it like a list of ingredients for software.", "tokens": [51636, 393, 519, 295, 309, 411, 257, 1329, 295, 6952, 337, 4722, 13, 51776], "temperature": 0.0, "avg_logprob": -0.13880480014211763, "compression_ratio": 1.7465277777777777, "no_speech_prob": 0.0037032112013548613}, {"id": 42, "seek": 16612, "start": 166.12, "end": 171.92000000000002, "text": " So you go to the supermarket, you see a package, then you read the labels and you get a list", "tokens": [50364, 407, 291, 352, 281, 264, 25180, 11, 291, 536, 257, 7372, 11, 550, 291, 1401, 264, 16949, 293, 291, 483, 257, 1329, 50654], "temperature": 0.0, "avg_logprob": -0.17245225345387177, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.026977160945534706}, {"id": 43, "seek": 16612, "start": 171.92000000000002, "end": 173.76, "text": " of all the ingredients that are in there.", "tokens": [50654, 295, 439, 264, 6952, 300, 366, 294, 456, 13, 50746], "temperature": 0.0, "avg_logprob": -0.17245225345387177, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.026977160945534706}, {"id": 44, "seek": 16612, "start": 173.76, "end": 178.0, "text": " So an S-bomb is pretty much the same thing but for your software applications.", "tokens": [50746, 407, 364, 318, 12, 65, 3548, 307, 1238, 709, 264, 912, 551, 457, 337, 428, 4722, 5821, 13, 50958], "temperature": 0.0, "avg_logprob": -0.17245225345387177, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.026977160945534706}, {"id": 45, "seek": 16612, "start": 178.0, "end": 182.72, "text": " So you get either an XML or a JSON and from that you can get a list of the packages, their", "tokens": [50958, 407, 291, 483, 2139, 364, 43484, 420, 257, 31828, 293, 490, 300, 291, 393, 483, 257, 1329, 295, 264, 17401, 11, 641, 51194], "temperature": 0.0, "avg_logprob": -0.17245225345387177, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.026977160945534706}, {"id": 46, "seek": 16612, "start": 182.72, "end": 185.52, "text": " versions, etc., etc.", "tokens": [51194, 9606, 11, 5183, 7933, 5183, 13, 51334], "temperature": 0.0, "avg_logprob": -0.17245225345387177, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.026977160945534706}, {"id": 47, "seek": 16612, "start": 185.52, "end": 189.8, "text": " When we're talking about provenance, what we're talking about really is how did the", "tokens": [51334, 1133, 321, 434, 1417, 466, 12785, 719, 11, 437, 321, 434, 1417, 466, 534, 307, 577, 630, 264, 51548], "temperature": 0.0, "avg_logprob": -0.17245225345387177, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.026977160945534706}, {"id": 48, "seek": 16612, "start": 189.8, "end": 190.8, "text": " artifact get here?", "tokens": [51548, 34806, 483, 510, 30, 51598], "temperature": 0.0, "avg_logprob": -0.17245225345387177, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.026977160945534706}, {"id": 49, "seek": 16612, "start": 190.8, "end": 194.92000000000002, "text": " Like who created it, who packaged it, how was it modified along the way until it actually", "tokens": [51598, 1743, 567, 2942, 309, 11, 567, 38162, 309, 11, 577, 390, 309, 15873, 2051, 264, 636, 1826, 309, 767, 51804], "temperature": 0.0, "avg_logprob": -0.17245225345387177, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.026977160945534706}, {"id": 50, "seek": 19492, "start": 194.92, "end": 200.11999999999998, "text": " reaches the user basically.", "tokens": [50364, 14235, 264, 4195, 1936, 13, 50624], "temperature": 0.0, "avg_logprob": -0.18420965021306818, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.02033342607319355}, {"id": 51, "seek": 19492, "start": 200.11999999999998, "end": 206.32, "text": " So that is all good but if we think about a list of ingredients, what are the guarantees", "tokens": [50624, 407, 300, 307, 439, 665, 457, 498, 321, 519, 466, 257, 1329, 295, 6952, 11, 437, 366, 264, 32567, 50934], "temperature": 0.0, "avg_logprob": -0.18420965021306818, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.02033342607319355}, {"id": 52, "seek": 19492, "start": 206.32, "end": 209.2, "text": " that what we get is actually what we're promised?", "tokens": [50934, 300, 437, 321, 483, 307, 767, 437, 321, 434, 10768, 30, 51078], "temperature": 0.0, "avg_logprob": -0.18420965021306818, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.02033342607319355}, {"id": 53, "seek": 19492, "start": 209.2, "end": 213.76, "text": " So for instance you could have an NPM package and you can generate an S-bomb for your NPM", "tokens": [51078, 407, 337, 5197, 291, 727, 362, 364, 426, 18819, 7372, 293, 291, 393, 8460, 364, 318, 12, 65, 3548, 337, 428, 426, 18819, 51306], "temperature": 0.0, "avg_logprob": -0.18420965021306818, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.02033342607319355}, {"id": 54, "seek": 19492, "start": 213.76, "end": 217.76, "text": " package saying that these are the ingredients that are there but then in a package you could", "tokens": [51306, 7372, 1566, 300, 613, 366, 264, 6952, 300, 366, 456, 457, 550, 294, 257, 7372, 291, 727, 51506], "temperature": 0.0, "avg_logprob": -0.18420965021306818, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.02033342607319355}, {"id": 55, "seek": 19492, "start": 217.76, "end": 222.32, "text": " get a foothold somewhere in your build pipeline and inject something that was not originally", "tokens": [51506, 483, 257, 726, 900, 2641, 4079, 294, 428, 1322, 15517, 293, 10711, 746, 300, 390, 406, 7993, 51734], "temperature": 0.0, "avg_logprob": -0.18420965021306818, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.02033342607319355}, {"id": 56, "seek": 19492, "start": 222.32, "end": 223.44, "text": " there.", "tokens": [51734, 456, 13, 51790], "temperature": 0.0, "avg_logprob": -0.18420965021306818, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.02033342607319355}, {"id": 57, "seek": 22344, "start": 223.44, "end": 228.28, "text": " So another key component here besides generating the S-bomb and the provenance formation is", "tokens": [50364, 407, 1071, 2141, 6542, 510, 11868, 17746, 264, 318, 12, 65, 3548, 293, 264, 12785, 719, 11723, 307, 50606], "temperature": 0.0, "avg_logprob": -0.11241696015843805, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.01517334021627903}, {"id": 58, "seek": 22344, "start": 228.28, "end": 233.84, "text": " really having some attestation around the integrity of the generated artifacts.", "tokens": [50606, 534, 1419, 512, 951, 377, 399, 926, 264, 16000, 295, 264, 10833, 24617, 13, 50884], "temperature": 0.0, "avg_logprob": -0.11241696015843805, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.01517334021627903}, {"id": 59, "seek": 22344, "start": 233.84, "end": 238.72, "text": " So anyone should be able to cryptographically verify that at least what we're promised has", "tokens": [50884, 407, 2878, 820, 312, 1075, 281, 9844, 3108, 984, 16888, 300, 412, 1935, 437, 321, 434, 10768, 575, 51128], "temperature": 0.0, "avg_logprob": -0.11241696015843805, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.01517334021627903}, {"id": 60, "seek": 22344, "start": 238.72, "end": 244.12, "text": " not been tampered with and that the contents of the S-bomb were coming from an original", "tokens": [51128, 406, 668, 7677, 40004, 365, 293, 300, 264, 15768, 295, 264, 318, 12, 65, 3548, 645, 1348, 490, 364, 3380, 51398], "temperature": 0.0, "avg_logprob": -0.11241696015843805, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.01517334021627903}, {"id": 61, "seek": 22344, "start": 244.12, "end": 246.48, "text": " author, etc., etc.", "tokens": [51398, 3793, 11, 5183, 7933, 5183, 13, 51516], "temperature": 0.0, "avg_logprob": -0.11241696015843805, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.01517334021627903}, {"id": 62, "seek": 22344, "start": 246.48, "end": 250.32, "text": " And what's really important here is we need to have some clear assumptions around the", "tokens": [51516, 400, 437, 311, 534, 1021, 510, 307, 321, 643, 281, 362, 512, 1850, 17695, 926, 264, 51708], "temperature": 0.0, "avg_logprob": -0.11241696015843805, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.01517334021627903}, {"id": 63, "seek": 25032, "start": 250.32, "end": 255.28, "text": " threat model aka what can an attacker compromise and what are the security guarantees we're", "tokens": [50364, 4734, 2316, 28042, 437, 393, 364, 35871, 18577, 293, 437, 366, 264, 3825, 32567, 321, 434, 50612], "temperature": 0.0, "avg_logprob": -0.19432404462028952, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.017737211659550667}, {"id": 64, "seek": 25032, "start": 255.28, "end": 258.24, "text": " getting depending on that.", "tokens": [50612, 1242, 5413, 322, 300, 13, 50760], "temperature": 0.0, "avg_logprob": -0.19432404462028952, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.017737211659550667}, {"id": 65, "seek": 25032, "start": 258.24, "end": 263.36, "text": " So do we require the attacker say to compromise our build pipeline or do we require the attacker", "tokens": [50760, 407, 360, 321, 3651, 264, 35871, 584, 281, 18577, 527, 1322, 15517, 420, 360, 321, 3651, 264, 35871, 51016], "temperature": 0.0, "avg_logprob": -0.19432404462028952, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.017737211659550667}, {"id": 66, "seek": 25032, "start": 263.36, "end": 267.32, "text": " to get a foothold on developers boxes?", "tokens": [51016, 281, 483, 257, 726, 900, 2641, 322, 8849, 9002, 30, 51214], "temperature": 0.0, "avg_logprob": -0.19432404462028952, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.017737211659550667}, {"id": 67, "seek": 25032, "start": 267.32, "end": 269.65999999999997, "text": " What's our threat model?", "tokens": [51214, 708, 311, 527, 4734, 2316, 30, 51331], "temperature": 0.0, "avg_logprob": -0.19432404462028952, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.017737211659550667}, {"id": 68, "seek": 25032, "start": 269.65999999999997, "end": 273.8, "text": " And that's really important because if we think about DevOps pipelines in practice you", "tokens": [51331, 400, 300, 311, 534, 1021, 570, 498, 321, 519, 466, 43051, 40168, 294, 3124, 291, 51538], "temperature": 0.0, "avg_logprob": -0.19432404462028952, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.017737211659550667}, {"id": 69, "seek": 25032, "start": 273.8, "end": 279.12, "text": " have many components like developers are pushing codes, that code ends up in some provider", "tokens": [51538, 362, 867, 6677, 411, 8849, 366, 7380, 14211, 11, 300, 3089, 5314, 493, 294, 512, 12398, 51804], "temperature": 0.0, "avg_logprob": -0.19432404462028952, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.017737211659550667}, {"id": 70, "seek": 27912, "start": 279.12, "end": 284.0, "text": " like GitHub or GitLab, you have open source packages, you have container images, you have", "tokens": [50364, 411, 23331, 420, 16939, 37880, 11, 291, 362, 1269, 4009, 17401, 11, 291, 362, 10129, 5267, 11, 291, 362, 50608], "temperature": 0.0, "avg_logprob": -0.1907456661092824, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.010735061019659042}, {"id": 71, "seek": 27912, "start": 284.0, "end": 288.16, "text": " infrastructure code that modifies this code and pushes it out and then somehow it ends", "tokens": [50608, 6896, 3089, 300, 1072, 11221, 341, 3089, 293, 21020, 309, 484, 293, 550, 6063, 309, 5314, 50816], "temperature": 0.0, "avg_logprob": -0.1907456661092824, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.010735061019659042}, {"id": 72, "seek": 27912, "start": 288.16, "end": 290.56, "text": " up in the service or the cloud.", "tokens": [50816, 493, 294, 264, 2643, 420, 264, 4588, 13, 50936], "temperature": 0.0, "avg_logprob": -0.1907456661092824, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.010735061019659042}, {"id": 73, "seek": 27912, "start": 290.56, "end": 294.56, "text": " And as we're building out all these graphs of components attackers could get a foothold", "tokens": [50936, 400, 382, 321, 434, 2390, 484, 439, 613, 24877, 295, 6677, 45129, 727, 483, 257, 726, 900, 2641, 51136], "temperature": 0.0, "avg_logprob": -0.1907456661092824, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.010735061019659042}, {"id": 74, "seek": 27912, "start": 294.56, "end": 296.2, "text": " at various places.", "tokens": [51136, 412, 3683, 3190, 13, 51218], "temperature": 0.0, "avg_logprob": -0.1907456661092824, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.010735061019659042}, {"id": 75, "seek": 27912, "start": 296.2, "end": 298.84000000000003, "text": " So this is where Salsa comes in.", "tokens": [51218, 407, 341, 307, 689, 318, 24816, 1487, 294, 13, 51350], "temperature": 0.0, "avg_logprob": -0.1907456661092824, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.010735061019659042}, {"id": 76, "seek": 27912, "start": 298.84000000000003, "end": 305.84000000000003, "text": " Salsa is an open SSF project and essentially gives us some framework to talk about the", "tokens": [51350, 318, 24816, 307, 364, 1269, 12238, 37, 1716, 293, 4476, 2709, 505, 512, 8388, 281, 751, 466, 264, 51700], "temperature": 0.0, "avg_logprob": -0.1907456661092824, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.010735061019659042}, {"id": 77, "seek": 27912, "start": 305.84000000000003, "end": 308.6, "text": " security posture of our applications.", "tokens": [51700, 3825, 18502, 295, 527, 5821, 13, 51838], "temperature": 0.0, "avg_logprob": -0.1907456661092824, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.010735061019659042}, {"id": 78, "seek": 30860, "start": 308.6, "end": 314.16, "text": " And we have different levels for the supply chain security of our artifacts.", "tokens": [50364, 400, 321, 362, 819, 4358, 337, 264, 5847, 5021, 3825, 295, 527, 24617, 13, 50642], "temperature": 0.0, "avg_logprob": -0.19728964772717705, "compression_ratio": 1.848375451263538, "no_speech_prob": 0.024459851905703545}, {"id": 79, "seek": 30860, "start": 314.16, "end": 319.04, "text": " At level one essentially all we're doing is we are providing information about how the", "tokens": [50642, 1711, 1496, 472, 4476, 439, 321, 434, 884, 307, 321, 366, 6530, 1589, 466, 577, 264, 50886], "temperature": 0.0, "avg_logprob": -0.19728964772717705, "compression_ratio": 1.848375451263538, "no_speech_prob": 0.024459851905703545}, {"id": 80, "seek": 30860, "start": 319.04, "end": 324.28000000000003, "text": " package was built and have a report but we don't really have guarantees around the report", "tokens": [50886, 7372, 390, 3094, 293, 362, 257, 2275, 457, 321, 500, 380, 534, 362, 32567, 926, 264, 2275, 51148], "temperature": 0.0, "avg_logprob": -0.19728964772717705, "compression_ratio": 1.848375451263538, "no_speech_prob": 0.024459851905703545}, {"id": 81, "seek": 30860, "start": 324.28000000000003, "end": 326.52000000000004, "text": " whether it has been tampered with or not.", "tokens": [51148, 1968, 309, 575, 668, 7677, 40004, 365, 420, 406, 13, 51260], "temperature": 0.0, "avg_logprob": -0.19728964772717705, "compression_ratio": 1.848375451263538, "no_speech_prob": 0.024459851905703545}, {"id": 82, "seek": 30860, "start": 326.52000000000004, "end": 329.44, "text": " At level two we get signed provenance.", "tokens": [51260, 1711, 1496, 732, 321, 483, 8175, 12785, 719, 13, 51406], "temperature": 0.0, "avg_logprob": -0.19728964772717705, "compression_ratio": 1.848375451263538, "no_speech_prob": 0.024459851905703545}, {"id": 83, "seek": 30860, "start": 329.44, "end": 333.84000000000003, "text": " Essentially at this point we say okay once the thing has been generated there has not", "tokens": [51406, 23596, 412, 341, 935, 321, 584, 1392, 1564, 264, 551, 575, 668, 10833, 456, 575, 406, 51626], "temperature": 0.0, "avg_logprob": -0.19728964772717705, "compression_ratio": 1.848375451263538, "no_speech_prob": 0.024459851905703545}, {"id": 84, "seek": 30860, "start": 333.84000000000003, "end": 338.56, "text": " been tampering on that artifact but you don't get guarantees around the build platform etc.", "tokens": [51626, 668, 7677, 20055, 322, 300, 34806, 457, 291, 500, 380, 483, 32567, 926, 264, 1322, 3663, 5183, 13, 51862], "temperature": 0.0, "avg_logprob": -0.19728964772717705, "compression_ratio": 1.848375451263538, "no_speech_prob": 0.024459851905703545}, {"id": 85, "seek": 33856, "start": 339.56, "end": 344.96, "text": " And as you move up the layers you get stronger and stronger security guarantees.", "tokens": [50414, 400, 382, 291, 1286, 493, 264, 7914, 291, 483, 7249, 293, 7249, 3825, 32567, 13, 50684], "temperature": 0.0, "avg_logprob": -0.20095038414001465, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.007205037400126457}, {"id": 86, "seek": 33856, "start": 344.96, "end": 348.8, "text": " So today we're going to be talking about chocking how easy it is to get to Salsa level", "tokens": [50684, 407, 965, 321, 434, 516, 281, 312, 1417, 466, 417, 31730, 577, 1858, 309, 307, 281, 483, 281, 318, 24816, 1496, 50876], "temperature": 0.0, "avg_logprob": -0.20095038414001465, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.007205037400126457}, {"id": 87, "seek": 33856, "start": 348.8, "end": 351.88, "text": " two if you deploy chocking to your built pipelines.", "tokens": [50876, 732, 498, 291, 7274, 417, 31730, 281, 428, 3094, 40168, 13, 51030], "temperature": 0.0, "avg_logprob": -0.20095038414001465, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.007205037400126457}, {"id": 88, "seek": 33856, "start": 351.88, "end": 356.92, "text": " So how does one start to do this?", "tokens": [51030, 407, 577, 775, 472, 722, 281, 360, 341, 30, 51282], "temperature": 0.0, "avg_logprob": -0.20095038414001465, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.007205037400126457}, {"id": 89, "seek": 33856, "start": 356.92, "end": 361.6, "text": " This is good, we all want to improve the security posture of our applications, we want to deploy", "tokens": [51282, 639, 307, 665, 11, 321, 439, 528, 281, 3470, 264, 3825, 18502, 295, 527, 5821, 11, 321, 528, 281, 7274, 51516], "temperature": 0.0, "avg_logprob": -0.20095038414001465, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.007205037400126457}, {"id": 90, "seek": 33856, "start": 361.6, "end": 365.4, "text": " these things in our organization, how does one start to do it?", "tokens": [51516, 613, 721, 294, 527, 4475, 11, 577, 775, 472, 722, 281, 360, 309, 30, 51706], "temperature": 0.0, "avg_logprob": -0.20095038414001465, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.007205037400126457}, {"id": 91, "seek": 36540, "start": 365.4, "end": 370.79999999999995, "text": " One could think that okay that surely a solved problem there must be tools for this already", "tokens": [50364, 1485, 727, 519, 300, 1392, 300, 11468, 257, 13041, 1154, 456, 1633, 312, 3873, 337, 341, 1217, 50634], "temperature": 0.0, "avg_logprob": -0.17005300521850586, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.017985187470912933}, {"id": 92, "seek": 36540, "start": 370.79999999999995, "end": 376.47999999999996, "text": " and you're right to some extent but the tooling ecosystem is really in its infancy and it's", "tokens": [50634, 293, 291, 434, 558, 281, 512, 8396, 457, 264, 46593, 11311, 307, 534, 294, 1080, 1536, 6717, 293, 309, 311, 50918], "temperature": 0.0, "avg_logprob": -0.17005300521850586, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.017985187470912933}, {"id": 93, "seek": 36540, "start": 376.47999999999996, "end": 378.91999999999996, "text": " largely fragmented at this point.", "tokens": [50918, 11611, 9241, 14684, 412, 341, 935, 13, 51040], "temperature": 0.0, "avg_logprob": -0.17005300521850586, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.017985187470912933}, {"id": 94, "seek": 36540, "start": 378.91999999999996, "end": 384.35999999999996, "text": " So it's not necessarily obvious to a newcomer which tool or framework they should pick and", "tokens": [51040, 407, 309, 311, 406, 4725, 6322, 281, 257, 40014, 260, 597, 2290, 420, 8388, 436, 820, 1888, 293, 51312], "temperature": 0.0, "avg_logprob": -0.17005300521850586, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.017985187470912933}, {"id": 95, "seek": 36540, "start": 384.35999999999996, "end": 389.67999999999995, "text": " even if you say select a space like S-Bombs the outputs of different tools are inconsistent", "tokens": [51312, 754, 498, 291, 584, 3048, 257, 1901, 411, 318, 12, 33, 298, 929, 264, 23930, 295, 819, 3873, 366, 36891, 51578], "temperature": 0.0, "avg_logprob": -0.17005300521850586, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.017985187470912933}, {"id": 96, "seek": 36540, "start": 389.67999999999995, "end": 390.67999999999995, "text": " with each other.", "tokens": [51578, 365, 1184, 661, 13, 51628], "temperature": 0.0, "avg_logprob": -0.17005300521850586, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.017985187470912933}, {"id": 97, "seek": 39068, "start": 390.68, "end": 396.44, "text": " One tool get a certain report, another tool get some different report and there might", "tokens": [50364, 1485, 2290, 483, 257, 1629, 2275, 11, 1071, 2290, 483, 512, 819, 2275, 293, 456, 1062, 50652], "temperature": 0.0, "avg_logprob": -0.15881361280168807, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.04039570689201355}, {"id": 98, "seek": 39068, "start": 396.44, "end": 400.12, "text": " be assumptions around how these things should be getting set up, how you should be deploying", "tokens": [50652, 312, 17695, 926, 577, 613, 721, 820, 312, 1242, 992, 493, 11, 577, 291, 820, 312, 34198, 50836], "temperature": 0.0, "avg_logprob": -0.15881361280168807, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.04039570689201355}, {"id": 99, "seek": 39068, "start": 400.12, "end": 405.0, "text": " all these things so it's not a straightforward way and what's really really hard is thinking", "tokens": [50836, 439, 613, 721, 370, 309, 311, 406, 257, 15325, 636, 293, 437, 311, 534, 534, 1152, 307, 1953, 51080], "temperature": 0.0, "avg_logprob": -0.15881361280168807, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.04039570689201355}, {"id": 100, "seek": 39068, "start": 405.0, "end": 407.0, "text": " about how can you do this at scale.", "tokens": [51080, 466, 577, 393, 291, 360, 341, 412, 4373, 13, 51180], "temperature": 0.0, "avg_logprob": -0.15881361280168807, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.04039570689201355}, {"id": 101, "seek": 39068, "start": 407.0, "end": 412.16, "text": " If you have a large organization with multiple repositories, different providers, how do you", "tokens": [51180, 759, 291, 362, 257, 2416, 4475, 365, 3866, 22283, 2083, 11, 819, 11330, 11, 577, 360, 291, 51438], "temperature": 0.0, "avg_logprob": -0.15881361280168807, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.04039570689201355}, {"id": 102, "seek": 39068, "start": 412.16, "end": 418.08, "text": " make it easy for your teams to just set this and let it run and have it be easy to consume", "tokens": [51438, 652, 309, 1858, 337, 428, 5491, 281, 445, 992, 341, 293, 718, 309, 1190, 293, 362, 309, 312, 1858, 281, 14732, 51734], "temperature": 0.0, "avg_logprob": -0.15881361280168807, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.04039570689201355}, {"id": 103, "seek": 41808, "start": 418.08, "end": 422.44, "text": " the data and also generate data that are of interest to you.", "tokens": [50364, 264, 1412, 293, 611, 8460, 1412, 300, 366, 295, 1179, 281, 291, 13, 50582], "temperature": 0.0, "avg_logprob": -0.15799032675253377, "compression_ratio": 1.875, "no_speech_prob": 0.03884226456284523}, {"id": 104, "seek": 41808, "start": 422.44, "end": 428.59999999999997, "text": " So yeah it's not an easy problem and hopefully Chalk will help here.", "tokens": [50582, 407, 1338, 309, 311, 406, 364, 1858, 1154, 293, 4696, 761, 667, 486, 854, 510, 13, 50890], "temperature": 0.0, "avg_logprob": -0.15799032675253377, "compression_ratio": 1.875, "no_speech_prob": 0.03884226456284523}, {"id": 105, "seek": 41808, "start": 428.59999999999997, "end": 435.71999999999997, "text": " The main idea behind Chalk is really we have some metadata that we care about and then", "tokens": [50890, 440, 2135, 1558, 2261, 761, 667, 307, 534, 321, 362, 512, 26603, 300, 321, 1127, 466, 293, 550, 51246], "temperature": 0.0, "avg_logprob": -0.15799032675253377, "compression_ratio": 1.875, "no_speech_prob": 0.03884226456284523}, {"id": 106, "seek": 41808, "start": 435.71999999999997, "end": 439.76, "text": " we want to embed that metadata that we call Chalkmarks into the artifact.", "tokens": [51246, 321, 528, 281, 12240, 300, 26603, 300, 321, 818, 761, 667, 37307, 666, 264, 34806, 13, 51448], "temperature": 0.0, "avg_logprob": -0.15799032675253377, "compression_ratio": 1.875, "no_speech_prob": 0.03884226456284523}, {"id": 107, "seek": 41808, "start": 439.76, "end": 443.91999999999996, "text": " So the artifact could be a binary or it could be a docker image and you want to embed this", "tokens": [51448, 407, 264, 34806, 727, 312, 257, 17434, 420, 309, 727, 312, 257, 360, 9178, 3256, 293, 291, 528, 281, 12240, 341, 51656], "temperature": 0.0, "avg_logprob": -0.15799032675253377, "compression_ratio": 1.875, "no_speech_prob": 0.03884226456284523}, {"id": 108, "seek": 41808, "start": 443.91999999999996, "end": 447.24, "text": " metadata into the artifact during the build time or post build time.", "tokens": [51656, 26603, 666, 264, 34806, 1830, 264, 1322, 565, 420, 2183, 1322, 565, 13, 51822], "temperature": 0.0, "avg_logprob": -0.15799032675253377, "compression_ratio": 1.875, "no_speech_prob": 0.03884226456284523}, {"id": 109, "seek": 44724, "start": 447.24, "end": 453.2, "text": " So you could have an L file in a box and then you can inject metadata into that L file", "tokens": [50364, 407, 291, 727, 362, 364, 441, 3991, 294, 257, 2424, 293, 550, 291, 393, 10711, 26603, 666, 300, 441, 3991, 50662], "temperature": 0.0, "avg_logprob": -0.15319442749023438, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.022894028574228287}, {"id": 110, "seek": 44724, "start": 453.2, "end": 457.24, "text": " and say okay that was indeed here, you can have information that you care about like", "tokens": [50662, 293, 584, 1392, 300, 390, 6451, 510, 11, 291, 393, 362, 1589, 300, 291, 1127, 466, 411, 50864], "temperature": 0.0, "avg_logprob": -0.15319442749023438, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.022894028574228287}, {"id": 111, "seek": 44724, "start": 457.24, "end": 461.52, "text": " the security settings on that box like if a partner is enabled or what are the users", "tokens": [50864, 264, 3825, 6257, 322, 300, 2424, 411, 498, 257, 4975, 307, 15172, 420, 437, 366, 264, 5022, 51078], "temperature": 0.0, "avg_logprob": -0.15319442749023438, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.022894028574228287}, {"id": 112, "seek": 44724, "start": 461.52, "end": 467.84000000000003, "text": " or the network connections, you embed that metadata on it and now that artifact is tagged", "tokens": [51078, 420, 264, 3209, 9271, 11, 291, 12240, 300, 26603, 322, 309, 293, 586, 300, 34806, 307, 40239, 51394], "temperature": 0.0, "avg_logprob": -0.15319442749023438, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.022894028574228287}, {"id": 113, "seek": 44724, "start": 467.84000000000003, "end": 473.2, "text": " and once you have that tagged artifact you basically let it go and it gets deployed somewhere", "tokens": [51394, 293, 1564, 291, 362, 300, 40239, 34806, 291, 1936, 718, 309, 352, 293, 309, 2170, 17826, 4079, 51662], "temperature": 0.0, "avg_logprob": -0.15319442749023438, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.022894028574228287}, {"id": 114, "seek": 44724, "start": 473.2, "end": 474.40000000000003, "text": " in production.", "tokens": [51662, 294, 4265, 13, 51722], "temperature": 0.0, "avg_logprob": -0.15319442749023438, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.022894028574228287}, {"id": 115, "seek": 47440, "start": 474.4, "end": 479.0, "text": " So think of Chalk pretty much like air tags for your code so you embed the air tags and", "tokens": [50364, 407, 519, 295, 761, 667, 1238, 709, 411, 1988, 18632, 337, 428, 3089, 370, 291, 12240, 264, 1988, 18632, 293, 50594], "temperature": 0.0, "avg_logprob": -0.16345717175172106, "compression_ratio": 1.8385826771653544, "no_speech_prob": 0.0017827384872362018}, {"id": 116, "seek": 47440, "start": 479.0, "end": 484.28, "text": " then you're tracking it across the ecosystem of your infrastructure and once the artifact", "tokens": [50594, 550, 291, 434, 11603, 309, 2108, 264, 11311, 295, 428, 6896, 293, 1564, 264, 34806, 50858], "temperature": 0.0, "avg_logprob": -0.16345717175172106, "compression_ratio": 1.8385826771653544, "no_speech_prob": 0.0017827384872362018}, {"id": 117, "seek": 47440, "start": 484.28, "end": 491.59999999999997, "text": " actually gets executed what's interesting is you can get back reports with metadata", "tokens": [50858, 767, 2170, 17577, 437, 311, 1880, 307, 291, 393, 483, 646, 7122, 365, 26603, 51224], "temperature": 0.0, "avg_logprob": -0.16345717175172106, "compression_ratio": 1.8385826771653544, "no_speech_prob": 0.0017827384872362018}, {"id": 118, "seek": 47440, "start": 491.59999999999997, "end": 493.35999999999996, "text": " that you configured there.", "tokens": [51224, 300, 291, 30538, 456, 13, 51312], "temperature": 0.0, "avg_logprob": -0.16345717175172106, "compression_ratio": 1.8385826771653544, "no_speech_prob": 0.0017827384872362018}, {"id": 119, "seek": 47440, "start": 493.35999999999996, "end": 500.0, "text": " So essentially you can scan what has been out there in production, you can grab for all", "tokens": [51312, 407, 4476, 291, 393, 11049, 437, 575, 668, 484, 456, 294, 4265, 11, 291, 393, 4444, 337, 439, 51644], "temperature": 0.0, "avg_logprob": -0.16345717175172106, "compression_ratio": 1.8385826771653544, "no_speech_prob": 0.0017827384872362018}, {"id": 120, "seek": 47440, "start": 500.0, "end": 503.76, "text": " this metadata that has been embedded in the artifacts or you could configure the artifacts", "tokens": [51644, 341, 26603, 300, 575, 668, 16741, 294, 264, 24617, 420, 291, 727, 22162, 264, 24617, 51832], "temperature": 0.0, "avg_logprob": -0.16345717175172106, "compression_ratio": 1.8385826771653544, "no_speech_prob": 0.0017827384872362018}, {"id": 121, "seek": 50376, "start": 504.12, "end": 509.48, "text": " some cases to phone home and give you the report themselves and you could do this once", "tokens": [50382, 512, 3331, 281, 2593, 1280, 293, 976, 291, 264, 2275, 2969, 293, 291, 727, 360, 341, 1564, 50650], "temperature": 0.0, "avg_logprob": -0.2004838466644287, "compression_ratio": 1.6464646464646464, "no_speech_prob": 0.022631259635090828}, {"id": 122, "seek": 50376, "start": 509.48, "end": 516.16, "text": " or you could do it periodically for instance configuring Chalk to send you heartbeat reports.", "tokens": [50650, 420, 291, 727, 360, 309, 38916, 337, 5197, 6662, 1345, 761, 667, 281, 2845, 291, 34851, 7122, 13, 50984], "temperature": 0.0, "avg_logprob": -0.2004838466644287, "compression_ratio": 1.6464646464646464, "no_speech_prob": 0.022631259635090828}, {"id": 123, "seek": 50376, "start": 516.16, "end": 518.8, "text": " So let's see this in action.", "tokens": [50984, 407, 718, 311, 536, 341, 294, 3069, 13, 51116], "temperature": 0.0, "avg_logprob": -0.2004838466644287, "compression_ratio": 1.6464646464646464, "no_speech_prob": 0.022631259635090828}, {"id": 124, "seek": 50376, "start": 518.8, "end": 527.68, "text": " I have set up here a very very basic git repository and this repository all it does is it deploys", "tokens": [51116, 286, 362, 992, 493, 510, 257, 588, 588, 3875, 18331, 25841, 293, 341, 25841, 439, 309, 775, 307, 309, 368, 49522, 51560], "temperature": 0.0, "avg_logprob": -0.2004838466644287, "compression_ratio": 1.6464646464646464, "no_speech_prob": 0.022631259635090828}, {"id": 125, "seek": 50376, "start": 527.68, "end": 529.36, "text": " a lambda function.", "tokens": [51560, 257, 13607, 2445, 13, 51644], "temperature": 0.0, "avg_logprob": -0.2004838466644287, "compression_ratio": 1.6464646464646464, "no_speech_prob": 0.022631259635090828}, {"id": 126, "seek": 52936, "start": 529.36, "end": 535.28, "text": " So we have the main code of the lambda function here and as you can see there's nothing really", "tokens": [50364, 407, 321, 362, 264, 2135, 3089, 295, 264, 13607, 2445, 510, 293, 382, 291, 393, 536, 456, 311, 1825, 534, 50660], "temperature": 0.0, "avg_logprob": -0.1448779279535467, "compression_ratio": 1.85, "no_speech_prob": 0.011606317013502121}, {"id": 127, "seek": 52936, "start": 535.28, "end": 540.96, "text": " special to it, we just sleep and return it to 100k and we're building this lambda function", "tokens": [50660, 2121, 281, 309, 11, 321, 445, 2817, 293, 2736, 309, 281, 2319, 74, 293, 321, 434, 2390, 341, 13607, 2445, 50944], "temperature": 0.0, "avg_logprob": -0.1448779279535467, "compression_ratio": 1.85, "no_speech_prob": 0.011606317013502121}, {"id": 128, "seek": 52936, "start": 540.96, "end": 545.96, "text": " using a docker file and there's nothing specific to Chalk in this docker file pulling from", "tokens": [50944, 1228, 257, 360, 9178, 3991, 293, 456, 311, 1825, 2685, 281, 761, 667, 294, 341, 360, 9178, 3991, 8407, 490, 51194], "temperature": 0.0, "avg_logprob": -0.1448779279535467, "compression_ratio": 1.85, "no_speech_prob": 0.011606317013502121}, {"id": 129, "seek": 52936, "start": 545.96, "end": 552.96, "text": " a well known image and we're actually building the lambda using a github action.", "tokens": [51194, 257, 731, 2570, 3256, 293, 321, 434, 767, 2390, 264, 13607, 1228, 257, 290, 355, 836, 3069, 13, 51544], "temperature": 0.0, "avg_logprob": -0.1448779279535467, "compression_ratio": 1.85, "no_speech_prob": 0.011606317013502121}, {"id": 130, "seek": 52936, "start": 552.96, "end": 559.2, "text": " So during the github action we check out the code, we set up the build environment and", "tokens": [51544, 407, 1830, 264, 290, 355, 836, 3069, 321, 1520, 484, 264, 3089, 11, 321, 992, 493, 264, 1322, 2823, 293, 51856], "temperature": 0.0, "avg_logprob": -0.1448779279535467, "compression_ratio": 1.85, "no_speech_prob": 0.011606317013502121}, {"id": 131, "seek": 55920, "start": 559.2800000000001, "end": 561.36, "text": " then here we're setting up Chalk.", "tokens": [50368, 550, 510, 321, 434, 3287, 493, 761, 667, 13, 50472], "temperature": 0.0, "avg_logprob": -0.18434536564457524, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.004693913273513317}, {"id": 132, "seek": 55920, "start": 561.36, "end": 568.08, "text": " So we're telling our build ecosystem that Chalk should drop this build of the image", "tokens": [50472, 407, 321, 434, 3585, 527, 1322, 11311, 300, 761, 667, 820, 3270, 341, 1322, 295, 264, 3256, 50808], "temperature": 0.0, "avg_logprob": -0.18434536564457524, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.004693913273513317}, {"id": 133, "seek": 55920, "start": 568.08, "end": 572.84, "text": " and embed metadata on it and what sort of metadata we choose to embed is completely", "tokens": [50808, 293, 12240, 26603, 322, 309, 293, 437, 1333, 295, 26603, 321, 2826, 281, 12240, 307, 2584, 51046], "temperature": 0.0, "avg_logprob": -0.18434536564457524, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.004693913273513317}, {"id": 134, "seek": 55920, "start": 572.84, "end": 573.84, "text": " up to us.", "tokens": [51046, 493, 281, 505, 13, 51096], "temperature": 0.0, "avg_logprob": -0.18434536564457524, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.004693913273513317}, {"id": 135, "seek": 55920, "start": 573.84, "end": 576.5600000000001, "text": " It comes like Chalk comes with defaults.", "tokens": [51096, 467, 1487, 411, 761, 667, 1487, 365, 7576, 82, 13, 51232], "temperature": 0.0, "avg_logprob": -0.18434536564457524, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.004693913273513317}, {"id": 136, "seek": 55920, "start": 576.5600000000001, "end": 580.6400000000001, "text": " So these are the only lines of code we ever need to do for our build pipeline so that", "tokens": [51232, 407, 613, 366, 264, 787, 3876, 295, 3089, 321, 1562, 643, 281, 360, 337, 527, 1322, 15517, 370, 300, 51436], "temperature": 0.0, "avg_logprob": -0.18434536564457524, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.004693913273513317}, {"id": 137, "seek": 55920, "start": 580.6400000000001, "end": 587.72, "text": " Chalk can embed sbombs and actually use, you know, provide cryptographic guarantees around", "tokens": [51436, 761, 667, 393, 12240, 262, 65, 298, 929, 293, 767, 764, 11, 291, 458, 11, 2893, 9844, 12295, 32567, 926, 51790], "temperature": 0.0, "avg_logprob": -0.18434536564457524, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.004693913273513317}, {"id": 138, "seek": 58772, "start": 587.76, "end": 593.72, "text": " the integrity of the generated reports and we're also creating attestation manifests using", "tokens": [50366, 264, 16000, 295, 264, 10833, 7122, 293, 321, 434, 611, 4084, 951, 377, 399, 50252, 1228, 50664], "temperature": 0.0, "avg_logprob": -0.2195172223177823, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.007026678416877985}, {"id": 139, "seek": 58772, "start": 593.72, "end": 598.6, "text": " SIGS-Tor for those of you who are aware of that framework.", "tokens": [50664, 318, 10489, 50, 12, 51, 284, 337, 729, 295, 291, 567, 366, 3650, 295, 300, 8388, 13, 50908], "temperature": 0.0, "avg_logprob": -0.2195172223177823, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.007026678416877985}, {"id": 140, "seek": 58772, "start": 598.6, "end": 602.5600000000001, "text": " So cool, let's go ahead and trigger this.", "tokens": [50908, 407, 1627, 11, 718, 311, 352, 2286, 293, 7875, 341, 13, 51106], "temperature": 0.0, "avg_logprob": -0.2195172223177823, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.007026678416877985}, {"id": 141, "seek": 58772, "start": 602.5600000000001, "end": 609.1600000000001, "text": " I'm going to go here in the action, kind of re-trigger the action once more and what", "tokens": [51106, 286, 478, 516, 281, 352, 510, 294, 264, 3069, 11, 733, 295, 319, 12, 6903, 6812, 264, 3069, 1564, 544, 293, 437, 51436], "temperature": 0.0, "avg_logprob": -0.2195172223177823, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.007026678416877985}, {"id": 142, "seek": 58772, "start": 609.1600000000001, "end": 614.48, "text": " we're doing here is we're building a docker image and we're telling Chalk to encapsulate", "tokens": [51436, 321, 434, 884, 510, 307, 321, 434, 2390, 257, 360, 9178, 3256, 293, 321, 434, 3585, 761, 667, 281, 38745, 5256, 51702], "temperature": 0.0, "avg_logprob": -0.2195172223177823, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.007026678416877985}, {"id": 143, "seek": 58772, "start": 614.48, "end": 617.24, "text": " the whole build and inject metadata in here.", "tokens": [51702, 264, 1379, 1322, 293, 10711, 26603, 294, 510, 13, 51840], "temperature": 0.0, "avg_logprob": -0.2195172223177823, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.007026678416877985}, {"id": 144, "seek": 61724, "start": 617.24, "end": 620.84, "text": " And that metadata, we can choose how we want to emit it.", "tokens": [50364, 400, 300, 26603, 11, 321, 393, 2826, 577, 321, 528, 281, 32084, 309, 13, 50544], "temperature": 0.0, "avg_logprob": -0.14741795279762962, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0016445108922198415}, {"id": 145, "seek": 61724, "start": 620.84, "end": 625.84, "text": " So we can choose to emit a report in there or the CLI or in some destination that we", "tokens": [50544, 407, 321, 393, 2826, 281, 32084, 257, 2275, 294, 456, 420, 264, 12855, 40, 420, 294, 512, 12236, 300, 321, 50794], "temperature": 0.0, "avg_logprob": -0.14741795279762962, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0016445108922198415}, {"id": 146, "seek": 61724, "start": 625.84, "end": 628.64, "text": " care like S3 or some server.", "tokens": [50794, 1127, 411, 318, 18, 420, 512, 7154, 13, 50934], "temperature": 0.0, "avg_logprob": -0.14741795279762962, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0016445108922198415}, {"id": 147, "seek": 61724, "start": 628.64, "end": 632.44, "text": " So I have here a dummy server that's running and it's waiting for reports.", "tokens": [50934, 407, 286, 362, 510, 257, 35064, 7154, 300, 311, 2614, 293, 309, 311, 3806, 337, 7122, 13, 51124], "temperature": 0.0, "avg_logprob": -0.14741795279762962, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0016445108922198415}, {"id": 148, "seek": 61724, "start": 632.44, "end": 635.96, "text": " There's nothing here currently.", "tokens": [51124, 821, 311, 1825, 510, 4362, 13, 51300], "temperature": 0.0, "avg_logprob": -0.14741795279762962, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0016445108922198415}, {"id": 149, "seek": 61724, "start": 635.96, "end": 641.24, "text": " And I'm going to go back into one of the previous actions and show you a report that was emitted", "tokens": [51300, 400, 286, 478, 516, 281, 352, 646, 666, 472, 295, 264, 3894, 5909, 293, 855, 291, 257, 2275, 300, 390, 44897, 51564], "temperature": 0.0, "avg_logprob": -0.14741795279762962, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0016445108922198415}, {"id": 150, "seek": 61724, "start": 641.24, "end": 643.16, "text": " by Chalk on the CLI.", "tokens": [51564, 538, 761, 667, 322, 264, 12855, 40, 13, 51660], "temperature": 0.0, "avg_logprob": -0.14741795279762962, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0016445108922198415}, {"id": 151, "seek": 64316, "start": 643.16, "end": 648.36, "text": " So during the build, after we've actually pushed the image, you can see down here we", "tokens": [50364, 407, 1830, 264, 1322, 11, 934, 321, 600, 767, 9152, 264, 3256, 11, 291, 393, 536, 760, 510, 321, 50624], "temperature": 0.0, "avg_logprob": -0.16332691056387766, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0058899084106087685}, {"id": 152, "seek": 64316, "start": 648.36, "end": 654.7199999999999, "text": " have a Chalk report and this is basically a JSON file that has metadata that we care", "tokens": [50624, 362, 257, 761, 667, 2275, 293, 341, 307, 1936, 257, 31828, 3991, 300, 575, 26603, 300, 321, 1127, 50942], "temperature": 0.0, "avg_logprob": -0.16332691056387766, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0058899084106087685}, {"id": 153, "seek": 64316, "start": 654.7199999999999, "end": 655.7199999999999, "text": " about.", "tokens": [50942, 466, 13, 50992], "temperature": 0.0, "avg_logprob": -0.16332691056387766, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0058899084106087685}, {"id": 154, "seek": 64316, "start": 655.7199999999999, "end": 660.28, "text": " So here we know that some image could build, that was a daytime, that was a docker file", "tokens": [50992, 407, 510, 321, 458, 300, 512, 3256, 727, 1322, 11, 300, 390, 257, 31908, 11, 300, 390, 257, 360, 9178, 3991, 51220], "temperature": 0.0, "avg_logprob": -0.16332691056387766, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0058899084106087685}, {"id": 155, "seek": 64316, "start": 660.28, "end": 665.56, "text": " path, the exact contents of the docker file, the commit ID, the author of the committer,", "tokens": [51220, 3100, 11, 264, 1900, 15768, 295, 264, 360, 9178, 3991, 11, 264, 5599, 7348, 11, 264, 3793, 295, 264, 800, 3904, 11, 51484], "temperature": 0.0, "avg_logprob": -0.16332691056387766, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0058899084106087685}, {"id": 156, "seek": 64316, "start": 665.56, "end": 672.1999999999999, "text": " but you also get a cryptographic signature about the integrity of this report essentially.", "tokens": [51484, 457, 291, 611, 483, 257, 9844, 12295, 13397, 466, 264, 16000, 295, 341, 2275, 4476, 13, 51816], "temperature": 0.0, "avg_logprob": -0.16332691056387766, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0058899084106087685}, {"id": 157, "seek": 67220, "start": 672.2, "end": 676.6, "text": " You get interesting things like the environment variables, arguments.", "tokens": [50364, 509, 483, 1880, 721, 411, 264, 2823, 9102, 11, 12869, 13, 50584], "temperature": 0.0, "avg_logprob": -0.16332573488534216, "compression_ratio": 1.5758928571428572, "no_speech_prob": 0.02782762609422207}, {"id": 158, "seek": 67220, "start": 676.6, "end": 682.12, "text": " You can configure this to be however you like it.", "tokens": [50584, 509, 393, 22162, 341, 281, 312, 4461, 291, 411, 309, 13, 50860], "temperature": 0.0, "avg_logprob": -0.16332573488534216, "compression_ratio": 1.5758928571428572, "no_speech_prob": 0.02782762609422207}, {"id": 159, "seek": 67220, "start": 682.12, "end": 687.32, "text": " And this is generated on the CLI, but we can send the exact report, the exact same report", "tokens": [50860, 400, 341, 307, 10833, 322, 264, 12855, 40, 11, 457, 321, 393, 2845, 264, 1900, 2275, 11, 264, 1900, 912, 2275, 51120], "temperature": 0.0, "avg_logprob": -0.16332573488534216, "compression_ratio": 1.5758928571428572, "no_speech_prob": 0.02782762609422207}, {"id": 160, "seek": 67220, "start": 687.32, "end": 691.48, "text": " or variance of that report in other destinations.", "tokens": [51120, 420, 21977, 295, 300, 2275, 294, 661, 37787, 13, 51328], "temperature": 0.0, "avg_logprob": -0.16332573488534216, "compression_ratio": 1.5758928571428572, "no_speech_prob": 0.02782762609422207}, {"id": 161, "seek": 67220, "start": 691.48, "end": 699.88, "text": " So going back here to the action we just triggered, hopefully once this completes, we will be", "tokens": [51328, 407, 516, 646, 510, 281, 264, 3069, 321, 445, 21710, 11, 4696, 1564, 341, 36362, 11, 321, 486, 312, 51748], "temperature": 0.0, "avg_logprob": -0.16332573488534216, "compression_ratio": 1.5758928571428572, "no_speech_prob": 0.02782762609422207}, {"id": 162, "seek": 69988, "start": 699.88, "end": 702.92, "text": " seeing a report populated to our server.", "tokens": [50364, 2577, 257, 2275, 32998, 281, 527, 7154, 13, 50516], "temperature": 0.0, "avg_logprob": -0.21339477090274586, "compression_ratio": 1.4477611940298507, "no_speech_prob": 0.01270432211458683}, {"id": 163, "seek": 69988, "start": 702.92, "end": 711.72, "text": " So not only will we see a report here on the CLI, but we'll also get the metadata in the", "tokens": [50516, 407, 406, 787, 486, 321, 536, 257, 2275, 510, 322, 264, 12855, 40, 11, 457, 321, 603, 611, 483, 264, 26603, 294, 264, 50956], "temperature": 0.0, "avg_logprob": -0.21339477090274586, "compression_ratio": 1.4477611940298507, "no_speech_prob": 0.01270432211458683}, {"id": 164, "seek": 69988, "start": 711.72, "end": 713.56, "text": " endpoint we configured.", "tokens": [50956, 35795, 321, 30538, 13, 51048], "temperature": 0.0, "avg_logprob": -0.21339477090274586, "compression_ratio": 1.4477611940298507, "no_speech_prob": 0.01270432211458683}, {"id": 165, "seek": 69988, "start": 713.56, "end": 714.72, "text": " What could possibly go wrong?", "tokens": [51048, 708, 727, 6264, 352, 2085, 30, 51106], "temperature": 0.0, "avg_logprob": -0.21339477090274586, "compression_ratio": 1.4477611940298507, "no_speech_prob": 0.01270432211458683}, {"id": 166, "seek": 69988, "start": 714.72, "end": 718.0, "text": " This is just a live demo here.", "tokens": [51106, 639, 307, 445, 257, 1621, 10723, 510, 13, 51270], "temperature": 0.0, "avg_logprob": -0.21339477090274586, "compression_ratio": 1.4477611940298507, "no_speech_prob": 0.01270432211458683}, {"id": 167, "seek": 69988, "start": 718.0, "end": 722.72, "text": " So you can make this as fine-grained as you like.", "tokens": [51270, 407, 291, 393, 652, 341, 382, 2489, 12, 20735, 2001, 382, 291, 411, 13, 51506], "temperature": 0.0, "avg_logprob": -0.21339477090274586, "compression_ratio": 1.4477611940298507, "no_speech_prob": 0.01270432211458683}, {"id": 168, "seek": 69988, "start": 722.72, "end": 725.08, "text": " So Chalk supports plugins.", "tokens": [51506, 407, 761, 667, 9346, 33759, 13, 51624], "temperature": 0.0, "avg_logprob": -0.21339477090274586, "compression_ratio": 1.4477611940298507, "no_speech_prob": 0.01270432211458683}, {"id": 169, "seek": 72508, "start": 725.08, "end": 730.8000000000001, "text": " So if you want to run, say, your static analysis tools like SemGrid or CodeQL, you can embed", "tokens": [50364, 407, 498, 291, 528, 281, 1190, 11, 584, 11, 428, 13437, 5215, 3873, 411, 14421, 38, 8558, 420, 15549, 13695, 11, 291, 393, 12240, 50650], "temperature": 0.0, "avg_logprob": -0.2194917745757521, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.005864533130079508}, {"id": 170, "seek": 72508, "start": 730.8000000000001, "end": 737.0400000000001, "text": " this metadata into the report as part of your regular other metadata that you're tracking.", "tokens": [50650, 341, 26603, 666, 264, 2275, 382, 644, 295, 428, 3890, 661, 26603, 300, 291, 434, 11603, 13, 50962], "temperature": 0.0, "avg_logprob": -0.2194917745757521, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.005864533130079508}, {"id": 171, "seek": 72508, "start": 737.0400000000001, "end": 738.76, "text": " So it looks like this got finished.", "tokens": [50962, 407, 309, 1542, 411, 341, 658, 4335, 13, 51048], "temperature": 0.0, "avg_logprob": -0.2194917745757521, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.005864533130079508}, {"id": 172, "seek": 72508, "start": 738.76, "end": 740.2, "text": " So we did get a report here.", "tokens": [51048, 407, 321, 630, 483, 257, 2275, 510, 13, 51120], "temperature": 0.0, "avg_logprob": -0.2194917745757521, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.005864533130079508}, {"id": 173, "seek": 72508, "start": 740.2, "end": 745.08, "text": " And if I go here, essentially we see that we got a build operation, so that got sent", "tokens": [51120, 400, 498, 286, 352, 510, 11, 4476, 321, 536, 300, 321, 658, 257, 1322, 6916, 11, 370, 300, 658, 2279, 51364], "temperature": 0.0, "avg_logprob": -0.2194917745757521, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.005864533130079508}, {"id": 174, "seek": 72508, "start": 745.08, "end": 747.08, "text": " over to our server.", "tokens": [51364, 670, 281, 527, 7154, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2194917745757521, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.005864533130079508}, {"id": 175, "seek": 72508, "start": 747.08, "end": 751.0400000000001, "text": " And this is essentially just a pre-defined rendering of the JSON, right?", "tokens": [51464, 400, 341, 307, 4476, 445, 257, 659, 12, 37716, 22407, 295, 264, 31828, 11, 558, 30, 51662], "temperature": 0.0, "avg_logprob": -0.2194917745757521, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.005864533130079508}, {"id": 176, "seek": 75104, "start": 751.04, "end": 756.92, "text": " You can send it wherever you see fit and render it however you would like.", "tokens": [50364, 509, 393, 2845, 309, 8660, 291, 536, 3318, 293, 15529, 309, 4461, 291, 576, 411, 13, 50658], "temperature": 0.0, "avg_logprob": -0.1653503860746111, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.02849816530942917}, {"id": 177, "seek": 75104, "start": 756.92, "end": 758.48, "text": " But we get some interesting information.", "tokens": [50658, 583, 321, 483, 512, 1880, 1589, 13, 50736], "temperature": 0.0, "avg_logprob": -0.1653503860746111, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.02849816530942917}, {"id": 178, "seek": 75104, "start": 758.48, "end": 762.04, "text": " We get some signal that we collected S-Bomb and Signing data.", "tokens": [50736, 492, 483, 512, 6358, 300, 321, 11087, 318, 12, 33, 3548, 293, 318, 9676, 1412, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1653503860746111, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.02849816530942917}, {"id": 179, "seek": 75104, "start": 762.04, "end": 767.64, "text": " And indeed, if I scroll down here, I do see that I have the full S-Bomb.", "tokens": [50914, 400, 6451, 11, 498, 286, 11369, 760, 510, 11, 286, 360, 536, 300, 286, 362, 264, 1577, 318, 12, 33, 3548, 13, 51194], "temperature": 0.0, "avg_logprob": -0.1653503860746111, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.02849816530942917}, {"id": 180, "seek": 75104, "start": 767.64, "end": 771.68, "text": " And I can fetch information about the attestation of the artifact.", "tokens": [51194, 400, 286, 393, 23673, 1589, 466, 264, 951, 377, 399, 295, 264, 34806, 13, 51396], "temperature": 0.0, "avg_logprob": -0.1653503860746111, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.02849816530942917}, {"id": 181, "seek": 75104, "start": 771.68, "end": 778.48, "text": " But I also get a bunch of interesting metadata that might not have been obvious just by seeing", "tokens": [51396, 583, 286, 611, 483, 257, 3840, 295, 1880, 26603, 300, 1062, 406, 362, 668, 6322, 445, 538, 2577, 51736], "temperature": 0.0, "avg_logprob": -0.1653503860746111, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.02849816530942917}, {"id": 182, "seek": 75104, "start": 778.48, "end": 779.48, "text": " the build.", "tokens": [51736, 264, 1322, 13, 51786], "temperature": 0.0, "avg_logprob": -0.1653503860746111, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.02849816530942917}, {"id": 183, "seek": 77948, "start": 779.48, "end": 782.64, "text": " So I see here CloudProvider is Azure.", "tokens": [50364, 407, 286, 536, 510, 8061, 12681, 85, 1438, 307, 11969, 13, 50522], "temperature": 0.0, "avg_logprob": -0.19649046771931197, "compression_ratio": 1.7172131147540983, "no_speech_prob": 0.008462954312562943}, {"id": 184, "seek": 77948, "start": 782.64, "end": 786.24, "text": " And we have information about the actual Azure instance metadata in which the build", "tokens": [50522, 400, 321, 362, 1589, 466, 264, 3539, 11969, 5197, 26603, 294, 597, 264, 1322, 50702], "temperature": 0.0, "avg_logprob": -0.19649046771931197, "compression_ratio": 1.7172131147540983, "no_speech_prob": 0.008462954312562943}, {"id": 185, "seek": 77948, "start": 786.24, "end": 787.24, "text": " happened.", "tokens": [50702, 2011, 13, 50752], "temperature": 0.0, "avg_logprob": -0.19649046771931197, "compression_ratio": 1.7172131147540983, "no_speech_prob": 0.008462954312562943}, {"id": 186, "seek": 77948, "start": 787.24, "end": 794.44, "text": " So essentially what happens here is GitHub runs their machines on Azure in this particular", "tokens": [50752, 407, 4476, 437, 2314, 510, 307, 23331, 6676, 641, 8379, 322, 11969, 294, 341, 1729, 51112], "temperature": 0.0, "avg_logprob": -0.19649046771931197, "compression_ratio": 1.7172131147540983, "no_speech_prob": 0.008462954312562943}, {"id": 187, "seek": 77948, "start": 794.44, "end": 795.44, "text": " instance.", "tokens": [51112, 5197, 13, 51162], "temperature": 0.0, "avg_logprob": -0.19649046771931197, "compression_ratio": 1.7172131147540983, "no_speech_prob": 0.008462954312562943}, {"id": 188, "seek": 77948, "start": 795.44, "end": 799.44, "text": " And so that build triggered in one of the Azure instances.", "tokens": [51162, 400, 370, 300, 1322, 21710, 294, 472, 295, 264, 11969, 14519, 13, 51362], "temperature": 0.0, "avg_logprob": -0.19649046771931197, "compression_ratio": 1.7172131147540983, "no_speech_prob": 0.008462954312562943}, {"id": 189, "seek": 77948, "start": 799.44, "end": 800.44, "text": " So that's nice.", "tokens": [51362, 407, 300, 311, 1481, 13, 51412], "temperature": 0.0, "avg_logprob": -0.19649046771931197, "compression_ratio": 1.7172131147540983, "no_speech_prob": 0.008462954312562943}, {"id": 190, "seek": 77948, "start": 800.44, "end": 802.32, "text": " We can also see the build command.", "tokens": [51412, 492, 393, 611, 536, 264, 1322, 5622, 13, 51506], "temperature": 0.0, "avg_logprob": -0.19649046771931197, "compression_ratio": 1.7172131147540983, "no_speech_prob": 0.008462954312562943}, {"id": 191, "seek": 77948, "start": 802.32, "end": 806.0, "text": " And you can see here how the normal build command is now wrapped under Choc.", "tokens": [51506, 400, 291, 393, 536, 510, 577, 264, 2710, 1322, 5622, 307, 586, 14226, 833, 761, 905, 13, 51690], "temperature": 0.0, "avg_logprob": -0.19649046771931197, "compression_ratio": 1.7172131147540983, "no_speech_prob": 0.008462954312562943}, {"id": 192, "seek": 80600, "start": 806.0, "end": 812.76, "text": " So Choc is in charge of the build and embeds the metadata into your image.", "tokens": [50364, 407, 761, 905, 307, 294, 4602, 295, 264, 1322, 293, 12240, 82, 264, 26603, 666, 428, 3256, 13, 50702], "temperature": 0.0, "avg_logprob": -0.14520063205641143, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.005946253892034292}, {"id": 193, "seek": 80600, "start": 812.76, "end": 813.76, "text": " So that's nice.", "tokens": [50702, 407, 300, 311, 1481, 13, 50752], "temperature": 0.0, "avg_logprob": -0.14520063205641143, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.005946253892034292}, {"id": 194, "seek": 80600, "start": 813.76, "end": 819.32, "text": " What we did do here is we pushed this demo lambda essentially.", "tokens": [50752, 708, 321, 630, 360, 510, 307, 321, 9152, 341, 10723, 13607, 4476, 13, 51030], "temperature": 0.0, "avg_logprob": -0.14520063205641143, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.005946253892034292}, {"id": 195, "seek": 80600, "start": 819.32, "end": 822.4, "text": " You can see this was modified just now.", "tokens": [51030, 509, 393, 536, 341, 390, 15873, 445, 586, 13, 51184], "temperature": 0.0, "avg_logprob": -0.14520063205641143, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.005946253892034292}, {"id": 196, "seek": 80600, "start": 822.4, "end": 826.28, "text": " So I'm going to go ahead and execute the image.", "tokens": [51184, 407, 286, 478, 516, 281, 352, 2286, 293, 14483, 264, 3256, 13, 51378], "temperature": 0.0, "avg_logprob": -0.14520063205641143, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.005946253892034292}, {"id": 197, "seek": 80600, "start": 826.28, "end": 832.12, "text": " And hopefully, if things work as expected, the lambda will execute.", "tokens": [51378, 400, 4696, 11, 498, 721, 589, 382, 5176, 11, 264, 13607, 486, 14483, 13, 51670], "temperature": 0.0, "avg_logprob": -0.14520063205641143, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.005946253892034292}, {"id": 198, "seek": 80600, "start": 832.12, "end": 834.24, "text": " And I'm going to get a second report here.", "tokens": [51670, 400, 286, 478, 516, 281, 483, 257, 1150, 2275, 510, 13, 51776], "temperature": 0.0, "avg_logprob": -0.14520063205641143, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.005946253892034292}, {"id": 199, "seek": 83424, "start": 834.24, "end": 836.96, "text": " And that second report is an exec.", "tokens": [50364, 400, 300, 1150, 2275, 307, 364, 4454, 13, 50500], "temperature": 0.0, "avg_logprob": -0.15899124466070608, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.005610624328255653}, {"id": 200, "seek": 83424, "start": 836.96, "end": 842.4, "text": " And if I zoom into the exec, you now see that the command that got executed is actually", "tokens": [50500, 400, 498, 286, 8863, 666, 264, 4454, 11, 291, 586, 536, 300, 264, 5622, 300, 658, 17577, 307, 767, 50772], "temperature": 0.0, "avg_logprob": -0.15899124466070608, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.005610624328255653}, {"id": 201, "seek": 83424, "start": 842.4, "end": 844.76, "text": " running within the lambda environment.", "tokens": [50772, 2614, 1951, 264, 13607, 2823, 13, 50890], "temperature": 0.0, "avg_logprob": -0.15899124466070608, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.005610624328255653}, {"id": 202, "seek": 83424, "start": 844.76, "end": 849.2, "text": " So Choc is wrapping the entry point of the execution for that Docker image and tells", "tokens": [50890, 407, 761, 905, 307, 21993, 264, 8729, 935, 295, 264, 15058, 337, 300, 33772, 3256, 293, 5112, 51112], "temperature": 0.0, "avg_logprob": -0.15899124466070608, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.005610624328255653}, {"id": 203, "seek": 83424, "start": 849.2, "end": 853.76, "text": " you like, hey, this Choc mark that you inserted, the metadata that you've all captured is still", "tokens": [51112, 291, 411, 11, 4177, 11, 341, 761, 905, 1491, 300, 291, 27992, 11, 264, 26603, 300, 291, 600, 439, 11828, 307, 920, 51340], "temperature": 0.0, "avg_logprob": -0.15899124466070608, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.005610624328255653}, {"id": 204, "seek": 83424, "start": 853.76, "end": 857.12, "text": " here, but now I'm executing in lambda.", "tokens": [51340, 510, 11, 457, 586, 286, 478, 32368, 294, 13607, 13, 51508], "temperature": 0.0, "avg_logprob": -0.15899124466070608, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.005610624328255653}, {"id": 205, "seek": 83424, "start": 857.12, "end": 863.36, "text": " And indeed, if I go here and see the Cloud metadata, you can see the region, the role", "tokens": [51508, 400, 6451, 11, 498, 286, 352, 510, 293, 536, 264, 8061, 26603, 11, 291, 393, 536, 264, 4458, 11, 264, 3090, 51820], "temperature": 0.0, "avg_logprob": -0.15899124466070608, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.005610624328255653}, {"id": 206, "seek": 86336, "start": 863.44, "end": 867.44, "text": " they are in, the account ID, et cetera, et cetera.", "tokens": [50368, 436, 366, 294, 11, 264, 2696, 7348, 11, 1030, 11458, 11, 1030, 11458, 13, 50568], "temperature": 0.0, "avg_logprob": -0.14845501207837872, "compression_ratio": 1.6032388663967612, "no_speech_prob": 0.01585204340517521}, {"id": 207, "seek": 86336, "start": 867.44, "end": 876.6, "text": " So with this, we can basically say the metadata that we injected in our build pipeline here", "tokens": [50568, 407, 365, 341, 11, 321, 393, 1936, 584, 264, 26603, 300, 321, 36967, 294, 527, 1322, 15517, 510, 51026], "temperature": 0.0, "avg_logprob": -0.14845501207837872, "compression_ratio": 1.6032388663967612, "no_speech_prob": 0.01585204340517521}, {"id": 208, "seek": 86336, "start": 876.6, "end": 879.88, "text": " is still present throughout wherever we deploy the image.", "tokens": [51026, 307, 920, 1974, 3710, 8660, 321, 7274, 264, 3256, 13, 51190], "temperature": 0.0, "avg_logprob": -0.14845501207837872, "compression_ratio": 1.6032388663967612, "no_speech_prob": 0.01585204340517521}, {"id": 209, "seek": 86336, "start": 879.88, "end": 882.8000000000001, "text": " And we can keep track of where the thing actually executes.", "tokens": [51190, 400, 321, 393, 1066, 2837, 295, 689, 264, 551, 767, 4454, 1819, 13, 51336], "temperature": 0.0, "avg_logprob": -0.14845501207837872, "compression_ratio": 1.6032388663967612, "no_speech_prob": 0.01585204340517521}, {"id": 210, "seek": 86336, "start": 882.8000000000001, "end": 888.88, "text": " So if I take into this Choc mark, I'm sorry, let me zoom out here.", "tokens": [51336, 407, 498, 286, 747, 666, 341, 761, 905, 1491, 11, 286, 478, 2597, 11, 718, 385, 8863, 484, 510, 13, 51640], "temperature": 0.0, "avg_logprob": -0.14845501207837872, "compression_ratio": 1.6032388663967612, "no_speech_prob": 0.01585204340517521}, {"id": 211, "seek": 86336, "start": 888.88, "end": 893.12, "text": " I can see that there's two reports essentially associated with this.", "tokens": [51640, 286, 393, 536, 300, 456, 311, 732, 7122, 4476, 6615, 365, 341, 13, 51852], "temperature": 0.0, "avg_logprob": -0.14845501207837872, "compression_ratio": 1.6032388663967612, "no_speech_prob": 0.01585204340517521}, {"id": 212, "seek": 89312, "start": 893.12, "end": 896.92, "text": " One was a build and the other one was an exec for the exact same hash.", "tokens": [50364, 1485, 390, 257, 1322, 293, 264, 661, 472, 390, 364, 4454, 337, 264, 1900, 912, 22019, 13, 50554], "temperature": 0.0, "avg_logprob": -0.17828911041544976, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.005791580770164728}, {"id": 213, "seek": 89312, "start": 896.92, "end": 901.88, "text": " So the exact same hash that I build in that machine has been executed in the other machine.", "tokens": [50554, 407, 264, 1900, 912, 22019, 300, 286, 1322, 294, 300, 3479, 575, 668, 17577, 294, 264, 661, 3479, 13, 50802], "temperature": 0.0, "avg_logprob": -0.17828911041544976, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.005791580770164728}, {"id": 214, "seek": 89312, "start": 901.88, "end": 904.6, "text": " So what did we do here?", "tokens": [50802, 407, 437, 630, 321, 360, 510, 30, 50938], "temperature": 0.0, "avg_logprob": -0.17828911041544976, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.005791580770164728}, {"id": 215, "seek": 89312, "start": 904.6, "end": 911.48, "text": " First of all, with four lines of YAML in our GitHub action, we generate and distribute", "tokens": [50938, 2386, 295, 439, 11, 365, 1451, 3876, 295, 398, 2865, 43, 294, 527, 23331, 3069, 11, 321, 8460, 293, 20594, 51282], "temperature": 0.0, "avg_logprob": -0.17828911041544976, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.005791580770164728}, {"id": 216, "seek": 89312, "start": 911.48, "end": 913.68, "text": " the desktops.", "tokens": [51282, 264, 730, 2320, 3370, 13, 51392], "temperature": 0.0, "avg_logprob": -0.17828911041544976, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.005791580770164728}, {"id": 217, "seek": 89312, "start": 913.68, "end": 918.88, "text": " And we also have provenance information because we can track where the build happened and", "tokens": [51392, 400, 321, 611, 362, 12785, 719, 1589, 570, 321, 393, 2837, 689, 264, 1322, 2011, 293, 51652], "temperature": 0.0, "avg_logprob": -0.17828911041544976, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.005791580770164728}, {"id": 218, "seek": 89312, "start": 918.88, "end": 922.5600000000001, "text": " where the actual image got executed.", "tokens": [51652, 689, 264, 3539, 3256, 658, 17577, 13, 51836], "temperature": 0.0, "avg_logprob": -0.17828911041544976, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.005791580770164728}, {"id": 219, "seek": 92256, "start": 922.56, "end": 924.3599999999999, "text": " And we also get artifact integrity.", "tokens": [50364, 400, 321, 611, 483, 34806, 16000, 13, 50454], "temperature": 0.0, "avg_logprob": -0.18491273962933083, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.009161309339106083}, {"id": 220, "seek": 92256, "start": 924.3599999999999, "end": 926.1999999999999, "text": " So in our case, we're using cosine.", "tokens": [50454, 407, 294, 527, 1389, 11, 321, 434, 1228, 23565, 13, 50546], "temperature": 0.0, "avg_logprob": -0.18491273962933083, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.009161309339106083}, {"id": 221, "seek": 92256, "start": 926.1999999999999, "end": 930.04, "text": " You could use different frameworks to do this.", "tokens": [50546, 509, 727, 764, 819, 29834, 281, 360, 341, 13, 50738], "temperature": 0.0, "avg_logprob": -0.18491273962933083, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.009161309339106083}, {"id": 222, "seek": 92256, "start": 930.04, "end": 933.4799999999999, "text": " But essentially, we're meeting the basic requirements here.", "tokens": [50738, 583, 4476, 11, 321, 434, 3440, 264, 3875, 7728, 510, 13, 50910], "temperature": 0.0, "avg_logprob": -0.18491273962933083, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.009161309339106083}, {"id": 223, "seek": 92256, "start": 933.4799999999999, "end": 934.8, "text": " So we're checking those boxes.", "tokens": [50910, 407, 321, 434, 8568, 729, 9002, 13, 50976], "temperature": 0.0, "avg_logprob": -0.18491273962933083, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.009161309339106083}, {"id": 224, "seek": 92256, "start": 934.8, "end": 938.92, "text": " And that's with minimal effort, in my opinion.", "tokens": [50976, 400, 300, 311, 365, 13206, 4630, 11, 294, 452, 4800, 13, 51182], "temperature": 0.0, "avg_logprob": -0.18491273962933083, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.009161309339106083}, {"id": 225, "seek": 92256, "start": 938.92, "end": 942.56, "text": " Like all you need to do is you need to configure whatever destinations you want for these", "tokens": [51182, 1743, 439, 291, 643, 281, 360, 307, 291, 643, 281, 22162, 2035, 37787, 291, 528, 337, 613, 51364], "temperature": 0.0, "avg_logprob": -0.18491273962933083, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.009161309339106083}, {"id": 226, "seek": 92256, "start": 942.56, "end": 944.7199999999999, "text": " reports to be sent.", "tokens": [51364, 7122, 281, 312, 2279, 13, 51472], "temperature": 0.0, "avg_logprob": -0.18491273962933083, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.009161309339106083}, {"id": 227, "seek": 92256, "start": 944.7199999999999, "end": 947.1199999999999, "text": " So you say, OK, that's cool.", "tokens": [51472, 407, 291, 584, 11, 2264, 11, 300, 311, 1627, 13, 51592], "temperature": 0.0, "avg_logprob": -0.18491273962933083, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.009161309339106083}, {"id": 228, "seek": 92256, "start": 947.1199999999999, "end": 950.04, "text": " What more can you do?", "tokens": [51592, 708, 544, 393, 291, 360, 30, 51738], "temperature": 0.0, "avg_logprob": -0.18491273962933083, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.009161309339106083}, {"id": 229, "seek": 95004, "start": 950.04, "end": 958.9599999999999, "text": " So let's think about typical scenarios that happen during kind of live production environments.", "tokens": [50364, 407, 718, 311, 519, 466, 7476, 15077, 300, 1051, 1830, 733, 295, 1621, 4265, 12388, 13, 50810], "temperature": 0.0, "avg_logprob": -0.13102343230120903, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.007925187237560749}, {"id": 230, "seek": 95004, "start": 958.9599999999999, "end": 961.76, "text": " You might be on call for a given service.", "tokens": [50810, 509, 1062, 312, 322, 818, 337, 257, 2212, 2643, 13, 50950], "temperature": 0.0, "avg_logprob": -0.13102343230120903, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.007925187237560749}, {"id": 231, "seek": 95004, "start": 961.76, "end": 964.16, "text": " And you get a page in the middle of the night.", "tokens": [50950, 400, 291, 483, 257, 3028, 294, 264, 2808, 295, 264, 1818, 13, 51070], "temperature": 0.0, "avg_logprob": -0.13102343230120903, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.007925187237560749}, {"id": 232, "seek": 95004, "start": 964.16, "end": 965.5999999999999, "text": " And there is some issue.", "tokens": [51070, 400, 456, 307, 512, 2734, 13, 51142], "temperature": 0.0, "avg_logprob": -0.13102343230120903, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.007925187237560749}, {"id": 233, "seek": 95004, "start": 965.5999999999999, "end": 966.7199999999999, "text": " There's a bug.", "tokens": [51142, 821, 311, 257, 7426, 13, 51198], "temperature": 0.0, "avg_logprob": -0.13102343230120903, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.007925187237560749}, {"id": 234, "seek": 95004, "start": 966.7199999999999, "end": 968.28, "text": " There's a vulnerability.", "tokens": [51198, 821, 311, 257, 24210, 13, 51276], "temperature": 0.0, "avg_logprob": -0.13102343230120903, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.007925187237560749}, {"id": 235, "seek": 95004, "start": 968.28, "end": 969.0799999999999, "text": " Something is off.", "tokens": [51276, 6595, 307, 766, 13, 51316], "temperature": 0.0, "avg_logprob": -0.13102343230120903, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.007925187237560749}, {"id": 236, "seek": 95004, "start": 969.0799999999999, "end": 972.8399999999999, "text": " And you want to figure out, OK, what's the component that's responsible for this?", "tokens": [51316, 400, 291, 528, 281, 2573, 484, 11, 2264, 11, 437, 311, 264, 6542, 300, 311, 6250, 337, 341, 30, 51504], "temperature": 0.0, "avg_logprob": -0.13102343230120903, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.007925187237560749}, {"id": 237, "seek": 95004, "start": 972.8399999999999, "end": 978.64, "text": " You could have, say, a pretty complex application with multiple teams pushing code.", "tokens": [51504, 509, 727, 362, 11, 584, 11, 257, 1238, 3997, 3861, 365, 3866, 5491, 7380, 3089, 13, 51794], "temperature": 0.0, "avg_logprob": -0.13102343230120903, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.007925187237560749}, {"id": 238, "seek": 97864, "start": 978.64, "end": 983.0, "text": " And for large organizations, usually the pattern for resolving these issues is you cut", "tokens": [50364, 400, 337, 2416, 6150, 11, 2673, 264, 5102, 337, 49940, 613, 2663, 307, 291, 1723, 50582], "temperature": 0.0, "avg_logprob": -0.1690647509670997, "compression_ratio": 1.7003484320557491, "no_speech_prob": 0.013966369442641735}, {"id": 239, "seek": 97864, "start": 983.0, "end": 984.4399999999999, "text": " the tickets to the team.", "tokens": [50582, 264, 12628, 281, 264, 1469, 13, 50654], "temperature": 0.0, "avg_logprob": -0.1690647509670997, "compression_ratio": 1.7003484320557491, "no_speech_prob": 0.013966369442641735}, {"id": 240, "seek": 97864, "start": 984.4399999999999, "end": 987.6, "text": " You wait for somebody else to be seen and be like, hey, that's the responsibility of", "tokens": [50654, 509, 1699, 337, 2618, 1646, 281, 312, 1612, 293, 312, 411, 11, 4177, 11, 300, 311, 264, 6357, 295, 50812], "temperature": 0.0, "avg_logprob": -0.1690647509670997, "compression_ratio": 1.7003484320557491, "no_speech_prob": 0.013966369442641735}, {"id": 241, "seek": 97864, "start": 987.6, "end": 988.6, "text": " that person.", "tokens": [50812, 300, 954, 13, 50862], "temperature": 0.0, "avg_logprob": -0.1690647509670997, "compression_ratio": 1.7003484320557491, "no_speech_prob": 0.013966369442641735}, {"id": 242, "seek": 97864, "start": 988.6, "end": 989.96, "text": " Potentially, you grab into code.", "tokens": [50862, 9145, 3137, 11, 291, 4444, 666, 3089, 13, 50930], "temperature": 0.0, "avg_logprob": -0.1690647509670997, "compression_ratio": 1.7003484320557491, "no_speech_prob": 0.013966369442641735}, {"id": 243, "seek": 97864, "start": 989.96, "end": 991.4, "text": " You say, OK, what was the last commit?", "tokens": [50930, 509, 584, 11, 2264, 11, 437, 390, 264, 1036, 5599, 30, 51002], "temperature": 0.0, "avg_logprob": -0.1690647509670997, "compression_ratio": 1.7003484320557491, "no_speech_prob": 0.013966369442641735}, {"id": 244, "seek": 97864, "start": 991.4, "end": 992.72, "text": " Or you have metrics.", "tokens": [51002, 1610, 291, 362, 16367, 13, 51068], "temperature": 0.0, "avg_logprob": -0.1690647509670997, "compression_ratio": 1.7003484320557491, "no_speech_prob": 0.013966369442641735}, {"id": 245, "seek": 97864, "start": 992.72, "end": 994.92, "text": " And you track from your metrics what chains.", "tokens": [51068, 400, 291, 2837, 490, 428, 16367, 437, 12626, 13, 51178], "temperature": 0.0, "avg_logprob": -0.1690647509670997, "compression_ratio": 1.7003484320557491, "no_speech_prob": 0.013966369442641735}, {"id": 246, "seek": 97864, "start": 994.92, "end": 997.12, "text": " And you try to correlate it to somebody else.", "tokens": [51178, 400, 291, 853, 281, 48742, 309, 281, 2618, 1646, 13, 51288], "temperature": 0.0, "avg_logprob": -0.1690647509670997, "compression_ratio": 1.7003484320557491, "no_speech_prob": 0.013966369442641735}, {"id": 247, "seek": 97864, "start": 997.12, "end": 1003.96, "text": " If you're using Chalk for your build pipelines, it's much, much easier to correlate what exact", "tokens": [51288, 759, 291, 434, 1228, 761, 667, 337, 428, 1322, 40168, 11, 309, 311, 709, 11, 709, 3571, 281, 48742, 437, 1900, 51630], "temperature": 0.0, "avg_logprob": -0.1690647509670997, "compression_ratio": 1.7003484320557491, "no_speech_prob": 0.013966369442641735}, {"id": 248, "seek": 100396, "start": 1003.96, "end": 1008.88, "text": " version of an image is running where and what the components are.", "tokens": [50364, 3037, 295, 364, 3256, 307, 2614, 689, 293, 437, 264, 6677, 366, 13, 50610], "temperature": 0.0, "avg_logprob": -0.16442156772987515, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.03478480130434036}, {"id": 249, "seek": 100396, "start": 1008.88, "end": 1011.1600000000001, "text": " And potentially, like, who are the code owners, et cetera.", "tokens": [50610, 400, 7263, 11, 411, 11, 567, 366, 264, 3089, 7710, 11, 1030, 11458, 13, 50724], "temperature": 0.0, "avg_logprob": -0.16442156772987515, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.03478480130434036}, {"id": 250, "seek": 100396, "start": 1011.1600000000001, "end": 1016.12, "text": " Because if we go back here, you see that we have things like the cometer and the commit", "tokens": [50724, 1436, 498, 321, 352, 646, 510, 11, 291, 536, 300, 321, 362, 721, 411, 264, 395, 2398, 293, 264, 5599, 50972], "temperature": 0.0, "avg_logprob": -0.16442156772987515, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.03478480130434036}, {"id": 251, "seek": 100396, "start": 1016.12, "end": 1017.96, "text": " ID.", "tokens": [50972, 7348, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16442156772987515, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.03478480130434036}, {"id": 252, "seek": 100396, "start": 1017.96, "end": 1019.48, "text": " So we have the commit ID.", "tokens": [51064, 407, 321, 362, 264, 5599, 7348, 13, 51140], "temperature": 0.0, "avg_logprob": -0.16442156772987515, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.03478480130434036}, {"id": 253, "seek": 100396, "start": 1019.48, "end": 1025.96, "text": " You can start building these profiles about ownership incrementally as you go.", "tokens": [51140, 509, 393, 722, 2390, 613, 23693, 466, 15279, 26200, 379, 382, 291, 352, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16442156772987515, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.03478480130434036}, {"id": 254, "seek": 100396, "start": 1025.96, "end": 1031.2, "text": " So instead of having a process which could potentially take a couple of hours to determine", "tokens": [51464, 407, 2602, 295, 1419, 257, 1399, 597, 727, 7263, 747, 257, 1916, 295, 2496, 281, 6997, 51726], "temperature": 0.0, "avg_logprob": -0.16442156772987515, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.03478480130434036}, {"id": 255, "seek": 103120, "start": 1031.2, "end": 1039.68, "text": " the root cause of an outage or an issue, you now can have this in a few clicks, hopefully.", "tokens": [50364, 264, 5593, 3082, 295, 364, 484, 609, 420, 364, 2734, 11, 291, 586, 393, 362, 341, 294, 257, 1326, 18521, 11, 4696, 13, 50788], "temperature": 0.0, "avg_logprob": -0.15845579571194118, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.015267604030668736}, {"id": 256, "seek": 103120, "start": 1039.68, "end": 1044.4, "text": " Another common use case is application inventory and change management.", "tokens": [50788, 3996, 2689, 764, 1389, 307, 3861, 14228, 293, 1319, 4592, 13, 51024], "temperature": 0.0, "avg_logprob": -0.15845579571194118, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.015267604030668736}, {"id": 257, "seek": 103120, "start": 1044.4, "end": 1049.88, "text": " So say, for instance, you're part of a large organization.", "tokens": [51024, 407, 584, 11, 337, 5197, 11, 291, 434, 644, 295, 257, 2416, 4475, 13, 51298], "temperature": 0.0, "avg_logprob": -0.15845579571194118, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.015267604030668736}, {"id": 258, "seek": 103120, "start": 1049.88, "end": 1051.32, "text": " You want to deprecate the framework.", "tokens": [51298, 509, 528, 281, 1367, 13867, 473, 264, 8388, 13, 51370], "temperature": 0.0, "avg_logprob": -0.15845579571194118, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.015267604030668736}, {"id": 259, "seek": 103120, "start": 1051.32, "end": 1053.8, "text": " You want to deprecate, say, AngularJS.", "tokens": [51370, 509, 528, 281, 1367, 13867, 473, 11, 584, 11, 34107, 41, 50, 13, 51494], "temperature": 0.0, "avg_logprob": -0.15845579571194118, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.015267604030668736}, {"id": 260, "seek": 103120, "start": 1053.8, "end": 1056.3600000000001, "text": " So AngularJS is running production.", "tokens": [51494, 407, 34107, 41, 50, 307, 2614, 4265, 13, 51622], "temperature": 0.0, "avg_logprob": -0.15845579571194118, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.015267604030668736}, {"id": 261, "seek": 103120, "start": 1056.3600000000001, "end": 1058.44, "text": " And you want to figure out, OK, what is the impact?", "tokens": [51622, 400, 291, 528, 281, 2573, 484, 11, 2264, 11, 437, 307, 264, 2712, 30, 51726], "temperature": 0.0, "avg_logprob": -0.15845579571194118, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.015267604030668736}, {"id": 262, "seek": 105844, "start": 1058.44, "end": 1059.8400000000001, "text": " How many teams are using it?", "tokens": [50364, 1012, 867, 5491, 366, 1228, 309, 30, 50434], "temperature": 0.0, "avg_logprob": -0.13071676284547837, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.038517486304044724}, {"id": 263, "seek": 105844, "start": 1059.8400000000001, "end": 1062.3600000000001, "text": " Is the code even live?", "tokens": [50434, 1119, 264, 3089, 754, 1621, 30, 50560], "temperature": 0.0, "avg_logprob": -0.13071676284547837, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.038517486304044724}, {"id": 264, "seek": 105844, "start": 1062.3600000000001, "end": 1064.72, "text": " And what was the last time things got executed?", "tokens": [50560, 400, 437, 390, 264, 1036, 565, 721, 658, 17577, 30, 50678], "temperature": 0.0, "avg_logprob": -0.13071676284547837, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.038517486304044724}, {"id": 265, "seek": 105844, "start": 1064.72, "end": 1068.92, "text": " You can figure out, you can get reports around these things.", "tokens": [50678, 509, 393, 2573, 484, 11, 291, 393, 483, 7122, 926, 613, 721, 13, 50888], "temperature": 0.0, "avg_logprob": -0.13071676284547837, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.038517486304044724}, {"id": 266, "seek": 105844, "start": 1068.92, "end": 1073.48, "text": " More importantly, you can see how applications change over time.", "tokens": [50888, 5048, 8906, 11, 291, 393, 536, 577, 5821, 1319, 670, 565, 13, 51116], "temperature": 0.0, "avg_logprob": -0.13071676284547837, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.038517486304044724}, {"id": 267, "seek": 105844, "start": 1073.48, "end": 1078.8400000000001, "text": " So many of the people we've been talking to have processes where, for instance, they do", "tokens": [51116, 407, 867, 295, 264, 561, 321, 600, 668, 1417, 281, 362, 7555, 689, 11, 337, 5197, 11, 436, 360, 51384], "temperature": 0.0, "avg_logprob": -0.13071676284547837, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.038517486304044724}, {"id": 268, "seek": 105844, "start": 1078.8400000000001, "end": 1080.72, "text": " a sort of change management meeting.", "tokens": [51384, 257, 1333, 295, 1319, 4592, 3440, 13, 51478], "temperature": 0.0, "avg_logprob": -0.13071676284547837, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.038517486304044724}, {"id": 269, "seek": 105844, "start": 1080.72, "end": 1083.8400000000001, "text": " Like, once a week, they say, OK, what has changed?", "tokens": [51478, 1743, 11, 1564, 257, 1243, 11, 436, 584, 11, 2264, 11, 437, 575, 3105, 30, 51634], "temperature": 0.0, "avg_logprob": -0.13071676284547837, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.038517486304044724}, {"id": 270, "seek": 105844, "start": 1083.8400000000001, "end": 1085.96, "text": " What has been deployed?", "tokens": [51634, 708, 575, 668, 17826, 30, 51740], "temperature": 0.0, "avg_logprob": -0.13071676284547837, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.038517486304044724}, {"id": 271, "seek": 105844, "start": 1085.96, "end": 1087.72, "text": " Do we need to go through a security review?", "tokens": [51740, 1144, 321, 643, 281, 352, 807, 257, 3825, 3131, 30, 51828], "temperature": 0.0, "avg_logprob": -0.13071676284547837, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.038517486304044724}, {"id": 272, "seek": 108772, "start": 1087.72, "end": 1089.8, "text": " What's the exact list of changes?", "tokens": [50364, 708, 311, 264, 1900, 1329, 295, 2962, 30, 50468], "temperature": 0.0, "avg_logprob": -0.18738243796608664, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.04983054846525192}, {"id": 273, "seek": 108772, "start": 1089.8, "end": 1093.84, "text": " And that process is manual to a large extent.", "tokens": [50468, 400, 300, 1399, 307, 9688, 281, 257, 2416, 8396, 13, 50670], "temperature": 0.0, "avg_logprob": -0.18738243796608664, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.04983054846525192}, {"id": 274, "seek": 108772, "start": 1093.84, "end": 1097.68, "text": " Using Choc, you can automate this, because you can generate an exact report of the diff", "tokens": [50670, 11142, 761, 905, 11, 291, 393, 31605, 341, 11, 570, 291, 393, 8460, 364, 1900, 2275, 295, 264, 7593, 50862], "temperature": 0.0, "avg_logprob": -0.18738243796608664, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.04983054846525192}, {"id": 275, "seek": 108772, "start": 1097.68, "end": 1103.44, "text": " and you can get some integrity guarantees around that report.", "tokens": [50862, 293, 291, 393, 483, 512, 16000, 32567, 926, 300, 2275, 13, 51150], "temperature": 0.0, "avg_logprob": -0.18738243796608664, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.04983054846525192}, {"id": 276, "seek": 108772, "start": 1103.44, "end": 1107.68, "text": " But more importantly, besides these things, you can do much, much more rightly.", "tokens": [51150, 583, 544, 8906, 11, 11868, 613, 721, 11, 291, 393, 360, 709, 11, 709, 544, 32879, 13, 51362], "temperature": 0.0, "avg_logprob": -0.18738243796608664, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.04983054846525192}, {"id": 277, "seek": 108772, "start": 1107.68, "end": 1111.04, "text": " It's not necessary that you can only Choc containers.", "tokens": [51362, 467, 311, 406, 4818, 300, 291, 393, 787, 761, 905, 17089, 13, 51530], "temperature": 0.0, "avg_logprob": -0.18738243796608664, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.04983054846525192}, {"id": 278, "seek": 108772, "start": 1111.04, "end": 1117.1200000000001, "text": " You can run essentially tools of your choosing, or you can submit custom plugins for metadata", "tokens": [51530, 509, 393, 1190, 4476, 3873, 295, 428, 10875, 11, 420, 291, 393, 10315, 2375, 33759, 337, 26603, 51834], "temperature": 0.0, "avg_logprob": -0.18738243796608664, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.04983054846525192}, {"id": 279, "seek": 111712, "start": 1117.12, "end": 1119.04, "text": " surfacing.", "tokens": [50364, 9684, 5615, 13, 50460], "temperature": 0.0, "avg_logprob": -0.20447704027283867, "compression_ratio": 1.5115384615384615, "no_speech_prob": 0.014761491678655148}, {"id": 280, "seek": 111712, "start": 1119.04, "end": 1124.6399999999999, "text": " And currently, the open source implementation that we have on GitHub only supports the entry", "tokens": [50460, 400, 4362, 11, 264, 1269, 4009, 11420, 300, 321, 362, 322, 23331, 787, 9346, 264, 8729, 50740], "temperature": 0.0, "avg_logprob": -0.20447704027283867, "compression_ratio": 1.5115384615384615, "no_speech_prob": 0.014761491678655148}, {"id": 281, "seek": 111712, "start": 1124.6399999999999, "end": 1129.52, "text": " pod wrapping for containers, but we're working to expand Choc functionality with more and", "tokens": [50740, 2497, 21993, 337, 17089, 11, 457, 321, 434, 1364, 281, 5268, 761, 905, 14980, 365, 544, 293, 50984], "temperature": 0.0, "avg_logprob": -0.20447704027283867, "compression_ratio": 1.5115384615384615, "no_speech_prob": 0.014761491678655148}, {"id": 282, "seek": 111712, "start": 1129.52, "end": 1130.52, "text": " more features.", "tokens": [50984, 544, 4122, 13, 51034], "temperature": 0.0, "avg_logprob": -0.20447704027283867, "compression_ratio": 1.5115384615384615, "no_speech_prob": 0.014761491678655148}, {"id": 283, "seek": 111712, "start": 1130.52, "end": 1135.8799999999999, "text": " You can still Choc L files and PYC files and jars, et cetera.", "tokens": [51034, 509, 393, 920, 761, 905, 441, 7098, 293, 430, 56, 34, 7098, 293, 38239, 11, 1030, 11458, 13, 51302], "temperature": 0.0, "avg_logprob": -0.20447704027283867, "compression_ratio": 1.5115384615384615, "no_speech_prob": 0.014761491678655148}, {"id": 284, "seek": 111712, "start": 1135.8799999999999, "end": 1138.76, "text": " So yeah, the framework is out there.", "tokens": [51302, 407, 1338, 11, 264, 8388, 307, 484, 456, 13, 51446], "temperature": 0.0, "avg_logprob": -0.20447704027283867, "compression_ratio": 1.5115384615384615, "no_speech_prob": 0.014761491678655148}, {"id": 285, "seek": 111712, "start": 1138.76, "end": 1139.76, "text": " It's written in NIM.", "tokens": [51446, 467, 311, 3720, 294, 426, 6324, 13, 51496], "temperature": 0.0, "avg_logprob": -0.20447704027283867, "compression_ratio": 1.5115384615384615, "no_speech_prob": 0.014761491678655148}, {"id": 286, "seek": 111712, "start": 1139.76, "end": 1143.76, "text": " NIM is a very, very cool statically compiled type safe language.", "tokens": [51496, 426, 6324, 307, 257, 588, 11, 588, 1627, 2219, 984, 36548, 2010, 3273, 2856, 13, 51696], "temperature": 0.0, "avg_logprob": -0.20447704027283867, "compression_ratio": 1.5115384615384615, "no_speech_prob": 0.014761491678655148}, {"id": 287, "seek": 114376, "start": 1143.76, "end": 1149.72, "text": " So any fans of NIM here, feel free to contribute.", "tokens": [50364, 407, 604, 4499, 295, 426, 6324, 510, 11, 841, 1737, 281, 10586, 13, 50662], "temperature": 0.0, "avg_logprob": -0.29134567578633624, "compression_ratio": 1.2307692307692308, "no_speech_prob": 0.1095498725771904}, {"id": 288, "seek": 114376, "start": 1149.72, "end": 1152.56, "text": " And we're welcoming feature requests.", "tokens": [50662, 400, 321, 434, 17378, 4111, 12475, 13, 50804], "temperature": 0.0, "avg_logprob": -0.29134567578633624, "compression_ratio": 1.2307692307692308, "no_speech_prob": 0.1095498725771904}, {"id": 289, "seek": 114376, "start": 1152.56, "end": 1153.84, "text": " And I think that's my talk.", "tokens": [50804, 400, 286, 519, 300, 311, 452, 751, 13, 50868], "temperature": 0.0, "avg_logprob": -0.29134567578633624, "compression_ratio": 1.2307692307692308, "no_speech_prob": 0.1095498725771904}, {"id": 290, "seek": 114376, "start": 1153.84, "end": 1160.84, "text": " I'm happy to take questions or discuss this.", "tokens": [50868, 286, 478, 2055, 281, 747, 1651, 420, 2248, 341, 13, 51218], "temperature": 0.0, "avg_logprob": -0.29134567578633624, "compression_ratio": 1.2307692307692308, "no_speech_prob": 0.1095498725771904}, {"id": 291, "seek": 116084, "start": 1160.84, "end": 1167.84, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50714], "temperature": 0.0, "avg_logprob": -0.9154667131828539, "compression_ratio": 1.0864197530864197, "no_speech_prob": 0.17866863310337067}, {"id": 292, "seek": 116084, "start": 1167.84, "end": 1174.84, "text": " Thank you.", "tokens": [50714, 1044, 291, 13, 51064], "temperature": 0.0, "avg_logprob": -0.9154667131828539, "compression_ratio": 1.0864197530864197, "no_speech_prob": 0.17866863310337067}, {"id": 293, "seek": 116084, "start": 1174.84, "end": 1175.84, "text": " Yep.", "tokens": [51064, 7010, 13, 51114], "temperature": 0.0, "avg_logprob": -0.9154667131828539, "compression_ratio": 1.0864197530864197, "no_speech_prob": 0.17866863310337067}, {"id": 294, "seek": 116084, "start": 1175.84, "end": 1180.84, "text": " You talked about large organization.", "tokens": [51114, 509, 2825, 466, 2416, 4475, 13, 51364], "temperature": 0.0, "avg_logprob": -0.9154667131828539, "compression_ratio": 1.0864197530864197, "no_speech_prob": 0.17866863310337067}, {"id": 295, "seek": 116084, "start": 1180.84, "end": 1183.84, "text": " I'm very open to second.", "tokens": [51364, 286, 478, 588, 1269, 281, 1150, 13, 51514], "temperature": 0.0, "avg_logprob": -0.9154667131828539, "compression_ratio": 1.0864197530864197, "no_speech_prob": 0.17866863310337067}, {"id": 296, "seek": 118384, "start": 1183.84, "end": 1188.84, "text": " Yes.", "tokens": [50364, 1079, 13, 50614], "temperature": 0.0, "avg_logprob": -0.3191027487477949, "compression_ratio": 1.4662576687116564, "no_speech_prob": 0.15802571177482605}, {"id": 297, "seek": 118384, "start": 1188.84, "end": 1199.0, "text": " So the question here is I brought up large organizations, but given a concrete example", "tokens": [50614, 407, 264, 1168, 510, 307, 286, 3038, 493, 2416, 6150, 11, 457, 2212, 257, 9859, 1365, 51122], "temperature": 0.0, "avg_logprob": -0.3191027487477949, "compression_ratio": 1.4662576687116564, "no_speech_prob": 0.15802571177482605}, {"id": 298, "seek": 118384, "start": 1199.0, "end": 1203.52, "text": " of what are some use cases that this would apply in, right?", "tokens": [51122, 295, 437, 366, 512, 764, 3331, 300, 341, 576, 3079, 294, 11, 558, 30, 51348], "temperature": 0.0, "avg_logprob": -0.3191027487477949, "compression_ratio": 1.4662576687116564, "no_speech_prob": 0.15802571177482605}, {"id": 299, "seek": 118384, "start": 1203.52, "end": 1208.84, "text": " So just to make this clear, this does not only apply to a larger, a small organization.", "tokens": [51348, 407, 445, 281, 652, 341, 1850, 11, 341, 775, 406, 787, 3079, 281, 257, 4833, 11, 257, 1359, 4475, 13, 51614], "temperature": 0.0, "avg_logprob": -0.3191027487477949, "compression_ratio": 1.4662576687116564, "no_speech_prob": 0.15802571177482605}, {"id": 300, "seek": 120884, "start": 1208.84, "end": 1210.48, "text": " It applies to everyone.", "tokens": [50364, 467, 13165, 281, 1518, 13, 50446], "temperature": 0.0, "avg_logprob": -0.1558709327991192, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.5627484321594238}, {"id": 301, "seek": 120884, "start": 1210.48, "end": 1216.0, "text": " It's just that if you are having a single application with a single repository, pretty", "tokens": [50446, 467, 311, 445, 300, 498, 291, 366, 1419, 257, 2167, 3861, 365, 257, 2167, 25841, 11, 1238, 50722], "temperature": 0.0, "avg_logprob": -0.1558709327991192, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.5627484321594238}, {"id": 302, "seek": 120884, "start": 1216.0, "end": 1219.6399999999999, "text": " much you know exactly what version is deployed where.", "tokens": [50722, 709, 291, 458, 2293, 437, 3037, 307, 17826, 689, 13, 50904], "temperature": 0.0, "avg_logprob": -0.1558709327991192, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.5627484321594238}, {"id": 303, "seek": 120884, "start": 1219.6399999999999, "end": 1225.1999999999998, "text": " The complexities of these situations start to be amplified the bigger and bigger you", "tokens": [50904, 440, 48705, 295, 613, 6851, 722, 281, 312, 49237, 264, 3801, 293, 3801, 291, 51182], "temperature": 0.0, "avg_logprob": -0.1558709327991192, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.5627484321594238}, {"id": 304, "seek": 120884, "start": 1225.1999999999998, "end": 1226.1999999999998, "text": " get, right?", "tokens": [51182, 483, 11, 558, 30, 51232], "temperature": 0.0, "avg_logprob": -0.1558709327991192, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.5627484321594238}, {"id": 305, "seek": 120884, "start": 1226.1999999999998, "end": 1230.3999999999999, "text": " So if you have, say, a web application, and that web application has multiple components", "tokens": [51232, 407, 498, 291, 362, 11, 584, 11, 257, 3670, 3861, 11, 293, 300, 3670, 3861, 575, 3866, 6677, 51442], "temperature": 0.0, "avg_logprob": -0.1558709327991192, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.5627484321594238}, {"id": 306, "seek": 120884, "start": 1230.3999999999999, "end": 1237.6, "text": " that are live at any given time, or say you have a distributed service and you have microservices", "tokens": [51442, 300, 366, 1621, 412, 604, 2212, 565, 11, 420, 584, 291, 362, 257, 12631, 2643, 293, 291, 362, 15547, 47480, 51802], "temperature": 0.0, "avg_logprob": -0.1558709327991192, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.5627484321594238}, {"id": 307, "seek": 123760, "start": 1237.6, "end": 1243.6399999999999, "text": " running, you have multiple teams committing different versions of their component at any", "tokens": [50364, 2614, 11, 291, 362, 3866, 5491, 26659, 819, 9606, 295, 641, 6542, 412, 604, 50666], "temperature": 0.0, "avg_logprob": -0.18461150782448904, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.07835255563259125}, {"id": 308, "seek": 123760, "start": 1243.6399999999999, "end": 1245.24, "text": " given time.", "tokens": [50666, 2212, 565, 13, 50746], "temperature": 0.0, "avg_logprob": -0.18461150782448904, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.07835255563259125}, {"id": 309, "seek": 123760, "start": 1245.24, "end": 1248.6, "text": " And potentially some of these teams change.", "tokens": [50746, 400, 7263, 512, 295, 613, 5491, 1319, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18461150782448904, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.07835255563259125}, {"id": 310, "seek": 123760, "start": 1248.6, "end": 1253.1599999999999, "text": " So you could end up with a repository having outdated code, right?", "tokens": [50914, 407, 291, 727, 917, 493, 365, 257, 25841, 1419, 36313, 3089, 11, 558, 30, 51142], "temperature": 0.0, "avg_logprob": -0.18461150782448904, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.07835255563259125}, {"id": 311, "seek": 123760, "start": 1253.1599999999999, "end": 1257.8799999999999, "text": " There's a mission now, something has failed, and you go into the code, say, what was the", "tokens": [51142, 821, 311, 257, 4447, 586, 11, 746, 575, 7612, 11, 293, 291, 352, 666, 264, 3089, 11, 584, 11, 437, 390, 264, 51378], "temperature": 0.0, "avg_logprob": -0.18461150782448904, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.07835255563259125}, {"id": 312, "seek": 123760, "start": 1257.8799999999999, "end": 1258.8799999999999, "text": " last commit?", "tokens": [51378, 1036, 5599, 30, 51428], "temperature": 0.0, "avg_logprob": -0.18461150782448904, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.07835255563259125}, {"id": 313, "seek": 123760, "start": 1258.8799999999999, "end": 1260.56, "text": " It was six months ago.", "tokens": [51428, 467, 390, 2309, 2493, 2057, 13, 51512], "temperature": 0.0, "avg_logprob": -0.18461150782448904, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.07835255563259125}, {"id": 314, "seek": 123760, "start": 1260.56, "end": 1265.0, "text": " The committer of that application has left the team, potentially has left the company.", "tokens": [51512, 440, 800, 3904, 295, 300, 3861, 575, 1411, 264, 1469, 11, 7263, 575, 1411, 264, 2237, 13, 51734], "temperature": 0.0, "avg_logprob": -0.18461150782448904, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.07835255563259125}, {"id": 315, "seek": 123760, "start": 1265.0, "end": 1267.4399999999998, "text": " Who do you contact?", "tokens": [51734, 2102, 360, 291, 3385, 30, 51856], "temperature": 0.0, "avg_logprob": -0.18461150782448904, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.07835255563259125}, {"id": 316, "seek": 126744, "start": 1267.44, "end": 1271.16, "text": " How do you know that's actually that part that has been outdated?", "tokens": [50364, 1012, 360, 291, 458, 300, 311, 767, 300, 644, 300, 575, 668, 36313, 30, 50550], "temperature": 0.0, "avg_logprob": -0.11840696502150151, "compression_ratio": 1.784, "no_speech_prob": 0.019718926399946213}, {"id": 317, "seek": 126744, "start": 1271.16, "end": 1278.3600000000001, "text": " But if you keep track of your builds and your executions, you have the ability now to tap", "tokens": [50550, 583, 498, 291, 1066, 2837, 295, 428, 15182, 293, 428, 4454, 3666, 11, 291, 362, 264, 3485, 586, 281, 5119, 50910], "temperature": 0.0, "avg_logprob": -0.11840696502150151, "compression_ratio": 1.784, "no_speech_prob": 0.019718926399946213}, {"id": 318, "seek": 126744, "start": 1278.3600000000001, "end": 1284.04, "text": " into all the history of, like, all the provenance of a certain artifact and surface metrics", "tokens": [50910, 666, 439, 264, 2503, 295, 11, 411, 11, 439, 264, 12785, 719, 295, 257, 1629, 34806, 293, 3753, 16367, 51194], "temperature": 0.0, "avg_logprob": -0.11840696502150151, "compression_ratio": 1.784, "no_speech_prob": 0.019718926399946213}, {"id": 319, "seek": 126744, "start": 1284.04, "end": 1285.04, "text": " that you care about.", "tokens": [51194, 300, 291, 1127, 466, 13, 51244], "temperature": 0.0, "avg_logprob": -0.11840696502150151, "compression_ratio": 1.784, "no_speech_prob": 0.019718926399946213}, {"id": 320, "seek": 126744, "start": 1285.04, "end": 1290.0, "text": " So if you cared about, say, show me all the components that haven't been updated in the", "tokens": [51244, 407, 498, 291, 19779, 466, 11, 584, 11, 855, 385, 439, 264, 6677, 300, 2378, 380, 668, 10588, 294, 264, 51492], "temperature": 0.0, "avg_logprob": -0.11840696502150151, "compression_ratio": 1.784, "no_speech_prob": 0.019718926399946213}, {"id": 321, "seek": 126744, "start": 1290.0, "end": 1295.64, "text": " last month or that haven't been executed the last month, it's way, way easier to do this.", "tokens": [51492, 1036, 1618, 420, 300, 2378, 380, 668, 17577, 264, 1036, 1618, 11, 309, 311, 636, 11, 636, 3571, 281, 360, 341, 13, 51774], "temperature": 0.0, "avg_logprob": -0.11840696502150151, "compression_ratio": 1.784, "no_speech_prob": 0.019718926399946213}, {"id": 322, "seek": 129564, "start": 1295.64, "end": 1297.96, "text": " I'm not sure if I answered the question.", "tokens": [50364, 286, 478, 406, 988, 498, 286, 10103, 264, 1168, 13, 50480], "temperature": 0.0, "avg_logprob": -0.3548742294311523, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.21196293830871582}, {"id": 323, "seek": 129564, "start": 1297.96, "end": 1298.96, "text": " Yeah.", "tokens": [50480, 865, 13, 50530], "temperature": 0.0, "avg_logprob": -0.3548742294311523, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.21196293830871582}, {"id": 324, "seek": 129564, "start": 1298.96, "end": 1299.96, "text": " Yeah.", "tokens": [50530, 865, 13, 50580], "temperature": 0.0, "avg_logprob": -0.3548742294311523, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.21196293830871582}, {"id": 325, "seek": 129564, "start": 1299.96, "end": 1307.96, "text": " So you showed how to do it in a GitHub action, but could you generalize and do this manually", "tokens": [50580, 407, 291, 4712, 577, 281, 360, 309, 294, 257, 23331, 3069, 11, 457, 727, 291, 2674, 1125, 293, 360, 341, 16945, 50980], "temperature": 0.0, "avg_logprob": -0.3548742294311523, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.21196293830871582}, {"id": 326, "seek": 129564, "start": 1307.96, "end": 1313.0, "text": " in a one-prime or in a different pipeline environment as well?", "tokens": [50980, 294, 257, 472, 12, 1424, 1312, 420, 294, 257, 819, 15517, 2823, 382, 731, 30, 51232], "temperature": 0.0, "avg_logprob": -0.3548742294311523, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.21196293830871582}, {"id": 327, "seek": 129564, "start": 1313.0, "end": 1314.0, "text": " Yes, yes.", "tokens": [51232, 1079, 11, 2086, 13, 51282], "temperature": 0.0, "avg_logprob": -0.3548742294311523, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.21196293830871582}, {"id": 328, "seek": 129564, "start": 1314.0, "end": 1319.3600000000001, "text": " Chalk, you can actually, if you go now, if you visit the GitHub repo or the website, you", "tokens": [51282, 761, 667, 11, 291, 393, 767, 11, 498, 291, 352, 586, 11, 498, 291, 3441, 264, 23331, 49040, 420, 264, 3144, 11, 291, 51550], "temperature": 0.0, "avg_logprob": -0.3548742294311523, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.21196293830871582}, {"id": 329, "seek": 129564, "start": 1319.3600000000001, "end": 1321.0, "text": " can just download it.", "tokens": [51550, 393, 445, 5484, 309, 13, 51632], "temperature": 0.0, "avg_logprob": -0.3548742294311523, "compression_ratio": 1.5373831775700935, "no_speech_prob": 0.21196293830871582}, {"id": 330, "seek": 132100, "start": 1321.0, "end": 1322.12, "text": " And it's a binary that runs.", "tokens": [50364, 400, 309, 311, 257, 17434, 300, 6676, 13, 50420], "temperature": 0.0, "avg_logprob": -0.1786967424245981, "compression_ratio": 1.8078602620087336, "no_speech_prob": 0.030406499281525612}, {"id": 331, "seek": 132100, "start": 1322.12, "end": 1328.48, "text": " You can run locally and embed metadata into any artifact that you care about on your machine.", "tokens": [50420, 509, 393, 1190, 16143, 293, 12240, 26603, 666, 604, 34806, 300, 291, 1127, 466, 322, 428, 3479, 13, 50738], "temperature": 0.0, "avg_logprob": -0.1786967424245981, "compression_ratio": 1.8078602620087336, "no_speech_prob": 0.030406499281525612}, {"id": 332, "seek": 132100, "start": 1328.48, "end": 1334.3, "text": " So you can download on your laptop and scan all the L files in your system or the jar", "tokens": [50738, 407, 291, 393, 5484, 322, 428, 10732, 293, 11049, 439, 264, 441, 7098, 294, 428, 1185, 420, 264, 15181, 51029], "temperature": 0.0, "avg_logprob": -0.1786967424245981, "compression_ratio": 1.8078602620087336, "no_speech_prob": 0.030406499281525612}, {"id": 333, "seek": 132100, "start": 1334.3, "end": 1338.36, "text": " files or whatever, or even scan a whole directory.", "tokens": [51029, 7098, 420, 2035, 11, 420, 754, 11049, 257, 1379, 21120, 13, 51232], "temperature": 0.0, "avg_logprob": -0.1786967424245981, "compression_ratio": 1.8078602620087336, "no_speech_prob": 0.030406499281525612}, {"id": 334, "seek": 132100, "start": 1338.36, "end": 1340.52, "text": " You can specify whatever you want.", "tokens": [51232, 509, 393, 16500, 2035, 291, 528, 13, 51340], "temperature": 0.0, "avg_logprob": -0.1786967424245981, "compression_ratio": 1.8078602620087336, "no_speech_prob": 0.030406499281525612}, {"id": 335, "seek": 132100, "start": 1340.52, "end": 1344.68, "text": " And then you can configure metadata that you care about, and these will be embedded there.", "tokens": [51340, 400, 550, 291, 393, 22162, 26603, 300, 291, 1127, 466, 11, 293, 613, 486, 312, 16741, 456, 13, 51548], "temperature": 0.0, "avg_logprob": -0.1786967424245981, "compression_ratio": 1.8078602620087336, "no_speech_prob": 0.030406499281525612}, {"id": 336, "seek": 132100, "start": 1344.68, "end": 1346.24, "text": " And you can then extract it.", "tokens": [51548, 400, 291, 393, 550, 8947, 309, 13, 51626], "temperature": 0.0, "avg_logprob": -0.1786967424245981, "compression_ratio": 1.8078602620087336, "no_speech_prob": 0.030406499281525612}, {"id": 337, "seek": 134624, "start": 1346.24, "end": 1349.66, "text": " So you don't necessarily need to have Chalk report back to you or run it in a GitHub", "tokens": [50364, 407, 291, 500, 380, 4725, 643, 281, 362, 761, 667, 2275, 646, 281, 291, 420, 1190, 309, 294, 257, 23331, 50535], "temperature": 0.0, "avg_logprob": -0.21974505800189395, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.003547275671735406}, {"id": 338, "seek": 134624, "start": 1349.66, "end": 1351.16, "text": " action.", "tokens": [50535, 3069, 13, 50610], "temperature": 0.0, "avg_logprob": -0.21974505800189395, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.003547275671735406}, {"id": 339, "seek": 134624, "start": 1351.16, "end": 1356.1200000000001, "text": " You can just use it to embed information and then surface it.", "tokens": [50610, 509, 393, 445, 764, 309, 281, 12240, 1589, 293, 550, 3753, 309, 13, 50858], "temperature": 0.0, "avg_logprob": -0.21974505800189395, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.003547275671735406}, {"id": 340, "seek": 134624, "start": 1356.1200000000001, "end": 1359.96, "text": " So you can both insert and extract if that makes sense.", "tokens": [50858, 407, 291, 393, 1293, 8969, 293, 8947, 498, 300, 1669, 2020, 13, 51050], "temperature": 0.0, "avg_logprob": -0.21974505800189395, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.003547275671735406}, {"id": 341, "seek": 134624, "start": 1359.96, "end": 1360.96, "text": " Yep.", "tokens": [51050, 7010, 13, 51100], "temperature": 0.0, "avg_logprob": -0.21974505800189395, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.003547275671735406}, {"id": 342, "seek": 134624, "start": 1360.96, "end": 1366.36, "text": " So that's a great question.", "tokens": [51100, 407, 300, 311, 257, 869, 1168, 13, 51370], "temperature": 0.0, "avg_logprob": -0.21974505800189395, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.003547275671735406}, {"id": 343, "seek": 134624, "start": 1366.36, "end": 1373.88, "text": " I think one of the big benefits of Chalk is that you can embed information even in generated", "tokens": [51370, 286, 519, 472, 295, 264, 955, 5311, 295, 761, 667, 307, 300, 291, 393, 12240, 1589, 754, 294, 10833, 51746], "temperature": 0.0, "avg_logprob": -0.21974505800189395, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.003547275671735406}, {"id": 344, "seek": 134624, "start": 1373.88, "end": 1375.36, "text": " images or artifacts, right?", "tokens": [51746, 5267, 420, 24617, 11, 558, 30, 51820], "temperature": 0.0, "avg_logprob": -0.21974505800189395, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.003547275671735406}, {"id": 345, "seek": 137536, "start": 1375.36, "end": 1379.8, "text": " So if you're using, say you have some third party software like a library that you're", "tokens": [50364, 407, 498, 291, 434, 1228, 11, 584, 291, 362, 512, 2636, 3595, 4722, 411, 257, 6405, 300, 291, 434, 50586], "temperature": 0.0, "avg_logprob": -0.15975806496360084, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.008787725120782852}, {"id": 346, "seek": 137536, "start": 1379.8, "end": 1384.28, "text": " consuming, perhaps you don't know where it came from, but you know that you saw it in", "tokens": [50586, 19867, 11, 4317, 291, 500, 380, 458, 689, 309, 1361, 490, 11, 457, 291, 458, 300, 291, 1866, 309, 294, 50810], "temperature": 0.0, "avg_logprob": -0.15975806496360084, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.008787725120782852}, {"id": 347, "seek": 137536, "start": 1384.28, "end": 1387.0, "text": " a certain machine at a certain hash.", "tokens": [50810, 257, 1629, 3479, 412, 257, 1629, 22019, 13, 50946], "temperature": 0.0, "avg_logprob": -0.15975806496360084, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.008787725120782852}, {"id": 348, "seek": 137536, "start": 1387.0, "end": 1393.12, "text": " And then you can use Chalk to encapsulate that information for your artifact.", "tokens": [50946, 400, 550, 291, 393, 764, 761, 667, 281, 38745, 5256, 300, 1589, 337, 428, 34806, 13, 51252], "temperature": 0.0, "avg_logprob": -0.15975806496360084, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.008787725120782852}, {"id": 349, "seek": 137536, "start": 1393.12, "end": 1398.24, "text": " And basically, if you run a query across all your applications that say are importing", "tokens": [51252, 400, 1936, 11, 498, 291, 1190, 257, 14581, 2108, 439, 428, 5821, 300, 584, 366, 43866, 51508], "temperature": 0.0, "avg_logprob": -0.15975806496360084, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.008787725120782852}, {"id": 350, "seek": 137536, "start": 1398.24, "end": 1402.8799999999999, "text": " a given library, you can see all the versions of that library that are running.", "tokens": [51508, 257, 2212, 6405, 11, 291, 393, 536, 439, 264, 9606, 295, 300, 6405, 300, 366, 2614, 13, 51740], "temperature": 0.0, "avg_logprob": -0.15975806496360084, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.008787725120782852}, {"id": 351, "seek": 140288, "start": 1402.88, "end": 1407.7600000000002, "text": " So you can start building these application inventories very easily, even if it's a third", "tokens": [50364, 407, 291, 393, 722, 2390, 613, 3861, 7962, 2083, 588, 3612, 11, 754, 498, 309, 311, 257, 2636, 50608], "temperature": 0.0, "avg_logprob": -0.22430849520959587, "compression_ratio": 1.7551867219917012, "no_speech_prob": 0.018800534307956696}, {"id": 352, "seek": 140288, "start": 1407.7600000000002, "end": 1409.0, "text": " party software.", "tokens": [50608, 3595, 4722, 13, 50670], "temperature": 0.0, "avg_logprob": -0.22430849520959587, "compression_ratio": 1.7551867219917012, "no_speech_prob": 0.018800534307956696}, {"id": 353, "seek": 140288, "start": 1409.0, "end": 1413.0, "text": " Is it the total of the bottom third party container?", "tokens": [50670, 1119, 309, 264, 3217, 295, 264, 2767, 2636, 3595, 10129, 30, 50870], "temperature": 0.0, "avg_logprob": -0.22430849520959587, "compression_ratio": 1.7551867219917012, "no_speech_prob": 0.018800534307956696}, {"id": 354, "seek": 140288, "start": 1413.0, "end": 1414.44, "text": " It's still the same premise, right?", "tokens": [50870, 467, 311, 920, 264, 912, 22045, 11, 558, 30, 50942], "temperature": 0.0, "avg_logprob": -0.22430849520959587, "compression_ratio": 1.7551867219917012, "no_speech_prob": 0.018800534307956696}, {"id": 355, "seek": 140288, "start": 1414.44, "end": 1419.4, "text": " Because if you have a container, you have several layers.", "tokens": [50942, 1436, 498, 291, 362, 257, 10129, 11, 291, 362, 2940, 7914, 13, 51190], "temperature": 0.0, "avg_logprob": -0.22430849520959587, "compression_ratio": 1.7551867219917012, "no_speech_prob": 0.018800534307956696}, {"id": 356, "seek": 140288, "start": 1419.4, "end": 1424.0400000000002, "text": " So you can start saying, okay, these are the layers I have seen here.", "tokens": [51190, 407, 291, 393, 722, 1566, 11, 1392, 11, 613, 366, 264, 7914, 286, 362, 1612, 510, 13, 51422], "temperature": 0.0, "avg_logprob": -0.22430849520959587, "compression_ratio": 1.7551867219917012, "no_speech_prob": 0.018800534307956696}, {"id": 357, "seek": 140288, "start": 1424.0400000000002, "end": 1429.92, "text": " And potentially you don't have the full information, but you can at least ensure that you can attest", "tokens": [51422, 400, 7263, 291, 500, 380, 362, 264, 1577, 1589, 11, 457, 291, 393, 412, 1935, 5586, 300, 291, 393, 951, 377, 51716], "temperature": 0.0, "avg_logprob": -0.22430849520959587, "compression_ratio": 1.7551867219917012, "no_speech_prob": 0.018800534307956696}, {"id": 358, "seek": 142992, "start": 1429.96, "end": 1433.24, "text": " that, okay, these are, this is the hash that I have seen.", "tokens": [50366, 300, 11, 1392, 11, 613, 366, 11, 341, 307, 264, 22019, 300, 286, 362, 1612, 13, 50530], "temperature": 0.0, "avg_logprob": -0.34520150807278216, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.018125465139746666}, {"id": 359, "seek": 142992, "start": 1433.24, "end": 1438.6000000000001, "text": " We are starting to add support to actually wrap entry points of different layers if you'd", "tokens": [50530, 492, 366, 2891, 281, 909, 1406, 281, 767, 7019, 8729, 2793, 295, 819, 7914, 498, 291, 1116, 50798], "temperature": 0.0, "avg_logprob": -0.34520150807278216, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.018125465139746666}, {"id": 360, "seek": 142992, "start": 1438.6000000000001, "end": 1439.6000000000001, "text": " like to.", "tokens": [50798, 411, 281, 13, 50848], "temperature": 0.0, "avg_logprob": -0.34520150807278216, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.018125465139746666}, {"id": 361, "seek": 142992, "start": 1439.6000000000001, "end": 1443.88, "text": " So you should be able to interpose yourself in another layer should you like to, but", "tokens": [50848, 407, 291, 820, 312, 1075, 281, 728, 43501, 1803, 294, 1071, 4583, 820, 291, 411, 281, 11, 457, 51062], "temperature": 0.0, "avg_logprob": -0.34520150807278216, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.018125465139746666}, {"id": 362, "seek": 142992, "start": 1443.88, "end": 1445.68, "text": " that's not currently up yet.", "tokens": [51062, 300, 311, 406, 4362, 493, 1939, 13, 51152], "temperature": 0.0, "avg_logprob": -0.34520150807278216, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.018125465139746666}, {"id": 363, "seek": 142992, "start": 1445.68, "end": 1448.04, "text": " It's not up on the open source limitation.", "tokens": [51152, 467, 311, 406, 493, 322, 264, 1269, 4009, 27432, 13, 51270], "temperature": 0.0, "avg_logprob": -0.34520150807278216, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.018125465139746666}, {"id": 364, "seek": 142992, "start": 1448.04, "end": 1449.04, "text": " Yeah?", "tokens": [51270, 865, 30, 51320], "temperature": 0.0, "avg_logprob": -0.34520150807278216, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.018125465139746666}, {"id": 365, "seek": 142992, "start": 1449.04, "end": 1453.04, "text": " How does Chalk play together with the useful bits?", "tokens": [51320, 1012, 775, 761, 667, 862, 1214, 365, 264, 4420, 9239, 30, 51520], "temperature": 0.0, "avg_logprob": -0.34520150807278216, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.018125465139746666}, {"id": 366, "seek": 142992, "start": 1453.04, "end": 1457.24, "text": " Are they to include them in the library?", "tokens": [51520, 2014, 436, 281, 4090, 552, 294, 264, 6405, 30, 51730], "temperature": 0.0, "avg_logprob": -0.34520150807278216, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.018125465139746666}, {"id": 367, "seek": 142992, "start": 1457.24, "end": 1459.0800000000002, "text": " That's a great question.", "tokens": [51730, 663, 311, 257, 869, 1168, 13, 51822], "temperature": 0.0, "avg_logprob": -0.34520150807278216, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.018125465139746666}, {"id": 368, "seek": 145908, "start": 1459.08, "end": 1461.8, "text": " No, you don't need to include any compiler.", "tokens": [50364, 883, 11, 291, 500, 380, 643, 281, 4090, 604, 31958, 13, 50500], "temperature": 0.0, "avg_logprob": -0.2521438313953912, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0347108356654644}, {"id": 369, "seek": 145908, "start": 1461.8, "end": 1464.0, "text": " All you need is the binary.", "tokens": [50500, 1057, 291, 643, 307, 264, 17434, 13, 50610], "temperature": 0.0, "avg_logprob": -0.2521438313953912, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0347108356654644}, {"id": 370, "seek": 145908, "start": 1464.0, "end": 1467.84, "text": " And then if you have a reducible build for, in your pipeline, you should still be able", "tokens": [50610, 400, 550, 498, 291, 362, 257, 2783, 32128, 1322, 337, 11, 294, 428, 15517, 11, 291, 820, 920, 312, 1075, 50802], "temperature": 0.0, "avg_logprob": -0.2521438313953912, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0347108356654644}, {"id": 371, "seek": 145908, "start": 1467.84, "end": 1469.52, "text": " to achieve the same guarantees.", "tokens": [50802, 281, 4584, 264, 912, 32567, 13, 50886], "temperature": 0.0, "avg_logprob": -0.2521438313953912, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0347108356654644}, {"id": 372, "seek": 145908, "start": 1469.52, "end": 1476.24, "text": " For instance, if you have, say, an L file, we'll embed metadata into a section and that", "tokens": [50886, 1171, 5197, 11, 498, 291, 362, 11, 584, 11, 364, 441, 3991, 11, 321, 603, 12240, 26603, 666, 257, 3541, 293, 300, 51222], "temperature": 0.0, "avg_logprob": -0.2521438313953912, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0347108356654644}, {"id": 373, "seek": 145908, "start": 1476.24, "end": 1478.32, "text": " will survive stripping and all that.", "tokens": [51222, 486, 7867, 3575, 3759, 293, 439, 300, 13, 51326], "temperature": 0.0, "avg_logprob": -0.2521438313953912, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0347108356654644}, {"id": 374, "seek": 145908, "start": 1478.32, "end": 1484.84, "text": " So once you have a build, then assuming you know that you're running with Chalk, right,", "tokens": [51326, 407, 1564, 291, 362, 257, 1322, 11, 550, 11926, 291, 458, 300, 291, 434, 2614, 365, 761, 667, 11, 558, 11, 51652], "temperature": 0.0, "avg_logprob": -0.2521438313953912, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0347108356654644}, {"id": 375, "seek": 145908, "start": 1484.84, "end": 1489.04, "text": " and you don't modify the thing later on inappropriately, you would at least know that you're running", "tokens": [51652, 293, 291, 500, 380, 16927, 264, 551, 1780, 322, 24728, 1592, 11, 291, 576, 412, 1935, 458, 300, 291, 434, 2614, 51862], "temperature": 0.0, "avg_logprob": -0.2521438313953912, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0347108356654644}, {"id": 376, "seek": 148904, "start": 1489.04, "end": 1490.04, "text": " with Chalk, right?", "tokens": [50364, 365, 761, 667, 11, 558, 30, 50414], "temperature": 0.0, "avg_logprob": -0.4750528261643048, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.06942279636859894}, {"id": 377, "seek": 148904, "start": 1490.04, "end": 1493.04, "text": " So that if you're getting a report, that report has not been tampered with.", "tokens": [50414, 407, 300, 498, 291, 434, 1242, 257, 2275, 11, 300, 2275, 575, 406, 668, 7677, 40004, 365, 13, 50564], "temperature": 0.0, "avg_logprob": -0.4750528261643048, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.06942279636859894}, {"id": 378, "seek": 148904, "start": 1493.04, "end": 1494.04, "text": " Yep?", "tokens": [50564, 7010, 30, 50614], "temperature": 0.0, "avg_logprob": -0.4750528261643048, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.06942279636859894}, {"id": 379, "seek": 148904, "start": 1494.04, "end": 1499.04, "text": " Let's imagine I have a jar which I have Chalk, right?", "tokens": [50614, 961, 311, 3811, 286, 362, 257, 15181, 597, 286, 362, 761, 667, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.4750528261643048, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.06942279636859894}, {"id": 380, "seek": 148904, "start": 1499.04, "end": 1504.04, "text": " Then I modify it and zip change it.", "tokens": [50864, 1396, 286, 16927, 309, 293, 20730, 1319, 309, 13, 51114], "temperature": 0.0, "avg_logprob": -0.4750528261643048, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.06942279636859894}, {"id": 381, "seek": 148904, "start": 1504.04, "end": 1509.04, "text": " So at which point Q and V8 and then I Chalk it again.", "tokens": [51114, 407, 412, 597, 935, 1249, 293, 691, 23, 293, 550, 286, 761, 667, 309, 797, 13, 51364], "temperature": 0.0, "avg_logprob": -0.4750528261643048, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.06942279636859894}, {"id": 382, "seek": 148904, "start": 1509.04, "end": 1512.04, "text": " At which point how do you pull the code?", "tokens": [51364, 1711, 597, 935, 577, 360, 291, 2235, 264, 3089, 30, 51514], "temperature": 0.0, "avg_logprob": -0.4750528261643048, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.06942279636859894}, {"id": 383, "seek": 148904, "start": 1512.04, "end": 1513.04, "text": " Right.", "tokens": [51514, 1779, 13, 51564], "temperature": 0.0, "avg_logprob": -0.4750528261643048, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.06942279636859894}, {"id": 384, "seek": 148904, "start": 1513.04, "end": 1517.32, "text": " So the question is, suppose you have a jar, you Chalk it, and then you modify it and then", "tokens": [51564, 407, 264, 1168, 307, 11, 7297, 291, 362, 257, 15181, 11, 291, 761, 667, 309, 11, 293, 550, 291, 16927, 309, 293, 550, 51778], "temperature": 0.0, "avg_logprob": -0.4750528261643048, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.06942279636859894}, {"id": 385, "seek": 148904, "start": 1517.32, "end": 1518.82, "text": " you Chalk it again.", "tokens": [51778, 291, 761, 667, 309, 797, 13, 51853], "temperature": 0.0, "avg_logprob": -0.4750528261643048, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.06942279636859894}, {"id": 386, "seek": 151882, "start": 1518.82, "end": 1520.74, "text": " How does the tool help you here?", "tokens": [50364, 1012, 775, 264, 2290, 854, 291, 510, 30, 50460], "temperature": 0.0, "avg_logprob": -0.12160381723622807, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.02540445141494274}, {"id": 387, "seek": 151882, "start": 1520.74, "end": 1525.7, "text": " So Chalk does not allow you to have just a single Chalk mark within a binary.", "tokens": [50460, 407, 761, 667, 775, 406, 2089, 291, 281, 362, 445, 257, 2167, 761, 667, 1491, 1951, 257, 17434, 13, 50708], "temperature": 0.0, "avg_logprob": -0.12160381723622807, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.02540445141494274}, {"id": 388, "seek": 151882, "start": 1525.7, "end": 1530.5, "text": " You can wrap Chalk marks within Chalk marks within Chalk marks essentially.", "tokens": [50708, 509, 393, 7019, 761, 667, 10640, 1951, 761, 667, 10640, 1951, 761, 667, 10640, 4476, 13, 50948], "temperature": 0.0, "avg_logprob": -0.12160381723622807, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.02540445141494274}, {"id": 389, "seek": 151882, "start": 1530.5, "end": 1535.3799999999999, "text": " So if you're making modifications and you'd want it to, you can maintain past information", "tokens": [50948, 407, 498, 291, 434, 1455, 26881, 293, 291, 1116, 528, 309, 281, 11, 291, 393, 6909, 1791, 1589, 51192], "temperature": 0.0, "avg_logprob": -0.12160381723622807, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.02540445141494274}, {"id": 390, "seek": 151882, "start": 1535.3799999999999, "end": 1537.02, "text": " about past Chalk marks.", "tokens": [51192, 466, 1791, 761, 667, 10640, 13, 51274], "temperature": 0.0, "avg_logprob": -0.12160381723622807, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.02540445141494274}, {"id": 391, "seek": 151882, "start": 1537.02, "end": 1543.26, "text": " Or if you're building a jar, say, out of other jars and those have Chalk marks, you can use", "tokens": [51274, 1610, 498, 291, 434, 2390, 257, 15181, 11, 584, 11, 484, 295, 661, 38239, 293, 729, 362, 761, 667, 10640, 11, 291, 393, 764, 51586], "temperature": 0.0, "avg_logprob": -0.12160381723622807, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.02540445141494274}, {"id": 392, "seek": 151882, "start": 1543.26, "end": 1546.7, "text": " this information and embed them into your final jar, if that makes sense.", "tokens": [51586, 341, 1589, 293, 12240, 552, 666, 428, 2572, 15181, 11, 498, 300, 1669, 2020, 13, 51758], "temperature": 0.0, "avg_logprob": -0.12160381723622807, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.02540445141494274}, {"id": 393, "seek": 154670, "start": 1546.7, "end": 1550.7, "text": " So you can wrap and encapsulate all the metadata from all the components.", "tokens": [50364, 407, 291, 393, 7019, 293, 38745, 5256, 439, 264, 26603, 490, 439, 264, 6677, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2427742041430427, "compression_ratio": 1.5866141732283465, "no_speech_prob": 0.021942678838968277}, {"id": 394, "seek": 154670, "start": 1550.7, "end": 1554.7, "text": " So I need to focus more on this.", "tokens": [50564, 407, 286, 643, 281, 1879, 544, 322, 341, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2427742041430427, "compression_ratio": 1.5866141732283465, "no_speech_prob": 0.021942678838968277}, {"id": 395, "seek": 154670, "start": 1554.7, "end": 1558.14, "text": " Well it wouldn't be more complex than just saying Chalk insert.", "tokens": [50764, 1042, 309, 2759, 380, 312, 544, 3997, 813, 445, 1566, 761, 667, 8969, 13, 50936], "temperature": 0.0, "avg_logprob": -0.2427742041430427, "compression_ratio": 1.5866141732283465, "no_speech_prob": 0.021942678838968277}, {"id": 396, "seek": 154670, "start": 1558.14, "end": 1567.14, "text": " Like Chalk would take care of all the build dependencies and make sure it injects it automatically.", "tokens": [50936, 1743, 761, 667, 576, 747, 1127, 295, 439, 264, 1322, 36606, 293, 652, 988, 309, 10711, 82, 309, 6772, 13, 51386], "temperature": 0.0, "avg_logprob": -0.2427742041430427, "compression_ratio": 1.5866141732283465, "no_speech_prob": 0.021942678838968277}, {"id": 397, "seek": 154670, "start": 1567.14, "end": 1570.1000000000001, "text": " At least that's where we're heading at.", "tokens": [51386, 1711, 1935, 300, 311, 689, 321, 434, 9864, 412, 13, 51534], "temperature": 0.0, "avg_logprob": -0.2427742041430427, "compression_ratio": 1.5866141732283465, "no_speech_prob": 0.021942678838968277}, {"id": 398, "seek": 154670, "start": 1570.1000000000001, "end": 1573.82, "text": " It might not be full feature for all the flavors of what can be choked currently, but that's", "tokens": [51534, 467, 1062, 406, 312, 1577, 4111, 337, 439, 264, 16303, 295, 437, 393, 312, 417, 9511, 4362, 11, 457, 300, 311, 51720], "temperature": 0.0, "avg_logprob": -0.2427742041430427, "compression_ratio": 1.5866141732283465, "no_speech_prob": 0.021942678838968277}, {"id": 399, "seek": 157382, "start": 1573.82, "end": 1580.82, "text": " where we want to go for sure.", "tokens": [50364, 689, 321, 528, 281, 352, 337, 988, 13, 50714], "temperature": 0.0, "avg_logprob": -0.5567688465118408, "compression_ratio": 0.8846153846153846, "no_speech_prob": 0.020831210538744926}, {"id": 400, "seek": 157382, "start": 1580.82, "end": 1584.1, "text": " Cool.", "tokens": [50714, 8561, 13, 50878], "temperature": 0.0, "avg_logprob": -0.5567688465118408, "compression_ratio": 0.8846153846153846, "no_speech_prob": 0.020831210538744926}, {"id": 401, "seek": 157382, "start": 1584.1, "end": 1584.3, "text": " Thank you.", "tokens": [50878, 1044, 291, 13, 50888], "temperature": 0.0, "avg_logprob": -0.5567688465118408, "compression_ratio": 0.8846153846153846, "no_speech_prob": 0.020831210538744926}], "language": "en"}