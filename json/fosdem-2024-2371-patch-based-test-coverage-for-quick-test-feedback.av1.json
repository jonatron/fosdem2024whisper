{"text": " All right, next up, Shivam Gupta. Okay. Good afternoon, everyone. Today I will be talking about my GSOC project. It was mentored by Henrik Olson, and this GSOC project is about patch-based code coverage testing for LLVM patches. And so in this talk, the agenda is, first we will introduce what is the project is about, and then what is the terminology we use, like how LLVM test cases are written, and then we will see what is LLVM source-based code coverage that is used to get the code coverage of a patch. And then we see how it is implemented. It is basically a Python script, so we will see what are the functions used to implement this tool. And then we will see a demo, like it's already a patch is there in LLVM community. We will see like what is the lines are covered or what lines are not covered with this patch. So we will start by introducing introductions. So LLVM test cases are written in a lit format. Regression tests are written in lit format, and unit test cases are written in Google test format, or Google mock these formats. So the goal of this project is to help developer to create good test coverage for their patches, and it will also help the reviewers to know that the code they are submitting, it has a good test coverage or not. So this is the project, and to accomplish this project we have created a Python tool. It's around 800 lines of Python code, so it will fetch the patch as input, and then it will extract some information like what are the source lines in the patch, what are the test cases in the patch, test case lines in the patch, and then we build a LLVM project with the code coverage enabled. So it will instrument our binary. So whenever we run the test case with this binary, so it will generate a prof data file that will be further converted, further processed, and then it will show the lines which lines are covered or not covered by the source code of the patch. So LLVM test suits basically like they have two kind of test cases written for any patches. One is regression test, and second is unit test. Mainly the regression tests are written for most of the patches. So these regression tests are in .LL format or .C format for different tools. So mostly our focus is on regression tests, and some test cases are written in unit test case. So these test cases are test for libraries, like support libraries or FSEG data types. So these are checking the feature in the system, how it is well indicated in the system. So this is unit test case. Regression test is very small, but you can see at the top there is one run line which will actually run for this test case. Then there is unit test case. This is using Google Gold Test Library. So it has some micros to check. It is not important, but these two kind of test cases are in LLVM for any patches. And then we will see what is the source-based code coverage. So source-based code coverage consists of three steps. The first step is compiling the program with the coverage enabled. We want to instrument any binary. So we will use a fropile generate this flag we will use and this will generate foo binary which is instrumented. So in the next step when we run this binary, it will generate a prof data file. That prof data file contains the data for further creating coverage reports. So next is the tool is LLVM prof data. This tool is used to convert the prof format to prof data format which is further used by LLVM cov to generate or show the report of what lines are covered or not. In the next slide we have a simple test case and I have generated the report. It checks if the number is even or odd. If we pass suppose 5, it will say that the number is odd and this line, this if condition will not run. So it will show like this. This is the report of LLVM cov for any program. Next is implementation. So for implementation I have submitted two patches. For this first one is about the change in LLVM lit. This is the testing tool that is used to run the test case, regression test case in LLVM. So initially whenever we run a test case, it will generate prof data in some random name. So we have modified that and we have given a proper name for every test case. So it will generate a proper name in a specific directory. So this is categorization of prof data. Next we have the main tool that has all the functions that will pass the patch and then build the project, LLVM project and then generate data. Then process the data to show the coverage report to reviewer or a patch author. Next these are the some functions that are implemented in the tool. First two function is just a logging function. And then it is sequentially like we as a name suggest we have first we create the patch from the last commit or from the patch itself. And then we accept the source file and then we have write source file allow list that is used to reduce the coverage data. Because if we generate the coverage data for all the files of LLVM then it will be around 150 MB for each test case. So it will be difficult to process later. So we will use, we have used a flag afro file list. This flag used to generate coverage data for only the files in the patch. So next we accept the modified source line from the patch and then we build the project. We build the project with a flag LLVM build instrumentation. So this flag is passed during the CMECH invocation. So when we pass this the binary that will be old for LLVM project will have instrumentation enabled. And then we run the single test case with coverage and that is helper function next the modified lit test case or unit test case. Whichever they if it is if the patch contains a lit test case then it will run the regression. It will run the that function and if it has unit test case then this function will call and the test case will run. And next we have a process coverage data which will process the data. And next similarly we have a coverage file and it will run. Then we will have a print coverage detail that will actually be printing the coverage detail. We will also have a log file. So print coverage detail have a print a lot of details. So it will print something to log file and then we will print common uncovered line which is so in a patch there is one source file. But there are many test case. If one test case is covering the source file then it is covered. But if all the test case are not covering a source line then it means that this line is uncovered. So it will print the uncovered line this way and then there are some helper functions which is not important. This is the GitHub CI workflow that is actually is a file that is used to compile the project like on GitHub. So it is like it is holding the project and then at the end it is running a Python Git code coverage. This is the file name. So it will run here in the Python code and then it will print the coverage result. I will show this is the format. It will show the common uncovered line for the...", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " All right, next up, Shivam Gupta.", "tokens": [50364, 1057, 558, 11, 958, 493, 11, 47839, 335, 2694, 47366, 13, 50864], "temperature": 0.0, "avg_logprob": -0.41464802424112956, "compression_ratio": 1.2753623188405796, "no_speech_prob": 0.30404040217399597}, {"id": 1, "seek": 0, "start": 10.0, "end": 11.0, "text": " Okay.", "tokens": [50864, 1033, 13, 50914], "temperature": 0.0, "avg_logprob": -0.41464802424112956, "compression_ratio": 1.2753623188405796, "no_speech_prob": 0.30404040217399597}, {"id": 2, "seek": 0, "start": 11.0, "end": 13.0, "text": " Good afternoon, everyone.", "tokens": [50914, 2205, 6499, 11, 1518, 13, 51014], "temperature": 0.0, "avg_logprob": -0.41464802424112956, "compression_ratio": 1.2753623188405796, "no_speech_prob": 0.30404040217399597}, {"id": 3, "seek": 0, "start": 13.0, "end": 17.0, "text": " Today I will be talking about my GSOC project.", "tokens": [51014, 2692, 286, 486, 312, 1417, 466, 452, 460, 17188, 34, 1716, 13, 51214], "temperature": 0.0, "avg_logprob": -0.41464802424112956, "compression_ratio": 1.2753623188405796, "no_speech_prob": 0.30404040217399597}, {"id": 4, "seek": 0, "start": 17.0, "end": 23.0, "text": " It was mentored by Henrik Olson, and this GSOC project is about", "tokens": [51214, 467, 390, 3074, 2769, 538, 8651, 14456, 6141, 3015, 11, 293, 341, 460, 17188, 34, 1716, 307, 466, 51514], "temperature": 0.0, "avg_logprob": -0.41464802424112956, "compression_ratio": 1.2753623188405796, "no_speech_prob": 0.30404040217399597}, {"id": 5, "seek": 2300, "start": 23.0, "end": 28.0, "text": " patch-based code coverage testing for LLVM patches.", "tokens": [50364, 9972, 12, 6032, 3089, 9645, 4997, 337, 441, 43, 53, 44, 26531, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14715935872948688, "compression_ratio": 1.7329842931937174, "no_speech_prob": 0.30193063616752625}, {"id": 6, "seek": 2300, "start": 28.0, "end": 36.0, "text": " And so in this talk, the agenda is, first we will introduce what is the project is about,", "tokens": [50614, 400, 370, 294, 341, 751, 11, 264, 9829, 307, 11, 700, 321, 486, 5366, 437, 307, 264, 1716, 307, 466, 11, 51014], "temperature": 0.0, "avg_logprob": -0.14715935872948688, "compression_ratio": 1.7329842931937174, "no_speech_prob": 0.30193063616752625}, {"id": 7, "seek": 2300, "start": 36.0, "end": 41.0, "text": " and then what is the terminology we use, like how LLVM test cases are written,", "tokens": [51014, 293, 550, 437, 307, 264, 27575, 321, 764, 11, 411, 577, 441, 43, 53, 44, 1500, 3331, 366, 3720, 11, 51264], "temperature": 0.0, "avg_logprob": -0.14715935872948688, "compression_ratio": 1.7329842931937174, "no_speech_prob": 0.30193063616752625}, {"id": 8, "seek": 2300, "start": 41.0, "end": 51.0, "text": " and then we will see what is LLVM source-based code coverage that is used to get the code coverage of a patch.", "tokens": [51264, 293, 550, 321, 486, 536, 437, 307, 441, 43, 53, 44, 4009, 12, 6032, 3089, 9645, 300, 307, 1143, 281, 483, 264, 3089, 9645, 295, 257, 9972, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14715935872948688, "compression_ratio": 1.7329842931937174, "no_speech_prob": 0.30193063616752625}, {"id": 9, "seek": 5100, "start": 51.0, "end": 53.0, "text": " And then we see how it is implemented.", "tokens": [50364, 400, 550, 321, 536, 577, 309, 307, 12270, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11394682238178869, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.018165472894906998}, {"id": 10, "seek": 5100, "start": 53.0, "end": 61.0, "text": " It is basically a Python script, so we will see what are the functions used to implement this tool.", "tokens": [50464, 467, 307, 1936, 257, 15329, 5755, 11, 370, 321, 486, 536, 437, 366, 264, 6828, 1143, 281, 4445, 341, 2290, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11394682238178869, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.018165472894906998}, {"id": 11, "seek": 5100, "start": 61.0, "end": 65.0, "text": " And then we will see a demo, like it's already a patch is there in LLVM community.", "tokens": [50864, 400, 550, 321, 486, 536, 257, 10723, 11, 411, 309, 311, 1217, 257, 9972, 307, 456, 294, 441, 43, 53, 44, 1768, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11394682238178869, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.018165472894906998}, {"id": 12, "seek": 5100, "start": 65.0, "end": 71.0, "text": " We will see like what is the lines are covered or what lines are not covered with this patch.", "tokens": [51064, 492, 486, 536, 411, 437, 307, 264, 3876, 366, 5343, 420, 437, 3876, 366, 406, 5343, 365, 341, 9972, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11394682238178869, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.018165472894906998}, {"id": 13, "seek": 5100, "start": 71.0, "end": 76.0, "text": " So we will start by introducing introductions.", "tokens": [51364, 407, 321, 486, 722, 538, 15424, 48032, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11394682238178869, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.018165472894906998}, {"id": 14, "seek": 7600, "start": 76.0, "end": 80.0, "text": " So LLVM test cases are written in a lit format.", "tokens": [50364, 407, 441, 43, 53, 44, 1500, 3331, 366, 3720, 294, 257, 7997, 7877, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1411533550340302, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.04786190763115883}, {"id": 15, "seek": 7600, "start": 80.0, "end": 87.0, "text": " Regression tests are written in lit format, and unit test cases are written in Google test format,", "tokens": [50564, 4791, 2775, 6921, 366, 3720, 294, 7997, 7877, 11, 293, 4985, 1500, 3331, 366, 3720, 294, 3329, 1500, 7877, 11, 50914], "temperature": 0.0, "avg_logprob": -0.1411533550340302, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.04786190763115883}, {"id": 16, "seek": 7600, "start": 87.0, "end": 90.0, "text": " or Google mock these formats.", "tokens": [50914, 420, 3329, 17362, 613, 25879, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1411533550340302, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.04786190763115883}, {"id": 17, "seek": 7600, "start": 90.0, "end": 95.0, "text": " So the goal of this project is to help developer to create good test coverage for their patches,", "tokens": [51064, 407, 264, 3387, 295, 341, 1716, 307, 281, 854, 10754, 281, 1884, 665, 1500, 9645, 337, 641, 26531, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1411533550340302, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.04786190763115883}, {"id": 18, "seek": 7600, "start": 95.0, "end": 102.0, "text": " and it will also help the reviewers to know that the code they are submitting,", "tokens": [51314, 293, 309, 486, 611, 854, 264, 45837, 281, 458, 300, 264, 3089, 436, 366, 31836, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1411533550340302, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.04786190763115883}, {"id": 19, "seek": 7600, "start": 102.0, "end": 104.0, "text": " it has a good test coverage or not.", "tokens": [51664, 309, 575, 257, 665, 1500, 9645, 420, 406, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1411533550340302, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.04786190763115883}, {"id": 20, "seek": 10400, "start": 104.0, "end": 110.0, "text": " So this is the project, and to accomplish this project we have created a Python tool.", "tokens": [50364, 407, 341, 307, 264, 1716, 11, 293, 281, 9021, 341, 1716, 321, 362, 2942, 257, 15329, 2290, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10015844561390042, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.011457706801593304}, {"id": 21, "seek": 10400, "start": 110.0, "end": 117.0, "text": " It's around 800 lines of Python code, so it will fetch the patch as input,", "tokens": [50664, 467, 311, 926, 13083, 3876, 295, 15329, 3089, 11, 370, 309, 486, 23673, 264, 9972, 382, 4846, 11, 51014], "temperature": 0.0, "avg_logprob": -0.10015844561390042, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.011457706801593304}, {"id": 22, "seek": 10400, "start": 117.0, "end": 122.0, "text": " and then it will extract some information like what are the source lines in the patch,", "tokens": [51014, 293, 550, 309, 486, 8947, 512, 1589, 411, 437, 366, 264, 4009, 3876, 294, 264, 9972, 11, 51264], "temperature": 0.0, "avg_logprob": -0.10015844561390042, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.011457706801593304}, {"id": 23, "seek": 10400, "start": 122.0, "end": 126.0, "text": " what are the test cases in the patch, test case lines in the patch,", "tokens": [51264, 437, 366, 264, 1500, 3331, 294, 264, 9972, 11, 1500, 1389, 3876, 294, 264, 9972, 11, 51464], "temperature": 0.0, "avg_logprob": -0.10015844561390042, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.011457706801593304}, {"id": 24, "seek": 10400, "start": 126.0, "end": 131.0, "text": " and then we build a LLVM project with the code coverage enabled.", "tokens": [51464, 293, 550, 321, 1322, 257, 441, 43, 53, 44, 1716, 365, 264, 3089, 9645, 15172, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10015844561390042, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.011457706801593304}, {"id": 25, "seek": 13100, "start": 131.0, "end": 136.0, "text": " So it will instrument our binary.", "tokens": [50364, 407, 309, 486, 7198, 527, 17434, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11211815165049994, "compression_ratio": 1.6626506024096386, "no_speech_prob": 0.01184830442070961}, {"id": 26, "seek": 13100, "start": 136.0, "end": 140.0, "text": " So whenever we run the test case with this binary,", "tokens": [50614, 407, 5699, 321, 1190, 264, 1500, 1389, 365, 341, 17434, 11, 50814], "temperature": 0.0, "avg_logprob": -0.11211815165049994, "compression_ratio": 1.6626506024096386, "no_speech_prob": 0.01184830442070961}, {"id": 27, "seek": 13100, "start": 140.0, "end": 148.0, "text": " so it will generate a prof data file that will be further converted, further processed,", "tokens": [50814, 370, 309, 486, 8460, 257, 1740, 1412, 3991, 300, 486, 312, 3052, 16424, 11, 3052, 18846, 11, 51214], "temperature": 0.0, "avg_logprob": -0.11211815165049994, "compression_ratio": 1.6626506024096386, "no_speech_prob": 0.01184830442070961}, {"id": 28, "seek": 13100, "start": 148.0, "end": 156.0, "text": " and then it will show the lines which lines are covered or not covered by the source code of the patch.", "tokens": [51214, 293, 550, 309, 486, 855, 264, 3876, 597, 3876, 366, 5343, 420, 406, 5343, 538, 264, 4009, 3089, 295, 264, 9972, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11211815165049994, "compression_ratio": 1.6626506024096386, "no_speech_prob": 0.01184830442070961}, {"id": 29, "seek": 15600, "start": 156.0, "end": 166.0, "text": " So LLVM test suits basically like they have two kind of test cases written for any patches.", "tokens": [50364, 407, 441, 43, 53, 44, 1500, 15278, 1936, 411, 436, 362, 732, 733, 295, 1500, 3331, 3720, 337, 604, 26531, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15752073744652975, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.01570453867316246}, {"id": 30, "seek": 15600, "start": 166.0, "end": 168.0, "text": " One is regression test, and second is unit test.", "tokens": [50864, 1485, 307, 24590, 1500, 11, 293, 1150, 307, 4985, 1500, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15752073744652975, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.01570453867316246}, {"id": 31, "seek": 15600, "start": 168.0, "end": 173.0, "text": " Mainly the regression tests are written for most of the patches.", "tokens": [50964, 47468, 264, 24590, 6921, 366, 3720, 337, 881, 295, 264, 26531, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15752073744652975, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.01570453867316246}, {"id": 32, "seek": 15600, "start": 173.0, "end": 181.0, "text": " So these regression tests are in .LL format or .C format for different tools.", "tokens": [51214, 407, 613, 24590, 6921, 366, 294, 2411, 24010, 7877, 420, 2411, 34, 7877, 337, 819, 3873, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15752073744652975, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.01570453867316246}, {"id": 33, "seek": 18100, "start": 182.0, "end": 190.0, "text": " So mostly our focus is on regression tests, and some test cases are written in unit test case.", "tokens": [50414, 407, 5240, 527, 1879, 307, 322, 24590, 6921, 11, 293, 512, 1500, 3331, 366, 3720, 294, 4985, 1500, 1389, 13, 50814], "temperature": 0.0, "avg_logprob": -0.21725372314453126, "compression_ratio": 1.757396449704142, "no_speech_prob": 0.009104457683861256}, {"id": 34, "seek": 18100, "start": 190.0, "end": 196.0, "text": " So these test cases are test for libraries, like support libraries or FSEG data types.", "tokens": [50814, 407, 613, 1500, 3331, 366, 1500, 337, 15148, 11, 411, 1406, 15148, 420, 479, 5879, 38, 1412, 3467, 13, 51114], "temperature": 0.0, "avg_logprob": -0.21725372314453126, "compression_ratio": 1.757396449704142, "no_speech_prob": 0.009104457683861256}, {"id": 35, "seek": 18100, "start": 196.0, "end": 204.0, "text": " So these are checking the feature in the system, how it is well indicated in the system.", "tokens": [51114, 407, 613, 366, 8568, 264, 4111, 294, 264, 1185, 11, 577, 309, 307, 731, 16176, 294, 264, 1185, 13, 51514], "temperature": 0.0, "avg_logprob": -0.21725372314453126, "compression_ratio": 1.757396449704142, "no_speech_prob": 0.009104457683861256}, {"id": 36, "seek": 18100, "start": 204.0, "end": 207.0, "text": " So this is unit test case.", "tokens": [51514, 407, 341, 307, 4985, 1500, 1389, 13, 51664], "temperature": 0.0, "avg_logprob": -0.21725372314453126, "compression_ratio": 1.757396449704142, "no_speech_prob": 0.009104457683861256}, {"id": 37, "seek": 20700, "start": 207.0, "end": 225.0, "text": " Regression test is very small, but you can see at the top there is one run line which will actually run for this test case.", "tokens": [50364, 4791, 2775, 1500, 307, 588, 1359, 11, 457, 291, 393, 536, 412, 264, 1192, 456, 307, 472, 1190, 1622, 597, 486, 767, 1190, 337, 341, 1500, 1389, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2642212662042356, "compression_ratio": 1.4296296296296296, "no_speech_prob": 0.011164725758135319}, {"id": 38, "seek": 20700, "start": 225.0, "end": 227.0, "text": " Then there is unit test case.", "tokens": [51264, 1396, 456, 307, 4985, 1500, 1389, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2642212662042356, "compression_ratio": 1.4296296296296296, "no_speech_prob": 0.011164725758135319}, {"id": 39, "seek": 20700, "start": 227.0, "end": 233.0, "text": " This is using Google Gold Test Library.", "tokens": [51364, 639, 307, 1228, 3329, 6731, 9279, 12806, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2642212662042356, "compression_ratio": 1.4296296296296296, "no_speech_prob": 0.011164725758135319}, {"id": 40, "seek": 23300, "start": 233.0, "end": 235.0, "text": " So it has some micros to check.", "tokens": [50364, 407, 309, 575, 512, 15547, 281, 1520, 13, 50464], "temperature": 0.0, "avg_logprob": -0.20160372873370566, "compression_ratio": 1.5687203791469195, "no_speech_prob": 0.015365082770586014}, {"id": 41, "seek": 23300, "start": 235.0, "end": 242.0, "text": " It is not important, but these two kind of test cases are in LLVM for any patches.", "tokens": [50464, 467, 307, 406, 1021, 11, 457, 613, 732, 733, 295, 1500, 3331, 366, 294, 441, 43, 53, 44, 337, 604, 26531, 13, 50814], "temperature": 0.0, "avg_logprob": -0.20160372873370566, "compression_ratio": 1.5687203791469195, "no_speech_prob": 0.015365082770586014}, {"id": 42, "seek": 23300, "start": 242.0, "end": 248.0, "text": " And then we will see what is the source-based code coverage.", "tokens": [50814, 400, 550, 321, 486, 536, 437, 307, 264, 4009, 12, 6032, 3089, 9645, 13, 51114], "temperature": 0.0, "avg_logprob": -0.20160372873370566, "compression_ratio": 1.5687203791469195, "no_speech_prob": 0.015365082770586014}, {"id": 43, "seek": 23300, "start": 248.0, "end": 252.0, "text": " So source-based code coverage consists of three steps.", "tokens": [51114, 407, 4009, 12, 6032, 3089, 9645, 14689, 295, 1045, 4439, 13, 51314], "temperature": 0.0, "avg_logprob": -0.20160372873370566, "compression_ratio": 1.5687203791469195, "no_speech_prob": 0.015365082770586014}, {"id": 44, "seek": 23300, "start": 252.0, "end": 256.0, "text": " The first step is compiling the program with the coverage enabled.", "tokens": [51314, 440, 700, 1823, 307, 715, 4883, 264, 1461, 365, 264, 9645, 15172, 13, 51514], "temperature": 0.0, "avg_logprob": -0.20160372873370566, "compression_ratio": 1.5687203791469195, "no_speech_prob": 0.015365082770586014}, {"id": 45, "seek": 23300, "start": 256.0, "end": 261.0, "text": " We want to instrument any binary.", "tokens": [51514, 492, 528, 281, 7198, 604, 17434, 13, 51764], "temperature": 0.0, "avg_logprob": -0.20160372873370566, "compression_ratio": 1.5687203791469195, "no_speech_prob": 0.015365082770586014}, {"id": 46, "seek": 26100, "start": 261.0, "end": 270.0, "text": " So we will use a fropile generate this flag we will use and this will generate foo binary which is instrumented.", "tokens": [50364, 407, 321, 486, 764, 257, 283, 1513, 794, 8460, 341, 7166, 321, 486, 764, 293, 341, 486, 8460, 726, 78, 17434, 597, 307, 7198, 292, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2308809973976829, "compression_ratio": 1.7243589743589745, "no_speech_prob": 0.01181073673069477}, {"id": 47, "seek": 26100, "start": 270.0, "end": 277.0, "text": " So in the next step when we run this binary, it will generate a prof data file.", "tokens": [50814, 407, 294, 264, 958, 1823, 562, 321, 1190, 341, 17434, 11, 309, 486, 8460, 257, 1740, 1412, 3991, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2308809973976829, "compression_ratio": 1.7243589743589745, "no_speech_prob": 0.01181073673069477}, {"id": 48, "seek": 26100, "start": 277.0, "end": 286.0, "text": " That prof data file contains the data for further creating coverage reports.", "tokens": [51164, 663, 1740, 1412, 3991, 8306, 264, 1412, 337, 3052, 4084, 9645, 7122, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2308809973976829, "compression_ratio": 1.7243589743589745, "no_speech_prob": 0.01181073673069477}, {"id": 49, "seek": 28600, "start": 286.0, "end": 289.0, "text": " So next is the tool is LLVM prof data.", "tokens": [50364, 407, 958, 307, 264, 2290, 307, 441, 43, 53, 44, 1740, 1412, 13, 50514], "temperature": 0.0, "avg_logprob": -0.18889669368141576, "compression_ratio": 1.6374269005847952, "no_speech_prob": 0.0854533240199089}, {"id": 50, "seek": 28600, "start": 289.0, "end": 302.0, "text": " This tool is used to convert the prof format to prof data format which is further used by LLVM cov to generate or show the report of what lines are covered or not.", "tokens": [50514, 639, 2290, 307, 1143, 281, 7620, 264, 1740, 7877, 281, 1740, 1412, 7877, 597, 307, 3052, 1143, 538, 441, 43, 53, 44, 598, 85, 281, 8460, 420, 855, 264, 2275, 295, 437, 3876, 366, 5343, 420, 406, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18889669368141576, "compression_ratio": 1.6374269005847952, "no_speech_prob": 0.0854533240199089}, {"id": 51, "seek": 28600, "start": 302.0, "end": 309.0, "text": " In the next slide we have a simple test case and I have generated the report.", "tokens": [51164, 682, 264, 958, 4137, 321, 362, 257, 2199, 1500, 1389, 293, 286, 362, 10833, 264, 2275, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18889669368141576, "compression_ratio": 1.6374269005847952, "no_speech_prob": 0.0854533240199089}, {"id": 52, "seek": 30900, "start": 310.0, "end": 313.0, "text": " It checks if the number is even or odd.", "tokens": [50414, 467, 13834, 498, 264, 1230, 307, 754, 420, 7401, 13, 50564], "temperature": 0.0, "avg_logprob": -0.16283985546657018, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.04712711647152901}, {"id": 53, "seek": 30900, "start": 313.0, "end": 322.0, "text": " If we pass suppose 5, it will say that the number is odd and this line, this if condition will not run.", "tokens": [50564, 759, 321, 1320, 7297, 1025, 11, 309, 486, 584, 300, 264, 1230, 307, 7401, 293, 341, 1622, 11, 341, 498, 4188, 486, 406, 1190, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16283985546657018, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.04712711647152901}, {"id": 54, "seek": 30900, "start": 322.0, "end": 323.0, "text": " So it will show like this.", "tokens": [51014, 407, 309, 486, 855, 411, 341, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16283985546657018, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.04712711647152901}, {"id": 55, "seek": 30900, "start": 323.0, "end": 329.0, "text": " This is the report of LLVM cov for any program.", "tokens": [51064, 639, 307, 264, 2275, 295, 441, 43, 53, 44, 598, 85, 337, 604, 1461, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16283985546657018, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.04712711647152901}, {"id": 56, "seek": 30900, "start": 329.0, "end": 333.0, "text": " Next is implementation.", "tokens": [51364, 3087, 307, 11420, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16283985546657018, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.04712711647152901}, {"id": 57, "seek": 30900, "start": 333.0, "end": 336.0, "text": " So for implementation I have submitted two patches.", "tokens": [51564, 407, 337, 11420, 286, 362, 14405, 732, 26531, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16283985546657018, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.04712711647152901}, {"id": 58, "seek": 33600, "start": 336.0, "end": 343.0, "text": " For this first one is about the change in LLVM lit.", "tokens": [50364, 1171, 341, 700, 472, 307, 466, 264, 1319, 294, 441, 43, 53, 44, 7997, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1662147668691782, "compression_ratio": 1.5298013245033113, "no_speech_prob": 0.007097003050148487}, {"id": 59, "seek": 33600, "start": 343.0, "end": 351.0, "text": " This is the testing tool that is used to run the test case, regression test case in LLVM.", "tokens": [50714, 639, 307, 264, 4997, 2290, 300, 307, 1143, 281, 1190, 264, 1500, 1389, 11, 24590, 1500, 1389, 294, 441, 43, 53, 44, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1662147668691782, "compression_ratio": 1.5298013245033113, "no_speech_prob": 0.007097003050148487}, {"id": 60, "seek": 33600, "start": 351.0, "end": 359.0, "text": " So initially whenever we run a test case, it will generate prof data in some random name.", "tokens": [51114, 407, 9105, 5699, 321, 1190, 257, 1500, 1389, 11, 309, 486, 8460, 1740, 1412, 294, 512, 4974, 1315, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1662147668691782, "compression_ratio": 1.5298013245033113, "no_speech_prob": 0.007097003050148487}, {"id": 61, "seek": 35900, "start": 359.0, "end": 365.0, "text": " So we have modified that and we have given a proper name for every test case.", "tokens": [50364, 407, 321, 362, 15873, 300, 293, 321, 362, 2212, 257, 2296, 1315, 337, 633, 1500, 1389, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15096583366394042, "compression_ratio": 1.675392670157068, "no_speech_prob": 0.02902768738567829}, {"id": 62, "seek": 35900, "start": 365.0, "end": 368.0, "text": " So it will generate a proper name in a specific directory.", "tokens": [50664, 407, 309, 486, 8460, 257, 2296, 1315, 294, 257, 2685, 21120, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15096583366394042, "compression_ratio": 1.675392670157068, "no_speech_prob": 0.02902768738567829}, {"id": 63, "seek": 35900, "start": 368.0, "end": 375.0, "text": " So this is categorization of prof data.", "tokens": [50814, 407, 341, 307, 19250, 2144, 295, 1740, 1412, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15096583366394042, "compression_ratio": 1.675392670157068, "no_speech_prob": 0.02902768738567829}, {"id": 64, "seek": 35900, "start": 375.0, "end": 388.0, "text": " Next we have the main tool that has all the functions that will pass the patch and then build the project, LLVM project and then generate data.", "tokens": [51164, 3087, 321, 362, 264, 2135, 2290, 300, 575, 439, 264, 6828, 300, 486, 1320, 264, 9972, 293, 550, 1322, 264, 1716, 11, 441, 43, 53, 44, 1716, 293, 550, 8460, 1412, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15096583366394042, "compression_ratio": 1.675392670157068, "no_speech_prob": 0.02902768738567829}, {"id": 65, "seek": 38800, "start": 388.0, "end": 395.0, "text": " Then process the data to show the coverage report to reviewer or a patch author.", "tokens": [50364, 1396, 1399, 264, 1412, 281, 855, 264, 9645, 2275, 281, 3131, 260, 420, 257, 9972, 3793, 13, 50714], "temperature": 0.0, "avg_logprob": -0.24310625516451323, "compression_ratio": 1.65, "no_speech_prob": 0.011436471715569496}, {"id": 66, "seek": 38800, "start": 395.0, "end": 400.0, "text": " Next these are the some functions that are implemented in the tool.", "tokens": [50714, 3087, 613, 366, 264, 512, 6828, 300, 366, 12270, 294, 264, 2290, 13, 50964], "temperature": 0.0, "avg_logprob": -0.24310625516451323, "compression_ratio": 1.65, "no_speech_prob": 0.011436471715569496}, {"id": 67, "seek": 38800, "start": 400.0, "end": 404.0, "text": " First two function is just a logging function.", "tokens": [50964, 2386, 732, 2445, 307, 445, 257, 27991, 2445, 13, 51164], "temperature": 0.0, "avg_logprob": -0.24310625516451323, "compression_ratio": 1.65, "no_speech_prob": 0.011436471715569496}, {"id": 68, "seek": 38800, "start": 404.0, "end": 413.0, "text": " And then it is sequentially like we as a name suggest we have first we create the patch from the last commit or from the patch itself.", "tokens": [51164, 400, 550, 309, 307, 5123, 3137, 411, 321, 382, 257, 1315, 3402, 321, 362, 700, 321, 1884, 264, 9972, 490, 264, 1036, 5599, 420, 490, 264, 9972, 2564, 13, 51614], "temperature": 0.0, "avg_logprob": -0.24310625516451323, "compression_ratio": 1.65, "no_speech_prob": 0.011436471715569496}, {"id": 69, "seek": 41300, "start": 413.0, "end": 426.0, "text": " And then we accept the source file and then we have write source file allow list that is used to reduce the coverage data.", "tokens": [50364, 400, 550, 321, 3241, 264, 4009, 3991, 293, 550, 321, 362, 2464, 4009, 3991, 2089, 1329, 300, 307, 1143, 281, 5407, 264, 9645, 1412, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16536123999233904, "compression_ratio": 1.6989795918367347, "no_speech_prob": 0.06267135590314865}, {"id": 70, "seek": 41300, "start": 426.0, "end": 435.0, "text": " Because if we generate the coverage data for all the files of LLVM then it will be around 150 MB for each test case.", "tokens": [51014, 1436, 498, 321, 8460, 264, 9645, 1412, 337, 439, 264, 7098, 295, 441, 43, 53, 44, 550, 309, 486, 312, 926, 8451, 28866, 337, 1184, 1500, 1389, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16536123999233904, "compression_ratio": 1.6989795918367347, "no_speech_prob": 0.06267135590314865}, {"id": 71, "seek": 41300, "start": 435.0, "end": 437.0, "text": " So it will be difficult to process later.", "tokens": [51464, 407, 309, 486, 312, 2252, 281, 1399, 1780, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16536123999233904, "compression_ratio": 1.6989795918367347, "no_speech_prob": 0.06267135590314865}, {"id": 72, "seek": 41300, "start": 437.0, "end": 442.0, "text": " So we will use, we have used a flag afro file list.", "tokens": [51564, 407, 321, 486, 764, 11, 321, 362, 1143, 257, 7166, 3238, 340, 3991, 1329, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16536123999233904, "compression_ratio": 1.6989795918367347, "no_speech_prob": 0.06267135590314865}, {"id": 73, "seek": 44200, "start": 442.0, "end": 449.0, "text": " This flag used to generate coverage data for only the files in the patch.", "tokens": [50364, 639, 7166, 1143, 281, 8460, 9645, 1412, 337, 787, 264, 7098, 294, 264, 9972, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1393651693639621, "compression_ratio": 1.5536723163841808, "no_speech_prob": 0.01908738911151886}, {"id": 74, "seek": 44200, "start": 449.0, "end": 455.0, "text": " So next we accept the modified source line from the patch and then we build the project.", "tokens": [50714, 407, 958, 321, 3241, 264, 15873, 4009, 1622, 490, 264, 9972, 293, 550, 321, 1322, 264, 1716, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1393651693639621, "compression_ratio": 1.5536723163841808, "no_speech_prob": 0.01908738911151886}, {"id": 75, "seek": 44200, "start": 455.0, "end": 458.0, "text": " We build the project with a flag LLVM build instrumentation.", "tokens": [51014, 492, 1322, 264, 1716, 365, 257, 7166, 441, 43, 53, 44, 1322, 7198, 399, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1393651693639621, "compression_ratio": 1.5536723163841808, "no_speech_prob": 0.01908738911151886}, {"id": 76, "seek": 44200, "start": 458.0, "end": 464.0, "text": " So this flag is passed during the CMECH invocation.", "tokens": [51164, 407, 341, 7166, 307, 4678, 1830, 264, 383, 15454, 5462, 1048, 27943, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1393651693639621, "compression_ratio": 1.5536723163841808, "no_speech_prob": 0.01908738911151886}, {"id": 77, "seek": 46400, "start": 464.0, "end": 474.0, "text": " So when we pass this the binary that will be old for LLVM project will have instrumentation enabled.", "tokens": [50364, 407, 562, 321, 1320, 341, 264, 17434, 300, 486, 312, 1331, 337, 441, 43, 53, 44, 1716, 486, 362, 7198, 399, 15172, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1870579352745643, "compression_ratio": 1.649746192893401, "no_speech_prob": 0.0353582389652729}, {"id": 78, "seek": 46400, "start": 474.0, "end": 484.0, "text": " And then we run the single test case with coverage and that is helper function next the modified lit test case or unit test case.", "tokens": [50864, 400, 550, 321, 1190, 264, 2167, 1500, 1389, 365, 9645, 293, 300, 307, 36133, 2445, 958, 264, 15873, 7997, 1500, 1389, 420, 4985, 1500, 1389, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1870579352745643, "compression_ratio": 1.649746192893401, "no_speech_prob": 0.0353582389652729}, {"id": 79, "seek": 46400, "start": 484.0, "end": 489.0, "text": " Whichever they if it is if the patch contains a lit test case then it will run the regression.", "tokens": [51364, 3013, 1054, 436, 498, 309, 307, 498, 264, 9972, 8306, 257, 7997, 1500, 1389, 550, 309, 486, 1190, 264, 24590, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1870579352745643, "compression_ratio": 1.649746192893401, "no_speech_prob": 0.0353582389652729}, {"id": 80, "seek": 48900, "start": 489.0, "end": 498.0, "text": " It will run the that function and if it has unit test case then this function will call and the test case will run.", "tokens": [50364, 467, 486, 1190, 264, 300, 2445, 293, 498, 309, 575, 4985, 1500, 1389, 550, 341, 2445, 486, 818, 293, 264, 1500, 1389, 486, 1190, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13811142836944967, "compression_ratio": 1.937142857142857, "no_speech_prob": 0.030689941719174385}, {"id": 81, "seek": 48900, "start": 498.0, "end": 503.0, "text": " And next we have a process coverage data which will process the data.", "tokens": [50814, 400, 958, 321, 362, 257, 1399, 9645, 1412, 597, 486, 1399, 264, 1412, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13811142836944967, "compression_ratio": 1.937142857142857, "no_speech_prob": 0.030689941719174385}, {"id": 82, "seek": 48900, "start": 503.0, "end": 510.0, "text": " And next similarly we have a coverage file and it will run.", "tokens": [51064, 400, 958, 14138, 321, 362, 257, 9645, 3991, 293, 309, 486, 1190, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13811142836944967, "compression_ratio": 1.937142857142857, "no_speech_prob": 0.030689941719174385}, {"id": 83, "seek": 48900, "start": 510.0, "end": 515.0, "text": " Then we will have a print coverage detail that will actually be printing the coverage detail.", "tokens": [51414, 1396, 321, 486, 362, 257, 4482, 9645, 2607, 300, 486, 767, 312, 14699, 264, 9645, 2607, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13811142836944967, "compression_ratio": 1.937142857142857, "no_speech_prob": 0.030689941719174385}, {"id": 84, "seek": 51500, "start": 515.0, "end": 522.0, "text": " We will also have a log file. So print coverage detail have a print a lot of details.", "tokens": [50364, 492, 486, 611, 362, 257, 3565, 3991, 13, 407, 4482, 9645, 2607, 362, 257, 4482, 257, 688, 295, 4365, 13, 50714], "temperature": 0.0, "avg_logprob": -0.17947293535063538, "compression_ratio": 1.7988505747126438, "no_speech_prob": 0.06813406199216843}, {"id": 85, "seek": 51500, "start": 522.0, "end": 535.0, "text": " So it will print something to log file and then we will print common uncovered line which is so in a patch there is one source file.", "tokens": [50714, 407, 309, 486, 4482, 746, 281, 3565, 3991, 293, 550, 321, 486, 4482, 2689, 37729, 1622, 597, 307, 370, 294, 257, 9972, 456, 307, 472, 4009, 3991, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17947293535063538, "compression_ratio": 1.7988505747126438, "no_speech_prob": 0.06813406199216843}, {"id": 86, "seek": 51500, "start": 535.0, "end": 537.0, "text": " But there are many test case.", "tokens": [51364, 583, 456, 366, 867, 1500, 1389, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17947293535063538, "compression_ratio": 1.7988505747126438, "no_speech_prob": 0.06813406199216843}, {"id": 87, "seek": 51500, "start": 537.0, "end": 542.0, "text": " If one test case is covering the source file then it is covered.", "tokens": [51464, 759, 472, 1500, 1389, 307, 10322, 264, 4009, 3991, 550, 309, 307, 5343, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17947293535063538, "compression_ratio": 1.7988505747126438, "no_speech_prob": 0.06813406199216843}, {"id": 88, "seek": 54200, "start": 542.0, "end": 550.0, "text": " But if all the test case are not covering a source line then it means that this line is uncovered.", "tokens": [50364, 583, 498, 439, 264, 1500, 1389, 366, 406, 10322, 257, 4009, 1622, 550, 309, 1355, 300, 341, 1622, 307, 37729, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12311025460561116, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00912147294729948}, {"id": 89, "seek": 54200, "start": 550.0, "end": 560.0, "text": " So it will print the uncovered line this way and then there are some helper functions which is not important.", "tokens": [50764, 407, 309, 486, 4482, 264, 37729, 1622, 341, 636, 293, 550, 456, 366, 512, 36133, 6828, 597, 307, 406, 1021, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12311025460561116, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00912147294729948}, {"id": 90, "seek": 56000, "start": 560.0, "end": 572.0, "text": " This is the GitHub CI workflow that is actually is a file that is used to compile the project like on GitHub.", "tokens": [50364, 639, 307, 264, 23331, 37777, 20993, 300, 307, 767, 307, 257, 3991, 300, 307, 1143, 281, 31413, 264, 1716, 411, 322, 23331, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17958809779240534, "compression_ratio": 1.5179856115107915, "no_speech_prob": 0.20798802375793457}, {"id": 91, "seek": 56000, "start": 572.0, "end": 580.0, "text": " So it is like it is holding the project and then at the end it is running a Python Git code coverage.", "tokens": [50964, 407, 309, 307, 411, 309, 307, 5061, 264, 1716, 293, 550, 412, 264, 917, 309, 307, 2614, 257, 15329, 16939, 3089, 9645, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17958809779240534, "compression_ratio": 1.5179856115107915, "no_speech_prob": 0.20798802375793457}, {"id": 92, "seek": 58000, "start": 580.0, "end": 595.0, "text": " This is the file name. So it will run here in the Python code and then it will print the coverage result.", "tokens": [50364, 639, 307, 264, 3991, 1315, 13, 407, 309, 486, 1190, 510, 294, 264, 15329, 3089, 293, 550, 309, 486, 4482, 264, 9645, 1874, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1463515507547479, "compression_ratio": 1.3047619047619048, "no_speech_prob": 0.09583872556686401}, {"id": 93, "seek": 58000, "start": 595.0, "end": 600.0, "text": " I will show this is the format.", "tokens": [51114, 286, 486, 855, 341, 307, 264, 7877, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1463515507547479, "compression_ratio": 1.3047619047619048, "no_speech_prob": 0.09583872556686401}, {"id": 94, "seek": 60000, "start": 600.0, "end": 605.0, "text": " It will show the common uncovered line for the...", "tokens": [50364, 467, 486, 855, 264, 2689, 37729, 1622, 337, 264, 485, 50614], "temperature": 0.0, "avg_logprob": -0.3402404418358436, "compression_ratio": 0.8909090909090909, "no_speech_prob": 0.9178397059440613}], "language": "en"}