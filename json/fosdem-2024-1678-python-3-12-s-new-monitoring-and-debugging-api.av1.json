{"text": " It's time. So thank you very much to Johannes Bergberg. He will be speaking about Python 3.12 new monitoring and debugging API. For those who were in the previous talk, there was a brief about the profiling features. Johannes is a JVM developer working on profilers at SAP. He also writes blogs about profiling and debugging topics. Thank you very much. Thank you for introducing me. Before we start, I want to introduce you to the concept of debugging, because I'm sure nobody of you have ever debunked. So the first bug that's what's like found was in the 50s when they found a moth that was in between their relays and it makes zip and the whole system like crash because like in the olden days it relays. At SCADAISCAR once wrote, if debugging is the process of removing Zafferbacks and pro pro-peering must be the process of putting them in. As I'm sure all of you are doing lots of programming, I'm sure you're also doing lots of debugging. So that's what we're here. So consider this example program. It's called a counterprong. It just counts the lines in this example in itself and it returns zero. And we're like, why? So that's the problem because the file actually has 26 lines. So let's look at the code. I'm going to see clans of the code. Make this shortly so you don't see what it's about. But the idea is we use the debugger through this because the debugger is a great to understand our system. And the cool thing is now with the new APIs that we get in Python 3.12, writing the debugger is far easier and far faster as I show you in the following. I'm Johannes Pechberger as you already heard. I work at SAP machine, which is the third biggest contributor to the OpenJK, which is like the major Java runtime. And I started talking to people about Python because I also like Python. So it's a bit easier to VM than JVM. The question is now, why do we need a monitoring and debugging API? Because when I'm from the Java world and in Java, we haven't built in debugging API. So we have the ability to set breakpoints, to ask for values, to walk the stack and have everything. But in Python, does the Python interpreter know about the concept of breakpoints? So because I'm here, not only, but with a few here, who of you thinks that the Python interpreter knows about the concept of breakpoints? Please raise your hand. And two of your things. It doesn't know about the concept of breakpoints. Okay. It's a trick question, of course, no, because otherwise I wouldn't be asking this question. So it doesn't know anything about breakpoints, which is not a bad thing. So any ideas how we could implement it? So the first idea that came to my mind was like, we have this code. This is actually the code that was part of the code. So the idea was we just place in front of every line a debug statement, like a debug method that we define somewhere. The idea is simply put in the debug method, we check are we currently at a breakpoint in this file online, and if yes, we open some kind of debank shell. If you've ever used PDB before, that's essentially what, so it's the PDB shoulder, we could be opening. But the question is how did we get this file online? And the easy answer is we have this get frame method. It has an underscore, and the important thing is it has an underscore because it's kind of in C-Python implementation detail, which is great. Because it's pretty slow in PyPy. But we have to live with it because that's the only way we can walk the stack. We've seen before that we can do some EBS stuff, which is nice, but usually most profiles, not most debuggers, don't do it. So the idea is here, we have O stack, and the bottom is like the main, and then the count count line is code line, and then our debug method, and essentially what we do, we can ask get frame, the zero frame, that's the top frame, because we currently in the debugging method, and so we ask it for the frame one. And also get it from the other frames, and essentially what we can get is get information on like which are the local variables, which is the file name, which is the line number, and such, and that's quite nice because this allows us to easily implement the debugging shell because we can just open the shell that contains these locals. And so that's how we implement our first debugging method. And it's nice, and it works, and we can even write some basic debuggers with it. The problem is we want to automate this because we don't want to put this DBG statement in front of everything. So how do we do this? And first I'm going to tell you about the pre-3.12 way so you know the pain points of debugger developers here. The pre-3.12 way was the way of this dot set trace, which was an arcane way to do it, but the idea is essentially we pass it a handler and this handler is called multiple times. Essentially this handler gets parsed the frame type, the frame, and an event type, which could be like call line return or exceptional opcode, and it would be called regular time. So when we then register it, we register a specific handler and this handler then is called at every call. So every time the method called call lines is called, it's called, and every time then also a method is called line, this use here is called. And that's nice, but we want no more. We want to know also we want to get a handler on every line. So what we do, we can return from this handler and inner handler that's called for every line and this has the same signature. So the idea is we essentially implement our debugger here by not using like our writing manual here with the DBG but just setting an inner handler. This is called at every line. And that's quite nice because it works, but you might expect to show later it's quite slow. But it's okay. We can even go to the opcode level, to the bytecode level in here. But the problem is, and the question here is do we need a line event for every function? Because we know when we set a breakpoint somewhere, we only need to like, set a breakpoint, set, we only need to like check the lines there. But for example, consider that we have here this con-con-lines and our user decides that he adds a breakpoint when we're in isCodeLine. So it's a breakpoint also and isCodeLine and in isCodeLine decides, hey, I want to add a breakpoint into code con-lines. The problem is when we haven't passed like inner handler to the method here for con-con-lines, we can't enable line tracing for con-con-lines. So we have to enable it for every, every line of our whole application, which is kind of a mess. So this is slow. So there are multiple ideas how to improve this. And one of the best ideas, and if you're a Python developer, you should do this. If you're a Python core developer, is to add a new API. And this API is called Python, and this API is defined in the PEP669 and it's really, really cool. And the cool thing is also this PEP is written in a style that you can easily digest. I come from the Java world. This is not always the case with the Java PEP. So I'm with the JEP, so I'm quite happy that Python does things a little bit better. So the JEP is called Low Impact Monitoring for C Python and hopefully other runtimes will support it in the future. And it's here since October. So the idea here is that we have more fine-train support, that we learn from the lessons that like having to enable the line handlers for every line is probably not the best thing. So typically when we develop, when we use this PEP, we define some shortcuts here in the top. So we don't run to write, is this not monitoring all the time because that's where the monitoring functions live. So we call it mon and events, it's also bit long, so we call it mon not events. Then we have to assign it the tool ideas, tool ideas. So the idea is that you can have multiple tools that are registered here. And for each tool we register some callbacks. So what we do here in our example, we register a callback for our tool. Our tool is a debugger, they're like six possible tool ideas, a tool idea is one of them is a debugger, another one is a profile. And we register here callback for the line because we want to still have like line callbacks sometimes. And we also register a callback for the start function. So for the start event, when a method is called, then we have these handlers and the start handler is just passed like the code object. That's what you get when you form a method for function called f underscore code. And the offset word is located in a byte code and for line handler, we get the line word in. And the cool thing is here we can return from this, as you see in the bottom, the line handler also can return either disable or any. And the cool thing is when we disable, it's disabled all the time for this specific line and it's called for coverage. So we can also make coverage testing easier. So yes, we enable them the start events and that's fine. And then we run our program, we get the start event for every function that we call and then at every time we ask, hey, do we have a break point as an function? If yes, we enable the line here. But only specifically for this function. And then for every line we check it. The cool thing is that these line events are emitted per fret. So the ideas that were set, sister, etc., it was emitted like in the main fret per interpreter. But here it's emitted for every line in the fret that the function is currently executing. And this is really cool. And Lukas Lange wrote in a PR discussion, the biggest opportunity of PEP 6.9 isn't even the speed. It's the fact that the debugger built on top of it will automatically support all threats and supports threats properly. And with the incoming changes with PEP 703, with making the global interpreter lock option in Cpython, it will get far more important. Because with then we will probably see multi-threading Python applications and then the old approach is just not usable anymore. So the idea is that we can enable events globally and locally. And the sum of both is like they're enabled events per function. And the cool thing here is the power is in the fine-train configuration. So you can set events in a function f for the function f while this function is running. So consider this example here. We have a simple line handler here and you register a callback for each line. And then in f you decide at some point, hey, I want to set the local events. I want enable line events. And later you disable them. And it works. So here we emit hello and then we emit like, hey, we're at line 18. We're just like the line that prints n, then we print n. And that's really cool. That wasn't possible before. That's really great because this enables us to only enable line events for the functions that we will need them. And so the question is, of course, what's far as there are several methods in this PEP and this API. And what's really fast is to register callbacks. We can easily switch out the callbacks and get the tools so we can say, oh, please get the tool ID. What's kind of fast is that setting local events because where it does it modifies the bytecode to the VMS executing. And what's pretty slow is to use the tool ID to start the debugger here and to set the global events because this potentially recompars or modifies all bytecode of every function. So do it all the way. So then it's fine. So back to the debugger. We had here our start handler and our line handler. And they look essentially the same as before. The only difference is that we enable the line breakpoints when we're needed. So they're different than events kinds because we've seen that line events are pretty powerful for implementing basic debuggers. One of them we've seen already the start events. There's the regime return, yield events for everything that you do in your Python application. And there are also then in-signary events. These are events that aren't like an A that you can't enable or disable because they come from, they are controlled by other events. So for example, you have to call event that is triggered whenever you call a function and then we have C relighted, a C raise whenever exception is flown in C code or whenever fact a C function returns. And there are of course then also other events that are not enabled locally but only globally. Essentially the idea is we cannot locate them properly. And the cool thing here is maybe you've seen that we have a new event called stop iteration because we think it was in this Python version we're not using in iterators. When we were returning from iterator previously we wrote an exception but that's pretty slow. So we don't throw an exception anymore but to debug this to still notice it we have a new stop iteration event. Of course what you'll be waiting for is performance because the performance is like the thing that besides the threading support is pretty neat. So what I did I looked around and I found some people also doing some performances but they were using Fibonacci functions. Now I'm like that's a bit small, that's not representative. So I started looking into Python benchmarking suites and there's this pi performance benchmarking suite and I just hacked it. I just wrote random code in it because you can do some kind of monkey patching in Python and it's great. In Java we have like private functions and everything in Python you don't have to care and that's why you like using Python you can do things that you're not supposed to do to get some change results. So if you want I'm using Python all the time when fixing bugs in the OpenJK to write test suite because it's faster in Python than to do in Java so you get the OpenJK, some bugs were fixed because I wrote some weird Python script here. But essentially what I was then going to test is I wanted to check the minimal implementation of a debugger. So minimal implementation with such trace, so debugger that doesn't have any breakpoints here is just call handler and then an inner handler calls it every line. Then the minimal implementation for monitoring API wouldn't enable any line events because when we don't set a breakpoint we don't need to set any line events. So that's how we implement this but I thought like it's a bit sneaky because we're comparing something that triggers an event that is relying with something that only triggers an event every function calls are also made a third comparison with like setting all the line events and it turns out that's still faster which is quite nice. So I use this Py for matchpoint suite that's quite representable and what I found is that it's really, really fast. So the CessetRace when you run it, when you run all the five benchmarks with CessetRace on you have a 3.5 times larger runtime. That's pretty slow. When you're using monitoring you only have a runtime increase of like a factor of 1.2 which is like 20% slower which is pretty, pretty awesome because this means you can debug all your tight loops, you can debug your whole applications without worrying about like debating slowing down and when you enable all line events it's still 30% faster than with CessetRace and people here probably like RAS. That's essentially all the benchmarks that are in Py performance and what you see here are the orange bars. These are the bars for the monitoring solutions and here it's just at one so if the bar is not visible it means you don't have any overhead in this benchmark but essentially you see that the blue bars for CessetRace can get high, can get up to like 10, 12 so it's really good. At least in my opinion and then when we switch over and use CessetRace monitoring with all line events enabled it gets worse but still we see that it's still significantly faster. Another question is of course is this whole thing used because it's implemented and I'm working in OmTek so I noted when you implement a cool feature the chances are that nobody will use it for like a year but here in OmTek people started, but here in CPython people started using it especially the vendors like PyCharm. So it's not yet used in PVP and they're showing the second pie but IDEs like PyCharm with their version 2093.3 use it and they've seen significant performance improvements. And there's currently a pending pull request on GitHub so if you want to help PVP implement it go to this pull request and try another discussion here so I would really recommend it's an open source project to CPython and you can make PVP better so what not to like is simple. Here's a quote from the pull request from Chen Gao who wrote this pull request and he wrote after this change we will have the chance to build a much faster debugger for break points we don't need to trigger trace functions all the time and change for a line number so I'll show it to you. The bad news is it's almost impossible to do a completely backward compatible transition because the mechanism is quite different. So there's an ongoing discussion how to do this. You could take part there so scan this QR code, be part of the community, give something back and not just use CPython. So because I have like tiny tiny town left I want to just show you shortly how single stepping works because single stepping is just break points because essentially the idea is here when we have always take here with the Scona and step out of this for example we just check for the next line where only the frame before changed like the current code lines changed. Stepping is also pretty simple we just check that only like the line number changed it's also nice and then check in to is we check the next time where we just put the frame on top. So that's all from me I'm part of Northern Twitter you can find my team at sub machine A O so if you want to use a JVM use the sub machine it's the best JVM. I'm contractually obliged to say this. We work at SAP we're one of the many cool open source projects at SAP you can follow me on my blog where I write on DBegging, EBPS stuff and everything else. So thank you for being here. Thank you very much Johans we have time for probably two three questions maybe. Does anyone we have one there? Thank you very much for this talk and for this pep because it actually solves a lot of problems I had when I started back in the days developing a tool for performance analysis for Python. However at some point choose to use the C interface of set rays and profiling and whatever do your does your proposal as I said is implemented already also support the C interface? So I have to correct I'm not I have nothing to do with the nice people who implemented this sorry but so please ask them they're probably in some discord somewhere I'm just telling you about the good news because programmers usually don't want to go to conferences and speak in front of people so that's why I'll be giving talks on this. So sadly I don't know. Thank you do we have any more questions to Johans? Can raise your hand. No questions apparently. I just want to choose this opportunity to thank Mark Andre and David also known as Flypeg for organizing this dev room. You guys did an amazing work. Thank you very very much. And thanks Johans again.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.4, "text": " It's time.", "tokens": [50364, 467, 311, 565, 13, 50734], "temperature": 0.0, "avg_logprob": -0.27203460693359377, "compression_ratio": 1.385786802030457, "no_speech_prob": 0.44530078768730164}, {"id": 1, "seek": 0, "start": 7.4, "end": 13.120000000000001, "text": " So thank you very much to Johannes Bergberg.", "tokens": [50734, 407, 1309, 291, 588, 709, 281, 48455, 27511, 6873, 13, 51020], "temperature": 0.0, "avg_logprob": -0.27203460693359377, "compression_ratio": 1.385786802030457, "no_speech_prob": 0.44530078768730164}, {"id": 2, "seek": 0, "start": 13.120000000000001, "end": 19.28, "text": " He will be speaking about Python 3.12 new monitoring and debugging API.", "tokens": [51020, 634, 486, 312, 4124, 466, 15329, 805, 13, 4762, 777, 11028, 293, 45592, 9362, 13, 51328], "temperature": 0.0, "avg_logprob": -0.27203460693359377, "compression_ratio": 1.385786802030457, "no_speech_prob": 0.44530078768730164}, {"id": 3, "seek": 0, "start": 19.28, "end": 24.76, "text": " For those who were in the previous talk, there was a brief about the profiling features.", "tokens": [51328, 1171, 729, 567, 645, 294, 264, 3894, 751, 11, 456, 390, 257, 5353, 466, 264, 1740, 4883, 4122, 13, 51602], "temperature": 0.0, "avg_logprob": -0.27203460693359377, "compression_ratio": 1.385786802030457, "no_speech_prob": 0.44530078768730164}, {"id": 4, "seek": 0, "start": 24.76, "end": 29.28, "text": " Johannes is a JVM developer working on profilers at SAP.", "tokens": [51602, 48455, 307, 257, 508, 53, 44, 10754, 1364, 322, 1740, 388, 433, 412, 27743, 13, 51828], "temperature": 0.0, "avg_logprob": -0.27203460693359377, "compression_ratio": 1.385786802030457, "no_speech_prob": 0.44530078768730164}, {"id": 5, "seek": 2928, "start": 29.28, "end": 32.68, "text": " He also writes blogs about profiling and debugging topics.", "tokens": [50364, 634, 611, 13657, 31038, 466, 1740, 4883, 293, 45592, 8378, 13, 50534], "temperature": 0.0, "avg_logprob": -0.37863163615381995, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.013614394702017307}, {"id": 6, "seek": 2928, "start": 32.68, "end": 40.480000000000004, "text": " Thank you very much.", "tokens": [50534, 1044, 291, 588, 709, 13, 50924], "temperature": 0.0, "avg_logprob": -0.37863163615381995, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.013614394702017307}, {"id": 7, "seek": 2928, "start": 40.480000000000004, "end": 43.120000000000005, "text": " Thank you for introducing me.", "tokens": [50924, 1044, 291, 337, 15424, 385, 13, 51056], "temperature": 0.0, "avg_logprob": -0.37863163615381995, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.013614394702017307}, {"id": 8, "seek": 2928, "start": 43.120000000000005, "end": 47.400000000000006, "text": " Before we start, I want to introduce you to the concept of debugging, because I'm sure", "tokens": [51056, 4546, 321, 722, 11, 286, 528, 281, 5366, 291, 281, 264, 3410, 295, 45592, 11, 570, 286, 478, 988, 51270], "temperature": 0.0, "avg_logprob": -0.37863163615381995, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.013614394702017307}, {"id": 9, "seek": 2928, "start": 47.400000000000006, "end": 50.0, "text": " nobody of you have ever debunked.", "tokens": [51270, 5079, 295, 291, 362, 1562, 3001, 3197, 292, 13, 51400], "temperature": 0.0, "avg_logprob": -0.37863163615381995, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.013614394702017307}, {"id": 10, "seek": 2928, "start": 50.0, "end": 55.8, "text": " So the first bug that's what's like found was in the 50s when they found a moth that", "tokens": [51400, 407, 264, 700, 7426, 300, 311, 437, 311, 411, 1352, 390, 294, 264, 2625, 82, 562, 436, 1352, 257, 275, 900, 300, 51690], "temperature": 0.0, "avg_logprob": -0.37863163615381995, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.013614394702017307}, {"id": 11, "seek": 5580, "start": 55.8, "end": 61.0, "text": " was in between their relays and it makes zip and the whole system like crash because", "tokens": [50364, 390, 294, 1296, 641, 1039, 3772, 293, 309, 1669, 710, 647, 293, 264, 1379, 1185, 411, 8252, 570, 50624], "temperature": 0.4, "avg_logprob": -0.45404815673828125, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.1186554878950119}, {"id": 12, "seek": 5580, "start": 61.0, "end": 64.2, "text": " like in the olden days it relays.", "tokens": [50624, 411, 294, 264, 1331, 268, 1708, 309, 1039, 3772, 13, 50784], "temperature": 0.4, "avg_logprob": -0.45404815673828125, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.1186554878950119}, {"id": 13, "seek": 5580, "start": 64.2, "end": 69.8, "text": " At SCADAISCAR once wrote, if debugging is the process of removing Zafferbacks and pro", "tokens": [50784, 1711, 9028, 45852, 2343, 34, 1899, 1564, 4114, 11, 498, 45592, 307, 264, 1399, 295, 12720, 1176, 2518, 260, 17758, 293, 447, 51064], "temperature": 0.4, "avg_logprob": -0.45404815673828125, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.1186554878950119}, {"id": 14, "seek": 5580, "start": 69.8, "end": 72.25999999999999, "text": " pro-peering must be the process of putting them in.", "tokens": [51064, 447, 12, 494, 1794, 1633, 312, 264, 1399, 295, 3372, 552, 294, 13, 51187], "temperature": 0.4, "avg_logprob": -0.45404815673828125, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.1186554878950119}, {"id": 15, "seek": 5580, "start": 72.25999999999999, "end": 79.24, "text": " As I'm sure all of you are doing lots of programming, I'm sure you're also doing lots of debugging.", "tokens": [51187, 1018, 286, 478, 988, 439, 295, 291, 366, 884, 3195, 295, 9410, 11, 286, 478, 988, 291, 434, 611, 884, 3195, 295, 45592, 13, 51536], "temperature": 0.4, "avg_logprob": -0.45404815673828125, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.1186554878950119}, {"id": 16, "seek": 5580, "start": 79.24, "end": 80.92, "text": " So that's what we're here.", "tokens": [51536, 407, 300, 311, 437, 321, 434, 510, 13, 51620], "temperature": 0.4, "avg_logprob": -0.45404815673828125, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.1186554878950119}, {"id": 17, "seek": 5580, "start": 80.92, "end": 82.75999999999999, "text": " So consider this example program.", "tokens": [51620, 407, 1949, 341, 1365, 1461, 13, 51712], "temperature": 0.4, "avg_logprob": -0.45404815673828125, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.1186554878950119}, {"id": 18, "seek": 8276, "start": 82.76, "end": 83.76, "text": " It's called a counterprong.", "tokens": [50364, 467, 311, 1219, 257, 5682, 1424, 556, 13, 50414], "temperature": 0.0, "avg_logprob": -0.37663298138117385, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.009793979115784168}, {"id": 19, "seek": 8276, "start": 83.76, "end": 90.4, "text": " It just counts the lines in this example in itself and it returns zero.", "tokens": [50414, 467, 445, 14893, 264, 3876, 294, 341, 1365, 294, 2564, 293, 309, 11247, 4018, 13, 50746], "temperature": 0.0, "avg_logprob": -0.37663298138117385, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.009793979115784168}, {"id": 20, "seek": 8276, "start": 90.4, "end": 92.32000000000001, "text": " And we're like, why?", "tokens": [50746, 400, 321, 434, 411, 11, 983, 30, 50842], "temperature": 0.0, "avg_logprob": -0.37663298138117385, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.009793979115784168}, {"id": 21, "seek": 8276, "start": 92.32000000000001, "end": 96.48, "text": " So that's the problem because the file actually has 26 lines.", "tokens": [50842, 407, 300, 311, 264, 1154, 570, 264, 3991, 767, 575, 7551, 3876, 13, 51050], "temperature": 0.0, "avg_logprob": -0.37663298138117385, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.009793979115784168}, {"id": 22, "seek": 8276, "start": 96.48, "end": 97.60000000000001, "text": " So let's look at the code.", "tokens": [51050, 407, 718, 311, 574, 412, 264, 3089, 13, 51106], "temperature": 0.0, "avg_logprob": -0.37663298138117385, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.009793979115784168}, {"id": 23, "seek": 8276, "start": 97.60000000000001, "end": 101.2, "text": " I'm going to see clans of the code.", "tokens": [51106, 286, 478, 516, 281, 536, 596, 599, 295, 264, 3089, 13, 51286], "temperature": 0.0, "avg_logprob": -0.37663298138117385, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.009793979115784168}, {"id": 24, "seek": 8276, "start": 101.2, "end": 103.76, "text": " Make this shortly so you don't see what it's about.", "tokens": [51286, 4387, 341, 13392, 370, 291, 500, 380, 536, 437, 309, 311, 466, 13, 51414], "temperature": 0.0, "avg_logprob": -0.37663298138117385, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.009793979115784168}, {"id": 25, "seek": 8276, "start": 103.76, "end": 108.80000000000001, "text": " But the idea is we use the debugger through this because the debugger is a great to understand", "tokens": [51414, 583, 264, 1558, 307, 321, 764, 264, 24083, 1321, 807, 341, 570, 264, 24083, 1321, 307, 257, 869, 281, 1223, 51666], "temperature": 0.0, "avg_logprob": -0.37663298138117385, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.009793979115784168}, {"id": 26, "seek": 8276, "start": 108.80000000000001, "end": 109.80000000000001, "text": " our system.", "tokens": [51666, 527, 1185, 13, 51716], "temperature": 0.0, "avg_logprob": -0.37663298138117385, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.009793979115784168}, {"id": 27, "seek": 10980, "start": 109.8, "end": 116.8, "text": " And the cool thing is now with the new APIs that we get in Python 3.12, writing the debugger", "tokens": [50364, 400, 264, 1627, 551, 307, 586, 365, 264, 777, 21445, 300, 321, 483, 294, 15329, 805, 13, 4762, 11, 3579, 264, 24083, 1321, 50714], "temperature": 0.0, "avg_logprob": -0.3164786918490541, "compression_ratio": 1.5418326693227091, "no_speech_prob": 0.0018879036651924253}, {"id": 28, "seek": 10980, "start": 116.8, "end": 120.24, "text": " is far easier and far faster as I show you in the following.", "tokens": [50714, 307, 1400, 3571, 293, 1400, 4663, 382, 286, 855, 291, 294, 264, 3480, 13, 50886], "temperature": 0.0, "avg_logprob": -0.3164786918490541, "compression_ratio": 1.5418326693227091, "no_speech_prob": 0.0018879036651924253}, {"id": 29, "seek": 10980, "start": 120.24, "end": 122.44, "text": " I'm Johannes Pechberger as you already heard.", "tokens": [50886, 286, 478, 48455, 2396, 339, 42226, 382, 291, 1217, 2198, 13, 50996], "temperature": 0.0, "avg_logprob": -0.3164786918490541, "compression_ratio": 1.5418326693227091, "no_speech_prob": 0.0018879036651924253}, {"id": 30, "seek": 10980, "start": 122.44, "end": 128.4, "text": " I work at SAP machine, which is the third biggest contributor to the OpenJK, which is", "tokens": [50996, 286, 589, 412, 27743, 3479, 11, 597, 307, 264, 2636, 3880, 42859, 281, 264, 7238, 41, 42, 11, 597, 307, 51294], "temperature": 0.0, "avg_logprob": -0.3164786918490541, "compression_ratio": 1.5418326693227091, "no_speech_prob": 0.0018879036651924253}, {"id": 31, "seek": 10980, "start": 128.4, "end": 131.32, "text": " like the major Java runtime.", "tokens": [51294, 411, 264, 2563, 10745, 34474, 13, 51440], "temperature": 0.0, "avg_logprob": -0.3164786918490541, "compression_ratio": 1.5418326693227091, "no_speech_prob": 0.0018879036651924253}, {"id": 32, "seek": 10980, "start": 131.32, "end": 135.76, "text": " And I started talking to people about Python because I also like Python.", "tokens": [51440, 400, 286, 1409, 1417, 281, 561, 466, 15329, 570, 286, 611, 411, 15329, 13, 51662], "temperature": 0.0, "avg_logprob": -0.3164786918490541, "compression_ratio": 1.5418326693227091, "no_speech_prob": 0.0018879036651924253}, {"id": 33, "seek": 13576, "start": 135.76, "end": 140.95999999999998, "text": " So it's a bit easier to VM than JVM.", "tokens": [50364, 407, 309, 311, 257, 857, 3571, 281, 18038, 813, 508, 53, 44, 13, 50624], "temperature": 0.0, "avg_logprob": -0.2251677938026957, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.02268480323255062}, {"id": 34, "seek": 13576, "start": 140.95999999999998, "end": 144.95999999999998, "text": " The question is now, why do we need a monitoring and debugging API?", "tokens": [50624, 440, 1168, 307, 586, 11, 983, 360, 321, 643, 257, 11028, 293, 45592, 9362, 30, 50824], "temperature": 0.0, "avg_logprob": -0.2251677938026957, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.02268480323255062}, {"id": 35, "seek": 13576, "start": 144.95999999999998, "end": 150.76, "text": " Because when I'm from the Java world and in Java, we haven't built in debugging API.", "tokens": [50824, 1436, 562, 286, 478, 490, 264, 10745, 1002, 293, 294, 10745, 11, 321, 2378, 380, 3094, 294, 45592, 9362, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2251677938026957, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.02268480323255062}, {"id": 36, "seek": 13576, "start": 150.76, "end": 156.6, "text": " So we have the ability to set breakpoints, to ask for values, to walk the stack and have", "tokens": [51114, 407, 321, 362, 264, 3485, 281, 992, 1821, 20552, 11, 281, 1029, 337, 4190, 11, 281, 1792, 264, 8630, 293, 362, 51406], "temperature": 0.0, "avg_logprob": -0.2251677938026957, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.02268480323255062}, {"id": 37, "seek": 13576, "start": 156.6, "end": 157.6, "text": " everything.", "tokens": [51406, 1203, 13, 51456], "temperature": 0.0, "avg_logprob": -0.2251677938026957, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.02268480323255062}, {"id": 38, "seek": 13576, "start": 157.6, "end": 164.79999999999998, "text": " But in Python, does the Python interpreter know about the concept of breakpoints?", "tokens": [51456, 583, 294, 15329, 11, 775, 264, 15329, 34132, 458, 466, 264, 3410, 295, 1821, 20552, 30, 51816], "temperature": 0.0, "avg_logprob": -0.2251677938026957, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.02268480323255062}, {"id": 39, "seek": 16480, "start": 164.8, "end": 171.04000000000002, "text": " So because I'm here, not only, but with a few here, who of you thinks that the Python", "tokens": [50364, 407, 570, 286, 478, 510, 11, 406, 787, 11, 457, 365, 257, 1326, 510, 11, 567, 295, 291, 7309, 300, 264, 15329, 50676], "temperature": 0.0, "avg_logprob": -0.27739413149721986, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.03487131744623184}, {"id": 40, "seek": 16480, "start": 171.04000000000002, "end": 173.96, "text": " interpreter knows about the concept of breakpoints?", "tokens": [50676, 34132, 3255, 466, 264, 3410, 295, 1821, 20552, 30, 50822], "temperature": 0.0, "avg_logprob": -0.27739413149721986, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.03487131744623184}, {"id": 41, "seek": 16480, "start": 173.96, "end": 177.16000000000003, "text": " Please raise your hand.", "tokens": [50822, 2555, 5300, 428, 1011, 13, 50982], "temperature": 0.0, "avg_logprob": -0.27739413149721986, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.03487131744623184}, {"id": 42, "seek": 16480, "start": 177.16000000000003, "end": 178.68, "text": " And two of your things.", "tokens": [50982, 400, 732, 295, 428, 721, 13, 51058], "temperature": 0.0, "avg_logprob": -0.27739413149721986, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.03487131744623184}, {"id": 43, "seek": 16480, "start": 178.68, "end": 182.0, "text": " It doesn't know about the concept of breakpoints.", "tokens": [51058, 467, 1177, 380, 458, 466, 264, 3410, 295, 1821, 20552, 13, 51224], "temperature": 0.0, "avg_logprob": -0.27739413149721986, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.03487131744623184}, {"id": 44, "seek": 16480, "start": 182.0, "end": 183.16000000000003, "text": " Okay.", "tokens": [51224, 1033, 13, 51282], "temperature": 0.0, "avg_logprob": -0.27739413149721986, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.03487131744623184}, {"id": 45, "seek": 16480, "start": 183.16000000000003, "end": 188.56, "text": " It's a trick question, of course, no, because otherwise I wouldn't be asking this question.", "tokens": [51282, 467, 311, 257, 4282, 1168, 11, 295, 1164, 11, 572, 11, 570, 5911, 286, 2759, 380, 312, 3365, 341, 1168, 13, 51552], "temperature": 0.0, "avg_logprob": -0.27739413149721986, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.03487131744623184}, {"id": 46, "seek": 16480, "start": 188.56, "end": 193.08, "text": " So it doesn't know anything about breakpoints, which is not a bad thing.", "tokens": [51552, 407, 309, 1177, 380, 458, 1340, 466, 1821, 20552, 11, 597, 307, 406, 257, 1578, 551, 13, 51778], "temperature": 0.0, "avg_logprob": -0.27739413149721986, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.03487131744623184}, {"id": 47, "seek": 19308, "start": 193.08, "end": 197.12, "text": " So any ideas how we could implement it?", "tokens": [50364, 407, 604, 3487, 577, 321, 727, 4445, 309, 30, 50566], "temperature": 0.0, "avg_logprob": -0.3205484350522359, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.04556559398770332}, {"id": 48, "seek": 19308, "start": 197.12, "end": 200.84, "text": " So the first idea that came to my mind was like, we have this code.", "tokens": [50566, 407, 264, 700, 1558, 300, 1361, 281, 452, 1575, 390, 411, 11, 321, 362, 341, 3089, 13, 50752], "temperature": 0.0, "avg_logprob": -0.3205484350522359, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.04556559398770332}, {"id": 49, "seek": 19308, "start": 200.84, "end": 206.56, "text": " This is actually the code that was part of the code.", "tokens": [50752, 639, 307, 767, 264, 3089, 300, 390, 644, 295, 264, 3089, 13, 51038], "temperature": 0.0, "avg_logprob": -0.3205484350522359, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.04556559398770332}, {"id": 50, "seek": 19308, "start": 206.56, "end": 211.8, "text": " So the idea was we just place in front of every line a debug statement, like a debug", "tokens": [51038, 407, 264, 1558, 390, 321, 445, 1081, 294, 1868, 295, 633, 1622, 257, 24083, 5629, 11, 411, 257, 24083, 51300], "temperature": 0.0, "avg_logprob": -0.3205484350522359, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.04556559398770332}, {"id": 51, "seek": 19308, "start": 211.8, "end": 215.24, "text": " method that we define somewhere.", "tokens": [51300, 3170, 300, 321, 6964, 4079, 13, 51472], "temperature": 0.0, "avg_logprob": -0.3205484350522359, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.04556559398770332}, {"id": 52, "seek": 19308, "start": 215.24, "end": 221.12, "text": " The idea is simply put in the debug method, we check are we currently at a breakpoint", "tokens": [51472, 440, 1558, 307, 2935, 829, 294, 264, 24083, 3170, 11, 321, 1520, 366, 321, 4362, 412, 257, 1821, 6053, 51766], "temperature": 0.0, "avg_logprob": -0.3205484350522359, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.04556559398770332}, {"id": 53, "seek": 22112, "start": 221.12, "end": 225.04, "text": " in this file online, and if yes, we open some kind of debank shell.", "tokens": [50364, 294, 341, 3991, 2950, 11, 293, 498, 2086, 11, 321, 1269, 512, 733, 295, 3001, 657, 8720, 13, 50560], "temperature": 0.0, "avg_logprob": -0.2979384202223558, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004720705095678568}, {"id": 54, "seek": 22112, "start": 225.04, "end": 233.72, "text": " If you've ever used PDB before, that's essentially what, so it's the PDB shoulder, we could be", "tokens": [50560, 759, 291, 600, 1562, 1143, 10464, 33, 949, 11, 300, 311, 4476, 437, 11, 370, 309, 311, 264, 10464, 33, 7948, 11, 321, 727, 312, 50994], "temperature": 0.0, "avg_logprob": -0.2979384202223558, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004720705095678568}, {"id": 55, "seek": 22112, "start": 233.72, "end": 234.72, "text": " opening.", "tokens": [50994, 5193, 13, 51044], "temperature": 0.0, "avg_logprob": -0.2979384202223558, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004720705095678568}, {"id": 56, "seek": 22112, "start": 234.72, "end": 237.36, "text": " But the question is how did we get this file online?", "tokens": [51044, 583, 264, 1168, 307, 577, 630, 321, 483, 341, 3991, 2950, 30, 51176], "temperature": 0.0, "avg_logprob": -0.2979384202223558, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004720705095678568}, {"id": 57, "seek": 22112, "start": 237.36, "end": 240.4, "text": " And the easy answer is we have this get frame method.", "tokens": [51176, 400, 264, 1858, 1867, 307, 321, 362, 341, 483, 3920, 3170, 13, 51328], "temperature": 0.0, "avg_logprob": -0.2979384202223558, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004720705095678568}, {"id": 58, "seek": 22112, "start": 240.4, "end": 245.76, "text": " It has an underscore, and the important thing is it has an underscore because it's kind", "tokens": [51328, 467, 575, 364, 37556, 11, 293, 264, 1021, 551, 307, 309, 575, 364, 37556, 570, 309, 311, 733, 51596], "temperature": 0.0, "avg_logprob": -0.2979384202223558, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004720705095678568}, {"id": 59, "seek": 22112, "start": 245.76, "end": 250.76, "text": " of in C-Python implementation detail, which is great.", "tokens": [51596, 295, 294, 383, 12, 47, 88, 11943, 11420, 2607, 11, 597, 307, 869, 13, 51846], "temperature": 0.0, "avg_logprob": -0.2979384202223558, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004720705095678568}, {"id": 60, "seek": 25076, "start": 250.76, "end": 254.48, "text": " Because it's pretty slow in PyPy.", "tokens": [50364, 1436, 309, 311, 1238, 2964, 294, 9953, 47, 88, 13, 50550], "temperature": 0.0, "avg_logprob": -0.31311220441545756, "compression_ratio": 1.7833935018050542, "no_speech_prob": 0.033089179545640945}, {"id": 61, "seek": 25076, "start": 254.48, "end": 259.96, "text": " But we have to live with it because that's the only way we can walk the stack.", "tokens": [50550, 583, 321, 362, 281, 1621, 365, 309, 570, 300, 311, 264, 787, 636, 321, 393, 1792, 264, 8630, 13, 50824], "temperature": 0.0, "avg_logprob": -0.31311220441545756, "compression_ratio": 1.7833935018050542, "no_speech_prob": 0.033089179545640945}, {"id": 62, "seek": 25076, "start": 259.96, "end": 265.68, "text": " We've seen before that we can do some EBS stuff, which is nice, but usually most profiles,", "tokens": [50824, 492, 600, 1612, 949, 300, 321, 393, 360, 512, 462, 8176, 1507, 11, 597, 307, 1481, 11, 457, 2673, 881, 23693, 11, 51110], "temperature": 0.0, "avg_logprob": -0.31311220441545756, "compression_ratio": 1.7833935018050542, "no_speech_prob": 0.033089179545640945}, {"id": 63, "seek": 25076, "start": 265.68, "end": 267.15999999999997, "text": " not most debuggers, don't do it.", "tokens": [51110, 406, 881, 3001, 3562, 433, 11, 500, 380, 360, 309, 13, 51184], "temperature": 0.0, "avg_logprob": -0.31311220441545756, "compression_ratio": 1.7833935018050542, "no_speech_prob": 0.033089179545640945}, {"id": 64, "seek": 25076, "start": 267.15999999999997, "end": 271.32, "text": " So the idea is here, we have O stack, and the bottom is like the main, and then the", "tokens": [51184, 407, 264, 1558, 307, 510, 11, 321, 362, 422, 8630, 11, 293, 264, 2767, 307, 411, 264, 2135, 11, 293, 550, 264, 51392], "temperature": 0.0, "avg_logprob": -0.31311220441545756, "compression_ratio": 1.7833935018050542, "no_speech_prob": 0.033089179545640945}, {"id": 65, "seek": 25076, "start": 271.32, "end": 276.03999999999996, "text": " count count line is code line, and then our debug method, and essentially what we do,", "tokens": [51392, 1207, 1207, 1622, 307, 3089, 1622, 11, 293, 550, 527, 24083, 3170, 11, 293, 4476, 437, 321, 360, 11, 51628], "temperature": 0.0, "avg_logprob": -0.31311220441545756, "compression_ratio": 1.7833935018050542, "no_speech_prob": 0.033089179545640945}, {"id": 66, "seek": 25076, "start": 276.03999999999996, "end": 280.2, "text": " we can ask get frame, the zero frame, that's the top frame, because we currently in the", "tokens": [51628, 321, 393, 1029, 483, 3920, 11, 264, 4018, 3920, 11, 300, 311, 264, 1192, 3920, 11, 570, 321, 4362, 294, 264, 51836], "temperature": 0.0, "avg_logprob": -0.31311220441545756, "compression_ratio": 1.7833935018050542, "no_speech_prob": 0.033089179545640945}, {"id": 67, "seek": 28020, "start": 280.2, "end": 284.15999999999997, "text": " debugging method, and so we ask it for the frame one.", "tokens": [50364, 3001, 697, 3249, 3170, 11, 293, 370, 321, 1029, 309, 337, 264, 3920, 472, 13, 50562], "temperature": 0.0, "avg_logprob": -0.24551894601467436, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009013358503580093}, {"id": 68, "seek": 28020, "start": 284.15999999999997, "end": 289.32, "text": " And also get it from the other frames, and essentially what we can get is get information", "tokens": [50562, 400, 611, 483, 309, 490, 264, 661, 12083, 11, 293, 4476, 437, 321, 393, 483, 307, 483, 1589, 50820], "temperature": 0.0, "avg_logprob": -0.24551894601467436, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009013358503580093}, {"id": 69, "seek": 28020, "start": 289.32, "end": 294.71999999999997, "text": " on like which are the local variables, which is the file name, which is the line number,", "tokens": [50820, 322, 411, 597, 366, 264, 2654, 9102, 11, 597, 307, 264, 3991, 1315, 11, 597, 307, 264, 1622, 1230, 11, 51090], "temperature": 0.0, "avg_logprob": -0.24551894601467436, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009013358503580093}, {"id": 70, "seek": 28020, "start": 294.71999999999997, "end": 298.59999999999997, "text": " and such, and that's quite nice because this allows us to easily implement the debugging", "tokens": [51090, 293, 1270, 11, 293, 300, 311, 1596, 1481, 570, 341, 4045, 505, 281, 3612, 4445, 264, 3001, 697, 3249, 51284], "temperature": 0.0, "avg_logprob": -0.24551894601467436, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009013358503580093}, {"id": 71, "seek": 28020, "start": 298.59999999999997, "end": 304.44, "text": " shell because we can just open the shell that contains these locals.", "tokens": [51284, 8720, 570, 321, 393, 445, 1269, 264, 8720, 300, 8306, 613, 23335, 13, 51576], "temperature": 0.0, "avg_logprob": -0.24551894601467436, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009013358503580093}, {"id": 72, "seek": 28020, "start": 304.44, "end": 308.64, "text": " And so that's how we implement our first debugging method.", "tokens": [51576, 400, 370, 300, 311, 577, 321, 4445, 527, 700, 3001, 697, 3249, 3170, 13, 51786], "temperature": 0.0, "avg_logprob": -0.24551894601467436, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009013358503580093}, {"id": 73, "seek": 30864, "start": 308.64, "end": 314.56, "text": " And it's nice, and it works, and we can even write some basic debuggers with it.", "tokens": [50364, 400, 309, 311, 1481, 11, 293, 309, 1985, 11, 293, 321, 393, 754, 2464, 512, 3875, 3001, 3562, 433, 365, 309, 13, 50660], "temperature": 0.0, "avg_logprob": -0.19327956751773231, "compression_ratio": 1.5233644859813085, "no_speech_prob": 0.005682419985532761}, {"id": 74, "seek": 30864, "start": 314.56, "end": 319.71999999999997, "text": " The problem is we want to automate this because we don't want to put this DBG statement in", "tokens": [50660, 440, 1154, 307, 321, 528, 281, 31605, 341, 570, 321, 500, 380, 528, 281, 829, 341, 26754, 38, 5629, 294, 50918], "temperature": 0.0, "avg_logprob": -0.19327956751773231, "compression_ratio": 1.5233644859813085, "no_speech_prob": 0.005682419985532761}, {"id": 75, "seek": 30864, "start": 319.71999999999997, "end": 321.15999999999997, "text": " front of everything.", "tokens": [50918, 1868, 295, 1203, 13, 50990], "temperature": 0.0, "avg_logprob": -0.19327956751773231, "compression_ratio": 1.5233644859813085, "no_speech_prob": 0.005682419985532761}, {"id": 76, "seek": 30864, "start": 321.15999999999997, "end": 324.08, "text": " So how do we do this?", "tokens": [50990, 407, 577, 360, 321, 360, 341, 30, 51136], "temperature": 0.0, "avg_logprob": -0.19327956751773231, "compression_ratio": 1.5233644859813085, "no_speech_prob": 0.005682419985532761}, {"id": 77, "seek": 30864, "start": 324.08, "end": 329.32, "text": " And first I'm going to tell you about the pre-3.12 way so you know the pain points of", "tokens": [51136, 400, 700, 286, 478, 516, 281, 980, 291, 466, 264, 659, 12, 18, 13, 4762, 636, 370, 291, 458, 264, 1822, 2793, 295, 51398], "temperature": 0.0, "avg_logprob": -0.19327956751773231, "compression_ratio": 1.5233644859813085, "no_speech_prob": 0.005682419985532761}, {"id": 78, "seek": 30864, "start": 329.32, "end": 331.08, "text": " debugger developers here.", "tokens": [51398, 24083, 1321, 8849, 510, 13, 51486], "temperature": 0.0, "avg_logprob": -0.19327956751773231, "compression_ratio": 1.5233644859813085, "no_speech_prob": 0.005682419985532761}, {"id": 79, "seek": 33108, "start": 331.08, "end": 339.0, "text": " The pre-3.12 way was the way of this dot set trace, which was an arcane way to do it,", "tokens": [50364, 440, 659, 12, 18, 13, 4762, 636, 390, 264, 636, 295, 341, 5893, 992, 13508, 11, 597, 390, 364, 10346, 1929, 636, 281, 360, 309, 11, 50760], "temperature": 0.0, "avg_logprob": -0.29493980822355853, "compression_ratio": 1.7, "no_speech_prob": 0.027481481432914734}, {"id": 80, "seek": 33108, "start": 339.0, "end": 345.76, "text": " but the idea is essentially we pass it a handler and this handler is called multiple times.", "tokens": [50760, 457, 264, 1558, 307, 4476, 321, 1320, 309, 257, 41967, 293, 341, 41967, 307, 1219, 3866, 1413, 13, 51098], "temperature": 0.0, "avg_logprob": -0.29493980822355853, "compression_ratio": 1.7, "no_speech_prob": 0.027481481432914734}, {"id": 81, "seek": 33108, "start": 345.76, "end": 352.32, "text": " Essentially this handler gets parsed the frame type, the frame, and an event type, which", "tokens": [51098, 23596, 341, 41967, 2170, 21156, 292, 264, 3920, 2010, 11, 264, 3920, 11, 293, 364, 2280, 2010, 11, 597, 51426], "temperature": 0.0, "avg_logprob": -0.29493980822355853, "compression_ratio": 1.7, "no_speech_prob": 0.027481481432914734}, {"id": 82, "seek": 33108, "start": 352.32, "end": 359.0, "text": " could be like call line return or exceptional opcode, and it would be called regular time.", "tokens": [51426, 727, 312, 411, 818, 1622, 2736, 420, 19279, 999, 22332, 11, 293, 309, 576, 312, 1219, 3890, 565, 13, 51760], "temperature": 0.0, "avg_logprob": -0.29493980822355853, "compression_ratio": 1.7, "no_speech_prob": 0.027481481432914734}, {"id": 83, "seek": 35900, "start": 359.0, "end": 367.88, "text": " So when we then register it, we register a specific handler and this handler then is", "tokens": [50364, 407, 562, 321, 550, 7280, 309, 11, 321, 7280, 257, 2685, 41967, 293, 341, 41967, 550, 307, 50808], "temperature": 0.0, "avg_logprob": -0.30753667303856386, "compression_ratio": 1.9385474860335195, "no_speech_prob": 0.0011256576981395483}, {"id": 84, "seek": 35900, "start": 367.88, "end": 369.6, "text": " called at every call.", "tokens": [50808, 1219, 412, 633, 818, 13, 50894], "temperature": 0.0, "avg_logprob": -0.30753667303856386, "compression_ratio": 1.9385474860335195, "no_speech_prob": 0.0011256576981395483}, {"id": 85, "seek": 35900, "start": 369.6, "end": 374.88, "text": " So every time the method called call lines is called, it's called, and every time then", "tokens": [50894, 407, 633, 565, 264, 3170, 1219, 818, 3876, 307, 1219, 11, 309, 311, 1219, 11, 293, 633, 565, 550, 51158], "temperature": 0.0, "avg_logprob": -0.30753667303856386, "compression_ratio": 1.9385474860335195, "no_speech_prob": 0.0011256576981395483}, {"id": 86, "seek": 35900, "start": 374.88, "end": 378.2, "text": " also a method is called line, this use here is called.", "tokens": [51158, 611, 257, 3170, 307, 1219, 1622, 11, 341, 764, 510, 307, 1219, 13, 51324], "temperature": 0.0, "avg_logprob": -0.30753667303856386, "compression_ratio": 1.9385474860335195, "no_speech_prob": 0.0011256576981395483}, {"id": 87, "seek": 35900, "start": 378.2, "end": 380.68, "text": " And that's nice, but we want no more.", "tokens": [51324, 400, 300, 311, 1481, 11, 457, 321, 528, 572, 544, 13, 51448], "temperature": 0.0, "avg_logprob": -0.30753667303856386, "compression_ratio": 1.9385474860335195, "no_speech_prob": 0.0011256576981395483}, {"id": 88, "seek": 35900, "start": 380.68, "end": 385.52, "text": " We want to know also we want to get a handler on every line.", "tokens": [51448, 492, 528, 281, 458, 611, 321, 528, 281, 483, 257, 41967, 322, 633, 1622, 13, 51690], "temperature": 0.0, "avg_logprob": -0.30753667303856386, "compression_ratio": 1.9385474860335195, "no_speech_prob": 0.0011256576981395483}, {"id": 89, "seek": 38552, "start": 385.52, "end": 390.84, "text": " So what we do, we can return from this handler and inner handler that's called for every line", "tokens": [50364, 407, 437, 321, 360, 11, 321, 393, 2736, 490, 341, 41967, 293, 7284, 41967, 300, 311, 1219, 337, 633, 1622, 50630], "temperature": 0.0, "avg_logprob": -0.2418100416045828, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.004209588281810284}, {"id": 90, "seek": 38552, "start": 390.84, "end": 395.91999999999996, "text": " and this has the same signature.", "tokens": [50630, 293, 341, 575, 264, 912, 13397, 13, 50884], "temperature": 0.0, "avg_logprob": -0.2418100416045828, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.004209588281810284}, {"id": 91, "seek": 38552, "start": 395.91999999999996, "end": 402.4, "text": " So the idea is we essentially implement our debugger here by not using like our writing", "tokens": [50884, 407, 264, 1558, 307, 321, 4476, 4445, 527, 24083, 1321, 510, 538, 406, 1228, 411, 527, 3579, 51208], "temperature": 0.0, "avg_logprob": -0.2418100416045828, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.004209588281810284}, {"id": 92, "seek": 38552, "start": 402.4, "end": 406.08, "text": " manual here with the DBG but just setting an inner handler.", "tokens": [51208, 9688, 510, 365, 264, 26754, 38, 457, 445, 3287, 364, 7284, 41967, 13, 51392], "temperature": 0.0, "avg_logprob": -0.2418100416045828, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.004209588281810284}, {"id": 93, "seek": 38552, "start": 406.08, "end": 408.52, "text": " This is called at every line.", "tokens": [51392, 639, 307, 1219, 412, 633, 1622, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2418100416045828, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.004209588281810284}, {"id": 94, "seek": 38552, "start": 408.52, "end": 413.91999999999996, "text": " And that's quite nice because it works, but you might expect to show later it's quite", "tokens": [51514, 400, 300, 311, 1596, 1481, 570, 309, 1985, 11, 457, 291, 1062, 2066, 281, 855, 1780, 309, 311, 1596, 51784], "temperature": 0.0, "avg_logprob": -0.2418100416045828, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.004209588281810284}, {"id": 95, "seek": 41392, "start": 413.92, "end": 414.92, "text": " slow.", "tokens": [50364, 2964, 13, 50414], "temperature": 0.0, "avg_logprob": -0.25976273321336313, "compression_ratio": 1.7043010752688172, "no_speech_prob": 0.004940908867865801}, {"id": 96, "seek": 41392, "start": 414.92, "end": 417.84000000000003, "text": " But it's okay.", "tokens": [50414, 583, 309, 311, 1392, 13, 50560], "temperature": 0.0, "avg_logprob": -0.25976273321336313, "compression_ratio": 1.7043010752688172, "no_speech_prob": 0.004940908867865801}, {"id": 97, "seek": 41392, "start": 417.84000000000003, "end": 423.08000000000004, "text": " We can even go to the opcode level, to the bytecode level in here.", "tokens": [50560, 492, 393, 754, 352, 281, 264, 999, 22332, 1496, 11, 281, 264, 40846, 22332, 1496, 294, 510, 13, 50822], "temperature": 0.0, "avg_logprob": -0.25976273321336313, "compression_ratio": 1.7043010752688172, "no_speech_prob": 0.004940908867865801}, {"id": 98, "seek": 41392, "start": 423.08000000000004, "end": 428.28000000000003, "text": " But the problem is, and the question here is do we need a line event for every function?", "tokens": [50822, 583, 264, 1154, 307, 11, 293, 264, 1168, 510, 307, 360, 321, 643, 257, 1622, 2280, 337, 633, 2445, 30, 51082], "temperature": 0.0, "avg_logprob": -0.25976273321336313, "compression_ratio": 1.7043010752688172, "no_speech_prob": 0.004940908867865801}, {"id": 99, "seek": 41392, "start": 428.28000000000003, "end": 435.88, "text": " Because we know when we set a breakpoint somewhere, we only need to like, set a breakpoint,", "tokens": [51082, 1436, 321, 458, 562, 321, 992, 257, 1821, 6053, 4079, 11, 321, 787, 643, 281, 411, 11, 992, 257, 1821, 6053, 11, 51462], "temperature": 0.0, "avg_logprob": -0.25976273321336313, "compression_ratio": 1.7043010752688172, "no_speech_prob": 0.004940908867865801}, {"id": 100, "seek": 41392, "start": 435.88, "end": 439.52000000000004, "text": " set, we only need to like check the lines there.", "tokens": [51462, 992, 11, 321, 787, 643, 281, 411, 1520, 264, 3876, 456, 13, 51644], "temperature": 0.0, "avg_logprob": -0.25976273321336313, "compression_ratio": 1.7043010752688172, "no_speech_prob": 0.004940908867865801}, {"id": 101, "seek": 43952, "start": 439.52, "end": 445.88, "text": " But for example, consider that we have here this con-con-lines and our user decides that", "tokens": [50364, 583, 337, 1365, 11, 1949, 300, 321, 362, 510, 341, 416, 12, 1671, 12, 11045, 293, 527, 4195, 14898, 300, 50682], "temperature": 0.0, "avg_logprob": -0.29360933123894456, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.005437684711068869}, {"id": 102, "seek": 43952, "start": 445.88, "end": 449.91999999999996, "text": " he adds a breakpoint when we're in isCodeLine.", "tokens": [50682, 415, 10860, 257, 1821, 6053, 562, 321, 434, 294, 307, 34, 1429, 43, 533, 13, 50884], "temperature": 0.0, "avg_logprob": -0.29360933123894456, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.005437684711068869}, {"id": 103, "seek": 43952, "start": 449.91999999999996, "end": 455.91999999999996, "text": " So it's a breakpoint also and isCodeLine and in isCodeLine decides, hey, I want to add", "tokens": [50884, 407, 309, 311, 257, 1821, 6053, 611, 293, 307, 34, 1429, 43, 533, 293, 294, 307, 34, 1429, 43, 533, 14898, 11, 4177, 11, 286, 528, 281, 909, 51184], "temperature": 0.0, "avg_logprob": -0.29360933123894456, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.005437684711068869}, {"id": 104, "seek": 43952, "start": 455.91999999999996, "end": 458.91999999999996, "text": " a breakpoint into code con-lines.", "tokens": [51184, 257, 1821, 6053, 666, 3089, 416, 12, 11045, 13, 51334], "temperature": 0.0, "avg_logprob": -0.29360933123894456, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.005437684711068869}, {"id": 105, "seek": 43952, "start": 458.91999999999996, "end": 467.0, "text": " The problem is when we haven't passed like inner handler to the method here for con-con-lines,", "tokens": [51334, 440, 1154, 307, 562, 321, 2378, 380, 4678, 411, 7284, 41967, 281, 264, 3170, 510, 337, 416, 12, 1671, 12, 11045, 11, 51738], "temperature": 0.0, "avg_logprob": -0.29360933123894456, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.005437684711068869}, {"id": 106, "seek": 46700, "start": 467.0, "end": 470.28, "text": " we can't enable line tracing for con-con-lines.", "tokens": [50364, 321, 393, 380, 9528, 1622, 25262, 337, 416, 12, 1671, 12, 11045, 13, 50528], "temperature": 0.0, "avg_logprob": -0.2151309486449234, "compression_ratio": 1.732, "no_speech_prob": 0.041732680052518845}, {"id": 107, "seek": 46700, "start": 470.28, "end": 476.56, "text": " So we have to enable it for every, every line of our whole application, which is kind of", "tokens": [50528, 407, 321, 362, 281, 9528, 309, 337, 633, 11, 633, 1622, 295, 527, 1379, 3861, 11, 597, 307, 733, 295, 50842], "temperature": 0.0, "avg_logprob": -0.2151309486449234, "compression_ratio": 1.732, "no_speech_prob": 0.041732680052518845}, {"id": 108, "seek": 46700, "start": 476.56, "end": 477.56, "text": " a mess.", "tokens": [50842, 257, 2082, 13, 50892], "temperature": 0.0, "avg_logprob": -0.2151309486449234, "compression_ratio": 1.732, "no_speech_prob": 0.041732680052518845}, {"id": 109, "seek": 46700, "start": 477.56, "end": 478.56, "text": " So this is slow.", "tokens": [50892, 407, 341, 307, 2964, 13, 50942], "temperature": 0.0, "avg_logprob": -0.2151309486449234, "compression_ratio": 1.732, "no_speech_prob": 0.041732680052518845}, {"id": 110, "seek": 46700, "start": 478.56, "end": 482.2, "text": " So there are multiple ideas how to improve this.", "tokens": [50942, 407, 456, 366, 3866, 3487, 577, 281, 3470, 341, 13, 51124], "temperature": 0.0, "avg_logprob": -0.2151309486449234, "compression_ratio": 1.732, "no_speech_prob": 0.041732680052518845}, {"id": 111, "seek": 46700, "start": 482.2, "end": 486.36, "text": " And one of the best ideas, and if you're a Python developer, you should do this.", "tokens": [51124, 400, 472, 295, 264, 1151, 3487, 11, 293, 498, 291, 434, 257, 15329, 10754, 11, 291, 820, 360, 341, 13, 51332], "temperature": 0.0, "avg_logprob": -0.2151309486449234, "compression_ratio": 1.732, "no_speech_prob": 0.041732680052518845}, {"id": 112, "seek": 46700, "start": 486.36, "end": 489.32, "text": " If you're a Python core developer, is to add a new API.", "tokens": [51332, 759, 291, 434, 257, 15329, 4965, 10754, 11, 307, 281, 909, 257, 777, 9362, 13, 51480], "temperature": 0.0, "avg_logprob": -0.2151309486449234, "compression_ratio": 1.732, "no_speech_prob": 0.041732680052518845}, {"id": 113, "seek": 46700, "start": 489.32, "end": 496.12, "text": " And this API is called Python, and this API is defined in the PEP669 and it's really,", "tokens": [51480, 400, 341, 9362, 307, 1219, 15329, 11, 293, 341, 9362, 307, 7642, 294, 264, 430, 8929, 15237, 24, 293, 309, 311, 534, 11, 51820], "temperature": 0.0, "avg_logprob": -0.2151309486449234, "compression_ratio": 1.732, "no_speech_prob": 0.041732680052518845}, {"id": 114, "seek": 49612, "start": 496.16, "end": 497.16, "text": " really cool.", "tokens": [50366, 534, 1627, 13, 50416], "temperature": 0.0, "avg_logprob": -0.2094282351042095, "compression_ratio": 1.57421875, "no_speech_prob": 0.006650566589087248}, {"id": 115, "seek": 49612, "start": 497.16, "end": 501.96, "text": " And the cool thing is also this PEP is written in a style that you can easily digest.", "tokens": [50416, 400, 264, 1627, 551, 307, 611, 341, 430, 8929, 307, 3720, 294, 257, 3758, 300, 291, 393, 3612, 13884, 13, 50656], "temperature": 0.0, "avg_logprob": -0.2094282351042095, "compression_ratio": 1.57421875, "no_speech_prob": 0.006650566589087248}, {"id": 116, "seek": 49612, "start": 501.96, "end": 503.44, "text": " I come from the Java world.", "tokens": [50656, 286, 808, 490, 264, 10745, 1002, 13, 50730], "temperature": 0.0, "avg_logprob": -0.2094282351042095, "compression_ratio": 1.57421875, "no_speech_prob": 0.006650566589087248}, {"id": 117, "seek": 49612, "start": 503.44, "end": 505.92, "text": " This is not always the case with the Java PEP.", "tokens": [50730, 639, 307, 406, 1009, 264, 1389, 365, 264, 10745, 430, 8929, 13, 50854], "temperature": 0.0, "avg_logprob": -0.2094282351042095, "compression_ratio": 1.57421875, "no_speech_prob": 0.006650566589087248}, {"id": 118, "seek": 49612, "start": 505.92, "end": 511.6, "text": " So I'm with the JEP, so I'm quite happy that Python does things a little bit better.", "tokens": [50854, 407, 286, 478, 365, 264, 508, 8929, 11, 370, 286, 478, 1596, 2055, 300, 15329, 775, 721, 257, 707, 857, 1101, 13, 51138], "temperature": 0.0, "avg_logprob": -0.2094282351042095, "compression_ratio": 1.57421875, "no_speech_prob": 0.006650566589087248}, {"id": 119, "seek": 49612, "start": 511.6, "end": 517.68, "text": " So the JEP is called Low Impact Monitoring for C Python and hopefully other runtimes", "tokens": [51138, 407, 264, 508, 8929, 307, 1219, 17078, 31005, 33799, 278, 337, 383, 15329, 293, 4696, 661, 49435, 1532, 51442], "temperature": 0.0, "avg_logprob": -0.2094282351042095, "compression_ratio": 1.57421875, "no_speech_prob": 0.006650566589087248}, {"id": 120, "seek": 49612, "start": 517.68, "end": 520.2, "text": " will support it in the future.", "tokens": [51442, 486, 1406, 309, 294, 264, 2027, 13, 51568], "temperature": 0.0, "avg_logprob": -0.2094282351042095, "compression_ratio": 1.57421875, "no_speech_prob": 0.006650566589087248}, {"id": 121, "seek": 49612, "start": 520.2, "end": 522.64, "text": " And it's here since October.", "tokens": [51568, 400, 309, 311, 510, 1670, 7617, 13, 51690], "temperature": 0.0, "avg_logprob": -0.2094282351042095, "compression_ratio": 1.57421875, "no_speech_prob": 0.006650566589087248}, {"id": 122, "seek": 52264, "start": 522.68, "end": 528.88, "text": " So the idea here is that we have more fine-train support, that we learn from the lessons that", "tokens": [50366, 407, 264, 1558, 510, 307, 300, 321, 362, 544, 2489, 12, 83, 7146, 1406, 11, 300, 321, 1466, 490, 264, 8820, 300, 50676], "temperature": 0.0, "avg_logprob": -0.27287677764892576, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0021916907280683517}, {"id": 123, "seek": 52264, "start": 528.88, "end": 535.4, "text": " like having to enable the line handlers for every line is probably not the best thing.", "tokens": [50676, 411, 1419, 281, 9528, 264, 1622, 1011, 11977, 337, 633, 1622, 307, 1391, 406, 264, 1151, 551, 13, 51002], "temperature": 0.0, "avg_logprob": -0.27287677764892576, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0021916907280683517}, {"id": 124, "seek": 52264, "start": 535.4, "end": 540.4, "text": " So typically when we develop, when we use this PEP, we define some shortcuts here in", "tokens": [51002, 407, 5850, 562, 321, 1499, 11, 562, 321, 764, 341, 430, 8929, 11, 321, 6964, 512, 34620, 510, 294, 51252], "temperature": 0.0, "avg_logprob": -0.27287677764892576, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0021916907280683517}, {"id": 125, "seek": 52264, "start": 540.4, "end": 541.4, "text": " the top.", "tokens": [51252, 264, 1192, 13, 51302], "temperature": 0.0, "avg_logprob": -0.27287677764892576, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0021916907280683517}, {"id": 126, "seek": 52264, "start": 541.4, "end": 547.12, "text": " So we don't run to write, is this not monitoring all the time because that's where the monitoring", "tokens": [51302, 407, 321, 500, 380, 1190, 281, 2464, 11, 307, 341, 406, 11028, 439, 264, 565, 570, 300, 311, 689, 264, 11028, 51588], "temperature": 0.0, "avg_logprob": -0.27287677764892576, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0021916907280683517}, {"id": 127, "seek": 52264, "start": 547.12, "end": 550.48, "text": " functions live.", "tokens": [51588, 6828, 1621, 13, 51756], "temperature": 0.0, "avg_logprob": -0.27287677764892576, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0021916907280683517}, {"id": 128, "seek": 55048, "start": 550.48, "end": 556.2, "text": " So we call it mon and events, it's also bit long, so we call it mon not events.", "tokens": [50364, 407, 321, 818, 309, 1108, 293, 3931, 11, 309, 311, 611, 857, 938, 11, 370, 321, 818, 309, 1108, 406, 3931, 13, 50650], "temperature": 0.0, "avg_logprob": -0.2721199247572157, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0028703126590698957}, {"id": 129, "seek": 55048, "start": 556.2, "end": 560.16, "text": " Then we have to assign it the tool ideas, tool ideas.", "tokens": [50650, 1396, 321, 362, 281, 6269, 309, 264, 2290, 3487, 11, 2290, 3487, 13, 50848], "temperature": 0.0, "avg_logprob": -0.2721199247572157, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0028703126590698957}, {"id": 130, "seek": 55048, "start": 560.16, "end": 565.9200000000001, "text": " So the idea is that you can have multiple tools that are registered here.", "tokens": [50848, 407, 264, 1558, 307, 300, 291, 393, 362, 3866, 3873, 300, 366, 13968, 510, 13, 51136], "temperature": 0.0, "avg_logprob": -0.2721199247572157, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0028703126590698957}, {"id": 131, "seek": 55048, "start": 565.9200000000001, "end": 570.9200000000001, "text": " And for each tool we register some callbacks.", "tokens": [51136, 400, 337, 1184, 2290, 321, 7280, 512, 818, 17758, 13, 51386], "temperature": 0.0, "avg_logprob": -0.2721199247572157, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0028703126590698957}, {"id": 132, "seek": 55048, "start": 570.9200000000001, "end": 575.48, "text": " So what we do here in our example, we register a callback for our tool.", "tokens": [51386, 407, 437, 321, 360, 510, 294, 527, 1365, 11, 321, 7280, 257, 818, 3207, 337, 527, 2290, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2721199247572157, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0028703126590698957}, {"id": 133, "seek": 57548, "start": 575.48, "end": 582.04, "text": " Our tool is a debugger, they're like six possible tool ideas, a tool idea is one of them is", "tokens": [50364, 2621, 2290, 307, 257, 24083, 1321, 11, 436, 434, 411, 2309, 1944, 2290, 3487, 11, 257, 2290, 1558, 307, 472, 295, 552, 307, 50692], "temperature": 0.0, "avg_logprob": -0.28188620294843403, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0032033950556069613}, {"id": 134, "seek": 57548, "start": 582.04, "end": 585.04, "text": " a debugger, another one is a profile.", "tokens": [50692, 257, 24083, 1321, 11, 1071, 472, 307, 257, 7964, 13, 50842], "temperature": 0.0, "avg_logprob": -0.28188620294843403, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0032033950556069613}, {"id": 135, "seek": 57548, "start": 585.04, "end": 589.72, "text": " And we register here callback for the line because we want to still have like line callbacks", "tokens": [50842, 400, 321, 7280, 510, 818, 3207, 337, 264, 1622, 570, 321, 528, 281, 920, 362, 411, 1622, 818, 17758, 51076], "temperature": 0.0, "avg_logprob": -0.28188620294843403, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0032033950556069613}, {"id": 136, "seek": 57548, "start": 589.72, "end": 590.72, "text": " sometimes.", "tokens": [51076, 2171, 13, 51126], "temperature": 0.0, "avg_logprob": -0.28188620294843403, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0032033950556069613}, {"id": 137, "seek": 57548, "start": 590.72, "end": 594.76, "text": " And we also register a callback for the start function.", "tokens": [51126, 400, 321, 611, 7280, 257, 818, 3207, 337, 264, 722, 2445, 13, 51328], "temperature": 0.0, "avg_logprob": -0.28188620294843403, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0032033950556069613}, {"id": 138, "seek": 57548, "start": 594.76, "end": 599.84, "text": " So for the start event, when a method is called, then we have these handlers and the start", "tokens": [51328, 407, 337, 264, 722, 2280, 11, 562, 257, 3170, 307, 1219, 11, 550, 321, 362, 613, 1011, 11977, 293, 264, 722, 51582], "temperature": 0.0, "avg_logprob": -0.28188620294843403, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0032033950556069613}, {"id": 139, "seek": 57548, "start": 599.84, "end": 602.24, "text": " handler is just passed like the code object.", "tokens": [51582, 41967, 307, 445, 4678, 411, 264, 3089, 2657, 13, 51702], "temperature": 0.0, "avg_logprob": -0.28188620294843403, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0032033950556069613}, {"id": 140, "seek": 60224, "start": 602.24, "end": 608.44, "text": " That's what you get when you form a method for function called f underscore code.", "tokens": [50364, 663, 311, 437, 291, 483, 562, 291, 1254, 257, 3170, 337, 2445, 1219, 283, 37556, 3089, 13, 50674], "temperature": 0.0, "avg_logprob": -0.25957973330628636, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.010009781457483768}, {"id": 141, "seek": 60224, "start": 608.44, "end": 615.6, "text": " And the offset word is located in a byte code and for line handler, we get the line word", "tokens": [50674, 400, 264, 18687, 1349, 307, 6870, 294, 257, 40846, 3089, 293, 337, 1622, 41967, 11, 321, 483, 264, 1622, 1349, 51032], "temperature": 0.0, "avg_logprob": -0.25957973330628636, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.010009781457483768}, {"id": 142, "seek": 60224, "start": 615.6, "end": 616.6, "text": " in.", "tokens": [51032, 294, 13, 51082], "temperature": 0.0, "avg_logprob": -0.25957973330628636, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.010009781457483768}, {"id": 143, "seek": 60224, "start": 616.6, "end": 622.48, "text": " And the cool thing is here we can return from this, as you see in the bottom, the line handler", "tokens": [51082, 400, 264, 1627, 551, 307, 510, 321, 393, 2736, 490, 341, 11, 382, 291, 536, 294, 264, 2767, 11, 264, 1622, 41967, 51376], "temperature": 0.0, "avg_logprob": -0.25957973330628636, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.010009781457483768}, {"id": 144, "seek": 60224, "start": 622.48, "end": 626.08, "text": " also can return either disable or any.", "tokens": [51376, 611, 393, 2736, 2139, 28362, 420, 604, 13, 51556], "temperature": 0.0, "avg_logprob": -0.25957973330628636, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.010009781457483768}, {"id": 145, "seek": 60224, "start": 626.08, "end": 631.44, "text": " And the cool thing is when we disable, it's disabled all the time for this specific line", "tokens": [51556, 400, 264, 1627, 551, 307, 562, 321, 28362, 11, 309, 311, 15191, 439, 264, 565, 337, 341, 2685, 1622, 51824], "temperature": 0.0, "avg_logprob": -0.25957973330628636, "compression_ratio": 1.8294930875576036, "no_speech_prob": 0.010009781457483768}, {"id": 146, "seek": 63144, "start": 631.44, "end": 632.8000000000001, "text": " and it's called for coverage.", "tokens": [50364, 293, 309, 311, 1219, 337, 9645, 13, 50432], "temperature": 0.0, "avg_logprob": -0.22805225956547368, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.003318584756925702}, {"id": 147, "seek": 63144, "start": 632.8000000000001, "end": 638.0, "text": " So we can also make coverage testing easier.", "tokens": [50432, 407, 321, 393, 611, 652, 9645, 4997, 3571, 13, 50692], "temperature": 0.0, "avg_logprob": -0.22805225956547368, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.003318584756925702}, {"id": 148, "seek": 63144, "start": 638.0, "end": 641.12, "text": " So yes, we enable them the start events and that's fine.", "tokens": [50692, 407, 2086, 11, 321, 9528, 552, 264, 722, 3931, 293, 300, 311, 2489, 13, 50848], "temperature": 0.0, "avg_logprob": -0.22805225956547368, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.003318584756925702}, {"id": 149, "seek": 63144, "start": 641.12, "end": 647.72, "text": " And then we run our program, we get the start event for every function that we call and", "tokens": [50848, 400, 550, 321, 1190, 527, 1461, 11, 321, 483, 264, 722, 2280, 337, 633, 2445, 300, 321, 818, 293, 51178], "temperature": 0.0, "avg_logprob": -0.22805225956547368, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.003318584756925702}, {"id": 150, "seek": 63144, "start": 647.72, "end": 651.9200000000001, "text": " then at every time we ask, hey, do we have a break point as an function?", "tokens": [51178, 550, 412, 633, 565, 321, 1029, 11, 4177, 11, 360, 321, 362, 257, 1821, 935, 382, 364, 2445, 30, 51388], "temperature": 0.0, "avg_logprob": -0.22805225956547368, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.003318584756925702}, {"id": 151, "seek": 63144, "start": 651.9200000000001, "end": 655.48, "text": " If yes, we enable the line here.", "tokens": [51388, 759, 2086, 11, 321, 9528, 264, 1622, 510, 13, 51566], "temperature": 0.0, "avg_logprob": -0.22805225956547368, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.003318584756925702}, {"id": 152, "seek": 63144, "start": 655.48, "end": 659.0, "text": " But only specifically for this function.", "tokens": [51566, 583, 787, 4682, 337, 341, 2445, 13, 51742], "temperature": 0.0, "avg_logprob": -0.22805225956547368, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.003318584756925702}, {"id": 153, "seek": 63144, "start": 659.0, "end": 661.08, "text": " And then for every line we check it.", "tokens": [51742, 400, 550, 337, 633, 1622, 321, 1520, 309, 13, 51846], "temperature": 0.0, "avg_logprob": -0.22805225956547368, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.003318584756925702}, {"id": 154, "seek": 66108, "start": 661.08, "end": 665.8000000000001, "text": " The cool thing is that these line events are emitted per fret.", "tokens": [50364, 440, 1627, 551, 307, 300, 613, 1622, 3931, 366, 44897, 680, 24189, 13, 50600], "temperature": 0.0, "avg_logprob": -0.32367078808770666, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.007697248365730047}, {"id": 155, "seek": 66108, "start": 665.8000000000001, "end": 676.48, "text": " So the ideas that were set, sister, etc., it was emitted like in the main fret per interpreter.", "tokens": [50600, 407, 264, 3487, 300, 645, 992, 11, 4892, 11, 5183, 7933, 309, 390, 44897, 411, 294, 264, 2135, 24189, 680, 34132, 13, 51134], "temperature": 0.0, "avg_logprob": -0.32367078808770666, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.007697248365730047}, {"id": 156, "seek": 66108, "start": 676.48, "end": 686.96, "text": " But here it's emitted for every line in the fret that the function is currently executing.", "tokens": [51134, 583, 510, 309, 311, 44897, 337, 633, 1622, 294, 264, 24189, 300, 264, 2445, 307, 4362, 32368, 13, 51658], "temperature": 0.0, "avg_logprob": -0.32367078808770666, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.007697248365730047}, {"id": 157, "seek": 66108, "start": 686.96, "end": 687.96, "text": " And this is really cool.", "tokens": [51658, 400, 341, 307, 534, 1627, 13, 51708], "temperature": 0.0, "avg_logprob": -0.32367078808770666, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.007697248365730047}, {"id": 158, "seek": 68796, "start": 687.96, "end": 697.4000000000001, "text": " And Lukas Lange wrote in a PR discussion, the biggest opportunity of PEP 6.9 isn't", "tokens": [50364, 400, 34992, 296, 441, 933, 4114, 294, 257, 11568, 5017, 11, 264, 3880, 2650, 295, 430, 8929, 1386, 13, 24, 1943, 380, 50836], "temperature": 0.0, "avg_logprob": -0.3034248967324534, "compression_ratio": 1.4873949579831933, "no_speech_prob": 0.1792573630809784}, {"id": 159, "seek": 68796, "start": 697.4000000000001, "end": 698.4000000000001, "text": " even the speed.", "tokens": [50836, 754, 264, 3073, 13, 50886], "temperature": 0.0, "avg_logprob": -0.3034248967324534, "compression_ratio": 1.4873949579831933, "no_speech_prob": 0.1792573630809784}, {"id": 160, "seek": 68796, "start": 698.4000000000001, "end": 703.2, "text": " It's the fact that the debugger built on top of it will automatically support all threats", "tokens": [50886, 467, 311, 264, 1186, 300, 264, 24083, 1321, 3094, 322, 1192, 295, 309, 486, 6772, 1406, 439, 14909, 51126], "temperature": 0.0, "avg_logprob": -0.3034248967324534, "compression_ratio": 1.4873949579831933, "no_speech_prob": 0.1792573630809784}, {"id": 161, "seek": 68796, "start": 703.2, "end": 706.48, "text": " and supports threats properly.", "tokens": [51126, 293, 9346, 14909, 6108, 13, 51290], "temperature": 0.0, "avg_logprob": -0.3034248967324534, "compression_ratio": 1.4873949579831933, "no_speech_prob": 0.1792573630809784}, {"id": 162, "seek": 68796, "start": 706.48, "end": 713.6800000000001, "text": " And with the incoming changes with PEP 703, with making the global interpreter lock option", "tokens": [51290, 400, 365, 264, 22341, 2962, 365, 430, 8929, 5285, 18, 11, 365, 1455, 264, 4338, 34132, 4017, 3614, 51650], "temperature": 0.0, "avg_logprob": -0.3034248967324534, "compression_ratio": 1.4873949579831933, "no_speech_prob": 0.1792573630809784}, {"id": 163, "seek": 68796, "start": 713.6800000000001, "end": 716.72, "text": " in Cpython, it will get far more important.", "tokens": [51650, 294, 383, 8200, 11943, 11, 309, 486, 483, 1400, 544, 1021, 13, 51802], "temperature": 0.0, "avg_logprob": -0.3034248967324534, "compression_ratio": 1.4873949579831933, "no_speech_prob": 0.1792573630809784}, {"id": 164, "seek": 71672, "start": 716.72, "end": 724.5600000000001, "text": " Because with then we will probably see multi-threading Python applications and then the old approach", "tokens": [50364, 1436, 365, 550, 321, 486, 1391, 536, 4825, 12, 392, 35908, 15329, 5821, 293, 550, 264, 1331, 3109, 50756], "temperature": 0.0, "avg_logprob": -0.19802887439727784, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.03926435112953186}, {"id": 165, "seek": 71672, "start": 724.5600000000001, "end": 727.08, "text": " is just not usable anymore.", "tokens": [50756, 307, 445, 406, 29975, 3602, 13, 50882], "temperature": 0.0, "avg_logprob": -0.19802887439727784, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.03926435112953186}, {"id": 166, "seek": 71672, "start": 727.08, "end": 732.76, "text": " So the idea is that we can enable events globally and locally.", "tokens": [50882, 407, 264, 1558, 307, 300, 321, 393, 9528, 3931, 18958, 293, 16143, 13, 51166], "temperature": 0.0, "avg_logprob": -0.19802887439727784, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.03926435112953186}, {"id": 167, "seek": 71672, "start": 732.76, "end": 738.12, "text": " And the sum of both is like they're enabled events per function.", "tokens": [51166, 400, 264, 2408, 295, 1293, 307, 411, 436, 434, 15172, 3931, 680, 2445, 13, 51434], "temperature": 0.0, "avg_logprob": -0.19802887439727784, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.03926435112953186}, {"id": 168, "seek": 71672, "start": 738.12, "end": 742.5600000000001, "text": " And the cool thing here is the power is in the fine-train configuration.", "tokens": [51434, 400, 264, 1627, 551, 510, 307, 264, 1347, 307, 294, 264, 2489, 12, 83, 7146, 11694, 13, 51656], "temperature": 0.0, "avg_logprob": -0.19802887439727784, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.03926435112953186}, {"id": 169, "seek": 74256, "start": 742.56, "end": 748.1199999999999, "text": " So you can set events in a function f for the function f while this function is running.", "tokens": [50364, 407, 291, 393, 992, 3931, 294, 257, 2445, 283, 337, 264, 2445, 283, 1339, 341, 2445, 307, 2614, 13, 50642], "temperature": 0.0, "avg_logprob": -0.257867870908795, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.12191488593816757}, {"id": 170, "seek": 74256, "start": 748.1199999999999, "end": 749.4, "text": " So consider this example here.", "tokens": [50642, 407, 1949, 341, 1365, 510, 13, 50706], "temperature": 0.0, "avg_logprob": -0.257867870908795, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.12191488593816757}, {"id": 171, "seek": 74256, "start": 749.4, "end": 754.92, "text": " We have a simple line handler here and you register a callback for each line.", "tokens": [50706, 492, 362, 257, 2199, 1622, 41967, 510, 293, 291, 7280, 257, 818, 3207, 337, 1184, 1622, 13, 50982], "temperature": 0.0, "avg_logprob": -0.257867870908795, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.12191488593816757}, {"id": 172, "seek": 74256, "start": 754.92, "end": 759.16, "text": " And then in f you decide at some point, hey, I want to set the local events.", "tokens": [50982, 400, 550, 294, 283, 291, 4536, 412, 512, 935, 11, 4177, 11, 286, 528, 281, 992, 264, 2654, 3931, 13, 51194], "temperature": 0.0, "avg_logprob": -0.257867870908795, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.12191488593816757}, {"id": 173, "seek": 74256, "start": 759.16, "end": 761.5999999999999, "text": " I want enable line events.", "tokens": [51194, 286, 528, 9528, 1622, 3931, 13, 51316], "temperature": 0.0, "avg_logprob": -0.257867870908795, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.12191488593816757}, {"id": 174, "seek": 74256, "start": 761.5999999999999, "end": 763.1999999999999, "text": " And later you disable them.", "tokens": [51316, 400, 1780, 291, 28362, 552, 13, 51396], "temperature": 0.0, "avg_logprob": -0.257867870908795, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.12191488593816757}, {"id": 175, "seek": 74256, "start": 763.1999999999999, "end": 764.1999999999999, "text": " And it works.", "tokens": [51396, 400, 309, 1985, 13, 51446], "temperature": 0.0, "avg_logprob": -0.257867870908795, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.12191488593816757}, {"id": 176, "seek": 74256, "start": 764.1999999999999, "end": 768.4, "text": " So here we emit hello and then we emit like, hey, we're at line 18.", "tokens": [51446, 407, 510, 321, 32084, 7751, 293, 550, 321, 32084, 411, 11, 4177, 11, 321, 434, 412, 1622, 2443, 13, 51656], "temperature": 0.0, "avg_logprob": -0.257867870908795, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.12191488593816757}, {"id": 177, "seek": 74256, "start": 768.4, "end": 772.4, "text": " We're just like the line that prints n, then we print n.", "tokens": [51656, 492, 434, 445, 411, 264, 1622, 300, 22305, 297, 11, 550, 321, 4482, 297, 13, 51856], "temperature": 0.0, "avg_logprob": -0.257867870908795, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.12191488593816757}, {"id": 178, "seek": 77240, "start": 773.24, "end": 774.24, "text": " And that's really cool.", "tokens": [50406, 400, 300, 311, 534, 1627, 13, 50456], "temperature": 0.0, "avg_logprob": -0.25481209132982335, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.0033504802267998457}, {"id": 179, "seek": 77240, "start": 774.24, "end": 776.4399999999999, "text": " That wasn't possible before.", "tokens": [50456, 663, 2067, 380, 1944, 949, 13, 50566], "temperature": 0.0, "avg_logprob": -0.25481209132982335, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.0033504802267998457}, {"id": 180, "seek": 77240, "start": 776.4399999999999, "end": 782.24, "text": " That's really great because this enables us to only enable line events for the functions", "tokens": [50566, 663, 311, 534, 869, 570, 341, 17077, 505, 281, 787, 9528, 1622, 3931, 337, 264, 6828, 50856], "temperature": 0.0, "avg_logprob": -0.25481209132982335, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.0033504802267998457}, {"id": 181, "seek": 77240, "start": 782.24, "end": 784.04, "text": " that we will need them.", "tokens": [50856, 300, 321, 486, 643, 552, 13, 50946], "temperature": 0.0, "avg_logprob": -0.25481209132982335, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.0033504802267998457}, {"id": 182, "seek": 77240, "start": 784.04, "end": 789.88, "text": " And so the question is, of course, what's far as there are several methods in this PEP", "tokens": [50946, 400, 370, 264, 1168, 307, 11, 295, 1164, 11, 437, 311, 1400, 382, 456, 366, 2940, 7150, 294, 341, 430, 8929, 51238], "temperature": 0.0, "avg_logprob": -0.25481209132982335, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.0033504802267998457}, {"id": 183, "seek": 77240, "start": 789.88, "end": 790.9599999999999, "text": " and this API.", "tokens": [51238, 293, 341, 9362, 13, 51292], "temperature": 0.0, "avg_logprob": -0.25481209132982335, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.0033504802267998457}, {"id": 184, "seek": 77240, "start": 790.9599999999999, "end": 794.04, "text": " And what's really fast is to register callbacks.", "tokens": [51292, 400, 437, 311, 534, 2370, 307, 281, 7280, 818, 17758, 13, 51446], "temperature": 0.0, "avg_logprob": -0.25481209132982335, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.0033504802267998457}, {"id": 185, "seek": 77240, "start": 794.04, "end": 799.88, "text": " We can easily switch out the callbacks and get the tools so we can say, oh, please get", "tokens": [51446, 492, 393, 3612, 3679, 484, 264, 818, 17758, 293, 483, 264, 3873, 370, 321, 393, 584, 11, 1954, 11, 1767, 483, 51738], "temperature": 0.0, "avg_logprob": -0.25481209132982335, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.0033504802267998457}, {"id": 186, "seek": 77240, "start": 799.88, "end": 801.88, "text": " the tool ID.", "tokens": [51738, 264, 2290, 7348, 13, 51838], "temperature": 0.0, "avg_logprob": -0.25481209132982335, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.0033504802267998457}, {"id": 187, "seek": 80188, "start": 801.88, "end": 807.84, "text": " What's kind of fast is that setting local events because where it does it modifies the", "tokens": [50364, 708, 311, 733, 295, 2370, 307, 300, 3287, 2654, 3931, 570, 689, 309, 775, 309, 1072, 11221, 264, 50662], "temperature": 0.0, "avg_logprob": -0.28806065989064644, "compression_ratio": 1.625, "no_speech_prob": 0.0019471814157441258}, {"id": 188, "seek": 80188, "start": 807.84, "end": 810.12, "text": " bytecode to the VMS executing.", "tokens": [50662, 40846, 22332, 281, 264, 691, 10288, 32368, 13, 50776], "temperature": 0.0, "avg_logprob": -0.28806065989064644, "compression_ratio": 1.625, "no_speech_prob": 0.0019471814157441258}, {"id": 189, "seek": 80188, "start": 810.12, "end": 818.92, "text": " And what's pretty slow is to use the tool ID to start the debugger here and to set the", "tokens": [50776, 400, 437, 311, 1238, 2964, 307, 281, 764, 264, 2290, 7348, 281, 722, 264, 24083, 1321, 510, 293, 281, 992, 264, 51216], "temperature": 0.0, "avg_logprob": -0.28806065989064644, "compression_ratio": 1.625, "no_speech_prob": 0.0019471814157441258}, {"id": 190, "seek": 80188, "start": 818.92, "end": 826.76, "text": " global events because this potentially recompars or modifies all bytecode of every function.", "tokens": [51216, 4338, 3931, 570, 341, 7263, 48000, 685, 420, 1072, 11221, 439, 40846, 22332, 295, 633, 2445, 13, 51608], "temperature": 0.0, "avg_logprob": -0.28806065989064644, "compression_ratio": 1.625, "no_speech_prob": 0.0019471814157441258}, {"id": 191, "seek": 80188, "start": 826.76, "end": 827.8, "text": " So do it all the way.", "tokens": [51608, 407, 360, 309, 439, 264, 636, 13, 51660], "temperature": 0.0, "avg_logprob": -0.28806065989064644, "compression_ratio": 1.625, "no_speech_prob": 0.0019471814157441258}, {"id": 192, "seek": 80188, "start": 827.8, "end": 829.16, "text": " So then it's fine.", "tokens": [51660, 407, 550, 309, 311, 2489, 13, 51728], "temperature": 0.0, "avg_logprob": -0.28806065989064644, "compression_ratio": 1.625, "no_speech_prob": 0.0019471814157441258}, {"id": 193, "seek": 82916, "start": 829.16, "end": 832.6, "text": " So back to the debugger.", "tokens": [50364, 407, 646, 281, 264, 24083, 1321, 13, 50536], "temperature": 0.0, "avg_logprob": -0.25794836815367356, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0009628914995118976}, {"id": 194, "seek": 82916, "start": 832.6, "end": 836.04, "text": " We had here our start handler and our line handler.", "tokens": [50536, 492, 632, 510, 527, 722, 41967, 293, 527, 1622, 41967, 13, 50708], "temperature": 0.0, "avg_logprob": -0.25794836815367356, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0009628914995118976}, {"id": 195, "seek": 82916, "start": 836.04, "end": 838.6, "text": " And they look essentially the same as before.", "tokens": [50708, 400, 436, 574, 4476, 264, 912, 382, 949, 13, 50836], "temperature": 0.0, "avg_logprob": -0.25794836815367356, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0009628914995118976}, {"id": 196, "seek": 82916, "start": 838.6, "end": 843.4399999999999, "text": " The only difference is that we enable the line breakpoints when we're needed.", "tokens": [50836, 440, 787, 2649, 307, 300, 321, 9528, 264, 1622, 1821, 20552, 562, 321, 434, 2978, 13, 51078], "temperature": 0.0, "avg_logprob": -0.25794836815367356, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0009628914995118976}, {"id": 197, "seek": 82916, "start": 843.4399999999999, "end": 848.16, "text": " So they're different than events kinds because we've seen that line events are pretty powerful", "tokens": [51078, 407, 436, 434, 819, 813, 3931, 3685, 570, 321, 600, 1612, 300, 1622, 3931, 366, 1238, 4005, 51314], "temperature": 0.0, "avg_logprob": -0.25794836815367356, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0009628914995118976}, {"id": 198, "seek": 82916, "start": 848.16, "end": 851.52, "text": " for implementing basic debuggers.", "tokens": [51314, 337, 18114, 3875, 3001, 3562, 433, 13, 51482], "temperature": 0.0, "avg_logprob": -0.25794836815367356, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0009628914995118976}, {"id": 199, "seek": 82916, "start": 851.52, "end": 855.0799999999999, "text": " One of them we've seen already the start events.", "tokens": [51482, 1485, 295, 552, 321, 600, 1612, 1217, 264, 722, 3931, 13, 51660], "temperature": 0.0, "avg_logprob": -0.25794836815367356, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0009628914995118976}, {"id": 200, "seek": 85508, "start": 855.4000000000001, "end": 861.0, "text": " There's the regime return, yield events for everything that you do in your Python application.", "tokens": [50380, 821, 311, 264, 13120, 2736, 11, 11257, 3931, 337, 1203, 300, 291, 360, 294, 428, 15329, 3861, 13, 50660], "temperature": 0.0, "avg_logprob": -0.36944889377903295, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.0037519235629588366}, {"id": 201, "seek": 85508, "start": 861.0, "end": 864.0, "text": " And there are also then in-signary events.", "tokens": [50660, 400, 456, 366, 611, 550, 294, 12, 82, 788, 822, 3931, 13, 50810], "temperature": 0.0, "avg_logprob": -0.36944889377903295, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.0037519235629588366}, {"id": 202, "seek": 85508, "start": 864.0, "end": 870.5600000000001, "text": " These are events that aren't like an A that you can't enable or disable because they come", "tokens": [50810, 1981, 366, 3931, 300, 3212, 380, 411, 364, 316, 300, 291, 393, 380, 9528, 420, 28362, 570, 436, 808, 51138], "temperature": 0.0, "avg_logprob": -0.36944889377903295, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.0037519235629588366}, {"id": 203, "seek": 85508, "start": 870.5600000000001, "end": 873.8000000000001, "text": " from, they are controlled by other events.", "tokens": [51138, 490, 11, 436, 366, 10164, 538, 661, 3931, 13, 51300], "temperature": 0.0, "avg_logprob": -0.36944889377903295, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.0037519235629588366}, {"id": 204, "seek": 85508, "start": 873.8000000000001, "end": 878.2800000000001, "text": " So for example, you have to call event that is triggered whenever you call a function", "tokens": [51300, 407, 337, 1365, 11, 291, 362, 281, 818, 2280, 300, 307, 21710, 5699, 291, 818, 257, 2445, 51524], "temperature": 0.0, "avg_logprob": -0.36944889377903295, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.0037519235629588366}, {"id": 205, "seek": 85508, "start": 878.2800000000001, "end": 885.0400000000001, "text": " and then we have C relighted, a C raise whenever exception is flown in C code or whenever fact", "tokens": [51524, 293, 550, 321, 362, 383, 1039, 397, 292, 11, 257, 383, 5300, 5699, 11183, 307, 34536, 294, 383, 3089, 420, 5699, 1186, 51862], "temperature": 0.0, "avg_logprob": -0.36944889377903295, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.0037519235629588366}, {"id": 206, "seek": 88504, "start": 885.04, "end": 886.8, "text": " a C function returns.", "tokens": [50364, 257, 383, 2445, 11247, 13, 50452], "temperature": 0.0, "avg_logprob": -0.2090669575304088, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.0006250322330743074}, {"id": 207, "seek": 88504, "start": 886.8, "end": 893.68, "text": " And there are of course then also other events that are not enabled locally but only globally.", "tokens": [50452, 400, 456, 366, 295, 1164, 550, 611, 661, 3931, 300, 366, 406, 15172, 16143, 457, 787, 18958, 13, 50796], "temperature": 0.0, "avg_logprob": -0.2090669575304088, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.0006250322330743074}, {"id": 208, "seek": 88504, "start": 893.68, "end": 896.3199999999999, "text": " Essentially the idea is we cannot locate them properly.", "tokens": [50796, 23596, 264, 1558, 307, 321, 2644, 22370, 552, 6108, 13, 50928], "temperature": 0.0, "avg_logprob": -0.2090669575304088, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.0006250322330743074}, {"id": 209, "seek": 88504, "start": 896.3199999999999, "end": 901.4399999999999, "text": " And the cool thing here is maybe you've seen that we have a new event called stop iteration", "tokens": [50928, 400, 264, 1627, 551, 510, 307, 1310, 291, 600, 1612, 300, 321, 362, 257, 777, 2280, 1219, 1590, 24784, 51184], "temperature": 0.0, "avg_logprob": -0.2090669575304088, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.0006250322330743074}, {"id": 210, "seek": 88504, "start": 901.4399999999999, "end": 907.36, "text": " because we think it was in this Python version we're not using in iterators.", "tokens": [51184, 570, 321, 519, 309, 390, 294, 341, 15329, 3037, 321, 434, 406, 1228, 294, 17138, 3391, 13, 51480], "temperature": 0.0, "avg_logprob": -0.2090669575304088, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.0006250322330743074}, {"id": 211, "seek": 88504, "start": 907.36, "end": 913.4399999999999, "text": " When we were returning from iterator previously we wrote an exception but that's pretty slow.", "tokens": [51480, 1133, 321, 645, 12678, 490, 17138, 1639, 8046, 321, 4114, 364, 11183, 457, 300, 311, 1238, 2964, 13, 51784], "temperature": 0.0, "avg_logprob": -0.2090669575304088, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.0006250322330743074}, {"id": 212, "seek": 91344, "start": 913.44, "end": 919.0400000000001, "text": " So we don't throw an exception anymore but to debug this to still notice it we have a", "tokens": [50364, 407, 321, 500, 380, 3507, 364, 11183, 3602, 457, 281, 24083, 341, 281, 920, 3449, 309, 321, 362, 257, 50644], "temperature": 0.0, "avg_logprob": -0.2338052267556662, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0005777585902251303}, {"id": 213, "seek": 91344, "start": 919.0400000000001, "end": 922.6800000000001, "text": " new stop iteration event.", "tokens": [50644, 777, 1590, 24784, 2280, 13, 50826], "temperature": 0.0, "avg_logprob": -0.2338052267556662, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0005777585902251303}, {"id": 214, "seek": 91344, "start": 922.6800000000001, "end": 927.08, "text": " Of course what you'll be waiting for is performance because the performance is like the thing", "tokens": [50826, 2720, 1164, 437, 291, 603, 312, 3806, 337, 307, 3389, 570, 264, 3389, 307, 411, 264, 551, 51046], "temperature": 0.0, "avg_logprob": -0.2338052267556662, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0005777585902251303}, {"id": 215, "seek": 91344, "start": 927.08, "end": 934.0400000000001, "text": " that besides the threading support is pretty neat.", "tokens": [51046, 300, 11868, 264, 7207, 278, 1406, 307, 1238, 10654, 13, 51394], "temperature": 0.0, "avg_logprob": -0.2338052267556662, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0005777585902251303}, {"id": 216, "seek": 91344, "start": 934.0400000000001, "end": 939.2800000000001, "text": " So what I did I looked around and I found some people also doing some performances but", "tokens": [51394, 407, 437, 286, 630, 286, 2956, 926, 293, 286, 1352, 512, 561, 611, 884, 512, 16087, 457, 51656], "temperature": 0.0, "avg_logprob": -0.2338052267556662, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0005777585902251303}, {"id": 217, "seek": 91344, "start": 939.2800000000001, "end": 941.0, "text": " they were using Fibonacci functions.", "tokens": [51656, 436, 645, 1228, 479, 897, 266, 43870, 6828, 13, 51742], "temperature": 0.0, "avg_logprob": -0.2338052267556662, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0005777585902251303}, {"id": 218, "seek": 94100, "start": 941.44, "end": 944.6, "text": " Now I'm like that's a bit small, that's not representative.", "tokens": [50386, 823, 286, 478, 411, 300, 311, 257, 857, 1359, 11, 300, 311, 406, 12424, 13, 50544], "temperature": 0.0, "avg_logprob": -0.2601460187863081, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.005292763002216816}, {"id": 219, "seek": 94100, "start": 944.6, "end": 949.76, "text": " So I started looking into Python benchmarking suites and there's this pi performance benchmarking", "tokens": [50544, 407, 286, 1409, 1237, 666, 15329, 18927, 278, 459, 3324, 293, 456, 311, 341, 3895, 3389, 18927, 278, 50802], "temperature": 0.0, "avg_logprob": -0.2601460187863081, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.005292763002216816}, {"id": 220, "seek": 94100, "start": 949.76, "end": 951.84, "text": " suite and I just hacked it.", "tokens": [50802, 14205, 293, 286, 445, 36218, 309, 13, 50906], "temperature": 0.0, "avg_logprob": -0.2601460187863081, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.005292763002216816}, {"id": 221, "seek": 94100, "start": 951.84, "end": 956.68, "text": " I just wrote random code in it because you can do some kind of monkey patching in Python", "tokens": [50906, 286, 445, 4114, 4974, 3089, 294, 309, 570, 291, 393, 360, 512, 733, 295, 17847, 9972, 278, 294, 15329, 51148], "temperature": 0.0, "avg_logprob": -0.2601460187863081, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.005292763002216816}, {"id": 222, "seek": 94100, "start": 956.68, "end": 958.8, "text": " and it's great.", "tokens": [51148, 293, 309, 311, 869, 13, 51254], "temperature": 0.0, "avg_logprob": -0.2601460187863081, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.005292763002216816}, {"id": 223, "seek": 94100, "start": 958.8, "end": 964.56, "text": " In Java we have like private functions and everything in Python you don't have to care", "tokens": [51254, 682, 10745, 321, 362, 411, 4551, 6828, 293, 1203, 294, 15329, 291, 500, 380, 362, 281, 1127, 51542], "temperature": 0.0, "avg_logprob": -0.2601460187863081, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.005292763002216816}, {"id": 224, "seek": 94100, "start": 964.56, "end": 969.6, "text": " and that's why you like using Python you can do things that you're not supposed to do to", "tokens": [51542, 293, 300, 311, 983, 291, 411, 1228, 15329, 291, 393, 360, 721, 300, 291, 434, 406, 3442, 281, 360, 281, 51794], "temperature": 0.0, "avg_logprob": -0.2601460187863081, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.005292763002216816}, {"id": 225, "seek": 96960, "start": 969.6, "end": 971.72, "text": " get some change results.", "tokens": [50364, 483, 512, 1319, 3542, 13, 50470], "temperature": 0.0, "avg_logprob": -0.3146973450978597, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.007302265614271164}, {"id": 226, "seek": 96960, "start": 971.72, "end": 978.0400000000001, "text": " So if you want I'm using Python all the time when fixing bugs in the OpenJK to write test", "tokens": [50470, 407, 498, 291, 528, 286, 478, 1228, 15329, 439, 264, 565, 562, 19442, 15120, 294, 264, 7238, 41, 42, 281, 2464, 1500, 50786], "temperature": 0.0, "avg_logprob": -0.3146973450978597, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.007302265614271164}, {"id": 227, "seek": 96960, "start": 978.0400000000001, "end": 985.52, "text": " suite because it's faster in Python than to do in Java so you get the OpenJK, some bugs", "tokens": [50786, 14205, 570, 309, 311, 4663, 294, 15329, 813, 281, 360, 294, 10745, 370, 291, 483, 264, 7238, 41, 42, 11, 512, 15120, 51160], "temperature": 0.0, "avg_logprob": -0.3146973450978597, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.007302265614271164}, {"id": 228, "seek": 96960, "start": 985.52, "end": 989.12, "text": " were fixed because I wrote some weird Python script here.", "tokens": [51160, 645, 6806, 570, 286, 4114, 512, 3657, 15329, 5755, 510, 13, 51340], "temperature": 0.0, "avg_logprob": -0.3146973450978597, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.007302265614271164}, {"id": 229, "seek": 96960, "start": 989.12, "end": 994.8000000000001, "text": " But essentially what I was then going to test is I wanted to check the minimal implementation", "tokens": [51340, 583, 4476, 437, 286, 390, 550, 516, 281, 1500, 307, 286, 1415, 281, 1520, 264, 13206, 11420, 51624], "temperature": 0.0, "avg_logprob": -0.3146973450978597, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.007302265614271164}, {"id": 230, "seek": 96960, "start": 994.8000000000001, "end": 995.8000000000001, "text": " of a debugger.", "tokens": [51624, 295, 257, 24083, 1321, 13, 51674], "temperature": 0.0, "avg_logprob": -0.3146973450978597, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.007302265614271164}, {"id": 231, "seek": 99580, "start": 995.8, "end": 1000.8, "text": " So minimal implementation with such trace, so debugger that doesn't have any breakpoints", "tokens": [50364, 407, 13206, 11420, 365, 1270, 13508, 11, 370, 24083, 1321, 300, 1177, 380, 362, 604, 1821, 20552, 50614], "temperature": 0.0, "avg_logprob": -0.27334839588886983, "compression_ratio": 1.671875, "no_speech_prob": 0.0015899986028671265}, {"id": 232, "seek": 99580, "start": 1000.8, "end": 1008.24, "text": " here is just call handler and then an inner handler calls it every line.", "tokens": [50614, 510, 307, 445, 818, 41967, 293, 550, 364, 7284, 41967, 5498, 309, 633, 1622, 13, 50986], "temperature": 0.0, "avg_logprob": -0.27334839588886983, "compression_ratio": 1.671875, "no_speech_prob": 0.0015899986028671265}, {"id": 233, "seek": 99580, "start": 1008.24, "end": 1017.56, "text": " Then the minimal implementation for monitoring API wouldn't enable any line events because", "tokens": [50986, 1396, 264, 13206, 11420, 337, 11028, 9362, 2759, 380, 9528, 604, 1622, 3931, 570, 51452], "temperature": 0.0, "avg_logprob": -0.27334839588886983, "compression_ratio": 1.671875, "no_speech_prob": 0.0015899986028671265}, {"id": 234, "seek": 99580, "start": 1017.56, "end": 1022.4799999999999, "text": " when we don't set a breakpoint we don't need to set any line events.", "tokens": [51452, 562, 321, 500, 380, 992, 257, 1821, 6053, 321, 500, 380, 643, 281, 992, 604, 1622, 3931, 13, 51698], "temperature": 0.0, "avg_logprob": -0.27334839588886983, "compression_ratio": 1.671875, "no_speech_prob": 0.0015899986028671265}, {"id": 235, "seek": 102248, "start": 1022.48, "end": 1026.84, "text": " So that's how we implement this but I thought like it's a bit sneaky because we're comparing", "tokens": [50364, 407, 300, 311, 577, 321, 4445, 341, 457, 286, 1194, 411, 309, 311, 257, 857, 39518, 570, 321, 434, 15763, 50582], "temperature": 0.0, "avg_logprob": -0.26492971181869507, "compression_ratio": 1.7448559670781894, "no_speech_prob": 0.013646324165165424}, {"id": 236, "seek": 102248, "start": 1026.84, "end": 1031.92, "text": " something that triggers an event that is relying with something that only triggers an event", "tokens": [50582, 746, 300, 22827, 364, 2280, 300, 307, 24140, 365, 746, 300, 787, 22827, 364, 2280, 50836], "temperature": 0.0, "avg_logprob": -0.26492971181869507, "compression_ratio": 1.7448559670781894, "no_speech_prob": 0.013646324165165424}, {"id": 237, "seek": 102248, "start": 1031.92, "end": 1039.0, "text": " every function calls are also made a third comparison with like setting all the line", "tokens": [50836, 633, 2445, 5498, 366, 611, 1027, 257, 2636, 9660, 365, 411, 3287, 439, 264, 1622, 51190], "temperature": 0.0, "avg_logprob": -0.26492971181869507, "compression_ratio": 1.7448559670781894, "no_speech_prob": 0.013646324165165424}, {"id": 238, "seek": 102248, "start": 1039.0, "end": 1044.56, "text": " events and it turns out that's still faster which is quite nice.", "tokens": [51190, 3931, 293, 309, 4523, 484, 300, 311, 920, 4663, 597, 307, 1596, 1481, 13, 51468], "temperature": 0.0, "avg_logprob": -0.26492971181869507, "compression_ratio": 1.7448559670781894, "no_speech_prob": 0.013646324165165424}, {"id": 239, "seek": 102248, "start": 1044.56, "end": 1049.84, "text": " So I use this Py for matchpoint suite that's quite representable and what I found is that", "tokens": [51468, 407, 286, 764, 341, 9953, 337, 2995, 6053, 14205, 300, 311, 1596, 2906, 712, 293, 437, 286, 1352, 307, 300, 51732], "temperature": 0.0, "avg_logprob": -0.26492971181869507, "compression_ratio": 1.7448559670781894, "no_speech_prob": 0.013646324165165424}, {"id": 240, "seek": 104984, "start": 1049.84, "end": 1052.1599999999999, "text": " it's really, really fast.", "tokens": [50364, 309, 311, 534, 11, 534, 2370, 13, 50480], "temperature": 0.0, "avg_logprob": -0.3357855286261048, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.02428770624101162}, {"id": 241, "seek": 104984, "start": 1052.1599999999999, "end": 1060.36, "text": " So the CessetRace when you run it, when you run all the five benchmarks with CessetRace", "tokens": [50480, 407, 264, 383, 442, 302, 49, 617, 562, 291, 1190, 309, 11, 562, 291, 1190, 439, 264, 1732, 43751, 365, 383, 442, 302, 49, 617, 50890], "temperature": 0.0, "avg_logprob": -0.3357855286261048, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.02428770624101162}, {"id": 242, "seek": 104984, "start": 1060.36, "end": 1065.24, "text": " on you have a 3.5 times larger runtime.", "tokens": [50890, 322, 291, 362, 257, 805, 13, 20, 1413, 4833, 34474, 13, 51134], "temperature": 0.0, "avg_logprob": -0.3357855286261048, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.02428770624101162}, {"id": 243, "seek": 104984, "start": 1065.24, "end": 1066.3999999999999, "text": " That's pretty slow.", "tokens": [51134, 663, 311, 1238, 2964, 13, 51192], "temperature": 0.0, "avg_logprob": -0.3357855286261048, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.02428770624101162}, {"id": 244, "seek": 104984, "start": 1066.3999999999999, "end": 1072.76, "text": " When you're using monitoring you only have a runtime increase of like a factor of 1.2", "tokens": [51192, 1133, 291, 434, 1228, 11028, 291, 787, 362, 257, 34474, 3488, 295, 411, 257, 5952, 295, 502, 13, 17, 51510], "temperature": 0.0, "avg_logprob": -0.3357855286261048, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.02428770624101162}, {"id": 245, "seek": 104984, "start": 1072.76, "end": 1079.3999999999999, "text": " which is like 20% slower which is pretty, pretty awesome because this means you can debug", "tokens": [51510, 597, 307, 411, 945, 4, 14009, 597, 307, 1238, 11, 1238, 3476, 570, 341, 1355, 291, 393, 24083, 51842], "temperature": 0.0, "avg_logprob": -0.3357855286261048, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.02428770624101162}, {"id": 246, "seek": 107940, "start": 1079.4, "end": 1085.24, "text": " all your tight loops, you can debug your whole applications without worrying about like", "tokens": [50364, 439, 428, 4524, 16121, 11, 291, 393, 24083, 428, 1379, 5821, 1553, 18788, 466, 411, 50656], "temperature": 0.0, "avg_logprob": -0.3439452550182604, "compression_ratio": 1.5048543689320388, "no_speech_prob": 0.007776114158332348}, {"id": 247, "seek": 107940, "start": 1085.24, "end": 1092.52, "text": " debating slowing down and when you enable all line events it's still 30% faster than", "tokens": [50656, 3001, 990, 26958, 760, 293, 562, 291, 9528, 439, 1622, 3931, 309, 311, 920, 2217, 4, 4663, 813, 51020], "temperature": 0.0, "avg_logprob": -0.3439452550182604, "compression_ratio": 1.5048543689320388, "no_speech_prob": 0.007776114158332348}, {"id": 248, "seek": 107940, "start": 1092.52, "end": 1098.1200000000001, "text": " with CessetRace and people here probably like RAS.", "tokens": [51020, 365, 383, 442, 302, 49, 617, 293, 561, 510, 1391, 411, 497, 3160, 13, 51300], "temperature": 0.0, "avg_logprob": -0.3439452550182604, "compression_ratio": 1.5048543689320388, "no_speech_prob": 0.007776114158332348}, {"id": 249, "seek": 107940, "start": 1098.1200000000001, "end": 1102.52, "text": " That's essentially all the benchmarks that are in Py performance and what you see here", "tokens": [51300, 663, 311, 4476, 439, 264, 43751, 300, 366, 294, 9953, 3389, 293, 437, 291, 536, 510, 51520], "temperature": 0.0, "avg_logprob": -0.3439452550182604, "compression_ratio": 1.5048543689320388, "no_speech_prob": 0.007776114158332348}, {"id": 250, "seek": 110252, "start": 1102.52, "end": 1104.24, "text": " are the orange bars.", "tokens": [50364, 366, 264, 7671, 10228, 13, 50450], "temperature": 0.0, "avg_logprob": -0.21699530903886005, "compression_ratio": 1.546875, "no_speech_prob": 0.3279885947704315}, {"id": 251, "seek": 110252, "start": 1104.24, "end": 1113.84, "text": " These are the bars for the monitoring solutions and here it's just at one so if the bar is", "tokens": [50450, 1981, 366, 264, 10228, 337, 264, 11028, 6547, 293, 510, 309, 311, 445, 412, 472, 370, 498, 264, 2159, 307, 50930], "temperature": 0.0, "avg_logprob": -0.21699530903886005, "compression_ratio": 1.546875, "no_speech_prob": 0.3279885947704315}, {"id": 252, "seek": 110252, "start": 1113.84, "end": 1118.32, "text": " not visible it means you don't have any overhead in this benchmark but essentially you see", "tokens": [50930, 406, 8974, 309, 1355, 291, 500, 380, 362, 604, 19922, 294, 341, 18927, 457, 4476, 291, 536, 51154], "temperature": 0.0, "avg_logprob": -0.21699530903886005, "compression_ratio": 1.546875, "no_speech_prob": 0.3279885947704315}, {"id": 253, "seek": 110252, "start": 1118.32, "end": 1126.2, "text": " that the blue bars for CessetRace can get high, can get up to like 10, 12 so it's really", "tokens": [51154, 300, 264, 3344, 10228, 337, 383, 442, 302, 49, 617, 393, 483, 1090, 11, 393, 483, 493, 281, 411, 1266, 11, 2272, 370, 309, 311, 534, 51548], "temperature": 0.0, "avg_logprob": -0.21699530903886005, "compression_ratio": 1.546875, "no_speech_prob": 0.3279885947704315}, {"id": 254, "seek": 110252, "start": 1126.2, "end": 1127.2, "text": " good.", "tokens": [51548, 665, 13, 51598], "temperature": 0.0, "avg_logprob": -0.21699530903886005, "compression_ratio": 1.546875, "no_speech_prob": 0.3279885947704315}, {"id": 255, "seek": 112720, "start": 1127.64, "end": 1133.04, "text": " At least in my opinion and then when we switch over and use CessetRace monitoring with all", "tokens": [50386, 1711, 1935, 294, 452, 4800, 293, 550, 562, 321, 3679, 670, 293, 764, 383, 442, 302, 49, 617, 11028, 365, 439, 50656], "temperature": 0.0, "avg_logprob": -0.3039759454273042, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.0047116512432694435}, {"id": 256, "seek": 112720, "start": 1133.04, "end": 1140.0, "text": " line events enabled it gets worse but still we see that it's still significantly faster.", "tokens": [50656, 1622, 3931, 15172, 309, 2170, 5324, 457, 920, 321, 536, 300, 309, 311, 920, 10591, 4663, 13, 51004], "temperature": 0.0, "avg_logprob": -0.3039759454273042, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.0047116512432694435}, {"id": 257, "seek": 112720, "start": 1140.0, "end": 1144.8400000000001, "text": " Another question is of course is this whole thing used because it's implemented and I'm", "tokens": [51004, 3996, 1168, 307, 295, 1164, 307, 341, 1379, 551, 1143, 570, 309, 311, 12270, 293, 286, 478, 51246], "temperature": 0.0, "avg_logprob": -0.3039759454273042, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.0047116512432694435}, {"id": 258, "seek": 112720, "start": 1144.8400000000001, "end": 1151.2, "text": " working in OmTek so I noted when you implement a cool feature the chances are that nobody", "tokens": [51246, 1364, 294, 9757, 51, 916, 370, 286, 12964, 562, 291, 4445, 257, 1627, 4111, 264, 10486, 366, 300, 5079, 51564], "temperature": 0.0, "avg_logprob": -0.3039759454273042, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.0047116512432694435}, {"id": 259, "seek": 115120, "start": 1151.2, "end": 1156.72, "text": " will use it for like a year but here in OmTek people started, but here in CPython people", "tokens": [50364, 486, 764, 309, 337, 411, 257, 1064, 457, 510, 294, 9757, 51, 916, 561, 1409, 11, 457, 510, 294, 22431, 88, 11943, 561, 50640], "temperature": 0.0, "avg_logprob": -0.28237095333281015, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.01975560188293457}, {"id": 260, "seek": 115120, "start": 1156.72, "end": 1161.8400000000001, "text": " started using it especially the vendors like PyCharm.", "tokens": [50640, 1409, 1228, 309, 2318, 264, 22056, 411, 9953, 6546, 4452, 13, 50896], "temperature": 0.0, "avg_logprob": -0.28237095333281015, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.01975560188293457}, {"id": 261, "seek": 115120, "start": 1161.8400000000001, "end": 1170.44, "text": " So it's not yet used in PVP and they're showing the second pie but IDEs like PyCharm with their", "tokens": [50896, 407, 309, 311, 406, 1939, 1143, 294, 23035, 47, 293, 436, 434, 4099, 264, 1150, 1730, 457, 7348, 20442, 411, 9953, 6546, 4452, 365, 641, 51326], "temperature": 0.0, "avg_logprob": -0.28237095333281015, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.01975560188293457}, {"id": 262, "seek": 115120, "start": 1170.44, "end": 1178.1200000000001, "text": " version 2093.3 use it and they've seen significant performance improvements.", "tokens": [51326, 3037, 945, 26372, 13, 18, 764, 309, 293, 436, 600, 1612, 4776, 3389, 13797, 13, 51710], "temperature": 0.0, "avg_logprob": -0.28237095333281015, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.01975560188293457}, {"id": 263, "seek": 117812, "start": 1178.12, "end": 1183.9599999999998, "text": " And there's currently a pending pull request on GitHub so if you want to help PVP implement", "tokens": [50364, 400, 456, 311, 4362, 257, 32110, 2235, 5308, 322, 23331, 370, 498, 291, 528, 281, 854, 23035, 47, 4445, 50656], "temperature": 0.0, "avg_logprob": -0.303448729462676, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.039753805845975876}, {"id": 264, "seek": 117812, "start": 1183.9599999999998, "end": 1188.7199999999998, "text": " it go to this pull request and try another discussion here so I would really recommend", "tokens": [50656, 309, 352, 281, 341, 2235, 5308, 293, 853, 1071, 5017, 510, 370, 286, 576, 534, 2748, 50894], "temperature": 0.0, "avg_logprob": -0.303448729462676, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.039753805845975876}, {"id": 265, "seek": 117812, "start": 1188.7199999999998, "end": 1194.8, "text": " it's an open source project to CPython and you can make PVP better so what not to like", "tokens": [50894, 309, 311, 364, 1269, 4009, 1716, 281, 22431, 88, 11943, 293, 291, 393, 652, 23035, 47, 1101, 370, 437, 406, 281, 411, 51198], "temperature": 0.0, "avg_logprob": -0.303448729462676, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.039753805845975876}, {"id": 266, "seek": 117812, "start": 1194.8, "end": 1197.76, "text": " is simple.", "tokens": [51198, 307, 2199, 13, 51346], "temperature": 0.0, "avg_logprob": -0.303448729462676, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.039753805845975876}, {"id": 267, "seek": 117812, "start": 1197.76, "end": 1202.9199999999998, "text": " Here's a quote from the pull request from Chen Gao who wrote this pull request and he wrote", "tokens": [51346, 1692, 311, 257, 6513, 490, 264, 2235, 5308, 490, 13682, 32235, 567, 4114, 341, 2235, 5308, 293, 415, 4114, 51604], "temperature": 0.0, "avg_logprob": -0.303448729462676, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.039753805845975876}, {"id": 268, "seek": 120292, "start": 1202.92, "end": 1207.6000000000001, "text": " after this change we will have the chance to build a much faster debugger for break", "tokens": [50364, 934, 341, 1319, 321, 486, 362, 264, 2931, 281, 1322, 257, 709, 4663, 24083, 1321, 337, 1821, 50598], "temperature": 0.0, "avg_logprob": -0.24577517122835726, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.11212865263223648}, {"id": 269, "seek": 120292, "start": 1207.6000000000001, "end": 1211.64, "text": " points we don't need to trigger trace functions all the time and change for a line number", "tokens": [50598, 2793, 321, 500, 380, 643, 281, 7875, 13508, 6828, 439, 264, 565, 293, 1319, 337, 257, 1622, 1230, 50800], "temperature": 0.0, "avg_logprob": -0.24577517122835726, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.11212865263223648}, {"id": 270, "seek": 120292, "start": 1211.64, "end": 1213.44, "text": " so I'll show it to you.", "tokens": [50800, 370, 286, 603, 855, 309, 281, 291, 13, 50890], "temperature": 0.0, "avg_logprob": -0.24577517122835726, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.11212865263223648}, {"id": 271, "seek": 120292, "start": 1213.44, "end": 1218.0800000000002, "text": " The bad news is it's almost impossible to do a completely backward compatible transition", "tokens": [50890, 440, 1578, 2583, 307, 309, 311, 1920, 6243, 281, 360, 257, 2584, 23897, 18218, 6034, 51122], "temperature": 0.0, "avg_logprob": -0.24577517122835726, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.11212865263223648}, {"id": 272, "seek": 120292, "start": 1218.0800000000002, "end": 1221.16, "text": " because the mechanism is quite different.", "tokens": [51122, 570, 264, 7513, 307, 1596, 819, 13, 51276], "temperature": 0.0, "avg_logprob": -0.24577517122835726, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.11212865263223648}, {"id": 273, "seek": 120292, "start": 1221.16, "end": 1226.24, "text": " So there's an ongoing discussion how to do this.", "tokens": [51276, 407, 456, 311, 364, 10452, 5017, 577, 281, 360, 341, 13, 51530], "temperature": 0.0, "avg_logprob": -0.24577517122835726, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.11212865263223648}, {"id": 274, "seek": 120292, "start": 1226.24, "end": 1231.16, "text": " You could take part there so scan this QR code, be part of the community, give something", "tokens": [51530, 509, 727, 747, 644, 456, 370, 11049, 341, 32784, 3089, 11, 312, 644, 295, 264, 1768, 11, 976, 746, 51776], "temperature": 0.0, "avg_logprob": -0.24577517122835726, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.11212865263223648}, {"id": 275, "seek": 123116, "start": 1231.16, "end": 1236.96, "text": " back and not just use CPython.", "tokens": [50364, 646, 293, 406, 445, 764, 22431, 88, 11943, 13, 50654], "temperature": 0.0, "avg_logprob": -0.43402936117989677, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.07548665255308151}, {"id": 276, "seek": 123116, "start": 1236.96, "end": 1243.0800000000002, "text": " So because I have like tiny tiny town left I want to just show you shortly how single", "tokens": [50654, 407, 570, 286, 362, 411, 5870, 5870, 3954, 1411, 286, 528, 281, 445, 855, 291, 13392, 577, 2167, 50960], "temperature": 0.0, "avg_logprob": -0.43402936117989677, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.07548665255308151}, {"id": 277, "seek": 123116, "start": 1243.0800000000002, "end": 1247.0800000000002, "text": " stepping works because single stepping is just break points because essentially the", "tokens": [50960, 16821, 1985, 570, 2167, 16821, 307, 445, 1821, 2793, 570, 4476, 264, 51160], "temperature": 0.0, "avg_logprob": -0.43402936117989677, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.07548665255308151}, {"id": 278, "seek": 123116, "start": 1247.0800000000002, "end": 1253.2, "text": " idea is here when we have always take here with the Scona and step out of this for example", "tokens": [51160, 1558, 307, 510, 562, 321, 362, 1009, 747, 510, 365, 264, 318, 1671, 64, 293, 1823, 484, 295, 341, 337, 1365, 51466], "temperature": 0.0, "avg_logprob": -0.43402936117989677, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.07548665255308151}, {"id": 279, "seek": 125320, "start": 1253.2, "end": 1261.64, "text": " we just check for the next line where only the frame before changed like the current", "tokens": [50364, 321, 445, 1520, 337, 264, 958, 1622, 689, 787, 264, 3920, 949, 3105, 411, 264, 2190, 50786], "temperature": 0.0, "avg_logprob": -0.2951288426176031, "compression_ratio": 1.759433962264151, "no_speech_prob": 0.20349739491939545}, {"id": 280, "seek": 125320, "start": 1261.64, "end": 1264.68, "text": " code lines changed.", "tokens": [50786, 3089, 3876, 3105, 13, 50938], "temperature": 0.0, "avg_logprob": -0.2951288426176031, "compression_ratio": 1.759433962264151, "no_speech_prob": 0.20349739491939545}, {"id": 281, "seek": 125320, "start": 1264.68, "end": 1268.72, "text": " Stepping is also pretty simple we just check that only like the line number changed it's", "tokens": [50938, 3592, 3759, 307, 611, 1238, 2199, 321, 445, 1520, 300, 787, 411, 264, 1622, 1230, 3105, 309, 311, 51140], "temperature": 0.0, "avg_logprob": -0.2951288426176031, "compression_ratio": 1.759433962264151, "no_speech_prob": 0.20349739491939545}, {"id": 282, "seek": 125320, "start": 1268.72, "end": 1275.4, "text": " also nice and then check in to is we check the next time where we just put the frame", "tokens": [51140, 611, 1481, 293, 550, 1520, 294, 281, 307, 321, 1520, 264, 958, 565, 689, 321, 445, 829, 264, 3920, 51474], "temperature": 0.0, "avg_logprob": -0.2951288426176031, "compression_ratio": 1.759433962264151, "no_speech_prob": 0.20349739491939545}, {"id": 283, "seek": 125320, "start": 1275.4, "end": 1276.4, "text": " on top.", "tokens": [51474, 322, 1192, 13, 51524], "temperature": 0.0, "avg_logprob": -0.2951288426176031, "compression_ratio": 1.759433962264151, "no_speech_prob": 0.20349739491939545}, {"id": 284, "seek": 125320, "start": 1276.4, "end": 1282.16, "text": " So that's all from me I'm part of Northern Twitter you can find my team at sub machine", "tokens": [51524, 407, 300, 311, 439, 490, 385, 286, 478, 644, 295, 14335, 5794, 291, 393, 915, 452, 1469, 412, 1422, 3479, 51812], "temperature": 0.0, "avg_logprob": -0.2951288426176031, "compression_ratio": 1.759433962264151, "no_speech_prob": 0.20349739491939545}, {"id": 285, "seek": 128216, "start": 1282.24, "end": 1287.5600000000002, "text": " A O so if you want to use a JVM use the sub machine it's the best JVM.", "tokens": [50368, 316, 422, 370, 498, 291, 528, 281, 764, 257, 508, 53, 44, 764, 264, 1422, 3479, 309, 311, 264, 1151, 508, 53, 44, 13, 50634], "temperature": 0.0, "avg_logprob": -0.384520069936688, "compression_ratio": 1.4095238095238096, "no_speech_prob": 0.025161823257803917}, {"id": 286, "seek": 128216, "start": 1287.5600000000002, "end": 1292.3200000000002, "text": " I'm contractually obliged to say this.", "tokens": [50634, 286, 478, 4364, 671, 47194, 281, 584, 341, 13, 50872], "temperature": 0.0, "avg_logprob": -0.384520069936688, "compression_ratio": 1.4095238095238096, "no_speech_prob": 0.025161823257803917}, {"id": 287, "seek": 128216, "start": 1292.3200000000002, "end": 1302.28, "text": " We work at SAP we're one of the many cool open source projects at SAP you can follow", "tokens": [50872, 492, 589, 412, 27743, 321, 434, 472, 295, 264, 867, 1627, 1269, 4009, 4455, 412, 27743, 291, 393, 1524, 51370], "temperature": 0.0, "avg_logprob": -0.384520069936688, "compression_ratio": 1.4095238095238096, "no_speech_prob": 0.025161823257803917}, {"id": 288, "seek": 128216, "start": 1302.28, "end": 1307.2, "text": " me on my blog where I write on DBegging, EBPS stuff and everything else.", "tokens": [51370, 385, 322, 452, 6968, 689, 286, 2464, 322, 26754, 1146, 3249, 11, 50148, 6273, 1507, 293, 1203, 1646, 13, 51616], "temperature": 0.0, "avg_logprob": -0.384520069936688, "compression_ratio": 1.4095238095238096, "no_speech_prob": 0.025161823257803917}, {"id": 289, "seek": 128216, "start": 1307.2, "end": 1309.2, "text": " So thank you for being here.", "tokens": [51616, 407, 1309, 291, 337, 885, 510, 13, 51716], "temperature": 0.0, "avg_logprob": -0.384520069936688, "compression_ratio": 1.4095238095238096, "no_speech_prob": 0.025161823257803917}, {"id": 290, "seek": 131216, "start": 1312.16, "end": 1322.68, "text": " Thank you very much Johans we have time for probably two three questions maybe.", "tokens": [50364, 1044, 291, 588, 709, 19180, 599, 321, 362, 565, 337, 1391, 732, 1045, 1651, 1310, 13, 50890], "temperature": 0.0, "avg_logprob": -0.43296215057373044, "compression_ratio": 1.4701492537313432, "no_speech_prob": 0.02795562334358692}, {"id": 291, "seek": 131216, "start": 1322.68, "end": 1325.68, "text": " Does anyone we have one there?", "tokens": [50890, 4402, 2878, 321, 362, 472, 456, 30, 51040], "temperature": 0.0, "avg_logprob": -0.43296215057373044, "compression_ratio": 1.4701492537313432, "no_speech_prob": 0.02795562334358692}, {"id": 292, "seek": 131216, "start": 1325.68, "end": 1340.96, "text": " Thank you very much for this talk and for this pep because it actually solves a lot of", "tokens": [51040, 1044, 291, 588, 709, 337, 341, 751, 293, 337, 341, 520, 79, 570, 309, 767, 39890, 257, 688, 295, 51804], "temperature": 0.0, "avg_logprob": -0.43296215057373044, "compression_ratio": 1.4701492537313432, "no_speech_prob": 0.02795562334358692}, {"id": 293, "seek": 134096, "start": 1340.96, "end": 1346.24, "text": " problems I had when I started back in the days developing a tool for performance analysis", "tokens": [50364, 2740, 286, 632, 562, 286, 1409, 646, 294, 264, 1708, 6416, 257, 2290, 337, 3389, 5215, 50628], "temperature": 0.0, "avg_logprob": -0.30758945240693936, "compression_ratio": 1.64, "no_speech_prob": 0.4760541021823883}, {"id": 294, "seek": 134096, "start": 1346.24, "end": 1347.68, "text": " for Python.", "tokens": [50628, 337, 15329, 13, 50700], "temperature": 0.0, "avg_logprob": -0.30758945240693936, "compression_ratio": 1.64, "no_speech_prob": 0.4760541021823883}, {"id": 295, "seek": 134096, "start": 1347.68, "end": 1356.88, "text": " However at some point choose to use the C interface of set rays and profiling and whatever", "tokens": [50700, 2908, 412, 512, 935, 2826, 281, 764, 264, 383, 9226, 295, 992, 24417, 293, 1740, 4883, 293, 2035, 51160], "temperature": 0.0, "avg_logprob": -0.30758945240693936, "compression_ratio": 1.64, "no_speech_prob": 0.4760541021823883}, {"id": 296, "seek": 134096, "start": 1356.88, "end": 1363.24, "text": " do your does your proposal as I said is implemented already also support the C interface?", "tokens": [51160, 360, 428, 775, 428, 11494, 382, 286, 848, 307, 12270, 1217, 611, 1406, 264, 383, 9226, 30, 51478], "temperature": 0.0, "avg_logprob": -0.30758945240693936, "compression_ratio": 1.64, "no_speech_prob": 0.4760541021823883}, {"id": 297, "seek": 134096, "start": 1363.24, "end": 1369.56, "text": " So I have to correct I'm not I have nothing to do with the nice people who implemented", "tokens": [51478, 407, 286, 362, 281, 3006, 286, 478, 406, 286, 362, 1825, 281, 360, 365, 264, 1481, 561, 567, 12270, 51794], "temperature": 0.0, "avg_logprob": -0.30758945240693936, "compression_ratio": 1.64, "no_speech_prob": 0.4760541021823883}, {"id": 298, "seek": 136956, "start": 1369.6799999999998, "end": 1375.96, "text": " this sorry but so please ask them they're probably in some discord somewhere I'm just", "tokens": [50370, 341, 2597, 457, 370, 1767, 1029, 552, 436, 434, 1391, 294, 512, 32989, 4079, 286, 478, 445, 50684], "temperature": 0.0, "avg_logprob": -0.2863023055227179, "compression_ratio": 1.5163934426229508, "no_speech_prob": 0.16392400860786438}, {"id": 299, "seek": 136956, "start": 1375.96, "end": 1381.04, "text": " telling you about the good news because programmers usually don't want to go to conferences and", "tokens": [50684, 3585, 291, 466, 264, 665, 2583, 570, 41504, 2673, 500, 380, 528, 281, 352, 281, 22032, 293, 50938], "temperature": 0.0, "avg_logprob": -0.2863023055227179, "compression_ratio": 1.5163934426229508, "no_speech_prob": 0.16392400860786438}, {"id": 300, "seek": 136956, "start": 1381.04, "end": 1386.52, "text": " speak in front of people so that's why I'll be giving talks on this.", "tokens": [50938, 1710, 294, 1868, 295, 561, 370, 300, 311, 983, 286, 603, 312, 2902, 6686, 322, 341, 13, 51212], "temperature": 0.0, "avg_logprob": -0.2863023055227179, "compression_ratio": 1.5163934426229508, "no_speech_prob": 0.16392400860786438}, {"id": 301, "seek": 136956, "start": 1386.52, "end": 1388.8, "text": " So sadly I don't know.", "tokens": [51212, 407, 22023, 286, 500, 380, 458, 13, 51326], "temperature": 0.0, "avg_logprob": -0.2863023055227179, "compression_ratio": 1.5163934426229508, "no_speech_prob": 0.16392400860786438}, {"id": 302, "seek": 136956, "start": 1388.8, "end": 1392.9199999999998, "text": " Thank you do we have any more questions to Johans?", "tokens": [51326, 1044, 291, 360, 321, 362, 604, 544, 1651, 281, 19180, 599, 30, 51532], "temperature": 0.0, "avg_logprob": -0.2863023055227179, "compression_ratio": 1.5163934426229508, "no_speech_prob": 0.16392400860786438}, {"id": 303, "seek": 136956, "start": 1392.9199999999998, "end": 1396.52, "text": " Can raise your hand.", "tokens": [51532, 1664, 5300, 428, 1011, 13, 51712], "temperature": 0.0, "avg_logprob": -0.2863023055227179, "compression_ratio": 1.5163934426229508, "no_speech_prob": 0.16392400860786438}, {"id": 304, "seek": 136956, "start": 1396.52, "end": 1398.28, "text": " No questions apparently.", "tokens": [51712, 883, 1651, 7970, 13, 51800], "temperature": 0.0, "avg_logprob": -0.2863023055227179, "compression_ratio": 1.5163934426229508, "no_speech_prob": 0.16392400860786438}, {"id": 305, "seek": 139828, "start": 1398.32, "end": 1405.08, "text": " I just want to choose this opportunity to thank Mark Andre and David also known as Flypeg", "tokens": [50366, 286, 445, 528, 281, 2826, 341, 2650, 281, 1309, 3934, 20667, 293, 4389, 611, 2570, 382, 25294, 494, 70, 50704], "temperature": 0.0, "avg_logprob": -0.3643830039284446, "compression_ratio": 1.3245033112582782, "no_speech_prob": 0.025571491569280624}, {"id": 306, "seek": 139828, "start": 1405.08, "end": 1407.16, "text": " for organizing this dev room.", "tokens": [50704, 337, 17608, 341, 1905, 1808, 13, 50808], "temperature": 0.0, "avg_logprob": -0.3643830039284446, "compression_ratio": 1.3245033112582782, "no_speech_prob": 0.025571491569280624}, {"id": 307, "seek": 139828, "start": 1407.16, "end": 1409.08, "text": " You guys did an amazing work.", "tokens": [50808, 509, 1074, 630, 364, 2243, 589, 13, 50904], "temperature": 0.0, "avg_logprob": -0.3643830039284446, "compression_ratio": 1.3245033112582782, "no_speech_prob": 0.025571491569280624}, {"id": 308, "seek": 139828, "start": 1409.08, "end": 1417.8, "text": " Thank you very very much.", "tokens": [50904, 1044, 291, 588, 588, 709, 13, 51340], "temperature": 0.0, "avg_logprob": -0.3643830039284446, "compression_ratio": 1.3245033112582782, "no_speech_prob": 0.025571491569280624}, {"id": 309, "seek": 139828, "start": 1417.8, "end": 1419.12, "text": " And thanks Johans again.", "tokens": [51340, 400, 3231, 19180, 599, 797, 13, 51406], "temperature": 0.0, "avg_logprob": -0.3643830039284446, "compression_ratio": 1.3245033112582782, "no_speech_prob": 0.025571491569280624}], "language": "en"}