{"text": " So people can hurry and sit down for the next speaker please. Okay, thanks for our next talk. We have met in talking about semantically driven data management solution for IO intensive HPC workflows. Thank you. My name is Metin Chakrachalov. I work at the European Center for Medium Range Weather Forecasts Department, Forecasts and Services Department at ECMWF. I will talk about the semantically driven data management solution for IO intensive HPC workflows, which the work was funded by the EUR HPC project called IOC. It is work done by many people. So a little bit background on the ECMWF, European Center for Medium Range Weather Forecasts. It is established in 1975 by 23 member states and 12 cooperating states as an intergovernmental organization. There are three base duty stations with more than 450 people, Redding, Great Britain, in Germany, Bonn and Bologna, Italy. So ECMWF is both a research institution and 24-7 operational services, producing numerical weather predictions and other data to member states. There are two big projects that ECMWF is a key player. One is Copernicus. It is the Earth Observation component of the EU's space program. We provide climate change information, atmospheric composition information and also flooding and fire danger information. The other big initiative, EU initiative, is the Destination Earth project, which is prototyping digital twins of the Earth. So the ECMWF's production workflow looks like this. There are per day 200 million observations, collected acquisitions and fed into the Earth system model. Those observations and the output from the weather predictions are archived. Also, these data are used to generate products, which are 300 terabytes of data per day, which then accounts for 65 terabytes of data per day as products disseminated to around 350 destinations to member states and other customers. So the information system, the data is central. It provides access to data, models and workflows, and the data management is very critical for the operations. We need transformation of data into information, insights and decisions. So, semantically driven data management, we have been doing this for a long time. It means managing data based on its meaningful logical description, rather than just storing data. We also abstract the backend technologies. We also abstract where and how the data is stored from the users. So instead of, we try to avoid nested folder structures or UIDs, such as this home user projects ECMWF and blah, blah, blah, or some cryptic UIDs that doesn't make much sense to the user. Instead, we want to use meaningful, scientifically meaningful metadata to describe the data. For example, in this case, this project is ECMWF experiment number 42. The data is 224 parameter pressure and level. So for that, as part of the IOC project, we developed DAISY, data access and storage interface. So we provide, we index and identify data using its meaningful description. And for that also, that allows us to implement optimized algorithms to retrieve archive and retrieve data. And this is based on the ECMWF, ECMWF object store called FTB, which is also free and open source on GitHub. And this abstracts, we also abstract the storage technologies behind POSIX. We support POSIX, DAOS, Moto, and Ceph. And we provide interfaces and tools as well as CEC and Python APIs. So the schema, the main complexity is the schema which describes the database. And it is a collection of rules and each rule is a tree of attributes. In this example, I have a schema file and inside that I have two rules and each rule is consisting of multiple parameters. For example, here project experiment date parameter level would translate to a key project ECMWF experiment 42 and so on. The other rule is event city year and this could translate into event for stem, city is Brussels and year is 2024. So the rules are, the rules have, the rules are blueprints of the database, how to construct the database. And they have three levels and they, each level can have multiple attributes. And to make a rule, it has to be unique and complete to describe the data so that we can identify data from other data. And we also need to think about the locality where data, where different data is related to it, we would like to store them together. So each level here, we can think of the first level as directory, the second level file level, and the third level as the indexes in the file. So the locality would be increased when we go deeper in the level. The other, we can set daisy, we can set up daisy by the configuration file in YAML configuration file. We can point to the schema file to find the schema file and we can set the backend storage technology by saying file in this case is reference. We can also have different parts to the databases. We can have multiple databases. It's called roots. And we can set multiple behavior to individual roots. So aside from data, we also need key and we also have query. The keys would refer to single objects while queries can be any number of objects. In this case, key defines, identifies, and single objects on the right, I have level as a list of values, 0, 1, and 3. So it means I make a query for three different data where the differences, the levels are 0, 1, and 3. So we provide multiple interfaces, command line tools, C, C++, and Python APIs. But here I present an example for Python API because it's simplest. So I need, for storing a data by key, I need a key and data. So data can be anything in this case. I just put a string here, but it can be PNG file or PDF file or any other type of data. Then I make a key. User is met in project is IOC. Date is 2023 and city is born. And I pass this key and data to Daisy and Daisy would archive it. Then the other main feature is list, searching for data in the database. I need to make a query, in this case, user met in project IOC. And in this case, I just want two data objects for two different dates. And I pass this query to Daisy and it returns me the keys that I need for retrieving. And in the next example, I have the retrieve getting a key, getting data by a key. I make a key, user is met in project IOC and so on. And I pass this key to Daisy and retrieve the data. So it's very simple. So to sum it up, we describe data semantically instead of your IDs and nest directories. And we index and identify data by its meaningful semantic information. And this also allows us fast and efficient retrieve and search and archive algorithms. Also, we abstract where how data is stored from the user. And we make blueprints called rules. And we make keys to attach to the data and pass it to Daisy. And Daisy would store and manage the data using multiple different storage technologies. So more about Daisy, we have, we published, Daisy is free and open source. We published on GitHub. We have example C API and Python API. We also provide binary packages on GitHub for C, C++ as well as Python. We also have Python packages for Linux, RPM and the beam packages are available. We also have documentation on read to docs. And that's all. Thank you for your attention. APPLAUSE Thank you. Do you have any questions for Metin? No? Oh, there is one. Thank you. Hey, next presentation. I was wondering if you can specify the type of the values in your SEMA. You mean integer? Yeah, yeah, like that. To facilitate the queries. Yes, attributes can have types. You can set integer, date, string, they can have multiple types. Okay, thanks. Hi, thank you for your talk. I was interested in indexes because you mentioned that you index and identify data. Did some standard type of indexes or you have some format on your own, optimized for this three type of data? Yes, indexing is based on the rules. So the rules here, we have three structures which has three levels. So it has to have three levels which translates to a directory file and data. And we have in-house mechanism algorithm to that indexes that translates this three into identifying data. So is it something like gene indexes? I couldn't hear. If it is something like gene indexes? I'm not sure if I understand. Gene index? Yeah, I'm not the right person, I think, because we use the FTB which has been developed since long time. And it's a big library. I cannot answer the question because I haven't worked with that level. No problem, thank you. Thank you for the talk. I would like to know where are these keys stored because you need to query this kind of index and where are they stored for the user? Yeah, so the indexes would be stored separately but together with the data. And they would go, for example, in this case, inside the roots. So each root would be different database. And if you would look inside the root one, output one, for example, here, you would have index keys inside as well as the data, so together. Okay, so it's a file system or a database stored inside these two directories? Yeah, for POSIX, it would be directory, but for object storage, it would be not directly contained or something like that. Okay, so the way how the index is stored depends on the type of the storage you describe here. Yes, but we can also have, we have two different abstractions. One is indexing and we call it catalog and we have a bulk data. So we can have indexing catalog inside POSIX directory and bulk data on an object store. Okay, thank you. Any more questions? Is the next speaker in the room? Okay, thank you again, Metin. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 27.0, "text": " So people can hurry and sit down for the next speaker please.", "tokens": [50364, 407, 561, 393, 11025, 293, 1394, 760, 337, 264, 958, 8145, 1767, 13, 51714], "temperature": 0.0, "avg_logprob": -0.5841427410350126, "compression_ratio": 0.9384615384615385, "no_speech_prob": 0.3920242190361023}, {"id": 1, "seek": 2700, "start": 27.0, "end": 56.0, "text": " Okay, thanks for our next talk. We have met in talking about semantically driven data management solution for IO intensive HPC workflows.", "tokens": [50364, 1033, 11, 3231, 337, 527, 958, 751, 13, 492, 362, 1131, 294, 1417, 466, 4361, 49505, 9555, 1412, 4592, 3827, 337, 39839, 18957, 12557, 34, 43461, 13, 51814], "temperature": 0.0, "avg_logprob": -0.29254350662231443, "compression_ratio": 1.191304347826087, "no_speech_prob": 0.4184078574180603}, {"id": 2, "seek": 5700, "start": 57.0, "end": 75.0, "text": " Thank you. My name is Metin Chakrachalov. I work at the European Center for Medium Range Weather Forecasts Department, Forecasts and Services Department at ECMWF.", "tokens": [50364, 1044, 291, 13, 1222, 1315, 307, 6377, 259, 761, 514, 81, 608, 304, 5179, 13, 286, 589, 412, 264, 6473, 5169, 337, 38915, 33778, 34441, 9018, 3734, 82, 5982, 11, 9018, 3734, 82, 293, 12124, 5982, 412, 19081, 44, 54, 37, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3490158163982889, "compression_ratio": 1.2755905511811023, "no_speech_prob": 0.5615106821060181}, {"id": 3, "seek": 7500, "start": 75.0, "end": 92.0, "text": " I will talk about the semantically driven data management solution for IO intensive HPC workflows, which the work was funded by the EUR HPC project called IOC.", "tokens": [50364, 286, 486, 751, 466, 264, 4361, 49505, 9555, 1412, 4592, 3827, 337, 39839, 18957, 12557, 34, 43461, 11, 597, 264, 589, 390, 14385, 538, 264, 462, 7932, 12557, 34, 1716, 1219, 286, 30087, 13, 51214], "temperature": 0.0, "avg_logprob": -0.21834493938245272, "compression_ratio": 1.2230769230769232, "no_speech_prob": 0.7496041059494019}, {"id": 4, "seek": 9200, "start": 93.0, "end": 98.0, "text": " It is work done by many people.", "tokens": [50414, 467, 307, 589, 1096, 538, 867, 561, 13, 50664], "temperature": 0.0, "avg_logprob": -0.35972849527994794, "compression_ratio": 0.7948717948717948, "no_speech_prob": 0.6559581160545349}, {"id": 5, "seek": 9800, "start": 99.0, "end": 107.0, "text": " So a little bit background on the ECMWF, European Center for Medium Range Weather Forecasts.", "tokens": [50414, 407, 257, 707, 857, 3678, 322, 264, 19081, 44, 54, 37, 11, 6473, 5169, 337, 38915, 33778, 34441, 9018, 3734, 82, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1294644511475855, "compression_ratio": 1.2784810126582278, "no_speech_prob": 0.8166124224662781}, {"id": 6, "seek": 9800, "start": 107.0, "end": 122.0, "text": " It is established in 1975 by 23 member states and 12 cooperating states as an intergovernmental organization.", "tokens": [50814, 467, 307, 7545, 294, 32454, 538, 6673, 4006, 4368, 293, 2272, 13414, 990, 4368, 382, 364, 728, 35329, 15875, 4475, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1294644511475855, "compression_ratio": 1.2784810126582278, "no_speech_prob": 0.8166124224662781}, {"id": 7, "seek": 12200, "start": 122.0, "end": 141.0, "text": " There are three base duty stations with more than 450 people, Redding, Great Britain, in Germany, Bonn and Bologna, Italy.", "tokens": [50364, 821, 366, 1045, 3096, 9776, 13390, 365, 544, 813, 26034, 561, 11, 4477, 3584, 11, 3769, 12960, 11, 294, 7244, 11, 7368, 77, 293, 363, 1132, 629, 11, 10705, 13, 51314], "temperature": 0.0, "avg_logprob": -0.21096372604370117, "compression_ratio": 1.1090909090909091, "no_speech_prob": 0.528738796710968}, {"id": 8, "seek": 14100, "start": 142.0, "end": 162.0, "text": " So ECMWF is both a research institution and 24-7 operational services, producing numerical weather predictions and other data to member states.", "tokens": [50414, 407, 19081, 44, 54, 37, 307, 1293, 257, 2132, 7818, 293, 4022, 12, 22, 16607, 3328, 11, 10501, 29054, 5503, 21264, 293, 661, 1412, 281, 4006, 4368, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17484842240810394, "compression_ratio": 1.2016806722689075, "no_speech_prob": 0.3324889540672302}, {"id": 9, "seek": 16200, "start": 163.0, "end": 184.0, "text": " There are two big projects that ECMWF is a key player. One is Copernicus. It is the Earth Observation component of the EU's space program.", "tokens": [50414, 821, 366, 732, 955, 4455, 300, 19081, 44, 54, 37, 307, 257, 2141, 4256, 13, 1485, 307, 11579, 1248, 36496, 13, 467, 307, 264, 4755, 20707, 6864, 6542, 295, 264, 10887, 311, 1901, 1461, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1392045387854943, "compression_ratio": 1.1596638655462186, "no_speech_prob": 0.41980093717575073}, {"id": 10, "seek": 18400, "start": 185.0, "end": 203.0, "text": " We provide climate change information, atmospheric composition information and also flooding and fire danger information.", "tokens": [50414, 492, 2893, 5659, 1319, 1589, 11, 28854, 12686, 1589, 293, 611, 24132, 293, 2610, 4330, 1589, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1664696420942034, "compression_ratio": 1.3595505617977528, "no_speech_prob": 0.2852034270763397}, {"id": 11, "seek": 20300, "start": 204.0, "end": 213.0, "text": " The other big initiative, EU initiative, is the Destination Earth project, which is prototyping digital twins of the Earth.", "tokens": [50414, 440, 661, 955, 11552, 11, 10887, 11552, 11, 307, 264, 16339, 2486, 4755, 1716, 11, 597, 307, 46219, 3381, 4562, 22555, 295, 264, 4755, 13, 50864], "temperature": 0.0, "avg_logprob": -0.209844784303145, "compression_ratio": 1.3461538461538463, "no_speech_prob": 0.2255508452653885}, {"id": 12, "seek": 20300, "start": 214.0, "end": 218.0, "text": " So the ECMWF's production workflow looks like this.", "tokens": [50914, 407, 264, 19081, 44, 54, 37, 311, 4265, 20993, 1542, 411, 341, 13, 51114], "temperature": 0.0, "avg_logprob": -0.209844784303145, "compression_ratio": 1.3461538461538463, "no_speech_prob": 0.2255508452653885}, {"id": 13, "seek": 21800, "start": 218.0, "end": 230.0, "text": " There are per day 200 million observations, collected acquisitions and fed into the Earth system model.", "tokens": [50364, 821, 366, 680, 786, 2331, 2459, 18163, 11, 11087, 17883, 2451, 293, 4636, 666, 264, 4755, 1185, 2316, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2571337348536441, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.10112796723842621}, {"id": 14, "seek": 21800, "start": 230.0, "end": 238.0, "text": " Those observations and the output from the weather predictions are archived.", "tokens": [50964, 3950, 18163, 293, 264, 5598, 490, 264, 5503, 21264, 366, 3912, 3194, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2571337348536441, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.10112796723842621}, {"id": 15, "seek": 23800, "start": 238.0, "end": 263.0, "text": " Also, these data are used to generate products, which are 300 terabytes of data per day, which then accounts for 65 terabytes of data per day as products disseminated to around 350 destinations to member states and other customers.", "tokens": [50364, 2743, 11, 613, 1412, 366, 1143, 281, 8460, 3383, 11, 597, 366, 6641, 1796, 24538, 295, 1412, 680, 786, 11, 597, 550, 9402, 337, 11624, 1796, 24538, 295, 1412, 680, 786, 382, 3383, 34585, 770, 281, 926, 18065, 37787, 281, 4006, 4368, 293, 661, 4581, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15089630126953124, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.10481242835521698}, {"id": 16, "seek": 26300, "start": 264.0, "end": 279.0, "text": " So the information system, the data is central. It provides access to data, models and workflows, and the data management is very critical for the operations.", "tokens": [50414, 407, 264, 1589, 1185, 11, 264, 1412, 307, 5777, 13, 467, 6417, 2105, 281, 1412, 11, 5245, 293, 43461, 11, 293, 264, 1412, 4592, 307, 588, 4924, 337, 264, 7705, 13, 51164], "temperature": 0.0, "avg_logprob": -0.21517353057861327, "compression_ratio": 1.3504273504273505, "no_speech_prob": 0.06434163451194763}, {"id": 17, "seek": 27900, "start": 280.0, "end": 288.0, "text": " We need transformation of data into information, insights and decisions.", "tokens": [50414, 492, 643, 9887, 295, 1412, 666, 1589, 11, 14310, 293, 5327, 13, 50814], "temperature": 0.0, "avg_logprob": -0.20006617903709412, "compression_ratio": 1.0746268656716418, "no_speech_prob": 0.1627333164215088}, {"id": 18, "seek": 28800, "start": 289.0, "end": 299.0, "text": " So, semantically driven data management, we have been doing this for a long time.", "tokens": [50414, 407, 11, 4361, 49505, 9555, 1412, 4592, 11, 321, 362, 668, 884, 341, 337, 257, 938, 565, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18718777633294825, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.2517646253108978}, {"id": 19, "seek": 28800, "start": 299.0, "end": 310.0, "text": " It means managing data based on its meaningful logical description, rather than just storing data.", "tokens": [50914, 467, 1355, 11642, 1412, 2361, 322, 1080, 10995, 14978, 3855, 11, 2831, 813, 445, 26085, 1412, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18718777633294825, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.2517646253108978}, {"id": 20, "seek": 31000, "start": 310.0, "end": 320.0, "text": " We also abstract the backend technologies. We also abstract where and how the data is stored from the users.", "tokens": [50364, 492, 611, 12649, 264, 38087, 7943, 13, 492, 611, 12649, 689, 293, 577, 264, 1412, 307, 12187, 490, 264, 5022, 13, 50864], "temperature": 0.0, "avg_logprob": -0.19966041564941406, "compression_ratio": 1.3012048192771084, "no_speech_prob": 0.17090047895908356}, {"id": 21, "seek": 32000, "start": 321.0, "end": 344.0, "text": " So instead of, we try to avoid nested folder structures or UIDs, such as this home user projects ECMWF and blah, blah, blah, or some cryptic UIDs that doesn't make much sense to the user.", "tokens": [50414, 407, 2602, 295, 11, 321, 853, 281, 5042, 15646, 292, 10820, 9227, 420, 624, 2777, 82, 11, 1270, 382, 341, 1280, 4195, 4455, 19081, 44, 54, 37, 293, 12288, 11, 12288, 11, 12288, 11, 420, 512, 9844, 299, 624, 2777, 82, 300, 1177, 380, 652, 709, 2020, 281, 264, 4195, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2029853647405451, "compression_ratio": 1.3169014084507042, "no_speech_prob": 0.17528024315834045}, {"id": 22, "seek": 34400, "start": 344.0, "end": 352.0, "text": " Instead, we want to use meaningful, scientifically meaningful metadata to describe the data.", "tokens": [50364, 7156, 11, 321, 528, 281, 764, 10995, 11, 39719, 10995, 26603, 281, 6786, 264, 1412, 13, 50764], "temperature": 0.0, "avg_logprob": -0.18325047492980956, "compression_ratio": 1.2635658914728682, "no_speech_prob": 0.2373885214328766}, {"id": 23, "seek": 34400, "start": 352.0, "end": 361.0, "text": " For example, in this case, this project is ECMWF experiment number 42.", "tokens": [50764, 1171, 1365, 11, 294, 341, 1389, 11, 341, 1716, 307, 19081, 44, 54, 37, 5120, 1230, 14034, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18325047492980956, "compression_ratio": 1.2635658914728682, "no_speech_prob": 0.2373885214328766}, {"id": 24, "seek": 36100, "start": 361.0, "end": 367.0, "text": " The data is 224 parameter pressure and level.", "tokens": [50364, 440, 1412, 307, 5853, 19, 13075, 3321, 293, 1496, 13, 50664], "temperature": 0.2, "avg_logprob": -0.40729498863220215, "compression_ratio": 1.184873949579832, "no_speech_prob": 0.18547260761260986}, {"id": 25, "seek": 36100, "start": 367.0, "end": 379.0, "text": " So for that, as part of the IOC project, we developed DAISY, data access and storage interface.", "tokens": [50664, 407, 337, 300, 11, 382, 644, 295, 264, 286, 30087, 1716, 11, 321, 4743, 9578, 2343, 56, 11, 1412, 2105, 293, 6725, 9226, 13, 51264], "temperature": 0.2, "avg_logprob": -0.40729498863220215, "compression_ratio": 1.184873949579832, "no_speech_prob": 0.18547260761260986}, {"id": 26, "seek": 37900, "start": 379.0, "end": 388.0, "text": " So we provide, we index and identify data using its meaningful description.", "tokens": [50364, 407, 321, 2893, 11, 321, 8186, 293, 5876, 1412, 1228, 1080, 10995, 3855, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16173332165449095, "compression_ratio": 1.378787878787879, "no_speech_prob": 0.07321999967098236}, {"id": 27, "seek": 37900, "start": 388.0, "end": 400.0, "text": " And for that also, that allows us to implement optimized algorithms to retrieve archive and retrieve data.", "tokens": [50814, 400, 337, 300, 611, 11, 300, 4045, 505, 281, 4445, 26941, 14642, 281, 30254, 23507, 293, 30254, 1412, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16173332165449095, "compression_ratio": 1.378787878787879, "no_speech_prob": 0.07321999967098236}, {"id": 28, "seek": 40000, "start": 401.0, "end": 414.0, "text": " And this is based on the ECMWF, ECMWF object store called FTB, which is also free and open source on GitHub.", "tokens": [50414, 400, 341, 307, 2361, 322, 264, 19081, 44, 54, 37, 11, 19081, 44, 54, 37, 2657, 3531, 1219, 46675, 33, 11, 597, 307, 611, 1737, 293, 1269, 4009, 322, 23331, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16735816451738467, "compression_ratio": 1.3142857142857143, "no_speech_prob": 0.13479885458946228}, {"id": 29, "seek": 40000, "start": 414.0, "end": 420.0, "text": " And this abstracts, we also abstract the storage technologies behind POSIX.", "tokens": [51064, 400, 341, 12649, 82, 11, 321, 611, 12649, 264, 6725, 7943, 2261, 430, 4367, 21124, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16735816451738467, "compression_ratio": 1.3142857142857143, "no_speech_prob": 0.13479885458946228}, {"id": 30, "seek": 42000, "start": 420.0, "end": 425.0, "text": " We support POSIX, DAOS, Moto, and Ceph.", "tokens": [50364, 492, 1406, 430, 4367, 21124, 11, 9578, 4367, 11, 37825, 11, 293, 383, 595, 71, 13, 50614], "temperature": 0.0, "avg_logprob": -0.35340948362608215, "compression_ratio": 1.0808080808080809, "no_speech_prob": 0.07747530937194824}, {"id": 31, "seek": 42000, "start": 425.0, "end": 438.0, "text": " And we provide interfaces and tools as well as CEC and Python APIs.", "tokens": [50614, 400, 321, 2893, 28416, 293, 3873, 382, 731, 382, 383, 8140, 293, 15329, 21445, 13, 51264], "temperature": 0.0, "avg_logprob": -0.35340948362608215, "compression_ratio": 1.0808080808080809, "no_speech_prob": 0.07747530937194824}, {"id": 32, "seek": 43800, "start": 439.0, "end": 448.0, "text": " So the schema, the main complexity is the schema which describes the database.", "tokens": [50414, 407, 264, 34078, 11, 264, 2135, 14024, 307, 264, 34078, 597, 15626, 264, 8149, 13, 50864], "temperature": 0.0, "avg_logprob": -0.26133586264945363, "compression_ratio": 1.3423423423423424, "no_speech_prob": 0.031929634511470795}, {"id": 33, "seek": 43800, "start": 448.0, "end": 456.0, "text": " And it is a collection of rules and each rule is a tree of attributes.", "tokens": [50864, 400, 309, 307, 257, 5765, 295, 4474, 293, 1184, 4978, 307, 257, 4230, 295, 17212, 13, 51264], "temperature": 0.0, "avg_logprob": -0.26133586264945363, "compression_ratio": 1.3423423423423424, "no_speech_prob": 0.031929634511470795}, {"id": 34, "seek": 45600, "start": 457.0, "end": 471.0, "text": " In this example, I have a schema file and inside that I have two rules and each rule is consisting of multiple parameters.", "tokens": [50414, 682, 341, 1365, 11, 286, 362, 257, 34078, 3991, 293, 1854, 300, 286, 362, 732, 4474, 293, 1184, 4978, 307, 33921, 295, 3866, 9834, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10094645193644933, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.030547333881258965}, {"id": 35, "seek": 45600, "start": 471.0, "end": 484.0, "text": " For example, here project experiment date parameter level would translate to a key project ECMWF experiment 42 and so on.", "tokens": [51114, 1171, 1365, 11, 510, 1716, 5120, 4002, 13075, 1496, 576, 13799, 281, 257, 2141, 1716, 19081, 44, 54, 37, 5120, 14034, 293, 370, 322, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10094645193644933, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.030547333881258965}, {"id": 36, "seek": 48400, "start": 484.0, "end": 497.0, "text": " The other rule is event city year and this could translate into event for stem, city is Brussels and year is 2024.", "tokens": [50364, 440, 661, 4978, 307, 2280, 2307, 1064, 293, 341, 727, 13799, 666, 2280, 337, 12312, 11, 2307, 307, 38717, 293, 1064, 307, 45237, 13, 51014], "temperature": 0.0, "avg_logprob": -0.19539279937744142, "compression_ratio": 1.556338028169014, "no_speech_prob": 0.03381139039993286}, {"id": 37, "seek": 48400, "start": 497.0, "end": 507.0, "text": " So the rules are, the rules have, the rules are blueprints of the database, how to construct the database.", "tokens": [51014, 407, 264, 4474, 366, 11, 264, 4474, 362, 11, 264, 4474, 366, 888, 23547, 47523, 295, 264, 8149, 11, 577, 281, 7690, 264, 8149, 13, 51514], "temperature": 0.0, "avg_logprob": -0.19539279937744142, "compression_ratio": 1.556338028169014, "no_speech_prob": 0.03381139039993286}, {"id": 38, "seek": 50700, "start": 507.0, "end": 515.0, "text": " And they have three levels and they, each level can have multiple attributes.", "tokens": [50364, 400, 436, 362, 1045, 4358, 293, 436, 11, 1184, 1496, 393, 362, 3866, 17212, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07408296565214793, "compression_ratio": 1.5038167938931297, "no_speech_prob": 0.13778626918792725}, {"id": 39, "seek": 50700, "start": 515.0, "end": 529.0, "text": " And to make a rule, it has to be unique and complete to describe the data so that we can identify data from other data.", "tokens": [50764, 400, 281, 652, 257, 4978, 11, 309, 575, 281, 312, 3845, 293, 3566, 281, 6786, 264, 1412, 370, 300, 321, 393, 5876, 1412, 490, 661, 1412, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07408296565214793, "compression_ratio": 1.5038167938931297, "no_speech_prob": 0.13778626918792725}, {"id": 40, "seek": 52900, "start": 529.0, "end": 545.0, "text": " And we also need to think about the locality where data, where different data is related to it, we would like to store them together.", "tokens": [50364, 400, 321, 611, 643, 281, 519, 466, 264, 1628, 1860, 689, 1412, 11, 689, 819, 1412, 307, 4077, 281, 309, 11, 321, 576, 411, 281, 3531, 552, 1214, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1271826239193187, "compression_ratio": 1.652694610778443, "no_speech_prob": 0.08289618045091629}, {"id": 41, "seek": 52900, "start": 545.0, "end": 558.0, "text": " So each level here, we can think of the first level as directory, the second level file level, and the third level as the indexes in the file.", "tokens": [51164, 407, 1184, 1496, 510, 11, 321, 393, 519, 295, 264, 700, 1496, 382, 21120, 11, 264, 1150, 1496, 3991, 1496, 11, 293, 264, 2636, 1496, 382, 264, 8186, 279, 294, 264, 3991, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1271826239193187, "compression_ratio": 1.652694610778443, "no_speech_prob": 0.08289618045091629}, {"id": 42, "seek": 55800, "start": 558.0, "end": 564.0, "text": " So the locality would be increased when we go deeper in the level.", "tokens": [50364, 407, 264, 1628, 1860, 576, 312, 6505, 562, 321, 352, 7731, 294, 264, 1496, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2075962653526893, "compression_ratio": 1.710382513661202, "no_speech_prob": 0.03987102955579758}, {"id": 43, "seek": 55800, "start": 564.0, "end": 575.0, "text": " The other, we can set daisy, we can set up daisy by the configuration file in YAML configuration file.", "tokens": [50664, 440, 661, 11, 321, 393, 992, 1120, 14169, 11, 321, 393, 992, 493, 1120, 14169, 538, 264, 11694, 3991, 294, 398, 2865, 43, 11694, 3991, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2075962653526893, "compression_ratio": 1.710382513661202, "no_speech_prob": 0.03987102955579758}, {"id": 44, "seek": 55800, "start": 575.0, "end": 587.0, "text": " We can point to the schema file to find the schema file and we can set the backend storage technology by saying file in this case is reference.", "tokens": [51214, 492, 393, 935, 281, 264, 34078, 3991, 281, 915, 264, 34078, 3991, 293, 321, 393, 992, 264, 38087, 6725, 2899, 538, 1566, 3991, 294, 341, 1389, 307, 6408, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2075962653526893, "compression_ratio": 1.710382513661202, "no_speech_prob": 0.03987102955579758}, {"id": 45, "seek": 58700, "start": 587.0, "end": 597.0, "text": " We can also have different parts to the databases. We can have multiple databases. It's called roots.", "tokens": [50364, 492, 393, 611, 362, 819, 3166, 281, 264, 22380, 13, 492, 393, 362, 3866, 22380, 13, 467, 311, 1219, 10669, 13, 50864], "temperature": 0.0, "avg_logprob": -0.23732932838233742, "compression_ratio": 1.4090909090909092, "no_speech_prob": 0.05649779364466667}, {"id": 46, "seek": 58700, "start": 597.0, "end": 607.0, "text": " And we can set multiple behavior to individual roots.", "tokens": [50864, 400, 321, 393, 992, 3866, 5223, 281, 2609, 10669, 13, 51364], "temperature": 0.0, "avg_logprob": -0.23732932838233742, "compression_ratio": 1.4090909090909092, "no_speech_prob": 0.05649779364466667}, {"id": 47, "seek": 60700, "start": 607.0, "end": 614.0, "text": " So aside from data, we also need key and we also have query.", "tokens": [50364, 407, 7359, 490, 1412, 11, 321, 611, 643, 2141, 293, 321, 611, 362, 14581, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11748668309804555, "compression_ratio": 1.311926605504587, "no_speech_prob": 0.09234821051359177}, {"id": 48, "seek": 60700, "start": 614.0, "end": 624.0, "text": " The keys would refer to single objects while queries can be any number of objects.", "tokens": [50714, 440, 9317, 576, 2864, 281, 2167, 6565, 1339, 24109, 393, 312, 604, 1230, 295, 6565, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11748668309804555, "compression_ratio": 1.311926605504587, "no_speech_prob": 0.09234821051359177}, {"id": 49, "seek": 62400, "start": 624.0, "end": 642.0, "text": " In this case, key defines, identifies, and single objects on the right, I have level as a list of values, 0, 1, and 3.", "tokens": [50364, 682, 341, 1389, 11, 2141, 23122, 11, 34597, 11, 293, 2167, 6565, 322, 264, 558, 11, 286, 362, 1496, 382, 257, 1329, 295, 4190, 11, 1958, 11, 502, 11, 293, 805, 13, 51264], "temperature": 0.0, "avg_logprob": -0.20658903651767307, "compression_ratio": 1.18, "no_speech_prob": 0.29845568537712097}, {"id": 50, "seek": 64200, "start": 642.0, "end": 655.0, "text": " So it means I make a query for three different data where the differences, the levels are 0, 1, and 3.", "tokens": [50364, 407, 309, 1355, 286, 652, 257, 14581, 337, 1045, 819, 1412, 689, 264, 7300, 11, 264, 4358, 366, 1958, 11, 502, 11, 293, 805, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13261398315429687, "compression_ratio": 1.3093525179856116, "no_speech_prob": 0.20602743327617645}, {"id": 51, "seek": 64200, "start": 655.0, "end": 663.0, "text": " So we provide multiple interfaces, command line tools, C, C++, and Python APIs.", "tokens": [51014, 407, 321, 2893, 3866, 28416, 11, 5622, 1622, 3873, 11, 383, 11, 383, 25472, 11, 293, 15329, 21445, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13261398315429687, "compression_ratio": 1.3093525179856116, "no_speech_prob": 0.20602743327617645}, {"id": 52, "seek": 66300, "start": 663.0, "end": 672.0, "text": " But here I present an example for Python API because it's simplest.", "tokens": [50364, 583, 510, 286, 1974, 364, 1365, 337, 15329, 9362, 570, 309, 311, 22811, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14097808158560976, "compression_ratio": 1.5029585798816567, "no_speech_prob": 0.4351035952568054}, {"id": 53, "seek": 66300, "start": 672.0, "end": 679.0, "text": " So I need, for storing a data by key, I need a key and data.", "tokens": [50814, 407, 286, 643, 11, 337, 26085, 257, 1412, 538, 2141, 11, 286, 643, 257, 2141, 293, 1412, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14097808158560976, "compression_ratio": 1.5029585798816567, "no_speech_prob": 0.4351035952568054}, {"id": 54, "seek": 66300, "start": 679.0, "end": 692.0, "text": " So data can be anything in this case. I just put a string here, but it can be PNG file or PDF file or any other type of data.", "tokens": [51164, 407, 1412, 393, 312, 1340, 294, 341, 1389, 13, 286, 445, 829, 257, 6798, 510, 11, 457, 309, 393, 312, 430, 30237, 3991, 420, 17752, 3991, 420, 604, 661, 2010, 295, 1412, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14097808158560976, "compression_ratio": 1.5029585798816567, "no_speech_prob": 0.4351035952568054}, {"id": 55, "seek": 69200, "start": 692.0, "end": 698.0, "text": " Then I make a key. User is met in project is IOC.", "tokens": [50364, 1396, 286, 652, 257, 2141, 13, 32127, 307, 1131, 294, 1716, 307, 286, 30087, 13, 50664], "temperature": 0.0, "avg_logprob": -0.20096785089244013, "compression_ratio": 1.226890756302521, "no_speech_prob": 0.05085931718349457}, {"id": 56, "seek": 69200, "start": 698.0, "end": 703.0, "text": " Date is 2023 and city is born.", "tokens": [50664, 31805, 307, 44377, 293, 2307, 307, 4232, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20096785089244013, "compression_ratio": 1.226890756302521, "no_speech_prob": 0.05085931718349457}, {"id": 57, "seek": 69200, "start": 703.0, "end": 713.0, "text": " And I pass this key and data to Daisy and Daisy would archive it.", "tokens": [50914, 400, 286, 1320, 341, 2141, 293, 1412, 281, 37472, 293, 37472, 576, 23507, 309, 13, 51414], "temperature": 0.0, "avg_logprob": -0.20096785089244013, "compression_ratio": 1.226890756302521, "no_speech_prob": 0.05085931718349457}, {"id": 58, "seek": 71300, "start": 713.0, "end": 720.0, "text": " Then the other main feature is list, searching for data in the database.", "tokens": [50364, 1396, 264, 661, 2135, 4111, 307, 1329, 11, 10808, 337, 1412, 294, 264, 8149, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1068910046627647, "compression_ratio": 1.4375, "no_speech_prob": 0.08531250804662704}, {"id": 59, "seek": 71300, "start": 720.0, "end": 727.0, "text": " I need to make a query, in this case, user met in project IOC.", "tokens": [50714, 286, 643, 281, 652, 257, 14581, 11, 294, 341, 1389, 11, 4195, 1131, 294, 1716, 286, 30087, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1068910046627647, "compression_ratio": 1.4375, "no_speech_prob": 0.08531250804662704}, {"id": 60, "seek": 71300, "start": 727.0, "end": 735.0, "text": " And in this case, I just want two data objects for two different dates.", "tokens": [51064, 400, 294, 341, 1389, 11, 286, 445, 528, 732, 1412, 6565, 337, 732, 819, 11691, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1068910046627647, "compression_ratio": 1.4375, "no_speech_prob": 0.08531250804662704}, {"id": 61, "seek": 73500, "start": 735.0, "end": 743.0, "text": " And I pass this query to Daisy and it returns me the keys that I need for retrieving.", "tokens": [50364, 400, 286, 1320, 341, 14581, 281, 37472, 293, 309, 11247, 385, 264, 9317, 300, 286, 643, 337, 19817, 798, 13, 50764], "temperature": 0.0, "avg_logprob": -0.119330927383068, "compression_ratio": 1.655367231638418, "no_speech_prob": 0.09017318487167358}, {"id": 62, "seek": 73500, "start": 743.0, "end": 749.0, "text": " And in the next example, I have the retrieve getting a key, getting data by a key.", "tokens": [50764, 400, 294, 264, 958, 1365, 11, 286, 362, 264, 30254, 1242, 257, 2141, 11, 1242, 1412, 538, 257, 2141, 13, 51064], "temperature": 0.0, "avg_logprob": -0.119330927383068, "compression_ratio": 1.655367231638418, "no_speech_prob": 0.09017318487167358}, {"id": 63, "seek": 73500, "start": 749.0, "end": 755.0, "text": " I make a key, user is met in project IOC and so on.", "tokens": [51064, 286, 652, 257, 2141, 11, 4195, 307, 1131, 294, 1716, 286, 30087, 293, 370, 322, 13, 51364], "temperature": 0.0, "avg_logprob": -0.119330927383068, "compression_ratio": 1.655367231638418, "no_speech_prob": 0.09017318487167358}, {"id": 64, "seek": 73500, "start": 755.0, "end": 759.0, "text": " And I pass this key to Daisy and retrieve the data.", "tokens": [51364, 400, 286, 1320, 341, 2141, 281, 37472, 293, 30254, 264, 1412, 13, 51564], "temperature": 0.0, "avg_logprob": -0.119330927383068, "compression_ratio": 1.655367231638418, "no_speech_prob": 0.09017318487167358}, {"id": 65, "seek": 73500, "start": 759.0, "end": 762.0, "text": " So it's very simple.", "tokens": [51564, 407, 309, 311, 588, 2199, 13, 51714], "temperature": 0.0, "avg_logprob": -0.119330927383068, "compression_ratio": 1.655367231638418, "no_speech_prob": 0.09017318487167358}, {"id": 66, "seek": 76200, "start": 762.0, "end": 774.0, "text": " So to sum it up, we describe data semantically instead of your IDs and nest directories.", "tokens": [50364, 407, 281, 2408, 309, 493, 11, 321, 6786, 1412, 4361, 49505, 2602, 295, 428, 48212, 293, 15646, 5391, 530, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15319236956144633, "compression_ratio": 1.2926829268292683, "no_speech_prob": 0.03999398276209831}, {"id": 67, "seek": 76200, "start": 774.0, "end": 783.0, "text": " And we index and identify data by its meaningful semantic information.", "tokens": [50964, 400, 321, 8186, 293, 5876, 1412, 538, 1080, 10995, 47982, 1589, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15319236956144633, "compression_ratio": 1.2926829268292683, "no_speech_prob": 0.03999398276209831}, {"id": 68, "seek": 78300, "start": 783.0, "end": 792.0, "text": " And this also allows us fast and efficient retrieve and search and archive algorithms.", "tokens": [50364, 400, 341, 611, 4045, 505, 2370, 293, 7148, 30254, 293, 3164, 293, 23507, 14642, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1651351081000434, "compression_ratio": 1.3923076923076922, "no_speech_prob": 0.34147772192955017}, {"id": 69, "seek": 78300, "start": 792.0, "end": 798.0, "text": " Also, we abstract where how data is stored from the user.", "tokens": [50814, 2743, 11, 321, 12649, 689, 577, 1412, 307, 12187, 490, 264, 4195, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1651351081000434, "compression_ratio": 1.3923076923076922, "no_speech_prob": 0.34147772192955017}, {"id": 70, "seek": 78300, "start": 798.0, "end": 804.0, "text": " And we make blueprints called rules.", "tokens": [51114, 400, 321, 652, 888, 23547, 47523, 1219, 4474, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1651351081000434, "compression_ratio": 1.3923076923076922, "no_speech_prob": 0.34147772192955017}, {"id": 71, "seek": 80400, "start": 804.0, "end": 810.0, "text": " And we make keys to attach to the data and pass it to Daisy.", "tokens": [50364, 400, 321, 652, 9317, 281, 5085, 281, 264, 1412, 293, 1320, 309, 281, 37472, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14779436972833448, "compression_ratio": 1.521472392638037, "no_speech_prob": 0.10728619247674942}, {"id": 72, "seek": 80400, "start": 810.0, "end": 820.0, "text": " And Daisy would store and manage the data using multiple different storage technologies.", "tokens": [50664, 400, 37472, 576, 3531, 293, 3067, 264, 1412, 1228, 3866, 819, 6725, 7943, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14779436972833448, "compression_ratio": 1.521472392638037, "no_speech_prob": 0.10728619247674942}, {"id": 73, "seek": 80400, "start": 820.0, "end": 827.0, "text": " So more about Daisy, we have, we published, Daisy is free and open source.", "tokens": [51164, 407, 544, 466, 37472, 11, 321, 362, 11, 321, 6572, 11, 37472, 307, 1737, 293, 1269, 4009, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14779436972833448, "compression_ratio": 1.521472392638037, "no_speech_prob": 0.10728619247674942}, {"id": 74, "seek": 80400, "start": 827.0, "end": 830.0, "text": " We published on GitHub.", "tokens": [51514, 492, 6572, 322, 23331, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14779436972833448, "compression_ratio": 1.521472392638037, "no_speech_prob": 0.10728619247674942}, {"id": 75, "seek": 83000, "start": 830.0, "end": 834.0, "text": " We have example C API and Python API.", "tokens": [50364, 492, 362, 1365, 383, 9362, 293, 15329, 9362, 13, 50564], "temperature": 0.0, "avg_logprob": -0.19691317198706454, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.31497254967689514}, {"id": 76, "seek": 83000, "start": 834.0, "end": 843.0, "text": " We also provide binary packages on GitHub for C, C++ as well as Python.", "tokens": [50564, 492, 611, 2893, 17434, 17401, 322, 23331, 337, 383, 11, 383, 25472, 382, 731, 382, 15329, 13, 51014], "temperature": 0.0, "avg_logprob": -0.19691317198706454, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.31497254967689514}, {"id": 77, "seek": 83000, "start": 843.0, "end": 850.0, "text": " We also have Python packages for Linux, RPM and the beam packages are available.", "tokens": [51014, 492, 611, 362, 15329, 17401, 337, 18734, 11, 37389, 293, 264, 14269, 17401, 366, 2435, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19691317198706454, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.31497254967689514}, {"id": 78, "seek": 83000, "start": 850.0, "end": 855.0, "text": " We also have documentation on read to docs.", "tokens": [51364, 492, 611, 362, 14333, 322, 1401, 281, 45623, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19691317198706454, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.31497254967689514}, {"id": 79, "seek": 85500, "start": 855.0, "end": 861.0, "text": " And that's all. Thank you for your attention.", "tokens": [50364, 400, 300, 311, 439, 13, 1044, 291, 337, 428, 3202, 13, 50664], "temperature": 0.0, "avg_logprob": -0.32609565083573505, "compression_ratio": 1.1272727272727272, "no_speech_prob": 0.029556874185800552}, {"id": 80, "seek": 85500, "start": 861.0, "end": 868.0, "text": " APPLAUSE", "tokens": [50664, 35298, 51014], "temperature": 0.0, "avg_logprob": -0.32609565083573505, "compression_ratio": 1.1272727272727272, "no_speech_prob": 0.029556874185800552}, {"id": 81, "seek": 85500, "start": 871.0, "end": 878.0, "text": " Thank you. Do you have any questions for Metin?", "tokens": [51164, 1044, 291, 13, 1144, 291, 362, 604, 1651, 337, 6377, 259, 30, 51514], "temperature": 0.0, "avg_logprob": -0.32609565083573505, "compression_ratio": 1.1272727272727272, "no_speech_prob": 0.029556874185800552}, {"id": 82, "seek": 85500, "start": 878.0, "end": 880.0, "text": " No? Oh, there is one.", "tokens": [51514, 883, 30, 876, 11, 456, 307, 472, 13, 51614], "temperature": 0.0, "avg_logprob": -0.32609565083573505, "compression_ratio": 1.1272727272727272, "no_speech_prob": 0.029556874185800552}, {"id": 83, "seek": 88500, "start": 885.0, "end": 890.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50614], "temperature": 0.0, "avg_logprob": -0.24409820556640624, "compression_ratio": 1.4808743169398908, "no_speech_prob": 0.04885220155119896}, {"id": 84, "seek": 88500, "start": 890.0, "end": 898.0, "text": " Hey, next presentation. I was wondering if you can specify the type of the values in your SEMA.", "tokens": [50614, 1911, 11, 958, 5860, 13, 286, 390, 6359, 498, 291, 393, 16500, 264, 2010, 295, 264, 4190, 294, 428, 318, 24891, 13, 51014], "temperature": 0.0, "avg_logprob": -0.24409820556640624, "compression_ratio": 1.4808743169398908, "no_speech_prob": 0.04885220155119896}, {"id": 85, "seek": 88500, "start": 898.0, "end": 900.0, "text": " You mean integer?", "tokens": [51014, 509, 914, 24922, 30, 51114], "temperature": 0.0, "avg_logprob": -0.24409820556640624, "compression_ratio": 1.4808743169398908, "no_speech_prob": 0.04885220155119896}, {"id": 86, "seek": 88500, "start": 900.0, "end": 904.0, "text": " Yeah, yeah, like that. To facilitate the queries.", "tokens": [51114, 865, 11, 1338, 11, 411, 300, 13, 1407, 20207, 264, 24109, 13, 51314], "temperature": 0.0, "avg_logprob": -0.24409820556640624, "compression_ratio": 1.4808743169398908, "no_speech_prob": 0.04885220155119896}, {"id": 87, "seek": 88500, "start": 904.0, "end": 913.0, "text": " Yes, attributes can have types. You can set integer, date, string, they can have multiple types.", "tokens": [51314, 1079, 11, 17212, 393, 362, 3467, 13, 509, 393, 992, 24922, 11, 4002, 11, 6798, 11, 436, 393, 362, 3866, 3467, 13, 51764], "temperature": 0.0, "avg_logprob": -0.24409820556640624, "compression_ratio": 1.4808743169398908, "no_speech_prob": 0.04885220155119896}, {"id": 88, "seek": 91300, "start": 913.0, "end": 915.0, "text": " Okay, thanks.", "tokens": [50364, 1033, 11, 3231, 13, 50464], "temperature": 0.0, "avg_logprob": -0.22278256977305694, "compression_ratio": 1.2857142857142858, "no_speech_prob": 0.09728792309761047}, {"id": 89, "seek": 91300, "start": 931.0, "end": 938.0, "text": " Hi, thank you for your talk. I was interested in indexes because you mentioned that you index and identify data.", "tokens": [51264, 2421, 11, 1309, 291, 337, 428, 751, 13, 286, 390, 3102, 294, 8186, 279, 570, 291, 2835, 300, 291, 8186, 293, 5876, 1412, 13, 51614], "temperature": 0.0, "avg_logprob": -0.22278256977305694, "compression_ratio": 1.2857142857142858, "no_speech_prob": 0.09728792309761047}, {"id": 90, "seek": 93800, "start": 938.0, "end": 950.0, "text": " Did some standard type of indexes or you have some format on your own, optimized for this three type of data?", "tokens": [50364, 2589, 512, 3832, 2010, 295, 8186, 279, 420, 291, 362, 512, 7877, 322, 428, 1065, 11, 26941, 337, 341, 1045, 2010, 295, 1412, 30, 50964], "temperature": 0.0, "avg_logprob": -0.20416502952575682, "compression_ratio": 1.280701754385965, "no_speech_prob": 0.28440093994140625}, {"id": 91, "seek": 93800, "start": 950.0, "end": 958.0, "text": " Yes, indexing is based on the rules.", "tokens": [50964, 1079, 11, 8186, 278, 307, 2361, 322, 264, 4474, 13, 51364], "temperature": 0.0, "avg_logprob": -0.20416502952575682, "compression_ratio": 1.280701754385965, "no_speech_prob": 0.28440093994140625}, {"id": 92, "seek": 95800, "start": 958.0, "end": 970.0, "text": " So the rules here, we have three structures which has three levels.", "tokens": [50364, 407, 264, 4474, 510, 11, 321, 362, 1045, 9227, 597, 575, 1045, 4358, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2016215721766154, "compression_ratio": 1.4356435643564356, "no_speech_prob": 0.1162569671869278}, {"id": 93, "seek": 95800, "start": 970.0, "end": 977.0, "text": " So it has to have three levels which translates to a directory file and data.", "tokens": [50964, 407, 309, 575, 281, 362, 1045, 4358, 597, 28468, 281, 257, 21120, 3991, 293, 1412, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2016215721766154, "compression_ratio": 1.4356435643564356, "no_speech_prob": 0.1162569671869278}, {"id": 94, "seek": 97700, "start": 977.0, "end": 990.0, "text": " And we have in-house mechanism algorithm to that indexes that translates this three into identifying data.", "tokens": [50364, 400, 321, 362, 294, 12, 6410, 7513, 9284, 281, 300, 8186, 279, 300, 28468, 341, 1045, 666, 16696, 1412, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1819922418305368, "compression_ratio": 1.5751633986928104, "no_speech_prob": 0.3294071853160858}, {"id": 95, "seek": 97700, "start": 990.0, "end": 994.0, "text": " So is it something like gene indexes?", "tokens": [51014, 407, 307, 309, 746, 411, 12186, 8186, 279, 30, 51214], "temperature": 0.0, "avg_logprob": -0.1819922418305368, "compression_ratio": 1.5751633986928104, "no_speech_prob": 0.3294071853160858}, {"id": 96, "seek": 97700, "start": 994.0, "end": 996.0, "text": " I couldn't hear.", "tokens": [51214, 286, 2809, 380, 1568, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1819922418305368, "compression_ratio": 1.5751633986928104, "no_speech_prob": 0.3294071853160858}, {"id": 97, "seek": 97700, "start": 996.0, "end": 1000.0, "text": " If it is something like gene indexes?", "tokens": [51314, 759, 309, 307, 746, 411, 12186, 8186, 279, 30, 51514], "temperature": 0.0, "avg_logprob": -0.1819922418305368, "compression_ratio": 1.5751633986928104, "no_speech_prob": 0.3294071853160858}, {"id": 98, "seek": 97700, "start": 1000.0, "end": 1004.0, "text": " I'm not sure if I understand. Gene index?", "tokens": [51514, 286, 478, 406, 988, 498, 286, 1223, 13, 18083, 8186, 30, 51714], "temperature": 0.0, "avg_logprob": -0.1819922418305368, "compression_ratio": 1.5751633986928104, "no_speech_prob": 0.3294071853160858}, {"id": 99, "seek": 100400, "start": 1004.0, "end": 1020.0, "text": " Yeah, I'm not the right person, I think, because we use the FTB which has been developed since long time.", "tokens": [50364, 865, 11, 286, 478, 406, 264, 558, 954, 11, 286, 519, 11, 570, 321, 764, 264, 46675, 33, 597, 575, 668, 4743, 1670, 938, 565, 13, 51164], "temperature": 0.0, "avg_logprob": -0.22206920276988637, "compression_ratio": 1.3245033112582782, "no_speech_prob": 0.14921210706233978}, {"id": 100, "seek": 100400, "start": 1020.0, "end": 1024.0, "text": " And it's a big library.", "tokens": [51164, 400, 309, 311, 257, 955, 6405, 13, 51364], "temperature": 0.0, "avg_logprob": -0.22206920276988637, "compression_ratio": 1.3245033112582782, "no_speech_prob": 0.14921210706233978}, {"id": 101, "seek": 100400, "start": 1024.0, "end": 1029.0, "text": " I cannot answer the question because I haven't worked with that level.", "tokens": [51364, 286, 2644, 1867, 264, 1168, 570, 286, 2378, 380, 2732, 365, 300, 1496, 13, 51614], "temperature": 0.0, "avg_logprob": -0.22206920276988637, "compression_ratio": 1.3245033112582782, "no_speech_prob": 0.14921210706233978}, {"id": 102, "seek": 102900, "start": 1029.0, "end": 1031.0, "text": " No problem, thank you.", "tokens": [50364, 883, 1154, 11, 1309, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2945696353912354, "compression_ratio": 0.7333333333333333, "no_speech_prob": 0.20336823165416718}, {"id": 103, "seek": 103100, "start": 1031.0, "end": 1050.0, "text": " Thank you for the talk.", "tokens": [50364, 1044, 291, 337, 264, 751, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2709573984146118, "compression_ratio": 0.7419354838709677, "no_speech_prob": 0.03483692184090614}, {"id": 104, "seek": 105000, "start": 1050.0, "end": 1064.0, "text": " I would like to know where are these keys stored because you need to query this kind of index and where are they stored for the user?", "tokens": [50364, 286, 576, 411, 281, 458, 689, 366, 613, 9317, 12187, 570, 291, 643, 281, 14581, 341, 733, 295, 8186, 293, 689, 366, 436, 12187, 337, 264, 4195, 30, 51064], "temperature": 0.0, "avg_logprob": -0.15894991159439087, "compression_ratio": 1.3571428571428572, "no_speech_prob": 0.4860319495201111}, {"id": 105, "seek": 106400, "start": 1064.0, "end": 1074.0, "text": " Yeah, so the indexes would be stored separately but together with the data.", "tokens": [50364, 865, 11, 370, 264, 8186, 279, 576, 312, 12187, 14759, 457, 1214, 365, 264, 1412, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17489020029703775, "compression_ratio": 1.4031007751937985, "no_speech_prob": 0.4113704264163971}, {"id": 106, "seek": 106400, "start": 1074.0, "end": 1080.0, "text": " And they would go, for example, in this case, inside the roots.", "tokens": [50864, 400, 436, 576, 352, 11, 337, 1365, 11, 294, 341, 1389, 11, 1854, 264, 10669, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17489020029703775, "compression_ratio": 1.4031007751937985, "no_speech_prob": 0.4113704264163971}, {"id": 107, "seek": 106400, "start": 1080.0, "end": 1084.0, "text": " So each root would be different database.", "tokens": [51164, 407, 1184, 5593, 576, 312, 819, 8149, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17489020029703775, "compression_ratio": 1.4031007751937985, "no_speech_prob": 0.4113704264163971}, {"id": 108, "seek": 108400, "start": 1084.0, "end": 1097.0, "text": " And if you would look inside the root one, output one, for example, here, you would have index keys inside as well as the data, so together.", "tokens": [50364, 400, 498, 291, 576, 574, 1854, 264, 5593, 472, 11, 5598, 472, 11, 337, 1365, 11, 510, 11, 291, 576, 362, 8186, 9317, 1854, 382, 731, 382, 264, 1412, 11, 370, 1214, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1415970618264717, "compression_ratio": 1.4797297297297298, "no_speech_prob": 0.1671384871006012}, {"id": 109, "seek": 108400, "start": 1097.0, "end": 1107.0, "text": " Okay, so it's a file system or a database stored inside these two directories?", "tokens": [51014, 1033, 11, 370, 309, 311, 257, 3991, 1185, 420, 257, 8149, 12187, 1854, 613, 732, 5391, 530, 30, 51514], "temperature": 0.0, "avg_logprob": -0.1415970618264717, "compression_ratio": 1.4797297297297298, "no_speech_prob": 0.1671384871006012}, {"id": 110, "seek": 110700, "start": 1107.0, "end": 1119.0, "text": " Yeah, for POSIX, it would be directory, but for object storage, it would be not directly contained or something like that.", "tokens": [50364, 865, 11, 337, 430, 4367, 21124, 11, 309, 576, 312, 21120, 11, 457, 337, 2657, 6725, 11, 309, 576, 312, 406, 3838, 16212, 420, 746, 411, 300, 13, 50964], "temperature": 0.0, "avg_logprob": -0.24640190818093038, "compression_ratio": 1.4662162162162162, "no_speech_prob": 0.21950845420360565}, {"id": 111, "seek": 110700, "start": 1119.0, "end": 1131.0, "text": " Okay, so the way how the index is stored depends on the type of the storage you describe here.", "tokens": [50964, 1033, 11, 370, 264, 636, 577, 264, 8186, 307, 12187, 5946, 322, 264, 2010, 295, 264, 6725, 291, 6786, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.24640190818093038, "compression_ratio": 1.4662162162162162, "no_speech_prob": 0.21950845420360565}, {"id": 112, "seek": 113100, "start": 1131.0, "end": 1137.0, "text": " Yes, but we can also have, we have two different abstractions.", "tokens": [50364, 1079, 11, 457, 321, 393, 611, 362, 11, 321, 362, 732, 819, 12649, 626, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15680784167665424, "compression_ratio": 1.5364238410596027, "no_speech_prob": 0.2867531478404999}, {"id": 113, "seek": 113100, "start": 1137.0, "end": 1143.0, "text": " One is indexing and we call it catalog and we have a bulk data.", "tokens": [50664, 1485, 307, 8186, 278, 293, 321, 818, 309, 19746, 293, 321, 362, 257, 16139, 1412, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15680784167665424, "compression_ratio": 1.5364238410596027, "no_speech_prob": 0.2867531478404999}, {"id": 114, "seek": 113100, "start": 1143.0, "end": 1156.0, "text": " So we can have indexing catalog inside POSIX directory and bulk data on an object store.", "tokens": [50964, 407, 321, 393, 362, 8186, 278, 19746, 1854, 430, 4367, 21124, 21120, 293, 16139, 1412, 322, 364, 2657, 3531, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15680784167665424, "compression_ratio": 1.5364238410596027, "no_speech_prob": 0.2867531478404999}, {"id": 115, "seek": 113100, "start": 1156.0, "end": 1160.0, "text": " Okay, thank you.", "tokens": [51614, 1033, 11, 1309, 291, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15680784167665424, "compression_ratio": 1.5364238410596027, "no_speech_prob": 0.2867531478404999}, {"id": 116, "seek": 116000, "start": 1160.0, "end": 1165.0, "text": " Any more questions?", "tokens": [50364, 2639, 544, 1651, 30, 50614], "temperature": 0.0, "avg_logprob": -0.1619002168828791, "compression_ratio": 1.0813953488372092, "no_speech_prob": 0.021490691229701042}, {"id": 117, "seek": 116000, "start": 1165.0, "end": 1169.0, "text": " Is the next speaker in the room?", "tokens": [50614, 1119, 264, 958, 8145, 294, 264, 1808, 30, 50814], "temperature": 0.0, "avg_logprob": -0.1619002168828791, "compression_ratio": 1.0813953488372092, "no_speech_prob": 0.021490691229701042}, {"id": 118, "seek": 116000, "start": 1169.0, "end": 1171.0, "text": " Okay, thank you again, Metin.", "tokens": [50814, 1033, 11, 1309, 291, 797, 11, 6377, 259, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1619002168828791, "compression_ratio": 1.0813953488372092, "no_speech_prob": 0.021490691229701042}, {"id": 119, "seek": 116000, "start": 1171.0, "end": 1173.0, "text": " Thank you.", "tokens": [50914, 1044, 291, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1619002168828791, "compression_ratio": 1.0813953488372092, "no_speech_prob": 0.021490691229701042}], "language": "en"}