{"text": " So, hello everyone. So, as I said, my name is Remedio Raffa, I'm a principal tech lead at Lino. I've been working on Open Source Project for a long time now, and I've been at FOSDEM for many years now, it's not my first FOSDEM presentation. So, I've been working on VLC media player on V8, Javascript Engine, and I joined Lino some years ago working on Lava and on Automation and CI in general. So, today I wanted to speak a bit about a really tiny project that I created some years ago, which is called Keyscash. And in order to present it, I have to explain why we are using Keyscash in Lino. So, at Lino we contribute a lot to the Linux channel, and not only by developing new stuff, drivers, and a lot of different things, but we also contribute a lot by testing the Linux channel. We have a project called LKFT, Linux channel functional testing project. That is, if you go to the website, it's written that the goal is to improve the Linux channel quality on the ARM architecture, because we are now mainly about ARM, but not only. By performing regression testing and reporting on seleting Linux channel branches on the Android command channel in real time. Okay. That's what is written on the website. More or less, it's a project led by Leno. It's an automated system to build and test a set of Linux channel trees. We mainly care about LTS, obviously, mainline and next. And by contract, we have to provide a report in 48 hours. So, it's quite tight between an RC on an LTS trees. In 48 hours, we have to provide an SLA. We have to provide a report, all right. So, if you look back at 2023, we built and tested 396 different RCs, so only LTS channels. As we also care about mainline and next, we built 2,443 different channel commits. That's 1.1 million builds. So, 1.1 million channels were built by the system by LKFT. And we ran 297 million tests just in one year. And if you look at the Android parts, Android command channel, that's 580 million tests. The tests are running both on virtual machines, so QMU and FVP. We have a specific system where we can instantiate in the cloud many machines for running QMU and FVP. That's a stock suite service that we created. We will not speak about it today. And we also have a physical lab. So, with physical devices in Cambridge, that is managed by a tool called Lava. That's a tool that I'm running inside in Salinaro. So, if you look at the LKFT, really simplified architecture because obviously it's way more complex than that. So, as I said, we care about LTS trees, mainline and next. So, we have GitLab repository that are just mirroring the different trees that we care about. And when there is changes, GitLab will pull it and we create a GitLab pipeline. The GitLab pipeline will send a set of instructions to our cloud service for building, called text build, that will run the builds. So, it will scale from zero machine to 5,000 machine in some seconds, do the builds, shut down the machine and then send the artifacts to an S3 like storage. So, the artifact will be the kernel, the TTB, the root file system, the modules, etc. And then these artifacts will be pulled by our lab in Cambridge to be tested on real devices. So, in the lab in Cambridge, we have some hundreds of boards, Raspberry Pi, Dragon boards, IKs, X15, etc. A lot of different boards. And at the same time, they will all pull the artifacts, deploy them on the hardware, depending on what kind of hardware you have, run the test and then report back. And obviously, everything will run in parallel and don't leave from the same storage. So, our CI system, as I said, will build and test artifacts, L, DTB, RAM, these modules, etc. And for each kernel, DTB and root file system, they will use multiple times because when we have one commit from the kernel, we'll build it for multiple architectures. We'll build it for x86, ARMv7, ARMv8, ARMv9, PPC, SH4, MIPS, etc. Then for each architecture, we'll have multiple configurations. I want to build with some virtio-specific configuration. I want to build in debug in release, etc. And then for each configuration, for each commit architecture configuration, I will run a set of tests. So, KSELTest, KUnit, libgperiod, the LTP, etc. Considering that LTP, for example, is broken into 20 different test suites that will be 20 different test jobs because it takes a lot of time to run. So, the CI system will run a lot of different jobs, of test jobs, that will actually pull the same artifacts all the time, which means that in the network, on the network in the lab in Cambridge, we have a lot of network usage and a lot of duplication. We are re-downloading always the same artifacts. So, that's normally really simple things to solve. You just add caching. So, just, I'm really adding that because that's really important. Our system, our CI system, the Lava Workers, will download multiple times the same artifacts at the same time in parallel. So, if you look for a caching proxy in the open-source community, you will obviously find that Squid is the main caching proxy and it's a perfectly good one. It's really working well. So, you should just install that on our network, point all the workers to it and it should work. Short answer is no, it's not working just because of the two reasons above. So, and also for another reason, this one. All artifacts, as I said, are published in an S3 like bucket. They are somewhere in the cloud. So, obviously, if you want to download them, you will download over HTTPS. You will not download a random binary from internet and run it in your local lab for testing. Not something that you will do. So, we have to validate. So, we use HTTPS to be sure that what we're downloading is what we're expecting. At least we are trusting the software. But when you add a Squid proxy in the connection, it will not work well with HTTPS. That written in the script documentation, you can make it work with that. It's not easy. The main problem is that as an HTTP client, when you connect to a website over HTTPS, you're expecting to get a certificate and the connection will be encrypted with the certificate and the certificate, you have to trust it. When you add Squid in the middle, Squid will have to connect on your BI to the server. So, the connection between Squid and the website is encrypted correctly. The certificate written by the website is a legit one, so it will work. But when Squid will have to decrypt the content to cache it and then re-encrypt it to send it back to you, it does not have the private certificate from the website, obviously. You don't have the private certificate of Google.com on your machine, so you cannot re-encrypt the traffic. So, Squid will need to have his own certificate and it will encrypt the traffic with its own asset certificate. And you will obviously not trust it. You will not trust your local Squid proxy to sign something from Google.com or AWS or any website or Linux Foundation. So, when the HTTP client receives the custom asset certificate, it will just say, no, I don't trust you. There is a workaround and it's written in the script documentation, obviously, which is create a wildcard certificate, which is a certificate that will be valid for absolutely every website on the planet, every DNS, so it's kind of a dangerous asset certificate. And you can install it on every of your HTTP clients. It's possible, but it's really crappy, honestly. That's the first problem. The second problem and that there is no way to work around it is that when Squid, when you try to download multiple times the same artifact in Squid, so, for example, you have two connections downloading the same root FS, Squid will download it twice and stream it back to the clients at the same time. And when it's downloaded, it's finished, then the third connection will have a cache version. But as long as it's not cached locally, it will re-download from the start. And as I said before, our system is by-designed running everything in parallel, so it's often the case that we have multiple downloads of the same artifact at the exact same time. So when using Squid, it was just not caching anything. Sorry. So that's why we created KeysCache. So Keys stands for keep it simple, stupid. It's a pretty simple and stupid caching service. But the main features that it has are exactly what we need for a API system. It allows to cache HTTPS resources without any acts or anything. It allows to download only once, even if you have multiple clients and they will all get a stream back, the stream of data back. And the reason why it's not, it's working for both cases is that it's not a transparent proxy. So it's not like clients that will know from an environment of the Bible that it has to go through a proxy. Instead, you have to prefix your URLs. So if you want to access example.com slash .fs.x4, for example, you have to prefix it by your KeysCache instance. So even if you're downloading over for HTTPS, your clients know that it goes to KeysCache and not example.com so that it's expecting a certificate from KeysCache, not from the original website. That's the first reason. And KeysCache also, we made it so it knows how to stream back to multiple clients, the same content. Fun thing, we also added a lot of automatic retries inside the KeysCache backends. So if for any reason, and it happens a lot, the connection between your network and the S3 like bucket breaks and it often breaks, honestly, KeysCache backend will automatically retries. This is a list of HTTP codes that we're retrying automatically. And it will also, so when it's retrying, it retries up to 50 times over a period of two hours because we had exponential backups. So sometimes a download will actually take two hours and 50 retries just because the S3 like bucket is just sometimes a bit buggy to answer. We also added partial download, which when you have, we do a retry, if the HTTP server knows how to do that, we only download the remaining content, not from the start. And the good thing is that with the automatic retries, the client will never see that there is a broken connection because from the client to KeysCache, the connection is kept alive. It's only the backends that sees the network issues. So it has been in production for 3.5 years. It downloaded 32 terabits of data from internet and served 1.6 petabytes of data locally just for a really small tiny software, which is an expansion ratio of 51 times. So we divided the network usage by 51 just by having a small working proxy. It also improved a lot of stability thanks to the automatic retries, I said, up to 50 retries, which is insane. And it also lowered a lot of the S3 egress cost because you have to pay for egress in the cloud. When you, for 1.6 petabytes of data, that's a lot of money. So yeah, we saved around 150 K of euros just by having a local proxy. Just because I have just two minutes, a look at the global architecture of the service, it suggests a Django application with a salary backends. So you have a reverse proxy and Ginex. It can be any reverse proxy in fact, that will receive an HTTP connection. It will send that to Giniacon, which is a Django runtime. The Django will see if the, we look at the database, but at the base, progress, to know if the artifact has been downloaded already or not. If it's a case, it will then look at the file system and just give that back to Ginex saying, please send that to the client. And I'm done with it. If it's not already downloaded, it will send a message to Redis that will spawn a salary task that will actually do the download and retry in the back end. And it's done only once. And it's then saving it to the file system, appending to a file, byte by byte. And at the same time, the Django process just reads the file on the file system and sends the bytes where they are available. And that's all. Waiting for the file, the file to be just finished. And if a second or third of many different users arrive for the same file, then they will just reuse what is already available in the file system and wait for the download to finish. And that's all. That's all. It's pretty simple and efficient. And it has been a really good use for us. And it might be useful for your CI system. So if you have any questions, I will be here after the talk. Thanks a lot. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.76, "text": " So, hello everyone. So, as I said, my name is Remedio Raffa, I'm a principal tech lead", "tokens": [50364, 407, 11, 7751, 1518, 13, 407, 11, 382, 286, 848, 11, 452, 1315, 307, 4080, 292, 1004, 497, 2518, 64, 11, 286, 478, 257, 9716, 7553, 1477, 51052], "temperature": 0.0, "avg_logprob": -0.353957630339123, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.30354684591293335}, {"id": 1, "seek": 0, "start": 13.76, "end": 19.86, "text": " at Lino. I've been working on Open Source Project for a long time now, and I've been", "tokens": [51052, 412, 441, 2982, 13, 286, 600, 668, 1364, 322, 7238, 29629, 9849, 337, 257, 938, 565, 586, 11, 293, 286, 600, 668, 51357], "temperature": 0.0, "avg_logprob": -0.353957630339123, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.30354684591293335}, {"id": 2, "seek": 0, "start": 19.86, "end": 26.400000000000002, "text": " at FOSDEM for many years now, it's not my first FOSDEM presentation. So, I've been working", "tokens": [51357, 412, 479, 4367, 35, 6683, 337, 867, 924, 586, 11, 309, 311, 406, 452, 700, 479, 4367, 35, 6683, 5860, 13, 407, 11, 286, 600, 668, 1364, 51684], "temperature": 0.0, "avg_logprob": -0.353957630339123, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.30354684591293335}, {"id": 3, "seek": 2640, "start": 26.4, "end": 33.12, "text": " on VLC media player on V8, Javascript Engine, and I joined Lino some years ago working on", "tokens": [50364, 322, 691, 14766, 3021, 4256, 322, 691, 23, 11, 508, 37331, 5944, 7659, 11, 293, 286, 6869, 441, 2982, 512, 924, 2057, 1364, 322, 50700], "temperature": 0.0, "avg_logprob": -0.2425401538026099, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.0829295814037323}, {"id": 4, "seek": 2640, "start": 33.12, "end": 39.519999999999996, "text": " Lava and on Automation and CI in general. So, today I wanted to speak a bit about a", "tokens": [50700, 441, 4061, 293, 322, 24619, 399, 293, 37777, 294, 2674, 13, 407, 11, 965, 286, 1415, 281, 1710, 257, 857, 466, 257, 51020], "temperature": 0.0, "avg_logprob": -0.2425401538026099, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.0829295814037323}, {"id": 5, "seek": 2640, "start": 39.519999999999996, "end": 44.68, "text": " really tiny project that I created some years ago, which is called Keyscash. And in order", "tokens": [51020, 534, 5870, 1716, 300, 286, 2942, 512, 924, 2057, 11, 597, 307, 1219, 43733, 66, 1299, 13, 400, 294, 1668, 51278], "temperature": 0.0, "avg_logprob": -0.2425401538026099, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.0829295814037323}, {"id": 6, "seek": 2640, "start": 44.68, "end": 52.0, "text": " to present it, I have to explain why we are using Keyscash in Lino. So, at Lino we contribute", "tokens": [51278, 281, 1974, 309, 11, 286, 362, 281, 2903, 983, 321, 366, 1228, 43733, 66, 1299, 294, 441, 2982, 13, 407, 11, 412, 441, 2982, 321, 10586, 51644], "temperature": 0.0, "avg_logprob": -0.2425401538026099, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.0829295814037323}, {"id": 7, "seek": 5200, "start": 52.0, "end": 57.72, "text": " a lot to the Linux channel, and not only by developing new stuff, drivers, and a lot", "tokens": [50364, 257, 688, 281, 264, 18734, 2269, 11, 293, 406, 787, 538, 6416, 777, 1507, 11, 11590, 11, 293, 257, 688, 50650], "temperature": 0.0, "avg_logprob": -0.22080497021945017, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.08497390151023865}, {"id": 8, "seek": 5200, "start": 57.72, "end": 63.08, "text": " of different things, but we also contribute a lot by testing the Linux channel. We have", "tokens": [50650, 295, 819, 721, 11, 457, 321, 611, 10586, 257, 688, 538, 4997, 264, 18734, 2269, 13, 492, 362, 50918], "temperature": 0.0, "avg_logprob": -0.22080497021945017, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.08497390151023865}, {"id": 9, "seek": 5200, "start": 63.08, "end": 70.8, "text": " a project called LKFT, Linux channel functional testing project. That is, if you go to the", "tokens": [50918, 257, 1716, 1219, 441, 42, 25469, 11, 18734, 2269, 11745, 4997, 1716, 13, 663, 307, 11, 498, 291, 352, 281, 264, 51304], "temperature": 0.0, "avg_logprob": -0.22080497021945017, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.08497390151023865}, {"id": 10, "seek": 5200, "start": 70.8, "end": 75.6, "text": " website, it's written that the goal is to improve the Linux channel quality on the ARM", "tokens": [51304, 3144, 11, 309, 311, 3720, 300, 264, 3387, 307, 281, 3470, 264, 18734, 2269, 3125, 322, 264, 45209, 51544], "temperature": 0.0, "avg_logprob": -0.22080497021945017, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.08497390151023865}, {"id": 11, "seek": 5200, "start": 75.6, "end": 81.52, "text": " architecture, because we are now mainly about ARM, but not only. By performing regression", "tokens": [51544, 9482, 11, 570, 321, 366, 586, 8704, 466, 45209, 11, 457, 406, 787, 13, 3146, 10205, 24590, 51840], "temperature": 0.0, "avg_logprob": -0.22080497021945017, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.08497390151023865}, {"id": 12, "seek": 8152, "start": 81.52, "end": 86.6, "text": " testing and reporting on seleting Linux channel branches on the Android command channel in", "tokens": [50364, 4997, 293, 10031, 322, 5851, 9880, 18734, 2269, 14770, 322, 264, 8853, 5622, 2269, 294, 50618], "temperature": 0.0, "avg_logprob": -0.20983863144778134, "compression_ratio": 1.5482456140350878, "no_speech_prob": 0.016720598563551903}, {"id": 13, "seek": 8152, "start": 86.6, "end": 91.03999999999999, "text": " real time. Okay. That's what is written on the website. More or less, it's a project", "tokens": [50618, 957, 565, 13, 1033, 13, 663, 311, 437, 307, 3720, 322, 264, 3144, 13, 5048, 420, 1570, 11, 309, 311, 257, 1716, 50840], "temperature": 0.0, "avg_logprob": -0.20983863144778134, "compression_ratio": 1.5482456140350878, "no_speech_prob": 0.016720598563551903}, {"id": 14, "seek": 8152, "start": 91.03999999999999, "end": 98.39999999999999, "text": " led by Leno. It's an automated system to build and test a set of Linux channel trees.", "tokens": [50840, 4684, 538, 45661, 13, 467, 311, 364, 18473, 1185, 281, 1322, 293, 1500, 257, 992, 295, 18734, 2269, 5852, 13, 51208], "temperature": 0.0, "avg_logprob": -0.20983863144778134, "compression_ratio": 1.5482456140350878, "no_speech_prob": 0.016720598563551903}, {"id": 15, "seek": 8152, "start": 98.39999999999999, "end": 106.39999999999999, "text": " We mainly care about LTS, obviously, mainline and next. And by contract, we have to provide", "tokens": [51208, 492, 8704, 1127, 466, 441, 7327, 11, 2745, 11, 2135, 1889, 293, 958, 13, 400, 538, 4364, 11, 321, 362, 281, 2893, 51608], "temperature": 0.0, "avg_logprob": -0.20983863144778134, "compression_ratio": 1.5482456140350878, "no_speech_prob": 0.016720598563551903}, {"id": 16, "seek": 10640, "start": 106.4, "end": 114.32000000000001, "text": " a report in 48 hours. So, it's quite tight between an RC on an LTS trees. In 48 hours,", "tokens": [50364, 257, 2275, 294, 11174, 2496, 13, 407, 11, 309, 311, 1596, 4524, 1296, 364, 28987, 322, 364, 441, 7327, 5852, 13, 682, 11174, 2496, 11, 50760], "temperature": 0.0, "avg_logprob": -0.17779188156127929, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.02270827256143093}, {"id": 17, "seek": 10640, "start": 114.32000000000001, "end": 120.36000000000001, "text": " we have to provide an SLA. We have to provide a report, all right. So, if you look back", "tokens": [50760, 321, 362, 281, 2893, 364, 318, 11435, 13, 492, 362, 281, 2893, 257, 2275, 11, 439, 558, 13, 407, 11, 498, 291, 574, 646, 51062], "temperature": 0.0, "avg_logprob": -0.17779188156127929, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.02270827256143093}, {"id": 18, "seek": 10640, "start": 120.36000000000001, "end": 132.20000000000002, "text": " at 2023, we built and tested 396 different RCs, so only LTS channels. As we also care", "tokens": [51062, 412, 44377, 11, 321, 3094, 293, 8246, 15238, 21, 819, 28987, 82, 11, 370, 787, 441, 7327, 9235, 13, 1018, 321, 611, 1127, 51654], "temperature": 0.0, "avg_logprob": -0.17779188156127929, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.02270827256143093}, {"id": 19, "seek": 13220, "start": 132.2, "end": 141.44, "text": " about mainline and next, we built 2,443 different channel commits. That's 1.1 million builds.", "tokens": [50364, 466, 2135, 1889, 293, 958, 11, 321, 3094, 568, 11, 13912, 18, 819, 2269, 48311, 13, 663, 311, 502, 13, 16, 2459, 15182, 13, 50826], "temperature": 0.0, "avg_logprob": -0.20132314241849458, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.030736586079001427}, {"id": 20, "seek": 13220, "start": 141.44, "end": 148.76, "text": " So, 1.1 million channels were built by the system by LKFT. And we ran 297 million tests", "tokens": [50826, 407, 11, 502, 13, 16, 2459, 9235, 645, 3094, 538, 264, 1185, 538, 441, 42, 25469, 13, 400, 321, 5872, 9413, 22, 2459, 6921, 51192], "temperature": 0.0, "avg_logprob": -0.20132314241849458, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.030736586079001427}, {"id": 21, "seek": 13220, "start": 148.76, "end": 155.28, "text": " just in one year. And if you look at the Android parts, Android command channel, that's 580", "tokens": [51192, 445, 294, 472, 1064, 13, 400, 498, 291, 574, 412, 264, 8853, 3166, 11, 8853, 5622, 2269, 11, 300, 311, 1025, 4702, 51518], "temperature": 0.0, "avg_logprob": -0.20132314241849458, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.030736586079001427}, {"id": 22, "seek": 15528, "start": 155.28, "end": 162.72, "text": " million tests. The tests are running both on virtual machines, so QMU and FVP. We have", "tokens": [50364, 2459, 6921, 13, 440, 6921, 366, 2614, 1293, 322, 6374, 8379, 11, 370, 1249, 44, 52, 293, 479, 53, 47, 13, 492, 362, 50736], "temperature": 0.0, "avg_logprob": -0.2678653315493935, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.19304542243480682}, {"id": 23, "seek": 15528, "start": 162.72, "end": 167.72, "text": " a specific system where we can instantiate in the cloud many machines for running QMU", "tokens": [50736, 257, 2685, 1185, 689, 321, 393, 9836, 13024, 294, 264, 4588, 867, 8379, 337, 2614, 1249, 44, 52, 50986], "temperature": 0.0, "avg_logprob": -0.2678653315493935, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.19304542243480682}, {"id": 24, "seek": 15528, "start": 167.72, "end": 172.44, "text": " and FVP. That's a stock suite service that we created. We will not speak about it today.", "tokens": [50986, 293, 479, 53, 47, 13, 663, 311, 257, 4127, 14205, 2643, 300, 321, 2942, 13, 492, 486, 406, 1710, 466, 309, 965, 13, 51222], "temperature": 0.0, "avg_logprob": -0.2678653315493935, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.19304542243480682}, {"id": 25, "seek": 15528, "start": 172.44, "end": 179.36, "text": " And we also have a physical lab. So, with physical devices in Cambridge, that is managed by a", "tokens": [51222, 400, 321, 611, 362, 257, 4001, 2715, 13, 407, 11, 365, 4001, 5759, 294, 24876, 11, 300, 307, 6453, 538, 257, 51568], "temperature": 0.0, "avg_logprob": -0.2678653315493935, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.19304542243480682}, {"id": 26, "seek": 17936, "start": 179.44000000000003, "end": 186.4, "text": " tool called Lava. That's a tool that I'm running inside in Salinaro. So, if you look at the", "tokens": [50368, 2290, 1219, 441, 4061, 13, 663, 311, 257, 2290, 300, 286, 478, 2614, 1854, 294, 5996, 259, 9708, 13, 407, 11, 498, 291, 574, 412, 264, 50716], "temperature": 0.0, "avg_logprob": -0.24580865053786444, "compression_ratio": 1.5341880341880343, "no_speech_prob": 0.010106397792696953}, {"id": 27, "seek": 17936, "start": 186.4, "end": 191.52, "text": " LKFT, really simplified architecture because obviously it's way more complex than that.", "tokens": [50716, 441, 42, 25469, 11, 534, 26335, 9482, 570, 2745, 309, 311, 636, 544, 3997, 813, 300, 13, 50972], "temperature": 0.0, "avg_logprob": -0.24580865053786444, "compression_ratio": 1.5341880341880343, "no_speech_prob": 0.010106397792696953}, {"id": 28, "seek": 17936, "start": 191.52, "end": 200.44000000000003, "text": " So, as I said, we care about LTS trees, mainline and next. So, we have GitLab repository that", "tokens": [50972, 407, 11, 382, 286, 848, 11, 321, 1127, 466, 441, 7327, 5852, 11, 2135, 1889, 293, 958, 13, 407, 11, 321, 362, 16939, 37880, 25841, 300, 51418], "temperature": 0.0, "avg_logprob": -0.24580865053786444, "compression_ratio": 1.5341880341880343, "no_speech_prob": 0.010106397792696953}, {"id": 29, "seek": 17936, "start": 200.44000000000003, "end": 206.20000000000002, "text": " are just mirroring the different trees that we care about. And when there is changes,", "tokens": [51418, 366, 445, 8013, 278, 264, 819, 5852, 300, 321, 1127, 466, 13, 400, 562, 456, 307, 2962, 11, 51706], "temperature": 0.0, "avg_logprob": -0.24580865053786444, "compression_ratio": 1.5341880341880343, "no_speech_prob": 0.010106397792696953}, {"id": 30, "seek": 20620, "start": 207.16, "end": 212.32, "text": " GitLab will pull it and we create a GitLab pipeline. The GitLab pipeline will send a set of", "tokens": [50412, 16939, 37880, 486, 2235, 309, 293, 321, 1884, 257, 16939, 37880, 15517, 13, 440, 16939, 37880, 15517, 486, 2845, 257, 992, 295, 50670], "temperature": 0.0, "avg_logprob": -0.2227501211495235, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.009899726137518883}, {"id": 31, "seek": 20620, "start": 212.32, "end": 217.64, "text": " instructions to our cloud service for building, called text build, that will run the builds.", "tokens": [50670, 9415, 281, 527, 4588, 2643, 337, 2390, 11, 1219, 2487, 1322, 11, 300, 486, 1190, 264, 15182, 13, 50936], "temperature": 0.0, "avg_logprob": -0.2227501211495235, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.009899726137518883}, {"id": 32, "seek": 20620, "start": 217.64, "end": 223.07999999999998, "text": " So, it will scale from zero machine to 5,000 machine in some seconds, do the builds, shut", "tokens": [50936, 407, 11, 309, 486, 4373, 490, 4018, 3479, 281, 1025, 11, 1360, 3479, 294, 512, 3949, 11, 360, 264, 15182, 11, 5309, 51208], "temperature": 0.0, "avg_logprob": -0.2227501211495235, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.009899726137518883}, {"id": 33, "seek": 20620, "start": 223.07999999999998, "end": 228.76, "text": " down the machine and then send the artifacts to an S3 like storage. So, the artifact will", "tokens": [51208, 760, 264, 3479, 293, 550, 2845, 264, 24617, 281, 364, 318, 18, 411, 6725, 13, 407, 11, 264, 34806, 486, 51492], "temperature": 0.0, "avg_logprob": -0.2227501211495235, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.009899726137518883}, {"id": 34, "seek": 20620, "start": 228.76, "end": 234.2, "text": " be the kernel, the TTB, the root file system, the modules, etc. And then these artifacts", "tokens": [51492, 312, 264, 28256, 11, 264, 32576, 33, 11, 264, 5593, 3991, 1185, 11, 264, 16679, 11, 5183, 13, 400, 550, 613, 24617, 51764], "temperature": 0.0, "avg_logprob": -0.2227501211495235, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.009899726137518883}, {"id": 35, "seek": 23420, "start": 234.2, "end": 241.0, "text": " will be pulled by our lab in Cambridge to be tested on real devices. So, in the lab", "tokens": [50364, 486, 312, 7373, 538, 527, 2715, 294, 24876, 281, 312, 8246, 322, 957, 5759, 13, 407, 11, 294, 264, 2715, 50704], "temperature": 0.0, "avg_logprob": -0.2508375289592337, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.004519924521446228}, {"id": 36, "seek": 23420, "start": 241.0, "end": 247.11999999999998, "text": " in Cambridge, we have some hundreds of boards, Raspberry Pi, Dragon boards, IKs, X15, etc.", "tokens": [50704, 294, 24876, 11, 321, 362, 512, 6779, 295, 13293, 11, 41154, 17741, 11, 11517, 13293, 11, 286, 42, 82, 11, 1783, 5211, 11, 5183, 13, 51010], "temperature": 0.0, "avg_logprob": -0.2508375289592337, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.004519924521446228}, {"id": 37, "seek": 23420, "start": 247.11999999999998, "end": 254.44, "text": " A lot of different boards. And at the same time, they will all pull the artifacts, deploy", "tokens": [51010, 316, 688, 295, 819, 13293, 13, 400, 412, 264, 912, 565, 11, 436, 486, 439, 2235, 264, 24617, 11, 7274, 51376], "temperature": 0.0, "avg_logprob": -0.2508375289592337, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.004519924521446228}, {"id": 38, "seek": 23420, "start": 254.44, "end": 259.4, "text": " them on the hardware, depending on what kind of hardware you have, run the test and then", "tokens": [51376, 552, 322, 264, 8837, 11, 5413, 322, 437, 733, 295, 8837, 291, 362, 11, 1190, 264, 1500, 293, 550, 51624], "temperature": 0.0, "avg_logprob": -0.2508375289592337, "compression_ratio": 1.5619469026548674, "no_speech_prob": 0.004519924521446228}, {"id": 39, "seek": 25940, "start": 259.44, "end": 264.52, "text": " report back. And obviously, everything will run in parallel and don't leave from the same", "tokens": [50366, 2275, 646, 13, 400, 2745, 11, 1203, 486, 1190, 294, 8952, 293, 500, 380, 1856, 490, 264, 912, 50620], "temperature": 0.0, "avg_logprob": -0.2521427273750305, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.006689482368528843}, {"id": 40, "seek": 25940, "start": 264.52, "end": 274.52, "text": " storage. So, our CI system, as I said, will build and test artifacts, L, DTB, RAM, these", "tokens": [50620, 6725, 13, 407, 11, 527, 37777, 1185, 11, 382, 286, 848, 11, 486, 1322, 293, 1500, 24617, 11, 441, 11, 413, 51, 33, 11, 14561, 11, 613, 51120], "temperature": 0.0, "avg_logprob": -0.2521427273750305, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.006689482368528843}, {"id": 41, "seek": 25940, "start": 274.52, "end": 281.67999999999995, "text": " modules, etc. And for each kernel, DTB and root file system, they will use multiple times", "tokens": [51120, 16679, 11, 5183, 13, 400, 337, 1184, 28256, 11, 413, 51, 33, 293, 5593, 3991, 1185, 11, 436, 486, 764, 3866, 1413, 51478], "temperature": 0.0, "avg_logprob": -0.2521427273750305, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.006689482368528843}, {"id": 42, "seek": 25940, "start": 281.67999999999995, "end": 288.47999999999996, "text": " because when we have one commit from the kernel, we'll build it for multiple architectures.", "tokens": [51478, 570, 562, 321, 362, 472, 5599, 490, 264, 28256, 11, 321, 603, 1322, 309, 337, 3866, 6331, 1303, 13, 51818], "temperature": 0.0, "avg_logprob": -0.2521427273750305, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.006689482368528843}, {"id": 43, "seek": 28848, "start": 288.6, "end": 297.12, "text": " We'll build it for x86, ARMv7, ARMv8, ARMv9, PPC, SH4, MIPS, etc. Then for each architecture,", "tokens": [50370, 492, 603, 1322, 309, 337, 2031, 22193, 11, 45209, 85, 22, 11, 45209, 85, 23, 11, 45209, 85, 24, 11, 430, 12986, 11, 7405, 19, 11, 13696, 6273, 11, 5183, 13, 1396, 337, 1184, 9482, 11, 50796], "temperature": 0.0, "avg_logprob": -0.28155040740966797, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06342710554599762}, {"id": 44, "seek": 28848, "start": 297.12, "end": 302.64000000000004, "text": " we'll have multiple configurations. I want to build with some virtio-specific configuration.", "tokens": [50796, 321, 603, 362, 3866, 31493, 13, 286, 528, 281, 1322, 365, 512, 4480, 1004, 12, 29258, 11694, 13, 51072], "temperature": 0.0, "avg_logprob": -0.28155040740966797, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06342710554599762}, {"id": 45, "seek": 28848, "start": 302.64000000000004, "end": 309.32, "text": " I want to build in debug in release, etc. And then for each configuration, for each commit", "tokens": [51072, 286, 528, 281, 1322, 294, 24083, 294, 4374, 11, 5183, 13, 400, 550, 337, 1184, 11694, 11, 337, 1184, 5599, 51406], "temperature": 0.0, "avg_logprob": -0.28155040740966797, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06342710554599762}, {"id": 46, "seek": 28848, "start": 309.32, "end": 316.20000000000005, "text": " architecture configuration, I will run a set of tests. So, KSELTest, KUnit, libgperiod,", "tokens": [51406, 9482, 11694, 11, 286, 486, 1190, 257, 992, 295, 6921, 13, 407, 11, 591, 50, 3158, 51, 377, 11, 591, 12405, 270, 11, 22854, 70, 610, 2695, 11, 51750], "temperature": 0.0, "avg_logprob": -0.28155040740966797, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06342710554599762}, {"id": 47, "seek": 31620, "start": 316.24, "end": 323.88, "text": " the LTP, etc. Considering that LTP, for example, is broken into 20 different test suites that", "tokens": [50366, 264, 441, 16804, 11, 5183, 13, 33854, 300, 441, 16804, 11, 337, 1365, 11, 307, 5463, 666, 945, 819, 1500, 459, 3324, 300, 50748], "temperature": 0.0, "avg_logprob": -0.18861972490946452, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.0047389748506248}, {"id": 48, "seek": 31620, "start": 323.88, "end": 329.59999999999997, "text": " will be 20 different test jobs because it takes a lot of time to run. So, the CI system", "tokens": [50748, 486, 312, 945, 819, 1500, 4782, 570, 309, 2516, 257, 688, 295, 565, 281, 1190, 13, 407, 11, 264, 37777, 1185, 51034], "temperature": 0.0, "avg_logprob": -0.18861972490946452, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.0047389748506248}, {"id": 49, "seek": 31620, "start": 329.59999999999997, "end": 335.2, "text": " will run a lot of different jobs, of test jobs, that will actually pull the same artifacts", "tokens": [51034, 486, 1190, 257, 688, 295, 819, 4782, 11, 295, 1500, 4782, 11, 300, 486, 767, 2235, 264, 912, 24617, 51314], "temperature": 0.0, "avg_logprob": -0.18861972490946452, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.0047389748506248}, {"id": 50, "seek": 31620, "start": 335.2, "end": 340.84, "text": " all the time, which means that in the network, on the network in the lab in Cambridge, we", "tokens": [51314, 439, 264, 565, 11, 597, 1355, 300, 294, 264, 3209, 11, 322, 264, 3209, 294, 264, 2715, 294, 24876, 11, 321, 51596], "temperature": 0.0, "avg_logprob": -0.18861972490946452, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.0047389748506248}, {"id": 51, "seek": 31620, "start": 340.84, "end": 345.84, "text": " have a lot of network usage and a lot of duplication. We are re-downloading always the same artifacts.", "tokens": [51596, 362, 257, 688, 295, 3209, 14924, 293, 257, 688, 295, 17154, 399, 13, 492, 366, 319, 12, 5093, 2907, 278, 1009, 264, 912, 24617, 13, 51846], "temperature": 0.0, "avg_logprob": -0.18861972490946452, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.0047389748506248}, {"id": 52, "seek": 34620, "start": 347.08, "end": 355.47999999999996, "text": " So, that's normally really simple things to solve. You just add caching. So, just, I'm", "tokens": [50408, 407, 11, 300, 311, 5646, 534, 2199, 721, 281, 5039, 13, 509, 445, 909, 269, 2834, 13, 407, 11, 445, 11, 286, 478, 50828], "temperature": 0.0, "avg_logprob": -0.25314144511799236, "compression_ratio": 1.587719298245614, "no_speech_prob": 0.0022938151378184557}, {"id": 53, "seek": 34620, "start": 355.47999999999996, "end": 359.76, "text": " really adding that because that's really important. Our system, our CI system, the Lava", "tokens": [50828, 534, 5127, 300, 570, 300, 311, 534, 1021, 13, 2621, 1185, 11, 527, 37777, 1185, 11, 264, 441, 4061, 51042], "temperature": 0.0, "avg_logprob": -0.25314144511799236, "compression_ratio": 1.587719298245614, "no_speech_prob": 0.0022938151378184557}, {"id": 54, "seek": 34620, "start": 359.76, "end": 366.52, "text": " Workers, will download multiple times the same artifacts at the same time in parallel. So,", "tokens": [51042, 42375, 11, 486, 5484, 3866, 1413, 264, 912, 24617, 412, 264, 912, 565, 294, 8952, 13, 407, 11, 51380], "temperature": 0.0, "avg_logprob": -0.25314144511799236, "compression_ratio": 1.587719298245614, "no_speech_prob": 0.0022938151378184557}, {"id": 55, "seek": 34620, "start": 366.52, "end": 371.15999999999997, "text": " if you look for a caching proxy in the open-source community, you will obviously find that Squid", "tokens": [51380, 498, 291, 574, 337, 257, 269, 2834, 29690, 294, 264, 1269, 12, 41676, 1768, 11, 291, 486, 2745, 915, 300, 46178, 51612], "temperature": 0.0, "avg_logprob": -0.25314144511799236, "compression_ratio": 1.587719298245614, "no_speech_prob": 0.0022938151378184557}, {"id": 56, "seek": 37116, "start": 371.20000000000005, "end": 378.36, "text": " is the main caching proxy and it's a perfectly good one. It's really working well. So, you", "tokens": [50366, 307, 264, 2135, 269, 2834, 29690, 293, 309, 311, 257, 6239, 665, 472, 13, 467, 311, 534, 1364, 731, 13, 407, 11, 291, 50724], "temperature": 0.0, "avg_logprob": -0.21547893602020887, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.015743527561426163}, {"id": 57, "seek": 37116, "start": 378.36, "end": 383.48, "text": " should just install that on our network, point all the workers to it and it should work. Short", "tokens": [50724, 820, 445, 3625, 300, 322, 527, 3209, 11, 935, 439, 264, 5600, 281, 309, 293, 309, 820, 589, 13, 16881, 50980], "temperature": 0.0, "avg_logprob": -0.21547893602020887, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.015743527561426163}, {"id": 58, "seek": 37116, "start": 383.48, "end": 392.48, "text": " answer is no, it's not working just because of the two reasons above. So, and also for another", "tokens": [50980, 1867, 307, 572, 11, 309, 311, 406, 1364, 445, 570, 295, 264, 732, 4112, 3673, 13, 407, 11, 293, 611, 337, 1071, 51430], "temperature": 0.0, "avg_logprob": -0.21547893602020887, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.015743527561426163}, {"id": 59, "seek": 37116, "start": 392.48, "end": 398.6, "text": " reason, this one. All artifacts, as I said, are published in an S3 like bucket. They are", "tokens": [51430, 1778, 11, 341, 472, 13, 1057, 24617, 11, 382, 286, 848, 11, 366, 6572, 294, 364, 318, 18, 411, 13058, 13, 814, 366, 51736], "temperature": 0.0, "avg_logprob": -0.21547893602020887, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.015743527561426163}, {"id": 60, "seek": 39860, "start": 398.64000000000004, "end": 402.32000000000005, "text": " somewhere in the cloud. So, obviously, if you want to download them, you will download over", "tokens": [50366, 4079, 294, 264, 4588, 13, 407, 11, 2745, 11, 498, 291, 528, 281, 5484, 552, 11, 291, 486, 5484, 670, 50550], "temperature": 0.0, "avg_logprob": -0.16912385691767154, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.013119439594447613}, {"id": 61, "seek": 39860, "start": 402.32000000000005, "end": 409.32000000000005, "text": " HTTPS. You will not download a random binary from internet and run it in your local lab", "tokens": [50550, 11751, 51, 6273, 13, 509, 486, 406, 5484, 257, 4974, 17434, 490, 4705, 293, 1190, 309, 294, 428, 2654, 2715, 50900], "temperature": 0.0, "avg_logprob": -0.16912385691767154, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.013119439594447613}, {"id": 62, "seek": 39860, "start": 409.32000000000005, "end": 414.64000000000004, "text": " for testing. Not something that you will do. So, we have to validate. So, we use HTTPS to", "tokens": [50900, 337, 4997, 13, 1726, 746, 300, 291, 486, 360, 13, 407, 11, 321, 362, 281, 29562, 13, 407, 11, 321, 764, 11751, 51, 6273, 281, 51166], "temperature": 0.0, "avg_logprob": -0.16912385691767154, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.013119439594447613}, {"id": 63, "seek": 39860, "start": 414.64000000000004, "end": 418.04, "text": " be sure that what we're downloading is what we're expecting. At least we are trusting", "tokens": [51166, 312, 988, 300, 437, 321, 434, 32529, 307, 437, 321, 434, 9650, 13, 1711, 1935, 321, 366, 28235, 51336], "temperature": 0.0, "avg_logprob": -0.16912385691767154, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.013119439594447613}, {"id": 64, "seek": 39860, "start": 418.04, "end": 426.32000000000005, "text": " the software. But when you add a Squid proxy in the connection, it will not work well with", "tokens": [51336, 264, 4722, 13, 583, 562, 291, 909, 257, 46178, 29690, 294, 264, 4984, 11, 309, 486, 406, 589, 731, 365, 51750], "temperature": 0.0, "avg_logprob": -0.16912385691767154, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.013119439594447613}, {"id": 65, "seek": 42632, "start": 426.36, "end": 432.36, "text": " HTTPS. That written in the script documentation, you can make it work with that. It's not easy.", "tokens": [50366, 11751, 51, 6273, 13, 663, 3720, 294, 264, 5755, 14333, 11, 291, 393, 652, 309, 589, 365, 300, 13, 467, 311, 406, 1858, 13, 50666], "temperature": 0.0, "avg_logprob": -0.23781403628262607, "compression_ratio": 1.7689243027888446, "no_speech_prob": 0.01429762877523899}, {"id": 66, "seek": 42632, "start": 432.36, "end": 437.36, "text": " The main problem is that as an HTTP client, when you connect to a website over HTTPS,", "tokens": [50666, 440, 2135, 1154, 307, 300, 382, 364, 33283, 6423, 11, 562, 291, 1745, 281, 257, 3144, 670, 11751, 51, 6273, 11, 50916], "temperature": 0.0, "avg_logprob": -0.23781403628262607, "compression_ratio": 1.7689243027888446, "no_speech_prob": 0.01429762877523899}, {"id": 67, "seek": 42632, "start": 437.36, "end": 441.76, "text": " you're expecting to get a certificate and the connection will be encrypted with the", "tokens": [50916, 291, 434, 9650, 281, 483, 257, 15953, 293, 264, 4984, 486, 312, 36663, 365, 264, 51136], "temperature": 0.0, "avg_logprob": -0.23781403628262607, "compression_ratio": 1.7689243027888446, "no_speech_prob": 0.01429762877523899}, {"id": 68, "seek": 42632, "start": 441.76, "end": 446.56, "text": " certificate and the certificate, you have to trust it. When you add Squid in the middle,", "tokens": [51136, 15953, 293, 264, 15953, 11, 291, 362, 281, 3361, 309, 13, 1133, 291, 909, 46178, 294, 264, 2808, 11, 51376], "temperature": 0.0, "avg_logprob": -0.23781403628262607, "compression_ratio": 1.7689243027888446, "no_speech_prob": 0.01429762877523899}, {"id": 69, "seek": 42632, "start": 446.56, "end": 452.4, "text": " Squid will have to connect on your BI to the server. So, the connection between Squid and", "tokens": [51376, 46178, 486, 362, 281, 1745, 322, 428, 23524, 281, 264, 7154, 13, 407, 11, 264, 4984, 1296, 46178, 293, 51668], "temperature": 0.0, "avg_logprob": -0.23781403628262607, "compression_ratio": 1.7689243027888446, "no_speech_prob": 0.01429762877523899}, {"id": 70, "seek": 45240, "start": 452.47999999999996, "end": 458.97999999999996, "text": " the website is encrypted correctly. The certificate written by the website is a legit one, so it", "tokens": [50368, 264, 3144, 307, 36663, 8944, 13, 440, 15953, 3720, 538, 264, 3144, 307, 257, 10275, 472, 11, 370, 309, 50693], "temperature": 0.0, "avg_logprob": -0.16615047617855236, "compression_ratio": 1.8930041152263375, "no_speech_prob": 0.028876272961497307}, {"id": 71, "seek": 45240, "start": 458.97999999999996, "end": 463.97999999999996, "text": " will work. But when Squid will have to decrypt the content to cache it and then re-encrypt", "tokens": [50693, 486, 589, 13, 583, 562, 46178, 486, 362, 281, 979, 627, 662, 264, 2701, 281, 19459, 309, 293, 550, 319, 12, 22660, 627, 662, 50943], "temperature": 0.0, "avg_logprob": -0.16615047617855236, "compression_ratio": 1.8930041152263375, "no_speech_prob": 0.028876272961497307}, {"id": 72, "seek": 45240, "start": 463.97999999999996, "end": 468.97999999999996, "text": " it to send it back to you, it does not have the private certificate from the website,", "tokens": [50943, 309, 281, 2845, 309, 646, 281, 291, 11, 309, 775, 406, 362, 264, 4551, 15953, 490, 264, 3144, 11, 51193], "temperature": 0.0, "avg_logprob": -0.16615047617855236, "compression_ratio": 1.8930041152263375, "no_speech_prob": 0.028876272961497307}, {"id": 73, "seek": 45240, "start": 468.97999999999996, "end": 473.97999999999996, "text": " obviously. You don't have the private certificate of Google.com on your machine, so you cannot", "tokens": [51193, 2745, 13, 509, 500, 380, 362, 264, 4551, 15953, 295, 3329, 13, 1112, 322, 428, 3479, 11, 370, 291, 2644, 51443], "temperature": 0.0, "avg_logprob": -0.16615047617855236, "compression_ratio": 1.8930041152263375, "no_speech_prob": 0.028876272961497307}, {"id": 74, "seek": 45240, "start": 473.97999999999996, "end": 480.28, "text": " re-encrypt the traffic. So, Squid will need to have his own certificate and it will encrypt", "tokens": [51443, 319, 12, 22660, 627, 662, 264, 6419, 13, 407, 11, 46178, 486, 643, 281, 362, 702, 1065, 15953, 293, 309, 486, 17972, 662, 51758], "temperature": 0.0, "avg_logprob": -0.16615047617855236, "compression_ratio": 1.8930041152263375, "no_speech_prob": 0.028876272961497307}, {"id": 75, "seek": 48028, "start": 480.35999999999996, "end": 484.67999999999995, "text": " the traffic with its own asset certificate. And you will obviously not trust it. You will", "tokens": [50368, 264, 6419, 365, 1080, 1065, 11999, 15953, 13, 400, 291, 486, 2745, 406, 3361, 309, 13, 509, 486, 50584], "temperature": 0.0, "avg_logprob": -0.20613632202148438, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.008320457302033901}, {"id": 76, "seek": 48028, "start": 484.67999999999995, "end": 491.67999999999995, "text": " not trust your local Squid proxy to sign something from Google.com or AWS or any website", "tokens": [50584, 406, 3361, 428, 2654, 46178, 29690, 281, 1465, 746, 490, 3329, 13, 1112, 420, 17650, 420, 604, 3144, 50934], "temperature": 0.0, "avg_logprob": -0.20613632202148438, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.008320457302033901}, {"id": 77, "seek": 48028, "start": 491.67999999999995, "end": 496.88, "text": " or Linux Foundation. So, when the HTTP client receives the custom asset certificate, it", "tokens": [50934, 420, 18734, 10335, 13, 407, 11, 562, 264, 33283, 6423, 20717, 264, 2375, 11999, 15953, 11, 309, 51194], "temperature": 0.0, "avg_logprob": -0.20613632202148438, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.008320457302033901}, {"id": 78, "seek": 48028, "start": 496.88, "end": 502.15999999999997, "text": " will just say, no, I don't trust you. There is a workaround and it's written in the script", "tokens": [51194, 486, 445, 584, 11, 572, 11, 286, 500, 380, 3361, 291, 13, 821, 307, 257, 589, 25762, 293, 309, 311, 3720, 294, 264, 5755, 51458], "temperature": 0.0, "avg_logprob": -0.20613632202148438, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.008320457302033901}, {"id": 79, "seek": 48028, "start": 502.15999999999997, "end": 506.59999999999997, "text": " documentation, obviously, which is create a wildcard certificate, which is a certificate", "tokens": [51458, 14333, 11, 2745, 11, 597, 307, 1884, 257, 4868, 22259, 15953, 11, 597, 307, 257, 15953, 51680], "temperature": 0.0, "avg_logprob": -0.20613632202148438, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.008320457302033901}, {"id": 80, "seek": 50660, "start": 506.6, "end": 513.6, "text": " that will be valid for absolutely every website on the planet, every DNS, so it's kind of", "tokens": [50364, 300, 486, 312, 7363, 337, 3122, 633, 3144, 322, 264, 5054, 11, 633, 35153, 11, 370, 309, 311, 733, 295, 50714], "temperature": 0.0, "avg_logprob": -0.2640793973749334, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.02288387157022953}, {"id": 81, "seek": 50660, "start": 513.6, "end": 520.6, "text": " a dangerous asset certificate. And you can install it on every of your HTTP clients.", "tokens": [50714, 257, 5795, 11999, 15953, 13, 400, 291, 393, 3625, 309, 322, 633, 295, 428, 33283, 6982, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2640793973749334, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.02288387157022953}, {"id": 82, "seek": 50660, "start": 520.84, "end": 527.84, "text": " It's possible, but it's really crappy, honestly. That's the first problem. The second problem", "tokens": [51076, 467, 311, 1944, 11, 457, 309, 311, 534, 36531, 11, 6095, 13, 663, 311, 264, 700, 1154, 13, 440, 1150, 1154, 51426], "temperature": 0.0, "avg_logprob": -0.2640793973749334, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.02288387157022953}, {"id": 83, "seek": 50660, "start": 527.84, "end": 533.28, "text": " and that there is no way to work around it is that when Squid, when you try to download", "tokens": [51426, 293, 300, 456, 307, 572, 636, 281, 589, 926, 309, 307, 300, 562, 46178, 11, 562, 291, 853, 281, 5484, 51698], "temperature": 0.0, "avg_logprob": -0.2640793973749334, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.02288387157022953}, {"id": 84, "seek": 53328, "start": 533.64, "end": 537.52, "text": " multiple times the same artifact in Squid, so, for example, you have two connections", "tokens": [50382, 3866, 1413, 264, 912, 34806, 294, 46178, 11, 370, 11, 337, 1365, 11, 291, 362, 732, 9271, 50576], "temperature": 0.0, "avg_logprob": -0.24182957142322986, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.015738874673843384}, {"id": 85, "seek": 53328, "start": 537.52, "end": 544.4, "text": " downloading the same root FS, Squid will download it twice and stream it back to the", "tokens": [50576, 32529, 264, 912, 5593, 41138, 11, 46178, 486, 5484, 309, 6091, 293, 4309, 309, 646, 281, 264, 50920], "temperature": 0.0, "avg_logprob": -0.24182957142322986, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.015738874673843384}, {"id": 86, "seek": 53328, "start": 544.4, "end": 549.68, "text": " clients at the same time. And when it's downloaded, it's finished, then the third connection will", "tokens": [50920, 6982, 412, 264, 912, 565, 13, 400, 562, 309, 311, 21748, 11, 309, 311, 4335, 11, 550, 264, 2636, 4984, 486, 51184], "temperature": 0.0, "avg_logprob": -0.24182957142322986, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.015738874673843384}, {"id": 87, "seek": 53328, "start": 549.68, "end": 554.16, "text": " have a cache version. But as long as it's not cached locally, it will re-download from", "tokens": [51184, 362, 257, 19459, 3037, 13, 583, 382, 938, 382, 309, 311, 406, 269, 15095, 16143, 11, 309, 486, 319, 12, 5093, 2907, 490, 51408], "temperature": 0.0, "avg_logprob": -0.24182957142322986, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.015738874673843384}, {"id": 88, "seek": 53328, "start": 554.16, "end": 560.56, "text": " the start. And as I said before, our system is by-designed running everything in parallel,", "tokens": [51408, 264, 722, 13, 400, 382, 286, 848, 949, 11, 527, 1185, 307, 538, 12, 14792, 16690, 2614, 1203, 294, 8952, 11, 51728], "temperature": 0.0, "avg_logprob": -0.24182957142322986, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.015738874673843384}, {"id": 89, "seek": 56056, "start": 560.56, "end": 565.56, "text": " so it's often the case that we have multiple downloads of the same artifact at the exact", "tokens": [50364, 370, 309, 311, 2049, 264, 1389, 300, 321, 362, 3866, 36553, 295, 264, 912, 34806, 412, 264, 1900, 50614], "temperature": 0.0, "avg_logprob": -0.1969709710760431, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.0012774831848219037}, {"id": 90, "seek": 56056, "start": 565.56, "end": 572.56, "text": " same time. So when using Squid, it was just not caching anything. Sorry. So that's why", "tokens": [50614, 912, 565, 13, 407, 562, 1228, 46178, 11, 309, 390, 445, 406, 269, 2834, 1340, 13, 4919, 13, 407, 300, 311, 983, 50964], "temperature": 0.0, "avg_logprob": -0.1969709710760431, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.0012774831848219037}, {"id": 91, "seek": 56056, "start": 573.76, "end": 580.76, "text": " we created KeysCache. So Keys stands for keep it simple, stupid. It's a pretty simple and", "tokens": [51024, 321, 2942, 43733, 34, 6000, 13, 407, 43733, 7382, 337, 1066, 309, 2199, 11, 6631, 13, 467, 311, 257, 1238, 2199, 293, 51374], "temperature": 0.0, "avg_logprob": -0.1969709710760431, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.0012774831848219037}, {"id": 92, "seek": 56056, "start": 580.9599999999999, "end": 585.56, "text": " stupid caching service. But the main features that it has are exactly what we need for a", "tokens": [51384, 6631, 269, 2834, 2643, 13, 583, 264, 2135, 4122, 300, 309, 575, 366, 2293, 437, 321, 643, 337, 257, 51614], "temperature": 0.0, "avg_logprob": -0.1969709710760431, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.0012774831848219037}, {"id": 93, "seek": 58556, "start": 585.56, "end": 591.68, "text": " API system. It allows to cache HTTPS resources without any acts or anything. It allows to", "tokens": [50364, 9362, 1185, 13, 467, 4045, 281, 19459, 11751, 51, 6273, 3593, 1553, 604, 10672, 420, 1340, 13, 467, 4045, 281, 50670], "temperature": 0.0, "avg_logprob": -0.23030530360707066, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.043099239468574524}, {"id": 94, "seek": 58556, "start": 591.68, "end": 596.76, "text": " download only once, even if you have multiple clients and they will all get a stream back,", "tokens": [50670, 5484, 787, 1564, 11, 754, 498, 291, 362, 3866, 6982, 293, 436, 486, 439, 483, 257, 4309, 646, 11, 50924], "temperature": 0.0, "avg_logprob": -0.23030530360707066, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.043099239468574524}, {"id": 95, "seek": 58556, "start": 596.76, "end": 601.8, "text": " the stream of data back. And the reason why it's not, it's working for both cases is that", "tokens": [50924, 264, 4309, 295, 1412, 646, 13, 400, 264, 1778, 983, 309, 311, 406, 11, 309, 311, 1364, 337, 1293, 3331, 307, 300, 51176], "temperature": 0.0, "avg_logprob": -0.23030530360707066, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.043099239468574524}, {"id": 96, "seek": 58556, "start": 601.8, "end": 606.5999999999999, "text": " it's not a transparent proxy. So it's not like clients that will know from an environment", "tokens": [51176, 309, 311, 406, 257, 12737, 29690, 13, 407, 309, 311, 406, 411, 6982, 300, 486, 458, 490, 364, 2823, 51416], "temperature": 0.0, "avg_logprob": -0.23030530360707066, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.043099239468574524}, {"id": 97, "seek": 58556, "start": 606.5999999999999, "end": 611.7199999999999, "text": " of the Bible that it has to go through a proxy. Instead, you have to prefix your URLs. So", "tokens": [51416, 295, 264, 6544, 300, 309, 575, 281, 352, 807, 257, 29690, 13, 7156, 11, 291, 362, 281, 46969, 428, 43267, 13, 407, 51672], "temperature": 0.0, "avg_logprob": -0.23030530360707066, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.043099239468574524}, {"id": 98, "seek": 61172, "start": 611.76, "end": 617.46, "text": " if you want to access example.com slash .fs.x4, for example, you have to prefix it by your", "tokens": [50366, 498, 291, 528, 281, 2105, 1365, 13, 1112, 17330, 2411, 16883, 13, 87, 19, 11, 337, 1365, 11, 291, 362, 281, 46969, 309, 538, 428, 50651], "temperature": 0.0, "avg_logprob": -0.22185117548162286, "compression_ratio": 1.6081081081081081, "no_speech_prob": 0.0036558727733790874}, {"id": 99, "seek": 61172, "start": 617.46, "end": 624.46, "text": " KeysCache instance. So even if you're downloading over for HTTPS, your clients know that it", "tokens": [50651, 43733, 34, 6000, 5197, 13, 407, 754, 498, 291, 434, 32529, 670, 337, 11751, 51, 6273, 11, 428, 6982, 458, 300, 309, 51001], "temperature": 0.0, "avg_logprob": -0.22185117548162286, "compression_ratio": 1.6081081081081081, "no_speech_prob": 0.0036558727733790874}, {"id": 100, "seek": 61172, "start": 624.5600000000001, "end": 630.28, "text": " goes to KeysCache and not example.com so that it's expecting a certificate from KeysCache,", "tokens": [51006, 1709, 281, 43733, 34, 6000, 293, 406, 1365, 13, 1112, 370, 300, 309, 311, 9650, 257, 15953, 490, 43733, 34, 6000, 11, 51292], "temperature": 0.0, "avg_logprob": -0.22185117548162286, "compression_ratio": 1.6081081081081081, "no_speech_prob": 0.0036558727733790874}, {"id": 101, "seek": 61172, "start": 630.28, "end": 637.28, "text": " not from the original website. That's the first reason. And KeysCache also, we made", "tokens": [51292, 406, 490, 264, 3380, 3144, 13, 663, 311, 264, 700, 1778, 13, 400, 43733, 34, 6000, 611, 11, 321, 1027, 51642], "temperature": 0.0, "avg_logprob": -0.22185117548162286, "compression_ratio": 1.6081081081081081, "no_speech_prob": 0.0036558727733790874}, {"id": 102, "seek": 63728, "start": 637.76, "end": 644.76, "text": " it so it knows how to stream back to multiple clients, the same content. Fun thing, we also", "tokens": [50388, 309, 370, 309, 3255, 577, 281, 4309, 646, 281, 3866, 6982, 11, 264, 912, 2701, 13, 11166, 551, 11, 321, 611, 50738], "temperature": 0.0, "avg_logprob": -0.2533982664674193, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006220259238034487}, {"id": 103, "seek": 63728, "start": 645.0799999999999, "end": 650.8, "text": " added a lot of automatic retries inside the KeysCache backends. So if for any reason,", "tokens": [50754, 3869, 257, 688, 295, 12509, 1533, 2244, 1854, 264, 43733, 34, 6000, 646, 2581, 13, 407, 498, 337, 604, 1778, 11, 51040], "temperature": 0.0, "avg_logprob": -0.2533982664674193, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006220259238034487}, {"id": 104, "seek": 63728, "start": 650.8, "end": 656.36, "text": " and it happens a lot, the connection between your network and the S3 like bucket breaks", "tokens": [51040, 293, 309, 2314, 257, 688, 11, 264, 4984, 1296, 428, 3209, 293, 264, 318, 18, 411, 13058, 9857, 51318], "temperature": 0.0, "avg_logprob": -0.2533982664674193, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006220259238034487}, {"id": 105, "seek": 63728, "start": 656.36, "end": 661.16, "text": " and it often breaks, honestly, KeysCache backend will automatically retries. This is a list", "tokens": [51318, 293, 309, 2049, 9857, 11, 6095, 11, 43733, 34, 6000, 646, 521, 486, 6772, 1533, 2244, 13, 639, 307, 257, 1329, 51558], "temperature": 0.0, "avg_logprob": -0.2533982664674193, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006220259238034487}, {"id": 106, "seek": 66116, "start": 661.16, "end": 668.16, "text": " of HTTP codes that we're retrying automatically. And it will also, so when it's retrying, it", "tokens": [50364, 295, 33283, 14211, 300, 321, 434, 1533, 19076, 6772, 13, 400, 309, 486, 611, 11, 370, 562, 309, 311, 1533, 19076, 11, 309, 50714], "temperature": 0.0, "avg_logprob": -0.2874687896377739, "compression_ratio": 1.5739910313901346, "no_speech_prob": 0.009984463453292847}, {"id": 107, "seek": 66116, "start": 668.24, "end": 675.24, "text": " retries up to 50 times over a period of two hours because we had exponential backups.", "tokens": [50718, 1533, 2244, 493, 281, 2625, 1413, 670, 257, 2896, 295, 732, 2496, 570, 321, 632, 21510, 50160, 13, 51068], "temperature": 0.0, "avg_logprob": -0.2874687896377739, "compression_ratio": 1.5739910313901346, "no_speech_prob": 0.009984463453292847}, {"id": 108, "seek": 66116, "start": 675.28, "end": 680.12, "text": " So sometimes a download will actually take two hours and 50 retries just because the", "tokens": [51070, 407, 2171, 257, 5484, 486, 767, 747, 732, 2496, 293, 2625, 1533, 2244, 445, 570, 264, 51312], "temperature": 0.0, "avg_logprob": -0.2874687896377739, "compression_ratio": 1.5739910313901346, "no_speech_prob": 0.009984463453292847}, {"id": 109, "seek": 66116, "start": 680.12, "end": 686.04, "text": " S3 like bucket is just sometimes a bit buggy to answer. We also added partial download,", "tokens": [51312, 318, 18, 411, 13058, 307, 445, 2171, 257, 857, 7426, 1480, 281, 1867, 13, 492, 611, 3869, 14641, 5484, 11, 51608], "temperature": 0.0, "avg_logprob": -0.2874687896377739, "compression_ratio": 1.5739910313901346, "no_speech_prob": 0.009984463453292847}, {"id": 110, "seek": 68604, "start": 686.16, "end": 691.1999999999999, "text": " which when you have, we do a retry, if the HTTP server knows how to do that, we only", "tokens": [50370, 597, 562, 291, 362, 11, 321, 360, 257, 1533, 627, 11, 498, 264, 33283, 7154, 3255, 577, 281, 360, 300, 11, 321, 787, 50622], "temperature": 0.0, "avg_logprob": -0.17100507562810724, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.014334063045680523}, {"id": 111, "seek": 68604, "start": 691.1999999999999, "end": 695.9599999999999, "text": " download the remaining content, not from the start. And the good thing is that with", "tokens": [50622, 5484, 264, 8877, 2701, 11, 406, 490, 264, 722, 13, 400, 264, 665, 551, 307, 300, 365, 50860], "temperature": 0.0, "avg_logprob": -0.17100507562810724, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.014334063045680523}, {"id": 112, "seek": 68604, "start": 695.9599999999999, "end": 700.92, "text": " the automatic retries, the client will never see that there is a broken connection because", "tokens": [50860, 264, 12509, 1533, 2244, 11, 264, 6423, 486, 1128, 536, 300, 456, 307, 257, 5463, 4984, 570, 51108], "temperature": 0.0, "avg_logprob": -0.17100507562810724, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.014334063045680523}, {"id": 113, "seek": 68604, "start": 700.92, "end": 705.28, "text": " from the client to KeysCache, the connection is kept alive. It's only the backends that", "tokens": [51108, 490, 264, 6423, 281, 43733, 34, 6000, 11, 264, 4984, 307, 4305, 5465, 13, 467, 311, 787, 264, 646, 2581, 300, 51326], "temperature": 0.0, "avg_logprob": -0.17100507562810724, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.014334063045680523}, {"id": 114, "seek": 68604, "start": 705.28, "end": 712.28, "text": " sees the network issues. So it has been in production for 3.5 years. It downloaded 32", "tokens": [51326, 8194, 264, 3209, 2663, 13, 407, 309, 575, 668, 294, 4265, 337, 805, 13, 20, 924, 13, 467, 21748, 8858, 51676], "temperature": 0.0, "avg_logprob": -0.17100507562810724, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.014334063045680523}, {"id": 115, "seek": 71228, "start": 713.28, "end": 720.12, "text": " terabits of data from internet and served 1.6 petabytes of data locally just for a really", "tokens": [50414, 1796, 455, 1208, 295, 1412, 490, 4705, 293, 7584, 502, 13, 21, 3817, 24538, 295, 1412, 16143, 445, 337, 257, 534, 50756], "temperature": 0.0, "avg_logprob": -0.24001562330457898, "compression_ratio": 1.56, "no_speech_prob": 0.013495748862624168}, {"id": 116, "seek": 71228, "start": 720.12, "end": 725.72, "text": " small tiny software, which is an expansion ratio of 51 times. So we divided the network", "tokens": [50756, 1359, 5870, 4722, 11, 597, 307, 364, 11260, 8509, 295, 18485, 1413, 13, 407, 321, 6666, 264, 3209, 51036], "temperature": 0.0, "avg_logprob": -0.24001562330457898, "compression_ratio": 1.56, "no_speech_prob": 0.013495748862624168}, {"id": 117, "seek": 71228, "start": 725.72, "end": 732.72, "text": " usage by 51 just by having a small working proxy. It also improved a lot of stability", "tokens": [51036, 14924, 538, 18485, 445, 538, 1419, 257, 1359, 1364, 29690, 13, 467, 611, 9689, 257, 688, 295, 11826, 51386], "temperature": 0.0, "avg_logprob": -0.24001562330457898, "compression_ratio": 1.56, "no_speech_prob": 0.013495748862624168}, {"id": 118, "seek": 71228, "start": 732.76, "end": 739.76, "text": " thanks to the automatic retries, I said, up to 50 retries, which is insane. And it also", "tokens": [51388, 3231, 281, 264, 12509, 1533, 2244, 11, 286, 848, 11, 493, 281, 2625, 1533, 2244, 11, 597, 307, 10838, 13, 400, 309, 611, 51738], "temperature": 0.0, "avg_logprob": -0.24001562330457898, "compression_ratio": 1.56, "no_speech_prob": 0.013495748862624168}, {"id": 119, "seek": 73976, "start": 740.24, "end": 746.24, "text": " lowered a lot of the S3 egress cost because you have to pay for egress in the cloud. When", "tokens": [50388, 28466, 257, 688, 295, 264, 318, 18, 308, 3091, 2063, 570, 291, 362, 281, 1689, 337, 308, 3091, 294, 264, 4588, 13, 1133, 50688], "temperature": 0.0, "avg_logprob": -0.28525312323319285, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.003936414141207933}, {"id": 120, "seek": 73976, "start": 746.24, "end": 753.24, "text": " you, for 1.6 petabytes of data, that's a lot of money. So yeah, we saved around 150", "tokens": [50688, 291, 11, 337, 502, 13, 21, 3817, 24538, 295, 1412, 11, 300, 311, 257, 688, 295, 1460, 13, 407, 1338, 11, 321, 6624, 926, 8451, 51038], "temperature": 0.0, "avg_logprob": -0.28525312323319285, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.003936414141207933}, {"id": 121, "seek": 73976, "start": 754.88, "end": 761.88, "text": " K of euros just by having a local proxy. Just because I have just two minutes, a look at", "tokens": [51120, 591, 295, 14160, 445, 538, 1419, 257, 2654, 29690, 13, 1449, 570, 286, 362, 445, 732, 2077, 11, 257, 574, 412, 51470], "temperature": 0.0, "avg_logprob": -0.28525312323319285, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.003936414141207933}, {"id": 122, "seek": 73976, "start": 763.36, "end": 768.76, "text": " the global architecture of the service, it suggests a Django application with a salary", "tokens": [51544, 264, 4338, 9482, 295, 264, 2643, 11, 309, 13409, 257, 33464, 17150, 3861, 365, 257, 15360, 51814], "temperature": 0.0, "avg_logprob": -0.28525312323319285, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.003936414141207933}, {"id": 123, "seek": 76876, "start": 769.16, "end": 775.68, "text": " backends. So you have a reverse proxy and Ginex. It can be any reverse proxy in fact,", "tokens": [50384, 646, 2581, 13, 407, 291, 362, 257, 9943, 29690, 293, 460, 533, 87, 13, 467, 393, 312, 604, 9943, 29690, 294, 1186, 11, 50710], "temperature": 0.0, "avg_logprob": -0.3749136876578283, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.008402911946177483}, {"id": 124, "seek": 76876, "start": 775.68, "end": 782.68, "text": " that will receive an HTTP connection. It will send that to Giniacon, which is a Django", "tokens": [50710, 300, 486, 4774, 364, 33283, 4984, 13, 467, 486, 2845, 300, 281, 460, 3812, 326, 266, 11, 597, 307, 257, 33464, 17150, 51060], "temperature": 0.0, "avg_logprob": -0.3749136876578283, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.008402911946177483}, {"id": 125, "seek": 76876, "start": 782.8, "end": 788.8, "text": " runtime. The Django will see if the, we look at the database, but at the base, progress,", "tokens": [51066, 34474, 13, 440, 33464, 17150, 486, 536, 498, 264, 11, 321, 574, 412, 264, 8149, 11, 457, 412, 264, 3096, 11, 4205, 11, 51366], "temperature": 0.0, "avg_logprob": -0.3749136876578283, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.008402911946177483}, {"id": 126, "seek": 76876, "start": 788.8, "end": 794.04, "text": " to know if the artifact has been downloaded already or not. If it's a case, it will then", "tokens": [51366, 281, 458, 498, 264, 34806, 575, 668, 21748, 1217, 420, 406, 13, 759, 309, 311, 257, 1389, 11, 309, 486, 550, 51628], "temperature": 0.0, "avg_logprob": -0.3749136876578283, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.008402911946177483}, {"id": 127, "seek": 79404, "start": 794.0799999999999, "end": 798.4399999999999, "text": " look at the file system and just give that back to Ginex saying, please send that to", "tokens": [50366, 574, 412, 264, 3991, 1185, 293, 445, 976, 300, 646, 281, 460, 533, 87, 1566, 11, 1767, 2845, 300, 281, 50584], "temperature": 0.0, "avg_logprob": -0.20665224393208823, "compression_ratio": 1.768, "no_speech_prob": 0.014170607551932335}, {"id": 128, "seek": 79404, "start": 798.4399999999999, "end": 803.7199999999999, "text": " the client. And I'm done with it. If it's not already downloaded, it will send a message", "tokens": [50584, 264, 6423, 13, 400, 286, 478, 1096, 365, 309, 13, 759, 309, 311, 406, 1217, 21748, 11, 309, 486, 2845, 257, 3636, 50848], "temperature": 0.0, "avg_logprob": -0.20665224393208823, "compression_ratio": 1.768, "no_speech_prob": 0.014170607551932335}, {"id": 129, "seek": 79404, "start": 803.7199999999999, "end": 809.4, "text": " to Redis that will spawn a salary task that will actually do the download and retry in", "tokens": [50848, 281, 4477, 271, 300, 486, 17088, 257, 15360, 5633, 300, 486, 767, 360, 264, 5484, 293, 1533, 627, 294, 51132], "temperature": 0.0, "avg_logprob": -0.20665224393208823, "compression_ratio": 1.768, "no_speech_prob": 0.014170607551932335}, {"id": 130, "seek": 79404, "start": 809.4, "end": 815.12, "text": " the back end. And it's done only once. And it's then saving it to the file system, appending", "tokens": [51132, 264, 646, 917, 13, 400, 309, 311, 1096, 787, 1564, 13, 400, 309, 311, 550, 6816, 309, 281, 264, 3991, 1185, 11, 724, 2029, 51418], "temperature": 0.0, "avg_logprob": -0.20665224393208823, "compression_ratio": 1.768, "no_speech_prob": 0.014170607551932335}, {"id": 131, "seek": 79404, "start": 815.12, "end": 822.12, "text": " to a file, byte by byte. And at the same time, the Django process just reads the file on", "tokens": [51418, 281, 257, 3991, 11, 40846, 538, 40846, 13, 400, 412, 264, 912, 565, 11, 264, 33464, 17150, 1399, 445, 15700, 264, 3991, 322, 51768], "temperature": 0.0, "avg_logprob": -0.20665224393208823, "compression_ratio": 1.768, "no_speech_prob": 0.014170607551932335}, {"id": 132, "seek": 82212, "start": 822.12, "end": 825.72, "text": " the file system and sends the bytes where they are available. And that's all. Waiting", "tokens": [50364, 264, 3991, 1185, 293, 14790, 264, 36088, 689, 436, 366, 2435, 13, 400, 300, 311, 439, 13, 37291, 50544], "temperature": 0.0, "avg_logprob": -0.21334123075678108, "compression_ratio": 1.8131313131313131, "no_speech_prob": 0.005264608189463615}, {"id": 133, "seek": 82212, "start": 825.72, "end": 832.72, "text": " for the file, the file to be just finished. And if a second or third of many different", "tokens": [50544, 337, 264, 3991, 11, 264, 3991, 281, 312, 445, 4335, 13, 400, 498, 257, 1150, 420, 2636, 295, 867, 819, 50894], "temperature": 0.0, "avg_logprob": -0.21334123075678108, "compression_ratio": 1.8131313131313131, "no_speech_prob": 0.005264608189463615}, {"id": 134, "seek": 82212, "start": 833.88, "end": 839.4, "text": " users arrive for the same file, then they will just reuse what is already available in the", "tokens": [50952, 5022, 8881, 337, 264, 912, 3991, 11, 550, 436, 486, 445, 26225, 437, 307, 1217, 2435, 294, 264, 51228], "temperature": 0.0, "avg_logprob": -0.21334123075678108, "compression_ratio": 1.8131313131313131, "no_speech_prob": 0.005264608189463615}, {"id": 135, "seek": 82212, "start": 839.4, "end": 846.4, "text": " file system and wait for the download to finish. And that's all. That's all. It's pretty simple", "tokens": [51228, 3991, 1185, 293, 1699, 337, 264, 5484, 281, 2413, 13, 400, 300, 311, 439, 13, 663, 311, 439, 13, 467, 311, 1238, 2199, 51578], "temperature": 0.0, "avg_logprob": -0.21334123075678108, "compression_ratio": 1.8131313131313131, "no_speech_prob": 0.005264608189463615}, {"id": 136, "seek": 84640, "start": 847.4, "end": 852.4, "text": " and efficient. And it has been a really good use for us. And it might be useful for your", "tokens": [50414, 293, 7148, 13, 400, 309, 575, 668, 257, 534, 665, 764, 337, 505, 13, 400, 309, 1062, 312, 4420, 337, 428, 50664], "temperature": 0.0, "avg_logprob": -0.2885610292542656, "compression_ratio": 1.3503649635036497, "no_speech_prob": 0.012229816056787968}, {"id": 137, "seek": 84640, "start": 852.4, "end": 857.4, "text": " CI system. So if you have any questions, I will be here after the talk. Thanks a lot.", "tokens": [50664, 37777, 1185, 13, 407, 498, 291, 362, 604, 1651, 11, 286, 486, 312, 510, 934, 264, 751, 13, 2561, 257, 688, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2885610292542656, "compression_ratio": 1.3503649635036497, "no_speech_prob": 0.012229816056787968}, {"id": 138, "seek": 84640, "start": 857.4, "end": 858.4, "text": " Thank you.", "tokens": [50914, 1044, 291, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2885610292542656, "compression_ratio": 1.3503649635036497, "no_speech_prob": 0.012229816056787968}], "language": "en"}