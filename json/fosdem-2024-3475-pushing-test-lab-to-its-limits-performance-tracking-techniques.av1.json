{"text": " Hello everyone, my name is Pawe\u0142 Wietzorek. I work with Colabora and I've been involved in maintenance of server side components of Colabora's automated testing laboratory. Today, I would like to share with you a few lessons learned from that experience, particularly related to tracking laboratories performance and pushing beyond the limits of the software that it runs. We'll start with some background information. Next I will move to interactive approaches for tracking its performance, I mean the lab performance. After that, I'll describe a few solutions for automating that and finally I will also share some thoughts on data generation. So let's start with the reason why, I mean what brought us here today. Thanks to Remi's talk, we now know and have an idea of what Lava is, what it provides for testing automation and how it supports all these efforts. Some of you might also recall a talk given by my colleague Laura at last year's FOSDEM. Laura described in her talk how the lab at Colabora is set up, what its day-to-day maintenance tasks look like. What main challenges are while running this kind of laboratory and also shared some best practices. The key piece of information for us today is that Colabora's lab is a fairly large Lava instance that is continuously growing and together with high number of devices also comes high numbers of test job submissions to process which unsurprisingly can result in higher load on server side of things. And that in fact was our case. There was no need to panic though, at least not right away. High load means that the resources that were allocated for lab purposes are in use and that's what they are meant to do after all. Interestingly, especially high load was observed on the nodes running database processes. And all of that is mostly fine until the system becomes unresponsive. This might lead to potentially unreliable lab or even unusable for higher level test systems like MESA-CI or Kernel-CI on the screenshot which other Colabora's are involved in development, maintenance and of course usage as well. My first thought was to simply estimate what resources are required for day to day operations and simply throw them at this workload. This could work short term but it wouldn't really solve the problem. To do it the right way, a deeper understanding of the root cause for all these issues was needed. And by the way, this photo is from Polish IT Olympics where hardware component throwing contest is held. And while this is hard drive throwing contest which might not be the type of resource we needed, that was the initial idea. Thanks to RemiStock we also have rough idea of what main components for Lava are but let's recap them real quick. At the very high level Lava on the server side has two main components, a scheduler and a connection to the database. If we take a closer look, those are respectively a jungle application and by default a Postgres database. These are widely known used and mostly loved software components so we can make use of several already available performance tracking tools for them. So let's go through a few interactive or semi interactive ones. As tribal as it might sound, it is equally as important to start with simply enabling verbose logging on affected instances. This way we get first insights from redoing user stories based either on direct reports from users or maybe motomo statistics collected by recent Lava releases or maybe some logs from load balancer which shows us which API endpoints are mostly used by users or which views are most commonly requested. In case of Django we get a few other perks. It's as easy as literally flipping a switch. Django for database also allows to log every statement executed on the database in debug mode and it can be also easily extended with some additional profiling information. But even though there are all these perks, all this information is a post-action information. To collect it in a truly interactive manner, fortunately Django already has us covered and provides just the right tool for this purpose which is Django Debug toolbar. It isn't much harder to enable than just verbose logging. It just requires adding an additional Python package to your deployment, set internal IPs from which Debug toolbar would be available, confirm enabling it and you're good to go. Debug toolbar not only provides great and immediate feedback but also includes traces, some additional profiling information and it gives you all of that in an easy to use graphical user-friendly way. As you can see on the right-hand side of the screenshot you even get all the requests sent, the instance and all the SQL statements run. But even though these tools are easy to enable, it comes with some drawbacks as well. These tools should not be used on any user-facing instance which brings us to setting up a personal local lava instance just for debugging and performance tracking purposes. Such a local instance would often come in a clean slate state. So with empty database with no devices and most local instances would not be able to connect to physical devices, at least not in the numbers as the production instances run. And even though we could fake multiple devices like Remy mentioned in his talk, that wouldn't solve the problem of having a database pre-populated with some additional data. We could potentially prepare a database fixture for that purpose. But it might not be particularly easy to mock the entire database like you see on the model graph for lava server. It's non-trivial task especially when it comes to keeping large numbers of processed jobs as archives. But the question is do we really have to mock the database? It is all done locally in our private debugging and performance tracking instance. Maybe we don't have to create a new database but reuse a backup from staging or production instance that we also run. And as the old saying goes about two groups of people and backups, I believe we all belong to the group that already makes them. There is also an important second part of this saying to make sure that restoring your backup works properly as well. And with reusing your PGDump output as the input for your performance tests, you can tick off this task from your administrator tasks list. Also if you base your Postgres Docker images from the official one, there is a really simple data initialization method which requires just mounting the volume with PGDump output and everything else is taken care of by the INE-DB itself. It also supports the compression on the fly for the most popular archive formats as you can see on the snippet directly from the INE-DB code for Postgres. Since we already have this database in our local instance, it would be useful to incorporate even more statistics from the database itself. For this, we could simply use PGAdmin or even PostgresQL command line tool just to check the actual runtimes and other statistics with explain-analyze queries. This would highlight for us database operation bottlenecks. And this way, having the database level tool, we would also be able to run various experiments on the database like changes in indexes or maybe adding query planner hints. It almost doesn't cost us anything just running another container in our local setup or if PGAdmin is too much, you could also opt to use the online available graphical tool which would highlight the bottlenecks for you with this heat map showing you where the issue might lie. Using this database level utility completes our tool set for off interactive solutions and while it is really important to be able to perform all those actions, it's paramount to do that again sometime soon and again and again and again and that moves us to automation solutions. By now, we know what to look for or what to watch out for in our lava instances and from user stories or bug reports or the motomo statistics or load balancer logs I mentioned earlier, we know and have specific code components to track or maybe even test cases ready to check for that. But the question is how to run those test cases to get the statistically valid feedback. We would have to take into consideration cache warm-ups, test case calibration, preferably also a way to compare between benchmark runs and it would be also great if it fit well into the test suites that are currently used by the upstream project which by the way is based on PyTest. Fortunately, it turns out that there is a PyTest feature that provides all of that and even more. In the case of lava bottlenecks found in the collaboration instance, the next step was just to wrap the test cases prepared with this fixture and wrapping the key pieces of code allowed to have benchmarks ready to run. Next step, once the test suite was prepared, was to plug it into the pipeline. Both upstream lava project and downstream lava tree makes heavy use of GitLab CI and it shouldn't be surprising. Many projects already do the same. For example, DRM CI merged in kernel 6.6 release. So currently, job definitions for those GitLab CI pipelines above the downstream one and below the upstream one don't share any reusable code. This might be subject to change in the future. For now, downstream changes are made with ease of importing them later in mind. Moving to external definitions could make the GitLab CI pipelines a bit more complex, but that's something that we'll see if it brings any value in future. Of course, GitLab CI jobs need a run environment and to get a baseline of what should be expected from benchmarks run, the easy way out is having a dedicated runner that would provide most stable results that are not affected by, for example, other test suites run in parallel on the same GitLab runner. A good choice would be to select a machine that has similar resources to a node, which your lava instance is run on. And for proof of concept purposes, I used a small desktop computer which gave just that. GitLab runners are also really easy to plug into a GitLab server. And while we are already optimizing the pipeline, we should also take into consideration caching the CI data resources for benchmarks runs. For that, we could easily use already available upstream lava caching solution, which is based on specific CI images to run tests on. But that would also mean that production data from database we used earlier is no longer a valid option for us. And we need to revisit the lava server model, which brings us to data generation, which we no longer could omit. That brought us to creating a dummy database generator, which was focused just on a key few tables and relations according to Postgres planner statistics. It was implemented with very limited scope to only support the worst bottlenecks that were found in collaborators instance. And for that, we used standard Python tools, which were FactoryBoy and Faker. As a bonus addition, you might also want to ask a few questions. Should lava actually archive all the test jobs that are run, or maybe archiving those jobs can be delegated to a higher level test systems? Fortunately, retention mechanism is already available. In upstream lava, it just required enabling it in Helm charts, which is used to deploy lava instances for collaboration. To summarize all of that, I've got three final thoughts that I would like to share with you. Constructing in testing laboratories is not a one-time job. It's a process that might differ from instance to instance, depending on your specific workload. But it's something that I hope could be easier for you if you come across the same set of issues. It also requires frequent revisiting and adjusting according to the results you see. But even small changes can bring huge boosts in performance. But that probably is a topic for another talk. And that's all I have prepared for you today. Thanks for your attention. Do we have time for questions? If there is some question, I will be happy to answer it.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " Hello everyone, my name is Pawe\u0142 Wietzorek.", "tokens": [50364, 2425, 1518, 11, 452, 1315, 307, 3426, 826, 1221, 343, 1684, 89, 418, 74, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3262632065925045, "compression_ratio": 1.38860103626943, "no_speech_prob": 0.0930807888507843}, {"id": 1, "seek": 0, "start": 10.0, "end": 17.52, "text": " I work with Colabora and I've been involved in maintenance of server side components of", "tokens": [50864, 286, 589, 365, 4004, 455, 3252, 293, 286, 600, 668, 3288, 294, 11258, 295, 7154, 1252, 6677, 295, 51240], "temperature": 0.0, "avg_logprob": -0.3262632065925045, "compression_ratio": 1.38860103626943, "no_speech_prob": 0.0930807888507843}, {"id": 2, "seek": 0, "start": 17.52, "end": 20.84, "text": " Colabora's automated testing laboratory.", "tokens": [51240, 4004, 455, 3252, 311, 18473, 4997, 16523, 13, 51406], "temperature": 0.0, "avg_logprob": -0.3262632065925045, "compression_ratio": 1.38860103626943, "no_speech_prob": 0.0930807888507843}, {"id": 3, "seek": 0, "start": 20.84, "end": 28.32, "text": " Today, I would like to share with you a few lessons learned from that experience, particularly", "tokens": [51406, 2692, 11, 286, 576, 411, 281, 2073, 365, 291, 257, 1326, 8820, 3264, 490, 300, 1752, 11, 4098, 51780], "temperature": 0.0, "avg_logprob": -0.3262632065925045, "compression_ratio": 1.38860103626943, "no_speech_prob": 0.0930807888507843}, {"id": 4, "seek": 2832, "start": 28.32, "end": 39.32, "text": " related to tracking laboratories performance and pushing beyond the limits of the software", "tokens": [50364, 4077, 281, 11603, 41013, 3389, 293, 7380, 4399, 264, 10406, 295, 264, 4722, 50914], "temperature": 0.0, "avg_logprob": -0.28126366355202415, "compression_ratio": 1.539877300613497, "no_speech_prob": 0.019538694992661476}, {"id": 5, "seek": 2832, "start": 39.32, "end": 41.64, "text": " that it runs.", "tokens": [50914, 300, 309, 6676, 13, 51030], "temperature": 0.0, "avg_logprob": -0.28126366355202415, "compression_ratio": 1.539877300613497, "no_speech_prob": 0.019538694992661476}, {"id": 6, "seek": 2832, "start": 41.64, "end": 45.400000000000006, "text": " We'll start with some background information.", "tokens": [51030, 492, 603, 722, 365, 512, 3678, 1589, 13, 51218], "temperature": 0.0, "avg_logprob": -0.28126366355202415, "compression_ratio": 1.539877300613497, "no_speech_prob": 0.019538694992661476}, {"id": 7, "seek": 2832, "start": 45.400000000000006, "end": 52.08, "text": " Next I will move to interactive approaches for tracking its performance, I mean the lab", "tokens": [51218, 3087, 286, 486, 1286, 281, 15141, 11587, 337, 11603, 1080, 3389, 11, 286, 914, 264, 2715, 51552], "temperature": 0.0, "avg_logprob": -0.28126366355202415, "compression_ratio": 1.539877300613497, "no_speech_prob": 0.019538694992661476}, {"id": 8, "seek": 2832, "start": 52.08, "end": 53.08, "text": " performance.", "tokens": [51552, 3389, 13, 51602], "temperature": 0.0, "avg_logprob": -0.28126366355202415, "compression_ratio": 1.539877300613497, "no_speech_prob": 0.019538694992661476}, {"id": 9, "seek": 5308, "start": 53.08, "end": 59.68, "text": " After that, I'll describe a few solutions for automating that and finally I will also", "tokens": [50364, 2381, 300, 11, 286, 603, 6786, 257, 1326, 6547, 337, 3553, 990, 300, 293, 2721, 286, 486, 611, 50694], "temperature": 0.0, "avg_logprob": -0.18098979949951172, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.004224845673888922}, {"id": 10, "seek": 5308, "start": 59.68, "end": 64.92, "text": " share some thoughts on data generation.", "tokens": [50694, 2073, 512, 4598, 322, 1412, 5125, 13, 50956], "temperature": 0.0, "avg_logprob": -0.18098979949951172, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.004224845673888922}, {"id": 11, "seek": 5308, "start": 64.92, "end": 72.56, "text": " So let's start with the reason why, I mean what brought us here today.", "tokens": [50956, 407, 718, 311, 722, 365, 264, 1778, 983, 11, 286, 914, 437, 3038, 505, 510, 965, 13, 51338], "temperature": 0.0, "avg_logprob": -0.18098979949951172, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.004224845673888922}, {"id": 12, "seek": 5308, "start": 72.56, "end": 80.32, "text": " Thanks to Remi's talk, we now know and have an idea of what Lava is, what it provides", "tokens": [51338, 2561, 281, 4080, 72, 311, 751, 11, 321, 586, 458, 293, 362, 364, 1558, 295, 437, 441, 4061, 307, 11, 437, 309, 6417, 51726], "temperature": 0.0, "avg_logprob": -0.18098979949951172, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.004224845673888922}, {"id": 13, "seek": 8032, "start": 80.32, "end": 85.27999999999999, "text": " for testing automation and how it supports all these efforts.", "tokens": [50364, 337, 4997, 17769, 293, 577, 309, 9346, 439, 613, 6484, 13, 50612], "temperature": 0.0, "avg_logprob": -0.19047851293859347, "compression_ratio": 1.3957219251336899, "no_speech_prob": 0.05328753590583801}, {"id": 14, "seek": 8032, "start": 85.27999999999999, "end": 95.47999999999999, "text": " Some of you might also recall a talk given by my colleague Laura at last year's FOSDEM.", "tokens": [50612, 2188, 295, 291, 1062, 611, 9901, 257, 751, 2212, 538, 452, 13532, 13220, 412, 1036, 1064, 311, 479, 4367, 35, 6683, 13, 51122], "temperature": 0.0, "avg_logprob": -0.19047851293859347, "compression_ratio": 1.3957219251336899, "no_speech_prob": 0.05328753590583801}, {"id": 15, "seek": 8032, "start": 95.47999999999999, "end": 103.35999999999999, "text": " Laura described in her talk how the lab at Colabora is set up, what its day-to-day maintenance", "tokens": [51122, 13220, 7619, 294, 720, 751, 577, 264, 2715, 412, 4004, 455, 3252, 307, 992, 493, 11, 437, 1080, 786, 12, 1353, 12, 810, 11258, 51516], "temperature": 0.0, "avg_logprob": -0.19047851293859347, "compression_ratio": 1.3957219251336899, "no_speech_prob": 0.05328753590583801}, {"id": 16, "seek": 8032, "start": 103.35999999999999, "end": 106.63999999999999, "text": " tasks look like.", "tokens": [51516, 9608, 574, 411, 13, 51680], "temperature": 0.0, "avg_logprob": -0.19047851293859347, "compression_ratio": 1.3957219251336899, "no_speech_prob": 0.05328753590583801}, {"id": 17, "seek": 10664, "start": 106.64, "end": 114.6, "text": " What main challenges are while running this kind of laboratory and also shared some best", "tokens": [50364, 708, 2135, 4759, 366, 1339, 2614, 341, 733, 295, 16523, 293, 611, 5507, 512, 1151, 50762], "temperature": 0.0, "avg_logprob": -0.17827750034019596, "compression_ratio": 1.4648648648648648, "no_speech_prob": 0.05183304101228714}, {"id": 18, "seek": 10664, "start": 114.6, "end": 117.4, "text": " practices.", "tokens": [50762, 7525, 13, 50902], "temperature": 0.0, "avg_logprob": -0.17827750034019596, "compression_ratio": 1.4648648648648648, "no_speech_prob": 0.05183304101228714}, {"id": 19, "seek": 10664, "start": 117.4, "end": 124.2, "text": " The key piece of information for us today is that Colabora's lab is a fairly large Lava", "tokens": [50902, 440, 2141, 2522, 295, 1589, 337, 505, 965, 307, 300, 4004, 455, 3252, 311, 2715, 307, 257, 6457, 2416, 441, 4061, 51242], "temperature": 0.0, "avg_logprob": -0.17827750034019596, "compression_ratio": 1.4648648648648648, "no_speech_prob": 0.05183304101228714}, {"id": 20, "seek": 10664, "start": 124.2, "end": 133.64, "text": " instance that is continuously growing and together with high number of devices also", "tokens": [51242, 5197, 300, 307, 15684, 4194, 293, 1214, 365, 1090, 1230, 295, 5759, 611, 51714], "temperature": 0.0, "avg_logprob": -0.17827750034019596, "compression_ratio": 1.4648648648648648, "no_speech_prob": 0.05183304101228714}, {"id": 21, "seek": 13364, "start": 133.64, "end": 141.39999999999998, "text": " comes high numbers of test job submissions to process which unsurprisingly can result", "tokens": [50364, 1487, 1090, 3547, 295, 1500, 1691, 40429, 281, 1399, 597, 2693, 374, 34408, 393, 1874, 50752], "temperature": 0.0, "avg_logprob": -0.14486423698631493, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.039848361164331436}, {"id": 22, "seek": 13364, "start": 141.39999999999998, "end": 145.55999999999997, "text": " in higher load on server side of things.", "tokens": [50752, 294, 2946, 3677, 322, 7154, 1252, 295, 721, 13, 50960], "temperature": 0.0, "avg_logprob": -0.14486423698631493, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.039848361164331436}, {"id": 23, "seek": 13364, "start": 145.55999999999997, "end": 149.07999999999998, "text": " And that in fact was our case.", "tokens": [50960, 400, 300, 294, 1186, 390, 527, 1389, 13, 51136], "temperature": 0.0, "avg_logprob": -0.14486423698631493, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.039848361164331436}, {"id": 24, "seek": 13364, "start": 149.07999999999998, "end": 153.56, "text": " There was no need to panic though, at least not right away.", "tokens": [51136, 821, 390, 572, 643, 281, 14783, 1673, 11, 412, 1935, 406, 558, 1314, 13, 51360], "temperature": 0.0, "avg_logprob": -0.14486423698631493, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.039848361164331436}, {"id": 25, "seek": 13364, "start": 153.56, "end": 160.79999999999998, "text": " High load means that the resources that were allocated for lab purposes are in use and", "tokens": [51360, 5229, 3677, 1355, 300, 264, 3593, 300, 645, 29772, 337, 2715, 9932, 366, 294, 764, 293, 51722], "temperature": 0.0, "avg_logprob": -0.14486423698631493, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.039848361164331436}, {"id": 26, "seek": 16080, "start": 160.8, "end": 164.4, "text": " that's what they are meant to do after all.", "tokens": [50364, 300, 311, 437, 436, 366, 4140, 281, 360, 934, 439, 13, 50544], "temperature": 0.0, "avg_logprob": -0.17675123612085977, "compression_ratio": 1.3716216216216217, "no_speech_prob": 0.08596531301736832}, {"id": 27, "seek": 16080, "start": 164.4, "end": 174.24, "text": " Interestingly, especially high load was observed on the nodes running database processes.", "tokens": [50544, 30564, 11, 2318, 1090, 3677, 390, 13095, 322, 264, 13891, 2614, 8149, 7555, 13, 51036], "temperature": 0.0, "avg_logprob": -0.17675123612085977, "compression_ratio": 1.3716216216216217, "no_speech_prob": 0.08596531301736832}, {"id": 28, "seek": 16080, "start": 174.24, "end": 183.72000000000003, "text": " And all of that is mostly fine until the system becomes unresponsive.", "tokens": [51036, 400, 439, 295, 300, 307, 5240, 2489, 1826, 264, 1185, 3643, 517, 28930, 488, 13, 51510], "temperature": 0.0, "avg_logprob": -0.17675123612085977, "compression_ratio": 1.3716216216216217, "no_speech_prob": 0.08596531301736832}, {"id": 29, "seek": 18372, "start": 183.72, "end": 190.36, "text": " This might lead to potentially unreliable lab or even unusable for higher level test", "tokens": [50364, 639, 1062, 1477, 281, 7263, 20584, 2081, 712, 2715, 420, 754, 10054, 712, 337, 2946, 1496, 1500, 50696], "temperature": 0.0, "avg_logprob": -0.2351034617019912, "compression_ratio": 1.3550295857988166, "no_speech_prob": 0.4496994912624359}, {"id": 30, "seek": 18372, "start": 190.36, "end": 201.84, "text": " systems like MESA-CI or Kernel-CI on the screenshot which other Colabora's are involved in development,", "tokens": [50696, 3652, 411, 376, 2358, 32, 12, 25240, 420, 40224, 338, 12, 25240, 322, 264, 27712, 597, 661, 4004, 455, 3252, 311, 366, 3288, 294, 3250, 11, 51270], "temperature": 0.0, "avg_logprob": -0.2351034617019912, "compression_ratio": 1.3550295857988166, "no_speech_prob": 0.4496994912624359}, {"id": 31, "seek": 18372, "start": 201.84, "end": 206.8, "text": " maintenance and of course usage as well.", "tokens": [51270, 11258, 293, 295, 1164, 14924, 382, 731, 13, 51518], "temperature": 0.0, "avg_logprob": -0.2351034617019912, "compression_ratio": 1.3550295857988166, "no_speech_prob": 0.4496994912624359}, {"id": 32, "seek": 20680, "start": 206.8, "end": 216.08, "text": " My first thought was to simply estimate what resources are required for day to day operations", "tokens": [50364, 1222, 700, 1194, 390, 281, 2935, 12539, 437, 3593, 366, 4739, 337, 786, 281, 786, 7705, 50828], "temperature": 0.0, "avg_logprob": -0.1437254892268651, "compression_ratio": 1.5463917525773196, "no_speech_prob": 0.18470273911952972}, {"id": 33, "seek": 20680, "start": 216.08, "end": 219.84, "text": " and simply throw them at this workload.", "tokens": [50828, 293, 2935, 3507, 552, 412, 341, 20139, 13, 51016], "temperature": 0.0, "avg_logprob": -0.1437254892268651, "compression_ratio": 1.5463917525773196, "no_speech_prob": 0.18470273911952972}, {"id": 34, "seek": 20680, "start": 219.84, "end": 226.36, "text": " This could work short term but it wouldn't really solve the problem.", "tokens": [51016, 639, 727, 589, 2099, 1433, 457, 309, 2759, 380, 534, 5039, 264, 1154, 13, 51342], "temperature": 0.0, "avg_logprob": -0.1437254892268651, "compression_ratio": 1.5463917525773196, "no_speech_prob": 0.18470273911952972}, {"id": 35, "seek": 20680, "start": 226.36, "end": 232.64000000000001, "text": " To do it the right way, a deeper understanding of the root cause for all these issues was", "tokens": [51342, 1407, 360, 309, 264, 558, 636, 11, 257, 7731, 3701, 295, 264, 5593, 3082, 337, 439, 613, 2663, 390, 51656], "temperature": 0.0, "avg_logprob": -0.1437254892268651, "compression_ratio": 1.5463917525773196, "no_speech_prob": 0.18470273911952972}, {"id": 36, "seek": 20680, "start": 232.64000000000001, "end": 233.8, "text": " needed.", "tokens": [51656, 2978, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1437254892268651, "compression_ratio": 1.5463917525773196, "no_speech_prob": 0.18470273911952972}, {"id": 37, "seek": 23380, "start": 233.8, "end": 242.12, "text": " And by the way, this photo is from Polish IT Olympics where hardware component throwing", "tokens": [50364, 400, 538, 264, 636, 11, 341, 5052, 307, 490, 18504, 6783, 19854, 689, 8837, 6542, 10238, 50780], "temperature": 0.0, "avg_logprob": -0.21888484954833984, "compression_ratio": 1.4522292993630572, "no_speech_prob": 0.06759053468704224}, {"id": 38, "seek": 23380, "start": 242.12, "end": 245.60000000000002, "text": " contest is held.", "tokens": [50780, 10287, 307, 5167, 13, 50954], "temperature": 0.0, "avg_logprob": -0.21888484954833984, "compression_ratio": 1.4522292993630572, "no_speech_prob": 0.06759053468704224}, {"id": 39, "seek": 23380, "start": 245.60000000000002, "end": 254.12, "text": " And while this is hard drive throwing contest which might not be the type of resource we", "tokens": [50954, 400, 1339, 341, 307, 1152, 3332, 10238, 10287, 597, 1062, 406, 312, 264, 2010, 295, 7684, 321, 51380], "temperature": 0.0, "avg_logprob": -0.21888484954833984, "compression_ratio": 1.4522292993630572, "no_speech_prob": 0.06759053468704224}, {"id": 40, "seek": 23380, "start": 254.12, "end": 261.32, "text": " needed, that was the initial idea.", "tokens": [51380, 2978, 11, 300, 390, 264, 5883, 1558, 13, 51740], "temperature": 0.0, "avg_logprob": -0.21888484954833984, "compression_ratio": 1.4522292993630572, "no_speech_prob": 0.06759053468704224}, {"id": 41, "seek": 26132, "start": 261.32, "end": 269.2, "text": " Thanks to RemiStock we also have rough idea of what main components for Lava are but let's", "tokens": [50364, 2561, 281, 4080, 72, 50, 1353, 547, 321, 611, 362, 5903, 1558, 295, 437, 2135, 6677, 337, 441, 4061, 366, 457, 718, 311, 50758], "temperature": 0.0, "avg_logprob": -0.2012005066603757, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.06670578569173813}, {"id": 42, "seek": 26132, "start": 269.2, "end": 271.28, "text": " recap them real quick.", "tokens": [50758, 20928, 552, 957, 1702, 13, 50862], "temperature": 0.0, "avg_logprob": -0.2012005066603757, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.06670578569173813}, {"id": 43, "seek": 26132, "start": 271.28, "end": 277.68, "text": " At the very high level Lava on the server side has two main components, a scheduler", "tokens": [50862, 1711, 264, 588, 1090, 1496, 441, 4061, 322, 264, 7154, 1252, 575, 732, 2135, 6677, 11, 257, 12000, 260, 51182], "temperature": 0.0, "avg_logprob": -0.2012005066603757, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.06670578569173813}, {"id": 44, "seek": 26132, "start": 277.68, "end": 280.52, "text": " and a connection to the database.", "tokens": [51182, 293, 257, 4984, 281, 264, 8149, 13, 51324], "temperature": 0.0, "avg_logprob": -0.2012005066603757, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.06670578569173813}, {"id": 45, "seek": 26132, "start": 280.52, "end": 287.76, "text": " If we take a closer look, those are respectively a jungle application and by default a Postgres", "tokens": [51324, 759, 321, 747, 257, 4966, 574, 11, 729, 366, 25009, 257, 18228, 3861, 293, 538, 7576, 257, 10223, 45189, 51686], "temperature": 0.0, "avg_logprob": -0.2012005066603757, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.06670578569173813}, {"id": 46, "seek": 26132, "start": 287.76, "end": 289.2, "text": " database.", "tokens": [51686, 8149, 13, 51758], "temperature": 0.0, "avg_logprob": -0.2012005066603757, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.06670578569173813}, {"id": 47, "seek": 28920, "start": 289.2, "end": 298.08, "text": " These are widely known used and mostly loved software components so we can make use of", "tokens": [50364, 1981, 366, 13371, 2570, 1143, 293, 5240, 4333, 4722, 6677, 370, 321, 393, 652, 764, 295, 50808], "temperature": 0.0, "avg_logprob": -0.201230709369366, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.022329112514853477}, {"id": 48, "seek": 28920, "start": 298.08, "end": 303.48, "text": " several already available performance tracking tools for them.", "tokens": [50808, 2940, 1217, 2435, 3389, 11603, 3873, 337, 552, 13, 51078], "temperature": 0.0, "avg_logprob": -0.201230709369366, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.022329112514853477}, {"id": 49, "seek": 28920, "start": 303.48, "end": 309.71999999999997, "text": " So let's go through a few interactive or semi interactive ones.", "tokens": [51078, 407, 718, 311, 352, 807, 257, 1326, 15141, 420, 12909, 15141, 2306, 13, 51390], "temperature": 0.0, "avg_logprob": -0.201230709369366, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.022329112514853477}, {"id": 50, "seek": 28920, "start": 309.71999999999997, "end": 318.88, "text": " As tribal as it might sound, it is equally as important to start with simply enabling", "tokens": [51390, 1018, 20958, 382, 309, 1062, 1626, 11, 309, 307, 12309, 382, 1021, 281, 722, 365, 2935, 23148, 51848], "temperature": 0.0, "avg_logprob": -0.201230709369366, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.022329112514853477}, {"id": 51, "seek": 31888, "start": 318.88, "end": 324.64, "text": " verbose logging on affected instances.", "tokens": [50364, 9595, 541, 27991, 322, 8028, 14519, 13, 50652], "temperature": 0.0, "avg_logprob": -0.27555739879608154, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.06613211333751678}, {"id": 52, "seek": 31888, "start": 324.64, "end": 332.48, "text": " This way we get first insights from redoing user stories based either on direct reports", "tokens": [50652, 639, 636, 321, 483, 700, 14310, 490, 29956, 278, 4195, 3676, 2361, 2139, 322, 2047, 7122, 51044], "temperature": 0.0, "avg_logprob": -0.27555739879608154, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.06613211333751678}, {"id": 53, "seek": 31888, "start": 332.48, "end": 342.8, "text": " from users or maybe motomo statistics collected by recent Lava releases or maybe some logs", "tokens": [51044, 490, 5022, 420, 1310, 2184, 13395, 12523, 11087, 538, 5162, 441, 4061, 16952, 420, 1310, 512, 20820, 51560], "temperature": 0.0, "avg_logprob": -0.27555739879608154, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.06613211333751678}, {"id": 54, "seek": 34280, "start": 342.8, "end": 351.96000000000004, "text": " from load balancer which shows us which API endpoints are mostly used by users or which", "tokens": [50364, 490, 3677, 3119, 28347, 597, 3110, 505, 597, 9362, 917, 20552, 366, 5240, 1143, 538, 5022, 420, 597, 50822], "temperature": 0.0, "avg_logprob": -0.202500513621739, "compression_ratio": 1.3973509933774835, "no_speech_prob": 0.02745325304567814}, {"id": 55, "seek": 34280, "start": 351.96000000000004, "end": 358.84000000000003, "text": " views are most commonly requested.", "tokens": [50822, 6809, 366, 881, 12719, 16436, 13, 51166], "temperature": 0.0, "avg_logprob": -0.202500513621739, "compression_ratio": 1.3973509933774835, "no_speech_prob": 0.02745325304567814}, {"id": 56, "seek": 34280, "start": 358.84000000000003, "end": 363.72, "text": " In case of Django we get a few other perks.", "tokens": [51166, 682, 1389, 295, 33464, 17150, 321, 483, 257, 1326, 661, 36991, 13, 51410], "temperature": 0.0, "avg_logprob": -0.202500513621739, "compression_ratio": 1.3973509933774835, "no_speech_prob": 0.02745325304567814}, {"id": 57, "seek": 34280, "start": 363.72, "end": 368.04, "text": " It's as easy as literally flipping a switch.", "tokens": [51410, 467, 311, 382, 1858, 382, 3736, 26886, 257, 3679, 13, 51626], "temperature": 0.0, "avg_logprob": -0.202500513621739, "compression_ratio": 1.3973509933774835, "no_speech_prob": 0.02745325304567814}, {"id": 58, "seek": 36804, "start": 368.04, "end": 376.12, "text": " Django for database also allows to log every statement executed on the database in debug", "tokens": [50364, 33464, 17150, 337, 8149, 611, 4045, 281, 3565, 633, 5629, 17577, 322, 264, 8149, 294, 24083, 50768], "temperature": 0.0, "avg_logprob": -0.19148645729854188, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0705818459391594}, {"id": 59, "seek": 36804, "start": 376.12, "end": 384.32000000000005, "text": " mode and it can be also easily extended with some additional profiling information.", "tokens": [50768, 4391, 293, 309, 393, 312, 611, 3612, 10913, 365, 512, 4497, 1740, 4883, 1589, 13, 51178], "temperature": 0.0, "avg_logprob": -0.19148645729854188, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0705818459391594}, {"id": 60, "seek": 36804, "start": 384.32000000000005, "end": 393.20000000000005, "text": " But even though there are all these perks, all this information is a post-action information.", "tokens": [51178, 583, 754, 1673, 456, 366, 439, 613, 36991, 11, 439, 341, 1589, 307, 257, 2183, 12, 2894, 1589, 13, 51622], "temperature": 0.0, "avg_logprob": -0.19148645729854188, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0705818459391594}, {"id": 61, "seek": 39320, "start": 393.2, "end": 400.15999999999997, "text": " To collect it in a truly interactive manner, fortunately Django already has us covered", "tokens": [50364, 1407, 2500, 309, 294, 257, 4908, 15141, 9060, 11, 25511, 33464, 17150, 1217, 575, 505, 5343, 50712], "temperature": 0.0, "avg_logprob": -0.1491584232875279, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.3105461597442627}, {"id": 62, "seek": 39320, "start": 400.15999999999997, "end": 408.48, "text": " and provides just the right tool for this purpose which is Django Debug toolbar.", "tokens": [50712, 293, 6417, 445, 264, 558, 2290, 337, 341, 4334, 597, 307, 33464, 17150, 27347, 697, 47715, 13, 51128], "temperature": 0.0, "avg_logprob": -0.1491584232875279, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.3105461597442627}, {"id": 63, "seek": 39320, "start": 408.48, "end": 412.84, "text": " It isn't much harder to enable than just verbose logging.", "tokens": [51128, 467, 1943, 380, 709, 6081, 281, 9528, 813, 445, 9595, 541, 27991, 13, 51346], "temperature": 0.0, "avg_logprob": -0.1491584232875279, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.3105461597442627}, {"id": 64, "seek": 39320, "start": 412.84, "end": 418.84, "text": " It just requires adding an additional Python package to your deployment, set internal", "tokens": [51346, 467, 445, 7029, 5127, 364, 4497, 15329, 7372, 281, 428, 19317, 11, 992, 6920, 51646], "temperature": 0.0, "avg_logprob": -0.1491584232875279, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.3105461597442627}, {"id": 65, "seek": 41884, "start": 418.91999999999996, "end": 427.56, "text": " IPs from which Debug toolbar would be available, confirm enabling it and you're good to go.", "tokens": [50368, 8671, 82, 490, 597, 27347, 697, 47715, 576, 312, 2435, 11, 9064, 23148, 309, 293, 291, 434, 665, 281, 352, 13, 50800], "temperature": 0.0, "avg_logprob": -0.17274157206217447, "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.1206553503870964}, {"id": 66, "seek": 41884, "start": 427.56, "end": 435.59999999999997, "text": " Debug toolbar not only provides great and immediate feedback but also includes traces,", "tokens": [50800, 27347, 697, 47715, 406, 787, 6417, 869, 293, 11629, 5824, 457, 611, 5974, 26076, 11, 51202], "temperature": 0.0, "avg_logprob": -0.17274157206217447, "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.1206553503870964}, {"id": 67, "seek": 41884, "start": 435.59999999999997, "end": 441.67999999999995, "text": " some additional profiling information and it gives you all of that in an easy to use", "tokens": [51202, 512, 4497, 1740, 4883, 1589, 293, 309, 2709, 291, 439, 295, 300, 294, 364, 1858, 281, 764, 51506], "temperature": 0.0, "avg_logprob": -0.17274157206217447, "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.1206553503870964}, {"id": 68, "seek": 41884, "start": 441.67999999999995, "end": 444.59999999999997, "text": " graphical user-friendly way.", "tokens": [51506, 35942, 4195, 12, 22864, 636, 13, 51652], "temperature": 0.0, "avg_logprob": -0.17274157206217447, "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.1206553503870964}, {"id": 69, "seek": 44460, "start": 444.6, "end": 452.96000000000004, "text": " As you can see on the right-hand side of the screenshot you even get all the requests", "tokens": [50364, 1018, 291, 393, 536, 322, 264, 558, 12, 5543, 1252, 295, 264, 27712, 291, 754, 483, 439, 264, 12475, 50782], "temperature": 0.0, "avg_logprob": -0.2693236836215906, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.08268183469772339}, {"id": 70, "seek": 44460, "start": 452.96000000000004, "end": 462.44, "text": " sent, the instance and all the SQL statements run.", "tokens": [50782, 2279, 11, 264, 5197, 293, 439, 264, 19200, 12363, 1190, 13, 51256], "temperature": 0.0, "avg_logprob": -0.2693236836215906, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.08268183469772339}, {"id": 71, "seek": 44460, "start": 462.44, "end": 473.48, "text": " But even though these tools are easy to enable, it comes with some drawbacks as well.", "tokens": [51256, 583, 754, 1673, 613, 3873, 366, 1858, 281, 9528, 11, 309, 1487, 365, 512, 2642, 17758, 382, 731, 13, 51808], "temperature": 0.0, "avg_logprob": -0.2693236836215906, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.08268183469772339}, {"id": 72, "seek": 47348, "start": 474.48, "end": 484.56, "text": " These tools should not be used on any user-facing instance which brings us to setting up a personal", "tokens": [50414, 1981, 3873, 820, 406, 312, 1143, 322, 604, 4195, 12, 44046, 5197, 597, 5607, 505, 281, 3287, 493, 257, 2973, 50918], "temperature": 0.0, "avg_logprob": -0.262827414732713, "compression_ratio": 1.5324675324675325, "no_speech_prob": 0.006704376544803381}, {"id": 73, "seek": 47348, "start": 484.56, "end": 491.16, "text": " local lava instance just for debugging and performance tracking purposes.", "tokens": [50918, 2654, 22097, 5197, 445, 337, 45592, 293, 3389, 11603, 9932, 13, 51248], "temperature": 0.0, "avg_logprob": -0.262827414732713, "compression_ratio": 1.5324675324675325, "no_speech_prob": 0.006704376544803381}, {"id": 74, "seek": 47348, "start": 491.16, "end": 498.84000000000003, "text": " Such a local instance would often come in a clean slate state.", "tokens": [51248, 9653, 257, 2654, 5197, 576, 2049, 808, 294, 257, 2541, 39118, 1785, 13, 51632], "temperature": 0.0, "avg_logprob": -0.262827414732713, "compression_ratio": 1.5324675324675325, "no_speech_prob": 0.006704376544803381}, {"id": 75, "seek": 49884, "start": 499.2, "end": 512.8, "text": " So with empty database with no devices and most local instances would not be able to", "tokens": [50382, 407, 365, 6707, 8149, 365, 572, 5759, 293, 881, 2654, 14519, 576, 406, 312, 1075, 281, 51062], "temperature": 0.0, "avg_logprob": -0.2551985276051057, "compression_ratio": 1.4146341463414633, "no_speech_prob": 0.029520874843001366}, {"id": 76, "seek": 49884, "start": 512.8, "end": 522.0, "text": " connect to physical devices, at least not in the numbers as the production instances run.", "tokens": [51062, 1745, 281, 4001, 5759, 11, 412, 1935, 406, 294, 264, 3547, 382, 264, 4265, 14519, 1190, 13, 51522], "temperature": 0.0, "avg_logprob": -0.2551985276051057, "compression_ratio": 1.4146341463414633, "no_speech_prob": 0.029520874843001366}, {"id": 77, "seek": 52200, "start": 522.0, "end": 531.68, "text": " And even though we could fake multiple devices like Remy mentioned in his talk, that wouldn't", "tokens": [50364, 400, 754, 1673, 321, 727, 7592, 3866, 5759, 411, 497, 3633, 2835, 294, 702, 751, 11, 300, 2759, 380, 50848], "temperature": 0.0, "avg_logprob": -0.2082804044087728, "compression_ratio": 1.4397590361445782, "no_speech_prob": 0.02932821214199066}, {"id": 78, "seek": 52200, "start": 531.68, "end": 540.64, "text": " solve the problem of having a database pre-populated with some additional data.", "tokens": [50848, 5039, 264, 1154, 295, 1419, 257, 8149, 659, 12, 13872, 6987, 365, 512, 4497, 1412, 13, 51296], "temperature": 0.0, "avg_logprob": -0.2082804044087728, "compression_ratio": 1.4397590361445782, "no_speech_prob": 0.02932821214199066}, {"id": 79, "seek": 52200, "start": 540.64, "end": 550.72, "text": " We could potentially prepare a database fixture for that purpose.", "tokens": [51296, 492, 727, 7263, 5940, 257, 8149, 47680, 337, 300, 4334, 13, 51800], "temperature": 0.0, "avg_logprob": -0.2082804044087728, "compression_ratio": 1.4397590361445782, "no_speech_prob": 0.02932821214199066}, {"id": 80, "seek": 55072, "start": 550.72, "end": 559.64, "text": " But it might not be particularly easy to mock the entire database like you see on the", "tokens": [50364, 583, 309, 1062, 406, 312, 4098, 1858, 281, 17362, 264, 2302, 8149, 411, 291, 536, 322, 264, 50810], "temperature": 0.0, "avg_logprob": -0.2487758091517857, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.015753723680973053}, {"id": 81, "seek": 55072, "start": 559.64, "end": 565.48, "text": " model graph for lava server.", "tokens": [50810, 2316, 4295, 337, 22097, 7154, 13, 51102], "temperature": 0.0, "avg_logprob": -0.2487758091517857, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.015753723680973053}, {"id": 82, "seek": 55072, "start": 565.48, "end": 572.4, "text": " It's non-trivial task especially when it comes to keeping large numbers of processed jobs", "tokens": [51102, 467, 311, 2107, 12, 83, 470, 22640, 5633, 2318, 562, 309, 1487, 281, 5145, 2416, 3547, 295, 18846, 4782, 51448], "temperature": 0.0, "avg_logprob": -0.2487758091517857, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.015753723680973053}, {"id": 83, "seek": 55072, "start": 572.4, "end": 574.44, "text": " as archives.", "tokens": [51448, 382, 25607, 13, 51550], "temperature": 0.0, "avg_logprob": -0.2487758091517857, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.015753723680973053}, {"id": 84, "seek": 55072, "start": 574.44, "end": 580.32, "text": " But the question is do we really have to mock the database?", "tokens": [51550, 583, 264, 1168, 307, 360, 321, 534, 362, 281, 17362, 264, 8149, 30, 51844], "temperature": 0.0, "avg_logprob": -0.2487758091517857, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.015753723680973053}, {"id": 85, "seek": 58032, "start": 580.9200000000001, "end": 586.9200000000001, "text": " It is all done locally in our private debugging and performance tracking instance.", "tokens": [50394, 467, 307, 439, 1096, 16143, 294, 527, 4551, 45592, 293, 3389, 11603, 5197, 13, 50694], "temperature": 0.0, "avg_logprob": -0.24499050982586748, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.017274241894483566}, {"id": 86, "seek": 58032, "start": 586.9200000000001, "end": 594.24, "text": " Maybe we don't have to create a new database but reuse a backup from staging or production", "tokens": [50694, 2704, 321, 500, 380, 362, 281, 1884, 257, 777, 8149, 457, 26225, 257, 14807, 490, 41085, 420, 4265, 51060], "temperature": 0.0, "avg_logprob": -0.24499050982586748, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.017274241894483566}, {"id": 87, "seek": 58032, "start": 594.24, "end": 598.0400000000001, "text": " instance that we also run.", "tokens": [51060, 5197, 300, 321, 611, 1190, 13, 51250], "temperature": 0.0, "avg_logprob": -0.24499050982586748, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.017274241894483566}, {"id": 88, "seek": 58032, "start": 598.0400000000001, "end": 605.4000000000001, "text": " And as the old saying goes about two groups of people and backups, I believe we all belong", "tokens": [51250, 400, 382, 264, 1331, 1566, 1709, 466, 732, 3935, 295, 561, 293, 50160, 11, 286, 1697, 321, 439, 5784, 51618], "temperature": 0.0, "avg_logprob": -0.24499050982586748, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.017274241894483566}, {"id": 89, "seek": 58032, "start": 605.4000000000001, "end": 609.5200000000001, "text": " to the group that already makes them.", "tokens": [51618, 281, 264, 1594, 300, 1217, 1669, 552, 13, 51824], "temperature": 0.0, "avg_logprob": -0.24499050982586748, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.017274241894483566}, {"id": 90, "seek": 60952, "start": 609.52, "end": 615.28, "text": " There is also an important second part of this saying to make sure that restoring your", "tokens": [50364, 821, 307, 611, 364, 1021, 1150, 644, 295, 341, 1566, 281, 652, 988, 300, 36349, 428, 50652], "temperature": 0.0, "avg_logprob": -0.2186573685192671, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.019737321883440018}, {"id": 91, "seek": 60952, "start": 615.28, "end": 619.36, "text": " backup works properly as well.", "tokens": [50652, 14807, 1985, 6108, 382, 731, 13, 50856], "temperature": 0.0, "avg_logprob": -0.2186573685192671, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.019737321883440018}, {"id": 92, "seek": 60952, "start": 619.36, "end": 627.68, "text": " And with reusing your PGDump output as the input for your performance tests, you can", "tokens": [50856, 400, 365, 319, 7981, 428, 40975, 35, 1420, 5598, 382, 264, 4846, 337, 428, 3389, 6921, 11, 291, 393, 51272], "temperature": 0.0, "avg_logprob": -0.2186573685192671, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.019737321883440018}, {"id": 93, "seek": 60952, "start": 627.68, "end": 636.0, "text": " tick off this task from your administrator tasks list.", "tokens": [51272, 5204, 766, 341, 5633, 490, 428, 25529, 9608, 1329, 13, 51688], "temperature": 0.0, "avg_logprob": -0.2186573685192671, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.019737321883440018}, {"id": 94, "seek": 63600, "start": 636.64, "end": 645.52, "text": " Also if you base your Postgres Docker images from the official one, there is a really simple", "tokens": [50396, 2743, 498, 291, 3096, 428, 10223, 45189, 33772, 5267, 490, 264, 4783, 472, 11, 456, 307, 257, 534, 2199, 50840], "temperature": 0.0, "avg_logprob": -0.2388645473279451, "compression_ratio": 1.3542857142857143, "no_speech_prob": 0.03519802168011665}, {"id": 95, "seek": 63600, "start": 645.52, "end": 654.28, "text": " data initialization method which requires just mounting the volume with PGDump output", "tokens": [50840, 1412, 5883, 2144, 3170, 597, 7029, 445, 22986, 264, 5523, 365, 40975, 35, 1420, 5598, 51278], "temperature": 0.0, "avg_logprob": -0.2388645473279451, "compression_ratio": 1.3542857142857143, "no_speech_prob": 0.03519802168011665}, {"id": 96, "seek": 63600, "start": 654.28, "end": 662.68, "text": " and everything else is taken care of by the INE-DB itself.", "tokens": [51278, 293, 1203, 1646, 307, 2726, 1127, 295, 538, 264, 6892, 36, 12, 27735, 2564, 13, 51698], "temperature": 0.0, "avg_logprob": -0.2388645473279451, "compression_ratio": 1.3542857142857143, "no_speech_prob": 0.03519802168011665}, {"id": 97, "seek": 66268, "start": 662.68, "end": 668.88, "text": " It also supports the compression on the fly for the most popular archive formats as you", "tokens": [50364, 467, 611, 9346, 264, 19355, 322, 264, 3603, 337, 264, 881, 3743, 23507, 25879, 382, 291, 50674], "temperature": 0.0, "avg_logprob": -0.1446566500906217, "compression_ratio": 1.4195402298850575, "no_speech_prob": 0.01744900457561016}, {"id": 98, "seek": 66268, "start": 668.88, "end": 679.3599999999999, "text": " can see on the snippet directly from the INE-DB code for Postgres.", "tokens": [50674, 393, 536, 322, 264, 35623, 302, 3838, 490, 264, 6892, 36, 12, 27735, 3089, 337, 10223, 45189, 13, 51198], "temperature": 0.0, "avg_logprob": -0.1446566500906217, "compression_ratio": 1.4195402298850575, "no_speech_prob": 0.01744900457561016}, {"id": 99, "seek": 66268, "start": 679.3599999999999, "end": 688.7199999999999, "text": " Since we already have this database in our local instance, it would be useful to incorporate", "tokens": [51198, 4162, 321, 1217, 362, 341, 8149, 294, 527, 2654, 5197, 11, 309, 576, 312, 4420, 281, 16091, 51666], "temperature": 0.0, "avg_logprob": -0.1446566500906217, "compression_ratio": 1.4195402298850575, "no_speech_prob": 0.01744900457561016}, {"id": 100, "seek": 68872, "start": 688.72, "end": 696.0400000000001, "text": " even more statistics from the database itself.", "tokens": [50364, 754, 544, 12523, 490, 264, 8149, 2564, 13, 50730], "temperature": 0.0, "avg_logprob": -0.16889353898855355, "compression_ratio": 1.34640522875817, "no_speech_prob": 0.04607027769088745}, {"id": 101, "seek": 68872, "start": 696.0400000000001, "end": 706.6800000000001, "text": " For this, we could simply use PGAdmin or even PostgresQL command line tool just to check", "tokens": [50730, 1171, 341, 11, 321, 727, 2935, 764, 40975, 15830, 2367, 420, 754, 10223, 45189, 13695, 5622, 1622, 2290, 445, 281, 1520, 51262], "temperature": 0.0, "avg_logprob": -0.16889353898855355, "compression_ratio": 1.34640522875817, "no_speech_prob": 0.04607027769088745}, {"id": 102, "seek": 68872, "start": 706.6800000000001, "end": 714.6800000000001, "text": " the actual runtimes and other statistics with explain-analyze queries.", "tokens": [51262, 264, 3539, 49435, 1532, 293, 661, 12523, 365, 2903, 12, 282, 5222, 1381, 24109, 13, 51662], "temperature": 0.0, "avg_logprob": -0.16889353898855355, "compression_ratio": 1.34640522875817, "no_speech_prob": 0.04607027769088745}, {"id": 103, "seek": 71468, "start": 714.68, "end": 720.56, "text": " This would highlight for us database operation bottlenecks.", "tokens": [50364, 639, 576, 5078, 337, 505, 8149, 6916, 44641, 2761, 13, 50658], "temperature": 0.0, "avg_logprob": -0.16628544671194895, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.04325452074408531}, {"id": 104, "seek": 71468, "start": 720.56, "end": 729.12, "text": " And this way, having the database level tool, we would also be able to run various experiments", "tokens": [50658, 400, 341, 636, 11, 1419, 264, 8149, 1496, 2290, 11, 321, 576, 611, 312, 1075, 281, 1190, 3683, 12050, 51086], "temperature": 0.0, "avg_logprob": -0.16628544671194895, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.04325452074408531}, {"id": 105, "seek": 71468, "start": 729.12, "end": 737.56, "text": " on the database like changes in indexes or maybe adding query planner hints.", "tokens": [51086, 322, 264, 8149, 411, 2962, 294, 8186, 279, 420, 1310, 5127, 14581, 31268, 27271, 13, 51508], "temperature": 0.0, "avg_logprob": -0.16628544671194895, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.04325452074408531}, {"id": 106, "seek": 71468, "start": 737.56, "end": 744.5999999999999, "text": " It almost doesn't cost us anything just running another container in our local setup or", "tokens": [51508, 467, 1920, 1177, 380, 2063, 505, 1340, 445, 2614, 1071, 10129, 294, 527, 2654, 8657, 420, 51860], "temperature": 0.0, "avg_logprob": -0.16628544671194895, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.04325452074408531}, {"id": 107, "seek": 74460, "start": 745.0400000000001, "end": 756.64, "text": " if PGAdmin is too much, you could also opt to use the online available graphical tool", "tokens": [50386, 498, 40975, 15830, 2367, 307, 886, 709, 11, 291, 727, 611, 2427, 281, 764, 264, 2950, 2435, 35942, 2290, 50966], "temperature": 0.0, "avg_logprob": -0.2434971586186835, "compression_ratio": 1.3795620437956204, "no_speech_prob": 0.0581335574388504}, {"id": 108, "seek": 74460, "start": 756.64, "end": 766.88, "text": " which would highlight the bottlenecks for you with this heat map showing you where the issue", "tokens": [50966, 597, 576, 5078, 264, 44641, 2761, 337, 291, 365, 341, 3738, 4471, 4099, 291, 689, 264, 2734, 51478], "temperature": 0.0, "avg_logprob": -0.2434971586186835, "compression_ratio": 1.3795620437956204, "no_speech_prob": 0.0581335574388504}, {"id": 109, "seek": 74460, "start": 766.88, "end": 770.96, "text": " might lie.", "tokens": [51478, 1062, 4544, 13, 51682], "temperature": 0.0, "avg_logprob": -0.2434971586186835, "compression_ratio": 1.3795620437956204, "no_speech_prob": 0.0581335574388504}, {"id": 110, "seek": 77096, "start": 770.96, "end": 778.08, "text": " Using this database level utility completes our tool set for off interactive solutions", "tokens": [50364, 11142, 341, 8149, 1496, 14877, 36362, 527, 2290, 992, 337, 766, 15141, 6547, 50720], "temperature": 0.0, "avg_logprob": -0.2430515904580393, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.2807730436325073}, {"id": 111, "seek": 77096, "start": 778.08, "end": 785.76, "text": " and while it is really important to be able to perform all those actions, it's paramount", "tokens": [50720, 293, 1339, 309, 307, 534, 1021, 281, 312, 1075, 281, 2042, 439, 729, 5909, 11, 309, 311, 6220, 792, 51104], "temperature": 0.0, "avg_logprob": -0.2430515904580393, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.2807730436325073}, {"id": 112, "seek": 77096, "start": 785.76, "end": 794.8000000000001, "text": " to do that again sometime soon and again and again and again and that moves us to automation", "tokens": [51104, 281, 360, 300, 797, 15053, 2321, 293, 797, 293, 797, 293, 797, 293, 300, 6067, 505, 281, 17769, 51556], "temperature": 0.0, "avg_logprob": -0.2430515904580393, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.2807730436325073}, {"id": 113, "seek": 77096, "start": 794.8000000000001, "end": 799.12, "text": " solutions.", "tokens": [51556, 6547, 13, 51772], "temperature": 0.0, "avg_logprob": -0.2430515904580393, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.2807730436325073}, {"id": 114, "seek": 79912, "start": 799.12, "end": 806.76, "text": " By now, we know what to look for or what to watch out for in our lava instances and", "tokens": [50364, 3146, 586, 11, 321, 458, 437, 281, 574, 337, 420, 437, 281, 1159, 484, 337, 294, 527, 22097, 14519, 293, 50746], "temperature": 0.0, "avg_logprob": -0.18762483596801757, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.17713312804698944}, {"id": 115, "seek": 79912, "start": 806.76, "end": 814.84, "text": " from user stories or bug reports or the motomo statistics or load balancer logs I mentioned", "tokens": [50746, 490, 4195, 3676, 420, 7426, 7122, 420, 264, 2184, 13395, 12523, 420, 3677, 3119, 28347, 20820, 286, 2835, 51150], "temperature": 0.0, "avg_logprob": -0.18762483596801757, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.17713312804698944}, {"id": 116, "seek": 79912, "start": 814.84, "end": 824.6800000000001, "text": " earlier, we know and have specific code components to track or maybe even test cases ready to", "tokens": [51150, 3071, 11, 321, 458, 293, 362, 2685, 3089, 6677, 281, 2837, 420, 1310, 754, 1500, 3331, 1919, 281, 51642], "temperature": 0.0, "avg_logprob": -0.18762483596801757, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.17713312804698944}, {"id": 117, "seek": 79912, "start": 824.6800000000001, "end": 826.04, "text": " check for that.", "tokens": [51642, 1520, 337, 300, 13, 51710], "temperature": 0.0, "avg_logprob": -0.18762483596801757, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.17713312804698944}, {"id": 118, "seek": 82604, "start": 826.04, "end": 836.28, "text": " But the question is how to run those test cases to get the statistically valid feedback.", "tokens": [50364, 583, 264, 1168, 307, 577, 281, 1190, 729, 1500, 3331, 281, 483, 264, 36478, 7363, 5824, 13, 50876], "temperature": 0.0, "avg_logprob": -0.17837202353555648, "compression_ratio": 1.5310734463276836, "no_speech_prob": 0.08937348425388336}, {"id": 119, "seek": 82604, "start": 836.28, "end": 843.8399999999999, "text": " We would have to take into consideration cache warm-ups, test case calibration, preferably", "tokens": [50876, 492, 576, 362, 281, 747, 666, 12381, 19459, 4561, 12, 7528, 11, 1500, 1389, 38732, 11, 45916, 51254], "temperature": 0.0, "avg_logprob": -0.17837202353555648, "compression_ratio": 1.5310734463276836, "no_speech_prob": 0.08937348425388336}, {"id": 120, "seek": 82604, "start": 843.8399999999999, "end": 855.8, "text": " also a way to compare between benchmark runs and it would be also great if it fit well into", "tokens": [51254, 611, 257, 636, 281, 6794, 1296, 18927, 6676, 293, 309, 576, 312, 611, 869, 498, 309, 3318, 731, 666, 51852], "temperature": 0.0, "avg_logprob": -0.17837202353555648, "compression_ratio": 1.5310734463276836, "no_speech_prob": 0.08937348425388336}, {"id": 121, "seek": 85580, "start": 856.56, "end": 865.3199999999999, "text": " the test suites that are currently used by the upstream project which by the way is based", "tokens": [50402, 264, 1500, 459, 3324, 300, 366, 4362, 1143, 538, 264, 33915, 1716, 597, 538, 264, 636, 307, 2361, 50840], "temperature": 0.0, "avg_logprob": -0.23502437488452807, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.04640810191631317}, {"id": 122, "seek": 85580, "start": 865.3199999999999, "end": 867.0, "text": " on PyTest.", "tokens": [50840, 322, 9953, 51, 377, 13, 50924], "temperature": 0.0, "avg_logprob": -0.23502437488452807, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.04640810191631317}, {"id": 123, "seek": 85580, "start": 867.0, "end": 873.88, "text": " Fortunately, it turns out that there is a PyTest feature that provides all of that and", "tokens": [50924, 20652, 11, 309, 4523, 484, 300, 456, 307, 257, 9953, 51, 377, 4111, 300, 6417, 439, 295, 300, 293, 51268], "temperature": 0.0, "avg_logprob": -0.23502437488452807, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.04640810191631317}, {"id": 124, "seek": 85580, "start": 873.88, "end": 875.4799999999999, "text": " even more.", "tokens": [51268, 754, 544, 13, 51348], "temperature": 0.0, "avg_logprob": -0.23502437488452807, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.04640810191631317}, {"id": 125, "seek": 85580, "start": 875.4799999999999, "end": 881.92, "text": " In the case of lava bottlenecks found in the collaboration instance, the next step was", "tokens": [51348, 682, 264, 1389, 295, 22097, 44641, 2761, 1352, 294, 264, 9363, 5197, 11, 264, 958, 1823, 390, 51670], "temperature": 0.0, "avg_logprob": -0.23502437488452807, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.04640810191631317}, {"id": 126, "seek": 88192, "start": 881.92, "end": 892.68, "text": " just to wrap the test cases prepared with this fixture and wrapping the key pieces of", "tokens": [50364, 445, 281, 7019, 264, 1500, 3331, 4927, 365, 341, 47680, 293, 21993, 264, 2141, 3755, 295, 50902], "temperature": 0.0, "avg_logprob": -0.15720201006122664, "compression_ratio": 1.4788732394366197, "no_speech_prob": 0.023575399070978165}, {"id": 127, "seek": 88192, "start": 892.68, "end": 902.0, "text": " code allowed to have benchmarks ready to run.", "tokens": [50902, 3089, 4350, 281, 362, 43751, 1919, 281, 1190, 13, 51368], "temperature": 0.0, "avg_logprob": -0.15720201006122664, "compression_ratio": 1.4788732394366197, "no_speech_prob": 0.023575399070978165}, {"id": 128, "seek": 88192, "start": 902.0, "end": 909.3199999999999, "text": " Next step, once the test suite was prepared, was to plug it into the pipeline.", "tokens": [51368, 3087, 1823, 11, 1564, 264, 1500, 14205, 390, 4927, 11, 390, 281, 5452, 309, 666, 264, 15517, 13, 51734], "temperature": 0.0, "avg_logprob": -0.15720201006122664, "compression_ratio": 1.4788732394366197, "no_speech_prob": 0.023575399070978165}, {"id": 129, "seek": 90932, "start": 909.32, "end": 920.6800000000001, "text": " Both upstream lava project and downstream lava tree makes heavy use of GitLab CI and", "tokens": [50364, 6767, 33915, 22097, 1716, 293, 30621, 22097, 4230, 1669, 4676, 764, 295, 16939, 37880, 37777, 293, 50932], "temperature": 0.0, "avg_logprob": -0.30127437159700216, "compression_ratio": 1.3133333333333332, "no_speech_prob": 0.19253228604793549}, {"id": 130, "seek": 90932, "start": 920.6800000000001, "end": 922.5600000000001, "text": " it shouldn't be surprising.", "tokens": [50932, 309, 4659, 380, 312, 8830, 13, 51026], "temperature": 0.0, "avg_logprob": -0.30127437159700216, "compression_ratio": 1.3133333333333332, "no_speech_prob": 0.19253228604793549}, {"id": 131, "seek": 90932, "start": 922.5600000000001, "end": 924.44, "text": " Many projects already do the same.", "tokens": [51026, 5126, 4455, 1217, 360, 264, 912, 13, 51120], "temperature": 0.0, "avg_logprob": -0.30127437159700216, "compression_ratio": 1.3133333333333332, "no_speech_prob": 0.19253228604793549}, {"id": 132, "seek": 90932, "start": 924.44, "end": 931.8000000000001, "text": " For example, DRM CI merged in kernel 6.6 release.", "tokens": [51120, 1171, 1365, 11, 12118, 44, 37777, 36427, 294, 28256, 1386, 13, 21, 4374, 13, 51488], "temperature": 0.0, "avg_logprob": -0.30127437159700216, "compression_ratio": 1.3133333333333332, "no_speech_prob": 0.19253228604793549}, {"id": 133, "seek": 93180, "start": 931.8, "end": 941.04, "text": " So currently, job definitions for those GitLab CI pipelines above the downstream one and below", "tokens": [50364, 407, 4362, 11, 1691, 21988, 337, 729, 16939, 37880, 37777, 40168, 3673, 264, 30621, 472, 293, 2507, 50826], "temperature": 0.0, "avg_logprob": -0.19891643524169922, "compression_ratio": 1.3125, "no_speech_prob": 0.09162675589323044}, {"id": 134, "seek": 93180, "start": 941.04, "end": 949.56, "text": " the upstream one don't share any reusable code.", "tokens": [50826, 264, 33915, 472, 500, 380, 2073, 604, 41807, 3089, 13, 51252], "temperature": 0.0, "avg_logprob": -0.19891643524169922, "compression_ratio": 1.3125, "no_speech_prob": 0.09162675589323044}, {"id": 135, "seek": 93180, "start": 949.56, "end": 954.1999999999999, "text": " This might be subject to change in the future.", "tokens": [51252, 639, 1062, 312, 3983, 281, 1319, 294, 264, 2027, 13, 51484], "temperature": 0.0, "avg_logprob": -0.19891643524169922, "compression_ratio": 1.3125, "no_speech_prob": 0.09162675589323044}, {"id": 136, "seek": 95420, "start": 954.2, "end": 962.84, "text": " For now, downstream changes are made with ease of importing them later in mind.", "tokens": [50364, 1171, 586, 11, 30621, 2962, 366, 1027, 365, 12708, 295, 43866, 552, 1780, 294, 1575, 13, 50796], "temperature": 0.0, "avg_logprob": -0.16145203794751847, "compression_ratio": 1.3742690058479532, "no_speech_prob": 0.0791238471865654}, {"id": 137, "seek": 95420, "start": 962.84, "end": 970.08, "text": " Moving to external definitions could make the GitLab CI pipelines a bit more complex,", "tokens": [50796, 14242, 281, 8320, 21988, 727, 652, 264, 16939, 37880, 37777, 40168, 257, 857, 544, 3997, 11, 51158], "temperature": 0.0, "avg_logprob": -0.16145203794751847, "compression_ratio": 1.3742690058479532, "no_speech_prob": 0.0791238471865654}, {"id": 138, "seek": 95420, "start": 970.08, "end": 978.6400000000001, "text": " but that's something that we'll see if it brings any value in future.", "tokens": [51158, 457, 300, 311, 746, 300, 321, 603, 536, 498, 309, 5607, 604, 2158, 294, 2027, 13, 51586], "temperature": 0.0, "avg_logprob": -0.16145203794751847, "compression_ratio": 1.3742690058479532, "no_speech_prob": 0.0791238471865654}, {"id": 139, "seek": 97864, "start": 978.64, "end": 987.6, "text": " Of course, GitLab CI jobs need a run environment and to get a baseline of what should be expected", "tokens": [50364, 2720, 1164, 11, 16939, 37880, 37777, 4782, 643, 257, 1190, 2823, 293, 281, 483, 257, 20518, 295, 437, 820, 312, 5176, 50812], "temperature": 0.0, "avg_logprob": -0.1585637967880458, "compression_ratio": 1.4926108374384237, "no_speech_prob": 0.25431910157203674}, {"id": 140, "seek": 97864, "start": 987.6, "end": 995.3199999999999, "text": " from benchmarks run, the easy way out is having a dedicated runner that would provide most", "tokens": [50812, 490, 43751, 1190, 11, 264, 1858, 636, 484, 307, 1419, 257, 8374, 24376, 300, 576, 2893, 881, 51198], "temperature": 0.0, "avg_logprob": -0.1585637967880458, "compression_ratio": 1.4926108374384237, "no_speech_prob": 0.25431910157203674}, {"id": 141, "seek": 97864, "start": 995.3199999999999, "end": 1004.2, "text": " stable results that are not affected by, for example, other test suites run in parallel", "tokens": [51198, 8351, 3542, 300, 366, 406, 8028, 538, 11, 337, 1365, 11, 661, 1500, 459, 3324, 1190, 294, 8952, 51642], "temperature": 0.0, "avg_logprob": -0.1585637967880458, "compression_ratio": 1.4926108374384237, "no_speech_prob": 0.25431910157203674}, {"id": 142, "seek": 97864, "start": 1004.2, "end": 1007.56, "text": " on the same GitLab runner.", "tokens": [51642, 322, 264, 912, 16939, 37880, 24376, 13, 51810], "temperature": 0.0, "avg_logprob": -0.1585637967880458, "compression_ratio": 1.4926108374384237, "no_speech_prob": 0.25431910157203674}, {"id": 143, "seek": 100756, "start": 1007.56, "end": 1017.4, "text": " A good choice would be to select a machine that has similar resources to a node, which", "tokens": [50364, 316, 665, 3922, 576, 312, 281, 3048, 257, 3479, 300, 575, 2531, 3593, 281, 257, 9984, 11, 597, 50856], "temperature": 0.0, "avg_logprob": -0.22223407146977445, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.03957746550440788}, {"id": 144, "seek": 100756, "start": 1017.4, "end": 1020.4, "text": " your lava instance is run on.", "tokens": [50856, 428, 22097, 5197, 307, 1190, 322, 13, 51006], "temperature": 0.0, "avg_logprob": -0.22223407146977445, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.03957746550440788}, {"id": 145, "seek": 100756, "start": 1020.4, "end": 1032.72, "text": " And for proof of concept purposes, I used a small desktop computer which gave just that.", "tokens": [51006, 400, 337, 8177, 295, 3410, 9932, 11, 286, 1143, 257, 1359, 14502, 3820, 597, 2729, 445, 300, 13, 51622], "temperature": 0.0, "avg_logprob": -0.22223407146977445, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.03957746550440788}, {"id": 146, "seek": 103272, "start": 1032.72, "end": 1039.32, "text": " GitLab runners are also really easy to plug into a GitLab server.", "tokens": [50364, 16939, 37880, 33892, 366, 611, 534, 1858, 281, 5452, 666, 257, 16939, 37880, 7154, 13, 50694], "temperature": 0.0, "avg_logprob": -0.1473811373991125, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.2438182681798935}, {"id": 147, "seek": 103272, "start": 1039.32, "end": 1046.56, "text": " And while we are already optimizing the pipeline, we should also take into consideration caching", "tokens": [50694, 400, 1339, 321, 366, 1217, 40425, 264, 15517, 11, 321, 820, 611, 747, 666, 12381, 269, 2834, 51056], "temperature": 0.0, "avg_logprob": -0.1473811373991125, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.2438182681798935}, {"id": 148, "seek": 103272, "start": 1046.56, "end": 1052.2, "text": " the CI data resources for benchmarks runs.", "tokens": [51056, 264, 37777, 1412, 3593, 337, 43751, 6676, 13, 51338], "temperature": 0.0, "avg_logprob": -0.1473811373991125, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.2438182681798935}, {"id": 149, "seek": 103272, "start": 1052.2, "end": 1058.52, "text": " For that, we could easily use already available upstream lava caching solution, which is based", "tokens": [51338, 1171, 300, 11, 321, 727, 3612, 764, 1217, 2435, 33915, 22097, 269, 2834, 3827, 11, 597, 307, 2361, 51654], "temperature": 0.0, "avg_logprob": -0.1473811373991125, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.2438182681798935}, {"id": 150, "seek": 105852, "start": 1058.52, "end": 1064.12, "text": " on specific CI images to run tests on.", "tokens": [50364, 322, 2685, 37777, 5267, 281, 1190, 6921, 322, 13, 50644], "temperature": 0.0, "avg_logprob": -0.1802705667786679, "compression_ratio": 1.4875, "no_speech_prob": 0.022977158427238464}, {"id": 151, "seek": 105852, "start": 1064.12, "end": 1071.32, "text": " But that would also mean that production data from database we used earlier is no longer", "tokens": [50644, 583, 300, 576, 611, 914, 300, 4265, 1412, 490, 8149, 321, 1143, 3071, 307, 572, 2854, 51004], "temperature": 0.0, "avg_logprob": -0.1802705667786679, "compression_ratio": 1.4875, "no_speech_prob": 0.022977158427238464}, {"id": 152, "seek": 105852, "start": 1071.32, "end": 1073.4, "text": " a valid option for us.", "tokens": [51004, 257, 7363, 3614, 337, 505, 13, 51108], "temperature": 0.0, "avg_logprob": -0.1802705667786679, "compression_ratio": 1.4875, "no_speech_prob": 0.022977158427238464}, {"id": 153, "seek": 105852, "start": 1073.4, "end": 1087.6, "text": " And we need to revisit the lava server model, which brings us to data generation, which", "tokens": [51108, 400, 321, 643, 281, 32676, 264, 22097, 7154, 2316, 11, 597, 5607, 505, 281, 1412, 5125, 11, 597, 51818], "temperature": 0.0, "avg_logprob": -0.1802705667786679, "compression_ratio": 1.4875, "no_speech_prob": 0.022977158427238464}, {"id": 154, "seek": 108760, "start": 1087.6, "end": 1092.12, "text": " we no longer could omit.", "tokens": [50364, 321, 572, 2854, 727, 3406, 270, 13, 50590], "temperature": 0.0, "avg_logprob": -0.22624372033511891, "compression_ratio": 1.463768115942029, "no_speech_prob": 0.03806125000119209}, {"id": 155, "seek": 108760, "start": 1092.12, "end": 1099.76, "text": " That brought us to creating a dummy database generator, which was focused just on a key", "tokens": [50590, 663, 3038, 505, 281, 4084, 257, 35064, 8149, 19265, 11, 597, 390, 5178, 445, 322, 257, 2141, 50972], "temperature": 0.0, "avg_logprob": -0.22624372033511891, "compression_ratio": 1.463768115942029, "no_speech_prob": 0.03806125000119209}, {"id": 156, "seek": 108760, "start": 1099.76, "end": 1107.48, "text": " few tables and relations according to Postgres planner statistics.", "tokens": [50972, 1326, 8020, 293, 2299, 4650, 281, 10223, 45189, 31268, 12523, 13, 51358], "temperature": 0.0, "avg_logprob": -0.22624372033511891, "compression_ratio": 1.463768115942029, "no_speech_prob": 0.03806125000119209}, {"id": 157, "seek": 108760, "start": 1107.48, "end": 1114.12, "text": " It was implemented with very limited scope to only support the worst bottlenecks that", "tokens": [51358, 467, 390, 12270, 365, 588, 5567, 11923, 281, 787, 1406, 264, 5855, 44641, 2761, 300, 51690], "temperature": 0.0, "avg_logprob": -0.22624372033511891, "compression_ratio": 1.463768115942029, "no_speech_prob": 0.03806125000119209}, {"id": 158, "seek": 108760, "start": 1114.12, "end": 1117.32, "text": " were found in collaborators instance.", "tokens": [51690, 645, 1352, 294, 39789, 5197, 13, 51850], "temperature": 0.0, "avg_logprob": -0.22624372033511891, "compression_ratio": 1.463768115942029, "no_speech_prob": 0.03806125000119209}, {"id": 159, "seek": 111732, "start": 1117.32, "end": 1125.24, "text": " And for that, we used standard Python tools, which were FactoryBoy and Faker.", "tokens": [50364, 400, 337, 300, 11, 321, 1143, 3832, 15329, 3873, 11, 597, 645, 36868, 31378, 293, 479, 4003, 13, 50760], "temperature": 0.0, "avg_logprob": -0.19704981644948324, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.016451263800263405}, {"id": 160, "seek": 111732, "start": 1125.24, "end": 1135.56, "text": " As a bonus addition, you might also want to ask a few questions.", "tokens": [50760, 1018, 257, 10882, 4500, 11, 291, 1062, 611, 528, 281, 1029, 257, 1326, 1651, 13, 51276], "temperature": 0.0, "avg_logprob": -0.19704981644948324, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.016451263800263405}, {"id": 161, "seek": 111732, "start": 1135.56, "end": 1141.3999999999999, "text": " Should lava actually archive all the test jobs that are run, or maybe archiving those", "tokens": [51276, 6454, 22097, 767, 23507, 439, 264, 1500, 4782, 300, 366, 1190, 11, 420, 1310, 3912, 2123, 729, 51568], "temperature": 0.0, "avg_logprob": -0.19704981644948324, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.016451263800263405}, {"id": 162, "seek": 111732, "start": 1141.3999999999999, "end": 1146.4399999999998, "text": " jobs can be delegated to a higher level test systems?", "tokens": [51568, 4782, 393, 312, 15824, 770, 281, 257, 2946, 1496, 1500, 3652, 30, 51820], "temperature": 0.0, "avg_logprob": -0.19704981644948324, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.016451263800263405}, {"id": 163, "seek": 114644, "start": 1147.44, "end": 1150.44, "text": " Fortunately, retention mechanism is already available.", "tokens": [50414, 20652, 11, 22871, 7513, 307, 1217, 2435, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2539586869497148, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.02644880674779415}, {"id": 164, "seek": 114644, "start": 1150.44, "end": 1157.88, "text": " In upstream lava, it just required enabling it in Helm charts, which is used to deploy", "tokens": [50564, 682, 33915, 22097, 11, 309, 445, 4739, 23148, 309, 294, 6128, 76, 17767, 11, 597, 307, 1143, 281, 7274, 50936], "temperature": 0.0, "avg_logprob": -0.2539586869497148, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.02644880674779415}, {"id": 165, "seek": 114644, "start": 1157.88, "end": 1163.24, "text": " lava instances for collaboration.", "tokens": [50936, 22097, 14519, 337, 9363, 13, 51204], "temperature": 0.0, "avg_logprob": -0.2539586869497148, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.02644880674779415}, {"id": 166, "seek": 114644, "start": 1163.24, "end": 1171.92, "text": " To summarize all of that, I've got three final thoughts that I would like to share with you.", "tokens": [51204, 1407, 20858, 439, 295, 300, 11, 286, 600, 658, 1045, 2572, 4598, 300, 286, 576, 411, 281, 2073, 365, 291, 13, 51638], "temperature": 0.0, "avg_logprob": -0.2539586869497148, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.02644880674779415}, {"id": 167, "seek": 117192, "start": 1171.92, "end": 1177.52, "text": " Constructing in testing laboratories is not a one-time job.", "tokens": [50364, 8574, 1757, 278, 294, 4997, 41013, 307, 406, 257, 472, 12, 3766, 1691, 13, 50644], "temperature": 0.0, "avg_logprob": -0.19432954490184784, "compression_ratio": 1.4682080924855492, "no_speech_prob": 0.23457174003124237}, {"id": 168, "seek": 117192, "start": 1177.52, "end": 1185.68, "text": " It's a process that might differ from instance to instance, depending on your specific workload.", "tokens": [50644, 467, 311, 257, 1399, 300, 1062, 743, 490, 5197, 281, 5197, 11, 5413, 322, 428, 2685, 20139, 13, 51052], "temperature": 0.0, "avg_logprob": -0.19432954490184784, "compression_ratio": 1.4682080924855492, "no_speech_prob": 0.23457174003124237}, {"id": 169, "seek": 117192, "start": 1185.68, "end": 1192.24, "text": " But it's something that I hope could be easier for you if you come across the same set of", "tokens": [51052, 583, 309, 311, 746, 300, 286, 1454, 727, 312, 3571, 337, 291, 498, 291, 808, 2108, 264, 912, 992, 295, 51380], "temperature": 0.0, "avg_logprob": -0.19432954490184784, "compression_ratio": 1.4682080924855492, "no_speech_prob": 0.23457174003124237}, {"id": 170, "seek": 117192, "start": 1192.24, "end": 1195.68, "text": " issues.", "tokens": [51380, 2663, 13, 51552], "temperature": 0.0, "avg_logprob": -0.19432954490184784, "compression_ratio": 1.4682080924855492, "no_speech_prob": 0.23457174003124237}, {"id": 171, "seek": 119568, "start": 1195.68, "end": 1203.8, "text": " It also requires frequent revisiting and adjusting according to the results you see.", "tokens": [50364, 467, 611, 7029, 18004, 20767, 1748, 293, 23559, 4650, 281, 264, 3542, 291, 536, 13, 50770], "temperature": 0.0, "avg_logprob": -0.19556187294624947, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.08295587450265884}, {"id": 172, "seek": 119568, "start": 1203.8, "end": 1211.24, "text": " But even small changes can bring huge boosts in performance.", "tokens": [50770, 583, 754, 1359, 2962, 393, 1565, 2603, 9194, 82, 294, 3389, 13, 51142], "temperature": 0.0, "avg_logprob": -0.19556187294624947, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.08295587450265884}, {"id": 173, "seek": 119568, "start": 1211.24, "end": 1217.3200000000002, "text": " But that probably is a topic for another talk.", "tokens": [51142, 583, 300, 1391, 307, 257, 4829, 337, 1071, 751, 13, 51446], "temperature": 0.0, "avg_logprob": -0.19556187294624947, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.08295587450265884}, {"id": 174, "seek": 119568, "start": 1217.3200000000002, "end": 1221.48, "text": " And that's all I have prepared for you today.", "tokens": [51446, 400, 300, 311, 439, 286, 362, 4927, 337, 291, 965, 13, 51654], "temperature": 0.0, "avg_logprob": -0.19556187294624947, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.08295587450265884}, {"id": 175, "seek": 119568, "start": 1221.48, "end": 1222.48, "text": " Thanks for your attention.", "tokens": [51654, 2561, 337, 428, 3202, 13, 51704], "temperature": 0.0, "avg_logprob": -0.19556187294624947, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.08295587450265884}, {"id": 176, "seek": 119568, "start": 1222.48, "end": 1224.68, "text": " Do we have time for questions?", "tokens": [51704, 1144, 321, 362, 565, 337, 1651, 30, 51814], "temperature": 0.0, "avg_logprob": -0.19556187294624947, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.08295587450265884}, {"id": 177, "seek": 122468, "start": 1224.68, "end": 1228.68, "text": " If there is some question, I will be happy to answer it.", "tokens": [50364, 759, 456, 307, 512, 1168, 11, 286, 486, 312, 2055, 281, 1867, 309, 13, 50564], "temperature": 0.0, "avg_logprob": -0.4032134729273179, "compression_ratio": 0.9032258064516129, "no_speech_prob": 0.08348441123962402}], "language": "en"}