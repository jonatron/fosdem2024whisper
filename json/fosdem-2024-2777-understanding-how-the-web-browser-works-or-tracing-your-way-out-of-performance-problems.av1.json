{"text": " Hello. Can you hear me all right? Welcome to the next installment in your regular scheduled entertainment. I think that's the great benefit of being scheduled after such great talks that everyone is in and with no break you can't leave. So you're kind of stuck with me for the next half an hour. So welcome. I hope you enjoy. So this is a very ambitious talk or at least as title suggests that it is. So let me actually start with talking what it really is about. And this talk is really about me and my experience. And hope, frustrations, aspirations, experience and some illustrations. So I'm Alex. I have been doing web performance, mobile web performance at Google in Chromium for the last eight or so years. So this talk is going to be pretty much about that. This is not going to be a practical talk. So because we don't have time. This is the first reason. The second is that there are way too many rough edges. So I wouldn't recommend this at this point to start to reproduce this. But hopefully this will be a source of inspiration. And those of you who are desperate enough and frustrated enough and have seen the problems outlined in this talk too many times, hopefully they will be brave enough to venture and try. There is a practical guide that I would recommend in the recent web performance calendar by the one and only great Annie Sullivan. I would recommend you go and check it out, preferably after this talk. But you know, I can't prohibit you from doing so. So this talk really is about problem solving and working with complex systems and trying to make sense of them. So the examples will be from Chromium. I will talk about Perfetto. But I think these examples hopefully will be an inspiration for a great variety of projects, both when building your sites and building other and working with other complex systems as well. So let's talk about performance and improvement performance. If you want to improve performance, as I imagine you are not totally adverse to, given that you are in this room, then I want to remind certain trivial things that you probably already know. The first is that performance problems are nasty. They are unpolite and they don't have the common courtesy of locating themselves in a nice isolated area of code in your project that you can master and just work on and don't bother with the rest of the stuff. So because of that, kind of knowing what to improve and where to improve and how to improve takes a substantial effort of the performance work. And the fact that more and more web is mind boggling complex with new APIs, both performance and non-performance, browsers getting more complex and bigger every day, various sites, drawing in diversity and in complexity and libraries and so on. So these all leads to all of us working on performance, spending a lot of time on a regular basis trying to understand what the hell is going on here. And what are the approaches? So the first approach that I have to mention is you can go and read the code. You are a very brave person and I wish you the very best of luck if you decide to do it, but it's not very practical. So modern projects are layers of up and layers of obstructions and then you have a listener and then you have 30 possible callbacks or entry points and then good luck. Usually, I give up at this point when I see like, hey, no, this is probably one of these 30 things. The second one is printf and it's possible variations. So it's console, it's log, it's other, just log it statements. And the second is the buggers. So GDB, LLDB, RR, Chrome DevTools, some of them are better than the others. But all of them, these approaches effectively don't scale to complex systems, especially if you talk about indeterminism when you test sometimes reproduces and the error sometimes reproduces sometimes doesn't then you are in a bit of fun. So when you have multiple processes and multiple architectures, multiple architecture components, then all of these, you know, these tools don't work particularly well. So they focus on low level details. Hey, what is this variable? And most often you want to know, hey, what this component is doing? And am I doing a good job? So enter tracing. How many of you are familiar with tracing in some form or the other? Some of them. So pretty much tracing is structured logging and visualization. I will go into this a little bit more further down the line. But as far as chromium is concerned, from the practical perspective, it means turning these annotations. So here we have a request resource from Java function that is being annotated with tracement macro. So we in C++, we emit some information when we enter this function. And we emit some information that we exit this function when tracing is enabled. And this will allow us to look at this nice timeline. Pretty much the x axis is time advancing in time. And here you can see that we have entered this function here, you have exited this function. And you can see which other functions were called inside of it, how long it took. And you can see zooming out what else the system has been doing across different threads across different processes, which is I think a good starting point and the basic infrastructure talk about. So if you wanted to use it to trade yourself, you can actually go to your IPF at a depth. And the examples in this talk are pretty much all from open Chrome example. So if you have a laptop, then you can go and follow it. Then the links to the slides should be on the FOSDEM site for this talk. But we'll back talking about how to make this useful. So you have this wonderful instrumentation. And this already is, you can use it as a fancy fprintf with search functionality. You can just record a lot of information and then look at it. But this is basically instrumenting the code you're already working on as a fancy fprintf. It's powerful and flexible, but not necessarily most convenient. And it doesn't win either compared to fprintf or debuggers out of the box. For fprintf, the basic debug loop is still faster. You had a single statement, you don't have to bother with opening anything anywhere. You just see the console output and you're done. So it gets less pleasant when you have to do it multiple times. And with debuggers, every all information is present. You can take you a bit of time to find it, but you don't have to bother with adding more annotations, recompiling and wasting time there. So like, and it's unrealistic to have all of the functions instrumented and captured in this race, because it's too much information both to record, which adds all of overhead and slow downs, but also it's a lot of information to go through and looking at it is not pleasant and not fast. So I will talk about finding opportunities for scaling this instrumentation and finding the opportunities where a few instrumentation points can give us a lot of information and substantially advance our ability to reason about what the code is doing. And enter Chrome task schedule. Chromium is implemented based on an event loop model. So we have a bunch of name threads. We have browser process with a browser main thread where which is responsible for coordinating everything. We have the render process with the main thread, which is responsible for running JavaScript, bling DOM and whatever. Or we have worker pulls, sorry, dedicated workers, which sites can create using new worker API. There is a thread pool for miscellaneous background work. And these is pretty much all there is and various place in the code. In the code base, I think we have, you know, a few thousand places. So maybe 10,000 nowadays, which basically post tasks. They get a task runner from somewhere and they post a task. No, the from here macro will talk about this in a second. But otherwise it's just a fancy lambda with some of safety thrown in. And here it is. So you post this task somewhere, you know, some thread or thread pool picks this up and it will run this task. Voila. And this is a great point for tracing instrumentation. And this is a great point to start looking at. So what it gives us, it gives us that we will have pretty much all of the places running Chromium and code. We will know about them and we will have some basic information. Here specifically look at posted from information. This is the result of from here macro expansion, which using some of the C macro tricks, give automatically without any further support, gives us file name on the function. So at least for every function, you have a basic idea of where this information, where this task has been posted from. And you can go to that part of the code base and start understanding what the hell is going on and why this task might be running. So then we can zoom out and we also have instrumentation for post task. And the post task and run tasks conveniently are linked through a flow event. And what this actually means that you, instead of looking at a single task, which might or might not be useful, you can also explore which tasks this came from and which tasks the task it came from came from here because of, I can't really zoom out and I can't really make it interactive. So I can't show the entire to all threads involved. So this is a view of a single thread. But hey, you can see I selected a single task. I can see an incoming flow that is coming from a thread pool. And as you can see that that thread pool task is coming from another task from the main thread. And so you can see that actually all of these smaller tasks running after a larger task, they have been posted from it. And these are, we know, these are related. So this is pretty much a very good starting point, but it doesn't give us everything. So there are a few other chalk points that might be useful to instrument and that have been useful to instrument that can, that improve our ability to reason about what is going on. Task scheduling is inherently inter-intro process. So it doesn't tell us about inter-pros communication, but fortunately in Chromium we have Mojo, which is an IPC subsystem, which we can also instrument and get pretty much the same information. But for cross-pros communication, we can know who is sending messages and we can connect the place which posted the message and places that received the message and to be able to trace this back through the flow. Capturing console logs and DV logs and debug logs and the both logs is also not a great source of information. If someone bothered to log it somewhere in the system, that's probably already useful for us. And being able to correlate this additional source with data with actual tasks that Chromium is using have been proven useful in many investigations. Capturing, instrumenting all of the blink binings and pretty much capturing all of the JavaScript functions, JavaScript calls that end up being implemented in blink is another great way to reason about what is going on and what the website is doing on and a couple of other similar infrastructure pieces. So the key takeaway here is that, hey, if you have complex systems, then probably you would do some good to instrument some of the widely used things and if you are familiar with this codebase, you will be able to make some informed judgment of what is going on and you will be able to spot outliers, something taking too long, log being held in case of performance regression or a functional regression or a flaky test, etc. And that's already a great step forward. So you have, you can look at it, you can like, if your test is flaky, you can run a thousand times, it will fail five times, you can open five phrases, look and see if you're lucky enough, you will be able to spot noticeable difference. But this is still not good enough for me. And the problem is that, despite having visibility into everything we're doing, this is very, very, very expertise intensive. So in order to be able to make good use of it, you have to kind of know everything. You have to know a lot about Chromium architecture. So as some of my colleagues say, you have to have a PhD in tracing and Chromium architecture to truly make this useful. And I have an inspiration of, hey, let's get it to the point that anyone, so any web developer can open and trace and instead of being discouraged and being intimidated by all of this mumbo jumbo, they can learn something about how Chromium actually works and get more knowledge about this. So an inspiration that I have is this slide and this diagram from a life of a delegation, talk from Chromium University, that is kind of similar to what we have already seen. It's a kind of a virtual timeline with a kind of boxers being connected by arrows. But if you look at it, then even if you are not deeply familiar with the browser architecture, then you probably kind of make some sense and you can make some educated guesses of what is going on. For example, if you see network stack doing start URL request as a one off stage, it's something that you can develop or get a reasonably good intuition for. And that's kind of the status quo that we currently have, which is pretty much exactly the same information, but slightly less useful, slightly less easier to read and slightly more intimidating. So for example, you can see tasks, you can see that hey, some of them are related to URL load the client, so you're getting information from the network. Someone, a navigation client, which kind of you know the navigation stack, you kind of guess what it is, but the level of intuitiveness is starkly different. So there are existing examples where we already do this in Chromium and we like take the care to reconstruct the high level events and the high level timeline for specific things. For example, this is an example of event latency. So specifically breaking down the timeline of steps and sequence of steps involved in presenting a frame. We're doing great on time. So the downside is that it's plumbing is very expensive and scaling this up is very difficult. When you have a big project, you have information, you need information from different you know corners of this project and plumbing is very expensive, both in terms of serialization costs, in terms of layering concerns, in terms of the amount of plumbing code that you need to maintain. And this you know difficult to scale and you know we haven't implemented this for too many exciting things. So let me talk about Perfetto a little bit. So Perfetto is the new generation tracing framework born from the ashes of Chromium tracing by a few great folks who have been working on Chromium tracing, got fed up with it, learned all of the mistakes that happened there and all of the things that we should shouldn't have done in the first place. And Voila Perfetto, which is nowadays widely used for Chromium and Android tracing. So it has fancy new UI, it has more efficient format, but the thing that brings a special place in my heart for it is the new SQL data model and query engine. So essentially everything that you can see in the UI is backed by a data model and UI is just running queries in the data model, against this data model and presenting it. And presenting it and you can very easily do it yourself. And you know we trace processor actually is compiled as a was module and running in your browser in a background thread. Voila web, we have gone, we've came very far. And this allows us to separate recording the trace and emitting the low level instrumentation and actually analyzing it and building high level data models. So this is probably the best example of Perfetto powers. I could hit it in a single slide. You can replicate this yourself if you go to Perfetto, if you go to open the Chrome trace, if you type colon into the search box, you will enter the SQL query mode and then you can copy and paste the query that I inserted there. Once again, you should have access to the slides and then it will pretty much give you the list of top 100 longest tasks that we ended up running there, which is already useful for analysis and can allow you to build more and more complex data models through different tables within SQL, which is kind of cool. So what are the next steps here? So I am right now trying to build in a navigation instrumentation, fancy navigation instrumentation as a proof of concept. The current prototype is kind of there, so you can see that we have a timeline. This is all pretty much based on the same low level information, but presents it in a more fancy version. And this then can be further integrated with the documentation. So this is just a not standalone box with a couple of words scribble on it, but we can also link to parts of Chromium documentation that outline what this stage is actually about, what are the concepts that you need to think about and make it generally more useful. One of the major complexities, why we haven't done this before, is that the complexity in a number of corner cases. When you talk about navigation, when you talk about typing the URL into the OmniBox, there are like a mind boggling complex number of cases from redirects to navigation, turning it to downloads to server returning to O4 and canceling the navigation that you kind of need to think about. And building this instrumentation without being able to test it is kind of a losing game. And the Sequel support actually allows us to feasibly write this testing coverage for these corner cases. I think I'm 15 out of 50 at this point, so some work to do. So yeah, I think that's all of the main content that I have. I have a bonus demo, which is kind of about DevTools, but I can also take questions. Eyeballing is, so the question is what's the best way of comparing these traces? I think eyeballing is probably a good place to start. So there are some early experiments of opening the traces times, say to some it and being able to link the timelines, but this is greatly depends on what kind of problem you're looking at. For example, if you are comparing the traces from tests, then the workload is more repeatable and you can actually go further in comparing it. For example, writing some Sequel queries and instructing some high-level metrics can get you very, very far and spotting if any high-level metrics changed or any derived things changed. If it's user's interactive, then probably eyeballing and going from there and seeing how much variance there is. Yes. Yes, some Sequel statement there. Yes, great question. The question is we have Sequel, but where is the database? The answer is it's all done locally. So this is Sequelite compiled into a WOS module with some helpers on top. So when you're opening a trace, it's running in your background thread in Sequelite instance. Everything is local. More questions? If not, I can actually go and show you my favorite House Party trick, which is an illustration of why it's actually quite important, I think, to think about data presentation. Sorry? More questions? No. Let me try to do this. Can you see what's going on? So let me open a trace in the performance in ground deftos that I have recorded earlier this morning. And this is something that you should be already familiar with, but the thing that some of you might not have realized, that there is nothing inherently magical or special about these deftal traces, apart from very good UI and a lot of UX thoughts that went into that. But fundamentally, they are just JSON-chrome traces, just with a bunch of categories. And you can actually open the very same information in Perfetto and actually look at it. And you already can see that the usefulness of this information is a bit different. We have to zoom and find our relevant parts, and we have been exposed with a low-level information, but no high-level insights. But then we have the network tracing. And the best way to illustrate that, further, is look at one of the network requests. Let me... Not this one. I want to find a network request from the deftools with the URL. And you can see that it... Some high-level stats, and you can see where it fits with other stuff. Psyllium shots also help. But then I can search by this URL, and I can also find the request ID and find all of the events, the low-level events that Chrome is tracing has actually meted. So all of the information about this network request is there. So if you can be bothered, you can actually go and correlate and go to all of these specific events and correlate them and reconstruct the same level, high-level takeaways. But it's going to be a little bit slower, a little bit less useful, and you won't actually be using it that much yourself, probably. So, yeah.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.56, "text": " Hello. Can you hear me all right? Welcome to the next installment in your regular scheduled", "tokens": [50364, 2425, 13, 1664, 291, 1568, 385, 439, 558, 30, 4027, 281, 264, 958, 39413, 294, 428, 3890, 15678, 50942], "temperature": 0.0, "avg_logprob": -0.20485618114471435, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.7220745086669922}, {"id": 1, "seek": 0, "start": 11.56, "end": 16.36, "text": " entertainment. I think that's the great benefit of being scheduled after such great talks", "tokens": [50942, 12393, 13, 286, 519, 300, 311, 264, 869, 5121, 295, 885, 15678, 934, 1270, 869, 6686, 51182], "temperature": 0.0, "avg_logprob": -0.20485618114471435, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.7220745086669922}, {"id": 2, "seek": 0, "start": 16.36, "end": 21.6, "text": " that everyone is in and with no break you can't leave. So you're kind of stuck with", "tokens": [51182, 300, 1518, 307, 294, 293, 365, 572, 1821, 291, 393, 380, 1856, 13, 407, 291, 434, 733, 295, 5541, 365, 51444], "temperature": 0.0, "avg_logprob": -0.20485618114471435, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.7220745086669922}, {"id": 3, "seek": 0, "start": 21.6, "end": 27.16, "text": " me for the next half an hour. So welcome. I hope you enjoy.", "tokens": [51444, 385, 337, 264, 958, 1922, 364, 1773, 13, 407, 2928, 13, 286, 1454, 291, 2103, 13, 51722], "temperature": 0.0, "avg_logprob": -0.20485618114471435, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.7220745086669922}, {"id": 4, "seek": 2716, "start": 27.16, "end": 33.4, "text": " So this is a very ambitious talk or at least as title suggests that it is. So let me actually", "tokens": [50364, 407, 341, 307, 257, 588, 20239, 751, 420, 412, 1935, 382, 4876, 13409, 300, 309, 307, 13, 407, 718, 385, 767, 50676], "temperature": 0.0, "avg_logprob": -0.17447504467434352, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.08482401818037033}, {"id": 5, "seek": 2716, "start": 33.4, "end": 40.24, "text": " start with talking what it really is about. And this talk is really about me and my experience.", "tokens": [50676, 722, 365, 1417, 437, 309, 534, 307, 466, 13, 400, 341, 751, 307, 534, 466, 385, 293, 452, 1752, 13, 51018], "temperature": 0.0, "avg_logprob": -0.17447504467434352, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.08482401818037033}, {"id": 6, "seek": 2716, "start": 40.24, "end": 46.68, "text": " And hope, frustrations, aspirations, experience and some illustrations. So I'm Alex. I have", "tokens": [51018, 400, 1454, 11, 7454, 12154, 11, 32458, 11, 1752, 293, 512, 34540, 13, 407, 286, 478, 5202, 13, 286, 362, 51340], "temperature": 0.0, "avg_logprob": -0.17447504467434352, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.08482401818037033}, {"id": 7, "seek": 2716, "start": 46.68, "end": 53.519999999999996, "text": " been doing web performance, mobile web performance at Google in Chromium for the last eight or", "tokens": [51340, 668, 884, 3670, 3389, 11, 6013, 3670, 3389, 412, 3329, 294, 1721, 298, 2197, 337, 264, 1036, 3180, 420, 51682], "temperature": 0.0, "avg_logprob": -0.17447504467434352, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.08482401818037033}, {"id": 8, "seek": 5352, "start": 53.52, "end": 59.080000000000005, "text": " so years. So this talk is going to be pretty much about that. This is not going to be a", "tokens": [50364, 370, 924, 13, 407, 341, 751, 307, 516, 281, 312, 1238, 709, 466, 300, 13, 639, 307, 406, 516, 281, 312, 257, 50642], "temperature": 0.0, "avg_logprob": -0.14193548182005522, "compression_ratio": 1.6651785714285714, "no_speech_prob": 0.07872015237808228}, {"id": 9, "seek": 5352, "start": 59.080000000000005, "end": 66.92, "text": " practical talk. So because we don't have time. This is the first reason. The second is that", "tokens": [50642, 8496, 751, 13, 407, 570, 321, 500, 380, 362, 565, 13, 639, 307, 264, 700, 1778, 13, 440, 1150, 307, 300, 51034], "temperature": 0.0, "avg_logprob": -0.14193548182005522, "compression_ratio": 1.6651785714285714, "no_speech_prob": 0.07872015237808228}, {"id": 10, "seek": 5352, "start": 66.92, "end": 73.84, "text": " there are way too many rough edges. So I wouldn't recommend this at this point to start to reproduce", "tokens": [51034, 456, 366, 636, 886, 867, 5903, 8819, 13, 407, 286, 2759, 380, 2748, 341, 412, 341, 935, 281, 722, 281, 29501, 51380], "temperature": 0.0, "avg_logprob": -0.14193548182005522, "compression_ratio": 1.6651785714285714, "no_speech_prob": 0.07872015237808228}, {"id": 11, "seek": 5352, "start": 73.84, "end": 78.92, "text": " this. But hopefully this will be a source of inspiration. And those of you who are desperate", "tokens": [51380, 341, 13, 583, 4696, 341, 486, 312, 257, 4009, 295, 10249, 13, 400, 729, 295, 291, 567, 366, 17601, 51634], "temperature": 0.0, "avg_logprob": -0.14193548182005522, "compression_ratio": 1.6651785714285714, "no_speech_prob": 0.07872015237808228}, {"id": 12, "seek": 7892, "start": 78.92, "end": 85.72, "text": " enough and frustrated enough and have seen the problems outlined in this talk too many", "tokens": [50364, 1547, 293, 15751, 1547, 293, 362, 1612, 264, 2740, 27412, 294, 341, 751, 886, 867, 50704], "temperature": 0.0, "avg_logprob": -0.20736147717731754, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.1468116044998169}, {"id": 13, "seek": 7892, "start": 85.72, "end": 92.4, "text": " times, hopefully they will be brave enough to venture and try. There is a practical guide", "tokens": [50704, 1413, 11, 4696, 436, 486, 312, 12653, 1547, 281, 18474, 293, 853, 13, 821, 307, 257, 8496, 5934, 51038], "temperature": 0.0, "avg_logprob": -0.20736147717731754, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.1468116044998169}, {"id": 14, "seek": 7892, "start": 92.4, "end": 100.76, "text": " that I would recommend in the recent web performance calendar by the one and only great Annie Sullivan.", "tokens": [51038, 300, 286, 576, 2748, 294, 264, 5162, 3670, 3389, 12183, 538, 264, 472, 293, 787, 869, 26781, 37226, 13, 51456], "temperature": 0.0, "avg_logprob": -0.20736147717731754, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.1468116044998169}, {"id": 15, "seek": 7892, "start": 100.76, "end": 105.88, "text": " I would recommend you go and check it out, preferably after this talk. But you know, I", "tokens": [51456, 286, 576, 2748, 291, 352, 293, 1520, 309, 484, 11, 45916, 934, 341, 751, 13, 583, 291, 458, 11, 286, 51712], "temperature": 0.0, "avg_logprob": -0.20736147717731754, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.1468116044998169}, {"id": 16, "seek": 10588, "start": 106.19999999999999, "end": 112.11999999999999, "text": " can't prohibit you from doing so. So this talk really is about problem solving and working", "tokens": [50380, 393, 380, 16015, 270, 291, 490, 884, 370, 13, 407, 341, 751, 534, 307, 466, 1154, 12606, 293, 1364, 50676], "temperature": 0.0, "avg_logprob": -0.2023157890026386, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.1651122123003006}, {"id": 17, "seek": 10588, "start": 112.11999999999999, "end": 116.92, "text": " with complex systems and trying to make sense of them. So the examples will be from Chromium.", "tokens": [50676, 365, 3997, 3652, 293, 1382, 281, 652, 2020, 295, 552, 13, 407, 264, 5110, 486, 312, 490, 1721, 298, 2197, 13, 50916], "temperature": 0.0, "avg_logprob": -0.2023157890026386, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.1651122123003006}, {"id": 18, "seek": 10588, "start": 116.92, "end": 121.84, "text": " I will talk about Perfetto. But I think these examples hopefully will be an inspiration for", "tokens": [50916, 286, 486, 751, 466, 3026, 69, 23778, 13, 583, 286, 519, 613, 5110, 4696, 486, 312, 364, 10249, 337, 51162], "temperature": 0.0, "avg_logprob": -0.2023157890026386, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.1651122123003006}, {"id": 19, "seek": 10588, "start": 121.84, "end": 126.32, "text": " a great variety of projects, both when building your sites and building other and working", "tokens": [51162, 257, 869, 5673, 295, 4455, 11, 1293, 562, 2390, 428, 7533, 293, 2390, 661, 293, 1364, 51386], "temperature": 0.0, "avg_logprob": -0.2023157890026386, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.1651122123003006}, {"id": 20, "seek": 10588, "start": 126.32, "end": 135.6, "text": " with other complex systems as well. So let's talk about performance and improvement performance.", "tokens": [51386, 365, 661, 3997, 3652, 382, 731, 13, 407, 718, 311, 751, 466, 3389, 293, 10444, 3389, 13, 51850], "temperature": 0.0, "avg_logprob": -0.2023157890026386, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.1651122123003006}, {"id": 21, "seek": 13560, "start": 135.96, "end": 142.2, "text": " If you want to improve performance, as I imagine you are not totally adverse to, given that", "tokens": [50382, 759, 291, 528, 281, 3470, 3389, 11, 382, 286, 3811, 291, 366, 406, 3879, 27590, 281, 11, 2212, 300, 50694], "temperature": 0.0, "avg_logprob": -0.17920720135724102, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.02044069953262806}, {"id": 22, "seek": 13560, "start": 142.2, "end": 147.79999999999998, "text": " you are in this room, then I want to remind certain trivial things that you probably already", "tokens": [50694, 291, 366, 294, 341, 1808, 11, 550, 286, 528, 281, 4160, 1629, 26703, 721, 300, 291, 1391, 1217, 50974], "temperature": 0.0, "avg_logprob": -0.17920720135724102, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.02044069953262806}, {"id": 23, "seek": 13560, "start": 147.79999999999998, "end": 153.07999999999998, "text": " know. The first is that performance problems are nasty. They are unpolite and they don't", "tokens": [50974, 458, 13, 440, 700, 307, 300, 3389, 2740, 366, 17923, 13, 814, 366, 517, 12892, 642, 293, 436, 500, 380, 51238], "temperature": 0.0, "avg_logprob": -0.17920720135724102, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.02044069953262806}, {"id": 24, "seek": 13560, "start": 153.07999999999998, "end": 159.28, "text": " have the common courtesy of locating themselves in a nice isolated area of code in your project", "tokens": [51238, 362, 264, 2689, 41704, 295, 1628, 990, 2969, 294, 257, 1481, 14621, 1859, 295, 3089, 294, 428, 1716, 51548], "temperature": 0.0, "avg_logprob": -0.17920720135724102, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.02044069953262806}, {"id": 25, "seek": 13560, "start": 159.28, "end": 165.04, "text": " that you can master and just work on and don't bother with the rest of the stuff. So because", "tokens": [51548, 300, 291, 393, 4505, 293, 445, 589, 322, 293, 500, 380, 8677, 365, 264, 1472, 295, 264, 1507, 13, 407, 570, 51836], "temperature": 0.0, "avg_logprob": -0.17920720135724102, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.02044069953262806}, {"id": 26, "seek": 16504, "start": 165.07999999999998, "end": 170.72, "text": " of that, kind of knowing what to improve and where to improve and how to improve takes a", "tokens": [50366, 295, 300, 11, 733, 295, 5276, 437, 281, 3470, 293, 689, 281, 3470, 293, 577, 281, 3470, 2516, 257, 50648], "temperature": 0.0, "avg_logprob": -0.21920137405395507, "compression_ratio": 1.7281553398058251, "no_speech_prob": 0.006536825560033321}, {"id": 27, "seek": 16504, "start": 170.72, "end": 176.51999999999998, "text": " substantial effort of the performance work. And the fact that more and more web is mind", "tokens": [50648, 16726, 4630, 295, 264, 3389, 589, 13, 400, 264, 1186, 300, 544, 293, 544, 3670, 307, 1575, 50938], "temperature": 0.0, "avg_logprob": -0.21920137405395507, "compression_ratio": 1.7281553398058251, "no_speech_prob": 0.006536825560033321}, {"id": 28, "seek": 16504, "start": 176.51999999999998, "end": 182.88, "text": " boggling complex with new APIs, both performance and non-performance, browsers getting more", "tokens": [50938, 26132, 70, 1688, 3997, 365, 777, 21445, 11, 1293, 3389, 293, 2107, 12, 50242, 11, 36069, 1242, 544, 51256], "temperature": 0.0, "avg_logprob": -0.21920137405395507, "compression_ratio": 1.7281553398058251, "no_speech_prob": 0.006536825560033321}, {"id": 29, "seek": 16504, "start": 182.88, "end": 189.12, "text": " complex and bigger every day, various sites, drawing in diversity and in complexity and", "tokens": [51256, 3997, 293, 3801, 633, 786, 11, 3683, 7533, 11, 6316, 294, 8811, 293, 294, 14024, 293, 51568], "temperature": 0.0, "avg_logprob": -0.21920137405395507, "compression_ratio": 1.7281553398058251, "no_speech_prob": 0.006536825560033321}, {"id": 30, "seek": 18912, "start": 189.16, "end": 195.4, "text": " libraries and so on. So these all leads to all of us working on performance, spending a lot of", "tokens": [50366, 15148, 293, 370, 322, 13, 407, 613, 439, 6689, 281, 439, 295, 505, 1364, 322, 3389, 11, 6434, 257, 688, 295, 50678], "temperature": 0.0, "avg_logprob": -0.16080119692046066, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.036407023668289185}, {"id": 31, "seek": 18912, "start": 195.4, "end": 200.4, "text": " time on a regular basis trying to understand what the hell is going on here. And what are", "tokens": [50678, 565, 322, 257, 3890, 5143, 1382, 281, 1223, 437, 264, 4921, 307, 516, 322, 510, 13, 400, 437, 366, 50928], "temperature": 0.0, "avg_logprob": -0.16080119692046066, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.036407023668289185}, {"id": 32, "seek": 18912, "start": 200.4, "end": 205.08, "text": " the approaches? So the first approach that I have to mention is you can go and read the code.", "tokens": [50928, 264, 11587, 30, 407, 264, 700, 3109, 300, 286, 362, 281, 2152, 307, 291, 393, 352, 293, 1401, 264, 3089, 13, 51162], "temperature": 0.0, "avg_logprob": -0.16080119692046066, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.036407023668289185}, {"id": 33, "seek": 18912, "start": 206.68, "end": 210.44, "text": " You are a very brave person and I wish you the very best of luck if you decide to do it, but", "tokens": [51242, 509, 366, 257, 588, 12653, 954, 293, 286, 3172, 291, 264, 588, 1151, 295, 3668, 498, 291, 4536, 281, 360, 309, 11, 457, 51430], "temperature": 0.0, "avg_logprob": -0.16080119692046066, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.036407023668289185}, {"id": 34, "seek": 18912, "start": 210.44, "end": 214.84, "text": " it's not very practical. So modern projects are layers of up and layers of obstructions and then", "tokens": [51430, 309, 311, 406, 588, 8496, 13, 407, 4363, 4455, 366, 7914, 295, 493, 293, 7914, 295, 45579, 626, 293, 550, 51650], "temperature": 0.0, "avg_logprob": -0.16080119692046066, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.036407023668289185}, {"id": 35, "seek": 21484, "start": 214.88, "end": 219.68, "text": " you have a listener and then you have 30 possible callbacks or entry points and then good luck.", "tokens": [50366, 291, 362, 257, 31569, 293, 550, 291, 362, 2217, 1944, 818, 17758, 420, 8729, 2793, 293, 550, 665, 3668, 13, 50606], "temperature": 0.0, "avg_logprob": -0.2590881756373814, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.00893242284655571}, {"id": 36, "seek": 21484, "start": 220.48, "end": 225.84, "text": " Usually, I give up at this point when I see like, hey, no, this is probably one of these 30 things.", "tokens": [50646, 11419, 11, 286, 976, 493, 412, 341, 935, 562, 286, 536, 411, 11, 4177, 11, 572, 11, 341, 307, 1391, 472, 295, 613, 2217, 721, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2590881756373814, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.00893242284655571}, {"id": 37, "seek": 21484, "start": 226.72, "end": 233.04, "text": " The second one is printf and it's possible variations. So it's console, it's log, it's other,", "tokens": [50958, 440, 1150, 472, 307, 4482, 69, 293, 309, 311, 1944, 17840, 13, 407, 309, 311, 11076, 11, 309, 311, 3565, 11, 309, 311, 661, 11, 51274], "temperature": 0.0, "avg_logprob": -0.2590881756373814, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.00893242284655571}, {"id": 38, "seek": 21484, "start": 233.68, "end": 242.8, "text": " just log it statements. And the second is the buggers. So GDB, LLDB, RR, Chrome DevTools, some of", "tokens": [51306, 445, 3565, 309, 12363, 13, 400, 264, 1150, 307, 264, 7426, 9458, 13, 407, 460, 27735, 11, 441, 23704, 33, 11, 497, 49, 11, 15327, 9096, 51, 29298, 11, 512, 295, 51762], "temperature": 0.0, "avg_logprob": -0.2590881756373814, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.00893242284655571}, {"id": 39, "seek": 24280, "start": 242.8, "end": 249.04000000000002, "text": " them are better than the others. But all of them, these approaches effectively don't scale to complex", "tokens": [50364, 552, 366, 1101, 813, 264, 2357, 13, 583, 439, 295, 552, 11, 613, 11587, 8659, 500, 380, 4373, 281, 3997, 50676], "temperature": 0.0, "avg_logprob": -0.19275969046133537, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.017537111416459084}, {"id": 40, "seek": 24280, "start": 249.04000000000002, "end": 256.04, "text": " systems, especially if you talk about indeterminism when you test sometimes reproduces and the error", "tokens": [50676, 3652, 11, 2318, 498, 291, 751, 466, 1016, 35344, 259, 1434, 562, 291, 1500, 2171, 11408, 887, 293, 264, 6713, 51026], "temperature": 0.0, "avg_logprob": -0.19275969046133537, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.017537111416459084}, {"id": 41, "seek": 24280, "start": 256.04, "end": 261.32, "text": " sometimes reproduces sometimes doesn't then you are in a bit of fun. So when you have multiple", "tokens": [51026, 2171, 11408, 887, 2171, 1177, 380, 550, 291, 366, 294, 257, 857, 295, 1019, 13, 407, 562, 291, 362, 3866, 51290], "temperature": 0.0, "avg_logprob": -0.19275969046133537, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.017537111416459084}, {"id": 42, "seek": 24280, "start": 261.32, "end": 266.88, "text": " processes and multiple architectures, multiple architecture components, then all of these,", "tokens": [51290, 7555, 293, 3866, 6331, 1303, 11, 3866, 9482, 6677, 11, 550, 439, 295, 613, 11, 51568], "temperature": 0.0, "avg_logprob": -0.19275969046133537, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.017537111416459084}, {"id": 43, "seek": 24280, "start": 266.88, "end": 271.48, "text": " you know, these tools don't work particularly well. So they focus on low level details. Hey,", "tokens": [51568, 291, 458, 11, 613, 3873, 500, 380, 589, 4098, 731, 13, 407, 436, 1879, 322, 2295, 1496, 4365, 13, 1911, 11, 51798], "temperature": 0.0, "avg_logprob": -0.19275969046133537, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.017537111416459084}, {"id": 44, "seek": 27148, "start": 271.52000000000004, "end": 277.40000000000003, "text": " what is this variable? And most often you want to know, hey, what this component is doing? And am I", "tokens": [50366, 437, 307, 341, 7006, 30, 400, 881, 2049, 291, 528, 281, 458, 11, 4177, 11, 437, 341, 6542, 307, 884, 30, 400, 669, 286, 50660], "temperature": 0.0, "avg_logprob": -0.15070646636340082, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.005555812735110521}, {"id": 45, "seek": 27148, "start": 277.44, "end": 283.8, "text": " doing a good job? So enter tracing. How many of you are familiar with tracing in some form or the other?", "tokens": [50662, 884, 257, 665, 1691, 30, 407, 3242, 25262, 13, 1012, 867, 295, 291, 366, 4963, 365, 25262, 294, 512, 1254, 420, 264, 661, 30, 50980], "temperature": 0.0, "avg_logprob": -0.15070646636340082, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.005555812735110521}, {"id": 46, "seek": 27148, "start": 286.12, "end": 291.32, "text": " Some of them. So pretty much tracing is structured logging and visualization. I will go into this a", "tokens": [51096, 2188, 295, 552, 13, 407, 1238, 709, 25262, 307, 18519, 27991, 293, 25801, 13, 286, 486, 352, 666, 341, 257, 51356], "temperature": 0.0, "avg_logprob": -0.15070646636340082, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.005555812735110521}, {"id": 47, "seek": 27148, "start": 291.32, "end": 296.20000000000005, "text": " little bit more further down the line. But as far as chromium is concerned, from the practical", "tokens": [51356, 707, 857, 544, 3052, 760, 264, 1622, 13, 583, 382, 1400, 382, 16209, 2197, 307, 5922, 11, 490, 264, 8496, 51600], "temperature": 0.0, "avg_logprob": -0.15070646636340082, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.005555812735110521}, {"id": 48, "seek": 29620, "start": 296.44, "end": 302.52, "text": " perspective, it means turning these annotations. So here we have a request resource from Java", "tokens": [50376, 4585, 11, 309, 1355, 6246, 613, 25339, 763, 13, 407, 510, 321, 362, 257, 5308, 7684, 490, 10745, 50680], "temperature": 0.0, "avg_logprob": -0.2054667680159859, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.02165103890001774}, {"id": 49, "seek": 29620, "start": 302.52, "end": 309.68, "text": " function that is being annotated with tracement macro. So we in C++, we emit some information when we", "tokens": [50680, 2445, 300, 307, 885, 25339, 770, 365, 504, 326, 1712, 18887, 13, 407, 321, 294, 383, 25472, 11, 321, 32084, 512, 1589, 562, 321, 51038], "temperature": 0.0, "avg_logprob": -0.2054667680159859, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.02165103890001774}, {"id": 50, "seek": 29620, "start": 309.68, "end": 314.91999999999996, "text": " enter this function. And we emit some information that we exit this function when tracing is enabled.", "tokens": [51038, 3242, 341, 2445, 13, 400, 321, 32084, 512, 1589, 300, 321, 11043, 341, 2445, 562, 25262, 307, 15172, 13, 51300], "temperature": 0.0, "avg_logprob": -0.2054667680159859, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.02165103890001774}, {"id": 51, "seek": 29620, "start": 314.91999999999996, "end": 323.0, "text": " And this will allow us to look at this nice timeline. Pretty much the x axis is time advancing in", "tokens": [51300, 400, 341, 486, 2089, 505, 281, 574, 412, 341, 1481, 12933, 13, 10693, 709, 264, 2031, 10298, 307, 565, 27267, 294, 51704], "temperature": 0.0, "avg_logprob": -0.2054667680159859, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.02165103890001774}, {"id": 52, "seek": 32300, "start": 323.04, "end": 327.28, "text": " time. And here you can see that we have entered this function here, you have exited this function.", "tokens": [50366, 565, 13, 400, 510, 291, 393, 536, 300, 321, 362, 9065, 341, 2445, 510, 11, 291, 362, 454, 1226, 341, 2445, 13, 50578], "temperature": 0.0, "avg_logprob": -0.2941786061162534, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.00938490778207779}, {"id": 53, "seek": 32300, "start": 327.28, "end": 332.68, "text": " And you can see which other functions were called inside of it, how long it took. And you can see", "tokens": [50578, 400, 291, 393, 536, 597, 661, 6828, 645, 1219, 1854, 295, 309, 11, 577, 938, 309, 1890, 13, 400, 291, 393, 536, 50848], "temperature": 0.0, "avg_logprob": -0.2941786061162534, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.00938490778207779}, {"id": 54, "seek": 32300, "start": 333.24, "end": 339.12, "text": " zooming out what else the system has been doing across different threads across different processes,", "tokens": [50876, 48226, 484, 437, 1646, 264, 1185, 575, 668, 884, 2108, 819, 19314, 2108, 819, 7555, 11, 51170], "temperature": 0.0, "avg_logprob": -0.2941786061162534, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.00938490778207779}, {"id": 55, "seek": 32300, "start": 339.12, "end": 346.12, "text": " which is I think a good starting point and the basic infrastructure talk about. So if you wanted to", "tokens": [51170, 597, 307, 286, 519, 257, 665, 2891, 935, 293, 264, 3875, 6896, 751, 466, 13, 407, 498, 291, 1415, 281, 51520], "temperature": 0.0, "avg_logprob": -0.2941786061162534, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.00938490778207779}, {"id": 56, "seek": 32300, "start": 346.12, "end": 351.16, "text": " use it to trade yourself, you can actually go to your IPF at a depth. And the examples in this", "tokens": [51520, 764, 309, 281, 4923, 1803, 11, 291, 393, 767, 352, 281, 428, 8671, 37, 412, 257, 7161, 13, 400, 264, 5110, 294, 341, 51772], "temperature": 0.0, "avg_logprob": -0.2941786061162534, "compression_ratio": 1.7956204379562044, "no_speech_prob": 0.00938490778207779}, {"id": 57, "seek": 35116, "start": 351.24, "end": 357.28000000000003, "text": " talk are pretty much all from open Chrome example. So if you have a laptop, then you can go and follow", "tokens": [50368, 751, 366, 1238, 709, 439, 490, 1269, 15327, 1365, 13, 407, 498, 291, 362, 257, 10732, 11, 550, 291, 393, 352, 293, 1524, 50670], "temperature": 0.0, "avg_logprob": -0.20325876486421834, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.012496717274188995}, {"id": 58, "seek": 35116, "start": 357.28000000000003, "end": 364.44000000000005, "text": " it. Then the links to the slides should be on the FOSDEM site for this talk. But we'll back", "tokens": [50670, 309, 13, 1396, 264, 6123, 281, 264, 9788, 820, 312, 322, 264, 479, 4367, 35, 6683, 3621, 337, 341, 751, 13, 583, 321, 603, 646, 51028], "temperature": 0.0, "avg_logprob": -0.20325876486421834, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.012496717274188995}, {"id": 59, "seek": 35116, "start": 364.44000000000005, "end": 370.64000000000004, "text": " talking about how to make this useful. So you have this wonderful instrumentation. And this", "tokens": [51028, 1417, 466, 577, 281, 652, 341, 4420, 13, 407, 291, 362, 341, 3715, 7198, 399, 13, 400, 341, 51338], "temperature": 0.0, "avg_logprob": -0.20325876486421834, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.012496717274188995}, {"id": 60, "seek": 35116, "start": 370.64000000000004, "end": 376.68, "text": " already is, you can use it as a fancy fprintf with search functionality. You can just record a lot", "tokens": [51338, 1217, 307, 11, 291, 393, 764, 309, 382, 257, 10247, 283, 14030, 69, 365, 3164, 14980, 13, 509, 393, 445, 2136, 257, 688, 51640], "temperature": 0.0, "avg_logprob": -0.20325876486421834, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.012496717274188995}, {"id": 61, "seek": 37668, "start": 376.72, "end": 382.28000000000003, "text": " of information and then look at it. But this is basically instrumenting the code you're already", "tokens": [50366, 295, 1589, 293, 550, 574, 412, 309, 13, 583, 341, 307, 1936, 7198, 278, 264, 3089, 291, 434, 1217, 50644], "temperature": 0.0, "avg_logprob": -0.16468808085648054, "compression_ratio": 1.5443548387096775, "no_speech_prob": 0.018774371594190598}, {"id": 62, "seek": 37668, "start": 382.28000000000003, "end": 387.48, "text": " working on as a fancy fprintf. It's powerful and flexible, but not necessarily most convenient.", "tokens": [50644, 1364, 322, 382, 257, 10247, 283, 14030, 69, 13, 467, 311, 4005, 293, 11358, 11, 457, 406, 4725, 881, 10851, 13, 50904], "temperature": 0.0, "avg_logprob": -0.16468808085648054, "compression_ratio": 1.5443548387096775, "no_speech_prob": 0.018774371594190598}, {"id": 63, "seek": 37668, "start": 387.48, "end": 396.84000000000003, "text": " And it doesn't win either compared to fprintf or debuggers out of the box. For fprintf, the basic", "tokens": [50904, 400, 309, 1177, 380, 1942, 2139, 5347, 281, 283, 14030, 69, 420, 3001, 3562, 433, 484, 295, 264, 2424, 13, 1171, 283, 14030, 69, 11, 264, 3875, 51372], "temperature": 0.0, "avg_logprob": -0.16468808085648054, "compression_ratio": 1.5443548387096775, "no_speech_prob": 0.018774371594190598}, {"id": 64, "seek": 37668, "start": 396.84000000000003, "end": 400.84000000000003, "text": " debug loop is still faster. You had a single statement, you don't have to bother with opening", "tokens": [51372, 24083, 6367, 307, 920, 4663, 13, 509, 632, 257, 2167, 5629, 11, 291, 500, 380, 362, 281, 8677, 365, 5193, 51572], "temperature": 0.0, "avg_logprob": -0.16468808085648054, "compression_ratio": 1.5443548387096775, "no_speech_prob": 0.018774371594190598}, {"id": 65, "seek": 40084, "start": 400.84, "end": 408.28, "text": " anything anywhere. You just see the console output and you're done. So it gets less pleasant when", "tokens": [50364, 1340, 4992, 13, 509, 445, 536, 264, 11076, 5598, 293, 291, 434, 1096, 13, 407, 309, 2170, 1570, 16232, 562, 50736], "temperature": 0.0, "avg_logprob": -0.16163995530870226, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.019316617399454117}, {"id": 66, "seek": 40084, "start": 408.28, "end": 413.4, "text": " you have to do it multiple times. And with debuggers, every all information is present. You can take", "tokens": [50736, 291, 362, 281, 360, 309, 3866, 1413, 13, 400, 365, 3001, 3562, 433, 11, 633, 439, 1589, 307, 1974, 13, 509, 393, 747, 50992], "temperature": 0.0, "avg_logprob": -0.16163995530870226, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.019316617399454117}, {"id": 67, "seek": 40084, "start": 413.4, "end": 418.91999999999996, "text": " you a bit of time to find it, but you don't have to bother with adding more annotations,", "tokens": [50992, 291, 257, 857, 295, 565, 281, 915, 309, 11, 457, 291, 500, 380, 362, 281, 8677, 365, 5127, 544, 25339, 763, 11, 51268], "temperature": 0.0, "avg_logprob": -0.16163995530870226, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.019316617399454117}, {"id": 68, "seek": 40084, "start": 418.91999999999996, "end": 425.08, "text": " recompiling and wasting time there. So like, and it's unrealistic to have all of the functions", "tokens": [51268, 48000, 4883, 293, 20457, 565, 456, 13, 407, 411, 11, 293, 309, 311, 42867, 281, 362, 439, 295, 264, 6828, 51576], "temperature": 0.0, "avg_logprob": -0.16163995530870226, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.019316617399454117}, {"id": 69, "seek": 40084, "start": 425.08, "end": 430.35999999999996, "text": " instrumented and captured in this race, because it's too much information both to record, which adds", "tokens": [51576, 7198, 292, 293, 11828, 294, 341, 4569, 11, 570, 309, 311, 886, 709, 1589, 1293, 281, 2136, 11, 597, 10860, 51840], "temperature": 0.0, "avg_logprob": -0.16163995530870226, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.019316617399454117}, {"id": 70, "seek": 43036, "start": 430.44, "end": 436.28000000000003, "text": " all of overhead and slow downs, but also it's a lot of information to go through and looking at it is", "tokens": [50368, 439, 295, 19922, 293, 2964, 21554, 11, 457, 611, 309, 311, 257, 688, 295, 1589, 281, 352, 807, 293, 1237, 412, 309, 307, 50660], "temperature": 0.0, "avg_logprob": -0.16007701740708463, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.007092090789228678}, {"id": 71, "seek": 43036, "start": 436.28000000000003, "end": 442.76, "text": " not pleasant and not fast. So I will talk about finding opportunities for scaling this instrumentation", "tokens": [50660, 406, 16232, 293, 406, 2370, 13, 407, 286, 486, 751, 466, 5006, 4786, 337, 21589, 341, 7198, 399, 50984], "temperature": 0.0, "avg_logprob": -0.16007701740708463, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.007092090789228678}, {"id": 72, "seek": 43036, "start": 442.76, "end": 448.12, "text": " and finding the opportunities where a few instrumentation points can give us a lot of information", "tokens": [50984, 293, 5006, 264, 4786, 689, 257, 1326, 7198, 399, 2793, 393, 976, 505, 257, 688, 295, 1589, 51252], "temperature": 0.0, "avg_logprob": -0.16007701740708463, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.007092090789228678}, {"id": 73, "seek": 43036, "start": 448.12, "end": 454.6, "text": " and substantially advance our ability to reason about what the code is doing. And enter Chrome task", "tokens": [51252, 293, 30797, 7295, 527, 3485, 281, 1778, 466, 437, 264, 3089, 307, 884, 13, 400, 3242, 15327, 5633, 51576], "temperature": 0.0, "avg_logprob": -0.16007701740708463, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.007092090789228678}, {"id": 74, "seek": 45460, "start": 454.6, "end": 462.6, "text": " schedule. Chromium is implemented based on an event loop model. So we have a bunch of name threads.", "tokens": [50364, 7567, 13, 1721, 298, 2197, 307, 12270, 2361, 322, 364, 2280, 6367, 2316, 13, 407, 321, 362, 257, 3840, 295, 1315, 19314, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20397776435403264, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.012783962301909924}, {"id": 75, "seek": 45460, "start": 462.6, "end": 467.08000000000004, "text": " We have browser process with a browser main thread where which is responsible for coordinating", "tokens": [50764, 492, 362, 11185, 1399, 365, 257, 11185, 2135, 7207, 689, 597, 307, 6250, 337, 37824, 50988], "temperature": 0.0, "avg_logprob": -0.20397776435403264, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.012783962301909924}, {"id": 76, "seek": 45460, "start": 467.08000000000004, "end": 471.08000000000004, "text": " everything. We have the render process with the main thread, which is responsible for running", "tokens": [50988, 1203, 13, 492, 362, 264, 15529, 1399, 365, 264, 2135, 7207, 11, 597, 307, 6250, 337, 2614, 51188], "temperature": 0.0, "avg_logprob": -0.20397776435403264, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.012783962301909924}, {"id": 77, "seek": 45460, "start": 471.08000000000004, "end": 479.0, "text": " JavaScript, bling DOM and whatever. Or we have worker pulls, sorry, dedicated workers, which", "tokens": [51188, 15778, 11, 888, 278, 35727, 293, 2035, 13, 1610, 321, 362, 11346, 16982, 11, 2597, 11, 8374, 5600, 11, 597, 51584], "temperature": 0.0, "avg_logprob": -0.20397776435403264, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.012783962301909924}, {"id": 78, "seek": 47900, "start": 479.0, "end": 485.32, "text": " sites can create using new worker API. There is a thread pool for miscellaneous background work.", "tokens": [50364, 7533, 393, 1884, 1228, 777, 11346, 9362, 13, 821, 307, 257, 7207, 7005, 337, 3346, 4164, 15447, 3678, 589, 13, 50680], "temperature": 0.0, "avg_logprob": -0.17711726256779262, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.019365373998880386}, {"id": 79, "seek": 47900, "start": 485.32, "end": 491.56, "text": " And these is pretty much all there is and various place in the code. In the code base,", "tokens": [50680, 400, 613, 307, 1238, 709, 439, 456, 307, 293, 3683, 1081, 294, 264, 3089, 13, 682, 264, 3089, 3096, 11, 50992], "temperature": 0.0, "avg_logprob": -0.17711726256779262, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.019365373998880386}, {"id": 80, "seek": 47900, "start": 491.56, "end": 495.72, "text": " I think we have, you know, a few thousand places. So maybe 10,000 nowadays,", "tokens": [50992, 286, 519, 321, 362, 11, 291, 458, 11, 257, 1326, 4714, 3190, 13, 407, 1310, 1266, 11, 1360, 13434, 11, 51200], "temperature": 0.0, "avg_logprob": -0.17711726256779262, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.019365373998880386}, {"id": 81, "seek": 47900, "start": 495.72, "end": 500.68, "text": " which basically post tasks. They get a task runner from somewhere and they post a task.", "tokens": [51200, 597, 1936, 2183, 9608, 13, 814, 483, 257, 5633, 24376, 490, 4079, 293, 436, 2183, 257, 5633, 13, 51448], "temperature": 0.0, "avg_logprob": -0.17711726256779262, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.019365373998880386}, {"id": 82, "seek": 47900, "start": 500.68, "end": 506.6, "text": " No, the from here macro will talk about this in a second. But otherwise it's just a fancy lambda", "tokens": [51448, 883, 11, 264, 490, 510, 18887, 486, 751, 466, 341, 294, 257, 1150, 13, 583, 5911, 309, 311, 445, 257, 10247, 13607, 51744], "temperature": 0.0, "avg_logprob": -0.17711726256779262, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.019365373998880386}, {"id": 83, "seek": 50660, "start": 506.6, "end": 515.4, "text": " with some of safety thrown in. And here it is. So you post this task somewhere, you know, some", "tokens": [50364, 365, 512, 295, 4514, 11732, 294, 13, 400, 510, 309, 307, 13, 407, 291, 2183, 341, 5633, 4079, 11, 291, 458, 11, 512, 50804], "temperature": 0.0, "avg_logprob": -0.13604578679921675, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.005160569678992033}, {"id": 84, "seek": 50660, "start": 515.4, "end": 521.96, "text": " thread or thread pool picks this up and it will run this task. Voila. And this is a great point", "tokens": [50804, 7207, 420, 7207, 7005, 16137, 341, 493, 293, 309, 486, 1190, 341, 5633, 13, 7518, 7371, 13, 400, 341, 307, 257, 869, 935, 51132], "temperature": 0.0, "avg_logprob": -0.13604578679921675, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.005160569678992033}, {"id": 85, "seek": 50660, "start": 521.96, "end": 528.12, "text": " for tracing instrumentation. And this is a great point to start looking at. So what it gives us,", "tokens": [51132, 337, 25262, 7198, 399, 13, 400, 341, 307, 257, 869, 935, 281, 722, 1237, 412, 13, 407, 437, 309, 2709, 505, 11, 51440], "temperature": 0.0, "avg_logprob": -0.13604578679921675, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.005160569678992033}, {"id": 86, "seek": 50660, "start": 528.76, "end": 533.8000000000001, "text": " it gives us that we will have pretty much all of the places running Chromium and code.", "tokens": [51472, 309, 2709, 505, 300, 321, 486, 362, 1238, 709, 439, 295, 264, 3190, 2614, 1721, 298, 2197, 293, 3089, 13, 51724], "temperature": 0.0, "avg_logprob": -0.13604578679921675, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.005160569678992033}, {"id": 87, "seek": 53380, "start": 534.4399999999999, "end": 539.16, "text": " We will know about them and we will have some basic information. Here specifically look at", "tokens": [50396, 492, 486, 458, 466, 552, 293, 321, 486, 362, 512, 3875, 1589, 13, 1692, 4682, 574, 412, 50632], "temperature": 0.0, "avg_logprob": -0.14525873093377975, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007346215192228556}, {"id": 88, "seek": 53380, "start": 539.16, "end": 545.0799999999999, "text": " posted from information. This is the result of from here macro expansion, which using some of the", "tokens": [50632, 9437, 490, 1589, 13, 639, 307, 264, 1874, 295, 490, 510, 18887, 11260, 11, 597, 1228, 512, 295, 264, 50928], "temperature": 0.0, "avg_logprob": -0.14525873093377975, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007346215192228556}, {"id": 89, "seek": 53380, "start": 545.0799999999999, "end": 552.52, "text": " C macro tricks, give automatically without any further support, gives us file name on the function.", "tokens": [50928, 383, 18887, 11733, 11, 976, 6772, 1553, 604, 3052, 1406, 11, 2709, 505, 3991, 1315, 322, 264, 2445, 13, 51300], "temperature": 0.0, "avg_logprob": -0.14525873093377975, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007346215192228556}, {"id": 90, "seek": 53380, "start": 552.52, "end": 557.64, "text": " So at least for every function, you have a basic idea of where this information, where", "tokens": [51300, 407, 412, 1935, 337, 633, 2445, 11, 291, 362, 257, 3875, 1558, 295, 689, 341, 1589, 11, 689, 51556], "temperature": 0.0, "avg_logprob": -0.14525873093377975, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007346215192228556}, {"id": 91, "seek": 53380, "start": 557.64, "end": 561.9599999999999, "text": " this task has been posted from. And you can go to that part of the code base and start", "tokens": [51556, 341, 5633, 575, 668, 9437, 490, 13, 400, 291, 393, 352, 281, 300, 644, 295, 264, 3089, 3096, 293, 722, 51772], "temperature": 0.0, "avg_logprob": -0.14525873093377975, "compression_ratio": 1.7701149425287357, "no_speech_prob": 0.007346215192228556}, {"id": 92, "seek": 56196, "start": 561.96, "end": 565.8000000000001, "text": " understanding what the hell is going on and why this task might be running.", "tokens": [50364, 3701, 437, 264, 4921, 307, 516, 322, 293, 983, 341, 5633, 1062, 312, 2614, 13, 50556], "temperature": 0.0, "avg_logprob": -0.09874511900402251, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.006067309062927961}, {"id": 93, "seek": 56196, "start": 567.72, "end": 575.24, "text": " So then we can zoom out and we also have instrumentation for post task. And the post task and run", "tokens": [50652, 407, 550, 321, 393, 8863, 484, 293, 321, 611, 362, 7198, 399, 337, 2183, 5633, 13, 400, 264, 2183, 5633, 293, 1190, 51028], "temperature": 0.0, "avg_logprob": -0.09874511900402251, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.006067309062927961}, {"id": 94, "seek": 56196, "start": 575.24, "end": 581.72, "text": " tasks conveniently are linked through a flow event. And what this actually means that you,", "tokens": [51028, 9608, 44375, 366, 9408, 807, 257, 3095, 2280, 13, 400, 437, 341, 767, 1355, 300, 291, 11, 51352], "temperature": 0.0, "avg_logprob": -0.09874511900402251, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.006067309062927961}, {"id": 95, "seek": 56196, "start": 581.72, "end": 586.52, "text": " instead of looking at a single task, which might or might not be useful, you can also explore", "tokens": [51352, 2602, 295, 1237, 412, 257, 2167, 5633, 11, 597, 1062, 420, 1062, 406, 312, 4420, 11, 291, 393, 611, 6839, 51592], "temperature": 0.0, "avg_logprob": -0.09874511900402251, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.006067309062927961}, {"id": 96, "seek": 58652, "start": 586.6, "end": 592.52, "text": " which tasks this came from and which tasks the task it came from came from here because of,", "tokens": [50368, 597, 9608, 341, 1361, 490, 293, 597, 9608, 264, 5633, 309, 1361, 490, 1361, 490, 510, 570, 295, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11922090193804573, "compression_ratio": 2.021551724137931, "no_speech_prob": 0.009825179353356361}, {"id": 97, "seek": 58652, "start": 592.52, "end": 599.0, "text": " I can't really zoom out and I can't really make it interactive. So I can't show the entire", "tokens": [50664, 286, 393, 380, 534, 8863, 484, 293, 286, 393, 380, 534, 652, 309, 15141, 13, 407, 286, 393, 380, 855, 264, 2302, 50988], "temperature": 0.0, "avg_logprob": -0.11922090193804573, "compression_ratio": 2.021551724137931, "no_speech_prob": 0.009825179353356361}, {"id": 98, "seek": 58652, "start": 600.28, "end": 604.52, "text": " to all threads involved. So this is a view of a single thread. But hey, you can see I selected", "tokens": [51052, 281, 439, 19314, 3288, 13, 407, 341, 307, 257, 1910, 295, 257, 2167, 7207, 13, 583, 4177, 11, 291, 393, 536, 286, 8209, 51264], "temperature": 0.0, "avg_logprob": -0.11922090193804573, "compression_ratio": 2.021551724137931, "no_speech_prob": 0.009825179353356361}, {"id": 99, "seek": 58652, "start": 604.52, "end": 610.68, "text": " a single task. I can see an incoming flow that is coming from a thread pool. And as you can see", "tokens": [51264, 257, 2167, 5633, 13, 286, 393, 536, 364, 22341, 3095, 300, 307, 1348, 490, 257, 7207, 7005, 13, 400, 382, 291, 393, 536, 51572], "temperature": 0.0, "avg_logprob": -0.11922090193804573, "compression_ratio": 2.021551724137931, "no_speech_prob": 0.009825179353356361}, {"id": 100, "seek": 58652, "start": 610.68, "end": 616.12, "text": " that that thread pool task is coming from another task from the main thread. And so you can see", "tokens": [51572, 300, 300, 7207, 7005, 5633, 307, 1348, 490, 1071, 5633, 490, 264, 2135, 7207, 13, 400, 370, 291, 393, 536, 51844], "temperature": 0.0, "avg_logprob": -0.11922090193804573, "compression_ratio": 2.021551724137931, "no_speech_prob": 0.009825179353356361}, {"id": 101, "seek": 61612, "start": 616.12, "end": 623.48, "text": " that actually all of these smaller tasks running after a larger task, they have been posted from", "tokens": [50364, 300, 767, 439, 295, 613, 4356, 9608, 2614, 934, 257, 4833, 5633, 11, 436, 362, 668, 9437, 490, 50732], "temperature": 0.0, "avg_logprob": -0.14643695328261827, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.004941453225910664}, {"id": 102, "seek": 61612, "start": 623.48, "end": 632.04, "text": " it. And these are, we know, these are related. So this is pretty much a very good starting point,", "tokens": [50732, 309, 13, 400, 613, 366, 11, 321, 458, 11, 613, 366, 4077, 13, 407, 341, 307, 1238, 709, 257, 588, 665, 2891, 935, 11, 51160], "temperature": 0.0, "avg_logprob": -0.14643695328261827, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.004941453225910664}, {"id": 103, "seek": 61612, "start": 632.04, "end": 637.72, "text": " but it doesn't give us everything. So there are a few other chalk points that might be useful to", "tokens": [51160, 457, 309, 1177, 380, 976, 505, 1203, 13, 407, 456, 366, 257, 1326, 661, 28660, 2793, 300, 1062, 312, 4420, 281, 51444], "temperature": 0.0, "avg_logprob": -0.14643695328261827, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.004941453225910664}, {"id": 104, "seek": 61612, "start": 637.72, "end": 645.24, "text": " instrument and that have been useful to instrument that can, that improve our ability to reason about", "tokens": [51444, 7198, 293, 300, 362, 668, 4420, 281, 7198, 300, 393, 11, 300, 3470, 527, 3485, 281, 1778, 466, 51820], "temperature": 0.0, "avg_logprob": -0.14643695328261827, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.004941453225910664}, {"id": 105, "seek": 64524, "start": 645.8, "end": 653.24, "text": " what is going on. Task scheduling is inherently inter-intro process. So it doesn't tell us about", "tokens": [50392, 437, 307, 516, 322, 13, 30428, 29055, 307, 27993, 728, 12, 686, 340, 1399, 13, 407, 309, 1177, 380, 980, 505, 466, 50764], "temperature": 0.0, "avg_logprob": -0.17956954874890915, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.004543112590909004}, {"id": 106, "seek": 64524, "start": 653.24, "end": 658.6800000000001, "text": " inter-pros communication, but fortunately in Chromium we have Mojo, which is an IPC subsystem,", "tokens": [50764, 728, 12, 1424, 329, 6101, 11, 457, 25511, 294, 1721, 298, 2197, 321, 362, 3335, 5134, 11, 597, 307, 364, 8671, 34, 2090, 9321, 11, 51036], "temperature": 0.0, "avg_logprob": -0.17956954874890915, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.004543112590909004}, {"id": 107, "seek": 64524, "start": 658.6800000000001, "end": 663.96, "text": " which we can also instrument and get pretty much the same information. But for cross-pros", "tokens": [51036, 597, 321, 393, 611, 7198, 293, 483, 1238, 709, 264, 912, 1589, 13, 583, 337, 3278, 12, 1424, 329, 51300], "temperature": 0.0, "avg_logprob": -0.17956954874890915, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.004543112590909004}, {"id": 108, "seek": 64524, "start": 663.96, "end": 670.92, "text": " communication, we can know who is sending messages and we can connect the place which posted the", "tokens": [51300, 6101, 11, 321, 393, 458, 567, 307, 7750, 7897, 293, 321, 393, 1745, 264, 1081, 597, 9437, 264, 51648], "temperature": 0.0, "avg_logprob": -0.17956954874890915, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.004543112590909004}, {"id": 109, "seek": 67092, "start": 670.92, "end": 676.1999999999999, "text": " message and places that received the message and to be able to trace this back through the flow.", "tokens": [50364, 3636, 293, 3190, 300, 4613, 264, 3636, 293, 281, 312, 1075, 281, 13508, 341, 646, 807, 264, 3095, 13, 50628], "temperature": 0.0, "avg_logprob": -0.16365616409866898, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.008345556445419788}, {"id": 110, "seek": 67092, "start": 677.3199999999999, "end": 682.68, "text": " Capturing console logs and DV logs and debug logs and the both logs is also not a great source of", "tokens": [50684, 9480, 1345, 11076, 20820, 293, 17021, 20820, 293, 24083, 20820, 293, 264, 1293, 20820, 307, 611, 406, 257, 869, 4009, 295, 50952], "temperature": 0.0, "avg_logprob": -0.16365616409866898, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.008345556445419788}, {"id": 111, "seek": 67092, "start": 682.68, "end": 688.04, "text": " information. If someone bothered to log it somewhere in the system, that's probably already useful", "tokens": [50952, 1589, 13, 759, 1580, 22996, 281, 3565, 309, 4079, 294, 264, 1185, 11, 300, 311, 1391, 1217, 4420, 51220], "temperature": 0.0, "avg_logprob": -0.16365616409866898, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.008345556445419788}, {"id": 112, "seek": 67092, "start": 688.04, "end": 693.24, "text": " for us. And being able to correlate this additional source with data with actual tasks that Chromium", "tokens": [51220, 337, 505, 13, 400, 885, 1075, 281, 48742, 341, 4497, 4009, 365, 1412, 365, 3539, 9608, 300, 1721, 298, 2197, 51480], "temperature": 0.0, "avg_logprob": -0.16365616409866898, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.008345556445419788}, {"id": 113, "seek": 67092, "start": 693.24, "end": 699.0, "text": " is using have been proven useful in many investigations. Capturing, instrumenting all of the", "tokens": [51480, 307, 1228, 362, 668, 12785, 4420, 294, 867, 25582, 13, 9480, 1345, 11, 7198, 278, 439, 295, 264, 51768], "temperature": 0.0, "avg_logprob": -0.16365616409866898, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.008345556445419788}, {"id": 114, "seek": 69900, "start": 699.0, "end": 705.4, "text": " blink binings and pretty much capturing all of the JavaScript functions, JavaScript calls that", "tokens": [50364, 24667, 5171, 1109, 293, 1238, 709, 23384, 439, 295, 264, 15778, 6828, 11, 15778, 5498, 300, 50684], "temperature": 0.0, "avg_logprob": -0.1271242073604039, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0066370051354169846}, {"id": 115, "seek": 69900, "start": 705.4, "end": 710.92, "text": " end up being implemented in blink is another great way to reason about what is going on and what", "tokens": [50684, 917, 493, 885, 12270, 294, 24667, 307, 1071, 869, 636, 281, 1778, 466, 437, 307, 516, 322, 293, 437, 50960], "temperature": 0.0, "avg_logprob": -0.1271242073604039, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0066370051354169846}, {"id": 116, "seek": 69900, "start": 710.92, "end": 719.32, "text": " the website is doing on and a couple of other similar infrastructure pieces. So the key takeaway", "tokens": [50960, 264, 3144, 307, 884, 322, 293, 257, 1916, 295, 661, 2531, 6896, 3755, 13, 407, 264, 2141, 30681, 51380], "temperature": 0.0, "avg_logprob": -0.1271242073604039, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0066370051354169846}, {"id": 117, "seek": 69900, "start": 719.32, "end": 725.64, "text": " here is that, hey, if you have complex systems, then probably you would do some good to instrument", "tokens": [51380, 510, 307, 300, 11, 4177, 11, 498, 291, 362, 3997, 3652, 11, 550, 1391, 291, 576, 360, 512, 665, 281, 7198, 51696], "temperature": 0.0, "avg_logprob": -0.1271242073604039, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0066370051354169846}, {"id": 118, "seek": 72564, "start": 725.64, "end": 731.72, "text": " some of the widely used things and if you are familiar with this codebase, you will be able to", "tokens": [50364, 512, 295, 264, 13371, 1143, 721, 293, 498, 291, 366, 4963, 365, 341, 3089, 17429, 11, 291, 486, 312, 1075, 281, 50668], "temperature": 0.0, "avg_logprob": -0.14176346393341713, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.013845449313521385}, {"id": 119, "seek": 72564, "start": 731.72, "end": 738.68, "text": " make some informed judgment of what is going on and you will be able to spot outliers, something", "tokens": [50668, 652, 512, 11740, 12216, 295, 437, 307, 516, 322, 293, 291, 486, 312, 1075, 281, 4008, 484, 23646, 11, 746, 51016], "temperature": 0.0, "avg_logprob": -0.14176346393341713, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.013845449313521385}, {"id": 120, "seek": 72564, "start": 738.68, "end": 744.04, "text": " taking too long, log being held in case of performance regression or a functional regression", "tokens": [51016, 1940, 886, 938, 11, 3565, 885, 5167, 294, 1389, 295, 3389, 24590, 420, 257, 11745, 24590, 51284], "temperature": 0.0, "avg_logprob": -0.14176346393341713, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.013845449313521385}, {"id": 121, "seek": 72564, "start": 744.04, "end": 751.16, "text": " or a flaky test, etc. And that's already a great step forward. So you have, you can look at it,", "tokens": [51284, 420, 257, 932, 15681, 1500, 11, 5183, 13, 400, 300, 311, 1217, 257, 869, 1823, 2128, 13, 407, 291, 362, 11, 291, 393, 574, 412, 309, 11, 51640], "temperature": 0.0, "avg_logprob": -0.14176346393341713, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.013845449313521385}, {"id": 122, "seek": 75116, "start": 751.16, "end": 755.8, "text": " you can like, if your test is flaky, you can run a thousand times, it will fail five times,", "tokens": [50364, 291, 393, 411, 11, 498, 428, 1500, 307, 932, 15681, 11, 291, 393, 1190, 257, 4714, 1413, 11, 309, 486, 3061, 1732, 1413, 11, 50596], "temperature": 0.0, "avg_logprob": -0.13815777276151925, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.014844091609120369}, {"id": 123, "seek": 75116, "start": 755.8, "end": 761.0799999999999, "text": " you can open five phrases, look and see if you're lucky enough, you will be able to spot", "tokens": [50596, 291, 393, 1269, 1732, 20312, 11, 574, 293, 536, 498, 291, 434, 6356, 1547, 11, 291, 486, 312, 1075, 281, 4008, 50860], "temperature": 0.0, "avg_logprob": -0.13815777276151925, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.014844091609120369}, {"id": 124, "seek": 75116, "start": 762.12, "end": 769.3199999999999, "text": " noticeable difference. But this is still not good enough for me. And the problem is that,", "tokens": [50912, 26041, 2649, 13, 583, 341, 307, 920, 406, 665, 1547, 337, 385, 13, 400, 264, 1154, 307, 300, 11, 51272], "temperature": 0.0, "avg_logprob": -0.13815777276151925, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.014844091609120369}, {"id": 125, "seek": 75116, "start": 769.3199999999999, "end": 775.56, "text": " despite having visibility into everything we're doing, this is very, very, very expertise intensive.", "tokens": [51272, 7228, 1419, 19883, 666, 1203, 321, 434, 884, 11, 341, 307, 588, 11, 588, 11, 588, 11769, 18957, 13, 51584], "temperature": 0.0, "avg_logprob": -0.13815777276151925, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.014844091609120369}, {"id": 126, "seek": 77556, "start": 775.56, "end": 781.4, "text": " So in order to be able to make good use of it, you have to kind of know everything. You have to", "tokens": [50364, 407, 294, 1668, 281, 312, 1075, 281, 652, 665, 764, 295, 309, 11, 291, 362, 281, 733, 295, 458, 1203, 13, 509, 362, 281, 50656], "temperature": 0.0, "avg_logprob": -0.11037655632094581, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.008181607350707054}, {"id": 127, "seek": 77556, "start": 781.4, "end": 787.4, "text": " know a lot about Chromium architecture. So as some of my colleagues say, you have to have a PhD", "tokens": [50656, 458, 257, 688, 466, 1721, 298, 2197, 9482, 13, 407, 382, 512, 295, 452, 7734, 584, 11, 291, 362, 281, 362, 257, 14476, 50956], "temperature": 0.0, "avg_logprob": -0.11037655632094581, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.008181607350707054}, {"id": 128, "seek": 77556, "start": 787.4, "end": 793.3199999999999, "text": " in tracing and Chromium architecture to truly make this useful. And I have an inspiration of, hey,", "tokens": [50956, 294, 25262, 293, 1721, 298, 2197, 9482, 281, 4908, 652, 341, 4420, 13, 400, 286, 362, 364, 10249, 295, 11, 4177, 11, 51252], "temperature": 0.0, "avg_logprob": -0.11037655632094581, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.008181607350707054}, {"id": 129, "seek": 77556, "start": 793.3199999999999, "end": 801.0, "text": " let's get it to the point that anyone, so any web developer can open and trace and instead of being", "tokens": [51252, 718, 311, 483, 309, 281, 264, 935, 300, 2878, 11, 370, 604, 3670, 10754, 393, 1269, 293, 13508, 293, 2602, 295, 885, 51636], "temperature": 0.0, "avg_logprob": -0.11037655632094581, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.008181607350707054}, {"id": 130, "seek": 80100, "start": 801.08, "end": 808.92, "text": " discouraged and being intimidated by all of this mumbo jumbo, they can learn something about how", "tokens": [50368, 35010, 293, 885, 40234, 538, 439, 295, 341, 14697, 1763, 29067, 1763, 11, 436, 393, 1466, 746, 466, 577, 50760], "temperature": 0.0, "avg_logprob": -0.17321591907077366, "compression_ratio": 1.6, "no_speech_prob": 0.00535417627543211}, {"id": 131, "seek": 80100, "start": 808.92, "end": 817.08, "text": " Chromium actually works and get more knowledge about this. So an inspiration that I have is", "tokens": [50760, 1721, 298, 2197, 767, 1985, 293, 483, 544, 3601, 466, 341, 13, 407, 364, 10249, 300, 286, 362, 307, 51168], "temperature": 0.0, "avg_logprob": -0.17321591907077366, "compression_ratio": 1.6, "no_speech_prob": 0.00535417627543211}, {"id": 132, "seek": 80100, "start": 817.08, "end": 821.64, "text": " this slide and this diagram from a life of a delegation, talk from Chromium University,", "tokens": [51168, 341, 4137, 293, 341, 10686, 490, 257, 993, 295, 257, 36602, 11, 751, 490, 1721, 298, 2197, 3535, 11, 51396], "temperature": 0.0, "avg_logprob": -0.17321591907077366, "compression_ratio": 1.6, "no_speech_prob": 0.00535417627543211}, {"id": 133, "seek": 80100, "start": 822.36, "end": 827.64, "text": " that is kind of similar to what we have already seen. It's a kind of a virtual timeline with a kind", "tokens": [51432, 300, 307, 733, 295, 2531, 281, 437, 321, 362, 1217, 1612, 13, 467, 311, 257, 733, 295, 257, 6374, 12933, 365, 257, 733, 51696], "temperature": 0.0, "avg_logprob": -0.17321591907077366, "compression_ratio": 1.6, "no_speech_prob": 0.00535417627543211}, {"id": 134, "seek": 82764, "start": 827.72, "end": 833.4, "text": " of boxers being connected by arrows. But if you look at it, then even if you are not deeply familiar", "tokens": [50368, 295, 2424, 433, 885, 4582, 538, 19669, 13, 583, 498, 291, 574, 412, 309, 11, 550, 754, 498, 291, 366, 406, 8760, 4963, 50652], "temperature": 0.0, "avg_logprob": -0.09846924883978707, "compression_ratio": 1.6482758620689655, "no_speech_prob": 0.004766550846397877}, {"id": 135, "seek": 82764, "start": 833.4, "end": 839.16, "text": " with the browser architecture, then you probably kind of make some sense and you can make some", "tokens": [50652, 365, 264, 11185, 9482, 11, 550, 291, 1391, 733, 295, 652, 512, 2020, 293, 291, 393, 652, 512, 50940], "temperature": 0.0, "avg_logprob": -0.09846924883978707, "compression_ratio": 1.6482758620689655, "no_speech_prob": 0.004766550846397877}, {"id": 136, "seek": 82764, "start": 839.16, "end": 845.24, "text": " educated guesses of what is going on. For example, if you see network stack doing start URL request", "tokens": [50940, 15872, 42703, 295, 437, 307, 516, 322, 13, 1171, 1365, 11, 498, 291, 536, 3209, 8630, 884, 722, 12905, 5308, 51244], "temperature": 0.0, "avg_logprob": -0.09846924883978707, "compression_ratio": 1.6482758620689655, "no_speech_prob": 0.004766550846397877}, {"id": 137, "seek": 82764, "start": 845.24, "end": 852.04, "text": " as a one off stage, it's something that you can develop or get a reasonably good intuition for.", "tokens": [51244, 382, 257, 472, 766, 3233, 11, 309, 311, 746, 300, 291, 393, 1499, 420, 483, 257, 23551, 665, 24002, 337, 13, 51584], "temperature": 0.0, "avg_logprob": -0.09846924883978707, "compression_ratio": 1.6482758620689655, "no_speech_prob": 0.004766550846397877}, {"id": 138, "seek": 82764, "start": 853.08, "end": 857.4, "text": " And that's kind of the status quo that we currently have, which is pretty much exactly", "tokens": [51636, 400, 300, 311, 733, 295, 264, 6558, 28425, 300, 321, 4362, 362, 11, 597, 307, 1238, 709, 2293, 51852], "temperature": 0.0, "avg_logprob": -0.09846924883978707, "compression_ratio": 1.6482758620689655, "no_speech_prob": 0.004766550846397877}, {"id": 139, "seek": 85740, "start": 857.4, "end": 861.72, "text": " the same information, but slightly less useful, slightly less easier to read and slightly more", "tokens": [50364, 264, 912, 1589, 11, 457, 4748, 1570, 4420, 11, 4748, 1570, 3571, 281, 1401, 293, 4748, 544, 50580], "temperature": 0.0, "avg_logprob": -0.18494040101439088, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.005076077301055193}, {"id": 140, "seek": 85740, "start": 861.72, "end": 868.04, "text": " intimidating. So for example, you can see tasks, you can see that hey, some of them are related to", "tokens": [50580, 29714, 13, 407, 337, 1365, 11, 291, 393, 536, 9608, 11, 291, 393, 536, 300, 4177, 11, 512, 295, 552, 366, 4077, 281, 50896], "temperature": 0.0, "avg_logprob": -0.18494040101439088, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.005076077301055193}, {"id": 141, "seek": 85740, "start": 868.04, "end": 872.76, "text": " URL load the client, so you're getting information from the network. Someone, a navigation client,", "tokens": [50896, 12905, 3677, 264, 6423, 11, 370, 291, 434, 1242, 1589, 490, 264, 3209, 13, 8734, 11, 257, 17346, 6423, 11, 51132], "temperature": 0.0, "avg_logprob": -0.18494040101439088, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.005076077301055193}, {"id": 142, "seek": 85740, "start": 872.76, "end": 879.88, "text": " which kind of you know the navigation stack, you kind of guess what it is, but the level of", "tokens": [51132, 597, 733, 295, 291, 458, 264, 17346, 8630, 11, 291, 733, 295, 2041, 437, 309, 307, 11, 457, 264, 1496, 295, 51488], "temperature": 0.0, "avg_logprob": -0.18494040101439088, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.005076077301055193}, {"id": 143, "seek": 87988, "start": 879.96, "end": 887.4, "text": " intuitiveness is starkly different. So there are existing examples where we already do this in", "tokens": [50368, 16224, 8477, 307, 17417, 356, 819, 13, 407, 456, 366, 6741, 5110, 689, 321, 1217, 360, 341, 294, 50740], "temperature": 0.0, "avg_logprob": -0.10660155252976851, "compression_ratio": 1.6936170212765957, "no_speech_prob": 0.0070380521938204765}, {"id": 144, "seek": 87988, "start": 887.4, "end": 892.92, "text": " Chromium and we like take the care to reconstruct the high level events and the high level timeline", "tokens": [50740, 1721, 298, 2197, 293, 321, 411, 747, 264, 1127, 281, 31499, 264, 1090, 1496, 3931, 293, 264, 1090, 1496, 12933, 51016], "temperature": 0.0, "avg_logprob": -0.10660155252976851, "compression_ratio": 1.6936170212765957, "no_speech_prob": 0.0070380521938204765}, {"id": 145, "seek": 87988, "start": 892.92, "end": 899.56, "text": " for specific things. For example, this is an example of event latency. So specifically breaking down", "tokens": [51016, 337, 2685, 721, 13, 1171, 1365, 11, 341, 307, 364, 1365, 295, 2280, 27043, 13, 407, 4682, 7697, 760, 51348], "temperature": 0.0, "avg_logprob": -0.10660155252976851, "compression_ratio": 1.6936170212765957, "no_speech_prob": 0.0070380521938204765}, {"id": 146, "seek": 87988, "start": 899.56, "end": 906.12, "text": " the timeline of steps and sequence of steps involved in presenting a frame. We're doing great on time.", "tokens": [51348, 264, 12933, 295, 4439, 293, 8310, 295, 4439, 3288, 294, 15578, 257, 3920, 13, 492, 434, 884, 869, 322, 565, 13, 51676], "temperature": 0.0, "avg_logprob": -0.10660155252976851, "compression_ratio": 1.6936170212765957, "no_speech_prob": 0.0070380521938204765}, {"id": 147, "seek": 90612, "start": 906.92, "end": 912.6, "text": " So the downside is that it's plumbing is very expensive and scaling this up is very difficult.", "tokens": [50404, 407, 264, 25060, 307, 300, 309, 311, 39993, 307, 588, 5124, 293, 21589, 341, 493, 307, 588, 2252, 13, 50688], "temperature": 0.0, "avg_logprob": -0.13313290732247487, "compression_ratio": 1.9227642276422765, "no_speech_prob": 0.0034582079388201237}, {"id": 148, "seek": 90612, "start": 913.32, "end": 917.16, "text": " When you have a big project, you have information, you need information from different", "tokens": [50724, 1133, 291, 362, 257, 955, 1716, 11, 291, 362, 1589, 11, 291, 643, 1589, 490, 819, 50916], "temperature": 0.0, "avg_logprob": -0.13313290732247487, "compression_ratio": 1.9227642276422765, "no_speech_prob": 0.0034582079388201237}, {"id": 149, "seek": 90612, "start": 918.04, "end": 922.92, "text": " you know corners of this project and plumbing is very expensive, both in terms of serialization", "tokens": [50960, 291, 458, 12413, 295, 341, 1716, 293, 39993, 307, 588, 5124, 11, 1293, 294, 2115, 295, 17436, 2144, 51204], "temperature": 0.0, "avg_logprob": -0.13313290732247487, "compression_ratio": 1.9227642276422765, "no_speech_prob": 0.0034582079388201237}, {"id": 150, "seek": 90612, "start": 922.92, "end": 926.92, "text": " costs, in terms of layering concerns, in terms of the amount of plumbing code that you need to", "tokens": [51204, 5497, 11, 294, 2115, 295, 40754, 7389, 11, 294, 2115, 295, 264, 2372, 295, 39993, 3089, 300, 291, 643, 281, 51404], "temperature": 0.0, "avg_logprob": -0.13313290732247487, "compression_ratio": 1.9227642276422765, "no_speech_prob": 0.0034582079388201237}, {"id": 151, "seek": 90612, "start": 926.92, "end": 932.44, "text": " maintain. And this you know difficult to scale and you know we haven't implemented this for too many", "tokens": [51404, 6909, 13, 400, 341, 291, 458, 2252, 281, 4373, 293, 291, 458, 321, 2378, 380, 12270, 341, 337, 886, 867, 51680], "temperature": 0.0, "avg_logprob": -0.13313290732247487, "compression_ratio": 1.9227642276422765, "no_speech_prob": 0.0034582079388201237}, {"id": 152, "seek": 93244, "start": 932.44, "end": 939.32, "text": " exciting things. So let me talk about Perfetto a little bit. So Perfetto is the new generation", "tokens": [50364, 4670, 721, 13, 407, 718, 385, 751, 466, 3026, 69, 23778, 257, 707, 857, 13, 407, 3026, 69, 23778, 307, 264, 777, 5125, 50708], "temperature": 0.0, "avg_logprob": -0.15819364605527936, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.004265132360160351}, {"id": 153, "seek": 93244, "start": 939.32, "end": 945.8000000000001, "text": " tracing framework born from the ashes of Chromium tracing by a few great folks who have been working", "tokens": [50708, 25262, 8388, 4232, 490, 264, 32942, 295, 1721, 298, 2197, 25262, 538, 257, 1326, 869, 4024, 567, 362, 668, 1364, 51032], "temperature": 0.0, "avg_logprob": -0.15819364605527936, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.004265132360160351}, {"id": 154, "seek": 93244, "start": 945.8000000000001, "end": 951.96, "text": " on Chromium tracing, got fed up with it, learned all of the mistakes that happened there and all of", "tokens": [51032, 322, 1721, 298, 2197, 25262, 11, 658, 4636, 493, 365, 309, 11, 3264, 439, 295, 264, 8038, 300, 2011, 456, 293, 439, 295, 51340], "temperature": 0.0, "avg_logprob": -0.15819364605527936, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.004265132360160351}, {"id": 155, "seek": 93244, "start": 951.96, "end": 956.6, "text": " the things that we should shouldn't have done in the first place. And Voila Perfetto, which is", "tokens": [51340, 264, 721, 300, 321, 820, 4659, 380, 362, 1096, 294, 264, 700, 1081, 13, 400, 7518, 7371, 3026, 69, 23778, 11, 597, 307, 51572], "temperature": 0.0, "avg_logprob": -0.15819364605527936, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.004265132360160351}, {"id": 156, "seek": 95660, "start": 956.6, "end": 962.44, "text": " nowadays widely used for Chromium and Android tracing. So it has fancy new UI, it has more", "tokens": [50364, 13434, 13371, 1143, 337, 1721, 298, 2197, 293, 8853, 25262, 13, 407, 309, 575, 10247, 777, 15682, 11, 309, 575, 544, 50656], "temperature": 0.0, "avg_logprob": -0.11303231957253446, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.017335442826151848}, {"id": 157, "seek": 95660, "start": 962.44, "end": 970.2, "text": " efficient format, but the thing that brings a special place in my heart for it is the new", "tokens": [50656, 7148, 7877, 11, 457, 264, 551, 300, 5607, 257, 2121, 1081, 294, 452, 1917, 337, 309, 307, 264, 777, 51044], "temperature": 0.0, "avg_logprob": -0.11303231957253446, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.017335442826151848}, {"id": 158, "seek": 95660, "start": 970.2, "end": 976.44, "text": " SQL data model and query engine. So essentially everything that you can see in the UI is backed", "tokens": [51044, 19200, 1412, 2316, 293, 14581, 2848, 13, 407, 4476, 1203, 300, 291, 393, 536, 294, 264, 15682, 307, 20391, 51356], "temperature": 0.0, "avg_logprob": -0.11303231957253446, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.017335442826151848}, {"id": 159, "seek": 95660, "start": 976.44, "end": 982.28, "text": " by a data model and UI is just running queries in the data model, against this data model and", "tokens": [51356, 538, 257, 1412, 2316, 293, 15682, 307, 445, 2614, 24109, 294, 264, 1412, 2316, 11, 1970, 341, 1412, 2316, 293, 51648], "temperature": 0.0, "avg_logprob": -0.11303231957253446, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.017335442826151848}, {"id": 160, "seek": 98228, "start": 982.28, "end": 988.68, "text": " presenting it. And presenting it and you can very easily do it yourself. And you know we trace", "tokens": [50364, 15578, 309, 13, 400, 15578, 309, 293, 291, 393, 588, 3612, 360, 309, 1803, 13, 400, 291, 458, 321, 13508, 50684], "temperature": 0.0, "avg_logprob": -0.18169489572214526, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.009770096279680729}, {"id": 161, "seek": 98228, "start": 988.68, "end": 993.3199999999999, "text": " processor actually is compiled as a was module and running in your browser in a background", "tokens": [50684, 15321, 767, 307, 36548, 382, 257, 390, 10088, 293, 2614, 294, 428, 11185, 294, 257, 3678, 50916], "temperature": 0.0, "avg_logprob": -0.18169489572214526, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.009770096279680729}, {"id": 162, "seek": 98228, "start": 993.3199999999999, "end": 1002.6, "text": " thread. Voila web, we have gone, we've came very far. And this allows us to separate recording", "tokens": [50916, 7207, 13, 7518, 7371, 3670, 11, 321, 362, 2780, 11, 321, 600, 1361, 588, 1400, 13, 400, 341, 4045, 505, 281, 4994, 6613, 51380], "temperature": 0.0, "avg_logprob": -0.18169489572214526, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.009770096279680729}, {"id": 163, "seek": 98228, "start": 1002.6, "end": 1007.3199999999999, "text": " the trace and emitting the low level instrumentation and actually analyzing it and building high", "tokens": [51380, 264, 13508, 293, 846, 2414, 264, 2295, 1496, 7198, 399, 293, 767, 23663, 309, 293, 2390, 1090, 51616], "temperature": 0.0, "avg_logprob": -0.18169489572214526, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.009770096279680729}, {"id": 164, "seek": 100732, "start": 1007.32, "end": 1013.08, "text": " level data models. So this is probably the best example of Perfetto powers. I could", "tokens": [50364, 1496, 1412, 5245, 13, 407, 341, 307, 1391, 264, 1151, 1365, 295, 3026, 69, 23778, 8674, 13, 286, 727, 50652], "temperature": 0.0, "avg_logprob": -0.14894081683869057, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.004728376865386963}, {"id": 165, "seek": 100732, "start": 1013.08, "end": 1019.4000000000001, "text": " hit it in a single slide. You can replicate this yourself if you go to Perfetto, if you go to", "tokens": [50652, 2045, 309, 294, 257, 2167, 4137, 13, 509, 393, 25356, 341, 1803, 498, 291, 352, 281, 3026, 69, 23778, 11, 498, 291, 352, 281, 50968], "temperature": 0.0, "avg_logprob": -0.14894081683869057, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.004728376865386963}, {"id": 166, "seek": 100732, "start": 1021.1600000000001, "end": 1027.24, "text": " open the Chrome trace, if you type colon into the search box, you will enter the SQL query mode", "tokens": [51056, 1269, 264, 15327, 13508, 11, 498, 291, 2010, 8255, 666, 264, 3164, 2424, 11, 291, 486, 3242, 264, 19200, 14581, 4391, 51360], "temperature": 0.0, "avg_logprob": -0.14894081683869057, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.004728376865386963}, {"id": 167, "seek": 100732, "start": 1027.24, "end": 1035.0, "text": " and then you can copy and paste the query that I inserted there. Once again, you should have", "tokens": [51360, 293, 550, 291, 393, 5055, 293, 9163, 264, 14581, 300, 286, 27992, 456, 13, 3443, 797, 11, 291, 820, 362, 51748], "temperature": 0.0, "avg_logprob": -0.14894081683869057, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.004728376865386963}, {"id": 168, "seek": 103500, "start": 1035.0, "end": 1042.84, "text": " access to the slides and then it will pretty much give you the list of top 100 longest tasks that", "tokens": [50364, 2105, 281, 264, 9788, 293, 550, 309, 486, 1238, 709, 976, 291, 264, 1329, 295, 1192, 2319, 15438, 9608, 300, 50756], "temperature": 0.0, "avg_logprob": -0.10472867705605247, "compression_ratio": 1.5528455284552845, "no_speech_prob": 0.005280734039843082}, {"id": 169, "seek": 103500, "start": 1042.84, "end": 1048.84, "text": " we ended up running there, which is already useful for analysis and can allow you to build more and", "tokens": [50756, 321, 4590, 493, 2614, 456, 11, 597, 307, 1217, 4420, 337, 5215, 293, 393, 2089, 291, 281, 1322, 544, 293, 51056], "temperature": 0.0, "avg_logprob": -0.10472867705605247, "compression_ratio": 1.5528455284552845, "no_speech_prob": 0.005280734039843082}, {"id": 170, "seek": 103500, "start": 1048.84, "end": 1056.6, "text": " more complex data models through different tables within SQL, which is kind of cool. So what are", "tokens": [51056, 544, 3997, 1412, 5245, 807, 819, 8020, 1951, 19200, 11, 597, 307, 733, 295, 1627, 13, 407, 437, 366, 51444], "temperature": 0.0, "avg_logprob": -0.10472867705605247, "compression_ratio": 1.5528455284552845, "no_speech_prob": 0.005280734039843082}, {"id": 171, "seek": 103500, "start": 1056.6, "end": 1062.12, "text": " the next steps here? So I am right now trying to build in a navigation instrumentation,", "tokens": [51444, 264, 958, 4439, 510, 30, 407, 286, 669, 558, 586, 1382, 281, 1322, 294, 257, 17346, 7198, 399, 11, 51720], "temperature": 0.0, "avg_logprob": -0.10472867705605247, "compression_ratio": 1.5528455284552845, "no_speech_prob": 0.005280734039843082}, {"id": 172, "seek": 106212, "start": 1062.1999999999998, "end": 1068.84, "text": " fancy navigation instrumentation as a proof of concept. The current prototype is kind of there,", "tokens": [50368, 10247, 17346, 7198, 399, 382, 257, 8177, 295, 3410, 13, 440, 2190, 19475, 307, 733, 295, 456, 11, 50700], "temperature": 0.0, "avg_logprob": -0.13402106041132017, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.010103807784616947}, {"id": 173, "seek": 106212, "start": 1068.84, "end": 1073.32, "text": " so you can see that we have a timeline. This is all pretty much based on the same low level", "tokens": [50700, 370, 291, 393, 536, 300, 321, 362, 257, 12933, 13, 639, 307, 439, 1238, 709, 2361, 322, 264, 912, 2295, 1496, 50924], "temperature": 0.0, "avg_logprob": -0.13402106041132017, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.010103807784616947}, {"id": 174, "seek": 106212, "start": 1073.32, "end": 1081.8799999999999, "text": " information, but presents it in a more fancy version. And this then can be further integrated", "tokens": [50924, 1589, 11, 457, 13533, 309, 294, 257, 544, 10247, 3037, 13, 400, 341, 550, 393, 312, 3052, 10919, 51352], "temperature": 0.0, "avg_logprob": -0.13402106041132017, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.010103807784616947}, {"id": 175, "seek": 106212, "start": 1081.8799999999999, "end": 1086.84, "text": " with the documentation. So this is just a not standalone box with a couple of words scribble", "tokens": [51352, 365, 264, 14333, 13, 407, 341, 307, 445, 257, 406, 37454, 2424, 365, 257, 1916, 295, 2283, 39435, 638, 51600], "temperature": 0.0, "avg_logprob": -0.13402106041132017, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.010103807784616947}, {"id": 176, "seek": 108684, "start": 1086.9199999999998, "end": 1092.4399999999998, "text": " on it, but we can also link to parts of Chromium documentation that outline what this stage", "tokens": [50368, 322, 309, 11, 457, 321, 393, 611, 2113, 281, 3166, 295, 1721, 298, 2197, 14333, 300, 16387, 437, 341, 3233, 50644], "temperature": 0.0, "avg_logprob": -0.1987779140472412, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.02751069702208042}, {"id": 177, "seek": 108684, "start": 1092.4399999999998, "end": 1099.08, "text": " is actually about, what are the concepts that you need to think about and make it generally more", "tokens": [50644, 307, 767, 466, 11, 437, 366, 264, 10392, 300, 291, 643, 281, 519, 466, 293, 652, 309, 5101, 544, 50976], "temperature": 0.0, "avg_logprob": -0.1987779140472412, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.02751069702208042}, {"id": 178, "seek": 108684, "start": 1099.08, "end": 1106.12, "text": " useful. One of the major complexities, why we haven't done this before, is that the complexity", "tokens": [50976, 4420, 13, 1485, 295, 264, 2563, 48705, 11, 983, 321, 2378, 380, 1096, 341, 949, 11, 307, 300, 264, 14024, 51328], "temperature": 0.0, "avg_logprob": -0.1987779140472412, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.02751069702208042}, {"id": 179, "seek": 108684, "start": 1106.12, "end": 1110.84, "text": " in a number of corner cases. When you talk about navigation, when you talk about typing the URL", "tokens": [51328, 294, 257, 1230, 295, 4538, 3331, 13, 1133, 291, 751, 466, 17346, 11, 562, 291, 751, 466, 18444, 264, 12905, 51564], "temperature": 0.0, "avg_logprob": -0.1987779140472412, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.02751069702208042}, {"id": 180, "seek": 108684, "start": 1110.84, "end": 1115.9599999999998, "text": " into the OmniBox, there are like a mind boggling complex number of cases from redirects to", "tokens": [51564, 666, 264, 9757, 3722, 34980, 11, 456, 366, 411, 257, 1575, 26132, 70, 1688, 3997, 1230, 295, 3331, 490, 29066, 82, 281, 51820], "temperature": 0.0, "avg_logprob": -0.1987779140472412, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.02751069702208042}, {"id": 181, "seek": 111684, "start": 1117.1599999999999, "end": 1123.32, "text": " navigation, turning it to downloads to server returning to O4 and canceling the navigation", "tokens": [50380, 17346, 11, 6246, 309, 281, 36553, 281, 7154, 12678, 281, 422, 19, 293, 10373, 278, 264, 17346, 50688], "temperature": 0.0, "avg_logprob": -0.34442467322716347, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.007982944138348103}, {"id": 182, "seek": 111684, "start": 1123.32, "end": 1129.32, "text": " that you kind of need to think about. And building this instrumentation without being able to test", "tokens": [50688, 300, 291, 733, 295, 643, 281, 519, 466, 13, 400, 2390, 341, 7198, 399, 1553, 885, 1075, 281, 1500, 50988], "temperature": 0.0, "avg_logprob": -0.34442467322716347, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.007982944138348103}, {"id": 183, "seek": 111684, "start": 1129.32, "end": 1137.56, "text": " it is kind of a losing game. And the Sequel support actually allows us to feasibly write this", "tokens": [50988, 309, 307, 733, 295, 257, 7027, 1216, 13, 400, 264, 1100, 20593, 1406, 767, 4045, 505, 281, 21781, 3545, 2464, 341, 51400], "temperature": 0.0, "avg_logprob": -0.34442467322716347, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.007982944138348103}, {"id": 184, "seek": 113756, "start": 1137.8799999999999, "end": 1146.84, "text": " testing coverage for these corner cases. I think I'm 15 out of 50 at this point, so some work to do.", "tokens": [50380, 4997, 9645, 337, 613, 4538, 3331, 13, 286, 519, 286, 478, 2119, 484, 295, 2625, 412, 341, 935, 11, 370, 512, 589, 281, 360, 13, 50828], "temperature": 0.0, "avg_logprob": -0.18255408150809152, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.03135838732123375}, {"id": 185, "seek": 113756, "start": 1148.9199999999998, "end": 1154.9199999999998, "text": " So yeah, I think that's all of the main content that I have. I have a bonus demo,", "tokens": [50932, 407, 1338, 11, 286, 519, 300, 311, 439, 295, 264, 2135, 2701, 300, 286, 362, 13, 286, 362, 257, 10882, 10723, 11, 51232], "temperature": 0.0, "avg_logprob": -0.18255408150809152, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.03135838732123375}, {"id": 186, "seek": 113756, "start": 1155.48, "end": 1160.04, "text": " which is kind of about DevTools, but I can also take questions.", "tokens": [51260, 597, 307, 733, 295, 466, 9096, 51, 29298, 11, 457, 286, 393, 611, 747, 1651, 13, 51488], "temperature": 0.0, "avg_logprob": -0.18255408150809152, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.03135838732123375}, {"id": 187, "seek": 116756, "start": 1168.2, "end": 1178.52, "text": " Eyeballing is, so the question is what's the best way of comparing these traces? I think eyeballing", "tokens": [50396, 21603, 3129, 278, 307, 11, 370, 264, 1168, 307, 437, 311, 264, 1151, 636, 295, 15763, 613, 26076, 30, 286, 519, 3313, 3129, 278, 50912], "temperature": 0.0, "avg_logprob": -0.24865676987339075, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.01466064527630806}, {"id": 188, "seek": 116756, "start": 1178.52, "end": 1185.24, "text": " is probably a good place to start. So there are some early experiments of opening the traces", "tokens": [50912, 307, 1391, 257, 665, 1081, 281, 722, 13, 407, 456, 366, 512, 2440, 12050, 295, 5193, 264, 26076, 51248], "temperature": 0.0, "avg_logprob": -0.24865676987339075, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.01466064527630806}, {"id": 189, "seek": 116756, "start": 1185.24, "end": 1190.9199999999998, "text": " times, say to some it and being able to link the timelines, but this is greatly depends on what", "tokens": [51248, 1413, 11, 584, 281, 512, 309, 293, 885, 1075, 281, 2113, 264, 45886, 11, 457, 341, 307, 14147, 5946, 322, 437, 51532], "temperature": 0.0, "avg_logprob": -0.24865676987339075, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.01466064527630806}, {"id": 190, "seek": 119092, "start": 1190.92, "end": 1202.1200000000001, "text": " kind of problem you're looking at. For example, if you are comparing the traces from tests,", "tokens": [50364, 733, 295, 1154, 291, 434, 1237, 412, 13, 1171, 1365, 11, 498, 291, 366, 15763, 264, 26076, 490, 6921, 11, 50924], "temperature": 0.0, "avg_logprob": -0.13017718932207892, "compression_ratio": 1.574585635359116, "no_speech_prob": 0.005040529649704695}, {"id": 191, "seek": 119092, "start": 1202.1200000000001, "end": 1210.52, "text": " then the workload is more repeatable and you can actually go further in comparing it. For example,", "tokens": [50924, 550, 264, 20139, 307, 544, 7149, 712, 293, 291, 393, 767, 352, 3052, 294, 15763, 309, 13, 1171, 1365, 11, 51344], "temperature": 0.0, "avg_logprob": -0.13017718932207892, "compression_ratio": 1.574585635359116, "no_speech_prob": 0.005040529649704695}, {"id": 192, "seek": 119092, "start": 1210.52, "end": 1214.6000000000001, "text": " writing some Sequel queries and instructing some high-level metrics can get you very, very far", "tokens": [51344, 3579, 512, 1100, 20593, 24109, 293, 7232, 278, 512, 1090, 12, 12418, 16367, 393, 483, 291, 588, 11, 588, 1400, 51548], "temperature": 0.0, "avg_logprob": -0.13017718932207892, "compression_ratio": 1.574585635359116, "no_speech_prob": 0.005040529649704695}, {"id": 193, "seek": 121460, "start": 1215.48, "end": 1221.56, "text": " and spotting if any high-level metrics changed or any derived things changed.", "tokens": [50408, 293, 4008, 783, 498, 604, 1090, 12, 12418, 16367, 3105, 420, 604, 18949, 721, 3105, 13, 50712], "temperature": 0.0, "avg_logprob": -0.1701948600902892, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.018129006028175354}, {"id": 194, "seek": 121460, "start": 1222.28, "end": 1227.9599999999998, "text": " If it's user's interactive, then probably eyeballing and going from there and seeing", "tokens": [50748, 759, 309, 311, 4195, 311, 15141, 11, 550, 1391, 3313, 3129, 278, 293, 516, 490, 456, 293, 2577, 51032], "temperature": 0.0, "avg_logprob": -0.1701948600902892, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.018129006028175354}, {"id": 195, "seek": 121460, "start": 1227.9599999999998, "end": 1235.8, "text": " how much variance there is. Yes. Yes, some Sequel statement there.", "tokens": [51032, 577, 709, 21977, 456, 307, 13, 1079, 13, 1079, 11, 512, 1100, 20593, 5629, 456, 13, 51424], "temperature": 0.0, "avg_logprob": -0.1701948600902892, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.018129006028175354}, {"id": 196, "seek": 124460, "start": 1244.6, "end": 1251.7199999999998, "text": " Yes, great question. The question is we have Sequel, but where is the database? The answer is it's", "tokens": [50364, 1079, 11, 869, 1168, 13, 440, 1168, 307, 321, 362, 1100, 20593, 11, 457, 689, 307, 264, 8149, 30, 440, 1867, 307, 309, 311, 50720], "temperature": 0.0, "avg_logprob": -0.19622731763263082, "compression_ratio": 1.5285714285714285, "no_speech_prob": 0.01366676576435566}, {"id": 197, "seek": 124460, "start": 1251.7199999999998, "end": 1258.28, "text": " all done locally. So this is Sequelite compiled into a WOS module with some helpers on top.", "tokens": [50720, 439, 1096, 16143, 13, 407, 341, 307, 1100, 20593, 642, 36548, 666, 257, 343, 4367, 10088, 365, 512, 854, 433, 322, 1192, 13, 51048], "temperature": 0.0, "avg_logprob": -0.19622731763263082, "compression_ratio": 1.5285714285714285, "no_speech_prob": 0.01366676576435566}, {"id": 198, "seek": 124460, "start": 1258.9199999999998, "end": 1264.52, "text": " So when you're opening a trace, it's running in your background thread in Sequelite instance.", "tokens": [51080, 407, 562, 291, 434, 5193, 257, 13508, 11, 309, 311, 2614, 294, 428, 3678, 7207, 294, 1100, 20593, 642, 5197, 13, 51360], "temperature": 0.0, "avg_logprob": -0.19622731763263082, "compression_ratio": 1.5285714285714285, "no_speech_prob": 0.01366676576435566}, {"id": 199, "seek": 124460, "start": 1264.52, "end": 1265.32, "text": " Everything is local.", "tokens": [51360, 5471, 307, 2654, 13, 51400], "temperature": 0.0, "avg_logprob": -0.19622731763263082, "compression_ratio": 1.5285714285714285, "no_speech_prob": 0.01366676576435566}, {"id": 200, "seek": 124460, "start": 1271.3999999999999, "end": 1272.04, "text": " More questions?", "tokens": [51704, 5048, 1651, 30, 51736], "temperature": 0.0, "avg_logprob": -0.19622731763263082, "compression_ratio": 1.5285714285714285, "no_speech_prob": 0.01366676576435566}, {"id": 201, "seek": 127460, "start": 1275.32, "end": 1283.56, "text": " If not, I can actually go and show you my favorite House Party trick, which is an illustration of", "tokens": [50400, 759, 406, 11, 286, 393, 767, 352, 293, 855, 291, 452, 2954, 4928, 8552, 4282, 11, 597, 307, 364, 22645, 295, 50812], "temperature": 0.0, "avg_logprob": -0.12574553133836433, "compression_ratio": 1.3956043956043955, "no_speech_prob": 0.005827863700687885}, {"id": 202, "seek": 127460, "start": 1283.56, "end": 1287.6399999999999, "text": " why it's actually quite important, I think, to think about data presentation. Sorry?", "tokens": [50812, 983, 309, 311, 767, 1596, 1021, 11, 286, 519, 11, 281, 519, 466, 1412, 5860, 13, 4919, 30, 51016], "temperature": 0.0, "avg_logprob": -0.12574553133836433, "compression_ratio": 1.3956043956043955, "no_speech_prob": 0.005827863700687885}, {"id": 203, "seek": 127460, "start": 1288.6, "end": 1294.6799999999998, "text": " More questions? No. Let me try to do this. Can you see what's going on?", "tokens": [51064, 5048, 1651, 30, 883, 13, 961, 385, 853, 281, 360, 341, 13, 1664, 291, 536, 437, 311, 516, 322, 30, 51368], "temperature": 0.0, "avg_logprob": -0.12574553133836433, "compression_ratio": 1.3956043956043955, "no_speech_prob": 0.005827863700687885}, {"id": 204, "seek": 129468, "start": 1295.24, "end": 1307.24, "text": " So let me open a trace in the performance in ground deftos that I have recorded earlier this morning.", "tokens": [50392, 407, 718, 385, 1269, 257, 13508, 294, 264, 3389, 294, 2727, 368, 844, 329, 300, 286, 362, 8287, 3071, 341, 2446, 13, 50992], "temperature": 0.0, "avg_logprob": -0.25646756300285684, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.020933011546730995}, {"id": 205, "seek": 129468, "start": 1307.24, "end": 1312.68, "text": " And this is something that you should be already familiar with, but the thing that some of you", "tokens": [50992, 400, 341, 307, 746, 300, 291, 820, 312, 1217, 4963, 365, 11, 457, 264, 551, 300, 512, 295, 291, 51264], "temperature": 0.0, "avg_logprob": -0.25646756300285684, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.020933011546730995}, {"id": 206, "seek": 129468, "start": 1312.68, "end": 1317.64, "text": " might not have realized, that there is nothing inherently magical or special about these deftal", "tokens": [51264, 1062, 406, 362, 5334, 11, 300, 456, 307, 1825, 27993, 12066, 420, 2121, 466, 613, 368, 844, 304, 51512], "temperature": 0.0, "avg_logprob": -0.25646756300285684, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.020933011546730995}, {"id": 207, "seek": 131764, "start": 1318.3600000000001, "end": 1325.5600000000002, "text": " traces, apart from very good UI and a lot of UX thoughts that went into that. But fundamentally,", "tokens": [50400, 26076, 11, 4936, 490, 588, 665, 15682, 293, 257, 688, 295, 40176, 4598, 300, 1437, 666, 300, 13, 583, 17879, 11, 50760], "temperature": 0.0, "avg_logprob": -0.1659692128499349, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.05214497819542885}, {"id": 208, "seek": 131764, "start": 1325.5600000000002, "end": 1332.2800000000002, "text": " they are just JSON-chrome traces, just with a bunch of categories. And you can actually open", "tokens": [50760, 436, 366, 445, 31828, 12, 339, 11505, 26076, 11, 445, 365, 257, 3840, 295, 10479, 13, 400, 291, 393, 767, 1269, 51096], "temperature": 0.0, "avg_logprob": -0.1659692128499349, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.05214497819542885}, {"id": 209, "seek": 131764, "start": 1332.2800000000002, "end": 1338.92, "text": " the very same information in Perfetto and actually look at it. And you already can see that the", "tokens": [51096, 264, 588, 912, 1589, 294, 3026, 69, 23778, 293, 767, 574, 412, 309, 13, 400, 291, 1217, 393, 536, 300, 264, 51428], "temperature": 0.0, "avg_logprob": -0.1659692128499349, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.05214497819542885}, {"id": 210, "seek": 131764, "start": 1338.92, "end": 1344.0400000000002, "text": " usefulness of this information is a bit different. We have to zoom and find our relevant parts,", "tokens": [51428, 4420, 1287, 295, 341, 1589, 307, 257, 857, 819, 13, 492, 362, 281, 8863, 293, 915, 527, 7340, 3166, 11, 51684], "temperature": 0.0, "avg_logprob": -0.1659692128499349, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.05214497819542885}, {"id": 211, "seek": 134404, "start": 1344.12, "end": 1348.68, "text": " and we have been exposed with a low-level information, but no high-level insights.", "tokens": [50368, 293, 321, 362, 668, 9495, 365, 257, 2295, 12, 12418, 1589, 11, 457, 572, 1090, 12, 12418, 14310, 13, 50596], "temperature": 0.0, "avg_logprob": -0.20224792162577312, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.007973437197506428}, {"id": 212, "seek": 134404, "start": 1348.68, "end": 1352.6, "text": " But then we have the network tracing. And the best way to illustrate that,", "tokens": [50596, 583, 550, 321, 362, 264, 3209, 25262, 13, 400, 264, 1151, 636, 281, 23221, 300, 11, 50792], "temperature": 0.0, "avg_logprob": -0.20224792162577312, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.007973437197506428}, {"id": 213, "seek": 134404, "start": 1353.1599999999999, "end": 1360.44, "text": " further, is look at one of the network requests. Let me... Not this one. I want to find a network", "tokens": [50820, 3052, 11, 307, 574, 412, 472, 295, 264, 3209, 12475, 13, 961, 385, 485, 1726, 341, 472, 13, 286, 528, 281, 915, 257, 3209, 51184], "temperature": 0.0, "avg_logprob": -0.20224792162577312, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.007973437197506428}, {"id": 214, "seek": 134404, "start": 1360.44, "end": 1368.2, "text": " request from the deftools with the URL. And you can see that it... Some high-level stats, and you", "tokens": [51184, 5308, 490, 264, 368, 844, 29298, 365, 264, 12905, 13, 400, 291, 393, 536, 300, 309, 485, 2188, 1090, 12, 12418, 18152, 11, 293, 291, 51572], "temperature": 0.0, "avg_logprob": -0.20224792162577312, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.007973437197506428}, {"id": 215, "seek": 134404, "start": 1368.2, "end": 1373.96, "text": " can see where it fits with other stuff. Psyllium shots also help. But then I can search by this", "tokens": [51572, 393, 536, 689, 309, 9001, 365, 661, 1507, 13, 430, 3187, 285, 2197, 8305, 611, 854, 13, 583, 550, 286, 393, 3164, 538, 341, 51860], "temperature": 0.0, "avg_logprob": -0.20224792162577312, "compression_ratio": 1.7607843137254902, "no_speech_prob": 0.007973437197506428}, {"id": 216, "seek": 137396, "start": 1373.96, "end": 1383.4, "text": " URL, and I can also find the request ID and find all of the events, the low-level events that", "tokens": [50364, 12905, 11, 293, 286, 393, 611, 915, 264, 5308, 7348, 293, 915, 439, 295, 264, 3931, 11, 264, 2295, 12, 12418, 3931, 300, 50836], "temperature": 0.0, "avg_logprob": -0.12796863913536072, "compression_ratio": 1.7336244541484715, "no_speech_prob": 0.007640677038580179}, {"id": 217, "seek": 137396, "start": 1383.4, "end": 1388.6000000000001, "text": " Chrome is tracing has actually meted. So all of the information about this network request is there.", "tokens": [50836, 15327, 307, 25262, 575, 767, 1131, 292, 13, 407, 439, 295, 264, 1589, 466, 341, 3209, 5308, 307, 456, 13, 51096], "temperature": 0.0, "avg_logprob": -0.12796863913536072, "compression_ratio": 1.7336244541484715, "no_speech_prob": 0.007640677038580179}, {"id": 218, "seek": 137396, "start": 1388.6000000000001, "end": 1394.6000000000001, "text": " So if you can be bothered, you can actually go and correlate and go to all of these specific events", "tokens": [51096, 407, 498, 291, 393, 312, 22996, 11, 291, 393, 767, 352, 293, 48742, 293, 352, 281, 439, 295, 613, 2685, 3931, 51396], "temperature": 0.0, "avg_logprob": -0.12796863913536072, "compression_ratio": 1.7336244541484715, "no_speech_prob": 0.007640677038580179}, {"id": 219, "seek": 137396, "start": 1394.6000000000001, "end": 1401.16, "text": " and correlate them and reconstruct the same level, high-level takeaways. But it's going to be a little", "tokens": [51396, 293, 48742, 552, 293, 31499, 264, 912, 1496, 11, 1090, 12, 12418, 45584, 13, 583, 309, 311, 516, 281, 312, 257, 707, 51724], "temperature": 0.0, "avg_logprob": -0.12796863913536072, "compression_ratio": 1.7336244541484715, "no_speech_prob": 0.007640677038580179}, {"id": 220, "seek": 140116, "start": 1401.16, "end": 1408.0400000000002, "text": " bit slower, a little bit less useful, and you won't actually be using it that much yourself, probably.", "tokens": [50364, 857, 14009, 11, 257, 707, 857, 1570, 4420, 11, 293, 291, 1582, 380, 767, 312, 1228, 309, 300, 709, 1803, 11, 1391, 13, 50708], "temperature": 0.0, "avg_logprob": -0.22261817753314972, "compression_ratio": 1.1546391752577319, "no_speech_prob": 0.021189961582422256}, {"id": 221, "seek": 140116, "start": 1408.8400000000001, "end": 1412.0400000000002, "text": " So, yeah.", "tokens": [50748, 407, 11, 1338, 13, 50908], "temperature": 0.0, "avg_logprob": -0.22261817753314972, "compression_ratio": 1.1546391752577319, "no_speech_prob": 0.021189961582422256}], "language": "en"}