{"text": " Okay, next up we have Peter telling us about vector search in modern databases. Okay. Well, hello everyone. My name is Peter Zaitsev. I was supposed to speak here together with Sergey Nikolayev, who is actually a much better expert in this space, but unfortunately he couldn't get a visa. So, guess what? You are stuck with me. And we are going to talk about the vector search. How many of you are familiar with the vector searches? Oh, well, that's a good number of hands. Some not, so that's a fun audience to have. Let me maybe start with ruining the suspense and kind of showing the highlight what is a state or a vector search in a variety of databases. And I think what is interesting happening here is what we have, A, the number of new databases started in the last few years, which is specifically focused on vector search and related applications. And then also at the same time we can see a lot of mainstream databases have added support for vector search. You can see that starting back in 2019, which is actually relatively recently and relatively quickly. I think that is very interesting because databases are often rather conservative, kind of relatively slow moving product. And what that reminds me is something what we saw with Jason. We saw databases like MongoDB came and really got a lot of developers' hearts and minds. And then later on, Jason support came to pretty much every major database out there and was also added as SQL standard. Now, what is their unfortunate mission here? What you can see is the MySQL, right? Well, you can say, well, MySQL is now owned by Oracle, which is a big, fast, slow moving corporation. Right, so they're not doing that. But there is another problem is what with MySQL, it's actually exist with a heat wave solution, which is cloud-only Oracle's MySQL variant, right? And it's, I think, not very clear to what extent it will come in the MySQL proper. At the same time, MariaDB is working on solution in the MySQL space. The planet scale announced what we are going to implement vector search. So if not in the MySQL community itself, it will come in some variants, right? And obviously, if you look at the PostgreSQL, I think that's always wonderful ecosystem, right? So something has done the process with like a multiple way of doing this stuff, and there have been number of vector search extensions created, but the PG vector seems to be one which is getting the most support and most of attention those days. Now, what is also interesting, what we can see with vector search being so hot, right, with AI, and some databases like Elastic in this case here, they are pretty much focusing, calling themselves the vector search database first, right, and the full-text search database second, right? So I think for me that was very interesting to see that change. Well, anyway, with kind of a big picture out of the way, let's talk a little bit about the vectors and vector search, right? And why suddenly this becomes kind of important those days. Now, if you, Leon, I don't know, let's say, is it like a high school algebra or something else, right? You probably know what the vectors are, right? We can think about 2D, 3D space, that's all very clear. But also if we think about that, we can think about vector as something as represent colors, right? As in a software engineer, they probably know like well, colors, this is red, green, blue, right? We often encode that in one byte each, right? And that actually can be seen as a vector in color space, right? And we also can think about similarity of colors as similarity between the different vectors. Now, if you think about the vectors, there are actually a number of different approaches how you can think about what vectors are being similar and the same, right? Like there is, like, Euclidean distance, but if you think what is the most common this case is a cosine similarity, right? For example, if you go and ask our, you know, the third leaders in the eye space, open the eye, say, hey guys, then I'm using your API, what you suggest as a distance between vectors, they would suggest you to use their cosine similarity. Now, let's talk a little bit about the history, right, and how they have been using the database and specifically in their information retrieval space, right? Vectors are actually not something which is suddenly became new, right, as you may think, a vector search database, right? If you think about the full-text search application specifically, there have been vectors used for a long time, right? For example, if you, one approach would be to look at the sparse vector and say, hey, we have a document, we can actually look at all the words we have in the dictionary and let's say state a frequency of that word in that vector, and then if you want to compute the similarity between two different documents, well, we can essentially look at cosine similarity between those two kind of massive sparse vectors, right? That was something which exists for, again, very, very long time. You can find that in, you know, Lucene, Elastic, Libraries, and so on and so forth. Now, if you think about what we use in the vector search for much more is we are looking at more of a dense vector, which are called vector embeddings, right? Which are different, right? What is interesting in those kind of sparse vectors, like also referred as a bag of words, we can actually think about the every dimension in a vector as a mean something, right? We can say, oh, this dimension means if a word, you know, cat was seen in a document and how many times, right, or a relative frequency, right? Then we compute the dense vector, right, or embeddings, what that means is we take a document, right, we learn that from the model which generates those embeddings, and then we don't really understand very well what those different bits mean, right? What we know, though, is what the similar documents should be close by, right, how this system works. Or it doesn't have to be even a document, right? Like if you think about their, let's say, image recognition process, right? I can train the system, let's say, you know, faces of a bunch of people, though I know it's kind of totally legal in Europe, but let's say imagine we are in China and we are going to do that, right? Then we can look at, you know, vector embeddings, computer of somebody face, right, and look at what is a people, people in the database it looks the most like, right? That should give us, give us the closest much. So here is also something interesting. This is embeddings which are computed for single word documents, right, using one of the open source frameworks by Glove model by the guy called Jay Alamard, right? And what is interesting in this case is what we get, you know, sort of, you know, cardinality, we see, you know, bunch of words we don't really quite know, right, what those different dimensions means, right? And it's kind of like very common in AI, right? We know those things work, but we can't really figure out how exactly this works, right? And then you can think about and rationalize about what those things could be, like for example, all of those things have time to do with humans besides water, right? And then you can see there is those, you know, you see like a straight blue line which is blue for everybody by that. They say, well, maybe that is something which is related to humans, right? Or we can see something else, right, like a king and queen, right? They also have a lot of similarity. They may say, well, you know what, AI, we don't really exactly know how, but it may have something to, you know, think about their royalty. Okay. So if you think about their vector search, right, in a nutshell, we have vectors. That typically will be dense vectors which came from the AI applications. That would be some embeddings which in some database systems, right, they are, they focus on supporting operations with those embedding systems, right? For example, postgres, PgVector, it's just say, hey guys, I don't know how you compute those vectors. That is not our problem, right? We just going to help you to operate FM. Some of the more advanced features they may, they vector databases, they also may support creating embeddings. Maybe even, you know, from the database itself, right, especially in the cloud database, being able to, you know, call open AI's API in the background to compute the, you know, embeddings for you. So anyway, let's talk more about the technology, how does that work, right? So what exactly operations do we typically see in the vector search applications, right? Well, typically we do have a bunch of vector store, we have a vector on input, right, and we are looking to find a vector which is closest to it. Right? Through the distance we want to define vector search is a cosine, cosine distance. Right? Now, if you look at this problem, if you, of course, you can just, you know, as with about everything, right, you can just scan all the data and find the closest vectors, right? And that is, you know, fantastic way you can do it, you can do it exact, but it is also very slow, right? So that means why it's not used in practice instead, using special index structures, right? This HN SW, right, that seems to be the most popular algorithm, right, which industry seems to be coalescing about. And I think what is important to note about that is compared, like, different from many other things in database, this index is not exact, right? So it gives us rather, well, good accuracy, but it does not guarantee what that always will give you the, you know, let's say the closest vector when you ask it, right? And if you are familiar with database, you can say, ooh, that looks strange. But in AI applications, how those vectors are computed, right, they are not really exact to begin with, right? So these are quite usable. Okay. So let's talk about those solutions, right, what we are really using this for, right? As I mentioned, the most common features in this case will be found in your nearest. There are some other more kind of global operations in this case, which can be used in terms of clustering the data or classification, which is supported by advanced features, advanced systems. Okay. Let me show maybe in this case a little bit of example. And here we'll use their, an interesting way, we'll use their multi-core search system, right, that's where one of the search is created for. But we'll connect to that through a MySQL protocol, right, that's what it's for. And we can see what we can go ahead and let's say, create the table, as you can see, right? We've defined in the flow of vector, in the traditional database type, but something what this engine support, right? And you would see typically vectors support some sort of vector store functionality, right? And then we can find their distance between the given vector we have as well as the vectors in the database, right? And it can give us the information, right? So what we actually had in this case is their different images, which would run through creating embedding for them. And then you can see what, what was that? Yeah, I think that was like an image of a bag, right, which was saying, hey, you know what, it's much more similar to a yellow bag compared to their, to a white bag, right? Which you can, that is a kind of pretty common, what we have. Okay. I mentioned what, when you speak about embedded computation, right? There, if you look at especially non-vector databases, not specialized databases like Postgres where we would say, hey guys, you guys can use external encoder, but they don't see that as a database problem, right? You process some information out there, your favorite find, external API, or use some local open source model, that's fine. Though there are some libraries, right, with Microsoft last time, a library, right, they are being added. And I think over time, of course, if our desire to enable developers simplicity, we'll see more of a direct support. Now, something else I think what is interesting about their embedding and the information retrieval tasks specifically. I think what is very interesting, if you look at the search applications, for years, there have been a lot of time spent about sort of like a hard coding, like in the engineering, the structure of the language, defining synonyms, defining antonyms, right, and so on and so forth, right, and so we can run our search quality. The other approach, though, is we can use the AI, right, for this approach, right, so we can look at the matching document through embeddings, right, and that really allows to avoid a lot of that thing which has to be manual processing if a good quality. But what is an interesting there, though, is this new generation AI search may not be best, and also it may not be the most effective, right, because if you think about that, if I have like a lot of a document in my database, right, billions, right, then actually the search can be relatively expensive. So one of the approaches which is used is dual approaches, right, and you can say, well, we may be getting, you know, like a top thousand documents or something through, you know, like a legacy kind of frequency-based search methods, right, and then we can use AI to rank those, right, and you can see what that's sort of like a combination that's like a last-second, like a Vespa hybrid, right, it shows better, right, on many benchmarks. Okay, well, oh, close, thank you, is very usable, both in information retrieval tasks as well as many other applications, and what we see also what the vector support in databases in relatively early stages, right, it just happened implemented in the last few years, right, I would say what the current implementation is related to basics, and we would see a lot more of the features, right, as we kind of figure out, right, what we actually want to empower us to go into the continuing innovation in this data structures to support us in the fast vector applications, right, and as well improve accuracy. Well, that's all I have, right, you had some, yes, the gentleman was about to show me zero minutes, yeah, and if there is any questions, I would be happy to try to answer. Any questions? Yes. Hi. Okay, okay, so given the fact that so you say that databases are going to transition from just supporting externally modelled embeddings to generating internally, but given the fact that we see a lot of many advanced models that generate embeddings, for example, the GPT, the GPT is a machine that basically takes entire concepts, translates very efficiently into embeddings and then outputs also, and is the standard like you provide a single external embedding generator model compared to a traditional BWAC, and then everyone can just benefit from an on par model. Well, yes, yeah, absolutely. Well, what I'm saying in this case, I think it's being supported, right, maybe in different things, right, I'm not saying, oh, you know what, you should expect postgres incorporate inside that over possible models, right, I think I think the same would be say like if, let's say, Python support those things, right, that just means it's easier to do from a Python standpoint, right, so now if you think about this case, if I want to generate embedding, even for data I have in the database, I kind of have to do that, that externally, right, what I'm saying is, well, get some, you know, fun, was us by talking externally, not creative, so that is what I'm speaking about, make sense? Thank you. Any other questions? Thanks. Thanks for the talk. As a database developer, how do you deal with very rapidly moving target of both algorithms and storage formats? In the sense, do you deprecate rapidly? Because if today you have a KNN function that has this, some certain API, and a month later, you know, ML is very rapidly moving target, you have a better KNN function, and same goes with densely, dense format. Well, yeah, I think that is a good point. Now, I think what is interesting what you're saying, right, you know, thing on one extent it's always interesting and then you have a, you know, interesting and then you have this, the industry in like an early stage, because on one extent things are changing quickly, on the other hand, often people implement something, they put it in production, it's good enough, and the fact that that kind of is a better state of art out there, that doesn't mean what they want to change, right, and that means, like at least in the database world, right, you always have to say, well, you know what, there are certain things you would love to kill, but actually some very big corporations already deployed in production and they're not freaking changing that in the next 10 years, right. So, reality what that's going to be, right, I think is that evolution on that side, but then we'll still have to have a competitive I think if you look at, I mentioned like a postgres, like a PG vector extensions, well, you support like a whole bunch of different options in this case, right, so yeah, I think that's what we'll expect. Okay, hi, yeah, can you go back to the embeddings slide where you showed the similarity between Queen and Queen, I think, the similarities between Queen and King, I think, or was like the first, Oh, this one? Yeah, yeah, yeah, yeah, yeah, you mentioned you use Ada, I think. What do you use? Chart GPT embeddings. No, so yeah, this one is actually, you know, a global model, right, that was one of the open source models, right. Ah, okay, okay, yeah. But I mean, in this case, I think that is just like an example, right, I think what you wanted to look in this case to visualize, right, I mean, when you say, hey guys, you generate those kind of embeddings, and they do not particularly mean anything, right, but can you just plot them, you know, as we plot, let's say, you know, DNA of a frog and a fish, right, can we see something maybe in this case, right. So that was the case to do here, right, to visualize how particular embedding generation model. Yeah, no, no, I find it really helpful also for, I find it really helpful also for like my future students, because like it really grasped the accents of embedding and similarities, I think. Yeah, I mean, again, like in this case, that is just to visualize to show people what that things look like, right, and my point I was trying to make is, on one extent, we cannot really state what exactly are those dimensions, what it corresponds to, right, but as a human, we can, you know, try to rationalize over, it seems to be like, you know, something. Yeah, yeah, also. You know, something there, right. Also because other, the embedding of OpenIE has like 100, no, 1000 and a half features. Well, that's right. Yes. Really difficult to like. That's right. That's right. So like in this case, that was specifically kind of cut, right, to have any less features, right. Yes, it's because, you know, 1500 or like, well, like, you know, 3000 tried for large. Yeah, that would be too many to display. Okay. Thank you. Thank you. you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.0, "text": " Okay, next up we have Peter telling us about vector search in modern databases.", "tokens": [50364, 1033, 11, 958, 493, 321, 362, 6508, 3585, 505, 466, 8062, 3164, 294, 4363, 22380, 13, 51164], "temperature": 0.0, "avg_logprob": -0.3199660619099935, "compression_ratio": 1.2761194029850746, "no_speech_prob": 0.5360761284828186}, {"id": 1, "seek": 0, "start": 16.0, "end": 27.0, "text": " Okay. Well, hello everyone. My name is Peter Zaitsev. I was supposed to speak here together", "tokens": [51164, 1033, 13, 1042, 11, 7751, 1518, 13, 1222, 1315, 307, 6508, 1176, 1001, 405, 85, 13, 286, 390, 3442, 281, 1710, 510, 1214, 51714], "temperature": 0.0, "avg_logprob": -0.3199660619099935, "compression_ratio": 1.2761194029850746, "no_speech_prob": 0.5360761284828186}, {"id": 2, "seek": 2700, "start": 27.0, "end": 35.0, "text": " with Sergey Nikolayev, who is actually a much better expert in this space, but unfortunately he couldn't get a visa.", "tokens": [50364, 365, 49238, 13969, 401, 320, 13379, 11, 567, 307, 767, 257, 709, 1101, 5844, 294, 341, 1901, 11, 457, 7015, 415, 2809, 380, 483, 257, 18589, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1576361453279536, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.13922351598739624}, {"id": 3, "seek": 2700, "start": 35.0, "end": 39.0, "text": " So, guess what? You are stuck with me.", "tokens": [50764, 407, 11, 2041, 437, 30, 509, 366, 5541, 365, 385, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1576361453279536, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.13922351598739624}, {"id": 4, "seek": 2700, "start": 39.0, "end": 47.0, "text": " And we are going to talk about the vector search. How many of you are familiar with the vector searches?", "tokens": [50964, 400, 321, 366, 516, 281, 751, 466, 264, 8062, 3164, 13, 1012, 867, 295, 291, 366, 4963, 365, 264, 8062, 26701, 30, 51364], "temperature": 0.0, "avg_logprob": -0.1576361453279536, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.13922351598739624}, {"id": 5, "seek": 2700, "start": 47.0, "end": 53.0, "text": " Oh, well, that's a good number of hands. Some not, so that's a fun audience to have.", "tokens": [51364, 876, 11, 731, 11, 300, 311, 257, 665, 1230, 295, 2377, 13, 2188, 406, 11, 370, 300, 311, 257, 1019, 4034, 281, 362, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1576361453279536, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.13922351598739624}, {"id": 6, "seek": 5300, "start": 53.0, "end": 66.0, "text": " Let me maybe start with ruining the suspense and kind of showing the highlight what is a state or a vector search in a variety of databases.", "tokens": [50364, 961, 385, 1310, 722, 365, 38938, 264, 47803, 293, 733, 295, 4099, 264, 5078, 437, 307, 257, 1785, 420, 257, 8062, 3164, 294, 257, 5673, 295, 22380, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1407157241321001, "compression_ratio": 1.5647058823529412, "no_speech_prob": 0.06081360578536987}, {"id": 7, "seek": 5300, "start": 66.0, "end": 77.0, "text": " And I think what is interesting happening here is what we have, A, the number of new databases started in the last few years,", "tokens": [51014, 400, 286, 519, 437, 307, 1880, 2737, 510, 307, 437, 321, 362, 11, 316, 11, 264, 1230, 295, 777, 22380, 1409, 294, 264, 1036, 1326, 924, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1407157241321001, "compression_ratio": 1.5647058823529412, "no_speech_prob": 0.06081360578536987}, {"id": 8, "seek": 7700, "start": 77.0, "end": 83.0, "text": " which is specifically focused on vector search and related applications.", "tokens": [50364, 597, 307, 4682, 5178, 322, 8062, 3164, 293, 4077, 5821, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10002067533590026, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.07160738855600357}, {"id": 9, "seek": 7700, "start": 83.0, "end": 94.0, "text": " And then also at the same time we can see a lot of mainstream databases have added support for vector search.", "tokens": [50664, 400, 550, 611, 412, 264, 912, 565, 321, 393, 536, 257, 688, 295, 15960, 22380, 362, 3869, 1406, 337, 8062, 3164, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10002067533590026, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.07160738855600357}, {"id": 10, "seek": 7700, "start": 94.0, "end": 102.0, "text": " You can see that starting back in 2019, which is actually relatively recently and relatively quickly.", "tokens": [51214, 509, 393, 536, 300, 2891, 646, 294, 6071, 11, 597, 307, 767, 7226, 3938, 293, 7226, 2661, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10002067533590026, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.07160738855600357}, {"id": 11, "seek": 10200, "start": 102.0, "end": 111.0, "text": " I think that is very interesting because databases are often rather conservative, kind of relatively slow moving product.", "tokens": [50364, 286, 519, 300, 307, 588, 1880, 570, 22380, 366, 2049, 2831, 13780, 11, 733, 295, 7226, 2964, 2684, 1674, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1622559388478597, "compression_ratio": 1.4702702702702704, "no_speech_prob": 0.1888767033815384}, {"id": 12, "seek": 10200, "start": 111.0, "end": 117.0, "text": " And what that reminds me is something what we saw with Jason.", "tokens": [50814, 400, 437, 300, 12025, 385, 307, 746, 437, 321, 1866, 365, 11181, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1622559388478597, "compression_ratio": 1.4702702702702704, "no_speech_prob": 0.1888767033815384}, {"id": 13, "seek": 10200, "start": 117.0, "end": 127.0, "text": " We saw databases like MongoDB came and really got a lot of developers' hearts and minds.", "tokens": [51114, 492, 1866, 22380, 411, 48380, 27735, 1361, 293, 534, 658, 257, 688, 295, 8849, 6, 8852, 293, 9634, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1622559388478597, "compression_ratio": 1.4702702702702704, "no_speech_prob": 0.1888767033815384}, {"id": 14, "seek": 12700, "start": 127.0, "end": 138.0, "text": " And then later on, Jason support came to pretty much every major database out there and was also added as SQL standard.", "tokens": [50364, 400, 550, 1780, 322, 11, 11181, 1406, 1361, 281, 1238, 709, 633, 2563, 8149, 484, 456, 293, 390, 611, 3869, 382, 19200, 3832, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15138286880300014, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.24119487404823303}, {"id": 15, "seek": 12700, "start": 138.0, "end": 147.0, "text": " Now, what is their unfortunate mission here? What you can see is the MySQL, right?", "tokens": [50914, 823, 11, 437, 307, 641, 17843, 4447, 510, 30, 708, 291, 393, 536, 307, 264, 1222, 39934, 11, 558, 30, 51364], "temperature": 0.0, "avg_logprob": -0.15138286880300014, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.24119487404823303}, {"id": 16, "seek": 12700, "start": 147.0, "end": 154.0, "text": " Well, you can say, well, MySQL is now owned by Oracle, which is a big, fast, slow moving corporation.", "tokens": [51364, 1042, 11, 291, 393, 584, 11, 731, 11, 1222, 39934, 307, 586, 11684, 538, 25654, 11, 597, 307, 257, 955, 11, 2370, 11, 2964, 2684, 22197, 13, 51714], "temperature": 0.0, "avg_logprob": -0.15138286880300014, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.24119487404823303}, {"id": 17, "seek": 15400, "start": 154.0, "end": 163.0, "text": " Right, so they're not doing that. But there is another problem is what with MySQL, it's actually exist with a heat wave solution,", "tokens": [50364, 1779, 11, 370, 436, 434, 406, 884, 300, 13, 583, 456, 307, 1071, 1154, 307, 437, 365, 1222, 39934, 11, 309, 311, 767, 2514, 365, 257, 3738, 5772, 3827, 11, 50814], "temperature": 0.0, "avg_logprob": -0.18613045836148195, "compression_ratio": 1.413978494623656, "no_speech_prob": 0.18716607987880707}, {"id": 18, "seek": 15400, "start": 163.0, "end": 168.0, "text": " which is cloud-only Oracle's MySQL variant, right?", "tokens": [50814, 597, 307, 4588, 12, 25202, 25654, 311, 1222, 39934, 17501, 11, 558, 30, 51064], "temperature": 0.0, "avg_logprob": -0.18613045836148195, "compression_ratio": 1.413978494623656, "no_speech_prob": 0.18716607987880707}, {"id": 19, "seek": 15400, "start": 168.0, "end": 175.0, "text": " And it's, I think, not very clear to what extent it will come in the MySQL proper.", "tokens": [51064, 400, 309, 311, 11, 286, 519, 11, 406, 588, 1850, 281, 437, 8396, 309, 486, 808, 294, 264, 1222, 39934, 2296, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18613045836148195, "compression_ratio": 1.413978494623656, "no_speech_prob": 0.18716607987880707}, {"id": 20, "seek": 17500, "start": 175.0, "end": 181.0, "text": " At the same time, MariaDB is working on solution in the MySQL space.", "tokens": [50364, 1711, 264, 912, 565, 11, 12734, 27735, 307, 1364, 322, 3827, 294, 264, 1222, 39934, 1901, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12830549404944902, "compression_ratio": 1.456221198156682, "no_speech_prob": 0.3026839792728424}, {"id": 21, "seek": 17500, "start": 181.0, "end": 186.0, "text": " The planet scale announced what we are going to implement vector search.", "tokens": [50664, 440, 5054, 4373, 7548, 437, 321, 366, 516, 281, 4445, 8062, 3164, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12830549404944902, "compression_ratio": 1.456221198156682, "no_speech_prob": 0.3026839792728424}, {"id": 22, "seek": 17500, "start": 186.0, "end": 193.0, "text": " So if not in the MySQL community itself, it will come in some variants, right?", "tokens": [50914, 407, 498, 406, 294, 264, 1222, 39934, 1768, 2564, 11, 309, 486, 808, 294, 512, 21669, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.12830549404944902, "compression_ratio": 1.456221198156682, "no_speech_prob": 0.3026839792728424}, {"id": 23, "seek": 17500, "start": 193.0, "end": 199.0, "text": " And obviously, if you look at the PostgreSQL, I think that's always wonderful ecosystem, right?", "tokens": [51264, 400, 2745, 11, 498, 291, 574, 412, 264, 10223, 33248, 39934, 11, 286, 519, 300, 311, 1009, 3715, 11311, 11, 558, 30, 51564], "temperature": 0.0, "avg_logprob": -0.12830549404944902, "compression_ratio": 1.456221198156682, "no_speech_prob": 0.3026839792728424}, {"id": 24, "seek": 19900, "start": 199.0, "end": 203.0, "text": " So something has done the process with like a multiple way of doing this stuff,", "tokens": [50364, 407, 746, 575, 1096, 264, 1399, 365, 411, 257, 3866, 636, 295, 884, 341, 1507, 11, 50564], "temperature": 0.0, "avg_logprob": -0.23764958793734325, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.02615218423306942}, {"id": 25, "seek": 19900, "start": 203.0, "end": 212.0, "text": " and there have been number of vector search extensions created, but the PG vector seems to be one which is getting the most support", "tokens": [50564, 293, 456, 362, 668, 1230, 295, 8062, 3164, 25129, 2942, 11, 457, 264, 40975, 8062, 2544, 281, 312, 472, 597, 307, 1242, 264, 881, 1406, 51014], "temperature": 0.0, "avg_logprob": -0.23764958793734325, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.02615218423306942}, {"id": 26, "seek": 19900, "start": 212.0, "end": 217.0, "text": " and most of attention those days.", "tokens": [51014, 293, 881, 295, 3202, 729, 1708, 13, 51264], "temperature": 0.0, "avg_logprob": -0.23764958793734325, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.02615218423306942}, {"id": 27, "seek": 19900, "start": 217.0, "end": 224.0, "text": " Now, what is also interesting, what we can see with vector search being so hot, right, with AI,", "tokens": [51264, 823, 11, 437, 307, 611, 1880, 11, 437, 321, 393, 536, 365, 8062, 3164, 885, 370, 2368, 11, 558, 11, 365, 7318, 11, 51614], "temperature": 0.0, "avg_logprob": -0.23764958793734325, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.02615218423306942}, {"id": 28, "seek": 22400, "start": 224.0, "end": 234.0, "text": " and some databases like Elastic in this case here, they are pretty much focusing, calling themselves the vector search database first, right,", "tokens": [50364, 293, 512, 22380, 411, 2699, 2750, 294, 341, 1389, 510, 11, 436, 366, 1238, 709, 8416, 11, 5141, 2969, 264, 8062, 3164, 8149, 700, 11, 558, 11, 50864], "temperature": 0.0, "avg_logprob": -0.16522587900576385, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.014776519499719143}, {"id": 29, "seek": 22400, "start": 234.0, "end": 237.0, "text": " and the full-text search database second, right?", "tokens": [50864, 293, 264, 1577, 12, 25111, 3164, 8149, 1150, 11, 558, 30, 51014], "temperature": 0.0, "avg_logprob": -0.16522587900576385, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.014776519499719143}, {"id": 30, "seek": 22400, "start": 237.0, "end": 242.0, "text": " So I think for me that was very interesting to see that change.", "tokens": [51014, 407, 286, 519, 337, 385, 300, 390, 588, 1880, 281, 536, 300, 1319, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16522587900576385, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.014776519499719143}, {"id": 31, "seek": 22400, "start": 242.0, "end": 251.0, "text": " Well, anyway, with kind of a big picture out of the way, let's talk a little bit about the vectors and vector search, right?", "tokens": [51264, 1042, 11, 4033, 11, 365, 733, 295, 257, 955, 3036, 484, 295, 264, 636, 11, 718, 311, 751, 257, 707, 857, 466, 264, 18875, 293, 8062, 3164, 11, 558, 30, 51714], "temperature": 0.0, "avg_logprob": -0.16522587900576385, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.014776519499719143}, {"id": 32, "seek": 25100, "start": 251.0, "end": 256.0, "text": " And why suddenly this becomes kind of important those days.", "tokens": [50364, 400, 983, 5800, 341, 3643, 733, 295, 1021, 729, 1708, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10849444389343262, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.010655366815626621}, {"id": 33, "seek": 25100, "start": 256.0, "end": 262.0, "text": " Now, if you, Leon, I don't know, let's say, is it like a high school algebra or something else, right?", "tokens": [50614, 823, 11, 498, 291, 11, 13244, 11, 286, 500, 380, 458, 11, 718, 311, 584, 11, 307, 309, 411, 257, 1090, 1395, 21989, 420, 746, 1646, 11, 558, 30, 50914], "temperature": 0.0, "avg_logprob": -0.10849444389343262, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.010655366815626621}, {"id": 34, "seek": 25100, "start": 262.0, "end": 265.0, "text": " You probably know what the vectors are, right?", "tokens": [50914, 509, 1391, 458, 437, 264, 18875, 366, 11, 558, 30, 51064], "temperature": 0.0, "avg_logprob": -0.10849444389343262, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.010655366815626621}, {"id": 35, "seek": 25100, "start": 265.0, "end": 270.0, "text": " We can think about 2D, 3D space, that's all very clear.", "tokens": [51064, 492, 393, 519, 466, 568, 35, 11, 805, 35, 1901, 11, 300, 311, 439, 588, 1850, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10849444389343262, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.010655366815626621}, {"id": 36, "seek": 25100, "start": 270.0, "end": 277.0, "text": " But also if we think about that, we can think about vector as something as represent colors, right?", "tokens": [51314, 583, 611, 498, 321, 519, 466, 300, 11, 321, 393, 519, 466, 8062, 382, 746, 382, 2906, 4577, 11, 558, 30, 51664], "temperature": 0.0, "avg_logprob": -0.10849444389343262, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.010655366815626621}, {"id": 37, "seek": 27700, "start": 277.0, "end": 285.0, "text": " As in a software engineer, they probably know like well, colors, this is red, green, blue, right?", "tokens": [50364, 1018, 294, 257, 4722, 11403, 11, 436, 1391, 458, 411, 731, 11, 4577, 11, 341, 307, 2182, 11, 3092, 11, 3344, 11, 558, 30, 50764], "temperature": 0.0, "avg_logprob": -0.12606198447091238, "compression_ratio": 1.597883597883598, "no_speech_prob": 0.009455813094973564}, {"id": 38, "seek": 27700, "start": 285.0, "end": 290.0, "text": " We often encode that in one byte each, right?", "tokens": [50764, 492, 2049, 2058, 1429, 300, 294, 472, 40846, 1184, 11, 558, 30, 51014], "temperature": 0.0, "avg_logprob": -0.12606198447091238, "compression_ratio": 1.597883597883598, "no_speech_prob": 0.009455813094973564}, {"id": 39, "seek": 27700, "start": 290.0, "end": 295.0, "text": " And that actually can be seen as a vector in color space, right?", "tokens": [51014, 400, 300, 767, 393, 312, 1612, 382, 257, 8062, 294, 2017, 1901, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.12606198447091238, "compression_ratio": 1.597883597883598, "no_speech_prob": 0.009455813094973564}, {"id": 40, "seek": 27700, "start": 295.0, "end": 304.0, "text": " And we also can think about similarity of colors as similarity between the different vectors.", "tokens": [51264, 400, 321, 611, 393, 519, 466, 32194, 295, 4577, 382, 32194, 1296, 264, 819, 18875, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12606198447091238, "compression_ratio": 1.597883597883598, "no_speech_prob": 0.009455813094973564}, {"id": 41, "seek": 30400, "start": 304.0, "end": 323.0, "text": " Now, if you think about the vectors, there are actually a number of different approaches how you can think about what vectors are being similar and the same, right?", "tokens": [50364, 823, 11, 498, 291, 519, 466, 264, 18875, 11, 456, 366, 767, 257, 1230, 295, 819, 11587, 577, 291, 393, 519, 466, 437, 18875, 366, 885, 2531, 293, 264, 912, 11, 558, 30, 51314], "temperature": 0.0, "avg_logprob": -0.12108191928348026, "compression_ratio": 1.3553719008264462, "no_speech_prob": 0.023545773699879646}, {"id": 42, "seek": 32300, "start": 323.0, "end": 335.0, "text": " Like there is, like, Euclidean distance, but if you think what is the most common this case is a cosine similarity, right?", "tokens": [50364, 1743, 456, 307, 11, 411, 11, 462, 1311, 31264, 282, 4560, 11, 457, 498, 291, 519, 437, 307, 264, 881, 2689, 341, 1389, 307, 257, 23565, 32194, 11, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.2035868135217118, "compression_ratio": 1.449438202247191, "no_speech_prob": 0.3206801414489746}, {"id": 43, "seek": 32300, "start": 335.0, "end": 347.0, "text": " For example, if you go and ask our, you know, the third leaders in the eye space, open the eye, say, hey guys, then I'm using your API,", "tokens": [50964, 1171, 1365, 11, 498, 291, 352, 293, 1029, 527, 11, 291, 458, 11, 264, 2636, 3523, 294, 264, 3313, 1901, 11, 1269, 264, 3313, 11, 584, 11, 4177, 1074, 11, 550, 286, 478, 1228, 428, 9362, 11, 51564], "temperature": 0.0, "avg_logprob": -0.2035868135217118, "compression_ratio": 1.449438202247191, "no_speech_prob": 0.3206801414489746}, {"id": 44, "seek": 34700, "start": 347.0, "end": 362.0, "text": " what you suggest as a distance between vectors, they would suggest you to use their cosine similarity.", "tokens": [50364, 437, 291, 3402, 382, 257, 4560, 1296, 18875, 11, 436, 576, 3402, 291, 281, 764, 641, 23565, 32194, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12148148438026166, "compression_ratio": 1.5, "no_speech_prob": 0.012220734730362892}, {"id": 45, "seek": 34700, "start": 362.0, "end": 374.0, "text": " Now, let's talk a little bit about the history, right, and how they have been using the database and specifically in their information retrieval space, right?", "tokens": [51114, 823, 11, 718, 311, 751, 257, 707, 857, 466, 264, 2503, 11, 558, 11, 293, 577, 436, 362, 668, 1228, 264, 8149, 293, 4682, 294, 641, 1589, 19817, 3337, 1901, 11, 558, 30, 51714], "temperature": 0.0, "avg_logprob": -0.12148148438026166, "compression_ratio": 1.5, "no_speech_prob": 0.012220734730362892}, {"id": 46, "seek": 37400, "start": 374.0, "end": 381.0, "text": " Vectors are actually not something which is suddenly became new, right, as you may think, a vector search database, right?", "tokens": [50364, 691, 557, 830, 366, 767, 406, 746, 597, 307, 5800, 3062, 777, 11, 558, 11, 382, 291, 815, 519, 11, 257, 8062, 3164, 8149, 11, 558, 30, 50714], "temperature": 0.0, "avg_logprob": -0.11357486609256628, "compression_ratio": 1.7125, "no_speech_prob": 0.017872421070933342}, {"id": 47, "seek": 37400, "start": 381.0, "end": 389.0, "text": " If you think about the full-text search application specifically, there have been vectors used for a long time, right?", "tokens": [50714, 759, 291, 519, 466, 264, 1577, 12, 25111, 3164, 3861, 4682, 11, 456, 362, 668, 18875, 1143, 337, 257, 938, 565, 11, 558, 30, 51114], "temperature": 0.0, "avg_logprob": -0.11357486609256628, "compression_ratio": 1.7125, "no_speech_prob": 0.017872421070933342}, {"id": 48, "seek": 37400, "start": 389.0, "end": 402.0, "text": " For example, if you, one approach would be to look at the sparse vector and say, hey, we have a document, we can actually look at all the words we have in the dictionary", "tokens": [51114, 1171, 1365, 11, 498, 291, 11, 472, 3109, 576, 312, 281, 574, 412, 264, 637, 11668, 8062, 293, 584, 11, 4177, 11, 321, 362, 257, 4166, 11, 321, 393, 767, 574, 412, 439, 264, 2283, 321, 362, 294, 264, 25890, 51764], "temperature": 0.0, "avg_logprob": -0.11357486609256628, "compression_ratio": 1.7125, "no_speech_prob": 0.017872421070933342}, {"id": 49, "seek": 40200, "start": 402.0, "end": 413.0, "text": " and let's say state a frequency of that word in that vector, and then if you want to compute the similarity between two different documents,", "tokens": [50364, 293, 718, 311, 584, 1785, 257, 7893, 295, 300, 1349, 294, 300, 8062, 11, 293, 550, 498, 291, 528, 281, 14722, 264, 32194, 1296, 732, 819, 8512, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09772126457907937, "compression_ratio": 1.55, "no_speech_prob": 0.03940022364258766}, {"id": 50, "seek": 40200, "start": 413.0, "end": 424.0, "text": " well, we can essentially look at cosine similarity between those two kind of massive sparse vectors, right?", "tokens": [50914, 731, 11, 321, 393, 4476, 574, 412, 23565, 32194, 1296, 729, 732, 733, 295, 5994, 637, 11668, 18875, 11, 558, 30, 51464], "temperature": 0.0, "avg_logprob": -0.09772126457907937, "compression_ratio": 1.55, "no_speech_prob": 0.03940022364258766}, {"id": 51, "seek": 42400, "start": 424.0, "end": 428.0, "text": " That was something which exists for, again, very, very long time.", "tokens": [50364, 663, 390, 746, 597, 8198, 337, 11, 797, 11, 588, 11, 588, 938, 565, 13, 50564], "temperature": 0.0, "avg_logprob": -0.20157888957432338, "compression_ratio": 1.54, "no_speech_prob": 0.02431783638894558}, {"id": 52, "seek": 42400, "start": 428.0, "end": 436.0, "text": " You can find that in, you know, Lucene, Elastic, Libraries, and so on and so forth.", "tokens": [50564, 509, 393, 915, 300, 294, 11, 291, 458, 11, 9593, 1450, 11, 2699, 2750, 11, 12006, 4889, 11, 293, 370, 322, 293, 370, 5220, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20157888957432338, "compression_ratio": 1.54, "no_speech_prob": 0.02431783638894558}, {"id": 53, "seek": 42400, "start": 436.0, "end": 451.0, "text": " Now, if you think about what we use in the vector search for much more is we are looking at more of a dense vector, which are called vector embeddings, right?", "tokens": [50964, 823, 11, 498, 291, 519, 466, 437, 321, 764, 294, 264, 8062, 3164, 337, 709, 544, 307, 321, 366, 1237, 412, 544, 295, 257, 18011, 8062, 11, 597, 366, 1219, 8062, 12240, 29432, 11, 558, 30, 51714], "temperature": 0.0, "avg_logprob": -0.20157888957432338, "compression_ratio": 1.54, "no_speech_prob": 0.02431783638894558}, {"id": 54, "seek": 45100, "start": 451.0, "end": 454.0, "text": " Which are different, right?", "tokens": [50364, 3013, 366, 819, 11, 558, 30, 50514], "temperature": 0.0, "avg_logprob": -0.10522193258458917, "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.04444785416126251}, {"id": 55, "seek": 45100, "start": 454.0, "end": 466.0, "text": " What is interesting in those kind of sparse vectors, like also referred as a bag of words, we can actually think about the every dimension in a vector as a mean something, right?", "tokens": [50514, 708, 307, 1880, 294, 729, 733, 295, 637, 11668, 18875, 11, 411, 611, 10839, 382, 257, 3411, 295, 2283, 11, 321, 393, 767, 519, 466, 264, 633, 10139, 294, 257, 8062, 382, 257, 914, 746, 11, 558, 30, 51114], "temperature": 0.0, "avg_logprob": -0.10522193258458917, "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.04444785416126251}, {"id": 56, "seek": 45100, "start": 466.0, "end": 476.0, "text": " We can say, oh, this dimension means if a word, you know, cat was seen in a document and how many times, right, or a relative frequency, right?", "tokens": [51114, 492, 393, 584, 11, 1954, 11, 341, 10139, 1355, 498, 257, 1349, 11, 291, 458, 11, 3857, 390, 1612, 294, 257, 4166, 293, 577, 867, 1413, 11, 558, 11, 420, 257, 4972, 7893, 11, 558, 30, 51614], "temperature": 0.0, "avg_logprob": -0.10522193258458917, "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.04444785416126251}, {"id": 57, "seek": 47600, "start": 476.0, "end": 491.0, "text": " Then we compute the dense vector, right, or embeddings, what that means is we take a document, right, we learn that from the model which generates those embeddings,", "tokens": [50364, 1396, 321, 14722, 264, 18011, 8062, 11, 558, 11, 420, 12240, 29432, 11, 437, 300, 1355, 307, 321, 747, 257, 4166, 11, 558, 11, 321, 1466, 300, 490, 264, 2316, 597, 23815, 729, 12240, 29432, 11, 51114], "temperature": 0.0, "avg_logprob": -0.199583371480306, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.032181281596422195}, {"id": 58, "seek": 47600, "start": 491.0, "end": 496.0, "text": " and then we don't really understand very well what those different bits mean, right?", "tokens": [51114, 293, 550, 321, 500, 380, 534, 1223, 588, 731, 437, 729, 819, 9239, 914, 11, 558, 30, 51364], "temperature": 0.0, "avg_logprob": -0.199583371480306, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.032181281596422195}, {"id": 59, "seek": 47600, "start": 496.0, "end": 503.0, "text": " What we know, though, is what the similar documents should be close by, right, how this system works.", "tokens": [51364, 708, 321, 458, 11, 1673, 11, 307, 437, 264, 2531, 8512, 820, 312, 1998, 538, 11, 558, 11, 577, 341, 1185, 1985, 13, 51714], "temperature": 0.0, "avg_logprob": -0.199583371480306, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.032181281596422195}, {"id": 60, "seek": 50300, "start": 503.0, "end": 506.0, "text": " Or it doesn't have to be even a document, right?", "tokens": [50364, 1610, 309, 1177, 380, 362, 281, 312, 754, 257, 4166, 11, 558, 30, 50514], "temperature": 0.0, "avg_logprob": -0.14576048261663888, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.1222979947924614}, {"id": 61, "seek": 50300, "start": 506.0, "end": 511.0, "text": " Like if you think about their, let's say, image recognition process, right?", "tokens": [50514, 1743, 498, 291, 519, 466, 641, 11, 718, 311, 584, 11, 3256, 11150, 1399, 11, 558, 30, 50764], "temperature": 0.0, "avg_logprob": -0.14576048261663888, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.1222979947924614}, {"id": 62, "seek": 50300, "start": 511.0, "end": 523.0, "text": " I can train the system, let's say, you know, faces of a bunch of people, though I know it's kind of totally legal in Europe, but let's say imagine we are in China and we are going to do that, right?", "tokens": [50764, 286, 393, 3847, 264, 1185, 11, 718, 311, 584, 11, 291, 458, 11, 8475, 295, 257, 3840, 295, 561, 11, 1673, 286, 458, 309, 311, 733, 295, 3879, 5089, 294, 3315, 11, 457, 718, 311, 584, 3811, 321, 366, 294, 3533, 293, 321, 366, 516, 281, 360, 300, 11, 558, 30, 51364], "temperature": 0.0, "avg_logprob": -0.14576048261663888, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.1222979947924614}, {"id": 63, "seek": 52300, "start": 523.0, "end": 534.0, "text": " Then we can look at, you know, vector embeddings, computer of somebody face, right, and look at what is a people, people in the database it looks the most like, right?", "tokens": [50364, 1396, 321, 393, 574, 412, 11, 291, 458, 11, 8062, 12240, 29432, 11, 3820, 295, 2618, 1851, 11, 558, 11, 293, 574, 412, 437, 307, 257, 561, 11, 561, 294, 264, 8149, 309, 1542, 264, 881, 411, 11, 558, 30, 50914], "temperature": 0.0, "avg_logprob": -0.14018928643428918, "compression_ratio": 1.5149700598802396, "no_speech_prob": 0.0800289511680603}, {"id": 64, "seek": 52300, "start": 534.0, "end": 540.0, "text": " That should give us, give us the closest much.", "tokens": [50914, 663, 820, 976, 505, 11, 976, 505, 264, 13699, 709, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14018928643428918, "compression_ratio": 1.5149700598802396, "no_speech_prob": 0.0800289511680603}, {"id": 65, "seek": 52300, "start": 540.0, "end": 547.0, "text": " So here is also something interesting.", "tokens": [51214, 407, 510, 307, 611, 746, 1880, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14018928643428918, "compression_ratio": 1.5149700598802396, "no_speech_prob": 0.0800289511680603}, {"id": 66, "seek": 54700, "start": 547.0, "end": 565.0, "text": " This is embeddings which are computed for single word documents, right, using one of the open source frameworks by Glove model by the guy called Jay Alamard, right?", "tokens": [50364, 639, 307, 12240, 29432, 597, 366, 40610, 337, 2167, 1349, 8512, 11, 558, 11, 1228, 472, 295, 264, 1269, 4009, 29834, 538, 10786, 303, 2316, 538, 264, 2146, 1219, 11146, 967, 335, 515, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.25265820026397706, "compression_ratio": 1.28125, "no_speech_prob": 0.05392956733703613}, {"id": 67, "seek": 56500, "start": 565.0, "end": 582.0, "text": " And what is interesting in this case is what we get, you know, sort of, you know, cardinality, we see, you know, bunch of words we don't really quite know, right, what those different dimensions means, right?", "tokens": [50364, 400, 437, 307, 1880, 294, 341, 1389, 307, 437, 321, 483, 11, 291, 458, 11, 1333, 295, 11, 291, 458, 11, 2920, 259, 1860, 11, 321, 536, 11, 291, 458, 11, 3840, 295, 2283, 321, 500, 380, 534, 1596, 458, 11, 558, 11, 437, 729, 819, 12819, 1355, 11, 558, 30, 51214], "temperature": 0.0, "avg_logprob": -0.13358014562855597, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.2812751531600952}, {"id": 68, "seek": 56500, "start": 582.0, "end": 586.0, "text": " And it's kind of like very common in AI, right?", "tokens": [51214, 400, 309, 311, 733, 295, 411, 588, 2689, 294, 7318, 11, 558, 30, 51414], "temperature": 0.0, "avg_logprob": -0.13358014562855597, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.2812751531600952}, {"id": 69, "seek": 56500, "start": 586.0, "end": 590.0, "text": " We know those things work, but we can't really figure out how exactly this works, right?", "tokens": [51414, 492, 458, 729, 721, 589, 11, 457, 321, 393, 380, 534, 2573, 484, 577, 2293, 341, 1985, 11, 558, 30, 51614], "temperature": 0.0, "avg_logprob": -0.13358014562855597, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.2812751531600952}, {"id": 70, "seek": 59000, "start": 590.0, "end": 600.0, "text": " And then you can think about and rationalize about what those things could be, like for example, all of those things have time to do with humans besides water, right?", "tokens": [50364, 400, 550, 291, 393, 519, 466, 293, 15090, 1125, 466, 437, 729, 721, 727, 312, 11, 411, 337, 1365, 11, 439, 295, 729, 721, 362, 565, 281, 360, 365, 6255, 11868, 1281, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.15135705882105335, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.20445920526981354}, {"id": 71, "seek": 59000, "start": 600.0, "end": 607.0, "text": " And then you can see there is those, you know, you see like a straight blue line which is blue for everybody by that.", "tokens": [50864, 400, 550, 291, 393, 536, 456, 307, 729, 11, 291, 458, 11, 291, 536, 411, 257, 2997, 3344, 1622, 597, 307, 3344, 337, 2201, 538, 300, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15135705882105335, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.20445920526981354}, {"id": 72, "seek": 59000, "start": 607.0, "end": 614.0, "text": " They say, well, maybe that is something which is related to humans, right?", "tokens": [51214, 814, 584, 11, 731, 11, 1310, 300, 307, 746, 597, 307, 4077, 281, 6255, 11, 558, 30, 51564], "temperature": 0.0, "avg_logprob": -0.15135705882105335, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.20445920526981354}, {"id": 73, "seek": 61400, "start": 614.0, "end": 619.0, "text": " Or we can see something else, right, like a king and queen, right?", "tokens": [50364, 1610, 321, 393, 536, 746, 1646, 11, 558, 11, 411, 257, 4867, 293, 12206, 11, 558, 30, 50614], "temperature": 0.0, "avg_logprob": -0.16439413255260837, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.06024901568889618}, {"id": 74, "seek": 61400, "start": 619.0, "end": 620.0, "text": " They also have a lot of similarity.", "tokens": [50614, 814, 611, 362, 257, 688, 295, 32194, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16439413255260837, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.06024901568889618}, {"id": 75, "seek": 61400, "start": 620.0, "end": 633.0, "text": " They may say, well, you know what, AI, we don't really exactly know how, but it may have something to, you know, think about their royalty.", "tokens": [50664, 814, 815, 584, 11, 731, 11, 291, 458, 437, 11, 7318, 11, 321, 500, 380, 534, 2293, 458, 577, 11, 457, 309, 815, 362, 746, 281, 11, 291, 458, 11, 519, 466, 641, 40929, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16439413255260837, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.06024901568889618}, {"id": 76, "seek": 61400, "start": 633.0, "end": 634.0, "text": " Okay.", "tokens": [51314, 1033, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16439413255260837, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.06024901568889618}, {"id": 77, "seek": 61400, "start": 634.0, "end": 643.0, "text": " So if you think about their vector search, right, in a nutshell, we have vectors.", "tokens": [51364, 407, 498, 291, 519, 466, 641, 8062, 3164, 11, 558, 11, 294, 257, 37711, 11, 321, 362, 18875, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16439413255260837, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.06024901568889618}, {"id": 78, "seek": 64300, "start": 643.0, "end": 649.0, "text": " That typically will be dense vectors which came from the AI applications.", "tokens": [50364, 663, 5850, 486, 312, 18011, 18875, 597, 1361, 490, 264, 7318, 5821, 13, 50664], "temperature": 0.0, "avg_logprob": -0.20552735328674315, "compression_ratio": 1.5352112676056338, "no_speech_prob": 0.027868639677762985}, {"id": 79, "seek": 64300, "start": 649.0, "end": 664.0, "text": " That would be some embeddings which in some database systems, right, they are, they focus on supporting operations with those embedding systems, right?", "tokens": [50664, 663, 576, 312, 512, 12240, 29432, 597, 294, 512, 8149, 3652, 11, 558, 11, 436, 366, 11, 436, 1879, 322, 7231, 7705, 365, 729, 12240, 3584, 3652, 11, 558, 30, 51414], "temperature": 0.0, "avg_logprob": -0.20552735328674315, "compression_ratio": 1.5352112676056338, "no_speech_prob": 0.027868639677762985}, {"id": 80, "seek": 64300, "start": 664.0, "end": 670.0, "text": " For example, postgres, PgVector, it's just say, hey guys, I don't know how you compute those vectors.", "tokens": [51414, 1171, 1365, 11, 2183, 45189, 11, 430, 70, 53, 20814, 11, 309, 311, 445, 584, 11, 4177, 1074, 11, 286, 500, 380, 458, 577, 291, 14722, 729, 18875, 13, 51714], "temperature": 0.0, "avg_logprob": -0.20552735328674315, "compression_ratio": 1.5352112676056338, "no_speech_prob": 0.027868639677762985}, {"id": 81, "seek": 67000, "start": 670.0, "end": 672.0, "text": " That is not our problem, right?", "tokens": [50364, 663, 307, 406, 527, 1154, 11, 558, 30, 50464], "temperature": 0.0, "avg_logprob": -0.22837528652615016, "compression_ratio": 1.5720524017467248, "no_speech_prob": 0.06178249046206474}, {"id": 82, "seek": 67000, "start": 672.0, "end": 676.0, "text": " We just going to help you to operate FM.", "tokens": [50464, 492, 445, 516, 281, 854, 291, 281, 9651, 29614, 13, 50664], "temperature": 0.0, "avg_logprob": -0.22837528652615016, "compression_ratio": 1.5720524017467248, "no_speech_prob": 0.06178249046206474}, {"id": 83, "seek": 67000, "start": 676.0, "end": 687.0, "text": " Some of the more advanced features they may, they vector databases, they also may support creating embeddings.", "tokens": [50664, 2188, 295, 264, 544, 7339, 4122, 436, 815, 11, 436, 8062, 22380, 11, 436, 611, 815, 1406, 4084, 12240, 29432, 13, 51214], "temperature": 0.0, "avg_logprob": -0.22837528652615016, "compression_ratio": 1.5720524017467248, "no_speech_prob": 0.06178249046206474}, {"id": 84, "seek": 67000, "start": 687.0, "end": 699.0, "text": " Maybe even, you know, from the database itself, right, especially in the cloud database, being able to, you know, call open AI's API in the background to compute the, you know,", "tokens": [51214, 2704, 754, 11, 291, 458, 11, 490, 264, 8149, 2564, 11, 558, 11, 2318, 294, 264, 4588, 8149, 11, 885, 1075, 281, 11, 291, 458, 11, 818, 1269, 7318, 311, 9362, 294, 264, 3678, 281, 14722, 264, 11, 291, 458, 11, 51814], "temperature": 0.0, "avg_logprob": -0.22837528652615016, "compression_ratio": 1.5720524017467248, "no_speech_prob": 0.06178249046206474}, {"id": 85, "seek": 69900, "start": 699.0, "end": 703.0, "text": " embeddings for you.", "tokens": [50364, 12240, 29432, 337, 291, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13580088388352168, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0048142531886696815}, {"id": 86, "seek": 69900, "start": 703.0, "end": 708.0, "text": " So anyway, let's talk more about the technology, how does that work, right?", "tokens": [50564, 407, 4033, 11, 718, 311, 751, 544, 466, 264, 2899, 11, 577, 775, 300, 589, 11, 558, 30, 50814], "temperature": 0.0, "avg_logprob": -0.13580088388352168, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0048142531886696815}, {"id": 87, "seek": 69900, "start": 708.0, "end": 714.0, "text": " So what exactly operations do we typically see in the vector search applications, right?", "tokens": [50814, 407, 437, 2293, 7705, 360, 321, 5850, 536, 294, 264, 8062, 3164, 5821, 11, 558, 30, 51114], "temperature": 0.0, "avg_logprob": -0.13580088388352168, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0048142531886696815}, {"id": 88, "seek": 69900, "start": 714.0, "end": 728.0, "text": " Well, typically we do have a bunch of vector store, we have a vector on input, right, and we are looking to find a vector which is closest to it.", "tokens": [51114, 1042, 11, 5850, 321, 360, 362, 257, 3840, 295, 8062, 3531, 11, 321, 362, 257, 8062, 322, 4846, 11, 558, 11, 293, 321, 366, 1237, 281, 915, 257, 8062, 597, 307, 13699, 281, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13580088388352168, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0048142531886696815}, {"id": 89, "seek": 72800, "start": 728.0, "end": 737.0, "text": " Right? Through the distance we want to define vector search is a cosine, cosine distance.", "tokens": [50364, 1779, 30, 8927, 264, 4560, 321, 528, 281, 6964, 8062, 3164, 307, 257, 23565, 11, 23565, 4560, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18756281988961357, "compression_ratio": 1.5930232558139534, "no_speech_prob": 0.015459517017006874}, {"id": 90, "seek": 72800, "start": 737.0, "end": 753.0, "text": " Right? Now, if you look at this problem, if you, of course, you can just, you know, as with about everything, right, you can just scan all the data and find the closest vectors, right?", "tokens": [50814, 1779, 30, 823, 11, 498, 291, 574, 412, 341, 1154, 11, 498, 291, 11, 295, 1164, 11, 291, 393, 445, 11, 291, 458, 11, 382, 365, 466, 1203, 11, 558, 11, 291, 393, 445, 11049, 439, 264, 1412, 293, 915, 264, 13699, 18875, 11, 558, 30, 51614], "temperature": 0.0, "avg_logprob": -0.18756281988961357, "compression_ratio": 1.5930232558139534, "no_speech_prob": 0.015459517017006874}, {"id": 91, "seek": 75300, "start": 753.0, "end": 760.0, "text": " And that is, you know, fantastic way you can do it, you can do it exact, but it is also very slow, right?", "tokens": [50364, 400, 300, 307, 11, 291, 458, 11, 5456, 636, 291, 393, 360, 309, 11, 291, 393, 360, 309, 1900, 11, 457, 309, 307, 611, 588, 2964, 11, 558, 30, 50714], "temperature": 0.0, "avg_logprob": -0.16734432291101525, "compression_ratio": 1.4275362318840579, "no_speech_prob": 0.04722532257437706}, {"id": 92, "seek": 75300, "start": 760.0, "end": 771.0, "text": " So that means why it's not used in practice instead, using special index structures, right?", "tokens": [50714, 407, 300, 1355, 983, 309, 311, 406, 1143, 294, 3124, 2602, 11, 1228, 2121, 8186, 9227, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.16734432291101525, "compression_ratio": 1.4275362318840579, "no_speech_prob": 0.04722532257437706}, {"id": 93, "seek": 77100, "start": 771.0, "end": 782.0, "text": " This HN SW, right, that seems to be the most popular algorithm, right, which industry seems to be coalescing about.", "tokens": [50364, 639, 389, 45, 20346, 11, 558, 11, 300, 2544, 281, 312, 264, 881, 3743, 9284, 11, 558, 11, 597, 3518, 2544, 281, 312, 598, 4229, 2175, 466, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1734807674701397, "compression_ratio": 1.45, "no_speech_prob": 0.09962484240531921}, {"id": 94, "seek": 77100, "start": 782.0, "end": 797.0, "text": " And I think what is important to note about that is compared, like, different from many other things in database, this index is not exact, right?", "tokens": [50914, 400, 286, 519, 437, 307, 1021, 281, 3637, 466, 300, 307, 5347, 11, 411, 11, 819, 490, 867, 661, 721, 294, 8149, 11, 341, 8186, 307, 406, 1900, 11, 558, 30, 51664], "temperature": 0.0, "avg_logprob": -0.1734807674701397, "compression_ratio": 1.45, "no_speech_prob": 0.09962484240531921}, {"id": 95, "seek": 79700, "start": 797.0, "end": 812.0, "text": " So it gives us rather, well, good accuracy, but it does not guarantee what that always will give you the, you know, let's say the closest vector when you ask it, right?", "tokens": [50364, 407, 309, 2709, 505, 2831, 11, 731, 11, 665, 14170, 11, 457, 309, 775, 406, 10815, 437, 300, 1009, 486, 976, 291, 264, 11, 291, 458, 11, 718, 311, 584, 264, 13699, 8062, 562, 291, 1029, 309, 11, 558, 30, 51114], "temperature": 0.0, "avg_logprob": -0.1392175555229187, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.023166727274656296}, {"id": 96, "seek": 79700, "start": 812.0, "end": 816.0, "text": " And if you are familiar with database, you can say, ooh, that looks strange.", "tokens": [51114, 400, 498, 291, 366, 4963, 365, 8149, 11, 291, 393, 584, 11, 17024, 11, 300, 1542, 5861, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1392175555229187, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.023166727274656296}, {"id": 97, "seek": 81600, "start": 816.0, "end": 828.0, "text": " But in AI applications, how those vectors are computed, right, they are not really exact to begin with, right?", "tokens": [50364, 583, 294, 7318, 5821, 11, 577, 729, 18875, 366, 40610, 11, 558, 11, 436, 366, 406, 534, 1900, 281, 1841, 365, 11, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.22498795721266004, "compression_ratio": 1.2232142857142858, "no_speech_prob": 0.11096653342247009}, {"id": 98, "seek": 81600, "start": 828.0, "end": 835.0, "text": " So these are quite usable.", "tokens": [50964, 407, 613, 366, 1596, 29975, 13, 51314], "temperature": 0.0, "avg_logprob": -0.22498795721266004, "compression_ratio": 1.2232142857142858, "no_speech_prob": 0.11096653342247009}, {"id": 99, "seek": 83500, "start": 836.0, "end": 850.0, "text": " Okay. So let's talk about those solutions, right, what we are really using this for, right?", "tokens": [50414, 1033, 13, 407, 718, 311, 751, 466, 729, 6547, 11, 558, 11, 437, 321, 366, 534, 1228, 341, 337, 11, 558, 30, 51114], "temperature": 0.0, "avg_logprob": -0.19309939508852753, "compression_ratio": 1.3037037037037038, "no_speech_prob": 0.016618900001049042}, {"id": 100, "seek": 83500, "start": 850.0, "end": 858.0, "text": " As I mentioned, the most common features in this case will be found in your nearest.", "tokens": [51114, 1018, 286, 2835, 11, 264, 881, 2689, 4122, 294, 341, 1389, 486, 312, 1352, 294, 428, 23831, 13, 51514], "temperature": 0.0, "avg_logprob": -0.19309939508852753, "compression_ratio": 1.3037037037037038, "no_speech_prob": 0.016618900001049042}, {"id": 101, "seek": 85800, "start": 858.0, "end": 876.0, "text": " There are some other more kind of global operations in this case, which can be used in terms of clustering the data or classification, which is supported by advanced features, advanced systems.", "tokens": [50364, 821, 366, 512, 661, 544, 733, 295, 4338, 7705, 294, 341, 1389, 11, 597, 393, 312, 1143, 294, 2115, 295, 596, 48673, 264, 1412, 420, 21538, 11, 597, 307, 8104, 538, 7339, 4122, 11, 7339, 3652, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22755001812446407, "compression_ratio": 1.4402985074626866, "no_speech_prob": 0.11543692648410797}, {"id": 102, "seek": 87600, "start": 877.0, "end": 882.0, "text": " Okay. Let me show maybe in this case a little bit of example.", "tokens": [50414, 1033, 13, 961, 385, 855, 1310, 294, 341, 1389, 257, 707, 857, 295, 1365, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3285835266113281, "compression_ratio": 1.4195804195804196, "no_speech_prob": 0.07635098695755005}, {"id": 103, "seek": 87600, "start": 882.0, "end": 897.0, "text": " And here we'll use their, an interesting way, we'll use their multi-core search system, right, that's where one of the search is created for.", "tokens": [50664, 400, 510, 321, 603, 764, 641, 11, 364, 1880, 636, 11, 321, 603, 764, 641, 4825, 12, 12352, 3164, 1185, 11, 558, 11, 300, 311, 689, 472, 295, 264, 3164, 307, 2942, 337, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3285835266113281, "compression_ratio": 1.4195804195804196, "no_speech_prob": 0.07635098695755005}, {"id": 104, "seek": 89700, "start": 897.0, "end": 903.0, "text": " But we'll connect to that through a MySQL protocol, right, that's what it's for.", "tokens": [50364, 583, 321, 603, 1745, 281, 300, 807, 257, 1222, 39934, 10336, 11, 558, 11, 300, 311, 437, 309, 311, 337, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2834669259878305, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.02033957839012146}, {"id": 105, "seek": 89700, "start": 903.0, "end": 913.0, "text": " And we can see what we can go ahead and let's say, create the table, as you can see, right?", "tokens": [50664, 400, 321, 393, 536, 437, 321, 393, 352, 2286, 293, 718, 311, 584, 11, 1884, 264, 3199, 11, 382, 291, 393, 536, 11, 558, 30, 51164], "temperature": 0.0, "avg_logprob": -0.2834669259878305, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.02033957839012146}, {"id": 106, "seek": 89700, "start": 913.0, "end": 922.0, "text": " We've defined in the flow of vector, in the traditional database type, but something what this engine support, right?", "tokens": [51164, 492, 600, 7642, 294, 264, 3095, 295, 8062, 11, 294, 264, 5164, 8149, 2010, 11, 457, 746, 437, 341, 2848, 1406, 11, 558, 30, 51614], "temperature": 0.0, "avg_logprob": -0.2834669259878305, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.02033957839012146}, {"id": 107, "seek": 92200, "start": 922.0, "end": 930.0, "text": " And you would see typically vectors support some sort of vector store functionality, right?", "tokens": [50364, 400, 291, 576, 536, 5850, 18875, 1406, 512, 1333, 295, 8062, 3531, 14980, 11, 558, 30, 50764], "temperature": 0.0, "avg_logprob": -0.11873200844074118, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.028829416260123253}, {"id": 108, "seek": 92200, "start": 930.0, "end": 942.0, "text": " And then we can find their distance between the given vector we have as well as the vectors in the database, right?", "tokens": [50764, 400, 550, 321, 393, 915, 641, 4560, 1296, 264, 2212, 8062, 321, 362, 382, 731, 382, 264, 18875, 294, 264, 8149, 11, 558, 30, 51364], "temperature": 0.0, "avg_logprob": -0.11873200844074118, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.028829416260123253}, {"id": 109, "seek": 92200, "start": 942.0, "end": 947.0, "text": " And it can give us the information, right?", "tokens": [51364, 400, 309, 393, 976, 505, 264, 1589, 11, 558, 30, 51614], "temperature": 0.0, "avg_logprob": -0.11873200844074118, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.028829416260123253}, {"id": 110, "seek": 94700, "start": 947.0, "end": 957.0, "text": " So what we actually had in this case is their different images, which would run through creating embedding for them.", "tokens": [50364, 407, 437, 321, 767, 632, 294, 341, 1389, 307, 641, 819, 5267, 11, 597, 576, 1190, 807, 4084, 12240, 3584, 337, 552, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17761027812957764, "compression_ratio": 1.3057851239669422, "no_speech_prob": 0.050974246114492416}, {"id": 111, "seek": 94700, "start": 957.0, "end": 964.0, "text": " And then you can see what, what was that?", "tokens": [50864, 400, 550, 291, 393, 536, 437, 11, 437, 390, 300, 30, 51214], "temperature": 0.0, "avg_logprob": -0.17761027812957764, "compression_ratio": 1.3057851239669422, "no_speech_prob": 0.050974246114492416}, {"id": 112, "seek": 96400, "start": 964.0, "end": 978.0, "text": " Yeah, I think that was like an image of a bag, right, which was saying, hey, you know what, it's much more similar to a yellow bag compared to their, to a white bag, right?", "tokens": [50364, 865, 11, 286, 519, 300, 390, 411, 364, 3256, 295, 257, 3411, 11, 558, 11, 597, 390, 1566, 11, 4177, 11, 291, 458, 437, 11, 309, 311, 709, 544, 2531, 281, 257, 5566, 3411, 5347, 281, 641, 11, 281, 257, 2418, 3411, 11, 558, 30, 51064], "temperature": 0.0, "avg_logprob": -0.1500230970836821, "compression_ratio": 1.54, "no_speech_prob": 0.031413059681653976}, {"id": 113, "seek": 96400, "start": 978.0, "end": 984.0, "text": " Which you can, that is a kind of pretty common, what we have.", "tokens": [51064, 3013, 291, 393, 11, 300, 307, 257, 733, 295, 1238, 2689, 11, 437, 321, 362, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1500230970836821, "compression_ratio": 1.54, "no_speech_prob": 0.031413059681653976}, {"id": 114, "seek": 96400, "start": 984.0, "end": 989.0, "text": " Okay. I mentioned what, when you speak about embedded computation, right?", "tokens": [51364, 1033, 13, 286, 2835, 437, 11, 562, 291, 1710, 466, 16741, 24903, 11, 558, 30, 51614], "temperature": 0.0, "avg_logprob": -0.1500230970836821, "compression_ratio": 1.54, "no_speech_prob": 0.031413059681653976}, {"id": 115, "seek": 98900, "start": 989.0, "end": 1004.0, "text": " There, if you look at especially non-vector databases, not specialized databases like Postgres where we would say, hey guys, you guys can use external encoder, but they don't see that as a database problem, right?", "tokens": [50364, 821, 11, 498, 291, 574, 412, 2318, 2107, 12, 303, 1672, 22380, 11, 406, 19813, 22380, 411, 10223, 45189, 689, 321, 576, 584, 11, 4177, 1074, 11, 291, 1074, 393, 764, 8320, 2058, 19866, 11, 457, 436, 500, 380, 536, 300, 382, 257, 8149, 1154, 11, 558, 30, 51114], "temperature": 0.0, "avg_logprob": -0.24314579963684083, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.02656678482890129}, {"id": 116, "seek": 98900, "start": 1004.0, "end": 1015.0, "text": " You process some information out there, your favorite find, external API, or use some local open source model, that's fine.", "tokens": [51114, 509, 1399, 512, 1589, 484, 456, 11, 428, 2954, 915, 11, 8320, 9362, 11, 420, 764, 512, 2654, 1269, 4009, 2316, 11, 300, 311, 2489, 13, 51664], "temperature": 0.0, "avg_logprob": -0.24314579963684083, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.02656678482890129}, {"id": 117, "seek": 101500, "start": 1015.0, "end": 1027.0, "text": " Though there are some libraries, right, with Microsoft last time, a library, right, they are being added.", "tokens": [50364, 10404, 456, 366, 512, 15148, 11, 558, 11, 365, 8116, 1036, 565, 11, 257, 6405, 11, 558, 11, 436, 366, 885, 3869, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15865402221679686, "compression_ratio": 1.4050632911392404, "no_speech_prob": 0.02917701192200184}, {"id": 118, "seek": 101500, "start": 1027.0, "end": 1038.0, "text": " And I think over time, of course, if our desire to enable developers simplicity, we'll see more of a direct support.", "tokens": [50964, 400, 286, 519, 670, 565, 11, 295, 1164, 11, 498, 527, 7516, 281, 9528, 8849, 25632, 11, 321, 603, 536, 544, 295, 257, 2047, 1406, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15865402221679686, "compression_ratio": 1.4050632911392404, "no_speech_prob": 0.02917701192200184}, {"id": 119, "seek": 103800, "start": 1038.0, "end": 1047.0, "text": " Now, something else I think what is interesting about their embedding and the information retrieval tasks specifically.", "tokens": [50364, 823, 11, 746, 1646, 286, 519, 437, 307, 1880, 466, 641, 12240, 3584, 293, 264, 1589, 19817, 3337, 9608, 4682, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14559920889432312, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.007573782000690699}, {"id": 120, "seek": 103800, "start": 1047.0, "end": 1057.0, "text": " I think what is very interesting, if you look at the search applications, for years, there have been a lot of time spent about sort of like a hard coding,", "tokens": [50814, 286, 519, 437, 307, 588, 1880, 11, 498, 291, 574, 412, 264, 3164, 5821, 11, 337, 924, 11, 456, 362, 668, 257, 688, 295, 565, 4418, 466, 1333, 295, 411, 257, 1152, 17720, 11, 51314], "temperature": 0.0, "avg_logprob": -0.14559920889432312, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.007573782000690699}, {"id": 121, "seek": 105700, "start": 1057.0, "end": 1072.0, "text": " like in the engineering, the structure of the language, defining synonyms, defining antonyms, right, and so on and so forth, right, and so we can run our search quality.", "tokens": [50364, 411, 294, 264, 7043, 11, 264, 3877, 295, 264, 2856, 11, 17827, 5451, 2526, 2592, 11, 17827, 364, 1756, 88, 2592, 11, 558, 11, 293, 370, 322, 293, 370, 5220, 11, 558, 11, 293, 370, 321, 393, 1190, 527, 3164, 3125, 13, 51114], "temperature": 0.0, "avg_logprob": -0.418711869612984, "compression_ratio": 1.5363636363636364, "no_speech_prob": 0.16242046654224396}, {"id": 122, "seek": 107200, "start": 1072.0, "end": 1093.0, "text": " The other approach, though, is we can use the AI, right, for this approach, right, so we can look at the matching document through embeddings, right,", "tokens": [50364, 440, 661, 3109, 11, 1673, 11, 307, 321, 393, 764, 264, 7318, 11, 558, 11, 337, 341, 3109, 11, 558, 11, 370, 321, 393, 574, 412, 264, 14324, 4166, 807, 12240, 29432, 11, 558, 11, 51414], "temperature": 0.0, "avg_logprob": -0.130773075043209, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.06671884655952454}, {"id": 123, "seek": 107200, "start": 1093.0, "end": 1099.0, "text": " and that really allows to avoid a lot of that thing which has to be manual processing if a good quality.", "tokens": [51414, 293, 300, 534, 4045, 281, 5042, 257, 688, 295, 300, 551, 597, 575, 281, 312, 9688, 9007, 498, 257, 665, 3125, 13, 51714], "temperature": 0.0, "avg_logprob": -0.130773075043209, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.06671884655952454}, {"id": 124, "seek": 109900, "start": 1099.0, "end": 1113.0, "text": " But what is an interesting there, though, is this new generation AI search may not be best, and also it may not be the most effective, right,", "tokens": [50364, 583, 437, 307, 364, 1880, 456, 11, 1673, 11, 307, 341, 777, 5125, 7318, 3164, 815, 406, 312, 1151, 11, 293, 611, 309, 815, 406, 312, 264, 881, 4942, 11, 558, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1054602910394538, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.05691054090857506}, {"id": 125, "seek": 109900, "start": 1113.0, "end": 1125.0, "text": " because if you think about that, if I have like a lot of a document in my database, right, billions, right, then actually the search can be relatively expensive.", "tokens": [51064, 570, 498, 291, 519, 466, 300, 11, 498, 286, 362, 411, 257, 688, 295, 257, 4166, 294, 452, 8149, 11, 558, 11, 17375, 11, 558, 11, 550, 767, 264, 3164, 393, 312, 7226, 5124, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1054602910394538, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.05691054090857506}, {"id": 126, "seek": 112500, "start": 1125.0, "end": 1145.0, "text": " So one of the approaches which is used is dual approaches, right, and you can say, well, we may be getting, you know, like a top thousand documents or something through, you know, like a legacy kind of frequency-based search methods, right,", "tokens": [50364, 407, 472, 295, 264, 11587, 597, 307, 1143, 307, 11848, 11587, 11, 558, 11, 293, 291, 393, 584, 11, 731, 11, 321, 815, 312, 1242, 11, 291, 458, 11, 411, 257, 1192, 4714, 8512, 420, 746, 807, 11, 291, 458, 11, 411, 257, 11711, 733, 295, 7893, 12, 6032, 3164, 7150, 11, 558, 11, 51364], "temperature": 0.0, "avg_logprob": -0.17419068566684065, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.11438523977994919}, {"id": 127, "seek": 114500, "start": 1145.0, "end": 1167.0, "text": " and then we can use AI to rank those, right, and you can see what that's sort of like a combination that's like a last-second, like a Vespa hybrid, right, it shows better, right, on many benchmarks.", "tokens": [50364, 293, 550, 321, 393, 764, 7318, 281, 6181, 729, 11, 558, 11, 293, 291, 393, 536, 437, 300, 311, 1333, 295, 411, 257, 6562, 300, 311, 411, 257, 1036, 12, 27375, 11, 411, 257, 691, 279, 4306, 13051, 11, 558, 11, 309, 3110, 1101, 11, 558, 11, 322, 867, 43751, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2231647318059748, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.3246159553527832}, {"id": 128, "seek": 116700, "start": 1167.0, "end": 1190.0, "text": " Okay, well, oh, close, thank you, is very usable, both in information retrieval tasks as well as many other applications,", "tokens": [50364, 1033, 11, 731, 11, 1954, 11, 1998, 11, 1309, 291, 11, 307, 588, 29975, 11, 1293, 294, 1589, 19817, 3337, 9608, 382, 731, 382, 867, 661, 5821, 11, 51514], "temperature": 0.0, "avg_logprob": -0.30541104078292847, "compression_ratio": 1.2222222222222223, "no_speech_prob": 0.08675505965948105}, {"id": 129, "seek": 119000, "start": 1190.0, "end": 1208.0, "text": " and what we see also what the vector support in databases in relatively early stages, right, it just happened implemented in the last few years, right, I would say what the current implementation is related to basics,", "tokens": [50364, 293, 437, 321, 536, 611, 437, 264, 8062, 1406, 294, 22380, 294, 7226, 2440, 10232, 11, 558, 11, 309, 445, 2011, 12270, 294, 264, 1036, 1326, 924, 11, 558, 11, 286, 576, 584, 437, 264, 2190, 11420, 307, 4077, 281, 14688, 11, 51264], "temperature": 0.0, "avg_logprob": -0.19387794577557108, "compression_ratio": 1.528169014084507, "no_speech_prob": 0.16656115651130676}, {"id": 130, "seek": 120800, "start": 1208.0, "end": 1236.0, "text": " and we would see a lot more of the features, right, as we kind of figure out, right, what we actually want to empower us to go into the continuing innovation in this data structures to support us in the fast vector", "tokens": [50364, 293, 321, 576, 536, 257, 688, 544, 295, 264, 4122, 11, 558, 11, 382, 321, 733, 295, 2573, 484, 11, 558, 11, 437, 321, 767, 528, 281, 11071, 505, 281, 352, 666, 264, 9289, 8504, 294, 341, 1412, 9227, 281, 1406, 505, 294, 264, 2370, 8062, 51764], "temperature": 0.0, "avg_logprob": -0.2309872055053711, "compression_ratio": 1.5070422535211268, "no_speech_prob": 0.18688589334487915}, {"id": 131, "seek": 123600, "start": 1236.0, "end": 1254.0, "text": " applications, right, and as well improve accuracy. Well, that's all I have, right, you had some, yes, the gentleman was about to show me zero minutes, yeah, and if there is any questions, I would be happy to try to answer.", "tokens": [50364, 5821, 11, 558, 11, 293, 382, 731, 3470, 14170, 13, 1042, 11, 300, 311, 439, 286, 362, 11, 558, 11, 291, 632, 512, 11, 2086, 11, 264, 15761, 390, 466, 281, 855, 385, 4018, 2077, 11, 1338, 11, 293, 498, 456, 307, 604, 1651, 11, 286, 576, 312, 2055, 281, 853, 281, 1867, 13, 51264], "temperature": 0.0, "avg_logprob": -0.18355561947000437, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.42811378836631775}, {"id": 132, "seek": 125400, "start": 1255.0, "end": 1265.0, "text": " Any questions? Yes.", "tokens": [50414, 2639, 1651, 30, 1079, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3642365137736003, "compression_ratio": 0.7037037037037037, "no_speech_prob": 0.30877092480659485}, {"id": 133, "seek": 126500, "start": 1265.0, "end": 1273.0, "text": " Hi.", "tokens": [50364, 2421, 13, 50764], "temperature": 0.0, "avg_logprob": -0.700018564860026, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.19852444529533386}, {"id": 134, "seek": 127300, "start": 1273.0, "end": 1300.0, "text": " Okay, okay, so given the fact that so you say that databases are going to transition from just supporting externally modelled embeddings to generating internally, but given the fact that we see a lot of many advanced models that generate", "tokens": [50364, 1033, 11, 1392, 11, 370, 2212, 264, 1186, 300, 370, 291, 584, 300, 22380, 366, 516, 281, 6034, 490, 445, 7231, 40899, 1072, 41307, 12240, 29432, 281, 17746, 19501, 11, 457, 2212, 264, 1186, 300, 321, 536, 257, 688, 295, 867, 7339, 5245, 300, 8460, 51714], "temperature": 0.0, "avg_logprob": -0.16730213165283203, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.3435744047164917}, {"id": 135, "seek": 130000, "start": 1300.0, "end": 1317.0, "text": " embeddings, for example, the GPT, the GPT is a machine that basically takes entire concepts, translates very efficiently into embeddings and then outputs also, and is the standard like you provide a single external", "tokens": [50364, 12240, 29432, 11, 337, 1365, 11, 264, 26039, 51, 11, 264, 26039, 51, 307, 257, 3479, 300, 1936, 2516, 2302, 10392, 11, 28468, 588, 19621, 666, 12240, 29432, 293, 550, 23930, 611, 11, 293, 307, 264, 3832, 411, 291, 2893, 257, 2167, 8320, 51214], "temperature": 0.0, "avg_logprob": -0.17782531900608795, "compression_ratio": 1.436241610738255, "no_speech_prob": 0.5802001357078552}, {"id": 136, "seek": 131700, "start": 1317.0, "end": 1336.0, "text": " embedding generator model compared to a traditional BWAC, and then everyone can just benefit from an on par model. Well, yes, yeah, absolutely. Well, what I'm saying in this case, I think it's being supported, right, maybe in different things, right, I'm not saying, oh, you know what, you should expect postgres", "tokens": [50364, 12240, 3584, 19265, 2316, 5347, 281, 257, 5164, 363, 54, 4378, 11, 293, 550, 1518, 393, 445, 5121, 490, 364, 322, 971, 2316, 13, 1042, 11, 2086, 11, 1338, 11, 3122, 13, 1042, 11, 437, 286, 478, 1566, 294, 341, 1389, 11, 286, 519, 309, 311, 885, 8104, 11, 558, 11, 1310, 294, 819, 721, 11, 558, 11, 286, 478, 406, 1566, 11, 1954, 11, 291, 458, 437, 11, 291, 820, 2066, 2183, 45189, 51314], "temperature": 0.0, "avg_logprob": -0.25989542251978165, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.4434705972671509}, {"id": 137, "seek": 133600, "start": 1336.0, "end": 1353.0, "text": " incorporate inside that over possible models, right, I think I think the same would be say like if, let's say, Python support those things, right, that just means it's easier to do from a Python standpoint, right, so now if you think about this case, if I want to generate", "tokens": [50364, 16091, 1854, 300, 670, 1944, 5245, 11, 558, 11, 286, 519, 286, 519, 264, 912, 576, 312, 584, 411, 498, 11, 718, 311, 584, 11, 15329, 1406, 729, 721, 11, 558, 11, 300, 445, 1355, 309, 311, 3571, 281, 360, 490, 257, 15329, 15827, 11, 558, 11, 370, 586, 498, 291, 519, 466, 341, 1389, 11, 498, 286, 528, 281, 8460, 51214], "temperature": 0.0, "avg_logprob": -0.19225497612586387, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.40201085805892944}, {"id": 138, "seek": 135300, "start": 1353.0, "end": 1374.0, "text": " embedding, even for data I have in the database, I kind of have to do that, that externally, right, what I'm saying is, well, get some, you know, fun, was us by talking externally, not creative, so that is what I'm speaking about, make sense?", "tokens": [50364, 12240, 3584, 11, 754, 337, 1412, 286, 362, 294, 264, 8149, 11, 286, 733, 295, 362, 281, 360, 300, 11, 300, 40899, 11, 558, 11, 437, 286, 478, 1566, 307, 11, 731, 11, 483, 512, 11, 291, 458, 11, 1019, 11, 390, 505, 538, 1417, 40899, 11, 406, 5880, 11, 370, 300, 307, 437, 286, 478, 4124, 466, 11, 652, 2020, 30, 51414], "temperature": 0.0, "avg_logprob": -0.3366582754886512, "compression_ratio": 1.5414012738853504, "no_speech_prob": 0.47012859582901}, {"id": 139, "seek": 137400, "start": 1374.0, "end": 1377.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50514], "temperature": 0.0, "avg_logprob": -0.21899936817310475, "compression_ratio": 1.4013157894736843, "no_speech_prob": 0.34071987867355347}, {"id": 140, "seek": 137400, "start": 1377.0, "end": 1380.0, "text": " Any other questions?", "tokens": [50514, 2639, 661, 1651, 30, 50664], "temperature": 0.0, "avg_logprob": -0.21899936817310475, "compression_ratio": 1.4013157894736843, "no_speech_prob": 0.34071987867355347}, {"id": 141, "seek": 137400, "start": 1380.0, "end": 1401.0, "text": " Thanks. Thanks for the talk. As a database developer, how do you deal with very rapidly moving target of both algorithms and storage formats? In the sense, do you deprecate rapidly?", "tokens": [50664, 2561, 13, 2561, 337, 264, 751, 13, 1018, 257, 8149, 10754, 11, 577, 360, 291, 2028, 365, 588, 12910, 2684, 3779, 295, 1293, 14642, 293, 6725, 25879, 30, 682, 264, 2020, 11, 360, 291, 1367, 13867, 473, 12910, 30, 51714], "temperature": 0.0, "avg_logprob": -0.21899936817310475, "compression_ratio": 1.4013157894736843, "no_speech_prob": 0.34071987867355347}, {"id": 142, "seek": 140100, "start": 1401.0, "end": 1421.0, "text": " Because if today you have a KNN function that has this, some certain API, and a month later, you know, ML is very rapidly moving target, you have a better KNN function, and same goes with densely, dense format.", "tokens": [50364, 1436, 498, 965, 291, 362, 257, 26967, 45, 2445, 300, 575, 341, 11, 512, 1629, 9362, 11, 293, 257, 1618, 1780, 11, 291, 458, 11, 21601, 307, 588, 12910, 2684, 3779, 11, 291, 362, 257, 1101, 26967, 45, 2445, 11, 293, 912, 1709, 365, 24505, 736, 11, 18011, 7877, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2295241636388442, "compression_ratio": 1.7260869565217392, "no_speech_prob": 0.43433040380477905}, {"id": 143, "seek": 140100, "start": 1421.0, "end": 1430.0, "text": " Well, yeah, I think that is a good point. Now, I think what is interesting what you're saying, right, you know, thing on one extent it's always interesting and then you have a, you know,", "tokens": [51364, 1042, 11, 1338, 11, 286, 519, 300, 307, 257, 665, 935, 13, 823, 11, 286, 519, 437, 307, 1880, 437, 291, 434, 1566, 11, 558, 11, 291, 458, 11, 551, 322, 472, 8396, 309, 311, 1009, 1880, 293, 550, 291, 362, 257, 11, 291, 458, 11, 51814], "temperature": 0.0, "avg_logprob": -0.2295241636388442, "compression_ratio": 1.7260869565217392, "no_speech_prob": 0.43433040380477905}, {"id": 144, "seek": 143000, "start": 1430.0, "end": 1446.0, "text": " interesting and then you have this, the industry in like an early stage, because on one extent things are changing quickly, on the other hand, often people implement something, they put it in production, it's good enough, and the fact that", "tokens": [50364, 1880, 293, 550, 291, 362, 341, 11, 264, 3518, 294, 411, 364, 2440, 3233, 11, 570, 322, 472, 8396, 721, 366, 4473, 2661, 11, 322, 264, 661, 1011, 11, 2049, 561, 4445, 746, 11, 436, 829, 309, 294, 4265, 11, 309, 311, 665, 1547, 11, 293, 264, 1186, 300, 51164], "temperature": 0.0, "avg_logprob": -0.22492931200110394, "compression_ratio": 1.7927272727272727, "no_speech_prob": 0.24105782806873322}, {"id": 145, "seek": 143000, "start": 1446.0, "end": 1459.0, "text": " that kind of is a better state of art out there, that doesn't mean what they want to change, right, and that means, like at least in the database world, right, you always have to say, well, you know what, there are certain things you would love to kill,", "tokens": [51164, 300, 733, 295, 307, 257, 1101, 1785, 295, 1523, 484, 456, 11, 300, 1177, 380, 914, 437, 436, 528, 281, 1319, 11, 558, 11, 293, 300, 1355, 11, 411, 412, 1935, 294, 264, 8149, 1002, 11, 558, 11, 291, 1009, 362, 281, 584, 11, 731, 11, 291, 458, 437, 11, 456, 366, 1629, 721, 291, 576, 959, 281, 1961, 11, 51814], "temperature": 0.0, "avg_logprob": -0.22492931200110394, "compression_ratio": 1.7927272727272727, "no_speech_prob": 0.24105782806873322}, {"id": 146, "seek": 145900, "start": 1459.0, "end": 1477.0, "text": " but actually some very big corporations already deployed in production and they're not freaking changing that in the next 10 years, right. So, reality what that's going to be, right, I think is that evolution on that side, but then we'll still have to have a competitive", "tokens": [50364, 457, 767, 512, 588, 955, 17676, 1217, 17826, 294, 4265, 293, 436, 434, 406, 14612, 4473, 300, 294, 264, 958, 1266, 924, 11, 558, 13, 407, 11, 4103, 437, 300, 311, 516, 281, 312, 11, 558, 11, 286, 519, 307, 300, 9303, 322, 300, 1252, 11, 457, 550, 321, 603, 920, 362, 281, 362, 257, 10043, 51264], "temperature": 0.0, "avg_logprob": -0.2486356735229492, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.04705307260155678}, {"id": 147, "seek": 147700, "start": 1477.0, "end": 1495.0, "text": " I think if you look at, I mentioned like a postgres, like a PG vector extensions, well, you support like a whole bunch of different options in this case, right, so yeah, I think that's what we'll expect.", "tokens": [50364, 286, 519, 498, 291, 574, 412, 11, 286, 2835, 411, 257, 2183, 45189, 11, 411, 257, 40975, 8062, 25129, 11, 731, 11, 291, 1406, 411, 257, 1379, 3840, 295, 819, 3956, 294, 341, 1389, 11, 558, 11, 370, 1338, 11, 286, 519, 300, 311, 437, 321, 603, 2066, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2924310936118072, "compression_ratio": 1.3904109589041096, "no_speech_prob": 0.18009378015995026}, {"id": 148, "seek": 149500, "start": 1495.0, "end": 1514.0, "text": " Okay, hi, yeah, can you go back to the embeddings slide where you showed the similarity between Queen and Queen, I think, the similarities between Queen and King, I think, or was like the first,", "tokens": [50364, 1033, 11, 4879, 11, 1338, 11, 393, 291, 352, 646, 281, 264, 12240, 29432, 4137, 689, 291, 4712, 264, 32194, 1296, 10077, 293, 10077, 11, 286, 519, 11, 264, 24197, 1296, 10077, 293, 3819, 11, 286, 519, 11, 420, 390, 411, 264, 700, 11, 51314], "temperature": 0.0, "avg_logprob": -0.30548067887624103, "compression_ratio": 1.5275590551181102, "no_speech_prob": 0.4353768229484558}, {"id": 149, "seek": 151400, "start": 1514.0, "end": 1521.0, "text": " Oh, this one? Yeah, yeah, yeah, yeah, yeah, you mentioned you use Ada, I think.", "tokens": [50364, 876, 11, 341, 472, 30, 865, 11, 1338, 11, 1338, 11, 1338, 11, 1338, 11, 291, 2835, 291, 764, 32276, 11, 286, 519, 13, 50714], "temperature": 0.0, "avg_logprob": -0.3385001911836512, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.6611308455467224}, {"id": 150, "seek": 151400, "start": 1521.0, "end": 1523.0, "text": " What do you use?", "tokens": [50714, 708, 360, 291, 764, 30, 50814], "temperature": 0.0, "avg_logprob": -0.3385001911836512, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.6611308455467224}, {"id": 151, "seek": 151400, "start": 1523.0, "end": 1525.0, "text": " Chart GPT embeddings.", "tokens": [50814, 49762, 26039, 51, 12240, 29432, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3385001911836512, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.6611308455467224}, {"id": 152, "seek": 151400, "start": 1525.0, "end": 1532.0, "text": " No, so yeah, this one is actually, you know, a global model, right, that was one of the open source models, right.", "tokens": [50914, 883, 11, 370, 1338, 11, 341, 472, 307, 767, 11, 291, 458, 11, 257, 4338, 2316, 11, 558, 11, 300, 390, 472, 295, 264, 1269, 4009, 5245, 11, 558, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3385001911836512, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.6611308455467224}, {"id": 153, "seek": 151400, "start": 1532.0, "end": 1533.0, "text": " Ah, okay, okay, yeah.", "tokens": [51264, 2438, 11, 1392, 11, 1392, 11, 1338, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3385001911836512, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.6611308455467224}, {"id": 154, "seek": 153300, "start": 1533.0, "end": 1557.0, "text": " But I mean, in this case, I think that is just like an example, right, I think what you wanted to look in this case to visualize, right, I mean, when you say, hey guys, you generate those kind of embeddings, and they do not particularly mean anything, right, but can you just plot them, you know, as we plot, let's say, you know, DNA of a frog and a fish, right, can we see something maybe in this case, right.", "tokens": [50364, 583, 286, 914, 11, 294, 341, 1389, 11, 286, 519, 300, 307, 445, 411, 364, 1365, 11, 558, 11, 286, 519, 437, 291, 1415, 281, 574, 294, 341, 1389, 281, 23273, 11, 558, 11, 286, 914, 11, 562, 291, 584, 11, 4177, 1074, 11, 291, 8460, 729, 733, 295, 12240, 29432, 11, 293, 436, 360, 406, 4098, 914, 1340, 11, 558, 11, 457, 393, 291, 445, 7542, 552, 11, 291, 458, 11, 382, 321, 7542, 11, 718, 311, 584, 11, 291, 458, 11, 8272, 295, 257, 17259, 293, 257, 3506, 11, 558, 11, 393, 321, 536, 746, 1310, 294, 341, 1389, 11, 558, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1597947191309046, "compression_ratio": 1.759656652360515, "no_speech_prob": 0.21036210656166077}, {"id": 155, "seek": 155700, "start": 1557.0, "end": 1569.0, "text": " So that was the case to do here, right, to visualize how particular embedding generation model.", "tokens": [50364, 407, 300, 390, 264, 1389, 281, 360, 510, 11, 558, 11, 281, 23273, 577, 1729, 12240, 3584, 5125, 2316, 13, 50964], "temperature": 0.0, "avg_logprob": -0.25031894186268683, "compression_ratio": 1.625, "no_speech_prob": 0.16703231632709503}, {"id": 156, "seek": 155700, "start": 1569.0, "end": 1585.0, "text": " Yeah, no, no, I find it really helpful also for, I find it really helpful also for like my future students, because like it really grasped the accents of embedding and similarities, I think.", "tokens": [50964, 865, 11, 572, 11, 572, 11, 286, 915, 309, 534, 4961, 611, 337, 11, 286, 915, 309, 534, 4961, 611, 337, 411, 452, 2027, 1731, 11, 570, 411, 309, 534, 29444, 3452, 264, 35012, 295, 12240, 3584, 293, 24197, 11, 286, 519, 13, 51764], "temperature": 0.0, "avg_logprob": -0.25031894186268683, "compression_ratio": 1.625, "no_speech_prob": 0.16703231632709503}, {"id": 157, "seek": 158500, "start": 1585.0, "end": 1610.0, "text": " Yeah, I mean, again, like in this case, that is just to visualize to show people what that things look like, right, and my point I was trying to make is, on one extent, we cannot really state what exactly are those dimensions, what it corresponds to, right, but as a human, we can, you know, try to rationalize over, it seems to be like, you know, something.", "tokens": [50364, 865, 11, 286, 914, 11, 797, 11, 411, 294, 341, 1389, 11, 300, 307, 445, 281, 23273, 281, 855, 561, 437, 300, 721, 574, 411, 11, 558, 11, 293, 452, 935, 286, 390, 1382, 281, 652, 307, 11, 322, 472, 8396, 11, 321, 2644, 534, 1785, 437, 2293, 366, 729, 12819, 11, 437, 309, 23249, 281, 11, 558, 11, 457, 382, 257, 1952, 11, 321, 393, 11, 291, 458, 11, 853, 281, 15090, 1125, 670, 11, 309, 2544, 281, 312, 411, 11, 291, 458, 11, 746, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16483609610741293, "compression_ratio": 1.7226890756302522, "no_speech_prob": 0.22488880157470703}, {"id": 158, "seek": 158500, "start": 1610.0, "end": 1611.0, "text": " Yeah, yeah, also.", "tokens": [51614, 865, 11, 1338, 11, 611, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16483609610741293, "compression_ratio": 1.7226890756302522, "no_speech_prob": 0.22488880157470703}, {"id": 159, "seek": 158500, "start": 1611.0, "end": 1612.0, "text": " You know, something there, right.", "tokens": [51664, 509, 458, 11, 746, 456, 11, 558, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16483609610741293, "compression_ratio": 1.7226890756302522, "no_speech_prob": 0.22488880157470703}, {"id": 160, "seek": 161200, "start": 1612.0, "end": 1623.0, "text": " Also because other, the embedding of OpenIE has like 100, no, 1000 and a half features.", "tokens": [50364, 2743, 570, 661, 11, 264, 12240, 3584, 295, 7238, 6550, 575, 411, 2319, 11, 572, 11, 9714, 293, 257, 1922, 4122, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2499677642317843, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.5489091277122498}, {"id": 161, "seek": 161200, "start": 1623.0, "end": 1624.0, "text": " Well, that's right.", "tokens": [50914, 1042, 11, 300, 311, 558, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2499677642317843, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.5489091277122498}, {"id": 162, "seek": 161200, "start": 1624.0, "end": 1625.0, "text": " Yes.", "tokens": [50964, 1079, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2499677642317843, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.5489091277122498}, {"id": 163, "seek": 161200, "start": 1625.0, "end": 1626.0, "text": " Really difficult to like.", "tokens": [51014, 4083, 2252, 281, 411, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2499677642317843, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.5489091277122498}, {"id": 164, "seek": 161200, "start": 1626.0, "end": 1627.0, "text": " That's right.", "tokens": [51064, 663, 311, 558, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2499677642317843, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.5489091277122498}, {"id": 165, "seek": 161200, "start": 1627.0, "end": 1628.0, "text": " That's right.", "tokens": [51114, 663, 311, 558, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2499677642317843, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.5489091277122498}, {"id": 166, "seek": 161200, "start": 1628.0, "end": 1632.0, "text": " So like in this case, that was specifically kind of cut, right, to have any less features, right.", "tokens": [51164, 407, 411, 294, 341, 1389, 11, 300, 390, 4682, 733, 295, 1723, 11, 558, 11, 281, 362, 604, 1570, 4122, 11, 558, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2499677642317843, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.5489091277122498}, {"id": 167, "seek": 161200, "start": 1632.0, "end": 1637.0, "text": " Yes, it's because, you know, 1500 or like, well, like, you know, 3000 tried for large.", "tokens": [51364, 1079, 11, 309, 311, 570, 11, 291, 458, 11, 22671, 420, 411, 11, 731, 11, 411, 11, 291, 458, 11, 20984, 3031, 337, 2416, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2499677642317843, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.5489091277122498}, {"id": 168, "seek": 161200, "start": 1637.0, "end": 1640.0, "text": " Yeah, that would be too many to display.", "tokens": [51614, 865, 11, 300, 576, 312, 886, 867, 281, 4674, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2499677642317843, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.5489091277122498}, {"id": 169, "seek": 164000, "start": 1640.0, "end": 1641.0, "text": " Okay.", "tokens": [50364, 1033, 13, 50414], "temperature": 0.6, "avg_logprob": -0.6003577444288466, "compression_ratio": 1.1923076923076923, "no_speech_prob": 0.4334813058376312}, {"id": 170, "seek": 164000, "start": 1641.0, "end": 1642.0, "text": " Thank you.", "tokens": [50414, 1044, 291, 13, 50464], "temperature": 0.6, "avg_logprob": -0.6003577444288466, "compression_ratio": 1.1923076923076923, "no_speech_prob": 0.4334813058376312}, {"id": 171, "seek": 164000, "start": 1642.0, "end": 1643.0, "text": " Thank you.", "tokens": [50464, 1044, 291, 13, 50514], "temperature": 0.6, "avg_logprob": -0.6003577444288466, "compression_ratio": 1.1923076923076923, "no_speech_prob": 0.4334813058376312}, {"id": 172, "seek": 164000, "start": 1643.0, "end": 1644.0, "text": " you", "tokens": [50514, 291, 50564], "temperature": 0.6, "avg_logprob": -0.6003577444288466, "compression_ratio": 1.1923076923076923, "no_speech_prob": 0.4334813058376312}], "language": "en"}